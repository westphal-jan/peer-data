{"id": "1602.04799", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Quantum Perceptron Models", "abstract": "we demonstrate how quantum computation can continually provide non - trivial such improvements considerably in the advanced computational and statistical complexity of the perceptron model. we develop out two quantum algorithms for perceptron learning. the first algorithm exploits quantum information processing to determine a separating hyperplane using a number of error steps sublinear in the number rank of data points $ n $, namely $ o ( \\ sqrt { n } ) $. the second algorithm illustrates how the classical errors mistake bound of $ < o ( \\ frac { \u00a7 1 } { \\ gamma ^ 2 } ) $ can be further improved only to $'o ( \\ frac { 1 } { \\ sqrt { \\ gamma } } ) $ through quantum means, \\ where $ \\ gamma $ rr denotes the margin. such improvements are achieved through the application of quantum amplitude amplification protocols to the version space optimal interpretation of the perceptron error model.", "histories": [["v1", "Mon, 15 Feb 2016 20:45:35 GMT  (173kb,D)", "http://arxiv.org/abs/1602.04799v1", null]], "reviews": [], "SUBJECTS": "quant-ph cs.LG stat.ML", "authors": ["ashish kapoor", "nathan wiebe", "krysta marie svore"], "accepted": true, "id": "1602.04799"}, "pdf": {"name": "1602.04799.pdf", "metadata": {"source": "CRF", "title": "Quantum Perceptron Models", "authors": ["Nathan Wiebe", "Ashish Kapoor", "Krysta M. Svore"], "emails": [], "sections": [{"heading": null, "text": "Quantum Perceptron Models\nNathan Wiebe, Ashish Kapoor, Krysta M. Svore Microsoft Research, One Microsoft Way, Redmond WA 98052\nWe demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N , namely O( \u221a N). The second algorithm illustrates how the classical mistake bound of O( 1\n\u03b32 ) can be further\nimproved to O( 1\u221a \u03b3 ) through quantum means, where \u03b3 denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.\nI. INTRODUCTION\nQuantum computation is an emerging technology that utilizes quantum effects to achieve significant, and in some cases exponential, speed-ups of algorithms over their classical counterparts. The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].\nWhile a number of important quantum speedups have been found, the majority of these speedups are due to replacing a classical subroutine with an equivalent albeit faster quantum algorithm. The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22]. Here we consider an alternate approach: we devise a new machine learning algorithm that is tailored to the speedups that quantum computers can provide.\nWe illustrate our approach by focusing on perceptron training [18]. The perceptron is a fundamental building block for various machine learning models including neural networks and support vector machines [20]. Unlike many other machine learning algorithms, tight bounds are known for the computational and statistical complexity of traditional perceptron training. Consequently, we are able to rigorously show different performance improvements that stem from either using quantum computers to improve traditional perceptron training or from devising a new form of perceptron training that aligns with the capabilities of quantum computers.\nWe provide two quantum approaches to perceptron training. The first approach focuses on the computational aspect of the problem and the proposed method quadratically reduces the scaling of the complexity of training with respect to the number of training vectors. The second algorithm focuses on statistical efficiency. In particular, we use the mistake bounds for traditional perceptron training methods and ask if quantum computation lends any advantages. To this end, we propose an algorithm that quadratically improves the scaling of the training algorithm with respect to the margin between the classes in the training data. The latter algorithm combines quantum amplitude estimation in the version space interpretation of the perceptron learning problem. Our approaches showcase the trade-offs that one can consider in developing quantum algorithms, and the ultimate advantages of performing learning tasks on a quantum computer.\nThe rest of the paper is organized as follows: we first cover the background on perceptrons, version space and Grover search. We then present our two quantum algorithms and provide analysis of their computational and statistical efficiency before concluding."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Perceptrons and Version Space", "text": "Given a set of N separable training examples {\u03c61, .., \u03c6N} \u2208 RD with corresponding labels {y1, .., yN}, yi \u2208 {+1,\u22121}, the goal of perceptron learning is to recover a hyperplane w that perfectly classifies the training set [18]. Formally, we want w such that yi \u00b7 wT\u03c6i > 0 for all i. There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.\nA remarkable feature of the perceptron model is that upper bounds exist for the number of updates that need to be made during this training procedure. In particular, if the training data is composed of unit vectors, \u03c6i \u2208 RD, that are separated by a margin of \u03b3 then there are perceptron training algorithms that make at most O( 1\u03b32 ) mistakes [16],\nar X\niv :1\n60 2.\n04 79\n9v 1\n[ qu\nan t-\nph ]\n1 5\nFe b\n20 16\n2 Feature Space Version Space\nindependent of the dimension of the training vectors. Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19]. Note that in the worst case, the algorithm will need to look at all points in the training set at least once, consequently the computation complexity will be O(N).\nOur goal is to explore if the quantum procedures can provide improvements both in terms of computational complexity (that is better than O(N)) and statistical efficiency (improve upon O( 1\u03b32 ). Instead of solely applying quantum constructs to the feature space, we also consider the version space interpretation of perceptrons which leads to the improved scaling with \u03b3.\nFormally, version space is defined as the set of all possible hyperplanes that perfectly separate the data: VS := {w|yi \u00b7 wT\u03c6i > 0 for all i}. Given a training datum, the traditional representation is to depict data as points in the feature space and use hyperplanes to depict the classifiers. However, there exists a dual representation where the hyperplanes are depicted as points and the data points are represented as hyperplanes that induce constraints on the feasible set of classifiers. Figure 1, which is borrowed from [14], illustrates the version space interpretation of perceptrons. Given three labeled data points in a 2D space, the dual space illustrates the set of normalized hyperplanes as a yellow ball with unit radius. The third dimension corresponds to the weights that multiply the two dimensions of the input data and the bias term. The planes represent the constraints imposed by observing the labeled data as every labeled data renders one-half of the space infeasible. The version space is then the intersection of all the half-spaces that are valid. Naturally, classifiers including SVMs [20] and Bayes point machines [10] lie in the version space.\nWe note that there are quantum constructs such as Grover search and amplitude amplification which provide nontrivial speedups for the search task. This is the main reason why we resort to the version space interpretation. We can use this formalism to simply pose the problem of determining the separating hyperplane as a search problem in the dual space. For example given a set of candidates hyperplanes, our problem reduces to searching amongst the sample set for the classifier that will successfully classify the entire set. Therefore training the perceptron is equivalent to finding any feasible point in the version space. We describe these quantum constructs in detail below."}, {"heading": "B. Grover\u2019s Search", "text": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5]. Rather than sampling from a probability distribution until a given marked element is found, the Grover search algorthm draws only one sample and then uses quantum operations to modify the distribution from which it sampled. The probability distribution is rotated, or more accurately the quantum state that yields the distribution is rotated, into one whose probability is sharply concentrated on the marked element. Once a sharply peaked distribution is identified, the marked item can be found using just one sample. In general, if the probability of finding such an element is known to be a then amplitude amplification requires O( \u221a 1/a) operations to find the marked item with certainty. While Grover\u2019s search is a quantum subroutine, it can in fact be understood using only geometric arguments. The only notions from quantum mechanics used are those of the quantum state vector and that of Born\u2019s rule\n(measurement). A quantum state vector is a complex unit vector whose components have magnitudes that are equal to the square\u2013roots of the probabilities. In particular, if v is a quantum state vector and p is the corresponding probability distribution then\np = v\u2020 \u25e6 v, (1)\nwhere the unit column vector v is called the quatum state vector which sits in the vector space Cn, \u25e6 is the Hadamard (pointwise) product and \u2020 is the complex conjugate transpose. A quantum state can be measured such that if we have a quantum state vector v and a basis vector w then the probability of measuring v = w is |\u3008v, w\u3009|2, where \u3008\u00b7, \u00b7\u3009 denotes the inner product. One of the main differences between quantum and classical distributions is that the probability distribution resulting from measurement depends strongly on the basis in which the vector is measured. This basis dependence of measurement is the root of many of the differences between quantum and classical probability theory and also gives rise to many celebrated results in the foundations of quantum mechanics such as Bell\u2019s theorem [3].\nAt first glance, introducing the quantum state vector v may not seem to provide any advantages over working with p for the purposes of sampling. More careful consideration reveals that the fact that v is complex valued allows transformations on v to be performed that cannot be performed on p. In particular, we can reflect the quantum state vector about any axis, whereas we cannot do the same to p without violating its positivity. Grover\u2019s search, in fact, is a cunning way to perform a series of reflections on v to bias p towards the marked state we wish to find. While such reflections may not make sense from a classical perspective, quantum computers can be used to realize them efficiently.\nThe key feature of a quantum computer is that it permits any unitary transformation to be performed on the unit vector v, within arbitrarily small approximation error. We define the initial quantum state vector to be \u03c8 and define P to be a projection matrix onto a set of configurations that we want to find. In particular, if we define \u03bdgood to be the set of all items that we want the quantum algorithm to find then\nP\u03c8 = { \u03c8, if \u03c8 \u2208 \u03bdgood 0, otherwise\n(2)\nHere being able to apply P does not imply that \u03bdgood is known. Instead, it implies that a subroutine that checks to see if \u03c8 \u2208 \u03bdgood exists. The fact that P is implemented by a linear transformation of the state vector also allows it to be simultaneously applied to exponentially many v via P\u03c8 = 1\u2016a\u2016 \u2211N j=1 ajv = 1 \u2016a\u2016 \u2211N j=1 ajPv. These two features allow a single application of 1 \u2212 2P to be efficiently applied, assuming membership in vgood can be efficiently tested, even though \u03c8 is a sum of exponentially many basis vectors.\nIn order to perform the search algorithm we need to implement two unitary operations:\nUinit = 2\u03c8\u03c8 \u2020 \u2212 1 , Utarg = 1 \u2212 2P. (3)\nThe operators Uinit and Utarg can be interpreted geometrically as reflections within a two\u2013dimensional space spanned by the vectors \u03c8 and P\u03c8. If we assume that P\u03c8 6= 0 and P\u03c8 6= \u03c8 then these two reflection operations can be used to rotate \u03c8 in the space span(\u03c8, P\u03c8). Specifically this rotation is Ugrover = UinitUtarg. Its action is illustrated in Figure 2. If the angle between the vector \u03c8 and P\u03c8/\u2016P\u03c8\u2016 is \u03c0/2\u2212 \u03b8a, where \u03b8a := sin\u22121(|\u3008\u03c8, P\u03c8/\u2016P\u03c8\u2016\u3009|). It then follows from elementary geometry and the rule for computing the probability distribution from a quantum state (known as Born\u2019s rule) that after j iterations of Grover\u2019s algorithm the probability of measuring a desirable outcome is\np(v \u2208 \u03bdgood|j) = sin2((2j + 1)\u03b8a). (4)\n4 It is then easy to see that if \u03b8a 1 and a probability of success greater than 1/4 is desired then j \u2208 O(1/ \u221a \u03b8a) suffices to find a marked outcome. This is quadratically faster than is possible from statistical sampling which requires O(1/\u03b8a) samples on average.\nAs an example, if the initial success probability is 1/4 then \u03b8a = sin \u22121(1/2) = \u03c0/6. Therefore if we take j = 1 then p(v \u2208 \u03bdgood|j) = 1. As a result a desirable outcome can be found after only 3 quantum operations whereas 4 samples from the initial distribution would be needed on average to find a marked outcome if quantum methods were not used.\nIf on the other hand, the success probability were 1/2 then \u03b8a = \u03c0/4 and sin 2((2j + 1)\u03c0/4) = 1/2 for all j. This problem can be easily addressed by doing something that would not make any sense classically: purposefully lowering the success probability to 1/4 by requiring a new event w = (v, u) where we define w to be a good state if the independent variables v is good and u \u223c Bern(1/2) is 0. The independence assumption means that the probability that both conditions are satisfied is 1/4 and hence a good v can be found with certainty by applying amplitude amplification on w. More generally, if \u03b8a is known then this trick can be applied to make \u03b8a 7\u2192 \u03c0/(2[2j + 1]) (for positive integer j) which makes the search procedure deterministic.\nOn the other hand, if \u03b8a is not known then it isn\u2019t clear how j should be chosen to make the success probability greater than 1/4. Fortunately, methods are known to deal with such issues [4, 5]. The simplest one exploits the fact that the average of p over a range of j = 0, . . . ,M \u2212 1 can be easily computed:\np(v \u2208 \u03bdgood;M) = 1M \u2211M\u22121 j=0 sin 2((2j + 1)\u03b8a)\n= 12 ( 1\u2212 sin(4M\u03b8a)2M sin(2\u03b8a) ) . (5)\nIf M \u2265M0 := 1sin(2\u03b8a) then it is straight forward to see that\n1\n2\n( 1\u2212 sin(4M\u03b8a)\n2M sin(2\u03b8a)\n) \u2265 1\n2\n( 1\u2212 1\n2M sin(2\u03b8a)\n) \u2265 1\n4 . (6)\nThe average probability is then guaranteed to be at least 1/4 if j is chosen to be drawn uniformly from {0, . . . ,M \u22121} if M \u2265 M0. If a lower bound on \u03b8a is known a good sample can be drawn, then an appropriate value of M can be computed.\nIf no lower bound on \u03b8a is known then a marked element can nonetheless be found with high probability by exponential searching. Exponential searching involves, for step i taking M = ci for some c \u2208 (1, 2). After a logarithmic number of applications of amplitude amplification it will attainM \u2265M0 with high probability. After which the average success probability is known to be bounded below by 1/4 and the algorithm will succeed with high probability in a constant number of attempts. Thus the quadratic speedup holds even if the success probability is not known apriori."}, {"heading": "III. ONLINE QUANTUM PERCEPTRON", "text": "Now that we have discussed Grover\u2019s search we turn our attention to applying it to speed up online perceptron training. In order to do so, we first need to define the quantum model that we wish to use as our quantum analogue of perceptron training. While there are many ways of defining such a model but the following approach is perhaps the most direct. Although the traditional feature space perceptron training algorithm is online [16], meaning that the training examples are provided one at a time to it in a streaming fashion, we deviate from this model slightly by instead requiring that the algorithm be fed training examples that are, in effect, sampled uniformly from the training set. This is a slightly weaker model, as it allows for the possibility that some training examples will be drawn multiple times. However, the ability to draw quantum states that are in a uniform superposition over all vectors in the training set enables quantum computing to provide advantages over both classical methods that use either access model.\nWe assume without loss of generality that the training set consists of N unit vectors, \u03c61, . . . , \u03c6N . If we then define \u03a61, . . . ,\u03a6N to be the basis vectors whose indices each coincide with a (B+ 1)-bit representation of the corresponding (\u03c6j , yj) where yj \u2208 {\u22121, 1} is the class assigned to \u03c6j and let \u03a60 be a fixed unit vector that is chosen to represent a blank memory register.\nWe introduce the vectors \u03a6j to make it clear that the quantum vectors states used to represent training vectors do not live in the same vector space as the training vectors themselves. We choose the quantum state vectors here to occupy a larger space than the training vectors because the Heisenberg uncertainty principle makes it much more difficult for a quantum computer to compute the class that the perceptron assigns to a training vector in such cases.\nFor example, the training vector (\u03c6j , yj) \u2261 ([0, 0, 1, 0]T , 1) can be encoded as an unsigned integer 00101 \u2261 5, which in turn can be represented by the unit vector \u03a6 = [0, 0, 0, 0, 0, 1]T . More generally, if \u03c6j \u2208 RD were a vector of\n5 Algorithm 1 Online quantum perceptron training algorithm\nfor k = 1, . . . , dlog3/4 \u03b32 e do for j = 1 : dlogc(1/ sin(2 sin\u22121(1/ \u221a N)))e do Draw m uniformly from {0, . . . , dcje}. Prepare quantum state \u03a8. \u03a8\u2190 ((2\u03a8\u03a8\u2020 \u2212 1 )Fw)m\u03a8. Measure \u03a8, assume outcome is uq. (\u03c6, y)\u2190 Uc(q). if fw(\u03c6, y) = 1 then Return w\u2032 \u2190 w + y\u03c6 end if end for end for Return w\u2032\nfloating point numbers then a similar vector could be constructed by concatenating the binary representations of the D floating point numbers that comprise it with (yj + 1)/2 and express the bit string as an unsigned integer, Q. The integer can then be expressed as a unit vector \u03a6 : [\u03a6]q = \u03b4q,Q. While encoding the training data as an exponentially long vector is inefficient in a classical computer, it is not in a quantum computer because of the quantum computer\u2019s innate ability to store and manipulate exponentially large quantum state vectors.\nAny machine learning algorithm, be it quantum or classical, needs to have a mechanism to access the training data. We assume that the data is accessed via an oracle that not only accesses the training data but also determines whether the data is misclassified. To clarify, let {uj : j = 1 : N} be an orthonormal basis of quantum state vectors that serve as addresses for the training vectors in the database. Given an input address for the training datum, the unitary operations U and U\u2020 allow the quantum computer to access the corresponding vector. Specifically, for all j\nU [uj \u2297 \u03a60] = uj \u2297 \u03a6j U\u2020[uj \u2297 \u03a6j ] = uj \u2297 \u03a60. (7)\nGiven an input address vector uj , the former corresponds to a database access and the latter inverts the database access. Note that because U and U\u2020 are linear operators we have that U \u2211N j=1 uj\u2297\u03a60 = \u2211 j uj\u2297\u03a6j . A quantum computer can therefore access each training vector simultaneously using a single operation, while only requiring enough memory to store one of the \u03a6j . The resultant vector is often called in the physics literature a quantum superposition of states and this feature of linear transformations is referred to as quantum parallelism within quantum computing.\nThe next ingredient that we need is a method to test if the perceptron correctly assigns a training vector addressed by a particular uj . This process can be pictured as being performed by a unitary transformation that flips the sign of any basis-vector that is misclassified. By linearity, a single application of this process flips the sign of any component of the quantum state vector that coincides with a misclassified training vector. It therefore is no more expensive than testing if a given training vector is misclassified in a classical setting. We denote the operator, which depends on the perceptron weights w, Fw and require that\nFw[uj \u2297 \u03a60] = (\u22121)fw(\u03c6j ,yj)[uj \u2297 \u03a60], (8)\nwhere fw(\u03c6j) is a Boolean function that is 1 if and only if the perceptron with weights w misclassifies training vector \u03c6j . Since the classification step involves computing the dot\u2013products of finite size vectors, this process is efficient given that the \u03a6j are efficiently computable.\nWe apply Fw in the following way. Let Fw be a unitary operation such that\nFw\u03a6j = (\u22121)fw(\u03c6j ,yj)\u03a6j . (9)\nFw is easy to implement in the quantum computer using a multiply controlled phase gate and a quantum implementation of the perceptron classification algorithm, fw. We can then write\nFw = U \u2020(1 \u2297Fw)U. (10)\nClassifying the data based on the phases (the minus signs) output by Fw naturally leads to a very memory efficient training algorithm because only one training vector is ever stored in memory during the implementation of Fw given in (10). We can then use Fw to perform Grover\u2019s search algorithm, by taking Utarg = Fw and Uinit = 2\u03c8\u03c8 \u2020 \u2212 1\n6 with \u03c8 = \u03a8 := 1\u221a N \u2211N j=1 uj , to seek out training vectors that the current perceptron model misclassifies. This leads to a quadratic reduction in the number of times that the training vectors need to be accessed by Fw or its classical analogue.\nIn the classical setting, the natural object to query is slightly different. The oracle that is usually assumed in online algorithms takes the form U c : Z 7\u2192 CD where\nU c(j) = \u03c6j . (11)\nWe will assume that a similar function exists in both the classical and the quantum settings for simplicity. In both cases, we will consider the cost of a query to U c to be proportional to the cost of a query to Fw.\nWe use these operations in Algorithm 1 to implement a quantum search for training vectors that the perceptron misclassifies. This leads to a quadratic speedup relative to classical methods as shown in the following theorem.\nTheorem 1. Given a training set that consists of unit vectors \u03a61, . . . ,\u03a6N that are separated by a margin of \u03b3 in feature space, the number of applications of Fw needed to infer a perceptron model, w, such that P (\u2203 j : fw(\u03c6j) = 1) \u2264 using a quantum computer is Nquant where\n\u2126( \u221a N) 3 Nquant \u2208 O\n(\u221a N\n\u03b32 log\n[ 1\n\u03b32\n]) ,\nwhereas the number of queries to fw needed in the classical setting, Nclass, where the training vectors are found by sampling uniformly from the training data is bounded by\n\u2126(N) 3 Nclass \u2208 O ( N\n\u03b32 log\n[ 1\n\u03b32\n]) .\nWe assume in Theorem 1 that the training data in the classical case is accessed in a manner that is analogous to the sampling procedure used in the quantum setting. If instead the training data is supplied by a stream (as in the standard online model) then the upper bound changes to Nclass \u2208 O(N/\u03b32) because all N training vectors can be deterministically checked to see if they are correctly classified by the perceptron. A quantum advantage is therefore obtained if N log2(1/ \u03b32).\nIn order to prove Theorem 1 we need to have two technical lemmas (proven in the appendix). The first bounds the complexity of the classical analogue to our training method:\nLemma 1. Given only the ability to sample uniformly from the training vectors, the number of queries to fw needed to find a training vector that the current perceptron model fails to classify correctly, or conclude that no such example exists, with probability 1\u2212 \u03b32 is at most O(N log(1/ \u03b32)).\nThe second proves the correctness of Algorithm 1 and bounds the complexity of the algorithm:\nLemma 2. Assuming that the training vectors {\u03c61, . . . , \u03c6N} are unit vectors and that they are drawn from two classes separated by a margin of \u03b3 in feature space, Algorithm 2 will either update the perceptron weights, or conclude that the current model provides a separating hyperplane between the two classes, using a number of queries to Fw that is bounded above by O( \u221a N log(1/ \u03b32)) with probability of failure at most \u03b32.\nAfter stating these results, we can now provide the proof of Theorem 1.\nProof of Theorem 1. The upper bounds follow as direct consequences of Lemma 2 and Lemma 1. Novikoff\u2019s theorem [6, 16] states that the algorithms described in both lemmas must be applied at most 1/\u03b32 times before finding the result. However, either the classical or the quantum algorithm may fail to find a misclassified vector at each of the O(1/\u03b32) steps. The union bound states that the probability that this happens is at most the sum of the respective probabilities in each step. These probabilities are constrained to be \u03b32 , which means that the total probability of failing to correctly find a mistake is at most if both algorithms are repeated 1/\u03b32 times (which is the worst case number of times that they need to be repeated).\nThe lower bound on the quantum query complexity follows from contradiction. Assume that there exists an algorithm that can train an arbitrary perceptron using o( \u221a N) query operations. Now we want to show that unstructured search with one marked element can be expressed as a perceptron training algorithm. Let w be a known set of perceptron weights and assume that the perceptron only misclassifies one vector \u03c61. Thus if perceptron training succeeds then w the value of \u03c61 can be extracted from the updated weights. This training problem is therefore equivalent to searching for a misclassified vector. Now let \u03c6j = [1 \u2295 F (j), F (j)]T \u2297 \u03c7j where \u03c7j is a unit vector that represents the bit string j and F (j) is a Boolean function. Assume that F (0) = 1 and F (j) = 0 if j 6= 0, which is without\n7 loss of generality equivalent to Grover\u2019s problem [4, 9]. Now assume that \u03c6j is assigned to class 2F (j) \u2212 1 and take w = [1/ \u221a 2, 1/ \u221a\n2]T \u2297 1\u221a N \u2211 j \u03c7j . This perceptron therefore misclassifies \u03c60 and no other vector in the training set.\nThus updating the weights yields \u03c6j , which in turn yields the value of j such that F (j) = 1, and therefore Grover\u2019s search reduces to perceptron training.\nSince Grover\u2019s search reduces to perceptron training in the case of one marked item the lower bound of \u2126( \u221a N) queries for Grover\u2019s search [4] applies to perceptron training. Since we assumed that perceptron training requires o( \u221a N) queries this is a contradiction. Thus the true lower bound must be \u2126( \u221a N).\nWe have assumed that in the classical setting that the user only has access to the training vectors through an oracle that is promised to draw a uniform sample from {(\u03c61, y1), . . . , (\u03c6N , yN )}. Since we are counting the number of queries to fw it is clear that in the worst possible case that the training vector that the perceptron makes a mistake on can be the last unique value sampled from this list. Thus if the query complexity were o(N) there would be a contradiction, hence the query complexity is \u2126(N) classically."}, {"heading": "IV. QUANTUM VERSION SPACE PERCEPTRON", "text": "The strategy for our quantum version space training algorithm is to pose the problem of determining a separating hyperplane as search. Specifically, the idea is to first generate K sample hyperplanes w1, . . . , wK from a spherical Gaussian distribution N (0, 1 ). Given a large enough K, we are guaranteed to have at least one hyperplane amongst the samples that would lie in the version space and perfectly separate the data. As discussed earlier Grover\u2019s algorithm can provide quadratic speedup over the classical search consequently the efficiency of the algorithm is determined by K. Theorem 2 provides an insight on how to determine this number of hyperplanes to be sampled.\nTheorem 2. Given a training set that consists of d-dimensional unit vectors \u03a61, . . . ,\u03a6N with labels y1, . . . , yN that are separated by a margin of \u03b3 in feature space, then a D-dimensional vector w sampled from N (0, 1 ) perfectly separates the data with probability \u0398(\u03b3).\nThe proof of this theorem is provided in the supplementary material. The consequence of Theorem 2 stated below is that the expected number of samples K, required such that a separating hyperplane exists in the set, only needs to scale as O( 1\u03b3 ). Thus if amplitude amplification is used to boost the probability of finding a vector in the version space then the resulting quantum algorithm will need only O( 1\u221a\u03b3 ) quantum steps on average.\nNext we show how to use Grover\u2019s algorithm to search for a hyperplane that lies in the version space. Let us take K = 2m, for positive integer m. Then given w1, . . . , wK be the sampled hyperplanes, we represent W1, . . . ,WK to be vectors that encode a binary representation of these random perceptron vectors. In analogy to \u03a60, we also define W0 to be a vector that represents an empty data register. We define the unitary operator V to generate these weights given an address vector uj using the following\nV [uj \u2297W0] = [uj \u2297Wj ]. (12)\nIn this context we can also think of the address vector, uj , as representing a seed for a pseudo\u2013random number generator that yields perceptron weights Wj .\nAlso let us define the classical analogue of V to be V c which obeys V c(j) = wj . Now using V (and applying the Hadamard transform [15]) we can prepare the following quantum state\n\u03a8 := 1\u221a K K\u2211 k=1 uk \u2297Wk, (13)\nwhich corresponds to a uniform distribution over the randomly chosen w. Now that we have defined the initial state, \u03a8, for Grover\u2019s search we need to define an oracle that marks the vectors inside the version space. Let us define the operator F\u0302\u03c6,y via\nF\u0302\u03c6,y[uj \u2297W0] = (\u22121)1+fwj (\u03c6,y)[uj \u2297W0]. (14)\nThis unitary operation looks at an address vector, uj , computes the corresponding perceptron model Wj , flips the sign of any component of the quantum state vector that is in the half space in version space specified by \u03c6 and then uncomputes Wj . This process can be realized using a quantum subroutine that computes fw, an application of V and V \u2020 and also the application of a conditional phase gate (which is a fundamental quantum operation that is usually denoted Z) [15].\n8 Algorithm 2 Quantum version space perceptron training algorithm\nfor k = 1, . . . , dlog3/4 e do for j = 1 : dlogc(1/ sin(2 sin\u22121(1/ \u221a K)))e do Draw m uniformly from {0, . . . , dcje}. Prepare quantum state \u03a8 = 1\u221a\nK\n\u2211K p=1 up \u2297W0.\n\u03a8\u2190 ((2\u03a8\u03a8\u2020 \u2212 1 )G)m\u03a8. Measure \u03a8, assume outcome is uq. w \u2190 V c(q). if fw(\u03c6`, y`) = 0 for all ` \u2208 {1, . . . , N} then Return w end if end for end for Return w = 0\nThe oracle F\u0302\u03c6,y does not allow us to directly use Grover\u2019s search to rotate a quantum state vector that is outside the version space towards the version space boundary because it effectively only checks one of the half\u2013space inequalities that define the version space. It can, however, be used to build an operation, G\u0302, that reflects about the version space:\nG\u0302[uj \u2297W0] = (\u22121)1+(fwj (\u03c61,y1)\u2228\u00b7\u00b7\u00b7\u2228fwj (\u03c6N ,yN ))[uj \u2297W0]. (15)\nThe operation G\u0302 can be implemented using 2N applications of F\u0302\u03c6 as well as a sequence of O(N) elementary quantum gates, hence we cost a query to G\u0302 as O(N) queries to F\u0302\u03c6,y.\nWe use these components in Algorithm 2 to, in effect, amplify the margin between the two classes from \u03b3 to \u221a \u03b3.\nWe give the asymptotic scaling of this algorithm in the following theorem (see appendix for proof).\nTheorem 3. Given a training set that consists of unit vectors \u03a61, . . . ,\u03a6N that are separated by a margin of \u03b3 in feature space, the number of queries to F\u0302\u03c6,y needed to infer a perceptron model with probability at least 1\u2212 , w, such that w is in the version space using a quantum computer is Nquant where\nNquant \u2208 O ( N \u221a \u03b3 log3/2 [ 1 ]) .\nProof. The proof of the theorem follows directly from bounds on K and the validity of Algorithm 2. It is clear from previous discussions that Algorithm 2 carries out Grover\u2019s search, but instead of searching for a \u03c6 that is misclassified it instead searches for a w in version space. Its validity therefore follows by following the exact same steps followed in the proof of Lemma 2 but with N = K. However, since the algorithm need is not repeated 1/\u03b32 times in this context we can replace \u03b3 with 1 in the proof. Thus if we wish to have a probability of failure of at most \u2032 then the number of queries made to G\u0302 is in\nO( \u221a K log(1/ \u2032)).\nThis also guarantees that if any of the K vectors are in the version space then the probability of failing to find that vector is at most \u2032.\nNext since one query to G\u0302 is costed at N queries to F\u0302\u03c6,y the query complexity (in units of queries to F\u0302\u03c6,y) becomes O(N \u221a K log(1/ \u2032)). The only thing that then remains is to bound the value of K needed.\nThe probability of finding a vector in the version space is \u0398(\u03b3) from Theorem 2. This means that there exists \u03b1 > 0 such that the probability of failing to find a vector in the version space K times is at most\n(1\u2212 \u03b1\u03b3)K \u2264 e\u2212\u03b1\u03b3K . (16)\nThus this probability is at most \u03b4 for\nK \u2208 \u2126 ( 1\n\u03b3 log(1/\u03b4)\n) . (17)\nIt then suffices to pick K \u2208 \u0398 (\n1 \u03b3 log(1/\u03b4)\n) for the algorithm.\nThe union bound implies that the probability that either none of the vectors lie in the version space or that Grover\u2019s search failing to find such an element is at most \u2032 + \u03b4 \u2264 . Thus it suffices to pick \u2032 \u2208 \u0398( ) and \u03b4 \u2208 \u0398( ) to ensure that the total probability is at most . Therefore the total number of queries made to F\u0302\u03c6,y is in O( N\u221a\u03b3 log 3/2(1/ )) as claimed.\n9 The classical algorithm discussed previously has complexity O(N log(1/ )/\u03b3), which follows from the fact (proven in the appendix) that K \u2208 \u0398(log(1/ )/\u03b3) suffices to make the probability of not drawing an element of the version space at most . This demonstrates a quantum advantage if 1\u03b3 log(1/ ), and illustrates that quantum computing can be used to boost the effective margins of the training data. Quantum models of perceptrons therefore not only provide advantages in terms of the number of vectors that need to be queried in the training process, they also can make the perceptron much more perceptive by making training less sensitive to small margins.\nThese performance improvements can also be viewed as mistake bounds for the version space perceptron. The inner loop in Algorithm 2 attempts to sample from the version space and then once it draws a sample it tests it against the training vectors to see if it errs on any example. Since the inner loop is repeated O( \u221a K log(1/ )) times, the maximum number of misclassified vectors that arises from this training process is from Theorem 2 O( 1\u221a\u03b3 log 3/2(1/ )) which, for constant , constitutes a quartic improvement over the standard mistake bound of 1/\u03b32 [16]."}, {"heading": "V. CONCLUSION", "text": "We have provided two distinct ways to look at quantum perceptron training that each afford different speedups relative to the other. The first provides a quadratic speedup with respect to the size of the training data. We further show that this algorithm is asymptotically optimal in that if a super\u2013quadratic speedup were possible then it would violate known lower bounds for quantum searching. The second provides a quadratic reduction in the scaling of the training time (as measured by the number of interactions with the training data) with the margin between the two classes. This latter result is especially interesting because it constitutes a quartic speedup relative to the typical perceptron training bounds that are usually seen in the literature.\nPerhaps the most significant feature of our work is that it demonstrates that quantum computing can provide provable speedups for perceptron training, which is a foundational machine learning method. While our work gives two possible ways of viewing the perceptron model through the lens of quantum computing, other quantum variants of the perceptron model may exist. Seeking new models for perceptron learning that deviate from these classical approaches may not only provide a deeper understanding of what form learning takes within quantum systems, but also may lead to richer classes of quantum models that have no classical analogue and are not efficiently simulatable on classical hardware. Such models may not only revolutionize quantum learning but also lead to a deeper understanding of the challenges and opportunities that the laws of physics place on our ability to learn."}, {"heading": "Appendix A: Proofs", "text": "Here we provide proofs of several of our results stated in the main body. In particular, we give the proofs of Theorem 2, Lemma 1 and Lemma 2 here.\nProof of Theorem 2. Given that the margin of the training set if \u03b3 there exist a hyperplane u such that yi \u00b7 uT\u03a6i > \u03b3 for all i. If w be a sample from N (0, 1 ), then lets first compute what is the probability that perturbing the maximum margin classifier u by amount w would lead still lead to a perfect separation. If we consider a data point \u03a6\u2217 that lies on the margin, i.e. yi \u00b7uT\u03a6\u2217 = \u03b3, we are interested in the probability that yi \u00b7(u+w)T\u03a6\u2217 > 0 and yi \u00b7(u+w)T\u03a6\u2217 < 2\u03b3. The first inequality corresponds to preventing misclassification of \u03a6\u2217, while the second one corresponds to preventing misclassification of the point belonging to the other class and on the margin. This is same as asking what is the probability that:\n\u2212 \u03b3 < yi \u00b7 wT\u03a6\u2217 < \u03b3 (A1)\nLet us define zi := yi \u00b7 wT\u03a6\u2217 . Since w \u223c N (0, 1 ) and \u2016\u03a6\u2016 = 1 we can show that zi \u223c N (0, 1). Thus, we can write the probability that \u2212\u03b3 < zi < \u03b3 as:\nP (\u2212\u03b3 < zi < \u03b3) = erf ( \u03b3\u221a 2 ) (A2)\nHere erf(z) = 1\u221a \u03c0 \u222b z \u2212z e \u2212 x22 dx is the error function for the standard normal distribution. Since \u03a6\u2217 is on the margin, the probability that the sample w will lie in the version space can be simply characterized as the above probability P (\u2212\u03b3 < zi < \u03b3). It is straightforward to show using Maclaurin series expansion that:\nP (w\u2208VS) = 2\u221a \u03c0 ( \u03b3\u221a 2 \u2212 \u03b3 3 23/23 + \u03b35 25/210 \u2212 \u03b3 7 27/242 . . . ) (A3)\n10\nNote, that in our case \u03a6i are unit normalized for all i, thus \u03b3 < 1. Which in turn implies that most of the higher order terms will be close to zero in the limit of small \u03b3 and:\nP (w \u2208 VS) = \u03b3\u221a 2\u03c0 +O(\u03b33), (A4)\nwhich proves our theorem for \u03b3 < 1.\nProof of Lemma 1. There exists a simple algorithm for achieving this upper bound. Draw Ndlog(1/ \u03b32)e samples from the set of training vectors. If any are misclassified perform the update, otherwise report that the model classifies all the data.\nThe proof of validity of this algorithm is trivial and the success probability claim is also quite simple. Given that we draw k samples from the distribution the probability that any of them fail to detect a mistake, given such a mistake exists, is at most\n(1\u2212 1/N)k \u2264 exp(\u2212k/N). (A5)\nIf we want this error to be at most \u03b4 then it suffices to take\nk = dN log(1/\u03b4)e. (A6)\nOne query to fw is required per k, which means that k is also equal to the query complexity. Thus if at least one mistake occurs then the algorithm will find it with the aforementioned probability if \u03b4 = \u03b32. If such an example does not exist, then the algorithm will correctly conclude that a separating hyperplane has already been found. Therefore in either case the success probability is at least 1\u2212 \u03b32 as required.\nProof of Lemma 2. In order to see this, let us first examine the inner loop of Algorithm 2, which involves performing the update \u03a8 \u2190 ((2\u03a8\u03a8\u2020 \u2212 1 )Fw)m\u03a8. We know from our discussion of Grover\u2019s algorithm in the main body that if we define the initial probability of successfully find a mistake to be sin2(\u03b8a) then the probability of finding a j such that Fw\u03a6j = \u22121 after m updates is sin2((2m+ 1)\u03b8a). Since this corresponds to finding a vector that the perceptron fails to classify properly, these steps amplify the probability of finding a perceptron error. The query to U c that follows identifying the index of this training vector then converts this result into a classical bitstring that can then be used to perform a perceptron update. Therefore the inner loop performs a perceptron update with probability sin2((2m+ 1)\u03b8a) using m queries to Fw.\nUnder the assumption that c \u2208 (1, 2) the next loop repeats this sampling process until m \u2265M0 in order to ensure that the probability of finding a misclassified element is at least 1/4 [5]. This can be seen using the following argument. First we need to show that the exponential search heuristic requires O(M0) queries. Each iteration of the middle loop requires requires a number of queries that is at most proportional to dcie. Therefore the total number of queries is at most proportional to\nblogcM0c\u2211 i=0 dcie \u2264 c blogc M0c+1 c\u22121 + dlogcM0e\n\u2264 cc\u22121M0 + dlogcM0e. (A7)\nGiven c is a constant we have that (c\u2212 1) \u2208 \u0398(1) and thus \u2211blogcM0c i=0 c\ni \u2208 O(1/ sin(\u03b8a)) from (4) in the main body. If there exists an element that the algorithm makes a mistake on then \u03b8a \u2265 sin\u22121( \u221a 1/N) \u2208 \u2126(1/ \u221a N) because the lowest probability of success corresponds to the case where there is only one training vector that is misclassified out of N . From this we see that, if a misclassified vector exists, then the middle loop is repeated at least logc(M0) times which means that the final iteration taken corresponds to m \u2265M0 for the purposes of (6) in the main body. Therefore, under these assumptions, the probability that the middle loop updates the perceptron weights is at least 1/4 from [5]. Given that a mistake exists to be found the middle loop outputs such an element with a probability of failure that is at most 3/4 from (6) in the main body. Furthermore, O(M0) = O(1/ \u221a N) queries to Fw are required by the inner loop. The outer loop serves to amplify the success probability to at least 1\u2212 from the (average) success probability for m \u2265M0, which is at least 1/4, given that the perceptron makes a mistake on at least one training vector [5]. Let us assume that we repeat the middle loop of Algorithm 1 k times past this point and terminate searching for a marked state if the probability of failing to detect the element is at most \u03b4. Since the probability of the middle loop failing to find such an element, given that it exists, is at most 3/4 the probability of failing to find a marked state all k times is at most (3/4)k which implies that it suffices to choose\nk = dlog3/4(\u03b4)e. (A8)\n11\nGiven this error bound, the number of Grover iterations needed for the algorithm to find the marked element is\nO( \u221a N log3/4 \u03b4) \u2208 O( \u221a N log(1/\u03b4)).\nThe result then follows by taking \u03b4 = \u03b32. Therefore the lemma holds if \u03b8a > 0. If \u03b8a = 0 then the algorithm will never find a quantum state vector that the perceptron misclassifies and will\nsuccessfully conclude that there is not a marked state after O( \u221a N log(1/ \u03b32)) queries. Therefore the lemma also holds in the trivial case.\n[1] A\u0131\u0308meur, Esma, Brassard, Gilles, and Gambs, Se\u0301bastien. Machine learning in a quantum world. In Advances in artificial intelligence, pp. 431\u2013442. Springer, 2006. [2] Amin, Mohammad H, Andriyash, Evgeny, Rolfe, Jason, Kulchytskyy, Bohdan, and Melko, Roger. Quantum boltzmann machine. arXiv preprint arXiv:1601.02036, 2016. [3] Bell, John S. On the einstein podolsky rosen paradox, 1964. [4] Boyer, Michel, Brassard, Gilles, H\u00f8yer, Peter, and Tapp, Alain. Tight bounds on quantum searching. arXiv preprint\nquant-ph/9605034, 1996. [5] Brassard, Gilles, Hoyer, Peter, Mosca, Michele, and Tapp, Alain. Quantum amplitude amplification and estimation.\nContemporary Mathematics, 305:53\u201374, 2002. [6] Freund, Yoav and Schapire, Robert E. Large margin classification using the perceptron algorithm. Machine learning, 37\n(3):277\u2013296, 1999. [7] Garnerone, Silvano, Zanardi, Paolo, and Lidar, Daniel A. Adiabatic quantum algorithm for search engine ranking. Physical\nreview letters, 108(23):230506, 2012. [8] Gentile, Claudio. A new approximate maximal margin classification algorithm. The Journal of Machine Learning Research,\n2:213\u2013242, 2002. [9] Grover, Lov K. A fast quantum mechanical algorithm for database search. In Proceedings of the twenty-eighth annual\nACM symposium on Theory of computing, pp. 212\u2013219. ACM, 1996. [10] Herbrich, Ralf, Graepel, Thore, and Campbell, Colin. Bayes point machines: Estimating the bayes point in kernel space.\nIn IJCAI Workshop SVMs, pp. 23\u201327, 1999. [11] Li, Yaoyong, Zaragoza, Hugo, Herbrich, Ralf, Shawe-Taylor, John, and Kandola, Jaz. The perceptron algorithm with\nuneven margins. In ICML, volume 2, pp. 379\u2013386, 2002. [12] Lloyd, Seth, Mohseni, Masoud, and Rebentrost, Patrick. Quantum algorithms for supervised and unsupervised machine\nlearning. arXiv preprint arXiv:1307.0411, 2013. [13] Lloyd, Seth, Mohseni, Masoud, and Rebentrost, Patrick. Quantum principal component analysis. Nature Physics, 10(9):\n631\u2013633, 2014. [14] Minka, Thomas P. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of\nTechnology, 2001. [15] Nielsen, Michael A and Chuang, Isaac L. Quantum computation and quantum information. Cambridge university press,\n2010. [16] Novikoff, Albert BJ. On convergence proofs for perceptrons. Technical report, DTIC Document, 1963. [17] Rebentrost, Patrick, Mohseni, Masoud, and Lloyd, Seth. Quantum support vector machine for big data classification.\nPhysical review letters, 113(13):130503, 2014. [18] Rosenblatt, Frank. The perceptron: a probabilistic model for information storage and organization in the brain. Psycho-\nlogical review, 65(6):386, 1958. [19] Shalev-Shwartz, Shai and Singer, Yoram. A new perspective on an old perceptron algorithm. In Learning Theory, pp.\n264\u2013278. Springer, 2005. [20] Suykens, Johan AK and Vandewalle, Joos. Least squares support vector machine classifiers. Neural processing letters, 9\n(3):293\u2013300, 1999. [21] Wiebe, Nathan and Granade, Christopher. Can small quantum systems learn? arXiv preprint arXiv:1512.03145, 2015. [22] Wiebe, Nathan, Kapoor, Ashish, and Svore, Krysta M. Quantum deep learning. arXiv preprint arXiv:1412.3489, 2014. [23] Wiebe, Nathan, Kapoor, Ashish, and Svore, Krysta. Quantum nearest-neighbor algorithms for machine learning. Quantum\nInformation and Computation, 15:318\u2013358, 2015."}], "references": [{"title": "Machine learning in a quantum world", "author": ["A\u0131\u0308meur", "Esma", "Brassard", "Gilles", "Gambs", "S\u00e9bastien"], "venue": "In Advances in artificial intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Quantum boltzmann machine", "author": ["Amin", "Mohammad H", "Andriyash", "Evgeny", "Rolfe", "Jason", "Kulchytskyy", "Bohdan", "Melko", "Roger"], "venue": "arXiv preprint arXiv:1601.02036,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "On the einstein podolsky rosen paradox", "author": ["Bell", "John S"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1964}, {"title": "Tight bounds on quantum searching", "author": ["Boyer", "Michel", "Brassard", "Gilles", "H\u00f8yer", "Peter", "Tapp", "Alain"], "venue": "arXiv preprint quant-ph/9605034,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Quantum amplitude amplification and estimation", "author": ["Brassard", "Gilles", "Hoyer", "Peter", "Mosca", "Michele", "Tapp", "Alain"], "venue": "Contemporary Mathematics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "Machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Adiabatic quantum algorithm for search engine ranking", "author": ["Garnerone", "Silvano", "Zanardi", "Paolo", "Lidar", "Daniel A"], "venue": "Physical review letters,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A new approximate maximal margin classification algorithm", "author": ["Gentile", "Claudio"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "A fast quantum mechanical algorithm for database search", "author": ["Grover", "Lov K"], "venue": "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Bayes point machines: Estimating the bayes point in kernel space", "author": ["Herbrich", "Ralf", "Graepel", "Thore", "Campbell", "Colin"], "venue": "In IJCAI Workshop SVMs,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "The perceptron algorithm with uneven margins", "author": ["Li", "Yaoyong", "Zaragoza", "Hugo", "Herbrich", "Ralf", "Shawe-Taylor", "John", "Kandola", "Jaz"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Quantum algorithms for supervised and unsupervised machine learning", "author": ["Lloyd", "Seth", "Mohseni", "Masoud", "Rebentrost", "Patrick"], "venue": "arXiv preprint arXiv:1307.0411,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Quantum principal component analysis", "author": ["Lloyd", "Seth", "Mohseni", "Masoud", "Rebentrost", "Patrick"], "venue": "Nature Physics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["Minka", "Thomas P"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "Quantum computation and quantum information", "author": ["Nielsen", "Michael A", "Chuang", "Isaac L"], "venue": "Cambridge university press,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On convergence proofs for perceptrons", "author": ["Novikoff", "Albert BJ"], "venue": "Technical report, DTIC Document,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Quantum support vector machine for big data classification", "author": ["Rebentrost", "Patrick", "Mohseni", "Masoud", "Lloyd", "Seth"], "venue": "Physical review letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["Rosenblatt", "Frank"], "venue": "Psychological review,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1958}, {"title": "A new perspective on an old perceptron algorithm", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In Learning Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Least squares support vector machine classifiers", "author": ["Suykens", "Johan AK", "Vandewalle", "Joos"], "venue": "Neural processing letters,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Can small quantum systems learn", "author": ["Wiebe", "Nathan", "Granade", "Christopher"], "venue": "arXiv preprint arXiv:1512.03145,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Quantum deep learning", "author": ["Wiebe", "Nathan", "Kapoor", "Ashish", "Svore", "Krysta M"], "venue": "arXiv preprint arXiv:1412.3489,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Quantum nearest-neighbor algorithms for machine learning", "author": ["Wiebe", "Nathan", "Kapoor", "Ashish", "Svore", "Krysta"], "venue": "Quantum Information and Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 1, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 11, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 12, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 16, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 20, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 21, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 22, "context": "The growing importance of machine learning has in recent years led to a host of studies that investigate the promise of quantum computers for machine learning [1, 2, 12, 13, 17, 21\u201323].", "startOffset": 159, "endOffset": 184}, {"referenceID": 1, "context": "The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22].", "startOffset": 200, "endOffset": 210}, {"referenceID": 6, "context": "The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22].", "startOffset": 200, "endOffset": 210}, {"referenceID": 21, "context": "The true potential of quantum algorithms may therefore remain underexploited since quantum algorithms have been constrainted to follow the same methodology behind traditional machine learning methods [2, 7, 22].", "startOffset": 200, "endOffset": 210}, {"referenceID": 17, "context": "We illustrate our approach by focusing on perceptron training [18].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The perceptron is a fundamental building block for various machine learning models including neural networks and support vector machines [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": ", yN}, yi \u2208 {+1,\u22121}, the goal of perceptron learning is to recover a hyperplane w that perfectly classifies the training set [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 7, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 10, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 17, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 18, "context": "There are various simple online algorithms that start with a random initialization of the hyperplane and make updates as they encounter more and more data [8, 11, 18, 19]; however, the rule that we consider for online perceptron training is, upon misclassifying a vector (\u03c6, y), w \u2190 w + y\u03c6.", "startOffset": 155, "endOffset": 170}, {"referenceID": 15, "context": "In particular, if the training data is composed of unit vectors, \u03c6i \u2208 R, that are separated by a margin of \u03b3 then there are perceptron training algorithms that make at most O( 1 \u03b32 ) mistakes [16], ar X iv :1 60 2.", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "This figure is from [14].", "startOffset": 20, "endOffset": 24}, {"referenceID": 5, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 119, "endOffset": 130}, {"referenceID": 10, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 119, "endOffset": 130}, {"referenceID": 18, "context": "Similar bounds also exist when the data is not separated [6] and also for other generalizations of perceptron training [8, 11, 19].", "startOffset": 119, "endOffset": 130}, {"referenceID": 13, "context": "Figure 1, which is borrowed from [14], illustrates the version space interpretation of perceptrons.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "Naturally, classifiers including SVMs [20] and Bayes point machines [10] lie in the version space.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Naturally, classifiers including SVMs [20] and Bayes point machines [10] lie in the version space.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5].", "startOffset": 136, "endOffset": 142}, {"referenceID": 8, "context": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5].", "startOffset": 136, "endOffset": 142}, {"referenceID": 4, "context": "Both quantum approaches introduced in this work and their corresponding speed-ups stem from a quantum subroutine called Grover\u2019s search [4, 9], which is a special case of a more general method referred to as amplitude amplification [5].", "startOffset": 232, "endOffset": 235}, {"referenceID": 2, "context": "This basis dependence of measurement is the root of many of the differences between quantum and classical probability theory and also gives rise to many celebrated results in the foundations of quantum mechanics such as Bell\u2019s theorem [3].", "startOffset": 235, "endOffset": 238}, {"referenceID": 3, "context": "Fortunately, methods are known to deal with such issues [4, 5].", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Fortunately, methods are known to deal with such issues [4, 5].", "startOffset": 56, "endOffset": 62}, {"referenceID": 15, "context": "Although the traditional feature space perceptron training algorithm is online [16], meaning that the training examples are provided one at a time to it in a streaming fashion, we deviate from this model slightly by instead requiring that the algorithm be fed training examples that are, in effect, sampled uniformly from the training set.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "For example, the training vector (\u03c6j , yj) \u2261 ([0, 0, 1, 0] , 1) can be encoded as an unsigned integer 00101 \u2261 5, which in turn can be represented by the unit vector \u03a6 = [0, 0, 0, 0, 0, 1] .", "startOffset": 46, "endOffset": 58}, {"referenceID": 0, "context": "For example, the training vector (\u03c6j , yj) \u2261 ([0, 0, 1, 0] , 1) can be encoded as an unsigned integer 00101 \u2261 5, which in turn can be represented by the unit vector \u03a6 = [0, 0, 0, 0, 0, 1] .", "startOffset": 169, "endOffset": 187}, {"referenceID": 5, "context": "Novikoff\u2019s theorem [6, 16] states that the algorithms described in both lemmas must be applied at most 1/\u03b3 times before finding the result.", "startOffset": 19, "endOffset": 26}, {"referenceID": 15, "context": "Novikoff\u2019s theorem [6, 16] states that the algorithms described in both lemmas must be applied at most 1/\u03b3 times before finding the result.", "startOffset": 19, "endOffset": 26}, {"referenceID": 3, "context": "loss of generality equivalent to Grover\u2019s problem [4, 9].", "startOffset": 50, "endOffset": 56}, {"referenceID": 8, "context": "loss of generality equivalent to Grover\u2019s problem [4, 9].", "startOffset": 50, "endOffset": 56}, {"referenceID": 3, "context": "Since Grover\u2019s search reduces to perceptron training in the case of one marked item the lower bound of \u03a9( \u221a N) queries for Grover\u2019s search [4] applies to perceptron training.", "startOffset": 139, "endOffset": 142}, {"referenceID": 14, "context": "Now using V (and applying the Hadamard transform [15]) we can prepare the following quantum state", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "This process can be realized using a quantum subroutine that computes fw, an application of V and V \u2020 and also the application of a conditional phase gate (which is a fundamental quantum operation that is usually denoted Z) [15].", "startOffset": 224, "endOffset": 228}, {"referenceID": 0, "context": "Nquant \u2208 O ( N \u221a \u03b3 log [ 1 ])", "startOffset": 23, "endOffset": 28}, {"referenceID": 15, "context": "Since the inner loop is repeated O( \u221a K log(1/ )) times, the maximum number of misclassified vectors that arises from this training process is from Theorem 2 O( 1 \u221a\u03b3 log (1/ )) which, for constant , constitutes a quartic improvement over the standard mistake bound of 1/\u03b3 [16].", "startOffset": 272, "endOffset": 276}, {"referenceID": 4, "context": "Under the assumption that c \u2208 (1, 2) the next loop repeats this sampling process until m \u2265M0 in order to ensure that the probability of finding a misclassified element is at least 1/4 [5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Therefore, under these assumptions, the probability that the middle loop updates the perceptron weights is at least 1/4 from [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The outer loop serves to amplify the success probability to at least 1\u2212 from the (average) success probability for m \u2265M0, which is at least 1/4, given that the perceptron makes a mistake on at least one training vector [5].", "startOffset": 219, "endOffset": 222}, {"referenceID": 0, "context": "[1] A\u0131\u0308meur, Esma, Brassard, Gilles, and Gambs, S\u00e9bastien.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Amin, Mohammad H, Andriyash, Evgeny, Rolfe, Jason, Kulchytskyy, Bohdan, and Melko, Roger.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Bell, John S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Boyer, Michel, Brassard, Gilles, H\u00f8yer, Peter, and Tapp, Alain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Brassard, Gilles, Hoyer, Peter, Mosca, Michele, and Tapp, Alain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Freund, Yoav and Schapire, Robert E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Garnerone, Silvano, Zanardi, Paolo, and Lidar, Daniel A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Gentile, Claudio.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Grover, Lov K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Herbrich, Ralf, Graepel, Thore, and Campbell, Colin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Li, Yaoyong, Zaragoza, Hugo, Herbrich, Ralf, Shawe-Taylor, John, and Kandola, Jaz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Lloyd, Seth, Mohseni, Masoud, and Rebentrost, Patrick.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Lloyd, Seth, Mohseni, Masoud, and Rebentrost, Patrick.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Minka, Thomas P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Nielsen, Michael A and Chuang, Isaac L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Novikoff, Albert BJ.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Rebentrost, Patrick, Mohseni, Masoud, and Lloyd, Seth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Rosenblatt, Frank.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Shalev-Shwartz, Shai and Singer, Yoram.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Suykens, Johan AK and Vandewalle, Joos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Wiebe, Nathan and Granade, Christopher.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Wiebe, Nathan, Kapoor, Ashish, and Svore, Krysta M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Wiebe, Nathan, Kapoor, Ashish, and Svore, Krysta.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N , namely O( \u221a N). The second algorithm illustrates how the classical mistake bound of O( 1 \u03b32 ) can be further improved to O( 1 \u221a \u03b3 ) through quantum means, where \u03b3 denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.", "creator": "LaTeX with hyperref package"}}}