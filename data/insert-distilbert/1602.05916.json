{"id": "1602.05916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning", "abstract": "we show a talagrand - type of concentration inequality for mixed multi - task learning ( mtl ), using which we establish sharp excess risk bounds for mtl in terms of distribution - and data - dependent versions of the local enhanced rademacher complexity ( lrc ). we also give a new bound on the lrc for various strongly convex hypothesis classes, which applies not only to mtl but also to the standard i. 2 i. d. setting. actually combining both results, one can now easily derive fast - rate bounds on the excess risk for many prominent mtl methods, including - - - as we demonstrate - - - schatten - norm, group - norm, and graph - regularized mtl. the derived bounds evidently reflect a relationship akeen to a conservation law constraint of asymptotic convergence rates. this overly very relationship allows for trading o? slower rates w. r. t. the number of tasks for faster rates with respect to the number of available samples per task, reduced when compared to the rates obtained via a traditional, global limited rademacher analysis.", "histories": [["v1", "Thu, 18 Feb 2016 19:13:23 GMT  (250kb)", "http://arxiv.org/abs/1602.05916v1", null], ["v2", "Thu, 9 Feb 2017 22:48:06 GMT  (267kb)", "http://arxiv.org/abs/1602.05916v2", "In this version, some arguments and results (of the previous version) have been corrected, or modified"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["niloofar yousefi", "yunwen lei", "marius kloft", "mansooreh mollaghasemi", "georgios anagnostopoulos"], "accepted": false, "id": "1602.05916"}, "pdf": {"name": "1602.05916.pdf", "metadata": {"source": "META", "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning", "authors": ["Niloofar Yousefi", "Yunwen Lei", "Marius Kloft", "Mansooreh Mollaghasemi", "Georgios Anagnastapolous"], "emails": ["niloofaryousefi@knights.ucf.edu,", "yunwen.lei@hotmail.com,", "kloft@hu-berlin.de,", "Mansooreh.Mollaghasemi@ucf.edu", "georgio@fit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n05 91\n6v 1\n[ cs\n.L G\nKeywords: Multi-task Learning, Kernel Methods, Generalization Bound, Local Rademacher Complexity\n1 Introduction\nMulti-Task Learning (MTL) refers to the concurrent learning of a collection of conceptually related tasks, each of which features its own data. Such an approach can be advantageous over learning each task independently, when the tasks lack a sufficient body of observed data. MTL is accomplished by jointly constraining the tasks\u2019 hypothesis spaces, so that tasks mutually regularize the learning of others, based on their inter-task relatedness; this exchange mechanism is often referred to as information sharing. Pioneering works on MTL include [16, 9, 2, 4]. Nowadays, MTL frameworks are routinely employed in a variety of settings. Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.\nMTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect. 1.3. Let T denote the number of tasks being co-learned and n denote the number of available observations per task. Then, in terms of convergence rates w.r.t. n and T , the fastest-converging error or excess risk bounds derived in these works, whether distribution- or data-dependent, are of the order O(1/ \u221a nT ).\nMore recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)). This new, modified function class complexity measure is attention-worthy, since, as shown in [7], a LRCs-based (local) analysis is capable of producing more rapidly-converging excess risk bounds, when compared to the ones obtained via a GRC (global) analysis. This can be attributed to the fact that, unlike LRCs, GRCs ignore the fact that learning algorithms typically choose well-performing hypotheses that belong only to a subset of the entire hypothesis space under consideration. The end result of this distinction empowers a local analysis to provide less conservative and, hence, sharper bounds than when a global analysis is employed. To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].\n1.1 Our Contributions\nThrough one of Bousquet\u2019s Talagrand-type concentration inequalities adapted to the MTL case, this paper\u2019s main contribution is the derivation of sharp bounds on the excess MTL risk in terms of the distribution- and data-dependent LRC. For a given number of tasks T , these bounds admit faster (asymptotic) convergence characteristics in the number of observations per task n, when compared to corresponding bounds hinging on the GRC. Thence, these faster rates allow for heighten confidence that the MTL hypothesis selected by a learning algorithm approaches the best-in-class solution as n increases beyond a certain threshold. We also prove a new bound on the LRC, which generally holds for hypothesis classes using strongly convex regularizers. This bound readily facilitates the bounding of the LRC for a range of such regularizers (not only for MTL, but also for the standard i.i.d. setting), as we demonstrate for classes induced by graph-based, Schatten- and group-norm regularizers. Moreover, we prove matching lower bounds showing that, aside from constants, the LRC-based bounds are tight for the considered applications.\nOur derived bounds reflect that one can trade off a slow convergence speed w.r.t. T for an improved convergence rate w.r.t. n. The latter one ranges, in the worst case, from the typical GRC-based bounds O(1/ \u221a n), all the way up to the fastest rate of order O(1/n) by allowing the bound to depend less on T . This trade-off is perhaps best exemplified in the case of Schatten norms, for which the two rates (exponents) always sum up to \u22121. Nevertheless, the premium in question becomes less relevant to MTL, in which T is typically considered as fixed.\n1.2 Organization of the paper\nThe paper is organized as follows: Sect. 2 lays the foundations for our analysis by considering a Talagrandtype concentration inequality suitable for deriving our bounds. Next, in Sect. 3, after suitably defining LRCs for MTL hypothesis spaces, we provide our LRC-based excess MTL risk bounds. Based on these bounds, we follow up this section with a local analysis of linear MTL frameworks, in which task-relatedness is presumed and enforced by imposing a strongly-convex norm constraint. In more detail, leveraging off the Fenchel-Young inequality, Sect. 4 presents a generic upper bound for the relevant LRC, which is subsequently specialized to the case of group norm, Schatten norm and graph-regularized linear MTL. After illustrating the tightness of the upper bounds, Sect. 5 supplies the corresponding excess risk bounds. The paper concludes with Sect. 6, which compares side by side the GRC- and LRC-based excess risk bounds for the aforementioned hypothesis spaces, as well as two additional related cases.\n1.3 Previous Related Works\nEarlier works that investigate MTL generalization guarantees employing Rademacher averages include [38], which considers linear MTL frameworks for binary classification. In these frameworks, all tasks are preprocessed by a common bounded linear operator and operator norm constraints are used to control the complexity of the associated hypothesis spaces. The GRC-based error bounds derived are of order O(1/ \u221a n) and non-vanishing w.r.t. T in the distribution-dependent case and of order O(1/ \u221a nT ) in the data-dependent case. Another study, [39], provides bounds for the empirical and expected Rademacher complexities of\nlinear transformation classes. Based on Ho\u0308lder\u2019s inequality, GRC-based risk bounds of order O(1/ \u221a n) and non-vanishing w.r.t. T are established for MTL hypothesis spaces with graph-based and LSq -Schatten norm regularizers, where q \u2208 {2} \u222a [4,\u221e].\nThe subject of MTL generalization guarantees experienced renewed attention in more years. In [21], the authors take advantage of the strongly-convex nature of certain matrix-norm regularizers to easily obtain generalization bounds for a variety of machine learning problems. Part of their work is devoted to the realm of online and off-line MTL. In the latter case, which pertains to the focus of our work, the paper provides a distribution-dependent GRC-based excess risk bound of order O(1/ \u221a nT ). Moreover, [41] presents a global Rademacher complexity analysis leading to both data and distribution-dependent excess risk bounds of order O( \u221a\nlog(n)/n) and non-vanishing w.r.t. T for a trace norm regularized MTL model. Also, [40] examines the bounding of (global) Gaussian complexities of function classes that result from considering composite maps, as it is typical in MTL among other settings. An application of the paper\u2019s results yields data-dependent MTL risk bounds of order O(1/ \u221a n) and non-vanishing w.r.t. T . More recently, [42] presents excess risk bounds for both MTL and Learning-To-Learn (LTL) settings and reveals conditions, under which MTL is more beneficial over learning tasks independently. The accompanying bounds are of order O(1/ \u221a nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T \u2192 +\u221e. Finally, due to being domains related to MTL, but, at the same time, less connected to the focus of this paper, we only mention in passing a few works that pertain to generalization guarantees in the realm of life-long learning and domain adaptation. Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45]. Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].\n1.4 Basic Assumptions & Notation\nConsider T supervised learning tasks sampled from the same input-output space X \u00d7 Y. Each task t is presented by an independent random variable (Xt, Yt) governed by a probability distribution \u00b5t. Also, the i.i.d. samples related to each task t are described by the sequence (X it , Y i t ) n i=1, which is distributed according to \u00b5t. In what follows, we use the following notational conventions: vectors and matrices are depicted in bold face. The superscript T , when applied to a vector/matrix, denotes the transpose of that quantity. We define NS := {1, . . . , S}. For any random variablesX,Y and functions f we use Ef(X,Y ) and EXf(X,Y ) to denote the expectation w.r.t. all the involved random variables and the conditional expectation w.r.t. the random variable X . For any vector-valued function f = (f1, . . . , fT ), we introduce the following two notations:\nPf := 1\nT\nT\u2211\nt=1\nPft = 1\nT\nT\u2211\nt=1\nE(f(Xt)), Pnf := 1\nT\nT\u2211\nt=1\nPnft = 1\nT\nT\u2211\nt=1\n1\nn\nn\u2211\ni=1\nf(X it).\nFor any loss function \u2113 and any f = (f1, . . . , fT ) we define \u2113ft the function defined by \u2113ft((Xt, Yt) T t=1) = \u2113(ft(Xt), Yt).\n2 Talagrand-Type Inequality for Multi-Task Learning\nThe derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand\u2019s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F . We defer the proof in Appendix.\nTheorem 1 (Talagrand-Type Inequality for MTL). Let F = {f := (f1, . . . , fT )} be a class of vectorvalued functions satisfying supt,x |ft(x)| \u2264 b. Let X := (X it) (T,Nt) (t,i)=(1,1) be a vector of \u2211T t=1 Nt independent random variables where X1t , . . . , X n t , \u2200t are identically distributed. Let {\u03c3it}t,i be a sequence of independent Rademacher variables. If 1T \u2211T t=1 supf\u2208F E [ ft(X 1 t ) ]2 \u2264 r, then, for every x > 0, with probability at least\n1\u2212 e\u2212x,\nsup f\u2208F\n(Pf \u2212 Pnf) \u2264 4R(F) + \u221a 2xr\nnT +\n8bx 3nT , (1)\nwhere n := mint\u2208NT Nt, and the multi-task Rademacher complexity of function class F is defined as\nR(F) := EX,\u03c3 {\nsup f=(f1,...,fT )\u2208F\n1\nT\nT\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n\u03c3itft(X i t)\n}\n.\nNote that the same bound also holds for supf\u2208F(Pnf \u2212 Pf). In Theorem 1, the data from different tasks assumed to be mutually independent, which is typical in the MTL setting [38]. To present the results in a clear way we always assume in the following that the available data for each task is the same, namely n.\n3 Excess MTL Risk Bounds based on Local Rademacher Com-\nplexities\nTheorem 1 motivates us to extend the classical LRC R(F , r) for a scalar-valued function class F : R(F , r) := EX,\u03c3 [ supf\u2208F ,V (f)\u2264r 1 n \u2211n i=1 \u03c3if(Xi) ] to the Multi-Task Local Rademacher Complexity (MT-LRC) R(F , r) and its empirical counterpart R\u0302(F , r) for a vector-valued function class F as follows:\nR(F , r) :=E [\nsup f=(f1,...,fT )\u2208F\nV (f)\u2264r\n1\nnT\n\u2211\nt\u2208NT i\u2208Nn\n\u03c3itft(X i t ) ] , R\u0302(F , r) := E\u03c3 [\nsup f=(f1,...,fT )\u2208F\nVn(f)\u2264r\n1\nnT\n\u2211\nt\u2208NT i\u2208Nn\n\u03c3itft(X i t) ] , (2)\nwhere V (f ) and Vn(f ) are upper bounds on the variance and conditional variances of the functions in F , respectively. This paper takes the choice V (f ) = Pf2 and Vn(f) = Pnf\n2. Analogous to single task learning, the challenge in applying MT-LRC (2) to refine the existing learning rates is to find an optimal radius trading-off the size of the set {f \u2208 F : V (f) \u2264 r} and its complexity, which, as we show below, reduces to the calculation of the fixed-point of a sub-root function. We call a function \u03c8 sub-root if it is non-decreasing, non-negative and r 7\u2192 \u03c8(r)/\u221ar is non-increasing for r \u2265 0. We call the unique solution of the equation \u03c8(r) = r the fixed point of \u03c8. We suppose that the loss function \u2113 and the hypothesis space F satisfy the following conditions: Assumptions 1.\n1. There is a function f\u2217 = (f\u22171 , . . . , f \u2217 T ) \u2208 F satisfying P\u2113f\u2217 = inff\u2208F P\u2113f .\n2. There is constant B\u2032 \u2265 1, such that for every ft \u2208 F we have P (ft \u2212 f\u2217t )2 \u2264 B\u2032P (\u2113ft \u2212 \u2113f\u2217t ).\n3. There is a constant L, such that the loss function \u2113 is L-Lipschitz in its first argument.\nWe now present the main result of this section showing that the excess error of MTL can be bounded by the fixed-point of a sub-root function dominating the MT-LRC.\nTheorem 2 (Distribution-dependent excess risk bound for MTL). Let F = {f := (f1, . . . , fT )} be a class of vector-valued functions satisfying supt,x |ft(x)| \u2264 b. Let X := (X it , Y it ) (T,n) (t,i)=(1,1) be a vector of nT independent random variables where (X1t , Y 1 t ) . . . , (X n t , Y n t ), \u2200t are identically distributed. Suppose that Assumptions 1 holds. Let \u03c8 be a sub-root function with the fixed point r\u2217 such that BLR(F\u2217, r) \u2264 \u03c8(r), \u2200r \u2265 r\u2217, where R(F\u2217, r) is the LRC of the functions class F\u2217:\nR(F\u2217, r) := EX,\u03c3 [\nsup f\u2208F ,L2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t) ] . (3)\nThen, we have the following bounds in terms of the fixed point r\u2217 of \u03c8(r):\n1. For any function class F , K > 1 and x > 0, with probability at least 1\u2212 e\u2212x,\nP (\u2113f \u2212 \u2113f\u2217) \u2264 K\nK \u2212 1Pn(\u2113f \u2212 \u2113f \u2217) +\n500K\nB r\u2217 +\n(6Lb+ 10BK)x\nnT .\n2. For any convex function class F , K > 1 and x > 0, with probability at least 1\u2212 e\u2212x,\nP (\u2113f \u2212 \u2113f\u2217) \u2264 K\nK \u2212 1Pn(\u2113f \u2212 \u2113f \u2217) +\n32K\nB r\u2217 +\n(3Lb+ 4BK)x\nnT .\nProof. Introduce the following class of excess loss functions:\nH\u2217F := { hf : (Xt, Yt) T t=1 7\u2192 (\u2113(ft(Xt), Yt)\u2212 \u2113(f\u2217t (Xt), Yt)) T t=1 ,f = (f1, . . . , fT ) \u2208 F } . (4)\nFrom Assumptions 1, it can be seen that for any function h \u2208 H\u2217F , P (\u2113f \u2212 \u2113f\u2217)2 \u2264 L2P (f \u2212 f\u2217)2 \u2264 BP (\u2113f \u2212 \u2113f\u2217), where B = B\u2032L2. This implies\nV (hf ) = Ph 2 f \u2264 L2P (f \u2212 f\u2217)2 \u2264 BP (f \u2212 f\u2217).\nAlso, using Talagrand\u2019s Lemma [29], one can verify\nBR(H\u2217F , r) = BEX,\u03c3\n\n  sup\nf\u2208F , V (hf )\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itht(X i t , Y i t )\n\n \n= BEX,\u03c3\n\n  sup\nf\u2208F , V (hf )\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3it\u2113ft(X i t , Y i t )\n\n \n\u2264 BLR(F\u2217, r) \u2264 \u03c8(r).\nApplying Theorem A.2 (which is the extension of Theorem 3.3 of [7] to MTL function classes) to the function class H\u2217F completes the proof.\nThe following excess-risk bound is immediate by noting that Pn(\u2113f\u0302 \u2212 \u2113f\u2217) \u2264 0.\nCorollary 3. Let f\u0302 be any element of F satisfying Pn\u2113f\u0302 = inff\u2208F Pn\u2113f . Assume that the conditions of Theorem 2 hold. Then for any x > 0 and r > \u03c8(r), with probability at least 1\u2212 e\u2212x,\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264\n500K\nB r\u2217 +\n(6Lb+ 10BK)x\nnT . (5)\nThe next theorem, analogous to Theorem 5.4 in [7], presents a data-dependent version of (5) replacing the Rademacher complexity in Corollary 3 with its empirical counterpart.\nTheorem 4 (Data-dependent excess risk bound for MTL). Let f\u0302 be any element of F satisfying Pn\u2113f\u0302 = inff\u2208F Pn\u2113f . Assume that the condition of Theorem 2 hold. Define\n\u03c8\u0302n(r) = c1R\u0302(F\u2217, c3r) + c2x nT , R\u0302(F\u2217, c3r) := E\u03c3\n[\nsup f\u2208F ,\nL2Pn(f\u2212f\u0302)2\u2264c3r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t) ] ,\nwhere c1 = 2Lmax(B, 16Lb), c2 = 8L 2b2+c1 and c3 = 4+128K+4B(3Lb+4BK)/c2. Then for any K > 1 and x > 0, with probability at least 1\u2212 4e\u2212x, we have\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264\n500K\nB r\u0302\u2217 +\n(6Lb+ 10BK)x\nnT ,\nwhere r\u0302\u2217 is the fixed point of the sub-root function \u03c8\u0302n(r).\nProof. The proof of this Theorem repeats the same basic steps utilized in Theorem 5.4 in [7] and, therefore, can be found in the Appendix.\n4 Local Rademacher Complexity Bounds for MTL models with\nStrongly Convex Regularizers\nThis section presents very general MT-LRC bounds, based on the distribution-dependent excess risks established in Theorem 2, for hypothesis spaces defined by strongly convex regularizers, which allows us to immediately derive, as specific application cases, LRC bounds for group-norm, Schatten-norm, and graphregularized MTL models. It should be mentioned that similar data-dependent MT-LRC bounds are also available by a similar deduction process.\n4.1 Preliminaries\nWe consider linear MTL models where we associate to each task-wise function ft a weight wt \u2208 H by ft(X) = \u3008wt, \u03c6(X)\u3009. Here \u03c6 is a feature map associated to a Mercer kernel k satisfying k(X, X\u0303) = \u3008\u03c6(X), \u03c6(X\u0303)\u3009, \u2200X, X\u0303 \u2208 X and wt belongs to the reproducing kernel Hilbert space HK induced by k with inner product \u3008\u00b7, \u00b7\u3009. We assume that the multi-task model W = (w1, . . . ,wT ) \u2208 H\u00d7 . . .\u00d7H is learned by a regularization scheme:\nmin W R (W ) + C\nT\u2211\nt=1\nn\u2211\ni=1\n\u2113( \u2329 wt, \u03c6(X i t ) \u232a H , Y i t ), (6)\nwhere the regularizer R(\u00b7) is used to enforce a priori information to avoid over-fitting. This regularization scheme amounts to performing Empirical Risk Minimization (ERM) in the hypothesis space\nF := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , . . . , \u3008wT , \u03c6(XT )\u3009]T : R (W ) \u2264 R } , (7)\nwhere for generality we consider regularizers of the form R(W ) = \u2225 \u2225D1/2W \u2225 \u2225 with a positive operator D defined in H. The hypothesis space associated to group and Schatten norms can be recovered by taking D = I and appropriate norms. Furthermore, the graph-regularized MTL can be specialized by taking D = L+ \u03b7I with L being a graph-Laplacian and \u03b7 being a regularization constant. Our general discussion shows that all these MTL models can be covered in a framework with the notion of strong convexity.\n4.2 General Bound on the LRC\nNow, we can provide the main result of this section which gives a LRC bound for any general MTL hypothesis space of the form (7).\nTheorem 5 (Distribution-dependent MT-LRC bounds by strong convexity). Suppose that R(W ) in (6) is \u00b5-strongly convex with R\u2217(0) = 0 and \u2016k\u2016\u221e \u2264 \u03b2 \u2264 \u221e. Let X1t , . . . , Xnt be an i.i.d. sample drawn from Pt. Also, assume that for each task t, the eigenvalue-eigenvector decomposition of the Hilbert-Schmidt covariance operator is given by Jt = E(\u03c6(Xt) \u2297 \u03c6(Xt)) = \u2211\u221e j=1 \u03bb j tu j t \u2297 ujt , where (ujt )\u221ej=1 forms an orthonormal basis of H, and (\u03bbjt )\u221ej=1 are the corresponding eigenvalues, for the task t, arranged in non-increasing order. Then for every positive operator D on RT , any r > 0 and any non-negative integers h1, . . . , hT :\nR(F , r) \u2264 min 0\u2264ht\u2264\u221e\n \n\n\u221a\nr \u2211T\nt=1 ht nT + 1 T\n\u221a\n2R\n\u00b5 EX,\u03c3\n\u2225 \u2225 \u2225D \u22121/2V \u2225 \u2225 \u2225 2\n\u2217\n   , (8)\nwhere V = ( \u2211\nj>ht \u2329 1 n \u2211n i=1 \u03c3 i t\u03c6(X i t),u j t \u232a u j t )T\nt=1 .\nProof. Note that with the help of LRC definition, we have for any function class F ,\nR(F , r) = 1 nT EX,\u03c3\n \n sup f=(f1,...,fT )\u2208F ,\nPf2\u2264r\nn\u2211\ni=1\n\u2329\n(wt) T t=1 , ( \u03c3it\u03c6(X i t ) )T\nt=1\n\u232a\n \n\n= 1\nT EX,\u03c3\n \n sup f\u2208F , Pf2\u2264r\n\u2329\n(wt) T t=1 ,\n\n\n\u221e\u2211\nj=1\n\u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t),u j t\n\u232a\nu j t\n\n\nT\nt=1\n\u232a \n\n\n\u2264 1\nT EX,\u03c3\n \n sup Pf2\u2264r\n\u2329\n\nht\u2211\nj=1\n\u221a\n\u03bbjt\n\u2329\nwt,u j t\n\u232a\nu j t\n\n\nT\nt=1\n,\n\n\nht\u2211\nj=1\n\u221a\n\u03bbjt\n\u22121 \u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t ),u j t\n\u232a\nu j t\n\n\nT\nt=1\n\u232a \n\n\n\ufe38 \ufe37\ufe37 \ufe38\nA1 (9)\n+ 1\nT EX,\u03c3\n \n sup f\u2208F\n\u2329\n(wt) T t=1 ,\n  \u2211\nj>ht\n\u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t),u j t\n\u232a\nu j t\n\n\nT\nt=1\n\u232a \n\n\n\ufe38 \ufe37\ufe37 \ufe38\nA2 . (10)\nStep 1. Controlling A1: Applying Cauchy-Schwartz (C.S.) and Ho\u0308lder inequalities on A1 yields the following\nA1 \u2264 1\nT EX,\u03c3\n \n\nsup Pf2\u2264r\n\n  \n\n \nT\u2211\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ht\u2211\nj=1\n\u221a\n\u03bbjt\n\u2329\nwt,u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n2 \n \n1 2\n\n \nT\u2211\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ht\u2211\nj=1\n\u221a\n\u03bbjt\n\u22121 \u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t),u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n2 \n \n1 2\n\n  \n \n\n= 1\nT EX,\u03c3\n \n sup Pf2\u2264r\n\n \n\n\nT\u2211\nt=1\nht\u2211\nj=1\n\u03bbjt\n\u2329\nwt,u j t\n\u232a2\n\n\n1 2\n\n\nT\u2211\nt=1\nht\u2211\nj=1\n\u03bbjt \u22121\n\u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t),u j t\n\u232a2 \n\n1 2\n\n \n \n\n.\nWith the help of Jensen\u2019s inequality and regarding the fact that EX,\u03c3 \u2329 1 n \u2211n i=1 \u03c3 i t\u03c6(X i t ),u j t \u232a2 = \u03bbjt n and Pf2 \u2264 r implies 1T \u2211T t=1 \u2211\u221e j=1 \u03bb j t \u2329 wt,u j t \u232a2\n\u2264 r (see Lemma 2 in the Appendix for the proof), we can further bound A1 as\nA1 \u2264\n\u221a\nr \u2211T\nt=1 ht nT . (11)\nStep 2. Controlling A2: We use strong convexity assumption on the regularizer in order to further bound the second term A2.\nLet \u03bb > 0. Applying (A.27) with w = D1/2W and v = \u03bbD\u22121/2V gives\n\u2329 D1/2W , \u03bbD\u22121/2V \u232a \u2264 R(W ) + \u2329 \u25bdR\u2217 (0) , \u03bbD\u22121/2V \u232a + \u03bb2\n2\u00b5\n\u2225 \u2225 \u2225D \u22121/2V \u2225 \u2225 \u2225 2\n\u2217 .\nTaking expectation on both sides and optimizing \u03bb gives\nA2 = 1\nT EX,\u03c3\n{\nsup f\u2208F\n\u2329 D1/2W ,D\u22121/2V \u232a }\n\u2264 min 0<\u03bb<\u221e\n{ R\n\u03bbT +\n\u03bb\n2\u00b5T EX,\u03c3\n\u2225 \u2225 \u2225D \u22121/2V \u2225 \u2225 \u2225 2\n\u2217\n}\n= 1\nT\n\u221a\n2R\n\u00b5 EX,\u03c3\n\u2225 \u2225 \u2225D \u22121/2V \u2225 \u2225 \u2225 2\n\u2217 . (12)\nCombining (12) with (11) completes the proof.\nIn the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].\n4.3 Group Norm Regularized MTL\nWe first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form\nFq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , . . . , \u3008wT , \u03c6(XT )\u3009]T : \u2016W \u20162,q \u2264 Rmax } . (13)\nCorollary 6. If 1 \u2264 q \u2264 2 in (13), the LRC of function class Fq can be bounded as\nR(Fq, r) \u2264 \u221a \u221a \u221a \u221a \u221a \u221a 4\nnT \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u221e\u2211 j=1 min ( rT 1\u2212 2 q\u2217 , 2eq\u22173Rmax T \u03bbjt )   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217 32T 1 q\u2217\nnT . (14)\nProof. The proof follows by applying Khintchine (A.28) and Rosenthal (A.29) inequalities to further bound the expectation term in (8) which gives,\nA2(Fq) \u2264 \u221a \u221a \u221a \u221a \u221a \u221a 2eq\u22172Rmax nT 2\u00b5 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n. (15)\nNow, combining (11) and (15) provides the bound on R(Fq, r) as\nR(Fq, r) \u2264\n\u221a\nr \u2211T\nt=1 ht nT + \u221a \u221a \u221a \u221a \u221a \u221a 2eq\u22172Rmax nT 2\u00b5 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n, (16)\nThen using the following inequalities according which for any non-negative numbers \u03b11, \u03b12 \u2208 R+ and any non-negative vectors a1,a2 \u2208 RT with 0 \u2264 q \u2264 p \u2264 \u221e any s \u2265 1,\n(\u22c6) \u221a \u03b11 + \u221a \u03b12 \u2264 \u221a 2(\u03b11 + \u03b12) (17)\n(\u22c6\u22c6) lp \u2212 to\u2212 lq : \u2016a1\u2016q = \u30081,a1\u3009 1 q Ho\u0308lder \u2264\n(\n\u20161\u2016(p/q)\u2217 \u2016a q 1\u2016(p/q)\n) 1 q\n= T 1 q\u2212 1p \u2016a1\u2016p (18)\n(\u22c6 \u22c6 \u22c6) \u2016a1\u2016s + \u2016a2\u2016s \u2264 21\u2212 1 s \u2016a1 + a2\u2016s \u2264 2 \u2016a1 + a2\u2016, (19)\nwe can obtain the desired result. See the Appendix for the detailed proof.\nRemark 7. Since the LRC bound above is not monotonic in q it is more reasonable to state the above bound in terms of q \u2264 \u03ba, as taking \u03ba = q is not always the optimal choice. Trivially for the group norm regularizer with any \u03ba \u2265 q, it holds that \u2016W \u20162,\u03ba \u2264 \u2016W\u20162,q and therefore R(Fq, r) \u2264 R(F\u03ba, r). Thus we have the following bound on R(Fq, r) for any \u03ba \u2208 [q, 2],\nR(Fq, r) \u2264 \u221a \u221a \u221a \u221a \u221a \u221a 4\nnT \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u221e\u2211 j=1 min ( rT 1\u2212 2 \u03ba\u2217 , 2e\u03ba\u22173Rmax T \u03bbjt )   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n\u03ba\u2217\n2\n+\n\u221a 2\u03b2eRmax\u03ba \u2217 32T 1 \u03ba\u2217\nnT .\nRemark 8 (Sparsity-inducing group-norm). Among p \u2265 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take \u03ba\u2217 = logT to get that\nR(F1, r) \u2264\n\u221a \u221a \u221a \u221a 4\nnT\n\u2225 \u2225 \u2225\n( \u221e\u2211\nj=1\nmin ( rT 1\u2212 2 \u03ba\u2217 , 2e\u03ba\u22173Rmax T \u03bbjt ))T t=1 \u2225 \u2225 \u2225 \u03ba\u2217\n2\n+\n\u221a 2\u03b2eRmax\u03ba \u2217 32T 1 \u03ba\u2217\nnT\n(l\u03ba\u2217 2\n\u2212to\u2212l\u221e)\n\u2264\n\u221a \u221a \u221a \u221a 4\nnT\n\u2225 \u2225 \u2225\n( \u221e\u2211\nj=1\nmin ( rT, 2e3(logT )3\nT Rmax\u03bb\nj t\n))T\nt=1 \u2225 \u2225 \u2225 \u221e +\n\u221a 2\u03b2Rmaxe 3 2 (logT ) 3 2\nnT .\nTo investigate the tightness of the bound in (14), we derive the lower bound which holds for the LRC of Fq with any q \u2265 1.\nTheorem 9 (Lower bound). The following lower bound holds for the local Rademacher complexity of Fq in (14) with any q \u2265 1. There is an absolute constant c so that \u2200t, if \u03bb1t \u2265 1/(nR2max) then for all r \u2265 1n and q \u2265 1,\nR(Fq,R,T , r) \u2265\n\u221a \u221a \u221a \u221a\nc\nnT 1\u2212 2 q\u2217\n\u221e\u2211\nj=1\nmin\n(\nrT 1\u2212 2 q\u2217 , R2max T \u03bbj1\n)\n. (20)\nProof. The proof can be found in the Appendix.\nA comparison between the lower bound in (20) and the upper bound in (14) can be clearly illustrated by assuming identical eigenvalue tail sums \u2211\nj\u2265\u221e \u03bb j t for all tasks, for which the upper bound translates to\nR(Fq,R,T , r) \u2264\n\u221a \u221a \u221a \u221a\n4\nnT 1\u2212 2 q\u2217\n\u221e\u2211\nj=1\nmin\n(\nrT 1\u2212 2 q\u2217 , 2eq\u22173Rmax T \u03bbjt\n)\n+\n\u221a 2\u03b2eRmaxq \u2217 32T 1 q\u2217\nnT ,\nmatching the lower bound in (20) up to constants of factors. A similar comparison analysis can be performed for MTL models with Schatten norm and graph regularizers.\n4.4 Schatten Norm Regularized MTL\n[6] developed a spectral regularization framework for MTL where the Schatten p-norm \u2016W\u2016Sq := [ tr ( W TW ) q 2 ] 1 q is studied as a concrete example, corresponding to perform ERM in the following hypothesis space:\nFSq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , . . . , \u3008wT , \u03c6(XT )\u3009]T : \u2016W\u2016Sq \u2264 R\u2032max } . (21)\nCorollary 10. For any 1 \u2264 q \u2264 2 in (21), the LRC of function class FSq is bounded as\nR(FSq , r) \u2264\n\u221a \u221a \u221a \u221a 4\nnT\n\u2225 \u2225 \u2225\n( \u221e\u2211\nj=1\nmin ( r, 2q\u2217(q\u2217 \u2212 1)R\u2032max\nT \u03bbjt\n))T\nt=1 \u2225 \u2225 \u2225 1 .\nProof. The proof is provided in the Appendix.\nRemark 11 (Sparsity-inducing Schatten-norm (trace norm)). Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22]. Note that for any q \u2265 1, it holds that R(FS1 , r) \u2264 R(FSq , r). Therefore, choosing the optimal q\u2217 = 2 yields that\nR(FS1 , r) \u2264\n\u221a \u221a \u221a \u221a 4\nnT\n\u2225 \u2225 \u2225\n( \u221e\u2211\nj=1\nmin ( r, 4R\u2032max\nT \u03bbjt\n))T\nt=1 \u2225 \u2225 \u2225 1 .\n4.5 Graph Regularized MTL\nThe idea underlying graph regularized MTL is to force the classifiers of related tasks close to each other by penalizing the squared distance \u2016wt \u2212ws\u20162 with different weights \u03c9ts. We consider the following graph regularized MTL [39]\nR(W ) = 1\n2T\nT\u2211\nt=1\nT\u2211\ns=1\n\u03c9ts\u2016wt \u2212ws\u20162 + \u03b7\nT\nT\u2211\nt=1\n\u2016wt\u20162 = 1\nT\nT\u2211\nt=1\nT\u2211\ns=1\n(L+ \u03b7I)ts \u3008wt,ws\u3009 ,\nwhere L is the graph-Laplacian associated to a matrix of edge-weights (\u03c9ts)ts, I is the identity in R T , and \u03b7 > 0 is a regularization parameter. According to the identity 1T \u2211T t=1 \u2211T s=1 ( L + \u03b7I ) ts \u2329 wt,ws \u232a = (1/T )\u2016(L+ \u03b7I)1/2W \u20162F , the corresponding hypothesis space is:\nFG := { X 7\u2192 [ \u2329 w1, \u03c6(X1) \u232a , . . . , \u2329 wT , \u03c6(XT ) \u232a ]T : \u2016D1/2W \u2016F \u2264 R\u2032\u2032max } . (22)\nCorollary 12. For any positive definite matrix D in (22), the LRC of FG is bounded by\nR(FG, r) \u2264\n\u221a \u221a \u221a \u221a 4\nnT\n\u2225 \u2225 \u2225\n( \u221e\u2211\nj=1\nmin ( r, 2D\u22121tt R \u2032\u2032 max\nT \u03bbjt )\n))T\nt=1 \u2225 \u2225 \u2225 1 . (23)\nProof. See the Appendix for the proof.\n5 Excess Risk Bounds for MTL models with Strongly Convex\nRegularizers\nIn this section we will provide the distribution and data-dependent excess risk bounds for the hypothesis spaces considered earlier. Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q \u2208 [1, 2] in (13). However, for the group and LSq -Schatten norm (q \u2208 [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.\nTheorem 13 (Distribution-dependent excess risk bound for a MTL problem with a strong convex L2,q group-norm regularizer). Assume the convex class Fq in (13) has ranges in [\u2212b, b], and let the loss function \u2113 in Problem (6) be such that Assumptions 1 are satisfied. Let f\u0302 be any element of Fq with 1 \u2264 q \u2264 2 which satisfies Pn\u2113f\u0302 = inff\u2208Fq Pn\u2113f . Assume moreover that k is a positive semi-definite kernel on X such that \u2016k\u2016\u221e \u2264 \u03b2 \u2264 \u221e. Denote by r\u2217 the fixed point of 2BLR(Fq, r4L2 ). Then, for any K > 1 and x > 0, with probability at least 1\u2212 e\u2212x, the excess loss of function class Fq is bounded as\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264\n32K\nB r\u2217 +\n(3Lb+ 4BK)x\nnT , (24)\nwhere for the fixed point r\u2217 of the local Rademacher complexity 2BLR(Fq, r4L2 ), it holds\nr\u2217 \u2264 min 0\u2264ht\u2264\u221e\nB2 \u2211T\nt=1 ht Tn + 4BL \u221a \u221a \u221a \u221a \u221a \u221a 2eq\u22173Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+ 4 \u221a 2\u03b2eRmaxq \u2217 32 T 1 q\u2217\nnT , (25)\nwhere h1, . . . , hT are arbitrary non-negative integers.\nProof. First notice that Fq is convex, thus it is star-shaped around any of its points. Hence according to Lemma 3.4 in [7], Fq is a sub-root function. Moreover, because of the symmetry of \u03c3it and because Fq is convex and symmetric, it can be shown that R(F\u2217q , r) \u2264 2R(Fq, r4L2 ), where R(F\u2217q , r) is defined according to (3) for the class of functions Fq. Therefore, it suffices to find the fixed point of 2BLR(Fq, r4L2 ) by solving \u03c6(r) = r. For this purpose, we will use (16) as a bound for R(Fq, r), and we solve \u221a \u03b1r + \u03b3 = r which is equivalent to solving r2 \u2212 (\u03b1+ 2\u03b3)r + \u03b32 = 0, where we define\n\u03b1 = B2\n\u2211T t=1 ht\nTn , and \u03b3 = 2BL\n\u221a \u221a \u221a \u221a \u221a \u221a 2eq\u22173Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+ 2 \u221a 2\u03b2eRmaxBLq \u2217 32T 1 q\u2217\nnT . (26)\nIt is not hard to verify that r\u2217 \u2264 \u03b1+ 2\u03b3. Substituting the definition of \u03b1 and \u03b3 gives the result.\nRegarding the fact that \u03bbjt are decreasing with respect to j, we can assume \u2203dt : \u03bbjt \u2264 dtj\u2212\u03b1t for some \u03b1t > 1. As examples, this assumption holds for finite rank kernels as well as convolution kernels. Thus, it can be shown\n\u2211\nj>ht\n\u03bbjt \u2264 dt \u2211\nj>ht\nj\u2212\u03b1t \u2264 dt \u222b \u221e\nht\nx\u2212\u03b1tdx = dt\n[ 1\n1\u2212 \u03b1t x1\u2212\u03b1t\n]\u221e\nht\n= \u2212 dt 1\u2212 \u03b1t h1\u2212\u03b1tt . (27)\nnote that by lp \u2212 to\u2212 lq conversion, we have\nB2 \u2211T\nt=1 ht Tn \u2264 B\n\u221a\nB2T \u2211T\nt=1 h 2 t\nn2T 2 \u2264 B\n\u221a \u221a \u221a \u221a B2T 2\u2212 2 q\u2217 \u2225 \u2225 \u2225(h2t ) T t=1 \u2225 \u2225 \u2225 q\u2217\n2\nn2T 2 .\nNow, applying, (17) and (19), and inserting (27) into (25), it holds for a group norm regularized MTL with 1 \u2264 q \u2264 2,\nr\u2217 \u2264 min 0\u2264ht\u2264\u221e 2B\n\u221a \u221a \u221a \u221a \u221a \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( B2T 2\u2212 2 q\u2217 h2t n2T 2 \u2212 32dteq \u22173RmaxL2 nT 2(1\u2212 \u03b1t) h1\u2212\u03b1tt )T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+ 4 \u221a 2\u03b2eRmaxBLq \u2217 32T 1 q\u2217\nnT . (28)\nTaking the derivative of the above bound with respect to ht and setting it to zero yields the optimal ht as\nht = ( 16dteq \u22173RmaxB \u22122L2T 2 q\u2217 \u22122n ) 1 1+\u03b1t .\nNote that substituting the above for \u03b1 := mint\u2208NT \u03b1t and d = maxt\u2208NT dt into (28), we can upper-bound the fixed point of r\u2217 as\nr\u2217 \u2264 8B 2\nn\n\u221a\ne \u03b1+ 1 \u03b1\u2212 1 ( dq\u22173RmaxB \u22122L2T 2 q\u2217 \u22122n ) 1 1+\u03b1\n+ 4 \u221a 2\u03b2eRmaxBLq \u2217 32T 1 q\u2217\nnT .\nAlso, the convergence rate of r\u2217 can be determined as\nr\u2217 = O\n\n\n(\nT 2\u2212 2 q\u2217\nq\u22173\n) \u22121 1+\u03b1\nn \u2212\u03b1 1+\u03b1\n\n .\nIt can be seen that the convergence rate can be as slow as O ( q\u22173/2T 1/q \u2217\nT \u221a n\n)\n(for small \u03b1 where at least one\n\u03b1t \u2248 1), and as fast as O(n\u22121) (for large \u03b1 where for all t, \u03b1t 7\u2192 \u221e). Therefore, one can observe that the bound obtained for the fixed point together with Theorem 13 provides a bound for the excess risk which leads to the following theorem.\nRemark 14 (Excess risk bounds for some strong convex matrix norm regularized MTL problems). Assume the convex class Fq in (13) has ranges in [\u2212b, b], and let the loss function \u2113 in Problem (6) be such that Assumptions 1 are satisfied. Assume moreover that k is a positive semidefinite kernel on X such that \u2016k\u2016\u221e \u2264 \u03b2 \u2264 \u221e. Also, denote \u03b1 := mint\u2208NT \u03b1t and d = maxt\u2208NT dt. Also,\n\u2022 Group norm (1 \u2264 q \u2264 2): If f\u0302 satisfies Pn\u2113f\u0302 = inff\u2208Fq Pn\u2113f , and r\u2217 is the fixed point of the local Rademacher complexity 2BLR(Fq, r4L2 ) with any 1 \u2264 q \u2264 2 in (13) and any K > 1, it holds with probability at least 1\u2212 e\u2212x,\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 min\n\u03ba\u2208[q,2] 423K\n\u221a\n\u03b1+ 1 \u03b1\u2212 1 ( d\u03ba\u22173RmaxL 2 ) 1 1+\u03b1 B \u03b1\u22121 \u03b1+1 ( T 2 \u03ba\u2217 \u22122 ) 1 1+\u03b1 n \u2212\u03b1 1+\u03b1\n+ 299\n\u221a \u03b2RmaxKL\u03ba \u2217 32T 1 \u03ba\u2217\nnT +\n(3Lb+ 4BK)x\nnT . (29)\n\u2022 Schatten-norm (1 \u2264 q \u2264 2): If f\u0302 satisfies Pn\u2113f\u0302 = inff\u2208FSq Pn\u2113f , and r\u2217 is the fixed point of the local Rademacher complexity 2BLR(FSq , r4L2 ) with any 1 \u2264 q \u2264 2 in (21) and any K > 1, it holds with probability at least 1\u2212 e\u2212x,\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 256K\n\u221a\n\u03b1+ 1 \u03b1\u2212 1 ( dq\u2217(q\u2217 \u2212 1)R\u2032maxL2 ) 1 1+\u03b1 B \u03b1\u22121 \u03b1+1 T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1\n+ (3Lb+ 4BK)x\nnT . (30)\n\u2022 Graph regularizer: If f\u0302 satisfies Pn\u2113f\u0302 = inff\u2208Fq Pn\u2113f , and r\u2217 is the fixed point of the local Rademacher complexity 2BLR(FG, r4L2 ) with any positive operator D in (22) and any K > 1, it holds with probability at least 1\u2212 e\u2212x,\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 256K\n\u221a\n\u03b1+ 1 \u03b1\u2212 1 ( dR\u2032\u2032maxL 2D\u22121max ) 1 1+\u03b1 B \u03b1\u22121 \u03b1+1 T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1\n+ (3Lb+ 4BK)x\nnT . (31)\nwhere D\u22121max := maxt\u2208NT D \u22121 tt .\nCorollary 15 (Data-dependent excess risk bound for a MTL problem with a strong convex L2,q group-norm regularizer). Assume the convex class Fq in (13) has ranges in [\u2212b, b], and let the loss function \u2113 in Problem (6) be such that Assumptions 1 are satisfied. Let f\u0302 be any element of Fq with 1 \u2264 q \u2264 2 which satisfies Pn\u2113f\u0302 = inff\u2208Fq Pn\u2113f . Assume moreover that k is a positive semidefinite kernel on X such that \u2016k\u2016\u221e \u2264 \u03b2 \u2264 \u221e. Let Kt be the n\u00d7n kernel Gram matrix of task t with entries (Kt)ij := k(X it , Xjt ); denote \u03bb\u03021t , . . . \u03bb\u0302nt its ordered eigenvalues. Let r\u0302\u2217 be the fixed point of\n\u03c8\u0302n(r) = c1R\u0302(F\u2217q , c3r) + c2x\nnT ,\nwhere c1 = 2Lmax (B, 16Lb), c2 = 8L 2b2 + c1 and c3 = 4 + 128K + 4B(3Lb+ 4BK)/c2, and\nR\u0302(F\u2217q , c3r) := E\u03c3\n\n  \nsup f\u2208Fq,\nL2Pn(f\u2212f\u0302) 2\u2264c3r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t) \u2223 \u2223 \u2223 \u2223 \u2223 { xit } t\u2208NT ,i\u2208Nn\n\n   . (32)\nThen, for any K > 1 and x > 0, with probability at least 1 \u2212 4e\u2212x the excess loss of function class Fq is bounded as\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264\n32K\nB r\u0302\u2217 +\n(3Lb+ 4BK)x\nnT , (33)\nwhere for the fixed point r\u0302\u2217 of the empirical local Rademacher complexity \u03c8\u0302n(r), it holds\nr\u0302\u2217 \u2264 c 2 1c3\n\u2211T t=1 ht\nnTL2 + 4\n\u221a \u221a \u221a \u221a \u221a \u221a 2c21q \u22172Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   n\u2211 j>ht \u03bb\u0302jt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+ 2c2x\nnT ,\nwhere h1, . . . , hT are arbitrary non-negative integers, and ( \u02c6 \u03bbjt ) n j=1 are eigenvalues of the empirical Gram matrix K obtained from kernel function k.\nProof. The proof of the result is provided in the Appendix.\n6 Discussion\n6.1 Global vs. Local Rademacher Complexity Bounds\nThis section is devoted to compare the excess risk bounds based on local Rademacher complexity to those of the global ones.\nFirst, note that to obtain the GRC-based bounds, we apply Theorem 16 of [38], as we consider the same setting and assumptions for tasks\u2019 distributions as considered in this work. This theorem presents an MTL bound based on the notion of GRC.\nTheorem 16 (MTL excess risk bound based on GRC; Theorem 16 of [38] ). Let F be a class of vector-valued functions f = (f1, . . . , fT ) : X 7\u2192 RT , that maps X into [\u2212b, b]T , and let X = (Xti ) (n,T ) (i,t)=(1,1) be a vector of independent random variables where for all fixed t, Xt1, . . . , X t n are identically distributed according to Pt. Then for every x > 0, with probability at least 1\u2212 e\u2212x,\nsup f\u2208F\n(Pf \u2212 Pnf) \u2264 2R(F) + \u221a bx\nnT . (34)\nProof. As it has been shown in [38], the proof of this theorem is based on using McDiarmid\u2019s inequality for Z defined in Theorem 1, and noticing that for the function class F with values in [\u2212b, b], it holds that |Z \u2212 Zs,j | \u2264 2b/nT .\nIt can be observed that, in order to obtain the excess risk bound in the above theorem, one has to bound the GRC term R(F) in (34). Therefore, we first upper-bound the GRC of different hypothesis spaces considered in the previous sections.\nTheorem 17 (Distribution-dependent GRC bounds). Assume that the conditions of Theorem 5 hold. Then, the following results hold for the GRC of the hypothesis spaces in (13), (21) and (22), respectively.\n\u2022 Group-norm regularizer: For any q \u2265 1 in (13), the GRC of the function class Fq can be bounded as\n\u2200\u03ba > q : R(Fq) \u2264 \u221a\n2e\u03ba\u22173Rmax nT 2 \u2225 \u2225 \u2225(tr (Jt)) T t=1 \u2225 \u2225 \u2225 \u03ba\u2217\n2\n+\n\u221a 2\u03b2eRmax\u03ba \u2217 32 T 1 \u03ba\u2217\nnT .\n\u2022 Schatten-norm regularizer: For any q \u2265 1 in (21), the GRC of the function class FSq can be bounded as\nR(FSq ) \u2264 \u221a 2R\u2032maxq \u2217(q\u2217 \u2212 1)\nnT 2\n\u2225 \u2225 \u2225(tr (Jt)) T t=1 \u2225 \u2225 \u2225 1 . (35)\n\u2022 Graph regularizer: For any positive operator D in (22), the GRC of the function class FG can be bounded as\nR(FG) \u2264 \u221a\n2R\u2032\u2032max nT 2 \u2225 \u2225 \u2225 ( D\u22121tt tr(Jt) )T t=1 \u2225 \u2225 \u2225 1 . (36)\nProof. The proof of the results can be found in the Appendix.\nNotice that, assuming a unique bound for the traces of all task\u2019s kernels, the bound above is determined\nby O\n(\nq\u2217 3 2 T 1 q\u2217\nT \u221a n\n)\n. Also, taking q\u2217 = logT , we obtain the bound R(F1) of order (log T ) 3 2\nT \u221a n . We can also remark\nthat when the traces of the kernels are bounded, the bounds for the Schatten norm and graph regularizers are the order of O (\n1\u221a nT\n)\n.\nNote that for the purpose of comparison, we concentrate only on the parameters R, n, T, q\u2217 and \u03b1 and assume all the other parameters are fixed and hidden in the big-O notation. Also, for the sake of simplicity, we assume that the eigenvalues of all tasks satisfy \u03bbjt \u2264 dj\u2212\u03b1 (with \u03b1 \u2265 1). Note that from Theorem 16, it follows that a bound on the global Rademacher complexity provides also a bound on the excess risk. This together with Theorem 17, gives the GRC-based excess risk bounds of the following forms\nGroup norm: \u2200\u03ba \u2208 [q, 2], P (\u2113 f\u0302 \u2212 \u2113f\u2217) = O\n(\n(Rmax\u03ba \u22173) 1 2\n(\nT 2\u2212 2 \u03ba\u2217 )\u2212 12 n\u2212 1 2\n)\n.\nSchatten-norm: \u2200q \u2208 [1, 2], P (\u2113 f\u0302 \u2212 \u2113f\u2217) = O\n(\n(R\u2032maxq \u2217(q\u2217 \u2212 1)) 12 T\u2212 12n\u2212 12\n)\n.\nGraph regularizer: P (\u2113 f\u0302 \u2212 \u2113f\u2217) = O\n(\n(R\u2032\u2032max) 1 2 T\u2212 1 2n\u2212 1 2\n)\n. (37)\nwhich can be compared to their LRC-based counterparts as following\nGroup norm: \u2200\u03ba \u2208 [q, 2], P (\u2113 f\u0302 \u2212 \u2113f\u2217) = O\n(\n(Rmax\u03ba \u22173) 1 1+\u03b1\n(\nT 2\u2212 2 \u03ba\u2217 )\u2212 11+\u03b1 n \u2212\u03b1 1+\u03b1\n)\n.\nSchatten-norm: \u2200q \u2208 [1, 2], P (\u2113 f\u0302 \u2212 \u2113f\u2217) = O\n(\n(R\u2032maxq \u2217(q\u2217 \u2212 1)) 11+\u03b1T \u221211+\u03b1n \u2212\u03b11+\u03b1\n)\n.\nGraph regularizer: P (\u2113 f\u0302 \u2212 \u2113f\u2217) = O\n(\n(R\u2032\u2032max) 1 1+\u03b1T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1\n)\n. (38)\nAs mentioned earlier in Remark 7, the bounds for the class of group norm regularizers for 1 \u2264 q \u2264 2 is not monotonic in q; they are minimized for q\u2217 = 23 logT . Therefore, we split our analysis for the group norm into two cases: first, we consider q\u2217 \u2265 23 logT , which leads to the optimal choice \u03ba\u2217 = q\u2217, and taking the minimum of the global and local bounds gives\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 O\n(\nmin\n{\n(Rmax\u03ba \u22173) 1 2\n(\nT 2\u2212 2 \u03ba\u2217 )\u2212 12 n\u2212 1 2 , (Rmax\u03ba \u22173) 1 1+\u03b1 ( T 2\u2212 2 \u03ba\u2217 )\u2212 11+\u03b1 n \u2212\u03b1 1+\u03b1\n})\n.\nIt can be seen that for \u03b1 \u2248 1, the minimum of the two terms are the same and local analysis has no advantages over the global one, however, for large value of \u03b1 (more specially when \u03b1 \u2192 \u221e), it can be shown that local analysis improves over the global one, if T and Rmax can grow with n such that T/ \u221a Rmax = O( \u221a n).\nsecondly, we assume q\u2217 \u2264 23 logT in which the best choice is \u03ba\u2217 = 23 logT . Then, the excess risk bound reads as\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 O\n( min {\nR1/2max(logT ) 3/2T\u22121n\u22121/2, n\u22121\n})\n,\nand the local analysis improves over the global one, when T/ \u221a Rmax(logT )3 = O( \u221a n). A similar analysis for Schatten norm and graph regularized hypothesis spaces shows that the local analysis is beneficial over the global one whenever T/R = O(n).\nA close appraisal of the results in (37) and (38) points to a conservation of asymptotic rates between n and T , when all other remaining quantities are held fixed. This phenomenon is more apparent for the Shatten norm and graph-based regularization cases. It can be seen that, for both the global and local analysis results, the rates (exponents) of n and T sum up to \u22121. In the local analysis case, the trade-off is determined by the value of \u03b1, which can facilitate faster n-rates and compromise with slower T -rates. A similar trade-off is witnessed in the case of group norm regularization, but this time between n and T 2(1\u2212 1 \u03ba\u2217\n), instead of T , due to specific character of the group norm.\n6.2 Comparisons to Related Works\nAlso, it would be interesting to compare our (global and local) results for the trace norm regularized MTL with the GRC-baesd excess risk bound provided in [41] wherein they apply a trace norm regularizer to capture the tasks\u2019 relatedness. It is worth mentioning that they consider a very slightly different hypothesis space for W , which in our notation reads as\nFS1 = { W : \u2016W \u2016S1 \u2264 R \u2032 max \u221a T } . (39)\nThe intuition behind this assumption is interpreted as: assuming a common vector w for all tasks, the regularizer should not be a function of number of tasks [41]. Given the task averaged covariance operator C := 1/T \u2211T\nt=1 Jt, the excess risk bound in [41] reads as (for the L-Lipschitz loss function \u2113, and F with ranges in [\u2212b, b])\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 2LR\u2032max\n(\u221a\n\u2016C\u2016\u221e n + 5\n\u221a\nln(nT ) + 1\nnT\n)\n+\n\u221a\nbLx nT .\nOne can easily verify that the trace norm is a Schatten norm with q = 1. Note that for any q \u2265 1 it holds that FS1 \u2286 FSq , which implies R(FS1) \u2264 R(FSq ). This together with Theorem 17 and Theorem 16 (applied to the class of excess loss functions) can provide a GRC-based excess risk bound. Therefore, considering the trace norm hypothesis space (39) and the optimal value of q\u2217 = 2, translates our global and local bounds to the following\n1. GRC-based excess risk bound:\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264\n4L\nT 1/4\n\u221a\nR\u2032max nT \u2225 \u2225 \u2225(tr (Jt)) T t=1 \u2225 \u2225 \u2225 1 +\n\u221a\nbLx nT .\n2. LRC-based excess risk bound (\u2200\u03b1 > 1):\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 256K\n\u221a\n\u03b1+ 1 \u03b1\u2212 1 ( 2dR\u2032maxL 2 ) 1 1+\u03b1 B \u03b1\u22121 \u03b1+1 T \u22121 2(1+\u03b1)n \u2212\u03b1 1+\u03b1 + (3Lb+ 4BK)x nT . (40)\nNow, assume that each operator Jt is of rank M and denote its maximum eigenvalue by \u03bb max t . If \u03bbmax := maxt\u2208NT {\u03bbmaxt }, then it is easy to verify that tr(Jt) \u2264 M\u03bbmaxt and \u2016C\u2016\u221e \u2264 \u03bbmax, which leads to the following GRC-based bounds\nOurs: P (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264\n4L\nT 1/4\n\u221a\nM\u03bbmaxR\u2032max n +\n\u221a\nbLx nT , (41)\n[41]: P (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 2LR\u2032max\n(\u221a\n\u03bbmax n + 5\n\u221a\nln(nT ) + 1\nnT\n)\n+\n\u221a\nbLx nT . (42)\nOne can observe that as n \u2192 \u221e, in all cases the bound approaches to zero, however our local bound in (40) at a rate of n\u2212\u03b1/1+\u03b1, our global bound in (41) at a slower rate of 1/ \u221a n, and the one in (42) at the slowest rate of \u221a lnn/n.\nWe remark that, as T \u2192 \u221e, the bound in (40) and (41) vanish at a rate of T\u22121/2(1+\u03b1) and \u221a 1/T 1/2,\nrespectively. Also, the bound in (42) approaches to the limiting value 2LR\u2032max \u221a\n\u03bbmax/n at a faster rate of \u221a\nlnT/T , however it does not vanish even for very large value of T . Another interesting comparison can be performed between our bounds and the one introduced in [39] for a graph regularized hypothesis spaces similar to (22). [39] provides a bound on the empirical GRC, however, similar to the proof of Corollary 12, we can easily convert it to a distribution dependent GRC bound which in our notation reads as (assuming that \u2225 \u2225 \u2225D 1/2W \u2225 \u2225 \u2225 \u2264 \u221a TR\u2032\u2032max)\nR (FG) \u2264 \u221a\nR\u2032\u20322max nT \u2225 \u2225 \u2225 ( D\u22121tt tr(Jt) )T t=1 \u2225 \u2225 \u2225 1 .\nNow, with D = L+ \u03b7I and the assumption of rank M for Jts, it can be shown that\n\u2225 \u2225 \u2225 ( D\u22121tt tr(Jt) )T\nt=1 \u2225 \u2225 \u2225 1 = T\u2211\nt=1\nD\u22121tt tr(Jt) \u2264 M\u03bbmax ( T\u2211\nt=1\nD\u22121tt\n)\n= M\u03bbmaxtr ( D\u22121 ) =\n= M\u03bbmaxtr (L+ \u03b7I) \u22121 = M\u03bbmax\n( T\u2211\nt=1\n1\n\u03b4t + \u03b7 +\n1\n\u03b7\n)\n= M\u03bbmax\n( T\n\u03b4min + \u03b7 +\n1\n\u03b7\n)\n.\nwhere \u03bbmax is defined as before, and (\u03b4t) T t=1 are the eigenvalues of Laplacian matrix L with \u03b4min := mint\u2208NT \u03b4t. Therefore, the GRC-based excess risk bounds are obtained as\nOurs: P (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 2L\u221a n\n\u221a\n2M\u03bbmaxR\u2032\u2032max T 1/2\n( 1\n\u03b4min +\n1\nT\u03b7\n)\n+\n\u221a\nbLx nT . (43)\n[39]: P (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 2L\u221a n\n\u221a\nM\u03bbmaxR\u2032\u20322max\n( 1\n\u03b4min +\n1\nT\u03b7\n)\n+\n\u221a\nbLx nT . (44)\nalso, from Remark 14, the LRC-based bound is given as\nP (\u2113 f\u0302 \u2212 \u2113f\u2217) \u2264 256K\n\u221a\n\u03b1+ 1 \u03b1\u2212 1 ( dR\u2032\u2032maxL 2D\u22121max ) 1 1+\u03b1 B \u03b1\u22121 \u03b1+1 T \u22121 2(1+\u03b1)n \u2212\u03b1 1+\u03b1 + (3Lb+ 4BK)x nT . (45)\nThe above results show that when n \u2192 \u221e, all three bounds above approach zero, albeit, the global bounds with a rate of \u221a\n1/n, and the local one with a faster rate of n\u2212\u03b1/\u03b1+1, as \u03b1 > 1. Also, as T \u2192 \u221e, our bound in (43) vanishes at a rate of \u221a 1/T 1/2, while the bound in (44) approaches to the limiting value\n2LR\u2032\u2032max\u221a n\n\u221a\nM\u03bbmax \u03b4min\nat a faster rate of \u221a\n1/T , however the cost of learning does not vanish. At the end, our local bound in (45) approaches zero at the slowest rate of T\u22121/2(1+\u03b1).\nA Appendix\nProofs of the results in Sect. 2: \u201cTalagrand-Type Inequality for\nMulti-Task Learning\u201d\nOur proof of Theorem 1 is based on the following Bousquet\u2019s version of Talagrand inequality. Theorem A.1 (Theorem 6.1 in [14]). Let (Z,Z \u20321, . . . , Z \u2032 n) be a sequence of A-measurable random variables and (Zk) n k=1 be a sequence of random variables Akn-measurable, where Akn is a sigma field generated by {Z1, . . . , Zn}/Zk. Assume that there exist c > 0, such that for all k = 1, . . . , n the following inequalities are satisfied\nZ \u2032k \u2264 Z \u2212 Zk \u2264 c, EknZ \u2032k = 0, n\u2211\nk=1\n(Z \u2212 Zk) \u2264 Z.\nIf we have \u2211n\nk=1 E k n [Z \u2032 k] 2 \u2264 n\u03c32 and \u03bd = 2cE(Z)+n\u03c32. Then with probability at least 1\u2212e\u2212x, for all x > 0, we have\nZ \u2264 EZ + \u221a 2\u03bdx+ cx\n3 .\nProof of Theorem 1\nDefine the quantities\nZ := sup f\u2208F\n1\nT\nT\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n[ Eft(Xt)\u2212 ft(X it) ] ,\nZs,j := sup f\u2208F\n1\nT\nT\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n[ Eft(Xt)\u2212 ft(X it) ] \u2212 1\nTNs\n[ Efs(Xs)\u2212 fs(Xjs ) ] .\nAlso, let f\u0302 := (f\u03021, . . . f\u0302T ) be such that Z = 1 T \u2211T t=1 1 Nt \u2211Nt i=1\n[ Ef\u0302t(Xt)\u2212 f\u0302t(X it) ] and similarly f\u0302 s,j =\n(f\u03021 s,j , . . . f\u0302T s,j\n) be the function achieving the supremum in the definition of Zs,j . Based on the definition of supremum, taking n := mint\u2208NT Nt, one can easily verify that\n1\nNsT\n[\nEf\u0302s s,j (Xs)\u2212 f\u0302s s,j (Xjs ) ] \u2264 Z \u2212 Zs,j \u2264 1\nNsT\n[ Ef\u0302s(Xs)\u2212 f\u0302s(Xjs ) ] \u2264 2b nT .\nLet Z \u2032s,j := 1\nNsT\n[\nE[f\u0302s s,j (Xs)]\u2212 f\u0302s s,j (Xjs ) ]\nand c := 2bnT . One can verify that Z is sub-additive, that is \u2211T\ns=1 \u2211Ns j=1(Z \u2212 Zs,j) \u2264 Z, and EZ \u2032s,j = 0. Thus, all the conditions of Theorem A.1 are satisfied. Also, we\nhave\nE [ Z \u2032s,j ]2 = 1\nN2s T 2 E\n[\nEf\u0302s s,j (Xs)\u2212 f\u0302s s,j (Xjs ) ]2 \u2264 1 N2s T 2 E [ f\u0302s s,j (Xjs ) ]2 . (A.1)\nTherefore,\nT\u2211\ns=1\nNs\u2211\nj=1\nE [ Z \u2032s,j ]2 \u2264 T\u2211\ns=1\nNs\u2211\nj=1\n1\nN2s T 2 E\n[\nf\u0302s s,j (Xjs ) ]2\n\u2264 T\u2211\ns=1\nNs\u2211\nj=1\n1\nN2s T 2 sup f\u2208F\nE [ fs(X j s ) ]2\n= T\u2211\ns=1\nNs\u2211\nj=1\n1\nN2s T 2 sup f\u2208F\nE [ fs(X 1 s ) ]2\n\u2264 r nT ,\nwhere in the equality above, we used the fact that for fixed s, Xjss are identically distributed. Therefore, applying Theorem A.1 for Z implies the following with probability at least 1\u2212 e\u2212x\nZ \u2264 EZ + \u221a 2x ( r\nnT + 2cEZ\n)\n+ cx\n3 .\nSubstituting c = 2bnT , and using the simple inequalities \u221a u+ v \u2264 \u221au+\u221av and 2\u221auv \u2264 u+v for any u, v \u2265 0, we get\nZ \u2264 2EZ + \u221a 2xr\nnT +\n8bx 3nT . (A.2)\nThe first term in the right-hand side of the above inequality, EZ, can also be upper-bounded using the same approach as in Theorem 16 in [38]. Let X \u2032 be an i.i.d. copy of the XnT -valued random variable X . Then,\nEZ = EX\n[\nsup f\u2208F\n1 T EX\u2032\n[ T\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n( ft ( X \u2032it ) \u2212 ft ( X it ))\n]]\n\u2264 EXX\u2032 [\nsup f\u2208F\n1\nT\nT\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n( ft ( X \u2032it ) \u2212 ft ( X it ))\n]\n= EXX\u2032\n[\nsup f\u2208F\n1\nT\nT\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n\u03c3it ( ft ( X \u2032it ) \u2212 ft ( X it ))\n]\n,\nsince for any i and t, the random variable ft ( X \u2032it ) \u2212 f ( X it ) has a symmetric distribution w.r.t. 0, therefore\nEZ \u2264 EXE\u03c3 [\nsup f\u2208F\n2\nT\nT\u2211\nt=1\n1\nNt\nNt\u2211\ni=1\n\u03c3itft(X i t)\n]\n= 2R(F),\nwhere R(F) is the Rademacher complexity of function class F . Therefore, upper-bounding (A.2), one can obtain with probability at least 1\u2212 e\u2212x,\nZ \u2264 4R(F) + \u221a 2xr\nnT +\n8bx 3nT ,\nwhich completes the proof.\nProofs of the results in Sect. 3: \u201cExcess MTL Risk Bounds based\non Local Rademacher Complexities\u201d\nThe following theorem in the core of the proof of Theorem 2 in Sect. 3.\nTheorem A.2 (Distribution-dependent bound for MTL). Let F = {f := (f1, . . . , fT )} be a class of vectorvalued functions satisfying supt,x |ft(x)| \u2264 b. Let X := (X it , Y it ) (T,n) (t,i)=(1,1) be a vector of nT independent random variables where (X1t , Y 1 t ) . . . , (X n t , Y n t ), \u2200t are identically distributed. Assume that there exist a constant B and a function T : F 7\u2192 R+ such that for every f \u2208 F , it holds that Pf2 \u2264 V (f ) \u2264 BPf . Let \u03c8 be a sub-root function with the fixed point r\u2217. If \u03c8 satisfies, for any r \u2265 r\u2217,\nBR(F , r) \u2264 \u03c8(r),\nwhere R(F , r) is the LRC of the function class F defined as\nR(F , r) := E\n\n  sup\nf\u2208F , V (f)\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t )\n\n  . (A.3)\nThen,\n1. For any function class F , K > 1 and x > 0, with probability at least 1\u2212 e\u2212x,\nPf \u2264 K K \u2212 1Pnf + 500K B r\u2217 + (6b+ 10BK)x nT . (A.4)\n2. For any convex function class F , K > 1 and x > 0, with probability at least 1\u2212 e\u2212x,\nPf \u2264 K K \u2212 1Pnf + 32K B r\u2217 + (3b+ 4BK)x nT . (A.5)\nProof. Define the rescaled version of F (its restriction to variance radius r) as\nFr := { f \u2032 = (f \u20321, . . . , f \u2032 T ) , f \u2032 t :=\nr\nmax (r, V (f )) ft, \u2200t, ft \u2208 F\n}\n. (A.6)\nFirst, we will show that every f \u2032 \u2208 Fr satisfies Pf \u20322 \u2264 r. Indeed, if we consider f \u2208 F such that V (f) \u2264 r, then by the definition of Fr, f \u2032t = ft, hence Pf \u20322 = Pf2 \u2264 V (f) \u2264 r. Otherwise, if V (f) \u2265 r, then f \u2032t = rft/V (f). Thus we have\nPf \u20322 = 1\nT\nT\u2211\nt=1\nPf \u20322t = r2\n(V (f )) 2\n(\n1\nT\nT\u2211\nt=1\nPf2t\n)\n= r2\n(V (f)) 2Pf\n2 \u2264 r 2\n(V (f )) 2V (f) \u2264 r.\nTherefore, we can conclude that for any f \u2032 \u2208 Fr, it holds Pf \u20322 \u2264 r. Also, as the functions in F has ranges in [\u2212b, b] and 0 \u2264 r/max(r, V (f)) \u2264 1, it can be seen that any f \u2032 \u2208 Fr satisfies \u2223 \u2223f \u2032 \u2223 \u2223 \u2264 2b, and \u2016f \u2032 \u2212 Pf \u2032\u2016\u221e \u2264 2b, consequently. Applying Theorem 1 on function class Fr, for all x > 0, with probability greater than 1\u2212 e\u2212x, gives\nsup f \u2032\u2208Fr\n[Pf \u2032 \u2212 Pnf \u2032] \u2264 4R(Fr) + \u221a 2xr\nnT +\n8bx 3nT . (A.7)\nNow for the proof of the first part, let F(u, v) := {f : u \u2264 V (f ) \u2264 v}, and define for class Fr,\nRnf := 1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t), Rn(F) := sup\nf\u2208F Rnf . (A.8)\nClearly, the Rademacher complexity of Fr is ERn(Fr). Also, it can be shown for any sets A and B\nE\n[\nsup f \u2032\u2208A\u222aB\nRnf \u2032 ] \u2264 E [\nsup f \u2032\u2208A\nRnf \u2032 ] + E [\nsup f \u2032\u2208B\nRnf \u2032 ] . (A.9)\nNote that for every f \u2208 F it holds that V (f ) \u2264 BPf \u2264 Bb. Also, let \u03bb > 1 and define k to be the smallest integer such that r\u03bbk+1 \u2265 Bb. Thus by (A.9),\nR(Fr) = E [\nsup f \u2032\u2208Fr\nRnf \u2032 ] = E [\nsup f\u2208F\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\nr\nmax(r, V (f )) \u03c3itft(X i t)\n]\n\u2264 E [\nsup f\u2208F(0,r)\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t)\n]\n+ E\n[\nsup f\u2208F(r,bB)\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\nr\nV (f ) \u03c3itft(X i t)\n]\n\u2264 R(F , r) + k\u2211\nj=0\n\u03bb\u2212jE\n[\nsup f\u2208F(r\u03bbj,r\u03bbj+1) Rnf\n]\n, (A.10)\nwhere in the last inequality, we applied (A.9) for the union of intervals Aj := (r\u03bbj , r\u03bbj+1). Therefore, it can be seen that \u03c8(r) \u2265 BR(F , r) implies\nR(Fr) \u2264 R(F , r) + k\u2211\nj=0\n\u03bb\u2212jE\n[\nsup f\u2208F(r\u03bbj,r\u03bbj+1) Rnf\n]\n\u2264 \u03c8(r) B + 1 B\nk\u2211\nj=0\n\u03bb\u2212j\u03c8(r\u03bbj+1). (A.11)\nNow since \u03c8(r) is a sub-root function, by the assumption it follows that for any \u03bb \u2265 1, \u03c8(\u03bbr) \u2264 \u221a \u03bb\u03c8(r), hence\nR(Fr) \u2264 \u03c8(r)\nB\n 1 + \u221a \u03bb k\u2211\nj=0\n\u03bb\u2212j/2\n\n .\nBy the optimal \u03bb = 4, the right-hand side can be upper-bounded by 5\u03c8(r)/B. Finally, for r \u2265 r\u2217, it holds \u03c8(r) \u2264 \u221a r/r\u2217\u03c8(r\u2217) = \u221a rr\u2217 and thus\nR(Fr) \u2264 5\nB\n\u221a rr\u2217. (A.12)\nCombining (A.7) and (A.12), for any r \u2265 r\u2217 and x > 0, with probability at least 1\u2212 e\u2212x, we have:\nsup f \u2032\u2208Fr\nPf \u2032 \u2212 Pnf \u2032 \u2264 20\nB\n\u221a rr\u2217 +\n\u221a\n2xr nT + 8bx 3nT . (A.13)\nFinally, in the following we need to convert the upper bound for functions in the weighted class Fr into a bound for functions in the initial class F . Denote the left hand side of the above inequality by V +r . We will show that if V +r \u2264 rBK , then\nPf \u2264 K K \u2212 1Pnf + r BK . (A.14)\nFirst, note that for any f \u2032 \u2208 Fr, it holds that Pf \u2032 \u2264 Pnf \u2032 + V +r . We also showed earlier, if V (f ) \u2264 r then f \u2032 = f , hence\nPf \u2264 Pnf + r\nBK . (A.15)\nOtherwise, if V (f) \u2265 r, then f \u2032 = rf/V (f). Thus, r\nV (f ) Pf \u2264 r V (f ) Pnf + V + r \u2264 r V (f) Pnf + r BK , (A.16)\nwhich coupled with V (f ) \u2264 BPf , implies that\nPf \u2264 Pnf + Pf\nK . (A.17)\nCombining (A.15) and (A.17) implies that if supf \u2032\u2208Fr Pf \u2032 \u2212 Pnf \u2032 \u2264 rBK , then (A.14) holds. Setting\nA = (20 \u221a r\u2217/B + \u221a 2x/nT ) and C = 8bx/3nT , the upper bound (A.13) can be written as A \u221a r + C. Now, we want to choose r0 \u2265 r\u2217 such that the upper bound of (A.13) becomes of a form r0/BK. We achieve this by considering the largest solution of A \u221a r0 + C = r0/BK which satisfies r0 \u2264 (BK)2A2 + 2BKC. Therefore, for every f \u2208 F we have\nPf \u2264 K K \u2212 1Pnf + r0 BK\n\u2264 K K \u2212 1Pnf +BKA 2 + 2C\n\u2264 K K \u2212 1Pnf +BK\n(\n400 B2 r\u2217 + 40 B\n\u221a\n2xr\u2217\nnT +\n2x\nnT\n)\n+ 16bx\n3nT . (A.18)\nUsing the fact that for any u, v \u2265 0 and \u03b1 > 0 it holds 2\u221auv \u2264 \u03b1u+v/\u03b1, we have \u221a\n2xr\u2217/nT \u2264 Bx/(5nT )+ 5r\u2217/(2B), we can complete the proof.\nAlso, the proof of the second part follows from the fact that Fr \u2286 {f \u2208 star(F , 0) : V (f ) \u2264 r}. From the other side, any convex function class F is star-shaped around any of its points. Therefore, R(Fr) in (A.7) can be bounded as R(Fr) \u2264 R(F , r) \u2264 \u03c8(r)/B. Then, following the same argument to convert the bound for the weighted class Fr into a bound for the functions in F completes the proof.\nThe following lemma which is a consequence of Corollary 2.2 from [7] is essential component of the proof in Theorem 4.\nLemma 1. Assume F is a class of vector-valued functions that map X into [\u2212b, b] with b > 0. For every x > 0, if r satisfies\nr \u2265 16L2bE\u03c3,X\n \n\nsup f\u2208F ,\nL2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t )\n \n\n+ 8L2b2x\nnT ,\nthen, with probability at least 1\u2212 e\u2212x, {\nf \u2208 F : L2P (f \u2212 f\u2217)2 \u2264 r } \u2282 { f \u2208 F : L2Pn (f \u2212 f\u2217)2 \u2264 2r } .\nProof. First, define\nF\u2217r := { f \u2032 = (f \u20321, . . . , f \u2032 T ) : \u2200t, f \u2032t = (ft \u2212 f\u2217t )2, ft \u2208 F , L2P (f \u2212 f\u2217)2 \u2264 r } .\nNote that for all t \u2208 NT , (ft \u2212 f\u2217t )2 \u2208 [0, b2]. Also, for any function in F\u2217r , it holds that\nPf \u20322 = 1\nT\nT\u2211\nt=1\nPf \u20322t = 1\nT\nT\u2211\nt=1\nP (ft \u2212 f\u2217t )4 \u2264 b2\nT\nT\u2211\nt=1\nP (ft \u2212 f\u2217t )2 = b2P (f \u2212 f\u2217) 2 \u2264 b\n2r\nL2 .\nTherefore, by Theorem 1, with probability at least 1\u2212 e\u2212x, every f \u2032 \u2208 F\u2217r satisfies\nPnf \u2032 \u2264 Pf \u2032 + 4R(F\u2217r ) +\n\u221a\n2b2xr nTL2 + 8b2x 3nT , (A.19)\nwhere\nR(F\u2217r ) = E\u03c3,X\n \n\nsup f\u2208F ,\nL2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3it(ft(X i t)\u2212 f\u2217t (X it))2\n \n\n\u2264 2bE\u03c3,X\n \n\nsup f\u2208F ,\nL2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t)\n \n\n. (A.20)\nThe last inequality follows from the facts that g(x) = x2 is 2b-Lipschitz on [\u2212b, b] and f is fixed. This together with (A.19), gives\nPnf \u2032 \u2264 Pf \u2032 + 8bE\u03c3,X\n \n\nsup f\u2208F ,\nL2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t)\n \n\n+\n\u221a\n2b2xr nTL2 + 8b2x 3nT\n\u2264 r L2 + 8bE\u03c3,X\n \n\nsup f\u2208F ,\nL2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t )\n \n\n+ r\n2L2 +\n11b2x\n3nT . (A.21)\nNote that multiplying both side by L2 completes the proof.\nproof of Theorem 4\nDefine the function \u03c8(r) as\n\u03c8(r) = c1 2 E\n\n  sup\nf\u2208F , L2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t )\n\n + (c2 \u2212 c1)x nT . (A.22)\nSince F is convex, it is star-shaped around any of its points, thus using Lemma 3.4 in [7] it can be shown that \u03c8(r) defined in (A.22) is a sub-root function. with the help of Corollary 3 and Assumptions 1, we have with probability at least 1\u2212 e\u2212x\nL2P ( f\u0302 \u2212 f\u2217 )2 \u2264 BP (\n\u2113 f\u0302 \u2212 \u2113f\u2217\n)\n\u2264 32Kr + (3Lb+ 4BK)Bx nT . (A.23)\nDenote the right hand side of the above inequality by s. Since s \u2265 r \u2265 r\u2217, then by the property of sub-root functions it holds that s \u2265 \u03c8(s), and thus\ns \u2265 16L2bE\n\n  sup\nf\u2208F , L2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t)\n\n +\n8L2b2x\nnT .\nApplying Lemma 1, we have with probability at least 1\u2212 e\u2212x, {\nf \u2208 F , L2P (f \u2212 f\u2217)2 \u2264 s } \u2282 { f \u2208 F , L2Pn (f \u2212 f\u2217)2 \u2264 2s } .\nCombining this with (A.23), gives with probability at least 1\u2212 2e\u2212x,\nL2Pn\n( f\u0302 \u2212 f\u2217 )2 \u2264 2 ( 32Kr + (3Lb+ 4BK)Bx\nnT\n)\n\u2264 2 ( 32K + (3Lb+ 4BK)B\nc2\n)\nr. (A.24)\nwhere in the last inequality we used the fact that r \u2265 \u03c8(r) \u2265 c2x/nT . Taking c = 2(32K+(3Lb+4BK)B/c2), and applying triangle inequality, if (A.24) holds, then for any f \u2208 F , we have\nL2Pn\n( f \u2212 f\u0302 )2 \u2264 ( \u221a L2Pn (f \u2212 f\u2217)2 + \u221a L2Pn ( f\u2217 \u2212 f\u0302 )2 )2\n(\u221a\nL2Pn (f \u2212 f\u2217)2 + \u221a cr\n)2\n. (A.25)\nNow, applying Lemma 1 for r \u2265 \u03c8(r), implies that with probability at least 1\u2212 3e\u2212x, {\nf \u2208 F , L2P (f \u2212 f\u2217)2 \u2264 r } \u2282 { f \u2208 F , L2Pn (f \u2212 f\u2217)2 \u2264 2r } ,\nwhich coupled with (A.25), implies that with probability at least 1\u2212 3e\u2212x, {\nf \u2208 F , L2P (f \u2212 f\u2217)2 \u2264 r } \u2282 { f \u2208 F , L2Pn ( f \u2212 f\u0302 )2 \u2264 (\u221a 2 + \u221a c )2 r } .\nThus, with the help of Lemma A.4 in [7], it can be shown that with probability at least 1\u2212 4e\u2212x,\n\u03c8(r) \u2264 c1E\u03c3\n\n  sup\nf\u2208F , L2P (f\u2212f\u2217)2\u2264r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t) \u2223 \u2223 \u2223 \u2223 \u2223 { xit } t\u2208NT ,i\u2208Nn\n\n +\nc2x nT\n\u2264 c1E\u03c3\n\n  \nsup f\u2208F ,\nL2Pn(f\u2212f\u0302)2\u2264( \u221a 2+ \u221a c)2r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t) \u2223 \u2223 \u2223 \u2223 \u2223 { xit } t\u2208NT ,i\u2208Nn\n\n   + c2x\nnT\n\u2264 c1E\u03c3\n\n  sup\n\u2200t, f\u2208F , L2Pn(f\u2212f\u0302)2\u2264(4+2c)r\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t ) \u2223 \u2223 \u2223 \u2223 \u2223 { xit } t\u2208NT ,i\u2208Nn\n\n +\nc2x nT\n\u2264 \u03c8\u0302(r). (A.26)\nSetting r = r\u2217 and applying Lemma 4.3 of [7], gives r\u2217 \u2264 r\u0302\u2217 which together with (A.23) yields the result.\nProofs of the results in Sect. 4: \u201cLocal Rademacher Complexity\nBounds for MTL models with Strongly Convex Regularizers\u201d\nIn the following, we would like to provide some basic notions of convex analysis which are helpful in understanding the results of Sect. 4.\nDefinition 1 (Strong Convexity). A function R : X 7\u2192 R is \u00b5-strong convex w.r.t. a norm \u2016.\u2016 if and only if \u2200x, y \u2208 X and \u2200\u03b1 \u2208 (0, 1), we have\nR(\u03b1x + (1\u2212 \u03b1)y) \u2264 \u03b1R(x) + (1\u2212 \u03b1)R(y)\u2212 \u00b5 2 \u03b1(1 \u2212 \u03b1)\u2016x\u2212 y\u20162.\nDefinition 2 (Strong Smoothness). A function R\u2217 : X 7\u2192 R is 1\u00b5 -strong smooth w.r.t. a norm \u2016.\u2016\u2217 if and only if R\u2217 is everywhere differentiable and \u2200x, y \u2208 X , we have\nR\u2217(x+ y) \u2264 R\u2217(x) + \u3008\u25bdR\u2217(x), y\u3009+ 1 2\u00b5 \u2016y\u20162\u2217 .\nProperty 1 (Theorem 3 in [21]: Strong convexity/strong smoothness duality). A function R is \u00b5-strongly convex w.r.t. the norm \u2016.\u2016 if and only if its Fenchel conjugate R\u2217 is 1\u00b5 -strongly smooth w.r.t. the dual norm \u2016.\u2016\u2217. The Fenchel conjugate R\u2217 is defined as\nR\u2217(w) := sup v {\u3008w,v\u3009 \u2212R(v)} .\nProperty 2 (Fenchel-Young inequality). The definition of Fenchel dual implies that for any strong convex function R,\n\u2200w,v \u2208 S, \u3008w,v\u3009 \u2264 R(w) +R\u2217(v).\nCombining this with the strong duality property of R\u2217 gives the following\n\u3008w,v\u3009 \u2212R(w) \u2264 R\u2217(v) \u2264 R\u2217(0) + \u3008\u25bdR\u2217(0),v\u3009+ 1 2\u00b5 \u2016v\u20162\u2217 . (A.27)\nLemma 2. Assume that the conditions of Theorem 5 hold. Then, for ever f \u2208 Fq,\n(a) Pf2 \u2264 r implies 1/T \u2211Tt=1 \u2211\u221e j=1 \u03bb j t\n\u2329\nwt,u j t\n\u232a2\n\u2264 r.\n(b) EX,\u03c3 \u2329 1 n \u2211n i=1 \u03c3 i t\u03c6(X i t ),u j t \u232a2 = \u03bbjt n .\nProof. Part (a)\nPf2 = 1\nT\nT\u2211\nt=1\nE (\u2329 wt, \u03c6(X i t) \u232a)2 1\nT\nT\u2211\nt=1\nE (\u2329 wt \u2297wt, \u03c6(X it )\u2297 \u03c6(X it) \u232a)\n= 1\nT\nT\u2211\nt=1\n\u2329 wt \u2297wt,EX ( \u03c6(X it)\u2297 \u03c6(X it ) )\u232a = 1\nT\nT\u2211\nt=1\n\u221e\u2211\nj=1\n\u03bbjt\n\u2329 wt \u2297wt,ujt \u2297 ujt \u232a\n= 1\nT\nT\u2211\nt=1\n\u221e\u2211\nj=1\n\u03bbjt\n\u2329\nwt,u j t\n\u232a\u2329\nwt,u j t\n\u232a\n= 1\nT\nT\u2211\nt=1\n\u221e\u2211\nj=1\n\u03bbjt\n\u2329\nwt,u j t\n\u232a2\n\u2264 r.\nPart (b)\nEX,\u03c3\n\u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t ),u j t\n\u232a2\n= 1\nn2 EX,\u03c3\nn\u2211\ni,k=1\n\u03c3it\u03c3 k t\n\u2329\n\u03c6(X it ),u j t\n\u232a\u2329\n\u03c6(Xkt ),u j t\n\u232a\n\u03c3ti.i.d.= 1\nn2 EX\n( n\u2211\ni=1\n\u2329\n\u03c6(X it ),u j t\n\u232a2 )\n= 1\nn\n\u2329\n1\nn\nn\u2211\ni=1\nEX ( \u03c6(X it )\u2297 \u03c6(X it) ) ,ujt \u2297 ujt\n\u232a\n= 1\nn\n\u221e\u2211\nl=1\n\u03bblt\n\u2329 ult \u2297 ult,ujt \u2297 ujt \u232a = \u03bbjt n .\nWe will use following lemmas in the proof of the LRC bound for the L2,q-group norm regularized MTL in Corollary 6.\nLemma 3 (Khintchine-Kahane Inequality [47]). Let H be an inner-product space with induced norm \u2016\u00b7\u2016H, v1, . . . , vM \u2208 H and \u03c31, . . . , \u03c3n i.i.d. Rademacher random variables. Then, for any p \u2265 1, we have that\nE\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 n\u2211\ni=1\n\u03c3ivi \u2225 \u2225 \u2225 \u2225 \u2225 p\nH\n\u2264 ( c n\u2211\ni=1\n\u2016vi\u20162H\n) p 2\n. (A.28)\nwhere c := max {1, p\u2212 1}. The inequality also holds for p in place of c.\nLemma 4 (Rosenthal-Young Inequality; Lemma 3 of [24]). Let X1, . . . , Xn be independent, non-negative random variables satisfying Xi \u2264 B < +\u221e almost surely for all i = 1, . . . , n. If q \u2265 12 , cq := (2qe)q, then it holds\nE\n(\n1\nn\nn\u2211\ni=1\nXi\n)q \u2264 Cq [( B\nn\n)q\n+\n(\n1\nn\nn\u2211\ni=1\nEXi\n)q]\n. (A.29)\nProof of Corollary 6\nFor the group norm regularizer \u2016W \u20162,q, we can further bound the expectation term in (12) for D = I as following\nE := EX,\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u2329 1 n n\u2211 i=1 \u03c3it\u03c6(X i t ),u j t \u232a u j t   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2,q\u2217\n= EX,\u03c3\n\n \nT\u2211\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj>ht\n\u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t),u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217 \n \n2 q\u2217\nJensen \u2264 EX\n\n \nT\u2211\nt=1\nE\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj>ht\n\u2329\n1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t),u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217 \n \n2 q\u2217\n(A.28)\n\u2264 EX\n\n  \nT\u2211\nt=1\n\n q\n\u2217 n\u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj>ht\n\u2329 1\nn \u03c6(X it ),u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n2 \n \nq\u2217\n2\n\n  \n2 q\u2217\n= q\u2217\nn EX\n\n \nT\u2211\nt=1\n  \u2211\nj>ht\n1\nn\nn\u2211\ni=1\n\u2329\n\u03c6(X it ),u j t\n\u232a2\n\n\nq\u2217\n2\n\n \n2 q\u2217\nJensen \u2264 q\n\u2217\nn\n\n \nT\u2211\nt=1\nEX\n  \u2211\nj>ht\n1\nn\nn\u2211\ni=1\n\u2329\n\u03c6(X it ),u j t\n\u232a2\n\n\nq\u2217\n2\n\n \n2 q\u2217\n(A.30)\nNote that for q \u2264 2, it holds that q\u2217/2 \u2265 1. Therefore we cannot employ Jensen inequality to move the expectation operator inside the inner term, and we need to apply the Rosenthal-Young (R+Y) inequality (see Lemma 4 in the Appendix), which yields\nE R+Y \u2264 q\n\u2217\nn\n\n \nT\u2211\nt=1\n(eq\u2217) q\u2217 2\n\n \n( \u03b2\nn\n) q\u2217 2\n+\n  \u2211\nj>ht\n1\nn\nn\u2211\ni=1\nEX\n\u2329\n\u03c6(X it),u j t\n\u232a2\n\n\nq\u2217\n2\n\n \n\n \n2 q\u2217\n= q\u2217\nn\n\n \nT\u2211\nt=1\n(eq\u2217) q\u2217 2\n\n \n( \u03b2\nn\n) q\u2217 2\n+\n  \u2211\nj>ht\n\u03bbjt\n\n\nq\u2217\n2\n\n \n\n \n2 q\u2217\n. (A.31)\nThis can be further bounded, using the sub-additivity of q\u2217 \u221a . and \u221a . respectively in (\u2020\u2020) and (\u2020) below,\nE (\u2020) \u2264 eq\n\u22172\nn\n\n   \n\nT\n( \u03b2\nn\n) q\u2217 2\n\n\n2 q\u2217\n+\n\n \nT\u2211\nt=1\n  \u2211\nj>ht\n\u03bbjt\n\n\nq\u2217\n2\n\n \n2 q\u2217\n\n   \n(\u2020\u2020) \u2264 eq\n\u22172\nn\n\n  \n\u03b2T 2 q\u2217\nn + \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n\n  \n= \u03b2eq\u22172T 2 q\u2217\nn2 +\neq\u22172\nn \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n. (A.32)\nPlugging this into (12), and using subadditivity of \u221a . gives,\nA2(Fq) \u2264 \u221a \u221a \u221a \u221a \u221a \u221a 2eq\u22172Rmax nT 2\u00b5 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n. (A.33)\nNow, combining (11) and (A.33) provides the bound on R(Fq, r) as\nR(Fq, r) \u2264\n\u221a\nr \u2211T\nt=1 ht nT + \u221a \u221a \u221a \u221a \u221a \u221a 2eq\u22172Rmax nT 2\u00b5 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n(A.34)\n(\u22c6) \u2264 \u221a \u221a \u221a \u221a \u221a \u221a \u221a 2\nnT\n\n   r\nT\u2211\nt=1\nht + 2eq\u22172Rmax\nT\u00b5\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n\n   +\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n(\u22c6\u22c6)\n\u2264 \u221a \u221a \u221a \u221a \u221a \u221a \u221a 2\nnT\n\n   rT 1\u2212 2 q\u2217 \u2225 \u2225 \u2225(ht) T t=1 \u2225 \u2225 \u2225 q\u2217\n2\n+ 2eq\u22172Rmax\nT\u00b5\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n\n   +\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n(\u22c6\u22c6\u22c6)\n\u2264 \u221a \u221a \u221a \u221a \u221a \u221a 4\nnT \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225  rT 1\u2212 2 q\u2217 ht + 2eq\u22172Rmax T\u00b5 \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n.\nwhere in (\u22c6), (\u22c6\u22c6) and (\u22c6\u22c6\u22c6) we applied following inequalities receptively, according which for all non-negative numbers \u03b11 and \u03b12, and non-negative vectors a1,a2 \u2208 RT with 0 \u2264 q \u2264 p \u2264 \u221e and s \u2265 1 it holds\n(\u22c6) \u221a \u03b11 + \u221a \u03b12 \u2264 \u221a 2(\u03b11 + \u03b12)\n(\u22c6\u22c6) lp \u2212 to\u2212 lq : \u2016a1\u2016q = \u30081,a1\u3009 1 q Ho\u0308lder \u2264\n(\n\u20161\u2016(p/q)\u2217 \u2016a q 1\u2016(p/q)\n) 1 q\n= T 1 q\u2212 1p \u2016a1\u2016p\n(\u22c6 \u22c6 \u22c6) \u2016a1\u2016s + \u2016a2\u2016s \u2264 21\u2212 1 s \u2016a1 + a2\u2016s \u2264 2 \u2016a1 + a2\u2016s .\nSince inequality (\u22c6 \u22c6 \u22c6) holds for all non-negative ht, it follows\nR(Fq, r) \u2264 \u221a \u221a \u221a \u221a \u221a \u221a 4\nnT \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225  min ht\u22650 rT 1\u2212 2 q\u2217 ht + 2eq\u22172Rmax T\u00b5 \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n\u2264 \u221a \u221a \u221a \u221a \u221a \u221a 4\nnT \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u221e\u2211 j=1 min ( rT 1\u2212 2 q\u2217 , 2eq\u22172Rmax T\u00b5 \u03bbjt )   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+\n\u221a 2\u03b2eRmaxq \u2217T 1 q\u2217\nnT \u221a \u00b5\n.\nAlso, from Theorem 3 and Theorem 13 in [21], it can be shown that R(W ) = \u2016W \u201622,q is 1q\u2217 -strongly convex w.r.t. the group norm \u2016.\u20162,q. , we can conclude the result.\nProof of Theorem 9\nR(Fq,R,T , r) = 1\nT EX,\u03c3\n \n\nsup Pf2\u2264r,\n\u2016W\u20162,q\u2264Rmax\nT\u2211\nt=1\n\u2329\nwt, 1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t )\n\u232a\n \n\n= 1\nT EX,\u03c3\n  \n\nsup 1/T\n\u2211T t=1 E\u3008wt,\u03c6(Xt)\u3009\n2\u2264r, \u2016W \u20162,q\u2264Rmax\nT\u2211\nt=1\n\u2329\nwt, 1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t )\n\u232a\n  \n\n\u2265 1 T EX,\u03c3\n \n\nsup \u2200t EX\u3008wt,\u03c6(Xt)\u30092\u2264r,\n\u2016W \u20162,q\u2264Rmax, \u2016w1\u20162=...=\u2016wt\u20162\nT\u2211\nt=1\n\u2329\nwt, 1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t )\n\u232a\n \n\n= 1\nT EX,\u03c3\n \n sup \u2200t EX\u3008wt,\u03c6(Xt)\u30092\u2264r, \u2200t \u2016wt\u20162\u2264RmaxT \u2212 1 q\nT\u2211\nt=1\n\u2329\nwt, 1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t )\n\u232a\n \n\n= 1\nT\nT\u2211\nt=1\nEX,\u03c3\n \n sup \u2200t EX\u3008wt,\u03c6(Xt)\u30092\u2264r, \u2200t \u2016wt\u20162\u2264RmaxT \u2212 1 q\n\u2329\nwt, 1\nn\nn\u2211\ni=1\n\u03c3it\u03c6(X i t )\n\u232a\n \n\n= EX,\u03c3\n \n sup EX\u3008w1,\u03c6(X1)\u30092\u2264r, \u2016w1\u20162\u2264RmaxT \u2212 1 q\n\u2329\nw1, 1\nn\nn\u2211\ni=1\n\u03c3i1\u03c6(X i 1)\n\u232a\n \n\n= R(F 1,RT \u2212 1 q ,1 , r).\nAccording to [43], it can be shown that there is a constant c such that if \u03bb1t \u2265 1nR2max , then for all r \u2265 1 n\nit holds R(F 1,RT \u2212 1 q ,1\n, r) \u2265 \u221a\nc n \u2211\u221e j=1 min\n(\nr, R2T\u2212 2 q \u03bbj1\n)\n, which with some algebra manipulations gives the\ndesired result. The following lemma is used in the proof of the LRC bounds for the LSq -Schatten norm regularized MTL in Corollary 10.\nLemma 5 (Non-commutative Khintchine\u2019s inequality [33]). Let Q1, . . . ,Qn be a set of arbitrary m \u00d7 n matrices, and let \u03c31, . . . , \u03c3n be a sequence of independent Bernoulli random variables. Than for all p \u2265 2,\n\nE\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 n\u2211\ni=1\n\u03c3iQi \u2225 \u2225 \u2225 \u2225 \u2225 p\nSp\n\n\n1/p\n\u2264 p1/2 max\n \n \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( n\u2211\ni=1\nQTi Qi )1/2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 Sp , \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( n\u2211 i=1 QiQ T i )1/2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 Sp\n \n\n. (A.35)\nProof of Corollary 10\nIn order to find an LRC bound for a LSq -Schatten norm regularized hypothesis space (21), one just needs to bound the expectation term in (8). Define U it as a matrix with T columns where its only non-zero t th column is defined as \u2211\nj>ht \u2329 1 n\u03c6(X i t ),u j t \u232a u j t . Also, note that for the Schatten norm regularized hypothesis\nspace (21), it holds that D = I. Therefore, applying Lemma 5 yields,\nEX,\u03c3 \u2225 \u2225 \u2225D \u22121/2V \u2225 \u2225 \u2225 2\n\u2217 = EX,\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u2329 1 n n\u2211 i=1 \u03c3it\u03c6(X i t),u j t \u232a u j t   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nSq\u2217\n= EX,\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 T\u2211\nt=1\nn\u2211\ni=1\n\u03c3itU i t \u2225 \u2225 \u2225 \u2225 \u2225 2\nSq\u2217\nJensen \u2264 EX\n \n E\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 T\u2211\nt=1\nn\u2211\ni=1\n\u03c3itU i t \u2225 \u2225 \u2225 \u2225 \u2225 q\u2217\nSq\u2217\n \n\n2 q\u2217\n(A.35)\n\u2264 EX\n \n\nq\u22171/2 max\n \n \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( T\u2211\nt=1\nn\u2211\ni=1\n( U it )T U it )1/2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 Sq\u2217 , \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( T\u2211 t=1 n\u2211 i=1 U it ( U it )T )1/2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 Sq\u2217\n \n\n \n\n2\nA\u00a9 = q\u2217EX \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( T\u2211\nt=1\nn\u2211\ni=1\n( U it )T U it )1/2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\nSq\u2217\n= q\u2217EX\n\n tr\n( T\u2211\nt=1\nn\u2211\ni=1\n( U it )T U it\n) q \u2217\n2\n\n \n2 q\u2217\n= q\u2217EX\n\n  \n\n \nT\u2211\nt=1\nn\u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj>ht\n\u2329 1\nn \u03c6(X it ),u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n2 \n \nq\u2217\n2\n\n  \n2 q\u2217\n= q\u2217EX\n\n \nT\u2211\nt=1\nn\u2211\ni=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2211\nj>ht\n\u2329 1\nn \u03c6(X it ),u j t\n\u232a\nu j t \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\n2 \n \n= q\u2217\nn2 EX\n\n\nT\u2211\nt=1\nn\u2211\ni=1\n\u2211\nj>ht\n\u2329\n\u03c6(X it ),u j t\n\u232a2\n\n\nJensen \u2264 q\n\u2217\nn2\n\n\nT\u2211\nt=1\nn\u2211\ni=1\n\u2211\nj>ht\n\u03bbjt\n  = q\u2217\nn \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 . (A.36)\nwhere in A\u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.r.t. q-Schatten norm \u2016.\u2016Sq . Plugging this into (8) completes the proof.\nProof of Corollary 12\nSimilar to the proof of Corollary 10, for the graph regularized hypothesis space (22), one can bound the expectation term in (8) as\nEX,\u03c3 \u2225 \u2225 \u2225D \u22121/2V \u2225 \u2225 \u2225 2\n\u2217 = EX,\u03c3\n[ tr ( V TD\u22121V )]\nJensen \u2264 EX\n\n 1\nn2\nT,T \u2211\nt,s=1\nn,n \u2211\ni,l=1\n\u2211\nj>ht\n\u2211\nk>hs\nD\u22121st E\u03c3 ( \u03c3it\u03c3 l s\n) \u2329\n\u03c6(X it),u j t \u232a \u2329 \u03c6(X ls),u k s \u232a \u2329 u j t ,u k s \u232a\n\n\n= EX\n\n 1\nn\nT\u2211\nt=1\nD\u22121tt \u2211\nj>ht\n1\nn\nn\u2211\ni=1\n\u2329\n\u03c6(X it ),u j t\n\u232a2\n\n\n= 1\nn\nT\u2211\nt=1\nD\u22121tt \u2211\nj>ht\n1\nn\nn\u2211\ni=1\nEX\n\u2329\n\u03c6(X it ),u j t\n\u232a2\n= 1\nn\nT\u2211\nt=1\n\u2211\nj>ht\nD\u22121tt \u03bb j t =\n1\nn \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225  D\u22121tt \u2211 j>ht \u03bbjt   T\nt=1 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 . (A.37)\nproof of Corollary 15\nFirst notice that R\u0302(F\u2217q , c3r) \u2264 2R\u0302(Fq, c3r4L2 ). Also, similar to the proof of Remark 6 it can be show that\nR\u0302(Fq, c3r\n4L2 ) \u2264\n\u221a\nc3r \u2211T\nt=1 ht 4nTL2 + \u221a \u221a \u221a \u221a \u221a \u221a 2q\u22172Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u2211 j>ht \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n.\nTherefore,\n\u03c8\u0302n(r) \u2264 2c1\n\n  \n\u221a\nc3r \u2211T\nt=1 ht 4nTL2 + \u221a \u221a \u221a \u221a \u221a \u221a 2q\u22172Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   n\u2211 j>ht \u03bb\u0302jt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n\n   + c2x\nnT\n=\n\u221a\nc21c3r \u2211T\nt=1 ht nTL2 + \u221a \u221a \u221a \u221a \u221a \u221a 8c21q \u22172Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   n\u2211 j>ht \u03bb\u0302jt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+ c2x\nnT .\nDenote the right hand side by \u03c8\u0302ubn (r). Solving the fixed point equation \u03c8\u0302 ub n (r) =\n\u221a \u03b1r + \u03b3 = r for\n\u03b1 = c21c3\n\u2211T t=1 ht\nnTL2 , \u03b3 =\n\u221a \u221a \u221a \u221a \u221a \u221a 8c21q \u22172Rmax nT 2 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   n\u2211 j>ht \u03bb\u0302jt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n+ c2x\nnT (A.38)\ngives r\u0302\u2217 \u2264 \u03b1+ 2\u03b3. Substituting \u03b1 and \u03b3 completes the proof.\nProof of the results in Sect. 6.1: \u201cGlobal vs. Local Rademacher\nComplexity Bounds\u201d\nProof of Theorem 17\nNote that regarding the definition of A2 in (10), the global rademacher complexity for each case can be obtained by replacing the tail-sum \u2211\nj>ht \u03bbjt in the bound of its corresponding A2(F) by \u2211\u221e j=1 \u03bb j t = tr(Jt).\nIndeed, similar to the proof of Theorem 5, it can be shown that for the group norm with \u03ba = q \u2208 [1, 2],\nR(Fq) = EX,\u03c3 {\nsup f=(f1,...,fT )\u2208Fq\n1\nnT\nT\u2211\nt=1\nn\u2211\ni=1\n\u03c3itft(X i t)\n}\n\u2264 1 T\n\u221a \u221a \u221a \u221a \u221a 2R\n\u00b5 EX,\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( 1 n n\u2211\ni=1\n\u03c3it\u03c6(X i t )\n)T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2,q\u2217\n.\nAlso, one can verify the following\nEX,\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 ( 1 n n\u2211\ni=1\n\u03c3it\u03c6(X i t)\n)T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2,q\u2217\n= EX,\u03c3 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u221e\u2211 j=1 \u2329 1 n n\u2211 i=1 \u03c3it\u03c6(X i t),u j t \u232a u j t   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2,q\u2217\n\u2264 q \u22172\u03b2eT 2 q\u2217\nn2 +\nq\u22172e\nn \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225   \u221e\u2211 j=1 \u03bbjt   T\nt=1\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225\nq\u2217\n2\n= q\u22172\u03b2eT 2 q\u2217\nn2 +\nq\u22172e\nn\n\u2225 \u2225 \u2225(tr (Jt)) T t=1 \u2225 \u2225 \u2225\nq\u2217\n2\n. (A.39)\nwhere the last inequality obtained in a similar way in (A.32). The GRC bounds for the other cases can be easily derived in a very similar way.\nReferences\n[1] Qi An, Chunping Wang, Ivo Shterev, Eric Wang, Lawrence Carin, and David B Dunson. Hierarchical kernel stick-breaking process for multi-task image analysis. In Proceedings of the 25th international conference on Machine learning, pages 17\u201324. ACM, 2008.\n[2] Rie Kubota Ando and Tong Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. The Journal of Machine Learning Research, 6:1817\u20131853, 2005.\n[3] Andreas Argyriou, Ste\u0301phan Cle\u0301menc\u0327on, and Ruocong Zhang. Learning the graph of relations among multiple tasks. ICML workshop on New Learning Frameworks and Models for Big Data, 2014.\n[4] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243\u2013272, 2008.\n[5] Andreas Argyriou, Andreas Maurer, and Massimiliano Pontil. An algorithm for transfer learning in a heterogeneous environment. In Machine Learning and Knowledge Discovery in Databases, pages 71\u201385. Springer, 2008.\n[6] Andreas Argyriou, Massimiliano Pontil, Yiming Ying, and Charles A Micchelli. A spectral regularization framework for multi-task structure learning. In Advances in neural information processing systems, pages 25\u201332, 2007.\n[7] Peter L Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Annals of Statistics, pages 1497\u20131537, 2005.\n[8] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463\u2013482, March 2003. Available from: http://dl.acm.org/citation.cfm?id=944919.944944.\n[9] Jonathan Baxter. A model of inductive bias learning. J. Artif. Intell. Res.(JAIR), 12(149-198):3, 2000.\n[10] Shai Ben-David and Reba Schuller Borbely. A notion of task relatedness yielding provable multiple-task learning guarantees. Machine learning, 73(3):273\u2013287, 2008.\n[11] Shai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In Learning Theory and Kernel Machines, pages 567\u2013580. Springer, 2003.\n[12] Steffen Bickel, Jasmina Bogojeska, Thomas Lengauer, and Tobias Scheffer. Multi-task learning for hiv therapy screening. In Proceedings of the 25th international conference on Machine learning, pages 56\u201363. ACM, 2008.\n[13] Olivier Bousquet. A bennett concentration inequality and its application to suprema of empirical processes. Comptes Rendus Mathematique, 334(6):495\u2013500, 2002.\n[14] Olivier Bousquet. Concentration inequalities for sub-additive functions using the entropy method. In Evariste Gin, Christian Houdr, and David Nualart, editors, Stochastic Inequalities and Applications, volume 56 of Progress in Probability, pages 213\u2013247. Birkhuser Basel, 2003. Available from: http://dx.doi.org/10.1007/978-3-0348-8069-5_14, doi:10.1007/978-3-0348-8069-5_14.\n[15] Bin Cao, Nathan N Liu, and Qiang Yang. Transfer learning for collective link prediction in multiple heterogenous domains. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 159\u2013166, 2010.\n[16] Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n[17] Corinna Cortes, Marius Kloft, and Mehryar Mohri. Learning kernels using local rademacher complexity. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 2760\u20132768. Curran Associates, Inc., 2013. Available from: http://papers.nips.cc/paper/4896-learning-kernels-using-local-rademacher-complexity.pdf.\n[18] Corinna Cortes and Mehryar Mohri. Algorithmic Learning Theory: 22nd International Conference, ALT 2011, Espoo, Finland, October 5-7, 2011. Proceedings, chapter Domain Adaptation in Regression, pages 308\u2013323. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. Available from: http://dx.doi.org/10.1007/978-3-642-24412-4_25, doi:10.1007/978-3-642-24412-4_25.\n[19] Corinna Cortes and Mehryar Mohri. Domain adaptation and sample bias correction theory and algorithm for regression. Theoretical Computer Science, 519:103 \u2013 126, 2014. Algorithmic Learning Theory. Available from: http://www.sciencedirect.com/science/article/pii/S0304397513007184, doi:http://dx.doi.org/10.1016/j.tcs.2013.09.027.\n[20] A Evgeniou and Massimiliano Pontil. Multi-task feature learning. Advances in neural information processing systems, 19:41, 2007.\n[21] Sham M Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for learning with matrices. The Journal of Machine Learning Research, 13(1):1865\u20131890, 2012.\n[22] Zhuoliang Kang, Kristen Grauman, and Fei Sha. Learning with whom to share in multi-task feature learning. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 521\u2013528, 2011.\n[23] Marius Kloft and Gilles Blanchard. The local rademacher complexity of lpnorm multiple kernel learning. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2438\u20132446. Curran Associates, Inc., 2011. Available from: http://papers.nips.cc/paper/4259-the-local-rademacher-complexity-of-lp-norm-multiple-kernel-learning.pdf.\n[24] Marius Kloft and Gilles Blanchard. On the convergence rate of lp-norm multiple kernel learning. The Journal of Machine Learning Research, 13(1):2465\u20132502, 2012.\n[25] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error of combined classifiers. Ann. Statist., 30(1):1\u201350, 02 2002. Available from: http://dx.doi.org/10.1214/aos/1015362183, doi:10.1214/aos/1015362183.\n[26] Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimization. Ann. Statist., 34(6):2593\u20132656, 12 2006. Available from: http://dx.doi.org/10.1214/009053606000001019, doi:10.1214/009053606000001019.\n[27] Vladimir Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. J. Mach. Learn. Res., 11:2457\u20132485, December 2010. Available from: http://dl.acm.org/citation.cfm?id=1756006.1953014.\n[28] Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. arXiv preprint arXiv:1206.6417, 2012.\n[29] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer Science & Business Media, 2013.\n[30] Yunwen Lei, Lixin Ding, and Yingzhou Bi. Local rademacher complexity bounds based on covering numbers. arXiv:1510.01463 [cs.AI], 2015.\n[31] Cong Li, Michael Georgiopoulos, and Georgios C Anagnostopoulos. Multitask classification hypothesis space with improved generalization bounds. Neural Networks and Learning Systems, IEEE Transactions on, 2013.\n[32] K Lounici, M Pontil, AB Tsybakov, and SA Van De Geer. Taking advantage of sparsity in multi-task learning. In COLT 2009-The 22nd Conference on Learning Theory, 2009.\n[33] F. Lust-Piquard. Khintchine inequalities in cp (1 < p < \u221e). COMPTES RENDUS DE L ACADEMIE DES SCIENCES SERIE I-MATHEMATIQUE, 303(7):289\u2013292, 1986.\n[34] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009). Omnipress, June 2009.\n[35] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1041\u20131048. Curran Associates, Inc., 2009. Available from: http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf.\n[36] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Multiple source adaptation and the rE\u0301nyi divergence. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, pages 367\u2013374, Arlington, Virginia, United States, 2009. AUAI Press. Available from: http://dl.acm.org/citation.cfm?id=1795114.1795157.\n[37] Yishay Mansour and Mariano Schain. Robust domain adaptation. Annals of Mathematics and Artificial Intelligence, 71(4):365\u2013380, 2013. Available from: http://dx.doi.org/10.1007/s10472-013-9391-5, doi:10.1007/s10472-013-9391-5.\n[38] Andreas Maurer. Bounds for linear multi-task learning. The Journal of Machine Learning Research, 7:117\u2013139, 2006.\n[39] Andreas Maurer. The rademacher complexity of linear transformation classes. In Learning Theory, pages 65\u201378. Springer, 2006.\n[40] Andreas Maurer. Algorithmic Learning Theory: 25th International Conference, ALT 2014, Bled, Slovenia, October 8-10, 2014. Proceedings, chapter A Chain Rule for the Expected Suprema of Gaussian Processes, pages 245\u2013259. Springer International Publishing, Cham, 2014. Available from: http://dx.doi.org/10.1007/978-3-319-11662-4_18, doi:10.1007/978-3-319-11662-4_18.\n[41] Andreas Maurer and Massimiliano Pontil. Excess risk bounds for multitask learning with trace norm regularization. In Conference on Learning Theory, volume 30, pages 55\u201376, 2013.\n[42] Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask representation learning. arXiv preprint arXiv:1505.06279, 2015.\n[43] Shahar Mendelson. On the performance of kernel classes. The Journal of Machine Learning Research, 4:759\u2013771, 2003.\n[44] Luca Oneto, Alessandro Ghio, Sandro Ridella, and Davide Anguita. Local rademacher complexity: Sharper risk bounds with and without unlabeled samples. Neural Networks, 65:115 \u2013 125, 2015. Available from: http://www.sciencedirect.com/science/article/pii/S0893608015000404, doi:http://dx.doi.org/10.1016/j.neunet.2015.02.006.\n[45] Anastasia Pentina and Shai Ben-David. Multi-task and lifelong learning of kernels. In Algorithmic Learning Theory, pages 194\u2013208. Springer, 2015.\n[46] Anastasia Pentina and Christoph H Lampert. Lifelong learning with non-iid tasks. In Advances in Neural Information Processing Systems, pages 1540\u20131548, 2015.\n[47] G Peshkir and Albert Nikolaevich Shiryaev. The khintchine inequalities and martingale expanding sphere of their action. Russian Mathematical Surveys, 50(5):849\u2013904, 1995.\n[48] Ting Kei Pong, Paul Tseng, Shuiwang Ji, and Jieping Ye. Trace norm regularization: Reformulations, algorithms, and multi-task learning. SIAM Journal on Optimization, 20(6):3465\u20133489, 2010.\n[49] Bernardino Romera-Paredes, Andreas Argyriou, Nadia Berthouze, and Massimiliano Pontil. Exploiting unrelated tasks in multi-task learning. In International Conference on Artificial Intelligence and Statistics, pages 951\u2013959, 2012.\n[50] Michel Talagrand. New concentration inequalities in product spaces. Inventiones mathematicae, 126(3):505\u2013563, 1996.\n[51] S Thrun. Learning to learn: Introduction. In In Learning To Learn, 1996.\n[52] I. Tolstikhin, G. Blanchard, and M. Kloft. Localized complexities for transductive learning. In Proceedings of the 27th Conference on Learning Theory, volume 35, pages 857\u2013884. JMLR, 2014. Available from: http://jmlr.org/proceedings/papers/v35/tolstikhin14.pdf.\n[53] Qian Xu, Sinno Jialin Pan, Hannah Hong Xue, and Qiang Yang. Multitask learning for protein subcellular location prediction. IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB), 8(3):748\u2013759, 2011.\n[54] Chao Zhang, Lei Zhang, and Jieping Ye. Generalization bounds for domain adaptation. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 3320\u20133328. Curran Associates, Inc., 2012. Available from: http://papers.nips.cc/paper/4684-generalization-bounds-for-domain-adaptation.pdf.\n[55] Yu Zhang and Dit-Yan Yeung. Multi-task warped gaussian process for personalized age estimation. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 2622\u20132629. IEEE, 2010."}], "references": [{"title": "Hierarchical kernel stick-breaking process for multi-task image analysis", "author": ["Qi An", "Chunping Wang", "Ivo Shterev", "Eric Wang", "Lawrence Carin", "David B Dunson"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Learning the graph of relations among multiple tasks", "author": ["Andreas Argyriou", "St\u00e9phan Cl\u00e9men\u00e7on", "Ruocong Zhang"], "venue": "ICML workshop on New Learning Frameworks and Models for Big Data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "An algorithm for transfer learning in a heterogeneous environment", "author": ["Andreas Argyriou", "Andreas Maurer", "Massimiliano Pontil"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["Andreas Argyriou", "Massimiliano Pontil", "Yiming Ying", "Charles A Micchelli"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Local rademacher complexities", "author": ["Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "A model of inductive bias learning", "author": ["Jonathan Baxter"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "A notion of task relatedness yielding provable multiple-task learning guarantees", "author": ["Shai Ben-David", "Reba Schuller Borbely"], "venue": "Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["Shai Ben-David", "Reba Schuller"], "venue": "In Learning Theory and Kernel Machines,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Multi-task learning for hiv therapy screening", "author": ["Steffen Bickel", "Jasmina Bogojeska", "Thomas Lengauer", "Tobias Scheffer"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "A bennett concentration inequality and its application to suprema of empirical processes", "author": ["Olivier Bousquet"], "venue": "Comptes Rendus Mathematique,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Concentration inequalities for sub-additive functions using the entropy method", "author": ["Olivier Bousquet"], "venue": "Birkhuser Basel,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Transfer learning for collective link prediction in multiple heterogenous domains", "author": ["Bin Cao", "Nathan N Liu", "Qiang Yang"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Learning kernels using local rademacher complexity", "author": ["Corinna Cortes", "Marius Kloft", "Mehryar Mohri"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Algorithmic Learning Theory: 22nd International Conference, ALT", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Domain adaptation and sample bias correction theory and algorithm for regression", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": "Theoretical Computer Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "Advances in neural information processing systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Regularization techniques for learning with matrices", "author": ["Sham M Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The local rademacher complexity of lpnorm multiple kernel learning", "author": ["Marius Kloft", "Gilles Blanchard"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "On the convergence rate of lp-norm multiple kernel learning", "author": ["Marius Kloft", "Gilles Blanchard"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Ann. Statist., 30(1):1\u201350,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Local rademacher complexities and oracle inequalities in risk minimization", "author": ["Vladimir Koltchinskii"], "venue": "Ann. Statist., 34(6):2593\u20132656,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Rademacher complexities and bounding the excess risk in active learning", "author": ["Vladimir Koltchinskii"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daume III"], "venue": "arXiv preprint arXiv:1206.6417,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Probability in Banach Spaces: isoperimetry and processes", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": "Springer Science & Business Media,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Local rademacher complexity bounds based on covering numbers", "author": ["Yunwen Lei", "Lixin Ding", "Yingzhou Bi"], "venue": "[cs.AI],", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Multitask classification hypothesis space with improved generalization bounds", "author": ["Cong Li", "Michael Georgiopoulos", "Georgios C Anagnostopoulos"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Taking advantage of sparsity in multi-task learning", "author": ["K Lounici", "M Pontil", "AB Tsybakov", "SA Van De Geer"], "venue": "In COLT 2009-The 22nd Conference on Learning Theory,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Khintchine inequalities in cp (1 < p < \u221e)", "author": ["F. Lust-Piquard"], "venue": "COMPTES RENDUS DE L ACADEMIE DES SCIENCES SERIE I-MATHEMATIQUE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of The 22nd Annual Conference on Learning Theory (COLT", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Domain adaptation with multiple sources", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Multiple source adaptation and the r\u00c9nyi divergence", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Robust domain adaptation", "author": ["Yishay Mansour", "Mariano Schain"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Bounds for linear multi-task learning", "author": ["Andreas Maurer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "The rademacher complexity of linear transformation classes. In Learning Theory, pages 65\u201378", "author": ["Andreas Maurer"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Algorithmic Learning Theory: 25th International Conference, ALT", "author": ["Andreas Maurer"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Excess risk bounds for multitask learning with trace norm regularization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "In Conference on Learning Theory,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "The benefit of multitask representation learning", "author": ["Andreas Maurer", "Massimiliano Pontil", "Bernardino Romera-Paredes"], "venue": "arXiv preprint arXiv:1505.06279,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2015}, {"title": "On the performance of kernel classes", "author": ["Shahar Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2003}, {"title": "Local rademacher complexity: Sharper risk bounds with and without unlabeled samples", "author": ["Luca Oneto", "Alessandro Ghio", "Sandro Ridella", "Davide Anguita"], "venue": "Neural Networks,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2015}, {"title": "Multi-task and lifelong learning of kernels", "author": ["Anastasia Pentina", "Shai Ben-David"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Lifelong learning with non-iid tasks", "author": ["Anastasia Pentina", "Christoph H Lampert"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "The khintchine inequalities and martingale expanding sphere of their action", "author": ["G Peshkir", "Albert Nikolaevich Shiryaev"], "venue": "Russian Mathematical Surveys,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1995}, {"title": "Trace norm regularization: Reformulations, algorithms, and multi-task learning", "author": ["Ting Kei Pong", "Paul Tseng", "Shuiwang Ji", "Jieping Ye"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Exploiting unrelated tasks in multi-task learning", "author": ["Bernardino Romera-Paredes", "Andreas Argyriou", "Nadia Berthouze", "Massimiliano Pontil"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "New concentration inequalities in product spaces", "author": ["Michel Talagrand"], "venue": "Inventiones mathematicae,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1996}, {"title": "Learning to learn: Introduction", "author": ["S Thrun"], "venue": "In In Learning To Learn,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1996}, {"title": "Localized complexities for transductive learning", "author": ["I. Tolstikhin", "G. Blanchard", "M. Kloft"], "venue": "In Proceedings of the 27th Conference on Learning Theory,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "Multitask learning for protein subcellular location prediction", "author": ["Qian Xu", "Sinno Jialin Pan", "Hannah Hong Xue", "Qiang Yang"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "Generalization bounds for domain adaptation", "author": ["Chao Zhang", "Lei Zhang", "Jieping Ye"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Multi-task warped gaussian process for personalized age estimation", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 8, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 1, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 3, "context": "Pioneering works on MTL include [16, 9, 2, 4].", "startOffset": 32, "endOffset": 45}, {"referenceID": 0, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 105, "endOffset": 109}, {"referenceID": 54, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 145, "endOffset": 149}, {"referenceID": 52, "context": "Some recent examples include image segmentation [1], HIV therapy screening [12], collaborative filtering [15], age estimation from facial images [55], and sub-cellular location prediction [53] just to name a few prominent ones.", "startOffset": 188, "endOffset": 192}, {"referenceID": 24, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 145, "endOffset": 149}, {"referenceID": 7, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 175, "endOffset": 178}, {"referenceID": 37, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 209, "endOffset": 213}, {"referenceID": 38, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 215, "endOffset": 219}, {"referenceID": 20, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 221, "endOffset": 225}, {"referenceID": 40, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 227, "endOffset": 231}, {"referenceID": 39, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 233, "endOffset": 237}, {"referenceID": 41, "context": "MTL learning guarantees centered around the notion of (global) Rademacher averages and associated complexities, notions that were put forward in [25] and further developed in [8], have been notably pursued in [38], [39], [21], [41], [40] and [42]; these works are briefly surveyed in Sect.", "startOffset": 242, "endOffset": 246}, {"referenceID": 25, "context": "More recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)).", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "More recently, the seminal works in [26] and [7] introduced a more nuanced variant of these complexities, termed Local Rademacher Complexity (LRC) (as opposed to the original Global Rademacher Complexity (GRC)).", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "This new, modified function class complexity measure is attention-worthy, since, as shown in [7], a LRCs-based (local) analysis is capable of producing more rapidly-converging excess risk bounds, when compared to the ones obtained via a GRC (global) analysis.", "startOffset": 93, "endOffset": 96}, {"referenceID": 26, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 177, "endOffset": 181}, {"referenceID": 22, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 208, "endOffset": 212}, {"referenceID": 16, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 217, "endOffset": 221}, {"referenceID": 51, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 245, "endOffset": 249}, {"referenceID": 43, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 267, "endOffset": 271}, {"referenceID": 29, "context": "To date, there have been only a few additional works attempting to reap the benefits of such local analysis in various contexts: active learning for binary classification tasks [27], multiple kernel learning [23] and [17], transductive learning [52], semi-supervised [44] and bounds on the LRCs via covering numbers [30].", "startOffset": 316, "endOffset": 320}, {"referenceID": 37, "context": "3 Previous Related Works Earlier works that investigate MTL generalization guarantees employing Rademacher averages include [38], which considers linear MTL frameworks for binary classification.", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "Another study, [39], provides bounds for the empirical and expected Rademacher complexities of", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "In [21], the authors take advantage of the strongly-convex nature of certain matrix-norm regularizers to easily obtain generalization bounds for a variety of machine learning problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 40, "context": "Moreover, [41] presents a global Rademacher complexity analysis leading to both data and distribution-dependent excess risk bounds of order O( \u221a log(n)/n) and non-vanishing w.", "startOffset": 10, "endOffset": 14}, {"referenceID": 39, "context": "Also, [40] examines the bounding of (global) Gaussian complexities of function classes that result from considering composite maps, as it is typical in MTL among other settings.", "startOffset": 6, "endOffset": 10}, {"referenceID": 41, "context": "More recently, [42] presents excess risk bounds for both MTL and Learning-To-Learn (LTL) settings and reveals conditions, under which MTL is more beneficial over learning tasks independently.", "startOffset": 15, "endOffset": 19}, {"referenceID": 38, "context": "The accompanying bounds are of order O(1/ \u221a nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T \u2192 +\u221e.", "startOffset": 81, "endOffset": 89}, {"referenceID": 40, "context": "The accompanying bounds are of order O(1/ \u221a nT ) and, compared to the results in [39, 41], it enjoys the advantage of vanishing as T \u2192 +\u221e.", "startOffset": 81, "endOffset": 89}, {"referenceID": 50, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 10, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 9, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 45, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 83, "endOffset": 99}, {"referenceID": 44, "context": "Generalization performance analysis in life-long learning has been investigated in [51, 11, 10, 46] and [45].", "startOffset": 104, "endOffset": 108}, {"referenceID": 33, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 35, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 34, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 17, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 53, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 36, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 82, "endOffset": 106}, {"referenceID": 18, "context": "Also, in the context of domain adaptation, similar considerations are examined in [34, 36, 35, 18, 54, 37] and [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "2 Talagrand-Type Inequality for Multi-Task Learning The derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand\u2019s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F .", "startOffset": 179, "endOffset": 187}, {"referenceID": 49, "context": "2 Talagrand-Type Inequality for Multi-Task Learning The derivation of our LRC-based error bounds for MTL is founded on the following modified Talagrand\u2019s concentration inequality [13, 50] adapted to the context of MTL, showing that the uniform deviation between the true and empirical means in a vector-valued function class F can be dominated by the associated multitask Rademacher complexity plus a term involving the variance of functions in F .", "startOffset": 179, "endOffset": 187}, {"referenceID": 37, "context": "In Theorem 1, the data from different tasks assumed to be mutually independent, which is typical in the MTL setting [38].", "startOffset": 116, "endOffset": 120}, {"referenceID": 28, "context": "Also, using Talagrand\u2019s Lemma [29], one can verify", "startOffset": 30, "endOffset": 34}, {"referenceID": 6, "context": "3 of [7] to MTL function classes) to the function class H\u2217 F completes the proof.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "4 in [7], presents a data-dependent version of (5) replacing the Rademacher complexity in Corollary 3 with its empirical counterpart.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "4 in [7] and, therefore, can be found in the Appendix.", "startOffset": 5, "endOffset": 8}, {"referenceID": 38, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 19, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 5, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 3, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 30, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 2, "context": "In the following we demonstrate the power of Theorem 5 by applying it to study LRC bounds for popular MTL models, including group norm, Schatten norm and graph regularized MTL models extensively studied in the literature of MTL [39, 20, 6, 4, 31, 3].", "startOffset": 228, "endOffset": 249}, {"referenceID": 19, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 3, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 31, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 48, "context": "3 Group Norm Regularized MTL We first consider the group norm regularized MTL capturing the inter-task relationships by the group norm \u2016W \u20162,q := (\u2211T t=1 \u2016wt\u2016 q 2 )1/q [20, 4, 32, 49], for which the associated hypothesis space takes the form Fq := { X 7\u2192 [\u3008w1, \u03c6(X1)\u3009 , .", "startOffset": 168, "endOffset": 183}, {"referenceID": 5, "context": "Among p \u2265 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take \u03ba\u2217 = logT to get that R(F1, r) \u2264 \u221a \u221a \u221a \u221a 4 nT \u2225 \u2225 \u2225 ( \u221e \u2211", "startOffset": 115, "endOffset": 121}, {"referenceID": 3, "context": "Among p \u2265 1, a particular group-norm of independent interest is the sparsity-inducing group-norm achieved by q = 1 [6, 4], for which we can take \u03ba\u2217 = logT to get that R(F1, r) \u2264 \u221a \u221a \u221a \u221a 4 nT \u2225 \u2225 \u2225 ( \u221e \u2211", "startOffset": 115, "endOffset": 121}, {"referenceID": 5, "context": "4 Schatten Norm Regularized MTL [6] developed a spectral regularization framework for MTL where the Schatten p-norm \u2016W\u2016Sq := [ tr ( W W ) q 2 ] 1 q", "startOffset": 32, "endOffset": 35}, {"referenceID": 40, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 85, "endOffset": 93}, {"referenceID": 47, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 85, "endOffset": 93}, {"referenceID": 4, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 210, "endOffset": 221}, {"referenceID": 27, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 210, "endOffset": 221}, {"referenceID": 21, "context": "Trace-norm regularized MTL, corresponding to Schatten norm regularization with q = 1 [41, 48], imposes a low-rank structure on the spectrum of W and can also be interpreted as low dimensional subspace learning [5, 28, 22].", "startOffset": 210, "endOffset": 221}, {"referenceID": 38, "context": "We consider the following graph regularized MTL [39]", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q \u2208 [1, 2] in (13).", "startOffset": 121, "endOffset": 127}, {"referenceID": 1, "context": "Note that, due to the space limitation, the proofs of the results are provided only for the hypothesis space Fq with q \u2208 [1, 2] in (13).", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "However, for the group and LSq -Schatten norm (q \u2208 [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "However, for the group and LSq -Schatten norm (q \u2208 [1, 2]) regularized MTL, the proofs can be obtained in a very similar way.", "startOffset": 51, "endOffset": 57}, {"referenceID": 6, "context": "4 in [7], Fq is a sub-root function.", "startOffset": 5, "endOffset": 8}, {"referenceID": 37, "context": "First, note that to obtain the GRC-based bounds, we apply Theorem 16 of [38], as we consider the same setting and assumptions for tasks\u2019 distributions as considered in this work.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "Theorem 16 (MTL excess risk bound based on GRC; Theorem 16 of [38] ).", "startOffset": 62, "endOffset": 66}, {"referenceID": 37, "context": "As it has been shown in [38], the proof of this theorem is based on using McDiarmid\u2019s inequality for Z defined in Theorem 1, and noticing that for the function class F with values in [\u2212b, b], it holds that |Z \u2212 Zs,j | \u2264 2b/nT .", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 2 T\u2212 12n\u2212 12 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 2 T\u2212 12n\u2212 12 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 1+\u03b1T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 1, "context": "Schatten-norm: \u2200q \u2208 [1, 2], P (l f\u0302 \u2212 lf\u2217) = O ( (R\u2032 maxq \u2217(q\u2217 \u2212 1)) 1 1+\u03b1T \u22121 1+\u03b1n \u2212\u03b1 1+\u03b1 ) .", "startOffset": 20, "endOffset": 26}, {"referenceID": 40, "context": "2 Comparisons to Related Works Also, it would be interesting to compare our (global and local) results for the trace norm regularized MTL with the GRC-baesd excess risk bound provided in [41] wherein they apply a trace norm regularizer to capture the tasks\u2019 relatedness.", "startOffset": 187, "endOffset": 191}, {"referenceID": 40, "context": "(39) The intuition behind this assumption is interpreted as: assuming a common vector w for all tasks, the regularizer should not be a function of number of tasks [41].", "startOffset": 163, "endOffset": 167}, {"referenceID": 40, "context": "Given the task averaged covariance operator C := 1/T \u2211T t=1 Jt, the excess risk bound in [41] reads as (for the L-Lipschitz loss function l, and F with ranges in [\u2212b, b])", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "[41]: P (l f\u0302 \u2212 lf\u2217) \u2264 2LR\u2032 max (\u221a \u03bbmax n + 5 \u221a ln(nT ) + 1 nT ) + \u221a bLx nT .", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Another interesting comparison can be performed between our bounds and the one introduced in [39] for a graph regularized hypothesis spaces similar to (22).", "startOffset": 93, "endOffset": 97}, {"referenceID": 38, "context": "[39] provides a bound on the empirical GRC, however, similar to the proof of Corollary 12, we can easily convert it to a distribution dependent GRC bound which in our notation reads as (assuming that \u2225 \u2225 \u2225D W \u2225 \u2225 \u2225 \u2264 \u221a TR\u2032\u2032 max)", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39]: P (l f\u0302 \u2212 lf\u2217) \u2264 2L \u221a n \u221a M\u03bbmaxR\u2032\u20322 max ( 1 \u03b4min + 1 T\u03b7 ) + \u221a bLx nT .", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "1 in [14]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 37, "context": "2) The first term in the right-hand side of the above inequality, EZ, can also be upper-bounded using the same approach as in Theorem 16 in [38].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "2 from [7] is essential component of the proof in Theorem 4.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "4 in [7] it can be shown that \u03c8(r) defined in (A.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "4 in [7], it can be shown that with probability at least 1\u2212 4e\u2212x, \u03c8(r) \u2264 c1E\u03c3 \uf8ee", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "3 of [7], gives r\u2217 \u2264 r\u0302\u2217 which together with (A.", "startOffset": 5, "endOffset": 8}, {"referenceID": 20, "context": "Property 1 (Theorem 3 in [21]: Strong convexity/strong smoothness duality).", "startOffset": 25, "endOffset": 29}, {"referenceID": 46, "context": "Lemma 3 (Khintchine-Kahane Inequality [47]).", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "Lemma 4 (Rosenthal-Young Inequality; Lemma 3 of [24]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Also, from Theorem 3 and Theorem 13 in [21], it can be shown that R(W ) = \u2016W \u201622,q is 1 q -strongly convex w.", "startOffset": 39, "endOffset": 43}, {"referenceID": 42, "context": "According to [43], it can be shown that there is a constant c such that if \u03bbt \u2265 1 nRmax , then for all r \u2265 1 n it holds R(F 1,RT \u2212 1 q ,1 , r) \u2265 \u221a c n \u2211\u221e j=1 min ( r, R2T\u2212 2 q \u03bbj1 ) , which with some algebra manipulations gives the desired result.", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": "Lemma 5 (Non-commutative Khintchine\u2019s inequality [33]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "where in A \u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "where in A \u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.", "startOffset": 190, "endOffset": 196}, {"referenceID": 1, "context": "where in A \u00a9, we assumed that the first term in the max argument is the largest Note that using Theorem 11 in [21], it can be shown that the regularization function R(W ) = \u2016W \u20162Sq with q \u2208 [1, 2] is (q \u2212 1)-strongly convex w.", "startOffset": 190, "endOffset": 196}, {"referenceID": 0, "context": "Indeed, similar to the proof of Theorem 5, it can be shown that for the group norm with \u03ba = q \u2208 [1, 2], R(Fq) = EX,\u03c3 {", "startOffset": 96, "endOffset": 102}, {"referenceID": 1, "context": "Indeed, similar to the proof of Theorem 5, it can be shown that for the group norm with \u03ba = q \u2208 [1, 2], R(Fq) = EX,\u03c3 {", "startOffset": 96, "endOffset": 102}], "year": 2017, "abstractText": "We show a Talagrand-type of concentration inequality for MTL, using which we establish sharp excess risk bounds for Multi-Task Learning (MTL) in terms of distributionand data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including\u2014as we demonstrate\u2014Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading off slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.", "creator": "LaTeX with hyperref package"}}}