{"id": "1602.06483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Social planning for social HRI", "abstract": "making a computational agent'social'has implications for how it perceives itself and the environment in which environments it is situated, including the ability to recognise the behaviours of others. we subsequently point to recent work on social planning, i. e. planning in settings where the social context cue is relevant in the assessment of the beliefs and capabilities of others, assessing and in making themselves appropriate choices within of what to do toward next.", "histories": [["v1", "Sun, 21 Feb 2016 01:47:23 GMT  (145kb,D)", "http://arxiv.org/abs/1602.06483v1", "Presented at \"2nd Workshop on Cognitive Architectures for Social Human-Robot Interaction 2016 (arXiv:1602.01868)\""]], "COMMENTS": "Presented at \"2nd Workshop on Cognitive Architectures for Social Human-Robot Interaction 2016 (arXiv:1602.01868)\"", "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["liz sonenberg", "tim miller", "adrian pearce", "paolo felli", "christian muise", "frank dignum"], "accepted": false, "id": "1602.06483"}, "pdf": {"name": "1602.06483.pdf", "metadata": {"source": "CRF", "title": "Social planning for social HRI", "authors": ["Liz Sonenberg", "Tim Miller", "Adrian Pearce", "Paolo Felli", "Christian Muise", "Frank Dignum"], "emails": ["l.sonenberg@unimelb.edu.au"], "sections": [{"heading": null, "text": "Social planning for social HRI Liz Sonenberg\nTim Miller Adrian Pearce\nDepartment of Computing and Information Systems\nUniversity of Melbourne Australia\nContact: l.sonenberg@unimelb.edu.au\nPaolo Felli University of Nottingham\nUK\nChristian Muise MIT, Boston\nUSA\nFrank Dignum University of Utrecht\nThe Netherlands\nIndex Terms\u2014Nested belief reasoning; Theory of Mind reasoning; Multi-agent planning; Epistemic planning."}, {"heading": "I. RESEARCH CONTEXT", "text": "Making a computational agent \u2018social\u2019 has implications for how it perceives itself and the environment in which it is situated, including the ability to recognise the behaviours of others at various levels \u2013 simple actions, goals and intentions. Hence fundamental elements of an architecture for social agents must allow for management of social motivations - i.e. to reach social goals, not only practical goals - and must model and account for actions having both practical and social effects. Further it has been argued that to build social agents it is not sufficient to just add a few \u2018social modules\u2019 to existing architectures: while multilayer computational cognitive models have been studied for some time, c.f. [1], a new layered deliberation architecture is required that at the higher level(s) naturally accommodates analysis of decision choices that take into account both rich context and future projections of possible consequences, yet does not rely on computational expensive deep reasoning capability [2], [3], [4].\nIn the work reported here, we do not attempt to address the \u2018large\u2019 questions associated with the design of a fully integrated computational cognitive architecture; rather we adopt a relatively narrow focus on exploiting and extending epistemic planning mechanisms to achieve run-time generation of plans in rich multi-actor contexts, i.e. we seek to construct social plans in settings where the social context is relevant in the assessment of the beliefs and capabilities of others, and in making appropriate choices of what to do next.\nOur approach has been informed by our experience with the BDI model of agency [5] and several associated agent architectures - architectures that were introduced to support a balance of deliberative and reactive behaviours, and that in their instantiation are reliant on domain-specific expert knowledge acquisition to provide a knowledge level view [6], c.f. [7], [8]. We are also supporters of the position that logicbased techniques are well suited to represent social reasoning and through which to engineer effective mechanisms, c.f. [9], [10], [11].\nFundamental concepts we build on include: reasoning about the beliefs of others, including their beliefs about others; establishing common ground; and the use of stereotypes. So a few words about each.\nExploiting mutual awareness to enable a participant engaged in collaborative activity with others to select an appropriate action typically involves Theory of Mind (ToM) reasoning [12], [13], i.e., reasoning about the knowledge, beliefs, perspectives and reasoning of other participants. Agent-based computational models have been used to investigate higherorder ToM in varied scenarios, including alignment with human performance data in some cases e.g., [14], [15], [16], [17], [18], [19].\nA specific element of ToM reasoning is grounding, or establishing common ground, i.e. an important mechanism by which participants engaged in joint activity coordinate their respective understandings of matters at hand. This construct arises from a model of conversation developed by Herbert Clark [20] and since studied widely in many fields, including social psychology, e.g. [21], HCI, e.g. [22], philosophy, e.g. [23]. Finding computationally amenable representations and mechanisms that allow agents interacting with humans to keep track of the activity, and their understanding of other participants in the same activity, remains a challenge, c.f. [4], [24]. Exploring alternative definitions of grounding, allowing for subtle and important variations in the notions of knowledge, belief and acceptance, is one aspect we have investigated [25], [26].\nTo efficiently take action in settings without the forms of full information needed for ToM reasoning, humans often reason in terms of the (reference) groups to which they and others belong, and the role structures and stereotypical behaviours associated with those reference groups. Steps have been taken towards equipping agents with similar computational capabilities, e.g., [27], [28], [29].\nNow to social planning. Planning research has for some time yielded highly efficient mechanisms for plan synthesis suiting single-agent scenarios. Input to a planner includes descriptions of the world and effects of available actions, the initial state(s) that the world might be in before the plan-execution agent performs any actions, and the desired\nar X\niv :1\n60 2.\n06 48\n3v 1\n[ cs\n.R O\n] 2\n1 Fe\nb 20\n16\nobjectives, such as achieving a goal or performing a specific task. The output typically consists of either a plan (a sequence of actions for the agent to perform) or a policy (an action to perform for each state). However, such descriptions are often insuffient for agents operating in multi-agent environments. In such environments, a planning agent must consider that other agents have their own actions and mental states, and that these actions and mental states can affect the outcomes and interpretation of its own actions. Thus, such reasoning is inherently a social task.\nIn environments where an agent needs to plan its interactions with others, computational complexity increases: the actions of the other agents can induce a combinatorial explosion in the number of contingencies to be considered, making both the search space and the solution size exponentially larger, hence demanding novel methods. A recent advance is the development of epistemic planning [30]: planning according to the knowledge or belief (and iterated knowledge or belief) of other agents, allowing the specification of a more complex class of planning domains than those mostly concerned with simple facts about the world.\nBuilding on this work and on recent advances in nondeterministic planning, we have made progress towards the challenge of efficient reasoning both with and about the incomplete, higher-order, and possibly incorrect beliefs of other individuals as part of the planning process, and how we can plan considering the actions of others. Our work involves descriptions and demonstrations-in-use of novel mechanisms for stereotypical and empathetic reasoning, explorations of how this can be used as a theory of mind, and planning while considering the actions of others [27], [31], [32], [33], [34], [35]."}, {"heading": "II. CHALLENGE SCENARIOS", "text": "We offer three scenarios that provide challenging settings for social planning.\nSCENARIO 1\nThis scenario illustrates the need for complex reasoning with others, allowing for possibly limited or faulty perceptions by others of their environment.\nConsider a self-driving car and a pedestrian each approaching an intersection. A safe plan for each is to wait for the other to go, resulting in a stalemate. With human participants, such encounters are generally resolved with social cues: e.g. one signalling to the other using a nod of the head or hand signal. In such cases, cues such as establishing eye contact generate a common belief that each party understands who will go first, and each party understands that each understands this, etc. For a selfdriving car to achieve similar interactions with a pedestrian, it will need both sophisticated sensing technology (to accurately recognise the nod or hand-signal) and also rich internal computational mechanisms to interpret the signal. However, even physical signals often require social context for their correct interpretation. For example, a young child\u2019s inability to correctly assess the belief of others, and therefore, the common belief between themselves and\na driver, mean that the driver must consider this when planning its action, and may behave more cautiously.\nSCENARIO 2\nThis scenario is inspired by the Wumpus Hunt and demands agents engage in strategic and social reasoning. It has been used to demonstrate the power of theory of mind reasoning [27], [31].\nThe lord of a castle is informed by a peasant that a Wumpus is dwelling in a dungeon nearby. It is known that the Wumpus can be killed by one hunter alone only if asleep; if awake, two hunters are required. The lord then tasks the peasant to go to fetch the White Knight, his loyal champion, and hunt down the beast together. The White Knight is known for being irreprehensible, trustworthy and brave; however, the peasant does not know any knight, and neither their looks. While looking for the White Knight, he runs into the Black Knight and, believing him the White Knight, tells him about the quest.\nThere is some additional information that needs to be taken into account: on one hand, the knight knows how a Wumpus can be killed by two hunters, but he is aware that a simple peasant may get scared by the thought of confronting an awake Wumpus. Also, the peasant can not hunt and is unable to see whether the Wumpus is awake (he can not approach unnoticed), but the knight can. Therefore it is not clear to him whether the peasant can be of any help to the quest. On the other hand, the knight is aware of the misunderstanding: he knows that the peasant attributes to him all the good qualities of the White Knight, so the peasant is confident that the knight won\u2019t put him in danger whenever possible. While on the road, they agree on a protocol: they will enter the dungeon from two sides, and the Knight will use a whistle to signal whether the Wumpus is awake, then they will attack.\nSCENARIO 3\nA more difficult challenge problem can be found with the multi-player board game of deception and bluff, Hattari [36].\nHattari involves a crime scene, three suspects, one victim, and clues. The task is to guess who is the culprit, to accuse him or to deceive the other players! Each player receives a \u201csuspect profile\u201d and 5 accusation markers. Three suspect profiles are placed upright in the center of the table, and one profile is placed face down, next to the other three. That is the victim of the crime. The goal is to unmask the the culprit among the three standing suspects. The rules of the game involve selective sharing of information, but also manipulation of incomplete information among the players, through passing around of pieces as players take turns.\nAlthough we have incorporated some of our research on epistemic planning into a (limited) implementation of Hattari [37], creation of an artificial player that could participate meaningfully in a game where humans exploit and interpret body language as they navigate the possibilities of bluffing and deception seems far beyond current technologies."}, {"heading": "III. WORKSHOP DISCUSSION QUESTIONS", "text": "1) Why should you use cognitive architectures - how would they benefit your research as a theoretical framework, a tool and/or a methodology?\nOur interest is directly in the design of cognitive architectures as the basis for executable strategic collaboration and teamwork involving hybrid human-agent teams.\n2) Should cognitive architectures for social interaction be inspired and/or limited by models of human cognition?\nCognitive architectures should be inspired by models of human cognition. Modelling the cognitive architecture after concepts of human cognition seems to allows us to better prepare agents for human-agent interaction. Further, while explorations with computational models cannot directly shed light on human cognition, c.f. [38], experiments with computational cognitive models can contribute to analyses of potential building blocks for mechanisms involved in coordination in joint action, whether it be in purely human, or human-robot interaction contexts.\n3) What are the functional requirements for a cognitive architecture to support social interaction?\nToo many to enumerate here... But, as mentioned above, a cognitive architecture should at least have components modeling the (social) identities, social context and social triggers and effects of actions. In short, representations of the social reality of the partners in the interaction are required.\n4) How the requirements for social interaction would inform your choice of the fundamental computational structures of the architecture (e.g. symbolic, sub-symbolic, hybrid, ...)?\nComputational structures should be hybrid. For low level interactions and time constrained feedback loops, some very efficient and robust mechanisms are needed that seem best to be represented sub-symbolically. However, for longer term social actions, it is necessary to have symbolic representations in order to deliberate, on the run, about the (social) effects of actions.\n5) What is the primary outstanding challenge in developing and/or applying cognitive architectures to social HRI systems?\nOutstanding challenges include: identifying and exploiting \u2018sweet spots\u2019 in the expressivity-efficiency tradeoff in the engineering of computational artefacts; finding an effective (domain specific) balance between design-time knowledge engineering and run-time learning; signalling of state (in\nboth directions) between human and artificial participants in joint activity; integration of the diverse perceptual, cognitive and social aspects in a plausibly effective system; establishment of metrics and evaluation methods that allow terms such as \u201cplausibly effective\u201d to be precisely defined and formally demonstrated.\n6) Devise a social interaction scenario that current cognitive architectures would likely fail, and why.\nThe beginnings of candidate scenarios are offered above. To provoke failure, what is needed are scenarios exhibiting social brittleness - i.e. where the normal course of interaction fails due to different expectations or assumptions as a result of different social understandings, and a repair has to be found."}, {"heading": "IV. FINAL REMARKS", "text": "Even though our focus is on cognitive mechanisms as essential components of an integrated cognitive architecture for effective social robots, and we have some exploratory work on human communication patterns [39], we recognise there are many topics important in such architectures that we do not attempt to address \u2013 spatial reasoning [40], dialogue actions [7], multimodal inputs [41], action signalling [24], the link between perception and action [42], [43], and comparisons between logic-based reasoning and other approaches such as game theory [44] and probabilistic reasoning [45] ... to name but a few!!"}, {"heading": "ACKNOWLEDGEMENTS", "text": "Much of the work reported here was carried out while two of the authors (Felli & Muise) were employed by the University of Melbourne with the financial support of the Australian Research Council Discovery Projects Grant DP130102825 Foundations of Human-Agent Collaboration: Situation-Relevant Information Sharing. Additional information about the project can be found at http://agentlab.cis.unimelb.edu.au/project-hac. html"}], "references": [{"title": "Cognitive architectures,", "author": ["P. Thagard"], "venue": "The Cambridge handbook of cognitive science, pp", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "From autistic to social agents,", "author": ["F. Dignum", "R. Prada", "G.J. Hofstede"], "venue": "Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems, ser. AAMAS \u201914,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Curing robot autism: A challenge,", "author": ["G.A. Kaminka"], "venue": "Proceedings of the 2013 International Conference on Autonomous Agents and Multiagent Systems, ser. AAMAS", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Intentional joint agency: shared intention lite,", "author": ["E. Pacherie"], "venue": "Synthese, vol. 190,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "The Belief-Desire-Intention model of agency,", "author": ["M. Georgeff", "B. Pell", "M. Pollack", "M. Tambe", "M. Wooldridge"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "The Knowledge Level,", "author": ["A. Newell"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Explicit knowledge and the deliberative layer: Lessons learned,", "author": ["S. Lemaignan", "R. Alami"], "venue": "Intelligent Robots and Systems (IROS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "What should the agent know?: the challenge of capturing human knowledge,", "author": ["E. Norling"], "venue": "Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A dialogical argument for the usefulness of logic in MAS,", "author": ["F. Dignum", "L. Sonenberg"], "venue": "Journal of Artificial Societies and Social Simulation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Comments on a dialogical argument for the usefulness of logic in MAS,", "author": ["B. Edmonds"], "venue": "Journal of Artificial Societies and Social Simulation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Reasoning about other agents: a plea for logic-based methods,", "author": ["W. Reich"], "venue": "Journal of Artificial Societies and Social Simulation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Theory of mind,", "author": ["A.I. Goldman"], "venue": "The Oxford Handbook of Philosophy of Cognitive Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Does the chimpanzee have a theory of mind?", "author": ["D. Premack", "G. Woodruff"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1978}, {"title": "How much does it help to know what she knows you know? an agent-based simulation study,", "author": ["H. de Weerd", "R. Verbrugge", "B. Verheij"], "venue": "Artif. Intell.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Modeling how humans reason about others with partial information,", "author": ["S.G. Ficici", "A. Pfeffer"], "venue": "Proceedings of the 2008 International Conference on Autonomous Agents and Multi-agent Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Understanding second-order theory of mind,", "author": ["L.M. Hiatt", "J.G. Trafton"], "venue": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Social robots and the tree of social cognition,", "author": ["B.F. Malle"], "venue": "HRI Workshop Proceedings \u2013 Cognition: A Bridge between Robotics and Interaction,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Interaction as a bridge between cognition and robotics,", "author": ["S. Thill", "T. Ziemke"], "venue": "HRI Workshop Proceedings \u2013 Cognition: A Bridge between Robotics and Interaction,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Coordinating beliefs in conversation,", "author": ["D. Wilkes-Gibbs", "H.H. Clark"], "venue": "Journal of Memory and Language,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Grounding: Sharing information in social interaction,", "author": ["Y. Kashima", "O. Klein", "A.E. Clark"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Cognitive representation of common ground in user interfaces,", "author": ["D. Brock", "J. Trafton"], "venue": "User Modeling, ser. CISM International Centre for Mechanical Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "What is Common Ground?", "author": ["K. Allan"], "venue": "Perspectives on Linguistic Pragmatics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "How to support action prediction: Evidence from human coordination tasks,", "author": ["C. Vesper"], "venue": "The 23rd IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN:,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Logics of common ground,", "author": ["T. Miller", "J. Pfau", "L. Sonenberg", "Y. Kashima"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Modelling and using common ground in human-agent collaboration during spacecraft operations,", "author": ["J. Pfau", "T. Miller", "L. Sonenberg"], "venue": "Proceedings of SpaceOps 2014 Conference. American Institute of Aeronautics and Astronautics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Computing social behaviours using agent models,", "author": ["P. Felli", "T. Miller", "C. Muise", "A.R. Pearce", "L. Sonenberg"], "venue": "in International Joint Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Towards agent-based models of cultural dynamics: A case of stereotypes,", "author": ["J. Pfau", "Y. Kashima", "L. Sonenberg"], "venue": "Perspectives on Culture and Agent-based Simulations. P,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Rich socio-cognitive agents for immersive training environments: Case of NonKin Village,", "author": ["B.G. Silverman", "D. Pietrocola", "B. Nye", "N. Weyer", "O. Osin", "D. Johnson", "R. Weaver"], "venue": "Autonomous Agents and Multi- Agent Systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Epistemic planning for single- and multi-agent systems,", "author": ["T. Bolander", "M.B. Andersen"], "venue": "Journal of Applied Non-Classical Logics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Artificial social reasoning: Computational mechanisms for reasoning about others,", "author": ["P. Felli", "T. Miller", "C. Muise", "A. Pearce", "L. Sonenberg"], "venue": "ICSR", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Knowing whether\u2019 in proper epistemic knowledge bases,", "author": ["T. Miller", "C. Muise", "P. Felli", "A.R. Pearce", "L. Sonenberg"], "venue": "The 30th AAAI Conference on Artificial Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Towards team formation via automated planning,", "author": ["C. Muise", "F. Dignum", "P. Felli", "T. Miller", "A.R. Pearce", "L. Sonenberg"], "venue": "in International Workshop on Coordination, Organisation, Institutions and Norms in Multi-Agent Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Planning over multi-agent epistemic states: A classical planning approach,", "author": ["C. Muise", "V. Belle", "P. Felli", "S. McIlraith", "T. Miller", "A.R. Pearce", "L. Sonenberg"], "venue": "Proceedings of 29th AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Hattari demo,", "author": ["C. Muise"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Theoretical status of computational cognitive modeling,", "author": ["R. Sun"], "venue": "Cognitive Systems Research,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2009}, {"title": "Implicit coordination strategies for effective team communication,", "author": ["A. Butchibabu", "J. Shah", "L. Sonenberg"], "venue": "Human Factors,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "When the robot puts itself in your shoes: Managing and exploiting human and robot beliefs,", "author": ["M. Warnier", "J. Guitton", "S. Lemaignan", "R. Alami"], "venue": "in RO-MAN. IEEE,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Integrating visual learning and hierarchical planning for autonomy in human-robot collaboration,", "author": ["M. Sridharan"], "venue": "AAAI Spring Symposium on Designing Intelligent Robots: Reintegrating AI II,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Cognitive architecture for human\u2013robot interaction: towards behavioural alignment,", "author": ["P.E. Baxter", "J. de Greeff", "T. Belpaeme"], "venue": "Biologically Inspired Cognitive Architectures,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2013}, {"title": "A two-level computational architecture for modeling human joint action,", "author": ["J. Pfau", "Y. Kashima", "L. Sonenberg"], "venue": "Proceedings of 13th International Conference on Cognitive Modelling (ICCM),", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "Reasoning about reasoning by nested conditioning: Modeling theory of mind with probabilistic programs,", "author": ["A. Stuhlm\u00fcller", "N. Goodman"], "venue": "Cognitive Systems Research,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "[1], a new layered deliberation architecture is required that at the higher level(s) naturally accommodates analysis of decision choices that take into account both rich context and future projections of", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "possible consequences, yet does not rely on computational expensive deep reasoning capability [2], [3], [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "possible consequences, yet does not rely on computational expensive deep reasoning capability [2], [3], [4].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "possible consequences, yet does not rely on computational expensive deep reasoning capability [2], [3], [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Our approach has been informed by our experience with the BDI model of agency [5] and several associated agent architectures - architectures that were introduced to support", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "a balance of deliberative and reactive behaviours, and that in their instantiation are reliant on domain-specific expert knowledge acquisition to provide a knowledge level view [6], c.", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "[7], [8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], [8].", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "[9],", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[10], [11].", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "Exploiting mutual awareness to enable a participant engaged in collaborative activity with others to select an appropriate action typically involves Theory of Mind (ToM) reasoning [12], [13], i.", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "Exploiting mutual awareness to enable a participant engaged in collaborative activity with others to select an appropriate action typically involves Theory of Mind (ToM) reasoning [12], [13], i.", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": ", [14], [15], [16], [17], [18], [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [14], [15], [16], [17], [18], [19].", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": ", [14], [15], [16], [17], [18], [19].", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": ", [14], [15], [16], [17], [18], [19].", "startOffset": 26, "endOffset": 30}, {"referenceID": 17, "context": ", [14], [15], [16], [17], [18], [19].", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "This construct arises from a model of conversation developed by Herbert Clark [20] and since studied widely in many fields, including social psychology, e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "[21], HCI, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22], philosophy, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], [24].", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[4], [24].", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "Exploring alternative definitions of grounding, allowing for subtle and important variations in the notions of knowledge, belief and acceptance, is one aspect we have investigated [25], [26].", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "Exploring alternative definitions of grounding, allowing for subtle and important variations in the notions of knowledge, belief and acceptance, is one aspect we have investigated [25], [26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 25, "context": ", [27], [28], [29].", "startOffset": 2, "endOffset": 6}, {"referenceID": 26, "context": ", [27], [28], [29].", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": ", [27], [28], [29].", "startOffset": 14, "endOffset": 18}, {"referenceID": 28, "context": "A recent advance is the development of epistemic planning [30]: planning according to the knowledge or belief (and iterated knowledge or belief) of other agents, allowing the specification of a more complex", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "considering the actions of others [27], [31], [32], [33], [34], [35].", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "considering the actions of others [27], [31], [32], [33], [34], [35].", "startOffset": 40, "endOffset": 44}, {"referenceID": 30, "context": "considering the actions of others [27], [31], [32], [33], [34], [35].", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "considering the actions of others [27], [31], [32], [33], [34], [35].", "startOffset": 52, "endOffset": 56}, {"referenceID": 32, "context": "considering the actions of others [27], [31], [32], [33], [34], [35].", "startOffset": 58, "endOffset": 62}, {"referenceID": 25, "context": "It has been used to demonstrate the power of theory of mind reasoning [27], [31].", "startOffset": 70, "endOffset": 74}, {"referenceID": 29, "context": "It has been used to demonstrate the power of theory of mind reasoning [27], [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "tari [37], creation of an artificial player that could participate meaningfully in a game where humans exploit and interpret body language as they navigate the possibilities of bluffing and deception seems far beyond current technologies.", "startOffset": 5, "endOffset": 9}, {"referenceID": 34, "context": "[38], experiments with", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "essential components of an integrated cognitive architecture for effective social robots, and we have some exploratory work on human communication patterns [39], we recognise there are many topics important in such architectures that we do not attempt to address \u2013 spatial reasoning [40], dialogue actions [7], multimodal inputs [41], action signalling [24], the", "startOffset": 156, "endOffset": 160}, {"referenceID": 36, "context": "essential components of an integrated cognitive architecture for effective social robots, and we have some exploratory work on human communication patterns [39], we recognise there are many topics important in such architectures that we do not attempt to address \u2013 spatial reasoning [40], dialogue actions [7], multimodal inputs [41], action signalling [24], the", "startOffset": 283, "endOffset": 287}, {"referenceID": 6, "context": "essential components of an integrated cognitive architecture for effective social robots, and we have some exploratory work on human communication patterns [39], we recognise there are many topics important in such architectures that we do not attempt to address \u2013 spatial reasoning [40], dialogue actions [7], multimodal inputs [41], action signalling [24], the", "startOffset": 306, "endOffset": 309}, {"referenceID": 37, "context": "essential components of an integrated cognitive architecture for effective social robots, and we have some exploratory work on human communication patterns [39], we recognise there are many topics important in such architectures that we do not attempt to address \u2013 spatial reasoning [40], dialogue actions [7], multimodal inputs [41], action signalling [24], the", "startOffset": 329, "endOffset": 333}, {"referenceID": 22, "context": "essential components of an integrated cognitive architecture for effective social robots, and we have some exploratory work on human communication patterns [39], we recognise there are many topics important in such architectures that we do not attempt to address \u2013 spatial reasoning [40], dialogue actions [7], multimodal inputs [41], action signalling [24], the", "startOffset": 353, "endOffset": 357}, {"referenceID": 38, "context": "link between perception and action [42], [43], and comparisons between logic-based reasoning and other approaches such as game theory [44] and probabilistic reasoning [45] .", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "link between perception and action [42], [43], and comparisons between logic-based reasoning and other approaches such as game theory [44] and probabilistic reasoning [45] .", "startOffset": 41, "endOffset": 45}, {"referenceID": 40, "context": "link between perception and action [42], [43], and comparisons between logic-based reasoning and other approaches such as game theory [44] and probabilistic reasoning [45] .", "startOffset": 167, "endOffset": 171}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}