{"id": "1602.06539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Determining the best attributes for surveillance video keywords generation", "abstract": "automatic video keyword item generation is one limitation of the key ingredients in reducing the burden fees of security officers engage in analyzing surveillance videos. keywords or linked attributes are generally chosen manually based on expert knowledge of surveillance. most existing statistical works primarily show aim at either supervised learning approaches plus relying on equally extensive manual labelling or hierarchical probabilistic models that assume the features are extracted using the flexible bag - of - words approach ; thus limiting the utilization range of the other features. to better address this, we turn our attention to automatic attribute discovery approaches. however, it is not clear which automatic discovery approach can discover the most precisely meaningful attributes. furthermore, little focused research has been done on how beneficial to compare and choose successively the best automatic attribute discovery methods. in this paper, we originally propose a novel approach, based on the shared values structure exhibited amongst meaningful attributes, that enables us to compare between different automatic attribute discovery approaches. we then gradually validate our approach by successively comparing various attribute discovery methods such as picodes on two data attribute datasets. the evaluation shows that our approach is able to select the automatic discovery approach that discovers the most meaningful attributes. we then fully employ the best discovery approach to generate keywords for videos recorded from a surveillance system. this work shows it is generally possible to massively reduce the amount of manual compilation work exercised in generating video keywords without repeatedly limiting ourselves to a particular video feature descriptor.", "histories": [["v1", "Sun, 21 Feb 2016 15:08:51 GMT  (2392kb,D)", "http://arxiv.org/abs/1602.06539v1", "7 pages, ISBA 2016. arXiv admin note: text overlap witharXiv:1602.01940"]], "COMMENTS": "7 pages, ISBA 2016. arXiv admin note: text overlap witharXiv:1602.01940", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["liangchen liu", "arnold wiliem", "shaokang chen", "kun zhao", "brian c lovell"], "accepted": false, "id": "1602.06539"}, "pdf": {"name": "1602.06539.pdf", "metadata": {"source": "CRF", "title": "Determining the best attributes for surveillance video keywords generation", "authors": ["Liangchen Liu", "Arnold Wiliem", "Shaokang Chen", "Kun Zhao", "Brian C. Lovell"], "emails": ["l.liu9@uq.edu.au", "a.wiliem@uq.edu.au", "shaokangchenuq@gmail.com", "k.zhao1@uq.edu.au", "lovell@itee.uq.edu.au"], "sections": [{"heading": "1. Introduction", "text": "Automatic video analytics is one of the key components in smart surveillance systems to combat crime and terrorism. For example, they can be used to detect anomalous events to alert security officers [13]. In general, surveillance systems generate a large amount of video data. This makes finding critical information in surveillance video as challenging as finding the proverbial needle in a haystack [23]. Thus automation is highly desirable so one can reduce the amount of time to find this critical information.\nAutomatic video analytics have been gaining significant interest in the research community. Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].\nIn this work we tackle the problem of automatic generation of keywords for video description. Keywords are important ingredients in generating textual descriptions [27]. More specifically, once the keywords of a video are generated, the video can be searched using natural language to find events of interest.\nUnfortunately, existing approaches still require a great deal of manual labelling before the systems can be used to generate the keywords/description [11]. For example, the work proposed by Izadinia et al. in [12] uses extensive spatio temporal annotations to train action and role models for action recognition. The approach produces better descriptions than many other approaches. However, the significant manual labelling severely restricts its scalability. In addition, when relevant manual labels are not available, then it is not possible for the system to describe unusual events which would be extremely useful in anomaly event analysis.\nOne feasible way to circumvent this is to employ latent hierarchical probabilistic models such as probabilistic Latent Semantic Analysis (pLSA) [29] or Latent Dirichlet Allocation (LDA) [31]. These methods can automatically mine the latent topics which could represent keywords. Thus, when a topic is inferred in a video, then the associated text of the topic becomes the keyword. Unfortunately, despite their potential, these methods are based on the bag of words model requiring explicit modelling of visual words. Here, each video is assumed to have a collection of visual words. This explicit assumption may not be feasible for other recent video features not derived from bag of word features.\nInspired from the probabilistic latent topic discovery methods, in this work, we propose a method that can automatically discover video keywords with significantly less manual processing. More specifically, several attribute discovery methods such as PiCoDeS [1] and Spectral Hashing [32] can be employed.\nVisual attribute features are binary features indicating\nar X\niv :1\n60 2.\n06 53\n9v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\nthe presence/absence of visual concepts. For instance a car can be described as [\u2019has wheels\u2019, \u2019is metallic\u2019, \u2019does not have legs\u2019]. In practice, we can represent the binary features as [1 1 0]. The attribute features trained in one domain can be reused for another domain with minimum manual work [9]. As such, a system can be potentially trained to recognize unseen events [14].\nVisual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].\nOnce the attribute features are trained, they can be used to extract keywords. Unfortunately, training attributes features also require extensive manual labeling work. This is because, as each individual visual attribute is a binary classifier, then one needs to create the labeled training set for each attribute.\nTo that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33]. These methods primarily focus on learning an embedding function that maps the original descriptors into binary code space wherein each individual bit is expected to represent a visual attribute. We note that these approaches are also closely related to hashing approaches [10, 15, 32]. The difference is that unlike automatic attribute discovery approaches, hashing methods are primarily aimed at significantly reducing computational complexity and storage whilst maintaining system accuracy. Despite many works that have been proposed, it is not clear which methods produce the most meaningful attributes.\nHere, we present an approach that allows us to select the attribute discovering method that discovers the most meaningful attributes. We then find the keywords extracted from the best method to describe videos recorded from a surveillance system.\nThe intuition of our approach comes from a speculation proposed in [19, 20]. More specifically, Parikh et. al. suggest that meaningful attributes tend to occupy a subspace, here called the Meaningful Subspace, on a manifold. Thus, we can utilize any given set of meaningful attributes to be our \u2018yardstick\u2019 for comparing various attribute discovery methods.\nFig. 1 illustrates the differences between our attribute-\nbased keyword generation approach and the existing approaches. We can see our approach has two main advantages. First, is it does not require significant manual processing. Second, it is not constrained to one particular video feature. Contributions. We list our contributions as follows: (1) We propose an attribute-based video keyword generation approach by utilizing the attribute discovery method that discovers the most meaningful set of attributes; (2) To determine the attribute discovery method, we propose a selection approach enabling us to select which attribute discover methods provide meaningful attributes; (3) We use and validate our selection method in two known attribute datasets; (4) Finally, we validate the keywords extracted from the best attribute discovery method. These keywords can be used to describe videos recorded from a surveillance system.\nWe continue our paper as follows. Section 2 discusses related works. Section 3 presents our proposed approach to compare various attribute discovery methods. Section 4 describes the approach to generate video keywords using discovered attributes. Section 5 presents experiments and the section 6 concludes the discussion."}, {"heading": "2. Related Works", "text": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4]. For instance, Rohrbach et al. [27] proposed to generate a rich semantic representation of the visual content such as object and activity labels. They employed the Conditional Random Field (CRF) to model all the input visual components. In [26], they extended their work to a threelevel-of-detail video description scheme. Then they applied a machine translation framework to generate the natural language using the semantic representation as sources. Unfortunately, this model cannot be used to address our problem due to the extensive manual labelling work required.\nTo that end, some researchers rely on hierarchical probabilistic models. Wang et al. [31] and Varadarajan et al. [29] employ LDA and pLSA respectively to perform unsupervised activity analysis. However as mentioned, these methods can only be applied on the bag-of-words framework.\nThis means, more powerful features such as Fisher vectors [22] cannot be used directly.\nTo the best of our knowledge, there is only one work that specifically targets automatic video description problems in surveillance videos. Xu et al. [35] develop a novel distributed multiple-scene global understanding framework that clusters surveillance scenes by their ability to explain each others behaviours. However, their work only focuses on multiple-scene case and again, utilizes hierarchical probabilistic models."}, {"heading": "3. Selecting the attribute discovery method", "text": "We first describe the manifold space where the attributes lie. Then, we use this representation to select the attribute discovery method that discovers the most meaningful attributes. Technically, we will measure the meaningfulness of a set of discovered attributes."}, {"heading": "3.1. The manifold of decision boundaries", "text": "A visual attribute, or simply an attribute, can be represented as a decision boundary, as it partitions a given set of N images X = {xi}Ni=1 into two subsets, X+ \u222a X\u2212 = X : (1) the set of images/videos where the attribute exists, X+; and (2) the set of images/videos where the attribute does not exist, X\u2212. Hence, in this case, we assume that all attributes lie on a manifold of decision boundaries [20].\nIn our work, we represent an attribute as an Ndimensional binary vector whose i-th element is the outcome of the corresponding attribute binary classifier tested on image xi. Let us consider the corresponding attribute classifier \u03c6(\u00b7) \u2208 R, the function \u03c6 classifies an input image xi into either the positive or negative set (i.e., X+ or X\u2212), depending on the sign of the classifier output. We define z[X ] as the attribute representation w.r.t. a set of images X , z[X ] \u2208 {\u22121,+1}N , where the i-th dimension of z [X ] i = sign(\u03c6(xi)) \u2208 {\u22121,+1}. For the sake of clarity, we write z[X ] as z, whenever the context is clear. Thus, the manifold of decision boundaries w.r.t. X is then defined as the lower dimensional space embedded in an N-dimensional binary space,M[X ] \u2208 {\u22121,+1}N . Again, we writeM[X ] asM whenever the context is clear."}, {"heading": "3.2. Distance from the Meaningful Subspace", "text": "Given a set of images X and a set of discovered attributes, D = {zk}Kk=1, zk \u2208 {\u22121,+1}N , here our goal is to define the distance of the attribute set from the Meaningful Subspace. Ideally, this subspace contains all possible meaningful attributes. Unfortunately, it is impossible to enumerate all of them. One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20]. These attributes are considered to be meaningful as human annotators labelled them via the Amazon Mechanical Turk (AMT). We define this set of meaningful attribute as S = {hj}Jj=1,hj \u2208 {\u22121,+1}N .\nSince meaningful attributes are assumed to have shared structure, we could assume that a meaningful attribute must\nbe able to be described using the other meaningful attributes. For instance, a set of attributes of primary colors red, green and blue could be used to reconstruct the set of secondary colors such as yellow, magenta and cyan. The primary colors could also be used to reconstruct the other primary colors (e.g. red is not green and not blue).\nUnfortunately, a linear combination of attributes may not lie on the manifoldM. More precisely, \u2211 i wizi may\nnot be a member of M as it is possible that \u2211\ni wizi /\u2208 {\u22121,+1}N , rather \u2211 i wizi \u2208 RN . Thus, it is non-trivial to calculate the geodesic distance (i.e., the shortest distance between two points on the manifold) for determining the distance between a discovered attribute, zk and the Meaningful Subspace, S. Therefore, we consider an approximated geodesic distance by assuming the members of manifold M lie in RN . In this case both, magnitude and sign of the classifier output values are considered. Thus, the approximated geodesic distance is defined as:\nmin r \u2016Ar \u2212 zk\u201622, (1)\nwhere the matrix A \u2208 RN\u00d7J contains the attributes of set S arranged as column vectors; r \u2208 RJ\u00d71 is the reconstruction coefficient vector. The above distance is defined in terms of the reconstruction error of the attribute zk by the set of meaningful attributes S. When an attribute is meaningful, then its reconstruction error is minimized or close to zero due to the shared structure possessed by the Meaningful Subspace.\nWe then define the distance between the set of discovered attributes D and the Meaningful Subspace S on the manifold M w.r.t. the set of images X as the average reconstruction error:\n\u03b4(D,S;X ) = 1 K min R \u2016AR\u2212B\u20162F , (2)\nwhere \u2016 \u00b7 \u2016F is the matrix Frobenious norm; B \u2208 RN\u00d7K is the matrix of attributes D arranged as column vectors; R \u2208 RJ\u00d7K is the reconstruction matrix.\nThe distance in (1) and (2) may create dense reconstruction coefficients, suggesting that each meaningful attribute should contribute to the reconstruction. A more desired result is to have less dense coefficients (i.e., less number of non-zero coefficients). This is because there may be only a few meaningful attributes required to reconstruct another meaningful attribute. One possible way to address this is to add the convex hull regularization which has been shown in [3] to induce sparsity.\nWhen a convex hull constraint is considered, (2) becomes:\n\u03b4cvx(D,S;X ) = 1\nK min R \u2016AR\u2212B\u20162F s. t.\nR (i, j) \u2265 0 J\u2211\ni=1\nR(i, \u00b7) = 1.\n(3)\nThe above equation basically computes the average distance between each discovered attribute zk \u2208 D and the convex\nhull of S. The above optimization problem could be solved using the method proposed in [3]. In our approach, we assume that the lower the distance of a set of discovered attributes to a meaningful subspace, the more meaningful the attributes will be."}, {"heading": "4. Generating keywords using discovered attributes", "text": "Once meaningful attributes are discovered, one can extract the attribute features from the given data. However, one still needs to name the attributes. Despite this manual process, we argue that the manual process for naming meaningful attributes is significantly easier and quicker than the manual process of labelling images/videos to train attribute features.\nOne can name an attribute by first extracting the attribute features from a given set of images. As previously mentioned, each attribute divides any set of images/videos into two groups: the group of images in which the visual attribute is present (the positive class) and the group of images/videos in which the visual attribute is absent (the negative class).\nSome attributes may have similar names. In this case, these attributes are considered as duplicate and therefore they are merged."}, {"heading": "5. Experiment", "text": "In this section, we validate our proposed approach and evaluate the accuracy of the keywords extracted from the best discovered method to describe videos.\nIn the first part, we evaluate the ability of our approach to measure the meaningfulness of a set of attributes. Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15]. For this case, two datasets will be utilized: (1) a-Pascal a-Yahoo dataset (ApAy) [9]; (2) SUN Attribute dataset (ASUN) [21].\nIn the second part of our experiment, we apply the best attribute discovery method to discover keywords from a surveillance dataset. In this setting, we utilize the UT Tower aerial view dataset (UTTower) [8]. The efficacy of the keywords are then evaluated."}, {"heading": "5.1. Datasets and experiment setup", "text": "The following are the detailed description of each image dataset for validating our approach and evaluating the attribute discovery methods.\na-Pascal a-Yahoo dataset (ApAy) [9] \u2014 comprises two sources: a-Pascal and a-Yahoo. There are 12,695 cropped images in a-Pascal that are divided into 6,340 for training and 6,355 for testing with 20 categories. The a-Yahoo set has 12 categories disjoint from the a-Pascal categories. Moreover, it only has 2,644 test exemplars. There are 64 attributes provided for each cropped image. In total the\ndataset has 15,339 exemplars, 64 attributes and 32 categories. The dataset provides four features for each exemplar: local texture; HOG; edge and color descriptor. These are then concatenated into a 9,751 dimensional feature vector. We use the training set for discovering attributes and we perform our study in the test set. More precisely, we consider the test set as the set of images X . SUN Attribute dataset (ASUN) [21] \u2014 ASUN is a finegrained scene classification dataset consisting of 717 categories (20 images per category) and 14,340 images in total with 102 attributes. There are four types of features provided in this dataset: (1) GIST; (2) HOG; (3) selfsimilarity and (4) geometric context color histograms (See [34] for feature and kernel details). From 717 categories, we randomly select 144 categories for discovering attributes. As for our evaluation, we random select 1,434 images (i.e., 10% of 14,340 images) from the dataset. It means, in our evaluation, some images may or may not come from the 144 categories used for discovering attributes.\nFor the first experiment, we apply the following preprocessing described in [1]. We first lift each feature into a higher-dimensional space approximating the histogram intersection kernel by using the explicit feature maps proposed by Vedaldi and Zisserman [30]. More precisely, each feature is mapped into the space three times larger than the original space. This effectively allows us to apply linear classifiers in the explicit kernel space [1]. After the features are lifted, we then apply PCA to reduce the dimensionality of the feature space by 40 percent. This preprocessing step is crucial for PiCoDeS as it uses lifted feature space to simplify their training scheme while maintaining the information preserved in the Reproducing Kernel Hilbert Space (RKHS). Therefore, the method performance will be severely affected when lifting features are not used. In our empirical observations (results not presented), we also found that lifted feature space gives positive contributions to the other methods.\nEach method is trained using the training images to discover the attributes. Then we use the manifold M w.r.t. the test images for the evaluation. More precisely, each attribute descriptor is extracted from test images (i.e., zk, zk \u2208 {\u22121, 1}N , where N is the number of test images). For each dataset, we use the attribute labels from Amazon Mechanical Turk (AMT) to represent the Meaningful Subspace, S.\nUT Tower aerial view activity classification dataset (UTTower) [8] \u2014 consists of 108 low-resolution video sequences from 9 types of actions. Each action is performed 12 times by 6 individuals. The dataset is composed of two types of scenes: concrete square and lawn. There are 4 actions in the concrete square scene, they are \u201cpointing\u201d, \u201cstanding\u201d, \u201cdigging\u201d, \u201cwalking\u201d and 5 actions in the lawn scene: \u201ccarrying\u201d, \u201crunning\u201d, \u201cwave1\u201d, \u201cwave2\u201d, \u201cjumping\u201d. Ground truth labels for all actions videos are provided for the training and the testing.\nFor the second experiment, we use manifold feature proposed in [36] to extract visual information from the surveillance videos in the dataset. The video frames were first downsized into 16 \u00d7 16 and then Grassmann points on\nG128,8 were generated by performing the SVD on the normalized pixel intensities of 8 successive frames. In total, there are 216 manifold points. Note that, the features are not derived from the bag-of-words framework. It is also noteworthy to mention that our work is not primarily aimed to study feature discriminative power and robustness. Although, it is generally assumed that better features may provide more meaningful attributes, further studies are required in the future."}, {"heading": "5.2. Attribute meaningfulness evaluation", "text": "In this experiment, our aim is to verify whether the proposed approach does measure meaningfulness on the set of discovered attributes. One of the key assumptions in our proposal is that the meaningfulness is reflected in the distance between the meaningful subspace and the given attribute set, D. That is, if the distance is far, then it is assumed that the attribute set is less meaningful, and vice versa. In order to evaluate this assumption we create two sets of attributes, meaningful and non-meaningful attributes, and observe their distances to the meaningful subspace.\nFor the meaningful attribute set, we use the attributes from AMT provided in each dataset. More precisely, given manually labelled attribute set S, we divide the set into two subsets S1 \u222a S2 = S. Following the method used in Section 3, we use S1 to represent the Meaningful Subspace and consider S2 as a set of discovered attributes (i.e., D = S2). As human annotators are used to discover S2, these attributes are considered to be meaningful. We name this as the MeaningfulAttributeSet.\nFor the latter, we create attributes that are not meaningful by random generation. Note that random generation is important to ensure the division is not subjective. More precisely, we generate a finite set of random attributes N\u0303 . As the set N\u0303 is non-meaningful, it should have significantly large distance to the Meaningful Subspace. We name this set as NonMeaningfulAttributeSet. Furthermore, we progressively add random attributes to the set of attributes discovered from each method, to evaluate whether the distance to Meaningful Subspace is enlarged when the number of non-meaningful attributes increases.\nFig. ?? presents the evaluation results where the methods are configured to discover 32 attributes. From the results, it is clear that MeaningfulAttributeSet has the closest distance to the Meaningful Subspace in all datasets. As expected the NonMeaningfulAttributeSet has the largest distance compared with the others. In addition, as more random attributes are added, the distance between the sets of attributes discovered for every approach and the Meaningful Subspace increases. These results indicate that the proposed approach could measure the set of attribute meaningfulness. In addition, these also give a strong indication that meaningful attributes have the shared structure.\nThe results presented in Fig. ?? suggest that PiCoDeS consistently discovers the most meaningful attributes on both datasets. SH is the second best method to discover meaningful attributes. PiCoDeS utilizes max-margin framework to discover the attributes whereas SH uses spectral re-\nlaxation to preserve the similarity between data points in the binary space. In addition, as expected LSH employing random projection approach, is one of the worst performing methods."}, {"heading": "5.3. Generating video keywords using discovered", "text": "attributes\nIn this experiment, we will follow the strategy proposed in section 4. Here we ask experts to perform the attribute naming task for the three attribute discovery methods such as PiCoDeS, SH and LSH configured to discover 16 attributes on the UTTower surveillance video dataset. Then we will use the named attributes as the keywords. To make our work reproducible, our experiment results will be available online1 after this work is published.\nNote that we only take into account the attributes that can be named by experts. This means, any attribute that cannot be named will not be considered as a valid keyword. After performing this task, we found that there are 9 attributes for PiCoDeS, 8 attributes for SH and 3 attributes for LSH that can be named. These results suggest that our proposed approach is capable of guiding us in selecting the best attribute discovery methods as the experts are able to name most of the discovered attributes by PiCoDeS and SH.\nOnce attributes are named, the next step is to generate keywords of each video. Technically, the attributes are extracted from each video. Then, the keywords are generated using the terms of the associated positive attributes..\nWe evaluate the quality of the generated keywords to describe each video. We then ask human experts to determine whether a keyword is suitable to describe a video.\nFig. 2 presents two examples where videos are described with suitable keywords and two examples where videos are described with unsuitable keywords. The examples depicted in Fig 2, (a), (b), (c) and (d) are videos of digging, standing, carrying and waving, respectively.\nWe count the number of keywords correctly used in each video description and compute the correct hit rate for the whole testing set. The correct hit rate for PiCoDeS, SPH and LSH are 77.7%, 55.9% and 48.3%, respectively. This further validates our proposed approach to measure attribute meaningfulness. In addition, it also shows that using the best attribute discovery method, we can automatically generate keywords for videos in a more economical way. Fig. 3 presents further results in this evaluation. In particular, (a) and (b) report the hit rate for PiCoDeS of each attribute and action, respectively. The plots in (c) and (d) are the hit rate for SH of each attribute and action, respectively. Most attributes discovered by PiCoDeS have more than 70% hit rate with two attributes having 100% hit rate (all correct). The hit rate for each action also demonstrates an overall good hit rates with most videos being described with hit rate more than 60%. The results for SH are worse than PiCoDeS. Analysis on cost and time saving in the manual process Here we compare the time and cost required to perform manual work between our method and the traditional ap-\n1http://www.itee.uq.edu.au/sas/datasets\nproaches requiring extensive manual processing. The time and cost analysis is based on the AMT Human Intelligent Task (HIT). One HIT normally comprises a set of tasks that human could do to label one image/video data. Let J be the number of keywords and N be the number of training samples which is usually very large number. In our method, we are only required to name the discovered attributes. Hence, our method require just J HITs. On the other hand, traditional approaches require at least N HITs as these require all training samples to have the keywords. Indeed as J << N , then our method massively reduces the time and cost required as it has much less number of HITs."}, {"heading": "6. Conclusion", "text": "In this paper, we described an attribute-based video keyword generation approach. Our approach utilized an existing automatic attribute discovery approach to discover the keywords. Since there have been numerous attribute discovery approaches in the literature, we devise a selection method, based on the shared structure exhibited amongst meaningful attributes, that enables us to compare the efficacy between different automatic attribute discovery approaches. In particular, we devised a distance function that measures the meaningfulness of a set of discovered attributes. We used our approach to select the methods that are most likely to discover meaningful attributes. Then, we validated our approach on two attribute datasets. The results showed that our approach is able to determine which automatic attribute discovery method can generate the most meaningful keywords or attributes. Finally, we showed how the discovered attributes were used to generate keywords for videos recorded from a surveillance system.\nThe proposed approach indicates that it is possible to\ndramatically reduce the amount of manual work in generating video keywords without limiting ourselves to arbitrary preselected video feature descriptors.\nWe note that our proposed selection method only indicates the best attribute discovery method. Thus, a more quantitative approach may be required in future study. In addition, various regularizations such as the `1 regularization for (1) and (2) will be explored in the future. The `1 constraint is an explicit regularization to induce sparsity. As to the robustness aspect, our proposed system depends on the robustness of the selected attribute discovery methods, however, further studies on various surveillance datasets are required to fully understand the proposed system robustness."}], "references": [{"title": "Picodes: Learning a compact code for novel-category recognition", "author": ["A. Bergamo", "L. Torresani", "A.W. Fitzgibbon"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous active learning of classifiers & attributes via relative feedback", "author": ["A. Biswas", "D. Parikh"], "venue": "CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Face recognition based on image sets", "author": ["H. Cevikalp", "B. Triggs"], "venue": "CVPR,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic concept discovery for large-scale zero-shot event detection", "author": ["X. Chang", "Y. Yang", "A.G. Hauptmann", "E.P. Xing", "Y.- L. Yu"], "venue": "In IJCAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Dynamic concept composition for zero-example event detection", "author": ["X. Chang", "Y. Yang", "G. Long", "C. Zhang", "A.G. Hauptmann"], "venue": "AAAI,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Complex event detection using semantic saliency and nearly-isotonic svm", "author": ["X. Chang", "Y. Yang", "E.P. Xing", "Y.-L. Yu"], "venue": "ICML,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Searching persuasively: Joint event detection and evidence recounting with limited supervision", "author": ["X. Chang", "Y.-L. Yu", "Y. Yang", "A.G. Hauptmann"], "venue": "ACM Conference on Multimedia (MM),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Ut-tower dataset: Aerial View Activity Classification Challenge", "author": ["C.-C. Chen", "M.S. Ryoo", "J.K. Aggarwal"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "CVPR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Semanticbased surveillance video retrieval", "author": ["W. Hu", "D. Xie", "Z. Fu", "W. Zeng", "S. Maybank"], "venue": "Image Processing, IEEE Transactions on, 16(4):1168\u20131181,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Recognizing complex events using large margin joint low-level event model", "author": ["H. Izadinia", "M. Shah"], "venue": "ECCV.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A survey on behavior analysis in video surveillance for homeland security applications", "author": ["T. Ko"], "venue": "Applied Imagery Pattern Recognition Workshop, 2008. AIPR \u201908. 37th IEEE,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Attributebased classification for zero-shot learning of object categories", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 99:1,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining of Massive Datasets", "author": ["J. Leskovec", "A. Rajaraman", "J. Ullman"], "venue": "Cambridge university press,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A novel separating strategy for face hallucination", "author": ["L. Liu", "W. Li", "S. Tang", "W. Gong"], "venue": "ICIP,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Anomaly detection in crowded scenes", "author": ["V. Mahadevan", "W. Li", "V. Bhalodia", "N. Vasconcelos"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Interactive exploration of surveillance video through action shot summarization and trajectory visualization", "author": ["A.H. Meghdadi", "P. Irani"], "venue": "Visualization and Computer Graphics, IEEE Transactions on, 19(12):2119\u20132128,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Interactive discovery of taskspecific nameable attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Workshop on Fine-Grained Visual Categorization, CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Interactively building a discriminative vocabulary of nameable attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "CVPR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["G. Patterson", "J. Hays"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Large-scale image retrieval with compressed fisher vectors", "author": ["F. Perronnin", "Y. Liu", "J. S\u00e1nchez", "H. Poirier"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Clustered synopsis of surveillance video", "author": ["Y. Pritch", "S. Ratovitch", "A. Hendel", "S. Peleg"], "venue": "AVSS, pages 195\u2013200,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Video event understanding using natural language descriptions", "author": ["V. Ramanathan", "P. Liang", "L. Fei-Fei"], "venue": "ICCV,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Attribute discovery via predictable discriminative binary codes", "author": ["M. Rastegari", "A. Farhadi", "D. Forsyth"], "venue": "ECCV.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Coherent multi-sentence video description with variable level of detail", "author": ["A. Rohrbach", "M. Rohrbach", "W. Qiu", "A. Friedrich", "M. Pinkal", "B. Schiele"], "venue": "pages 184\u2013195,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["M. Rohrbach", "Q. Wei", "I. Titov", "S. Thater", "M. Pinkal", "B. Schiele"], "venue": "ICCV,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Spatiotemporal covariance descriptors for action and gesture recognition", "author": ["A. Sanin", "C. Sanderson", "M. Harandi", "B. Lovell"], "venue": "WACV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequential topic model for mining recurrent activities from long term video logs", "author": ["J. Varadarajan", "R. Emonet", "J.-M. Odobez"], "venue": "International journal of computer vision, 103(1):100\u2013126,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):480\u2013492, March", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models", "author": ["X. Wang", "X. Ma", "W.E.L. Grimson"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(3):539\u2013555,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering discriminative cell attributes for hep-2 specimen image classification", "author": ["A. Wiliem", "P. Hobson", "B.C. Lovell"], "venue": "WACV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Discovery of shared semantic spaces for multi-scene video query and summarization", "author": ["X. Xu", "T. Hospedales", "S. Gong"], "venue": "arXiv preprint arXiv:1507.07458,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernelised orthonormal random projection on grassmann manifolds with applications to action and gait-based gender recognition", "author": ["K. Zhao", "A. Wiliem", "B. Lovell"], "venue": "Identity, Security and Behavior Analysis (ISBA), 2015 IEEE International Conference on,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "For example, they can be used to detect anomalous events to alert security officers [13].", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "This makes finding critical information in surveillance video as challenging as finding the proverbial needle in a haystack [23].", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 5, "context": "Some examples of the current works are: action recognition [28], face hallucination [16], anomaly detection [17], video description [18] and video complex event detection [7, 6].", "startOffset": 171, "endOffset": 177}, {"referenceID": 26, "context": "Keywords are important ingredients in generating textual descriptions [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "Unfortunately, existing approaches still require a great deal of manual labelling before the systems can be used to generate the keywords/description [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "in [12] uses extensive spatio temporal annotations to train action and role models for action recognition.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "One feasible way to circumvent this is to employ latent hierarchical probabilistic models such as probabilistic Latent Semantic Analysis (pLSA) [29] or Latent Dirichlet Allocation (LDA) [31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 30, "context": "One feasible way to circumvent this is to employ latent hierarchical probabilistic models such as probabilistic Latent Semantic Analysis (pLSA) [29] or Latent Dirichlet Allocation (LDA) [31].", "startOffset": 186, "endOffset": 190}, {"referenceID": 0, "context": "More specifically, several attribute discovery methods such as PiCoDeS [1] and Spectral Hashing [32] can be employed.", "startOffset": 71, "endOffset": 74}, {"referenceID": 31, "context": "More specifically, several attribute discovery methods such as PiCoDeS [1] and Spectral Hashing [32] can be employed.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "In practice, we can represent the binary features as [1 1 0].", "startOffset": 53, "endOffset": 60}, {"referenceID": 0, "context": "In practice, we can represent the binary features as [1 1 0].", "startOffset": 53, "endOffset": 60}, {"referenceID": 8, "context": "The attribute features trained in one domain can be reused for another domain with minimum manual work [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "As such, a system can be potentially trained to recognize unseen events [14].", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "Visual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 23, "context": "Visual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].", "startOffset": 97, "endOffset": 105}, {"referenceID": 13, "context": "Visual attributes have shown promising results in many works which deal with video related tasks [27, 24] as well as in some novel problems such as the zero shot learning problem [14].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "To that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33].", "startOffset": 99, "endOffset": 110}, {"referenceID": 24, "context": "To that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33].", "startOffset": 99, "endOffset": 110}, {"referenceID": 32, "context": "To that end, some researchers have turned their attention to automatic attribute discovery methods [1, 25, 33].", "startOffset": 99, "endOffset": 110}, {"referenceID": 9, "context": "We note that these approaches are also closely related to hashing approaches [10, 15, 32].", "startOffset": 77, "endOffset": 89}, {"referenceID": 14, "context": "We note that these approaches are also closely related to hashing approaches [10, 15, 32].", "startOffset": 77, "endOffset": 89}, {"referenceID": 31, "context": "We note that these approaches are also closely related to hashing approaches [10, 15, 32].", "startOffset": 77, "endOffset": 89}, {"referenceID": 18, "context": "The intuition of our approach comes from a speculation proposed in [19, 20].", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "The intuition of our approach comes from a speculation proposed in [19, 20].", "startOffset": 67, "endOffset": 75}, {"referenceID": 25, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 26, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 30, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 28, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 34, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 4, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 3, "context": "There are several methods proposed recently that deal with video keyword and description extraction [26, 27, 31, 29, 35, 5, 4].", "startOffset": 100, "endOffset": 126}, {"referenceID": 26, "context": "[27] proposed to generate a rich semantic representation of the visual content such as object and activity labels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In [26], they extended their work to a threelevel-of-detail video description scheme.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[31] and Varadarajan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] employ LDA and pLSA respectively to perform unsupervised activity analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "This means, more powerful features such as Fisher vectors [22] cannot be used directly.", "startOffset": 58, "endOffset": 62}, {"referenceID": 34, "context": "[35] develop a novel distributed multiple-scene global understanding framework that clusters surveillance scenes by their ability to explain each others behaviours.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Hence, in this case, we assume that all attributes lie on a manifold of decision boundaries [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20].", "startOffset": 103, "endOffset": 114}, {"referenceID": 18, "context": "One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20].", "startOffset": 103, "endOffset": 114}, {"referenceID": 19, "context": "One possible solution is to use previously human labelled attributes in various image datasets such as [2, 19, 20].", "startOffset": 103, "endOffset": 114}, {"referenceID": 2, "context": "One possible way to address this is to add the convex hull regularization which has been shown in [3] to induce sparsity.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "The above optimization problem could be solved using the method proposed in [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15].", "startOffset": 171, "endOffset": 174}, {"referenceID": 31, "context": "Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15].", "startOffset": 237, "endOffset": 241}, {"referenceID": 14, "context": "Then, we use our proposed approach to evaluate attribute meaningfulness on the attribute sets generated from various automatic attribute discovery methods such as PiCoDeS [1] as well as the hashing methods such as Spectral Hashing (SPH) [32] and Locality Sensitivity Hashing (LSH) [15].", "startOffset": 281, "endOffset": 285}, {"referenceID": 8, "context": "For this case, two datasets will be utilized: (1) a-Pascal a-Yahoo dataset (ApAy) [9]; (2) SUN Attribute dataset (ASUN) [21].", "startOffset": 82, "endOffset": 85}, {"referenceID": 20, "context": "For this case, two datasets will be utilized: (1) a-Pascal a-Yahoo dataset (ApAy) [9]; (2) SUN Attribute dataset (ASUN) [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 7, "context": "In this setting, we utilize the UT Tower aerial view dataset (UTTower) [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "a-Pascal a-Yahoo dataset (ApAy) [9] \u2014 comprises two sources: a-Pascal and a-Yahoo.", "startOffset": 32, "endOffset": 35}, {"referenceID": 20, "context": "SUN Attribute dataset (ASUN) [21] \u2014 ASUN is a finegrained scene classification dataset consisting of 717 categories (20 images per category) and 14,340 images in total with 102 attributes.", "startOffset": 29, "endOffset": 33}, {"referenceID": 33, "context": "There are four types of features provided in this dataset: (1) GIST; (2) HOG; (3) selfsimilarity and (4) geometric context color histograms (See [34] for feature and kernel details).", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "For the first experiment, we apply the following preprocessing described in [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 29, "context": "We first lift each feature into a higher-dimensional space approximating the histogram intersection kernel by using the explicit feature maps proposed by Vedaldi and Zisserman [30].", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "This effectively allows us to apply linear classifiers in the explicit kernel space [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "UT Tower aerial view activity classification dataset (UTTower) [8] \u2014 consists of 108 low-resolution video sequences from 9 types of actions.", "startOffset": 63, "endOffset": 66}, {"referenceID": 35, "context": "For the second experiment, we use manifold feature proposed in [36] to extract visual information from the surveillance videos in the dataset.", "startOffset": 63, "endOffset": 67}], "year": 2016, "abstractText": "Automatic video keyword generation is one of the key ingredients in reducing the burden of security officers in analyzing surveillance videos.", "creator": "LaTeX with hyperref package"}}}