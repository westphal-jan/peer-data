{"id": "1602.07017", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2016", "title": "A survey of sparse representation: algorithms and applications", "abstract": "sparse representation has attracted much attention from researchers in most fields of noisy signal processing, like image processing, computer vision and pattern recognition. sparse representation also has gaining a also good reputation in both theoretical research and practical recognition applications. many different discrete algorithms have always been proposed for sparse noise representation. the main recognized purpose of this article is to provide a comprehensive study report and an updated review on sparse representation definitions and techniques to supply a guidance for researchers. the taxonomy of sparse representation methods can be studied from various viewpoints. for numerical example, in approximate terms of different norm minimizations used in sparsity constraints, the methods can typically be roughly categorized categorized into five programming groups : sparse representation with $ / l _ 0 $ - semi norm minimization, sparse representation with $ l _ p $ - norm ( 0 $ & lt ; $ p $ & lt ; $ 1 ) minimization, sparse network representation with $ l _ 1 $ - average norm variation minimization and sparse representation with $ l _ { 2, 1 } $ - sum norm minimization. in this paper, a comprehensive survey overview of sparse representation is provided. the available sparse representation algorithms can moreover also be empirically categorized into four groups : greedy strategy approximation, constrained optimization, proximity algorithm - based optimization, and homotopy algorithm - based sparse representation. the rationales of different programming algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal into the potential nature of the existing sparse representation theory. specifically, an experimentally comparative study of these sparse - representation algorithms was presented. the matlab code used in this paper can be available at :", "histories": [["v1", "Tue, 23 Feb 2016 02:44:53 GMT  (3200kb)", "http://arxiv.org/abs/1602.07017v1", "Published on IEEE Access, Vol. 3, pp. 490-530, 2015"]], "COMMENTS": "Published on IEEE Access, Vol. 3, pp. 490-530, 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["zheng zhang", "yong xu", "jian yang", "xuelong li", "david zhang"], "accepted": false, "id": "1602.07017"}, "pdf": {"name": "1602.07017.pdf", "metadata": {"source": "CRF", "title": "A survey of sparse representation: algorithms and applications", "authors": ["Zheng Zhang"], "emails": ["(yongxu@ymail.com).", "yongxu@ymail.com)."], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n07 01\n7v 1\n[ cs\n.C V\n] 2\n3 Fe\nb 20\n16 JOURNAL 1\nIndex Terms\u2014Sparse representation, compressive sensing, greedy algorithm, constrained optimization, proximal algorithm, homotopy algorithm, dictionary learning\nI. INTRODUCTION\nW ITH advancements in mathematics, linear represen-tation methods (LRBM) have been well studied and have recently received considerable attention [1, 2]. The sparse representation method is the most representative methodology of the LRBM and has also been proven to be an extraordinary powerful solution to a wide range of application fields, especially in signal processing, image processing, machine\nZheng Zhang and Yong Xu is with the Bio-Computing Research Center, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen 518055, Guangdong, P.R. China; Key Laboratory of Network Oriented Intelligent Computation, Shenzhen 518055, Guangdong, P.R. China e-mail: (yongxu@ymail.com).\nJian Yang is with the College of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094, P. R. China.\nXuelong Li is with the Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi\u2019an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi\u2019an 710119, Shaanxi, P. R. China.\nDavid Zhang is with the Biometrics Research Center, The Hong Kong Polytechnic University, Hong Kong\nCorresponding author: Yong Xu (email: yongxu@ymail.com).\nlearning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310]. Sparse representation has shown huge potential capabilities in handling these problems.\nSparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years. Donoho [11] first proposed the original concept of compressed sensing. CS theory suggests that if a signal is sparse or compressive, the original signal can be reconstructed by exploiting a few measured values, which are much less than the ones suggested by previously used theories such as Shannon\u2019s sampling theorem (SST). Candes et al. [13], from the mathematical perspective, demonstrated the rationale of CS theory, i.e. the original signal could be precisely reconstructed by utilizing a small portion of Fourier transformation coefficients. Baraniuk [12] provided a concrete analysis of compressed sensing and presented a specific interpretation on some solutions of different signal reconstruction algorithms. All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research. Thus, a large number of algorithms based on CS theory have been proposed to address different problems in various fields. Moreover, CS theory always includes the three basic components: sparse representation, encoding measuring, and reconstructing algorithm. As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields. For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].\nSparse representation has attracted much attention in recent years and many examples in different fields can be found where sparse representation is definitely beneficial and favorable [18, 19]. One example is image classification, where the basic goal is to classify the given test image into several predefined categories. It has been demonstrated that natural images can be sparsely represented from the perspective of the properties of visual neurons. The sparse representation based classification (SRC) method [20] first assumes that the test sample can be sufficiently represented by samples from the same subject. Specifically, SRC exploits the linear combination of training samples to represent the test sample and computes sparse representation coefficients of the linear representation system, and then calculates the reconstruction residuals of each class employing the sparse representation\ncoefficients and training samples. The test sample will be classified as a member of the class, which leads to the minimum reconstruction residual. The literature [20] has also demonstrated that the SRC method has great superiorities when addressing the image classification issue on corrupted or disguised images. In such cases, each natural image can be sparsely represented and the sparse representation theory can be utilized to fulfill the image classification task.\nFor signal processing, one important task is to extract key components from a large number of clutter signals or groups of complex signals in coordination with different requirements. Before the appearance of sparse representation, SST and Nyquist sampling law (NSL) were the traditional methods for signal acquisition and the general procedures included sampling, coding compression, transmission, and decoding. Under the frameworks of SST and NSL, the greatest difficulty of signal processing lies in efficient sampling from mass data with sufficient memory-saving. In such a case, sparse representation theory can simultaneously break the bottleneck of conventional sampling rules, i.e. SST and NSL, so that it has a very wide application prospect. Sparse representation theory proposes to integrate the processes of signal sampling and coding compression. Especially, sparse representation theory employs a more efficient sampling rate to measure the original sample by abandoning the pristine measurements of SST and NSL, and then adopts an optimal reconstruction algorithm to reconstruct samples. In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7]. Compared to the primary signal space, the size of the set of possible signals can be largely decreased under the constraint of sparsity. Thus, massive algorithms based on the sparse representation theory have been proposed to effectively tackle signal processing issues such as signal reconstruction and recovery. To this end, the sparse representation technique can save a significant amount of sampling time and sample storage space and it is favorable and advantageous."}, {"heading": "A. Categorization of sparse representation techniques", "text": "Sparse representation theory can be categorized from different viewpoints. Because different methods have their individual motivations, ideas, and concerns, there are varieties of strategies to separate the existing sparse representation methods into different categories from the perspective of taxonomy. For example, from the viewpoint of \u201catoms\u201d, available sparse representation methods can be categorized into two general groups: naive sample based sparse representation and dictionary learning based sparse representation. However, on the basis of the availability of labels of \u201catoms\u201d, sparse representation and learning methods can be coarsely divided into three groups: supervised learning, semi-supervised learning, and unsupervised learning methods. Because of the sparse constraint, sparse representation methods can be divided into two communities: structure constraint based sparse representation and sparse constraint based sparse representation. Moreover, in the field of image classification, the representation based classification methods consist of two main categories in terms\nof the way of exploiting the \u201catoms\u201d: the holistic representation based method and local representation based method [21]. More specifically, holistic representation based methods exploit training samples of all classes to represent the test sample, whereas local representation based methods only employ training samples (or atoms) of each class or several classes to represent the test sample. Most of the sparse representation methods are holistic representation based methods. A typical and representative local sparse representation methods is the two-phase test sample sparse representation (TPTSR) method [9]. In consideration of different methodologies, the sparse representation method can be grouped into two aspects: pure sparse representation and hybrid sparse representation, which improves the pre-existing sparse representation methods with the aid of other methods. The literature [22] suggests that sparse representation algorithms roughly fall into three classes: convex relaxation, greedy algorithms, and combinational methods. In the literature [23, 24], from the perspective of sparse problem modeling and problem solving, sparse decomposition algorithms are generally divided into two sections: greedy algorithms and convex relaxation algorithms. On the other hand, if the viewpoint of optimization is taken into consideration, the problems of sparse representation can be divided into four optimization problems: the smooth convex problem, nonsmooth nonconvex problem, smooth nonconvex problem, and nonsmooth convex problem. Furthermore, Schmidt et al. [25] reviewed some optimization techniques for solving l1norm regularization problems and roughly divided these approaches into three optimization strategies: sub-gradient methods, unconstrained approximation methods, and constrained optimization methods. The supplementary file attached with the paper also offers more useful information to make fully understandings of the \u2018taxonomy\u2019 of current sparse representation techniques in this paper.\nIn this paper, the available sparse representation methods are categorized into four groups, i.e. the greedy strategy approximation, constrained optimization strategy, proximity algorithm based optimization strategy, and homotopy algorithm based sparse representation, with respect to the analytical solution and optimization viewpoints.\n(1) In the greedy strategy approximation for solving sparse representation problem, the target task is mainly to solve the sparse representation method with l0-norm minimization. Because of the fact that this problem is an NP-hard problem [26], the greedy strategy provides an approximate solution to alleviate this difficulty. The greedy strategy searches for the best local optimal solution in each iteration with the goal of achieving the optimal holistic solution [27]. For the sparse representation method, the greedy strategy approximation only chooses the most k appropriate samples, which are called ksparsity, to approximate the measurement vector.\n(2) In the constrained optimization strategy, the core idea is to explore a suitable way to transform a non-differentiable optimization problem into a differentiable optimization problem by replacing the l1-norm minimization term, which is convex but nonsmooth, with a differentiable optimization term, which is convex and smooth. More specifically, the constrained optimization strategy substitutes the l1-norm minimization term\nwith an equal constraint condition on the original unconstraint problem. If the original unconstraint problem is reformulated into a differentiable problem with constraint conditions, it will become an uncomplicated problem in the consideration of the fact that l1-norm minimization is global non-differentiable.\n(3) Proximal algorithms can be treated as a powerful tool for solving nonsmooth, constrained, large-scale, or distributed versions of the optimization problem [28]. In the proximity algorithm based optimization strategy for sparse representation, the main task is to reformulate the original problem into the specific model of the corresponding proximal operator such as the soft thresholding operator, hard thresholding operator, and resolvent operator, and then exploits the proximity algorithms to address the original sparse optimization problem.\n(4) The general framework of the homotopy algorithm is to iteratively trace the final desired solution starting from the initial point to the optimal point by successively adjusting the homotopy parameter [29]. In homotopy algorithm based sparse representation, the homotopy algorithm is used to solve the l1norm minimization problem with k-sparse property."}, {"heading": "B. Motivation and objectives", "text": "In this paper, a survey on sparse representation and overview available sparse representation algorithms from viewpoints of the mathematical and theoretical optimization is provided. This paper is designed to provide foundations of the study on sparse representation and aims to give a good start to newcomers in computer vision and pattern recognition communities, who are interested in sparse representation methodology and its related\nfields. Extensive state-of-art sparse representation methods are summarized and the ideas, algorithms, and wide applications of sparse representation are comprehensively presented. Specifically, there is concentration on introducing an up-todate review of the existing literature and presenting some insights into the studies of the latest sparse representation methods. Moreover, the existing sparse representation methods are divided into different categories. Subsequently, corresponding typical algorithms in different categories are presented and their distinctness is explicitly shown. Finally, the wide applications of these sparse representation methods in different fields are introduced.\nThe remainder of this paper is mainly composed of four parts: basic concepts and frameworks are shown in Section II and Section III, representative algorithms are presented in Section IV-VII and extensive applications are illustrated in Section VIII, massive experimental evaluations are summarized in Section IX. More specifically, the fundamentals and preliminary mathematic concepts are presented in Section II, and then the general frameworks of the existing sparse representation with different norm regularizations are summarized in Section III. In Section IV, the greedy strategy approximation method is presented for obtaining a sparse representation solution, and in Section V, the constrained optimization strategy is introduced for solving the sparse representation issue. Furthermore, the proximity algorithm based optimization strategy and Homotopy strategy for addressing the sparse representation problem are outlined in Section VI and Section VII, respectively. Section VIII presents extensive applications of sparse represen-\ntation in widespread and prevalent fields including dictionary learning methods and real-world applications. Finally, Section IX offers massive experimental evaluations and conclusions are drawn and summarized in Section X. The structure of the this paper has been summarized in Fig. 1."}, {"heading": "II. FUNDAMENTALS AND PRELIMINARY CONCEPTS", "text": ""}, {"heading": "A. Notations", "text": "In this paper, vectors are denoted by lowercase letters with bold face, e.g. x. Matrices are denoted by uppercase letter, e.g. X and their elements are denoted with indexes such as Xi. In this paper, all the data are only real-valued.\nSuppose that the sample is from space Rd and thus all the samples are concatenated to form a matrix, denoted as D \u2208 R\nd\u00d7n. If any sample can be approximately represented by a linear combination of dictionary D and the number of the samples is larger than the dimension of samples in D, i.e. n > d, dictionary D is referred to as an over-complete dictionary. A signal is said to be compressible if it is a sparse signal in the original or transformed domain when there is no information or energy loss during the process of transformation.\n\u201csparse\u201d or \u201csparsity\u201d of a vector means that some elements of the vector are zero. We use a linear combination of a basis matrix A \u2208 RN\u00d7N to represent a signal x \u2208 RN\u00d71, i.e. x = As where s \u2208 RN\u00d71 is the column vector of weighting coefficients. If only k (k \u226a N ) elements of s are nonzero and the rest elements in s are zero, we call the signal x is k-sparse."}, {"heading": "B. Basic background", "text": "The standard inner product of two vectors, x and y from the set of real n dimensions, is defined as\n\u3008x,y\u3009 = xTy = x1y1 + x2y2 + \u00b7 \u00b7 \u00b7+ xnyn (II.1)\nThe standard inner product of two matrixes, X \u2208 Rm\u00d7n and Y \u2208 Rm\u00d7n from the set of real m \u00d7 n matrixes, is denoted as the following equation\n\u3008X,Y \u3009 = tr(XTY ) = m\u2211\ni=1\nn\u2211\nj=1\nXijYij (II.2)\nwhere the operator tr(A) denotes the trace of the matrix A, i.e. the sum of its diagonal entries.\nSuppose that v = [v1,v2, \u00b7 \u00b7 \u00b7 ,vn] is an n dimensional vector in Euclidean space, thus\n\u2016v\u2016p = ( n\u2211\ni=1\n|vi|p)1/p (II.3)\nis denoted as the p-norm or the lp-norm (1 \u2264 p \u2264 \u221e) of vector v.\nWhen p=1, it is called the l1-norm. It means the sum of absolute values of the elements in vector v , and its geometric interpretation is shown in Fig. 2b, which is a square with a forty-five degree rotation.\nWhen p=2, it is called the l2-norm or Euclidean norm. It is defined as \u2016v\u20162 = (v21 + v22 + \u00b7 \u00b7 \u00b7+v2n)1/2, and its geometric interpretation in 2-D space is shown in Fig. 2c which is a circle.\nIn the literature, the sparsity of a vector v is always related to the so-called l0-norm, which means the number of the nonzero elements of vector v. Actually, the l0-norm is the limit as p \u2192 0 of the lp-norms [8] and the definition of the l0-norm is formulated as\n\u2016v\u20160 = lim p\u21920 \u2016v\u2016pp = lim p\u21920\nn\u2211\ni=1\n|vi|p (II.4)\nWe can see that the notion of the l0-norm is very convenient and intuitive for defining the sparse representation problem. The property of the l0-norm can also be presented from the perspective of geometric interpretation in 2-D space, which is shown in Fig. 2a, and it is a crisscross.\nFurthermore, the geometric meaning of the lp-norm (0<p<1) is also presented, which is a form of similar recessed pentacle shown in Fig. 2d.\nOn the other hand, it is assumed that f(x) is the function of the lp-norm (p>0) on the parameter vector x, and then the following function is obtained:\nf(x) = \u2016x\u2016pp = ( n\u2211\ni=1\n|xi|p) (II.5)\nThe relationships between different norms are summarized in Fig. 3. From the illustration in Fig. 3, the conclusions are as follows. The l0-norm function is a nonconvex, nonsmooth, discontinuity, global nondifferentiable function. The lp-norm (0<p<1) is a nonconvex, nonsmooth, global nondifferentiable function. The l1-norm function is a convex, nonsmooth, global nondifferentiable function. The l2-norm function is a convex, smooth, global differentiable function.\nIn order to more specifically elucidate the meaning and solutions of different norm minimizations, the geometry in 2-D space is used to explicitly illustrate the solutions of the l0-norm minimization in Fig. 4a, l1-norm minimization in Fig. 4b, and l2-norm minimization in Fig. 4c. Let S = {x\u2217 : Ax = y} denote the line in 2-D space and a hyperplane will be formulated in higher dimensions. All possible solution x\u2217 must lie on the line of S. In order to visualize how to obtain the solution of different norm-based minimization problems, we take the l1-norm minimization problem as an example to explicitly interpret. Suppose that we inflate the l1ball from an original status until it hits the hyperplane S at some point. Thus, the solution of the l1-norm minimization problem is the aforementioned touched point. If the sparse solution of the linear system is localized on the coordinate axis, it will be sparse enough. From the perspective of Fig. 4, it can be seen that the solutions of both the l0-norm and l1-norm minimization are sparse, whereas for the l2norm minimization, it is very difficult to rigidly satisfy the\ncondition of sparsity. However, it has been demonstrated that the representation solution of the l2-norm minimization is not strictly sparse enough but \u201climitedly-sparse\u201d, which means it possesses the capability of discriminability [30].\nThe Frobenius norm, L1-norm of matrix X \u2208 Rm\u00d7n, and l2-norm or spectral norm are respectively defined as\n\u2016X\u2016F = ( n\u2211\ni=1\nm\u2211\nj=1\nX2j,i) 1/2, \u2016X\u2016L1 = maxj=1,...,n\nm\u2211\ni=1\n|xij |,\n\u2016X\u20162 = \u03b4max(X) = (\u03bbmax(XTX))1/2 (II.6)\nwhere \u03b4 is the singular value operator and the l2-norm of X is its maximum singular value [31].\nThe l2,1-norm or R1-norm is defined on matrix term, that is\n\u2016X\u20162,1 = n\u2211\ni=1\n(\nm\u2211\nj=1\nX2j,i) 1/2 (II.7)\nAs shown above, a norm can be viewed as a measure of the length of a vector v. The distance between two vectors x and y, or matrices X and Y , can be measured by the length of their differences, i.e.\ndist(x,y) = \u2016x\u2212 y\u201622, dist(X,Y ) = \u2016X \u2212 Y \u2016F (II.8) which are denoted as the distance between x and y in the context of the l2-norm and the distance between X and Y in the context of the Frobenius norm, respectively.\nAssume that X \u2208 Rm\u00d7n and the rank of X , i.e. rank(X) = r. The SVD of X is computed as\nX = U\u039bV T (II.9)\nwhere U \u2208 Rm\u00d7r with UTU = I and V \u2208 Rn\u00d7r with V TV = I . The columns of U and V are called left and right singular vectors of X , respectively. Additionally, \u039b is a diagonal matrix and its elements are composed of the singular values of X , i.e. \u039b = diag(\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbr) with \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbr > 0. Furthermore, the singular value decomposition can be rewritten as\nX =\nr\u2211\ni=1\n\u03bbiuivi (II.10)\nwhere \u03bbi, ui and vi are the i-th singular value, the i-th column of U , and the i-th column of V , respectively [31]."}, {"heading": "III. SPARSE REPRESENTATION PROBLEM WITH DIFFERENT NORM REGULARIZATIONS", "text": "In this section, sparse representation is summarized and grouped into different categories in terms of the norm regularizations used. The general framework of sparse representation is to exploit the linear combination of some samples or \u201catoms\u201d to represent the probe sample, to calculate the representation solution, i.e. the representation coefficients of these samples or \u201catoms\u201d, and then to utilize the representation solution to reconstruct the desired results. The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335]. Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].\nA. Sparse representation with l0-norm minimization Let x1,x2, \u00b7 \u00b7 \u00b7 ,xn \u2208 Rd be all the n known samples and matrix X \u2208 Rd\u00d7n (d<n), which is constructed by known samples, is the measurement matrix or the basis dictionary and should also be an over-completed dictionary. Each column of X is one sample and the probe sample is y \u2208 Rd , which is a column vector. Thus, if all the known samples are used to approximately represent the probe sample, it should be expressed as:\ny = x1\u03b11 + x2\u03b12 + \u00b7 \u00b7 \u00b7+ xn\u03b1n (III.1)\nwhere \u03b1i (i=1,2,\u00b7 \u00b7 \u00b7 ,n) is the coefficient of xi and Eq. III.1 can be rewritten into the following equation for convenient description:\ny = X\u03b1 (III.2)\nwhere matrix X=[x1,x2, \u00b7 \u00b7 \u00b7 ,xn] and \u03b1=[\u03b11,\u03b12, \u00b7 \u00b7 \u00b7 ,\u03b1n]T . However, problem III.2 is an underdetermined linear system of equations and the main problem is how to solve it. From the viewpoint of linear algebra, if there is not any prior knowledge or any constraint imposed on the representation solution \u03b1, problem III.2 is an ill-posed problem and will never have a unique solution. That is, it is impossible to utilize equation III.2 to uniquely represent the probe sample y using the measurement matrix X . To alleviate this difficulty, it is feasible to impose an appropriate regularizer constraint or regularizer function on representation solution \u03b1. The sparse representation method demands that the obtained representation solution should be sparse. Hereafter, the meaning of \u2018sparse\u2019 or \u2018sparsity\u2019 refers to the condition that when the linear combination of measurement matrix is exploited to represent the probe sample, many of the coefficients should\nbe zero or very close to zero and few of the entries in the representation solution are differentially large.\nThe sparsest representation solution can be acquired by solving the linear representation system III.2 with the l0norm minimization constraint [52]. Thus problem III.2 can be converted to the following optimization problem:\n\u03b1\u0302 = argmin \u2016\u03b1\u20160 s.t. y = X\u03b1 (III.3)\nwhere \u2016 \u00b7 \u20160 refers to the number of nonzero elements in the vector and is also viewed as the measure of sparsity. Moreover, if just k (k < n) atoms from the measurement matrix X are utilized to represent the probe sample, problem III.3 will be equivalent to the following optimization problem:\ny = X\u03b1 s.t. \u2016\u03b1\u20160 \u2264 k (III.4)\nProblem III.4 is called the k-sparse approximation problem. Because real data always contains noise, representation noise is unavoidable in most cases. Thus the original model III.2 can be revised to a modified model with respect to small possible noise by denoting\ny = X\u03b1+ s (III.5)\nwhere s \u2208 Rd refers to representation noise and is bounded as \u2016s\u20162 \u2264 \u03b5. With the presence of noise, the sparse solutions of problems III.3 and III.4 can be approximately obtained by resolving the following optimization problems:\n\u03b1\u0302 = argmin \u2016\u03b1\u20160 s.t. \u2016y \u2212X\u03b1\u201622 \u2264 \u03b5 (III.6)\nor\n\u03b1\u0302 = argmin \u2016y \u2212X\u03b1\u201622 s.t. \u2016\u03b1\u20160 \u2264 \u03b5 (III.7)\nFurthermore, according to the Lagrange multiplier theorem, a proper constant \u03bb exists such that problems III.6 and III.7 are equivalent to the following unconstrained minimization problem with a proper value of \u03bb.\n\u03b1\u0302 = L(\u03b1, \u03bb) = argmin \u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u20160 (III.8)\nwhere \u03bb refers to the Lagrange multiplier associated with \u2016\u03b1\u20160.\nB. Sparse representation with l1-norm minimization\nThe l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355]. Although the sparse representation method with l0-norm minimization can obtain the fundamental sparse solution of \u03b1 over the matrix X , the problem is still a non-deterministic polynomial-time hard (NP-hard) problem and the solution is difficult to approximate [26]. Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability. Moreover, the l1-norm optimization problem has an analytical solution and can be solved in polynomial time. Thus, extensive sparse representation methods with the l1-norm minimization have\nbeen proposed to enrich the sparse representation theory. The applications of sparse representation with the l1-norm minimization are extraordinarily and remarkably widespread. Correspondingly, the main popular structures of sparse representation with the l1-norm minimization , similar to sparse representation with l0-norm minimization, are generally used to solve the following problems:\n\u03b1\u0302 = argmin \u03b1\n\u2016\u03b1\u20161 s.t. y = X\u03b1 (III.9)\n\u03b1\u0302 = argmin \u03b1\n\u2016\u03b1\u20161 s.t. \u2016y \u2212X\u03b1\u201622 \u2264 \u03b5 (III.10)\nor\n\u03b1\u0302 = argmin \u03b1\n\u2016y \u2212X\u03b1\u201622 s.t. \u2016\u03b1\u20161 \u2264 \u03c4 (III.11)\n\u03b1\u0302 = L(\u03b1, \u03bb) = argmin \u03b1\n1 2 \u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 (III.12)\nwhere \u03bb and \u03c4 are both small positive constants.\nC. Sparse representation with lp-norm (0<p<1) minimization\nThe general sparse representation method is to solve a linear representation system with the lp-norm minimization problem. In addition to the l0-norm minimization and l1-norm minimization, some researchers are trying to solve the sparse representation problem with the lp-norm (0<p<1) minimization, especially p = 0.1, 12 , 1 3 , or 0.9 [59\u201361]. That is, the sparse representation problem with the lp-norm (0<p<1) minimization is to solve the following problem:\n\u03b1\u0302 = argmin \u03b1\n\u2016\u03b1\u2016pp s.t. \u2016y \u2212X\u03b1\u201622 \u2264 \u03b5 (III.13)\nor\n\u03b1\u0302 = L(\u03b1, \u03bb) = argmin \u03b1\n\u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u2016pp (III.14)\nIn spite of the fact that sparse representation methods with the lp-norm (0<p<1) minimization are not the mainstream methods to obtain the sparse representation solution, it tremendously influences the improvements of the sparse representation theory.\nD. Sparse representation with l2,1-norm minimization\nThe representation solution obtained by the l2-norm minimization is not rigorously sparse. It can only obtain a \u2018limitedlysparse\u2019 representation solution, i.e. the solution has the property that it is discriminative and distinguishable but is not really sparse enough [30]. The objective function of the sparse representation method with the l2-norm minimization is to solve the following problem:\n\u03b1\u0302 = argmin \u03b1\n\u2016\u03b1\u201622 s.t. \u2016y \u2212X\u03b1\u201622 \u2264 \u03b5 (III.15)\nor\n\u03b1\u0302 = L(\u03b1, \u03bb) = argmin \u03b1\n\u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u201622 (III.16)\nOn the other hand, the l2,1-norm is also called the rotation invariant l1-norm, which is proposed to overcome the difficulty of robustness to outliers [62]. The objective function of the sparse representation problem with the l2,1-norm minimization is to solve the following problem:\nargmin A\n\u2016Y \u2212XA\u20162,1 + \u00b5\u2016A\u20162,1 (III.17)\nwhere Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,yN ] refers to the matrix composed of samples, A = [a1,a2, \u00b7 \u00b7 \u00b7 ,aN ] is the corresponding coefficient matrix of X , and \u00b5 is a small positive constant. Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347]."}, {"heading": "IV. GREEDY STRATEGY APPROXIMATION", "text": "Greedy algorithms date back to the 1950s. The core idea of the greedy strategy [7, 23] is to determine the position based on the relationship between the atom and probe sample, and then to use the least square to evaluate the amplitude value. Greedy algorithms can obtain the local optimized solution in each step in order to address the problem. However, the greedy algorithm can always produce the global optimal solution or an approximate overall solution [7, 23]. Addressing sparse representation with l0-norm regularization, i.e. problem III.3, is an NP hard problem [20, 56]. The greedy strategy provides a special way to obtain an approximate sparse representation solution. The greedy strategy actually can not directly solve the optimization problem and it only seeks an approximate solution for problem III.3."}, {"heading": "A. Matching pursuit algorithm", "text": "The matching pursuit (MP) algorithm [63] is the earliest and representative method of using the greedy strategy to approximate problem III.3 or III.4. The main idea of the MP is to iteratively choose the best atom from the dictionary based on a certain similarity measurement to approximately obtain the sparse solution. Taking as an example of the sparse decomposition with a vector sample y over the over-complete dictionary D, the detailed algorithm description is presented as follows:\nSuppose that the initialized representation residual is R0 = y, D = [d1,d2, \u00b7 \u00b7 \u00b7 ,dN ] \u2208 Rd\u00d7N and each sample in dictionary D is an l2-norm unity vector, i.e. \u2016di\u2016 = 1. To approximate y, MP first chooses the best matching atom from D and the selected atom should satisfy the following condition:\n|\u3008R0,dl0\u3009| = sup|\u3008R0,di\u3009| (IV.1) where l0 is a label index from dictionary D. Thus y can be decomposed into the following equation:\ny = \u3008y,dl0\u3009dl0 +R1 (IV.2) So y = \u3008R0,dl0\u3009dl0 +R1 where \u3008R0,dl0\u3009dl0 represents the orthogonal projection of y onto dl0 , and R1 is the representation residual by using dl0 to represent y. Considering the fact that dl0 is orthogonal to R1, Eq. IV.2 can be rewritten as\n\u2016y\u20162 = |\u3008y,dl0\u3009|2 + \u2016R1\u20162 (IV.3)\nTo obtain the minimum representation residual, the MP algorithm iteratively figures out the best matching atom from the over-completed dictionary, and then utilizes the representation residual as the next approximation target until the termination condition of iteration is satisfied. For the t-th iteration, the best matching atom is dlt and the approximation result is found from the following equation:\nRt = \u3008Rt,dlt\u3009dlt +Rt+1 (IV.4)\nwhere the dlt satisfies the equation:\n|\u3008Rt,dlt\u3009| = sup|\u3008Rt,di\u3009| (IV.5)\nClearly, dlt is orthogonal to Rk+1, and then\n\u2016Rk\u20162 = |\u3008Rt,dlt\u3009|2 + \u2016Rt+1\u20162 (IV.6)\nFor the n-th iteration, the representation residual \u2016Rn\u20162 \u2264 \u03c4 where \u03c4 is a very small constant and the probe sample y can be formulated as:\ny = n\u22121\u2211\nj=1\n\u3008Rj ,dlj \u3009dlj +Rn (IV.7)\nIf the representation residual is small enough, the probe sample y can approximately satisfy the following equation: y \u2248 \u2211n\u22121j=1 \u3008Rj ,dlj \u3009dlj where n \u226a N . Thus, the probe sample can be represented by a small number of elements from a large dictionary. In the context of the specific representation error, the termination condition of sparse representation is that the representation residual is smaller than the presupposed value. More detailed analysis on matching pursuit algorithms can be found in the literature [63]."}, {"heading": "B. Orthogonal matching pursuit algorithm", "text": "The orthogonal matching pursuit (OMP) algorithm [36, 64] is an improvement of the MP algorithm. The OMP employs the process of orthogonalization to guarantee the orthogonal direction of projection in each iteration. It has been verified that the OMP algorithm can be converged in limited iterations [36]. The main steps of OMP algorithm have been summarized in Algorithm 1.\nAlgorithm 1. Orthogonal matching pursuit algorithm Task: Approximate the constraint problem: \u03b1\u0302 = argmin\u03b1 \u2016\u03b1\u20160 s.t. y = X\u03b1 Input: Probe sample y, measurement matrix X , sparse coefficients vector \u03b1 Initialization: t = 1, r0 = y, \u03b1 = 0, D0 = \u03c6, index set \u039b0 = \u03c6 where \u03c6 denotes empty set, \u03c4 is a small constant. While \u2016rt\u2016 > \u03c4 do\nStep 1: Find the best matching sample, i.e. the biggest inner product between rt\u22121 and xj (j 6\u2208 \u039bt\u22121) by exploiting \u03bbt = argmaxj 6\u2208\u039bt\u22121 |\u3008rt\u22121,xj\u3009|.\nStep 2: Update the index set \u039bt = \u039bt\u22121 \u22c3\n\u03bbt and reconstruct data set Dt = [Dt\u22121,x\u03bbt ].\nStep 3: Compute the sparse coefficient by using the least square algorithm \u03b1\u0303 = argmin \u2016y \u2212Dt\u03b1\u0303\u201622.\nStep 4: Update the representation residual using rt = y \u2212Dt\u03b1\u0303. Step 5: t = t + 1.\nEnd Output: D, \u03b1"}, {"heading": "C. Series of matching pursuit algorithms", "text": "It is an excellent choice to employ the greedy strategy to approximate the solution of sparse representation with the l0-norm minimization. These algorithms are typical greedy iterative algorithms. The earliest algorithms were the matching pursuit (MP) and orthogonal matching pursuit (OMP). The basic idea of the MP algorithm is to select the best matching atom from the overcomplete dictionary to construct sparse approximation during each iteration, to compute the signal representation residual, and then to choose the best matching atom till the stopping criterion of iteration is satisfied. Many more greedy algorithms based on the MP and OMP algorithm such as the efficient orthogonal matching pursuit algorithm [65] subsequently have been proposed to improve the pursuit algorithm. Needell et al. proposed an regularized version of orthogonal matching pursuit (ROMP) algorithm [37], which recovered all k sparse signals based on the Restricted Isometry Property of random frequency measurements, and then proposed another variant of OMP algorithm called compressive sampling matching pursuit (CoSaMP) algorithm [66], which incorporated several existing ideas such as restricted isometry property (RIP) and pruning technique into a greedy iterative structure of OMP. Some other algorithms also had an impressive influence on future research on CS. For example, Donoho et al. proposed an extension of OMP, called stage-wise orthogonal matching pursuit (StOMP) algorithm [67], which depicted an iterative algorithm with three main steps, i.e. threholding, selecting and projecting. Dai and Milenkovic proposed a new method for sparse signal reconstruction named subspace pursuit (SP) algorithm [68], which sampled signals satisfying the constraints of the RIP with a constant parameter. Do et al. presented a sparsity adaptive matching pursuit (SAMP) algorithm [69], which borrowed the idea of the EM algorithm to alternatively estimate the sparsity and support set. Jost et al. proposed a tree-based matching pursuit (TMP) algorithm [70], which constructed a tree structure and employed a structuring strategy to cluster similar signal atoms from a highly redundant dictionary as a new dictionary. Subsequently, La and Do proposed a new tree-based orthogonal matching pursuit (TBOMP) algorithm [71], which treated the sparse tree representation as an additional prior knowledge for linear inverse systems by using a small number of samples. Recently, Karahanoglu and Erdogan conceived a forward-backward pursuit (FBP) method [72] with two greedy stages, in which the forward stage enlarged the support estimation and the backward stage removed some unsatisfied atoms. More detailed treatments of the greedy pursuit for sparse representation can be found in the literature [23]."}, {"heading": "V. CONSTRAINED OPTIMIZATION STRATEGY", "text": "Constrained optimization strategy is always utilized to obtain the solution of sparse representation with the l1-norm regularization. The methods that address the non-differentiable unconstrained problem will be presented by reformulating it as a smooth differentiable constrained optimization problem. These methods exploit the constrained optimization method with efficient convergence to obtain the sparse solution. What\nis more, the constrained optimization strategy emphasizes the equivalent transformation of \u2016\u03b1\u20161 in problem III.12 and employs the new reformulated constrained problem to obtain a sparse representation solution. Some typical methods that employ the constrained optimization strategy to solve the original unconstrained non-smooth problem are introduced in this section."}, {"heading": "A. Gradient Projection Sparse Reconstruction", "text": "The core idea of the gradient projection sparse representation method is to find the sparse representation solution along with the gradient descent direction. The first key procedure of gradient projection sparse reconstruction (GPSR) [73] provides a constrained formulation where each value of \u03b1 can be split into its positive and negative parts. Vectors \u03b1+ and \u03b1\u2212 are introduced to denote the positive and negative coefficients of \u03b1, respectively. The sparse representation solution \u03b1 can be formulated as:\n\u03b1 = \u03b1+ \u2212\u03b1\u2212, \u03b1+ \u2265 0, \u03b1\u2212 \u2265 0 (V.1) where the operator (\u00b7)+ denotes the positive-part operator, which is defined as (x)+=max{0, x}. Thus, \u2016\u03b1\u20161 = 1Td\u03b1++ 1 T d\u03b1\u2212, where 1d = [1, 1, \u00b7 \u00b7 \u00b7 , 1\n\ufe38 \ufe37\ufe37 \ufe38\nd\n]T is a d\u2013dimensional vector\nwith d ones. Accordingly, problem III.12 can be reformulated as a constrained quadratic problem:\nargminL(\u03b1) = argmin 1\n2 \u2016y \u2212X [\u03b1+ \u2212\u03b1\u2212]\u201622+\n\u03bb(1Td\u03b1+ + 1 T d\u03b1\u2212) s.t. \u03b1+ \u2265 0, \u03b1\u2212 \u2265 0\n(V.2)\nor\nargminL(\u03b1) = argmin 1\n2 \u2016y \u2212 [X+, X\u2212][\u03b1+ \u2212\u03b1\u2212]\u201622\n+\u03bb(1Td \u03b1+ + 1 T d \u03b1\u2212) s.t. \u03b1+ \u2265 0, \u03b1\u2212 \u2265 0\n(V.3)\nFurthermore, problem V.3 can be rewritten as:\nargmin G(z) = cTz + 1\n2 zTAz s.t. z \u2265 0 (V.4)\nwhere z = [\u03b1+;\u03b1\u2212], c = \u03bb12d + [\u2212XTy;XTy], 12d = [1, \u00b7 \u00b7 \u00b7 , 1\n\ufe38 \ufe37\ufe37 \ufe38\n2d\n]T , A = ( XTX \u2212XTX \u2212XTX XTX ) .\nThe GPSR algorithm employs the gradient descent and standard line-search method [31] to address problem V.4. The value of z can be iteratively obtained by utilizing\nargmin zt+1 = zt \u2212 \u03c3\u2207G(zt) (V.5)\nwhere the gradient of \u2207G(zt) = c + Azt and \u03c3 is the step size of the iteration. For step size \u03c3, GPSR updates the step size by using\n\u03c3t = argmin \u03c3 G(zt \u2212 \u03c3gt) (V.6)\nwhere the function gt is pre-defined as\ngti = { (\u2207G(zt))i, if zti > 0 or (\u2207G(zt))i < 0 0, otherwise. (V.7)\nProblem V.6 can be addressed with the close-form solution\n\u03c3t = (gt)T (gt)\n(gt)TA(gt) (V.8)\nFurthermore, the basic GPSR algorithm employs the backtracking linear search method [31] to ensure that the step size of gradient descent, in each iteration, is a more proper value. The stop condition of the backtracking linear search should satisfy\nG((zt \u2212 \u03c3t\u2207G(zt))+) > G(zt)\u2212 \u03b2\u2207G(zt)T\n(zt \u2212 (zt \u2212 \u03c3t\u2207G(zt))+) (V.9)\nwhere \u03b2 is a small constant. The main steps of GPSR are summarized in Algorithm 2. For more detailed information, one can refer to the literature [73].\nAlgorithm 2. Gradient Projection Sparse Reconstruction (GPSR) Task: To address the unconstraint problem:\n\u03b1\u0302 = argmin\u03b1 1 2 \u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u20161\nInput: Probe sample y, the measurement matrix X , small constant \u03bb Initialization: t = 0, \u03b2 \u2208 (0, 0.5), \u03b3 \u2208 (0, 1), given \u03b1 so that z = [\u03b1+,\u03b1\u2212]. While not converged do\nStep 1: Compute \u03c3t exploiting Eq. V.8 and \u03c3t \u2190 mid(\u03c3min , \u03c3t, \u03c3max), where mid(\u00b7, \u00b7, \u00b7) denotes the middle value of the three parameters.\nStep 2: While Eq. V.9 not satisfied do \u03c3t \u2190 \u03b3\u03c3t end\nStep 3: zt+1 = (zt \u2212 \u03c3t\u2207G(zt))+ and t = t+ 1. End Output: zt+1, \u03b1\nB. Interior-point method based sparse representation strategy\nThe Interior-point method [31] is not an iterative algorithm but a smooth mathematic model and it always incorporates the Newton method to efficiently solve unconstrained smooth problems of modest size [28]. When the Newton method is used to address the optimization issue, a complex Newton equation should be solved iteratively which is very timeconsuming. A method named the truncated Newton method can effectively and efficiently obtain the solution of the Newton equation. A prominent algorithm called the truncated Newton based interior-point method (TNIPM) exists, which can be utilized to solve the large-scale l1-regularized least squares (i.e. l1 ls) problem [74].\nThe original problem of l1 ls is to solve problem III.12 and the core procedures of l1 ls are shown below: (1) Transform the original unconstrained non-smooth problem to a constrained smooth optimization problem. (2) Apply the interior-point method to reformulate the constrained smooth optimization problem as a new unconstrained smooth optimization problem. (3) Employ the truncated Newton method to solve this unconstrained smooth problem.\nThe main idea of the l1 ls will be briefly described. For simplicity of presentation, the following one-dimensional problem is used as an example.\n|\u03b1| = arg min \u2212\u03c3\u2264\u03b1\u2264\u03c3 \u03c3 (V.10)\nwhere \u03c3 is a proper positive constant.\nThus, problem III.12 can be rewritten as\n\u03b1\u0302 = argmin 12\u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 = argmin 12\u2016y \u2212X\u03b1\u201622 + \u03bb \u2211N i=1 min\u2212\u03c3i\u2264\u03b1i\u2264\u03c3i \u03c3i\n= argmin 12\u2016y \u2212X\u03b1\u201622 + \u03bbmin\u2212\u03c3i\u2264\u03b1i\u2264\u03c3i \u2211N i=1 \u03c3i = argmin\u2212\u03c3i\u2264\u03b1i\u2264\u03c3i 1 2\u2016y \u2212X\u03b1\u201622 + \u03bb \u2211N i=1 \u03c3i\n(V.11) Thus problem III.12 is also equivalent to solve the following\nproblem:\n\u03b1\u0302 = arg min \u03b1,\u03c3\u2208RN\n1 2 \u2016y\u2212X\u03b1\u201622+\u03bb\nN\u2211\ni=1 \u03c3i s.t. \u2212\u03c3i \u2264 \u03b1i \u2264 \u03c3i (V.12)\nor\n\u03b1\u0302 = arg min \u03b1,\u03c3\u2208RN\n1 2 \u2016y \u2212X\u03b1\u201622 + \u03bb\nN\u2211\ni=1\n\u03c3i\ns.t. \u03c3i + \u03b1i \u2265 0, \u03c3i \u2212\u03b1i \u2265 0 (V.13)\nThe interior-point strategy can be used to transform problem V.13 into an unconstrained smooth problem\n\u03b1\u0302 = arg min \u03b1,\u03c3\u2208RN\nG(\u03b1,\u03c3) = v\n2 \u2016y\u2212X\u03b1\u201622+\u03bbv\nN\u2211\ni=1\n\u03c3i\u2212B(\u03b1,\u03c3)\n(V.14) where B(\u03b1,\u03c3) =\n\u2211N i=1 log(\u03c3i+\u03b1i)+ \u2211N i=1 log(\u03c3i\u2212\u03b1i) is\na barrier function, which forces the algorithm to be performed within the feasible region in the context of unconstrained condition.\nSubsequently, l1 ls utilizes the truncated Newton method to solve problem V.14. The main procedures of addressing problem V.14 are presented as follows: First, the Newton system is constructed\nH [ \u25b3\u03b1 \u25b3\u03c3 ] = \u2212\u2207G(\u03b1,\u03c3) \u2208 R2N (V.15)\nwhere H = \u2212\u22072G(\u03b1,\u03c3) \u2208 R2N\u00d72N is the Hessian matrix, which is computed using the preconditioned conjugate gradient algorithm, and then the direction of linear search [\u25b3\u03b1,\u25b3\u03c3] is obtained. Second, the Lagrange dual of problem III.12 is used to construct the dual feasible point and duality gap: a) The Lagrangian function and Lagrange dual of problem III.12 are constructed. The Lagrangian function is reformulated as\nL(\u03b1, z,u) = zT z + \u03bb\u2016\u03b1\u20161 + u(X\u03b1\u2212 y \u2212 z) (V.16)\nwhere its corresponding Lagrange dual function is\n\u03b1\u0302 = argmaxF (u) = \u22121 4 uTu\u2212 uTy s.t.\n|(XTu)i| \u2264 \u03bbi (i = 1, 2, \u00b7 \u00b7 \u00b7 , N) (V.17)\nb) A dual feasible point is constructed\nu = 2s(y\u2212X\u03b1), s = min{\u03bb/|2yi\u22122(XTX\u03b1)i|}\u2200i (V.18)\nwhere u is a dual feasible point and s is the step size of the linear search.\nc) The duality gap is constructed, which is the gap between the primary problem and the dual problem:\ng = \u2016y \u2212X\u03b1\u2016+ \u03bb\u2016\u03b1\u20161 \u2212 F (u) (V.19)\nThird, the method of backtracking linear search is used to determine an optimal step size of the Newton linear search. The stopping condition of the backtracking linear search is\nG(\u03b1+\u03b7t\u25b3\u03b1,\u03c3+\u03b7t\u25b3\u03c3) > G(\u03b1,\u03c3)+\u03c1\u03b7t\u2207G(\u03b1,\u03c3)[\u25b3\u03b1,\u25b3\u03c3] (V.20) where \u03c1 \u2208 (0, 0.5) and \u03b7t \u2208 (0, 1) is the step size of the Newton linear search. Finally, the termination condition of the Newton linear search is set to\n\u03b6 = min{0.1, \u03b2g/\u2016h\u20162} (V.21)\nwhere the function h = \u2207G(\u03b1,\u03c3), \u03b2 is a small constant, and g is the duality gap. The main steps of algorithm l1 ls are summarized in Algorithm 3. For further description and analyses, please refer to the literature [74].\nAlgorithm 3. Truncated Newton based interior-point method (TNIPM) for l1 ls Task: To address the unconstraint problem:\n\u03b1\u0302 = argmin\u03b1 1 2 \u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u20161\nInput: Probe sample y, the measurement matrix X , small constant \u03bb Initialization: t = 1, v = 1\n\u03bb , \u03c1 \u2208 (0, 0.5), \u03c3 = 1N\nStep 1: Employ preconditioned conjugate gradient algorithm to obtain the approximation of H in Eq. V.15, and then obtain the descent direction of linear search [\u25b3\u03b1t,\u25b3\u03c3t].\nStep 2: Exploit the algorithm of backtracking linear search to find the optimal step size of Newton linear search \u03b7t , which satisfies the Eq. V.20.\nStep 3: Update the iteration point utilizing (\u03b1t+1,\u03c3t+1) = (\u03b1t,\u03c3t)+ (\u25b3\u03b1t +\u25b3\u03c3t).\nStep 4: Construct feasible point using eq. V.18 and duality gap in Eq. V.19, and compute the termination tolerance \u03b6 in Eq. V.21.\nStep 5: If the condition g/F (u) > \u03b6 is satisfied, stop; Otherwise, return to step 1, update v in Eq. V.14 and t = t+ 1. Output: \u03b1\nThe truncated Newton based interior-point method (TNIPM) [75] is a very effective method to solve the l1-norm regularization problems. Koh et al. [76] also utilized the TNIPM to solve large scale logistic regression problems, which employed a preconditioned conjugate gradient method to compute the search step size with warm-start techniques. Mehrotra proposed to exploit the interior-point method to address the primal-dual problem [77] and introduced the second-order derivation of Taylor polynomial to approximate a primal-dual trajectory. More analyses of interior-point method for sparse representation can be found in the literature [78]."}, {"heading": "C. Alternating direction method (ADM) based sparse representation strategy", "text": "This section shows how the ADM [43] is used to solve primal and dual problems in III.12. First, an auxiliary variable is introduced to convert problem in III.12 into a constrained problem with the form of problem V.22. Subsequently, the alternative direction method is used to efficiently address the sub-problems of problem V.22. By introducing the auxiliary\nterm s \u2208 Rd, problem III.12 is equivalent to a constrained problem\nargmin \u03b1,s\n1\n2\u03c4 \u2016s\u20162 + \u2016\u03b1\u20161 s.t. s = y \u2212X\u03b1 (V.22)\nThe optimization problem of the augmented Lagrangian function of problem V.22 is considered\narg min \u03b1,s,\u03bb\nL(\u03b1, s,\u03bb) = 1\n2\u03c4 \u2016s\u20162 + \u2016\u03b1\u20161 \u2212 \u03bbT (s+ X\u03b1\u2212 y) + \u00b5 2 \u2016s+X\u03b1\u2212 y\u201622 (V.23)\nwhere \u03bb \u2208 Rd is a Lagrange multiplier vector and \u00b5 is a penalty parameter. The general framework of ADM is used to solve problem V.23 as follows:\n \n st+1 = argminL(s,\u03b1t,\u03bbt) (a) \u03b1t+1 = argminL(st+1,\u03b1,\u03bbt) (b) \u03bbt+1 = \u03bbt \u2212 \u00b5(st+1 +X\u03b1t+1 \u2212 y) (c) (V.24)\nFirst, the first optimization problem V.24(a) is considered\nargminL(s,\u03b1t,\u03bbt) = 1\n2\u03c4 \u2016s\u20162 + \u2016\u03b1t\u20161 \u2212 (\u03bbt)T (s+X\u03b1t \u2212 y) + \u00b5 2 \u2016s+X\u03b1t \u2212 y\u201622\n= 1\n2\u03c4 \u2016s\u20162 \u2212 (\u03bbt)Ts+\n\u00b5 2 \u2016s+X\u03b1t \u2212 y\u201622+\n\u2016\u03b1t\u20161 \u2212 (\u03bbt)T (X\u03b1t \u2212 y) (V.25)\nThen, it is known that the solution of problem V.25 with respect to s is given by\nst+1 = \u03c4\n1 + \u00b5\u03c4 (\u03bbt \u2212 \u00b5(y \u2212X\u03b1t)) (V.26)\nSecond, the optimization problem V.24(b) is considered\nargminL(st+1,\u03b1, \u03bbt) = 1\n2\u03c4 \u2016st+1\u20162 + \u2016\u03b1\u20161 \u2212 (\u03bb)T (st+1\n+X\u03b1\u2212 y) + \u00b5 2 \u2016st+1 +X\u03b1\u2212 y\u201622\nwhich is equivalent to\nargmin{\u2016\u03b1\u20161 \u2212 (\u03bbt)T (st+1 +X\u03b1\u2212 y) + \u00b5\n2 \u2016st+1+\nX\u03b1\u2212 y\u201622} = \u2016\u03b1\u20161 + \u00b5\n2 \u2016st+1 +X\u03b1\u2212 y \u2212 \u03bbt/\u00b5\u201622\n= \u2016\u03b1\u20161 + f(\u03b1) (V.27)\nwhere f(\u03b1) = \u00b52 \u2016st+1+X\u03b1\u2212y\u2212\u03bbt/\u00b5\u201622. If the second order Taylor expansion is used to approximate f(\u03b1), the problem V.27 can be approximately reformulated as\nargmin{\u2016\u03b1\u20161 + (\u03b1\u2212\u03b1t)TXT (st+1 +X\u03b1t \u2212 y \u2212 \u03bbt/\u00b5)\n+ 1\n2\u03c4 \u2016\u03b1\u2212\u03b1t\u201622}\n(V.28)\nwhere \u03c4 is a proximal parameter. The solution of problem V.28 can be obtained by the soft thresholding operator\n\u03b1t+1 = soft{\u03b1t\u2212\u03c4XT (st+1+X\u03b1t\u2212y\u2212\u03bbt/\u00b5), \u03c4 \u00b5 } (V.29)\nwhere soft(\u03c3, \u03b7) = sign(\u03c3)max{|\u03c3| \u2212 \u03b7, 0}. Finally, the Lagrange multiplier vector \u03bb is updated by using Eq. V.24(c).\nThe algorithm presented above utilizes the second order Taylor expansion to approximately solve the sub-problem V.27 and thus the algorithm is denoted as an inexact ADM or approximate ADM. The main procedures of the inexact ADM based sparse representation method are summarized in Algorithm 4. More specifically, the inexact ADM described above is to reformulate the unconstrained problem as a constrained problem, and then utilizes the alternative strategy to effectively address the corresponding sub-optimization problem. Moreover, ADM can also efficiently solve the dual problems of the primal problems III.9-III.12. For more information, please refer to the literature [43, 79].\nAlgorithm 4. Alternating direction method (ADM) based sparse representation strategy Task: To address the unconstraint problem:\n\u03b1\u0302 = argmin\u03b1 1 2 \u2016y \u2212X\u03b1\u201622 + \u03c4\u2016\u03b1\u20161\nInput: Probe sample y, the measurement matrix X , small constant \u03bb Initialization: t = 0, s0 = 0, \u03b10 = 0, \u03bb0 = 0, \u03c4 = 1.01, \u00b5 is a small constant. Step 1: Construct the constraint optimization problem of problem III.12 by introducing the auxiliary parameter and its augmented Lagrangian function, i.e. problem (V.22) and (V.23). While not converged do\nStep 2: Update the value of the st+1 by using Eq. (V.25). Step 2: Update the value of the \u03b1t+1 by using Eq. (V.29). Step 3: Update the value of the \u03bbt+1 by using Eq. (V.24(c)). Step 4: \u00b5t+1 = \u03c4\u00b5t and t = t + 1.\nEnd While Output: \u03b1t+1"}, {"heading": "VI. PROXIMITY ALGORITHM BASED OPTIMIZATION STRATEGY", "text": "In this section, the methods that exploit the proximity algorithm to solve constrained convex optimization problems are discussed. The core idea of the proximity algorithm is to utilize the proximal operator to iteratively solve the sub-problem, which is much more computationally efficient than the original problem. The proximity algorithm is frequently employed to solve nonsmooth, constrained convex optimization problems [28]. Furthermore, the general problem of sparse representation with l1-norm regularization is a nonsmooth convex optimization problem, which can be effectively addressed by using the proximal algorithm.\nSuppose a simple constrained optimization problem is\nmin{h(x)|x \u2208 \u03c7} (VI.1) where \u03c7 \u2282 Rn. The general framework of addressing the constrained convex optimization problem VI.1 using the proximal algorithm can be reformulated as\nx\u0303t = argmin{h(x) + \u03c4 2 \u2016x\u2212 xt\u20162|x \u2208 \u03c7} (VI.2)\nwhere \u03c4 and xt are given. For definiteness and without loss of generality, it is assumed that there is the following linear constrained convex optimization problem\nargmin{F (x) +G(x)|x \u2208 \u03c7} (VI.3)\nThe solution of problem VI.3 obtained by employing the proximity algorithm is:\nxt+1 =argmin{F (x) + \u3008\u2207G(xt),x\u2212 xt\u3009+ 1 2\u03c4 \u2016x\u2212 xt\u20162}\n=argmin{F (x) + 1 2\u03c4\n\u2016x\u2212 \u03b8t\u20162} (VI.4)\nwhere \u03b8 = xt \u2212 \u03c4\u2207G(xt). More specifically, for the sparse representation problem with l1-norm regularization, the main problem can be reformulated as:\nminP (\u03b1) = {\u03bb\u2016\u03b1\u20161 | A\u03b1 = y} or minP (\u03b1) = {\u03bb\u2016\u03b1\u20161 + \u2016A\u03b1\u2212 y\u201622 | \u03b1 \u2208 Rn} (VI.5)\nwhich are considered as the constrained sparse representation of problem III.12."}, {"heading": "A. Soft thresholding or shrinkage operator", "text": "First, a simple form of problem III.12 is introduced, which has a closed-form solution, and it is formulated as:\n\u03b1\u2217 = min \u03b1 h(\u03b1) =\u03bb\u2016\u03b1\u20161 + 1 2 \u2016\u03b1\u2212 s\u20162\n= N\u2211\nj=1\n\u03bb|\u03b1j |+ N\u2211\nj=1\n1 2 (\u03b1j \u2212 sj)2\n(VI.6)\nwhere \u03b1\u2217 is the optimal solution of problem VI.6, and then there are the following conclusions: (1) if \u03b1j > 0, then h(\u03b1) = \u03bb\u03b1+ 12\u2016\u03b1\u2212s\u20162 and its derivative is h\u2032(\u03b1j) = \u03bb+\u03b1\u2217j \u2212 sj . Let h\u2032(\u03b1j) = 0 \u21d2 \u03b1\u2217j = sj \u2212 \u03bb, where it indicates sj > \u03bb; (2) if \u03b1j < 0, then h(\u03b1) = \u2212\u03bb\u03b1+ 12\u2016\u03b1\u2212s\u20162 and its derivative is h\u2032(\u03b1j) = \u2212\u03bb+\u03b1\u2217j \u2212 sj . Let h\u2032(\u03b1j) = 0 \u21d2 \u03b1\u2217j = sj + \u03bb, where it indicates sj < \u2212\u03bb; (3) if \u2212\u03bb \u2264 sj \u2264 \u03bb, and then \u03b1\u2217j = 0. So the solution of problem VI.6 is summarized as\n\u03b1\u2217j =\n \n sj \u2212 \u03bb, if sj > \u03bb sj + \u03bb, if sj < \u2212\u03bb 0, otherwise\n(VI.7)\nThe equivalent expression of the solution is \u03b1\u2217 = shrink(s, \u03bb), where the j-th component of shrink(s, \u03bb) is shrink(s, \u03bb)j = sign(sj)max{|sj | \u2212 \u03bb, 0}. The operator shrink(\u2022) can be regarded as a proximal operator.\nB. Iterative shrinkage thresholding algorithm (ISTA)\nThe objective function of ISTA [80] has the form of\nargminF (\u03b1) = 1\n2 \u2016X\u03b1\u2212 y\u201622 + \u03bb\u2016\u03b1\u20161 = f(\u03b1) + \u03bbg(\u03b1)\n(VI.8) and is usually difficult to solve. Problem VI.8 can be converted to the form of an easy problem VI.6 and the explicit procedures are presented as follows.\nFirst, Taylor expansion is used to approximate f(\u03b1) = 1 2\u2016X\u03b1 \u2212 y\u201622 at a point of \u03b1t. The second order Taylor expansion is\nf(\u03b1) = f(\u03b1t) + (\u03b1\u2212\u03b1t)T\u2207f(\u03b1t) + 1 2 (\u03b1\u2212 \u03b1t)T\nHf (\u03b1 t)(\u03b1\u2212\u03b1t) + \u00b7 \u00b7 \u00b7\n(VI.9)\nwhere Hf (\u03b1t) is the Hessian matrix of f(\u03b1) at \u03b1t. For the function f(\u03b1), \u2207f(\u03b1) = XT (X\u03b1\u2212 y) and Hf (\u03b1) = XTX can be obtained.\nf(\u03b1) = 1\n2 \u2016X\u03b1t \u2212 y\u201622 + (\u03b1\u2212\u03b1t)TXT (X\u03b1t \u2212 y)+\n1 2 (\u03b1\u2212\u03b1t)TXTX(\u03b1\u2212\u03b1t)\n(VI.10)\nIf the Hessian matrix Hf (\u03b1) is replaced or approximated in the third term above by using a scalar 1\u03c4 I , and then\nf(\u03b1) \u2248 1 2 \u2016X\u03b1t \u2212 y\u201622 + (\u03b1\u2212\u03b1t)TXT (X\u03b1t \u2212 y)\n+ 1\n2\u03c4 (\u03b1\u2212\u03b1t)T (\u03b1\u2212\u03b1t) = Qt(\u03b1,\u03b1t)\n(VI.11)\nThus problem VI.8 using the proximal algorithm can be successively addressed by\n\u03b1t+1 = argminQt(\u03b1,\u03b1 t) + \u03bb\u2016\u03b1\u20161 (VI.12)\nProblem VI.12 is reformulated to a simple form of problem VI.6 by\nQt(\u03b1,\u03b1 t) =\n1 2 \u2016X\u03b1t \u2212 y\u201622+(\u03b1\u2212\u03b1t)TXT (X\u03b1t \u2212 y)+\n1\n2\u03c4 \u2016\u03b1\u2212\u03b1t\u201622\n= 1\n2 \u2016X\u03b1t \u2212 y\u201622 +\n1\n2\u03c4 \u2016\u03b1\u2212\u03b1t + \u03c4XT (X\u03b1t \u2212 y)\u201622\n\u2212 \u03c4 2 \u2016XT (X\u03b1t \u2212 y)\u201622\n= 1\n2\u03c4 \u2016\u03b1\u2212 (\u03b1t \u2212 \u03c4XT (X\u03b1t \u2212 y))\u201622 +B(\u03b1t)\n(VI.13)\nwhere the term B(\u03b1t) = 12\u2016X\u03b1t\u2212y\u201622\u2212 \u03c42\u2016XT (X\u03b1t\u2212y)\u20162 in problem VI.12 is a constant with respect to variable \u03b1, and it can be omitted. As a result, problem VI.12 is equivalent to the following problem:\n\u03b1t+1 = argmin 1\n2\u03c4 \u2016\u03b1\u2212 \u03b8(\u03b1t)\u201622 + \u03bb\u2016\u03b1\u20161 (VI.14)\nwhere \u03b8(\u03b1t) = \u03b1t \u2212 \u03c4XT (X\u03b1t \u2212 y). The solution of the simple problem VI.6 is applied to solve problem VI.14 where the parameter t is replaced by the equation \u03b8(\u03b1t), and the solution of problem VI.14 is \u03b1t+1 = shrink(\u03b8(\u03b1t), \u03bb\u03c4). Thus, the solution of ISTA is reached. The techniques used here are called linearization or preconditioning and more detailed information can be found in the literature [80, 81]."}, {"heading": "C. Fast Iterative shrinkage thresholding algorithm (FISTA)", "text": "The fast iterative shrinkage thresholding algorithm (FISTA) is an improvement of ISTA. FISTA [82] not only preserves the efficiency of the original ISTA but also promotes the effectiveness of ISTA so that FISTA can obtain global convergence.\nConsidering that the Hessian matrix Hf (\u03b1) is approximated by using a scalar 1\u03c4 I for ISTA in Eq. VI.9, FISTA utilizes the minimum Lipschitz constant of the gradient \u2207f(\u03b1) to approximate the Hessian matrix of f(\u03b1), i.e. L(f) = 2\u03bbmax(XTX). Thus, the problem VI.8 can be converted to the problem below:\nf(\u03b1) \u2248 1 2 \u2016X\u03b1t \u2212 y\u201622 + (\u03b1\u2212\u03b1t)TXT (X\u03b1t \u2212 y)\n+ L\n2 (\u03b1\u2212\u03b1t)T (\u03b1\u2212\u03b1t) = Pt(\u03b1,\u03b1t)\n(VI.15)\nwhere the solution can be reformulated as\n\u03b1t+1 = argmin L\n2 \u2016\u03b1\u2212 \u03b8(\u03b1t)\u201622 + \u03bb\u2016\u03b1\u20161 (VI.16)\nwhere \u03b8(\u03b1t) = \u03b1t \u2212 1LXT (X\u03b1t \u2212 y). Moreover, to accelerate the convergence of the algorithm, FISTA also improves the sequence of iteration points, instead of employing the previous point it utilizes a specific linear combinations of the previous two points {\u03b1t,\u03b1t\u22121}, i.e.\n\u03b1t = \u03b1t + \u00b5t \u2212 1 \u00b5t+1 (\u03b1t \u2212\u03b1t\u22121) (VI.17)\nwhere \u00b5t is a positive sequence, which satisfies \u00b5t \u2265 (t+1)/2, and the main steps of FISTA are summarized in Algorithm 5. The backtracking linear research strategy can also be utilized to explore a more feasible value of L and more detailed analyses on FISTA can be found in the literature [82, 83].\nAlgorithm 5. Fast Iterative shrinkage thresholding algorithm (FISTA) Task: To address the problem \u03b1\u0302 = argminF (\u03b1) = 1\n2 \u2016X\u03b1 \u2212 y\u201622 +\n\u03bb\u2016\u03b1\u20161 Input: Probe sample y, the measurement matrix X , small constant \u03bb Initialization: t = 0, \u00b50 = 1, L = 2\u039bmax(XTX), i.e. Lipschitz constant of \u2207f . While not converged do\nStep 1: Exploit the shrinkage operator in equation VI.7 to solve problem VI.16. Step 2: Update the value of \u00b5 using \u00b5t+1 = 1+ \u221a 1+4(\u00b5t)2\n2 .\nStep 3: Update iteration sequence \u03b1t using equation VI.17. End Output: \u03b1"}, {"heading": "D. Sparse reconstruction by separable approximation (SpaRSA)", "text": "Sparse reconstruction by separable approximation (SpaRSA) [84] is another typical proximity algorithm based on sparse representation, which can be viewed as an accelerated version of ISTA. SpaRSA provides a general algorithmic framework for solving the sparse representation problem and here a simple specific SpaRSA with adaptive continuation on ISTA is introduced. The main contributions of SpaRSA are trying to optimize the parameter \u03bb in problem VI.8 by using the worm-starting technique, i.e. continuation, and choosing a\nmore reliable approximation of Hf (\u03b1) in problem VI.9 using the Barzilai-Borwein (BB) spectral method [85]. The wormstarting technique and BB spectral approach are introduced as follows. (1) Utilizing the worm-starting technique to optimize \u03bb\nThe values of \u03bb in the sparse representation methods discussed above are always set to be a specific small constant. However, Hale et al. [86] concluded that the technique that exploits a decreasing value of \u03bb from a warm-starting point can more efficiently solve the sub-problem VI.14 than ISTA that is a fixed point iteration scheme. SpaRSA uses an adaptive continuation technique to update the value of \u03bb so that it can lead to the fastest convergence. The procedure regenerates the value of \u03bb using\n\u03bb = max{\u03b3\u2016XTy\u2016\u221e, \u03bb} (VI.18)\nwhere \u03b3 is a small constant. (2) Utilizing the BB spectral method to approximate Hf (\u03b1)\nISTA employs 1\u03c4 I to approximate the matrix Hf (\u03b1), which is the Hessian matrix of f(\u03b1) in problem VI.9 and FISTA exploits the Lipschitz constant of \u2207f(\u03b1) to replace Hf (\u03b1). However, SpaRSA utilizes the BB spectral method to choose the value of \u03c4 to mimic the Hessian matrix. The value of \u03c4 is required to satisfy the condition:\n1\n\u03c4 t+1 (\u03b1t+1 \u2212\u03b1t) \u2248 \u2207f(\u03b1t+1)\u2212\u2207f(\u03b1t) (VI.19)\nwhich satisfies the minimization problem\n1 \u03c4 t+1 =argmin \u2016 1 \u03c4 (\u03b1t+1 \u2212\u03b1t)\u2212 (\u2207f(\u03b1t+1)\u2212\u2207f(\u03b1t))\u201622\n= (\u03b1t+1 \u2212\u03b1t)T (\u2207f(\u03b1t+1)\u2212\u2207f(\u03b1t))\n(\u03b1t+1 \u2212\u03b1t)T (\u03b1t+1 \u2212\u03b1t) (VI.20)\nFor problem VI.14, SpaRSA requires that the value of \u03bb is a decreasing sequence using the Eq. VI.18 and the value of \u03c4 should meet the condition of Eq. VI.20. The sparse reconstruction by separable approximation (SpaRSA) is summarized in Algorithm 6 and more information can be found in the literature [84].\nAlgorithm 6. Sparse reconstruction by separable approximation (SpaRSA) Task: To address the problem\n\u03b1\u0302 = argminF (\u03b1) = 1 2 \u2016X\u03b1\u2212 y\u201622 + \u03bb\u2016\u03b1\u20161\nInput: Probe sample y, the measurement matrix X , small constant \u03bb Initialization: t = 0, i = 0, y0 = y, 1\n\u03c40 I \u2248 Hf (\u03b1) = XTX , tolerance\n\u03b5 = 10\u22125. Step 1: \u03bbt = max{\u03b3\u2016XT yt\u2016\u221e, \u03bb}. Step 2: Exploit shrinkage operator to solve problem VI.14, i.e.\n\u03b1i+1 = shrink(\u03b1i \u2212 \u03c4 iXT (XT\u03b1t \u2212 y), \u03bbt\u03c4 i). Step 3: Update the value of 1\n\u03c4i+1 using the Eq. VI.20.\nStep 4: If \u2016\u03b1 i+1\u2212\u03b1i\u2016\n\u03b1i \u2264 \u03b5, go to step 5; Otherwise, return to step 2\nand i = i+ 1. Step 5: yt+1 = y \u2212X\u03b1t+1. Step 6: If \u03bbt = \u03bb, stop; Otherwise, return to step 1 and t = t+ 1.\nOutput: \u03b1i\nE. l1/2-norm regularization based sparse representation\nSparse representation with the lp-norm (0<p<1) regularization leads to a nonconvex, nonsmooth, and non-Lipschitz optimization problem and its general forms are described as problems III.13 and III.14. The lp-norm (0<p<1) regularization problem is always difficult to be efficiently addressed and it has also attracted wide interests from large numbers of research groups. However, the research group led by Zongben Xu summarizes the conclusion that the most impressive and representative algorithm of the lp-norm (0<p<1) regularization is sparse representation with the l1/2-norm regularization [87]. Moreover, they have proposed some effective methods to solve the l1/2norm regularization problem [60, 88].\nIn this section, a half proximal algorithm is introduced to solve the l1/2-norm regularization problem [60], which matches the iterative shrinkage thresholding algorithm for the l1-norm regularization discussed above and the iterative hard thresholding algorithm for the l0-norm regularization. Sparse representation with the l1/2-norm regularization is explicitly to solve the problem as follows:\n\u03b1\u0302 = argmin{F (\u03b1) = \u2016X\u03b1\u2212 y\u201622 + \u03bb\u2016\u03b1\u2016 1/2 1/2} (VI.21)\nwhere the first-order optimality condition of F (\u03b1) on \u03b1 can be formulated as\n\u2207F (\u03b1) = XT (X\u03b1\u2212 y) + \u03bb 2 \u2207(\u2016\u03b1\u20161/21/2) = 0 (VI.22)\nwhich admits the following equation:\nXT (y \u2212X\u03b1) = \u03bb 2 \u2207(\u2016\u03b1\u20161/21/2) (VI.23)\nwhere \u2207(\u2016\u03b1\u20161/21/2) denotes the gradient of the regularization term \u2016\u03b1\u20161/21/2. Subsequently, an equivalent transformation of Eq. VI.23 is made by multiplying a positive constant \u03c4 and adding a parameter \u03b1 to both sides. That is,\n\u03b1+ \u03c4XT (y \u2212X\u03b1) = \u03b1+ \u03c4 \u03bb 2 \u2207(\u2016\u03b1\u20161/21/2) (VI.24)\nTo this end, the resolvent operator [60] is introduced to compute the resolvent solution of the right part of Eq. VI.24, and the resolvent operator is defined as\nR\u03bb, 1 2 (\u2022) =\n(\nI + \u03bb\u03c4\n2 \u2207(\u2016 \u2022 \u20161/21/2)\n)\u22121 (VI.25)\nwhich is very similar to the inverse function of the right part of Eq. VI.24. The resolvent operator is always satisfied no matter whether the resolvent solution of \u2207(\u2016 \u2022 \u20161/21/2) exists or not [60]. Applying the resolvent operator to solve problem VI.24\n\u03b1 = (I + \u03bb\u03c4\n2 \u2207(\u2016 \u2022 \u20161/21/2)) \u22121(\u03b1+ \u03c4Xt(y \u2212X\u03b1)) = R\u03bb,1/2(\u03b1+ \u03c4X T (y \u2212X\u03b1)) (VI.26)\ncan be obtained which is well-defined. \u03b8(\u03b1) = \u03b1+ \u03c4XT (y\u2212 X\u03b1) is denoted and the resolvent operator can be explicitly expressed as:\nR\u03bb, 1 2 (x) = (f\u03bb, 1 2 (x1), f\u03bb, 1 2 (x2), \u00b7 \u00b7 \u00b7 , f\u03bb, 1 2 (xN )) T (VI.27)\nwhere\nf\u03bb, 1 2 (xi) =\n2 3 xi(1 + cos( 2\u03c0 3 \u2212 2 3 g\u03bb(xi)),\ng\u03bb(xi) = arg cos( \u03bb 8 ( |xi| 3 )\u2212 3 2 )\n(VI.28)\nwhich have been demonstrated in the literature [60]. Thus the half proximal thresholding function for the l1/2norm regularization is defined as below:\nh\u03bb\u03c4, 1 2 (xi) =\n{\nf\u03bb\u03c4, 1 2 (xi), if |xi| > 3 \u221a 54 4 (\u03bb\u03c4) 2 3\n0, otherwise (VI.29)\nwhere the threshold 3 \u221a 54 4 (\u03bb\u03c4) 2 3 has been conceived and demonstrated in the literature [60]. Therefore, if Eq. VI.29 is applied to Eq. VI.27, the half proximal thresholding function, instead of the resolvent operator, for the l1/2-norm regularization problem VI.25 can be explicitly reformulated as:\n\u03b1 = H\u03bb\u03c4, 1 2 (\u03b8(\u03b1)) (VI.30)\nwhere the half proximal thresholding operator H [60] is deductively constituted by Eq. VI.29.\nUp to now, the half proximal thresholding algorithm has been completely structured by Eq. VI.30. However, the options of the regularization parameter \u03bb in Eq. VI.24 can seriously dominate the quality of the representation solution in problem VI.21, and the values of \u03bb and \u03c4 can be specifically fixed by\n\u03c4 = 1\u2212 \u03b5 \u2016X\u20162 and \u03bb =\n\u221a 96\n9\u03c4 |[\u03b8(\u03b1)]k+1 | 3 2 (VI.31)\nwhere \u03b5 is a very small constant, which is very close to zero, the k denotes the limit of sparsity (i.e. k-sparsity), and [\u2022]k refers to the k-th largest component of [\u2022]. The half proximal thresholding algorithm for l1/2-norm regularization based sparse representation is summarized in Algorithm 7 and more detailed inferences and analyses can be found in the literature [60, 88].\nAlgorithm 7. The half proximal thresholding algorithm for l1/2-norm regularization Task: To address the problem\n\u03b1\u0302 = argminF (\u03b1) = \u2016X\u03b1\u2212 y\u201622 + \u03bb\u2016\u03b1\u2016 1/2 1/2\nInput: Probe sample y, the measurement matrix X Initialization: t = 0, \u03b5 = 0.01, \u03c4 = 1\u2212\u03b5\u2016X\u20162 . While not converged do\nStep 1: Compute \u03b8(\u03b1t) = \u03b1t + \u03c4XT (y \u2212X\u03b1t). Step 2: Compute \u03bbt = \u221a 96\n9\u03c4 |[\u03b8(\u03b1t)]k+1|\n3 2 in Eq. VI.31.\nStep 3: Apply the half proximal thresholding operator to obtain the representation solution \u03b1t+1 = H\u03bbt\u03c4, 12 (\u03b8(\u03b1t)).\nStep 4: t = t+ 1. End Output: \u03b1"}, {"heading": "F. Augmented Lagrange Multiplier based optimization strategy", "text": "The Lagrange multiplier is a widely used tool to eliminate the equality constrained problem and convert it to address the unconstrained problem with an appropriate penalty function.\nSpecifically, the sparse representation problem III.9 can be viewed as an equality constrained problem and the equivalent problem III.12 is an unconstrained problem, which augments the objective function of problem III.9 with a weighted constraint function. In this section, the augmented Lagrangian method (ALM) is introduced to solve the sparse representation problem III.9.\nFirst, the augmented Lagrangian function of problem III.9 is conceived by introducing an additional equality constrained function, which is enforced on the Lagrange function in problem III.12. That is,\nL(\u03b1, \u03bb) = \u2016\u03b1\u20161+ \u03bb\n2 \u2016y\u2212X\u03b1\u201622 s.t. y\u2212X\u03b1 = 0 (VI.32)\nThen, a new optimization problem VI.32 with the form of the Lagrangain function is reformulated as\nargminL\u03bb(\u03b1, z) = \u2016\u03b1\u20161 + \u03bb\n2 \u2016y \u2212X\u03b1\u201622 + zT (y \u2212X\u03b1)\n(VI.33) where z \u2208 Rd is called the Lagrange multiplier vector or dual variable and L\u03bb(\u03b1, z) is denoted as the augmented Lagrangian function of problem III.9. The optimization problem VI.33 is a joint optimization problem of the sparse representation coefficient \u03b1 and the Lagrange multiplier vector z. Problem VI.33 is solved by optimizing \u03b1 and z alternatively as follows:\n\u03b1t+1 = argminL\u03bb(\u03b1, z t)\n= argmin(\u2016\u03b1\u20161 + \u03bb\n2 \u2016y \u2212X\u03b1\u201622 + (zt)TX\u03b1)\n(VI.34)\nzt+1 = zt + \u03bb(y \u2212X\u03b1t+1) (VI.35)\nwhere problem VI.34 can be solved by exploiting the FISTA algorithm. Problem VI.34 is iteratively solved and the parameter z is updated using Eq. VI.35 until the termination condition is satisfied. Furthermore, if the method of employing ALM to solve problem VI.33 is denoted as the primal augmented Lagrangian method (PALM) [89], the dual function of problem III.9 can also be addressed by the ALM algorithm, which is denoted as the dual augmented Lagrangian method (DALM) [89]. Subsequently, the dual optimization problem III.9 is discussed and the ALM algorithm is utilized to solve it.\nFirst, consider the following equation:\n\u2016\u03b1\u20161 = max \u2016\u03b8\u2016\u221e\u22641 \u3008\u03b8,\u03b1\u3009 (VI.36)\nwhich can be rewritten as\n\u2016\u03b1\u20161 = max{\u3008\u03b8,\u03b1\u3009 \u2212 IB1 \u221e } or \u2016\u03b1\u20161 = sup{\u3008\u03b8,\u03b1\u3009 \u2212 IB1\n\u221e\n} (VI.37)\nwhere B\u03bbp = {x \u2208 RN | \u2016x\u2016p \u2264 \u03bb} and I\u2126(x) is a indicator function, which is defined as I\u2126(x) = { 0 ,x \u2208 \u2126 \u221e ,x 6\u2208 \u2126 .\nHence,\n\u2016\u03b1\u20161 = max{\u3008\u03b8,\u03b1\u3009 : \u03b8 \u2208 B1\u221e} (VI.38)\nSecond, consider the Lagrange dual problem of problem III.9 and its dual function is\ng(\u03bb) = inf \u03b1 {\u2016\u03b1\u20161+\u03bbT (y\u2212X\u03b1)} = \u03bbTy\u2212sup \u03b1 {\u03bbTX\u03b1\u2212\u2016\u03b1\u20161} (VI.39) where \u03bb \u2208 Rd is a Lagrangian multiplier. If the definition of conjugate function is applied to Eq. VI.37, it can be verified that the conjugate function of IB1\n\u221e (\u03b8) is \u2016\u03b1\u20161. Thus Eq. VI.39 can be equivalently reformulated as\ng(\u03bb) = \u03bbTy \u2212 IB1 \u221e (XT\u03bb) (VI.40)\nThe Lagrange dual problem, which is associated with the primal problem III.9, is an optimization problem:\nmax \u03bb\n\u03bbTy s.t. (XT\u03bb) \u2208 B1\u221e (VI.41)\nAccordingly,\nmin \u03bb,z\n\u2212\u03bbTy s.t. z \u2212XT\u03bb = 0, z \u2208 B1\u221e (VI.42)\nThen, the optimization problem VI.42 can be reconstructed as\narg min \u03bb,z,\u00b5\nL(\u03bb, z,\u00b5) = \u2212\u03bbTy \u2212 \u00b5T (z \u2212XT\u03bb)\n+ \u03c4\n2 \u2016z \u2212XT\u03bb\u201622 s.t. z \u2208 B1\u221e\n(VI.43)\nwhere \u00b5 \u2208 Rd is a Lagrangian multiplier and \u03c4 is a penalty parameter.\nFinally, the dual optimization problem VI.43 is solved and a similar alternating minimization idea of PALM can also be applied to problem VI.43, that is,\nzt+1 = arg min z\u2208B1\n\u221e\nL\u03c4 (\u03bb t, z,\u00b5t)\n= arg min z\u2208B1\n\u221e\n{\u2212\u00b5T (z \u2212XT\u03bbt) + \u03c4 2 \u2016z \u2212XT\u03bbt\u201622}\n= arg min z\u2208B1\n\u221e\n{\u03c4 2 \u2016z \u2212 (XT\u03bbt + 2 \u03c4 \u00b5T )\u201622}\n= PB1 \u221e\n(XT\u03bbt + 1\n\u03c4 \u00b5T )\n(VI.44)\nwhere PB1 \u221e (u) is a projection, or called a proximal operator, onto B1\u221e and it is also called group-wise soft-thresholding. For example, let x = PB1\n\u221e (u), then the i-th component of solution x satisfies xi = sign(ui)min{|ui|, 1} \u03bbt+1 = argmin\n\u03bb L\u03c4 (\u03bb, z\nt+1,\u00b5t)\n= argmin \u03bb {\u2212\u03bbTy + (\u00b5t)TXT\u03bb+ \u03c4 2 \u2016zt+1 \u2212XT\u03bb\u201622}\n= Q(\u03bb) (VI.45)\nTake the derivative of Q(\u03bb) with respect to \u03bb and obtain\n\u03bbt+1 = (\u03c4XXT )\u22121(\u03c4Xzt+1 + y \u2212X\u00b5t) (VI.46)\n\u00b5t+1 = \u00b5t \u2212 \u03c4(zt+1 \u2212XT\u03bbt+1) (VI.47)\nThe DALM for sparse representation with l1-norm regularization mainly exploits the augmented Lagrange method to address the dual optimization problem of problem III.9\nand a proximal operator, the projection operator, is utilized to efficiently solve the subproblem. The algorithm of DALM is summarized in Algorithm 8. For more detailed description, please refer to the literature [89].\nAlgorithm 8. Dual augmented Lagrangian method for l1-norm regularization Task: To address the dual problem of \u03b1\u0302 = argmin\u03b1 \u2016\u03b1\u20161 s.t. y = X\u03b1\nInput: Probe sample y, the measurement matrix X , a small constant \u03bb0. Initialization: t = 0, \u03b5 = 0.01, \u03c4 = 1\u2212\u03b5\u2016X\u20162 , \u00b5 0 = 0. While not converged do Step 1: Apply the projection operator to compute\nzt+1 = PB1 \u221e (XT\u03bbt + 1 \u03c4 \u00b5T ).\nStep 2: Update the value of \u03bbt+1 = (\u03c4XXT )\u22121(\u03c4Xzt+1+y\u2212X\u00b5t). Step 3: Update the value of \u00b5t+1 = \u00b5t \u2212 \u03c4(zt+1 \u2212XT\u03bbt+1). Step 4: t = t + 1.\nEnd While Output: \u03b1 = \u00b5[1 : N ]"}, {"heading": "G. Other proximity algorithm based optimization methods", "text": "The theoretical basis of the proximity algorithm is to first construct a proximal operator, and then utilize the proximal operator to solve the convex optimization problem. Massive proximity algorithms have followed up with improved techniques to improve the effectiveness and efficiency of proximity algorithm based optimization methods. For example, Elad et al. proposed an iterative method named parallel coordinate descent algorithm (PCDA) [90] by introducing the elementwise optimization algorithm to solve the regularized linear least squares with non-quadratic regularization problem.\nInspired by belief propagation in graphical models, Donoho et al. developed a modified version of the iterative thresholding method, called approximate message passing (AMP) method [91], to satisfy the requirement that the sparsity undersampling tradeoff of the new algorithm is equivalent to the corresponding convex optimization approach. Based on the development of the first-order method called Nesterov\u2019s smoothing framework in convex optimization, Becker et al. proposed a generalized Nesterov\u2019s algorithm (NESTA) [92] by employing the continuation-like scheme to accelerate the efficiency and flexibility. Subsequently, Becker et al. [93] further constructed a general framework, i.e. templates for convex cone solvers (TFOCS), for solving massive certain types of compressed sensing reconstruction problems by employing the optimal first-order method to solve the smoothed dual problem of the equivalent conic formulation of the original optimization problem. Further detailed analyses and inference information related to proximity algorithms can be found in the literature [28, 83]."}, {"heading": "VII. HOMOTOPY ALGORITHM BASED SPARSE REPRESENTATION", "text": "The concept of homotopy derives from topology and the homotopy technique is mainly applied to address a nonlinear system of equations problem. The homotopy method was originally proposed to solve the least square problem with the l1-penalty [94]. The main idea of homotopy is to solve\nthe original optimization problems by tracing a continuous parameterized path of solutions along with varying parameters. Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems. In contrast to LAR and OMP, the homotopy method is more favorable for sequentially updating the sparse solution by adding or removing elements from the active set. Some representative methods that exploit the homotopy-based strategy to solve the sparse representation problem with the l1-norm regularization are explicitly presented in the following parts of this section."}, {"heading": "A. LASSO homotopy", "text": "Because of the significance of parameters in l1-norm minimization, the well-known LASSO homotopy algorithm is proposed to solve the LASSO problem in III.9 by tracing the whole homotopy solution path in a range of decreasing values of parameter \u03bb. It is demonstrated that problem III.12 with an appropriate parameter value is equivalent to problem III.9 [29]. Moreover, it is apparent that as we change \u03bb from a very large value to zero, the solution of problem III.12 is converging to the solution of problem III.9 [29]. The set of varying value \u03bb conceives the solution path and any point on the solution path is the optimality condition of problem III.12. More specifically, the LASSO homotopy algorithm starts at an large initial value of parameter \u03bb and terminates at a point of \u03bb, which approximates zero, along the homotopy solution path so that the optimal solution converges to the solution of problem III.9. The fundamental of the homotopy algorithm is that the homotopy solution path is a piecewise linear path with a discrete number of operations while the value of the homotopy parameter changes, and the direction of each segment and the step size are absolutely determined by the sign sequence and the support of the solution on the corresponding segment, respectively [96].\nBased on the basic ideas in a convex optimization problem, it is a necessary condition that the zero vector should be a solution of the subgradient of the objective function of problem III.12. Thus, we can obtain the subgradiential of the objective function with respect to \u03b1 for any given value of \u03bb, that is,\n\u2202L \u2202\u03b1 = \u2212XT (y \u2212X\u03b1) + \u03bb\u2202\u2016\u03b1\u20161 (VII.1)\nwhere the first term r = XT (y \u2212X\u03b1) is called the vector of residual correlations, and \u2202\u2016\u03b1\u20161 is the subgradient obtained by\n\u2202\u2016\u03b1\u20161 = { \u03b8 \u2208 RN \u2223 \u2223 \u2223 \u2223 \u03b8i = sgn(\u03b1i), \u03b1i 6= 0 \u03b8i \u2208 [\u22121, 1], \u03b1i = 0 }\nLet \u039b and u denote the support of \u03b1 and the sign sequence of \u03b1 on its support \u039b, respectively. X\u039b denotes that the indices of all the samples in X\u039b are all included in the support set \u039b. If we analyze the KKT optimality condition for problem III.12, we can obtain the following two equivalent conditions of problem VII.1, i.e.\nX\u039b(y \u2212X\u03b1) = \u03bbu; \u2016XT\u039bc(y \u2212X\u03b1)\u2016\u221e \u2264 \u03bb (VII.2)\nwhere \u039bc denotes the complementary set of the set \u039b. Thus, the optimality conditions in VII.2 can be divided into N constraints and the homotopy algorithm maintains both of the conditions along the optimal homotopy solution path for any \u03bb \u2265 0. As we decrease the value of \u03bb to \u03bb \u2212 \u03c4 , for a small value of \u03c4 , the following conditions should be satisfied\nXT\u039b (y \u2212X\u03b1) + \u03c4XT\u039bX\u03b4 = (\u03bb\u2212 \u03c4)u (a) \u2016p+ \u03c4q\u2016\u221e \u2264 \u03bb\u2212 \u03c4 (b) (VII.3)\nwhere p = XT (y\u2212X\u03b1), q = XTX\u03b4 and \u03b4 is the update direction.\nGenerally, the homotopy algorithm is implemented iteratively and it follows the homotopy solution path by updating the support set by decreasing parameter \u03bb from a large value to the desired value. The support set of the solution will be updated and changed only at a critical point of \u03bb, where either an existing nonzero element shrinks to zero or a new nonzero element will be added into the support set. The two most important parameters are the step size \u03c4 and the update direction \u03b4. At the l-th stage (if (XT\u039bX\u039b)\n\u22121 exists), the homotopy algorithm first calculates the update direction, which can be obtained by solving\nXT\u039bX\u039b\u03b4l = u (VII.4)\nThus, the solution of problem VII.4 can be written as\n\u03b4l =\n{ (XT\u039bX\u039b) \u22121u, on \u039b 0, otherwise\n(VII.5)\nSubsequently, the homotopy algorithm computes the step size \u03c4 to the next critical point by tracing the homotopy solution path. i.e. the homotopy algorithm moves along the update direction until one of constraints in VII.3 is not satisfied. At this critical point, a new nonzero element must enter the support \u039b, or one of the nonzero elements in \u03b1 will be shrink to zero, i.e. this element must be removed from the support set \u039b. Two typical cases may lead to a new critical point, where either condition of VII.3 is violated. The minimum step size which leads to a critical point can be easily obtained by computing \u03c4\u2217l = min(\u03c4 + l , \u03c4 \u2212 l ), and \u03c4 + l and \u03c4 \u2212 l are computed by\n\u03c4+l = mini\u2208\u039bc\n( \u03bb\u2212 pi\n1\u2212 xTi X\u039b\u03b4l , \u03bb+ pi 1 + xTi X\u039b\u03b4l\n)\n+\n(VII.6)\n\u03c4\u2212l = mini\u2208\u039b (\u2212\u03b1il \u03b4il ) +\n(VII.7)\nwhere pi = xTi (y \u2212 xi\u03b1il) and min(\u00b7)+ denotes that the minimum is operated over only positive arguments. \u03c4+l is the minimum step size that turns an inactive element at the index i+ in to an active element, i.e. the index i+ should be added into the support set. \u03c4\u2212l is the minimum step size that shrinks the value of a nonzero active element to zero at the index i\u2212 and the index i\u2212 should be removed from the support set. The solution is updated by \u03b1l+1 = \u03b1l + \u03c4\u2217l \u03b4, and its support and sign sequence are renewed correspondingly.\nThe homotopy algorithm iteratively computes the step size and the update direction, and updates the homotopy solution and its corresponding support and sign sequence till the\ncondition \u2016p\u2016\u221e = 0 is satisfied so that the solution of problem III.9 is reached. The principal steps of the LASSO homotopy algorithm have been summarized in Algorithm 9. For further description and analyses, please refer to the literature [29, 96].\nAlgorithm 9. Lasso homotopy algorithm Task: To addrss the Lasso problem: \u03b1\u0302 = argmin\u03b1 \u2016y \u2212X\u03b1\u201622 s.t. \u2016\u03b1\u20161 \u2264 \u03b5 Input: Probe sample y, measurement matrix X . Initialization: l = 1, initial solution \u03b1l and its support set \u039bl. Repeat:\nStep 1: Compute update direction \u03b4l by using Eq. (VII.5). Step 2: Compute \u03c4+l and \u03c4 \u2212 l by using Eq. (VII.6) and Eq. (VII.7). Step 3: Compute the optimal minimum step size \u03c4\u2217l by using \u03c4\u2217l = min{\u03c4 + l , \u03c4 \u2212 l }. Step 4: Update the solution \u03b1l+1 by using \u03b1l+1 = \u03b1l + \u03c4\u2217l \u03b4l. Step 5: Update the support set:\nIf \u03c4+l == \u03c4 \u2212 l then\nRemove the i\u2212 from the support set, i.e. \u039bl+1 = \u039bl\\i\u2212. else\nAdd the i+ into the support set, i.e. \u039bl+1 = \u039bl \u22c3 i+\nEnd if Step 6: l = l+ 1.\nUntil \u2016XT (y \u2212X\u03b1)\u2016\u221e = 0 Output: \u03b1l+1"}, {"heading": "B. BPDN homotopy", "text": "Problem III.11, which is called basis pursuit denoising (BPDN) in signal processing, is the unconstrained Lagrangian function of the LASSO problem III.9, which is an unconstrained problem. The BPDN homotopy algorithm is very similar to the LASSO homotopy algorithm. If we consider the KKT optimality condition for problem III.12, the following condition should be satisfied for the solution \u03b1\n\u2016XT (y \u2212X\u03b1)\u2016\u221e \u2264 \u03bb (VII.8)\nAs for any given value of \u03bb and the support set \u039b, the following two conditions also need to be satisfied\nXT\u039b (y \u2212X\u03b1) = \u03bbu; \u2016XT\u039bc(y \u2212X\u03b1)\u2016\u221e \u2264 \u03bb (VII.9)\nThe BPDN homotopy algorithm directly computes the homotopy solution by\n\u03b1 =\n{ (XT\u039bX\u039b) \u22121(XT\u039by \u2212 \u03bbu), on \u039b 0, otherwise\n(VII.10)\nwhich is somewhat similar to the soft-thresholding operator. The value of the homotopy parameter \u03bb is initialized with a large value, which satisfies \u03bb0 > \u2016XTy\u2016\u221e. As the value of the homotopy parameter \u03bb decreases, the BPDN homotopy algorithm traces the solution in the direction of (XT\u039bX\u039b)\n\u22121u till the critical point is obtained. Each critical point is reached when either an inactive element is transferred into an active element, i.e. its corresponding index should be added into the support set, or an nonzero active element value in \u03b1 shrinks to zero, i.e. its corresponding index should be removed from the support set. Thus, at each critical point, only one element is updated, i.e. one element being either removed from or added into the active set, and each operation is very\ncomputationally efficient. The algorithm is terminated when the value of the homotopy parameter is lower than its desired value. The BPDN homotopy algorithm has been summarized in Algorithm 10. For further detail description and analyses, please refer to the literature [42].\nAlgorithm 10. BPDN homotopy algorithm Task: To address the Lasso problem: \u03b1\u0302 = argmin\u03b1 \u2016y \u2212X\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 Input: Probe sample y, measurement matrix X . Initialization: l = 0, initial solution \u03b10 and its support set \u039b0, a large value \u03bb0, step size \u03c4 , tolerance \u03b5. Repeat:\nStep 1: Compute update direction \u03b4l+1 by using \u03b4l+1 = (X T \u039bX\u039b)\n\u22121ul . Step 2: Update the solution \u03b1l+1 by using Eq. (VII.10). Step 3: Update the support set and the sign sequence set. Step 6: \u03bbl+1 = \u03bbl \u2212 \u03c4 , l = l+ 1.\nUntil \u03bb \u2264 \u03b5 Output: \u03b1l+1\nC. Iterative Reweighting l1-norm minimization via homotopy\nBased on the homotopy algorithm, Asif and Romberg [96] presented a enhanced sparse representation objective function, a weighted l1-norm minimization, and then provided two fast and accurate solutions, i.e. the iterative reweighting algorithm, which updated the weights with a new ones, and the adaptive reweighting algorithm, which adaptively selected the weights in each iteration. Here the iterative reweighting algorithm via homotopy is introduced. The objective function of the weighted l1-norm minimization is formulated as\nargmin 1\n2 \u2016X\u03b1\u2212 y\u201622 + \u2016W\u03b1\u20161 (VII.11)\nwhere W = diag[w1, w2, \u00b7 \u00b7 \u00b7 , wN ] is the weight of the l1-norm and also is a diagonal matrix. For more explicit description, problem VII.11 can be rewritten as\nargmin 1\n2 \u2016X\u03b1\u2212 y\u201622 +\nN\u2211\ni=1\nwi|\u03b1i| (VII.12)\nA common method [42, 73] to update the weight W is achieved by exploiting the solution of problem VII.12, i.e. \u03b1, at the previous iteration, and for the i-th element of the weight wi is updated by\nwi = \u03bb\n|\u03b1i|+ \u03c3 (VII.13)\nwhere parameters \u03bb and \u03c3 are both small constants. In order to efficiently update the solution of problem (7-9), the homotopy algorithm introduces a new weight of the l1-norm and a new homotopy based reweighting minimization problem is reformulated as\nargmin 1\n2 \u2016X\u03b1\u2212y\u201622+\nN\u2211\ni=1\n((1\u2212\u03c3)w\u0302i +\u03c3w\u0302i)|\u03b1i| (VII.14)\nwhere w\u0302i denotes the new obtained weight by the homotopy algorithm, parameter \u03c4 is denoted as the homotopy parameter varying from 0 to 1. Apparently, problem VII.14 can be evolved to problem VII.12 with the increasing value of the\nhomotopy parameter by tracing the homotopy solution path. Similar to the LASSO homotopy algorithm, problem VII.14 is also piecewise linear along the homotopy path, and for any value of \u03c3, the following conditions should be satisfied\nxTi (X\u03b1\u2212 y) = \u2212((1\u2212 \u03c3)wi + \u03c3w\u0302i)ui for i \u2208 \u039b (a) |xTi (X\u03b1\u2212 y)| < (1 \u2212 \u03c3)wi + \u03c3w\u0302i for i \u2208 \u039bc (b)\n(VII.15) where xi is the i-th column of the measurement X , wi and w\u0302i are the given weight and new obtained weight, respectively. Moreover, for the optimal step size \u03c3, when the homotopy parameter changes from \u03c3 to \u03c3+ \u03c4 in the update direction \u03b4, the following optimality conditions also should be satisfied\nXT\u039b (X\u03b1\u2212 y) + \u03c4XT\u039bX\u03b4 = \u2212((1\u2212 \u03c3)W + \u03c3W\u0302 )u + \u03c4(W \u2212 W\u0302 )u (a) |p\u2212 \u03c4q| \u2264 r + \u03c4s (b)\n(VII.16)\nwhere u is the sign sequence of \u03b1 on its support \u039b, pi = xTi (X\u03b1 \u2212 y), qi = xTi X\u03b4, ri = (1 \u2212 \u03c3)wi + \u03c3w\u0302i and si = w\u0302i \u2212 wi. Thus, at the l-th stage (if (XTi Xi)\u22121 exists), the update direction of the homotopy algorithm can be computed by\n\u03b4l =\n{\n(XT\u039bX\u039b) \u22121(W \u2212 W\u0302 )u, on \u039b 0, otherwise (VII.17)\nThe step size which can lead to a critical point can be computed by \u03c4\u2217l = min(\u03c4 + l , \u03c4 \u2212 l ), and \u03c4 + l and \u03c4 \u2212 l are computed by\n\u03c4+l = mini\u2208\u039bc ( ri \u2212 pi qi \u2212 si , \u2212ri \u2212 pi qi + si )\n+\n(VII.18)\n\u03c4\u2212l = mini\u2208\u039b (\u2212\u03b1il \u03b4il ) +\n(VII.19)\nwhere \u03c4+l is the minimum step size so that the index i + should be added into the support set and \u03c4\u2212l is the minimum step size that shrinks the value of a nonzero active element to zero at the index i\u2212. The solution and homotopy parameter are updated by \u03b1l+1 = \u03b1l + \u03c4\u2217l \u03b4, and \u03c3l+1 = \u03c3l + \u03c4 \u2217 l , respectively. The homotopy algorithm updates its support set and sign sequence accordingly until the new critical point of the homotopy parameter \u03c3l+1 = 1. The main steps of this algorithm are summarized in Algorithm 11 and more information can be found in literature [96]."}, {"heading": "D. Other homotopy algorithms for sparse representation", "text": "The general principle of the homotopy method is to reach the optimal solution along with the homotopy solution path by evolving the homotopy parameter from a known initial value to the final expected value. There are extensive hotomopy algorithms, which are related to the sparse representation with the l1-norm regularization. Malioutov et al. first exploited the homotopy method to choose a suitable parameter for l1-norm regularization with a noisy term in an underdetermined system and employed the homotopy continuation-based method to solve BPDN for sparse signal processing [97]. Garrigues and Ghaoui [98] proposed a modified homotopy algorithm to solve the Lasso problem with online observations by optimizing the\nAlgorithm 11. Iterative reweighting homotopy algorithm for weighted l1norm minimization Task: To addrss the weighted l1-norm minimization:\n\u03b1\u0302 = argmin 1 2 \u2016X\u03b1\u2212 y\u201622 +W\u2016\u03b1\u20161\nInput: Probe sample y, measurement matrix X . Initialization: l = 1, initial solution \u03b1l and its support set \u039bl, \u03c31 = 0. Repeat:\nStep 1: Compute update direction \u03b4l by using Eq. (VII.17). Step 2: Compute p, q, r and s by using Eq. (VII.16). Step 2: Compute \u03c4+l and \u03c4 \u2212 l by using Eq. (VII.18) and Eq. (VII.19). Step 3: Compute the step size \u03c4\u2217l by using \u03c4\u2217l = min{\u03c4 + l , \u03c4 \u2212 l }. Step 4: Update the solution \u03b1l+1 by using \u03b1l+1 = \u03b1l + \u03c4\u2217l \u03b4l. Step 5: Update the support set:\nIf \u03c4+l == \u03c4 \u2212 l then\nShrink the value to zero at the index i\u2212 and remove i\u2212, i.e. \u039bl+1 = \u039bl\\i\u2212 .\nelse Add the i+ into the support set, i.e. \u039bl+1 = \u039bl \u22c3 i+\nEnd if Step 6: \u03c3l+1 = \u03c3l + \u03c4l and l = l + 1.\nUntil \u03c3l+1 = 1 Output: \u03b1l+1\nhomotopy parameter from the current solution to the solution after obtaining the next new data point. Efron et al. [42] proposed a basic pursuit denoising (BPDN) homotopy algorithm, which shrinked the parameter to a final value with series of efficient optimization steps. Similar to BPDN homotopy, Asif [99] presented a homotopy algorithm for the Dantzing selector (DS) under the consideration of primal and dual solution. Asif and Romberg [100] proposed a framework of dynamic updating solutions for solving l1-norm minimization programs based on homotopy algorithm and demonstrated its effectiveness in addressing the decoding issue. More recent literature related to homotopy algorithms can be found in the streaming recovery framework [101] and a summary [102]."}, {"heading": "VIII. THE APPLICATIONS OF THE SPARSE REPRESENTATION METHOD", "text": "Sparse representation technique has been successfully implemented to numerous applications, especially in the fields of computer vision, image processing, pattern recognition and machine learning. More specifically, sparse representation has also been successfully applied to extensive real-world applications, such as image denoising, deblurring, inpainting, super-resolution, restoration, quality assessment, classification, segmentation, signal processing, object tracking, texture classification, image retrieval, bioinformatics, biometrics and other artificial intelligence systems. Moreover, dictionary learning is one of the most typical representative examples of sparse representation for realizing the sparse representation of a signal. In this paper, we only concentrate on the three applications of sparse representation, i.e. sparse representation in dictionary learning, image processing, image classification and visual tracking."}, {"heading": "A. Sparse representation in dictionary learning", "text": "The history of modeling dictionary could be traced back to 1960s, such as the fast Fourier transform (FFT) [103]. An\nover-complete dictionary that can lead to sparse representation is usually achieved by exploiting pre-specified set of transformation functions, i.e. transform domain method [5], or is devised based on learning, i.e. dictionary learning methods [104]. Both of the transform domain and dictionary learning based methods transform image samples into other domains and the similarity of transformation coefficients are exploited [105]. The difference between them is that the transform domain methods usually utilize a group of fixed transformation functions to represent the image samples, whereas the dictionary learning methods apply sparse representations on a over-complete dictionary with redundant information. Moreover, exploiting the pre-specified transform matrix in transform domain methods is attractive because of its fast and simplicity. Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111]. However, the dictionary learning methods exploiting sparse representation have the potential capabilities of outperforming the pre-determined dictionaries based on transformation functions. Thus, in this subsection we only focus on the modern over-complete dictionary learning methods.\nAn effective dictionary can lead to excellent reconstruction results and satisfactory applications, and the choice of dictionary is also significant to the success of sparse representation technique. Different tasks have different dictionary learning rules. For example, image classification requires that the dictionary contains discriminative information such that the solution of sparse representation possesses the capability of distinctiveness. The purpose of dictionary learning is motivated from sparse representation and aims to learn a faithful and effective dictionary to largely approximate or simulate the specific data. In this section, some parameters are defined as matrix Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,yN ], matrix X = [x1,x2, \u00b7 \u00b7 \u00b7 ,xt]T , and dictionary D = [d1,d2, \u00b7 \u00b7 \u00b7 ,dM ].\nFrom the notations of the literature [22, 112], the framework of dictionary learning can be generally formulated as an optimization problem\narg min D\u2208\u2126,xi\n{\n1\nN\nN\u2211\ni=1\n( 1\n2 \u2016yi \u2212Dxi\u201622 + \u03bbP (xi))\n}\n(VIII.1)\nwhere \u2126 = {D = [d1,d2, \u00b7 \u00b7 \u00b7 ,dM ] : dTi di = 1, i = 1, 2, \u00b7 \u00b7 \u00b7 ,M} (M here may not be equal to N ), N denotes the number of the known data set (eg. training samples in image classification), yi is the i-th sample vector from a known set, D is the learned dictionary and xi is the sparsity vector. P (xi) and \u03bb are the penalty or regularization term and a tuning parameter, respectively. The regularization term of problem VIII.1 controls the degree of sparsity. That is, different kinds of the regularization terms can immensely dominate the dictionary learning results.\nOne spontaneous idea of defining the penalty term P (xi) is to introduce the l0-norm regularization, which leads to the sparsest solution of problem VIII.1. As a result, the theory of sparse representation can be applied to dictionary\nlearning. The most representative dictionary learning based on the l0-norm penalty is the K-SVD algorithm [8], which is widely used in image denoising. Because the solution of l0-norm regularization is usually a NP-hard problem, utilizing a convex relaxation strategy to replace l0-norm regularization is an advisable choice for dictionary learning. As a convex relaxation method of l0-norm regularization, the l1-norm regularization based dictionary learning has been proposed in large numbers of dictionary learning schemes. In the stage of convex relaxation methods, there are three optimal forms for updating a dictionary: the one by one atom updating method, group atoms updating method, and all atoms updating method [112]. Furthermore, because of over-penalization in l1norm regularization, non-convex relaxation strategies also have been employed to address dictionary learning problems. For example, Fan and Li proposed a smoothly clipped absolution deviation (SCAD) penalty [113], which employed an iterative approximate Newton-Raphson method for penalizing least sequences and exploited the penalized likelihood approaches for variable selection in linear regression models. Zhang introduced and studied the non-convex minimax concave (MC) family [114] of non-convex piecewise quadratic penalties to make unbiased variable selection for the estimation of regression coefficients, which was demonstrated its effectiveness by employing an oracle inequality. Friedman proposed to use the logarithmic penalty for a model selection [115] and used it to solve the minimization problems with non-convex regularization terms. From the viewpoint of updating strategy, most of the dictionary learning methods always iteratively update the sparse approximation or representation solution and the dictionary alternatively, and more dictionary learning theoretical results and analyses can be found in the literature [116, 117].\nRecently, varieties of dictionary learning methods have been proposed and researchers have attempted to exploit different strategies for implementing dictionary learning tasks based on sparse representation. There are several means to categorize these dictionary learning algorithms into various groups. For example, dictionary learning methods can be divided into three groups in the context of different norms utilized in the penalty term, that is, l0-norm regularization based methods, convex relaxation methods and non-convex relaxation methods [118]. Moreover, dictionary learning algorithms can also be divided into three other categories in the presence of different structures. The first category is dictionary learning under the probabilistic framework such as maximum likelihood methods [119], the method of optimal directions (MOD) [120], and the maximum a posteriori probability method [121]. The second category is clustering based dictionary learning approaches such as KSVD [122], which can be viewed as a generalization of K-means. The third category is dictionary learning with certain structures, which are grouped into two significative aspects, i.e. directly modeling the relationship between each atom and structuring the corrections between each atom with purposive sparsity penalty functions. There are two typical models for these kinds of dictionary learning algorithms, sparse and shift-invariant representation of dictionary learning and structure sparse regularization based dictionary learning,\nsuch as hierarchical sparse dictionary learning [123] and group or block sparse dictionary learning [124]. Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].\nAlthough there are extensive strategies to divide the available sparse representation based dictionary learning methods into different categories, the strategy used here is to categorize the current prevailing dictionary learning approaches into two main classes: supervised dictionary learning and unsupervised dictionary learning, and then specific representative algorithms are explicitly introduced.\n1) Unsupervised dictionary learning: From the viewpoint of theoretical basis, the main difference of unsupervised and supervised dictionary learning relies on whether the class label is exploited in the process of learning for obtaining the dictionary. Unsupervised dictionary learning methods have been widely implemented to solve image processing problems, such as image compression, and feature coding of image representation [129, 130]. (1) KSVD for unsupervised dictionary learning\nOne of the most representative unsupervised dictionary learning algorithms is the KSVD method [122], which is a modification or an extension of method of directions (MOD) algorithm. The objective function of KSVD is\nargmin D,X {\u2016Y \u2212DX\u20162F} s.t. \u2016xi\u20160 \u2264 k, i = 1, 2, \u00b7 \u00b7 \u00b7 , N (VIII.2) where Y \u2208 Rd\u00d7N is the matrix composed of all the known examples, D \u2208 Rd\u00d7N is the learned dictionary, X \u2208 RN\u00d7N is the matrix of coefficients, k is the limit of sparsity and xi denotes the i-th row vector of the matrix X . Problem VIII.2 is a joint optimization problem with respect to D and X , and the natural method is to alternatively optimize the D and X iteratively.\nAlgorithm 12. The K-SVD algorithm for dictionary learning Task: Learning a dictionary D: argminD,X \u2016Y \u2212DX\u20162F s.t. \u2016xi\u20160 \u2264 k, i = 1, 2, \u00b7 \u00b7 \u00b7 , N Input: The matrix composed of given samples Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,ym]. Initialization: Set the initial dictionary D to the l2\u2013norm unit matrix, i = 1. While not converged do\nStep 1: For each given example yi, employing the classical sparse representation with l0-norm regularization to solve problem VIII.3 for further estimating Xi,set l = 1.\nWhile l is not equal to k do Step 2: Compute the overall representation residual\nEl = Y \u2212 \u2211 j 6=l djx T j .\nStep 3: Extract the column items of El which corresponds to the nonzero elements of xTl and obtain E P l . Step 4: SVD decomposes EPl into E P l = U\u039bV\nT . Step 5: Update dl to the first column of U and update\ncorresponding coefficients in xTl by \u039b(1, 1) times the first column of V .\nStep 6: l = l+ 1. End While\nStep 7: i = i+ 1. End While Output: dictionary D\nMore specifically, when fixing dictionary D, problem VIII.2\nis converted to\nargmin X \u2016Y \u2212DX\u20162F s.t. \u2016xi\u20160 \u2264 k, i = 1, 2, \u00b7 \u00b7 \u00b7 , N (VIII.3) which is called sparse coding and k is the limit of sparsity. Then, its subproblem is considered as follows:\nargmin xi\n\u2016yi \u2212Dxi\u201622 s.t. \u2016xi\u20160 \u2264 k, i = 1, 2, \u00b7 \u00b7 \u00b7 , N\nwhere we can iteratively resort to the classical sparse representation with l0 -norm regularization such as MP and OMP, for estimating xi.\nWhen fixing X , problem VIII.3 becomes a simple regression model for obtaining D, that is\nD\u0302 = argmin D\n\u2016Y \u2212DX\u20162F (VIII.4)\nwhere D\u0302 = Y X\u2020 = Y XT (XXT )\u22121 and the method is called MOD. Considering that the computational complexity of the inverse problem in solving problem VIII.4 is O(n3), it is favorable, for further improvement, to update dictionary D by fixing the other variables. The strategy of the KSVD algorithm rewrites the problem VIII.4 into\nD\u0302 =argmin D \u2016Y \u2212DX\u20162F = argmin D\n\u2016Y \u2212 N\u2211\nj=1\ndjx T j \u20162F\n=argmin D\n\u2016(Y \u2212 \u2211\nj 6=l djx\nT j )\u2212 dlxTl \u20162F\n(VIII.5)\nwhere xj is the j-th row vector of the matrix X . First the overall representation residual El = Y \u2212 \u2211 j 6=l djx T j is computed, and then dl and xl are updated. In order to maintain the sparsity of xTl in this step, only the nonzero elements of xTl should be preserved and only the nonzero items of El should be reserved, i.e. EPl , from dlx T l . Then, SVD decomposes EPl into E P l = U\u039bV\nT , and then updates dictionary dl. The specific KSVD algorithm for dictionary learning is summarized to Algorithm 12 and more information can be found in the literature [122]. (2) Locality constrained linear coding for unsupervised dictionary learning\nThe locality constrained linear coding (LLC) algorithm [130] is an efficient local coordinate linear coding method, which projects each descriptor into a local constraint system to obtain an effective codebook or dictionary. It has been demonstrated that the property of locality is more essential than sparsity, because the locality must lead to sparsity but not vice-versa, that is, a necessary condition of sparsity is locality, but not the reverse [130].\nAssume that Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,yN ] \u2208 Rd\u00d7N is a matrix composed of local descriptors extracted from examples and the objective dictionary D = [d1,d2, \u00b7 \u00b7 \u00b7 ,dN ] \u2208 Rd\u00d7N . The objective function of LLC is formulated as\nargmin xi,D\nN\u2211\ni=1\n\u2016yi \u2212Dxi\u201622 + \u00b5\u2016b\u2299 xi\u201622\ns.t. 1Txi = 1, i = 1, 2, \u00b7 \u00b7 \u00b7 , N (VIII.6)\nwhere \u00b5 is a small constant as a regularization parameter for adjusting the weighting decay speed, \u2299 is the operator of the element-wise multiplication, xi is the code for yi, 1 \u2208 RN\u00d71 is defined as a vector with all elements as 1 and vector b is the locality adaptor, which is, more specifically, set as\nb = exp\n( dist(yi, D)\n\u03c3\n)\n(VIII.7)\nwhere dist(yi, D) = [dist(yi,d1), \u00b7 \u00b7 \u00b7 , dist(yi,dN)] and dist(yi,dj) denotes the distance between yi and dj with different distance metrics, such as Euclidean distance and Chebyshev distance. Specifically, the i-th value of vector b is defined as bi = exp ( dist(yi,di)\n\u03c3\n)\n. The K-Means clustering algorithm is applied to generate the codebook D, and then the solution of LLC can be deduced as:\nx\u0302i = (Ci + \u00b5 diag 2(b))\\1 (VIII.8)\nxi = x\u0302i/ 1 T x\u0302i (VIII.9)\nwhere the operator a\\b denotes a\u22121b, and Ci = (DT \u2212 1yTi )(D\nT \u2212 1yTi )T is the covariance matrix with respect to yi. This is called the LLC algorithm. Furthermore, the incremental codebook optimization algorithm has also been proposed to obtain a more effective and optimal codebook, and the objective function is reformulated as\nargmin xi,D\nN\u2211\ni=1\n\u2016yi \u2212Dxi\u201622 + \u00b5\u2016b\u2299 xi\u201622\ns.t. 1Txi = 1, \u2200i; \u2016dj\u201622 \u2264 1, \u2200j (VIII.10)\nActually, the problem VIII.10 is a process of feature extraction and the property of \u2018locality\u2019 is achieved by constructing a local coordinate system by exploiting the local bases for each descriptor, and the local bases in the algorithm are simply obtained by using the K nearest neighbors of yi. The incremental codebook optimization algorithm in problem VIII.10 is a joint optimization problem with respect to D and xi, and it can be solved by iteratively optimizing one when fixing the other alternatively. The main steps of the incremental codebook optimization algorithm are summarized in Algorithm 13 and more information can be found in the literature [130]. (3) Other unsupervised dictionary learning methods\nA large number of different unsupervised dictionary learning methods have been proposed. The KSVD algorithm and LLC algorithm are only two typical unsupervised dictionary learning algorithms based on sparse representation. Additionally, Jenatton et al. [123] proposed a tree-structured dictionary learning problem, which exploited tree-structured sparse regularization to model the relationship between each atom and defined a proximal operator to solve the primaldual problem. Zhou et al. [131] developed a nonparametric Bayesian dictionary learning algorithm, which utilized hierarchical Bayesian to model parameters and employed the truncated beta-Bernoulli process to learn the dictionary. Ramirez and Shapiro [132] employed minimum description length to\nAlgorithm 13. The incremental codebook optimization algorithm Task: Learning a dictionary D: argminxi,D \u2211N i=1 \u2016yi\u2212Dxi\u201622+\u00b5\u2016b\u2299 xi\u201622 s.t. 1Txi = 1, \u2200i; \u2016dj\u201622 \u2264 1, \u2200j Input: The matrix composed of given samples Y = [y1,y2, \u00b7 \u00b7 \u00b7 ,yN ]. Initialization: i = 1, \u03b5 = 0.01, D initialized by K-Means clustering algorithm. While i is not equal to N do\nStep 1: Initialize b with 1\u00d7N zero vector. Step 2: Update locality constraint parameter b with\nbj = exp ( \u2212 dist(yi,dj) \u03c3 )\nfor \u2200j. Step 3: Normalize b using the equation b = b\u2212bmin\nbmax\u2212bmin . Step 4: Exploit the LLC coding algorithm to obtain xi. Step 5: Keep the set of Di, whose corresponding entries of the code xi\nare greater than \u03b5, and drop out other elements, i.e. index \u2190 {j | abs{xi(j)} > \u03b5} \u2200j and Di \u2190 D(:, index).\nStep 6: Update xi exploiting argmax \u2016yi\u2212Dixi\u201622 s.t. 1Txi = 1. Step 7: Update dictionary D using a classical gradient descent method\nwith respect to problem VIII.6. Step 8: i = i+ 1.\nEnd While Output: dictionary D\nmodel an effective framework of sparse representation and dictionary learning, and this framework could conveniently incorporate prior information into the process of sparse representation and dictionary learning. Some other unsupervised dictionary learning algorithms also have been validated. Mairal et al. proposed an online dictionary learning [133] algorithm based on stochastic approximations, which treated the dictionary learning problem as the optimization of a smooth convex problem over a convex set and employed an iterative online algorithm at each step to solve the subproblems. Yang and Zhang proposed a sparse variation dictionary learning (SVDL) algorithm [134] for face recognition with a single training sample, in which a joint learning framework of adaptive projection and a sparse variation dictionary with sparse bases were simultaneously constructed from the gallery image set to the generic image set. Shi et al. proposed a minimax concave penalty based sparse dictionary learning (MCPSDL) [112] algorithm, which employed a non-convex relaxation online scheme, i.e. a minimax concave penalty, instead of using regular convex relaxation approaches as approximation of l0norm penalty in sparse representation problem, and designed a coordinate descend algorithm to optimize it. Bao et al. proposed a dictionary learning by proximal algorithm (DLPM) [118], which provided an efficient alternating proximal algorithm for solving the l0-norm minimization based dictionary learning problem and demonstrated its global convergence property.\n2) Supervised dictionary learning: Unsupervised dictionary learning just considers that the examples can be sparsely represented by the learned dictionary and leaves out the label information of the examples. Thus, unsupervised dictionary learning can perform very well in data reconstruction, such as image denoising and image compressing, but is not beneficial to perform classification. On the contrary, supervised dictionary learning embeds the class label into the process of sparse representation and dictionary learning so that this leads to the learned dictionary with discriminative information for effective classification.\n(1) Discriminative KSVD for dictionary learning Discriminative KSVD (DKSVD) [127] was designed to solve image classification problems. Considering the priorities of supervised learning theory in classification, DKSVD incorporates the dictionary learning with discriminative information and classifier parameters into the objective function and employs the KSVD algorithm to obtain the global optimal solution for all parameters. The objective function of the DKSVD algorithm is formulated as\n\u3008D,C,X\u3009 = arg min D,C,X \u2016Y \u2212DX\u20162F + \u00b5\u2016H \u2212 CX\u20162F +\u03b7\u2016C\u20162F s.t. \u2016xi\u20160 \u2264 k\n(VIII.11)\nwhere Y is the given input samples, D is the learned dictionary, X is the coefficient term, H is the matrix composed of label information corresponding to Y , C is the parameter term for classifier, and \u03b7 and \u00b5 are the weights. With a view to the framework of KSVD, problem VIII.11 can be rewritten as\n\u3008D,C,X\u3009 = arg min D,C,X\n\u2016 (\nY\u221a \u00b5H\n) \u2212 (\nD\u221a \u00b5C\n)\nX\u20162F +\u03b7\u2016C\u20162F s.t. \u2016xi\u20160 \u2264 k\n(VIII.12)\nIn consideration of the KSVD algorithm, each column of the dictionary will be normalized to l2-norm unit vector and(\nD\u221a \u00b5C\n)\nwill also be normalized, and then the penalty\nterm \u2016C\u20162F will be dropped out and problem VIII.12 will be reformulated as\n\u3008Z,X\u3009 = argmin Z,X \u2016W \u2212ZX\u20162F s.t. \u2016xi\u20160 \u2264 k (VIII.13)\nwhere W = ( Y\u221a \u00b5H ) , Z = ( D\u221a \u00b5C ) and apparently the formulation VIII.13 is the same as the framework of KSVD [122] in Eq. VIII.2 and it can be efficiently solved by the KSVD algorithm.\nMore specifically, the DKSVD algorithm contains two main phases: the training phase and classification phase. For the training phase, Y is the matrix composed of the training samples and the objective is to learn a discriminative dictionary D and the classifier parameter C. DKSVD updates Z column by column and for each column vector zi, DKSVD employs the KSVD algorithm to obtain zi and its corresponding weight. Then, the DKSVD algorithm normalizes the dictionary D and classifier parameter C by\nD\u2032 = [d\u20321,d \u2032 2, \u00b7 \u00b7 \u00b7 ,d\u2032M ] = [ d1\u2016d1\u2016 , d2 \u2016d2\u2016 , \u00b7 \u00b7 \u00b7 , dM \u2016dM\u2016 ] C\u2032 = [c\u20321, c \u2032 2, \u00b7 \u00b7 \u00b7 , c\u2032M ] = [ c1\u2016d1\u2016 , c2 \u2016d2\u2016 , \u00b7 \u00b7 \u00b7 , cM \u2016dM\u2016 ] x\u2032i = xi \u00d7 \u2016di\u2016 (VIII.14) For the classification phase, Y is the matrix composed of the test samples. Based on the obtained learning results D\u2032 and C\u2032, the sparse coefficient matrix x\u0302i can be obtained for\neach test sample yi by exploiting the OMP algorithm, which is to solve\nx\u0302i = argmin \u2016yi \u2212D\u2032x\u2032i\u201622 s.t. \u2016x\u2032i\u20160 \u2264 k (VIII.15)\nOn the basis of the corresponding sparse coefficient x\u0302i, the final classification, for each test sample yi, can be performed by judging the label result by multiplying x\u0302i by classifier C\u2032, that is,\nlabel = C\u2032 \u00d7 x\u0302i (VIII.16)\nwhere the label is the final predicted label vector. The class label of yi is the determined class index of label.\nThe main highlight of DKSVD is that it employs the framework of KSVD to simultaneously learn a discriminative dictionary and classifier parameter, and then utilizes the efficient OMP algorithm to obtain a sparse representation solution and finally integrate the sparse solution and learned classifier for ultimate effective classification. (2) Label consistent KSVD for discriminative dictionary learning\nBecause of the classification term, a competent dictionary can lead to effectively classification results. The original sparse representation for face recognition [20] regards the raw data as the dictionary, and then reports its promising classification results. In this section, a label consistent KSVD (LC-KSVD) [135, 136] is introduced to learn an effective discriminative dictionary for image classification. As an extension of DKSVD, LC-KSVD exploits the supervised information to learn the dictionary and integrates the process of constructing the dictionary and optimal linear classifier into a mixed reconstructive and discriminative objective function, and then jointly obtains the learned dictionary and an effective classifier. The objective function of LC-KSVD is formulated as\n\u3008D,A,C,X\u3009 = arg min D,A,C,X \u2016Y \u2212DX\u20162F + \u00b5\u2016L\u2212AX\u20162F +\u03b7\u2016H \u2212 CX\u20162F s.t. \u2016xi\u20160 \u2264 k\n(VIII.17)\nwhere the first term denotes the reconstruction error, the second term denotes the discriminative sparse-code error, and the final term denotes the classification error. Y is the matrix composed of all the input data, D is the learned dictionary, X is the sparse code term, \u00b5 and \u03b7 are the weights of the corresponding contribution items, A is a linear transformation matrix, H is the matrix composed of label information corresponding to Y , C is the parameter term for classifier and L is a joint label matrix for labels of Y and D. For example, providing that Y = [y1 . . . y4] and D = [d1 . . . d4] where y1, y2, d1 and d2 are from the first class, and y3, y4, d3 and d4 are from the second class, and then the joint label matrix\nL can be defined as L =\n\n   1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1\n\n   . Similar to the\nDKSVD algorithm, the objective function VIII.17 can also be reformulated as\n\u3008Z,X\u3009 = argmin Z,X \u2016T \u2212 ZX\u201622 s.t. \u2016xi\u20160 \u2264 k (VIII.18)\nwhere T =\n\n Y\u221a \u00b5L\u221a \u03b7H\n\n, Z =\n\n D\u221a \u00b5A\u221a \u03b7C\n\n.\nThe learning process of the LC-KSVD algorithm, as is DKSVD, can be separated into two sections, the training term and the classification term. In the training section, since problem VIII.18 completely satisfies the framework of KSVD, the KSVD algorithm is applied to update Z atom by atom and compute X . Thus Z and X can be obtained. Then, the LCKSVD algorithm normalizes dictionary D, transform matrix A, and the classifier parameter C by\nD\u2032 = [d\u20321,d \u2032 2, \u00b7 \u00b7 \u00b7 ,d\u2032M ] = [ d1\u2016d1\u2016 , d2 \u2016d2\u2016 , \u00b7 \u00b7 \u00b7 , dM \u2016dM\u2016 ] A\u2032 = [a\u20321,a \u2032 2, \u00b7 \u00b7 \u00b7 ,a\u2032M ] = [ a1\u2016d1\u2016 , a2 \u2016d2\u2016 , \u00b7 \u00b7 \u00b7 , aM \u2016dM\u2016 ] C\u2032 = [c\u20321, c \u2032 2, \u00b7 \u00b7 \u00b7 , c\u2032M ] = [ c1\u2016d1\u2016 , c2 \u2016d2\u2016 , \u00b7 \u00b7 \u00b7 , cM \u2016dM\u2016 ]\n(VIII.19) In the classification section, Y is the matrix composed of the test samples. On the basis of the obtained dictionary D\u2032, the sparse coefficient x\u0302i can be obtained for each test sample yi by exploiting the OMP algorithm, which is to solve\nx\u0302i = argmin \u2016yi \u2212D\u2032x\u2032i\u201622 s.t. \u2016x\u2032i\u20160 \u2264 k (VIII.20) The final classification is based on a simple linear predictive\nfunction\nl = argmax f\n{f = C\u2032 \u00d7 x\u0302i} (VIII.21)\nwhere f is the final predicting label vector and the test sample yi is classified as a member of the l-th class.\nThe main contribution of LC-KSVD is to jointly incorporate the discriminative sparse coding term and classifier parameter term into the objective function for learning a discriminative dictionary and classifier parameter. The LC-KSVD demonstrates that the obtained solution, compared to other methods, can prevent learning a suboptimal or local optimal solution in the process of learning a dictionary [135]. (3) Fisher discrimination dictionary learning for sparse representation\nFisher discrimination dictionary learning (FDDL) [137] incorporates the supervised information (class label information) and the Fisher discrimination message into the objective function for learning a structured discriminative dictionary, which is used for pattern classification. The general model of FDDL is formulated as\nJ(D,X) = argmin D,X {f(Y,D,X) + \u00b5\u2016X\u20161 + \u03b7g(X)} (VIII.22) where Y is the matrix composed of input data, D is the learned dictionary, X is the sparse solution, and \u00b5 and \u03b7 are two constants for tradeoff contributions. The first component is the discriminative fidelity term, the second component is the sparse regularization term, and the third component is the discriminative coefficient term, such as Fisher discrimination criterion in Eq. (VIII.23).\nConsidering the importance of the supervised information, i.e. label information, in classification, FDDL respectively updates the dictionary and computes the sparse representation solution class by class. Assume that Y i denotes the matrix of i-th class of input data, vector X i denotes the sparse representation coefficient of the learned dictionary D over Y i and X ij denotes the matrix composed of the sparse representation solutions, which correspond to the j-th class coefficients from X i. Di is denoted as the learned dictionary corresponding to the i-th class. Thus, the objective function of FDDL is\nJ(D,X) = argmin D,X (\nc\u2211\ni=1\nf(Y i, D,X i) + \u00b5\u2016X\u20161+\n\u03b7(tr(SW (X)\u2212 SB(X)) + \u03bb\u2016X\u20162F )) (VIII.23)\nwhere f(Y i, D,X i) = \u2016Y i \u2212 DX i\u20162F + \u2016Y i \u2212 DiX ii\u20162F +\u2211 j 6=i \u2016DjX ij\u20162F and SW (X) and SB(X) are the within-class scatter of X and between-class scatter of X , respectively. c is the number of the classes. To solve problem VIII.23, a natural idea of optimization is to alternatively optimize D and X class by class, and then the process of optimization is briefly introduced.\nWhen fixing D, problem VIII.23 can be solved by computing X i class by class, and its sub-problem is formulated as\nJ(X i) = argmin Xi\n( f(Y i, D,X i) + \u00b5\u2016X i\u20161 + \u03b7g(X i) )\n(VIII.24) where g(X i) = \u2016X i\u2212Mi\u20162F \u2212 \u2211c t=1 \u2016Mt\u2212M\u20162F +\u03bb\u2016X i\u20162F and Mj and M denote the mean matrices corresponding to the j-th class of X i and X i, respectively. Problem VIII.23 can be solved by the iterative projection method in the literature [138].\nWhen fixing \u03b1, problem VIII.23 can be rewritten as\nJ(D) = argmin D\n(\u2016Y i \u2212DiX i \u2212 \u2211\nj 6=i DjXj\u20162F+\n\u2016Y i \u2212DiX ii\u20162F + \u2211\nj 6=i \u2016DiX ij\u20162F )\n(VIII.25)\nwhere X i here denotes the sparse representation of Y over Di. In this section, each column of the learned dictionary is normalized to a unit vector with l2-norm. The optimization of problem VIII.25 computes the dictionary class by class and it can be solved by exploiting the algorithm in the literature [139].\nThe main contribution of the FDDL algorithm lies in combining the Fisher discrimination criterion into the process of dictionary learning. The discriminative power comes from the method of constructing the discriminative dictionary using the function f in problem VIII.22 and simultaneously formulates the discriminative sparse representation coefficients by exploiting the function g in problem VIII.22. (4) Other supervised dictionary learning for sparse representation\nUnlike unsupervised dictionary learning, supervised dictionary learning emphasizes the significance of the class label\ninformation and incorporates it into the learning process to enforce the discrimination of the learned dictionary. Recently, massive supervised dictionary learning algorithms have been proposed. For example, Yang et al. [139] presented a metaface dictionary learning method, which is motivated by \u2018metagenes\u2019 in gene expression data analysis. Rodriguez and Sapiro [140] produced a discriminative non-parametric dictionary learning (DNDL) framework based on the OMP algorithm for image classification. Kong et al. [141] introduced a learned dictionary with commonalty and particularity, called DL-COPAR, which integrated an incoherence penalty term into the objective function for obtaining the class-specific sub-dictionary. Gao et al. [142] learned a hybrid dictionary, i.e. category-specific dictionary and shared dictionary, which incorporated a crossdictionary incoherence penalty and self-dictionary incoherence penalty into the objective function for learning a discriminative dictionary. Jafari and Plumbley [143] presented a greedy adaptive dictionary learning method, which updated the learned dictionary with a minimum sparsity index. Some other supervised dictionary learning methods are also competent in image classification, such as supervised dictionary learning in [144]. Zhou et al. [145] developed a joint dictionary learning algorithm for object categorization, which jointly learned a commonly shared dictionary and multiply category-specific dictionaries for correlated object classes and incorporated the Fisher discriminant fidelity term into the process of dictionary learning. Ramirez et al. proposed a method of dictionary learning with structured incoherence (DLSI) [140], which unified the dictionary learning and sparse decomposition into a sparse dictionary learning framework for image classification and data clustering. Ma et al. presented a discriminative lowrank dictionary learning for sparse representation (DLRD SR) [146], in which the sparsity and the low-rank properties were integrated into one dictionary learning scheme where subdictionary with discriminative power was required to be lowrank. Lu et al. developed a simultaneous feature and dictionary learning [147] method for face recognition, which jointly learned the feature projection matrix for subspace learning and the discriminative structured dictionary. Yang et al. introduced a latent dictionary learning (LDL) [148] method for sparse representation based image classification, which simultaneously learned a discriminative dictionary and a latent representation model based on the correlations between label information and dictionary atoms. Jiang et al. presented a submodular dictionary learning (SDL) [149] method, which integrated the entropy rate of a random walk on a graph and a discriminative term into a unified objective function and devised a greedybased approach to optimize it. Si et al. developed a support vector guided dictionary learning (SVGDL) [150] method, which constructed a discriminative term by using adaptively weighted summation of the squared distances for all pairwise of the sparse representation solutions."}, {"heading": "B. Sparse representation in image processing", "text": "Recently, sparse representation methods have been extensively applied to numerous real-world applications [151, 152]. The techniques of sparse representation have been gradually ex-\ntended and introduced to image processing, such as superresolution image processing, image denoising and image restoration.\nFirst, the general framework of image processing using sparse representation especially for image reconstruction should be introduced:\nStep 1: Partition the degraded image into overlapped patches or blocks.\nStep 2: Construct a dictionary, denoted as D, and assume that the following sparse representation formulation should be satisfied for each patch or block x of the image: \u03b1\u0302 = argmin \u2016\u03b1\u2016p s.t. \u2016x\u2212HD\u03b1\u201622 \u2264 \u03b5 where H is a degradation matrix and 0 \u2264 p \u2264 1. Step 3: Reconstruct each patch or block by exploiting x\u0302 = D\u03b1\u0302. Step 4: Put the reconstructed patch to the image at the corresponding location and average each overlapped patches to make the reconstructed image more consistent and natural.\nStep 5: Repeat step 1 to 4 several times till a termination condition is satisfied.\nThe following part of this subsection is to explicitly introduce some image processing techniques using sparse representation.\nThe main task of super-resolution image processing is to extract the high super-resolution image from its low resolution counterpart and this challenging problem has attracted much attention. The most representative work was proposed to exploit the sparse representation theory to generate a superresolution (SRSR) image from a single low-resolution image in literature [153].\nSRSR is mainly performed on two compact learned dictionaries Dl and Dh, which are denoted as dictionaries of low-resolution image patches and its corresponding highresolution image patches, respectively. Dl is directly employed to recover high-resolution images from dictionary Dh. Let X and Y denote the high-resolution and its corresponding lowresolution images, respectively. x and y are a high-resolution image patch and its corresponding low-resolution image patch, respectively. Thus, x = Py and P is the projection matrix. Moreover, if the low resolution image Y is produced by downsampling and blurring from the high resolution image X , the following reconstruction constraint should be satisfied\nY = SBX (VIII.26)\nwhere S and B are a downsampling operator and a blurring filter, respectively. However, the solution of problem VIII.26 is ill-posed because infinite solutions can be achieved for a given low-resolution input image Y . To this end, SRSR [153] provides a prior knowledge assumption, which is formulated as\nx = Dh\u03b1 s.t. \u2016\u03b1\u20160 \u2264 k (VIII.27)\nwhere k is a small constant. This assumption gives a prior knowledge condition that any image patch x can be approximately represented by a linear combination of a few training samples from dictionary Dh. As presented in Subsection III-B, problem VIII.27 is an NP-hard problem and sparse representation with l1-norm regularization is introduced. If\nthe desired representation solution \u03b1 is sufficiently sparse, problem VIII.27 can be converted into the following problem:\nargmin \u2016\u03b1\u20161 s.t. \u2016x\u2212Dh\u03b1\u201622 \u2264 \u03b5 (VIII.28)\nor argmin \u2016x\u2212Dh\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 (VIII.29)\nwhere \u03b5 is a small constant and \u03bb is the Lagrange multiplier. The solution of problem VIII.28 can be achieved by two main phases, i.e. local model based sparse representation (LMBSR) and enhanced global reconstruction constraint. The first phase of SRSR, i.e. LMBSR, is operated on each image patch, and for each low-resolution image patch y, the following equation is satisfied\nargmin \u2016Fy \u2212 FDl\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 (VIII.30)\nwhere F is a feature extraction operator. One-pass algorithm similar to that of [154] is introduced to enhance the compatibility between adjacent patches. Furthermore, a modified optimization problem is proposed to guarantee that the superresolution reconstruction coincides with the previously obtained adjacent high-resolution patches, and the problem is reformulated as\nargmin \u2016\u03b1\u20161 s.t. \u2016Fy\u2212FDl\u03b1\u201622 \u2264 \u03b51; \u2016v\u2212LDh\u03b1\u201622 \u2264 \u03b52 (VIII.31) where v is the previously obtained high-resolution image on the overlap region, and L refers to the region of overlap between the current patch and previously obtained highresolution image. Thus problem VIII.31 can be rewritten as\nargmin \u2016y\u0302 \u2212D\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 (VIII.32)\nwhere y\u0302 = [ Fy v ] and D = [ FDl LDh ] . Problem VIII.32 can be simply solved by previously introduced solution of the sparse representation with l1-norm minimization. Assume that the optimal solution of problem VIII.32, i.e. \u03b1\u2217, is achieved, the high-resolution patch can be easily reconstructed by x = Dh\u03b1\n\u2217. The second phase of SRSR enforces the global reconstruction constraint to eliminate possible unconformity or noise from the first phase and make the obtained image more consistent and compatible. Suppose that the high-resolution image obtained by the first phase is denoted as matrix X0, we project X0 onto the solution space of the reconstruction constraint VIII.26 and the problem is formulated as follows\nX\u2217 = argmin \u2016X \u2212X0\u201622 s.t. Y = SBX (VIII.33)\nProblem VIII.33 can be solved by the back-projection method in [155] and the obtained image X\u2217 is regarded as the final optimal high-resolution image. The entire super-resolution via sparse representation is summarized in algorithm 14 and more information can be found in the literature [153].\nFurthermore, extensive other methods based on sparse representation have been proposed to solve the super-resolution image processing problem. For example, Yang et al. presented a modified version called joint dictionary learning via sparse representation (JDLSR) [156], which jointly learned\nAlgorithm 14. Super-resolution via sparse representation\nInput: Training image patches dictionaries Dl and Dh, a low-resolution image Y . For each overlapped 3\u00d73 patches y of Y using one-pass algorithm, from left to right and top to bottom\nStep 1: Compute optimal sparse representation coefficients \u03b1\u2217 in problem (VIII.32). Step 2: Compute the high-resolution patch by x = Dh\u03b1\u2217. Step 3: Put the patch x into a high-resolution image X0 in corresponding\nlocation. End\nStep 4: Compute the final super-resolution image X\u2217 in problem (VIII.33). Output: X\u2217\ntwo dictionaries that enforced the similarity of sparse representation for low-resolution and high-resolution images. Tang et al. [157] first explicitly analyzed the rationales of the sparse representation theory in performing the super-resolution task, and proposed to exploit the L2-Boosting strategy to learn coupled dictionaries, which were employed to construct sparse coding space. Zhang et al. [158] presented an image super-resolution reconstruction scheme by employing the dualdictionary learning and sparse representation method for image super-resolution reconstruction and Gao et al. [159] proposed a sparse neighbor embedding method, which incorporated the sparse neighbor search and HoG clustering method into the process of image super-resolution reconstruction. FernandezGranda and Candes [160] designed a transform-invariant group sparse regularizer by implementing a data-driven nonparametric regularizers with learned domain transform on group sparse representation for high image super-resolution. Lu et al. [161] proposed a geometry constrained sparse representation method for single image super-resolution by jointly obtaining an optimal sparse solution and learning a discriminative and reconstructive dictionary. Dong et al. [162] proposed to harness an adaptive sparse optimization with nonlocal regularization based on adaptive principal component analysis enhanced by nonlocal similar patch grouping and nonlocal self-similarity quadratic constraint to solve the image high super-resolution problem. Dong et al. [163] proposed to integrate an adaptive sparse domain selection and an adaptive regularization based on piecewise autoregressive models into the sparse representations framework for single image superresolution reconstruction. Mallat and Yu [164] proposed a sparse mixing estimator for image super-resolution, which introduced an adaptive estimator models by combining a group of linear inverse estimators based on different prior knowledge for sparse representation.\nNoise in an image is unavoidable in the process of image acquisition. The need for sparse representation may arise when noise exists in image data. In such a case, the image with noise may lead to missing information or distortion such that this results in a decrease of the precision and accuracy of image processing. Eliminating such noise is greatly beneficial to many applications. The main goal of image denoising is to distinguish the actual signal and noise signal so that we can remove the noise and reconstruct the genuine image. In the presence of image sparsity and redundancy representation\n[4, 7], sparse representation for image denoising first extracts the sparse image components, which are regarded as useful information, and then abandons the representation residual, which is treated as the image noise term, and finally reconstructs the image exploiting the pre-obtained sparse components, i.e. noise-free image. Extensive research articles for image denoising based on sparse representation have been published. For example, Donoho [8, 29, 165] first discovered the connection between the compressed sensing and image denoising. Subsequently, the most representative work of using sparse representation to make image denoising was proposed in literature [166], in which a global sparse representation model over learned dictionaries (SRMLD) was used for image denoising. The following prior assumption should be satisfied: every image block of image x, denoted as z, can be sparsely represented over a dictionary D, i.e. the solution of the following problem is sufficiently sparse:\nargmin \u03b1\n\u2016\u03b1\u20160 s.t. D\u03b1 = z (VIII.34)\nAnd an equivalent problem can be reformulated for a proper value of \u03bb, i.e.\nargmin \u03b1\n\u2016D\u03b1\u2212 z\u201622 + \u03bb\u2016\u03b1\u20160 (VIII.35)\nIf we take the above prior knowledge into full consideration, the objective function of SRMLD based on Bayesian treatment is formulated as\narg min D,\u03b1i,x\n\u03b4\u2016x\u2212 y\u201622 + M\u2211\ni=1\n\u2016D\u03b1i \u2212 Pix\u201622 + M\u2211\ni=1 \u03bbi\u2016\u03b1i\u20160 (VIII.36)\nwhere x is the finally denoised image, y the measured image with white and additive Gaussian white noise, Pi is a projection operator that extracts the i-th block from image x, M is the number of the overlapping blocks, D is the learned dictionary, \u03b1i is the coefficients vector, \u03b4 is the weight of the first term and \u03bbi is the Lagrange multiplier. The first term in VIII.36 is the log-likelihood global constraint such that the obtained noise-free image x is sufficiently similar to the original image y. The second and third terms are the prior knowledge of the Bayesian treatment, which is presented in problem VIII.35. The optimization of problem VIII.35 is a joint optimization problem with respect to D, \u03b1i and x. It can be solved by alternatively optimizing one variable when fixing the others. The process of optimization is briefly introduced below.\nWhen dictionary D and the solution of sparse representation \u03b1i are fixed, problem VIII.36 can be rewritten as\nargmin x\n\u03b4\u2016x\u2212 y\u201622 + M\u2211\ni=1\n\u2016D\u03b1i \u2212 z\u201622 (VIII.37)\nwhere z = Pix. Apparently, problem VIII.37 is a simple convex optimization problem and has a closed-form solution, which is given by\nx =\n( M\u2211\ni=1\nPTi Pi + \u03b4I\n)\u22121( M\u2211\ni=1\nPTi D\u03b1i + \u03b4y\n)\u22121\n(VIII.38)\nWhen x is given, problem (VIII.36) can be written as\narg min D,\u03b1i\nM\u2211\ni=1\n\u2016D\u03b1i \u2212 Pix\u201622 + M\u2211\ni=1\n\u03bbi\u2016\u03b1i\u20160 (VIII.39)\nwhere the problem can be divided into M sub-problems and the i-th sub-problem can be reformulated as the following dictionary learning problem:\narg min D,\u03b1i\n\u2016D\u03b1i \u2212 z\u201622 s.t. \u2016\u03b1i\u20160 \u2264 \u03c4 (VIII.40)\nwhere z = Pix and \u03c4 is small constant. One can see that the sub-problem VIII.39 is the same as problem VIII.2 and it can be solved by the KSVD algorithm previously presented in Subsection VIII-A2. The algorithm of image denoising exploiting sparse and redundant representation over learned dictionary is summarized in Algorithm 15, and more information can be found in literature [166].\nAlgorithm 15. Image denoising via sparse and redundant representation over learned dictionary Task: To denoise a measured image y from white and additional Gaussian white noise: argminD,\u03b1i,x \u03b4\u2016x\u2212 y\u201622 + \u2211M i=1 \u2016D\u03b1i \u2212 Pix\u201622 + \u2211M i=1 \u03bbi\u2016\u03b1i\u20160 Input: Measured image sample y, the number of training iteration T . Initialization: t = 1, set x = y, D initialized by an overcomplete DCT dictionary. While t \u2264 T do\nStep 1: For each image patch Pix, employ the KSVD algorithm to update the values of sparse representation solution \u03b1i and corresponding dictionary D.\nStep 2: t = t + 1 End While\nStep 3: Compute the value of x by using Eq.(VIII.38). Output: denoised image x\nMoreover, extensive modified sparse representation based image denoising algorithms have been proposed. For example, Dabov et al. [167] proposed an enhanced sparse representation with a block-matching 3-D (BM3D) transform-domain filter based on self-similarities and an enhanced sparse representation by clustering similar 2-D image patches into 3-D data spaces and an iterative collaborative filtering procedure for image denoising. Mariral et al. [168] proposed the use of extending the KSVD-based grayscale algorithm and a generalized weighted average algorithm for color image denoising. Protter and Elad [169] extended the techniques of sparse and redundant representations for image sequence denoising by exploiting spatio-temporal atoms, dictionary propagation over time and dictionary learning. Dong et al. [170] designed a clustering based sparse representation algorithm, which was formulated by a double-header sparse optimization problem built upon dictionary learning and structural clustering. Recently, Jiang et al. [171] proposed a variational encoding framework with a weighted sparse nonlocal constraint, which was constructed by integrating image sparsity prior and nonlocal self-similarity prior into a unified regularization term to overcome the mixed noise removal problem. Gu et al. [172] studied a weighted nuclear norm minimization (WNNM) method with F -norm fidelity under different weighting rules optimized by non-local self-similarity for image denoising. Ji et al. [173] proposed a patch-based video denoising algorithm by stacking similar\npatches in both spatial and temporal domain to formulate a low-rank matrix problem with the nuclear norm. Cheng et al. [174] proposed an impressive image denoising method based on an extension of the KSVD algorithm via group sparse representation.\nThe primary purpose of image restoration is to recover the original image from the degraded or blurred image. The sparse representation theory has been extensively applied to image restoration. For example, Bioucas-Dias and Figueirdo [175] introduced a two-step iterative shrinkage/thresholding (TwIST) algorithm for image restoration, which is more efficient and can be viewed as an extension of the IST method. Mairal et al. [176] presented a multiscale sparse image representation framework based on the KSVD dictionary learning algorithm and shift-invariant sparsity prior knowledge for restoration of color images and video image sequence. Recently, Mairal et al. [177] proposed a learned simultaneous sparse coding (LSSC) model, which integrated sparse dictionary learning and nonlocal self-similarities of natural images into a unified framework for image restoration. Zoran and Weiss [178] proposed an expected patch log likelihood (EPLL) optimization model, which restored the image from patch to the whole image based on the learned prior knowledge of any patch acquired by Maximum A-Posteriori estimation instead of using simple patch averaging. Bao et al. [179] proposed a fast orthogonal dictionary learning algorithm, in which a sparse image representation based orthogonal dictionary was learned in image restoration. Zhang et al. [180] proposed a groupbased sparse representation, which combined characteristics from local sparsity and nonlocal self-similarity of natural images to the domain of the group. Dong et al. [181, 182] proposed a centralized sparse representation (CSR) model, which combined the local and nonlocal sparsity and redundancy properties for variational problem optimization by introducing a concept of sparse coding noise term.\nHere we mainly introduce a recently proposed simple but effective image restoration algorithm CSR model [181]. For a degraded image y, the problem of image restoration can be formulated as\ny = Hx+ v (VIII.41)\nwhere H is a degradation operator, x is the original highquality image and v is the Gaussian white noise. Suppose that the following two sparse optimization problems are satisfied\n\u03b1x = argmin \u2016\u03b1\u20161 s.t. \u2016x\u2212D\u03b1\u201622 \u2264 \u03b5 (VIII.42)\n\u03b1y = argmin \u2016\u03b1\u20161 s.t. \u2016x\u2212HD\u03b1\u201622 \u2264 \u03b5 (VIII.43)\nwhere y and x respectively denote the degraded image and original high-quality image, and \u03b5 is a small constant. A new concept called sparse coding noise (SCN) is defined\nv\u03b1 = \u03b1y \u2212\u03b1x (VIII.44)\nGiven a dictionary D, minimizing SCN can make the image better reconstructed and improve the quality of the image restoration because x\u2217 = x\u0302 \u2212 x\u0303 = D\u03b1y \u2212 D\u03b1x = Dv\u03b1.\nThus, the objective function is reformulated as\n\u03b1y = argmin \u03b1 \u2016y \u2212HD\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 + \u00b5\u2016\u03b1\u2212\u03b1x\u20161 (VIII.45) where \u03bb and \u00b5 are both constants. However, the value of \u03b1x is difficult to directly evaluate. Because many nonlocal similar patches are associated with the given image patch i, clustering these patches via block matching is advisable and the sparse code of searching similar patch l to patch i in cluster \u2126i, denoted by \u03b1il, can be computed. Moreover, the unbiased estimation of \u03b1x, denoted by E[\u03b1x], empirically can be approximate to \u03b1x under some prior knowledge [181], and then SCN algorithm employs the nonlocal means estimation method [183] to evaluate the unbiased estimation of \u03b1x, that is, using the weighted average of all \u03b1il to approach E[\u03b1x], i.e.\n\u03b8i = \u2211\nl\u2208\u2126i wil\u03b1il (VIII.46)\nwhere wil = exp ( \u2212\u2016xi \u2212 xil\u201622/h ) /N , xi = D\u03b1i, xil = D\u03b1il, N is a normalization parameter and h is a constant. Thus, the objective function VIII.45 can be rewritten as\n\u03b1y = argmin \u03b1\n\u2016y \u2212HD\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 + \u00b5 M\u2211\ni=1 \u2016\u03b1i \u2212 \u03b8i\u20161 (VIII.47)\nwhere M is the number of the separated patches. In the j-th iteration, the solution of problem VIII.47 is iteratively performed by\n\u03b1j+1y = argmin \u03b1\n\u2016y \u2212HD\u03b1\u201622 + \u03bb\u2016\u03b1\u20161 + \u00b5 M\u2211\ni=1 \u2016\u03b1i \u2212 \u03b8ji \u20161 (VIII.48)\nIt is obvious that problem VIII.47 can be optimized by the augmented Lagrange multiplier method [184] or the iterative shrinkage algorithm in [185]. According to the maximum average posterior principle and the distribution of the sparse coefficients, the regularization parameter \u03bb and constant \u00b5 can be adaptively determined by \u03bb = 2 \u221a 2\u03c12\n\u03c3i and \u00b5 = 2\n\u221a 2\u03c12\n\u03b7i ,\nwhere \u03c1, \u03c3i and \u03b7i are the standard deviations of the additive Gaussian noise, \u03b1i and the SCN signal, respectively. Moreover, in the process of image patches clustering for each given image patch, a local PCA dictionary is learned and employed to code each patch within its corresponding cluster. The main procedures of the CSR algorithm are summarized in Algorithm 16 and readers may refer to literature [181] for more details."}, {"heading": "C. Sparse representation in image classification and visual tracking", "text": "In addition to these effective applications in image processing, several other fields for sparse representation have been proposed and extensively studied in image classification and visual tracking. Since Wright et al. [20] proposed to employ sparse representation to perform robust face recognition, more and more researchers have been applying the sparse representation theory to the fields of computer vision and pattern recognition, especially in image classification and object tracking.\nAlgorithm 16. Centralized sparse representation for image restoration\nInitialization: Set x = y, initialize regularization parameter \u03bb and \u00b5, the number of training iteration T , t = 0, \u03b80 = 0. Step 1: Partition the degraded image into M overlapped patches. While t \u2264 T do Step 2: For each image patch, update the corresponding dictionary for each cluster via k-means and PCA. Step 3: Update the regularization parameters \u03bb and \u00b5 by using\n\u03bb = 2 \u221a 2\u03c12\n\u03c3t and \u00b5 = 2\n\u221a 2\u03c12\n\u03b7t .\nStep 4: Compute the nonlocal means estimation of the unbiased estimation of \u03b1x, i.e. \u03b8 t+1 i , by using Eq. (VIII.46) for each image patch. Step 5: For a given \u03b8t+1i , compute the sparse representation solution, i.e. \u03b1t+1y , in problem (VIII.48) by using the extended iterative shrinkage algorithm in literature [185]. Step 6: t = t + 1 End While Output: Restored image x = D\u03b1t+1y\nExperimental results have suggested that the sparse representation based classification method can somewhat overcome the challenging issues from illumination changes, random pixel corruption, large block occlusion or disguise.\nAs face recognition is a representative component of pattern recognition and computer vision applications, the applications of sparse representation in face recognition can sufficiently reveal the potential nature of sparse representation. The most representative sparse representation for face recognition has been presented in literature [18] and the general scheme of sparse representation based classification method is summarized in Algorithm 17. Suppose that there are n training samples, X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] from c classes. Let Xi denote the samples from the i-th class and the testing sample is y.\nAlgorithm 17. The scheme of sparse representation based classification method\nStep 1: Normalize all the samples to have unit l2-norm. Step 2: Exploit the linear combination of all the training samples to represent the test sample and the following l1-norm minimization problem is satisfied \u03b1\u2217 = argmin \u2016\u03b1\u20161 s.t. \u2016y \u2212X\u03b1\u201622 \u2264 \u03b5. Step 3: Compute the representation residual for each class ri = \u2016y \u2212Xi\u03b1\u2217i \u201622 where \u03b1\u2217i here denotes the representation coefficients vector associated with the i-th class. Step 4: Output the identity of the test sample y by judging\nlabel(y) = argmini(ri).\nNumerous sparse representation based classification methods have been proposed to improve the robustness, effectiveness and efficiency of face recognition. For example, Xu et al. [9] proposed a two-phase sparse representation based classification method, which exploited the l2-norm regularization rather than the l1-norm regularization to perform a coarse to fine sparse representation based classification, which was very efficient in comparison with the conventional l1norm regularization based sparse representation. Deng et al. [186] proposed an extended sparse representation method (ESRM) for improving the robustness of SRC by eliminating the variations in face recognition, such as disguise, occlusion, expression and illumination. Deng et al. [187] also proposed a framework of superposed sparse representation based classification, which emphasized the prototype and vari-\nation components from uncontrolled images. He et al. [188] proposed utilizing the maximum correntropy criterion named CESR embedding non-negative constraint and half-quadratic optimization to present a robust face recognition algorithm. Yang et al. [189] developed a new robust sparse coding (RSC) algorithm, which first obtained a sparsity-constrained regression model based on maximum likelihood estimation and exploited an iteratively reweighted regularized robust coding algorithm to solve the pre-proposed model. Some other sparse representation based image classification methods also have been developed. For example, Yang et al. [190] introduced an extension of the spatial pyramid matching (SPM) algorithm called ScSPM, which incorporated SIFT sparse representation into the spatial pyramid matching algorithm. Subsequently, Gao et al. [191] developed a kernel sparse representation with the SPM algorithm called KSRSPM, and then proposed another version of an improvement of the SPM called LScSPM [192], which integrated the Laplacian matrix with local features into the objective function of the sparse representation method. Kulkarni and Li [193] proposed a discriminative affine sparse codes method (DASC) on a learned affineinvariant feature dictionary from input images and exploited the AdaBoost-based classifier to perform image classification. Zhang et al. [194] proposed integrating the non-negative sparse coding, low-rank and sparse matrix decomposition (LR-Sc+SPM) method, which exploited non-negative sparse coding and SPM for achieving local features representation and employed low-rank and sparse matrix decomposition for sparse representation, for image classification. Recently, Zhang et al. [195] presented a low-rank sparse representation (LRSR) learning method, which preserved the sparsity and spatial consistency in each procedure of feature representation and jointly exploited local features from the same spatial proximal regions for image classification. Zhang et al. [196] developed a structured low-rank sparse representation (SLRSR) method for image classification, which constructed a discriminative dictionary in training terms and exploited low-rank matrix reconstruction for obtaining discriminative representations. Tao et al. [197] proposed a novel dimension reduction method based on the framework of rank preserving sparse learning, and then exploited the projected samples to make effective Kinect-based scene classification. Zhang et al. [198] proposed a discriminative tensor sparse coding (RTSC) method for robust image classification. Recently, low-rank based sparse representation became a popular topic such as non-negative low-rank and sparse graph [199]. Some sparse representation methods in face recognition can be found in a review [83] and other more image classification methods can be found in a more recent review [200].\nMei et al. employed the idea of sparse representation to visual tracking [201] and vehicle classification [202], which introduced nonnegative sparse constraints and dynamic template updating strategy. It, in the context of the particle filter framework, exploited the sparse technique to guarantee that each target candidate could be sparsely represented using the linear combinations of fewest targets and particle templates. It also demonstrated that sparse representation can be propagated to address object tracking problems. Extensive sparse\nrepresentation methods have been proposed to address the visual tracking problem. In order to design an accelerated algorithm for l1 tracker, Li et al. [203] proposed two realtime compressive sensing visual tracking algorithms based on sparse representation, which adopted dimension reduction and the OMP algorithm to improve the efficiency of recovery procedure in tracking, and also developed a modified version of fusing background templates into the tracking procedure for robust object tracking. Zhang et al. [204] directly treated object tracking as a pattern recognition problem by regarding all the targets as training samples, and then employed the sparse representation classification method to do effective object tracking. Zhang et al. [205] employed the concept of sparse representation based on a particle filter framework to construct a multi-task sparse learning method denoted as multi-task tracking for robust visual tracking. Additionally, because of the discriminative sparse representation between the target and the background, Jia et al. [206] conceived a structural local sparse appearance model for robust object tracking by integrating the partial and spatial information from the target based on an alignment-pooling algorithm. Liu et al. [207] proposed constructing a two-stage sparse optimization based online visual tracking method, which jointly minimized the objective reconstruction error and maximized the discriminative capability by choosing distinguishable features. Liu et al. [208] introduced a local sparse appearance model (SPT) with a static sparse dictionary learned from k-selection and dynamic updated basis distribution to eliminate potential drifting problems in the process of visual tracking. Bao et al. [209] developed a fast real time l1-tracker called the APG-l1tracker, which exploited the accelerated proximal gradient algorithm to improve the l1-tracker solver in [201]. Zhong et al. [210] addressed the object tracking problem by developing a sparsitybased collaborative model, which combined a sparsity-based classifier learned from holistic templates and a sparsity-based template model generated from local representations. Zhang et al. [211] proposed to formulate a sparse feature measurement matrix based on an appearance model by exploiting nonadaptive random projections, and employed a coarse-to-fine strategy to accelerate the computational efficiency of tracking task. Lu et al. [212] proposed to employ both non-local selfsimilarity and sparse representation to develop a non-local selfsimilarity regularized sparse representation method based on geometrical structure information of the target template data set. Wang et al. [213] proposed a sparse representation based online two-stage tracking algorithm, which learned a linear classifier based on local sparse representation on favorable image patches. More detailed visual tracking algorithms can be found in the recent reviews [214, 215]."}, {"heading": "IX. EXPERIMENTAL EVALUATION", "text": "In this section, we take the object categorization problem as an example to evaluate the performance of different sparse representation based classification methods. We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].\nPlenties of data sets have been collected for object categorization, especially for image classification. Several image data sets are used in our experimental evaluations.\nORL: The ORL database includes 400 face images taken from 40 subjects each providing 10 face images [216]. For some subjects, the images were taken at different times, with varying lighting, facial expressions, and facial details. All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement). Each image was resized to a 56\u00d746 image matrix by using the down-sampling algorithm.\nLFW face dataset: The Labeled Faces in the Wild (LFW) face database is designed for the study of unconstrained identity verification and face recognition [217]. It contains more than 13,000 images of faces collected from the web under the unconstrained conditions. Each face has been labeled with the name of the people pictured. 1680 of the people\npictured have two or more distinct photos in the database. In our experiments, we chose 1251 images from 86 peoples and each subject has 10-20 images [218]. Each image was manually cropped and was resized to 32\u00d732 pixels.\nExtended YaleB face dataset: The extended YaleB database contains 2432 front face images of 38 individuals and each subject having around 64 near frontal images under different illuminations [219]. The main challenge of this database is to overcome varying illumination conditions and expressions. The facial portion of each original image was cropped to a 192\u00d7168 image. All images in this data set for our experiments simply resized these face images to 32\u00d732 pixels.\nCOIL20 dataset: Columbia Object Image Library (COIL20) database consists of 1,440 size normalized gray-scale images of 20 objects [220]. Different object images are captured at every angle in a 360 rotation. Images of the objects were taken from varying angles at pose intervals of five degrees and\neach object has 72 images. Fifteen scene dataset: This dataset contains 4485 images under 15 natural scene categories presented in literature [221] and each category includes 210 to 410 images. The 15 scenes categories are office, kitchen, living room, bedroom, store, industrial, tall building, inside cite, street, highway, coast, open country, mountain, forest and suburb. A wide range of outdoor and indoor scenes are included in this dataset. The average image size is around 250\u00d7300 pixels and the spatial pyramid matching features are used in our experiments."}, {"heading": "A. Parameter selection", "text": "Parameter selection, especially selection of the regularization parameter \u03bb in different minimization problems, plays an important role in sparse representation. In order to make fair comparisons with different sparse representation algorithms, performing the optimal parameter selection for different sparse representation algorithms on different datasets is advisable and indispensable. In this subsection, we perform extensive experiments for selecting the best value of the regularization parameter \u03bb with a wide range of options. Specifically, we implement the l1 ls, FISTA, DALM, homotopy and TPTSR algorithms on different databases to analyze the importance of the regularization parameter. Fig. 5 summarizes the classification accuracies of exploiting different sparse representation based classification methods with varying values of regularization parameter \u03bb on the two face datasets, i.e. ORL and LFW face datasets, and two object datasets, i.e. COIL20 and Fifteen scene datasets. On the ORL and LFW face datasets, we respectively selected the first five and eight face images of each subject as training samples and the rest of image samples for testing. As for the experiments on the COIL20 and fifteen scene datasets, we respectively treated the first ten images of each subject in both datasets as training samples and used all the remaining images as test samples. Moreover, from Fig. 5, one can see that the value of regularization parameter \u03bb can significantly dominate the classification results, and the values of \u03bb for achieving the best classification results on different datasets are distinctly different. An interesting scenario is that the performance of the TPTSR algorithm is almost not influenced by the variation of regularization parameter \u03bb in the experiments on fifteen scene dataset, as shown in Fig. 5(d). However, the best classification accuracy can be always obtained within the range of 0.0001 to 1. Thus, the value of the regularization parameter is set within the range from 0.0001 to 1."}, {"heading": "B. Experimental results", "text": "In order to test the performance of different kinds of sparse representation methods, an empirical study of experimental results is conducted in this subsection and seven typical sparse representation based classification methods are selected for performance evaluation followed with extensive experimental results. For all datasets, following most previous published work, we randomly choose several samples of every class as training samples and used the rest as test samples and\nthe experiments are repeated 10 times with the optimal parameter obtained using the cross validation approach. The gray-level features of all images in these data sets are used to perform classification. For the sake of computational efficiency, principle component analysis algorithm is used as a preprocessing step to preserve 98% energy of all the data sets. The classification results and computational time have been summarized in Table I. From the experimental results on different databases, we can conclude that there still does not exist one extraordinary algorithm that can achieve the best classification accuracy on all databases. However, some algorithms are noteworthy to be paid much more attention. For example, the l1 ls algorithm in most cases can achieve better classification results than the other algorithms on the ORL database, and when the number of training samples of each class is five, the l1 ls algorithm can obtain the highest classification result of 95.90%. The TPTSR algorithm is very computationally efficient in comparison with other sparse representation with l1-norm minimization algorithms and the classification accuracies obtained by the TPTSR algorithm are very similar and sometimes even better than the other sparse representation based classification algorithms.\nThe computational time is another indicator for measuring the performance of one specific algorithm. As shown in Table I, the average computational time of each algorithm is shown at the bottom of the table for one specific number of training samples. Note that the computational time of OMP and TPTSR algorithms are drastically lower than that of other sparse representation with l1-norm minimization algorithms. This is mainly because the sparse representation with l1norm minimization algorithms always iteratively solve the l1norm minimization problem. However, the OMP and TPTSR algorithms both exploit the fast and efficient least squares technique, which guarantees that the computational time is significantly less than other l1-norm based sparse representation algorithms."}, {"heading": "C. Discussion", "text": "Lots of sparse representation methods have been available in past decades and this paper introduces various sparse representation methods from some viewpoints, including their motivations, mathematical representations and the main algorithms. Based on the experimental results summarized in Section IX, we have the following observations.\nFirst, a challenging task of choosing a suitable regularization parameter for sparse representation should make further extensive studies. We can see that the value of the regularization parameter can remarkably influence the performance of the sparse representation algorithms and adjusting the parameters in sparse representation algorithms requires expensive labor. Moreover, adaptive parameter selection based sparse representation methods is preferable and very few methods have been proposed to solve this critical issue.\nSecond, although sparse representation algorithms have achieved distinctly promising performance on some real-world databases, many efforts should be made in promoting the accuracy of sparse representation based classification, and the\nrobustness of sparse representation should be further enhanced. In terms of the recognition accuracy, the algorithms of l1 ls, homotopy and TPTSR achieve the best overall performance. Considering the experimental results of exploiting the seven algorithms on the five databases, the l1 ls algorithm has eight highest classification accuracies, followed by homotopy and TPTSR, in comparison with other algorithms. One can see that the sparse representation based classification methods still can not obtain satisfactory results on some challenge databases. For example, all these representative algorithms can achieve relatively inferior experimental results on the LFW dataset shown in Subsection IX-B, because the LFW dataset is designed for studying the problem of unconstrained face recognition [217] and most of the face images are captured under complex environments. One can see that the PALM algorithm has the worst classification accuracy on the LFW dataset and the classification accuracy even decreases mostly with the increase of the number of the training samples. Thus, devising more robust sparse representation algorithm is an urgent issue.\nThird, enough attention should be paid on the computational inefficiency of sparse representation with l1-norm minimization. One can see that high computational complexity is one of the most major drawbacks of the current sparse representation methods and also hampers its applications in real-time processing scenarios. In terms of speed, PALM, FISTA and DALM take much longer time to converge than the other methods. The\naverage computational time of OMP and TPTSR is the two lowest algorithms. Moreover, compared with the l1-regularized sparse representation based classification methods, the TPTSR has very competitive classification accuracy but significantly low complexity. Efficient and effective sparse representation methods are urgently needed by real-time applications. Thus, developing more efficient and effective methods is essential for future study on sparse representation.\nFinally, the extensive experimental results have demonstrated that there is no absolute winner that can achieve the best performance for all datasets in terms of classification accuracy and computational efficiency. However, l1 ls, TPTSR and homotopy algorithms as a whole outperform the other algorithms. As a compromising approach, the OMP algorithm can achieve distinct efficiency without sacrificing much recognition rate in comparison with other algorithms and it also has been extensively applied to some complex learning algorithms as a function."}, {"heading": "X. CONCLUSION", "text": "Sparse representation has been extensively studied in recent years. This paper summarizes and presents various available sparse representation methods and discusses their motivations, mathematical representations and extensive applications. More specifically, we have analyzed their relations in theory and empirically introduced the applications including dictionary\nlearning based on sparse representation and real-world applications such as image processing, image classification, and visual tracking.\nSparse representation has become a fundamental tool, which has been embedded into various learning systems and also has received dramatic improvements and unprecedented achievements. Furthermore, dictionary learning is an extremely popular topic and is closely connected with sparse representation. Currently, efficient sparse representation, robust sparse representation, and dictionary learning based on sparse representation seem to be the main streams of research on sparse representation methods. The low-rank representation technique has also recently aroused intensive research interests and sparse representation has been integrated into low-rank representation for constructing more reliable representation models. However, the mathematical justification of low-rank representation seems not to be elegant as sparse representation. Because employing the ideas of sparse representation as a prior can lead to state-of-the-art results, incorporating sparse representation with low-rank representation is worth further research. Moreover, subspace learning also has been becoming one of the most prevailing techniques in pattern recognition and computer vision. It is necessary to further study the relationship between sparse representation and subspace learning, and constructing more compact models for sparse subspace learning becomes one of the popular topics in various research fields. The transfer learning technique has emerged as a new learning framework for classification, regression and clustering problems in data mining and machine learning. However, sparse representation research still has been not fully applied to the transfer learning framework and it is significant to unify the sparse representation and low-rank representation techniques into the transfer learning framework to solve domain adaption, multitask learning, sample selection bias and covariate shift problems. Furthermore, researches on deep learning seems to become an overwhelming trend in the computer vision field. However, dramatically expensive training effort is the main limitation of current deep learning technique and how to fully introduce current sparse representation methods into the framework of deep learning is valuable and unsolved.\nThe application scope of sparse representation has emerged and has been widely extended to machine learning and computer vision fields. Nevertheless, the effectiveness and efficiency of sparse representation methods cannot perfectly meet the need for real-world applications. Especially, the complexities of sparse representation have greatly affected the applicability, especially the applicability to large scale problems. Enhancing the robustness of sparse representation is considered as another indispensable problem when researchers design algorithms. For image classification, the robustness should be seriously considered, such as the robustness to random corruptions, varying illuminations, outliers, occlusion and complex backgrounds. Thus, developing an efficient and robust sparse representation method for sparse representation is still the main challenge and to design a more effective dictionary is being expected and is beneficial to the performance improvement.\nSparse representation still has wide potential for various\npossible applications, such as event detection, scene reconstruction, video tracking, object recognition, object pose estimation, medical image processing, genetic expression and natural language processing. For example, the study of sparse representation in visual tracking is an important direction and more depth studies are essential to future further improvements of visual tracking research.\nIn addition, most sparse representation and dictionary learning algorithms focus on employing the l0-norm or l1-norm regularization to obtain a sparse solution. However, there are still only a few studies on l2,1-norm regularization based sparse representation and dictionary learning algorithms. Moreover, other extended studies of sparse representation may be fruitful. In summary, the recent prevalence of sparse representation has extensively influenced different fields. It is our hope that the review and analysis presented in this paper can help and motivate more researchers to propose perfect sparse representation methods."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 61370163, Grant 61233011, and Grant 61332011, and in part by the Shenzhen Municipal Science and Technology Innovation Council under Grant JCYJ20130329151843309, Grant JCYJ20130329151843309, Grant JCYJ20140417172417174, Grant CXZZ20140904154910774, the China Postdoctoral Science Foundation funded project, No. 2014M560264 and the Shaanxi Key Innovation Team of Science and Technology (Grant no.: 2012KCT-04).\nWe would like to thank Jian Wu for many inspiring discussions and he is ultimately responsible for many of the ideas in the algorithm and analysis. We would also like to thank Dr. Zhihui Lai, Dr. Jinxing Liu and Xiaozhao Fang for constructive suggestions. Moreover, we thank the editor, an associate editor, and referees for helpful comments and suggestions which greatly improved this paper."}], "references": [{"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM journal on computing, vol. 24, no. 2, pp. 227\u2013234, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Brain extraction based on locally linear representation-based classification", "author": ["M. Huang", "W. Yang", "J. Jiang", "Y. Wu", "Y. Zhang", "W. Chen", "Q. Feng"], "venue": "NeuroImage, vol. 92, pp. 322\u2013339, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Group sparse reconstruction for image segmentation", "author": ["X. Lu", "X. Li"], "venue": "Neurocomputing, vol. 136, pp. 41\u201348, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "On the role of sparse and redundant representations in image processing", "author": ["M. Elad", "M. Figueiredo", "Y. Ma"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 972\u2013 982, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "A wavelet tour of signal processing: the sparse way", "author": ["S. Mallat"], "venue": "Academic press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Sparse image and signal processing: wavelets, curvelets, morphological diversity", "author": ["J. Starck", "F. Murtagh", "J.M. Fadili"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Sparse and redundant representations: from theory to applications in signal and image processing", "author": ["M. Elad"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A.M. Bruckstein", "D.L. Donoho", "M. Elad"], "venue": "SIAM review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "A two-phase test sample sparse representation method for use with face recognition", "author": ["Y. Xu", "D. Zhang", "J. Yang", "J. Yang"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 21, no. 9, pp. 1255\u2013 1262, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse representation for computer vision and pattern recognition", "author": ["J. Wright", "Y. Ma", "J. Mairal", "G. Sapiro", "T. Huang", "S. Yan"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1031\u20131044, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u2013 1306, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sensing", "author": ["R.G. Baraniuk"], "venue": "IEEE signal processing magazine, vol. 24, no. 4, pp. 118\u2013121, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489\u2013 509, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to compressive sampling", "author": ["E.J. Candes", "M.B. Wakin"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 21\u201330, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Extensions of compressed sensing", "author": ["Y. Tsaig", "D.L. Donoho"], "venue": "Signal processing, vol. 86, no. 3, pp. 549\u2013571, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressive sampling", "author": ["E.J. Candes"], "venue": "Proceedings of the International Congress of Mathematicians: Madrid, August 22-30, 2006: invited lectures, 2006, pp. 1433\u20131452.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["E. Candes", "J. Romberg"], "venue": "Inverse problems, vol. 23, no. 3, p. 969, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Manifold regularized sparse nmf for hyperspectral unmixing", "author": ["X. Lu", "H. Wu", "Y. Yuan", "P. Yan", "X. Li"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 51, no. 5, pp. 2815\u20132826, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Binary sparse nonnegative matrix factorization", "author": ["Y. Yuan", "X. Li", "Y. Pang", "X. Lu", "D. Tao"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 19, no. 5, pp. 772\u2013779, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Integrating globality and locality for robust representation based classification", "author": ["Z. Zhang", "Z. Li", "B. Xie", "L. Wang", "Y. Chen"], "venue": "Mathematical Problems in Engineering, vol. 2014, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse representation and learning in visual recognition: Theory and applications", "author": ["H. Cheng", "Z. Liu", "L. Yang", "X. Chen"], "venue": "Signal Processing, vol. 93, no. 6, pp.  1408\u20131425, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for simultaneous sparse approximation. part i: Greedy pursuit", "author": ["J.A. Tropp", "A.C. Gilbert", "M.J. Strauss"], "venue": "Signal Processing, vol. 86, no. 3, pp. 572\u2013588, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for simultaneous sparse approximation. part ii: Convex relaxation", "author": ["J.A. Tropp"], "venue": "Signal Processing, vol. 86, no. 3, pp. 589\u2013602, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimization methods for l1-regularization", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "University of British Columbia, West Mall Vancouver, B.C. Canada V6T 1Z4, Tech. Rep., 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems", "author": ["E. Amaldi", "V. Kann"], "venue": "Theoretical Computer Science, vol. 209, no. 1, pp. 237\u2013260, 1998.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "IEEE Transactions on Information Theory, vol. 50, no. 10, pp. 2231\u20132242, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Proximal algorithms", "author": ["N. Parikh", "S. Boyd"], "venue": "Foundations and Trends in Optimization, vol. 1, no. 3, pp. 123\u2013231, 2013.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Noise modeling and representation based classification methods for face recognition", "author": ["Z. Zhang", "L. Wang", "Q. Zhu", "Z. Liu", "Y. Chen"], "venue": "Neurocomputing, vol. 148, pp. 420\u2013429, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Theoretical results on sparse representations of multiple-measurement vectors", "author": ["J. Chen", "X. Huo"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 12, pp. 4634\u20134643, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Unconstrained regularized lp-norm based algorithm for the reconstruction of sparse signals", "author": ["J.K. Pant", "W.S. Lu", "A. Antoniou"], "venue": "Proceedings of the IEEE International Symposium on Circuits and Systems, 2011, pp. 1740\u20131743.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Iteratively reweighted algorithms for compressive sensing", "author": ["R. Chartrand", "W. Yin"], "venue": "Proceedings of the IEEE international conference on Acoustics, speech and signal processing, 2008, pp. 3869\u20133872.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Beyond sparsity: The role of l1-optimizer in pattern classification", "author": ["J. Yang", "L. Zhang", "Y. Xu", "J. Yang"], "venue": "Pattern Recognition, vol. 45, no. 3, pp. 1104\u20131118, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Signal recovery from random measurements via orthogonal matching pursuit", "author": ["J.A. Tropp", "A.C. Gilbert"], "venue": "IEEE Transactions on Information Theory, vol. 53, no. 12, pp. 4655\u20134666, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit", "author": ["D. Needell", "R. Vershynin"], "venue": "Foundations of computational mathematics, vol. 9, no. 3, pp. 317\u2013334, 2009.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Stable sparse  JOURNAL  35 approximations via nonconvex optimization", "author": ["R. Saab", "R. Chartrand", "O. Yilmaz"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2008, pp. 3885\u20133888.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Exact reconstruction of sparse signals via nonconvex minimization", "author": ["R. Chartrand"], "venue": "IEEE Signal Processing Letters, vol. 14, no. 10, pp. 707\u2013710, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Data modeling: Visual psychology approach and l1/2 regularization theory", "author": ["Z.B. Xu"], "venue": "Proceeding of International Congress of Mathmaticians, 2010, pp. 3151\u2013 3184.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B, pp. 267\u2013288, 1996.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "I. Johnstone", "R. Tibshirani"], "venue": "The Annals of statistics, vol. 32, no. 2, pp. 407\u2013499, 2004.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}, {"title": "Alternating direction algorithms for l1-problems in compressive sensing", "author": ["J. Yang", "Y. Zhang"], "venue": "SIAM journal on scientific computing, vol. 33, no. 1, pp. 250\u2013278, 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast optimization methods for l1 regularization: A comparative study and two new approaches", "author": ["M. Schmidt", "G. Fung", "R. Rosales"], "venue": "Machine Learning: ECML 2007. Springer, 2007, pp. 286\u2013297.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient and robust feature selection via joint l2,1-norms minimization", "author": ["F. Nie", "H. Huang", "X. Cai", "C. Ding"], "venue": "Proceedings of the Advances in Neural Information Processing Systems, 2010, pp. 1813\u20131821.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "l2,1-norm regularized discriminative feature selection for unsupervised learning", "author": ["Y. Yang", "H.T. Shen", "Z. Ma", "Z. Huang", "X. Zhou"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence, vol. 22, no. 1, 2011, pp. 1589\u20131594.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Face recognition by sparse discriminant analysis via joint l2,1-norm minimization", "author": ["X.S. Shi", "Y.J. Yang", "Z.H. Guo", "Z.H. Lai"], "venue": "Pattern Recognition, vol. 47, no. 7, pp. 2447\u20132453, 2014.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-task feature learning via efficient l2,1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, 2009, pp. 339\u2013348.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint embedding learning and sparse regression: A framework for unsupervised feature selection", "author": ["C. Hou", "F. Nie", "X. Li", "D. Yi", "Y. Wu"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 6, pp. 793\u2013804, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Linear regression for face recognition", "author": ["I. Naseem", "R. Togneri", "M. Bennamoun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 11, pp. 2106\u20132112, 2010.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse representation or collaborative representation: Which helps face recognition?", "author": ["D. Zhang", "M. Yang", "X. Feng"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization", "author": ["D. Donoho", "M. Elad"], "venue": "Proceedings of the National Academy  of Sciences, vol. 100, no. 5, pp. 2197\u20132202, 2003.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2003}, {"title": "Realistic action recognition via sparsely-constructed gaussian processes", "author": ["L. Liu", "L. Shao", "F. Zheng", "X. Li"], "venue": "Pattern Recognition, vol. 47, no. 12, pp. 3819\u2013 3827, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse representations, compressive sensing and dictionaries for pattern recognition", "author": ["V.M. Patel", "R. Chellappa"], "venue": "Proceedings of the IEEE First Asian Conference on Pattern Recognition, 2011, pp. 325\u2013329.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hash functions using sparse reconstruction", "author": ["Y. Yuan", "X. Lu", "X. Li"], "venue": "Proceedings of International Conference on Internet Multimedia Computing and Service, 2014, pp. 14\u201318.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2014}, {"title": "For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution", "author": ["D. Donoho"], "venue": "Communications on pure and applied mathematics, vol. 59, no. 6, pp. 797\u2013829, 2006.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2006}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Candes", "J. Romberg", "T. Tao"], "venue": "Communications on pure and applied mathematics, vol. 59, no. 8, pp. 1207\u20131223, 2006.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2006}, {"title": "A comparison of typical lp minimization algorithms", "author": ["L. Qin", "Z.C. Lin", "Y.Y. She", "C. Zhang"], "venue": "Neurocomputing, vol. 119, pp. 413\u2013424, 2013.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2013}, {"title": "l1/2 regularization: A thresholding representation theory and a fast solver", "author": ["Z. Xu", "X. Chang", "F. Xu", "H. Zhang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 7, pp. 1013\u20131027, 2012.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "Enhancing sparsity via lp (0<p<1) minimization for robust face recognition", "author": ["S. Guo", "Z. Wang", "Q. Ruan"], "venue": "Neurocomputing, vol. 99, pp. 592\u2013602, 2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "R1-pca: rotational invariant l1-norm principal component analysis for robust subspace factorization", "author": ["C. Ding", "D. Zhou", "X. He", "H. Zha"], "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 281\u2013288.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2006}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S.G. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397\u20133415, 1993.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1993}, {"title": "Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaiifar", "P.S. Krishnaprasad"], "venue": "Proceedings of the Twenty-Seventh Asilomar Conference on Signals, Systems and Computers, 1993, pp. 40\u201344.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1993}, {"title": "Efficient orthogonal matching pursuit using sparse random projections for scene and video classification", "author": ["S.N. Vitaladevuni", "P. Natarajan", "R. Prasad"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2011, pp. 2312\u20132319.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2011}, {"title": "Cosamp: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J.A. Tropp"], "venue": "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301\u2013321, 2009.  JOURNAL  36", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse solution of underdetermined systems of linear equations by stagewise orthogonal matching pursuit", "author": ["D.L. Donoho", "Y. Tsaig", "I. Drori", "J. Starck"], "venue": "IEEE Transactions on Information Theory, vol. 58, no. 2, pp. 1094\u2013 1121, 2012.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Subspace pursuit for compressive sensing signal reconstruction", "author": ["W. Dai", "O. Milenkovic"], "venue": "IEEE Transactions on Information Theory, vol. 55, no. 5, pp. 2230\u2013 2249, 2009.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity adaptive matching pursuit algorithm for practical compressed sensing", "author": ["T.T. Do", "L. Gan", "N. Nguyen", "T. Tran"], "venue": "Proceedings of the 42nd Asilomar Conference on Signals, Systems and Computers, 2008, pp. 581\u2013587.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Tree-based pursuit: Algorithm and properties", "author": ["P. Jost", "P. Vandergheynst", "P. Frossard"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 12, pp. 4685\u20134697, 2006.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2006}, {"title": "Tree-based orthogonal matching pursuit algorithm for signal reconstruction", "author": ["C. La", "M.N. Do"], "venue": "IEEE International Conference on Image Processing, 2006, pp. 1277\u20131280.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2006}, {"title": "Compressed sensing signal recovery via forward\u2013backward pursuit", "author": ["N.B. Karahanoglu", "H. Erdogan"], "venue": "Digital Signal Processing, vol. 23, no. 5, pp. 1539\u2013 1548, 2013.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems", "author": ["M. Figueiredo", "R.D. Nowak", "S.J. Wright"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 1, no. 4, pp. 586\u2013597, 2007.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}, {"title": "An interior-point method for largescale l1-regularized least squares", "author": ["S.J. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 1, no. 4, pp. 606\u2013617, 2007.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "A truncated primal-infeasible dual-feasible network interior point method", "author": ["L.F. Portugal", "M. Resende", "G. Veiga", "J. Judice"], "venue": "Networks, vol. 35, no. 2, pp. 91\u2013108, 2000.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2000}, {"title": "An interior-point method for large-scale l1-regularized logistic regression.", "author": ["K. Koh", "S.J. Kim", "S.P. Boyd"], "venue": "Journal of Machine learning research,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2007}, {"title": "On the implementation of a primal-dual interior point method", "author": ["S. Mehrotra"], "venue": "SIAM Journal on optimization, vol. 2, no. 4, pp. 575\u2013601, 1992.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1992}, {"title": "Primal-dual interior-point methods", "author": ["S.J. Wright"], "venue": null, "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1997}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1\u2013122, 2011.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2011}, {"title": "A bound optimization approach to wavelet-based image deconvolution", "author": ["M. Figueiredo", "R. Nowak"], "venue": "Proceedings of the IEEE International Conference on Image Processing, vol. 2, 2005, pp. II\u2013782\u20135.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2005}, {"title": "Proximal splitting methods in signal processing", "author": ["P.L. Combettes", "J.C. Pesquet"], "venue": "Fixed-point algorithms for inverse problems in science and engineering, 2011, pp. 185\u2013212.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183\u2013202, 2009.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast l1-minimization algorithms and an application in robust face recognition: A review", "author": ["A.Y. Yang", "S. Sastryand", "A. Ganesh", "Y. Ma"], "venue": "Proceedings of the IEEE International Conference on Image Processing (ICIP), 2010, pp. 1849\u20131852.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "R.D. Nowak", "M. Figueiredo"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 7, pp. 2479\u20132493, 2009.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2009}, {"title": "The cyclic barzilai\u2014borwein method for unconstrained optimization", "author": ["Y.H. Dai", "W. Hager", "K. Schittkowski", "H. Zhang"], "venue": "IMA Journal of Numerical Analysis, vol. 26, no. 3, pp. 604\u2013627, 2006.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2006}, {"title": "A fixed-point continuation method for l1-regularized minimization with applications to compressed sensing", "author": ["E.T. Hale", "W. Yin", "Y. Zhang"], "venue": "Rice University, St, Houston, USA, Tech. Rep., 2007.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2007}, {"title": "Accelerated l1/2 regularization based sar imaging via bcr and reduced newton skills", "author": ["J. Zeng", "Z. Xu", "B. Zhang", "W. Hong", "Y. Wu"], "venue": "Signal Processing, vol. 93, no. 7, pp. 1831\u20131844, 2013.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1831}, {"title": "l1/2 regularization: convergence of iterative half thresholding algorithm", "author": ["H.J. Zeng", "S. Lin", "Y. Wang", "Z. Xu"], "venue": "IEEE Transactions on Signal Processing, vol. 62, no. 9, pp. 2317\u20132329, 2014.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast l1-minimization algorithms for robust face recognition", "author": ["A. Yang", "Z. Zhou", "A. Balasubramanian", "S. Sastry", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 8, pp. 3234\u20133246, 2013.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2013}, {"title": "Coordinate and subspace optimization methods for linear least squares with non-quadratic regularization", "author": ["M. Elad", "B. Matalon", "M. Zibulevsky"], "venue": "Applied and Computational Harmonic Analysis, vol. 23, no. 3, pp. 346\u2013367, 2007.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2007}, {"title": "Messagepassing algorithms for compressed sensing", "author": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proceedings of the National Academy of Sciences, vol. 106, no. 45, pp. 18 914\u201318 919, 2009.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2009}, {"title": "Nesta: a fast and accurate first-order method for sparse recovery", "author": ["S. Becker", "J. Bobin", "E.J. Candes"], "venue": "SIAM Journal on Imaging Sciences, vol. 4, no. 1, pp. 1\u201339, 2011.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2011}, {"title": "Templates for convex cone problems with applications to sparse signal recovery", "author": ["S.R. Becker", "E.J. Candes", "M.C. Grant"], "venue": "Mathematical Programming Computation, vol. 3, no. 3, pp. 165\u2013218, 2011.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2011}, {"title": "A new approach to variable selection in least squares problems", "author": ["M.R. Osborne", "B. Presnell", "B.A. Turlach"], "venue": "IMA journal of numerical analysis, vol. 20, no. 3, pp. 389\u2013403, 2000.  JOURNAL  37", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2000}, {"title": "Recovery of sparse representations by polytope faces pursuit", "author": ["M.D. Plumbley"], "venue": "Independent Component Analysis and Blind Signal Separation, 2006, pp. 206\u2013 213.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast and accurate algorithms for re-weighted l1-norm minimization", "author": ["M.S. Asif", "J. Romberg"], "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 23, pp. 5905 \u2013 5916, 2012.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Homotopy continuation for sparse signal representation", "author": ["D.M. Malioutov", "M. Cetin", "A.S. Willsky"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP\u201905), vol. 5, 2005, pp. v733\u2013v736.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2005}, {"title": "An homotopy algorithm for the lasso with online observations", "author": ["P. Garrigues", "L.E. Ghaoui"], "venue": "Advances in neural information processing systems, 2009, pp. 489\u2013 496.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2009}, {"title": "Primal dual pursuit: A homotopy based algorithm for the dantzig selector", "author": ["M.S. Asif"], "venue": "Master\u2019s thesis, Georgia Institute of Technology, Atlanta, Georgia, USA, 2008.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2008}, {"title": "Dynamic updating for l1 minimization", "author": ["M.S. Asif", "J. Romberg"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 421\u2013434, 2010.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse recovery of streaming signals using l1homotopy", "author": ["\u2014\u2014"], "venue": "arXiv preprint arXiv:1306.3331, 2013.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic compressive sensing: Sparse recovery algorithms for streaming signals and video", "author": ["M.S. Asif"], "venue": "Ph.D. dissertation, Georgia Institute of Technology, Atlanta, Georgia, Unit State of America, 2013.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2013}, {"title": "An algorithm for the machine calculation of complex fourier series", "author": ["J. Cooley", "J. Tukey"], "venue": "Mathematics of computation, vol. 19, no. 90, pp. 297\u2013301, 1965.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 1965}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2010}, {"title": "From heuristic optimization to dictionary learning: a review and comprehensive comparison of image denoising algorithms", "author": ["L. Shao", "R. Yan", "X. Li", "Y. Liu"], "venue": "IEEE Transactions on Cybernetics, vol. 44, no. 7, pp. 1001\u20131013, 2014.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2014}, {"title": "Noise removal via bayesian wavelet coring", "author": ["E. Simoncelli", "E. Adelson"], "venue": "International Conference on Image Processing, vol. 1, 1996, pp. 379\u2013382.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 1996}, {"title": "Automatic fault feature extraction of mechanical anomaly on induction motor bearing using ensemble super-wavelet transform", "author": ["W. He", "Y. Zi", "B. Chen", "F. Wu", "Z. He"], "venue": "Mechanical Systems and Signal Processing, vol. 54, pp. 457\u2013480, 2015.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse geometric image representations with bandelets", "author": ["E.L. Pennec", "S. Mallat"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 4, pp. 423\u2013438, 2005.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2005}, {"title": "The curvelet transform for image denoising", "author": ["J. Starck", "E. Candes", "D. Donoho"], "venue": "Image Processing, IEEE Transactions on, vol. 11, no. 6, pp. 670\u2013684, 2002.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2002}, {"title": "The contourlet transform: an efficient directional multiresolution image representa-  tion", "author": ["M. Do", "M. Vetterli"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 12, pp. 2091\u20132106, 2005.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2005}, {"title": "Shiftable multiscale transforms", "author": ["E. Simoncelli", "W. Freeman", "E. Adelson", "D. Heeger"], "venue": "IEEE Transactions on Information Theory, vol. 38, no. 2, pp. 587\u2013607, 1992.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 1992}, {"title": "A nonconvex relaxation approach to sparse dictionary learning", "author": ["J. Shi", "X. Ren", "G. Dai", "J. Wang", "Z. Zhang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 1809\u20131816.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2011}, {"title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "author": ["J. Fan", "R. Li"], "venue": "Journal of the American Statistical Association, vol. 96, no. 456, pp. 1348\u20131360, 2001.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2001}, {"title": "Nearly unbiased variable selection under minimax concave penalty", "author": ["C.H. Zhang"], "venue": "The Annals of Statistics, pp. 894\u2013942, 2010.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast sparse regression and classification", "author": ["J.H. Friedman"], "venue": "International Journal of Forecasting, vol. 28, no. 3, pp. 722\u2013738, 2012.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2010}, {"title": "Dictionary learning", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27\u201338, 2011.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2011}, {"title": "l0 norm based dictionary learning by proximal methods with global convergence", "author": ["C. Bao", "H. Ji", "Y. Quan", "Z. Shen"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 3858\u20133865.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-frame compression: Theory and design", "author": ["K. Engan", "S.O. Aase", "J.H.Husy"], "venue": "Signal Processing, vol. 80, no. 10, pp. 2121\u20132140, 2000.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2000}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.W. Lee", "T.J. Sejnowski"], "venue": "Neural computation, vol. 15, no. 2, pp. 349\u2013396, 2003.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2003}, {"title": "k-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M.E.M. Aharon", "A. Bruckstein"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2006}, {"title": "Proximal methods for sparse hierarchical dictionary learning", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Proceedings of the 27th International Conference on Machine Learning, 2010, pp. 487\u2013494.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2010}, {"title": "Group sparse coding", "author": ["S. Bengio", "F. Pereira", "Y. Singer", "D. Strelow"], "venue": "Proceedings of the Advances in Neural Information Processing Systems, 2009, pp. 82\u2013 89.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2009}, {"title": "Online detection of unusual events in videos via dynamic sparse coding", "author": ["B. Zhao", "F. L", "E.P. Xing"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 3313\u20133320.  JOURNAL  38", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2042\u20132049.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminative k-svd for dictionary learning in face recognition", "author": ["Q. Zhang", "B. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 2691\u20132698.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2010}, {"title": "Tag localization with spatial correlations and joint group sparsity", "author": ["Y. Yang", "Y. Yang", "Z. Huang", "H.T. Shen", "F. Nie"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 881\u2013888.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2011}, {"title": "Compression of facial images using the k-svd algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation, vol. 19, no. 4, pp. 270\u2013282, 2008.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2008}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3360\u20133367.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130\u2013144, 2012.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2012}, {"title": "An mdl framework for sparse coding and dictionary learning", "author": ["I. Ramirez", "G. Sapiro"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2913\u20132927, 2012.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2012}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 19\u2013 60, 2010.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse variation dictionary learning for face recognition with a single sample per person", "author": ["M. Yang", "L. Van", "L. Zhang"], "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2013, pp. 689\u2013696.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent k-svd", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 1697\u20131704.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2011}, {"title": "Label consistent k-svd: learning a discriminative dictionary for recognition", "author": ["Z. Jiang", "Z. Lin", "L. Davis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 11, pp. 2651\u20132664, 2013.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2013}, {"title": "Fisher discrimination dictionary learning for sparse representation", "author": ["M. Yang", "D. Zhang", "X. Feng"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 543\u2013550.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative projection methods for structured sparsity regularization", "author": ["L. Rosasco", "A. Verri", "M. Santoro", "S. Mosci", "S. Villa"], "venue": "Massachusetts institute of technology, cambridge, MA, USA, Tech. Rep., 2009.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2009}, {"title": "Metaface learning for sparse representation based face recogni-  tion", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": "Proceedings of the IEEE International Conference on Image Processing (ICIP), 2010, pp. 1601\u20131604.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3501\u20133508.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2010}, {"title": "A dictionary learning approach for classification: separating the particularity and the commonality", "author": ["S. Kong", "D. Wang"], "venue": "European Conference on Computer Vision (ECCV), 2012, pp. 186\u2013199.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning categoryspecific dictionary and shared dictionary for finegrained image categorization", "author": ["S. Gao", "I.W. Tsang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 2, pp. 623\u2013634, 2013.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast dictionary learning for sparse representations of speech signals", "author": ["M.G. Jafari", "M.D. Plumbley"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 5, pp. 1025\u20131031, 2011.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proceedings of the Advances in neural information processing systems, 2009, pp. 1033\u20131040.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning inter-related visual dictionary for object recognition", "author": ["N. Zhou", "Y. Shen", "J. Peng", "J. Fan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3490\u20133497.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2012}, {"title": "Sparse representation for face recognition based on discriminative low-rank dictionary learning", "author": ["L. Ma", "C. Wang", "B. Xiao", "W. Zhou"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2586\u20132593.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2012}, {"title": "Simultaneous feature and dictionary learning for image set based face recognition", "author": ["J. Lu", "G. Wang", "W. Deng", "P. Moulin"], "venue": "European Conference on Computer Vision (ECCV), 2014, pp. 265\u2013280.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2014}, {"title": "Latent dictionary learning for sparse representation based classification", "author": ["M. Yang", "D. Dai", "L. Shen", "L.V. Gool"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 4138\u20134145.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2014}, {"title": "Submodular dictionary learning for sparse coding", "author": ["Z. Jiang", "G. Zhang", "L.S. Davis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3418\u20133425.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector guided dictionary learning", "author": ["S. Cai", "W. Zuo", "L. Zhang", "X. Feng", "P. Wang"], "venue": "European Conference on Computer Vision (ECCV), 2014, pp. 624\u2013639.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2014}, {"title": "A new sparse featurebased patch for dense correspondence", "author": ["X. Qin", "J. Shen", "X. Li", "Y. Jia"], "venue": "Proceedings of the IEEE International Conference on Multimedia and Expo (ICME), 2014, pp. 1\u20136.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse representation for blind image quality assessment", "author": ["L. He", "D. Tao", "X. Li", "X. Gao"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 1146\u20131153.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2012}, {"title": "Image  JOURNAL  39 super-resolution as sparse representation of raw image patches", "author": ["J. Yang", "J. Wright", "Y. Ma", "T. Huang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2008, pp. 1\u2013 8.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2008}, {"title": "Example-based super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer Graphics and Applications, vol. 22, no. 2, pp. 56\u201365, 2002.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2002}, {"title": "Motion analysis for image enhancement: Resolution, occlusion, and transparency", "author": ["M. Irani", "S. Peleg"], "venue": "Journal of Visual Communication and Image Representation, vol. 4, no. 4, pp. 324\u2013335, 1993.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 1993}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T.S. Huang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 11, pp. 2861\u2013 2873, 2010.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2010}, {"title": "Greedy regression in sparse coding space for single-image superresolution", "author": ["Y. Tang", "Y. Yuan", "P. Yan", "X. Li"], "venue": "Journal of visual communication and image representation, vol. 24, no. 2, pp. 148\u2013159, 2013.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2013}, {"title": "Image super-resolution via dual-dictionary learning and sparse representation", "author": ["J. Zhang", "C. Zhao", "R. Xiong", "S. Ma", "D. Zhao"], "venue": "Proceedings of the IEEE International Symposium on Circuits and Systems (IS- CAS), 2012, pp. 1688\u20131691.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2012}, {"title": "Image superresolution with sparse neighbor embedding", "author": ["X. Gao", "K. Zhang", "D. Tao", "X. Li"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 7, pp. 3194\u20133205, 2012.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2012}, {"title": "Superresolution via transform-invariant group-sparse regularization", "author": ["C. Fernandez-Granda", "E.J. Candes"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2013, pp. 3336\u20133343.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2013}, {"title": "Geometry constrained sparse coding for single image superresolution", "author": ["X. Lu", "H. Yuan", "P. Yan", "Y. Yuan", "X. Li"], "venue": "Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 1648\u20131655.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2012}, {"title": "Superresolution with nonlocal regularized sparse representation", "author": ["W. Dong", "G. Shi", "L. Zhang", "X. Wu"], "venue": "Proceedings of the Visual Communications and Image Processing, 2010, pp. 77 440H\u201377 440H.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2010}, {"title": "Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization", "author": ["W. Dong", "D. Zhang", "G. Shi"], "venue": "IEEE Transactions on Image Processing, vol. 20, no. 7, pp. 1838\u20131857, 2011.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 1838}, {"title": "Super-resolution with sparse mixing estimators", "author": ["S. Mallat", "G. Yu"], "venue": "Transactions on Image Processing, vol. 19, no. 11, pp. 2889\u20132900, 2010.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2010}, {"title": "De-noising by soft-thresholding", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 41, no. 3, pp. 613\u2013627, 1995.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 1995}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2006}, {"title": "Image denoising by sparse 3-d transform-domain col-  laborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 8, pp. 2080\u20132095, 2007.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse representation for color image restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Transactions on Image Processing, vol. 17, no. 1, pp. 53\u201369, 2008.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2008}, {"title": "Image sequence denoising via sparse and redundant representations", "author": ["M. Protter", "M. Elad"], "venue": "IEEE Transactions on Image Processing, vol. 18, no. 1, pp. 27\u201335, 2009.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparsity-based image denoising via dictionary learning and structural clustering", "author": ["W. Dong", "X. Li", "D. Zhang", "G. Shi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 457\u2013464.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2011}, {"title": "Mixed noise removal by weighted encoding with sparse nonlocal regularization", "author": ["J. Jiang", "L. Zhang", "J. Yang"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 6, pp. 2651\u20132662, 2014.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2014}, {"title": "Weighted nuclear norm minimization with application to image denoising", "author": ["S. Gu", "L. Zhang", "W. Zuo", "X. Feng"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust video denoising using low rank matrix completion", "author": ["H. Ji", "C. Liu", "Z. Shen", "Y. Xu"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 1791\u20131798.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2010}, {"title": "Image denoising via group sparse representation over learned dictionary", "author": ["P. Cheng", "C. Deng", "S. Wang", "C. Zhang"], "venue": "Proceedings of the Eighth International Symposium on Multispectral Image Processing and Pattern Recognition, 2013, pp. 891 916\u2013891 916.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2013}, {"title": "A new twist: two-step iterative shrinkage/thresholding algorithms for image restoration", "author": ["J.M. Bioucas-Dias", "M. Figueiredo"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 12, pp. 2992\u20133004, 2007.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning multiscale sparse representations for image and video restoration", "author": ["J. Mairal", "G. Sapiro", "M. Elad"], "venue": "Multiscale Modeling and Simulation, vol. 7, no. 1, pp. 214\u2013241, 2008.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-local sparse models for image restoration", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2009, pp. 2272\u20132279.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2009}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["D. Zoran", "Y. Weiss"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2011, pp. 479\u2013486.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast sparsity-based orthogonal dictionary learning for image restoration", "author": ["C. Bao", "J.F. Cai", "H. Ji"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 3384\u20133391.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2013}, {"title": "Group-based sparse representation for image restoration", "author": ["J. Zhang", "D. Zhao", "W. Gao"], "venue": "IEEE Transactions on Image Processing, vol. 34, no. 9, pp. 1864\u2013 1870, 2014.", "citeRegEx": "180", "shortCiteRegEx": null, "year": 1864}, {"title": "Centralized sparse representation for image restoration", "author": ["W. Dong", "D. Zhang", "G. Shi"], "venue": "Proceedings of the IEEE International Conference on Computer Vision,  JOURNAL  40 2011, pp. 1259\u20131266.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonlocally centralized sparse representation for image restoration", "author": ["W. Dong", "L. Zhang", "G. Shi", "X. Li"], "venue": "IEEE Transactions on Image Processing, vol. 22, no. 4, pp. 1620\u20131630, 2013.", "citeRegEx": "182", "shortCiteRegEx": null, "year": 2013}, {"title": "A non-local algorithm for image denoising", "author": ["A. Buades", "B. Coll", "J.M. Morel"], "venue": "Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, vol. 2, 2005, pp. 60\u201365.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex analysis and optimization", "author": ["A. Nedic", "D. Bertsekas", "A. Ozdaglar"], "venue": "Athena Scientific, 2003.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2003}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C.D. Mol"], "venue": "Communications on pure and applied mathematics, vol. 57, no. 11, pp. 1413\u20131457, 2004.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2004}, {"title": "Extended src: Undersampled face recognition via intraclass variant dictionary", "author": ["W. Deng", "J. Hu", "J. Guo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 9, pp. 1864\u20131870, 2012.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 1864}, {"title": "In defense of sparsity based face recognition", "author": ["\u2014\u2014"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 399\u2013406.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum correntropy criterion for robust face recognition", "author": ["R. He", "W.S. Zheng", "B.G. Hu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1561\u20131576, 2011.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust sparse coding for face recognition", "author": ["M. Yang", "D. Zhang", "J. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 625\u2013632.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 2011}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 1794\u20131801.", "citeRegEx": "190", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel sparse representation for image classification and face recognition", "author": ["S. Gao", "I.W.H. Tsang", "L.T. Chia"], "venue": "European Conference on Computer Vision (ECCV), 2010, pp. 1\u201314.", "citeRegEx": "191", "shortCiteRegEx": null, "year": 2010}, {"title": "Local features are not lonely-laplacian sparse coding for image classification", "author": ["S. Gao", "I.W. Tsang", "L.T. Chia", "P. Zhao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2010, pp. 3555\u20133561.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2010}, {"title": "Discriminative affine sparse codes for image classification", "author": ["N. Kulkarni", "B. Li"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1609\u20131616.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2011}, {"title": "Image classification by non-negative sparse coding, low-rank and sparse decomposition", "author": ["C. Zhang", "J. Liu", "Q. Tian", "C. Xu", "H. Lu", "S. Ma"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1673\u20131680.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2011}, {"title": "Low-rank sparse coding for image classification", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "C. Xu", "N. Ahuja"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 281\u2013288.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning structured low-rank representations for image classification", "author": ["Y. Zhang", "Z. Jiang", "L.S. Davis"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2013, pp. 676\u2013683.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2013}, {"title": "Rank preserving sparse learning for kinect based scene classification", "author": ["D. Tao", "L. Jin", "Y. Zhao", "X. Li"], "venue": "IEEE Transactions on Cybernetics, vol. 43, no. 5, pp. 1406\u20131417, 2013.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative tensor sparse coding for image classification", "author": ["Y. Zhang", "Z. Jiang", "L.S. Davis"], "venue": "Proceedings of the British Machine Vision Conference, 2013.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2013}, {"title": "Constructing a non-negative low rank and sparse graph with data-adaptive features", "author": ["L. Zhuang", "S. Gao", "J. Tang", "J. Wang", "Z. Lin", "Y. Ma"], "venue": "arXiv preprint arXiv:1409.0964, 2014.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 2014}, {"title": "On the relevance of sparsity for image classification", "author": ["R. Rigamonti", "V. Lepetit", "G. Gonzlez", "E. Turetken", "F. Benmansour", "M. Brown", "P. Fua"], "venue": "Computer Vision and Image Understanding, vol. 125, pp. 115\u2013 127, 2014.", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust visual tracking using l1 minimization", "author": ["X. Mei", "H. Ling"], "venue": "Proceedings of the IEEE 12th International Conference on Computer Vision, 2009, pp. 1436\u20131443.", "citeRegEx": "201", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust visual tracking and vehicle classification via sparse representation", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 11, pp. 2259\u20132272, 2011.", "citeRegEx": "202", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time visual tracking using compressive sensing", "author": ["H. Li", "C. Shen", "Q. Shi"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1305\u20131312.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust object tracking based on sparse representation", "author": ["S. Zhang", "H. Yao", "X. Sun", "S. Liu"], "venue": "Proceedings of the Visual Communications and Image Processing, 2010, pp. 77 441N\u201377 441N\u20138.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust visual tracking via multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 2042\u20132049.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.H. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 1822\u20131829.", "citeRegEx": "206", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust and fast collaborative tracking with two stage sparse optimization", "author": ["B. Liu", "L. Yang", "J. Huang", "P. Meer", "L. Gong", "C. Kulikowski"], "venue": "European Conference on Computer Vision (ECCV), 2010, pp. 624\u2013637.", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust tracking using local sparse appearance model and kselection", "author": ["B. Liu", "J. Huang", "C. Kulikowski", "L. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2011, pp. 1313\u20131320.", "citeRegEx": "208", "shortCiteRegEx": null, "year": 2011}, {"title": "Real time robust l1 tracker using accelerated proximal gradient approach", "author": ["C. Bao", "Y. Wu", "H. Ling", "H. Ji"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 1830\u20131837.  JOURNAL  41", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust object tracking via sparsity-based collaborative model", "author": ["W. Zhong", "H. Lu", "M.H. Yang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 1838\u20131845.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast compressive tracking", "author": ["K. Zhang", "L. Zhang", "M. Yang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 10, pp. 2002\u20132015, 2014.", "citeRegEx": "211", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust visual tracking with discriminative sparse learning", "author": ["X. Lu", "Y. Yuan", "P. Yan"], "venue": "Pattern Recognition, vol. 46, no. 7, pp. 1762\u20131771, 2013.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2013}, {"title": "Online robust non-negative dictionary learning for visual tracking", "author": ["N. Wang", "J. Wang", "D.Y. Yeung"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 657\u2013664.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse coding based visual tracking: Review and experimental comparison", "author": ["S. Zhang", "H. Yao", "X. Sun", "X. Lu"], "venue": "Pattern Recognition, vol. 46, no. 7, pp. 1772\u2013 1788, 2013.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual tracking: An experimental survey", "author": ["A. Smeulders", "D. Chu", "R. Cucchiara", "S. Calderara", "A. Dehghan", "M. Shah"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 7, pp. 1442\u2013 1468, 2013.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2013}, {"title": "Parameterisation of a stochastic model for human face identification", "author": ["F. Samaria", "A. Harter"], "venue": "Proceedings of the Second IEEE Workshop on Applications of Computer Vision, 1994, pp. 138\u2013142.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 1994}, {"title": "Labeled faces in the wild: A database for studying face recognition in unconstrained environments", "author": ["G. Huang", "M. Ramesh", "T. Berg", "E. Learned-Miller"], "venue": "Technical Report 07-49, University of Massachusetts, Amherst, Tech. Rep., 2007.", "citeRegEx": "217", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse tensor discriminant color space for face verification", "author": ["S. Wang", "J. Yang", "M. Sun", "X. Peng", "M. Sun", "C. Zhou"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 6, pp. 876\u2013 888, 2012.", "citeRegEx": "218", "shortCiteRegEx": null, "year": 2012}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 643\u2013660, 2001.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2001}, {"title": "Columbia object image library (coil-20)", "author": ["S. Nene", "S. Nayar", "H. Murase"], "venue": "Technical Report CUCS-005- 96, Tech. Rep., 1996.", "citeRegEx": "220", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "W ITH advancements in mathematics, linear representation methods (LRBM) have been well studied and have recently received considerable attention [1, 2].", "startOffset": 145, "endOffset": 151}, {"referenceID": 1, "context": "W ITH advancements in mathematics, linear representation methods (LRBM) have been well studied and have recently received considerable attention [1, 2].", "startOffset": 145, "endOffset": 151}, {"referenceID": 2, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 3, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 4, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 5, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 6, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 7, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 8, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 9, "context": "learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmentation [3\u201310].", "startOffset": 177, "endOffset": 183}, {"referenceID": 10, "context": "Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years.", "startOffset": 104, "endOffset": 111}, {"referenceID": 11, "context": "Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years.", "startOffset": 104, "endOffset": 111}, {"referenceID": 12, "context": "Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11\u201313], which is one of the most popular topics in recent years.", "startOffset": 104, "endOffset": 111}, {"referenceID": 10, "context": "Donoho [11] first proposed the original concept of compressed sensing.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "[13], from the mathematical perspective, demonstrated the rationale of CS theory, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Baraniuk [12] provided a concrete analysis of compressed sensing and presented a specific interpretation on some solutions of different signal reconstruction algorithms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 12, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 13, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 14, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 15, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 16, "context": "All these literature [11\u201317] laid the foundation of CS theory and provided the theoretical basis for future research.", "startOffset": 21, "endOffset": 28}, {"referenceID": 3, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 6, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 7, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 8, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 9, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 16, "context": "As an indispensable prerequisite of CS theory, the sparse representation theory [4, 7\u201310, 17] is the most outstanding technique used to conquer difficulties that appear in many fields.", "startOffset": 80, "endOffset": 93}, {"referenceID": 3, "context": "For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].", "startOffset": 181, "endOffset": 186}, {"referenceID": 4, "context": "For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].", "startOffset": 181, "endOffset": 186}, {"referenceID": 5, "context": "For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4\u20136].", "startOffset": 181, "endOffset": 186}, {"referenceID": 17, "context": "Sparse representation has attracted much attention in recent years and many examples in different fields can be found where sparse representation is definitely beneficial and favorable [18, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 18, "context": "Sparse representation has attracted much attention in recent years and many examples in different fields can be found where sparse representation is definitely beneficial and favorable [18, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 19, "context": "The sparse representation based classification (SRC) method [20] first assumes that the test sample can be sufficiently represented by samples from the same subject.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "The literature [20] has also demonstrated that the SRC method has great superiorities when addressing the image classification issue on corrupted or disguised images.", "startOffset": 15, "endOffset": 19}, {"referenceID": 3, "context": "In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7].", "startOffset": 121, "endOffset": 130}, {"referenceID": 5, "context": "In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7].", "startOffset": 121, "endOffset": 130}, {"referenceID": 6, "context": "In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4, 6, 7].", "startOffset": 121, "endOffset": 130}, {"referenceID": 20, "context": "Moreover, in the field of image classification, the representation based classification methods consist of two main categories in terms of the way of exploiting the \u201catoms\u201d: the holistic representation based method and local representation based method [21].", "startOffset": 253, "endOffset": 257}, {"referenceID": 8, "context": "A typical and representative local sparse representation methods is the two-phase test sample sparse representation (TPTSR) method [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 21, "context": "The literature [22] suggests that sparse representation algorithms roughly fall into three classes: convex relaxation, greedy algorithms, and combinational methods.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In the literature [23, 24], from the perspective of sparse problem modeling and problem solving, sparse decomposition algorithms are generally divided into two sections: greedy algorithms and convex relaxation algorithms.", "startOffset": 18, "endOffset": 26}, {"referenceID": 23, "context": "In the literature [23, 24], from the perspective of sparse problem modeling and problem solving, sparse decomposition algorithms are generally divided into two sections: greedy algorithms and convex relaxation algorithms.", "startOffset": 18, "endOffset": 26}, {"referenceID": 24, "context": "[25] reviewed some optimization techniques for solving l1norm regularization problems and roughly divided these approaches into three optimization strategies: sub-gradient methods, unconstrained approximation methods, and constrained optimization methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Because of the fact that this problem is an NP-hard problem [26], the greedy strategy provides an approximate solution to alleviate this difficulty.", "startOffset": 60, "endOffset": 64}, {"referenceID": 26, "context": "The greedy strategy searches for the best local optimal solution in each iteration with the goal of achieving the optimal holistic solution [27].", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "(3) Proximal algorithms can be treated as a powerful tool for solving nonsmooth, constrained, large-scale, or distributed versions of the optimization problem [28].", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "2: Geometric interpretations of different norms in 2-D space [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Actually, the l0-norm is the limit as p \u2192 0 of the lp-norms [8] and the definition of the l0-norm is formulated as", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "4: The geometry of the solutions of different norm regularization in 2-D space [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "3: Geometric interpretations of different norms in 1-D space [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 28, "context": "However, it has been demonstrated that the representation solution of the l2-norm minimization is not strictly sparse enough but \u201climitedly-sparse\u201d, which means it possesses the capability of discriminability [30].", "startOffset": 209, "endOffset": 213}, {"referenceID": 29, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 30, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 31, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 32, "context": "The representation results in sparse representation, however, can be greatly dominated by the regularizer (or optimizer) imposed on the representation solution [32\u201335].", "startOffset": 160, "endOffset": 167}, {"referenceID": 33, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 197, "endOffset": 205}, {"referenceID": 34, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 197, "endOffset": 205}, {"referenceID": 37, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 267, "endOffset": 275}, {"referenceID": 41, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 329, "endOffset": 337}, {"referenceID": 42, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 43, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 44, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 45, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 46, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 393, "endOffset": 400}, {"referenceID": 8, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 454, "endOffset": 465}, {"referenceID": 47, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 454, "endOffset": 465}, {"referenceID": 48, "context": "Thus, in terms of the different norms used in optimizers, the sparse representation methods can be roughly grouped into five general categories: sparse representation with the l0-norm minimization [36, 37], sparse representation with the lp-norm (0<p<1) minimization [38\u2013 40], sparse representation with the l1-norm minimization [41\u2013 44], sparse representation with the l2,1-norm minimization [45\u201349], sparse representation with the l2-norm minimization [9, 50, 51].", "startOffset": 454, "endOffset": 465}, {"referenceID": 49, "context": "2 with the l0norm minimization constraint [52].", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 46, "endOffset": 54}, {"referenceID": 39, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 46, "endOffset": 54}, {"referenceID": 50, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 163, "endOffset": 170}, {"referenceID": 51, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 163, "endOffset": 170}, {"referenceID": 52, "context": "The l1-norm originates from the Lasso problem [41, 42] and it has been extensively used to address issues in machine learning, pattern recognition, and statistics [53\u201355].", "startOffset": 163, "endOffset": 170}, {"referenceID": 25, "context": "Although the sparse representation method with l0-norm minimization can obtain the fundamental sparse solution of \u03b1 over the matrix X , the problem is still a non-deterministic polynomial-time hard (NP-hard) problem and the solution is difficult to approximate [26].", "startOffset": 261, "endOffset": 265}, {"referenceID": 19, "context": "Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability.", "startOffset": 18, "endOffset": 29}, {"referenceID": 53, "context": "Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability.", "startOffset": 18, "endOffset": 29}, {"referenceID": 54, "context": "Recent literature [20, 56\u201358] has demonstrated that when the representation solution obtained by using the l1-norm minimization constraint is also content with the condition of sparsity and the solution using l1-norm minimization with sufficient sparsity can be equivalent to the solution obtained by l0-norm minimization with full probability.", "startOffset": 18, "endOffset": 29}, {"referenceID": 55, "context": "9 [59\u201361].", "startOffset": 2, "endOffset": 9}, {"referenceID": 56, "context": "9 [59\u201361].", "startOffset": 2, "endOffset": 9}, {"referenceID": 57, "context": "9 [59\u201361].", "startOffset": 2, "endOffset": 9}, {"referenceID": 28, "context": "the solution has the property that it is discriminative and distinguishable but is not really sparse enough [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 58, "context": "16) On the other hand, the l2,1-norm is also called the rotation invariant l1-norm, which is proposed to overcome the difficulty of robustness to outliers [62].", "startOffset": 155, "endOffset": 159}, {"referenceID": 42, "context": "Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347].", "startOffset": 125, "endOffset": 132}, {"referenceID": 43, "context": "Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347].", "startOffset": 125, "endOffset": 132}, {"referenceID": 44, "context": "Sparse representation with the l2,1-norm minimization can be implemented by exploiting the proposed algorithms in literature [45\u201347].", "startOffset": 125, "endOffset": 132}, {"referenceID": 6, "context": "The core idea of the greedy strategy [7, 23] is to determine the position based on the relationship between the atom and probe sample, and then to use the least square to evaluate the amplitude value.", "startOffset": 37, "endOffset": 44}, {"referenceID": 22, "context": "The core idea of the greedy strategy [7, 23] is to determine the position based on the relationship between the atom and probe sample, and then to use the least square to evaluate the amplitude value.", "startOffset": 37, "endOffset": 44}, {"referenceID": 6, "context": "However, the greedy algorithm can always produce the global optimal solution or an approximate overall solution [7, 23].", "startOffset": 112, "endOffset": 119}, {"referenceID": 22, "context": "However, the greedy algorithm can always produce the global optimal solution or an approximate overall solution [7, 23].", "startOffset": 112, "endOffset": 119}, {"referenceID": 19, "context": "3, is an NP hard problem [20, 56].", "startOffset": 25, "endOffset": 33}, {"referenceID": 53, "context": "3, is an NP hard problem [20, 56].", "startOffset": 25, "endOffset": 33}, {"referenceID": 59, "context": "The matching pursuit (MP) algorithm [63] is the earliest and representative method of using the greedy strategy to approximate problem III.", "startOffset": 36, "endOffset": 40}, {"referenceID": 59, "context": "More detailed analysis on matching pursuit algorithms can be found in the literature [63].", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "The orthogonal matching pursuit (OMP) algorithm [36, 64] is an improvement of the MP algorithm.", "startOffset": 48, "endOffset": 56}, {"referenceID": 60, "context": "The orthogonal matching pursuit (OMP) algorithm [36, 64] is an improvement of the MP algorithm.", "startOffset": 48, "endOffset": 56}, {"referenceID": 33, "context": "It has been verified that the OMP algorithm can be converged in limited iterations [36].", "startOffset": 83, "endOffset": 87}, {"referenceID": 61, "context": "Many more greedy algorithms based on the MP and OMP algorithm such as the efficient orthogonal matching pursuit algorithm [65] subsequently have been proposed to improve the pursuit algorithm.", "startOffset": 122, "endOffset": 126}, {"referenceID": 34, "context": "proposed an regularized version of orthogonal matching pursuit (ROMP) algorithm [37], which recovered all k sparse signals based on the Restricted Isometry Property of random frequency measurements, and then proposed another variant of OMP algorithm called compressive sampling matching pursuit (CoSaMP) algorithm [66], which incorporated several existing ideas such as restricted isometry property (RIP) and pruning technique into a greedy iterative structure of OMP.", "startOffset": 80, "endOffset": 84}, {"referenceID": 62, "context": "proposed an regularized version of orthogonal matching pursuit (ROMP) algorithm [37], which recovered all k sparse signals based on the Restricted Isometry Property of random frequency measurements, and then proposed another variant of OMP algorithm called compressive sampling matching pursuit (CoSaMP) algorithm [66], which incorporated several existing ideas such as restricted isometry property (RIP) and pruning technique into a greedy iterative structure of OMP.", "startOffset": 314, "endOffset": 318}, {"referenceID": 63, "context": "proposed an extension of OMP, called stage-wise orthogonal matching pursuit (StOMP) algorithm [67], which depicted an iterative algorithm with three main steps, i.", "startOffset": 94, "endOffset": 98}, {"referenceID": 64, "context": "Dai and Milenkovic proposed a new method for sparse signal reconstruction named subspace pursuit (SP) algorithm [68], which sampled signals satisfying the constraints of the RIP with a constant parameter.", "startOffset": 112, "endOffset": 116}, {"referenceID": 65, "context": "presented a sparsity adaptive matching pursuit (SAMP) algorithm [69], which borrowed the idea of the EM algorithm to alternatively estimate the sparsity and support set.", "startOffset": 64, "endOffset": 68}, {"referenceID": 66, "context": "proposed a tree-based matching pursuit (TMP) algorithm [70], which constructed a tree structure and employed a structuring strategy to cluster similar signal atoms from a highly redundant dictionary as a new dictionary.", "startOffset": 55, "endOffset": 59}, {"referenceID": 67, "context": "Subsequently, La and Do proposed a new tree-based orthogonal matching pursuit (TBOMP) algorithm [71], which treated the sparse tree representation as an additional prior knowledge for linear inverse systems by using a small number of samples.", "startOffset": 96, "endOffset": 100}, {"referenceID": 68, "context": "Recently, Karahanoglu and Erdogan conceived a forward-backward pursuit (FBP) method [72] with two greedy stages, in which the forward stage enlarged the support estimation and the backward stage removed some unsatisfied atoms.", "startOffset": 84, "endOffset": 88}, {"referenceID": 22, "context": "More detailed treatments of the greedy pursuit for sparse representation can be found in the literature [23].", "startOffset": 104, "endOffset": 108}, {"referenceID": 69, "context": "The first key procedure of gradient projection sparse reconstruction (GPSR) [73] provides a constrained formulation where each value of \u03b1 can be split into its positive and negative parts.", "startOffset": 76, "endOffset": 80}, {"referenceID": 69, "context": "For more detailed information, one can refer to the literature [73].", "startOffset": 63, "endOffset": 67}, {"referenceID": 27, "context": "The Interior-point method [31] is not an iterative algorithm but a smooth mathematic model and it always incorporates the Newton method to efficiently solve unconstrained smooth problems of modest size [28].", "startOffset": 202, "endOffset": 206}, {"referenceID": 70, "context": "l1 ls) problem [74].", "startOffset": 15, "endOffset": 19}, {"referenceID": 70, "context": "For further description and analyses, please refer to the literature [74].", "startOffset": 69, "endOffset": 73}, {"referenceID": 71, "context": "[75] is a very effective method to solve the l1-norm regularization problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 72, "context": "[76] also utilized the TNIPM to solve large scale logistic regression problems, which employed a preconditioned conjugate gradient method to compute the search step size with warm-start techniques.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Mehrotra proposed to exploit the interior-point method to address the primal-dual problem [77] and introduced the second-order derivation of Taylor polynomial to approximate a primal-dual trajectory.", "startOffset": 90, "endOffset": 94}, {"referenceID": 74, "context": "More analyses of interior-point method for sparse representation can be found in the literature [78].", "startOffset": 96, "endOffset": 100}, {"referenceID": 40, "context": "This section shows how the ADM [43] is used to solve primal and dual problems in III.", "startOffset": 31, "endOffset": 35}, {"referenceID": 40, "context": "For more information, please refer to the literature [43, 79].", "startOffset": 53, "endOffset": 61}, {"referenceID": 75, "context": "For more information, please refer to the literature [43, 79].", "startOffset": 53, "endOffset": 61}, {"referenceID": 27, "context": "The proximity algorithm is frequently employed to solve nonsmooth, constrained convex optimization problems [28].", "startOffset": 108, "endOffset": 112}, {"referenceID": 76, "context": "The objective function of ISTA [80] has the form of", "startOffset": 31, "endOffset": 35}, {"referenceID": 76, "context": "The techniques used here are called linearization or preconditioning and more detailed information can be found in the literature [80, 81].", "startOffset": 130, "endOffset": 138}, {"referenceID": 77, "context": "The techniques used here are called linearization or preconditioning and more detailed information can be found in the literature [80, 81].", "startOffset": 130, "endOffset": 138}, {"referenceID": 78, "context": "FISTA [82] not only preserves the efficiency of the original ISTA but also promotes the effectiveness of ISTA so that FISTA can obtain global convergence.", "startOffset": 6, "endOffset": 10}, {"referenceID": 78, "context": "The backtracking linear research strategy can also be utilized to explore a more feasible value of L and more detailed analyses on FISTA can be found in the literature [82, 83].", "startOffset": 168, "endOffset": 176}, {"referenceID": 79, "context": "The backtracking linear research strategy can also be utilized to explore a more feasible value of L and more detailed analyses on FISTA can be found in the literature [82, 83].", "startOffset": 168, "endOffset": 176}, {"referenceID": 80, "context": "Sparse reconstruction by separable approximation (SpaRSA) [84] is another typical proximity algorithm based on sparse representation, which can be viewed as an accelerated version of ISTA.", "startOffset": 58, "endOffset": 62}, {"referenceID": 81, "context": "9 using the Barzilai-Borwein (BB) spectral method [85].", "startOffset": 50, "endOffset": 54}, {"referenceID": 82, "context": "[86] concluded that the technique that exploits a decreasing value of \u03bb from a warm-starting point can more efficiently solve the sub-problem VI.", "startOffset": 0, "endOffset": 4}, {"referenceID": 80, "context": "The sparse reconstruction by separable approximation (SpaRSA) is summarized in Algorithm 6 and more information can be found in the literature [84].", "startOffset": 143, "endOffset": 147}, {"referenceID": 83, "context": "However, the research group led by Zongben Xu summarizes the conclusion that the most impressive and representative algorithm of the lp-norm (0<p<1) regularization is sparse representation with the l1/2-norm regularization [87].", "startOffset": 223, "endOffset": 227}, {"referenceID": 56, "context": "Moreover, they have proposed some effective methods to solve the l1/2norm regularization problem [60, 88].", "startOffset": 97, "endOffset": 105}, {"referenceID": 84, "context": "Moreover, they have proposed some effective methods to solve the l1/2norm regularization problem [60, 88].", "startOffset": 97, "endOffset": 105}, {"referenceID": 56, "context": "In this section, a half proximal algorithm is introduced to solve the l1/2-norm regularization problem [60], which matches the iterative shrinkage thresholding algorithm for the l1-norm regularization discussed above and the iterative hard thresholding algorithm for the l0-norm regularization.", "startOffset": 103, "endOffset": 107}, {"referenceID": 56, "context": "To this end, the resolvent operator [60] is introduced to compute the resolvent solution of the right part of Eq.", "startOffset": 36, "endOffset": 40}, {"referenceID": 56, "context": "The resolvent operator is always satisfied no matter whether the resolvent solution of \u2207(\u2016 \u2022 \u2016 1/2) exists or not [60].", "startOffset": 114, "endOffset": 118}, {"referenceID": 56, "context": "which have been demonstrated in the literature [60].", "startOffset": 47, "endOffset": 51}, {"referenceID": 56, "context": "3 has been conceived and demonstrated in the literature [60].", "startOffset": 56, "endOffset": 60}, {"referenceID": 56, "context": "where the half proximal thresholding operator H [60] is deductively constituted by Eq.", "startOffset": 48, "endOffset": 52}, {"referenceID": 56, "context": "The half proximal thresholding algorithm for l1/2-norm regularization based sparse representation is summarized in Algorithm 7 and more detailed inferences and analyses can be found in the literature [60, 88].", "startOffset": 200, "endOffset": 208}, {"referenceID": 84, "context": "The half proximal thresholding algorithm for l1/2-norm regularization based sparse representation is summarized in Algorithm 7 and more detailed inferences and analyses can be found in the literature [60, 88].", "startOffset": 200, "endOffset": 208}, {"referenceID": 85, "context": "33 is denoted as the primal augmented Lagrangian method (PALM) [89], the dual function of problem III.", "startOffset": 63, "endOffset": 67}, {"referenceID": 85, "context": "9 can also be addressed by the ALM algorithm, which is denoted as the dual augmented Lagrangian method (DALM) [89].", "startOffset": 110, "endOffset": 114}, {"referenceID": 85, "context": "For more detailed description, please refer to the literature [89].", "startOffset": 62, "endOffset": 66}, {"referenceID": 86, "context": "proposed an iterative method named parallel coordinate descent algorithm (PCDA) [90] by introducing the elementwise optimization algorithm to solve the regularized linear least squares with non-quadratic regularization problem.", "startOffset": 80, "endOffset": 84}, {"referenceID": 87, "context": "developed a modified version of the iterative thresholding method, called approximate message passing (AMP) method [91], to satisfy the requirement that the sparsity undersampling tradeoff of the new algorithm is equivalent to the corresponding convex optimization approach.", "startOffset": 115, "endOffset": 119}, {"referenceID": 88, "context": "proposed a generalized Nesterov\u2019s algorithm (NESTA) [92] by employing the continuation-like scheme to accelerate the efficiency and flexibility.", "startOffset": 52, "endOffset": 56}, {"referenceID": 89, "context": "[93] further constructed a general framework, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Further detailed analyses and inference information related to proximity algorithms can be found in the literature [28, 83].", "startOffset": 115, "endOffset": 123}, {"referenceID": 79, "context": "Further detailed analyses and inference information related to proximity algorithms can be found in the literature [28, 83].", "startOffset": 115, "endOffset": 123}, {"referenceID": 90, "context": "The homotopy method was originally proposed to solve the least square problem with the l1-penalty [94].", "startOffset": 98, "endOffset": 102}, {"referenceID": 39, "context": "Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems.", "startOffset": 126, "endOffset": 130}, {"referenceID": 60, "context": "Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems.", "startOffset": 136, "endOffset": 140}, {"referenceID": 91, "context": "Having a highly intimate relationship with the conventional sparse representation method such as least angle regression (LAR) [42], OMP [64] and polytope faces pursuit (PFP) [95], the homotopy algorithm has been successfully employed to solve the l1-norm minimization problems.", "startOffset": 174, "endOffset": 178}, {"referenceID": 92, "context": "The fundamental of the homotopy algorithm is that the homotopy solution path is a piecewise linear path with a discrete number of operations while the value of the homotopy parameter changes, and the direction of each segment and the step size are absolutely determined by the sign sequence and the support of the solution on the corresponding segment, respectively [96].", "startOffset": 366, "endOffset": 370}, {"referenceID": 92, "context": "For further description and analyses, please refer to the literature [29, 96].", "startOffset": 69, "endOffset": 77}, {"referenceID": 39, "context": "For further detail description and analyses, please refer to the literature [42].", "startOffset": 76, "endOffset": 80}, {"referenceID": 92, "context": "Based on the homotopy algorithm, Asif and Romberg [96] presented a enhanced sparse representation objective function, a weighted l1-norm minimization, and then provided two fast and accurate solutions, i.", "startOffset": 50, "endOffset": 54}, {"referenceID": 39, "context": "A common method [42, 73] to update the weight W is achieved by exploiting the solution of problem VII.", "startOffset": 16, "endOffset": 24}, {"referenceID": 69, "context": "A common method [42, 73] to update the weight W is achieved by exploiting the solution of problem VII.", "startOffset": 16, "endOffset": 24}, {"referenceID": 92, "context": "The main steps of this algorithm are summarized in Algorithm 11 and more information can be found in literature [96].", "startOffset": 112, "endOffset": 116}, {"referenceID": 93, "context": "first exploited the homotopy method to choose a suitable parameter for l1-norm regularization with a noisy term in an underdetermined system and employed the homotopy continuation-based method to solve BPDN for sparse signal processing [97].", "startOffset": 236, "endOffset": 240}, {"referenceID": 94, "context": "Garrigues and Ghaoui [98] proposed a modified homotopy algorithm to solve the Lasso problem with online observations by optimizing the", "startOffset": 21, "endOffset": 25}, {"referenceID": 39, "context": "[42] proposed a basic pursuit denoising (BPDN) homotopy algorithm, which shrinked the parameter to a final value with series of efficient optimization steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 95, "context": "Similar to BPDN homotopy, Asif [99] presented a homotopy algorithm for the Dantzing selector (DS) under the consideration of primal and dual solution.", "startOffset": 31, "endOffset": 35}, {"referenceID": 96, "context": "Asif and Romberg [100] proposed a framework of dynamic updating solutions for solving l1-norm minimization programs based on homotopy algorithm and demonstrated its effectiveness in addressing the decoding issue.", "startOffset": 17, "endOffset": 22}, {"referenceID": 97, "context": "More recent literature related to homotopy algorithms can be found in the streaming recovery framework [101] and a summary [102].", "startOffset": 103, "endOffset": 108}, {"referenceID": 98, "context": "More recent literature related to homotopy algorithms can be found in the streaming recovery framework [101] and a summary [102].", "startOffset": 123, "endOffset": 128}, {"referenceID": 99, "context": "The history of modeling dictionary could be traced back to 1960s, such as the fast Fourier transform (FFT) [103].", "startOffset": 107, "endOffset": 112}, {"referenceID": 4, "context": "transform domain method [5], or is devised based on learning, i.", "startOffset": 24, "endOffset": 27}, {"referenceID": 100, "context": "dictionary learning methods [104].", "startOffset": 28, "endOffset": 33}, {"referenceID": 101, "context": "Both of the transform domain and dictionary learning based methods transform image samples into other domains and the similarity of transformation coefficients are exploited [105].", "startOffset": 174, "endOffset": 179}, {"referenceID": 102, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 151, "endOffset": 156}, {"referenceID": 103, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 182, "endOffset": 187}, {"referenceID": 104, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 199, "endOffset": 204}, {"referenceID": 105, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 226, "endOffset": 231}, {"referenceID": 106, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 255, "endOffset": 260}, {"referenceID": 107, "context": "Specifically, the transform domain methods usually represent the image patches by using the orthonormal basis such as over-complete wavelets transform [106], super-wavelet transform [107], bandelets [108], curvelets transform [109], contourlets transform [110] and steerable wavelet filters [111].", "startOffset": 291, "endOffset": 296}, {"referenceID": 21, "context": "From the notations of the literature [22, 112], the framework of dictionary learning can be generally formulated as an optimization problem", "startOffset": 37, "endOffset": 46}, {"referenceID": 108, "context": "From the notations of the literature [22, 112], the framework of dictionary learning can be generally formulated as an optimization problem", "startOffset": 37, "endOffset": 46}, {"referenceID": 7, "context": "The most representative dictionary learning based on the l0-norm penalty is the K-SVD algorithm [8], which is widely used in image denoising.", "startOffset": 96, "endOffset": 99}, {"referenceID": 108, "context": "In the stage of convex relaxation methods, there are three optimal forms for updating a dictionary: the one by one atom updating method, group atoms updating method, and all atoms updating method [112].", "startOffset": 196, "endOffset": 201}, {"referenceID": 109, "context": "For example, Fan and Li proposed a smoothly clipped absolution deviation (SCAD) penalty [113], which employed an iterative approximate Newton-Raphson method for penalizing least sequences and exploited the penalized likelihood approaches for variable selection in linear regression models.", "startOffset": 88, "endOffset": 93}, {"referenceID": 110, "context": "Zhang introduced and studied the non-convex minimax concave (MC) family [114] of non-convex piecewise quadratic penalties to make unbiased variable selection for the estimation of regression coefficients, which was demonstrated its effectiveness by employing an oracle inequality.", "startOffset": 72, "endOffset": 77}, {"referenceID": 111, "context": "Friedman proposed to use the logarithmic penalty for a model selection [115] and used it to solve the minimization problems with non-convex regularization terms.", "startOffset": 71, "endOffset": 76}, {"referenceID": 112, "context": "From the viewpoint of updating strategy, most of the dictionary learning methods always iteratively update the sparse approximation or representation solution and the dictionary alternatively, and more dictionary learning theoretical results and analyses can be found in the literature [116, 117].", "startOffset": 286, "endOffset": 296}, {"referenceID": 113, "context": "From the viewpoint of updating strategy, most of the dictionary learning methods always iteratively update the sparse approximation or representation solution and the dictionary alternatively, and more dictionary learning theoretical results and analyses can be found in the literature [116, 117].", "startOffset": 286, "endOffset": 296}, {"referenceID": 114, "context": "For example, dictionary learning methods can be divided into three groups in the context of different norms utilized in the penalty term, that is, l0-norm regularization based methods, convex relaxation methods and non-convex relaxation methods [118].", "startOffset": 245, "endOffset": 250}, {"referenceID": 115, "context": "The first category is dictionary learning under the probabilistic framework such as maximum likelihood methods [119], the method of optimal directions (MOD) [120], and the maximum a posteriori probability method [121].", "startOffset": 157, "endOffset": 162}, {"referenceID": 116, "context": "The first category is dictionary learning under the probabilistic framework such as maximum likelihood methods [119], the method of optimal directions (MOD) [120], and the maximum a posteriori probability method [121].", "startOffset": 212, "endOffset": 217}, {"referenceID": 117, "context": "The second category is clustering based dictionary learning approaches such as KSVD [122], which can be viewed as a generalization of K-means.", "startOffset": 84, "endOffset": 89}, {"referenceID": 118, "context": "There are two typical models for these kinds of dictionary learning algorithms, sparse and shift-invariant representation of dictionary learning and structure sparse regularization based dictionary learning, such as hierarchical sparse dictionary learning [123] and group or block sparse dictionary learning [124].", "startOffset": 256, "endOffset": 261}, {"referenceID": 119, "context": "There are two typical models for these kinds of dictionary learning algorithms, sparse and shift-invariant representation of dictionary learning and structure sparse regularization based dictionary learning, such as hierarchical sparse dictionary learning [123] and group or block sparse dictionary learning [124].", "startOffset": 308, "endOffset": 313}, {"referenceID": 21, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 27, "endOffset": 31}, {"referenceID": 120, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 131, "endOffset": 136}, {"referenceID": 121, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 164, "endOffset": 169}, {"referenceID": 122, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 206, "endOffset": 211}, {"referenceID": 123, "context": "Recently, some researchers [22] categorized the latest methods of dictionary learning into four groups, online dictionary learning [125], joint dictionary learning [126], discriminative dictionary learning [127], and supervised dictionary learning [128].", "startOffset": 248, "endOffset": 253}, {"referenceID": 124, "context": "Unsupervised dictionary learning methods have been widely implemented to solve image processing problems, such as image compression, and feature coding of image representation [129, 130].", "startOffset": 176, "endOffset": 186}, {"referenceID": 125, "context": "Unsupervised dictionary learning methods have been widely implemented to solve image processing problems, such as image compression, and feature coding of image representation [129, 130].", "startOffset": 176, "endOffset": 186}, {"referenceID": 117, "context": "(1) KSVD for unsupervised dictionary learning One of the most representative unsupervised dictionary learning algorithms is the KSVD method [122], which is a modification or an extension of method of directions (MOD) algorithm.", "startOffset": 140, "endOffset": 145}, {"referenceID": 117, "context": "The specific KSVD algorithm for dictionary learning is summarized to Algorithm 12 and more information can be found in the literature [122].", "startOffset": 134, "endOffset": 139}, {"referenceID": 125, "context": "(2) Locality constrained linear coding for unsupervised dictionary learning The locality constrained linear coding (LLC) algorithm [130] is an efficient local coordinate linear coding method, which projects each descriptor into a local constraint system to obtain an effective codebook or dictionary.", "startOffset": 131, "endOffset": 136}, {"referenceID": 125, "context": "It has been demonstrated that the property of locality is more essential than sparsity, because the locality must lead to sparsity but not vice-versa, that is, a necessary condition of sparsity is locality, but not the reverse [130].", "startOffset": 227, "endOffset": 232}, {"referenceID": 125, "context": "The main steps of the incremental codebook optimization algorithm are summarized in Algorithm 13 and more information can be found in the literature [130].", "startOffset": 149, "endOffset": 154}, {"referenceID": 118, "context": "[123] proposed a tree-structured dictionary learning problem, which exploited tree-structured sparse regularization to model the relationship between each atom and defined a proximal operator to solve the primaldual problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 126, "context": "[131] developed a nonparametric Bayesian dictionary learning algorithm, which utilized hierarchical Bayesian to model parameters and employed the truncated beta-Bernoulli process to learn the dictionary.", "startOffset": 0, "endOffset": 5}, {"referenceID": 127, "context": "Ramirez and Shapiro [132] employed minimum description length to", "startOffset": 20, "endOffset": 25}, {"referenceID": 128, "context": "proposed an online dictionary learning [133] algorithm based on stochastic approximations, which treated the dictionary learning problem as the optimization of a smooth convex problem over a convex set and employed an iterative online algorithm at each step to solve the subproblems.", "startOffset": 39, "endOffset": 44}, {"referenceID": 129, "context": "Yang and Zhang proposed a sparse variation dictionary learning (SVDL) algorithm [134] for face recognition with a single training sample, in which a joint learning framework of adaptive projection and a sparse variation dictionary with sparse bases were simultaneously constructed from the gallery image set to the generic image set.", "startOffset": 80, "endOffset": 85}, {"referenceID": 108, "context": "proposed a minimax concave penalty based sparse dictionary learning (MCPSDL) [112] algorithm, which employed a non-convex relaxation online", "startOffset": 77, "endOffset": 82}, {"referenceID": 114, "context": "proposed a dictionary learning by proximal algorithm (DLPM) [118], which provided an efficient alternating proximal algorithm for solving the l0-norm minimization based dictionary learning problem and demonstrated its global convergence property.", "startOffset": 60, "endOffset": 65}, {"referenceID": 122, "context": "Discriminative KSVD (DKSVD) [127] was designed to solve image classification problems.", "startOffset": 28, "endOffset": 33}, {"referenceID": 117, "context": "13 is the same as the framework of KSVD [122] in Eq.", "startOffset": 40, "endOffset": 45}, {"referenceID": 19, "context": "The original sparse representation for face recognition [20] regards the raw data as the dictionary, and then reports its promising classification results.", "startOffset": 56, "endOffset": 60}, {"referenceID": 130, "context": "In this section, a label consistent KSVD (LC-KSVD) [135, 136] is introduced to learn an effective discriminative dictionary for image classification.", "startOffset": 51, "endOffset": 61}, {"referenceID": 131, "context": "In this section, a label consistent KSVD (LC-KSVD) [135, 136] is introduced to learn an effective discriminative dictionary for image classification.", "startOffset": 51, "endOffset": 61}, {"referenceID": 130, "context": "The LC-KSVD demonstrates that the obtained solution, compared to other methods, can prevent learning a suboptimal or local optimal solution in the process of learning a dictionary [135].", "startOffset": 180, "endOffset": 185}, {"referenceID": 132, "context": "(3) Fisher discrimination dictionary learning for sparse representation Fisher discrimination dictionary learning (FDDL) [137] incorporates the supervised information (class label information) and the Fisher discrimination message into the objective function for learning a structured discriminative dictionary, which is used for pattern classification.", "startOffset": 121, "endOffset": 126}, {"referenceID": 133, "context": "23 can be solved by the iterative projection method in the literature [138].", "startOffset": 70, "endOffset": 75}, {"referenceID": 134, "context": "25 computes the dictionary class by class and it can be solved by exploiting the algorithm in the literature [139].", "startOffset": 109, "endOffset": 114}, {"referenceID": 134, "context": "[139] presented a metaface dictionary learning method, which is motivated by \u2018metagenes\u2019 in gene expression data analysis.", "startOffset": 0, "endOffset": 5}, {"referenceID": 135, "context": "Rodriguez and Sapiro [140] produced a discriminative non-parametric dictionary learning (DNDL) framework based on the OMP algorithm for image classification.", "startOffset": 21, "endOffset": 26}, {"referenceID": 136, "context": "[141] introduced a learned dictionary with commonalty and particularity, called DL-COPAR, which integrated an incoherence penalty term into the objective function for obtaining the class-specific sub-dictionary.", "startOffset": 0, "endOffset": 5}, {"referenceID": 137, "context": "[142] learned a hybrid dictionary, i.", "startOffset": 0, "endOffset": 5}, {"referenceID": 138, "context": "Jafari and Plumbley [143] presented a greedy adaptive dictionary learning method, which updated the learned dictionary with a minimum sparsity index.", "startOffset": 20, "endOffset": 25}, {"referenceID": 139, "context": "Some other supervised dictionary learning methods are also competent in image classification, such as supervised dictionary learning in [144].", "startOffset": 136, "endOffset": 141}, {"referenceID": 140, "context": "[145] developed a joint dictionary learning algorithm for object categorization, which jointly learned a commonly shared dictionary and multiply category-specific dictionaries for correlated object classes and incorporated the Fisher discriminant fidelity term into the process of dictionary learning.", "startOffset": 0, "endOffset": 5}, {"referenceID": 135, "context": "proposed a method of dictionary learning with structured incoherence (DLSI) [140], which unified the dictionary learning and sparse decomposition into a sparse dictionary learning framework for image classification and data clustering.", "startOffset": 76, "endOffset": 81}, {"referenceID": 141, "context": "presented a discriminative lowrank dictionary learning for sparse representation (DLRD SR) [146], in which the sparsity and the low-rank properties were integrated into one dictionary learning scheme where subdictionary with discriminative power was required to be lowrank.", "startOffset": 91, "endOffset": 96}, {"referenceID": 142, "context": "developed a simultaneous feature and dictionary learning [147] method for face recognition, which jointly learned the feature projection matrix for subspace learning and the discriminative structured dictionary.", "startOffset": 57, "endOffset": 62}, {"referenceID": 143, "context": "introduced a latent dictionary learning (LDL) [148] method for sparse representation based image classification, which simultaneously learned a discriminative dictionary and a latent representation model based on the correlations between label information and dictionary atoms.", "startOffset": 46, "endOffset": 51}, {"referenceID": 144, "context": "presented a submodular dictionary learning (SDL) [149] method, which integrated the entropy rate of a random walk on a graph and a discriminative term into a unified objective function and devised a greedybased approach to optimize it.", "startOffset": 49, "endOffset": 54}, {"referenceID": 145, "context": "developed a support vector guided dictionary learning (SVGDL) [150] method, which constructed a discriminative term by using adaptively weighted summation of the squared distances for all pairwise of the sparse representation solutions.", "startOffset": 62, "endOffset": 67}, {"referenceID": 146, "context": "Recently, sparse representation methods have been extensively applied to numerous real-world applications [151, 152].", "startOffset": 106, "endOffset": 116}, {"referenceID": 147, "context": "Recently, sparse representation methods have been extensively applied to numerous real-world applications [151, 152].", "startOffset": 106, "endOffset": 116}, {"referenceID": 148, "context": "The most representative work was proposed to exploit the sparse representation theory to generate a superresolution (SRSR) image from a single low-resolution image in literature [153].", "startOffset": 178, "endOffset": 183}, {"referenceID": 148, "context": "To this end, SRSR [153] provides a prior knowledge assumption, which is formulated as", "startOffset": 18, "endOffset": 23}, {"referenceID": 149, "context": "One-pass algorithm similar to that of [154] is introduced to enhance the compatibility between adjacent patches.", "startOffset": 38, "endOffset": 43}, {"referenceID": 150, "context": "33 can be solved by the back-projection method in [155] and the obtained image X\u2217 is regarded as the final optimal high-resolution image.", "startOffset": 50, "endOffset": 55}, {"referenceID": 148, "context": "The entire super-resolution via sparse representation is summarized in algorithm 14 and more information can be found in the literature [153].", "startOffset": 136, "endOffset": 141}, {"referenceID": 151, "context": "presented a modified version called joint dictionary learning via sparse representation (JDLSR) [156], which jointly learned", "startOffset": 96, "endOffset": 101}, {"referenceID": 152, "context": "[157] first explicitly analyzed the rationales of the sparse representation theory in performing the super-resolution task, and proposed to exploit the L2-Boosting strategy to learn coupled dictionaries, which were employed to construct sparse coding space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 153, "context": "[158] presented an image super-resolution reconstruction scheme by employing the dualdictionary learning and sparse representation method for image super-resolution reconstruction and Gao et al.", "startOffset": 0, "endOffset": 5}, {"referenceID": 154, "context": "[159] proposed a sparse neighbor embedding method, which incorporated the sparse neighbor search and HoG clustering method into the process of image super-resolution reconstruction.", "startOffset": 0, "endOffset": 5}, {"referenceID": 155, "context": "FernandezGranda and Candes [160] designed a transform-invariant group sparse regularizer by implementing a data-driven nonparametric regularizers with learned domain transform on group sparse representation for high image super-resolution.", "startOffset": 27, "endOffset": 32}, {"referenceID": 156, "context": "[161] proposed a geometry constrained sparse representation method for single image super-resolution by jointly obtaining an optimal sparse solution and learning a discriminative and reconstructive dictionary.", "startOffset": 0, "endOffset": 5}, {"referenceID": 157, "context": "[162] proposed to harness an adaptive sparse optimization with nonlocal regularization based on adaptive principal component analysis enhanced by nonlocal similar patch grouping and", "startOffset": 0, "endOffset": 5}, {"referenceID": 158, "context": "[163] proposed to integrate an adaptive sparse domain selection and an adaptive regularization based on piecewise autoregressive models into the sparse representations framework for single image superresolution reconstruction.", "startOffset": 0, "endOffset": 5}, {"referenceID": 159, "context": "Mallat and Yu [164] proposed a sparse mixing estimator for image super-resolution, which introduced an adaptive estimator models by combining a group of linear inverse estimators based on different prior knowledge for sparse representation.", "startOffset": 14, "endOffset": 19}, {"referenceID": 3, "context": "In the presence of image sparsity and redundancy representation [4, 7], sparse representation for image denoising first extracts the sparse image components, which are regarded as useful information, and then abandons the representation residual, which is treated as the image noise term, and finally reconstructs the image exploiting the pre-obtained sparse components, i.", "startOffset": 64, "endOffset": 70}, {"referenceID": 6, "context": "In the presence of image sparsity and redundancy representation [4, 7], sparse representation for image denoising first extracts the sparse image components, which are regarded as useful information, and then abandons the representation residual, which is treated as the image noise term, and finally reconstructs the image exploiting the pre-obtained sparse components, i.", "startOffset": 64, "endOffset": 70}, {"referenceID": 7, "context": "For example, Donoho [8, 29, 165] first discovered the connection between the compressed sensing and image denoising.", "startOffset": 20, "endOffset": 32}, {"referenceID": 160, "context": "For example, Donoho [8, 29, 165] first discovered the connection between the compressed sensing and image denoising.", "startOffset": 20, "endOffset": 32}, {"referenceID": 161, "context": "Subsequently, the most representative work of using sparse representation to make image denoising was proposed in literature [166], in which a global sparse representation model over learned dictionaries (SRMLD) was used for image denoising.", "startOffset": 125, "endOffset": 130}, {"referenceID": 161, "context": "The algorithm of image denoising exploiting sparse and redundant representation over learned dictionary is summarized in Algorithm 15, and more information can be found in literature [166].", "startOffset": 183, "endOffset": 188}, {"referenceID": 162, "context": "[167] proposed an enhanced sparse representation with a block-matching 3-D (BM3D) transform-domain filter based on self-similarities and an enhanced sparse representation by clustering similar 2-D image patches into 3-D data spaces and an iterative collaborative filtering procedure for image denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 163, "context": "[168] proposed the use of extending the KSVD-based grayscale algorithm and a generalized weighted average algorithm for color image denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 164, "context": "Protter and Elad [169] extended the techniques of sparse and redundant representations for image sequence denoising by exploiting spatio-temporal atoms, dictionary propagation over time and dictionary learning.", "startOffset": 17, "endOffset": 22}, {"referenceID": 165, "context": "[170] designed a clustering based sparse representation algorithm, which was formulated by a double-header sparse optimization problem built upon dictionary learning and structural clustering.", "startOffset": 0, "endOffset": 5}, {"referenceID": 166, "context": "[171] proposed a variational encoding framework with a weighted sparse nonlocal constraint, which was constructed by integrating image sparsity prior and nonlocal self-similarity prior into a unified regularization term to overcome the mixed noise removal problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 167, "context": "[172] studied a weighted nuclear norm minimization (WNNM) method with F -norm fidelity under different weighting rules optimized by non-local self-similarity for image denoising.", "startOffset": 0, "endOffset": 5}, {"referenceID": 168, "context": "[173] proposed a patch-based video denoising algorithm by stacking similar patches in both spatial and temporal domain to formulate a low-rank matrix problem with the nuclear norm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 169, "context": "[174] proposed an impressive image denoising method based on an extension of the KSVD algorithm via group sparse representation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 170, "context": "For example, Bioucas-Dias and Figueirdo [175] introduced a two-step iterative shrinkage/thresholding (TwIST) algorithm for image restoration, which is more efficient and can be viewed as an extension of the IST method.", "startOffset": 40, "endOffset": 45}, {"referenceID": 171, "context": "[176] presented a multiscale sparse image representation framework based on the KSVD dictionary learning algorithm and shift-invariant sparsity prior knowledge for restoration of color images and video image sequence.", "startOffset": 0, "endOffset": 5}, {"referenceID": 172, "context": "[177] proposed a learned simultaneous sparse coding (LSSC) model, which integrated sparse dictionary learning and nonlocal self-similarities of natural images into a unified framework for image restoration.", "startOffset": 0, "endOffset": 5}, {"referenceID": 173, "context": "Zoran and Weiss [178] proposed an expected patch log likelihood (EPLL) optimization model, which restored the image from patch to the whole image based on the learned prior knowledge of any patch acquired by Maximum A-Posteriori estimation instead of using simple patch averaging.", "startOffset": 16, "endOffset": 21}, {"referenceID": 174, "context": "[179] proposed a fast orthogonal dictionary learning algorithm, in which a sparse image representation based orthogonal dictionary was learned in image restoration.", "startOffset": 0, "endOffset": 5}, {"referenceID": 175, "context": "[180] proposed a groupbased sparse representation, which combined characteristics from local sparsity and nonlocal self-similarity of natural images to the domain of the group.", "startOffset": 0, "endOffset": 5}, {"referenceID": 176, "context": "[181, 182] proposed a centralized sparse representation (CSR) model, which combined the local and nonlocal sparsity and redundancy properties for variational problem optimization by introducing a concept of sparse coding noise term.", "startOffset": 0, "endOffset": 10}, {"referenceID": 177, "context": "[181, 182] proposed a centralized sparse representation (CSR) model, which combined the local and nonlocal sparsity and redundancy properties for variational problem optimization by introducing a concept of sparse coding noise term.", "startOffset": 0, "endOffset": 10}, {"referenceID": 176, "context": "Here we mainly introduce a recently proposed simple but effective image restoration algorithm CSR model [181].", "startOffset": 104, "endOffset": 109}, {"referenceID": 176, "context": "Moreover, the unbiased estimation of \u03b1x, denoted by E[\u03b1x], empirically can be approximate to \u03b1x under some prior knowledge [181], and then SCN algorithm employs the nonlocal means estimation method [183] to evaluate the unbiased estimation of \u03b1x, that is, using the weighted average of all \u03b1il to approach E[\u03b1x], i.", "startOffset": 123, "endOffset": 128}, {"referenceID": 178, "context": "Moreover, the unbiased estimation of \u03b1x, denoted by E[\u03b1x], empirically can be approximate to \u03b1x under some prior knowledge [181], and then SCN algorithm employs the nonlocal means estimation method [183] to evaluate the unbiased estimation of \u03b1x, that is, using the weighted average of all \u03b1il to approach E[\u03b1x], i.", "startOffset": 198, "endOffset": 203}, {"referenceID": 179, "context": "47 can be optimized by the augmented Lagrange multiplier method [184] or the iterative shrinkage algorithm in [185].", "startOffset": 64, "endOffset": 69}, {"referenceID": 180, "context": "47 can be optimized by the augmented Lagrange multiplier method [184] or the iterative shrinkage algorithm in [185].", "startOffset": 110, "endOffset": 115}, {"referenceID": 176, "context": "The main procedures of the CSR algorithm are summarized in Algorithm 16 and readers may refer to literature [181] for more details.", "startOffset": 108, "endOffset": 113}, {"referenceID": 19, "context": "[20] proposed to employ sparse representation to perform robust face recognition, more and more researchers have been applying the sparse representation theory to the fields of computer vision and pattern recognition, especially in image classification and object tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 180, "context": "48) by using the extended iterative shrinkage algorithm in literature [185].", "startOffset": 70, "endOffset": 75}, {"referenceID": 17, "context": "The most representative sparse representation for face recognition has been presented in literature [18] and the general scheme of sparse representation based classification method is summarized in Algorithm 17.", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "[9] proposed a two-phase sparse representation based classification method, which exploited the l2-norm regularization rather than the l1-norm regularization to perform a coarse to fine sparse representation based classification, which was very efficient in comparison with the conventional l1norm regularization based sparse representation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 181, "context": "[186] proposed an extended sparse representation method (ESRM) for improving the robustness of SRC by eliminating the variations in face recognition, such as disguise, occlusion, expression and illumination.", "startOffset": 0, "endOffset": 5}, {"referenceID": 182, "context": "[187] also proposed a framework of superposed sparse representation based classification, which emphasized the prototype and vari-", "startOffset": 0, "endOffset": 5}, {"referenceID": 183, "context": "[188] proposed utilizing the maximum correntropy criterion named CESR embedding non-negative constraint and half-quadratic optimization to present a robust face recognition algorithm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 184, "context": "[189] developed a new robust sparse coding (RSC) algorithm, which first obtained a sparsity-constrained regression model based on maximum likelihood estimation and exploited an iteratively reweighted regularized robust coding algorithm to solve the pre-proposed model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 185, "context": "[190] introduced an extension of the spatial pyramid matching (SPM) algorithm called ScSPM, which incorporated SIFT sparse representation into the spatial pyramid matching algorithm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 186, "context": "[191] developed a kernel sparse representation with the SPM algorithm called KSRSPM, and then proposed another version of an improvement of the SPM called LScSPM [192], which integrated the Laplacian matrix with local features into the objective function of the sparse representation method.", "startOffset": 0, "endOffset": 5}, {"referenceID": 187, "context": "[191] developed a kernel sparse representation with the SPM algorithm called KSRSPM, and then proposed another version of an improvement of the SPM called LScSPM [192], which integrated the Laplacian matrix with local features into the objective function of the sparse representation method.", "startOffset": 162, "endOffset": 167}, {"referenceID": 188, "context": "Kulkarni and Li [193] proposed a discriminative affine sparse codes method (DASC) on a learned affineinvariant feature dictionary from input images and exploited the AdaBoost-based classifier to perform image classification.", "startOffset": 16, "endOffset": 21}, {"referenceID": 189, "context": "[194] proposed integrating the non-negative sparse coding, low-rank and sparse matrix decomposition (LR-ScSPM) method, which exploited non-negative sparse coding and SPM for achieving local features representation and employed low-rank and sparse matrix decomposition for sparse representation, for image classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 190, "context": "[195] presented a low-rank sparse representation (LRSR) learning method, which preserved the sparsity and spatial consistency in each procedure of feature representation and jointly exploited local features from the same spatial proximal regions for image classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 191, "context": "[196] developed a structured low-rank sparse representation (SLRSR) method for image classification, which constructed a discriminative dictionary in training terms and exploited low-rank matrix reconstruction for obtaining discriminative representations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 192, "context": "[197] proposed a novel dimension reduction method based on the framework of rank preserving sparse learning, and then exploited the projected samples to make effective Kinect-based scene classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 193, "context": "[198] proposed a discriminative tensor sparse coding (RTSC) method for robust image classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 194, "context": "Recently, low-rank based sparse representation became a popular topic such as non-negative low-rank and sparse graph [199].", "startOffset": 117, "endOffset": 122}, {"referenceID": 79, "context": "Some sparse representation methods in face recognition can be found in a review [83] and other more image classification methods can be found in a more recent review [200].", "startOffset": 80, "endOffset": 84}, {"referenceID": 195, "context": "Some sparse representation methods in face recognition can be found in a review [83] and other more image classification methods can be found in a more recent review [200].", "startOffset": 166, "endOffset": 171}, {"referenceID": 196, "context": "employed the idea of sparse representation to visual tracking [201] and vehicle classification [202], which introduced nonnegative sparse constraints and dynamic template updating strategy.", "startOffset": 62, "endOffset": 67}, {"referenceID": 197, "context": "employed the idea of sparse representation to visual tracking [201] and vehicle classification [202], which introduced nonnegative sparse constraints and dynamic template updating strategy.", "startOffset": 95, "endOffset": 100}, {"referenceID": 198, "context": "[203] proposed two realtime compressive sensing visual tracking algorithms based on sparse representation, which adopted dimension reduction and the OMP algorithm to improve the efficiency of recovery procedure in tracking, and also developed a modified version of fusing background templates into the tracking procedure for robust object tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 199, "context": "[204] directly treated object tracking as a pattern recognition problem by regarding all the targets as training samples, and then employed the sparse representation classification method to do effective object tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 200, "context": "[205] employed the concept of sparse representation based on a particle filter framework to construct a multi-task sparse learning method denoted as multi-task tracking for robust visual tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 201, "context": "[206] conceived a structural local sparse appearance model for robust object tracking by integrating the partial and spatial information from the target based on an alignment-pooling algorithm.", "startOffset": 0, "endOffset": 5}, {"referenceID": 202, "context": "[207] proposed constructing a two-stage sparse optimization based online visual tracking method, which jointly minimized the objective reconstruction error and maximized the discriminative capability by choosing distinguishable features.", "startOffset": 0, "endOffset": 5}, {"referenceID": 203, "context": "[208] introduced a local sparse appearance model (SPT) with a static sparse dictionary learned from k-selection and dynamic updated basis distribution to eliminate potential drifting problems in the process of visual tracking.", "startOffset": 0, "endOffset": 5}, {"referenceID": 204, "context": "[209] developed a fast real time l1-tracker called the APG-l1tracker, which exploited the accelerated proximal gradient algorithm to improve the l1-tracker solver in [201].", "startOffset": 0, "endOffset": 5}, {"referenceID": 196, "context": "[209] developed a fast real time l1-tracker called the APG-l1tracker, which exploited the accelerated proximal gradient algorithm to improve the l1-tracker solver in [201].", "startOffset": 166, "endOffset": 171}, {"referenceID": 205, "context": "[210] addressed the object tracking problem by developing a sparsitybased collaborative model, which combined a sparsity-based classifier learned from holistic templates and a sparsity-based template model generated from local representations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 206, "context": "[211] proposed to formulate a sparse feature measurement matrix based on an appearance model by exploiting nonadaptive random projections, and employed a coarse-to-fine strategy to accelerate the computational efficiency of tracking task.", "startOffset": 0, "endOffset": 5}, {"referenceID": 207, "context": "[212] proposed to employ both non-local selfsimilarity and sparse representation to develop a non-local selfsimilarity regularized sparse representation method based on geometrical structure information of the target template data set.", "startOffset": 0, "endOffset": 5}, {"referenceID": 208, "context": "[213] proposed a sparse representation based online two-stage tracking algorithm, which learned a linear classifier based on local sparse representation on favorable image patches.", "startOffset": 0, "endOffset": 5}, {"referenceID": 209, "context": "More detailed visual tracking algorithms can be found in the recent reviews [214, 215].", "startOffset": 76, "endOffset": 86}, {"referenceID": 210, "context": "More detailed visual tracking algorithms can be found in the recent reviews [214, 215].", "startOffset": 76, "endOffset": 86}, {"referenceID": 33, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 102, "endOffset": 106}, {"referenceID": 72, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 114, "endOffset": 118}, {"referenceID": 85, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 125, "endOffset": 129}, {"referenceID": 78, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 137, "endOffset": 141}, {"referenceID": 85, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 148, "endOffset": 152}, {"referenceID": 95, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 163, "endOffset": 167}, {"referenceID": 8, "context": "We analyze and compare the performance of sparse representation with the most typical algorithms: OMP [36], l1 ls [76], PALM [89], FISTA [82], DALM [89], homotopy [99] and TPTSR [9].", "startOffset": 178, "endOffset": 181}, {"referenceID": 211, "context": "ORL: The ORL database includes 400 face images taken from 40 subjects each providing 10 face images [216].", "startOffset": 100, "endOffset": 105}, {"referenceID": 212, "context": "LFW face dataset: The Labeled Faces in the Wild (LFW) face database is designed for the study of unconstrained identity verification and face recognition [217].", "startOffset": 154, "endOffset": 159}, {"referenceID": 213, "context": "In our experiments, we chose 1251 images from 86 peoples and each subject has 10-20 images [218].", "startOffset": 91, "endOffset": 96}, {"referenceID": 214, "context": "Extended YaleB face dataset: The extended YaleB database contains 2432 front face images of 38 individuals and each subject having around 64 near frontal images under different illuminations [219].", "startOffset": 191, "endOffset": 196}, {"referenceID": 215, "context": "COIL20 dataset: Columbia Object Image Library (COIL20) database consists of 1,440 size normalized gray-scale images of 20 objects [220].", "startOffset": 130, "endOffset": 135}, {"referenceID": 212, "context": "For example, all these representative algorithms can achieve relatively inferior experimental results on the LFW dataset shown in Subsection IX-B, because the LFW dataset is designed for studying the problem of unconstrained face recognition [217] and most of the face images are captured under complex environments.", "startOffset": 242, "endOffset": 247}], "year": 2016, "abstractText": "Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this article is to provide a comprehensive study and an updated review on sparse representation and to supply a guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: sparse representation with l0-norm minimization, sparse representation with lp-norm (0<p<1) minimization, sparse representation with l1-norm minimization and sparse representation with l2,1-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: greedy strategy approximation, constrained optimization, proximity algorithmbased optimization, and homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. Specifically, an experimentally comparative study of these sparse representation algorithms was presented. The Matlab code used in this paper can be available at: http://www.yongxu.org/lunwen.html.", "creator": "LaTeX with hyperref package"}}}