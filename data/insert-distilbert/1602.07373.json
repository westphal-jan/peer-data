{"id": "1602.07373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "On Study of the Binarized Deep Neural Network for Image Classification", "abstract": "recently, the deep neural network ( derived from the artificial neural network ) has attracted many researchers'attention by having its outstanding performance. however, since this large network requires high - performance gpus and large integer storage, it very is today very hard rational to use improving it on individual devices. in order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. this network is a pure binary system, in which all n the values and calculations are binarized. as a result, our network can save a lot of computational resource and storage. therefore, so it is possible to use it on various devices. moreover, the experimental experiments results proved the feasibility of the proposed network.", "histories": [["v1", "Wed, 24 Feb 2016 02:39:47 GMT  (272kb)", "http://arxiv.org/abs/1602.07373v1", "9 pages, 6 figures. Rejected conference (CVPR 2015) submission. Submission date: November, 2014. This work is patented in China (NO. 201410647710.3)"]], "COMMENTS": "9 pages, 6 figures. Rejected conference (CVPR 2015) submission. Submission date: November, 2014. This work is patented in China (NO. 201410647710.3)", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["song wang", "dongchun ren", "li chen", "wei fan", "jun sun", "satoshi naoi"], "accepted": false, "id": "1602.07373"}, "pdf": {"name": "1602.07373.pdf", "metadata": {"source": "CRF", "title": "On Study of the Binarized Deep Neural Network for Image Classification", "authors": ["Song Wang", "Dongchun Ren", "Li Chen", "Wei Fan", "Jun Sun", "Satoshi Naoi"], "emails": ["naoi}@cn.fujitsu.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n07 37\n3v 1\n[ cs\n.N E\n] 2\n4 Fe\nb 20"}, {"heading": "1. Introduction", "text": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25]. Especially in [25], a two-layer network is introduced for pattern recognition. Figure 1 shows the basic function of ANN, that is, the neuron of higher layer is calculated by the neurons of prior layer with the connecting weights. However, there was no solution for the network training until backpropagation (gradient descent) algorithm was created by Paul Werbos [30]. After James McClelland [26] introduced the ANN as simulation of natural neural process and its usage in artificial intelligence (AI), the research of ANN became popular. Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on. Moreover, a theoretical explanation of ANN\u2019s success was also given by Kurt Hornik [18].\nBecause of the limitation of computers, the ANN research stagnated until the new century came. With the development of computers, the training of large-scale neural\nnetwork became possible. As a result, the research of deep neural networks (DNN) emerged and attracted more and more attention. For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9]. The features of DNN can be summarized as follows.\n\u2022 Large-scale network: the DNN model usually has very large structure (with many layers), including millions of neurons and connections. Consequently, the computational cost of DNN is extremely high, thus most of the DNN experiments are done with GPUs.\n\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.\n\u2022 Lack of theoretical explanation: although the DNN achieved much better performance than conventional methods, there is no convincing explanation for such success. For example, it is well known that some trick in training could improve the performance of DNN significantly, but it was hard to explain such effect theoretically.\nObviously, although the performance of DNN is promising, it still needs to be improved in different ways. On\none hand, in order to pursue higher recognition rate, several optimization methods for training were proposed, such as dropout [14] and dropconnect [29]. Those methods can reduce the overfitting problem significantly. On the other hand, new understanding of the neural network emerges and may extend the ability of DNN. For example, in Ian Goodfellow\u2019s recent work [6] for digit string recognition, the output layer is trained to show both the digit number and the recognition result of each digit. By doing this, their DNN model is able to recognize the digit string directly, without any segmentation process. This work extended the DNN from single character recognition to character string recognition. Inspired by this work, we may change the basic framework of DNN to find more possibilities.\nIn this paper, we focus on the basic function of ANN and try to make it more suitable for computers. Usually, the ANN is seen as simulation of natural neural process, thus its neurons and weights are all real number. Such values can represent the electric signal generated by neural cells. However, the ANN models are often realized by computers. As we all know, the computer process data based on binary value, like \u201c0\u201d and \u201c1\u201d. In other words, we can say that the basic \u201cneural cell\u201d of computer generates binary signals. Inspired by this observation, we proposed a new type of neural network \u2014 the binarized deep neural network (BDNN). In BDNN, all the neurons and weights are binary value; at the same time, the calculation of the basic function of BDNN is also Boolean.\nActually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19]. Nevertheless, this kind of neural network is quite different from BDNN. Although the input and output of the binary neural network are binary values, the weights of which are real number. Therefore, the calculation of binary neural network is not different from the conventional neural network. In contrast, the BDNN is a pure binary system, in which all the variables and operations are all binarized (Boolean).\nCompared with conventional deep neural network, the BDNN is expected to have several promising merits, which are shown as follows.\n\u2022 Less storage request: since all the weights in BDNN are binary value, thus we can use only 1 bit to store one weight. As a result, we can save a large network with small storage. In contrast, we must use at least 16 bits to store one real number weight of conventional neural network.\n\u2022 Higher speed on CPU: the basic calculation of CPU is the Boolean calculation. For CPU, this calculation is the most efficient. Since the BDNN only uses Boolean calculation, thus the processing speed of which can be easily optimized on CPU. Consequently, it is possi-\nble to run BDNN of high speed on devices with only CPUs. In contrast, conventional DNN can only run fast with GPUs.\n\u2022 Clear network response: in BDNN, it is quite easy to observe the response of neurons and weights in the propagation process. This is because each neuron or weight only has two different kinds of status. This may be helpful for us to design proper learning strategy for BDNN.\nIn summary, with binary variables and Boolean operations, the BDNN is able to run with reasonable computational resource and storage. Although now the performance of BDNN may not be comparable with the conventional deep neural network of the same scale, it has the potential to be improved in the future.\nThis paper is organized as follows. In Section 2, the principles of the BDNN is introduced as well as the hybrid binarized deep neural network (hybrid-BDNN) for non-binary input data. In Section 3, a training method is proposed for BDNN and hybrid-BDNN. In Section 4, comparison experiments of BDNN and conventional DNN are analyzed. The last section is the conclusion part."}, {"heading": "2. The binarized deep neural network", "text": "In this section, the recognition process (forward propagation) of BDNN is introduced. As mentioned above, only binary variables and Boolean operations are used in this process. Moreover, in order to use BDNN on non-binary input data, we also introduced the hybrid-BDNN, which contained both conventional neural network part and BDNN part."}, {"heading": "2.1. The basic function for BDNN", "text": "As shown in Fig. 1, with this basic function, we can build network of any complicated structure. Therefore, the BDNN is actually a new definition of the basic function. Just like the conventional DNN, with the basic function of BDNN, any neural network structure can be built.\nUsually, a basic function of neural network should contain two different types of calculation: the linear and nonlinear calculations. For example, as shown in Fig. 1, in conventional neural network, the linear calculation is an inner product of the input neurons ~x and the corresponding weights ~w, that is, the ~x \u00b7 ~w; the nonlinear calculation of conventional neural network is activation function (sigmoid, hyperbolic tangent, etc.) or pooling (often used in DNN). Consequently, in the definition of the basic function of BDNN, we also defined both the linear and nonlinear calculations.\nFirst, since the BDNN is created by binary values, the basic calculation should be chosen from Boolean algebra.\nThe Boolean binary operations are \u201cand \u2227\u201d, \u201cor \u2228\u201d and several derived operations (\u201cexclusive or \u2295\u201d, \u201cequivalence \u2261\u201d, \u201cmaterial implication \u2192\u201d). The truth table of those operations is shown in Table 1. Obviously, the \u201cexclusive or\u201d and \u201cequivalence\u201d should be chosen because their results are balanced between \u201c0\u201d and \u201c1\u201d. In BDNN, we chose the \u201cequivalence\u201d as the linear calculation. If we use \u22121 to represent \u201c0\u201d and 1 to represent \u201c1\u201d, then the result of \u201cequivalence\u201d is equal to the multiplication of real number. For convenience, from here we will treat the binary values as real numbers and use the corresponding real number operation instead of the Boolean operation (just like the \u201cequivalence\u201d to multiplication). As shown in Fig. 1, assume that ~x = (x1, x2, ..., xn) are the input neurons of BDNN and ~w = (w1, w2, ..., wn) are the corresponding weights, y is the output neuron, then we have xi, wi, y \u2208 {\u22121, 1}. As mentioned above, the linear calculation of BDNN is\n~x\u00d7 ~wT = (x1w1, x2w2, ..., xnwn).\nSecond, the nonlinear calculation of BDNN is defined as follows: we count the number of \u201c\u22121\u201d and \u201c1\u201d in (x1w1, x2w2, ..., xnwn) and let the calculation return the one with larger number. Assume f is the basic function of BDNN, with the linear and nonlinear calculation defined above, then we have\ny = f(~x, ~w) =\n{\n1, if \u2211n\ni=1 xiwi \u2265 0\n\u22121, if \u2211n i=1 xiwi < 0. (2.1)\nFigure 2 shows an example of the calculation of (2.1). The inputs ~x are (1,\u22121,\u22121) and the corresponding weights ~w are (1, 1,\u22121). Then, we use Boolean operation as linear calculation and the results (1,\u22121, 1) are obtained. Finally, since in the results, there are two \u201c1\u201d and one \u201c\u22121\u201d, thus the value of output y is \u201c1\u201d. Please note that although (2.1) is written as real number function, it denotes the Boolean operation of \u201cequivalence\u201d and a counting operation. With (2.1), the forward propagation of BDNN can be realized. For a certain set of input binary data, the BDNN can calculate its corresponding output, which is also binary."}, {"heading": "2.2. The hybrid-BDNN", "text": "Clearly, the BDNN introduced above is only able to process binary input data. If the input data is non-binary, such as grayscale image, the BDNN can not be used directly.\nTherefore, we should first convert the non-binary data into binary before the BDNN is used. For such situation, we proposed the hybrid-BDNN, which is a combination of conventional neural network and BDNN.\nAs shown in Fig. 3, the Hybrid-BDNN contains three parts: the normal neural network part, the transition part and the BDNN part. The lower layers of hybrid-BDNN are normal neural network part, which is connected to the input data (binary or non-binary); the higher layers are the BDNN part, which generates the result. The transition part is a single layer between the normal neural network part and BDNN part, by which the two different neural networks are combined.\nThe basic function of the normal neural network part is the same with the conventional networks (inner product and activation function); the basic funtion of BDNN is just introduced above. Consequently, the forward propagation can be conducted in both the normal neural network and the BDNN part. The remaining problem is the transition part. If we define the basic function of the transition part, then we can conduct the forward propagation of the whole hybridBDNN.\nAs shown in Fig.3, assume that ~g = (g1, g2, ..., gn) are the input neurons of the transition part and the (x1, x2, x3...) are the output neurons. As mentioned above, ~g belongs to the normal neural network part, thus it is all real number. The (x1, x2, x3...) belongs to the BDNN part, so they are all binary values. Let t denote the basic function of transition part, if we take x1 as example and ~w = (w1, w2, ..., wn) are the corresponding weights, then we obtain\nt(~g, ~w) = x1.\nObviously, in order to calculate with ~g, the ~w is also real number. Then we define t as follows:\nx1 = t(~g, ~w) =\n{\n1, if A( \u2211n\ni=1 giwi) \u2265 T\n\u22121, if A( \u2211n i=1 giwi) < T, (2.2)\nwhere A is activation function and T is a fixed threshold. In fact, the calculation A( \u2211n\ni=1 giwi) is just the basic function\nof the normal neural network part. The (2.2) means that we use a threshold T to convert the output real number of the normal neural network part into binary value.\nWith (2.2), the forward propagation of the whole hybridBDNN can be realized. This network can be used for any input data. By the way, if the range of activation function A is (\u22121, 1), then we can set the threshold T = 0. This is convenient for the training.\nIn summary, the BDNN is very suitable for binary input data classification, such as binary image of characters. If the input data is non-binary, the hybrid-BDNN can be used. Please note that it is better not to use too many layers in normal neural network part, otherwise the computation speed of hybrid-BDNN may be slowed much. In the experiments, we only used one layer (including the input data) as the normal neural network part."}, {"heading": "3. Training method for BDNN and hybridBDNN", "text": "Usually, the gradient descent algorithm is seen as the training method for neural networks. For the training of BDNN and hybrid-BDNN, we also used this algorithm. However, compared with the conventional neural network, the BDNN has obviously different properties, thus we can not directly use this algorithm on BDNN training. In order to solve this problem, some approximation and conversion are applied to BDNN to make it suitable for gradient descent training."}, {"heading": "3.1. Gradient descent training for BDNN", "text": "In the conventional gradient descent training, in each iteration, the weights are adjusted by a small value (depends on the error propagation and learning rate). Nevertheless, since the weights in BDNN are binarized, thus it is hard to adjust the weights in the same way. Hence, in order to apply the gradient descent algorithm, in the training of BDNN, the\nreal numbers are used instead of the binary values. A conversion function C is defined to convert the real numbers to corresponding binary values, which is\nC(x) =\n{\n1, if x \u2265 0 \u22121, if x < 0. (3.1)\nWith function (3.1), we can convert the trained weights of real number to binary value.\nHowever, the key point of using real number is that we must keep the forward propagation result the same with the binarized network; otherwise, the training is meaningless. Therefore, in order to satisfy this request, a basic function f \u2032 for training is defined. First, assume ~x = (x1, x2, ..., xn), ~w = (w1, w2, ..., wn) and y are the input neurons, weights and output of the BDNN; ~x\u2032 = (x\u2032\n1 , x\u2032 2 , ..., x\u2032n), ~w \u2032 = (w\u2032\n1 , w\u2032 2 , ..., w\u2032n) and y \u2032 are the corresponding real numbers of training. Second, assume\nC(x\u2032i) = xi, C(w \u2032 i) = wi. (3.2)\nThen, the basic function f \u2032 of training is given by\ny\u2032 = f \u2032(~x\u2032, ~w\u2032) =\n\u2211n\ni=1\nx\u2032iw \u2032 i\n|x\u2032iw \u2032 i|\nn . (3.3)\nAs mentioned above, in (2.1), the basic function f of BDNN counts the number of \u201c1\u201d and \u201c-1\u201d in (x1w1, x2w2, ..., xnwn) and returns the one with larger number. Accordingly, in (3.3), for the basic function f \u2032, if in (x\u2032\n1 w\u2032 1 , x\u2032 2 w\u2032 2 , ..., x\u2032nw \u2032 n) the positive values are more\nthan negative ones, then y\u2032 is a positive value; otherwise, y\u2032 is negative. Consequently, because of (3.2), we obtain\nC(f(~x\u2032, ~w\u2032)) = f(~x, ~w)\nand C(y\u2032) = y.\nClearly, with (3.3), if the inputs of training are the same with BDNN, then after forward propagation, each real number neuron of training equals to the corresponding binary neuron of BDNN (by using conversion function C), including the output neurons. As a result, we can train the BDNN with real numbers. After training, we just need to convert the real number weights into binary values and then a trained BDNN is obtained.\nAfter the forward propagation of training is solved, the next problem is the backpropagation. In order to use gradient descent algorithm, we must define two partial derivatives: the \u2202y\u2032\n\u2202x\u2032i and\n\u2202y\u2032 \u2202w\u2032i . With such two partial derivatives,\nthe backpropagation can be conducted. Assume x\u2032k is a certain neuron from ~x \u2032 and w\u2032k (let w \u2032 k > 0) is its corresponding weight. Let l take the form\nl =\n\u2211n\ni=1,i6=k\nx\u2032iw \u2032 i\n|x\u2032iw \u2032 i|\nn .\nThen, we have\ny\u2032 = l + x\u2032kw \u2032 k\nn |x\u2032kw \u2032 k| .\nIt is obvious that l and w\u2032k are independent. Thus we can draw a curve of y\u2032 by w\u2032k, which is shown in Fig. 4 (a). This curve is not continuous. We can see that the \u2202y\u2032\n\u2202x\u2032k is +\u221e\nat x\u2032k = 0 and 0 at the rest of the positions. Clearly, we can not use such partial derivative for gradient descent algorithm. Therefore, an approximation of this curve (shown in Fig. 4 (b)) is used to calculate the \u2202y\u2032\n\u2202x\u2032k . The new curve is\na strait line, go through the point (\u22121, l\u2212 1\nn ) and (1, l+\n1 n ).\nHere, we assume that w\u2032k > 0. If w \u2032 k < 0, the slope of the approximation line will be reversed. Consequently, by us-\ning the approximation, we obtain\n\u2202y\u2032 \u2202x\u2032k = 2w\u2032k n |w\u2032k| . (3.4)\nSimilarly, since in function f \u2032, the x\u2032k and w \u2032 k are symmetric, so that \u2202y\u2032\n\u2202w\u2032k = 2x\u2032k n |x\u2032k| . (3.5)\nHere, in (3.4) and (3.5), the 2 n is the value of the slope; the w\u2032k |w\u2032k| or x\u2032k |x\u2032k| determines the sign of the slope. Please note that the slope value can be set to not only 2 n\nbut also other proper values.\nWith (3.4) and (3.5), the gradient descent algorithm can be applied to train the BDNN. Although an approximation was used, the following experimental results showed that, the BDNN could be trained well with such method."}, {"heading": "3.2. The training of transition part of hybrid-BDNN", "text": "As shown in Fig. 3, now the backpropagation can be applied to the normal neural network part and the BDNN part. However, in order to apply the backpropagation on hybridBDNN, we still need to consider about the backpropagation of the transition part.\nFirst, the same with the BDNN training, in the training of the transition part, the real numbers are used instead of the binary values. Assume that x\u2032\n1 is the corresponding real\nnumber of x1 and function t\u2032 is the real number version of function t. Then, the same with f \u2032, in order to keep the same forward propagation result, t\u2032 should satisfy the following equation\nC(t\u2032(~g, ~w)) = C(~g, ~w).\nTherefore, we define t\u2032 as\nt\u2032(~g, ~w) = A(\nn \u2211\ni=1\ngiwi)\u2212 T. (3.6)\nObviously, the t\u2032 of (3.6) satisfies the request. If we set T = 0, then t\u2032 is just the activation function. Naturally, for a certain neuron gk and its weight wk, the two derivatives of the transition part are given by\n\u2202x\u2032 1 \u2202gk = \u2202A \u2202gk , \u2202x\u2032 1 \u2202wk = \u2202A \u2202wk . (3.7)\nIn (3.7), the two partial derivatives are just the same with the normal neural network. With (3.7), the backpropagation of the transition part becomes possible.\nConsequently, by using the above method, the backpropagation can be applied to the whole hybrid-BDNN. Now we can train the hybrid-BDNN by using the gradient descent algorithm."}, {"heading": "3.3. Special training technique for BDNN", "text": "Clearly, the training method for BDNN proposed in this paper is very close to the conventional training method. Thus it is possible to borrow the training techniques of conventional neural networks to BDNN, such as dropout and dropconnect. Besides, there are several special training techniques, which are necessary for BDNN.\nFirst, in the training, we must keep the n of (3.3) an odd number all the time. This is because the output 0 of f \u2032 is not defined. The result of f \u2032 should be either a positive number or a negative number. Consequently, in the following experiments, in the BDNN, the neuron number n was always an odd number.\nSecond, in the training of conventional neural network, the error of the output is determined by the difference between the output neuron and its ground truth. However, in the training of BDNN, the error is determined by not only the difference but also the sign of each value. Assume the error of a certain output neuron y is e and the corresponding ground truth is yT , then the error calculation is given by\ne =\n{\n0, if yyT > 0 1 2 (y \u2212 yT ), if yyT < 0.\n(3.8)\nThe(3.8) means that if the output neuron and its ground truth are both positive or negative, then the error of this output is 0; otherwise, the error is calculated as conventional neural network. Actually, according to (3.1), if the output neuron has the same sign with its ground truth, then the output is seen as correct. This technique is essential for the BDNN training, without which the training can not converge."}, {"heading": "4. Experiments", "text": "In the experiments, two different network structures were studied \u2014 the classical three-layer network and the convolutional neural network (CNN). Besides the BDNN and hybrid-BDNN, the conventional neural networks of the\nsame structures were also tested for comparison. Since now we only have the basic training method for BDNN, so in order to make fair comparison, most of the training optimization techniques of the conventional neural network were not used. For example, in the experiments, the learning rate was fixed and the simple hyperbolic tangent function was used as activation function.\nMoreover, two datasets, MNIST and CIFAR10 were used for the experiments. As we know, MNIST is a dataset of handwritten digits, thus it is suitable for binary data classification test. In contrast, the CIFAR10 was used as nonbinary data, on which the hybrid-BDNN was tested. In each dataset, the training data was used to train the networks; the test data was used as validation data. After the training was finished, the iteration with the lowest error rate on test data was seen as the final result."}, {"heading": "4.1. The classical three-layer network", "text": "In the early stages of ANN, before the DNN was introduced, the three-layer network was widely used for classification tasks. As shown in Fig. 5, in such network, there are three different layers: the input layer, the hidden layer and the output layer. The input layer contains the input data while the output layer generates the classification results. Besides, all the layers are fully connected. Since this network structure was very classic, so the BDNN of this structure was first tested.\nFor the experiments of three-layer BDNN, the binary data of MNIST was used. Because the original images of MNIST were grayscale, so we used a simple binarization method (fixed threshold) to convert the grayscale images into binary. All the training data of MNIST was used to train the network and the test data of MNIST was used as validation data.\nIn MNIST, the image size is 28\u00d7 28, thus there are 784 pixels, which are used as input neurons. As mentioned above, in order to make the neurons of the input layer an odd number, one neuron was simply added to the input layer\nand set to 1. Consequently, the input layer contains 785 neurons. Moreover, since the digits have 10 classes, so the output layer is set to 10 neurons.\nFor the hidden layer, we used two different numbers, one is 1571 (double of the input neuron number) and the other is 2355 (three times of the input neuron number). Then, two different network structures are obtained as follows.\n\u2022 Structure A: the input layer contains 785 neurons, followed by a hidden layer of 1571 neurons, then an output layer of 10 neurons (one neuron stands for one digit class).\n\u2022 Structure B: the input layer contains 785 neurons, followed by a hidden layer of 2355 neurons, then an output layer of 10 neurons (one neuron stands for one digit class).\nIn the experiments, the BDNN of both structures were tested. Meanwhile, the conventional ANN of Structure A was also tested for comparison.\nThe experimental results are shown in Table 2. On one hand, by using the proposed training method, finally, the BDNN converged on the training dataset and on the test dataset, the classification rate was around 15%. By this result, the feasibility of BDNN is proved. On the other hand, it can be seen that the conventional ANN of Structure A has much lower error rate than the BDNN of same structure. This is may be because the BDNN was not fully trained by\nthe proposed method (borrowed from the conventional network training). If we can design specific training method for BDNN, the performance of which may be improved a lot. Another possible reason may be the complexity of the network. Since the binary neurons are used in BDNN, the complexity of which is lower than the conventional ANN, though they have the same structure. The result of BDNN of Structure B indicates that, when the scale of BDNN is increased (higher complexity), the performance becomes better."}, {"heading": "4.2. The convolutional neural network", "text": "In deep learning research, the structure of CNN is widely used for image classification. Therefore, in the experiments, we also tested the CNN structure. As shown in Fig. 6, we used a CNN structure of five layers. The first layer is the input data, followed by two layers of feature maps (calculated by the convolutional kernels). The last two layers are the fully connected neurons and one of them is used as output layer. In most of the CNNs, after convolutional operation, the feature map size is then decreased by pooling. Nevertheless, since it is hard to realize the pooling operation on binary values, thus the feature map size is controlled by skipping every other pixel in the convolutional operation.\nWith different sizes of the feature maps and kernels, three different CNN structures are obtained as follows.\n\u2022 Structure C: Layer 1 is the input image, the size is 29 \u00d7 29; Layer 2 contains 7 feature maps of size 13 \u00d7 13, obtained by using 7 kernels of size 5 \u00d7 5; Layer 3 contains 51 feature maps of size 5 \u00d7 5, obtained by using 357 kernels of size 5\u00d7 5; Layer 4 contains 201 neurons, obtained by using 10, 251 kernels of size 5\u00d7 5; Layer 5 contains 10 neurons for output.\nThe Structure C and D were used by BDNN and tested on MNIST. Structure E was used by the hybrid-BDNN and tested on CIFAR10 (non-binary data). Therefore, for Structure E of hybrid-BDNN, the input layer and the following kernels were real numbers (normal neural network part) and the rest of the network was the BDNN part.\nFirst, the error rates of MNIST are shown in Table 3. Clearly, by using deeper structure (more layers), the performance of BDNN was improved. Meanwhile, the normal CNN of the same structure still had much better result. In Structure D, when more feature maps were used, the result of BDNN became better. This also shows the potential of BDNN of larger scales.\nSecond, the error rates of CIFAR10 are shown in Table 4. CIFAR10 is a database of color images of 10 different kinds of objects. In the experiment, the color images were converted into grayscale and then used. Since this database was very difficult, the error rates of both networks were very high, though the largest network structure were used.\nAnother observation is that although the training error rate of hybrid-BDNN was much lower than CNN, its test error rate was lower. This indicates that (i) it is possible to use hybrid-BDNN for non-binary data classification and (ii) larger network structure of high complexity is necessary for reducing the training error of hybrid-BDNN."}, {"heading": "5. Conclusion and future work", "text": "In BDNN, we first tried the brand new binarized propagation function for neural networks. Moreover, a special gradient descent training method is also proposed for BDNN. The experimental results proved that it is able to use BDNN just like the conventional DNN. Besides changing the network structure and training strategy, our trial may provide new thought of improving the DNN and extend its usage.\nBecause of the computational limitation, we didn\u2019t test the BDNN of large scale in this paper. Therefore, the performance of BDNN may not be comparable with the stateof-the-art results. In the future, we will develop GPU based training program to train the BDNN of larger scale. Various network structures will also be studied, such as the deep fully connected neural network, the recurrent neural network and so on. Moreover, the optimization of the forward propagation of the BDNN on CPU will be studied."}, {"heading": "6. Acknowledgments", "text": "This work was started at the beginning of 2014 and based on this work, a patent (China) is applied in November, 2014. The application number of the patent is 201410647710.3. We also submitted this paper to CVPR 2015 but it was rejected. However, we still believe this work is valuable and the BDNN is a promising solution for low-performance device to use deep learning models."}], "references": [{"title": "Neural network approaches versus statistical methods in classification of multisource remote sensing data", "author": ["J. Benediktsson", "P.H. Swain", "O.K. Ersoy"], "venue": "IEEE Transactions on geoscience and remote sensing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Multispectral classification of landsat-images using neural networks", "author": ["H. Bischof", "W. Schneider", "A.J. Pinz"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Multicolumn deep neural network for traffic sign classification", "author": ["D. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Multi-digit number recognition from street view imagery using deep convolutional neural networks", "author": ["I.J. Goodfellow", "Y. Bulatov", "J. Ibarz", "S. Arnoud", "V. Shet"], "venue": "arXiv preprint arXiv:1312.6082,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "B. Roman", "B. Horst", "J. Schmidhuber"], "venue": "IEEE Transactions on 4328  Pattern Analysis and Machine Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G. Hinton"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "A training algorithm for binary feedforward neural networks", "author": ["D.L. Gray", "A.N. Michel"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "The organization of behavior", "author": ["D. Hebb"], "venue": "New York: Wiley,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1949}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Neural networks and physical systems with emergent collective computational abilities", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1982}, {"title": "Neurons with graded response have collective computational properties like those of two-state neurons", "author": ["J.J. Hopfield"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}, {"title": "Computing with neural circuits- a model", "author": ["J.J. Hopfield", "D.W. Tank"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1986}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "The geometrical learning of binary neural networks", "author": ["J.H. Kim", "S.K. Park"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi", "A.D. Back"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W.S. McCulloch", "W. Pitts"], "venue": "The bulletin of mathematical biophysics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1943}, {"title": "On sequential construction of binary neural networks", "author": ["M. Muselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "A neural network approach to character recognition", "author": ["A. Rajavelu", "M.T. Musavi", "M.V. Shirvaikar"], "venue": "Neural Networks,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1989}, {"title": "The perceptron: A probalistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1958}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["D.E. Rumelhart", "J. McClelland"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1986}, {"title": "An artificial hysteresis binary neuron: A model suppressing the oscillatory behaviors of neural dynamics", "author": ["Y. Takefuji", "K.C. Lee"], "venue": "Biological Cybernetics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1991}, {"title": "Modular construction of time-delay neural networks for speech recognition", "author": ["A. Waibel"], "venue": "Neural computation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1989}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML-", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Beyond regression: New tools for prediction and analysis in the behavioral sciences", "author": ["P. Werbos"], "venue": "PhD thesis, Harvard University,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1975}], "referenceMentions": [{"referenceID": 21, "context": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25].", "startOffset": 143, "endOffset": 147}, {"referenceID": 24, "context": "The research of artificial neural networks (ANN) began more than 70 years ago, proposed by Warren McCulloch and Walter Pitts [22], Donald Hebb [12] and Frank Rosenblatt [25].", "startOffset": 169, "endOffset": 173}, {"referenceID": 24, "context": "Especially in [25], a two-layer network is introduced for pattern recognition.", "startOffset": 14, "endOffset": 18}, {"referenceID": 29, "context": "However, there was no solution for the network training until backpropagation (gradient descent) algorithm was created by Paul Werbos [30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "After James McClelland [26] introduced the ANN as simulation of natural neural process and its usage in artificial intelligence (AI), the research of ANN became popular.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 65, "endOffset": 74}, {"referenceID": 1, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 65, "endOffset": 74}, {"referenceID": 4, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 65, "endOffset": 74}, {"referenceID": 23, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "Since then, ANN was successfully applied to image classification [1, 2, 5], character recognition [24], face recognition [20], speech recognition [28] and so on.", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "Moreover, a theoretical explanation of ANN\u2019s success was also given by Kurt Hornik [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 124, "endOffset": 131}, {"referenceID": 3, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 124, "endOffset": 131}, {"referenceID": 8, "context": "For example, Geoff Hinton\u2019s deep belief nets [13], Yann LeCun and Dan Ciresan\u2019s study on deep convolutional neural networks [21, 4], and Alex Graves\u2019s deep recurrent neural networks [9].", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 183, "endOffset": 190}, {"referenceID": 9, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 183, "endOffset": 190}, {"referenceID": 3, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 213, "endOffset": 219}, {"referenceID": 2, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 213, "endOffset": 219}, {"referenceID": 7, "context": "\u2022 Better performance: DNN archived the state-of-theart results on various tasks and competitions, and brought breakthroughs to those fields, such as handwritten character recognition [7, 10], image classification [4, 3], speech recognition [8] and so on.", "startOffset": 240, "endOffset": 243}, {"referenceID": 13, "context": "one hand, in order to pursue higher recognition rate, several optimization methods for training were proposed, such as dropout [14] and dropconnect [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 28, "context": "one hand, in order to pursue higher recognition rate, several optimization methods for training were proposed, such as dropout [14] and dropconnect [29].", "startOffset": 148, "endOffset": 152}, {"referenceID": 5, "context": "For example, in Ian Goodfellow\u2019s recent work [6] for digit string recognition, the output layer is trained to show both the digit number and the recognition result of each digit.", "startOffset": 45, "endOffset": 48}, {"referenceID": 14, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 15, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 16, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 22, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 26, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 83, "endOffset": 103}, {"referenceID": 10, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 142, "endOffset": 150}, {"referenceID": 18, "context": "Actually, there were researches of binary neural network (Hopfield neural network) [15, 16, 17, 23, 27] and corresponding training algorithms [11, 19].", "startOffset": 142, "endOffset": 150}], "year": 2016, "abstractText": "Recently, the deep neural network (derived from the artificial neural network) has attracted many researchers\u2019 attention by its outstanding performance. However, since this network requires high-performance GPUs and large storage, it is very hard to use it on individual devices. In order to improve the deep neural network, many trials have been made by refining the network structure or training strategy. Unlike those trials, in this paper, we focused on the basic propagation function of the artificial neural network and proposed the binarized deep neural network. This network is a pure binary system, in which all the values and calculations are binarized. As a result, our network can save a lot of computational resource and storage. Therefore, it is possible to use it on various devices. Moreover, the experimental results proved the feasibility of the proposed network.", "creator": "LaTeX with hyperref package"}}}