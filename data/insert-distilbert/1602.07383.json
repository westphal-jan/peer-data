{"id": "1602.07383", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Automatic Moth Detection from Trap Images for Pest Management", "abstract": "monitoring the trace number of insect pests is a crucial component in smart pheromone - based pest management systems. in this paper, we propose an automatic detection pipeline setup based on deep learning for identifying and counting poisonous pests in images taken inside field smart traps. applied to a commercial codling simulated moth dataset, our method shows promising performance both qualitatively and quantitatively. compared to previous attempts at pest detection, our approach uses no pest - specific engineering components which enables it to adapt automatically to specific other pest species and environments with minimal human effort. it is amenable to implementation on parallel desktop hardware and therefore capable of deployment in settings where real - clock time performance is required.", "histories": [["v1", "Wed, 24 Feb 2016 03:35:42 GMT  (6875kb,D)", "http://arxiv.org/abs/1602.07383v1", "Preprints accepted by Computers and electronics in agriculture"]], "COMMENTS": "Preprints accepted by Computers and electronics in agriculture", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["weiguang ding", "graham taylor"], "accepted": false, "id": "1602.07383"}, "pdf": {"name": "1602.07383.pdf", "metadata": {"source": "META", "title": "Automatic moth detection from trap images for pest management", "authors": ["Weiguang Ding", "Graham Taylor"], "emails": ["wding@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": null, "text": "Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required.\nKeywords: Precision agriculture, Integrated pest management, Pest control, Trap images, Object detection, Convolutional neural network"}, {"heading": "1. Introduction", "text": "Monitoring is a crucial component in pheromone-based pest control [1, 2] systems. In widely used trap-based pest monitoring, captured digital images are analysed by human experts for recognizing and counting pests. Manual counting is labour intensive, slow, expensive, and sometimes error-prone, which precludes reaching real-time performance and cost targets. Our goal is to apply state-of-the-art deep learning techniques to pest detection and counting, effectively removing the human from the loop to achieve a completely automated, real-time pest monitoring system.\nPlenty of previous work has considered insect classification. The past literature can be grouped along several dimensions, including image acquisition settings, features, and classification algorithms. In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8]. Specimens are usually well preserved and imaged in an ideal lab environment. Thus specimen images are consistent and captured at high resolution. In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15]. In this case, image quality is usually worse than the specimen case, but researchers still typically have a chance to adjust settings to control image quality, such as imaging all of the insects under a standard orientation or lighting.\nFrom an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24]. Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21]. In general, however, these proposed methods were not tested under real application scenarios, for example, images from real traps deployed for pest monitoring.\nObject detection involves also localizing objects in addition to classification. A few attempts have been made with respect to insect detection. One option is to perform a \u201csliding window\u201d approach, where a classifier scans over patches at different locations of the image. This technique was applied for inspection of bulk wheat samples [25], where local patches from the original image were represented by engineered features and classified by discriminant analysis. Another work on bulk grain inspection [26] employed different customized rule-based algorithms to detect\nPreprint \u2013 to appear in Computers and Electronics in Agriculture\nar X\niv :1\n60 2.\n07 38\n3v 1\n[ cs\n.C V\n] 2\n4 Fe\nb 20\n16\ndifferent objects, respectively. The other way of performing detection is to first propose initial detection candidates by performing image segmentations. These candidates are then represented by engineered features and classified [27, 28]. All of these insect detection methods are heavily engineered and work only on specific species under specific environments, and are not likely to be directly effective in the pest monitoring setting.\nThere are two main challenges in detecting pests from trap images. The first challenge is low image quality, due to constraints such as the cost of the imaging sensor, power consumption, and the speed by which images can be transmitted. This makes most of the previous work impractical, that is, those based on high image quality and fine structures. The second challenge comes from inconsistencies which are driven by many factors, including illumination, movement of the trap, movement of the moth, camera out of focus, appearance of other objects (such as leaves), decay or damage to the insect, appearance of non-pest (benign) insects, etc. These make it very hard to design rule-based systems. Therefore, an ideal detection method should be capable and flexible enough to adapt to different varying factors with a minimal amount of additional manual effort other than manually labelled data from a daily pest monitoring program.\nApart from the insect classification/detection community, general visual object category recognition and detection has been a mainstay of computer vision for a long time. Various methods and datasets [29, 30] have been proposed in the last several decades to push this area forward. Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].\nInspired by this line of research, we adopt the popular sliding window detection pipeline with convolutional neural networks as the image classifier. First, raw images are preprocessed with colour correction. Then, trained ConvNets are applied to densely sampled image patches to predict each patch\u2019s likelihood of containing pests. Patches are then filtered by non-maximum suppression, after which only those with probabilities higher than their neighbours are preserved. Finally, the remaining patches are thresholded. Patches whose probability meet the threshold are considered as proposed detections.\nThis paper makes two main contributions. First, we develop a ConvNet-based pest detection method, that is accurate, fast, easily extendable to other pest species, and requires minimal pre-processing of data. Second, we propose an evaluation metric for pest detection borrowing ideas from the pedestrian detection literature."}, {"heading": "2. Data collection", "text": "In this section, we describe the collection, curation, and preprocessing of images. Details of detection performed on processed images are provided in Section 3."}, {"heading": "2.1. Data acquisition", "text": "RGB colour images are captured by pheromone traps installed at multiple locations by a commercial provider of pheromone-based pest control solutions, whose name is withheld by request. The trap contains a pheromone lure, an adhesive liner, a digital camera and a radio transmitter. The pheromone attracts the pest of interest into the trap where they become stuck to the adhesive surface. The digital images are stored in JPEG format at 640\u00d7480 resolution, and transmitted to a remote server at fixed time point daily. Codling moths are identified and labelled with bounding boxes by technicians trained in entomology. Only one image from each temporal sequence is labelled and used in this study, so labelled images do not have temporal correlation with each other. As a result, all of the labelled moths are unique. Figure 1a shows a trap image with all the codling moth labelled with blue bounding boxes. Figure 1b shows an image containing no moths but cluttered with other types of insects. High resolution individual image patches are shown later in Figure 11, with their characteristics analysed in Section 5.3."}, {"heading": "2.2. Dataset construction", "text": "The set of collected images is split randomly into 3 sets: the training set, the validation set and the test set. After splitting, the statistics of each set is roughly the same as the entire dataset, including the ratio between the number of images with or without moths, and number of moths per image. Table 1 provides specific statistics on the entire dataset and the three splits subsequently constructed."}, {"heading": "2.3. Preprocessing", "text": "Trap images were collected in real production environments, which leads to different imaging conditions at different points in time. This is most apparent in illumination, which can be seen in Figure 2a. To eliminate the potential negative effects of illumination variability on detection performance, we perform colour correction using one variant [38] of the \u201cgrey-world\u201d method. This algorithm assumes that the average value of red (R), green (G) and blue (B) channels should equal to each other. Specifically, for each image, we set the gain of the R and B channels as follows:\nGred = \u00b5red/\u00b5green, Gblue = \u00b5blue/\u00b5green (1)\nwhere \u00b5red, \u00b5green and \u00b5blue are the original average intensities of the red, green and blue channels, respectively. Gred and Gblue are multiplicative gains applied to the pixel intensity values of the red and blue channels, respectively. Figure 2b shows images processed by the grey-world algorithm. We see that the images are white-balanced to have similar illumination, but still maintain rich colour information which can be a useful cue for detection downstream. In this paper, all images are white-balanced prior to detection."}, {"heading": "3. Detection pipeline", "text": "The automatic detection pipeline involves several steps, as shown in Figure 3. We take a sliding window approach, where a trained image classifier is applied to local windows at different locations of the entire image. The classifier\u2019s output is a single scalar p \u2208 [0, 1], which represents the probability that a particular patch contains a codling moth. These patches are regularly and densely arranged over the image, and thus largely overlapping. Therefore, we perform non-maximum suppression (NMS) to retain only the windows whose respective probability is locally maximal. The remaining boxes are then thresholded, such that only patches over a certain probability are kept. The location of these\npatches with their respective probabilities (confidence scores) are the final outputs of the detection pipeline. We now discuss each of these stages in more detail."}, {"heading": "3.1. Convolutional neural networks", "text": "In a sliding window approach, the detection problem breaks down into classifying each local patch, which is performed by the image classifier C, a mapping from the image I to a probability p: C : I 7\u2192 p. We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems. It is also very fast to deploy, and amenable to parallel hardware. Specifically, we used a network structure similar to Lenet5 [31]. As shown in Figure 4, our network contains 2 convolutional layers, 2 max-pooling layers, and 2 fully connected layers (described below). Before applying the ConvNet, each dimension of the input patch1 is normalized to have zero mean and unit variance.\n1Note that we distinguish patch-based normalization, as described here, with whole image normalization, described in Section 2.3."}, {"heading": "3.1.1. Convolutional layers", "text": "A convolutional layer applies a linear filterbank and element-wise nonlinearity to its input \u201cfeature maps\u201d, transforming them to a different set of feature maps. By applying convolutional layers several times, we can extract increasingly high-level feature representations of the input, at the same time preserving their spatial relationship. At the first layer, the input feature maps are simply the channels of the input. At subsequent layers, these represent more abstract transformations of the image. A convolutional layer is a special case of a fully connected layer, introduced in Subsection 3.1.3, where only local connections are have non-zero values, and weights are tied at all locations. Local connectivity is implemented efficiently by applying convolution:\nhlk = \u03d5 \u2211\nm\nWlm,k h l\u22121 m + b l k  , (2) where l is the layer index; m is the index of input feature maps; k is the index of output feature maps; input hl\u22121m is the mth feature map at layer l \u2212 1; output hlk the kth feature map at layer l; W is the convolutional weight tensor; b is the bias term; and we choose the element-wise nonlinearity \u03d5(\u00b7) to be the rectified linear unit (RELU) [41] function."}, {"heading": "3.1.2. Max-pooling layers", "text": "Each convolutional layer is followed by a max-pooling layer. This layer applies local pooling operations to its input feature maps, by only preserving the maximum value within a local receptive field and discarding all other values. It is similar to a convolutional layer in the sense that both operate locally. Applying max-pooling layers has 2 major benefits: 1) reducing the number of free parameters, and 2) introducing a small amount of translational invariance into the network."}, {"heading": "3.1.3. Fully connected layers", "text": "The last two layers in our ConvNet are fully connected. These are the kind of layers found in standard feedforward neural networks. The first fully connected layer flattens (vectorizes) all of the feature maps after the last max-pooling layer, treating this one-dimensional vector as a feature representation of the whole image. The second fully connected layer is parameterized like a linear classifier. Mathematically, the fully connected layer can be written as:\nhl = \u03d5(Wlhl\u22121 + bl), (3)\nwhere l is the layer index; input hl\u22121 is the vector representation at layer l \u2212 1; hl is the output of layer l; W is the weight matrix; b is the bias vector; and \u03d5(\u00b7) is an element-wise nonlinear function: we choose RELUs for the fully connected hidden layer and a softmax for the output layer.\nFor a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42]."}, {"heading": "3.2. Non-maximum suppression", "text": "After applying the ConvNet in a sliding window fashion, we obtain probabilities associated with each densely sampled patch. If we simply applied thresholding at this point, we would get many overlapping detections. This problem is commonly solved using non-maximum suppression (NMS) which aims to retain only patches with locally maximal probability. We adopted a strategy similar to [43]. Specifically, we first sort all the detections according to their probability. Then, from high to low probability, we look at each detection and remove other bounding boxes that overlap at least 10% with the current detection. After this greedy process, we generate final detection outputs as shown in Figure 3 and later in Section 5.1."}, {"heading": "4. Experiments and evaluation", "text": "We next introduce how we performed the experiments and the evaluation protocol."}, {"heading": "4.1. Classifier training", "text": "The classifier is trained on generated patches of different sizes, detailed in Section 4.2. Minibatch stochastic gradient descent (SGD) with momentum [44] was used to train the ConvNet. The gradient is estimated with the well known back-propagation algorithm [45]. We used a fixed learning rate of 0.002, a fixed minibatch size of 256, and a fixed momentum coefficient of 0.9. The validation set is used for monitoring the training process and selecting hyperparameters. We report performance using a classifier whose parameters are chosen according to the best observed validation set accuracy. The filters and fully-connected weight matrices of the ConvNets are initialized with values selected from a uniform random distribution on an interval that is a function of the number of pre-synaptic and postsynaptic units (see [46] for more detail)."}, {"heading": "4.2. Training data extraction", "text": "In the sliding window classification pipeline, the classifier takes a local window as its input. Therefore we need to extract small local patches from the original high-resolution to train the classifier. This is performed in a memoryefficient manner, using pointer arithmetic to create \u201cviews\u201d to the data as opposed to storing all patches in memory."}, {"heading": "4.2.1. Positive patches", "text": "Here, \u201cpositive patch\u201d refers to patches derived from manually labelled bounding boxes, where each one represents a codling moth. As the the ConvNet processes square inputs, we ignored the original aspect ratio of the manually labelled bounding boxes, and took the square region having the same centre as the original rectangular bounding box. Figure 5a shows positive patches extracted from the training set."}, {"heading": "4.2.2. Negative patches", "text": "It would be difficult to cover all the kinds of false positives that may arise by simply sampling the regions not covered by the labelled bounding boxes. This is because the area of regions not containing moths is much larger than the area covered by the bounding boxes. On images which are not very cluttered, most of the \u201cnegative\u201d area is uninteresting (e.g. trap liner).\nThus, to obtain negative training examples, we intentionally take \u201chard\u201d patches, meaning those which contain texture. Specifically, we apply the Canny edge detector [47] to find patches in \u201cnegative images\u201d, i.e., those that do not contain any moths. We set the threshold such that the number of negative patches roughly matches the number of labelled moths. Figure 5b shows a random sample of negative patches."}, {"heading": "4.2.3. Bootstrapping", "text": "After the initial set of negative patches are extracted, we use a bootstrapping approach to find useful negative training patches that can make the classifier more discriminative. In the first round of training, the initially generated patches are used to train the classifier. At test time, the false positive patches from the training set are collected, and we isolate the 6000 negative patches with highest probability assigned by the classifier. These are merged with the initially generated patches to form a new dataset for a second stage of training. One could potentially use more rounds of bootstrapping to collect more informative negative patches, but we found that including more than two training stages does not improve performance. Figure 5c shows randomly sampled patches collected in the test phase after one stage of training. For the validation set, the number of patches we collect is proportional to the number of images in the validation set."}, {"heading": "4.3. Data augmentation", "text": "For machine learning-based methods, it is usually the case that the larger the dataset, the better the generalization performance. In our case, the amount of training data, which is represented by the number of training patches, is much smaller than standard small-scale image classification datasets [40, 48] frequently used by the deep learning community, which have on the order of 50,000 training examples. Therefore, we performed data augmentation to increase the number of images for training, and also incorporate invariance to basic geometric transformations into the classifier. Based on the \u201ctop-view\u201d nature of the trap images, a certain patch will not change its class label when it is slightly translated, flipped, or rotated. Therefore, we apply these simple geometric transformations to the original patches to increase the number of training examples. For each patch, we create 8 translated copies by shifting \u00b1 3 pixels horizontally, vertically, and diagonally. We also create versions which are flipped across the horizontal- and vertical-axes. Finally, we create 4 rotated copies by rotating the original by 0, 90, 180 and 270 degrees. This produces 72 augmented patches from one original. Figure 6 shows the augmented versions which are produced from a single example."}, {"heading": "4.4. Detection", "text": "At the detection stage, we need to set the stride, which means the distance between adjacent sliding windows. A smaller stride means denser patch coverage, which lead to better localization of the moths, but also requires more computation. As a trade-off, we set the stride to be 14 the size of a patch."}, {"heading": "4.5. Evaluation protocol", "text": "Pest detection is still a relatively \u201cniche\u201d area of computer vision and therefore there is no standard evaluation protocol defined. We decided to adopt a protocol inspired by standardization within the pedestrian detection community. A complete overview is provided in [49] which we summarize below."}, {"heading": "4.5.1. Matching detections with ground truth", "text": "We evaluate detection performance based on the statistics of misdetections, correct detections and false positives. Here, a misdetection refers to a manually labelled region which is missed by the algorithm, and a false positive refers a bounding box proposed by the algorithm which does not correspond to any manually labelled region. To determine if a bounding box proposed by the detector is a correct detection or a misdetection, we determine its correspondence to a a manually labelled bounding box by calculating the intersection-over-minimum (IOMin) heuristic:\nA(BBdt, BBgt) = area(BBdt \u2229 BBgt)\nmin(area(BBdt), area(BBgt)) , (4)\nwhere BBdt represents a bounding box proposed by the algorithm (a detection) and BBgt represents a ground truth bounding box. We consider a specific ground truth bounding box, BBigt, to be correctly detected when there exists a detection, BB jdt, such that A(BB j dt, BB i gt) > 0.5. Otherwise, that ground truth bounding box is considered to be a misdetection. When multiple BBdt\u2019s satisfy the condition, the one with the highest probability under the classifier is chosen. After this is performed for all of the BBgt, the remaining unmatched BBdt are considered to be false positives.\nOur evaluation metric differs from that used by the pedestrian detection in that we use IOMin in place of the more popular Jaccard index, also called intersection-over-union (IOU). This is because of potential shape mismatches between the ground truth and detections. The ground truth bounding boxes are all rectangles, but our classifier outputs probabilities over square patches. In the case when the ground truth rectangle is nearly square, the IOU works well. In the case where the ground truth rectangle is tall or wide, however, the IOU tends to be small no matter how good the detection is. On the contrary, the IOMin heuristic performs well for both cases."}, {"heading": "4.5.2. Object level evaluation", "text": "Based on the statistics of correct detections (also known as true positives), misdetections (also known as false negatives) and false positives, we could evaluate the performance at two levels: (1) object level, where the focus is on the performance of detecting individual moths; and (2) image level, where the focus is on determining whether or not an image contains any moths.\nAt the object level, we use five threshold-dependent measures: miss rate, false positives per image (FPPI), precision, recall, and F\u03b2 score:\nmiss rate = number of misdetections\ntotal number of moths (5)\nFPPI = number of false positives total number of images\n(6)\nprecision = number of correct detections total number of detections\n(7)\nrecall = number of correct detections\ntotal number of moths (8)\nF\u03b2 = (1 + \u03b22) precision \u00b7 recall\n\u03b22 \u00b7 precision + recall (9)\nAll quantities, with the exception of FPPI, are calculated by including correspondences across the entire dataset, and do not represent averages over individual images.\nThere are two pairs of metrics that measure the trade-off between reducing misdetections and reducing false positives: miss rate vs. FPPI, and precision vs. recall. Miss rate vs. FPPI is a common performance measure in the pedestrian detection community [49]. It gives an estimate of the system accuracy under certain tolerances specified by\nthe number of false positives. Similarly, a precision vs. recall plot shows the trade-off between increasing the accuracy of detection and reducing misdetections.\nThe F\u03b2 score is simply a measure which aims to weight the importance of precision and recall at a single operating point along the precision-recall curve. The larger it is, the better the performance. The parameter \u03b2 adjusts the importance between precision and recall. In this paper, we consider detecting all moths more important than reducing false positives2, and therefore we more heavily weight recall, setting \u03b2 = 2 for all reported results.\nOf course, there will be one F\u03b2 score for each operating point (threshold) of the system. To summarize the information conveyed by the miss rate vs. FPPI and precision vs. recall plots by a single value, we employ two scalar performance measures: (1) log-average miss rate when FPPI is in the range [1, 10], and (2) area under the precisionrecall curve (AUC)."}, {"heading": "4.5.3. Image level evaluation", "text": "Image level performance evaluation is considered for the scenario of semi-automatic detection, where the algorithm proposes images for a technician to inspect and safely ignores images that do not contain any moths. In this setting, the algorithm simply needs to make a proposal of \u201cmoths\u201d or \u201cno moths\u201d per image regardless of how many moths it believes are present. Here we will call a \u201ctrue moth\u201d image an image that contains at least one moth, and a \u201cno moth\u201d image an image that contains no moths. Similar to Object Level Evaluation, there are five threshold-dependent measures: sensitivity (synonymous with recall), specificity, precision, and F\u03b2 score:\nsensitivity = recall = number of correctly identified true moth images\ntotal number of true moth images (10)\nspecificity = number of correctly identified no moth images\ntotal number of no moth images (11)\nprecision = number of correctly identified true moth images\ntotal number of moth image proposals (12)\nwhere F\u03b2 is defined in Eq. 9. Similar to object level evaluation, there are also two pairs of trade-offs: sensitivity vs. specificity, and precision vs. recall. For scalar performance measures, we calculate the AUC for both of these curves."}, {"heading": "5. Results", "text": "In this section, we first give some visual (qualitative) results and then describe the results of the performance evaluation introduced in Section 4."}, {"heading": "5.1. Qualitative", "text": "Figure 7 shows an example of our detector in operation. In both panels, the image on the left shows manually annotated bounding boxes in green and the proposals of our detector in magenta. The image on the right shows the results of matching annotations with proposals. Misdetections, false positives and correct detections are shown in blue, red, and yellow boxes respectively. In Figure 7a and 7b, the thresholds are set to maximize the F2-score at the object level and the image level respectively. Figures 9 and 10 show more examples of detection results on full-sized images."}, {"heading": "5.2. Quantitative", "text": "We chose logistic regression as a baseline3 for comparison to ConvNets. We tested both logistic regression and ConvNets at five different input sizes: 21\u00d721, 28\u00d728, 35\u00d735, 42\u00d742, and 49\u00d749. The results are shown in Table 2.\n2In this domain, the cost of not responding to a potential pest problem outweighs that of unnecessarily applying a treatment. 3We also tried a popular vision pipeline of local feature descriptors (SIFT [50]), followed by bag of visual words and a support vector machine\nclassifier. This approach did not give reasonable detection performance and is thus not reported.\nThe ConvNet with input size 21\u00d721 achieved the best performance at the object level and the ConvNet with input size 35\u00d735 had the best performance at the image level. Accordingly, Figure 8 shows different performance curves comparing the best performing ConvNet and logistic regression, both at the object and image level. Apparent from Figure 8 (d) and (e), the ConvNet achieved nearly perfect results at the image level. For the precision-recall curve, one usually expects precision to decrease as recall increases. Here, in Figure 8 (b) and (e), precision sometimes increases as the recall increases. This is because when the threshold is decreasing, it is possible that the newly included detections are all true detections, which results in an increase in both precision and recall.\nTo understand the effect of data augmentation (Section 4.3) on detector performance, we performed experiments on the ConvNet with input size 21\u00d721 by either using (1) both rotational and translational augmentation; (2) only rotational augmentation; (3) only translational augmentation; and (4) no augmentation. The results are shown in Table 3. We observed that both translational and rotational augmentation improved the performance compared to no augmentation at all. Using both translational and rotational augmentation improved performance at the object level but not at the image level where a single type of augmentation was sufficient.\nWe also evaluated the performance of the proposed method under limited training data, as shown in Table 4. We can see that the algorithm maintains a reasonable performance even when 80% of the training data are removed. This also indicates the effectiveness of the data augmentation strategy.\nSome of the moths in the dataset are occluded by other moths, which increased the difficulty of detection. If we\ncompletely remove occlusion from the dataset, by removing both occluded ground truths and detections with more than 50% overlapping with any of the occluded ground truths during evaluation, we achieve a slight performance improvement at the object level. Here, the precision-recall AUC increases from 0.931 to 0.934, and the log-average miss rate decreases from 0.099 to 0.0916. This indicates that our algorithm will perform even better in well-managed sites, where trap liners are changed often, resulting in less occlusions."}, {"heading": "5.3. Individual detection results", "text": "Figure 11 shows various image patches with different detection outcomes, including Figure 11a which shows correct detections, 11b which shows misdetections and 11c which shows false positives. These patches are all at 100\u00d7100 resolution. They are extracted based on the detection results using an input size of 21\u00d721. We can see that moth images show a high degree of variability due to multiple factors, including different wing poses, occlusion by other objects, different decay conditions, different illumination conditions, different background textures, and different blurring conditions. Some of these moths are successfully detected under these distorting factors and some are ignored by the detector. From Figure 11c, we can also see that inside the 21\u00d721 window considered by the classifier, some of the false positives are, to some extent, visually similar to the 21\u00d721 image patches that actually are moths. Although for a human reader, it seems easier to distinguish moth vs. non-moth by looking at the entire 100 \u00d7 100 patch (i.e. by considering context). This suggests that incorporating information from the peripheral region could help improve detection performance."}, {"heading": "6. Discussion", "text": "Compared to the majority of previous work, the proposed method relies more on data, and less on human knowledge. No knowledge about codling moths was considered in the design of our approach. The network learned to identify codling moths based only on positive and negative training examples. This characteristic makes it easier for the system to adapt to new pest species and new environments, without much manual effort, as long as relevant data is provided.\nErrors caused by different factors were analysed in Section 5.3. Many of them are related to time. The same moth could have different wing poses, levels of occlusion, illumination and decay conditions over time. Visual texture can also be related to time. For example, decaying insects could make the originally white trap liner become dirty, and reduce the contrast between moths and background. Errors caused by time-related factors could be largely avoided in real production systems, if temporal image sequences are provided with reasonable frequency. This leads to one possible future research direction, that is, to reason over image sequences while detecting moths on a single image, exploiting temporal correspondence. Errors caused by blurry images could potentially be solved by adding deblurring filters in the preprocessing pipeline. Non-moth objects contribute to a certain amount of false positives. One way to address this problem is to train detectors for common non-moth objects and combine it with the moth detector. Common objects here include pheromone lures, flies and leaves. This would also require a dataset with richer labelled information.\nAs a preliminary attempt on automatic pest detection from trap images, the methods introduced in this paper have many possible future extensions besides those have been mentioned based on error analysis. Deeper convolutional networks [37] could be applied to provide more accurate image patch classification. Detecting and classifying multiple types of insects would be a natural extension, which is closely related to the fine-grained image classification problem\n[51, 52]. The location information of detections could potentially be refined by proposing rectangular bounding boxes, polygons, or parameterized curves representing insect shapes."}, {"heading": "7. Conclusions", "text": "This paper describes an automatic method for monitoring pests from trap images. We propose a sliding windowbased detection pipeline, where a convolutional neural network is applied to image patches at different locations to determine the probability of containing a specific pest type. Image patches are then filtered by non-maximum suppression and thresholding, according to their locations and associated confidences, to produce the final detections. Qualitative and quantitative experiments demonstrate the effectiveness of the proposed method on a codling moth dataset. We also analysed detection errors, with corresponding influences to real production systems and potential future directions for improvements."}, {"heading": "Acknowledgements", "text": "This work was funded by the Natural Sciences and Engineering Research Council (NSERC) EGP 453816-13, EGP 453816-14, and an industry partner whose name was withheld by request. We would also like to thank Dr. Rebecca Hallett, Jordan Hazell and the industry partner for assistance with data collection."}], "references": [{"title": "Control of moth pests by mating disruption: successes and constraints", "author": ["R.T. Carde", "A.K. Minks"], "venue": "Annual review of entomology 40 (1) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Sex pheromones and their impact on pest management", "author": ["P. Witzgall", "P. Kirsch", "A. Cork"], "venue": "Journal of chemical ecology 36 (1) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Identification of butterfly species with a single neural network system", "author": ["S.-H. Kang", "S.-H. Song", "S.-H. Lee"], "venue": "Journal of Asia-Pacific Entomology 15 (3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Identification of butterfly based on their shapes when viewed from different angles using an artificial neural network", "author": ["S.-H. Kang", "J.-H. Cho", "S.-H. Lee"], "venue": "Journal of Asia-Pacific Entomology 17 (2) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Biodiversity informatics in action: identification and monitoring of bee species using abis", "author": ["T. Arbuckle", "S. Schroder", "V. Steinhage", "D. Wittmann"], "venue": "in: Proc. 15th Int. Symp. Informatics for Environmental Protection, Vol. 1, Citeseer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "M", "author": ["P. Weeks"], "venue": "O\u00a2\u00a2Neill, K. Gaston, I. Gauld, Automating insect identification: exploring the limitations of a prototype system, Journal of Applied Entomology 123 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Drawwing", "author": ["A. Tofilski"], "venue": "a program for numerical description of insect wings, Journal of Insect Science 4 (1) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "A new automatic identification system of insect images at the order level", "author": ["J. Wang", "C. Lin", "L. Ji", "A. Liang"], "venue": "Knowledge-Based Systems 33 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "E", "author": ["N. Larios", "H. Deng", "W. Zhang", "M. Sarpola", "J. Yuen", "R. Paasch", "A. Moldenke", "D.A. Lytle", "S.R. Correa"], "venue": "N. Mortensen, et al., Automated insect identification through concatenated histograms of local appearance features: feature vector generation and region detection for deformable objects, Machine Vision and Applications 19 (2) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["G. Martinez-Munoz", "N. Larios", "E. Mortensen", "W. Zhang", "A. Yamamuro", "R. Paasch", "N. Payet", "D. Lytle", "L. Shapiro"], "venue": "Todorovic, et al., Dictionary-free categorization of very similar objects via stacked evidence trees, in: Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "G", "author": ["N. Larios", "B. Soran", "L.G. Shapiro"], "venue": "Mart\u0131\u0301nez-Mu\u00f1oz, J. Lin, T. G. Dietterich, Haar random forest features and svm spatial matching kernel for stonefly species identification., in: ICPR, Vol. 1", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "G", "author": ["D.A. Lytle"], "venue": "Mart\u0131\u0301nez-Mu\u00f1oz, W. Zhang, N. Larios, L. Shapiro, R. Paasch, A. Moldenke, E. N. Mortensen, S. Todorovic, T. G. Dietterich, Automated processing and identification of benthic invertebrate samples, Journal of the North American Benthological Society 29 (3) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Identification of pecan weevils through image processing", "author": ["S. Al-Saqer", "P. Weckler", "J. Solie", "M. Stone", "A. Wayadande"], "venue": "American Journal of Agricultural and Biological Sciences 6 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "M", "author": ["J. Cho", "J. Choi"], "venue": "Qiao, C.-w. Ji, H.-y. Kim, K.-b. Uhm, T.-s. Chon, Automatic identification of whiteflies, aphids and thrips in greenhouse based on image analysis, Red 346 (246) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic species identification of live moths", "author": ["M. Mayo", "A.T. Watson"], "venue": "Knowledge-Based Systems 20 (2) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Auto-classification of insect images based on color histogram and glcm", "author": ["Z. Le-Qing", "Z. Zhen"], "venue": "in: Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, Vol. 6, IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Application of artificial neural network for automatic detection of butterfly species using color and texture features", "author": ["Y. Kaya", "L. Kayci"], "venue": "The Visual Computer 30 (1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Thrips (thysanoptera) identification using artificial neural networks", "author": ["P. Fedor", "I. Malenovsk\u1ef3", "J. Va\u0148hara", "W. Sierka", "J. Havel"], "venue": "Bulletin of entomological research 98 (05) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "An insect classification analysis based on shape features using quality threshold artmap and moment invariant", "author": ["S.N. Yaakob", "L. Jain"], "venue": "Applied Intelligence 37 (1) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Local feature-based identification and classification for orchard insects", "author": ["C. Wen", "D.E. Guyer", "W. Li"], "venue": "Biosystems engineering 104 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Image-based orchard insect automated identification and classification method", "author": ["C. Wen", "D. Guyer"], "venue": "Computers and Electronics in Agriculture 89 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Insect species recognition using discriminative local soft coding", "author": ["A. Lu", "X. Hou", "C.-L. Liu", "X. Chen"], "venue": "in: Pattern Recognition (ICPR), 2012 21st International Conference on, IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Stacked spatial-pyramid kernel: An object-class recognition method to combine scores from random trees", "author": ["N. Larios", "J. Lin", "M. Zhang", "D. Lytle", "A. Moldenke", "L. Shapiro", "T. Dietterich"], "venue": "in: Applications of Computer Vision (WACV), 2011 IEEE Workshop on, IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Knn-spectral regression lda for insect recognition", "author": ["L. Xiao-Lin", "H. Shi-Guo", "Z. Ming-Quan", "G. Guo-Hua"], "venue": "in: Information Science and Engineering (ICISE), 2009 1st International Conference on, IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Detection of insects in bulk wheat samples with machine vision", "author": ["I. Zayas", "P. Flinn"], "venue": "Transactions of the ASAE-American Society of Agricultural Engineers 41 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Rapid machine vision method for the detection of insects and other particulate bio-contaminants of bulk grain in transit", "author": ["C. Ridgway", "E. Davies", "J. Chambers", "D. Mason", "M. Bateman"], "venue": "Biosystems engineering 83 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "L", "author": ["Y. Qing"], "venue": "Jun, Q.-j. LIU, G.-q. DIAO, B.-j. YANG, H.-m. CHEN, T. Jian, An insect imaging system to automate rice light-trap pest identification, Journal of Integrative Agriculture 11 (6) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Segmentation of touching insects based on optical flow and ncuts", "author": ["Q. Yao", "Q. Liu", "T.G. Dietterich", "S. Todorovic", "J. Lin", "G. Diao", "B. Yang", "J. Tang"], "venue": "Biosystems Engineering 114 (2) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Object class detection: A survey", "author": ["X. Zhang", "Y.-H. Yang", "Z. Han", "H. Wang", "C. Gao"], "venue": "ACM Computing Surveys (CSUR) 46 (1) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "50 years of object recognition: Directions forward", "author": ["A. Andreopoulos", "J.K. Tsotsos"], "venue": "Computer Vision and Image Understanding 117 (8) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86 (11) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-task bayesian optimization", "author": ["K. Swersky", "J. Snoek", "R.P. Adams"], "venue": "in: Advances in Neural Information Processing Systems", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Applicability of white-balancing algorithms to restoring faded colour slides: An empirical evaluation", "author": ["D. Nikitenko", "M. Wirth", "K. Trudel"], "venue": "Journal of Multimedia 3 (5) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Computer Science Department, University of Toronto, Tech. Rep 1 (4) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep convolutional networks for scene parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "in: ICML 2009 Deep Learning Workshop, Vol. 3, Citeseer", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A", "author": ["L. Fei-Fei"], "venue": "Karpathy, Cs231n: Convolutional neural networks for visual recognition ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 32 (9) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "in: Neural networks: Tricks of the trade, Springer", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Tech. rep., DTIC Document ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1985}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "in: International conference on artificial intelligence and statistics", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "A computational approach to edge detection", "author": ["J. Canny"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on (6) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1986}, {"title": "C", "author": ["Y. LeCun"], "venue": "Cortes, The mnist database of handwritten digits ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1998}, {"title": "Pedestrian detection: An evaluation of the state of the art", "author": ["P. Dollar", "C. Wojek", "B. Schiele", "P. Perona"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 34 (4) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision 60 (2) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning fine-grained image similarity with deep ranking", "author": ["J. Wang", "Y. Song", "T. Leung", "C. Rosenberg", "J. Wang", "J. Philbin", "B. Chen", "Y. Wu"], "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, IEEE", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "in: Computer Vision and Pattern Recognition Workshops ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Monitoring is a crucial component in pheromone-based pest control [1, 2] systems.", "startOffset": 66, "endOffset": 72}, {"referenceID": 1, "context": "Monitoring is a crucial component in pheromone-based pest control [1, 2] systems.", "startOffset": 66, "endOffset": 72}, {"referenceID": 2, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 3, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 4, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 5, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 6, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 7, "context": "In terms of image sources, many previous methods have considered insect specimens [3, 4, 5, 6, 7, 8].", "startOffset": 82, "endOffset": 100}, {"referenceID": 8, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 9, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 10, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 11, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 12, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 13, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 14, "context": "In a less ideal but more practical scenario, some other works attempt to classify insects collected in the wild, but imaged under laboratory conditions [9, 10, 11, 12, 13, 14, 15].", "startOffset": 152, "endOffset": 179}, {"referenceID": 2, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 3, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 4, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 5, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 6, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 127, "endOffset": 142}, {"referenceID": 15, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 16, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 170, "endOffset": 178}, {"referenceID": 17, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 18, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 6, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 7, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 206, "endOffset": 220}, {"referenceID": 15, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 16, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 19, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 20, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 21, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 22, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 243, "endOffset": 267}, {"referenceID": 23, "context": "From an algorithmic perspective, various types of features have been used for insect classification, including wing structures [3, 4, 5, 6, 7], colour histogram features [16, 17], morphometric measurements [18, 19, 7, 8], local image features [16, 17, 20, 21, 22, 23], and global image features [24].", "startOffset": 295, "endOffset": 299}, {"referenceID": 19, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 129, "endOffset": 140}, {"referenceID": 7, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 129, "endOffset": 140}, {"referenceID": 10, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 129, "endOffset": 140}, {"referenceID": 7, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 175, "endOffset": 186}, {"referenceID": 16, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 175, "endOffset": 186}, {"referenceID": 17, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 175, "endOffset": 186}, {"referenceID": 23, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 215, "endOffset": 223}, {"referenceID": 20, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 215, "endOffset": 223}, {"referenceID": 8, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 246, "endOffset": 257}, {"referenceID": 9, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 246, "endOffset": 257}, {"referenceID": 20, "context": "Different classifiers were also used on top of these various feature extraction methods, including support vector machines (SVM) [20, 8, 11], artificial neural networks (ANN) [8, 17, 18], k-nearest neighbours (KNN) [24, 21], and ensemble methods [9, 10, 21].", "startOffset": 246, "endOffset": 257}, {"referenceID": 24, "context": "This technique was applied for inspection of bulk wheat samples [25], where local patches from the original image were represented by engineered features and classified by discriminant analysis.", "startOffset": 64, "endOffset": 68}, {"referenceID": 25, "context": "Another work on bulk grain inspection [26] employed different customized rule-based algorithms to detect", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "These candidates are then represented by engineered features and classified [27, 28].", "startOffset": 76, "endOffset": 84}, {"referenceID": 27, "context": "These candidates are then represented by engineered features and classified [27, 28].", "startOffset": 76, "endOffset": 84}, {"referenceID": 28, "context": "Various methods and datasets [29, 30] have been proposed in the last several decades to push this area forward.", "startOffset": 29, "endOffset": 37}, {"referenceID": 29, "context": "Various methods and datasets [29, 30] have been proposed in the last several decades to push this area forward.", "startOffset": 29, "endOffset": 37}, {"referenceID": 30, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 51, "endOffset": 59}, {"referenceID": 31, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 51, "endOffset": 59}, {"referenceID": 32, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 234, "endOffset": 246}, {"referenceID": 33, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 234, "endOffset": 246}, {"referenceID": 31, "context": "Recently, convolutional neural networks (ConvNets) [31, 32] and their variants have emerged as the most effective method for object recognition and detection, by achieving state-of-the-art performance on many well recognized datasets [33, 34, 35], and winning different object recognition challenges [36, 32, 37].", "startOffset": 300, "endOffset": 312}, {"referenceID": 34, "context": "To eliminate the potential negative effects of illumination variability on detection performance, we perform colour correction using one variant [38] of the \u201cgrey-world\u201d method.", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "The classifier\u2019s output is a single scalar p \u2208 [0, 1], which represents the probability that a particular patch contains a codling moth.", "startOffset": 47, "endOffset": 53}, {"referenceID": 34, "context": "(a) Original images (b) Images processed by the \u201cgrey world\u201d algorithm [38].", "startOffset": 71, "endOffset": 75}, {"referenceID": 30, "context": "We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems.", "startOffset": 177, "endOffset": 185}, {"referenceID": 35, "context": "We adopt convolutional neural network [31] (ConvNet) as our image classifier, as it is the most popular and best performing classifier for image recognition in both large scale [32, 37] and small scale [39, 40] problems.", "startOffset": 202, "endOffset": 210}, {"referenceID": 30, "context": "Specifically, we used a network structure similar to Lenet5 [31].", "startOffset": 60, "endOffset": 64}, {"referenceID": 36, "context": "where l is the layer index; m is the index of input feature maps; k is the index of output feature maps; input hl\u22121 m is the mth feature map at layer l \u2212 1; output hk the kth feature map at layer l; W is the convolutional weight tensor; b is the bias term; and we choose the element-wise nonlinearity \u03c6(\u00b7) to be the rectified linear unit (RELU) [41] function.", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "For a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42].", "startOffset": 89, "endOffset": 101}, {"referenceID": 31, "context": "For a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42].", "startOffset": 89, "endOffset": 101}, {"referenceID": 37, "context": "For a more detailed explanation of convolutional neural networks, we refer the reader to [31, 32, 42].", "startOffset": 89, "endOffset": 101}, {"referenceID": 38, "context": "We adopted a strategy similar to [43].", "startOffset": 33, "endOffset": 37}, {"referenceID": 39, "context": "Minibatch stochastic gradient descent (SGD) with momentum [44] was used to train the ConvNet.", "startOffset": 58, "endOffset": 62}, {"referenceID": 40, "context": "The gradient is estimated with the well known back-propagation algorithm [45].", "startOffset": 73, "endOffset": 77}, {"referenceID": 41, "context": "The filters and fully-connected weight matrices of the ConvNets are initialized with values selected from a uniform random distribution on an interval that is a function of the number of pre-synaptic and postsynaptic units (see [46] for more detail).", "startOffset": 228, "endOffset": 232}, {"referenceID": 42, "context": "Specifically, we apply the Canny edge detector [47] to find patches in \u201cnegative images\u201d, i.", "startOffset": 47, "endOffset": 51}, {"referenceID": 35, "context": "In our case, the amount of training data, which is represented by the number of training patches, is much smaller than standard small-scale image classification datasets [40, 48] frequently used by the deep learning community, which have on the order of 50,000 training examples.", "startOffset": 170, "endOffset": 178}, {"referenceID": 43, "context": "In our case, the amount of training data, which is represented by the number of training patches, is much smaller than standard small-scale image classification datasets [40, 48] frequently used by the deep learning community, which have on the order of 50,000 training examples.", "startOffset": 170, "endOffset": 178}, {"referenceID": 44, "context": "A complete overview is provided in [49] which we summarize below.", "startOffset": 35, "endOffset": 39}, {"referenceID": 44, "context": "FPPI is a common performance measure in the pedestrian detection community [49].", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "recall plots by a single value, we employ two scalar performance measures: (1) log-average miss rate when FPPI is in the range [1, 10], and (2) area under the precisionrecall curve (AUC).", "startOffset": 127, "endOffset": 134}, {"referenceID": 9, "context": "recall plots by a single value, we employ two scalar performance measures: (1) log-average miss rate when FPPI is in the range [1, 10], and (2) area under the precisionrecall curve (AUC).", "startOffset": 127, "endOffset": 134}, {"referenceID": 45, "context": "3We also tried a popular vision pipeline of local feature descriptors (SIFT [50]), followed by bag of visual words and a support vector machine classifier.", "startOffset": 76, "endOffset": 80}, {"referenceID": 46, "context": "[51, 52].", "startOffset": 0, "endOffset": 8}, {"referenceID": 47, "context": "[51, 52].", "startOffset": 0, "endOffset": 8}], "year": 2016, "abstractText": "Monitoring the number of insect pests is a crucial component in pheromone-based pest management systems. In this paper, we propose an automatic detection pipeline based on deep learning for identifying and counting pests in images taken inside field traps. Applied to a commercial codling moth dataset, our method shows promising performance both qualitatively and quantitatively. Compared to previous attempts at pest detection, our approach uses no pest-specific engineering which enables it to adapt to other species and environments with minimal human effort. It is amenable to implementation on parallel hardware and therefore capable of deployment in settings where real-time performance is required.", "creator": "LaTeX with hyperref package"}}}