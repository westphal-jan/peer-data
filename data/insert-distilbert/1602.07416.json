{"id": "1602.07416", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Learning to Generate with Memory", "abstract": "memory units have been widely used to enrich the capabilities of deep networks on capturing long - term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models ( dgms ) which are good at inferring explicit high - level invariant representations from unlabeled data. this paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom - up abstraction process in representation learning. by adopting a smooth attention model, the whole tree network is trained end - to - end by optimizing a variational bound of data likelihood via auto - encoding variational bayesian methods, where typically an asymmetric recognition network is learnt jointly processed to infer high - level and invariant representations. the asymmetric architecture can reduce the competition between bottom - up invariant feature extraction and top - bottom down generation of instance details. our experiments on employing several common datasets demonstrate that memory can significantly boost up the performance of dgms and even achieve state - of - the - art results on various tasks, principally including density estimation, image generation, and missing value imputation.", "histories": [["v1", "Wed, 24 Feb 2016 06:57:14 GMT  (722kb,D)", "http://arxiv.org/abs/1602.07416v1", null], ["v2", "Sat, 28 May 2016 03:41:27 GMT  (943kb,D)", "http://arxiv.org/abs/1602.07416v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["chongxuan li", "jun zhu", "bo zhang"], "accepted": true, "id": "1602.07416"}, "pdf": {"name": "1602.07416.pdf", "metadata": {"source": "META", "title": "Learning to Generate with Memory", "authors": ["Chongxuan Li", "Jun Zhu", "Bo Zhang"], "emails": ["LICX14@MAILS.TSINGHUA.EDU.CN", "DCSZJ@TSINGHUA.EDU.CN", "DCSZB@TSINGHUA.EDU.CN"], "sections": [{"heading": "1. Introduction", "text": "Deep learning models are able to extract abstract representations from low-level inputs by adopting a deep architecture with explicitly designed nonlinear transformations (Bengio et al., 2013a). Among many types of deep models, deep generative models (DGMs) learn abstract representations from unlabeled data and can perform a wide range of tasks, including density estimation, data generation and missing value imputation. Depending on the building blocks, various types of DGMs exist, including\nundirected models (Salakhutdinov & Hinton, 2009), directed models (Neal, 1992; Hinton et al., 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al., 2014), and Markov chain based models (Bengio et al., 2014). Recently, DGMs have attracted much attention on developing efficient and (approximately) accurate learning algorithms, such as stochastic variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Bornschein & Bengio, 2015; Burda et al., 2015) and Monte Carlo methods (Adams et al., 2010; Gan et al., 2015).\nAlthough current DGMs are able to extract high-level abstract representations, they may not be sufficient in generating high-quality input samples. This is because more abstract representations are generally invariant or less sensitive to most specific types of local changes of the input. This bottom-up abstraction progress is good for identifying predictive patterns, especially when a discriminative objective is optimized (Li et al., 2015); but it also loses the detail information that is necessary in the top-down generating process. It remains a challenge for DGMs to generate real data, especially for images that have complex structures. Simply increasing the model size is apparently not wise, as it may lead to serious over-fitting without proper regularization as well as heavy computation burden. Some recent progress has been made to improve the generation quality. For example, DRAW (Gregor et al., 2015) iteratively constructs complex images over time through a recurrent encoder and decoder together with an attention mechanism and LAPGAN (Denton et al., 2015) employs a cascade of generative adversarial networks (GANs) (Goodfellow et al., 2014) to generate high quality natural images through a Laplacian pyramid framework (Burt & Adelson, 1983). But none efforts have been made on enriching the capabilities of probabilistic DGMs by designing novel building blocks in the generative model.\nIn this paper, we address the above challenges by presenting a new architecture for building probabilistic deep generative models with a possibly large external memory and an attention mechanism. Although memory has been ex-\nar X\niv :1\n60 2.\n07 41\n6v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\nplored in various deep models for capturing long-term dependencies in reasoning and prediction tasks (See Section 2 for a review), our work represents a first attempt to leverage external memory to enrich the capabilities of probabilistic DGMs for better density estimation, data generation and missing value imputation. The overall architecture of our model is an interleave between stochastic layers and deterministic layers, where each deterministic layer is associated with an external memory to capture local variant information. An attention mechanism is used to record information in the memory during learning and retrieve information from the memory during data generation. This attention mechanism can be trained because the invariant information and local variant information are correlated, e.g., both containing implicit label information. Both the memory and attention mechanisms are parameterized as differentiable components with some smooth nonlinear transformation functions. Such a design allows us to learn the whole network end-to-end by developing a stochastic variational method, which introduces a recognition network without memory to characterize the variational distribution. Different from (Kingma & Welling, 2014; Burda et al., 2015), our recognition network is asymmetric to the generative network. This asymmetric recognition network is sufficient for extracting invariant representations in bottom-up inference, and is compact in parameterization. Furthermore, this asymmetry can help reduce the competition between bottom-up invariant feature extraction (using the recognition network) and top-down input generation (using the deep generative model with memory).\nWe quantitatively and qualitatively evaluate our method on several datasets in various tasks, including density estimation, data generation and missing value imputation. Our results demonstrate that an external memory together with a proper attention mechanism can significantly improve DGMs to obtain state-of-the-art performance."}, {"heading": "2. Related Work", "text": "Memory has recently been leveraged in deep models to capture long-term dependencies for various tasks, such as algorithm inference (Graves et al., 2014), question answering (Weston et al., 2015; Sukhbaatar et al., 2015) and neural language transduction (Grefenstette et al., 2015). The external memory in these models provides a way to record information stably and interact with the environment, and hence extends the capability of traditional learning models. Typically the interaction, e.g., reading from and writing on the memory, is done through an associated attention mechanism and the whole system is trained with supervision. The attention mechanism can be differentiable and trained in an end-to-end manner (Graves et al., 2014; Sukhbaatar et al., 2015), or really discrete and trained by a Reinforce-\nment Learning algorithm (Zaremba & Sutskever, 2015).\nIn addition to the memory-based models mentioned above, attention mechanisms have been used in other deep models for various tasks, such as image classification (Larochelle & Hinton, 2010; Ba et al., 2015), object tracking (Mnih et al., 2014), conditional caption generation (Xu et al., 2015), machine translation (Bahdanau et al., 2015) and image generation (Graves, 2013; Gregor et al., 2015). Recently, DRAW (Gregor et al., 2015) introduces a novel 2-D attention mechanism to decide \u201cwhere to read and write\u201d on the image and does well in generating objects with clear track, such as handwritten digits and sequences of real digits.\nCompared with previous memory-based networks (Graves et al., 2014; Weston et al., 2015), we propose to employ an external hierarchical memory to capture variant information at different abstraction levels trained in an unsupervised manner. Besides, our memory cannot be written directly like (Graves et al., 2014; Weston et al., 2015); instead it is updated through optimization. Compared with previous DGMs with visual attention (Tang et al., 2014; Gregor et al., 2015), we make different assumptions about the data, i.e., the main object has massive local features, which cannot be modeled by a limited number of latent factors. We employ an external memory to capture this and the associated attention mechanism is used to retrieve the memory, not to learn \u201cwhat-where\u201d combination on the images.\nConsidering the bottom-up inference procedure and topdown generation procedure together, additional memory mechanisms can help to reduce the competition between invariant feature extraction and local variant reconstruction, especially when label information is provided (e.g., in supervised or semi-supervised setting). Similar idea is highlighted in the Ladder Network (Valpola, 2014; Rasmus et al., 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al., 2010) with the help of lateral connections and achieves excellent performance on semi-supervised learning (Rasmus et al., 2015). Though it is possible to interpret the Ladder Network probabilistically as in (Bengio et al., 2013b), we model the data likelihood directly with the help of external memory instead of explicit lateral edges. Our method can also be extended to do supervised or semisupervised learning as in (Kingma et al., 2014), which is our future work."}, {"heading": "3. Probabilistic DGMs with Memory", "text": "We present a probabilistic deep generative model (DGM) with a possibly large external memory as well as a soft attention mechanism."}, {"heading": "3.1. Overall Architecture", "text": "Formally, given a set of training data D, we assume each x \u2208 D is independently generated with a set of hierarchically organized latent factors zL, . . . , z1 as follows:\n\u2022 Draw the top-layer factors zL \u223c N (0, I). \u2022 For l = L \u2212 1, . . . , 0, calculate the mean parameters \u00b5l = gl(zl+1;Ml) and draw the factors zl \u223c Pl(\u00b5l),\nwhere each gl is a nonlinear function, often assumed to be smooth for the ease of learning. To connect with observations, the bottom layer is clamped at z0 = x. Each zl is randomly sampled from a Gaussian distribution except z0 whose distribution depends on the properties of the data (e.g., Gaussian for continuous data or Multinomial for discrete case). All the distributions Pl are assumed to be of an exponential family form, with mean parameters \u00b5l.\nHere, we define gl as a feed-forward deep neural network with Il deterministic layers and a set of associated memories {M (i)l } Il\u22121 i=0 , one per layer. We parameterize each memory as a trainable matrix with dimension ds \u00d7 ns, where ds is the number of slots in the memory and ns is the dimension of each slot. Then, the network is formally parameterized as follows:\n\u2022 Initialize the top-layer factors h(Il)l = zl+1. \u2022 For i = Il \u2212 1, . . . , 0, do the transformation h(i)l = \u03c6(h\n(i+1) l ;M (i) l ),\nwhere \u03c6 is a proper (e.g., smooth) function for linear or nonlinear transformation. The bottom layer is our output \u00b5l = h 0 l , which is called a stochastic layer as it computes the mean parameters for a distribution to get samples from. All the other layers are called deterministic layers.\nCompared with previous DGMs, one key feature of our model is that it incorporates an external memory at each deterministic layer, as detailed below. The overall architecture is a stack of multiple such layers interleaved with stochastic layers as above. In such a DGM architecture, memory M (i)l can recover the information that is missing in higher-layers h(>i)l . In other words, the higher layers do not need to represent all details, but focusing on representing abstract invariant features if they seem more relevant to the task at hand than the more detailed information."}, {"heading": "3.2. General Memory Mechanism for a Single Layer", "text": "We now present a single layer with memory generally, which is our building block for the above DGM. For notation simplicity, we omit the sub-script l in the following text. Formally, let hin denote the input information, and hout denote the output after some deterministic transformation with memory. In our model, hin can be either the samples of latent factors or the output from a higher-level\ndeterministic layer; and similarly hout can be used as the input of either a stochastic layer or a lower-level deterministic layer.\nA layer of standard DGMs without memory generates the low-level generative information hg based on hin through a proper transformation, which can be generally put as:\nhg = \u03c6(hin;Wg, bg),\nwhere Wg and bg are the weights and biases of the transformation respectively and uses it as the final output, i.e. hout = hg .\nIn our DGM with memory M , we first compute the lowlevel generative information hg in the same way as a standard layer, and then retrieve the memory with some proper attention mechanism to get knowledge hm. Finally, we combine hg and hm to get the output hout. Formally, the memory retrieval process is parameterized as\nhm = fm(ha;M),\nwhere ha = fa(hg;A, bA) is the information used to access the memory and computed by an attention mechanism parameterized by a controlling matrix A and a bias vector bA. The attention mechanism takes the generative information hg , which is the final output of a vanilla layer described previously, as input. fa is the mapping function in the attention mechanism and fm is the mapping function in the memory mechanism, which are deterministic transformations to be specified. The final output hout is the combination of hg and hm as follows:\nhout = fc(hg,hm;C),\nwhere C is a set of trainable parameters in the combination function fc, which is another deterministic transformation to be specified. We visualize the computation flow of these two types of layers in Fig. 1, where each component will be specified next."}, {"heading": "3.3. A Concrete Example with Hierarchical Memory Mechanisms", "text": "With the above building blocks, we can stack multiple layers to build a DGM as in Section 3.1. For simplicity, here we consider a generative model with only one stochastic layer and I deterministic layers to explain our memory mechanism, which can be straightforwardly extended to cases with multiple stochastic layers.\nLet the top most information to be the random samples from the prior, i.e., h(I+1) = z. Using permutation invariant architecture as an example, we compute the low-level generative information h(i)g based on the input h(i+1) as:\nh(i)g = \u03c6(W (i) g h (i+1) + b(i)g ).\nWe further retrieve the knowledge h(i)m from memory. Though various strategies exist, we consider the simple one that adopts a linear combination of slots in memory as\nh(i)m = fm(h (i) a ) =M (i)h(i)a ,\nwhere the coefficients h(i)a are computed as\nh(i)a = fa(h (i) g ) = \u03c3(A (i)h(i)g + b (i) A ),\nand \u03c3(x) = 1/(1 + exp(\u2212x)) is the sigmoid function. Therefore, each element of h(i)a is a real value in the interval (0, 1), which represents the preference of x to the corresponding memory slot. We can view this as an unnormalized version of a soft attention mechanism (Bahdanau et al., 2015) trainable by standard back-propagation, which could be replaced by a hard attention mechanism trained with Reinforcement Learning (Xu et al., 2015).\nInspired by the Ladder Network (Valpola, 2014; Rasmus et al., 2015), we specify the combination function of h(i)m and h(i)g as element wise multiple layer perceptron with optionally final nonlinearity \u03c6:\nh(i) = fc(h (i) g ,h (i) m ) = \u03c6(a (i) + b (i) 1 c (i)),\nwhere the inside linear part a(i) is the summation of scaled inputs and cross terms as well as biases:\na(i) = a (i) 1 + a (i) 2 h(i)m + a (i) 3 h(i)g\n+ a (i) 4 h(i)g h(i)m ,\nand the inside nonlinear part c(i) is computed similarly but goes through a sigmoid function:\nc(i) = \u03c3(c (i) 1 + c (i) 2 h(i)m + c (i) 3 h(i)g\n+ c (i) 4 h(i)g h(i)m ),\nwhere is the element wise product. The output in our model only depends on the top-down signals hg initially, instead of the auxiliary information as in the Ladder Network, which will be discussed in the experiment setting. (W (i) g , b (i) g ,M (i), A(i), b (i) A , a (i) 1,2,3,4, b (i) 1 , c (i) 1,2,3,4) are trainable parameters in single layer. We illustrate each component in Fig. 1."}, {"heading": "4. Inference and Learning", "text": "Learning a DGM is generally challenging due to the highly nonlinear transformations in multiple layers plus a stochastic formalism. To develop a variational approximation method, it is important to have a rich family of variational distributions that can well-characterize the nonlinear transforms. Significant progress has been made recently on stochastic variational inference methods with a sophisticated recognition model to parameterize the variational distributions (Kingma & Welling, 2014; Rezende et al., 2014). In this section, we develop such an algorithm for our DGM with memory.\nLet \u03b8g be the collection of parameters in the DGM. Then the joint distribution of each data x and the corresponding latent factor z can be generally put in a factorized form:\np(x, z;\u03b8g) = p(z;\u03b8g)p(x|z;\u03b8g),\nwhere the prior is often of a simple form, such as spherical Gaussian in our experiments, and the form of the conditional distribution p(x|z;\u03b8g) is chosen according to the data and its mean parameters depend on the external memories through a deep architecture as stated above.\nAs in (Kingma & Welling, 2014), we adopt deep neural networks to parameterize a recognition model as the approximate posterior distribution q(z|x;\u03b8r), where \u03b8r is the collection of the parameters in the recognition model (denoted by Q-Net, as it characterizes distribution q). Since\ntheQ-Net implements the bottom-up abstraction process to identify invariant features, it is unnecessary to have an external memory. Furthermore, the Q-Net without memory is compact in parameterization. The overall architecture is asymmetric, as illustrated in Fig. 2, where the components at the left side of the dot line together with sampling z from q(z|x) is the Q-Net and the components at the right side of the dot line with z sampled from the prior is the generative model (denoted by P -Net, as it characterizes model distribution p). The solid arrow means the corresponding component is used as input of next component and the dash arrow means the corresponding component is used as the training target of next component, as explained below. The components representing external memory and associated attention mechanisms are filled with shallow gray. We omit the components corresponding to the combination functions for better visualization.\nWe define the Q-Net as follows. Following the example with one stochastic layer and I deterministic layers in the previous section, we extract the high-level features h\u0302(i+1) as follows:\nh\u0302(i+1) = \u03c6(V (i)h\u0302(i) + b(i)r ),\nwhere \u03c6 is a proper nonlinear function and (V (i), b(i)r ) are trainable parameters. The bottom layer is the input data, i.e. h\u0302(0) = x and the top layer is still factorized Gaussian distribution. The mean of z is computed by linear transformation of h\u0302(I) and the variance of z is computed similarly but with a final exponential nonlinearity.\nA variational lower bound of log-likelihood for per data x can be formulated as:\nL(\u03b8g,\u03b8r;x) , Eq(z|x;\u03b8r)[log p(x, z;\u03b8g)\u2212log q(z|x;\u03b8r)].\nWe add local reconstruction error terms as an optional regularizer, and jointly optimize the parameters in the generative model and the recognition model:\nmin \u03b8g,\u03b8r\n1 |D| \u2211 x\u2208D ( L(\u03b8g,\u03b8r;x) + I\u2211 i=1 \u03bb(i)||h(i) \u2212 h\u0302(i)||22 ) ,\nwhere the relative weights \u03bb(i) are prefixed hyperparameters. We optimize the objective with a stochastic gradient variational Bayes (SGVB) method (Kingma & Welling, 2014). Note that we cannot send the massage of a intermediate layer in the recognition model to a layer in the generative model through a lateral connection as in Ladder Network (Valpola, 2014; Rasmus et al., 2015) because that indeed changes the distribution of p(x|z) according to the data x. However, we do not use any information of x in the generative model explicitly and the correctness of the variational bound can be verified.\nWe employ batch normalization layers (Ioffe & Szegedy, 2015) in both the recognition model and generative model to accelerate the training procedure, and the intermediate features in local reconstruction error terms are replaced by a corresponding normalized version. To compare with statof-the-art results, we also train our method as in importance weighted autoencoders (IWAE) (Burda et al., 2015), which uses importance weighting estimate of log likelihood with multiple samples in the training procedure to achieve a strictly tighter variational lower bound."}, {"heading": "5. Experiments", "text": "We now present both quantitative and qualitative evaluations of our method on the real-valued MNIST, OCR-letters and Frey faces datasets for various tasks. The MNIST dataset (Lecun et al., 1998) consists of 50,000 training, 10,000 validation and 10,000 testing images of handwritten digits and each image is of 28 \u00d7 28 pixels. The OCRletters dataset (Bache & Lichman, 2013) consists of 32,152 training, 10,000 validation and 10,000 testing letter images of size 16 \u00d7 8 pixels. The Frey faces dataset consists of 1,965 real facial expression images of size 28 \u00d7 20 pixels. We model MNIST and OCR-letters datasets as Bernoulli distribution and model Frey faces dataset as Gaussian distribution at data level.\nOur implementation is based on Theano (Bastien et al., 2012). We use ADAM (Kingma & Ba, 2015) in all experiments with parameters \u03b21 = 0.9, \u03b22 = 0.999 (decay rates of moving averages) and = 10\u22124 (a constant that prevents overflow). As a default, the global learning rate is fixed as 10\u22123 for 1,000 epochs and annealed by a factor 0.998 for 2,000 epochs with minibatch size 100. Initially, We set ai3 and c i 3 as vectors filled with ones and (a (i) 1,2,4, b (i) 1 , c (i) 1,2,4) as vectors filled with zeros to avoid poor local optima. This means that we initialize the output as signals from top-down inference, which is different from the Ladder Network (Rasmus et al., 2015). We initialize the memory matrix as Gaussian random variables and other parameters following (Glorot & Bengio, 2010). We specify \u03c6 as rectified linear units (ReLu) (Nair & Hinton, 2010) in both the generative model and the recognition model.\nWe do not tune the hyper-parameters of our method heavily. We choose a model with one stochastic layer and two deterministic layers as the default setting. The values of \u03bb(1) and \u03bb(2) are fixed as 0.1 following Ladder Network (Rasmus et al., 2015). We do not include a local reconstruction error term at data level since the variational lower bound penalizes the reconstruction error of data already. The dimension of slots in memory ds is the same as that of the corresponding generative information hg because we use element-wise combination function fc. We employ the memory mechanism in both deterministic layers and make\nthe total number of slots n(1)s + n (2) s to be 100 to keep the number of additional parameters relatively small. We choose a 70-30 architecture according to the validation performance on the MNIST dataset and then make it default for all experiments if not mentioned.\nWe first compare the density estimation results of our model with state-of-the-art methods on MNIST and OCRletters datasets. Then we present several analysis experiments of our method on MNIST dataset and random generation on MNIST and Frey faces datasets. Finally we illustrate the results on missing value imputation. Our basic competitors are VAE (Kingma & Ba, 2015) and IWAE (Burda et al., 2015). We add the memory mechanisms to these methods and denote our models as MEMVAE and MEM-IWAE, respectively."}, {"heading": "5.1. Density Estimation", "text": "We follow (Burda et al., 2015) to split the MNIST dataset into 60,000 training data and 10,000 testing data after choosing the hyper-parameters. We train both the baselines and our models with 1, 5 and 50 importance samples respectively and evaluate the test likelihood with 5,000 importance samples as in (Burda et al., 2015). In each training epoch, we binarize the data stochastically as the input. The results of VAE, IWAE-5 (trained with 5 importance samples) and IWAE-50 (trained with 50 importance samples) with one stochastic layer in (Burda et al., 2015) are -86.76, -85.54 and -84.78 nats respectively. However, we use 500 hidden units in the deterministic layers and 100 latent variables in the stochastic layer to achieve a stronger baseline result with a different architecture and more parameters. We present our likelihood results in Table 1. We can see that our methods improve the results of baselines (both VAE and IWAE) significantly and achieve state-of-the-art results on the real-valued MNIST dataset with permutation invariant architectures. Our method MEM-IWAE-50 even outperforms S2-IWAE-50, which is the best model in (Burda et al., 2015) with two stochastic layers and four deterministic layers.\nTo compare with a broader family of benchmarks, we further quantitatively evaluate our model on the OCR-letters dataset. We use 200 hidden units in the deterministic layers and 50 latent variables in the stochastic layer as the dimension of the input is much smaller. The test log-likelihood is evaluated with 100,000 importance samples as in (Bornschein & Bengio, 2015) and shown in Table 1. Again, our methods outperform the baseline approaches significantly and are comparable with the best competitors, which often employ autoregressive connections (Larochelle & Murray, 2011; Gregor et al., 2014) that are effective on small images with simple structures. Note that these sophisticated structures are not exclusive to our memory mechanisms. A\nsystematic investigation of using memory with such structures is our future work."}, {"heading": "5.2. Analysis of Our Model", "text": "We now present a careful analysis of our model to investigate the possible reasons for the outstanding performance.\nNumber of parameters: As we employ external memory and attention mechanisms, the number of parameters in our building block is larger than that in a standard layer. However, the total number of parameters in the whole model is controlled given a limited number of slots in the memory (see Table 2), and we do not observe that our method suffers from overfitting.\nRecognition model: We feed the output of the last deterministic layer in the recognition models into a linear SVM to classify the MNIST digits to examine the invariance in features. We achieve slightly better classification accuracy (97.90% for VAE and 98.03% MEM-VAE), which means that additional memory mechanisms do not hurt or even improve the invariance of the features extracted by the recognition model.\nImportance of memory: We test the relative importance\nof the memory mechanism and local reconstruction error regularizer. MEM-VAE in the default settings but without local reconstruction error regularizer achieves a test log density estimation of -84.44 nats. VAE with additional local reconstruction regularizer achieves test log density estimation of -85.68 nats. These experiments demonstrate that the memory mechanism plays a central role in the recovery of detailed information. The local reconstruction error regularizer may help more provided supervision.\nPreference of memory slots over classes: We investigate the preference of memory slots over different classes in MEM-VAE. We average ha and normalize the activations for each class and visualize the matrices in Fig. 3(a-b), where each column represents a slot and each row represents a class (0-9 in top-down order). The averaged and normalized activations are used as the intensities for the corresponding positions in the matrices. Furthermore, we compute the correlation coefficients between activations of different classes and visualize them in a 2-D graph in Fig. 3(c-d), where each node represents a class and each edge represents the correlation between two endpoints. The larger the correlation is, the wider and darker the edge is. We observe that the trained attention model can access the memory based on the implicit label information in the input, which accords with our assumption. The activations are correlated for those digits that share similar structures such as \u201c7\u201d and \u201c9\u201d. Furthermore, different layers of memory focus on different patterns. For example, layer 1 has a strong activation of a vertical line pattern which is shared among digits \u201c1\u201d, \u201c4\u201d, \u201c7\u201d and \u201c9\u201d, while layer 2 activates most to a semi-circle pattern which is shared among digits \u201c3\u201d, \u201c5\u201d and \u201c8\u201d. Besides, layer 1 has almost the same 2D-visualization result as the raw data.\nDisabling memory: We investigate the performance of MEM-VAE when the memory is disabled (setting hm as a vector filled with ones) as in Fig. 4. The top row shows original samples; the middle row shows samples with memory of the first layer disabled; and the bottom row shows samples with memory of both layers disabled. It can be seen that, without information from memory, the main pattern of the generation does not change much but the local details are lost in some sense, which supports our\nassumption."}, {"heading": "5.3. Random Generation", "text": "We further evaluate the random generations from the baseline and our model empirically on MNIST and Frey faces datasets, which is shown in Fig. 5 and Fig. 6 respectively. We label unclear or meaningless images with red rectangles. This is done by majority voting of several volunteers. We do not select any pictures for both datasets. For the MNIST dataset, the setting is same as in Sec. 5.1. We observe that the memory mechanism helps a lot to get clear and meaningful samples as in Fig. 5.\nFor Frey faces dataset, we randomly split into 1,865 training data and 100 testing data. We use a single deterministic layer with 200 hidden units and a stochastic layer with 10 latent factors and set n(1)s to be 20 as the number of training samples is small. We use one sample of the recognition model in both of the training and testing procedure as in (Kingma & Welling, 2014). We find that the minibatch size effects the results a lot, and the quality of visualization and the averaged test log density are inconsistent (Theis et al., 2016). Specifically, setting the minibatch size to be 100, VAE achieves test log density of 1308 nats, which re-\nproduces the result with same architectures in (Kingma & Welling, 2014), but the visualization is somehow unclear; while setting the minibatch size to be 10, VAE achieves test log density of 1055 nats, but the visualization is much better. All of the parameters are set referred to (Kingma & Welling, 2014) or based on the performance of test log density of VAE. We also find that MEM-VAE outperforms VAE in both cases in terms of the qualitative test likelihood and quantitative visualization \u2014 the corresponding log density of MEM-VAE are 1330 and 1240 nats respectively. The random samples given minibatch size 100 is shown in Fig. 6, where we can see that all samples of MEM-VAE are clear but some of VAE cannot present all details in the facial expression successfully."}, {"heading": "5.4. Missing Value Imputation", "text": "Finally, we evaluate our method on the task of missing value imputation with three different types of noise, including (1) RECT-12 means that a centered rectangle of size 12 \u00d7 12 is missing; (2) RAND-0.6 means that each pixel\nis missing with a prefixed probability 0.6; and (3) HALF means that the left half of the image is missing. For both VAE and MEM-VAE, the missing values are randomly initialized and then inferred by a Markov chain that samples latent factors based on the current guess of missing values and then refines the missing values based on the current latent factors. We compare the mean square error (MSE) results after 100 epochs of inference as in Tab. 3 on the MNIST dataset. The results demonstrate that DGM with external memory can capture the underlying structures of data better than vanilla methods under different types of noise. Besides, MEM-VAE also has better visualization than VAE with fewer meaningless images, clearer digits and more accurate inference (see Fig. 7)."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper, we introduce a novel building block for deep generative models (DGMs) with an external memory and an associated soft attention mechanism. In the top-down generative procedure, the additional memory helps to recover the local detail information, which is often lost in the bottom-up abstraction procedure for learning invariant representations. Various experiments on handwritten digits and letters as well as real faces datasets demonstrate that our method can substantially improve the vanilla DGM on density estimation, random generation and missing value imputation tasks, and we can achieve state-of-the-art results among a broad family of benchmarks.\nThere are two possible extensions of our method:\n\u2022 A class conditional DGM (Kingma et al., 2014) with memory can potentially achieve better performance on both classification and generation because the external memory helps to reduce the competition between the invariant feature extraction and detailed generation, and explicit label information can make the whole system be easier to train. \u2022 Our method can be further applied to convolutional neural networks by sharing parameters across different channels and then employed in non-probabilistic DGMs such as LAPGAN (Denton et al., 2015) to refine generation on high-dimensional data."}], "references": [{"title": "Learning the structure of deep sparse graphical models", "author": ["R. Adams", "H. Wallach", "Z. Ghahramani"], "venue": "In AISTATS,", "citeRegEx": "Adams et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adams et al\\.", "year": 2010}, {"title": "Multiple object recognition with visual attention", "author": ["J.L. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I. Goodfellow", "A. Bergeron", "N. Bouchard", "D. Warde-Farley", "Y. Bengio"], "venue": "In Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Y. Bengio", "E. Thlhodeau-Laufer", "G. Alain", "J. Yosinski"], "venue": "In ICML,", "citeRegEx": "Bengio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2014}, {"title": "Reweighted wake-sleep", "author": ["J. Bornschein", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "Bornschein and Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bornschein and Bengio", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "The Laplacian pyramid as a compact image code", "author": ["P.J. Burt", "E.H. Adelson"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "Burt and Adelson,? \\Q1983\\E", "shortCiteRegEx": "Burt and Adelson", "year": 1983}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "NPIS,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Learning deep sigmoid belief networks with data augmentation", "author": ["Z. Gan", "R. Henao", "D.E. Carlson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Generative adversarial nets", "author": ["I.J. Goodfellow", "J.P. Abadie", "M. Mirza", "B. Xu", "D.W. Farley", "S.ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "In arXiv:1308.0850,", "citeRegEx": "Graves,? \\Q2013\\E", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Learning to transduce with unbounded memory", "author": ["E. Grefenstette", "K.M. Hermann", "M. Suleyman", "P. Blunsom"], "venue": "In NIPS,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "I. Danihelka", "A. Mnih", "C. Blundell", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "DRAW: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D.J. Rezende", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "author": ["H. Larochelle", "G. Hinton"], "venue": "In NIPS,", "citeRegEx": "Larochelle and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Larochelle and Hinton", "year": 2010}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In AISTATS,", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Gradientbased learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Max-margin deep generative models", "author": ["C. Li", "J. Zhu", "T. Shi", "B. Zhang"], "venue": "In NIPS,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Murray and Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Murray and Salakhutdinov", "year": 2009}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML,", "citeRegEx": "Nair and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Connectionist learning of belief networks", "author": ["R.M. Neal"], "venue": "In Artificial intelligence,", "citeRegEx": "Neal,? \\Q1992\\E", "shortCiteRegEx": "Neal", "year": 1992}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In NIPS,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning generative models with visual attention", "author": ["Y. Tang", "N. Srivastava", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. Oord", "M. Bethge"], "venue": "In arXiv:1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "From neural PCA to deep unsupervised learning", "author": ["H. Valpola"], "venue": "In arXiv:1411.7783,", "citeRegEx": "Valpola,? \\Q2014\\E", "shortCiteRegEx": "Valpola", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "JMLR, 11:3371C340,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J.L. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Reinforcement learning neural Turing machines", "author": ["W. Zaremba", "I. Sutskever"], "venue": "In arXiv:1505.00521,", "citeRegEx": "Zaremba and Sutskever,? \\Q2015\\E", "shortCiteRegEx": "Zaremba and Sutskever", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Depending on the building blocks, various types of DGMs exist, including undirected models (Salakhutdinov & Hinton, 2009), directed models (Neal, 1992; Hinton et al., 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al.", "startOffset": 139, "endOffset": 172}, {"referenceID": 18, "context": "Depending on the building blocks, various types of DGMs exist, including undirected models (Salakhutdinov & Hinton, 2009), directed models (Neal, 1992; Hinton et al., 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al.", "startOffset": 139, "endOffset": 172}, {"referenceID": 16, "context": ", 2006), autoregressive models (Larochelle & Murray, 2011; Gregor et al., 2014), and Markov chain based models (Bengio et al.", "startOffset": 31, "endOffset": 79}, {"referenceID": 6, "context": ", 2014), and Markov chain based models (Bengio et al., 2014).", "startOffset": 39, "endOffset": 60}, {"referenceID": 32, "context": "Recently, DGMs have attracted much attention on developing efficient and (approximately) accurate learning algorithms, such as stochastic variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Bornschein & Bengio, 2015; Burda et al., 2015) and Monte Carlo methods (Adams et al.", "startOffset": 158, "endOffset": 251}, {"referenceID": 8, "context": "Recently, DGMs have attracted much attention on developing efficient and (approximately) accurate learning algorithms, such as stochastic variational methods (Kingma & Welling, 2014; Rezende et al., 2014; Bornschein & Bengio, 2015; Burda et al., 2015) and Monte Carlo methods (Adams et al.", "startOffset": 158, "endOffset": 251}, {"referenceID": 0, "context": ", 2015) and Monte Carlo methods (Adams et al., 2010; Gan et al., 2015).", "startOffset": 32, "endOffset": 70}, {"referenceID": 11, "context": ", 2015) and Monte Carlo methods (Adams et al., 2010; Gan et al., 2015).", "startOffset": 32, "endOffset": 70}, {"referenceID": 26, "context": "This bottom-up abstraction progress is good for identifying predictive patterns, especially when a discriminative objective is optimized (Li et al., 2015); but it also loses the detail information that is necessary in the top-down generating process.", "startOffset": 137, "endOffset": 154}, {"referenceID": 17, "context": "For example, DRAW (Gregor et al., 2015) iteratively constructs complex images over time through a recurrent encoder and decoder together with an attention mechanism and LAPGAN (Denton et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 10, "context": ", 2015) iteratively constructs complex images over time through a recurrent encoder and decoder together with an attention mechanism and LAPGAN (Denton et al., 2015) employs a cascade of generative adversarial networks (GANs) (Goodfellow et al.", "startOffset": 144, "endOffset": 165}, {"referenceID": 13, "context": ", 2015) employs a cascade of generative adversarial networks (GANs) (Goodfellow et al., 2014) to generate high quality natural images through a Laplacian pyramid framework (Burt & Adelson, 1983).", "startOffset": 68, "endOffset": 93}, {"referenceID": 8, "context": "Different from (Kingma & Welling, 2014; Burda et al., 2015), our recognition network is asymmetric to the generative network.", "startOffset": 15, "endOffset": 59}, {"referenceID": 15, "context": ", 2015) and neural language transduction (Grefenstette et al., 2015).", "startOffset": 41, "endOffset": 68}, {"referenceID": 1, "context": "In addition to the memory-based models mentioned above, attention mechanisms have been used in other deep models for various tasks, such as image classification (Larochelle & Hinton, 2010; Ba et al., 2015), object tracking (Mnih et al.", "startOffset": 161, "endOffset": 205}, {"referenceID": 27, "context": ", 2015), object tracking (Mnih et al., 2014), conditional caption generation (Xu et al.", "startOffset": 25, "endOffset": 44}, {"referenceID": 37, "context": ", 2014), conditional caption generation (Xu et al., 2015), machine translation (Bahdanau et al.", "startOffset": 40, "endOffset": 57}, {"referenceID": 2, "context": ", 2015), machine translation (Bahdanau et al., 2015) and image generation (Graves, 2013; Gregor et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 14, "context": ", 2015) and image generation (Graves, 2013; Gregor et al., 2015).", "startOffset": 29, "endOffset": 64}, {"referenceID": 17, "context": ", 2015) and image generation (Graves, 2013; Gregor et al., 2015).", "startOffset": 29, "endOffset": 64}, {"referenceID": 17, "context": "Recently, DRAW (Gregor et al., 2015) introduces a novel 2-D attention mechanism to decide \u201cwhere to read and write\u201d on the image and does well in generating objects with clear track, such as handwritten digits and sequences of real digits.", "startOffset": 15, "endOffset": 36}, {"referenceID": 33, "context": "Compared with previous DGMs with visual attention (Tang et al., 2014; Gregor et al., 2015), we make different assumptions about the data, i.", "startOffset": 50, "endOffset": 90}, {"referenceID": 17, "context": "Compared with previous DGMs with visual attention (Tang et al., 2014; Gregor et al., 2015), we make different assumptions about the data, i.", "startOffset": 50, "endOffset": 90}, {"referenceID": 35, "context": "Similar idea is highlighted in the Ladder Network (Valpola, 2014; Rasmus et al., 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al.", "startOffset": 50, "endOffset": 86}, {"referenceID": 31, "context": "Similar idea is highlighted in the Ladder Network (Valpola, 2014; Rasmus et al., 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al.", "startOffset": 50, "endOffset": 86}, {"referenceID": 36, "context": ", 2015), which reconstructs the input hierarchically using an extension of denoising autoencoders (dAEs) (Vincent et al., 2010) with the help of lateral connections and achieves excellent performance on semi-supervised learning (Rasmus et al.", "startOffset": 105, "endOffset": 127}, {"referenceID": 31, "context": ", 2010) with the help of lateral connections and achieves excellent performance on semi-supervised learning (Rasmus et al., 2015).", "startOffset": 108, "endOffset": 129}, {"referenceID": 22, "context": "Our method can also be extended to do supervised or semisupervised learning as in (Kingma et al., 2014), which is our future work.", "startOffset": 82, "endOffset": 103}, {"referenceID": 2, "context": "We can view this as an unnormalized version of a soft attention mechanism (Bahdanau et al., 2015) trainable by standard back-propagation, which could be replaced by a hard attention mechanism trained with Reinforcement Learning (Xu et al.", "startOffset": 74, "endOffset": 97}, {"referenceID": 37, "context": ", 2015) trainable by standard back-propagation, which could be replaced by a hard attention mechanism trained with Reinforcement Learning (Xu et al., 2015).", "startOffset": 138, "endOffset": 155}, {"referenceID": 35, "context": "Inspired by the Ladder Network (Valpola, 2014; Rasmus et al., 2015), we specify the combination function of h m and h g as element wise multiple layer perceptron with optionally final nonlinearity \u03c6:", "startOffset": 31, "endOffset": 67}, {"referenceID": 31, "context": "Inspired by the Ladder Network (Valpola, 2014; Rasmus et al., 2015), we specify the combination function of h m and h g as element wise multiple layer perceptron with optionally final nonlinearity \u03c6:", "startOffset": 31, "endOffset": 67}, {"referenceID": 32, "context": "Significant progress has been made recently on stochastic variational inference methods with a sophisticated recognition model to parameterize the variational distributions (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 173, "endOffset": 219}, {"referenceID": 35, "context": "Note that we cannot send the massage of a intermediate layer in the recognition model to a layer in the generative model through a lateral connection as in Ladder Network (Valpola, 2014; Rasmus et al., 2015) because that indeed changes the distribution of p(x|z) according to the data x.", "startOffset": 171, "endOffset": 207}, {"referenceID": 31, "context": "Note that we cannot send the massage of a intermediate layer in the recognition model to a layer in the generative model through a lateral connection as in Ladder Network (Valpola, 2014; Rasmus et al., 2015) because that indeed changes the distribution of p(x|z) according to the data x.", "startOffset": 171, "endOffset": 207}, {"referenceID": 8, "context": "To compare with statof-the-art results, we also train our method as in importance weighted autoencoders (IWAE) (Burda et al., 2015), which uses importance weighting estimate of log likelihood with multiple samples in the training procedure to achieve a strictly tighter variational lower bound.", "startOffset": 111, "endOffset": 131}, {"referenceID": 25, "context": "The MNIST dataset (Lecun et al., 1998) consists of 50,000 training, 10,000 validation and 10,000 testing images of handwritten digits and each image is of 28 \u00d7 28 pixels.", "startOffset": 18, "endOffset": 38}, {"referenceID": 3, "context": "Our implementation is based on Theano (Bastien et al., 2012).", "startOffset": 38, "endOffset": 60}, {"referenceID": 31, "context": "This means that we initialize the output as signals from top-down inference, which is different from the Ladder Network (Rasmus et al., 2015).", "startOffset": 120, "endOffset": 141}, {"referenceID": 31, "context": "1 following Ladder Network (Rasmus et al., 2015).", "startOffset": 27, "endOffset": 48}, {"referenceID": 8, "context": "Our basic competitors are VAE (Kingma & Ba, 2015) and IWAE (Burda et al., 2015).", "startOffset": 59, "endOffset": 79}, {"referenceID": 8, "context": "We follow (Burda et al., 2015) to split the MNIST dataset into 60,000 training data and 10,000 testing data after choosing the hyper-parameters.", "startOffset": 10, "endOffset": 30}, {"referenceID": 8, "context": "We train both the baselines and our models with 1, 5 and 50 importance samples respectively and evaluate the test likelihood with 5,000 importance samples as in (Burda et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 8, "context": "The results of VAE, IWAE-5 (trained with 5 importance samples) and IWAE-50 (trained with 50 importance samples) with one stochastic layer in (Burda et al., 2015) are -86.", "startOffset": 141, "endOffset": 161}, {"referenceID": 8, "context": "Our method MEM-IWAE-50 even outperforms S2-IWAE-50, which is the best model in (Burda et al., 2015) with two stochastic layers and four deterministic layers.", "startOffset": 79, "endOffset": 99}, {"referenceID": 16, "context": "Again, our methods outperform the baseline approaches significantly and are comparable with the best competitors, which often employ autoregressive connections (Larochelle & Murray, 2011; Gregor et al., 2014) that are effective on small images with simple structures.", "startOffset": 160, "endOffset": 208}, {"referenceID": 8, "context": "Results are from [1] (Murray & Salakhutdinov, 2009), [2] (Burda et al., 2015), [3] (Bornschein & Bengio, 2015), [4] (Larochelle & Murray, 2011) and [5] (Gregor et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 16, "context": ", 2015), [3] (Bornschein & Bengio, 2015), [4] (Larochelle & Murray, 2011) and [5] (Gregor et al., 2014).", "startOffset": 82, "endOffset": 103}, {"referenceID": 34, "context": "We find that the minibatch size effects the results a lot, and the quality of visualization and the averaged test log density are inconsistent (Theis et al., 2016).", "startOffset": 143, "endOffset": 163}, {"referenceID": 22, "context": "\u2022 A class conditional DGM (Kingma et al., 2014) with memory can potentially achieve better performance on both classification and generation because the external memory helps to reduce the competition between the invariant feature extraction and detailed generation, and explicit label information can make the whole system be easier to train.", "startOffset": 26, "endOffset": 47}, {"referenceID": 10, "context": "\u2022 Our method can be further applied to convolutional neural networks by sharing parameters across different channels and then employed in non-probabilistic DGMs such as LAPGAN (Denton et al., 2015) to refine generation on high-dimensional data.", "startOffset": 176, "endOffset": 197}], "year": 2017, "abstractText": "Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs on various tasks, including density estimation, image generation, and missing value imputation, and DGMs with memory can achieve state-ofthe-art quantitative results.", "creator": "LaTeX with hyperref package"}}}