{"id": "1602.07435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Parametric Prediction from Parametric Agents", "abstract": "we mainly consider a problem of prediction based on opinions elicited behavior from heterogeneous rational agents with private information. properly making available an accurate prediction consistent with paying a minimal cost requires a joint design of strengthening the incentive mechanism and evolving the prediction algorithm. ultimately such a problem indeed lies at the nexus of statistical learning theory and game theory, and arises chiefly in many domains such as consumer surveys and mobile marketing crowdsourcing. in order to elicit heterogeneous agents'private information and incentivize agents with different capabilities to act in the principal's best interest, we design an optimal joint incentive mechanism and prediction algorithm called cope ( cost correlation and prediction attribute elicitation ), the analysis of which offers several valuable engineering capability insights. first, when the costs incurred by contracting the agents are purely linear in the exerted effort, cope usually corresponds to a \" crowd contending \" mechanism, such where paying the principal only employs the agent with the highest intelligence capability. unlike second, when the costs are quadratic, cope corresponds to a \" crowd - sourcing \" mechanism project that employs multiple agents with different capabilities at the same time. numerical simulations show that cope improves the principal's profit and the network profit significantly ( larger than 30 % in our simulations ), comparing to those mechanisms that assume theoretically all agents have equal capabilities.", "histories": [["v1", "Wed, 24 Feb 2016 08:56:46 GMT  (248kb,D)", "http://arxiv.org/abs/1602.07435v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["yuan luo", "nihar b shah", "jianwei huang", "jean walrand"], "accepted": false, "id": "1602.07435"}, "pdf": {"name": "1602.07435.pdf", "metadata": {"source": "CRF", "title": "Parametric Prediction from Parametric Agents", "authors": ["Yuan Luo", "Nihar B. Shah", "Jianwei Huang", "Jean Walrand"], "emails": ["yluo@ie.cuhk.edu.hk,", "nihar@eecs.berkeley.edu", "jwhuang@ie.cuhk.edu.hk", "walrand@berkeley.edu"], "sections": [{"heading": null, "text": "Parametric Prediction from Parametric Agents\nYuan Luo Department of Information Engineering, The Chinese University of Hong Kong,\nyluo@ie.cuhk.edu.hk,\nNihar B. Shah Department of Electrical Engineering and Computer Sciences, UC Berkeley,\nnihar@eecs.berkeley.edu\nJianwei Huang Department of Information Engineering, The Chinese University of Hong Kong,\njwhuang@ie.cuhk.edu.hk\nJean Walrand Department of Electrical Engineering and Computer Sciences, UC Berkeley,\nwalrand@berkeley.edu\nWe consider a problem of prediction based on opinions elicited from heterogeneous rational agents with private information. Making an accurate prediction with a minimal cost requires a joint design of the incentive mechanism and the prediction algorithm. Such a problem lies at the nexus of statistical learning theory and game theory, and arises in many domains such as consumer surveys and mobile crowdsourcing. In order to elicit heterogeneous agents\u2019 private information and incentivize agents with different capabilities to act in the principal\u2019s best interest, we design an optimal joint incentive mechanism and prediction algorithm called COPE (COst and Prediction Elicitation), the analysis of which offers several valuable engineering insights. First, when the costs incurred by the agents are linear in the exerted effort, COPE corresponds to a \u201ccrowd contending\u201d mechanism, where the principal only employs the agent with the highest capability. Second, when the costs are quadratic, COPE corresponds to a \u201ccrowd-sourcing\u201d mechanism that employs multiple agents with different capabilities at the same time. Numerical simulations show that COPE improves the principal\u2019s profit and the network profit significantly (larger than 30% in our simulations), comparing to those mechanisms that assume all agents have equal capabilities. Key words : mechanism design, aggregated prediction, crowdsourcing"}, {"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Background and Motivations", "text": "Prediction markets, which aggregate information elicited from people with private beliefs, have\nserved as a reliable tool for estimating the outcome of specific future events (see Berg and Rietz\n1\nar X\niv :1\n60 2.\n07 43\n5v 1\n[ cs\n.G T\n] 2\n4 Fe\nb 20\n16\n(2003)). For example, these markets have been used to predict the winners of election (see Wolfers\nand Rothschild (2011)), future demand for a product (see Hayes (1998)), and stock prices and\nreturns (see Gottschlich and Hinz (2014)). In these markets, values of traded information depend\non future outcomes, and the accuracy of prediction can be verified based on the realized outcomes.\nWith the emergence of several commercial crowdsourcing platforms such as Amazon Mechanical\nTurk and Microworkers, collecting information from people to make prediction has become much\ncheaper, easier and faster. However, the information collected from people (\u201cthe agents\u201d) can be\nhighly unreliable due to the agents\u2019 insufficient expertise and the lack of appropriate incentives.\nMore specifically, in a crowdsourcing platform, the agents are heterogeneous as they come from\ndifferent countries and have different skills, which leads to significant variations of the work quality\n(see Karger et al. (2014)). Furthermore, agents may exert different levels of effort to finish the\nallocated task based on different levels of payments, and different agents may react differently to\nthe same level of payment (see Liu et al. (2014)). The chosen level of effort affects their performance\ndramatically. Due to these issues, an appropriate incentive mechanism that exploits the agents\u2019\nheterogeneity whilst incentivizes appropriate effort levels is crucial to a successful prediction system.\nBesides eliciting high quality of information, the prediction performance also depends on the\nprediction behaviour of the surveyor (\u201cthe principal\u201d). Without an appropriate prediction rule,\nthe principal may not be able to effectively utilize the collected information and may obtain an\ninaccurate prediction results. This motivates us to study the incentive mechanism design together\nwith the prediction rule optimization.\nThe resultant problem of joint design poses a significant challenge due to the following reasons.\nFirst, due to the incorporation of the prediction problem, the objective of incentive mechanism\nchanges from eliciting agents\u2019 information truthfully to minimizing the prediction error. As the\nprediction error is a result of the agents\u2019 information and actions, the designed mechanisms not\nonly needs to motivate agents to report their truthful estimation information, but also needs to\nmake sure that agents take appropriate actions. Hence, we cannot directly implement existing\nstrictly proper scoring rules, e.g., the quadratic scoring rule (see Brier (1950), Selten (1998)), that only promotes truthfulness among agents to address the joint problem.\nSecond, the designed mechanism needs to solve both moral hazard and adverse selection problems simultaneously. Moral hazard results from the inability of the principal to observe an agent\u2019s actions (i.e., effort exerted by the agent), while adverse selection corresponds to the inability of knowing an agent\u2019s private information (i.e., the expertise of an agent). This is different from most incentive mechanisms designed in the existing works, which separate the mechanism design from the prediction problem and address either \u201chidden actions\u201d (i.e., moral hazard) (see e.g., Fang et al. (2007), Ioannidis and Loiseau (2013)) or \u201chidden information\u201d (i.e., adverse selection) (see e.g., Frongillo et al. (2015), Abernethy et al. (2015)).\nNevertheless, we formulate and optimally solve a \u201cparametric\u201d form of this joint design problem. More specifically, the principal desires to predict a parameter of a known distribution. Each agent is modeled in a parametric fashion, with her expertise governed by a single parameter that is the agent\u2019s private information. We assume that agents are heterogeneous as they have different levels of expertise. While each agent aims to maximize her own expected payoff (i.e., the revenue minus the effort cost), the principal optimizes a joint utility that trades off the prediction error and the monetary costs. For ease of exposition, we refer to the principal as \u201cshe\u201d and each agent as \u201che\u201d."}, {"heading": "1.2. Results and Contributions", "text": "We focus on the interactions among a principal and multiple agents, and design an appropriate incentive mechanism to facilitate the parametric prediction process. Specifically, we design a mechanism, which we call \u201cCOPE\u201d (standing for \u201cCOst and Prediction Elicitation\u201d), that jointly optimizes the principal\u2019s payoff in terms of the payments made to the agents and the prediction error incurred. COPE provides a systematic way for the principal to incentivize all participating agents to report their estimations truthfully and exert appropriate amounts of effort based on their respective capabilities.\nWe summarize our key results and the main contributions as follows.\n\u2022 Theoretic significance: We relax several critical assumptions that are common in papers in the\nrelated literature, i.e., the costs incurred by the agents are all known to the principal, the agents do\nnot incur costs for efforts, and the agents are all homogeneous. Hence, the proposed model pushes\nthis line of theoretical focused research into more realistic settings.\n\u2022 Optimal incentive mechanism design: We study a generic incentive mechanism design problem\nsituated in a prediction process. To study the optimal prediction solution, we design the COPE\nmechanism, which ensures that all participating agents report their estimations truthfully and\nexert appropriate amounts of effort based on their respective capabilities, meanwhile maximizes\nthe principal\u2019s expected payoff.\n\u2022 Observations and insights: Our results show that, with Gaussian estimation noise, when the\nagent\u2019s marginal cost is independent of his amount of exerted effort, the principal should conduct\na crowd-tender mechanism, by soliciting service only from the agent with the lowest reported cost.\nOn the other hand, when the marginal costs depends on the exerted effort, the optimal mechanism\nis in the form of crowd-sourcing, where the principal recruits multiple agents to complete the task.\n\u2022 Numerical results: Simulation in Section 5 show that COPE improves both the principal\u2019s\nprofit and the network profit, comparing to those mechanisms that assume all agents have equal\ncapabilities and incentivize agents exert the same amount of effort. Moreover, the performance gap\nbetween COPE and the centralized benchmark solution is less than 3% under the quadratic cost\nfunction and 10% under the linear cost function.\nThe rest of the paper is organized as follows. After reviewing the literature in Section 2, we\ndescribe the system model in Section 3, and design the incentive mechanism in Section 4. In Section\n5, we provide the simulations results. We conclude in Section 6."}, {"heading": "2. Related Work", "text": "Mechanism design for truthful elicitation of agents\u2019 opinions is an extensively studied problem, most\nrecently investigated are in the context of crowdsourcing (see, e.g., Cavallo and Jain (2015), Miller\net al. (2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)). In contrast to our work, this\nline of literature does not consider the prediction aspect, and only focuses on the elicitation problem\nalone. Mechanism design for truthful elicitation of agents\u2019 opinions is also studied in the context\nof prediction markets (see, e.g., Wolfers and Zitzewitz (2004), Conitzer (2009)). These results,\nhowever, study the scenario where the agents take the responsibility of aggregating information. Our\npaper concerns a different setting and objective, in which the principal is in charge of information\ngathering and making the final prediction. Hence, the mechanism we design should not only elicit\nthe agents\u2019 information but also incentivize agents to exert appropriate effort.\nThe scenario becomes quite different when prediction must be done by taking incentives into\naccount, and calls for the design of new procedures catering to both aspects. The recent studies (see,\ne.g., Fang et al. (2007), Ioannidis and Loiseau (2013), Frongillo et al. (2015), Cai et al. (2015),\nAbernethy et al. (2015)) address problems in this space. However, the models assumed in these\nworks are different, and generally more restrictive than the models considered in our paper in many\nrespects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation\nand the quality of each agent, under the assumption that the agents are homogeneous with the\nsame cost type. In contrast, we consider the more general and realistic setting where agents can\nhave different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative\ngame and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study\nhow a principal can make predictions by eliciting the agents\u2019 confidences, again without considering\nthe costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the\nagents cannot fabricate their observation, but may lie about their costs, and design a mechanism\nto ensure that agents truthfully report their costs. In contrast, we assume a more general scenario\nwhere the agents can be strategic in choosing and reporting their respective observations. Cai et al.\n(2015) propose a monetary mechanism to collect data and to perform an estimation of a function at\none random point. However, they assume that the agent always reports truthfully once he makes an\nobservation. In contrast, our work considers strategic agents and proposes an optimal mechanism\nto ensure truth-telling by the agents.\nWe begin with a formal description of the problem formulation. Through this description, we will set up notations to capture the behavior of the agents, the objective of the principal, the prediction problem, and the mechanism-design problem."}, {"heading": "3.1. System Model", "text": "We consider a setting where the principal wishes to make a parametric prediction, that is, to form an informed estimate about a parameter x\u2217 \u2208 X \u2286 R. Predicting the winner of an election and predicting box office results for movies are two motivating examples. We assume that x\u2217 has a prior distribution that is publicly known, for instance, from the results of an earlier election. We assume that the principal will come to know the precise value of x\u2217 sometime in the future, for instance, upon completion of the election. In order to make a prediction, the principal queries a set A= {1, . . . ,N} of N agents to report their observations.\nFigure 1 pictorially depicts the interaction between agents and the principal, including the agents\u2019 reporting strategy and the principal\u2019s prediction and payment decision. Before we explain each individual components of the figure, we first introduce some notations used to characterize the agents\u2019 strategies and types in Figure 1.\nEffort Level and Cost Type: When queried by the principal, an agent can put in some effort to form an \u201cobservation\u201d whose value is known only to that agent. We assume that the observation of any agent is noisy, and the distribution of the observation yn comes from a parameterised family of distributions \u03c6(x\u2217, qn), where qn represents the effort exerted by agent n to make observation. The higher value of qn, the more effort agent n exerts, and thus the better quality of agent n\u2019s\nobservation. An example that we focus on subsequently in the paper is additive Gaussian noise (see, e.g., Fang et al. (2007), Cai et al. (2015)), i.e., yn \u223cN (x\u2217, 1qn ), where yn follows from Gaussian distribution with mean x\u2217 and variance 1/qn. Conditioned on x \u2217, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,\u2200n \u2208A) be the observation vector and q= (qn,\u2200n\u2208A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others.\nIt is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent\u2019s costtype parameter \u03b8n. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of \u03b8n implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+\u00d7R+\u2192R+, and assume that the cost incurred by an agent n\u2208A with the cost type \u03b8n when exerting an effort qn is C(qn, \u03b8n). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2.\nThe cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed on support [\u03b8, \u03b8\u0304] for some 0\u2264 \u03b8 < \u03b8\u0304 <\u221e. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [\u03b8, \u03b8\u0304]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.\nNext, we will discuss each individual components of Figure 1 sequentially. Reporting Observations and Making Payments: The principal employs monetary incentives to ensure that agents make their observations and report them to the principal. In order to incentivize agents to participate the prediction task, the payment to an agent must, at the least, cover the cost incurred by that agent in putting effort to make the observation. However, since each agent\u2019s cost parameter is known only to that agent, the principal needs to ask each agent to\nreport his own cost type (Figure 1a). The agents are strategic, and any agent n\u2208A may report a cost type \u03b8\u0302n that is different from his true cost type \u03b8n. Let \u03b8\u0302 = (\u03b8\u0302n, n \u2208A) be the reporting cost type vector.\nAs we will see subsequently, incentivizing different agents to put different levels of effort depend-\ning on their respective cost types allows for a significantly better prediction performance. The\nprincipal must incentivize these different effort levels, and the choice of these effort levels is based on the agents\u2019 reported cost types \u03b8\u0302 (Figure 1b). Let function Qp : [\u03b8, \u03b8\u0304]N \u2192 R+ denote the effort that the principal requires an agent to exert. The function Qp depends on the cost types reported by the agents: Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) represents the effort required from agent n \u2208 A, where \u03b8\u0302\u2212n = [\u03b8\u03021, . . . , \u03b8\u0302n\u22121, \u03b8\u0302n+1, . . . , \u03b8\u0302N ] T is the reported cost parameters of all agents except agent n. Here (and elsewhere in the paper), we use the superscript \u201cP\u201d to represent the principal. We let Qp(\u03b8\u0302) = (Qp(\u03b8\u0302n, \u03b8\u0302\u2212n),\u2200n\u2208A) be the effort vector required by the principal.\nEach agent n\u2208A is strategic and may exert an effort qn 6=Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) to suit his own interests. When choosing the effort to exert, the agent may also exploit the fact that the principal cannot\ndirectly observe the actual effort exerted. Upon exerting the chosen effort qn, the agent obtains an observation yn (Figure 1c). The principal seeks to know the value of the observation yn, but the agent may report a strategically chosen value y\u0302n 6= yn to the principal (Figure 1d) that suits the agent\u2019s own interests. We adopt the shorthand y\u0302 = (y\u0302n, n \u2208 A) as the reporting vector. Based on the information obtained, the principal must make a prediction for the value of x\u2217 (Figure 1e).\nThe principal makes payment to each agent once she observes the true value of x\u2217. Specifically, we define the payment function as R : R\u00d7 R\u00d7 [\u03b8, \u03b8\u0304]N\u00d7\u2192 R+; the payment to agent n is R(x\u2217, y\u0302n, \u03b8\u0302n, \u03b8\u0302\u2212n), which depends on the value of x \u2217, the agent n\u2019s reported observation y\u0302n, and all agents\u2019 reported cost parameters \u03b8\u0302.\nAs indicated above, the model considered is a one-shot model, i.e., once the agents simultaneously\nreport their observations, the principal determines the payments based on the agents\u2019 reported cost\nparameters and observations only, with no further adjustments on reported values or the payments.\nPayoff of Agent: Given the payment function announced by the principal, each agent n\u2019s payoff Ua :R\u00d7 [\u03b8, \u03b8\u0304]\u00d7R+\u00d7R\u00d7 [\u03b8, \u03b8\u0304]N \u2192R is defined as the difference between the payment received from the principal and the cost incurred in making the observation, and is given as\nUa(x\u2217, \u03b8\u0302n, qn, y\u0302n, \u03b8n, \u03b8\u0302\u2212n) =R(x \u2217, y\u0302n, \u03b8\u0302n, \u03b8\u0302\u2212n)\u2212C ( qn, \u03b8n ) , (1)\nHere the superscript \u201cA\u201d indicates a term associated to the agents. Equation (1) shows that agent n\u2208A\u2019s payoff also depends on other agents\u2019 reported cost parameters \u03b8\u0302\u2212n. When each agent n\u2208A chooses his strategy, i.e., his cost reported value \u03b8\u0302n, exerted effort qn, and the reported observation y\u0302n, to maximize his expected payoff. The expected payoff of the agent n is calculated as\nE[Ua(x\u2217, \u03b8\u0302n, qn, y\u0302n, \u03b8n, \u03b8\u0302\u2212n)] =E[R(x\u2217, y\u0302n, \u03b8\u0302n, \u03b8\u0302\u2212n)]\u2212C(qn, \u03b8n), (2)\nwhere the expectation is taken with respect to the distributions of x\u2217 and all agents\u2019 cost parameters\n\u03b8\u2212n. Recall that each agent n only knows his own cost parameter \u03b8n, and only has distributional information about other agents\u2019 cost parameters.\nPayoff of the Principal: Let function x\u0302 :R\u00d7R+\u2192R characterize the prediction made by the principal based on the agents\u2019 reported observations y\u0302 and the effort assumed to be exerted by the agents Qp(\u03b8\u0302). Due to the inability of the principal to observe the agents\u2019 exerted effort, the\nprincipal makes prediction based on her own knowledge (i.e., the agents reported observation y\u0302 and the effort Qp(\u03b8\u0302) required from agents). Later in Section 4, we will show that the agents\u2019 true\neffort level are the same as that required by the principal by employing our proposed mechanism. Let `p : R\u00d7R\u2192 R+ be the loss function that characterizes the penalty term for mistakes in the prediction. For instance, one could consider the squared loss `p(x\u2217, x\u0302(y\u0302,qp)) = (x\u2217 \u2212 x\u0302(y\u0302,qp))2 as the penalty for the principal, where qpn = Q p(\u03b8\u0302n, \u03b8\u0302\u2212n) and q p = (qpn,\u2200n \u2208 A). We use qp as the shorthand notation for Qp(\u03b8\u0302n, \u03b8\u0302\u2212n), in order to show that q p n, for any n\u2208A, is a decision variable for the principal. In Section 4.2.1, we will show how the principal determines the desired effort from the agents by taking the first order derivative of her Bayes risk with respect to qpn.\nWe measure the utility gained by the principal through the prediction in terms of the Bayes risk incurred under this loss function. The reason that we use Bayes risk is that it yields a real number (not a function of x\u0302 or y\u0302) for each prediction, and the principal\u2019s posterior expected prediction loss is equivalent to the Bayes risk (see Robert (2007), Berger (2013)). If all agents report their true observations (i.e., y\u0302 = y) and exert efforts as desired by the principal (i.e., q = qp =Qp(\u03b8\u0302)), then the Bayes risk Bp :R\u00d7R\u2192R+ is (see Robert (2007), Berger (2013))\nBp(qp) = inf x\u0302 E[`p(x\u2217, x\u0302(y,qp))], (3)\nwhere the expectation is taken with respect to x\u2217 and y. By assuming that the principal\u2019s utility with perfect prediction is zero, the Bayes risk chacterzies the penality for the principal\u2019s mistakes in the prediction, and the prinicial\u2019s expected utility is just \u2212Bp(qp). Correspondingly, the payoff of the principal U p : R \u00d7 RN \u00d7 [\u03b8, \u03b8\u0304]N \u2192 R is then defined as the difference between her utility obtained from prediction and the monetary payments to all agents:\nU p(x\u2217,qp,y, \u03b8\u0302) =\u2212Bp(qp)\u2212 \u2211 n\u2208A R(x\u2217, yn, \u03b8\u0302n, \u03b8\u0302\u2212n). (4)\nHere, we assumed without loss of generality that the monetary payment and the prediction loss is normalized to be on the same scale.\nFor clarity, we list the key notations in Table 1."}, {"heading": "3.2. Design Objective", "text": "Before explaining the design objective, we begin by defining two standard game-theoretic terms that are required for subsequent discussions.\nDefinition 1. (BIC: Bayesian Incentive Compatibility) A mechanism satisfies the Bayesian incentive compatibility (BIC) if for every agent n\u2208A, his expected payoff satisfies (see Fudenberg and Tirole (1991), Myerson (1979))\nE [ Ua(x\u2217, \u03b8n,Q p(\u03b8n,\u03b8\u2212n), yn, \u03b8n,\u03b8\u2212n) ] \u2265E [ Ua(x\u2217, \u03b8\u0302n, qn, y\u0302n, \u03b8n,\u03b8\u2212n) ] ,\n\u2200(\u03b8\u0302n, qn, y\u0302n) 6=(\u03b8n,Qp(\u03b8n,\u03b8\u2212n), yn), (5)\nwhere the expectation is taken with respect to x\u2217 and all other agents cost parameters \u03b8\u2212n.\nBIC means that for any agent n, reporting the true cost parameter, exerting the effort requested by the principal, and reporting true observation will maximize his expected payoff, given common knowledge about the distribution on agents cost parameters and when other agents are truthfully report their cost parameters.\nDefinition 2. (BIR: Bayesian Individual Rationality) A mechanism satisfies the Bayesian incentive rationality (BIR), if the expected payoff of every agent n \u2208 A is non-negative, given that he reports truthfully, exerts effort as the principal desires, and assumes that all other agents report their cost parameters truthfully, that is (see Fudenberg and Tirole (1991), Myerson (1979)),\nE [ Ua(x\u2217, \u03b8n,Q p(\u03b8n,\u03b8\u2212n), yn, \u03b8n,\u03b8\u2212n) ] \u2265 0, \u2200n\u2208A, (6)\nwhere the expectation is taken with respect to x\u2217 and all other agents\u2019 cost parameters \u03b8\u2212n.\nAssuming (without loss of generality) that the payoff of an agent not participating in this process equals zero, BIR means that an agent will participate only if his expected payoff is at least as much as that of a non-participating agent.\nBased on the revelation principal (see Myerson (1979)), the problem of finding a mechanism that maximizes the principal\u2019s expected payoff can be restricted to the set of mechanisms where agents are willing to reveal their private information to the principal. Moreover, the principal cannot force agents to accept the task. Hence, the problem that we want to solve is formalized as follows. The goal is to design a mechanism, say M, that maximizes the principal\u2019s expected while ensuring truthful reports from the agents:\nsup M\nE [ U p(x\u2217,y,\u03b8) ] subject to: BIC and BIR in (5) and (6),\n(7)\nwhere the expectation is taken with respect to x\u2217, y and \u03b8, and the BIR condition makes sure that the agents are willing to participate in the game. In words, the goal is to design a mechanism such that: (i) the principal\u2019s payoff is maximized in expectation; (ii) the principal can elicit truthful information from all agents; and (iii) the principal can incentivise suitable effort from the agents based on their respective cost parameters."}, {"heading": "4. The COPE Mechanism", "text": "In this section, we present our mechanism \u201cCOPE\u201d (COst and Prediction Elicitation) that provides an optimal solution to the problem (7) of parametric prediction from parametric agents. We will first consider two specific settings in order to illustrate the key ideas behind COPE, and to obtain some concrete engineering insights. We will then proceed to present COPE in full generality."}, {"heading": "4.1. Two Example Settings", "text": "We consider the following specific setting in this section. We consider the Gaussian case, where we assume the prior x\u2217 \u223cN (\u00b50, \u03c320), and the observation of every agent n follows the distribution yn \u223cN (x\u2217, 1qn ), independent of all other events. The values of \u00b50 and \u03c30 are assumed to be public knowledge. We assume \u03b8n \u223cUniform[\u03b8, \u03b8\u0304], independent for every n \u2208A. We consider the squared `2-loss to measure the prediction error, namely, ` p(x\u2217, x\u0302) = (x\u2217\u2212 x\u0302)2. In what follows, we consider two cost functions: (i) the linear cost function C(q, \u03b8) = q\u03b8, and (ii) the quadratic cost function C(q, \u03b8) = 1 2 \u03b8q2.\n4.1.1. Linear Cost Function C(q, \u03b8) = q\u03b8 We first consider the linear cost function C(q, \u03b8) = q\u03b8 and discuss the corresponding COPE mechanism. Algorithm 1 presents the higher-level structure of the mechanism which corresponds to the steps in Figure 1. The optimality of the mechanism crucially relies on the careful construction of specific functions referred to in the algorithm, and these constructions are described below.\nRecall that the function Qp : [\u03b8, \u03b8\u0304]\u00d7 [\u03b8, \u03b8\u0304]N\u22121\u2192R+ specifies the effort that the principal requires an agent to exert, based on the cost parameters reported by all agents. In Theorem 1 subsequently, we show that when the cost function is linear, the principal requires only one agent to exert effort.\nAlgorithm 1 COPE Step 1: The principal announces a payment function R. Step 2: Every agent n\u2208A independently reports a cost type \u03b8\u0302n \u2208 [\u03b8, \u03b8\u0304] to the principal. Step 3: The principal sends each agent n \u2208 A a contract, which specifies the effort level Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) along with values of functions \u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n), K(\u03b8\u0302n, \u03b8\u0302\u2212n), and S(\u03b8\u0302n, \u03b8\u0302\u2212n) that comprise the function R. Step 4: Each agent n\u2208A exerts effort qn and makes an observation yn. Step 5: Each agent n\u2208A reports an estimate y\u0302n. Step 6: The principal makes prediction x\u0302. Step 7: The true value x\u2217 is realized. Step 8: The principal makes the payment R(x\u2217, y\u0302n, \u03b8\u0302n, \u03b8\u0302\u2212n) to every agent n\u2208A.\nIf there are multiple agents achieving the same minimum value of \u03b8\u0302n, the principal would randomly choose one agent to exert effort. This property is reflected in the following choice of function Qp:\nQp(\u03b8\u0302n, \u03b8\u0302\u2212n) =\n{ max{(2\u03b8\u0302n\u2212 \u03b8)\u2212 1 2 \u2212\u03c3\u221220 ,0} if n= arg minm\u2208A \u03b8\u0302m\n0 otherwise. (8)\nThe function Qp is designed to strike an optimal balance between the prediction error and the monetary expenditure, accommodating the fact that the agents are heterogeneous. We define n0 = argminm\u2208A\u03b8\u0302m, that is, n0 is the agent with the lowest reported cost parameter.\nWe now characterize the function R that governs the payment made by the principal to the agents. The payments to all agents other than agent n0 are zero, since these agents are not involved in the observation and prediction procedure. The payment made to agent n0 is\nR(x\u2217, y\u0302n0 , \u03b8\u0302n0 , \u03b8\u0302\u2212n0) = \u03c0(\u03b8\u0302n0 , \u03b8\u0302\u2212n0)\u2212 (x \u2217\u2212 y\u0302n0) 2 \u00b7K(\u03b8\u0302n0 , \u03b8\u0302\u2212n0) +S(\u03b8\u0302n0 , \u03b8\u0302\u2212n0), (9)\nwhere\n\u03c0(\u03b8\u0302n0 , \u03b8\u0302\u2212n0) = \u03c0(\u03b8\u0302n0) = \u03b8\u0302n0(2\u03b8\u0302n0 \u2212 \u03b8) \u2212 12 \u2212 \u03b8\u0304\u03c3\u221220 + 2[(2\u03b8\u0304\u2212 \u03b8) 1 2 \u2212 (2\u03b8\u0302n\u2212 \u03b8) 1 2 ],\nK(\u03b8\u0302n0 , \u03b8\u0302\u2212n0) =K(\u03b8\u0302n0) = \u03b8\u0302n0(2\u03b8\u0302n0 \u2212 \u03b8) \u22121, and S(\u03b8\u0302n0 , \u03b8\u0302\u2212n0) = S(\u03b8\u0302n0) = \u03b8\u0302n0(2\u03b8\u0302n0 \u2212 \u03b8) \u2212 12 . (10)\nLet us explain the main ideas behind the above choices. The detailed proof can be found in Appendix. First, The term (x\u2217\u2212 y\u0302n0)2 in (9) ensures that the agent reports his observation truthfully. This is because no other terms in (9) depend on y\u0302n0 , and the value of calculated by K(\u03b8\u0302n0)\nis always positive. Hence, when the agent chooses the reporting observation strategy to maximize his expected payoff, he focuses on minimizing the term Ex\u2217 [(x\u2217 \u2212 y\u0302n0)2] whose value is minimum only when the agent reports his truthful observation, i.e., y\u0302n0 = yn0 . Second, we can verify that the expected payoff of the agent is maximized only when the agent truthfully reports his cost param-\neter, given the agent reports his true observation. Third, the choices of functions K and S ensure that the agent exerts an effort as desired by the principal. As the term (x\u2217\u2212 y\u0302n0)2 makes the agent reports his true observation, we can verify that the expected payoff of the agent is maximized only when the agent chooses qn0 =Q p(\u03b8n0 ,\u03b8\u2212n0). Finally, the function \u03c0 is designed to ensure that the principal\u2019s expected payoff defined in (4) is maximized while ensuring BIC and BIR condition is satisfied. As we shown that the term (x\u2217\u2212 y\u0302n0)2 and the choices of functions K and S guarantee the truthful behavior of the agent, i.e., y\u0302n0 = y\u0302n and qn0 =Q p(\u03b8n0 ,\u03b8\u2212n0), the expected value of R simply equals to the value of the function \u03c0. In such case, we can focus on deriving the function of\n\u03c0 to maximize the expected payoff the principal.\nThen we characterize the prediction decision made by the principal. After collecting all agents\nreported observations, the principal makes the prediction x\u0302 based on the following equation:\nx\u0302(y\u0302n0 , q p n0\n) = \u00b50 \u00b7 1/\u03c320 + qpn0 \u00b7 g(y\u0302n0)\n1/\u03c320 + q p n0\n, (11)\nwhere qpn0 =Q p(\u03b8\u0302n0 , \u03b8\u0302\u2212n0), and function g :R\u2192R is defined as g(y\u0302n0) = y\u0302n0 + (y\u0302n0\u2212\u00b50) (qpn0 \u00b7\u03c320) .\nHere, the predictor x\u0302 employed by the principal is the standard Bayes estimator operating on\nthe agents\u2019 responses and this predictor does not affect the payoff of the agents.\nWe prove that the proposed COPE mechanism is optimal.\nTheorem 1. Under the linear cost function C(q, \u03b8) = q\u03b8, COPE satisfies the BIC and BIR con-\ndition defined in (5) and (6) and maximizes the principal\u2019s expected payoff defined in (7).\nWe provide the detailed proof in Appendix A. An important consequence of the theorem is that\nthe optimal mechanism in the case of linear costs awards the task to the single agent with the\nlowest bid. This corresponds to what we call a \u201ccrowd-tender\u201d system where all agents submit their\ncost parameters, and the lowest bidder is awarded the task. We will discuss the intuition behind such a result in more details in Section 4.1.3.\n4.1.2. Quadratic Cost Function C(q, \u03b8) = 1 2 \u03b8q2 We now consider a quadratic cost function C(q, \u03b8) = 1 2 \u03b8q2 and present COPE for this setting. The higher level structure of COPE is again given by Algorithm 1, and the specific functions referred to in the algorithm is provided below.\nUnder COPE, the function Qp : [\u03b8, \u03b8\u0304]\u00d7 [\u03b8, \u03b8\u0304]N\u22121\u2192R+ that governs the effort that the principal\nrequires an agent to exert is given as\nQp(\u03b8\u0302n, \u03b8\u0302\u2212n) = (2\u03b8\u0302n\u2212 \u03b8)\u22121(W (\u03b8\u0302))\u22122, (12)\nwhere W is the solution of the equation [W (\u03b8)]3\u2212 1 \u03c320\n[W (\u03b8)]2 = \u2211\nm\u2208A 1 2\u03b8m\u2212\u03b8 . An explicit (although\ncumbersome) solution of W is provided in (103) of Appendix B.\nAs in the case of linear costs, the function Qp is designed to optimally harness the heterogenity of the agents in order to minimize the prediction error with a small enough payment. Note that in contrast to the linear case (8), here the principal requires every agent to exert a positive effort.\nWe define the function R that governs the payment to any agent n as\nR(x\u2217, y\u0302n, \u03b8\u0302n, \u03b8\u0302\u2212n) = \u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n)\u2212 (x\u2217\u2212 y\u0302n)2 \u00b7K(\u03b8\u0302n, \u03b8\u0302\u2212n) +S(\u03b8\u0302n, \u03b8\u0302\u2212n), (13)\nwhere\n\u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n) = 1\n2\n( \u03b8\u0302n \u00b7 [ Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) ]2 + \u222b \u03b8\u0304 \u03b8\u0302n [ Qp(z, \u03b8\u0302\u2212n) ]2 dz ) ,\nK(\u03b8\u0302n, \u03b8\u0302\u2212n) = [ Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) + 1/\u03c3 2 0 ]2 \u03b8\u0302n \u00b7Qp(\u03b8\u0302n, \u03b8\u0302\u2212n),\nS(\u03b8\u0302n, \u03b8\u0302\u2212n) = [ Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) + 1/\u03c3 2 0 ] \u03b8\u0302n \u00b7Qp(\u03b8\u0302n, \u03b8\u0302\u2212n). (14)\nThese functions have a form similar to those in the case of linear costs (9), except that these functions depend on the reported cost parameters of all N agents, whereas the corresponding functions in the linear cost setting depended only on the reported cost parameter of one agent. The remaining higher level intuition behind this construction is similar to that behind the linear-cost case described in the previous section.\nThe principal uses the Bayes estimate as her predictor:\nx\u0302(y\u0302,qp) = (1\u2212N)\u00b50/\u03c320 +\n\u2211 n\u2208A (1/\u03c3 2 0 + q p n) \u00b7 y\u0302n\n1/\u03c320 + \u2211 n\u2208A q p n\n, (15)\nwhere qpn =Q p(\u03b8\u0302n, \u03b8\u0302\u2212n).\nThe following theorem establishes the optimality guarantee of COPE under quadratic costs.\nTheorem 2. Under the quadratic cost function C(q, \u03b8) = 1 2 \u03b8q2, COPE satisfies the BIC and BIR condition defined in (5) and (6) and maximizes the principal\u2019s expected payoff defined in (7).\nThe detailed proof is provided in Appendix B. As COPE under the quadratic cost function requires all agents to exert certain effort and to report their observation, this corresponds to a \u201ccrowd-sourcing\u201d system.\n4.1.3. Engineering Takeaways Our results show that interestingly, it is optimal for the principal to call for a crowd-tender when the cost function is linear, while it is optimal to design a crowd-sourcing mechanism when the cost function is quadratic. Informally, the cost function acts as a regularizer on the choice of effort levels qp, and the dichotomy of these two cost functions is related to the sparsity inducing properties of the `1-regularizer, and the lack thereof of the (squared) `2-regularizer."}, {"heading": "4.2. General Setting", "text": "In this section, we will present COPE under more general forms of the cost function, the noise distribution, the prior distribution, and the prediction loss function. Under these general conditions, the structure of the mechanism remains identical to Algorithm 1. We will show that COPE is optimal and feasible under certain regularity conditions.\n4.2.1. Assumptions Cost Function We first define the general cost function of the agent n \u2208 A. Specifically, for\nagent n\u2208A, his cost function C :R+\u00d7R+\u2192R+ is C(qn, \u03b8n) = \u222b qn\n0 c(z, \u03b8n)dz, where c(z, \u03b8n) is the\nthe marginal cost function. We assume that the marginal cost function c :R+\u00d7R+\u2192R+ satisfies:\n\u2202c(q, \u03b8)\n\u2202q > 0,\n\u2202c(q, \u03b8)\n\u2202\u03b8 > 0,\n\u22022c(q, \u03b8)\n\u2202\u03b82 > 0,\n\u22022c(q, \u03b8)\n\u2202q\u2202\u03b8 > 0. (16)\nwhere the first inequality shows a nondecreasing marginal cost in agent\u2019s effort level, the second and third inequalities show that the marginal cost is monotonically increasing and convex in the cost parameter \u03b8n, the last inequality implies that the marginal cost with respect to cost parameter \u03b8n is increasing in effort q. These assumptions are widely used to model the cost function (see, e.g., Che (1993), Chen et al. (2008) and references therein).\nThe cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [\u03b8, \u03b8\u0304]\u2192R+ and probability density function f : [\u03b8, \u03b8\u0304]\u2192 R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, differentiable, and log concave in [\u03b8, \u03b8\u0304]. This is a regularity condition often assumed in auction contexts (see, e.g., Myerson (1981)). This assumption is satisfied by a wide range of distributions, such as the uniform, gamma, and beta distributions. See Rosling (2002) for an extensive discussion on log concave probability distributions.\nObservation and Loss Function We assume that the distribution of the observation yn of any agent n\u2208A comes from a parameterized family of distributions \u03c6(x\u2217, qn), where qn represents the effort exerted by agent n to make observation. A typical parameterized distribution, for example, is the Gaussian distribution with mean x\u2217 and variance 1/qn.\nLet x\u0302 : RN \u00d7 RN+ \u2192 RN be the prediction function that characterizes the prediction made by the principal, and `p : R\u00d7 R\u2192 R+ be the loss function that characterizes the penalty term for mistakes in the principal\u2019s prediction. A typical loss function, for example, is the squared loss `p(x\u2217, x\u0302(y,qp)) = [x\u2217 \u2212 x\u0302(y,qp)]2, where qpn = Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) and qp = (qpn,\u2200n \u2208 A). We measure the utility gained by the principal through the prediction in terms of the Bayes risk. Here, the Bayes risk is calculated under the loss function `p. Specifically, if all agents report their true observations (i.e., y\u0302= y) and exert efforts as desired by the principal (i.e., q= qp =Qp(\u03b8\u0302)), then the Bayes risk Bp :R\u00d7R\u2192R+ is\nBp(qp) = inf x\u0302 E[`p(x\u2217, x\u0302(y,qp))], (17)\nwhere the expectation is taken with respect to x\u2217 and y. We assume that such a Bayes estimator minimizing the Bayes risk exists, and that the Bayes risk is finite.1\nWe will also assume the existence of a function `a : R\u00d7R\u2192R+ using which the principal may measure the accuracy of an agent\u2019s report. Specifically, we assume that if yn is generated according to agent n\u2019s observation distribution, then we assume that `a satisfies\nyn \u2208 arg inf y \u2020 n E[`a(x\u2217, y\u2020n(yn))], (18)\nwhere function y\u2020n : R\u2192 R characterizes the reporting strategy of the agent n \u2208 A given his true observation is yn, the infimum is over all measurable functions of the observation yn, and the assumption says that the identity function is a minimizer of the expected value of `a when its first argument is x\u2217. For instance, we considered `a(x\u2217, y\u0302n) = (x \u2217 \u2212 y\u0302n)2 earlier in the two motivating examples involving the Gaussian distribution.\nWe let Ba :R\u00d7R\u2192R+ denote the associated Bayes risk:\nBa(qn) = inf yn E[`a(x\u2217, yn)], (19)\nwhere the expectation is taken with respect to x\u2217 and yn, and the distribution of yn depends on the agent\u2019s exerted effort qn. The Bayes risk of the principal, i.e., B p(qp), characterizes the principle\u2019s expected payoff loss due to the difference between the true value x\u2217 and her own prediction x\u0302; while the Bayes risk of the agent, i.e., Ba(qn), characterizes the agent n\u2208A\u2019s expected payoff loss due to the difference between the true value x\u0302 and his reporting prediction y\u0302n.\nWe assume that the Bayes risk of the principal and the agent satisfy the following monotonicity\nconditions:\n\u2202Bp(qp)\n\u2202qpn 6 0,\ndBa(qn)\ndqn 6 0,\n\u22022Bp(qp)\n\u2202qpn 2 > 0,\nd2Ba(qn)\ndqn2 > 0,\n\u22022Bp(qp)\n\u2202qpn\u2202q p m\n> 0, \u2200m 6= n. (20)\nIn Section 4.1, we can verify that under the Gaussian distribution, the Bayes risk of the principal\nis Bp(qp) = 1 1/\u03c320+ \u2211 n\u2208A q p n and the Bayes risk of the agent is Ba(qn) = 1 1/\u03c320+qn , both satisfy (20).\nWe assume that the principal has designed a mechanism that can control the agents\u2019 exerted effort and elicit agents to report their observation truthfully. Hence, from the principal\u2019s point of view, qpn, for any n\u2208A is a decision variable for the principal, and we can take the derivative of the principal\u2019s Bayes risk with respect to qpn in (20). However, in reality, the agents would strategically\nchoose their exerted effort. Hence, we need to design a mechanism that involves a carefully designed function Qp, so that the agents would put the effort as the principal expected and truthfully report their observations.\nGiven these preliminaries, we now present our mechanism for this setting.\n4.2.2. Mechanism Our proposed mechanism COPE for the general setting also follows Algo-\nrithm 1, with the specific functions detailed below.\nThe function Qp : [\u03b8, \u03b8\u0304]\u00d7 [\u03b8, \u03b8\u0304]N\u22121\u2192 R+ that governs the effort that the principal requires an\nagent to exert is given as the solution of the equation\nmaxQp E\u03b8 [ \u2212Bp ( Qp(\u03b8\u0302) ) \u2212 \u2211 n\u2208A C ( Qp(\u03b8\u0302n, \u03b8\u0302\u2212n), \u03b8n ) \u2212 \u2211 n\u2208A \u2202C ( Qp(\u03b8\u0302n, \u03b8\u0302\u2212n), \u03b8n ) \u2202\u03b8n \u00b7 F (\u03b8n) f(\u03b8n) ] s.t. Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) is nonincreasing in \u03b8\u0302n,\u2200n\u2208A. (21)\nWe define the function R(\u00b7) that governs the payment to any agent n as\nR(x\u2217, y\u0302n, \u03b8\u0302n, \u03b8\u0302\u2212n) = \u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n)\u2212 `a(x\u2217, y\u0302n) \u00b7K(\u03b8\u0302n, \u03b8\u0302\u2212n) +S(\u03b8\u0302n, \u03b8\u0302\u2212n), (22)\nwhere\n\u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n) =C ( Qp(\u03b8n,\u03b8\u2212n), \u03b8n ) + \u222b \u03b8\u0304 \u03b8n \u2202C(Qp(z,\u03b8\u2212n), \u03b7) \u2202\u03b7 dz, (23)\nK(\u03b8\u0302n, \u03b8\u0302\u2212n) = \u2212 c(Qp(\u03b8\u0302n, \u03b8\u0302\u2212n), \u03b8\u0302n)\ndBa(qn)/dqn \u2223\u2223\u2223\u2223\u2223 qn=Qp(\u03b8\u0302n,\u03b8\u0302\u2212n) , (24)\nS(\u03b8\u0302n, \u03b8\u0302\u2212n) = \u2212 c(Qp(\u03b8\u0302n, \u03b8\u0302\u2212n), \u03b8\u0302n) \u00b7Ba(qn)\ndBa(qn)/dqn\n\u2223\u2223\u2223\u2223\u2223 qn=Qp(\u03b8\u0302n,\u03b8\u0302\u2212n) , (25)\nThe principal uses the Bayes estimate as her predictor: x\u0302(y\u0302,qp) = arg inf x\u0302E [ `p ( x\u2217, x\u0302(y\u0302,qp) )] . The detailed form of the principal\u2019s estimation depends on the distribution of each agent\u2019s observation \u03c6(x\u2217, qn) defined in Section 3.1 and the loss function ` p. With the Gaussian distribution and quadratic loss function adopted in Section 4, the principal\u2019s predictor is calculated as x\u0302(y\u0302,qp) = (1\u2212|Ap|)\u00b50/\u03c320+ \u2211 n\u2208Ap (1/\u03c3 2 0+q p n)\u00b7y\u0302n\n1/\u03c320+ \u2211 n\u2208Ap q p n\n, where qpn =Q p(\u03b8\u0302n, \u03b8\u0302\u2212n) is the decision variable of the principal, Ap\nis the set of agents recruited by the principal to report their estimation, and |Ap| is the number of agents in the set Ap.\n4.2.3. Guarantees The following theorem establishes the optimality guarantees of COPE.\nTheorem 3. COPE satisfies the BIC and BIR condition defined in (5) and (6) and maximizes the principal\u2019s expected payoff defined in (7) if\n\u2202c ( Qp(\u03b8n,\u03b8\u2212n), \u03b8n ) \u2202\u03b8n \u2264 0, (26)\nwhere function c characterizes the agent\u2019s magical cost and is defined in (16).\nCondition (26) implies that the marginal cost of the agent should decrease with the agent\u2019s cost type, so that COPE can induce the truthful behavior of the agents. We note that condition (26) is satisfied under the Gaussian case when \u03b8 follows from uniform distribution, as discussed in Appendix A and B. 5. Simulations\nWe conduct numerical studies to evaluate the performance of COPE. In particular, we investigate the amount of gain that can be achieved by (optimally) exploiting the heterogeneity of the agents. We first consider an integrated system, where the principal and all agents act as an integrated decision maker to maximize their aggregate profit (called network profit). We denote the network profit achieved under the integrated system as the centralized benchmark.2 Then we describe the details of the homogeneous benchmark mechanism under both the linear and quadratic cost function. Finally we compare the performance of COPE to the homogeneous benchmark in terms of principal\u2019s expected payoff, expected prediction error and total payment made by the principal to the agents."}, {"heading": "5.1. Centralized Mechanism", "text": "In the integrated system, the integrated player (the principal and all agents) knows the precise value of \u03b8 (i.e., all agents\u2019 cost types). Moreover, all participated agents would exert the effort that maximizes the network profit. Specifically, the expected network profit is defined as the difference between her utility obtained from prediction and the cost of all agents:\nUnp(q,\u03b8) =\u2212Bp(q)\u2212 \u2211 n\u2208A C ( qn, \u03b8n ) , (27)\nwhere the utility gained through the prediction is measured in terms of the Bayes risk given in (17). Let function qo : R+\u00d7 [\u03b8, \u03b8\u0304]\u2192R+ be the solution that maximizes the network profit defined in (27). By focusing on the case x\u2217 \u223c N (\u00b50, \u03c320), we have the optimal solution under linear cost function as:\nqon =\n{ max{1/ \u221a \u03b8n\u2212 1/\u03c320,0}, if n= arg minm\u2208A \u03b8m,\n0, otherwise. (28)\nThe optimal solution under quadratic cost function is\nqon = 1 \u03b8n \u00b7 1[ W o(\u03b8) ]2 , (29) where the function W o : [\u03b8, \u03b8\u0304]N \u2192R+ is the solution of the below equation:\n[ W o(\u03b8) ]3\u2212 1 \u03c320 \u00b7 [ W o(\u03b8) ]2\u2212\u2211 m\u2208A 1 \u03b8m = 0. (30)\nIn the integrated system, the integrated player makes the prediction as\nx\u0302= \u00b50/\u03c3\n2 0 + \u2211 n\u2208A y o n \u00b7 qon\n1/\u03c320 + \u2211 n\u2208A q o n , (31)\nwhere yon is agent n\u2208A\u2019s true observation."}, {"heading": "5.2. Homogenous Mechanism", "text": "The homogenous mechanism assumes all agents to be identical (although in practice they are not), and hence does not elicit the cost parameters of individual agents. In the absence of this knowledge, the principal operates under the belief that every agent\u2019s cost parameter equals \u03b8\u2020 \u2208 [\u03b8, \u03b8\u0304]. The principal thus chooses payment function Rhom := \u03b1(\u03b8 \u2020) \u2212 \u03b2(\u03b8\u2020) \u00b7 (x\u2217 \u2212 y\u0302n)2, where the function \u03b1 : [\u03b8, \u03b8\u0304]\u2192 R+ and the function \u03b2 : [\u03b8, \u03b8\u0304]\u2192 R+ are chosen to incentivize every agent n to exert optimal effort and report observations truthfully in a manner that maximizes the principal\u2019s payoff.\nWe focus on the case where x\u2217 \u223cN (\u00b50, \u03c320). Let function q\u2020 : R+\u00d7 [\u03b8, \u03b8\u0304]\u2192R+ be the effort that the principal requires every agent to exert, based on the principal\u2019s belief that every agent\u2019s cost parameter equals to \u03b8\u2020. Then the principal makes the prediction as\nx\u0302= \u00b50/\u03c3 2 0 + q\n\u2020\u2211 n\u2208A g(y\u0302n)\n1/\u03c320 +N \u00b7 q\u2020 , (32)\nwhere y\u0302n is the agent n\u2019s reported observation, and the function g :R\u2192R is defined as\ng(y\u0302n) = y\u0302n + y\u0302n\u2212\u00b50 q\u2020 \u00b7\u03c320 . (33)\nLinear Cost Function: Under the linear cost function, the choices of functions q\u2020, \u03b1, and \u03b2\nare\nq\u2020(N,\u03b8\u2020) = 1\nN ( 1\u221a \u03b8\u2020 \u2212 1 \u03c320 ) ,\n\u03b1(\u03b8\u2020) = ( 1/\u03c320 + q \u2020) \u00b7 \u03b8\u2020q\u2020+ \u03b8\u2020q\u2020, \u03b2(\u03b8\u2020) = (1/\u03c320 + q\u2020)2 \u00b7 \u03b8\u2020 The principal chooses the function \u03b1(\u00b7) to make sure that the agent n with the cost type \u03b8\u2020 is willing to participate the prediction task, and chooses the function \u03b2(\u00b7) to make sure that the agent n exerts the effort qn = q \u2020(N,\u03b8\u2020) as the principal desires.\nRecall that the actual cost parameter of the agent n \u2208 A is \u03b8n. Hence, the agent n will exert\neffort qn = \u221a \u03b2(\u03b8\u2020)/\u03b8n\u22121/\u03c320 and report y\u0302n = \u00b50/\u03c3 2 0+yn\u00b7qn\n1/\u03c320+qn to maximize his expected payoff. Besides,\nif the expected payoff of the agent n is negative, he will not participate this prediction task.\nAlso recall that the principal knows the prior information of x\u2217 \u223cN (\u00b50, \u03c320). Hence, the principal can always achieve a payoff of \u22121/\u03c320 by not making any payments, and simply choosing the prior mean has her prediction. Hence, the principal does not pay anything and simply sets x\u0302= \u00b50 if her expected payoff is smaller than \u22121/\u03c320.\nQuadratic Cost Function: Under the quadratic cost function, q\u2020 is the solution of the following\nequation:\n1( 1/\u03c320 +N \u00b7 q\u2020 )2 \u2212 \u03b8\u2020 \u00b7 q\u2020 = 0 The functions \u03b1(\u00b7) and \u03b2(\u00b7) are :\n\u03b1(\u03b8\u2020) = ( 1/\u03c320 + q \u2020) \u00b7 \u03b8\u2020q\u2020+ 1 2 \u03b8\u2020 [ q\u2020 ]2 , \u03b2(\u03b8\u2020) = ( 1/\u03c320 + q \u2020)2 \u00b7 \u03b8\u2020q\u2020. Recall that the actually cost parameter of the agent n \u2208A is \u03b8n. Hence, the agent n will exert\neffort qn to maximize his own expected payoff, where qn is the solution of\n\u03b2( 1/\u03c320 + qn )2 \u2212 \u03b8nqn = 0.\nBesides, if the expected payoff of the agent n is negative, he will not participate this prediction task. Also, the principal does not pay anything and simply sets x\u0302 = \u00b50 if her expected payoff is smaller than \u22121/\u03c320."}, {"heading": "5.3. Numerical Results", "text": "In the simulations, we draw x\u2217 \u223cN (0,1), and set \u03b8 = 0 and \u03b8\u0304 = 1. We vary the number of agents from N = 3 to N = 19. Each point in the plots is an average across 50000 trials. Without loss of generality, we have normalized the principal\u2019s payoff (see (4)) so that it equals zero in the ideal (unachievable) case of zero prediction error and a zero payment. Note that the principal can always achieve a payoff of \u22121 by not making any payments, and simply choosing the prior mean has her prediction.\nFigure 2 depicts the principal\u2019s expected payoff achieved under COPE and under the homogeneous mechanism for different values of \u03b8\u2020 when the cost function is linear and quadratic, respectively. We use the red line with circle markers to denote COPE, the blue dash line with square markers to denote homogeneous mechanism with \u03b8\u2020 = 0.2, the dark dash line with diamond markers to denote homogeneous mechanism with \u03b8\u2020 = 0.5, and the magenta line with right-pointing triangle markers to denote homogeneous mechanism with \u03b8\u2020 = 0.8.\nFigure 2 shows that COPE can improve the principal\u2019s payoff by exploring the heterogeneity of users. The improvement is at least 10% under the linear cost function and 5% under the quadratic cost function.\nBy comparing Figure 2a to Figure 2b, we can see that the value of the belief of principal (i.e., \u03b8\u2020) under the homogeneous mechanism will affect the final result. Under the linear cost function, a lower value of \u03b8\u2020 results in a better performance in terms of the principal\u2019s payoff. However, under the quadratic cost function, a higher value of \u03b8\u2020 results in a better performance. The reasons are as follows.\nUnder the homogeneous mechanism, the value of \u03b8\u2020 will determine the number and types of agents joining the task. Having a higher value of \u03b8\u2020 would incentivize more agents to participate.\nThis is because, for an agent n\u2208A whose cost parameter \u03b8n < \u03b8\u2020, he can put less effort to achieve the same performance as the agent with cost parameter \u03b8\u2020 can.\nUnder the linear cost function, similar as COPE, finding the most capable one would be optimal for the principal, as the marginal cost is nonnegative even the agent does not put any effort. Hence, having a lower value of \u03b8\u2020 would eliminate more agents, and have a higher chance to find the agent with \u03b8n \u2264 \u03b8\u2020.\nOn the contrary, under the quadratic cost function, it would be optimal to recruit as many agents as possible to improve the prediction accuracy. The benefit brought by the accuracy improvement would be higher than the additional payment to agents. Hence, having a higher value of \u03b8\u2020 would help the principal recruit more agents.\nFigure 3 depicts the network profit achieved under COPE and under the homogeneous mechanism for different values of \u03b8\u2020 when the cost function is linear and quadratic, respectively. We use the dash brown line to denote the benchmark solution where the principal and all agents acted as an integrated player.\nFrom Figure 3, we have the following observations. \u2022 COPE can achieve a network profit close to the integrated benchmark solution, e.g., the gap\nis less than 10% under the linear cost function and 3% under the quadratic cost function.\n\u2022 Network profit achieved under COPE increases with the number of agents. This is because\nthe increasing number of agents allows the principal to have a higher chance to incentivize agents\nwith high capability to improve the prediction accuracy. This is true under both costs, even though\nonly one agent will be recruited under the linear cost.\n\u2022 COPE leads to a much higher network profit, compared to the homogeneous mechanism. The\nreason is that COPE explores the heterogeneity of agents by eliciting their cost type information.\nRecall that the principal makes the prediction based on (32) in the homogeneous mechanism.\nAs the actually effort exerted by the agents are different from that desired by the principal (i.e., qn 6= q\u2020), the prediction made by the principal is inaccurate. On the contrary, COPE elicits the cost types of agents as well as incentivizes each agent to exert the appropriate level of effort, which\nresults in a better performance than the homogeneous mechanism. Due to the joint effect of the number of agents and the value of \u03b8\u2020, the performance of homogeneous mechanism is close to that\nof COPE (e.g., N < 4 under the linear cost function and N < 6 under the quadratic cost function) in terms of expected network profit. However, it is difficult to determine the proper choice of \u03b8\u2020 in terms of the number of agents. Finding the optimal value of \u03b8\u2020 given the number of agents will be\nan interesting future work."}, {"heading": "6. Conclusions", "text": "We study the parametric prediction market under information asymmetry. To elicit the truthful information of participating agents and exploit the heterogeneity in the agents, we propose mechanism COPE, which ensures agents to exert effort desired by the principal and report their true observation. Our analysis indicate that, under the Gaussian estimation noise scenario, when the costs incurred by the agents are linear in the amount of exerted effort, the principal should require service from only one agent with the lowest reported cost. On the other hand, when the costs are quadratic in the exerted effort, it is optimal for the principal to recruit multiple agents to complete the task. We also present the general form of COPE that is optimal for a wide variety of settings (e.g., general cost function and the noise distribution).\nIn this work, we have focused on the parametric setting, where the principal recruits agents to estimate a parameter (e.g., the winner of a election) that the realized value can be observed in the future. As in some cases, such as rating the quality of a book, the true outcome cannot be easily observed or verified. In the future, we will consider how to incentivize agents\u2019 behaviour when collecting subjective data. Moreover, in order to give theoretical insights into the problem of estimation from strategic agents, we use one parameter, i.e., the cost type \u03b8 to characterize the heterogeneity of the agents in terms of their cost. Relaxing the parametric assumption (e.g., characterizing agents\u2019 types with random functions) and designing mechanisms with theoretical guarantees is extremely challenging. In practice, heuristics (e.g., Brousseau and Glachant (2002), Chiappori and Salanie\u0301 (1997)) are employed to circumvent the parametric assumption when using these mechanisms. In the future, we would consider how to relax such parametric assumption."}, {"heading": "Appendix. Full Proofs", "text": ""}, {"heading": "A. Proof of Theorem 1: Linear Costs", "text": "The proof will proceed in four steps. The first three steps show that our mechanism incentivizes the agents to be truthful, and the fourth step proves optimality of our mechanism. First, we show that\nirrespective of what an agent reports as his cost parameter, and irrespective of the effort he exerts, the agent is always incentivized to report his true observation. We follow this up and show that irrespective of the effort that an agent exerts, he is always incentivized to report his cost parameter correctly. The third step completes the proof of truthfulness, showing that under truthful reporting of the cost parameter and the observation, in our mechanism, an agent is always incentivized to exert precisely the effort as desired by the principal. Finally, we show that among all mechanisms that ensure truthful reports, our mechanism maximizes the principal\u2019s expected utility.\nWe assume that the random variables {\u03b8n}n\u2208A are independently and identically distributed on support [\u03b8, \u03b8\u0304], with a cumulative distribution function F : [\u03b8, \u03b8\u0304]\u2192 R+ and a probability density function f : [\u03b8, \u03b8\u0304]\u2192 R+. We further assume that the c.d.f. function F is continues, differentiable, and log concave in [\u03b8, \u03b8\u0304]. This assumption is satisfied by a wide range of distributions, such as uniform, gamma, and beta distributions.\nStep 1. Truthful reporting of observation under COPE We will analyze the strategies of the agent who is recruited by the principal and the agents who\nare not recruited by the principal, respectively.\nWe first study the observation reporting strategy of the agent n0 who is recruited and rewarded\nby the principal, where n0 = argminm\u2208A\u03b8\u0302m.\nWe will show that the agent n0 will choose\ny\u0302n0 = \u00b50 \u00b7 1/\u03c320 + yn0 \u00b7 qn0\n1/\u03c320 + qn0 (34)\nto maximize his expected payoff given his exerting effort qn0 and own observation yn0 .\nAs shown in (9), \u03c0(\u03b8\u0302n0), K(\u03b8\u0302n0) and S(\u03b8\u0302n0) are independent of y\u0302n0 . Moreover, the value of calculated by K(\u03b8\u0302n0) is always positive. Hence, when the agent n0 makes reporting observation strategy to maximize his expected payoff, i.e.,\ny\u0302n0 \u2208 argmaxE [ \u03c0(\u03b8\u0302n0)\u2212K(\u03b8\u0302n0) \u00b7 (x \u2217\u2212 y\u0302n0) 2 +S(\u03b8\u0302n0) ] \u2212C ( qn0 , \u03b8n0 ) ,\nwhere the expectation is taken with respect to x\u2217 and cost parameters \u03b8\u2212n0 =\n[\u03b81, . . . , \u03b8n0\u22121, \u03b8n0+1, . . . , \u03b8N ] T except agent n0, it is equivalent for the agent n0 to choose reporting strategy such that\ny\u0302n0 \u2208 argminEx\u2217 [(x \u2217\u2212 y\u0302n0) 2]. (35)\nBased on theory of Bayesian estimation (see Myerson (1979), Lehmann and Casella (1998)), only\nwhen y\u0302n0 = \u00b50\u00b71/\u03c320+yn0 \u00b7qn0\n1/\u03c320+qn0 , the value of Ex\u2217 [(x\u2217\u2212 y\u0302n0)2] is minimized and the expected value is\nEx\u2217 [(x\u2217\u2212 y\u0302n0) 2] =\n1\n1/\u03c320 + qn0 .\nWe then study the observation reporting strategy of other agents who are not recruited and rewarded by the principal. For agent n\u2208A, n 6= n0, he will put zero effort as he does not receive any reward from the principal. In this case, only when reporting his observation y\u0302n = \u00b50 can minimize Ex\u2217 [(x\u2217\u2212 y\u0302n)2]. The expected value of Ex\u2217 [(x\u2217\u2212 y\u0302n)2] is\nEx\u2217 [(x\u2217\u2212 y\u0302n)2] = 1\n1/\u03c320 , n\u2208A, n 6= n0.\nStep 2. Truthful reporting of the cost parameter under COPE We first show that the agent n0 will truthfully reveals his cost type. We first rewrite functions\n\u03c0, K, and S as follows.\n\u03c0(\u03b8\u0302n0 ,\u03b8\u2212n0) =\u03b8\u0302n0 \u00b7Q p(\u03b8\u0302n0 ,\u03b8\u2212n0) + \u222b \u03b8\u0304 \u03b8\u0302n0 Qp(z,\u03b8\u2212n0)dz,\nK(\u03b8\u0302n0 ,\u03b8\u2212n0) = [ Qp(\u03b8\u0302n0 ,\u03b8\u2212n0) + 1/\u03c3 2 0 ]2 \u00b7 \u03b8\u0302n0 , S(\u03b8\u0302n0 ,\u03b8\u2212n0) = [ Qp(\u03b8\u0302n0 ,\u03b8\u2212n0) + 1/\u03c3 2 0 ] \u00b7 \u03b8\u0302n0 .\nThe expected payoff of the agent who has a cost type \u03b8n0 but reports \u03b8\u0302n0 is: E{x\u2217,yn0 ,\u03b8\u2212n0} [ Ua(x\u2217, \u03b8\u0302n0 , qn0 , yn0 , \u03b8n0 ,\u03b8\u2212n0) ] =E\u03b8\u2212n0 [ \u03c0(\u03b8\u0302n0 ,\u03b8\u2212n0)\u2212K(\u03b8\u0302n0 ,\u03b8\u2212n0) \u00b7 1\n1/\u03c320 + qn0 +S(\u03b8\u0302n0 ,\u03b8\u2212n0)\u2212 qn0\u03b8n0\n] .\n(36)\nFor notation convenience, we define the function Uae :R\u00d7 [\u03b8, \u03b8\u0304]\u00d7R+\u00d7 [\u03b8, \u03b8\u0304]N \u2192R+ as\nUae(\u03b8\u0302n0 , qn0 , \u03b8n0 ,\u03b8\u2212n0) = [ \u03c0(\u03b8\u0302n0 ,\u03b8\u2212n0)\u2212K(\u03b8\u0302n0 ,\u03b8\u2212n0)\n1\n1/\u03c320 + qn0 +S(\u03b8\u0302n0 ,\u03b8\u2212n0)\u2212 qn0\u03b8n0\n] , (37)\nwhere \u03b8\u2212n0 are the random variables of all agents\u2019 cost type except that of agent n0. By comparing (36) to (37), the expected payoff of the agent n is\nE{x\u2217,yn0 ,\u03b8\u2212n0} [ Ua(x\u2217, \u03b8\u0302n0 , qn0 , yn0 , \u03b8n0 ,\u03b8\u2212n0) ] =E\u03b8\u2212n0 [ Uae(\u03b8\u0302n0 , qn0 , \u03b8n0 ,\u03b8\u2212n0) ] .\nBy the mean value theorem, we have:\nE [ Uae(\u03b8n0 , qn0 , \u03b8n0 ,\u03b8\u2212n0) ] \u2212E [ Uae(\u03b8\u0302n0 , qn0 , \u03b8n0 ,\u03b8\u2212n0) ] =E\n[ \u2202Uae(\u03b7, qn0 , \u03b8n0 ,\u03b8\u2212n0)\n\u2202\u03b7\n] \u00b7 (\u03b8n0 \u2212 \u03b8\u0302n0),\n(38)\nwhere the expectation is taken with respect to \u03b8\u2212n0 , and \u03b7 lies between \u03b8n0 and \u03b8\u0302n0 .\nWe further have:\nE\u03b8\u2212n0\n[ \u2202Uae(\u03b7, qn0 , \u03b8n0 ,\u03b8\u2212n0)\n\u2202\u03b7 ] =E\u03b8\u2212n0 [ \u2202\n\u2202\u03b7\n( \u03b7Qp(\u03b7,\u03b8\u2212n0) + \u222b \u03b8\u0304 \u03b7 Qp(z,\u03b8\u2212n0)dz\u2212 [ Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 ]2 1/\u03c320 + qn0 \u03b7\n+ [ Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 ] \u03b7\u2212 qn0\u03b8n0 )] =E\u03b8\u2212n0 [ 2\u03b7 \u2202Qp(\u03b7,\u03b8\u2212n0) \u2202\u03b7 \u2212 [ Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 ]2 1/\u03c320 + qn0\n\u2212 2[Q p(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0] 1/\u03c320 + qn0 \u00b7 \u2202Q p(\u03b7,\u03b8\u2212n0) \u2202\u03b7 \u00b7 \u03b7+\n[ Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 ]] =E\u03b8\u2212n0 [( 1\u2212 Q p(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0\nqn0 + 1/\u03c3 2 0\n) \u00b7 ( 2\u03b7 \u00b7 \u2202Q p(\u03b7,\u03b8\u2212n0)\n\u2202\u03b7 +Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0\n)] .\n(39)\nIf we have\n\u2212 \u2202Qp(\u03b7,\u03b8\u2212n0)/\n( Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 ) \u2202\u03b8n0/\u03b8n0 \u2265 1 2 , (40)\nthen we have\n2\u03b7 \u00b7 \u2202Q p(\u03b7,\u03b8\u2212n0)\n\u2202\u03b7 +Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 \u2264 0.\nLemma 1. If \u03b8n \u223cUniform[\u03b8, \u03b8\u0304] which is independent for every n\u2208A, then (40) is satisfied.\nProof : First consider the case N = 1. Since there is only one agent, hence the principal can only\nselect that agent. So Qp is simply Q of that agent:\nQ(\u03b8) = 1/ \u221a \u03b8+F (\u03b8)/f(\u03b8)\u2212 1/\u03c320,\nhence\n\u2212\u2202Q(\u03b8) \u2202\u03b8\n\u03b8\nQ(\u03b8) + 1/\u03c320 =\n1 + \u2202 \u2202\u03b8\n( F (\u03b8)\nf(\u03b8) ) 2[\u03b8+F (\u03b8)/f(\u03b8)] 1\u221a \u03b8+F (\u03b8)/f(\u03b8)\n\u03b8 1/ \u221a \u03b8+F (\u03b8)/f(\u03b8)\n= 1\n2\n1 + \u2202 \u2202\u03b8\n( F (\u03b8)\nf(\u03b8) ) 1 + 1\n\u03b8\n( F (\u03b8)\nf(\u03b8)\n) \u2265 1 2 ,\nwhere the final inequality holds for uniform distribution.\nWe now consider N > 1. Observe that the calculation above will be violated only when the cost\nparameter of some other agent is infinitesimally close to \u03b8n0 (since in that case, \u2202Qp(\u03b8n0 ,\u03b8\u2212n0 )\n\u2202\u03b8n0 is\ndifferent from that calculated above). However given our assumptions that the distribution of \u03b8\nfollows some known distribution such as uniform and normal distributions, and given that the\nnumber of agents N is finite, \u03b8n0 will be well separated from the cost types of all other agents with probability 1.\nAs the agent is selfish, he will exert effort qn0 to maximize his expected payoff. Hence, the agent\u2019s exerted effort can be obtained by taking the first order derivative of (36) with respect to qn0 and setting it to zero, which is\n(1/\u03c320 + q p n0 )2 \u00b7 \u03b8\u0302n0 = (1/\u03c3 2 0 + qn0) 2 \u00b7 \u03b8n0 , (41)\nwhere qpn0 is the shorthand notation for Q p(\u03b8\u0302n0 ,\u03b8\u2212n0).\nBased on (41), we have (i) if \u03b8\u0302n0 > \u03b8n0 , q p n0 < qn0 , (ii) if \u03b8\u0302n0 < \u03b8n0 , q p n0 > qn0 , and (iii) if \u03b8\u0302n0 = \u03b8n0 ,\nqpn0 = qn0 .\nHence, If \u03b8\u0302n0 > \u03b8n0 , the equation (39) is negative and (38) is positive. This inequality also holds for \u03b8\u0302n0 < \u03b8n0 , by a similar argument. Therefore, agent n0 will truthfully report his own cost parameter.\nWe then show that an agent n \u2208 A, n 6= n0 will truthfully reveal his cost type. Recall that the principal does not recruit and reward the agent n \u2208 A, n 6= n0. Hence, the payment to the agent n\u2208A, n 6= n0 is zero. The we have\nE\u03b8\u2212n [ Uae(\u03b8n, qn, \u03b8n,\u03b8\u2212n) ] \u2212E\u03b8\u2212n [ Uae(\u03b8\u0302n, qn, \u03b8n,\u03b8\u2212n) ] = 0,\u2200n\u2208A, n 6= n0,\nwhich shows that there is no difference between truthfully reporting cost type or not in terms of expected payoff for the agent n. Without loss of generality, we assume that in this case, the agent will truthfully report their cost types.\nStep 3. Incentivize agent to exert precisely the effort as desired by the principal\nunder COPE\nAs we have proved in Step 2 that the agent n0 would truthfully report his cost type (\u03b8\u0302n = \u03b8n),\nthen we will show that the agent n0 exerts an effort level such that qn0 = q p n0 would maximize his expected payoff, which is given as\nE [ Uae(\u03b8n0 , q p n0 , \u03b8n0 ,\u03b8\u2212n0) ] = \u03c0(\u03b8n0 ,\u03b8\u2212n0)\u2212K(\u03b8n0 ,\u03b8\u2212n0)\n1\n1/\u03c320 + qn0 +S(\u03b8n0 ,\u03b8\u2212n0)\u2212 qn0\u03b8n0 ,\n(42)\nwhere the expectation is taken with respect to \u03b8\u2212n0 .\nIt can be verified that (42) is concave in qn0 . Hence, by taking the first order derivative of (42)\nwith respect to qn0 , we have\n\u2202 \u2202qn0 E [ Uae(\u03b8n0 , q p n0 , \u03b8n0 ,\u03b8\u2212n0) ] =\n[ 1/\u03c320 + q p n0\n1/\u03c320 + qn0\n]2 \u00b7 \u03b8n0 \u2212 \u03b8n0 . (43)\nWe can verify that the value of (43) equals to zero only when qn0 = q p n0 . Hence, agent n0 will exert the effort as the principal desires to maximize his expected payoff. Then (34) is rewritten as\ny\u0302n0 = \u00b50 \u00b7 1/\u03c320 + yn0 \u00b7 qpn0\n1/\u03c320 + q p n0\n. (44)\nBecause the principal knows the value of \u00b50, \u03c3 2 0, and q p n0 , she can infer the agent n0\u2019s truth\nobservation yn0 from (44).\nStep 4. Maximize the principal\u2019s expected utility under COPE Then we look at the expected payoff of the principal. The following lemma describes that COPE\nis the optimal mechanism that maximizes the principal\u2019s expected utility. Lemma 2. The optimal predictor x\u0302= \u00b50\u00b71/\u03c320+ \u2211 n\u2208A yn\u00b7q p n\n1/\u03c320+ \u2211 n\u2208A q p n\ndefined in COPE maximizes the principal\u2019s expected utility, and the Bayes risk of the principal\u2019s prediction is Bp ( qp )\n= 1 1/\u03c320+ \u2211 n\u2208A q p n .\nProof : Recall that qpn =Q p(\u03b8n,\u03b8\u2212n). Given all agents\u2019 observation y and agents\u2019 exert effort q p,\nthe principal\u2019s updated belief on the realization of x\u2217 can be expressed as\nx\u2217|(y,qp)\u223cN ( \u00b50 \u00b7 1/\u03c320 + \u2211 n\u2208A yn \u00b7 qpn\n1/\u03c320 + \u2211 n\u2208A q p n\n, 1 1/\u03c320 + \u2211 n\u2208A q p n\n) .\nTo maximize the expected utility for the prediction, the principal solves\nmax x\u0302\nE [ v\u2212 (x\u2217\u2212 x\u0302)2|(y,qp) ] = max\nx\u0302\n( v\u2212 { E [ x\u22172|(y,qp) ] \u2212 2x\u0302E [ x\u2217|(y,qp) ] + x\u03022 }) = max\nx\u0302\n( v\u2212 [ x\u0302\u2212 \u00b50 \u00b7 1/\u03c320 + \u2211\nn\u2208A yn \u00b7 qpn 1/\u03c320 + \u2211 n\u2208A q p n\n]2 \u2212 1\n1/\u03c320 + \u2211 n\u2208A q p n ) \u2264 v\u2212 1\n1/\u03c320 + \u2211 n\u2208A q p n\nThe equality holds only when\nx\u0302= \u00b50 \u00b7 1/\u03c320 +\n\u2211 n\u2208A yn \u00b7 qpn\n1/\u03c320 + \u2211 n\u2208A q p n\n. (45)\nHence, the optimal predictor that maximizes the principal\u2019s expected utility is\nx\u0302 ( y,qp ) = \u00b50 \u00b7 1/\u03c320 + \u2211 n\u2208A yn \u00b7 qpn\n1/\u03c320 + \u2211 n\u2208A q p n\n(46)\nand the Bayes risk is\nBp ( qp )\n= inf x\u0302 E[(x\u2217\u2212 x\u0302)2] = 1 1/\u03c320 + \u2211 n\u2208A q p n ,\nwhere the expectation is taken with respect to x\u2217 and y.\nRecall that under the linear cost function, the principal only recruits agent n0 to exert effort, in such case, qpn = 0, \u2200n \u2208 A, n 6= n0. Also recall that the principal can infer the true observation of the agent n0 through the function g : R\u2192R, and such an observation is defined as yn0 = g(y\u0302n0) = y\u0302n0 + (y\u0302n0 \u2212\u00b50)/(qpn0\u03c3 2 0). Then putting back to (46) we can get the conclusion.\nWe then show that the desired effort level Qp(\u03b8n,\u03b8\u2212n) defined in (8) and the function \u03c0(\u03b8n,\u03b8\u2212n)\ndefined in (9) can maximize the principal\u2019s expected payoff and satisfy BIC and BIR conditions.\nNotice that as the agent n0 exerts effort such that qn0 = q p n0 and reports y\u0302n0 = \u00b50\u00b71/\u03c320+yn0 \u00b7q p n0\n1/\u03c320+q p n0\n, the\nexpected payment function is reduced to\nE{x\u2217,yn0 ,\u03b8\u2212n0} [ R(x\u2217, yn0 , qn0 , \u03b8n0 ,\u03b8\u2212n0) ] =E\u03b8\u2212n0 [ \u03c0(\u03b8n0 ,\u03b8\u2212n0)\u2212K(\u03b8n0 ,\u03b8\u2212n0) \u00b7\n1\n1/\u03c320 + qn0 +S(\u03b8n0 ,\u03b8\u2212n0)\n] =E\u03b8\u2212n0 [ \u03c0(\u03b8n0 ,\u03b8\u2212n0) ] . (47)\nFor other agent n\u2208A, n 6= n0, as the principal does not require him to do the observation, we first assume that the expected payment to him is as follows,\nE{x\u2217,yn0 ,\u03b8\u2212n0}[R(x \u2217, yn, \u03b8n,\u03b8\u2212n)] =E\u03b8\u2212n0\n[ \u03c0(\u03b8n,\u03b8\u2212n) ] ,\u2200n\u2208A, n 6= n0. (48)\nLater we will show that \u03c0(\u03b8n,\u03b8\u2212n) = 0,\u2200n 6= n0.\nThe expected payoff of agent n\u2208A is\nE\u03b8\u2212n [ Uae ( \u03b8\u0302n0 , q p n0 , \u03b8n0 ,\u03b8\u2212n0 )] =E\u03b8\u2212n [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212 \u03b8nQp(\u03b8\u0302n,\u03b8\u2212n) ] ,\nwhere qpn0 is the shorthand notation of Q p(\u03b8\u0302n,\u03b8\u2212n). For notation convenience, we adopt\nUae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n ) in the later proof of Theorem 1, where the function Uae is rewritten as\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212 \u03b8nQp(\u03b8\u0302n,\u03b8\u2212n) ] . (49)\nCorrespondingly, BIC and BIR conditions, i.e., (5) and (6), can be rewritten as\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265E\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] , \u2200\u03b8\u0302n 6= \u03b8n, (50) E\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265 0, \u2200\u03b8n \u2208 [\u03b8, \u03b8\u0304]. (51)\nBase on Lemma 2, (47), and (48), the expected payoff of the principal is\nEx\u2217,y,\u03b8[U p(x\u2217,qp,y, \u03b8\u0302)] =\u2212Bp ( qp ) \u2212Ex\u2217,y,\u03b8 [\u2211 n\u2208A R(x\u2217, yn, \u03b8n,\u03b8\u2212n) ]\n=\u2212 1 1/\u03c320 + \u2211 n\u2208A q p n \u2212E\u03b8 [\u2211 n\u2208A \u03c0(\u03b8n,\u03b8\u2212n) ] .\nRecall that qpn =Q p(\u03b8n,\u03b8\u2212n), the principal\u2019s optimal problem defined in (7) can be rewritten as\nsup {Qp(\u03b8),\u03c0(\u03b8)},\u2200\u03b8n\u2208\u03b8,\u2200n\u2208A\nE[U p(x\u2217,qp,y, \u03b8\u0302)],\nsubject to : BIC and BIR in (50) and (51).\n(52)\nIn the following lemmas, we characterize an equivalent formulation for the feasible region defined by BIC and BIR. Using these lemmas, we show that Qp(\u03b8n,\u03b8\u2212n) defined in (8) and \u03c0(\u03b8n,\u03b8\u2212n) defined in (9) are the optimal solution that solves the principal\u2019s problem in (52).\nLemma 3. The solution of (52) is feasible if and only if it satisfies the following conditions for all \u03b8n \u2208 [\u03b8, \u03b8\u0304], \u2200n\u2208A:\n\u2022 the expected payoff of agent n is\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n Qp(x,\u03b8\u2212n)dx ] ; (53)\n\u2022 Qp(\u03b8n,\u03b8\u2212n) is non-increasing in \u03b8n.\nProof : The proof of Lemma 3 is as follows. We first show that BIC and BIR imply the condition\nin (53).\nNotice that the first derivative of (49) is:\n\u2202E\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )]\n\u2202\u03b8n =E\u03b8\u2212n\n[ \u2212Qp(\u03b8\u0302n,\u03b8\u2212n) ] \u2264 0. (54)\nThen, for any \u03b81n > \u03b8 2 n, we have E\u03b8\u2212n [ Uae(\u03c0(\u03b81n,\u03b8\u2212n),Q p(\u03b81n,\u03b8\u2212n), \u03b8 1 n) ] \u2264E\u03b8\u2212n [ Uae(\u03c0(\u03b81n,\u03b8\u2212n),Q p(\u03b81n,\u03b8\u2212n), \u03b8 2 n) ]\n\u2264E\u03b8\u2212n [ Uae(\u03c0(\u03b82n,\u03b8\u2212n),Q p(\u03b82n,\u03b8\u2212n), \u03b8 2 n) ] ;\n(55)\nwhere the first inequality is because (54) and the second is from the BIC condition defined in (50). In other words, for the agent n\u2208A whose cost parameter \u03b8\u2264 \u03b8n \u2264 \u03b8\u0304, we have\nE\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0304,\u03b8\u2212n),Q p(\u03b8\u0304,\u03b8\u2212n), \u03b8\u0304) ] \u2264E\u03b8\u2212n [ Uae(\u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n) ]\n\u2264E\u03b8\u2212n [ Uae(\u03c0(\u03b8,\u03b8\u2212n),Q p(\u03b8,\u03b8\u2212n), \u03b8) ] . (56)\nRecall that the BIR condition is\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265 0, \u2200\u03b8n \u2208 [\u03b8, \u03b8\u0304], (57)\nwhich implies that, for the agent n \u2208 A with any value \u03b8n \u2208 [\u03b8, \u03b8\u0304], his expected payoff should at least be zero. Then the expected payoff of the agent n with cost parameter \u03b8\u0304 must be binding at zero. Otherwise, the principal can reduce the \u03c0(\u03b8\u0304,\u03b8\u2212n) by a small value of \u03b4 > 0, which does not violate the constraint of (57) but raises the principal\u2019s expected payoff. Hence, we have\nE\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0304,\u03b8\u2212n),Q p(\u03b8\u0304,\u03b8\u2212n), \u03b8\u0304) ] = 0. (58)\nLet Uae(\u03b8n,\u03b8\u2212n) =U ae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n ) . From the BIC condition, we have\nE\u03b8\u2212n [ Uae(\u03b8n,\u03b8\u2212n) ] = max\n\u03b8\u0302n\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] .\nBy using the envelope theorem, we have:\n\u2202E\u03b8\u2212n [ Uae(\u03b8n,\u03b8\u2212n) ] \u2202\u03b8n = \u2202E\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n) ] \u2202\u03b8n \u2223\u2223\u2223\u2223\u2223 \u03b8\u0302n=\u03b8n =E\u03b8\u2212n [ \u2212Qp(\u03b8n,\u03b8\u2212n) ] ,\n(59)\nwhere \u03b8n is a parameter. By integrating both sides from the value of \u03b8n to \u03b8\u0304 and using (58) and the assumption that the random variable \u03b8n of the agent n is independent for every n\u2208A , we get\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n Qp(x,\u03b8\u2212n)dx ] . (60)\nWe prove that Qp(\u03b8n,\u03b8n) is nonincreasing in \u03b8n by contradiction. Let pn be the shorthand\nnotation for \u03c0(\u03b8n,\u03b8\u2212n). Suppose for any \u03b8 1 n > \u03b8 2 n, we have Q p(\u03b81n,\u03b8\u2212n)>Q p(\u03b82n,\u03b8\u2212n). Because\n\u22022Uae ( pn, q p n, \u03b8n ) \u2202qpn\u2202\u03b8n =\u22121< 0, (61)\n\u22022Uae ( pn, q p n, \u03b8n ) \u2202qpn 2 = 0, (62)\nwe have\n0 = \u2202Uae\n( pn, q p n, \u03b8 1 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b81n,\u03b8\u2212n)\n= \u2202Uae\n( pn, q p n, \u03b8 1 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n)\n< \u2202Uae\n( pn, q p n, \u03b8 2 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n) , (63)\nwhere the first equality is due to BIC when the agent n\u2019s cost parameter \u03b8n has the value of \u03b8 1 n, the second equality is due to (62), and the inequality is due to (61).\nHowever, based on the BIC condition, if the agent n\u2019s cost parameter \u03b8n has the value of \u03b8 2 n,\nthen we should have\n\u2202Uae ( pn, q p n, \u03b8 2 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n) = 0,\nwhich holds true for all scalar values of pn. Hence, for any \u03b8 1 n > \u03b8 2 n, Q p(\u03b81n,\u03b8\u2212n)\u2264Qp(\u03b82n,\u03b8\u2212n).\nThen we need to prove that (53) implies BIC and BIR defined in (50) and (51).\nBIR is verified by putting \u03b8n back to (53). Besides, by putting \u03b8n = \u03b8\u0304 back to (53), we have\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0304,\u03b8\u2212n),Q p(\u03b8\u0304,\u03b8\u2212n), \u03b8\u0304 )] = 0.\nThen we prove that (53) implies BIC. Notice that we have:\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )]\n1 =E\u03b8\u2212n [ \u2212 \u222b \u03b8\u0304 \u03b8n \u2202Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz ] 2 =E\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8\u0302n ) \u2212 \u222b \u03b8\u0302n \u03b8n \u2202Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz\n] 3 =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8\u0302n Qp(\u03b7,\u03b8\u2212n)d\u03b7\u2212 \u222b \u03b8\u0302n \u03b8n \u2202Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz\n] 4 =E\u03b8\u2212n [ \u2212 \u222b \u03b8n \u03b8\u0304 Qp(\u03b7,\u03b8\u2212n)d\u03b7\u2212 \u222b \u03b8\u0302n \u03b8n Qp(\u03b7,\u03b8\u2212n)d\u03b7+ \u222b \u03b8\u0302n \u03b8n Qp(\u03b8\u0302n,\u03b8\u2212n)dz\n] 5 =E\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n ) + \u222b \u03b8\u0302 \u03b8n ( Qp(\u03b8\u0302n,\u03b8\u2212n)\u2212Qp(\u03b7,\u03b8\u2212n) ) d\u03b7 ] ,\nwhere the third equality and the fifth equality are obtained by (53).\nIf \u03b8\u0302n > \u03b8n, then the above equation is non-positive (because Q p(\u03b7,\u03b8\u2212n) is non-increasing in \u03b7),\nhence\nE\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n) ] <E\u03b8\u2212n [ Uae(\u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n) ] .\nThis inequality also holds for \u03b8\u0302n < \u03b8n by a similar argument. Therefore, the two condition imply BIC.\nThen based on Lemma 3, we have the following Lemma.\nLemma 4. The optimisation problem in (52) has the following equivalent formulation:\nmax {Qp(\u03b8)},\u2200\u03b8n\u2208\u03b8\nE\u03b8 [ \u2212 1\n1/\u03c320 + \u2211 n\u2208AQ p(\u03b8n,\u03b8\u2212n) \u2212 \u2211 n\u2208A Qp(\u03b8n,\u03b8\u2212n) \u00b7 \u03b8n\u2212 \u2211 n\u2208A Qp(\u03b8n,\u03b8\u2212n) \u00b7 F (\u03b8n) f(\u03b8n) ] ,\ns.t. Qp(\u03b8n,\u03b8\u2212n) is nonincreasing in \u03b8n, (64)\nwhere the expectation is taken with respect to \u03b8.\nProof : The proof of Lemma 4 is as follows. The expected payoff of the principal can be written\nas: E\u03b8 [ \u2212 1\n1/\u03c320 + \u2211 n\u2208AQ p(\u03b8n,\u03b8\u2212n) \u2212 \u2211 n\u2208A Qp(\u03b8n,\u03b8\u2212n) \u00b7 \u03b8n\u2212 \u2211 n\u2208A Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )]\n=E\u03b8 [ \u2212 1\n1/\u03c320 + \u2211 n\u2208AQ p(\u03b8n,\u03b8\u2212n) \u2212 \u2211 n\u2208A Qp(\u03b8n,\u03b8\u2212n) \u00b7 \u03b8n\u2212 \u2211 n\u2208A \u222b \u03b8\u0304 \u03b8n Qp(x,\u03b8\u2212n)dx ] ,\n(65)\nwhere the expectation is taken with respect to \u03b8. Notice that E\u03b8n [\u222b \u03b8\u0304\n\u03b8n\nQp(x,\u03b8\u2212n)dx ] = \u222b \u03b8\u0304 \u03b8 \u222b \u03b8\u0304 z Qp(x,\u03b8\u2212n)dx \u00b7 f(z)dz = \u222b \u03b8\u0304 \u03b8 F (z)Qp(z,\u03b8\u2212n)dz\n= \u222b \u03b8\u0304 \u03b8 F (z) f(z) Qp(z,\u03b8\u2212n)f(z)dz =E\u03b8n [F (\u03b8n) f(\u03b8n) Qp(\u03b8n,\u03b8\u2212n) ] ,\nwhere the first equation is obtained by using integration by parts. Then by applying the above equation to (126) and the fact that {\u03b8n}n\u2208A are assumed to be random, independently and identically distributed on support [\u03b8, \u03b8\u0304], we can get the conclusion.\nBased on Lemma 4, the principal\u2019s problem reduces to choosing the desired effort qpn = Qp(\u03b8n,\u03b8\u2212n) for each agent n \u2208A. We first consider the problem in (64) without the constraint. If the optimal solution to this unconstrained problem is increasing, then it is also an optimal solution to the constrained problem.\nLemma 5. Qp(\u03b8n,\u03b8\u2212n) defined in (8) and \u03c0(\u03b8n,\u03b8\u2212n) defined in (9) are the optimal solution that solves the principal\u2019s problem in (52)\nProof : We first prove that for the agent \u2200n\u2208A,\nQp(\u03b8n,\u03b8\u2212n) =\n{ max{1/ \u221a \u03b3(\u03b8n)\u2212 1/\u03c320,0}, if n0 = argminm\u2208A\u03b8m,\n0, otherwise,\nis the optimal solution of (64) by contradiction.\nAs qpn =Q p(\u03b8n,\u03b8\u2212n), the problem in (64) is equivalent to\nmin qp\u22650\n1 1/\u03c320 + \u2211 n\u2208A q p n + \u2211 n\u2208A qpn \u00b7 \u03b3(\u03b8n),\ns.t. qpn is nonincreasing in \u03b8n, (66)\nwhere \u03b3(\u03b8n) = \u03b8n +F (\u03b8n)/f(\u03b8n).\nWithout loss of generality, let \u03b3(\u03b81)\u2264 \u03b3(\u03b82) . . .\u2264 \u03b3(\u03b8N). If the principal\u2019s desired efforts from all\nagents are positive, then the solution is\nqp1 = q p,\u2217 1 , q p 2 = q p,\u2217 2 , . . . , q p N = q p,\u2217 N . (67)\nSuppose that there is another solution such that qp,\u20201 = q p 1 + q p j , qp,\u2020i = q p i , \u2200i 6= 1, j, i\u2208A,\nqp,\u2020j = 0.\n(68)\nThen we can verify that\n\u2211 n\u2208A qp,\u2020n = \u2211 n\u2208A qpn and \u2211 n\u2208A ( qp,\u2020n \u00b7 \u03b3(\u03b8n) ) \u2264 \u2211 n\u2208A ( qpn \u00b7 \u03b3(\u03b8n) ) .\nHence, (67) is not an optimal solution. Then we let qpn = 0,\u2200n> 1, the problem in (66) becomes\nmin qp1\n1\n1/\u03c320 + q p 1\n+ qp1 \u00b7 \u03b3(\u03b81),\ns.t. qp1 \u2265 0. (69)\nBy solving the above problem we can get that qp1 = max{1/ \u221a \u03b3(\u03b81)\u22121/\u03c320,0}. As we define n0 =\nargminm\u2208A\u03b8m and the assumption that F is log-concave in [\u03b8, \u03b8\u0304], we have q p n0 = max{1/ \u221a \u03b3(\u03b8n0)\u2212 1/\u03c320,0}.\nAccording to (53), we have\nE\u03b8\u2212n [ \u03c0(\u03b8n,\u03b8\u2212n)\u2212Qp(\u03b8n,\u03b8\u2212n) \u00b7 \u03b8n ] =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n Qp(x,\u03b8\u2212n)dx ] .\nThen the optimal payment function given the agent n0 and 1/ \u221a \u03b3(\u03b8n0)\u2212 1/\u03c320 \u2265 0 is\n\u03c0(\u03b8n0) = \u03b8n0/ \u221a \u03b3(\u03b8n0)\u2212 \u03b8n0/\u03c3 2 0 + \u222b \u03b8\u0304 \u03b8n0 ( 1\u221a \u03b3(z) \u2212 1 \u03c320 ) dz = \u03b8n0/ \u221a \u03b3(\u03b8n0)\u2212 \u03b8\u0304/\u03c3 2 0 + \u222b \u03b8\u0304 \u03b8n0 1\u221a \u03b3(z) dz,\nand the payment will be zero if 1/ \u221a \u03b3(\u03b8n0)\u2212 1/\u03c320 < 0.\nFor other agents, the payments will be zero as they are not involved in the observation and\nprediction.\nAs in Theorem 1 , we assume that \u03b8n \u223c Uniform[\u03b8, \u03b8\u0304], which is independent for every n \u2208 A.\nPutting the expression of F and f back to the above equations, we can have the conclusion."}, {"heading": "B. Proof of Theorem 2: Quadratic Costs", "text": "The proof is similar to that in Section A. The difference is as follows.\nFirst, the function \u03c0 : [\u03b8, \u03b8\u0304]N \u2192R+, K,S : [\u03b8, \u03b8\u0304]N \u00d7R+\u2192R+ are defined as\n\u03c0(\u03b8\u0302n,\u03b8\u2212n) = 1 2 \u00b7 [ \u03b8\u0302n \u00b7 [ Qp(\u03b8\u0302n,\u03b8\u2212n) ]2 + \u222b \u03b8\u0304 \u03b8\u0302n ([ Qp(z,\u03b8\u2212n) ]2) dz ] , (70)\nK(\u03b8\u0302n,\u03b8\u2212n) = [ Qp(\u03b8\u0302n,\u03b8\u2212n) + 1/\u03c3 2 0 ]2 \u03b8\u0302n \u00b7Qp(\u03b8\u0302n,\u03b8\u2212n), (71)\nS(\u03b8\u0302n,\u03b8\u2212n) = [ Qp(\u03b8\u0302n,\u03b8\u2212n) + 1/\u03c3 2 0 ] \u03b8\u0302n \u00b7Qp(\u03b8\u0302n,\u03b8\u2212n). (72)\nStep 1. Truthful reporting of observation under COPE We will analyze the strategies of the agent n, \u2200n\u2208A. We will show that the agent n will choose\ny\u0302n = \u00b50 \u00b7 1/\u03c320 + yn \u00b7 qn\n1/\u03c320 + qn (73)\nto maximize his expected payoff given his exerting effort qn and own observation yn.\nAs \u03c0(\u03b8\u0302n,\u03b8\u2212n), K(\u03b8\u0302n,\u03b8\u2212n) and S(\u03b8\u0302n,\u03b8\u2212n) are independent of y\u0302n and the value of calculated by K(\u03b8\u0302n,\u03b8\u2212n) is always positive. Hence, when the agent n makes reporting observation strategy to maximize his expected payoff, i.e.,\ny\u0302n \u2208 argmaxE [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212K(\u03b8\u0302n,\u03b8\u2212n)(x\u2217\u2212 y\u0302n)2 +S(\u03b8\u0302n,\u03b8\u2212n) ] \u2212C ( qn, \u03b8n ) ,\nwhere the expectation is taken with respect to x\u2217 and cost parameters \u03b8\u2212n = [\u03b81, . . . , \u03b8n\u22121, \u03b8n+1, . . . , \u03b8N ] T except agent n, it is equivalent for the agent n to choose reporting strategy such that\ny\u0302n \u2208 argminEx\u2217 [(x\u2217\u2212 y\u0302n)2]. (74)\nThe value of Ex\u2217 [(x\u2217 \u2212 y\u0302n)2] is minimized when y\u0302n = \u00b50\u00b71/\u03c3 2 0+yn\u00b7qn\n1/\u03c320+qn . The expected value in this\ncase is\nEx\u2217 [(x\u2217\u2212 y\u0302n)2] = 1\n1/\u03c320 + qn .\nStep 2. Truthful reporting of cost parameter under COPE We will show that the agent n, \u2200n\u2208A will truthfully reveals his cost type. The expected payoff\nof the agent who has a cost type is \u03b8n but reports \u03b8\u0302n is: E{x\u2217,yn,\u03b8\u2212n} [ Ua(x\u2217, \u03b8\u0302n, qn, yn, \u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212K(\u03b8\u0302n,\u03b8\u2212n) \u00b7 1\n1/\u03c320 + qn +S(\u03b8\u0302n,\u03b8\u2212n)\u2212\n1 2 \u03b8nq 2 n\n] .\n(75)\nFor notation convenience, we define\nUa(\u03b8\u0302n, qn, \u03b8n,\u03b8\u2212n) = [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212 K(\u03b8\u0302n,\u03b8\u2212n)\n1/\u03c320 + qn +S(\u03b8\u0302n,\u03b8\u2212n)\u2212\n1 2 \u03b8nq 2 n ] By the mean value theorem, we have:\nE [ Ua(\u03b8n, qn, \u03b8n,\u03b8\u2212n) ] \u2212E [ Ua(\u03b8\u0302n, qn, \u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [\u2202Ua(\u03b7, qn, \u03b8n,\u03b8\u2212n) \u2202\u03b7 ] (\u03b8n\u2212 \u03b8\u0302n), (76)\nwhere the expectation is taken with respect to \u03b8\u2212n, and \u03b7 lies between \u03b8n and \u03b8\u0302n.\nWe further have\nE\u03b8\u2212n\n[ \u2202Ua(\u03b7, qn, \u03b8n,\u03b8\u2212n)\n\u2202\u03b7 ] =E\u03b8\u2212n0 [ \u2202\n\u2202\u03b7\n( 1\n2 \u00b7 \u03b7 [ Qp(\u03b7,\u03b8\u2212n) ]2 + \u222b \u03b8\u0304 \u03b7 [ Qp(z,\u03b8\u2212n) ]2 dz\n+ [ Qp(\u03b7,\u03b8\u2212n0) + 1/\u03c3 2 0 ] \u03b7 [ Qp(\u03b7,\u03b8\u2212n) ] \u2212 [ Qp(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0 ]2 1/\u03c320 + qn \u03b7 [ Qp(\u03b7,\u03b8\u2212n) ] \u2212 1 2 \u03b8nq 2 n )]\n=E\u03b8\u2212n\n[( 1\u2212 Q p(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0\nqn + 1/\u03c320\n)( 2Qp(\u03b7,\u03b8\u2212n) \u00b7 \u03b7 \u00b7 \u2202Qp(\u03b7,\u03b8\u2212n) \u2202\u03b7 + [ Qp(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0 ] \u00b7Qp(\u03b7,\u03b8\u2212n)\n+ [ Qp(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0 ] \u00b7 \u03b7 \u00b7 \u2202\n\u2202\u03b7\n[ Qp(\u03b7,\u03b8\u2212n) ]) \u2212 1\n2\n[ Qp(\u03b7,\u03b8\u2212n) ]2] . (77)\nWe can check that if\n\u2212\u2202Q p(\u03b7,\u03b8\u2212n)/(Q p(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0) \u2202\u03b8n/\u03b8n \u2265 1 2 ,\u2200n\u2208A, (78)\nwe have\n2Qp(\u03b7,\u03b8\u2212n) \u00b7 \u03b7 \u00b7 \u2202Qp(\u03b7,\u03b8\u2212n) \u2202\u03b7 + [ Qp(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0 ] \u00b7Qp(\u03b7,\u03b8\u2212n)\u2264 0.\nLater we will show that Qp(\u03b7,\u03b8\u2212n) is non-increasing in \u03b7, \u2200n\u2208A (i.e., Lemma 7), hence,\n2Qp(\u03b7,\u03b8\u2212n)\u03b7 \u2202Qp(\u03b7,\u03b8\u2212n) \u2202\u03b7 + [ Qp(\u03b7,\u03b8\u2212n) + 1/\u03c3 2 0 ] Qp(\u03b7,\u03b8\u2212n) + \u03b7\n\u03c320\n\u2202\n\u2202\u03b7\n[ Qp(\u03b7,\u03b8\u2212n) ] \u2264 0.\nLemma 6. If \u03b8n \u223cUniform[\u03b8, \u03b8\u0304] which is independent for every n\u2208A, then (78) is satisfied.\nProof : First, for an agent n\u2208A,\nQp(\u03b8n,\u03b8\u2212n) = 1\n\u03b8n + F (\u03b8n)\nf(\u03b8n)\n\u00b7 1[ W (\u03b8) ]2 , (79) where the function W : [\u03b8, \u03b8\u0304]N \u2192R+ is the solution of the below equation:\n[ W (\u03b8) ]3\u2212 1 \u03c320 \u00b7 [ W (\u03b8) ]2\u2212\u2211 m\u2208A\n1\n\u03b8m + F (\u03b8m)\nf(\u03b8m)\n= 0. (80)\nhence\n\u2212\u2202Q p(\u03b8n,\u03b8\u2212n)\n\u2202\u03b8n\n\u03b8n Qp(\u03b8n,\u03b8\u2212n) + 1/\u03c320\n\u2265 1 + \u2202 \u2202\u03b8\n( F (\u03b8)\nf(\u03b8) ) 1 + 1\n\u03b8\n( F (\u03b8)\nf(\u03b8)\n) \u00b7 1 1 + [W (\u03b8)] 2\n\u03c320\n( \u03b8n + F (\u03b8n)\nf(\u03b8n) ) \u2265 1\n2 ,\nwhere the final inequality holds for uniform distribution.\nAs the agent is selfish, he will exert effort qn0 to maximize his expected payoff. Hence, the agent\u2019s exerted effort can be obtained by taking the first order derivative of (75) with respect to qn and setting it to zero, which is\n(1/\u03c320 + q p n) 2 \u00b7 qpn \u00b7 \u03b8\u0302n = (1/\u03c320 + qn)2 \u00b7 qn \u00b7 \u03b8n, (81)\nwhere qpn is a shorthand for Q p(\u03b8\u0302n,\u03b8\u2212n).\nBased on (81), we have (i) if \u03b8\u0302n > \u03b8n, q p n < qn, (ii) if \u03b8\u0302n < \u03b8n, q p n > qn, and (iii) if \u03b8\u0302n = \u03b8n, q p n = qn. Then if \u03b8\u0302n > \u03b8n, the equation (77) is negative and (76) is positive. This inequality also holds for\n\u03b8\u0302n < \u03b8n, by a similar argument. Therefore, the agent n will truthfully report his own cost type.\nStep 3. Incentivize agents to exert precisely the efforts as desired by the principal\nunder COPE\nThen we will show that an agent n, \u2200n \u2208A exerts effort such that qn = qpn would maximize his\nexpected payoff as follows. E{x\u2217,yn,\u03b8\u2212n} [ Ua(x\u2217, qn, yn, \u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [ \u03c0(\u03b8n,\u03b8\u2212n)\u2212K(\u03b8n,\u03b8\u2212n) \u00b7\n1\n1/\u03c320 + qn +S(\u03b8n,\u03b8\u2212n)\u2212\n1 2 \u03b8nq 2 n\n] ,\n(82)\nwhere the expectation is taken with respect to \u03b8\u2212n, x \u2217, and yn.\nIt can be verified that (82) is concave in qn. Hence, by taking the first order derivative of (82)\nwith respect to qn, we have\n\u2202 \u2202qn E [ Ua(x\u2217, qn, yn, \u03b8n,\u03b8\u2212n) ] =\n[ 1/\u03c320 + q p n\n1/\u03c320 + qn\n]2 \u00b7 \u03b8n \u00b7 qpn\u2212 \u03b8n0 \u00b7 qn. (83)\nWe can verify that the value of (83) equals to zero only when qn = q p n. Hence, agent n will exert\nthe effort as the principal desires to maximize his expected payoff. Then (73) is rewritten as\ny\u0302n = \u00b50 \u00b7 1/\u03c320 + yn \u00b7 qpn\n1/\u03c320 + q p n\n. (84)\nBecause the principal knows the value of \u00b50, \u03c3 2 0, and q p n, she can infer the agent n\u2019s truth\nobservation yn from (84).\nStep 4. Maximize the principal\u2019s expected utility under COPE Then we look at the expected payoff of the principal. We will show that the desired effort level Qp(\u03b8n,\u03b8\u2212n) defined in (12) and the function \u03c0(\u03b8n,\u03b8\u2212n) defined in (70) can maximize the principal\u2019s expected payoff and satisfy BIC and BIR condition.\nNotice that when an agent n, \u2200n\u2208A exerts effort such that qn = qpn and reports y\u0302n = \u00b50\u00b71/\u03c320+yn\u00b7q p n\n1/\u03c320+q p n\n,\nthe expected payment function is reduced to\nE{x\u2217,yn,\u03b8\u2212n} [ R(x\u2217, yn, qn, \u03b8n,\u03b8\u2212n) ]\n=E\u03b8\u2212n [ \u03c0(\u03b8n,\u03b8\u2212n)\u2212K(\u03b8n,\u03b8\u2212n)(x\u2217\u2212 y\u0302n)2 +S(\u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [ \u03c0(\u03b8n,\u03b8\u2212n) ] . (85)\nThe expected payoff of the agent n is rewritten as\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212 1 2 \u03b8n \u00b7 [ Qp(\u03b8\u0302n,\u03b8\u2212n) ]2] , (86)\nand the BIC and BIR conditions, i.e., (5) and (6), can be rewritten as\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265E\u03b8\u2212n [ Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] , \u2200\u03b8\u0302n 6= \u03b8n, (87)\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265 0, \u2200\u03b8n. (88)\nBase on Lemma 2 and (85), the expected payoff of the principal is\nEx\u2217,y,\u03b8[U p(x\u2217,qp,y, \u03b8\u0302)] =\u2212Bp ( qp ) \u2212Ex\u2217,y,\u03b8 [\u2211 n\u2208A R(x\u2217, yn, \u03b8n,\u03b8\u2212n) ]\n=\u2212 1 1/\u03c320 + \u2211 n\u2208A q p n \u2212E\u03b8 [\u2211 n\u2208A \u03c0(\u03b8n,\u03b8\u2212n) ] .\nRecall that qpn =Q p(\u03b8n,\u03b8\u2212n), the principal\u2019s optimal problem defined in (7) can be rewritten as\nsup {Qp(\u03b8),\u03c0(\u03b8)},\u2200\u03b8n\u2208\u03b8,\u2200n\u2208A\nE[U p(x\u2217,qp,y, \u03b8\u0302)],\nsubject to : BIC and BIR in (87) and (88).\n(89)\nIn the following lemmas, we characterize an equivalent formulation for the feasible region defined by BIC and BIR. Using these lemmas, we show that Qp(\u03b8n,\u03b8\u2212n) defined in (12) and \u03c0(\u03b8n,\u03b8\u2212n) defined in (70) are the optimal solution that solves the principal\u2019s problem in (89).\nLemma 7. The solution of (89) is feasible if and only if it satisfies the following conditions for all \u03b8n \u2208 [\u03b8, \u03b8\u0304]:\n\u2022 The expected payoff of the agent n, \u2200n\u2208A is\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] = 1\n2 E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n [ Qp(x,\u03b8\u2212n) ]2 dx ] , (90)\n\u2022 Qp(\u03b8n,\u03b8\u2212n) is non-increasing in \u03b8n.\nProof : The proof of Lemma 7 is as follows. We first show that BIC and BIR imply the condition\nin (90).\nNotice that the first derivative of (86) is:\n\u2202E\u03b8\u2212n [ Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )]\n\u2202\u03b8n =E\u03b8\u2212n\n[ \u2212 1\n2\n[ Qp(\u03b8\u0302n,\u03b8\u2212n) ]2]\u2264 0. (91) Then, for any \u03b81n > \u03b8 2 n, we have\nE\u03b8\u2212n [ Ua(\u03c0(\u03b81n,\u03b8\u2212n),Q p(\u03b81n,\u03b8\u2212n), \u03b8 1 n) ] \u2264E\u03b8\u2212n [ Ua(\u03c0(\u03b81n,\u03b8\u2212n),Q p(\u03b81n,\u03b8\u2212n), \u03b8 2 n) ]\n\u2264E\u03b8\u2212n [ Ua(\u03c0(\u03b82n,\u03b8\u2212n),Q p(\u03b82n,\u03b8\u2212n), \u03b8 2 n) ] ,\n(92)\nwhere the first inequality is due to (91) and the second is due to the BIC condition defined in (87).\nRecall that the BIR condition is\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265 0, \u2200\u03b8n \u2208 [\u03b8, \u03b8\u0304], (93)\nwhich implies that, for an agent n \u2208 A with any value \u03b8n \u2208 [\u03b8, \u03b8\u0304], his expected payoff should be nonnegative. Then the expected payoff of the agent n with cost parameter \u03b8\u0304 must be binding at zero. Otherwise, the principal can raise the \u03c0(\u03b8\u0304,\u03b8\u2212n) by a small value of \u03b4 > 0, which does not violate the constraint of (93) but raises the principal\u2019s expected payoff. Hence, we have\nE\u03b8\u2212n [ Ua(\u03c0(\u03b8\u0304,\u03b8\u2212n),Q p(\u03b8\u0304,\u03b8\u2212n), \u03b8\u0304) ] = 0. (94)\nLet Ua(\u03b8n,\u03b8\u2212n) =U a ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n ) . From BIC condition, we have\nE\u03b8\u2212n [ Ua(\u03b8n,\u03b8\u2212n) ] = max\n\u03b8\u0302n\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] .\nBy using the envelope theorem, we have: \u2202E\u03b8\u2212n [ Ua(\u03b8n,\u03b8\u2212n) ] \u2202\u03b8n = \u2202E\u03b8\u2212n [ Ua(\u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n) ] \u2202\u03b8n \u2223\u2223\u2223\u2223\u2223 \u03b8\u0302n=\u03b8n =E\u03b8\u2212n [ \u2212 1 2 [ Qp(\u03b8n,\u03b8\u2212n) ]2] , where \u03b8n is a parameter. By integrating both sides from the value of \u03b8n to \u03b8\u0304 and using (94), we get\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] = 1\n2 E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n [ Qp(x,\u03b8\u2212n) ]2 dx ] (95)\nWe prove thatQp(\u03b8n,\u03b8n) is nonincreasing in \u03b8n by contradiction. Let pn as the shorthand notation\nfor \u03c0(\u03b8n,\u03b8\u2212n). Suppose for any \u03b8 1 n > \u03b8 2 n, we have Q p(\u03b81n,\u03b8\u2212n)>Q p(\u03b82n,\u03b8\u2212n). Because\n\u22022Ua ( pn, q p n, \u03b8n ) \u2202qpn\u2202\u03b8n =\u2212qpn < 0, and (96)\n\u22022Ua ( pn, q p n, \u03b8n ) \u2202qpn 2 =\u2212\u03b8n \u2264 0, (97)\nwe have\n0 = \u2202Ua\n( pn, q p n, \u03b8 1 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b81n,\u03b8\u2212n)\n\u2264 \u2202Ua\n( pn, q p n, \u03b8 1 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n)\n< \u2202Ua\n( pn, q p n, \u03b8 2 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n) ,\nwhere the first equality is due to BIC when the agent n\u2019s cost parameter \u03b8n has the value of \u03b8 1 n, the second equality is due to(96), and the inequality is due to (97).\nHowever, based on the BIC condition, if the agent n\u2019s cost parameter \u03b8n has the value of \u03b8 2 n,\nthen we should have\n\u2202Ua ( pn, q p n, \u03b8 2 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n) = 0,\nwhich holds true for all scalar value of pn. Hence, for any \u03b8 1 n > \u03b8 2 n, Q p(\u03b81n,\u03b8\u2212n)\u2264Qp(\u03b82n,\u03b8\u2212n).\nThen we need to prove that (90) implies BIC and BIR defined in (87) and (88). Notice that we\nhave:\nE\u03b8\u2212n [ Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )]\n=E\u03b8\u2212n [ \u2212 \u222b \u03b8\u0304 \u03b8n \u2202Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz ] =E\u03b8\u2212n [ 1\n2 \u222b \u03b8\u0304 \u03b8\u0302n [ Qp(\u03b7,\u03b8\u2212n) ]2 d\u03b7\u2212 \u222b \u03b8\u0302n \u03b8n \u2202Ua ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz ] =E\u03b8\u2212n [ \u2212 1\n2 \u222b \u03b8n \u03b8\u0304 [ Qp(\u03b7,\u03b8\u2212n) ]2 d\u03b7\u2212 1 2 \u222b \u03b8\u0302n \u03b8n [ Qp(\u03b7,\u03b8\u2212n) ]2 d\u03b7+ 1 2 \u222b \u03b8\u0302n \u03b8n [ Qp(\u03b8\u0302n,\u03b8\u2212n) ]2 dz ] =E\u03b8\u2212n [ Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n ) + 1\n2 \u222b \u03b8\u0302 \u03b8n ([ Qp(\u03b8\u0302n,\u03b8\u2212n) ]2\u2212 [Qp(\u03b7,\u03b8\u2212n)]2)d\u03b7], where the second equality and the forth equality is obtained by (90).\nIf \u03b8\u0302n > \u03b8n, then the above equation is non-positive (because Q p(\u03b7,\u03b8\u2212n) is non-increasing in \u03b7),\nhence\nE\u03b8\u2212n [ Ua(\u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n) ] <E\u03b8\u2212n [ Ua(\u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n) ] .\nThis inequality also holds for \u03b8\u0302n < \u03b8n by a similar argument. Therefore, the two condition imply BIC.\nBIR is verified by putting \u03b8n back to (90). Then based on Lemma 7, we have the following lemma.\nLemma 8. The optimisation problem in (89) has the following equivalent formulation:\nmax {Qp(\u03b8)},\u2200\u03b8n\u2208\u03b8\nE\u03b8 [ \u2212 1\n1/\u03c320 + \u2211 n\u2208AQ p(\u03b8n,\u03b8\u2212n) \u2212 1 2 \u2211 n\u2208A [ Qp(\u03b8n,\u03b8\u2212n) ]2 \u03b8n\u2212 1 2 \u2211 n\u2208A [ Qp(\u03b8n,\u03b8\u2212n) ]2F (\u03b8n) f(\u03b8n) ] ,\ns.t. Qp(\u03b8n,\u03b8\u2212n) is nonincreasing in \u03b8n. (98)\nProof : The proof of Lemma 8 is as follows. The expected payoff of the principal can be written\nas: E\u03b8 [ \u2212 1\n1/\u03c320 + \u2211 n\u2208AQ p(\u03b8n,\u03b8\u2212n) \u2212 1 2 \u2211 n\u2208A [ Qp(\u03b8n,\u03b8\u2212n) ]2 \u00b7 \u03b8n\u2212\u2211 n\u2208A Ua ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )]\n=E\u03b8 [ \u2212 1\n1/\u03c320 + \u2211 n\u2208AQ p(\u03b8n,\u03b8\u2212n) \u2212 1 2 \u2211 n\u2208A [ Qp(\u03b8n,\u03b8\u2212n) ]2\u2212 1 2 \u2211 n\u2208A \u222b \u03b8\u0304 \u03b8n [ Qp(x,\u03b8\u2212n) ]2 dx ] Using integration by parts and Lemma 7, we can get the conclusion.\nBased on Lemma 8, the principal\u2019s problem thus reduces to choosing the desired effort\nQp(\u03b8n,\u03b8\u2212n) for each agent n\u2208A.\nLet qpn =Q p(\u03b8n,\u03b8\u2212n) and\nM ( qp1, . . . , q p N ) =\u2212 1\n1/\u03c320 + \u2211 n\u2208A q p n \u2212 1 2 \u2211 n\u2208A [ qpn ]2 \u00b7 \u03b8n\u2212 1 2 \u2211 n\u2208A [ qpn ]2F (\u03b8n) f(\u03b8n) .\nLet G= [\u22022M/\u2202qpi \u2202q p j ] be the matrix of second order derivatives, and it is a symmetric matrix with negative diagonal terms as\n\u22022M\n\u2202qpi \u2202q p j =\u2212 2[ 1/\u03c320 + \u2211 n\u2208A q p n ]3 , j 6= i, \u22022M \u2202qpi 2 =\u2212 2[ 1/\u03c320 + \u2211 n\u2208A q p n\n]3 \u2212 \u03b8i\u2212 F (\u03b8i)f(\u03b8i) . As we can verify that, for k = 1, . . . ,N , the kth leading principal minors of G alternate in sign, hence G is negative definite and M is strictly concave. Thus, the principal\u2019s desired effort levels from agents qpn =Q p(\u03b8n,\u03b8\u2212n), \u2200n\u2208N are the solution of the below equations:\n1[ 1/\u03c320 + \u2211 n\u2208A q p n ]2 \u2212Qp(\u03b8n,\u03b8\u2212n) \u00b7 \u03b8n\u2212Qp(\u03b8n,\u03b8\u2212n) \u00b7 F (\u03b8n)f(\u03b8n) = 0, n= 1,2, . . . ,N (99)\nUsing Cramer\u2019s rule and the assumption that the function F is log concave in \u03b8 , we can verify\nthat\n\u2202Qp(\u03b8n,\u03b8\u2212n)\n\u2202\u03b8n =\u2212\u2202 2M/\u2202qpn\u2202\u03b8n \u22022M/\u2202qpn 2 \u2264 0, (100)\nwhich shows that Qp(\u03b8n,\u03b8\u2212n) derived from (99) is non-increasing in \u03b8n, so that it is the feasible solution of (98).\nThe solution of (99) is\nQp(\u03b8n,\u03b8\u2212n) = 1\n\u03b8n + F (\u03b8n)\nf(\u03b8n)\n\u00b7 1[ W (\u03b8) ]2 , (101) where the function W : [\u03b8, \u03b8\u0304]N \u2192R+ is the solution of the below equation:\n[ W (\u03b8) ]3\u2212 1 \u03c320 \u00b7 [ W (\u03b8) ]2\u2212\u2211 m\u2208A\n1\n\u03b8m + F (\u03b8m)\nf(\u03b8m)\n= 0. (102)\nThe real root of the above cubic equation is as follows.\nW (\u03b8\u0302) = 1\n3\u03c320 + 3 \u221a\u221a\u221a\u221a 1 27\u03c360 + 1 2 [\u2211 m\u2208A\n1\n\u03b8\u0302m + F (\u03b8\u0302m)\nf(\u03b8\u0302m)\n] + \u221a \u03bb(\u03b8\u0302) + 3 \u221a\u221a\u221a\u221a 1 27\u03c360 + 1 2 [\u2211 m\u2208A\n1\n\u03b8\u0302m + F (\u03b8\u0302m)\nf(\u03b8\u0302m)\n] \u2212 \u221a \u03bb(\u03b8\u0302),\n(103)\nwhere function \u03bb : [\u03b8, \u03b8\u0304]N \u2192R+ is given as\n\u03bb(\u03b8\u0302) = 1\n27\u03c360 [\u2211 m\u2208A\n1\n\u03b8\u0302m + F (\u03b8\u0302m)\nf(\u03b8\u0302m)\n] + 1\n4 [\u2211 m\u2208A\n1\n\u03b8\u0302m + F (\u03b8\u0302m)\nf(\u03b8\u0302m)\n]2 .\nAccording to (90), we have\nE\u03b8\u2212n [ \u03c0(\u03b8n,\u03b8\u2212n)\u2212 1\n2 \u00b7 [Qp(\u03b8n,\u03b8\u2212n)]2 \u00b7 \u03b8n\n] = 1\n2 E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n [ Qp(x,\u03b8\u2212n) ]2 dx ] .\nFrom the above equation, we can derive the optimal payment function as given in (70)."}, {"heading": "C. Proof of Theorem 3: General Setting", "text": "The proof will proceed in four steps. The first three steps show that our mechanism incentivizes the\nagents to be truthful, and the fourth step proves optimality of our mechanism. First, we show that\nirrespective of what an agent reports as his cost parameter, and irrespective of the effort he exerts,\nthe agent is always incentivized to report his true observation. We follow this up and show that\nirrespective of the effort that an agent exerts, he is always incentivized to report his cost parameter\ncorrectly. The third step completes the proof of truthfulness, showing that under truthful reporting\nof the cost parameter and the observation, in our mechanism, an agent is always incentivized to\nexert precisely the effort as desired by the principal. Finally, we show that among all mechanisms\nthat ensure truthful reports, our mechanism maximizes the principal\u2019s expected utility.\nStep 1. Truthful reporting of observation under COPE\nWe first show that the agent will choose\nyn = arg infy\u0302n(yn)E [ `a(x\u2217, y\u0302n) ] . (104)\nto maximize his expected payoff, given his exerting effort qn and own observation yn.\nAs shown in (22), \u03c0(\u03b8\u0302n,\u03b8\u2212n), K(\u03b8\u0302n,\u03b8\u2212n) and S(\u03b8\u0302n,\u03b8\u2212n) are independent of y\u0302n. Moreover, the value ofK(\u03b8\u0302n,\u03b8\u2212n) is always positive. Hence, when the agent n determines his reporting observation strategy to maximize his expected payoff, i.e.,\ny\u0302n \u2208 argmaxE [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212K(\u03b8\u0302n,\u03b8\u2212n) \u00b7 `a(x\u2217, y\u0302n) +S(\u03b8\u0302n,\u03b8\u2212n) ] \u2212C ( qn, \u03b8n ) ,\nwhere the expectation is taken with respect to x\u2217 and cost parameters \u03b8\u2212n = [\u03b81, . . . , \u03b8n\u22121, \u03b8n+1, . . . , \u03b8N ] T of all the agents except agent n, it is equivalent for the agent n to choose the reporting strategy such that\ny\u0302n \u2208 argminEx\u2217 [`a(x\u2217, y\u0302n)]. (105)\nAccording to (18) and (19) in Section 4.2.1, the value of Ex\u2217 [`a(x\u2217, y\u0302n)] is minimized when y\u0302n = yn,\nand the expected value is\nEx\u2217 [`a(x\u2217, y\u0302n)] =Ba ( qn ) .\nStep 2. Truthful reporting of cost parameter under COPE\nWe now show that the agent will truthfully reveals its cost type. The expected payoff of the\nagent whose cost type is \u03b8n0 but reports \u03b8\u0302n0 is: E{x\u2217,yn,\u03b8\u2212n} [ Ua(x\u2217, \u03b8\u0302n, qn, yn, \u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212K(\u03b8\u0302n,\u03b8\u2212n) \u00b7Ba ( qn ) +S(\u03b8\u0302n,\u03b8n)\u2212C(qn, \u03b8n) ] .\n(106)\nFor notational convenience, we define the function Uae :R\u00d7 [\u03b8, \u03b8\u0304]\u00d7R+\u00d7 [\u03b8, \u03b8\u0304]N \u2192R+ as\nUae(\u03b8\u0302n, qn, \u03b8n,\u03b8\u2212n) = [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212K(\u03b8\u0302n,\u03b8\u2212n) \u00b7Ba ( qn ) +S(\u03b8\u0302n,\u03b8n)\u2212C(qn, \u03b8n) ] , (107)\nwhere \u03b8\u2212n are the random variables of all agents cost type except that of agent n. By comparing (106) to (107), the expected payoff of the agent n is\nE{x\u2217,yn,\u03b8\u2212n} [ Ua(x\u2217, \u03b8\u0302n, qn, yn, \u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [ Uae(\u03b8\u0302n, qn, \u03b8n,\u03b8\u2212n) ] .\nThen, by the mean value theorem, we have:\nE\u03b8\u2212n [ Uae(\u03b8n, \u03b8n,\u03b8\u2212n) ] \u2212E\u03b8\u2212n [ Uae(\u03b8\u0302n, \u03b8n,\u03b8\u2212n) ] =E\u03b8\u2212n [ \u2202Uae(\u03b7, \u03b8n,\u03b8\u2212n)\n\u2202\u03b7\n] \u00b7 (\u03b8n\u2212 \u03b8\u0302n)\nWe further have:\nE\u03b8\u2212n\n[ \u2202Uae(\u03b7, \u03b8n,\u03b8\u2212n)\n\u2202\u03b7 ] =E [ Ba(qn)\ndBa(qpn)/dq p n\n\u00b7 ( 1\u2212 B a(qpn)\nBa(qn)\n) \u00b7 ( \u2202c(qpn, \u03b7)\n\u2202\u03b7 \u2212 c(q p n, \u03b7)\ndBa(qpn)/dq p n\n\u00b7 d 2Ba(qpn)\ndqpn 2 \u00b7 \u2202qpn \u2202\u03b7 )] (108) As the agent is selfish, he will exert effort qn to maximize his expected payoff. Hence, the agent\u2019s exerted effort can be obtained by taking the first order derivative of (107) with respect to qn and setting it to zero, which is\ndBa ( qn )\ndqn \u00b7 c(qpn, \u03b8\u0302n) =\ndBa ( qpn )\ndqpn \u00b7 c(qn, \u03b8n) (109)\nAs we assume that dB a(z)\ndz \u2264 0, then based on (109), we have (i) if \u03b8\u0302n > \u03b8n, qpn < qn, (ii) if \u03b8\u0302n < \u03b8n,\nqpn > qn, and (iii) if \u03b8\u0302n = \u03b8n, q p n = qn.\nAs we assume that dBa(qpn)/dq p n \u2264 0, d2Ba(qpn)/dqpn 2 > 0, and later we will prove in Lemma 10 that Qp(\u03b8n,\u03b8\u2212n) is nonincreasing in \u03b8n, if (26) holds, i.e., \u2202c (Q p(\u03b7,\u03b8\u2212n), \u03b7)/\u2202\u03b7 6 0, then the\nabove equation (108) is negative when \u03b8\u0302n > \u03b8n. Based on this, we have E\u03b8\u2212n [ Uae(\u03b8n, \u03b8n,\u03b8\u2212n) ] >\nE\u03b8\u2212n [ Uae(\u03b8\u0302n, \u03b8n,\u03b8\u2212n) ] . This inequality also holds for \u03b8\u0302n < \u03b8n, by a similar argument. Therefore, an agent will truthfully report his own cost parameter.\nStep 3. Incentivize agent to exert precisely the effort as desired by the principal under COPE As we have proved in Step 2 that the agent n would truthfully report his cost type (\u03b8\u0302n = \u03b8n). Next we will show that the agent n exerts effort such that qn = q p n would maximize his expected payoff as follows.\nE [ Uae(x\u2217, qpn, yn, \u03b8\u0302n, \u03b8\u0302\u2212n) ] = \u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n)\u2212K(\u03b8\u0302n) \u00b7Ba ( qn ) +S(\u03b8\u0302n)\u2212C(qn, \u03b8n), (110)\nwhere the expectation is taken with respect to \u03b8\u2212n.\nIt can be verified that (110) is concave in qn as we assume that d2Ba ( qn )\ndqn 2 \u2265 0.\nHence, by taking the first order derivative of (110) with respect to qn, we have\n\u2202 \u2202qn E [ Uae(x\u2217, qpn, yn, \u03b8\u0302n\u03b8\u0302\u2212n) ] =\nc ( qpn, \u03b8\u0302n ) dBa ( qpn ) /dqpn \u00b7 dBa ( qn ) dqn \u2212 c(qn, \u03b8n). (111)\nWe can verify that the value of (111) equals to zero when qn = q p n. Hence, agent n will exert the\neffort as the principal desires to maximize his expected payoff.\nStep 4. Maximize the principal\u2019s expected utility under COPE Then we look at the expected payoff of the principal. To maximize the expected utility for the\nprediction, the principal solves\nmax x\u0302\nE [ \u2212 `p ( x\u2217, x\u0302(y,qp)|(y\u0302n,qp) ] . (112)\nThe principal employs the Bayes estimate x\u0302 as follows:\nx\u0302 ( y\u0302,q\u2217 ) = arg infx\u0302E [ `p ( x\u2217, x\u0302(y\u0302,qp) )] . (113)\nIt follows that the expected utility of the principal is\nBp ( qp ) = inf x\u0302 E [ `p ( x\u2217, x\u0302(y,qp) )] . (114)\nWe then show that the desired effort level Qp(\u03b8\u0302n, \u03b8\u0302\u2212n) defined as the solution of (125) and the function \u03c0(\u03b8\u0302n, \u03b8\u0302\u2212n) defined in (23) can maximize the principal\u2019s expected payoff and satisfy BIC and BIR conditions.\nNotice that agent n exerts effort such that qn = q p n and reports yn \u2208 argminEx\u2217 [`a(x\u2217, y\u0302n)], the\nexpected payment function defined in (22) is reduced to\nE[R(x\u2217, yn, \u03b8n,\u03b8\u2212n)] = \u03c0(\u03b8n,\u03b8\u2212n),\nwhere the expectation is taken with respect to x\u2217 and yn.\nThen the expected payoff of agent n is rewritten as\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [ \u03c0(\u03b8\u0302n,\u03b8\u2212n)\u2212C ( Qp(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] , (115)\nand the BIC and BIR conditions, i.e., (5) and (6) can be rewritten as\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265E\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] \u2200\u03b8\u0302n 6= \u03b8n, (116) E\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265 0, \u2200\u03b8n. (117)\nThen the expected payoff of the principal is\nE[U p(x\u2217,qp,y,\u03b8)] =\u2212Bp ( qp ) \u2212 \u2211 n\u2208A \u03c0(\u03b8n,\u03b8\u2212n)\nwhere the expectation is taken with respect to x\u2217 and y.\nRecall that qpn =Q p(\u03b8n,\u03b8\u2212n), we then rewrite (7) as\nsup {Qp(\u03b8),\u03c0(\u03b8)},\u2200\u03b8n\u2208\u03b8\nE[U p(x\u2217,qp,y, \u03b8\u0302)],\nsubject to : BIC and BIR in (116) and (117).\n(118)\nFor the feasible region defined by BIC and BIR, we can characterise an equivalent formulation\nin the following lemma:\nLemma 9. The solution of (118) is feasible if and only if it satisfies the following conditions for all \u03b8n \u2208 [\u03b8, \u03b8\u0304]:\n\u2022 The expected payoff of agent n is\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [\u222b \u03b8\u0304\n\u03b8n\n\u2202C ( Qp(z,\u03b8\u2212n), \u03b7 ) \u2202\u03b7 dz ] , (119)\n\u2022 Qp(\u03b8n,\u03b8\u2212n) is non-increasing in \u03b8n.\nProof : We first show that BIC and BIR imply the condition 119. The first derivative of (115) is\n\u2202E\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0302,\u03b8\u2212n),Q p(\u03b8\u0302,\u03b8\u2212n), \u03b7) ]\n\u2202\u03b7 =E\u03b8\u2212n\n[ \u2212 \u2202C(Q p(\u03b8\u0302,\u03b8\u2212n), \u03b7)\n\u2202\u03b7\n] \u2264 0. (120)\nThen, for any \u03b81n > \u03b8 2 n, we have E\u03b8\u2212n [ Uae(\u03c0(\u03b81n,\u03b8\u2212n),Q p(\u03b81n,\u03b8\u2212n), \u03b8 1 n) ] \u2264E\u03b8\u2212n [ Uae(\u03c0(\u03b81n,\u03b8\u2212n),Q p(\u03b81n,\u03b8\u2212n), \u03b8 2 n) ]\n\u2264E\u03b8\u2212n [ Uae(\u03c0(\u03b82n,\u03b8\u2212n),Q p(\u03b82n,\u03b8\u2212n), \u03b8 2 n) ] ,\n(121)\nwhere the first inequality is due to (120) and the second is from the BIC condition defined in (116).\nRecall that the BIR condition is\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] \u2265 0, \u2200\u03b8n \u2208 [\u03b8, \u03b8\u0304], (122)\nwhich implies that, for the agent n \u2208 A with any value \u03b8n \u2208 [\u03b8, \u03b8\u0304], his expected payoff should be nonnegative. Then the expected payoff of the agent n with cost parameter \u03b8\u0304 must be binding at zero. Otherwise, the principal can reduce the \u03c0(\u03b8\u0304,\u03b8\u2212n) by a small value of \u03b4 > 0, which does not violate the constraint of (122) but raises the principal\u2019s expected payoff. Hence, we have\nE\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0304,\u03b8\u2212n),Q p(\u03b8\u0304,\u03b8\u2212n), \u03b8\u0304) ] = 0. (123)\nLet Uae(\u03b8n,\u03b8\u2212n) =U ae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n ) . From BIC condition, we have\nE\u03b8\u2212n [ Uae(\u03b8n,\u03b8\u2212n) ] = max\n\u03b8\u0302n\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )] .\nBy using the envelope theorem, we have: \u2202E\u03b8\u2212n [ Uae(\u03b8n,\u03b8\u2212n) ] \u2202\u03b7 = \u2202E\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n) ]\n\u2202\u03b8n\n\u2223\u2223\u2223\u2223\u2223 \u03b8\u0302n=\u03b8n\n=E\u03b8\u2212n\n[ \u2212 \u2202C(Q p(\u03b8n,\u03b8\u2212n), \u03b8n)\n\u2202\u03b8n\n] ,\nwhere \u03b8n is a parameter. By integrating both sides from \u03b8n to \u03b8\u0304 and using (123) and the assumption that the random variable \u03b8n of the agent n is independent for every n\u2208A, we get\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n \u2202C(Qp(z,\u03b8\u2212n), \u03b7) \u2202\u03b7 dz ] . (124)\nWe prove thatQp(\u03b8n,\u03b8n) is nonincreasing in \u03b8n by contradiction. Let pn as the shorthand notation\nfor \u03c0(\u03b8n,\u03b8\u2212n). Suppose for any \u03b8 1 n > \u03b8 2 n, we have Q p(\u03b81n,\u03b8\u2212n)>Q p(\u03b82n,\u03b8\u2212n). Because\n\u22022Uae ( pn, q p n, \u03b8n ) \u2202qpn\u2202\u03b8n =\u2212\u2202c(q, \u03b8n) \u2202\u03b8n < 0, and\n\u22022Uae ( pn, q p n, \u03b8n ) \u2202qpn 2 =\u2212 \u2202c(q, \u03b8n) \u2202q < 0,\nwe then have\n0 = \u2202Uae\n( pn, q p n, \u03b8 1 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b81n,\u03b8\u2212n)\n< \u2202Uae\n( pn, q p n, \u03b8 1 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n)\n< \u2202Uae\n( pn, q p n, \u03b8 2 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n) ,\nwhere the equality is due to BIC when the agent n\u2019s cost parameter \u03b8n has the value of \u03b8 1 n, the second equality is due to (C), and the inequality is due to (C).\nHowever, based on the BIC condition, if the agent n\u2019s cost parameter \u03b8n has the value of \u03b8 2 n,\nthen we should have\n\u2202Uae ( pn, q p n, \u03b8 2 n ) \u2202qpn \u2223\u2223\u2223\u2223\u2223 qpn=Q p(\u03b82n,\u03b8\u2212n) = 0,\nwhich holds true for all scalar values of pn. Hence, for any \u03b8 1 n > \u03b8 2 n, Q p(\u03b81n,\u03b8\u2212n)\u2264Qp(\u03b82n,\u03b8\u2212n).\nThen we need to prove that ((119)) implies BIC and BIR defined in ((116)) and ((117)).\nBIR is verified by putting \u03b8n back to (119). Besides, by putting \u03b8n = \u03b8\u0304 back to (119), we have\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0304,\u03b8\u2212n),Q p(\u03b8\u0304,\u03b8\u2212n), \u03b8\u0304 )] = 0.\nThen we prove that ((119)) implies BIC. Notice that we have:\nE\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n )]\n1 =E\u03b8\u2212n [ \u2212 \u222b \u03b8\u0304 \u03b8n \u2202Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz ] 2 =E\u03b8\u2212n [ Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8\u0302n ) \u2212 \u222b \u03b8\u0302n \u03b8n \u2202Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz\n] 3 =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8\u0302n \u2202C(Qp(\u03b7,\u03b8\u2212n), z) \u2202z d\u03b7\u2212 \u222b \u03b8\u0302n \u03b8n \u2202Uae ( \u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), z ) \u2202z dz\n] 4 =E\u03b8\u2212n [ \u2212 \u222b \u03b8n \u03b8\u0304 \u2202C(Qp(\u03b7,\u03b8\u2212n), z) \u2202z d\u03b7\u2212 \u222b \u03b8\u0302n \u03b8n \u2202C(Qp(\u03b7,\u03b8\u2212n), z) \u2202z d\u03b7+ \u222b \u03b8\u0302n \u03b8n \u2202C(Qp(\u03b8\u0302n,\u03b8\u2212n), z) \u2202z dz\n] 5 =E\u03b8\u2212n [ Uae ( \u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n ) + \u222b \u03b8\u0302 \u03b8n ( \u2202C(Qp(\u03b8\u0302n,\u03b8\u2212n), z) \u2202z \u2212 \u2202C(Q p(\u03b7,\u03b8\u2212n), z) \u2202z ) d\u03b7 ] ,\nwhere the third equality and the fifth equality is obtained by (119).\nIf \u03b8\u0302n > \u03b8n, then the above equation is non-positive. This is because Q p(\u03b7,\u03b8\u2212n) is non-increasing\nin \u03b7 and \u2202C(q, \u03b8)/\u2202\u03b8 > 0. Hence,\nE\u03b8\u2212n [ Uae(\u03c0(\u03b8\u0302n,\u03b8\u2212n),Q p(\u03b8\u0302n,\u03b8\u2212n), \u03b8n) ] <E\u03b8\u2212n [ Uae(\u03c0(\u03b8n,\u03b8\u2212n),Q p(\u03b8n,\u03b8\u2212n), \u03b8n) ] .\nThis inequality also holds for \u03b8\u0302n < \u03b8n by a similar argument. Therefore, the two condition imply BIC.\nThen based on Lemma 9 and let qpn =Q p(\u03b8n,\u03b8\u2212n), we have the following Lemma.\nLemma 10. The optimisation problem in (118) has the following equivalent formulation:\nmax qp\nE\u03b8 [ \u2212Bp ( qp ) \u2212 \u2211 n\u2208A C ( qpn, \u03b8n ) \u2212 \u2211 n\u2208A \u2202C ( qpn, \u03b8n ) \u2202\u03b8n \u00b7 F (\u03b8n) f(\u03b8n) ] s.t. qpn is nonincreasing in \u03b8n,\u2200n\u2208A. (125)\nwhere the expectation is taken with respect to \u03b8.\nproof : The proof of Lemma 4 is as follows. The expected payoff of the principal can be written\nas:\nE\u03b8 [ \u2212Bp ( qp ) \u2212 \u2211 n\u2208A C ( qpn, \u03b8n ) \u2212 \u2211 n\u2208A Ua ( \u03c0(\u03b8n,\u03b8\u2212n), q p n, \u03b8n )]\n=E\u03b8 [ \u2212Bp ( qp ) \u2212 \u2211 n\u2208A C ( qpn, \u03b8n ) \u2212 \u2211 n\u2208A \u222b \u03b8\u0304 \u03b8n \u2202C ( qpn, \u03b7 ) \u2202\u03b7 dx ] ,\n(126)\nwhere the expectation is taken with respect to \u03b8. Notice that E\u03b8n [\u222b \u03b8\u0304\n\u03b8n\n\u2202C ( qpn, \u03b7 ) \u2202\u03b7 dx ] = \u222b \u03b8\u0304 \u03b8 \u222b \u03b8\u0304 z \u2202C ( qpn, \u03b7 ) \u2202\u03b7 dx \u00b7 f(z)dz = \u222b \u03b8\u0304 \u03b8 F (z) \u2202C ( qpn, \u03b7 ) \u2202\u03b7 dz\n= \u222b \u03b8\u0304 \u03b8 F (z) f(z) \u2202C ( qpn, \u03b7 ) \u2202\u03b7 f(z)dz =E\u03b8n [ F (\u03b8n) f(\u03b8n) \u2202C ( qpn, \u03b7 ) \u2202\u03b7 ] ,\nwhere the first equation is obtained by using integration by parts. Then by applying the above equation to (126) and the fact that {\u03b8n}n\u2208A are assumed to be random, independently and identically distributed on support [\u03b8, \u03b8\u0304], we can get the conclusion.\nBased on Lemma 10, the principal\u2019s problem thus reduces to choosing the desired effort qpn = Qp(\u03b8n,\u03b8\u2212n) for each agent n\u2208A. We first consider the problem in (125) without the constraint. If the optimal solution to this unconstrained problem is increasing, then it is also an optimal solution to the constrained problem.\nLet qpn =Q p(\u03b8n,\u03b8\u2212n) and\nM ( qp1, . . . , q p N ) =\u2212Bp ( qp ) \u2212 \u2211 n\u2208A C ( qpn, \u03b8n ) \u2212 \u2211 n\u2208A\n\u2202C ( qpn, \u03b8n ) \u2202\u03b8n \u00b7 F (\u03b8n) f(\u03b8n) .\nAs we assume that \u22022Bp(qp)/\u2202qpi \u2202q p j > 0,\u2200j 6= i, \u2202C(qpn, \u03b8n)/\u2202qpn > 0 and \u22022C(qpn, \u03b8n)/\u2202qpn\u2202\u03b8n \u2265 0,\nwe can check that\n\u22022M\n\u2202qpi \u2202q p j\n=\u2212 \u22022Bp\n( qp )\n\u2202qpi \u2202q p j\n\u2264 0, j 6= i,\n\u22022M \u2202qpi 2 =\u2212\n\u22022Bp ( qp )\n\u2202qpi 2 \u2212 c(q p i , \u03b8i)\u2212\n\u2202c(qpi , \u03b8i) \u2202\u03b8i \u00b7 F (\u03b8i) f(\u03b8i) \u2264 0. (127)\nLet G = [\u22022M/\u2202qpi \u2202q p j ], i, j = 1, . . . ,N , be the matrix of second order derivatives. Matrix G is symmetric with negative diagonal terms as shown in (127).\nAs we can verify that, for k = 1, . . . ,N , the kth leading principal minors of G alternate in sign, so that G is negative definite and W is strictly concave. The computational complexity of finding the optimal solution of (125) will depend on the specific structure of the functions.\nUsing Cramer\u2019s rule, the assumption that the c.d.f. function F is log concave in \u03b8, and the\nassumption that \u2202C(qpi , \u03b8i)/\u2202q p i > 0 and \u2202 2c(qpi , \u03b8i)/\u2202q p i \u2202\u03b8i \u2265 0, we can verify that\n\u2202Qp(\u03b8n,\u03b8\u2212n)\n\u2202\u03b8n =\u2212\u2202 2M/\u2202qpn\u2202\u03b8n \u22022M/\u2202qpn 2 \u2264 0,\u2200n\u2208A, (128)\nwhich shows that Qp(\u03b8n,\u03b8\u2212n) derived by solving (125) is nonincreasing in \u03b8n, so that it is a feasible solution of (125).\nAccording to (119), we have\nE\u03b8\u2212n [ \u03c0(\u03b8n,\u03b8\u2212n)\u2212C ( Qp(\u03b8n,\u03b8\u2212n), \u03b8n )] =E\u03b8\u2212n [\u222b \u03b8\u0304 \u03b8n \u2202C ( Qp(z,\u03b8\u2212n), \u03b7 ) \u2202\u03b7 dz ] .\nFrom the above equation, we can derive the optimal payment function as given in (23).\nEndnotes\n1. We want to emphasize that whether the Bayes risk exists is a open problem. Scharf (1991),\nFigueiredo (2004) consider some special case such as Gaussian distribution. Characterizing the\ngeneral condition for the existence and uniqueness of Bayes risk will be interesting future work.\n2. To the best of our knowledge, this is the first paper that studies how to incentivize all agents to\nreport their truthful estimations and exert appropriate amounts of effort based on their respective\ncapabilities during a prediction process. Hence, we have not found an algorithm in the existing\nliterature as a fair benchmark to compare with the COPE performance."}], "references": [{"title": "Low-cost learning via active data", "author": ["Abernethy", "Jacob", "Yiling Chen", "Chien-Ju Ho", "Bo Waggoner"], "venue": null, "citeRegEx": "Abernethy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2015}, {"title": "Statistical decision theory and Bayesian analysis", "author": ["Berger", "James O"], "venue": null, "citeRegEx": "Berger and O.,? \\Q2013\\E", "shortCiteRegEx": "Berger and O.", "year": 2013}, {"title": "The economics of contracts: theories and applications", "author": ["Brousseau", "Eric", "Jean-Michel"], "venue": "Glachant", "citeRegEx": "Brousseau et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brousseau et al\\.", "year": 2002}, {"title": "Optimum statistical estimation with", "author": ["Press) Cai", "Yang", "Constantinos Daskalakis", "Christos H Papadimitriou"], "venue": null, "citeRegEx": "Cai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2015}, {"title": "Sourcing through auctions and audits", "author": ["Chen", "Ying-Ju", "Sridhar Seshadri", "Eitan Zemel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "A Kash (2015) Elicitation for aggregation", "author": ["Rafael M", "Yiling Chen", "Ian"], "venue": "Proceedings of the 29th", "citeRegEx": "Frongillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Frongillo et al\\.", "year": 2015}, {"title": "Decision support systems", "author": ["Hayes", "Bob E"], "venue": null, "citeRegEx": "Hayes and E.,? \\Q1998\\E", "shortCiteRegEx": "Hayes and E.", "year": 1998}, {"title": "Linear regression as a non-cooperative game", "author": ["Ioannidis", "Stratis", "Patrick Loiseau"], "venue": "International Journal of Quality & Reliability Management", "citeRegEx": "Ioannidis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ioannidis et al\\.", "year": 2013}, {"title": "Budget-optimal task allocation for reliable", "author": ["Economics. Springer", "277\u2013290. Karger", "David R", "Sewoong Oh", "Devavrat Shah"], "venue": null, "citeRegEx": "Springer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2014}, {"title": "Parametric Prediction from Parametric Agents", "author": ["Lehmann", "Erich Leo", "George Casella"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1998}, {"title": "Incentive compatibility and the bargaining problem", "author": ["Myerson", "Roger B"], "venue": "method. Management Science", "citeRegEx": "Myerson and B.,? \\Q1979\\E", "shortCiteRegEx": "Myerson and B.", "year": 1979}, {"title": "Statistical signal processing(Addison-Wesley, Reading Massachusetts)", "author": ["1007\u20131017. Scharf", "Louis L"], "venue": null, "citeRegEx": "Scharf and L.,? \\Q1991\\E", "shortCiteRegEx": "Scharf and L.", "year": 1991}, {"title": "Approval voting and incentives in crowdsourcing", "author": ["Shah", "Nihar B", "Dengyong Zhou", "Yuval Peres"], "venue": null, "citeRegEx": "Shah et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": ", Frongillo et al. (2015), Abernethy et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)).", "startOffset": 8, "endOffset": 32}, {"referenceID": 12, "context": "(2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)).", "startOffset": 23, "endOffset": 42}, {"referenceID": 12, "context": "(2005), Prelec (2004), Shah et al. (2015), Shah and Zhou (2015)).", "startOffset": 23, "endOffset": 64}, {"referenceID": 3, "context": "(2007), Ioannidis and Loiseau (2013), Frongillo et al. (2015), Cai et al.", "startOffset": 38, "endOffset": 62}, {"referenceID": 2, "context": "(2015), Cai et al. (2015), Abernethy et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)) address problems in this space.", "startOffset": 8, "endOffset": 32}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type.", "startOffset": 8, "endOffset": 246}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium.", "startOffset": 8, "endOffset": 545}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents\u2019 confidences, again without considering the costs that may be incurred by the agents.", "startOffset": 8, "endOffset": 688}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents\u2019 confidences, again without considering the costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the agents cannot fabricate their observation, but may lie about their costs, and design a mechanism to ensure that agents truthfully report their costs.", "startOffset": 8, "endOffset": 865}, {"referenceID": 0, "context": "(2015), Abernethy et al. (2015)) address problems in this space. However, the models assumed in these works are different, and generally more restrictive than the models considered in our paper in many respects. In particular, Fang et al. (2007) propose a betting mechanism for eliciting the observation and the quality of each agent, under the assumption that the agents are homogeneous with the same cost type. In contrast, we consider the more general and realistic setting where agents can have different types. Ioannidis and Loiseau (2013) formulate the noise addition as a non-cooperative game and prove the existence and uniqueness of the Nash equilibrium. Frongillo et al. (2015) study how a principal can make predictions by eliciting the agents\u2019 confidences, again without considering the costs that may be incurred by the agents. Abernethy et al. (2015) consider a model where the agents cannot fabricate their observation, but may lie about their costs, and design a mechanism to ensure that agents truthfully report their costs. In contrast, we assume a more general scenario where the agents can be strategic in choosing and reporting their respective observations. Cai et al. (2015) propose a monetary mechanism to collect data and to perform an estimation of a function at one random point.", "startOffset": 8, "endOffset": 1198}, {"referenceID": 3, "context": "(2007), Cai et al. (2015)), i.", "startOffset": 8, "endOffset": 26}, {"referenceID": 3, "context": "(2007), Cai et al. (2015)), i.e., yn \u223cN (x\u2217, 1 qn ), where yn follows from Gaussian distribution with mean x\u2217 and variance 1/qn. Conditioned on x \u2217, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,\u2200n \u2208A) be the observation vector and q= (qn,\u2200n\u2208A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others. It is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent\u2019s costtype parameter \u03b8n. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of \u03b8n implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+\u00d7R+\u2192R+, and assume that the cost incurred by an agent n\u2208A with the cost type \u03b8n when exerting an effort qn is C(qn, \u03b8n). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2. The cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed on support [\u03b8, \u03b8\u0304] for some 0\u2264 \u03b8 < \u03b8\u0304 <\u221e. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [\u03b8, \u03b8\u0304]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.", "startOffset": 8, "endOffset": 1635}, {"referenceID": 3, "context": "(2007), Cai et al. (2015)), i.e., yn \u223cN (x\u2217, 1 qn ), where yn follows from Gaussian distribution with mean x\u2217 and variance 1/qn. Conditioned on x \u2217, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,\u2200n \u2208A) be the observation vector and q= (qn,\u2200n\u2208A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others. It is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent\u2019s costtype parameter \u03b8n. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of \u03b8n implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+\u00d7R+\u2192R+, and assume that the cost incurred by an agent n\u2208A with the cost type \u03b8n when exerting an effort qn is C(qn, \u03b8n). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2. The cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed on support [\u03b8, \u03b8\u0304] for some 0\u2264 \u03b8 < \u03b8\u0304 <\u221e. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [\u03b8, \u03b8\u0304]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.", "startOffset": 8, "endOffset": 1657}, {"referenceID": 3, "context": "(2007), Cai et al. (2015)), i.e., yn \u223cN (x\u2217, 1 qn ), where yn follows from Gaussian distribution with mean x\u2217 and variance 1/qn. Conditioned on x \u2217, the observations of the agents are assumed to be mutually independent. As a shorthand, we let y = (yn,\u2200n \u2208A) be the observation vector and q= (qn,\u2200n\u2208A) be the effort vector. We further assume that agents do not collude with each other, as each agent submits his observation to the crowdsourcing platform anonymous and does not know the identities of others. It is costly for each agent to exert a high level effort when making an observation, and this cost not only depends on the effort chosen by the agent, but also is affected by the agent\u2019s costtype parameter \u03b8n. The cost types of different agents are allowed to be different, capturing the heterogeneity of the agents. A smaller value of \u03b8n implies a higher capability of agent n. Specifically, we consider a publicly known cost function C : R+\u00d7R+\u2192R+, and assume that the cost incurred by an agent n\u2208A with the cost type \u03b8n when exerting an effort qn is C(qn, \u03b8n). We will study two types of cost functions, i.e., the linear and quadratic cost functions in Section 4.1, and generalize the results to general cost functions in Section 4.2. The cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed on support [\u03b8, \u03b8\u0304] for some 0\u2264 \u03b8 < \u03b8\u0304 <\u221e. This distribution is assumed to be public knowledge. In this paper we focus on the case where the distribution is uniform on the interval [\u03b8, \u03b8\u0304]. Uniform assumption has been frequently used in the past incentive mechanism design (e.g., Fang et al. (2007), Sheng and Liu (2013), Koutsopoulos (2013)), and our analysis also holds for the general class of log-concave distributions such as exponential distribution and normal distribution.", "startOffset": 8, "endOffset": 1678}, {"referenceID": 4, "context": ", Che (1993), Chen et al. (2008) and references therein).", "startOffset": 14, "endOffset": 33}, {"referenceID": 4, "context": ", Che (1993), Chen et al. (2008) and references therein). The cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [\u03b8, \u03b8\u0304]\u2192R+ and probability density function f : [\u03b8, \u03b8\u0304]\u2192 R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, differentiable, and log concave in [\u03b8, \u03b8\u0304]. This is a regularity condition often assumed in auction contexts (see, e.g., Myerson (1981)).", "startOffset": 14, "endOffset": 507}, {"referenceID": 4, "context": ", Che (1993), Chen et al. (2008) and references therein). The cost types {\u03b8n}n\u2208A are assumed to be randomly, independently and identically distributed with a cumulative distribution function F : [\u03b8, \u03b8\u0304]\u2192R+ and probability density function f : [\u03b8, \u03b8\u0304]\u2192 R+. The functions F and f are public knowledge to all agents. We also assume that the c.d.f. function F is continuous, differentiable, and log concave in [\u03b8, \u03b8\u0304]. This is a regularity condition often assumed in auction contexts (see, e.g., Myerson (1981)). This assumption is satisfied by a wide range of distributions, such as the uniform, gamma, and beta distributions. See Rosling (2002) for an extensive discussion on log concave probability distributions.", "startOffset": 14, "endOffset": 643}], "year": 2016, "abstractText": "Yuan Luo Department of Information Engineering, The Chinese University of Hong Kong, yluo@ie.cuhk.edu.hk, Nihar B. Shah Department of Electrical Engineering and Computer Sciences, UC Berkeley, nihar@eecs.berkeley.edu Jianwei Huang Department of Information Engineering, The Chinese University of Hong Kong, jwhuang@ie.cuhk.edu.hk Jean Walrand Department of Electrical Engineering and Computer Sciences, UC Berkeley, walrand@berkeley.edu", "creator": "LaTeX with hyperref package"}}}