{"id": "1602.07570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "abstract": "nowadays we consider a ubiquitous scenario in launching the broad internet economy when individual decision - makers ( henceforth, software agents ) both produce and consume information as they make strategic choices in an uncertain alternative environment. this creates a three - way tradeoff between exploration ( primarily trying out insufficiently already explored alternatives to help others in the future ), exploitation ( making optimal feasible decisions even given the information discovered by other agents ), policy and incentives of the agents ( who are myopically widely interested in exploitation, while preferring the others to explore ). we posit a principal who controls the flow of information from agents alone that has came before, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. the goal is to first design a recommendation, policy for the principal market which respects agents'incentives and minimizes a suitable notion of regret.", "histories": [["v1", "Wed, 24 Feb 2016 15:57:28 GMT  (169kb)", "http://arxiv.org/abs/1602.07570v1", null], ["v2", "Wed, 16 Nov 2016 00:53:19 GMT  (199kb)", "http://arxiv.org/abs/1602.07570v2", null]], "reviews": [], "SUBJECTS": "cs.GT cs.DS cs.LG", "authors": ["yishay mansour", "aleksandrs slivkins", "vasilis syrgkanis", "zhiwei steven wu"], "accepted": false, "id": "1602.07570"}, "pdf": {"name": "1602.07570.pdf", "metadata": {"source": "CRF", "title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "authors": ["Yishay Mansour", "Aleksandrs Slivkins", "Vasilis Syrgkanis", "Zhiwei Steven Wu"], "emails": ["mansour@microsoft.com", "slivkins@microsoft.com", "vasy@microsoft.com", "wuzhiwei@cis.upenn.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n07 57\n0v 1\n[ cs\n.G T\n] 2\n4 Fe\nWe consider a ubiquitous scenario in the Internet economywhen individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before to the ones that arrive later, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents\u2019 incentives and minimizes a suitable notion of regret.\nWe extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game, receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment.\nOur main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (where the constant depends on the prior distribution, but not on the time horizon), and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions, the actions which some incentive-compatible policy can recommend with non-zero probability. We show how the principal can identify (and explore) all explorable actions, and use the revealed information to perform optimally. In particular, our results significantly improve over the prior work on the special case of a single agent per round, which relies on assumptions to guarantee that all actions are explorable. Interestingly, we do not require the principal\u2019s utility to be aligned with the cumulative utility of the agents; instead, the principal can optimize an arbitrary notion of per-round reward.\n\u2217Microsoft Research and Tel Aviv University. mansour@microsoft.com \u2020Microsoft Research, New York, NY, USA. slivkins@microsoft.com \u2021Microsoft Research, New York, NY, USA. vasy@microsoft.com \u00a7University of Pennsylvania, Philadelphia, PA, USA. wuzhiwei@cis.upenn.edu"}, {"heading": "1 Introduction", "text": "A common phenomenon of the Internet economy is that individual decision-makers (henceforth, agents) both produce and consume information as they make strategic decisions in an uncertain environment. Agents produce information through their selection of actions and the resulting outcomes.1 Agents consume information from other agents who made similar choices in the past, when and if such information is available, in order to optimize their utilities. The collection and dissemination of information relevant to agents\u2019 decisions can be instrumented on a very large scale. Numerous online services do it to provide recommendations concerning various products, services and experiences: movies to watch, products to buy, restaurants to dine in, and so forth.\nThe main issue of interest for this work is that the agents tend to be myopic, optimizing their own immediate reward, whereas the society would benefit if they also explore new or insufficiently explored alternatives. While the tension between acquisition and usage of information is extremely well-studied (under the name of exploration-exploitation tradeoff ), a crucial new dimension here is the incentives of the agents: since the agents are self-interested, one cannot expect agents to explore only because they are asked to do so. This creates a new intriguing three-way tradeoff between exploration, exploitation and incentives. To study this problem, we introduce a principal, who abstracts the society or a recommendation system, and whose goal is optimize some social objective function such as the social welfare. The principal can control the flow of information from past experiences to the agents, but is not allowed to use monetary transfers. Absent any new information, an agent will only perform the a priori better action, resulting in no exploration. Full transparency \u2014 revealing to an agent all information currently known by the principal \u2014 is not a good solution, either, because then an agent would only exploit. The goal is to understand how the principal can induce sufficient exploration for achieving near-optimal outcomes.\nThe prior work on this problem (Kremer et al., 2014; Mansour et al., 2015) has a crucial limitation: given the principal\u2019s recommendation, the utility of a given agent is assumed to be unaffected by the choices of other agents. This is often not the case in practice. We lift this restriction, and allow the agents to affect one another. Informally, we posit that the agents operate in a shared environment, and an agent\u2019s decision can affect this environment for a limited period of time after the decision is made.\nMotivating example. Let us consider a motivating example based on traffic routing. Consider a GPS-based navigation application such asWaze that gives each user a recommended driving route based on the current traffic conditions in the road network (consuming information received from other drivers), and uses his GPS signal to monitor his progress and the traffic conditions along the route (producing information for future recommendations). A natural goal for the navigation system (the \u201cprincipal\u201d in this example) is to minimize the average delay experienced by the users.\nIf the application is used by only a few drivers, we can view each user in isolation. However, once the application serves a substantial fraction of drivers in the region, there is a new aspect: recommendations may impact the traffic conditions experienced by other drivers. For example, if the application suggests to all the users in a given region to move to a certain lightly loaded route, then when/if they all follow the recommendation, the route may become highly congested. The uncertainty on how congestion affects delays on various routes can be reduced via exploration.\n1This information can be collected explicitly, e.g., as reviews on products, or implicitly, e.g., by observing the routes chosen by a driver and the associated driving times via a GPS-enabled device.\nA natural simplified model for this scenario is that at each round, multiple drivers arrive and interact through a routing game on fixed road network. In this game, the agents simultaneously choose their routes, and the delay at each link is determined by the load: the number of routes that use this link. Typically one assumes a parameterized Bayesian model, where the delay at a particular link is a known function of the load and a vector of parameters. The parameter vector is initially not known, but comes from a known Bayesian prior. Thus, we have a Bayesian game among the drivers. Furthermore, there is a principal that recommends routes to drivers: before each round, it recommends a route to each driver that arrives in this round, and observes the associated delays. Since the drivers are not obliged to follow the recommended routes, the recommendations must be \u201ccompatible\u201d with drivers\u2019 incentives, in the sense that they must form a correlated equilibrium in the routing game. At each time-step, the principal has a two-pronged goal: to minimize the driving times (exploit) and also to obtain information about the unknown parameters (explore) for the sake of giving better recommendations in the future. The challenge is to find an optimal balance between exploration and exploitation under the constraint that even the exploration must be compatible with drivers\u2019 incentives.\nOur contributions. We put forward a model, called Bayesian Exploration, which has the following ingredients. Initially, some realized state of nature \u03b80 is selected from a known prior distribution (and never changes). There are T rounds. In each round, a new set of n agents arrive and play a game (the same game in all rounds): each agent selects an action, and receives utility determined by the joint action and the state \u03b80. There is a single principal which, in each round, recommends an action to each participating agent, and observes the chosen actions and the resulting utilities. The recommendations must be Bayesian incentive-compatible (BIC), so that the agents are interested in following them. The principal has a reward function, also determined by the joint action and the state \u03b80. The goal of the principal is to maximize her cumulative reward over all rounds, in expectation over the prior (henceforth, expected reward).\nWe design policies for the principal that are near-optimal compared to the best-in-hindsight policy: a BIC policy which maximizes the expected reward for a particular problem instance. More formally, we strive to minimize a version of Bayesian regret: the difference between the expected reward of the best-in-hindsight policy and the reward of the principal. Following the literature on regret minimization, we are mainly interested in the asymptotic dependence of regret on the time horizon T . We consider two versions, depending on whether the utilities are deterministic or stochastic. For both versions, we achieve optimal asymptotic dependence on T : constant for deterministic utilities, and logarithmic for stochastic utilities (with a slightly altered definition of the best-in-hindsight policy). In fact, this dependence is known to be optimal even for policies not bound by BIC constraints. The asymptotic constants depend on the Bayesian prior, which is inevitable because of the BIC constraints. Our policies are computationally efficient: their running time per round is polynomial in the input size.\nThe major new contribution of our work over (Kremer et al., 2014; Mansour et al., 2015) is the introduction of multiple agents at each round, and considering an interaction through an arbitrary Bayesian game. This additional interaction allows us to abstract a new host of scenarios, and introduces a new level of complexity to the framework. In particular, the principal now has a variety of joint actions it can recommend, and in many cases it has to randomize its recommendation in order maintain incentive-compatibility. (Technically, in each round the principal needs to select a Bayes correlated equilibrium (Bergemann and Morris, 2013), whereas with a single agent in each round it suffices to select a single action, which is a much simpler object.)\nAnother important feature of our model is that the utilities of the principal may be unaligned with the (cumulative) utility of all agents. For example, the principal may be interested in some form of fairness, such as maximizing the minimum utility in each round. Such objective is very different from the sum of all utilities. At the extreme, the principal might want to minimize the cumulative utility. Interestingly, while the BIC constraint generally limits the principal\u2019s ability to harm the agents, the principal may still be able to significantly lower the Bayesian-expected social welfare, compared to the worst Bayes-Nash equilibrium that would exist without the principal. (In fact, this effect can be achieved even in a single round, see Bradonjic et al. (2014).) This is in stark contrast to the case of single agent per round, where the Bayesian-expected reward of any BIC policy must be at least that of the a-priori best action.\nOur policy for stochastic utilities does not need to know the distribution of \u201cnoise\u201d added to the utilities. This is somewhat surprising because, for example, one would need to know the noise distribution in order to do a Bayesian update on the noisy inputs. Such detail-free properties are deemed desirable in the economics literature.\nIdeas and techniques. One important technical contribution concerns the concept of the explorable joint actions: those that can be explored by some BIC policy in some round. In general, not all joint actions are explorable, and the set of explorable joint actions may depend on the state \u03b80. Thus, we are facing two challenges. First, we identify which joint actions are explorable (and explore them). This has to be done inductively, whereby the utilities revealed by exploring one joint action may enable the policy to explore some others, and so forth until no further progress can be achieved. The second challenge is to define optimal (randomized) recommendation after all explorable joint actions have been explored, and prove that it outperforms any other BIC policy. While the latter seems intuitive (and it is), the proof is far from simple.\nTo address these two challenges, we develop the theory of the single-round game in our setting. Termed the recommendation game, it is a generalization of the well-knownBayesian Persuasion game (Kamenica and Gentzkow, 2011) where the signal observed by the principal is distinct from, but correlated with, the unknown state of nature. (Here the principal\u2019s signal captures the history observed in the previous rounds.) This generalization allows us to argue about comparative statics: what happens if the principal\u2019s signal is modified to contain more information relevant to the state \u03b80 (and possibly less information irrelevant to \u03b80). We show that a more informative signal can only improve the optimal reward achievable by the principal, and can only increase the set of \u201cexplorable\u201d joint actions (under a suitable definition of \u201cexplorable\u201d). These results, intuitive but non-trivial to prove, may be of independent interest.\nMore generally, while our algorithms and analyses involvemany notions, steps and statements that seem intuitive, making these intuitions precise required a multi-layered framework to reason about Bayesian Exploration in precise terms. Building up this framework has been a major part of the overall effort.\nWith stochastic utilities, we have an additional obstacle: the deviations between the expected utilities and our estimates thereof, albeit small and low-probability, may distort agents\u2019 incentives. Moreover, the number of different possible observations from the same joint action becomes exponential in the number of rounds, which blows up the running time if we use the techniques from the deterministic case without modifications. To address these obstacles, we carefully go back and forth between the noisy observations and the corresponding problem instance with deterministic utilities.\nWe would like to stress that our \u201calgorithmic characterization\u201d of all explorable joint actions is an important contribution even for the case of single agent per round. While (Kremer et al., 2014; Mansour et al., 2015) provide necessary and sufficient conditions when there are only two actions, for multiple actions Mansour et al. (2015) only give a sufficient condition which is far from necessary. Our work rectifies the situation and provides an explicit BIC policy to identify all explorable joint actions.\nAdditional motivating examples. The essential features of Bayesian Exploration \u2014 the threeway tradeoff between exploration, exploitation and incentives, and presence of multiple agents in a shared environment\u2014 can be found in a variety of scenarios, in addition to the routing scenario described above.\nThe first scenario concerns coordinating participants in a market. Consider sellers which are selling tickets for a particular sports event on an online platform such as StubHub. Implicitly, these sellers are involved in a game where they set the prices and the buyers can select which tickets to buy. A principal (the same platform or a third party) can suggest to the sellers how to set the prices so as to optimize the sellers\u2019 combined revenue or other notion of social optimality. The principal learns about the demand of the buyers (through exploration) in order to help the sellers to set the right prices (exploitation). The price recommendations need to be incentivecompatible because the sellers need to be convinced to follow them. Similarly, one can imagine a principal that coordinates the buyers in the same type of setting, recommending bidding strategies that optimize some global notion of \u201chappiness\u201d.\nThe second scenario concerns optimizing a shared access to a computing resource. For a simple example, consider a university data center shared by the faculty members, where each user can specify the machine to run his jobs on. The principal can collect the information from multiple users (to learn their typical resource requirements), and recommend which machines they should connect to. As a non-critical component, such recommendation system would be easier to maintain compared to a scheduler which enforces the same resource allocation, e.g., it can be ignored when/if it malfunctions.\nThe third scenario addresses a congestion game in which the agents are incentivized towards homophily. Consider a population of individuals choosing which experience to attend in the near future (e.g., go to a movie, watch a video on Youtube, read a news article, or attend a sports event). Often people are interested not only in the inherent quality of the experience, but also in sharing it with a particular group. This group may be quite large, e.g., it could include many people that share the same demographic, political views, general tastes, etc. Thus, one could imagine a recommendation service that would coordinate people towards sharing the experiences they would like with the people that they would want to share them with. Such recommendation service would need to explore to learn people\u2019s tastes, and be compatible with people\u2019s incentives.\nOrganization of the paper. Model and results are described in Section 2. In Section 3, we solve a simplified version of the problem, and along the way develop some essential tools. Section 4 presents the main result for the deterministic utilities. Stochastic utilities are treated in Section 5. A useful fact about the single-round game (a major tool in the preceding sections) is summarized and proved in Section 6. Conclusions and open questions are in Section 7.\nRelated work. A version of Bayesian Exploration without strategic interactions between agents (i.e., with a single agent in each round) was introduced by Kremer et al. (2014). They focused on the special case of two actions, and provided an optimal policy for deterministic utilities,\nand a preliminary result for stochastic utilities. Mansour et al. (2015) obtained optimal regret for stochastic utilities and a constant number of actions, as well as a reduction from an arbitrary non-BIC policy to a BIC one. Bahar et al. (2015) enrich the model to allow agents to observe their \u201cfriends\u201d in a social network (but restrict to deterministic utilities and two actions). Frazier et al. (2014) and Che and Ho\u0308rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).\nBayesian Exploration is closely related to three prominent subareas of theoretical economics andmachine learning: multi-armed bandits, design of information structures, and strategic learning in games. We briefly survey these connections below.\nMulti-armed bandits (Bubeck and Cesa-Bianchi, 2012) is a well-studiedmodel for explorationexploitation tradeoff. Absent the BIC constraint (and assuming the agents always follow the recommendations) Bayesian Exploration reduces to the multi-armed bandit problem with action set A and rewards equal to the principal\u2019s utility.\nThe recommendation game (a single round of Bayesian Exploration) is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents, where the signal observed by the principal is distinct from, but correlated with, the unknown \u201cstate\u201d. Our analysis of this game contributes to the line of work on Bayesian Persuasion and, more generally, on the design of information structures, see (Bergemann and Morris, 2016; Taneva, 2016) for background and references, and Dughmi and Xu (2016) for a more algorithmic perspective. Our notion of BIC constitutes (an appropriate version of) Bayes Correlated Equilibrium (Bergemann and Morris, 2013).\nA version of our setting with long-lived agents and no principal to coordinate them has been studied in (Bolton and Harris, 1999; Keller et al., 2005) under the name strategic experimentation. A vast literature on learning in games (Fudenberg and Levine, 1998) posits reasonable learning dynamics for the agents, as a proxy for their strategic behavior, and studies convergence to a given solution concept (e.g., an equilibrium)."}, {"heading": "2 Bayesian Exploration: our model and results", "text": "Our model, called Bayesian Exploration, is a game between a principal and multiple agents. It consists of T rounds, where the time horizon T is common knowledge.\nThere is a global parameter \u03b80, called the (realized) state. It is drawn from a Bayesian prior distribution \u03c8 over a finite state space \u0398; it is chosen before the first round, and stays the same throughout the game. The state space \u0398 and the prior \u03c8 are common knowledge, but the state itself is not revealed neither to the principal, nor to the agents. (Instead, the principal can learn it over time.) Elements of \u0398 will be called (feasible) states, to distinguish them from the realized state \u03b80.\nIn each round, there is a fresh set of agents, denoted [n] := {1 , . . . ,n}, playing a simultaneous game. Each agent i chooses an action ai from some finite set Ai of possible actions. A tuple a = (ai)i\u2208[n] is called a joint action. The set of all possible joint actions, termed the action set, is A = A1 \u00d7 . . . \u00d7 An. The utility of each agent i is determined by the joint action a chosen by the agents and the realized state \u03b80. More formally, it is given by a function ui : A\u00d7\u0398 \u2192 [0,1], called the utility function of agent i. The utility of the principal, a.k.a. reward, is also determined by the\npair (a,\u03b80), and given by the reward function f : A\u00d7\u0398 \u2192 [0,1]. The action set A and the functions (f ;u1 , . . . ,un) are the same for all rounds, and are common knowledge.\nIn each round t, the principal recommends a joint action to the agents. Specifically, the round proceeds as follows: a fresh set of n agents arrives; the principal recommends a joint action a \u2208 A; each agent i only observes his recommended action ai ; the agents choose their actions in the simultaneous game, and the utilities are realized. The principal observes the chosen actions and the realized utilities. Agents know the t, but do not observe the previous rounds.\nThe principal commits to an algorithm \u03c0 that proceeds in rounds, so that in each round it outputs a joint action, and then inputs the chosen actions and the realized utilities. This algorithm is called the iterative recommendation policy.\nWe will now define the incentive-compatibility constraint. Let \u03c0t be the joint action recommended by \u03c0 in round t (as a random variable taking values in A), and let Et\u22121 be the event that the agents have followed principal\u2019s recommendations up to (but not including) round t. Here and henceforth, we will use a standard game-theoretic notation: we will represent a joint action a \u2208 A as a pair (ai ,a\u2212i ), where i is a agent, ai \u2208 A is its action, and a\u2212i = (a1, . . . ,ai\u22121,ai+1, . . . ,an) is the joint action of all other agents; we will sometimes write (ai ,a\u2212i ;\u03b8) to denote the pair (a,\u03b8), where \u03b8 \u2208 \u0398 is a state. We will write A\u2212i for the set of all possible a\u2212i \u2019s. In particular, \u03c0 t \u2212i \u2208 A\u2212i denotes the joint action of all other agents chosen by policy \u03c0 in round t.\nDefinition 2.1. An iterative recommendation policy \u03c0 is Bayesian Incentive Compatible (BIC) if for all rounds t and agents i \u2208 [n] we have\nE [ ui(ai , \u03c0 t \u2212i ; \u03b80)\u2212 ui(a \u2032 i , \u03c0 t \u2212i ; \u03b80) | \u03c0 t i = ai , Et\u22121 ] \u2265 0, (1)\nwhere ai ,a \u2032 i \u2208 Ai are any two distinct actions such that Pr[\u03c0 t i = ai | Et\u22121] > 0. (The probabilities are over the realized state \u03b80 \u223c \u03c8 and the internal randomization in \u03c0.)\nIn words, suppose agent i is recommended to play action ai in a given round t. Assume that all agents followed the recommendations in the previous rounds, and that all agents but i follow the recommendations in round t. Then agent i cannot improve his conditional expected utility (given the information available to this agent) by choosing a different action a\u2032i .\nThe goal of the policy is to optimize the expected reward, as defined by the reward function f , subject to the BIC constraint. Throughout, we assume that the agents follow recommendations of a BIC iterative recommendation policy, so that the expected reward is well-defined.\nStochastic utilities. We allow amore general version, termed Bayesian Exploration with stochastic utilities, where given the joint action a \u2208 A and state \u03b80, the vector of all realized utilities is drawn independently from some distribution D(a,\u03b80) over such vectors, so that the expected utility of each agent i and the principal are, respectively, ui(a,\u03b80) and f (a,\u03b80). All realized utilities lie in [0,1].2 The distributionsD(a,\u03b8), (a,\u03b8) \u2208 A\u00d7\u0398 are known to the agents; however, for our results we can assume they are not known to the principal. The special case when the realized utilities are always equal to their expectations is termed Bayesian Exploration with deterministic utilities.\nDiscussion. One can consider a more general version in which the principal can send arbitrary messages to agents. However, the restriction of messages to recommended actions is without loss of generality (see the discussion in Section 3.1).\n2Our results extend to a more general case of sub-Gaussian noise; we omit the easy details.\nOne natural reward function f is social welfare, i.e., the sum of utilities of all agents in this round. However, we allow for an arbitrary notion of principal\u2019s utility.\nUtility structure and time horizon. We define the utility structure U to be a tuple that consists of the action set A, the state space \u0398, the prior \u03c8, the utility functions (u1 , . . . ,un), and the reward function f . Note that U completely determines the simultaneous game. A problem instance of Bayesian Exploration consists of U and time time horizon T (and the distributions D(a,\u03b8), (a,\u03b8) \u2208 A\u00d7\u0398 for stochastic utilities).\nFor ease of presentation, we will assume that every BIC iterative recommendation policy \u03c0 is well-defined (and BIC) for the infinite time horizon, i.e., for all rounds t \u2208 N. This assumption is without loss of generality; e.g., one can extend \u03c0 by setting \u03c0t = \u03c0T for all rounds t > T . The extended policy is BIC for the infinite time horizon as long as the original policy \u03c0 is BIC for the original time horizon T .\nComputational model. Our results include bounds on the running time. These bounds assume infinite-precision arithmetic (infinite-precision arithmetic operations can be done in unit time), and continuous random seed (a number can be drawn independently and uniformly at random from interval (0,1) in unit time). Such assumptions are commonly used in theoretical computer science to simplify exposition.\nFrom the computational point of view, an iterative recommendation policy inputs the utility structure. We assume the utilities are represented generically, as a (n + 1) \u00d7 |A| \u00d7 |\u0398| table. In particular, the input size is (more than) |A| \u00b7 |\u0398|. We obtain per-round running times that are polynomial in the input size."}, {"heading": "2.1 Statement of the main results", "text": "Given an iterative recommendation policy \u03c0, let REWt(\u03c0) = E[f (\u03c0 t ,\u03b80)] denote the expected reward of \u03c0 in a given round t. Here the expectation is taken over the realized state \u03b80 and the internal randomness in \u03c0. Let REW(\u03c0) = \u2211T t=1REWt(\u03c0\nt) be the (cumulative) expected reward over all rounds. Given the class\u03a0 of BIC iterative recommendation policies, our benchmarkwill be the optimal\nexpected reward achieved in any one round by any policy in \u03a0:\nOPT(\u03a0) = sup t\u2208N,\u03c0\u2208\u03a0 REWt(\u03c0). (2)\nNote that our benchmark does not have a built-in time horizon, and instead takes a sup over all rounds t. It follows that OPT(\u03a0) is at least as large as the optimal time-averaged expected reward sup\u03c0\u2208\u03a0 REW(\u03c0)/T , andmay bemuch larger for some policy classes and/or some problem instances. However, the restriction to BIC policies is a real limitation, and is essential in our setting.\nDeterministic utilities. We compete with the class of all BIC iterative recommendation policies with infinite time horizon, henceforth denoted\u03a0BIC. Our main result is a BIC policy whose timeaveraged expected reward is close to OPT(\u03a0BIC).\nTheorem 2.2. Consider Bayesian Exploration with deterministic utilities. There exists a BIC iterative recommendation policy \u03c0 that satisfies\nREW(\u03c0) \u2265 (T \u2212C) OPT(\u03a0BIC), (3)\nwhere C is a constant that depends only on the utility structure, but not on the time horizon T . The per-round running time of \u03c0 is polynomial in |A| \u00b7 |\u0398|.\nThus, we construct an efficiently computable policy \u03c0 whose expected reward is close to optimal. Moreover, this policy achieves constant regret as a function of T . Note that a priori our benchmark could have been much larger than the optimal time-averaged expected reward of any BIC policy, but we show otherwise. The per-round running time is polynomial in the input size, assuming a generic representation of the input.\nStochastic utilities. Our results for stochastic utilities compete against a slightly restricted class of BIC policies. For a given parameter \u03b4 > 0, a policy is called \u03b4-BIC if it satisfies a stronger version of Definition 2.1 in which right-hand side of (1) is \u03b4 rather than 0. The class of all such policies is denoted\u03a0\u03b4. We construct a BIC policy whose time-averaged expected reward is close to OPT(\u03a0\u03b4).\nTheorem 2.3. Consider Bayesian Exploration with stochastic utilities. For any given \u03b4 > 0, there exists a BIC iterative recommendation policy \u03c0 such that\nREW(\u03c0) \u2265 (T \u2212C\u03b4 logT ) OPT(\u03a0\u03b4),\nwhere C\u03b4 is a constant that depends only on the utility structure and the parameter \u03b4, but not on the time horizon T . The per-round running time of \u03c0 is polynomial in |A| \u00b7 |\u0398|. Policy \u03c0 does not input the parameterized utility distributions D(a,\u03b8), (a,\u03b8) \u2208 A\u00d7\u0398.\nIn fact, we prove a stronger version of Theorem 2.3, in which OPT(\u03a0\u03b4) is replaced with a similar benchmark for deterministic utilities. Given a problem instance with stochastic utilities, we consider the deterministic instance: a version of the original problem instance where all utilities are deterministically equal to the corresponding expected utilities in the original instance. For a policy class\u03a0, we define OPTdet(\u03a0) as the value of OPT(\u03a0) for the deterministic instance. We focus on the class of all iterative recommendation policies that are \u03b4-BIC for the deterministic instance, denoted \u03a0det\u03b4 . The new (and stronger) benchmark is defined as OPT det(\u03a0det\u03b4 ).\nBayesian regret and optimality. The two theorems provide bounds on Bayesian regret, defined here as R(T ;\u03a0) = T OPT(\u03a0) \u2212 REW(\u03c0), where \u03a0 is the benchmark class of policies. Specifically: R(T ;\u03a0BIC) \u2264 C for deterministic utilities, and R(T ;\u03a0\u03b4) \u2264 C\u03b4 log(T ) for stochastic utilities and any \u03b4 > 0. These regret bounds are optimal in terms of the asymptotic dependence on T . This dependence is optimal even for the version without the BIC constraint, i.e., for multi-armed bandits (MAB). To see this, consider the special case of a single agent per round. First, it is implicit in (Mansour et al., 2015) that the benchmark OPT(\u03a0\u03b4) is the expected reward of the best fixed action (the standard benchmark in MAB) as long as \u03b4 is sufficiently small and all actions are explorable. Second, constant regret is obviously the best possible for the deterministic version, and O(logT ) regret is optimal for any instance of MAB with stochastic rewards (Lai and Robbins, 1985). Third, it is implicit in (Kremer et al., 2014; Mansour et al., 2015) that upper bounds on Bayesian regret must depend on the prior."}, {"heading": "3 Warm-up and tools", "text": "As a warm-up, let us consider the version with deterministic utilities, and focus on a relatively simple scenario when a BIC iterative recommendation policy explores all joint actions, and then exploits (in the sense that wemake precise later). We show that this policy can achieve optimal perround performance once all joint actions are explored. Recall that our benchmark is OPT(\u03a0BIC), where \u03a0BIC is the class of all BIC iterative recommendation policies, and OPT(\u00b7) is defined in (2).\nLemma 3.1. Consider Bayesian Exploration with deterministic utilities. Let \u03c0 be a BIC iterative recommendation policy that explores all joint actions by a fixed time T\u03c0 \u2264 T . Then there exists a BIC policy \u03c0\u2032 which coincides with \u03c0 before round T\u03c0, and achieves expected reward at least OPT(\u03a0BIC) in all subsequent rounds. Therefore, f (\u03c0\u2032) \u2265 (T \u2212T\u03c0) OPT(\u03a0BIC).\nWhile very intuitive, this result is surprisingly technical to prove from scratch. Essentially, one needs to specify what \u201cexploitation\u201d means in this context, and argue that this notion of exploitation is BIC and can only benefit from having full information about the utility structure. Thus, we develop a framework to reason about this, which will be an essential toolbox throughout the paper. More specifically, we define and analyze a game which captures a single round of Bayesian Exploration, and formulate a framework to combine BIC \u201csubroutines\u201d into a BIC iterative recommendation policy. A (very simple) proof of Lemma 3.1 using these tools is in the very end of this section.\nWhile Lemma 3.1 relies on the ability to explore all joint actions, this ability is not guaranteed. This can be seen even in the special case of a single agent per round and only two actions. For this special case, Kremer et al. (2014) and Mansour et al. (2015) present necessary and sufficient conditions under which all actions are explorable, as well as simple examples when these conditions fail. Mansour et al. (2015) also provides sufficient conditions for the version with a single agent per round and an arbitrary number of actions."}, {"heading": "3.1 The recommendation game: a single round of Bayesian Exploration", "text": "We view a single round of Bayesian Exploration as a stand-alone game between the principal and the agents, termed the recommendation game. Here the principal observes an auxiliary \u201csignal\u201d, which represents the information received in the previous rounds (and possibly also the internal random seed). Then the principal recommends a joint action, and the agents choose their actions.\nFormally, the recommendation game is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents. Unlike the original Bayesian Persuasion game, in our version the signal observed by the principal is distinct from (but correlated with) the unknown \u201cstate\u201d.\nGame specification. For a problem instance of Bayesian Explorationwith a given utility structure, the corresponding recommendation game proceeds as follows:\n\u2022 the state \u03b80 is drawn from a Bayesian prior distribution \u03c8 over \u0398; \u2022 the principal observes a signal S , then recommends action ai \u2208 Ai for each agent i; \u2022 the agents choose their actions in the simultaneous game; \u2022 the principal and the agents receive utilities according to the utility structure.\nSignal S is an arbitrary random variable with finite support X (the elements of X are called feasible signals). The signal can be correlated with the state: formally, S and \u03b80 are random variables on the same probability space. The signal structure associated with S is the tuple (\u0398,X ,\u03c8\u2217), where \u03c8\u2217 is the joint distribution of (S,\u03b80).\nThe utility structure and the signal structure are common knowledge. The realized state \u03b80 is not revealed to the principal (other than through the signal S). Each agent i only observes his own recommendation ai ; it does not observe the state \u03b80, the signal S , or the other recommendations\nThe principal commits to a recommendation policy: a randomized mapping \u03c0 : X \u2192 A, which takes as input a feasible signal and outputs an action for each agent. The corresponding incentivecompatibility constraint is defined as follows:\nDefinition 3.2. Given signal S , a recommendation policy \u03c0 is Bayesian incentive compatible (BIC) if for each agent i \u2208 [n] and any two distinct actions ai ,a \u2032 i \u2208 Ai such that Pr[\u03c0i(S) = ai ] > 0 we have\nE [ ui (ai , \u03c0\u2212i(S); \u03b80)\u2212 ui(a \u2032 i , \u03c0\u2212i (S) ;\u03b80) | \u03c0i (S) = ai ] \u2265 0. (4)\nIn words, whenever agent i is recommended to play some action ai , he could not improve his expected utility (given the information available to this agent, and assuming that all other agents follow the recommendations) by choosing a different action a\u2032i . We assume that the agents follow the recommendations of a BIC policy, so that the expected reward is well-defined.\nFor the recommendation game, the distinction between stochastic and deterministic utilities is unimportant (for statements that only involve expected utilities and rewards). In particular, a given recommendation policy is BIC for stochastic utilities if and only if it is BIC for the corresponding problem instance with deterministic utilities.\nAn important special case is the empty signal, defined as a signal which always takes the same value. Such signal will be denoted as S = \u22a5.\nFor the computational results, we assume that the joint distribution of (S,\u03b80) is given explicitly, as a |X | \u00d7 |\u0398| table of probabilities.\nRelation to Bayesian Exploration. Consider an iterative recommendation policy \u03c0. W.l.o.g., the internal random seed \u03c9 of policy \u03c0 is chosen once, before the first round, and persists throughout. Let Ht be the history up to round t: the chosen joint actions and the realized utilities over all past rounds. Then policy \u03c0 can be represented as a sequence (\u03c0(t) : t \u2208 N), where for each round t, \u03c0(t) is a randomized mapping from St = (\u03c9,Ht) to joint actions (called the restriction of \u03c0 to round t). Each round t can be seen as a recommendation game with signal St, and \u03c0\n(t) is a recommendation policy in this game. It is easy to see that \u03c0 is BIC if and only if \u03c0(t) is BIC for all t.\nDiscussion. The notion of Bayesian Incentive Compatibility in the recommendation game is closely connected to the notion of Bayes Correlated Equilibrium (Bergemann and Morris, 2013): modulo the differences in terminology, the former is a special case in which the agents do not receive private signals. In particular, it follows that a BIC recommendation policy always exists.\nOne can consider a more general version of the recommendation game in which the principal can send arbitrary messages to agents. Then the principal commits to a messaging policy: a randomized mapping which inputs the signal S and outputs a message mi for each agent i. However, such messaging policies can without loss of generality be restricted to recommendation policies. More precisely, suppose a messaging policy \u03c4 induces a Bayes Nash Equilibrium \u03c1 (which, in our notation, is a randomized mapping that inputs the joint message (m1 , . . . ,mn) and outputs a joint action a \u2208 A). Then the composition \u03c1 \u2295 \u03c4 is a randomized mapping that inputs signal S and outputs a joint action, i.e., a recommendation policy. According to Bergemann and Morris (2013), this composition is BIC.3 Thus, the principal can use \u03c1\u2295 \u03c4 instead of \u03c4."}, {"heading": "3.2 Properties of the recommendation game", "text": "Multiple signals. Throughout, we consider a fixed utility structure U , but allow the signal S to vary from one game instance to another. We will suppress U from our notation, but explicify the\n3This follows from the \u201donly if\u201d direction of Theorem 1 in Bergemann and Morris (2013). In their notation, we use the special case where the information structure S is empty, and S\u2217 is induced by the messaging policy \u03c4. Then the \u201cdecision rule\u201d \u03c0 in the theorem corresponds to the recommendation policy \u03c1\u2295 \u03c4.\ndependence on S . To study how the properties of a game depend on a particular signal S , we consider multiple signals such that the signals and the realized state \u03b80 are random variables in the same probability space; such signals will be called coupled. While each of these signals corresponds to a separate game instance (with a shared \u03b80), we will refer to all these game instances jointly as recommendation game with coupled signals.\nOptimality. Given a recommendation policy \u03c0 for signal S , its expected reward is\nREW(\u03c0) = E [ f (\u03c0(S),\u03b80) ] = E (s,\u03b8)\u223c\u03c8\u2217 [ f (\u03c0(s),\u03b8) ] .\nThe expectation is over the joint distribution of signal S and realized state \u03b80, and the internal randomness in policy \u03c0. The optimal reward given signal S is defined as\nREW\u2217[S] = sup BIC recommendation policies \u03c0 for S REW(\u03c0).\nThus, it is the largest expected reward achievable in a recommendation game with signal S .4 A BIC policy \u03c0 in this game will be called optimal for S if REW(\u03c0) = REW\u2217[S].\nLP representation. We will represent the problem of finding an optimal recommendation policy \u03c0 as a linear program (henceforth, LP). We represent \u03c0 as a set of numbers xa,s = Pr[\u03c0(s) = a], for each joint action a \u2208 A and each feasible signal s \u2208 X . These numbers, termed the LP-representation of \u03c0, will be the decision variables in the LP. The linear program is as follows:\nmaximize \u2211\na\u2208A, s\u2208X , \u03b8\u2208\u0398\n\u03c8(\u03b8) \u00b7\u03c8\u2217(s | \u03b8) \u00b7 xa,s \u00b7 f (a,\u03b8)\nfor all i \u2208 [n],ai ,a \u2032 i \u2208 Ai ,\n\u2211\na\u2212i\u2208A\u2212i , s\u2208X , \u03b8\u2208\u0398\n\u03c8(\u03b8) \u00b7\u03c8\u2217(s | \u03b8) \u00b7 ( ui (ai ,a\u2212i ;\u03b8)\u2212 ui(a \u2032 i ,a\u2212i ;\u03b8) ) \u00b7 xa,s \u2265 0\nfor all s \u2208 X ,a \u2208 A xa,s \u2265 0 and \u2211 a\u2208A xa,s = 1\nIn the above LP, \u03c8(\u03b8) stands for Pr[\u03b80 = \u03b8], and \u03c8\u2217(s | \u03b8) stands for Pr[S = s | \u03b80 = \u03b8]. The objective is REW(\u03c0), written in terms of the LP-representation, and the first constraint states that \u03c0 is BIC.5 The feasible region of this LP is polytope in R|X |\u00d7|A|, which we denote by BIC[S].\nClaim 3.3. A policy \u03c0 is BIC if and only if its LP-representation lies in BIC[S].\nCorollary 3.4. An optimal recommendation policy exists. Given the utility structure and the signal structure, an optimal recommendation policy and the optimal reward REW\u2217[S] can be computed in time polynomial in |A| \u00b7 |X | \u00b7 |\u0398|. Further, a convex combination of BIC policies is also BIC.\n4We write REW\u2217[S] rather than REW\u2217(S) to emphasize that the optimal reward is a function of the random variable S , rather than a function of a particular realization of this random variable. We will use a similar notation function[S] for some other functions of signal S as well, e.g., for the polytope BIC[S].\n5For a particular agent i and actions ai ,a \u2032 i \u2208 Ai , the BIC constraint in Definition 3.2 states that Pr[E] > 0 implies E[W | E] \u2265 0, where W = ui(ai ,\u03c0\u2212i (S),\u03b80) \u2212 ui (a \u2032 i ,\u03c0\u2212i (S),\u03b80) and E = {\u03c0i (S) = ai }. This is equivalent to E[1E \u00b7W ] \u2265 0. And E[1E \u00b7W ] is precisely the left-hand side of the first constraint in the LP.\nMonotonicity in information. We prove that REW\u2217[S] can only increase if signal S becomes more informative. To make this statement formal, we consider a recommendation game with coupled signals S,S \u2032. We give a definition that compares the two signals in terms of their \u201cstate-relevant\u201d information content, and state the \u201cmonotonicity-in-information\u201d lemma (proved in Appendix 6).\nDefinition 3.5. Consider a recommendation game with coupled signals S,S \u2032 , with resp. supports X ,X \u2032 . We say that signal S is at least as informative as S \u2032 if\nPr[\u03b80 = \u03b8 | S = s, S \u2032 = s\u2032] = Pr[\u03b80 = \u03b8 | S = s] \u2200s \u2208 X , s \u2032 \u2208 X ,\u03b8 \u2208\u0398.\nAn important special case is when S determines S \u2032 : S \u2032 = g(S) for some g : X \u2192 X \u2032.\nLemma 3.6 (monotonicity-in-information). In a recommendation game with coupled signals S,S \u2032 , if signal S is at least as informative as S \u2032 then REW\u2217[S] \u2265 REW\u2217[S \u2032].\nA recommendation policy \u03c0 for signal S is also well-defined for signal S \u2032 that determines S . Formally, \u03c0 induces a recommendation policy \u03c0\u2032 that inputs S \u2032, maps it to the corresponding value of S , and returns \u03c0(S). Note that \u03c0 and \u03c0\u2032 choose the same joint actions, and \u03c0 is BIC given S if and only if \u03c0\u2032 is BIC given S \u2032 . Henceforth we identify all such \u201cinduced\u201d policies \u03c0\u2032 with \u03c0.\nOptimality for limited exploration. Let us study a recommendation game with a signal that corresponds to exploring a given (possibly randomized) subset B \u2282 A of joint actions. Formally, the subset B \u2282 A will be a 2A-valued signal: a signal whose values are subsets of A; e.g., its realization may depend on the realized state \u03b80.\nLet us consider the signal which corresponds to exploring all joint actions in B. This signal consists of all relevant utilities, and also includes B itself:\nAllInfo(B) := ( B; (f (a,\u03b80);u1(a,\u03b80) , . . . ,un(a,\u03b80)) : a \u2208 B) . (5)\nNote that it is a random variable, because it depends on random variables B and \u03b80. Despite a rather complicated definition, S = AllInfo(B) is just a signal in a recommendation game. In particular, we can consider an optimal recommendation policy given this signal. By Corollary 3.4, such policy exists, and its LP-representation (and its expected reward) can be computed in time polynomial in |\u0398| \u00b7 |A| \u00b7 |support(S)|.\nWe prove that if a BIC iterative recommendation policy is restricted to joint actions in B, then its expected per-round reward cannot exceed REW\u2217[AllInfo(B)].\nClaim 3.7. Consider Bayesian Exploration with stochastic utilities. Let B \u2282 A be a 2A-valued signal (i.e., a randomized set of joint actions). Let \u03c0 be a BIC iterative recommendation policy such that before a given round t it can only choose joint actions in B. Then REWt(\u03c0) \u2264 REW \u2217[AllInfo(B)].\nProof. Consider the recommendation game that corresponds to the t-th round of Bayesian Exploration. Recall that the signal in this game is St = (\u03c9,Ht), where \u03c9 is the internal random seed of policy \u03c0, and Ht is the history before round t. Then \u03c0\n(t), the restriction of policy \u03c0 to round t, is a BIC recommendation policy in this game.\nThe expected reward of policy \u03c0 in round t is the same as the expected reward of the restricted policy \u03c0(t), i.e.: REWt(\u03c0) = REW(\u03c0 (t)). By optimality of REW\u2217[St] we have REW(\u03c0 (t)) \u2264 REW\u2217[St]. Since policy \u03c0 is restricted to actions in B, signal AllInfo(B) is at least as informative as signal St. So by Lemma 3.6 we have REW\u2217[St] \u2264 REW \u2217[AllInfo(B) ], completing the proof."}, {"heading": "3.3 Subroutines and the proof of Lemma 3.1", "text": "We design iterative recommendation policies in a modular way, via \u201csubroutines\u201d that comprise multiple rounds and accomplish a particular task. In particular, we need a formal framework to argue that our \u201csubroutines\u201d are BIC if considered separately, and jointly form a BIC policy. We model a \u201csubroutine\u201d as an iterative recommendation policy that inputs the history of the previous rounds and chooses its own duration.\nFormally, we consider a common generalization of Bayesian Exploration and the recommendation game: the state \u03b80 is drawn and the signal S is observed exactly as in the recommendation game, and then the game proceeds over multiple rounds, exactly as in Bayesian Exploration. Note that the recommendation game is simply a special case with time horizon T = 1.\nWe focus on a versionwhere the time horizon is infinite (and irrelevant). Instead, each iterative recommendation policy \u03c0 runs for T\u03c0 rounds, where the number T\u03c0, termed duration, is chosen by the policy rather than given exogenously. The duration is chosen before the signal is observed, and thus can only depend on the utility structure and the signal structure. (This restriction is crucial for proving Claim 3.8.) The notion of BIC carries over word-by-word from Definition 2.1. Iterative recommendation policies in this model will also be called subroutines.\nFrom the computational point of view, the input to a subroutine consists of the utility structure, the signal structure, and the realization of the signal. The output of a subroutine is the tuple (S,H), where H is the history: the chosen joint actions and the rewards/utilities for all rounds in the execution, or any function of (S,H).\nA recommendation game and aGeneral Bayesian Exploration game with the same utility/signal structure are called associated. As in Section 3.2, we fix the utility structure, but allow the signal S to vary from one subroutine to another. A subroutine \u03c0 initialized with a particular feasible signal s will be denoted \u03c0(s).\nNow we can define the composition of subroutines, so that the composition of BIC subroutines is BIC. Consider two subroutines \u03c0,\u03c0\u2032 with respective signals S,S \u2032 . Then the composition of \u03c0 followed by \u03c0\u2032, denoted \u03c0 \u2295 \u03c0\u2032, is a subroutine with signal S and duration T\u03c0 + T\u03c0\u2032 . The first T\u03c0 rounds of \u03c0\u2295\u03c0\u2032 are controlled by \u03c0, and the subsequent T\u03c0\u2032 rounds are controlled by \u03c0\n\u2032. In order for the composition to be well-defined, the signal for \u03c0\u2032 should be expressed in terms of the output of \u03c0. Formally, we say that \u03c0\u2032 is a valid sequel for \u03c0 if the pair (S,H) determines S \u2032 , where H is the history of \u03c0.\nClaim 3.8. Fix the utility structure. Consider BIC subroutines \u03c0,\u03c0\u2032 such that \u03c0\u2032 is a valid sequel for \u03c0. Then the composition \u03c0\u2295\u03c0\u2032 is BIC.\nA sequence of subroutines (\u03c0,\u03c0\u2032 ,\u03c0\u2032\u2032 , . . .) is called valid if the every next subroutine is a valid sequel for the previous one. It is easy to see that the composition is associative: \u03c0 \u2295 (\u03c0\u2032 \u2295 \u03c0\u2032\u2032) = (\u03c0 \u2295 \u03c0\u2032) \u2295 \u03c0\u2032\u2032, so it is uniquely defined by the sequence, and can be denoted \u03c0 \u2295 \u03c0\u2032 \u2295 \u03c0\u2032\u2032. Thus, a BIC iterative recommendation policy can be presented as a composition of a valid sequence of subroutines, where the first subroutine inputs an empty signal, and all durations sum up to T .\nProof of Lemma 3.1. Let \u03c0 be the policy from the lemma statement, and let \u03c3 be the subroutine of duration T\u03c0 which coincides with \u03c0 on the first T\u03c0 rounds. (In other words, \u03c3 is a version of policy \u03c0 that is \u201ctruncated\u201d after round T\u03c0.) Let \u03c0\n\u2217 be an optimal recommendation policy for signal S = AllInfo(A). Note that REW\u2217[S] \u2265 OPT(\u03a0BIC) by Claim 3.7. Let \u03c0\n\u2032 be an iterative recommendation policy defined as the composition of \u03c3 followed by T \u2212 T\u03c3 copies of \u03c0 \u2217. Note\nthat \u03c0\u2032 is BIC by Claim 3.8, and receives expected reward REW\u2217[A] in each round after T\u03c0 by construction."}, {"heading": "4 Bayesian Exploration with deterministic utilities", "text": "In this section, we focus on deterministic utilities, and prove Theorem 2.2. In particular, we will construct a BIC iterative recommendation policy \u03c0 whose expected reward REW(\u03c0) satisfies (3).\nRather than comparing \u03c0 directly to OPT(\u03a0BIC), we will use an intermediate benchmark of the form REW\u2217[AllInfo(Adet\u03b80 )], for some subset A det \u03b80 \u2282 A of joint actions that depends on the realized state \u03b80. While we usedA det \u03b80\n=A to prove Lemma 3.1, this is not a \u201cfair\u201d intermediate benchmark, because there may be some joint actions that cannot be explored by any BIC policy. Instead, we will only consider joint actions that can be explored.\nDefinition 4.1 (explorability). A joint action a \u2208 A is called eventually-explorable given a state \u03b8 \u2208\u0398 if Pr[\u03c0t = a | \u03b80 = \u03b8] > 0 for some BIC iterative recommendation policy \u03c0 and some round t \u2208 N. The set of all such joint actions is denoted Adet\u03b8 .\nThus,Adet\u03b80 is the set of all joint actions that can be explored by some BIC policy in some round, given the realized state \u03b80. Note that it is a random variable whose realization is determined by \u03b80. Using REW\n\u2217[Adet\u03b80 ] as an intermediate benchmark suffices because, by Claim 3.7 we have\nREW\u2217[Adet\u03b80 ] \u2265 OPT(\u03a0BIC). (6)\nWe construct a BIC iterative recommendation policy which explores all of Adet\u03b80 . Note that the existence of such policy does not immediately follow from Definition 4.1, because the definition only guarantees a BIC policy separately for each (\u03b8,a) pair, whereas we need one BIC policy which \u201cworks\u201d for the specific (and unknown) realized state \u03b80, and all \u201cmatching\u201d joint actions a.\nDefinition 4.2. A subroutine is called maximally-exploring if it is a BIC subroutine that inputs an empty signal and explores all joint actions in Adet\u03b80 by the time it stops.\nA maximally-exploring subroutine can be followed by exploitation, using the corresponding optimal recommendation policy. The resulting BIC iterative recommendation policy achieves the regret bound claimed in Theorem 2.2.\nLemma 4.3. Let \u03c3 be a maximally-exploring subroutine of duration T\u03c3 . Let \u03c0 \u2217 be an optimal recommendation policy for signal AllInfo(Adet\u03b80 ). Let \u03c0 be the composition of subroutine \u03c3 followed by T \u2212T\u03c3 copies of \u03c0\u2217. Then \u03c0 is a BIC iterative recommendation policy that satisfies\nREW(\u03c0) \u2265 (T \u2212T\u03c3 ) OPT(\u03a0BIC). (7)\nThe lemma easily follows from the machinery developed in Section 3. Namely, we use the notion and existence of Adet\u03b80 -optimal policy (see Corollary 3.4), the \u201cmonotonicity-in-information\u201d analysis which guarantees (6), and the \u201ccomposition of subroutines\u201d analysis which guarantees that \u03c0 is BIC (via Claim 3.8).\nThe rest of this section is organized as follows. In Section 4.1 we develop the theory of \u201cexplorability\u201d in a recommendation game. Then in Section 4.2 we use this theory to define a natural BIC subroutine and prove that this subroutine is maximally-exploring (and has the desired perround computation time). Finally, in Section 4.3 we put it all together to prove Theorem 2.2."}, {"heading": "4.1 Explorability in the recommendation game", "text": "In this subsectionwe investigate which joint actions can be explored in the recommendation game. We adopt a very permissive definition of \u201cexplorability\u201d, study some of its properties, and design a subroutine which explores all such joint actions. Throughout, we consider a recommendation game with signal S whose support is X .\nDefinition 4.4. Consider a recommendation game with signal S . A joint action a is called signalexplorable, for a given feasible signal s \u2208 X , if there exists a BIC recommendation policy \u03c0a,s such that Pr[\u03c0a,s(s) = a] > 0. The set of all such joint actions is denoted EXs[S]. The signal-explorable set is defined as EX[S] = EXS [S].\nNote that EXs[S] is a fixed subset of joint actions (determined by the feasible signal s \u2208 X ), whereas EX[S] is a random variable (in fact, a 2A-valued signal) whose realization is determined by the realization of signal S .\nWe show that the signal-explorable set EX[S] can only increase if signal S becomes more informative. (The proof is deferred to Appendix 6.)\nLemma 4.5 (monotonicity-in-information for explorability). Consider a recommendation game with coupled signals S,S \u2032 . If signal S is at least as informative as S \u2032 then EX[S] \u2283 EX[S \u2032].\nNote that EX[S] and EX[S \u2032] are set-valued random variables, and the claim asserts that one random set is always contained in the other. (In general, if X and Y are random variables, we will write X \u2208 Y and X \u2282 Y to mean that the corresponding event holds for all realizations of randomness.)\nA recommendation policy withmaximal support. Observe that policies \u03c0a,s in Definition 4.4 can be replaced with a single BIC recommendation policy \u03c0max such that EXs[S] = support(\u03c0\nmax(s)) for each feasible signal s \u2208 X . We will call such \u03c0max a max-support policy for signal S . For example, we can set\n\u03c0max = 1 |X | \u2211 s\u2208X 1 |EXs[S]| \u2211 a\u2208EXs[S] \u03c0a,s. (8)\nThis policy is BIC as a convex combination of BIC policies, and max-support by design. We compute a max-support policy as follows. For each joint action a \u2208 A and each feasible signal s \u2208 X , we solve the following LP:\nmax x\u2208BIC(S) \u03b7a,s subject to xa,s \u2265 \u03b7a,s. (9)\nThis LP always has a solution, because BIC policies exist and any BIC policy gives a solution with objective value \u03b7a,s = 0. Further, \u03b7a,s > 0 if and only if a \u2208 EXs[S], in which case the solution x is the LP-representation of a policy \u03c0a,s such that Pr[\u03c0a,s(s) = a] > 0. Then the max-support policy \u03c0max is computed via (8). The computational procedure is given in Algorithm 1. Note that ComputeMaxSupport computes subsets As = EXs[S], and the output x is the LP-representation of the policy given by (8).\nThe \u201cquality\u201d of a max-support policy \u03c0max is, for our purposes, expressed by the minimal probability of choosing a joint action in its support:\npmin(\u03c0 max) := min s\u2208X , a\u2208support(\u03c0max(s)) Pr[\u03c0max(s) = a]. (10)\nAlgorithm 1 ComputeMaxSupport: computes a max-support policy.\nInput: the utility structure and the signal structure. Output: an LP-representation of a max-support policy.\nFor each joint action a \u2208 A and feasible signal s \u2208 X : solve the linear program (9); let xa,s be a solution with objective value \u03b7a,s. Let As = {a \u2208 A : \u03b7a,s > 0} for each s \u2208 X . Output x = 1\n|X |\n\u2211 s\u2208X\n1 |As | \u2211 a\u2208As xa,s.\nWe relate this quantity to the min-max probability in Definition 4.4:\npmin[S] := min s\u2208X , a\u2208EXs[S] max x\u2208BIC[S] xa,s. (11)\nIt is easy to see that ComputeMaxSupport constructs policy \u03c0max with pmin(\u03c0 max) \u2265 1 |A|\u00b7|X | pmin[S]. To summarize, we have proved the following:\nClaim 4.6. There is a max-support policy \u03c0max with pmin(\u03c0 max) \u2265 1\n|A|\u00b7|X | pmin[S]. It can be computed\nby ComputeMaxSupport in time polynomial in |A|, |X |, and |\u0398|.\nMaximal exploration given a signal. Let us design a subroutine \u03c3 which explores all signalexplorable joint actions. More specifically, given a recommendation game with signal S , we are looking for a subroutine \u03c3 in the associated Generalized Bayesian Exploration game which explores all joint actions in EX[S]. Such subroutine will be called maximally-exploring for signal S .\nWe start with a max-support policy \u03c0max returned by ComputeMaxSupport. Let xmax be its LP-representation, so that xmaxa,s := Pr[\u03c0\nmax(S) = a]. Given a particular signal realization s \u2208 X , we proceed as follows. We compute the signal-explorable set EXs[S] as the support of \u03c0\nmax(s). For each joint action a \u2208 EXs[S], we choose the dedicated round \u03c4s(a) when this action is chosen. The dedicated rounds are chosen uniformly at random, in the sense that an injective function \u03c4s : EXs[S]\u2192 [T\u03c3 ] is chosen uniformly at random among all such functions (where T\u03c3 is the duration of the subroutine). To guarantee that the subroutine is BIC, we ensure that\nPr[\u03c3 t(s) = a] = xmaxa,s for each round t and joint action a \u2208 A. (12)\nTo this end, for each non-dedicated round the joint action is chosen independently from a fixed distribution Ds that is constructed to imply (12). Specifically, we write Ns = |EXs[S]| and define:\nDs(a) = xmaxa,s \u2212 1/T\u03c3 T\u03c3 \u2212Ns \u2200a \u2208 EXs[S]. (13)\nIt is easy to see that Ds is a distribution, provided that the duration is large enough. Specifically, it suffices to set T\u03c3 =max(1 +Ns , \u23081/pmin(\u03c0 max)\u2309). Then for each round t and action a \u2208 EXs[S],\nPr[\u03c3 t(s) = a] = 1/T\u03c3 +Ds(a) \u00b7 (T\u03c3 \u2212Ns) = x max a,s ,\nwhere the 1/T\u03c3 is the probability that round t is dedicated for action a. This completes the description of the subroutine, and the proof that it is BIC. The computational procedure for this subroutine is summarized in Algorithm 2.\nOur discussion of MaxExplore can be summarized as the following claim:\nAlgorithm 2 Subroutine MaxExplore: maximal exploration given signal S .\nInput: utility structure U , signal structure S , and signal realization s \u2208 X .\n// compute the parameters x\u2190 ComputeMaxSupport(U ,S ) // LP-representation of a max-support policy \u03c0max B\u2190 {a :\u2208 A : xa,s > 0} // signal-explorable set EXs[S] pmin \u2190min(xa,s\u2032 : a \u2208 B, s \u2032 \u2208 X ) // computes pmin(\u03c0 max) T \u2190max(1+ |B|, \u23081/pmin\u2309) // the duration of the subroutine Pick injective function \u03c4 : B\u2192 [T ] u.a.r. from all such functions. D(a)\u2190 xa,s\u22121/T T\u2212|B| \u2200a \u2208 B. // distribution (13) for non-dedicated rounds\n// issue the recommendations for rounds t = 1 . . .T do\nif t = \u03c4(a) for some a \u2208 B then at \u2190 a else choose at from distribution D Recommend action at , observe the corresponding utilities\nOutput: signal AllInfo({a1 , . . . ,aT }).\nClaim 4.7. MaxExplore is a maximally-exploring subroutine for signal S . Its duration is at most |A| \u00b7 |X | / pmin[S] rounds. The running time is polynomial in |A| \u00b7 |X | in pre-processing, and linear in |A| per each round."}, {"heading": "4.2 A maximally-exploring subroutine", "text": "We come back to Bayesian Exploration, and strive to explore as many joint actions as possible. We define a natural BIC subroutine for this goal, and prove that it is indeed maximally-exploring.\nOur BIC subroutine is based on the following intuition. Initially, one can explore some joint actions via \u201cmaximal exploration\u201d for an empty signal S1 = \u22a5. This gives some additional observations, so one can now run \u201dmaximal exploration\u201d for a new signal S2 which comprises these new observations. This in turn provides some new observations, and so forth. We stop after |A| iterations, which (as we prove) suffices to guarantee that no further progress can be made.\nFormally, the subroutine proceeds in phases \u2113 = 1,2,3 , . . . , |A|. In each phase, we start with the \u201ccurrent\u201d signal S\u2113, and perform \u201cmaximal exploration\u201d given this signal by calling MaxExplore. This call computes the \u201cnext\u201d signal S\u2113+1 = AllInfo(EX[S\u2113]). After the last phase, we outputs the latest signal S|A|+1 which (as we prove) encompasses all observations received throughout the subroutine. Each call to MaxExplore must be parameterized with the signal structure for the corresponding signal S\u2113. Since the signal is uniquely determined by the realized state \u03b80, the signal structure is completely specified by the tuple (S\u2113(\u03b8) : \u03b8 \u2208\u0398), where S\u2113(\u03b8) is the value of the signal that corresponds to realized state \u03b80 = \u03b8. The pseudocode is summarized in Algorithm 3.\nRepeatMaxExplore is BIC as a composition of BIC subroutines. It is easy to see, by induction on phase \u2113, that the realization of signal S\u2113 is determined by the state \u03b80. In particular, it follows that the support of each signal S\u2113 is of size at most |\u0398|. Therefore, the per-round running time is polynomial in |A| \u00b7 |\u0398|, because so is the the running time of ComputeMaxSupport and the per-round running time of MaxExplore. Thus:\nClaim 4.8. RepeatMaxExplore is BIC; its per-round running time is polynomial in |A| \u00b7 |\u0398|.\nAlgorithm 3 Subroutine RepeatMaxExplore: maximal exploration.\nInput: the utility structure U .\nInitialize: S1 = S1 =\u22a5. // \u201cphase-1 signal\u201d is empty For each phase \u2113 = 1,2 , . . . , |A|\nS\u2113+1 \u2190MaxExplore(U ,S\u2113 ,S\u2113) // compute signal S\u2113+1 = AllInfo(EX[S\u2113])\n// compute the signal structure S\u2113+1 for signal S\u2113+1 x \u2190 ComputeMaxSupport(U ,S\u2113) // LP-representation of a max-support policy S\u2113+1 \u2190 (S\u2113+1(\u03b8) : \u03b8 \u2208\u0398),\nwhere S\u2113+1(\u03b8)\u2190 AllInfo({a \u2208 A : xa,S\u2113(\u03b8) > 0})\nOutput: signal S|A|+1.\nLet B\u2113+1 be the set of all joint actions explored during phase \u2113. For the sake of the argument, let us define B1 = \u2205, and extend the definition of sets B\u2113 and signals S\u2113 to phases \u2113 = 1,2,3, . . . (i.e., without an upper bound on the number of phases). By construction, B\u2113+1 = EX[S\u2113] is a random variable whose realization is determined by the realized state \u03b80, and S\u2113 = AllInfo(B\u2113).\nWe show that B\u2113+1 is the set of all joint actions explored during the first \u2113 phases, and that stopping after |A| phases is without loss of generality:\nClaim 4.9. Sets (B\u2113 : \u2113 \u2208 N) are non-decreasing in \u2113, and identical for \u2113 \u2265 |A|+1.\nProof. To prove that sets (B\u2113 : \u2113 \u2208 N) are non-decreasing in \u2113, we use induction on \u2113 and Lemma 4.5(a) on \u201cmonotonicity-in-information\u201d. Now, a strictly increasing sequence of subsets of A, starting from an empty set, cannot have more than |A| + 1 elements. It follows that B\u2113 = B\u2113+1 for some phase \u2113 \u2264 |A|+1. By definition of B\u2113, the sets (B\u2113\u2032 : \u2113 \u2032 \u2265 \u2113) are identical.\nDepending on the state \u03b80, some of the later phases may be redundant in terms of exploration, in the sense that B\u2113 = B\u2113+1 = . . . = B|A|+1 starting from some phase \u2113. Nevertheless, we use a fixed number of phases so as to ensure that the duration is fixed in advance. (This is required by the definition of a subroutine so as to ensure composability, as per Claim 3.8.)\nLemma 4.10. Subroutine RepeatMaxExplore is maximally-exploring.\nProof. We consider an arbitrary BIC iterative recommendation policy \u03c0, and prove that all joint actions explored by \u03c0 are also explored by RepeatMaxExplore.\nDenote the internal random seed in \u03c0 by \u03c9. Without loss of generality, the \u03c9 is chosen once, before the first round, and persists throughout. Let At be the set of joint actions explored by \u03c0 in the first t \u2212 1 rounds. It is a random variable whose realization is determined by the random seed \u03c9 and realized state \u03b80. Let us represent policy \u03c0 as a sequence of (\u03c0\n(t) : t \u2208 [T ]), where each \u03c0(t) is a BIC recommendation policy which inputs signal S\u2217t which consists of the random seed and the observations so far: S\u2217t = (\u03c9,AllInfo(At)), and is deterministic given that signal. In each round t, the recommended joint action is chosen by policy \u03c0(t); it is denoted \u03c0(t)(S\u2217t ).\nWe will prove the following claim using induction on round t:\nfor each round t, there exists phase \u2113 \u2208 N such that \u03c0(t)(S\u2217t ) \u2208 B\u2113. (14)\nFor the induction base, consider round t = 1. Then A1 = \u2205, so the signal is simply S \u2217 1 = (\u03c9,\u22a5).\nNote that the empty signal \u22a5 is at least as informative as the signal S\u22171. Therefore:\n\u03c0(1)(S\u22171) \u2208 EX[S \u2217 1] (by definition of EX[S \u2217 1])\n\u2282 EX[\u22a5] (by Lemma 4.5)\n= B1 (by definition of B1).\nFor the induction step, assume that (14) holds for all rounds t < t0, for some t0. Then At0 \u2282 B\u2113 for some phase \u2113, so signal S\u2113 = AllInfo(B\u2113) for phase \u2113 of RepeatMaxExplore is at least as informative as signal S\u2217t0 = (\u03c9,AllInfo(At0)) for round t0 of policy \u03c0. Therefore:\n\u03c0(t0)(S\u2217t0) \u2208 EX[S \u2217 t0 ] (by definition of EX[S\u2217t0])\n\u2282 EX[S\u2113] (by Lemma 4.5)\n= B\u2113 (by definition of B\u2113).\nThis completes the proof of (14). Therefore policy \u03c0 can only explore joint actions in \u222a\u2113\u2208NB\u2113, and by Claim 4.9 this is just B|A+1|, the set of joint actions explored by RepeatMaxExplore."}, {"heading": "4.3 Putting this all together: proof of Theorem 2.2", "text": "We use the maximally-exploring subroutine RepeatMaxExplore from Section 4.2 (let T0 denote its duration), and an optimal recommendation policy for signal S = AllInfo(Adet\u03b80 ), denote it \u03c0\n\u2217. Let \u03c0 be the composition of subroutine RepeatMaxExplore followed by T \u2212 T0 copies of \u03c0\n\u2217. By Lemma 4.3, this policy has the claimed reward guarantee.\nLet us argue about the computational implementation of \u03c0. For brevity, let us say polytimecomputable to mean \u201ccomputable in time polynomial in |A| \u00b7 |\u0398|\u201d. We need to prove that each round of \u03c0 is polytime-computable. Each round of RepeatMaxExplore is polytime-computable by Claim 4.8, and each round of \u03c0\u2217 is polytime-computable by definition, so it remains to prove that a suitable \u03c0\u2217 is polytime-computable.\nBy Corollary 3.4, policy \u03c0\u2217 is poly-time computable given the signal structure for signal S (because the signal is determined by the realized state \u03b80, and therefore has support of size at most |\u0398|). Recall that S = S|A+1| is the signal computed in the last phase of RepeatMaxExplore. So the corresponding signal structure is poly-time computable using a version of RepeatMaxExplore without the calls to MaxExplore."}, {"heading": "5 Bayesian Exploration with stochastic utilities", "text": "We turn our attention to stochastic utilities. As we pointed out in Section 2.1, we compete against \u03b4-BIC iterative recommendation policies, for a given \u03b4 > 0.6 We construct a BIC policy whose time-averaged expected reward is close to OPT(\u03a0\u03b4), where \u03a0\u03b4 is the class of all \u03b4-BIC policies.\nIn fact, we achieve a stronger result. We will try to compete with a benchmark in the deterministic instance: a version of the original problem instance with deterministic utilities. Our benchmark is OPTdet(\u03a0det\u03b4 ), where \u03a0 det \u03b4 is the class of all policies that are \u03b4-BIC for the deterministic instance, and OPTdet(\u00b7) is the value of OPT(\u00b7) for the deterministic instance. This is indeed a stronger benchmark:\n6Recall that \u03b4-BIC policies satisfy a stronger version of Definition 2.1 in which right-hand side of (1) is \u03b4.\nClaim 5.1. OPTdet(\u03a0det\u03b4 ) \u2265 OPT(\u03a0\u03b4) for all \u03b4 > 0.\nOur main result for stochastic utilities (which implies Theorem 2.3) is stated as follows:\nTheorem 5.2. Consider Bayesian Exploration with stochastic utilities. Then, for any given parameter \u03b4 > 0, there exists an iterative recommendation policy \u03c0 that is BIC and satisfies\nREW(\u03c0) \u2265 (T \u2212C \u00b7 logT ) OPTdet(\u03a0det\u03b4 ).\nwhere C is a constant that depends only on the utility structure and the parameter \u03b4, but not on the time horizon T . The per-round running time of \u03c0 is polynomial in |A| \u00b7 |\u0398|. Moreover, the policy \u03c0 does not input the parameterized utility distributions Da,\u03b8 , (a,\u03b8) \u2208 A\u00d7\u0398.\nOur policy (and regret bounds) depend on an important parameter of the utility structure: the minimal amount of separation between the utilities for two states.\nDefinition 5.3 (separation parameter). The separation parameter is the smallest number \u03b6 > 0 such that for any states \u03b8,\u03b8\u2032 \u2208\u0398, any agent i \u2208 [n], and any joint action a \u2208 A we have\nui(a,\u03b8) , ui(a,\u03b8 \u2032) \u21d2 |ui(a,\u03b8)\u2212 ui(a,\u03b8 \u2032)| \u2265 \u03b6.\nThe main steps in our solution are as follows:\n1. We extend some basic tools and concepts of BIC recommendation policies to \u03b4-BIC recommendation policies.\n2. We give a useful subroutine that approximates the expected utilities using multiple samples of the stochastic utilities. We show that any \u03b4-BIC recommendation policy for the deterministic instance remains BIC when it inputs approximate utilities instead of the expected utilities, as long as the approximation is sufficiently good.\n3. We present a maximally-exploring subroutine, analogous to the one in Section 4, and use it to construct a BIC recommendation policy that achieves logarithmic regret compared to the class of all \u03b4-BIC policies."}, {"heading": "5.1 Preliminaries: from BIC to \u03b4-BIC", "text": "We extend some of the machinery developed in the previous sections to \u03b4-BIC policies. In the interest of space, we sometimes refer to the said machinery rather than spell out the full details.\nWe start with the notions of \u201ceventually-explorable joint action\u201d and \u201cmaximally-exploring subroutine\u201d. (Note that the former only considers the deterministic instance.)\nDefinition 5.4 (\u03b4-explorability). Consider the deterministic instance and fix \u03b4 \u2265 0. A joint action a \u2208 A is called \u03b4-eventually-explorable given a state \u03b8 \u2208 \u0398 if Pr[\u03c0t = a | \u03b8] > 0 for some \u03b4-BIC iterative recommendation policy \u03c0 and some round t \u2208 N. The set of all such joint actions is denotedA\u03b4\u03b8 .\nDefinition 5.5. A subroutine is called \u03b4-maximally-exploring, for a given \u03b4 > 0, if it is a BIC subroutine that inputs an empty signal and explores all joint actions in A\u03b4\u03b80 by the time it stops.\nLet us turn to the recommendation game.\nDefinition 5.6. Consider a recommendation game with signal S , and fix \u03b4 \u2265 0. Policy \u03c0 is called \u03b4-BIC if it satisfies a version of Definition 3.2 in which the right-hand side of (4) is \u03b4 rather than 0. Further:\n\u2022 A joint action a is called \u03b4-signal-explorable, for a particular feasible signal s \u2208 X , if there exists a \u03b4-BIC recommendation policy \u03c0a,s such that Pr[\u03c0a,s(s) = a] > 0. The set of all such joint actions is denoted EX\u03b4s [S]. The \u03b4-signal-explorable set is defined as EX \u03b4 S [S]. \u2022 A \u03b4-BIC policy \u03c0 is called \u03b4-max-support if support(\u03c0(s)) = EX\u03b4s [S] for each signal s \u2208 X .\nThe LP-representation of a \u03b4-BIC recommendation policy satisfies a version of the LP from Section 3.2 where the first constraint expresses \u03b4-BIC condition rather than BIC. Specifically, the right-hand side in the first constraint should be changed from 0 to \u03b4 \u00b7 Pr[\u03c0i(S) = ai ]. In terms of the LP variables, the right-hand side becomes \u03b4 \u00b7\n\u2211 a\u2212i\u2208A\u2212i , s\u2208X , \u03b8\u2208\u0398\n\u03c8(\u03b8) \u00b7\u03c8\u2217(s | \u03b8) \u00b7 xa,s. The feasible region of the modified LP will be denoted BIC\u03b4[S].\nClaim 5.7. A recommendation policy \u03c0 is \u03b4-BIC if and only if its LP-representation lies in BIC\u03b4[S].\nAssuming there exists a \u03b4-BIC policy, a max-support policy can be computed by a version of Algorithm 1, where in the linear program (9) the feasible set is BIC\u03b4[S] rather than BIC[S]. Let us call this modified algorithm ComputeMaxSupport\u03b4. Thus:\nClaim 5.8. If there exists a \u03b4-BIC policy, then there exists a \u03b4-max-support policy\u03c0max with pmin(\u03c0 max) \u2265\n1 |A|\u00b7|X | p\u03b4min[S]. It can be computed by ComputeMaxSupport \u03b4 in time polynomial in |A|, |X |, and |\u0398|.\nHere p\u03b4min[S] is defined as the natural extension of pmin[S] (see (11)), whereby EXs[S] is re-\nplaced with EX\u03b4s [S], and BIC[S] is replaced with BIC\u03b4[S]."}, {"heading": "5.2 Approximate deterministic utilities using stochastic utilities", "text": "Let us discuss how to to approximate the expected utilities using multiple samples of stochastic utilities, and how to use the resulting approximate signal. In particular, we introduce an important tool (Lemma 5.10): a fact about approximate signals which we use throughout this section.\nNotation. Formulating this discussion in precise terms requires some notation. Fix subset B \u2282 A of joint actions. Given state \u03b8, the expected utilities of joint actions in B form a table\nU(B,\u03b8) := ((f (a,\u03b8);u1(a,\u03b8) , . . . ,un(a,\u03b8)) : a \u2208 B) .\nRecall that in Section 3.2, full information pertaining to the joint actions in B is expressed as a signal AllInfo(B) = (B, U(B,\u03b80)), as defined in (5).\nNow, let \u03b8 = \u03b80 be the realized state. Let an IID utility vector for a joint action a \u2208 B be an independent sample from distribution D(a,\u03b8). Recall from Section 2 that such sample is a random vector in [0,1]n+1 whose expectation is U(a,\u03b8). Further, the d-sample for B, d \u2208 N, is a collection UB = (v(a, j) : a \u2208 B,j \u2208 [d]), where each v(a, j) is an IID utility vector for the corresponding joint action a. We would like to use the sample UB to approximate U(B,\u03b8).\nApproximation procedure. Our approximation procedure is fairly natural. We input a d-sample UB for some subset B \u2282 A. Then we compute average utilities across the d samples:\nU = ( 1 d \u2211d j=1 v(a, j) : a \u2208 B ) .\nThen we map these averages to the state \u03b8 which provides the best fit for the averages:\n\u03b8\u0302 = argmin \u03b8\u2208\u0398\n\u2225\u2225\u2225U \u2212U(B,\u03b8) \u2225\u2225\u2225 \u221e ,\nwhere the ties are broken uniformly at random (any fixed tie-breaking rule would suffice).\nWe will represent the output of this procedure as a signal DeNoise(UB) = ( B, U(B,\u03b8\u0302) ) . Note that such signal has the same structure as the \u201ccorrect\u201d signal AllInfo(B), in the sense that DeNoise(UB) \u2208 support(AllInfo(B)), so we can directly compare the two signals.\nProperties of the approximation. First, we observe that the approximate signal DeNoise(UB) is exactly equal to the correct signal AllInfo(B) with high probability, as long as the number of samples is large compared to log(n |B|) and the inverse of the separation parameter \u03b6.\nLemma 5.9. Fix subset B \u2286 A of joint actions. Let UB be a d-sample for B such that d is large enough, namely d \u2265 \u03b6\u22122 \u00b7 ln(2n |B|/\u03b2) for some \u03b2 > 0, where \u03b6 is the separation parameter. Then\nPr[AllInfo(B) = DeNoise(UB) | \u03b80 = \u03b8] \u2265 1\u2212 \u03b2, \u2200\u03b8 \u2208\u0398. (15)\n(This is an easy consequence of Chernoff-Hoeffding Bound, see Appendix A). The crucial point here is that DeNoise(UB) can be used instead of AllInfo(B) in the recommendation game. More specifically, if \u03c0 be a \u03b4-BIC recommendation policy for signal AllInfo(B), then it is also a BIC recommendation policy for signal DeNoise(B), as long as the approximation parameter \u03b2 is sufficiently small compared to the minimal probability pmin(\u03c0).\nLemma 5.10. Consider the setting of Lemma 5.9. Let \u03c0 be a \u03b4-BIC recommendation policy for signal AllInfo(B). Then \u03c0 is also a BIC recommendation policy for signal DeNoise(B), as long as \u03b2 \u2264 \u03b4 \u00b7 pmin(\u03c0)/2|\u0398|.\nWe derive Lemma 5.10 as a corollary of a general fact about approximate signals in a recommendation game, which we develop in the next subsection."}, {"heading": "5.3 Approximate signals in a recommendation game", "text": "We abstract (15) as a property of two coupled signals, and state the corresponding generalization of Lemma 5.10.\nDefinition 5.11. Consider a recommendation game with coupled signals S,S \u2032 , where the two signals have same support X . Signal S \u2032 is called a \u03b2-approximation for S , \u03b2 > 0, if\nPr[S = S \u2032 | \u03b80 = \u03b8] \u2265 1\u2212 \u03b2, \u2200\u03b8 \u2208\u0398,\nwhere the randomness is taken over the realization of (S,S \u2032 ,\u03b80).\nLemma 5.12. Consider the setting of Definition 5.11. Let \u03c0 be a \u03b4-BIC recommendation policy for signal S . Then \u03c0 is also a BIC recommendation policy for signal S \u2032 , as long as \u03b2 \u2264 \u03b4 \u00b7 pmin(\u03c0)/2|X |.\nAs an intermediate step, we will show the following technical lemma.\nLemma 5.13. Consider the setting of Definition 5.11. Let g be a random variable in the same probability space as (S,S \u2032 ,\u03b80), with bounded range [0,H]. Then\n\u2223\u2223\u2223Pr[S = s] \u00b7E [g | S = s] \u2212 Pr[S \u2032 = s] \u00b7E[g | S \u2032 = s] \u2223\u2223\u2223 \u2264 \u03b2H, \u2200s \u2208 X . (16)\nProof. Let E denote the event of (S = S \u2032), which occurs with probability at least 1\u2212\u03b2 by definition. We will also write \u00acE to denote the event of (S , S \u2032), and we know Pr[\u00acE] \u2264 \u03b2. Observe that the event (S \u2032 = s)\u2227E is equivalent to the event (S = s)\u2227E . Also note that g \u2208 [0,H], so for each s \u2208 X , we could write\nPr[(S \u2032 = s)]E[g | (S \u2032 = s)] =Pr[(S \u2032 = s),E]E[g | (S \u2032 = s),E] + Pr[(S \u2032 = s),\u00acE]E[g | (S \u2032 = s),\u00acE]\n\u2264Pr[(S \u2032 = s),E]E[g | (S \u2032 = s),E] + Pr[\u00acE]E[g | (S \u2032 = s),\u00acE]\n=Pr[(S = s),E]E[g | (S = s),E] + \u03b2H\n\u2264Pr[(S = s),E]E[g | (S = s),E] + Pr[(S = s),\u00acE]E[g | (S = s),\u00acE] + \u03b2H\n=Pr[(S = s)]E[g | S = s] + \u03b2H\nSimilarly,\nPr[(S \u2032 = s)]E[g | (S \u2032 = s)] =Pr[(S \u2032 = s),E]E[g | (S \u2032 = s),E] + Pr[(S \u2032 = s),\u00acE]E[g | (S \u2032 = s),\u00acE]\n\u2265Pr[(S \u2032 = s),E]E[g | (S \u2032 = s),E]\n=Pr[(S = s),E]E[g | (S = s),E]\n\u2265Pr[(S = s),E]E[g | (S = s),E] + Pr[(S = s),\u00acE]E[g | (S = s),\u00acE]\u2212 \u03b2H\n\u2265Pr[S = s]E[g | S = s]\u2212 \u03b2H,\nwhich completes the proof.\nProof of Lemma 5.12. For any agent i \u2208 [n] and any joint action a \u2208 A, the utility ui(a,\u03b8) is a random variable with bounded range [0,1]. In particular, we obtain (16) for random variable g = ui (a,\u03b80).\nLet x be the LP-representation of policy \u03c0 from the lemma statement. Pick some action ai \u2208 Ai such that Pr[\u03c0i(S) = ai ] > 0, and some other action a \u2032 i \u2208 Ai \\ {ai }. Denote\nW (a\u2212i ) = ui(ai ,a\u2212i ;\u03b80)\u2212 ui(a \u2032 i ,a\u2212i , ;\u03b80),\nwhere a\u2212i \u2208 A\u2212i is a joint action of all agents but i. Then\n\u2211\na\u2212i\u2208A\u2212i , s\u2208X\nPr[S = s] \u00b7E [W (a\u2212i ) | S = s] xa,s \u2265 \u03b4 \u00b7 \u2211\na\u2212i\u2208A\u2212i , s\u2208X\nPr[S = s] xa,s \u2265 \u03b4 \u00b7 pmin(\u03c0)\nTo show that policy\u03c0 is a BIC policy for signal S \u2032, it suffices to show thatE [W (\u03c0\u2212i (S) | \u03c0i(S) = ai ] is non-negative, and we could write\n\u2211\na\u2212i\u2208A\u2212i ,s\u2208X\nPr[S \u2032 = s]E [ W (a\u2212i ) | S \u2032 = s ] xa,s\n\u2265 \u2211\na\u2212i\u2208A\u2212i ,s\u2208X\n(Pr[S = s]E [W (a\u2212i ) | S = s]\u2212 2\u03b2) xa,s ((16) with g = ui (a,\u03b80))\n= \u2211\na\u2212i\u2208A\u2212i ,s\u2208X\nPr[S = s]E [W (a\u2212i ) | S = s] xa,s \u2212 2\u03b2 |X |\n\u2265\u03b4pmin(\u03c0)\u2212 2\u03b2 |X | \u2265 0 (by our assumption of Lemma 5.10)\nTherefore, we know that \u03c0 is BIC w.r.t. the signal S ."}, {"heading": "5.4 A \u03b4-maximally-exploring subroutine under stochastic utitlities", "text": "In this subsection, we will give our maximal exploration algorithm (with access to stochastic utilities) for exploring all the joint actions explorable by any \u03b4-strictly BIC policy (with access to deterministic utitlities). Our BIC subroutine is largely similar to the one in Section 4.2 except that our recommendation policy will only have access to approximate signals based on stochastic utilities.\nWe will first introduce a BIC subroutine MaxExplore\u03b4 that given an approximation signal S\u0302 to the signal S , explores all the joint actions in the set A\u2032 = EX\u03b4[S]. In the process, the subroutine will collect multiple utility samples of each explored joint action, which will allow us to construct a new signal approximation for the signal AllInfo(A\u2032).\nAlgorithm 4 Subroutine MaxExplore\u03b4(U ,S , S\u0302 ,\u03b2,\u03b2 \u2032): maximal exploration given an approximate signal S\u0302 .\nInput: the utility structure U , the signal structure S with associated signal S , S\u0302 as a \u03b2-signal approximation to S with \u03b2 being the input signal confidence parameter, and the confidence parameter for output signal \u03b2 \u2032\n// compute the parameters let s \u2208 X be the realization of signal S\u0302 x\u2190 ComputeMaxSupport\u03b4(U ,S ) // LP-representation of a maximal-support policy \u03c0max B\u2190 {a :\u2208 A : xa,s > 0} // \u03b4-strictly signal-explorable set EX \u03b4 s [S] pmin \u2190min(xa,s\u2032 : a \u2208 B, s \u2032 \u2208 X ) // computes pmin(\u03c0 max) T \u2190max(1+ |B|, \u23081/pmin\u2309) and R\u2190 1 \u03b62 ln ( 2n|B| \u03b2\u2032 ) // number of rounds and meta-rounds Pick injective function \u03c4 : B\u2192 [T ] u.a.r. from all such functions. // \u201cdedicated rounds\u201d D(a)\u2190 xa,s\u22121/T T\u2212|B| \u2200a \u2208 B. // distribution (13) for non-dedicated rounds\nInitiate a set UB for storing stochastic utilities samples. // issue the recommendations formeta-rounds r = 1 . . .R do for rounds t = 1 . . .T do\nif t = \u03c4(a) for some a \u2208 B then at \u2190 a else choose at from distribution D Recommend action at , store the corresponding utilities in UB\nS \u2032 = (B,U\u0302 )\u2190 DeNoise(B,UB,\u03b2 \u2032), which is a \u03b2 \u2032-signal approximation to AllInfo(S )\nOutput: the signal S \u2032\nWe can first show that the subroutine MaxExplore\u03b4 is BIC under some condition of the signal approximation parameter.\nLemma 5.14. The subroutine MaxExplore\u03b4 is BIC as long as the input signal approximation parameter of S\u0302 satisfies \u03b2 \u2264 \u03b4/(C|X |), where C is some prior-dependent constant.\nProof. Let \u03c0 be the policy computed by ComputeMaxSupport\u03b4(U ,S ) within our instantiation of MaxExplore\u03b4. Then there exists a constantC such that pmin(\u03c0) \u2265 1/C. Note thatComputeMaxSupport \u03b4 is simply a composition ofR copies of\u03c0, so the stated result follows fromClaim 3.8 and Lemma 5.10.\nNote that if our algorithm has access to deterministic utilities, in the end it will be able to construct a signal AllInfo(B), where B is the (random) set of actions explored by the algorithm. We now show that even though our algorithm only has access to stochastic utilities, the output signal will be a \u03b2-signal approximation to AllInfo(B).\nLemma 5.15. The output signal S \u2032 byMaxExplore\u03b4 is a (\u03b2+\u03b2 \u2032)-approximation to the signal AllInfo(EX\u03b4[S]), where \u03b2 and \u03b2 \u2032 are the input and output signal signal approximation parameters respectively.\nProof. Fix any state \u03b8 as our true state \u03b80. First, we know that with probability at least 1 \u2212 \u03b2 over the randomness of S and S\u0302 , we have S = S\u0302 . We will condition on this event, which is the case except with probability \u03b2. This means the subroutine will explore the same subset of joint actions: B = EX\u03b4S [S]. Then, the stated result simply follows from the accuracy guarantee of DeNoise (Lemma 5.9).\nWe now formally introduce our \u03b4-maximally exploring subroutine. Similar to RepeatMaxExplore,\nit will proceed in phases \u2113 = 1,2,3, . . . , |A|. In each phase \u2113, we will use the approximate signal S \u03b2 \u2113 to perform maximal exploration by instantiating the function MaxExplore\u03b4. This in turn will allow us to construct an approximate signal for the next phase. See Algorithm 5 for details of the parameters.\nAlgorithm 5 Subroutine RepeatMaxExplore\u03b4(U ,\u03b2): \u03b4-maximal exploration.\nInput: the utility structure U and confidence parameter \u03b2 Initialize: \u03b2 \u2032 = \u03b2/ |A| and S\u03021 = S1 = \u22a5. // \u201cphase-1 signal\u201d is empty for each phase \u2113 = 1,2 , . . . , |A| do\n// compute phase-(\u2113 +1) approximate signal to S\u2113+1 = AllInfo(EX \u03b4[S\u2113])\nS\u0302\u2113+1 = (B\u2113+1, U\u0302\u2113+1)\u2190MaxExplore \u03b4[U ,S\u2113, S\u0302\u2113 , (\u2113 \u2212 1)\u03b2 \u2032,\u03b2 \u2032] // compute signal structure S\u2113+1 \u2190 (S\u2113+1(\u03b8) : \u03b8 \u2208\u0398), where S\u2113+1(\u03b8)\u2190 AllInfo(EX\n\u03b4[S\u2113]) Output: final signal S\u0302|A|+1\nLemma 5.16. RepeatMaxExplore\u03b4(U ,\u03b2) is BIC as long as the parameter satisfies \u03b2 \u2264 \u03b4/(C|\u0398|), where C is some constant depending on the prior and \u03b4; its per-round running time is polynomial in |A| \u00b7 |\u0398|.\nProof. Note that RepeatMaxExplore\u03b4 could be viewed as a composition of subroutinesMaxExplore\u03b4 such that for each \u2113 \u2208 [|A|] the instantiation in phase (\u2113 + 1) is a valid sequel of the one in phase \u2113. Also observe that the number of realizations for the signal structure S\u2113 in each phase is bounded by the number of states |\u0398|. Then by Lemma 5.14, we know that the instantiation of MaxExplore\u03b4 in each phase is BIC as long as \u03b2 \u2264 O(\u03b4/ |\u0398|), so RepeatMaxExplore\u03b4(U ,\u03b2) is also BIC given the condition on \u03b2.\nFinally, we will show that RepeatMaxExplore\u03b4 is \u03b4-maximally exploring and outputs a \u03b2 approximate signal for AllInfo(A\u03b4\u03b8).\nLemma 5.17. The subroutine RepeatMaxExplore\u03b4 is \u03b4-maximally-exploring with probability at least 1 \u2212 \u03b2 over the randomness of the stochastic utilities, and outputs a signal S\u0302|A|+1 that is a \u03b2-signal approximation to AllInfo(A\u03b4\u03b80).\nProof. Fix any game state \u03b8 as our true state \u03b80. Let \u03c0 be any \u03b4-BIC iterative recommendation policy with access to deterministic utilities. Our goal is to show that all actions explored by \u03c0 are also explored by RepeatMaxExplore\u03b4 with probability at least 1\u2212 \u03b2.\nNote that in each phase \u2113 of the RepeatMaxExplore\u03b4, we know by Lemma 5.15 that except with probability at most \u03b2 \u2032 over the randomness of the stochastic utilities, the output signal satisfies\nS\u0302\u2113+1 = AllInfo(EX \u03b4[S\u2113])\nFor the remainder of the proof, we will condition on this event over all phases of RepeatMaxExplore\u03b4, which by union bound occurs with probability at least 1\u2212 |A|\u03b2 \u2032 = 1\u2212 \u03b2.\nDenote the internal random seed in \u03c0 by \u03c9. We will think of \u03c9 being drawn ahead of first round of our Bayesian exploration game and fixed throughout the game. Let At be the set of actions explored by \u03c0 in the first (t \u2212 1) rounds, which is determined by the realization of \u03c9 and the true state \u03b80 (since \u03c0 sees deterministic utitlities). We will represent \u03c0 as a sequence of( \u03c0(t) : t \u2208 [T ] ) , where each \u03c0(t) is a BIC recommendation policy which inputs the signal S\u2217t which consists of the random seed and the observations so far: S\u2217t = (\u03c9,AllInfo(At)), and is determined given that signal. In each round t, we will denote the recommended joint action by \u03c0(t)(S\u2217t ). We will prove the following claim using induction on round t:\nfor each round t, there exists phase \u2113 \u2208 N such that \u03c0(t)(S\u2217t ) \u2208 B\u2113 , (17)\nwhere B\u2113 is the random set of joint actions explored by the algorithm RepeatMaxExplore \u03b4 in phase \u2113. For the base case (t = 1), we know that A1 = \u2205, so the signal S \u2217 1 = (\u03c9,\u22a5). Since the random seed is independent of the game state, we know that the empty signal (\u22a5) is at least as informative as the signal S\u22171. This means\n\u03c0(1)(S\u22171) \u2208 EX \u03b4[S\u22171] (by Definition 5.6)\n\u2286 EX\u03b4[\u22a5] (by Lemma 4.5)\n= B1 (by definition of B1)\nFor the induction step, assume that (17) holds for all rounds t < t0, for some t0 \u2208 N. This impliesAt0 \u2286 B\u2113 for some phase \u2113 of RepeatMaxExplore\n\u03b4, and so the signal S\u0302\u2113 is at least as informative as the signal S\u2217t = (\u03c9,AllInfo(At0)). It follows that\n\u03c0(t0)(S\u2217t0) \u2208 EX \u03b4[S\u2217t0] (by Definition 5.6)\n\u2286 EX\u03b4[S\u0302\u2113] (by Lemma 4.5)\n= B\u2113 (by definition of B\u2113)\nThis gives a proof for the claim in 17. By the same reasoning of Claim 4.9, the (B\u2113 : \u2113 \u2208 N) are nondecreasing in \u2113, and identical for all \u2113 \u2265 |A|+1, so we have shown that all joint actions explored by \u03c0 must be contained in B|A|+1. It follows that RepeatMaxExplore \u03b4 is \u03b4-maximally exploring given access to stochastic utitlities, and S\u0302\u2113+1 = AllInfo(A \u03b4 \u03b80 )."}, {"heading": "5.5 Putting this all together: proof of Theorem 2.3", "text": "Lastly, we will show how the \u03b4-maximal exploration subroutine will lead to a near-optimal iterative recommendation policy. As an intermediate step, we show that if an exploration subroutine outputs an approximate signal for AllInfo(A\u03b4\u03b80), we can then obtain close to optimal rewards.\nLemma 5.18. Let \u03b4,\u03b2 \u2208 (0,1/2) and \u03c3 be a \u03b4-maximally exploring subroutine with duration T\u03c3 that outputs a \u03b2-signal approximation S\u0302 to the signal S\u2217 = AllInfo(A\u03b4\u03b8). Let \u03c0\n\u2217 be an optimal \u03b4-BIC recommendation policy for signal S\u2217, and \u03c0 be the composition of subroutine \u03c3 followed by T \u2212T\u03c3 copies of \u03c0\u2217(S\u0302). Then \u03c0 is BIC as long as \u03b2 \u2264 \u03b4/(2C|\u0398|), and has reward\nREW(\u03c0) \u2265 (T \u2212T\u03c3 )(1\u2212 \u03b2)OPT det(\u03a0det\u03b4 )\nwhere C is some prior-dependent constant.\nProof. First, we know that there exists some constantC > 0 such that pmin(\u03c0 \u2217) \u2265 1/C. Furthermore, we know that the number of realizations for the signal S\u2217 is at most the number of states |\u0398|. It follows from Lemma 5.10 that \u03c0\u2217 \u2208 BIC(S\u0302), and since \u03c3 is BIC, we also have that \u03c0 is BIC by Claim 3.8.\nIn the following, we will write s\u2217 and s\u0302 to denote the realization of signals S\u2217 and S\u0302 respectively, and use \u03c8\u2217 to denote the joint distribution over the two signals S\n\u2217, S\u0302 and the state \u03b8. By the definition of \u03b2-signal approximation in Definition 5.11, we have for each \u03b8 \u2208\u0398,\n\u03c8\u2217(S\u0302 = S \u2217 | \u03b80 = \u03b8) \u2265 1\u2212 \u03b2.\nFor t > T\u03c3 , we can write the reward at round t as\nE \u03c0,\u03c8\u2217\n[f (\u03c0t(s\u0302),\u03b80)] = \u2211\n\u03b8\u2208\u0398\n\u03c8(\u03b8) E \u03c0,\u03c8\u2217 [f (\u03c0t(s\u0302),\u03b8) | \u03b80 = \u03b8]\n= \u2211\n\u03b8\u2208\u0398\n\u03c8(\u03b8) ( \u03c8\u2217(S\u0302 = S\n\u2217 | \u03b80 = \u03b8) E \u03c0,\u03c8\u2217\n[ f (\u03c0t(s\u0302),\u03b8) | \u03b80 = \u03b8, S\u0302 = S \u2217 ]\n+\u03c8\u2217(S\u0302 , S \u2217 | \u03b8) E\n\u03c0,\u03c8\u2217\n[ f (\u03c0t(s\u0302),\u03b8) | \u03b80 = \u03b8, S\u0302 , S\n\u2217 ])\n\u2265 \u2211\n\u03b8\u2208\u0398\n\u03c8(\u03b8)(1\u2212 \u03b2) E \u03c0,\u03c8\u2217\n[ f (\u03c0t(s\u0302),\u03b8) | \u03b80 = \u03b8, S\u0302 = S \u2217 ]\n= (1\u2212 \u03b2) \u2211\n\u03b8\u2208\u0398\n\u03c8(\u03b8) E \u03c0,\u03c8\u2217\n[ f (\u03c0t(s \u2217),\u03b8) | \u03b80 = \u03b8, S\u0302 = S \u2217 ]\nNote that the random variable E\u03c0,\u03c8\u2217 [f (\u03c0t(s \u2217),\u03b8) | \u03b80 = \u03b8] is independent of 1[S\u0302 = S \u2217] since the reward is fully determined by the state and the randomness of \u03c0. In particular, for each \u03b8 \u2208\u0398, we have\nE \u03c0,\u03c8\u2217\n[ f (\u03c0t(s \u2217),\u03b8) | \u03b80 = \u03b8, S\u0302 = S \u2217 ] = E\n\u03c0 [f (\u03c0t(S\n\u2217(\u03b8),\u03b8))]\nIn other words,\nE \u03c0,\u03c8\u2217\n[f (\u03c0t(s\u0302),\u03b8)] \u2265 (1\u2212 \u03b2) \u2211\n\u03b8\u2208\u0398\n\u03c8(\u03b8)E \u03c0 [f (\u03c0t(S\n\u2217(\u03b8),\u03b8))] = (1\u2212 \u03b2)REW\u2217[S\u2217]\nNote that we also have REW\u2217[S\u2217] \u2265 OPTdet(\u03a0det\u03b4 ) by Claim 3.7. Therefore, in the last (T \u2212T\u03c3) rounds, the expected reward of \u03c0 is at least (T \u2212T\u03c3 )(1\u2212 \u03b2)OPT det(\u03a0det\u03b4 ).\nTo establish the result in Theorem 2.3, we will instantiate RepeatMaxExplore\u03b4(U ,\u03b2) as our \u03b4maximally exploring subroutine and works out the duration T\u03c3 \u2019s dependence on the confidence parameter \u03b2.\nRecall that RepeatMaxExplore\u03b4 consists of a total number of |A| phases, and in each phase it instantiates the subroutine MaxExplore\u03b4 with output signal approximation parameter \u03b2 \u2032 = \u03b2/ |A|, which takes ln(1/\u03b2)poly(|A|, |\u0398|) rounds, where poly denotes some polynomial on two variables. It follows that RepeatMaxExplore\u03b4(U ,\u03b2) also has duration at most ln(1/\u03b2)poly(|A|, |\u0398|) number of rounds.\nTherefore, if we instantiate \u03c3 = RepeatMaxExplore\u03b4 with confidence parameter \u03b2 = 1/T and followed by the optimal policy \u03c0\u2217 for the signal S\u2217 =A\u03b4\u03b8 for the remaining T \u2212T\u03c3 rounds, the total reward of \u03c0 satisfies\nREW(\u03c0) \u2265 (T \u2212 ln(T )poly(|A|, |\u0398|)) (1\u2212 1/T )OPTdet(\u03a0det\u03b4 ) \u2265 (T \u2212C ln(T ))OPT det(\u03a0det\u03b4 )\nwhere C is some constant depending on the prior and game specification. This recovers our stated bound of Theorem 2.3."}, {"heading": "6 Monotonicity in information: proof of Lemma 3.6 and Lemma 4.5", "text": "In this section we consider a recommendation game with coupled signals, and prove the following lemma which unifies Lemma 3.6 and Lemma 4.5.\nLemma 6.1 (monotonicity-in-information). Consider a recommendation game with coupled signals S,S \u2032 such that S is at least as informative than S \u2032 . Then REW\u2217[S] \u2265 REW\u2217[S \u2032] and EX[S \u2032] \u2282 EX[S].\nThroughout this section, signals S,S \u2032 are as in Lemma 6.1, and X ,X \u2032 are their respective supports. All expectations are over the random choice of (S,S \u2032 ,\u03b80), and also the internal randomness of recommendation policies (if applicable).\nWe will use the notion of \u201cat least as informative\u201d via the following corollary:\nClaim 6.2. For any function h :\u0398 \u2192 R and any feasible signal s \u2208 S,s\u2032 \u2208 S \u2032\nE[h(\u03b80) | S = s] = E[h(\u03b80) | S = s,S \u2032 = s\u2032].\nGiven a policy \u03c0\u2032 for signal S \u2032, one can define the induced policy \u03c0 for signal S by setting\nPr[\u03c0(s) = a] = Pr[\u03c0\u2032(S \u2032) = a | S = s] \u2200a \u2208 A, s \u2208 X .\nWe use Claim 6.2 to derive a more elaborate technical property: essentially, that policies \u03c0 and \u03c0\u2032 are equivalent when applied to any given function of h :A\u00d7\u0398 \u2192 R.\nClaim 6.3. Let \u03c0\u2032 be an arbitrary recommendation policy for signal S \u2032, and let \u03c0 be the induced policy for signal S . Then for any function h :A\u00d7\u0398 \u2192 R,\nE[h(\u03c0(S),\u03b80)] = E[h(\u03c0 \u2032(S \u2032),\u03b80)]. (18)\nProof. Fix a feasible signal s \u2208 X . Assume for now that policy \u03c0\u2032 is deterministic. Then for any joint action a \u2208 X we have\nPr[\u03c0(s) = a | S = s]\n= Pr[\u03c0(s) = a] (because randomization in \u03c0 is independent) = Pr[\u03c0\u2032(S \u2032) = a | S = s] (by definition of induced policy) = \u2211\ns\u2032\u2208X \u2032\nPr[\u03c0\u2032(s\u2032) = a] \u00b7Pr[S \u2032 = s\u2032 | S = s]\n= \u2211\ns\u2032\u2208X \u2032 : \u03c0\u2032(s\u2032 )=a\nPr[S \u2032 = s\u2032 | S = s] (since \u03c0\u2032 is deterministic). (19)\nTherefore,\nE[h(\u03c0(S),\u03b80) | S = s]\n= \u2211\na\u2208A\nE[h(a,\u03b80) | S = s] \u00b7Pr[\u03c0(s) = a | S = s]\n= \u2211\na\u2208A, s\u2032\u2208X \u2032 : \u03c0\u2032(s\u2032 )=a\nE[h(a,\u03b80) | S = s] \u00b7Pr[S \u2032 = s\u2032 | S = s] (by (19))\n= \u2211\ns\u2032\u2208X \u2032\nE[h(\u03c0\u2032(s\u2032),\u03b80) | S = s] \u00b7Pr[S \u2032 = s\u2032 | S = s]\n= \u2211\ns\u2032\u2208X \u2032\nE[h(\u03c0\u2032(s\u2032),\u03b80) | S = s, S = s \u2032] \u00b7Pr[S \u2032 = s\u2032 | S = s] (by Claim 6.2)\n= E[h(\u03c0\u2032(s\u2032),\u03b80) | S = s].\nTaking expectations over the realizations of signal S , we obtain the lemma (namely, (18)) for the special case when policy \u03c0\u2032 is deterministic. For a randomized policy \u03c0\u2032, we obtain the lemma by taking expectation over all possible realizations of \u03c0\u2032.\nWe use Claim 6.3 in two ways: to argue about the rewards and to argue about BIC.\nCorollary 6.4. Let \u03c0\u2032 be a policy for signal S \u2032, and let \u03c0 be the induced policy for signal S . (a) REW(\u03c0) \u2265 REW(\u03c0\u2032). (b) If \u03c0\u2032 is BIC, then the induced policy \u03c0 is BIC, too.\nProof. For part (a), simply use Claim 6.3 with function h(a,\u03b8) = f (a,\u03b8). For part (b), fix agent i and any two actions ai , a\u0303i \u2208 Ai . Let us use the following shorthand:\n\u2206u(b,\u03b8) := ui ((ai ,b\u2212i ),\u03b8)\u2212 ui ((a\u0303i ,b\u2212i ),\u03b8) , b \u2208 A, \u03b8 \u2208\u0398.\nUse Claim 6.3 with function\nh(b,\u03b8) := 1{bi=ai } \u00b7\u2206u(b,\u03b8), b \u2208 A, \u03b8 \u2208\u0398.\nIt follows that\nE [ 1{\u03c0i(S)=ai } \u00b7\u2206u(\u03c0(S);\u03b80) ] = E [ 1{\u03c0\u2032i (S \u2032)=ai } \u00b7\u2206u(\u03c0 \u2032(S \u2032);\u03b80) ] .\nConsequently, since Pr[\u03c0i(S) = ai ] = Pr[\u03c0 \u2032 i(S \u2032) = ai], we have\nE [\u2206u(\u03c0(S);\u03b80) | \u03c0i(S) = ai ] = E [ \u2206u(\u03c0\u2032(S \u2032);\u03b80) | \u03c0 \u2032 i(S \u2032) = ai ] ,\nwhenever Pr[\u03c0i(S) = ai ] > 0. The right-hand side of this equation is non-negative because policy \u03c0\u2032 is BIC for signal S \u2032. Since this holds for any agent i and any two actions ai , a\u0303i \u2208 Ai , the induced policy \u03c0 is BIC, too.\nProof of Lemma 6.1. To prove that REW\u2217[S] \u2265 REW\u2217[S \u2032], let \u03c0\u2032 be an optimal policy for signal S \u2032 , and \u03c0 be the corresponding induced policy for signal S . Then by Corollary 6.4 it follows that policy \u03c0 is BIC and has the same expected reward; therefore, REW\u2217[S] \u2265 REW(\u03c0) = REW(\u03c0\u2032) = REW\u2217[S \u2032].\nTo prove that EX[S \u2032] \u2282 EX[S], let \u03c0\u2032 be a maximal-support policy for signal S \u2032, and \u03c0 be the corresponding induced policy for signal S . Policy \u03c0 is BIC by Corollary 6.4(b). Moreover, for all feasible signals s \u2208 X , s\u2032 \u2208 X \u2032 such that Pr[S \u2032 = s\u2032 | S = s] > 0 we have:\nEXs[S] = {a \u2208 A : Pr[\u03c0(s) = a] > 0}\n= {a \u2208 A : Pr[\u03c0\u2032(S \u2032) = a | S = s] > 0} \u2283 {a \u2208 A : Pr[\u03c0\u2032(s\u2032) = a] > 0} = EXs\u2032 [S \u2032].\nIt follows that EX[S] \u2283 EX[S \u2032], as claimed."}, {"heading": "7 Conclusions and open questions", "text": "We introduce a model which captures incentivizing exploration in Bayesian games, and resolve the first-order issues in this model: explorability and constant/logarithmic regret. Our policies are computationally efficient: the per-round running time is polynomial in the input size under a generic game representation.\nOur results pave the way for futurework in several directions. Themost immediate direction is computational: can we achieve polynomial per-round running time if the game has a succinct representation? A simultaneous paper (Dughmi and Xu, 2016) studies a similar question for Bayesian Persuasion. In terms of statistical guarantees, one may want to improve the regret bounds, e.g., reduce the dependence of the asymptotic constants on the number of joint actions and parameters of the prior. In the economics direction, it is appealing to address agent heterogeneity by incorporating idiosyncratic signals that can be observed and/or elicited by the principal.\nAcknowledgements. The authors wish to thankDirk Bergemann, Yeon-Koo Che, ShaddinDughmi, Johannes Horner, Bobby Kleinberg, and Stephen Morris for stimulating discussions on Bayesian Exploration and related topics."}, {"heading": "A Approximating the expected utilities: proof of Lemma 5.9", "text": "We will use the Chernoff-Hoeffding Bound, a standard result on concentration of measure.\nLemma A.1 (Chernoff-Hoeffding Bound). Let X1, . . . ,Xn be i.i.d. random variables with EX[Xi] = \u00b5 and a \u2264 Xi \u2264 b for all i. Then for every \u03b4 > 0,\nPr [\u2223\u2223\u2223\u2223\u2223 \u2211 iXi n \u2212\u00b5 \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b4 ] \u2264 2exp ( \u22122\u03b42n (b \u2212 a)2 ) .\nTo show that S \u2032 is a \u03b2-approximation for S , we will first show that for any fixed state \u03b8, with probability at least 1\u2212\u03b2 over the realization of the stochastic utilities, the realizations of S \u2032 matches with S . In particular, we will show that the average utilities U = (ui(a))i\u2208[n],a\u2208A are close to the expected utilities ui(a,\u03b8). Note that each realized utilities has bounded range u j i (a) \u2208 [0,1], by Chernoff-Hoeffding bound and an application of union bound, we know with probability at least 1\u2212 \u03b2, the average utilities satisfy\nfor all i \u2208 [n],a \u2208 B, |ui (a)\u2212 ui(a)| \u2264\n\u221a 1\n2d ln\n( 2|B|n\n\u03b2\n)\nNote that for d \u2265 1 \u03b62 ln ( 2|B|n \u03b2 ) , then we will have |ui(a) \u2212 ui(a)| < \u03b6/2 for all i \u2208 [n],a \u2208 B with\nprobability at least 1 \u2212 \u03b2. We will condition on this event. This means \u2225\u2225\u2225U \u2212U(\u03b8) \u2225\u2225\u2225 \u221e\n< \u03b6, so U\u0302 = U(\u03b8) and therefore, S \u2032 = S ."}], "references": [{"title": "Incentivizing exploration", "author": ["Peter Frazier", "David Kempe", "Jon M. Kleinberg", "Robert Kleinberg"], "venue": null, "citeRegEx": "Frazier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Frazier et al\\.", "year": 2016}, {"title": "Strategic Experimentation with Exponential Ban", "author": ["Keller", "Sven Rady", "Martin Cripps"], "venue": null, "citeRegEx": "Keller et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keller et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Frazier et al. (2014) and Che and H\u00f6rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Frazier et al. (2014) and Che and H\u00f6rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).", "startOffset": 0, "endOffset": 48}, {"referenceID": 0, "context": "Frazier et al. (2014) and Che and H\u00f6rner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards). Bayesian Exploration is closely related to three prominent subareas of theoretical economics andmachine learning: multi-armed bandits, design of information structures, and strategic learning in games. We briefly survey these connections below. Multi-armed bandits (Bubeck and Cesa-Bianchi, 2012) is a well-studiedmodel for explorationexploitation tradeoff. Absent the BIC constraint (and assuming the agents always follow the recommendations) Bayesian Exploration reduces to the multi-armed bandit problem with action set A and rewards equal to the principal\u2019s utility. The recommendation game (a single round of Bayesian Exploration) is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents, where the signal observed by the principal is distinct from, but correlated with, the unknown \u201cstate\u201d. Our analysis of this game contributes to the line of work on Bayesian Persuasion and, more generally, on the design of information structures, see (Bergemann and Morris, 2016; Taneva, 2016) for background and references, and Dughmi and Xu (2016) for a more algorithmic perspective.", "startOffset": 0, "endOffset": 1358}], "year": 2017, "abstractText": "We consider a ubiquitous scenario in the Internet economywhen individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before to the ones that arrive later, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents\u2019 incentives and minimizes a suitable notion of regret. We extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game, receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment. Our main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (where the constant depends on the prior distribution, but not on the time horizon), and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions, the actions which some incentive-compatible policy can recommend with non-zero probability. We show how the principal can identify (and explore) all explorable actions, and use the revealed information to perform optimally. In particular, our results significantly improve over the prior work on the special case of a single agent per round, which relies on assumptions to guarantee that all actions are explorable. Interestingly, we do not require the principal\u2019s utility to be aligned with the cumulative utility of the agents; instead, the principal can optimize an arbitrary notion of per-round reward. \u2217Microsoft Research and Tel Aviv University. mansour@microsoft.com \u2020Microsoft Research, New York, NY, USA. slivkins@microsoft.com \u2021Microsoft Research, New York, NY, USA. vasy@microsoft.com \u00a7University of Pennsylvania, Philadelphia, PA, USA. wuzhiwei@cis.upenn.edu", "creator": "LaTeX with hyperref package"}}}