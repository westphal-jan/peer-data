{"id": "1602.07576", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Group Equivariant Convolutional Networks", "abstract": "we introduce group equivariant classical convolutional neural networks ( g - cnns ), a natural generalization property of convolutional composite neural networks that reduces sample complexity drastically by exploiting symmetries. by convolving over groups larger than the translation symmetric group, g - cnns build representations that are equivariant to these groups, which arguably makes it visually possible to greatly increase the degree value of parameter dimension sharing. we show how g - cnns can confidently be implemented with negligible computational overhead for studying discrete groups such as the group of translations, reflections and rotations by multiples of 90 degrees. g - cnns potentially achieve state of achieving the common art than results on rotated mnist and significantly improve over a competitive baseline on augmented and non - augmented cifar - 10.", "histories": [["v1", "Wed, 24 Feb 2016 16:17:15 GMT  (105kb)", "http://arxiv.org/abs/1602.07576v1", null], ["v2", "Fri, 11 Mar 2016 18:26:26 GMT  (106kb)", "http://arxiv.org/abs/1602.07576v2", null], ["v3", "Fri, 3 Jun 2016 10:54:16 GMT  (454kb)", "http://arxiv.org/abs/1602.07576v3", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["taco cohen", "max welling"], "accepted": true, "id": "1602.07576"}, "pdf": {"name": "1602.07576.pdf", "metadata": {"source": "CRF", "title": "Group Equivariant Convolutional Networks", "authors": ["Taco S. Cohen"], "emails": ["T.S.COHEN@UVA.NL", "M.WELLING@UVA.NL"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n07 57\n6v 1\n[ cs\n.L G\n] 2\n4 Fe"}, {"heading": "1. Introduction", "text": "Deep convolutional neural networks (CNNs, convnets) have proven to be very powerful models of sensory data such as images, video, and audio. Although a strong theory of neural network design is currently lacking, a large amount of empirical evidence supports the notion that both convolutional weight sharing and depth (among other factors) are important for good predictive performance.\nConvolutional weight sharing is effective because there is a translation symmetry in most vision problems: the label function and the data distribution are both approximately invariant to shifts. By using the same weights to analyze or model each part of the image, a convolution layer uses far fewer parameters than a fully connected layer, while preserving the capacity to learn many useful transformations.\nCopyright 2016 by the author(s).\nConvolution layers can be used effectively in a deep network because all the layers in such a network are translation equivariant: shifting the image and then feeding it through a number of layers is the same as feeding the original image through the same layers and then shifting the feature maps (at least up to edge-effects). In other words, the symmetry (translation) is preserved by each layer, which makes it possible to exploit it not just in the first but also in higher layers of the network.\nIn this paper we show how convolutional networks can be generalized to exploit larger groups of symmetries, including rotations and reflections. The notion of equivariance is key to this generalization, so in section 2 we provide an informal discussion of this concept and its role in deep representation learning more broadly. After discussing related work in section 3, we recall a number of mathematical concepts in section 4 that are useful for defining G-CNNs.\nIn section 5 we analyze the equivariance properties of standard CNNs, showing that they are equivariant to translations but may fail to equivary with more general isometries such as rotation. With this understanding we can define G-CNNs (in section 6) by complete analogy to standard CNNs, the latter being the G-CNN for the translation group G = Z2. In section 7 we derive the backpropagation equations for G-CNNs, which leads us to an elegant \u201cG-conv calculus\u201d. Having discussed the mathematical theory of GCNNs, in section 8 we provide concrete implementation details for implementing group convolutions.\nIn section 9 we report on our experiments with rotated MNIST, where G-CNNs significantly outperform the previous state of the art, and CIFAR-10, where we show a considerable improvement over a competitive baseline (the All-CNN-C architecture of (Springenberg et al., 2015)), thus outperforming DropConnect (Wan et al., 2013), Maxout (Goodfellow et al., 2013), Network-in-network (Lin et al., 2014), deeply supervised nets (Lee et al., 2015). In section 10 we discuss these results before concluding in\nsection 11."}, {"heading": "2. Equivariant Representations", "text": "Deep neural networks produce a sequence of progressively more abstract representations by mapping the input through a series of parameterized functions (LeCun et al., 2015). In the current generation of neural networks, the representation spaces are usually endowed with very minimal internal structure, such as that of a linear space Rn. Although vector addition has been shown to be surprisingly effective at modeling semantic relationships of words (Mikolov et al., 2013), it has inherent limitations such as being commutative and non-periodic.\nIn this paper we construct representations that have the structure of a linear G-space, for some group G. This means that an object or feature represented by the network has a pose associated with it, the pose being a transformation that takes a canonical reference frame into the local frame of the object or feature.\nThis additional structure allows us to model data more efficiently: A filter in a G-CNN detects co-occurrences of features that have the preferred relative pose, and can match such a feature constelation in any global pose through an operation called the G-convolution.\nA representation space obtains its structure from other representation spaces to which it is connected. This idea is formalized by the concept of equivariance:\nT \u2032g \u03a6(x) = \u03a6(Tg x), (1)\nwhich says that transforming an input x by a transformation g (forming Tg x) and then passing it through a network or layer \u03a6 should give the same result as first mapping x through the network and then transforming the representation. If eq. 1 is satisfied, the layer \u03a6 is called an equivariant map, and the representations are said to be homomorphic as G-spaces.\nEquivariance can be realized in many ways, and in particular the operators T and T \u2032 need not be the same. The only requirement for T and T \u2032 is that for any two transformations g and h that can be composed to form gh, we have T (gh) = T (g)T (h) (i.e. they define group homomorphism or representation of the the group).\nFrom equation 1 we see that invariance is a special kind of equivariance where T \u2032g is the identity transformation for all g (observe that such T \u2032 is indeed a group homomorphism). In deep learning, general equivariance is more useful than invariance because it is impossible to determine if low-level features are in the right spatial configuration if they are invariant.\nBesides improving statistical efficiency and facilitating ge-\nometrical reasoning, equivariance to symmetry transformations constrains the network in a way that can aid generalization. A network \u03a6 can be non-injective, meaning that non-identical vectors x and y in the input space become identical in the output space (for example, two instances of a face may be mapped onto a single abstract vector indicating the presence of any face). However, if \u03a6 is equivariant, then the G-transformed inputs Tg x and Tg y must also be mapped to the same output."}, {"heading": "3. Related Work", "text": "Lenc & Vedaldi (2015) show that the AlexNet CNN (Krizhevsky et al., 2012) trained on imagenet spontaneously learns representations that are equivariant to flips, scaling and rotation. This supports the idea that equivariance is a good inductive bias for deep networks.\nThere is a large literature on learning invariant representations, which can be achieved by pose normalization (Lowe, 2004; Jaderberg et al., 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks.\nMore closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation.\nBy diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012). Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al., 2011).\nAgrawal et al. (2015) show that equivariance to ego-motion can be learned, and that this can be used as an unsupervised\nsignal from which useful representations can be learned."}, {"heading": "4. Mathematical Framework", "text": "In this section we define a few mathematical concepts that we will use in defining and analyzing group equivariant convolutional networks. We begin by defining symmetry groups, and study in particular two groups that are used in the G-CNNs we have built so far. Then we take a look at feature maps, which we model as vector-valued functions on a group (a standard CNN feature map being a vectorvalued function on the group of integer translations, Z2)."}, {"heading": "4.1. Symmetry Groups", "text": "A symmetry of some (mathematical) object is a transformation that leaves that object invariant. For example, if we take the sampling grid of our image, Z2, and flip it over we get \u2212Z2 = {(\u2212n,\u2212m) | (n,m) \u2208 Z2} = Z2. So the flipping operation is a symmetry of the sampling grid. i If we have two symmetry transformations g and h and we compose them, the result gh is another symmetry transformation (i.e. it leaves the object invariant as well). Furthermore, the inverse transformation g\u22121 for any symmetry is also a symmetry. In other words, the set of all symmetries of an object is closed under composition and inverses. A set of transformations with these properties is called a symmetry group.\nOne simple example of a group is the set of 2D integer translations, Z2. Here the group operation (\u201ccomposition of transformations\u201d) is addition: (n,m) + (p, q) = (n + p,m + q). One can verify that the sum of two translations is again a translation, and that the inverse (negative) of a translation is a translation, so this is indeed a group.\nAlthough it may seem fancy to call 2-tuples of integers a group, this is helpful in our case because as we will see in section 6, a useful notion of convolution can be defined for functions on any discrete group, of which Z2 is only one example. The properties of this G-convolution, such as equivariance, arise primarily from the group structure."}, {"heading": "4.2. The group p4", "text": "The group p4 consists of all compositions of translations and rotations by 90 degrees about any center of rotation in a square grid. A convenient parameterization of this group in terms of three integers r, u, v is\ng(r, u, v) =\n\n cos (r\u03c0/2) \u2212 sin(r\u03c0/2) u sin(r\u03c0/2) cos(r\u03c0/2) v\n0 0 1\n\n , (2)\nwhere 0 \u2264 r < 4 and (u, v) \u2208 Z2. The group operation is given by matrix multiplication.\nThe group operation and inversion can also be represented directly in terms of integers (r, u, v), but the equations are cumbersome. Hence, our preferred method of composing two group elements represented by integer tuples is to convert them to matrices, multiply these matrices, and then convert the resulting matrix back to a tuple of integers (using the atan2 function to obtain r).\nFinally, the group acts on points in Z2 (pixel coordinates) by multiplying the matrix g(r, u, v) by the homogeneous coordinate vector x(u\u2032, v\u2032) of a point (u\u2032, v\u2032):\ngx \u2243\n\n cos(r\u03c0/2) \u2212 sin(r\u03c0/2) u sin(r\u03c0/2) cos(r\u03c0/2) v\n0 0 1\n\n\n\n\nu\u2032 v\u2032 1\n\n (3)\n4.3. The group p4m\nThe group p4m consists of all compositions of translations, mirror reflections, and rotations by 90 degrees about any center of rotation in the grid. Like p4, we can parameterize this group by integers:\ng(m, r, u, v) =\n\n\n(\u22121)m cos( r\u03c0 2 ) \u2212(\u22121)m sin( r\u03c0 2 ) u\nsin( r\u03c0 2 ) cos( r\u03c0 2 ) v\n0 0 1\n\n ,\nwhere m \u2208 {0, 1}, 0 \u2264 r < 4 and (u, v) \u2208 Z2. The reader may verify that this is indeed a group.\nAgain, composition is most easily performed using the matrix representation. Computing r, u, v from a given matrix g can be done using the same method we use for p4, and for m we have m = 1\n2 (1\u2212 det(g))."}, {"heading": "4.4. Feature maps", "text": "We model images and stacks of feature maps of a conventional CNN as functions f : Z2 \u2192 RK supported on a bounded domain supp(f) (typically a square centered on the origin). We think of the integers (p, q) in Z2 as the coordinates of the pixels. At each sampling point, a stack of feature maps returns a K-dimensional vector, where K is the number of color channels of the input image, or the number of feature maps in higher layers.\nAlthough the arrays used to store the feature maps must always have a finite size, modeling feature maps as functions that extend to infinity (being non-zero on a finite region only) simplifies our mathematical analysis.\nWe will be concerned with transformations of the feature maps, so we introduce the following notation for a transformation g acting on a set of feature maps:\n[Lgf ](x) = [f \u25e6 g \u22121](x) = f(g\u22121x) (4)\nComputationally, this says that to get the value of the gtransformed feature map Lgf at the point x, we need to do\na lookup in the original feature map f at the point g\u22121x, which is the unique point that gets mapped to x by g. This operatorLg is a concrete instantiation of the transformation operator Tg referenced in section 2, and one may verify that\nLgLh = Lgh. (5)\nIf g represents a pure translation t = (u, v) \u2208 Z2 then g\u22121x simply means x\u2212 t. The inverse on g in equation 4 makes sure that the function is shifted in the positive direction when using a positive translation, and that Lg satisfies the criterion for being a homomorphism (eq. 5) even for transformations g and h that do not commute (i.e. gh 6= hg).\nWe note that Lg is a unitary operator. That is, the standard inner product on the space of feature maps,\n\u3008f, f \u2032\u3009 = \u2211\nx\u2208Z2\nK \u2211\nk=1\nfk(x)f \u2032 k(x) (6)\nis invariant to Lg:\n\u3008Lgf, Lgf \u2032\u3009 = \u3008f, f \u2032\u3009,\n\u3008Lgf, f \u2032\u3009 = \u3008f, Lg\u22121f\n\u2032\u3009 (7)\nTo see this, note that Lg is a bijection, and a sum over the whole group Z2 (eq. 6) does not depend on the order of the summands.\nAs will be explained in section 6.1, feature maps in a GCNN are functions on G instead of functions on Z2. For such feature maps, the definition of Lg, as well as the definition and invariance of the inner product are still valid if we simply replace x (an element of Z2) by h (an element of G).\nIt is easy to visualize the transformation of a planar feature map f : Z2 \u2192 R, but we are not used to visualizing functions on groups. To visualize a feature map on p4, we plot the four patches associated with the four pure rotations on a circle, as shown in figure 1 (left).\nThe red arrows in fig. 1 correspond to right-multiplication by the 90 degree rotation r (which corresponds to incrementing the rotation coordinate while leaving translation\ncoordinates unchanged). We emphasize that the patches that are connected by arrows are not supposed to be rotated copies of eachother; a function on p4 can take arbitrary values at each value of the rotation coordinate. However, when we apply the rotation r to such a function, each planar patch follows the red r-arrow and simultaneously undergoes a 90-degree rotation. The result of this operation is shown on the right of figure 1.\nFor p4m, we can make a similar plot, as shown in figure 2. A p4m has 8 planar patches, each one associated with a mirroring m and rotation r. This group is generated by r and m, so in addition to the red r-arrows there are now blue m-lines that connect patches related by a right-mirroring.\nUpon rotation of a p4m function, each patch again follows its red r-arrows and undergoes a 90 degree rotation. Under a mirroring, the patches connected by a blue line will change places and undergo the mirroring transformation.\nThis rich transformation structure arises from the group composition of p4 and p4m, combined with equation 4 which describes the transformation of a function on a group. In section 6, we show that feature maps that result from G-convolution do indeed have this transformation law.\nFinally, we define the involution of a feature map, which will appear in section 7 where we compute the gradient of the G-convolution, and in section 6.1 when we study the behaviour of the G-convolution itself. We have:\nf\u2217(g) = f(g\u22121) (8)\nFor Z2 feature maps the involution is just a reflection, but for G-feature maps the meaning depends on the structure of G. In all cases, f\u2217\u2217 = f ."}, {"heading": "5. Equivariance properties of CNNs", "text": "In this section we recall the definitions of the convolution and correlation operations used in conventional CNNs, and show that these operations are equivariant to translations but not to other transformations such as rotation. This is certainly well known and easy to see by mental visualization, but deriving it explicitly will make it easier to follow\nthe derivitation of group equivariance of the group convolution defined in the next section.\nAt each layer l, a regular convnet takes as input a stack of feature maps f : Z2 \u2192 RKl and convolves or correlates it with a set of Kl+1 filters \u03c8i : Z2 \u2192 RKl :\n[f \u2217 \u03c8i](x) = \u2211\ny\u2208Z2\nKl \u2211\nk=1\nfk(y)\u03c8 i k(x \u2212 y)\n[f \u22c6 \u03c8i](x) = \u2211\ny\u2208Z2\nKl \u2211\nk=1\nfk(y)\u03c8 i k(y \u2212 x)\n(9)\nIf one employs convolution (\u2217) in the forward pass, the correlation (\u22c6) will appear in the backward pass when computing gradients, and vice versa (see section 7). We will use the correlation in the forward pass.\nUsing the substitution y \u2192 y+ t, and leaving out the summation over feature maps for clarity, we see that a translation followed by a correlation is the same as a correlation followed by a translation:\n[[Ltf ] \u22c6 \u03c8](x) = \u2211\ny\nf(y \u2212 t)\u03c8(y \u2212 x)\n= \u2211\ny\nf(y)\u03c8(y + t\u2212 x)\n= \u2211\ny\nf(y)\u03c8(y \u2212 (x\u2212 t))\n= [Lt[f \u22c6 \u03c8]](x).\n(10)\nAnd so we say that \u201ccorrelation is an equivariant map for the translation group\u201d, or that \u201ccorrelation and translation commute\u201d. Using an analogous computation one can show that also for the convolution, [Ltf ] \u2217 \u03c8 = Lt[f \u2217 \u03c8].\nAlthough convolutions are equivariant to translation, they are not equivariant to other isometries of the sampling lattice. For instance, rotating the image and then convolving with a fixed filter is not the same as first convolving and then rotating the result:\n[[Lrf ] \u22c6 \u03c8](x) = \u2211\ny\n\u2211\nk\nfk(r \u22121y)\u03c8k(y \u2212 x)\n= \u2211\ny\n\u2211\nk\nfk(y)\u03c8k(ry \u2212 x)\n= \u2211\ny\n\u2211\nk\nfk(y)\u03c8k(r(y \u2212 r \u22121x))\n= \u2211\ny\n\u2211\nk\nfk(y)Lr\u22121\u03c8(y \u2212 r \u22121x))\n= Lr[f \u22c6 [Lr\u22121\u03c8]](x)\n(11)\nIn words, this says that the correlation of a rotated image Lrf with a filter \u03c8 is the same as the rotation by r of the\noriginal image f convolved with the inverse-rotated filter Lr\u22121\u03c8. Hence, if an ordinary CNN learns rotated copies of the same filter, the stack of feature maps is equivariant, although individual feature maps are not."}, {"heading": "6. Group Equivariant Networks", "text": "In this section we will define the three layers used in a GCNN (G-convolution, G-pooling, non-linearity) and show that each one commutes with G-transformations of the domain of the image."}, {"heading": "6.1. G-Equivariant correlation", "text": "The correlation (eq. 9) is computed by shifting a filter and then computing a dot product with the feature maps. By replacing the shift by a more general transformation from some group G, we get the G-correlation used in the first layer of a G-CNN:\n[f \u22c6 \u03c8](g) = \u2211\ny\u2208Z2\n\u2211\nk\nfk(y)\u03c8k(g \u22121y). (12)\nNotice that both the input image f and the filter \u03c8 are functions of the plane Z2, but the feature map f \u22c6\u03c8 is a function on the discrete group G (which may contain translations as a subgroup). Hence, for all layers after the first, the filters \u03c8 must also be functions on G, and the correlation operation becomes\n[f \u22c6 \u03c8](g) = \u2211\nh\u2208G\n\u2211\nk\nfk(h)\u03c8k(g \u22121h). (13)\nThe equivariance of this operation is derived in complete analogy to eq. 10, now using the substitution h \u2192 uh:\n[[Luf ] \u22c6 \u03c8](g) = \u2211\nh\u2208G\n\u2211\nk\nfk(u \u22121h)\u03c8(g\u22121h)\n= \u2211\nh\u2208G\n\u2211\nk\nf(h)\u03c8(g\u22121uh)\n= \u2211\nh\u2208G\n\u2211\nk\nf(h)\u03c8((u\u22121g)\u22121h)\n= [Lu[f \u22c6 \u03c8]](g)\n(14)\nThe equivariance of eq. 12 is derived similarly. Note that although equivariance is expressed by the same formula ([Luf ] \u22c6 \u03c8 = Lu[f \u22c6 \u03c8]) for both first-layer G-correlation (eq. 12) and full G-correlation (13), the meaning of the operator Lu is not the same in the two cases: for the first layer correlation, the inputs f and \u03c8 are functions on Z2, so Luf denotes the transformation of such a function, while Lu[f \u22c6 \u03c8] denotes the transformation of the feature map, which is a function on G. For the full G-correlation, both the inputs f and \u03c8 and the output f \u22c6\u03c8 are functions on G.\nNote that if G is not commutative, neither the Gconvolution nor the G-correlation is commutative. However, the feature maps \u03c8 \u22c6 f and f \u22c6 \u03c8 are related by the involution (eq. 8):\nf \u22c6 \u03c8 = (\u03c8 \u22c6 f)\u2217. (15)\nSince the involution is invertible (it is its own inverse), the information content of f\u22c6\u03c8 and \u03c8\u22c6f is the same. However, f \u22c6 \u03c8 is more efficient to compute when using the method described in section 8.\nIt is customary to add a bias term to each feature map in a convolution layer. This can be done for G-conv layers as well, as long as there is only one bias per Gfeature map (instead of one bias per spatial feature plane within a G-feature map). Similarly, batch normalization (Ioffe & Szegedy, 2015) should be implemented with a single scale and bias parameter per G-feature map in order to preserve equivariance. The sum of two G-equivariant feature maps is also G-equivariant, thus G-conv layers can be used in highway networks and residual networks (Srivastava et al., 2015; He et al., 2015)."}, {"heading": "6.2. Pointwise non-linearities", "text": "Equation 14 shows that G-correlation preserves the transformation properties of the previous layer. What about the nonlinearity and pooling operations commonly used in convnets? The nonlinearity \u03bd : R \u2192 R is applied to a feature map f : G \u2192 R by composition to give the rectified feature map \u03bd \u25e6 f ; Evaluated at g, we have [\u03bd \u25e6 f ](g) = \u03bd(f(g)). Since Lhf = f \u25e6 h\u22121, and function composition is associative, one can see that\n\u03bd\u25e6 [Lhf ] = \u03bd\u25e6 [f \u25e6h \u22121] = [\u03bd\u25e6f ]\u25e6h\u22121 = Lh[\u03bd\u25e6f ], (16)\nso the rectified feature map inherits the transformation properties of the previous layer."}, {"heading": "6.3. Pooling", "text": "In order to simplify the analysis, we split the pooling operation into two steps: the pooling itself (performed without stride), and a subsampling step. The non-strided maxpooling operation applied to a feature map f : G \u2192 R can be modeled as an operator P that acts on f as\nPf(g) = max k\u2208gU f(k), (17)\nwhere gU = {gu |u \u2208 U} is the g-transformation of some pooling domain U \u2282 G (typically a neighborhood of the identity transformation). In a regular convnet U is usually a 2\u00d7 2 or 3\u00d7 3 square including the origin (0, 0).\nPooling commutes with transformations:\nPLhf(g) = max k\u2208gU Lhf(k)\n= max k\u2208gU\nf(h\u22121k)\n= max hk\u2208gU f(k)\n= max k\u2208h\u22121gU f(k)\n= LhPf(g)\n(18)\nSince pooling tends to reduce the variation in a feature map, it makes sense to sub-sample the pooled feature map, or equivalently, to do a \u201cpooling with stride\u201d. In a G-CNN, the notion of \u201cstride\u201d is generalized by subsampling on a subgroup H \u2282 G. That is, H is a subset of G that is itself a group (i.e. closed under multiplication and inverses). The resulting feature map is then equivariant to H but not G.\nIn a standard convnet, pooling with stride 2 is the same as pooling and then subsampling on H = {(2i, 2j) |(i, j) \u2208 Z 2} which is a subgroup of G = Z2. For the p4-CNN, we may subsample on the subgroup H containing all 4 rotations and shifts by multiples of 2n pixels, after n pooling layers.\nWe can obtain full G-equivariance by choosing our pooling region U to be a subgroup H \u2282 G, so that we are pooling over cosets gH = {gh |h \u2208 H}. For example, in a p4CNN, we can pool over all four rotations at each spatial position (the cosets of the subgroup R of rotations around the origin). Or, in a Z2-CNN, we can pool over the cosets of the subgroup of even horizontal shifts. The resulting feature map is invariant to the right-action of H , because the pooling domains are similarly invariant (ghH = gH), so we can arbitrarily choose a set of coset-representatives to subsample on. The resulting feature map may be thought of as a function on the quotient space G/H , which caries a natural G-action: g(g\u2032H) = (gg\u2032)H . For p4 and R, we have p4/R \u223c= Z2, i.e. the feature map resulting from \u201ccoset pooling\u201d with respect to R has the same transformation law as the input image.\nThis concludes our analysis of G-CNNs. Since all layer types are equivariant, we can freely stack them into deep networks, and expect to gain statistical efficiency in each G-conv layer."}, {"heading": "7. Backpropagating through G-convolutions", "text": "To train a group equivariant convolutional network, we need to compute gradients of a loss function with respect to the parameters of the filters. As usual, this is done by multiplying the Jacobians of the operations computed in each layer in right-associative order (i.e. backpropagation).\nLet feature map k at layer l be denoted f lk = f l\u22121 \u22c6 \u03c8lk, where f l\u22121 is the previous feature map. At some point in the backprop algorithm, we will have computed the derivative \u2202L/\u2202f lk for all k, and we need to compute \u2202L/\u2202f l\u22121 j (to backpropagate to lower layers) as well as \u2202L/\u2202\u03c8lkj (to update the parameters). We find that,\n\u2202L\n\u2202f l\u22121j (g) =\n[\n\u2202L \u2202f l \u22c6 \u03c8l\u2217j\n]\n(g) (19)\nwhere the superscript \u2217 denotes the involution (eq. 8), and \u03c8lj is the set of filter components applied to input feature map j at layer l:\n\u03c8lj(g) = (\u03c8 l1 j (g), . . . , \u03c8 lKl j (g)) (20)\nTo compute the gradient with respect to component j of filter k, we have to G-convolve the j-th input feature map with the k-th output feature map:\n\u2202L\n\u2202\u03c8lkj (g) =\n\u2211\nh\n\u2202L\n\u2202f lk(h) f l\u22121j (hg)\n=\n[\n\u2202L \u2202f lk \u2217 f l\u22121j\n]\n(g)\n(21)\nSo we see that both the forward and backward passes involve convolution or correlation operations, as is the case in the current generation of convnets (Vasilache et al., 2015)."}, {"heading": "8. Efficient Implementation", "text": "Computing a G-convolution involves nothing more than indexing arithmetic and inner products, so it can be implemented straightforwardly using a loop or as a parlellel GPU kernel. However, without a significant investment in code optimization, such an approach is likely to be slower than a comparable planar convolution, because there are now extremely efficient algorithms for Z2convolutions (Mathieu et al., 2014; Vasilache et al., 2015; Lavin & Gray, 2015). In this section we show how the Gconvolution can be split into a very cheap filter-indexing operation followed by a standard planar convolution, for which we can leverage existing implementations.\nIn a split group, any transformation g can be decomposed into a translation t \u2208 Z2 and a transformation r that leaves the origin invariant. For the group p4, we can write g = tr for t a translation and r a rotation. From this, and using the homomorphism property LgLh = Lgh, we can rewrite the G-correlation (eq. 12 and 13) as follows:\nf \u22c6 \u03c8(tr) = \u2211\nh\u2208X\n\u2211\nk\nfk(h)Lt [Lr\u03c8k(h)] (22)\nwhere X = Z2 in the first layer and X = G for further layers.\nThus, to compute the p4 correlation f \u22c6\u03c8 we can first compute Lr\u03c8 (\u201cfilter rotation\u201d) for r = 0, 1, 2, 3 and then correlate this upgraded stack of feature planes with f using a planar correlation routine."}, {"heading": "8.1. Filter rotation", "text": "The first part of the computation is to evaluateLr\u03c8 for each of four rotations r about the origin. For the first-layer convolution, each filter is stored as a K0 \u00d7 n \u00d7 n array for a filter of n\u00d7 n pixels and K0 number of input feature maps or color channels.\nTo rotate this filter by r (i.e. compute Lr\u03c8), each filter component \u03c8k is individually rotated as a planar function. The rotations are done in parallel in a GPU kernel that takes as input the filter and a set of precomputed indices for each pixel. The output for all of the K1 filters is stored in a K1 \u00d7 4\u00d7K0 \u00d7 n\u00d7 n array \u03c8+.\nAs was discussed in section 4.4, functions on p4 have a different transformation law. To create the indices used for rotating p4 functions, one can do the following. First, create a \u201cmeshgrid\u201d of shape 3 \u00d7 4 \u00d7 n \u00d7 n, containing for each cell in a 4 \u00d7 n \u00d7 n \u201cp4 array\u201d the coordinate vector (i, u, v) of that cell. By applying the rotation r\u22121 to each one of these coordinate vectors (by converting (i, u, v) to a matrix, composing the matrices, and converting back to indices), we obtain for each output cell the integer coordinates of the corresponding input cell.\nUsing this coordinate meshgrid, the p4 filter rotation can be performed by a simple kernel that, for each output cell reads the input coordinates from the meshgrid and then does a lookup in the input array at those coordinates. The output is a K l \u00d7 4\u00d7K l\u22121 \u00d7 4\u00d7 n\u00d7 n array \u03c8+."}, {"heading": "8.2. Planar convolution", "text": "The second part of the computation is to evaluate the inner products using an optimized implementation of the 2D convolution.\nFor the first layer correlation, the sum over X in eq. 22 is a sum over the translation coordinates u, v. Thus, by simply reshaping \u03c8+ from shape K1 \u00d7 4 \u00d7 K0 \u00d7 n \u00d7 n to 4K1 \u00d7K0 \u00d7 n\u00d7 n we can do the whole p4-correlation using a single call to a planar correlation routine.\nFor further layers, we have to sum over three coordinates i, u, v. This can be done by reshaping \u03c8+ from K l \u00d7 4 \u00d7 K l\u22121\u00d74\u00d7n\u00d7n to 4K l\u00d74K l\u22121\u00d7n\u00d7n, and then calling the planar correlation routine."}, {"heading": "9. Experiments", "text": ""}, {"heading": "9.1. Rotated MNIST", "text": "The rotated MNIST dataset (Larochelle et al., 2007) contains 62000 randomly rotated handwritten digits. The dataset is split into a training, validation and test sets of size 10000, 2000 and 50000, respectively.\nBecause data augmentation could potentially reduce the benefits of using G-convolutions, all experiments reported in this section use random rotations on each instance presentation. We found that at least for the rotated MNIST dataset this does not make much difference.\nWe performed model selection using the validation set, yielding a CNN architecture (Z2CNN, see table 1) that outperforms the models tested by Larochelle et al. (2007) (when trained on 12k and evaluated on 50k), but does not match the previous state of the art, which uses prior knowledge about rotations (Schmidt & Roth, 2012) (see table 2).\nWe then took this architecture and replaced each convolution by a p4-convolution followed by a max-pooling over all 4 rotation angles. The post-rotation-pooling feature maps transform in the same way as the input image (i.e. as a function on Z2 as opposed to a function p4), which means that all the convolutions used in this network have to be first-layer convolutions as defined in eq. 12. The resulting architecture, P4CNNRotPool, has exactly the same number of parameters as the Z2CNN, but performs better: it roughly matches the previous state of the art.\nFinally, we designed a network, P4CNN (table 1), that uses full p4-convolutions as defined in eq. 13. This network outperforms the previous state of the art (table 2)."}, {"heading": "9.2. CIFAR-10", "text": "The CIFAR-10 dataset consists of 60k images of size 32\u00d7 32, divided into 10 classes. The dataset is split into 40k\ntraining, 10k validation and 10k testing splits.\nWe took the All-CNN-C by Springenberg et al. (2015) as our baseline. Until recently, this architecture represented the state of the art on CIFAR-10 without data augmentation, and gives competitive results also in the augmented setting (adding translated and flipped copies to the dataset). The architecture consists of a sequence of strided and nonstrided convolutions, interspersed with rectified linear activation units, and nothing else.\nWe recreated the pipeline of (Springenberg et al., 2015) and used the same architectural, data augmentation, preprocessing, regularization and optimization parameters. We were unable to exactly reproduce the reported test-set error of 7.25% with augmentation and 9.07% without augmentation, instead obtaining 8.86% (when adding batch normalization) and 9.82% respectively. This discrepancy indicates that our pipeline may differ slightly from the one used by (Springenberg et al., 2015), for example in terms of weight initialization, but our results are in line with another reproduction by (NervanaSystems, 2016) who report 10.5% error without augmentation.\nTo test the p4-CNN, we replaced all convolution layers of the All-CNN-C and replaced them by p4 layers. For a constant number of filters, this increases the size of the feature maps 4-fold, which in turn increases the number of parameters required per filter in the next layer. Hence, we halve the number of filters in each layer, thereby keeping the number of parameters fixed while increasing the size of the internal representations by a factor of 2 (measured in terms of number of units).\nWe tuned the learning rate and regularization settings on the validation set, and then re-trained on the full training set. As can be seen in table 3, this architecture (the P4-AllCNN) improves test-set error by about 1 percentage point, from 9.82% to 8.84% without further architectural tuning. In the augmented setting, we find a similar improvement from 8.86% to 7.67%.\nAs can be seen in table 3 and 4, our network outperforms most other published architectures. In the non-augmented case, only the 18-layer networks with exponential linear\nunits by Clevert et al. (2015) achieve better performance, while in the augmented case the 19-layer highway networks perform about equal and fractional max pooling with massive data augmentation clearly outperforms our P4-AllCNN-BN architecture. Note that for evaluating the effectiveness of p4-convolutions, only the comparison against the baseline is relevant, because the other networks have a radically different architecture (more layers and more data augmentation)."}, {"heading": "10. Discussion & Future work", "text": "Our results show that p4 convolution layers can be used as a drop-in replacement of standard convolutions, thereby improving the statistical efficiency of a CNN. Whereas GCNNs exploit geometrical priors, other approaches such as exponential linear units and Highway Networks achieve good results through better optimization. It is thus quite possible that the benefits of these methods are synergistic.\nInterestingly, it appears that G-CNNs can benefit from data augmentation in the same way as convolutional networks, as long as the augmentation comes from a group larger than G For example, augmenting with flips and sub-pixel translations appears to improve results for the p4-CNN. Furthermore, the fact that rotation-equivariant networks improve results on the CIFAR-10 dataset shows that there need not\nbe a full symmetry in the data distributions for these networks to be beneficial.\nIn future work, we aim to implement G-CNNs that incorporate reflections, G-CNNs that work on hexagonal lattices which have an increased number of symmetries relative to square grids, and G-CNNs for 3D space groups and lattices. All of the theory presented in this paper is directly applicable to these groups, and can be implemented in a uniform way where for each group one simply needs to program a different group operation.\nThe mentioned hexagonal (p6m) convolutions could reduce the number of parameters by 12x compared to standard CNNs, whereas our current P4-CNNs achieve a 4x reduction. Filters that are locally supported in the non-spatial as well as spatial dimensions can further reduce the number of parameters per layer."}, {"heading": "11. Conclusion", "text": "We have introduced G-CNNs, a generalization of convolutional networks that can significantly reduce the number of parameters required per layer, without sacrificing much expressive capacity, or increase the expressive capacity of a network without increasing the number of parameters. We have developed the general theory for G-CNNs in the case of discrete groups, showing that all layer types are equivariant to the action of the chosen group G. Our experimental results show that G-convolutions can be used as a dropin replacement in convolutional networks, improving their performance with little to no further tuning."}, {"heading": "Acknowledgements", "text": "We would like to thank Chris Olah, Joan Bruna, Robert Gens, Sander Dieleman, and Stefano Soatto for helpful discussions. This research was supported by NWO (grant number NAI.14.108)."}], "references": [{"title": "Learning to See by Moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": null, "citeRegEx": "Anselmi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2014}, {"title": "Invariant scattering convolution networks. IEEE transactions on pattern analysis and machine intelligence", "author": ["J. Bruna", "S. Mallat"], "venue": null, "citeRegEx": "Bruna and Mallat,? \\Q2013\\E", "shortCiteRegEx": "Bruna and Mallat", "year": 2013}, {"title": "Learning intermediatelevel representations of form and motion from natural movies", "author": ["C.F. Cadieu", "B.A. Olshausen"], "venue": "Neural computation,", "citeRegEx": "Cadieu and Olshausen,? \\Q2012\\E", "shortCiteRegEx": "Cadieu and Olshausen", "year": 2012}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": null, "citeRegEx": "Clevert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clevert et al\\.", "year": 2015}, {"title": "Learning the Irreducible Representations of Commutative Lie Groups", "author": ["T. Cohen", "M. Welling"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Welling,? \\Q2014\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2014}, {"title": "Transformation Properties of Learned Visual Representations", "author": ["T.S. Cohen", "M. Welling"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Cohen and Welling,? \\Q2015\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2015}, {"title": "Rotationinvariant convolutional neural networks for galaxy morphology prediction", "author": ["S. Dieleman", "K.W. Willett", "J. Dambre"], "venue": "Monthly Notices of the Royal Astronomical Society,", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Deep Symmetry Networks", "author": ["R. Gens", "P. Domingos"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gens and Domingos,? \\Q2014\\E", "shortCiteRegEx": "Gens and Domingos", "year": 2014}, {"title": "Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces", "author": ["A. Hyvarinen", "P. Hoyer"], "venue": "Neural Computation,", "citeRegEx": "Hyvarinen and Hoyer,? \\Q2000\\E", "shortCiteRegEx": "Hyvarinen and Hoyer", "year": 2000}, {"title": "Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Spatial Transformer Networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "Transformation equivariant Boltzmann machines", "author": ["Kivinen", "Jyri J", "Williams", "Christopher K I"], "venue": "In 21st International Conference on Artificial Neural Networks,", "citeRegEx": "Kivinen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2011}, {"title": "Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map", "author": ["T. Kohonen"], "venue": "Biological Cybernetics,", "citeRegEx": "Kohonen,? \\Q1996\\E", "shortCiteRegEx": "Kohonen", "year": 1996}, {"title": "A novel set of rotationally and translationally invariant features for images based on the noncommutative bispectrum", "author": ["R. Kondor"], "venue": null, "citeRegEx": "Kondor,? \\Q2007\\E", "shortCiteRegEx": "Kondor", "year": 2007}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Deeply-Supervised Nets", "author": ["C. Lee", "S. Xie", "P.W. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Understanding image representations by measuring their equivariance and equivalence", "author": ["K. Lenc", "A. Vedaldi"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Lenc and Vedaldi,? \\Q2015\\E", "shortCiteRegEx": "Lenc and Vedaldi", "year": 2015}, {"title": "Distinctive Image Features from ScaleInvariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Lowe,? \\Q2004\\E", "shortCiteRegEx": "Lowe", "year": 2004}, {"title": "Fast Training of Convolutional Networks through FFTs", "author": ["M. Mathieu", "M. Henaff", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Mathieu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2014}, {"title": "Learning to Represent Spatial Transformations with Factored HigherOrder Boltzmann Machines", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Memisevic and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Memisevic and Hinton", "year": 2010}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning rotation-aware features: From invariant priors to equivariant descriptors", "author": ["U. Schmidt", "S. Roth"], "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Schmidt and Roth,? \\Q2012\\E", "shortCiteRegEx": "Schmidt and Roth", "year": 2012}, {"title": "Spherical Tensor Algebra for Biomedical Image Analysis", "author": ["H. Skibbe"], "venue": "PhD thesis, Albert-Ludwigs-Universitat Freiburg im Breisgau,", "citeRegEx": "Skibbe,? \\Q2013\\E", "shortCiteRegEx": "Skibbe", "year": 2013}, {"title": "Learning Invariant Representations with Local Transformations", "author": ["K. Sohn", "H. Lee"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Sohn and Lee,? \\Q2012\\E", "shortCiteRegEx": "Sohn and Lee", "year": 2012}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Training Very Deep Networks", "author": ["Srivastava", "Rupesh Kumar", "Greff", "Klaus", "Schmidhuber", "J\u00fcrgen"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Fast convolutional nets with fbfft: A GPU performance evaluation", "author": ["N. Vasilache", "J. Johnson", "M. Mathieu", "S. Chintala", "S. Piantino", "Y. LeCun"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Vasilache et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vasilache et al\\.", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 27, "context": "In section 9 we report on our experiments with rotated MNIST, where G-CNNs significantly outperform the previous state of the art, and CIFAR-10, where we show a considerable improvement over a competitive baseline (the All-CNN-C architecture of (Springenberg et al., 2015)), thus outperforming DropConnect (Wan et al.", "startOffset": 245, "endOffset": 272}, {"referenceID": 30, "context": ", 2015)), thus outperforming DropConnect (Wan et al., 2013), Maxout (Goodfellow et al.", "startOffset": 41, "endOffset": 59}, {"referenceID": 18, "context": ", 2014), deeply supervised nets (Lee et al., 2015).", "startOffset": 32, "endOffset": 50}, {"referenceID": 23, "context": "Although vector addition has been shown to be surprisingly effective at modeling semantic relationships of words (Mikolov et al., 2013), it has inherent limitations such as being commutative and non-periodic.", "startOffset": 113, "endOffset": 135}, {"referenceID": 15, "context": "Lenc & Vedaldi (2015) show that the AlexNet CNN (Krizhevsky et al., 2012) trained on imagenet spontaneously learns representations that are equivariant to flips, scaling and rotation.", "startOffset": 48, "endOffset": 73}, {"referenceID": 20, "context": "There is a large literature on learning invariant representations, which can be achieved by pose normalization (Lowe, 2004; Jaderberg et al., 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al.", "startOffset": 111, "endOffset": 147}, {"referenceID": 11, "context": "There is a large literature on learning invariant representations, which can be achieved by pose normalization (Lowe, 2004; Jaderberg et al., 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al.", "startOffset": 111, "endOffset": 147}, {"referenceID": 25, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 68, "endOffset": 140}, {"referenceID": 14, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 68, "endOffset": 140}, {"referenceID": 1, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 68, "endOffset": 140}, {"referenceID": 17, "context": "Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al., 2011).", "startOffset": 195, "endOffset": 237}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)).", "startOffset": 119, "endOffset": 200}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012).", "startOffset": 119, "endOffset": 553}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012).", "startOffset": 119, "endOffset": 614}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012).", "startOffset": 119, "endOffset": 636}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant.", "startOffset": 119, "endOffset": 694}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant.", "startOffset": 119, "endOffset": 718}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation.", "startOffset": 119, "endOffset": 880}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al.", "startOffset": 119, "endOffset": 1345}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al.", "startOffset": 119, "endOffset": 1386}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012).", "startOffset": 119, "endOffset": 1408}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012). Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al.", "startOffset": 119, "endOffset": 1435}, {"referenceID": 0, "context": ", 2015) or by averaging a possibly non-linear function over a group (Skibbe, 2013; Kondor, 2007; Bruna & Mallat, 2013; Anselmi et al., 2014) (the latter approach going back at least to Hurwitz (1897)). Both approaches are largely compatible with our contribution, but we focus on the more general notion of equivariance which is criticial when building deep networks. More closely related are a small number of papers with the explicit aim of learning equivariant representations. This includes work on transforming autoencoders by Hinton et al. (2011), equivariant Boltzmann machines by Kivinen & Williams (2011) and Sohn & Lee (2012), and the equivariant descriptors of Schmidt & Roth (2012). Gens & Domingos (2014) proposed a convolutional architecture that incorporates transformations of filters, but these networks are only approximately equivariant. Dieleman et al. (2015) showed that rotation symmetry can be exploited in convolutional networks for the problem of galaxy morphology prediction by rotating feature maps, effectively learning an equivariant representation. By diagonalizing or block-diagonalizing the transformation operators Tg, the representation becomes disentangled or \u201cirreducible\u201d in a certain precise sense (Cohen & Welling, 2014). This idea is implicit in a long line of work, going back at least to Kohonen (1996), and including Memisevic & Hinton (2010); Hinton et al. (2011); Cadieu & Olshausen (2012). Cohen & Welling (2015) showed that under certain conditions such disentangled representations are also decorrelated, thus relating this line of work to statistical approaches such as ICA and ISA (Hyvarinen & Hoyer, 2000; Le et al.", "startOffset": 119, "endOffset": 1459}, {"referenceID": 0, "context": "Agrawal et al. (2015) show that equivariance to ego-motion can be learned, and that this can be used as an unsupervised", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "The sum of two G-equivariant feature maps is also G-equivariant, thus G-conv layers can be used in highway networks and residual networks (Srivastava et al., 2015; He et al., 2015).", "startOffset": 138, "endOffset": 180}, {"referenceID": 29, "context": "So we see that both the forward and backward passes involve convolution or correlation operations, as is the case in the current generation of convnets (Vasilache et al., 2015).", "startOffset": 152, "endOffset": 176}, {"referenceID": 21, "context": "However, without a significant investment in code optimization, such an approach is likely to be slower than a comparable planar convolution, because there are now extremely efficient algorithms for Zconvolutions (Mathieu et al., 2014; Vasilache et al., 2015; Lavin & Gray, 2015).", "startOffset": 213, "endOffset": 279}, {"referenceID": 29, "context": "However, without a significant investment in code optimization, such an approach is likely to be slower than a comparable planar convolution, because there are now extremely efficient algorithms for Zconvolutions (Mathieu et al., 2014; Vasilache et al., 2015; Lavin & Gray, 2015).", "startOffset": 213, "endOffset": 279}, {"referenceID": 16, "context": "The rotated MNIST dataset (Larochelle et al., 2007) contains 62000 randomly rotated handwritten digits.", "startOffset": 26, "endOffset": 51}, {"referenceID": 16, "context": "The rotated MNIST dataset (Larochelle et al., 2007) contains 62000 randomly rotated handwritten digits. The dataset is split into a training, validation and test sets of size 10000, 2000 and 50000, respectively. Because data augmentation could potentially reduce the benefits of using G-convolutions, all experiments reported in this section use random rotations on each instance presentation. We found that at least for the rotated MNIST dataset this does not make much difference. We performed model selection using the validation set, yielding a CNN architecture (Z2CNN, see table 1) that outperforms the models tested by Larochelle et al. (2007) (when trained on 12k and evaluated on 50k), but does not match the previous state of the art, which uses prior knowledge about rotations (Schmidt & Roth, 2012) (see table 2).", "startOffset": 27, "endOffset": 650}, {"referenceID": 16, "context": "The dataset is split into 40k Model Test Error (%) Larochelle et al. (2007) 10.", "startOffset": 51, "endOffset": 76}, {"referenceID": 16, "context": "The dataset is split into 40k Model Test Error (%) Larochelle et al. (2007) 10.38 \u00b1 0.27 Sohn & Lee (2012) 4.", "startOffset": 51, "endOffset": 107}, {"referenceID": 16, "context": "The dataset is split into 40k Model Test Error (%) Larochelle et al. (2007) 10.38 \u00b1 0.27 Sohn & Lee (2012) 4.2 Schmidt & Roth (2012) 3.", "startOffset": 51, "endOffset": 133}, {"referenceID": 27, "context": "We recreated the pipeline of (Springenberg et al., 2015) and used the same architectural, data augmentation, preprocessing, regularization and optimization parameters.", "startOffset": 29, "endOffset": 56}, {"referenceID": 27, "context": "This discrepancy indicates that our pipeline may differ slightly from the one used by (Springenberg et al., 2015), for example in terms of weight initialization, but our results are in line with another reproduction by (NervanaSystems, 2016) who report 10.", "startOffset": 86, "endOffset": 113}, {"referenceID": 27, "context": "We took the All-CNN-C by Springenberg et al. (2015) as our baseline.", "startOffset": 25, "endOffset": 52}, {"referenceID": 4, "context": "units by Clevert et al. (2015) achieve better performance, while in the augmented case the 19-layer highway networks perform about equal and fractional max pooling with massive data augmentation clearly outperforms our P4-AllCNN-BN architecture.", "startOffset": 9, "endOffset": 31}, {"referenceID": 18, "context": "41 DSN (Lee et al., 2015) 9.", "startOffset": 7, "endOffset": 25}, {"referenceID": 27, "context": "82 All-CNN (Springenberg et al., 2015) 9.", "startOffset": 11, "endOffset": 38}, {"referenceID": 4, "context": "84 Elu (Clevert et al., 2015) 6.", "startOffset": 7, "endOffset": 29}, {"referenceID": 30, "context": "38 DropConnect (Wan et al., 2013) 9.", "startOffset": 15, "endOffset": 33}, {"referenceID": 18, "context": "81 DSN (Lee et al., 2015) 7.", "startOffset": 7, "endOffset": 25}, {"referenceID": 27, "context": "6 All-CNN (Springenberg et al., 2015) 7.", "startOffset": 10, "endOffset": 37}], "year": 2017, "abstractText": "We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. By convolving over groups larger than the translation group, G-CNNs build representations that are equivariant to these groups, which makes it possible to greatly increase the degree of parameter sharing. We show how G-CNNs can be implemented with negligible computational overhead for discrete groups such as the group of translations, reflections and rotations by multiples of 90 degrees. G-CNNs achieve state of the art results on rotated MNIST and significantly improve over a competitive baseline on augmented and non-augmented CIFAR-10.", "creator": "LaTeX with hyperref package"}}}