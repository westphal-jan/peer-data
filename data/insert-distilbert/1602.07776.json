{"id": "1602.07776", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Recurrent Neural Network Grammars", "abstract": "we introduce recurrent neural network grammars, simplified probabilistic models of sentences with explicit phrase structure. we explain efficient inference procedures that that allow application to both parsing and language modeling. experiments show that they provide better parsing in english than any single previously unpublished published loosely supervised generative case model approach and better verbal language modeling than state - of - the - art sequential rnns in english and chinese.", "histories": [["v1", "Thu, 25 Feb 2016 02:42:58 GMT  (166kb,D)", "http://arxiv.org/abs/1602.07776v1", "11 pages"], ["v2", "Fri, 1 Apr 2016 23:28:08 GMT  (170kb,D)", "http://arxiv.org/abs/1602.07776v2", "11 pages"], ["v3", "Thu, 6 Oct 2016 14:22:02 GMT  (222kb,D)", "http://arxiv.org/abs/1602.07776v3", "Proceedings of NAACL 2016 (contains corrigendum)"], ["v4", "Wed, 12 Oct 2016 04:47:45 GMT  (395kb,D)", "http://arxiv.org/abs/1602.07776v4", "Proceedings of NAACL 2016 (contains corrigendum)"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["chris dyer", "adhiguna kuncoro", "miguel ballesteros", "noah a smith"], "accepted": true, "id": "1602.07776"}, "pdf": {"name": "1602.07776.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Network Grammars", "authors": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "emails": ["cdyer@cs.cmu.edu,", "akuncoro@cs.cmu.edu,", "miguel.ballesteros@upf.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese."}, {"heading": "1 Introduction", "text": "Sequential recurrent neural networks (RNNs) are remarkably effective models of natural language. In the last few years, language model results that substantially improve over long-established state-ofthe-art baselines have been obtained using RNNs (Zaremba et al., 2015; Mikolov et al., 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al., 2015), image caption generation (Xu et al., 2015), and dialogue generation (Wen et al., 2015). Despite these impressive results, sequential models are a priori inappropriate models of natural language, since relationships among words are organized in terms of latent nested structures rather than sequential surface order (Chomsky, 1957).\nIn this paper, we introduce recurrent neural network grammars (RNNGs; \u00a72), a new generative probabilistic model of sentences that explicitly models nested, hierarchical relationships among words and phrases. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions.\nThe foundation of this work is a top-down variant of transition-based parsing (\u00a73). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these rely on bottom-up construction of syntactic structure. Bottom-up construction orders are appealing since they permit well-known shift-reduce or leftcorner parsing algorithms to be exploited for inference; however, they limit the use of top-down grammar information, which is useful for generation (Roark, 2001). RNNGs maintain the algorithmic convenience of transition-based parsing but incorporate top-down syntactic information (\u00a74).\nThe top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve a second practical challenge with RNNGs: approximating the marginal likelihood and MAP tree of a sentence under the generative model. We present a simple importance sampling algorithm which uses samples from the discriminative parser to do inferar X iv :1 60 2. 07 77\n6v 1\n[ cs\n.C L\n] 2\n5 Fe\nb 20\n16\nence problems in the generative model (\u00a75). Experiments show that RNNGs are effective for both language modeling and parsing (\u00a76). Our generative model obtains (i) the best-known parsing results using a supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models. Surprisingly\u2014although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)\u2014 parsing with the generative model obtains significantly better results than parsing with the discriminative model."}, {"heading": "2 RNN Grammars", "text": "Formally, a RNNG is a triple (N,\u03a3,\u0398) consisting of a finite set of nonterminal symbols (N ), a finite set of terminal symbols (\u03a3) such that N \u2229\u03a3 = \u2205, and a collection of neural network parameters \u0398. The algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition-based algorithm, which is outlined in the next section. In the section after that, the semantics of the parameters that are used to turn this into a stochastic algorithm that generates pairs of trees and strings are discussed."}, {"heading": "3 Top-down Parsing and Generation", "text": "RNNGs are based on a top-down generation algorithm that relies on a stack data structure of partially completed syntactic constituents. To emphasize the similarity of our algorithm to more familiar bottom-up shift-reduce recognition algorithms, we first present the parsing (rather than generation) version of our algorithm (\u00a73.1) and then present modifications to turn it into a generator (\u00a73.2)."}, {"heading": "3.1 Parser Transitions", "text": "The parsing algorithm transforms a sequence of words x into a parse tree y using two data structures (a stack, an input buffer). As with the bottom-up algorithm of Sagae and Lavie (2005), our algorithm begins with the stack (S) empty and the complete sequence of words on the input buffer (B). The buffer contains unprocessed terminal symbols, and the stack contains terminal symbols, \u201copen\u201d nonterminal symbols, and completed constituents. One of\nthe following three operations (Fig. 1) are successively applied, based on the evolving contents of the stack and buffer:\n\u2022 NT(X) introduces an \u201copen nonterminal\u201d X onto the top of the stack. Open nonterminals are written as a nonterminal symbol preceded by an open parenthesis, e.g. (VP, and they represent a nonterminal whose child nodes have not yet been fully constructed. Open nonterminals will be \u201cclosed\u201d to form complete subtrees by subsequent operations.\n\u2022 SHIFT removes the terminal symbol x from the front of the input buffer, and pushes it onto the top of the stack.\n\u2022 REDUCE repeatedly pops completed subtrees from the stack until an open nonterminal is encountered, and then this open NT is popped and used as the label of a new constituent that has the popped subtrees as its children. This new completed constituent is pushed onto the stack. A single REDUCE operation can thus create constituents with an unbounded number of children.\nThe parsing algorithm terminates when there is a single completed tree on the stack and the buffer is empty. Fig. 2 shows an example parse using our transition set. Note that in this paper we do not model preterminal symbols (i.e., part-of-speech tags) and our examples therefore do not include them.1\nOur transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the \u201clinearized\u201d parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).\n1Preterminal symbols are, from the parsing algorithm\u2019s point of view, just another kind of nonterminal symbol that requires no special handling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Finally, standard parsing evaluation scores do not consider preterminals.\nConstraints on parser transitions. To guarantee that only well-formed phrase-structure trees are produced by the parser, we impose the following constraints on the transitions that can be applied at each step which are a function of the parser state (B,S, n) where n is the number of open nonterminals on the stack:\n\u2022 The NT(X) can only be applied if B is not empty and n < 100.2\n\u2022 The SHIFT operation can only be applied if B is not empty and n \u2265 1. \u2022 The REDUCE operation can only be applied if the\ntop of the stack is not an open nonterminal symbol. \u2022 The REDUCE operation can only be applied if n \u2265\n2 or if the buffer is empty.\nTo designate the set of valid parser transitions, we write AD(B,S, n)."}, {"heading": "3.2 Generator Transitions", "text": "The parsing algorithm that maps from sequences of words to parse trees can be adapted with minor changes to produce an algorithm that generates trees and sequences of words. There are two primary changes: (i) there is no input buffer of unprocessed words, rather there is an output buffer (T ), and (ii) instead of a SHIFT operation there is a GEN(x) operation which generates a terminal symbol x and adds it to the top of the stack and the output buffer. The algorithm terminates when a single tree remains on the stack. Fig. 4 shows an example generation sequence.\nConstraints on generator transitions. Since the generator algorithm state has replaced the input buffer with an output buffer, the parser constraints must be reformulated for the generator. These are:\n\u2022 The GEN(x) operation can only be applied if n \u2265 1. 2Since our parser allows unary nonterminal productions, there are an infinite number of valid trees for finite length sentences. The n < 100 constraint prevents the classifier from misbehaving and generating excessively large numbers of nonterminals. Similar constraints have been proposed to deal with the analogous problem in bottom-up shift-reduce parsers (Sagae and Lavie, 2005).\n\u2022 The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n \u2265 1.\nTo designate the set of valid generator transitions, we write AG(T, S, n)."}, {"heading": "3.3 Transition Sequences from Trees", "text": "Any parse tree can be converted to a sequence of transitions via a depth-first, left-to-right traversal of a parse tree. Since there is a unique depth-first, leftro-right traversal of a tree, there is exactly one transition sequence of each tree. For a tree y and a sequence of symbols x, we write a(x,y) to indicate the corresponding sequence of generation transitions, and b(x,y) to indicate the parser transitions."}, {"heading": "3.4 Runtime Analysis", "text": "A detailed analysis of the algorithmic properties of our top-down parser is beyond the scope of this paper; however, we briefly state several facts. Assuming the availability of constant time push and pop operations, the runtime is linear in the number of the nodes in the parse tree that is generated by the parser/generator (intuitively, this is true since although an individual REDUCE operation may require applying a number of pops that is linear in the number of input symbols, the total number of pops operations across an entire parse/generation run will also be linear). Since there is no way to bound the number of output nodes in a parse tree as a function of the number of input words, stating the runtime complexity of the parsing algorithm as a function of the input size requires further assumptions. Assuming our fixed constraint on maximum depth, it is linear."}, {"heading": "3.5 Comparison to Other Models", "text": "Our generation algorithm algorithm differs from previous stack-based parsing/generation algorithms in two ways. First, it constructs rooted tree structures top down (rather than bottom up), and second, the transition operators are capable of directly generating arbitrary tree structures rather than, e.g., assuming binarized trees, as is the case in prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013). Although, left-corner based algorithms likewise can generate\narbitrary tree structures in a bottom-up order (Henderson, 2004)."}, {"heading": "4 Generative Model", "text": "RNNGs use the generator transition set just presented to define a joint distribution on syntax trees (y) and words (x). This distribution is defined as a sequence model over generator transitions that is parameterized using a continuous space embedding of the algorithm state at each time step (ut); i.e.,\np(x,y) =\n|a(x,y)|\u220f\nt=1\np(at | a<t)\n=\n|a(x,y)|\u220f\nt=1 exp r>atut + bat\u2211 a\u2032\u2208AG(Tt,St,nt) exp r > a\u2032ut + ba\u2032 ,\nand where action-specific embeddings ra and bias vector b are parameters in \u0398.\nThe representation of the algorithm state at time t, ut, is computed by combining the representation of the generator\u2019s three data structures: the output buffer (Tt), represented by an embedding ot, the stack (St), represented by an embedding st, and the history of actions (a<t) taken by the generator, represented by an embedding ht,\nut = tanh (W[ot; st;ht] + c) ,\nwhere W and c are parameters. Refer to Figure 5 for an illustration of the architecture.\nThe output buffer, stack, and history are sequences that grow unboundedly, and to obtain representations of them we use recurrent neural networks to \u201cencode\u201d their contents (Cho et al., 2014). Since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet, it is straightforward to apply a standard RNN encoding architecture. The stack (S) is more complicated for two reasons. First, the elements of the stack are more complicated objects than symbols from a discrete alphabet: open nonterminals, terminals, and full trees, are all present on the stack. Second, it is manipulated using both push and pop operations. To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015). To represent complex parse trees, we define a new syntactic composition\nfunction that recursively defines representations of trees."}, {"heading": "4.1 Syntactic Composition Function", "text": "When the REDUCE operation is executed, the parser pops a sequence of completed subtrees (together with their vector embeddings) from the stack and makes them children of the most recent open nonterminal. To compute an embedding of this new subtree, we use a composition function based on bidirectional LSTMs, which is illustrated in Fig. 6.\nAn embedding of the resulting constituent is the first element read by the LSTM in both the forward and reverse traversals of the embeddings of the child subtrees. Intuitively, this order serves to \u201cnotify\u201d each LSTM what sort of head it should be looking for as it processes the child node embeddings. The final state of the LSTMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.3 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an internal node), this composition function is a kind of recursive neural network."}, {"heading": "4.2 Word Generation", "text": "To reduce the size of AG(S, T, n), word generation is broken into two parts. First, the decision to generate is made (by predicting GEN as an action),\n3We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task.\nand then choosing the word, conditional on the current parser state. To further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu and Blunsom, 2015; Goodman, 2001). By using \u221a |\u03a3| classes for a vocabulary of size |\u03a3|, this prediction step runs in time O( \u221a |\u03a3|) rather than the O(|\u03a3|) of the full-vocabulary softmax. To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992)."}, {"heading": "4.3 Training", "text": "The parameters in the model are learned to maximize the likelihood of a corpus of trees."}, {"heading": "4.4 Discriminative Parsing Model", "text": "A discriminative parsing model can be obtained by replacing the embedding of Tt at each time step with an embedding of the input buffer Bt. To train this model, the conditional likelihood of each sequence of actions given the input string is maximized."}, {"heading": "5 Inference via Importance Sampling", "text": "Our generative model p(x,y) defines a joint distribution on trees (y) and sequences of words (x). To evaluate this as a language model, it is necessary to compute the marginal probability p(x) =\u2211\ny\u2032\u2208Y(x) p(x,y \u2032). And, to evaluate the model as a parser, we need to be able to find the MAP parse tree, i.e., the tree y \u2208 Y(x) that maximizes p(x,y). However, because of the unbounded dependencies across the sequence of parsing actions in our model,\nexactly solving either of these inference problems is intractable. To obtain estimates of these, we use a variant of importance sampling (Doucet and Johansen, 2011).\nOur importance sampling algorithm uses a conditional proposal distribution q(y | x) with the following properties: (i) p(x,y) > 0 =\u21d2 q(y | x) > 0; (ii) samples y \u223c q(y | x) can be obtained efficiently; and (iii) the conditional probabilities q(y | x) of these samples are known. While many such distributions are available, the discriminatively trained variant of our parser (\u00a74.4) fulfills these requirements: sequences of actions can be sampled using a simple ancestral sampling approach, and, since parse trees and action sequences exist in a one-to-one relationship, the product of the action probabilities is the conditional probability of the parse tree under q. We therefore use our discriminative parser as our proposal distribution.\nImportance sampling uses importance weights, which we define as w(x,y) = p(x,y)/q(y | x), to compute this estimate. Under this definition, we can derive the estimator as follows:\np(x) = \u2211\ny\u2208Y(x)\np(x,y) = \u2211\ny\u2208Y(x)\nq(y | x)w(x,y)\n= Eq(y|x)w(x,y).\nWe now replace this expectation with its Monte\nCarlo estimate as follows, using N samples from q:\ny(i) \u223c q(y | x) for i \u2208 {1, 2, . . . , N}\nEq(y|x)w(x,y) MC \u2248 1\nN\nN\u2211\ni=1\nw(x,y(i))\nTo obtain an estimate of the MAP tree y\u0302, we choose the sampled tree with the highest probability under the joint model p(x,y)."}, {"heading": "6 Experiments", "text": "We present results of our two models both on parsing (discriminative and generative) and as a language model (generative only) in English and Chinese.\nData. For English, sections 2\u201321 of the Penn Treebank are used as training corpus for both, with \u00a724 held out as validation, and \u00a723 used for evaluation. Singleton words in the training corpus with unknown word classes using the the Berkeley parser\u2019s mapping rules.4 Orthographic case distinctions are preserved, and numbers (beyond singletons) are not normalized. For Chinese, we use the Penn Chinese Treebank Version 5.1 (CTB) (Xue et al., 2005).5 For the Chinese experiments, we use a single unknown word class. Corpus statistics are given in Table 1.6\nModel and training parameters. For the discriminative model, we used hidden dimensions of 128 and 2-layer LSTMs (larger numbers of dimensions reduced validation set performance). For the generative model, we used 256 dimensions and 2- layer LSTMs. For both models, we tuned the\n4http://github.com/slavpetrov/ berkeleyparser\n5Sections 001-270 and 440-1151 for training, sections 301- 325 development data, and sections 271-300 for evaluation.\n6This preprocessing scheme is more similar to what is standard in parsing than what is standard in language modeling. However, since our model is both a parser and a language model, we opted for the parser normalization.\ndropout rate to maximize validation set likelihood, obtaining optimal rates of 0.2 (discriminative) and 0.3 (generative). For the sequential LSTM baseline for the language model, we also found an optimal dropout rate of 0.3. For training we used stochastic gradient descent with a learning rate of 0.1. All parameters were initialized according to recommendations given in Glorot and Bengio (2010).\nEnglish parsing results. Table 6 (last three rows) gives the performance of our parser on Section 23, as well as the performance of several representative models. For the discriminative model, we used a greedy decoding rule as opposed to beam search in some shift-reduce baselines. For the generative model, we obtained 100 independent samples from a flattened distribution of the discriminative parser (by exponentiating each probability by \u03b1 = 0.8 and renormalizing) and reranked them according to the generative model.7\nChinese parsing results. Chinese parsing results were obtained with the same methodology as in English and show the same pattern (Table 6).\nLanguage model results. We report held-out perword perplexities of three language models, both sequential and syntactic. Log probabilities are normalized by the number of words (excluding the stop symbol), inverted, and exponentiated to yield the perplexity. Results are summarized in Table 4. Random samples of text from the sequential LSTM\n7The value \u03b1 = 0.8 was chosen based on the diversity of the samples generated on the development set.\nmodel and the RNNG-based model are shown in Figure 7."}, {"heading": "7 Discussion", "text": "It is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model. This is the result of (i) relaxing conventional independence assumptions (e.g., context-freeness) and (ii) inferring continuous representations of symbols alongside non-linear models of their syntactic relationships. The most significant question that remains is why the discriminative model\u2014which has more information available to it than the generative model\u2014performs worse than the generative model. This pattern has been observed before in neural parsing by Henderson (2004), who hypothesized that larger, unstructured conditioning contexts are harder to learn from, and provide opportunities to overfit. Our discriminative model conditions on the entire history, stack, and buffer, while our generative model only accesses the history and stack. The fully discriminative model of Vinyals et al. (2015) was able to obtain results similar to those of our generative model (albeit using much larger training sets obtained through semisupervision) but similar results to those of our discriminative parser using the same data. In light of their results, we believe Henderson\u2019s hypothesis is correct, and that generative models should be considered as a more statistically efficient method for\nlearning neural networks from small data."}, {"heading": "8 Related Work", "text": "Our language model combines work from two modeling traditions: (i) recurrent neural network language models and (ii) syntactic language modeling. Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990). Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take."}, {"heading": "9 Outlook", "text": "RNNGs can be combined with a particle filter inference scheme (rather than the importance sampling method based on a discriminative parser, \u00a75) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models.\nA second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the\nRandom samples from the (baseline) sequential LSTM language model: Analysts say they are against UNK-LC-y countries for the equipment show that some of bottom Giraffe patrols plans diligence purchasing .\n\u201c We do n\u2019t have somebody passed or pushed up the UNK-LC-ed \u201d and did n\u2019t specify the numbers .\n\u201c We \u2019re advising the new experience , \u201d thus after that \u2019s deep direct , Storer \u2019s most UNK-LC-ing UNK-LC .\nThe example Ground may have implanted selectively out of the these women such so through the carriers already hold easily .\nThe company \u2019s development sales rose 0.1 % in East Germany , cameras , hospital health and merchandising products students .\nRandom samples from the RNNG language model (only showing terminals): Bankers Trust Co. said Moscow was completed on Nov. 2 , 1990 after Vietnam \u2019s croaker closed a unified credit in 1987 .\ncomputationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014).\nFinally, humans sentence processing takes place in a left-to-right, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010)."}, {"heading": "10 Conclusion", "text": "We introduced recurrent neural network grammars, a probabilistic model of phrase-structure trees that can be trained generatively and used as a language model or a parser, and a corresponding discriminative model that can be used as a parser. Apart from out-of-vocabulary preprocessing, the approach requires no feature design or transformations to treebank data. The generative model outperforms every previously published parser built on a single supervised generative model in English, and a bit behind the best-reported generative model in Chinese. As language models, RNNGs outperform the best single-sentence language models."}, {"heading": "Acknowledgments", "text": "We thank Brendan O\u2019Connor and Swabha Swayamdipta for feedback on drafts of this paper, and Jan Buys, Phil Blunsom, and Yue Zhang for help with data preparation. This work was spon-\nsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15C-0114; it was also supported in part by Contract No. W911NF-15-1-0543 with the DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA)."}], "references": [{"title": "Neural machine transation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Trainable grammars for speech recognition", "author": ["James K. Baker"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Baker.,? \\Q1979\\E", "shortCiteRegEx": "Baker.", "year": 1979}, {"title": "Pragmatic neural modelling in machine translation", "author": ["Baltescu", "Blunsom2015] Paul Baltescu", "Phil Blunsom"], "venue": "In Proc. NAACL", "citeRegEx": "Baltescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baltescu et al\\.", "year": 2015}, {"title": "On the parameter space of generative lexicalized statistical parsing models", "author": ["Dan Bikel"], "venue": "Ph.D. thesis,", "citeRegEx": "Bikel.,? \\Q2004\\E", "shortCiteRegEx": "Bikel.", "year": 2004}, {"title": "An efficient implementation of a new DOP model", "author": ["Rens Bod"], "venue": "In Proc. EACL", "citeRegEx": "Bod.,? \\Q2003\\E", "shortCiteRegEx": "Bod.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "2015a. A Bayesian model for generative transitionbased dependency parsing. CoRR, abs/1506.04334", "author": ["Buys", "Blunsom2015a] Jan Buys", "Phil Blunsom"], "venue": null, "citeRegEx": "Buys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Buys et al\\.", "year": 2015}, {"title": "Generative incremental dependency parsing with neural networks", "author": ["Buys", "Blunsom2015b] Jan Buys", "Phil Blunsom"], "venue": "In Proc. ACL", "citeRegEx": "Buys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Buys et al\\.", "year": 2015}, {"title": "A maximumentropy-inspired parser", "author": ["Eugene Charniak"], "venue": "In Proc. NAACL", "citeRegEx": "Charniak.,? \\Q2000\\E", "shortCiteRegEx": "Charniak.", "year": 2000}, {"title": "Top-down nearly-context-sensitive parsing", "author": ["Eugene Charniak"], "venue": "In Proc. EMNLP", "citeRegEx": "Charniak.,? \\Q2010\\E", "shortCiteRegEx": "Charniak.", "year": 2010}, {"title": "Structured language modeling", "author": ["Chelba", "Jelinek2000] Ciprian Chelba", "Frederick Jelinek"], "venue": "Computer Speech and Language,", "citeRegEx": "Chelba et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2000}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A tutorial on particle filtering and smoothing: Fifteen years later", "author": ["Doucet", "Johansen2011] Arnaud Doucet", "Adam M. Johansen"], "venue": "In Handbook of Nonlinear Filtering. Oxford", "citeRegEx": "Doucet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "An efficient contextfree parsing algorithm", "author": ["Jay Earley"], "venue": "Communications of the ACM,", "citeRegEx": "Earley.,? \\Q1970\\E", "shortCiteRegEx": "Earley.", "year": 1970}, {"title": "Finding structure in time", "author": ["Jeffrey L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "A neural syntactic language model", "author": ["Emami", "Jelinek2005] Ahmad Emami", "Frederick Jelinek"], "venue": "Machine Learning,", "citeRegEx": "Emami et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Emami et al\\.", "year": 2005}, {"title": "Scalable inference and training of context-rich syntactic translation models", "author": ["Galley et al.2006] Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer"], "venue": "In Proc. ACL", "citeRegEx": "Galley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "Phrase dependency machine translation with quasi-synchronous tree-to-tree features", "author": ["Gimpel", "Smith2014] Kevin Gimpel", "Noah A. Smith"], "venue": "Computational Linguistics,", "citeRegEx": "Gimpel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "In Proc. ICML", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Classes for fast maximum entropy training. CoRR, cs.CL/0108006", "author": ["Joshua Goodman"], "venue": null, "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson"], "venue": "In Proc. ACL", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Self-training PCFG grammars with latent annotations across languages", "author": ["Huang", "Harper2009] Zhongqiang Huang", "Mary Harper"], "venue": "In Proc. EMNLP", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "Computation of the probability of initial substring generation by stochastic context-free grammars", "author": ["Jelinek", "Lafferty1991] Frederick Jelinek", "John D. Lafferty"], "venue": "Computational Linguistics,", "citeRegEx": "Jelinek et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1991}, {"title": "Joint and conditional estimation of tagging and parsing models", "author": ["Mark Johnson"], "venue": "In Proc. ACL", "citeRegEx": "Johnson.,? \\Q2001\\E", "shortCiteRegEx": "Johnson.", "year": 2001}, {"title": "Effective self-training for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proc. NAACL", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In Proc. Interspeech", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Improved inference for unlexicalized parsing", "author": ["Petrov", "Klein2007] Slav Petrov", "Dan Klein"], "venue": "In Proc. NAACL", "citeRegEx": "Petrov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2007}, {"title": "Probabilistic top-down parsing and language modeling", "author": ["Brian Roark"], "venue": "Computational Linguistics,", "citeRegEx": "Roark.,? \\Q2001\\E", "shortCiteRegEx": "Roark.", "year": 2001}, {"title": "A classifier-based parser with linear run-time complexity", "author": ["Sagae", "Lavie2005] Kenji Sagae", "Alon Lavie"], "venue": "In Proc. IWPT", "citeRegEx": "Sagae et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2005}, {"title": "Bayesian symbol-refined tree substitution grammars for syntactic parsing", "author": ["Yusuke Miyao", "Akinori Fujino", "Masaaki Nagata"], "venue": "In Proc. ACL", "citeRegEx": "Shindo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shindo et al\\.", "year": 2012}, {"title": "Parsing with compositional vectors", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proc. ACL", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proc. EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A latent variable model for generative dependency parsing", "author": ["Titov", "Henderson2007] Ivan Titov", "James Henderson"], "venue": "In Proc. IWPT", "citeRegEx": "Titov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2007}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proc. ICLR", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Joint POS tagging and transition-based constituent parsing in Chinese with non-local features", "author": ["Wang", "Xue2014] Zhiguo Wang", "Nianwen Xue"], "venue": "In Proc. ACL", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Feature optimization for constituent parsing via neural networks", "author": ["Wang et al.2015] Zhiguo Wang", "Haitao Mi", "Nianwen Xue"], "venue": "In Proc. ACL-IJCNLP", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "In Proc. EMNLP", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Lei Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In Proc. ICML", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "The Penn Chinese TreeBank: Phrase structure annotation of a large corpus", "author": ["Xue et al.2005] Naiwen Xue", "Fei Xia", "Fu-dong Chiou", "Marta Palmer"], "venue": null, "citeRegEx": "Xue et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2005}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "In Proc. ICLR", "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Zhang", "Clark2011] Yue Zhang", "Stephen Clark"], "venue": "Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Fast and accurate shift-reduce constituent parsing", "author": ["Zhu et al.2013] Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu"], "venue": "In Proc. ACL", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 41, "context": "In the last few years, language model results that substantially improve over long-established state-ofthe-art baselines have been obtained using RNNs (Zaremba et al., 2015; Mikolov et al., 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al.", "startOffset": 151, "endOffset": 195}, {"referenceID": 26, "context": "In the last few years, language model results that substantially improve over long-established state-ofthe-art baselines have been obtained using RNNs (Zaremba et al., 2015; Mikolov et al., 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al.", "startOffset": 151, "endOffset": 195}, {"referenceID": 0, "context": ", 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al., 2015), image caption generation (Xu et al.", "startOffset": 94, "endOffset": 117}, {"referenceID": 39, "context": ", 2015), image caption generation (Xu et al., 2015), and dialogue generation (Wen et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 38, "context": ", 2015), and dialogue generation (Wen et al., 2015).", "startOffset": 33, "endOffset": 51}, {"referenceID": 21, "context": "While several transition-based neural models of syntactic generation exist (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these rely on bottom-up construction of syntactic structure.", "startOffset": 75, "endOffset": 169}, {"referenceID": 28, "context": "Bottom-up construction orders are appealing since they permit well-known shift-reduce or leftcorner parsing algorithms to be exploited for inference; however, they limit the use of top-down grammar information, which is useful for generation (Roark, 2001).", "startOffset": 242, "endOffset": 255}, {"referenceID": 21, "context": "Surprisingly\u2014although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)\u2014 parsing with the generative model obtains significantly better results than parsing with the discriminative model.", "startOffset": 107, "endOffset": 139}, {"referenceID": 24, "context": "Surprisingly\u2014although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)\u2014 parsing with the generative model obtains significantly better results than parsing with the discriminative model.", "startOffset": 107, "endOffset": 139}, {"referenceID": 14, "context": "Our transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970).", "startOffset": 240, "endOffset": 254}, {"referenceID": 12, "context": "Our transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the \u201clinearized\u201d parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).", "startOffset": 64, "endOffset": 353}, {"referenceID": 12, "context": "Our transition set is closely related to the operations used in Earley\u2019s algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the \u201clinearized\u201d parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).", "startOffset": 64, "endOffset": 436}, {"referenceID": 8, "context": "(2015) and to the top-down, left-to-right decompositions of trees used in by Roark (2001) and by Charniak (2010).", "startOffset": 97, "endOffset": 113}, {"referenceID": 43, "context": ", assuming binarized trees, as is the case in prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013).", "startOffset": 133, "endOffset": 197}, {"referenceID": 21, "context": "arbitrary tree structures in a bottom-up order (Henderson, 2004).", "startOffset": 47, "endOffset": 64}, {"referenceID": 11, "context": "The output buffer, stack, and history are sequences that grow unboundedly, and to obtain representations of them we use recurrent neural networks to \u201cencode\u201d their contents (Cho et al., 2014).", "startOffset": 173, "endOffset": 191}, {"referenceID": 13, "context": "To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015).", "startOffset": 93, "endOffset": 112}, {"referenceID": 13, "context": "First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015).", "startOffset": 138, "endOffset": 179}, {"referenceID": 33, "context": "Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task.", "startOffset": 88, "endOffset": 106}, {"referenceID": 20, "context": "To further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu and Blunsom, 2015; Goodman, 2001).", "startOffset": 117, "endOffset": 160}, {"referenceID": 5, "context": "To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992).", "startOffset": 76, "endOffset": 96}, {"referenceID": 40, "context": "1 (CTB) (Xue et al., 2005).", "startOffset": 8, "endOffset": 26}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.", "startOffset": 14, "endOffset": 31}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.", "startOffset": 14, "endOffset": 60}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.", "startOffset": 14, "endOffset": 85}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.4 Vinyals et al. (2015) \u2013 WSJ only D 90.", "startOffset": 14, "endOffset": 114}, {"referenceID": 20, "context": "Model type F1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.4 Vinyals et al. (2015) \u2013 WSJ only D 90.5 Petrov and Klein (2007) G 90.", "startOffset": 14, "endOffset": 156}, {"referenceID": 4, "context": "1 Bod (2003) G 90.", "startOffset": 2, "endOffset": 13}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.", "startOffset": 2, "endOffset": 41}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.", "startOffset": 2, "endOffset": 78}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.", "startOffset": 2, "endOffset": 114}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.", "startOffset": 2, "endOffset": 144}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.1 Vinyals et al. (2015) \u2013 single S 92.", "startOffset": 2, "endOffset": 173}, {"referenceID": 4, "context": "1 Bod (2003) G 90.7 Shindo et al. (2012) \u2013 single G 91.1 Shindo et al. (2012) \u2013 ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.1 Vinyals et al. (2015) \u2013 single S 92.5 Vinyals et al. (2015) \u2013 ensemble S 92.", "startOffset": 2, "endOffset": 211}, {"referenceID": 38, "context": "1 Model type F1 Zhu et al. (2013) D 82.", "startOffset": 16, "endOffset": 34}, {"referenceID": 33, "context": "6 Wang et al. (2015) D 83.", "startOffset": 2, "endOffset": 21}, {"referenceID": 33, "context": "6 Wang et al. (2015) D 83.2 Huang and Harper (2009) D 84.", "startOffset": 2, "endOffset": 52}, {"referenceID": 7, "context": "2 Charniak (2000) G 80.", "startOffset": 2, "endOffset": 18}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.", "startOffset": 2, "endOffset": 15}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.", "startOffset": 2, "endOffset": 46}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.3 Zhu et al. (2013) S 85.", "startOffset": 2, "endOffset": 71}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.3 Zhu et al. (2013) S 85.6 Wang and Xue (2014) S 86.", "startOffset": 2, "endOffset": 98}, {"referenceID": 3, "context": "8 Bikel (2004) G 80.6 Petrov and Klein (2007) G 83.3 Zhu et al. (2013) S 85.6 Wang and Xue (2014) S 86.3 Wang et al. (2015) S 86.", "startOffset": 2, "endOffset": 124}, {"referenceID": 21, "context": "This pattern has been observed before in neural parsing by Henderson (2004), who hypothesized that larger, unstructured conditioning contexts are harder to learn from, and provide opportunities to overfit.", "startOffset": 59, "endOffset": 76}, {"referenceID": 21, "context": "This pattern has been observed before in neural parsing by Henderson (2004), who hypothesized that larger, unstructured conditioning contexts are harder to learn from, and provide opportunities to overfit. Our discriminative model conditions on the entire history, stack, and buffer, while our generative model only accesses the history and stack. The fully discriminative model of Vinyals et al. (2015) was able to obtain results similar to those of our generative model (albeit using much larger training sets obtained through semisupervision) but similar results to those of our discriminative parser using the same data.", "startOffset": 59, "endOffset": 404}, {"referenceID": 41, "context": "Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990).", "startOffset": 144, "endOffset": 201}, {"referenceID": 26, "context": "Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990).", "startOffset": 144, "endOffset": 201}, {"referenceID": 15, "context": "Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990).", "startOffset": 144, "endOffset": 201}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991).", "startOffset": 89, "endOffset": 130}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model.", "startOffset": 90, "endOffset": 426}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001).", "startOffset": 90, "endOffset": 863}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take.", "startOffset": 90, "endOffset": 880}, {"referenceID": 1, "context": "Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neuralnetwork\u2013based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize a generative parsing model based on a leftcorner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a; Buys and Blunsom, 2015b). Modeling generation top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take.", "startOffset": 90, "endOffset": 931}, {"referenceID": 17, "context": "computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014).", "startOffset": 67, "endOffset": 112}, {"referenceID": 8, "context": "While an RNNG is not a processing model (it is a grammar), the fact that it is left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010).", "startOffset": 236, "endOffset": 252}], "year": 2016, "abstractText": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.", "creator": "LaTeX with hyperref package"}}}