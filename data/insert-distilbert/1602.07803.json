{"id": "1602.07803", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2016", "title": "Automated Word Prediction in Bangla Language Using Stochastic Language Models", "abstract": "word completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. all they can have profound impact on driving the typing of disable people. our work is based on word ordering prediction on bangla sentence by using stochastic, i. e. enhanced n - gram language comprehension model such such as unigram, bigram, trigram, deleted interpolation and backoff models for auto completing a sentence long by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. now we use large text data corpus of bangla language of different word types to predict correct word with the accuracy as much as possible. we have found promising results. we hope that changing our work will impact on the baseline for automated bangla typing.", "histories": [["v1", "Thu, 25 Feb 2016 05:35:16 GMT  (373kb)", "http://arxiv.org/abs/1602.07803v1", "in International Journal in Foundations of Computer Science &amp; Technology (IJFCST) Vol.5, No.6, November 2015"]], "COMMENTS": "in International Journal in Foundations of Computer Science &amp; Technology (IJFCST) Vol.5, No.6, November 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["md masudul haque", "md tarek habib", "md mokhlesur rahman"], "accepted": false, "id": "1602.07803"}, "pdf": {"name": "1602.07803.pdf", "metadata": {"source": "CRF", "title": "AUTOMATED WORD PREDICTION IN BANGLA LANGUAGE USING STOCHASTIC LANGUAGE MODELS", "authors": ["Md. Masudul Haque", "Tarek Habib", "Mokhlesur Rahman"], "emails": [], "sections": [{"heading": null, "text": "DOI:10.5121/ijfcst.2015.5607 67\nWord completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. They can have profound impact on the typing of disable people. Our work is based on word prediction on Bangla sentence by using stochastic, i.e. N-gram language model such as unigram, bigram, trigram, deleted Interpolation and backoff models for auto completing a sentence by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. We use large data corpus of Bangla language of different word types to predict correct word with the accuracy as much as possible. We have found promising results. We hope that our work will impact on the baseline for automated Bangla typing.\nKEYWORDS\nWord prediction, stochastic model, natural language processing, corpus, N-gram, deleted interpolation, backoff method."}, {"heading": "1. INTRODUCTION", "text": "Auto complete or word completion works so that the user types the first letter or letters of a word and the program provides one or more higher probable words. If the word he intends to type is included in the list he can select it, for example by using the number of keys. If the word that the user wants is not predicted, the user must type the next letter of the predicted word. At this time, the word choice(s) is altered so that the words provided begin with the same letters as those that have been selected or the word that the user wants appears it is selected. Word prediction technique predicts word by analyzing previous word flow for auto completing a sentence with more accuracy by saving maximum keystroke of any user or student and also reduces misspelling. N-gram language model is important technique for word prediction. We use large data corpus for training in N-gram language model for predicting correct Bangla word to complete a Bangla sentence with more accuracy.\nWord prediction means guessing the next word in a sentence . Word prediction helps disabled people for typing,speed up typing speed by decreasing keystrokes,helps in spelling and error detection and it also helps in speech recognition and hand writing recognition. Auto completion decreases misspelling of word. Word completion and word prediction also helps student to spell any word correctly and to type anything with fewer errors [1].\nWe survey many techniques to predict upcoming words of a sentence in different languages especially for English Language But there is no satisfactory analysis on Bangla language to predict words in a sentence. So we apply some N-gram language model, backoff and deleted interpolation techniques to predict Bangla words in a sentence. Word prediction is very important and complex task in natural language processing (NLP) to predict the correct word to complete a sentence in a very meaningful way.\nWe use statistical prediction technique such as N-gram technique as for example unigram, bigram, trigram, bakeoff propagation, deleted interpolation. We also use large data set of text word in Bangla which is collected from different news paper.\nThe paper is constructed as follows: related work in section 2, introduction of N-gram based word prediction in section 3, methodology in section 4, implementation in section 5, result analysis in section 6 and conclusion in section 7."}, {"heading": "2. RELATED WORK", "text": "In an analysis of predicting sentences [2] researcher developed a sentence completion method based on N-gram language models and they derived a k best Viterbi beam search decoder for strongly completing a sentence. We also observed use of Artificial Intelligence [3] for word prediction. Here syntactic and semantic analysis is done using the chart bottom-up technique for word prediction. Another researcher suggests an approach [4] of word prediction via a Clustered Optimal Binary Search Tree. They suggest using a cluster of computer to build optimal binary search tree which also contain extra link so that bigram and the trigram of the language also presented to achieve optimal performance of word prediction. Here the researcher does a lexical analysis of most probable appeared word in users\u2019 text.\nIn a paper of a learning classification based approach word prediction [5] they present an effective method of word prediction using machine learning and new feature extraction and selection techniques adapted from Mutual Information (MI) and Chi square (\u03a7 2 ). Some researchers use N-gram language model for word completion in Urdu language [6] and in Hindi language [7] for detecting disambiguation in Hindi word. There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].\nThere are different word prediction tools such as AutoComplete by Microsoft, AutoFill by Google Chrome, TypingAid [11], LetMeType [12] etc. There are some tools for word completion in Bangla language such as Avro [13] . Some other Bangla software provide word completion features only but theses software do not give word prediction or sentence completion features. That is why we show word prediction process by using N-gram language model to complete a Bangla sentence in our paper.\n3. N-GRAM BASED WORD PREDICTION\nN-gram language model is a type of probabilistic language model where the approximate matching of next item is very high. Probability is based on counting things or word in most cases. The probability of a word depends on the previous word which is called Markov assumption. Unigram looks single item from a given sequence. Bigram is called first-order Markov model which looks one word into the past and trigram is second-order Markov model which looks two words into the past and quadrigram is third-order Markov model which looks three words into the past and similarly an N-gram language model is N-1 Markov model which looks N-1 words into the past[14]. Thus the general equation for this N-gram approximation to the conditional probability of the next word in a sequence, w1,w2, ...,wn, is:\n(1)\nIf N = 1, 2, 3 in (1), the model becomes unigram, bigram and trigram language model, respectively, and so on.\nFor example, using unigram probability of the sentence \" I want to eat \" is,\nP(I want to eat) = P(I) \u00d7 P(want) \u00d7 P(to) \u00d7 P(eat)\nNow, using bigram probability of the sentence \"I want to eat \" is , P(I want to eat) = P( I | <start>) \u00d7 P(want| I) \u00d7 P(to | want) \u00d7P( eat | to)\nNow, using trigram probability of the sentence \"I want to eat \" is , P(coming back to the) = P(I | <s> <s>) \u00d7 P(want | <s> I) \u00d7 P(to | I want) \u00d7 P(eat | want to)\nNow if any of these four words are not in the training corpus then the probability of the sentence will be zero for the cause of multiplication. So in this statistical method if we want to consider these words then we need a huge data corpus that must contain all the words of the language. So there may arise the problems like:\n\u2022 Many entries in corpus are with low frequency \u2022 Probability of a word sequence will be very low or zero\nIf an entry does not exist in corpus then the probability of the sentence will become zero because of multiplication.\nTo solve the problem, we have applied back-off and deleted interpolation model. In the backoff method, for N = 3 in (1), i.e. for a trigram model, the word sequences will follow trigram probabilities at first; if it could not match then word sequences will follow bigram model; if it also could not match then word sequence will follow unigram model and predict at least a word. As the system is for word prediction and it is playing with word sequence, so though any probability of word sequence is zero for multiplication, it will predict at least a word. The Backoff N-gram modelling is a nonlinear method. The difference is that in backoff , if there are non-zero trigram counts, it rely on trigram counts and don\u2019t interpolate bigram and unigram\ncounts at all [14]. The N-gram version (N = c) of backoff model can be represented as follows:\n) n\nNn |w n P(w)\nn |w\nn P(w\n1 1 1 1 \u2212 +\u2212 \u2248 \u2212\n( ) ( )............ ............, 02 1 2 1( 1) ( 1)|P w w w w if C w w w wi ii i i ii c i c >\u2212 \u2212 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\n( ) ( ) ( ) 1 ............ ............ ............ 2 1 2 1( 2) ( 1)\n2 1( 2)\n, 0\nand 0\nw w w w w w wii i i ii c i c\nw w w wii ii c\nP if C\nC\n\u03b1 \u2212 \u2212 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\n\u2212 \u2212\u2212 \u2212 >\n=\n( )....... 1 2( 1)|P w w w wi i ii c =\u2212 \u2212\u2212 \u2212 (2)\n( )1\n.\n.\n.\n, c i P w\u03b1 \u2212\notherwise\nThe deleted interpolation algorithm combining different N-gram orders by linearly interpolating all three models when they are computing any trigram. The detailed interpolation formula is as follows:\n(3)\n3.1. WHY N-GRAM BASED WORD PREDICTION?\nWe choose N-gram based word prediction system because these are more statistical approach to predict upcoming word in a sentence with more accuracy and N-gram language models provide a natural approach to the construction of sentence completion systems. To measure the probabilities by statistical model like N-gram data is divided into training set and test set. We compute the probabilities of a test sentence from trained corpus which is designed very carefully. Suppose a word \u201c \u201d occurs 400 times in a corpus of one million Bangla words. Then the estimated\nprobability for the word \u201c \u201d is 4 0 0 0 .0 0 0 4 . 1 0 0 0 0 0 0 ="}, {"heading": "4. METHODOLOGY", "text": "Our approach starts with a sentence fragment and come up with a word using a stochastic language model as shown in Fig. 2. We use five stochastic language models, namely unigram, bigram, trigram, backoff and deleted interpolation.\nConditional frequency distributions come into play when language model learning takes place for word prediction. A conditional frequency distribution refers to collection of frequency distribution for the same experiment, run under different conditions. We intend to predict a word's text (outcome), based on the text of the word that it follows (context). To predict the outcomes of an experiment, a training corpus is examined first, where the context and outcome for each run of the experiment are known. When presented a new run of the experiment, the outcome that\n( ) ( ) ( ) ( ) ( ) ( ) ( )1 1 1 2 1 1 2 2 1 2 2 1 3 2 ^ | |P n n n w w w w P w w P w w w P wn n nn n n n n n n n w\u03bb \u03bb \u03bb\u2212 \u2212 \u2212= + + \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212\noccurred most frequently for the experiment's context is simply chosen. The total process is shown in Fig. 3."}, {"heading": "5. IMPLEMENTATION", "text": "Our work starts with a training corpus of size 0.25 million words. The corpus has been constructed from the popular Bangla newspaper the daily \u201cProthom Alo\u201d. The corpus contains 14,872 word forms. We have taken some test data in a file from the training corpus. After that comprehensive unigram, bi-gram and tri-gram statistics have been automatically generated and stored the sentences with predicted words in a file. Thus we have constructed N-gram models of word prediction by counting frequencies of words in a very large corpus, i.e. database and determine probabilities using N-gram. We employ a Java program in order to perform our experiments using unigram, bigram, trigram, backoff and deleted interpolation language models.\nFirst, we start working on unigram model. We split the entire corpus into two parts, namely training part and testing part. We use holdout method [14] for selecting the proportion of data reserved for training and for testing. We split the corpus at the proportion of two-thirds for training and one-third for testing. In order to avoid model overfitting problem, i.e. to have low training error as well as low generalization error, we use a validation set. In accordance with this approach, we divide the original training data into two smaller subsets. One of the subsets is used for training, while the other one, i.e. the validation set, is used for calculating the generalization error. We fix two-thirds of the training set for model building while the remaining one-third is used for error estimation. We repeat holdout method five times in order to find the best model. After finding out our best model, we compute the accuracy of the model from the test set. We evaluate the performance of the classifier by varying the length of test sentence. Likewise we work on bigram, trigram, backoff and deleted interpolation models. All results are shown in Table I and Fig. 4. For deleted interpolation model, we empirically found the linear weights \u03bb1 = 0.5, \u03bb2 = 0.33 and \u03bb3 = 0.17 such that they sum to 1:\n\u2211 =\n= 3\n1 .1 i\ni\u03bb (4)\nSo, the probability equation for our deleted interpolation becomes as follows:\n1 2 1 2 1 \u02c6( ) 0.5 ( ) 0.33 ( ) 0.17 ( ).n n n n n nn n nP w w w P w w w P w w P w\u2212 \u2212 \u2212 \u2212 \u2212= + + (5)\nWe get the following probability for the following sentence or sample test data.\nTest sample no. 1\nTest sentence fragment:\nWord to be predicted:\nFor unigram model, we get the following results.\nWord to predicted:\nP( ) = P( ) \u00d7 P( ) \u00d7 P( ) \u00d7 P( ) \u00d7\nP( ) \u00d7 P( ) \u00d7 P( ) = 5.09E-5 \u00d7 1.19E-4 \u00d7 8.48E-5 \u00d7 5.94E-4 \u00d7 1.02E-4 \u00d7 2.38E-4*\n0.01\nFor bigram model, we get the following results.\nWord predicted:\nP( ) = P( | <s>) \u00d7 P( | ) \u00d7 P( |\n) \u00d7 P( | ) \u00d7 P( | ) \u00d7 P( | ) \u00d7 P( | ) = 0.0 \u00d7 0.33 \u00d7\n0.14 \u00d7 0.2 \u00d7 0.03 \u00d7 0.17 \u00d7 0.14\nFor trigram model, we get the following results.\nWord predicted:\nP( ) = P( | <s> <s>) \u00d7 P( | <s> )\n\u00d7 P( | ) \u00d7 P( | ) \u00d7 P( | ) \u00d7 P( |\n) \u00d7 P( | ) = 0.0 \u00d7 0.0 \u00d7 1.0 \u00d7 1.0 \u00d7 1.0 \u00d7 1.0 \u00d7 1.0\nFor backoff model, we get the following results.\nWord predicted:\nP( ) = P( | <s> <s>) \u00d7 P( | <s>\n) \u00d7 P( | ) \u00d7 P( | ) \u00d7 P( | ) \u00d7 P( |\n) \u00d7 P( | ) = 0.0 \u00d7 0.0 \u00d7 1.0 \u00d7 1.0 \u00d7 1.0 \u00d7 1.0 \u00d7 1.0\nFor deleted interpolation model, we get the following results.\nWord predicted:"}, {"heading": "P( | ) = 0.5 \u00d7 P( | ) + 0.33 \u00d7 P( | ) + 0.17 \u00d7", "text": ""}, {"heading": "P( ) = 0.5 \u00d7 1.0 + 0.33 \u00d7 0.07 + 0.17 \u00d7 0.1E-3", "text": "Test sample no. 2\nTest sentence fragment:\nWord to be predicted:\nFor unigram model, we get the following results.\nWord predicted:"}, {"heading": "P( ) = P( ) \u00d7 P( ) \u00d7 P( ) = 0.0 \u00d7 1.02 E-4 \u00d7 0.01", "text": "For bigram model, we get the following results.\nWord predicted:\nP( ) = P( | <s>) \u00d7 P( | ) \u00d7 P( | ) = 0.0 \u00d7 0.0 \u00d7 0.33\nFor trigram model, we get the following results.\nWord predicted: Empty string, because there is no word following in the training\ncorpus\nFor backoff model, we get the following results.\nWord predicted:\nP( ) = P( | <s>) \u00d7 P( | ) \u00d7 P( | ) = 0.0 \u00d7 0.0 \u00d7 0.33\nFor deleted interpolation model, we get the following results.\nWord predicted:"}, {"heading": "P( | ) = 0.5 \u00d7 P( | ) + 0.33 \u00d7 P( | ) + 0.17 \u00d7 P( )", "text": "= 0.5 \u00d7 0.0 + 0.33 \u00d7 0.33 + 0.17 \u00d7 5.1E-5"}, {"heading": "6. RESULT ANALYSIS", "text": "In order to understand the merits of our work in predicting words in Bangla, we need to delve into all results found. We see from Table I and Fig. 4 that trigram, backoff and deleted interpolation\nlanguage model have performed almost in the same trend-line. Bigram model perform modestly, whereas unigram models performance is obviously very poor. The average accuracies of all models deployed are shown in Table II and Fig. 5. We see, from Fig. 5 and Table II, that the average accuracies of trigram, backoff and deleted interpolation model are close, where the accuracy (63.5%) of the backoff model is the maximum. Some results of predictions by all models deployed are given below.\nPerformance of word predictor depends on some important component such as which language model is used, the average length of the sentence in the language, amount of trained data set etc . In our work we use 5 to 20 words in a sentence of 100 sentence for each execution from .25 million trained data set. We see from Fig. 5 that initially performance of word prediction increases with the increase of N in N-gram language model."}, {"heading": "4. CONCLUSIONS", "text": "N-gram based word prediction works well for English but we use for Bangla language which is more challenging to get 100% performance because it depends on training corpus of large data. We use more smoothed corpus data and increase data corpus size in future to get higher performance. Here in our work we use single word prediction but we can develop our process to predict a set of word to complete a sentence in a very meaningful way."}], "references": [{"title": "Predicting Sentences using N-Gram Language Models", "author": ["Steffen Bickel", "Peter Haider", "Tobais Scheffer", "(2005"], "venue": "Proceedings of Conference on Empirical Methods in Natural language Processing.  Table II. Results of all models used Model  Average Accuracy Unigram 21.24 Bigram 45.84 Trigram 63.04 Backoff 63.50 Deleted Interpolation 62.86 Fig. 5. Graphs of the results of all models used International Journal in Foundations of Computer Science & Technology (IJFCST) Vol.5, No.6, November 2015 75", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Application of Artificial Intelligence Methods in a Word-Prediction Aid", "author": ["Nestor Garay-Vitoria", "Julio Gonzalez-Abascal", "(2005"], "venue": "Laboratory of Human-Computer Interaction for Special Needs.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "Word Prediction via a Clustered Optimal Binary Search Tree", "author": ["Eyas El-Qawasmeh", "(2004"], "venue": "International Arab Journal of Information Technology, Vol. 1, No. 1.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 0}, {"title": "A Learning-Classification Based Approach for Word Prediction", "author": ["Hisham Al-Mubaid"], "venue": "The International Arab Journal of Information Technology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A Stochastic Prediction Interface for Urdu", "author": ["Qaiser Abbas"], "venue": "Intelligent Systems and Applications ,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Disambiguating Hindi Words Using N- Gram Smoothing Models", "author": ["Umrinder Pal Singh", "Vishal Goyal", "Anisha Rani"], "venue": "International Journal of Engineering Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "N-gram based Statistical Grammar Checker for Bangla and English", "author": ["Jahangir Alam", "Naushad Uzzaman", "Mumit khan"], "venue": "In Proceedings of International Conference on Computer and Information Technology", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Checking the Correctness of Bangla Words using N-Gram", "author": ["Nur Hossain Khan", "Gonesh Chandra Saha", "Bappa Sarker", "Md"], "venue": "Habibur Rahman,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Verification of Bangla Sentence Structure using N-Gram", "author": ["Nur Hossain Khan", "Md. Farukuzzaman Khan", "Md. Mojahidul Islam", "Md. Habibur Rahman", "Bappa Sarker", "(2014"], "venue": "Global Journal of Computer Science and Technology , vol.14 ,issue-1 .", "citeRegEx": "10", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "In an analysis of predicting sentences [2] researcher developed a sentence completion method based on N-gram language models and they derived a k best Viterbi beam search decoder for strongly completing a sentence.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "We also observed use of Artificial Intelligence [3] for word prediction.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "Another researcher suggests an approach [4] of word prediction via a Clustered Optimal Binary Search Tree.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "In a paper of a learning classification based approach word prediction [5] they present an effective method of word prediction using machine learning and new feature extraction and selection techniques adapted from Mutual Information (MI) and Chi square (\u03a7 2 ).", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "Some researchers use N-gram language model for word completion in Urdu language [6] and in Hindi language [7] for detecting disambiguation in Hindi word.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "Some researchers use N-gram language model for word completion in Urdu language [6] and in Hindi language [7] for detecting disambiguation in Hindi word.", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": "There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "There are some related works also on Bangla language using N-gram language model such as grammar checker of Bangla language [8], checking the correctness of Bangla word [9] and verification of Bangla sentence structure [10].", "startOffset": 219, "endOffset": 223}], "year": 2015, "abstractText": "Word completion and word prediction are two important phenomena in typing that benefit users who type using keyboard or other similar devices. They can have profound impact on the typing of disable people. Our work is based on word prediction on Bangla sentence by using stochastic, i.e. N-gram language model such as unigram, bigram, trigram, deleted Interpolation and backoff models for auto completing a sentence by predicting a correct word in a sentence which saves time and keystrokes of typing and also reduces misspelling. We use large data corpus of Bangla language of different word types to predict correct word with the accuracy as much as possible. We have found promising results. We hope that our work will impact on the baseline for automated Bangla typing.", "creator": "PScript5.dll Version 5.2.2"}}}