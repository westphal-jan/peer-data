{"id": "1602.08225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2016", "title": "Multimodal Emotion Recognition Using Multimodal Deep Learning", "abstract": "to enhance the performance of affective models and reduce the cost of acquiring physiological signals for real - built world applications, we adopt multimodal deep learning cognitive approach to construct affective models from multiple physiological signals. for specific unimodal enhancement task, we indicate that the best recognition accuracy of 82. 11 % on seed dataset is achieved with shared representations generated by deep autoencoder ( dae ) interaction model. for multimodal facilitation tasks, we demonstrate that the bimodal deep autoencoder ( bdae ) achieves below the mean accuracies of 91. 01 % and 83. 25 % on seed and deap datasets, respectively, which are much superior to the state - of - the - art approaches. for cross - modal learning task, our experimental results demonstrate that the mean accuracy of 66. 34 % is achieved alternatively on seed dataset through shared representations generated by eeg - based dae as binding training samples and shared representations generated by eye - based dae as interaction testing sample, then and somewhat vice versa.", "histories": [["v1", "Fri, 26 Feb 2016 07:43:14 GMT  (162kb)", "http://arxiv.org/abs/1602.08225v1", null]], "reviews": [], "SUBJECTS": "cs.HC cs.CV cs.LG", "authors": ["wei liu", "wei-long zheng", "bao-liang lu"], "accepted": false, "id": "1602.08225"}, "pdf": {"name": "1602.08225.pdf", "metadata": {"source": "CRF", "title": "Multimodal Emotion Recognition Using Multimodal Deep Learning", "authors": ["Wei Liu", "Wei-Long Zheng", "Bao-Liang Lu"], "emails": ["bllu}@sjtu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n08 22\n5v 1\n[ cs\n.H C\n] 2\n6 Fe\nb 20"}, {"heading": "1 Introduction", "text": "For human-machine interface (HMI), emotion recognition is becoming more and more important. Emotion recognition could be done through texts, pictures and physiological signals. Bravo-Marquez et al. learned an expanded opinion (positive, neutral and negative) lexicon from emoticon annotated tweets [Bravo-Marquez et al., 2015]. Wang and Pal used constraint optimization framework to discover user\u2019 emotions from social media content [Wang and Pal, 2015].\nRecently, many researchers studied emotion recognition from EEG. Liu et al. used fractal dimension based algorithm to recognize and visualize emotions in real time [Liu et al., 2010]. Murugappan et al. employed discrete wavelet transform to extract frequency features from EEG signals and two classifiers are used to classify the features [Murugappan et al., 2010]. Duan et al. found that differential entropy features are more suited for emotion recognition tasks [Duan et al., 2013]. Zheng and Lu employed\n\u2217\nCorresponding author\ndeep neural network to classify EEG signals and examined critical bands and channels of EEG for emotion recognition [Zheng and Lu, 2015].\nBesides EEG signals, eye movement data can be used to find out what is attracting users\u2019 attention and observe users\u2019 unconscious behaviors. It is widely believed that when people are in different emotions, the paradigm of eye movements and pupil diameters will be different. Nelson et al. studied the relationship between attentional bias to threat and anxiety by recording eye movement signals in different situations [Nelson et al., 2015]. Bradley and Lang recorded eye movement signals to study the relationship between memory, emotion and pupil diameters [Bradley and Lang, 2015].\nTo deal with information from different modalities, Yang et al. proposed an auxiliary information regularized machine which treats different modalities with different strategies [Yang et al., 2015]. Zhang et al. proposed a multimodal ranking aggregation framework for fusion of multiple visual tracking algorithms [Zhang et al., 2015]. In [Ngiam et al., 2011], the authors built a single modal deep autoencoder and a bimodal deep autoencoder to generate shared representations of images and audios. Srivastava and Salakhutdinov extended the methods developed by [Ngiam et al., 2011] to bimodal deep Boltzmann machines to handle multimodal deep learning problems [Srivastava and Salakhutdinov, 2014].\nAs for multimodal emotion recognition, Verma and Tiwary carried out emotion classification experiments with EEG singals and peripheral physiological signals [Verma and Tiwary, 2014]. Lu et al. used two different fusion strategies for combining EEG and eye movement data: feature level fusion and decision level fusion [Lu et al., 2015]. Their experimental results indicated that the best recognition accuracy was achieved by using fuzzy integral method in decision level fusion. Vinola and Vimaladevi gave a detailed survey on human emotion recognition and listed many other multimodal datasets and methods [Vinola and Vimaladevi, 2015].\nTo our best knowledge, there is no research work reported in the literature dealing with emotion recognition from multiple physiological signals using multimodal deep learning algorithms. In this paper, we propose a novel multimodal emotion recognition method using multimodal deep learning techniques. In Section 2, we will introduce the unimodal deep au-\ntoencoder and bimodal deep autoencoder. Section 3 contains contents about data pre-proessing, feature extraction and experiment settings. The experiment results are described in Section 4. Following discusses in Section 5, conclusions and future work are represented in Section 6."}, {"heading": "2 Multimodal Deep Learning", "text": ""}, {"heading": "2.1 Restricted Boltzmann Machine", "text": "A restricted Boltzmann machine (RBM) is an undirected graph model, which has a visible layer and a hidden layer. Connections exist only between visible layer and hidden layer and there is no connection either in visible layer or in hidden layer. Assuming visible variables v \u2208 {0, 1}M and hidden variables h \u2208 {0, 1}N , we have the following energy function E:\nE(v,h; \u03b8) = \u2212\nM \u2211\ni=1\nN \u2211\nj=1\nWijvihj \u2212\nM \u2211\ni=1\nbivi \u2212\nN \u2211\nj=1\najhj (1)\nwhere \u03b8 = {a,b,W} are parameters, Wij is the symmetric weight between visible unit i and hidden unit j, bi, aj are bias terms of visible unit and hidden unit, respectively. With energy function, we can get the joint distribution over the visible and hidden units:\np(v,h; \u03b8) = 1\nZ(\u03b8) exp(E(v,h; \u03b8)) (2)\nZ(\u03b8) = \u2211\nv\n\u2211\nh\nexp(E(v,h; \u03b8))\nwhere Z(\u03b8) is the normalization constant. From Eqs. (1) and (2), we can derive the conditional distribution over hidden units h and visible units v as follows:\np(h|v; \u03b8) =\nN \u220f\nj=1\np(hj |v)\np(v|h; \u03b8) =\nM \u220f\ni=1\np(vi|h)\nwith\np(hj = 1|v; \u03b8) = g\n( M \u2211\ni=1\nWijvi + aj\n)\np(vj = 1|h; \u03b8) = g\n( N \u2211\nj=1\nWijhj + bi\n)\ng(x) = 1/(1 + exp(\u2212x))\nGiven a set of visible variables {vn}Nn=1, the derivative of log-likelihood with respect to weight W can be calculated from Eq. (2):\n1\nN\nN \u2211\ni=1\n\u2202 logP (vn; \u03b8)\n\u2202Wij = EPdata [vihj ]\u2212 EPmodel [vihj ]\nIn this paper, we use Contrastive Divergence (CD) algorithm [Hinton, 2002] or Persistent CD algorithm [Tieleman, 2008] to train a RBM."}, {"heading": "2.2 Model construction", "text": "To enhance emotion recognition accuracy by combining EEG and eye movement data, we adopt a Bimodal Deep autoencoder (BDAE) to extract shared representations of EEG and eye movement data. When only one modality is available, the unimodal deep autoencoder (DAE) is applied to extract shared representations. These two kinds of deep autoencoder models are depicted in Figure 1.\nBDAE training To train BADE, we first trained two RBMs for EEG signals and eye movement data, respectively, i.e., the first two layers in Figure 1(b). After training respective RBMs, two hidden layers indicated by hEEG and hEye were linked together directly and we treated the joint hidden layer as the visible layer of an upper RBM. When unfolding the stacked RBMs into a bimodal deep autoencoder, we kept the weights tied. From Figure 1(b), we can see that W1,W2,W3 and WT1 ,W T 2 ,W T 3 were tied weights. At last, we used unsupervised back-propagation algorithm to finely tune the weights and bias.\nDAE training A similar method was used when training DAE. Only one RBM was constructed for EEG features or eye tracking features, and the hidden layer of the first RBM was treated as the visible layer of the upper RBM. However, when unfolding the stacked RBMs, we only kept the weights of first EEG (or eye) layer and top EEG (or eye) layer tied. From Figure 1(a), we can see that W1 and WT1 were tied weights while other weights were not. Other weights and bias could be trained with CD algorithm or Persistent CD algorithm. Unsupervised back-propagation was also needed to finely tune the parameters.\nThere are three steps in total. The first step is to train the DAE network or the BDAE network. It is worth noting that both modality information is needed when training those autoencoders. We will call this step feature selection. The second step is supervised training. After training autoencoders, we can use them to generate shared representations and these shared representations can then be used to train a classifier. And the last step is a testing process, from which the recognition results are produced."}, {"heading": "3 Experiment settings", "text": ""}, {"heading": "3.1 Dataset", "text": "Two public datasets, the SEED dataset1 and the DEAP dataset2, were used in this paper. The SEED dataset was first introduced in [Zheng and Lu, 2015]. This dataset contains EEG singals and eye movement signals from 15 subjects during watching emotional movie clips. The dataset contains 15 movie clips and each clip lasts about 4 minutes long. The EEG signals are of 62 channels at a sampling rate of 1000 Hz and the eye movement signals contain information about blink, saccade fixation and so on. In order to compare our proposed method with [Lu et al., 2015], we use the same data\n1 http://bcmi.sjtu.edu.cn/\u02dcseed/index.html 2 http://www.eecs.qmul.ac.uk/mmv/datasets/deap/readme.html\nas in [Lu et al., 2015], that is, 27 data files from 9 subjects. For every data file, the data from the subjects watching the first 9 movie clips are used as training samples and the rest are used as test samples.\nThe DEAP dataset was first introduced in [Koelstra et al., 2012]. The EEG signals and peripheral physiological signals of 32 participants were recorded when they were watching music videos. The dataset contains 32 channel EEG signals and 8 peripheral physiological signals. The emotional music videos include 40 one-minute long small clips and subjects were asked to do self-assessment by assigning values from 1 to 9 to five different status, namely, valence, arousal, dominance, liking and familiarity. In order to compare the performance of our proposed method with previous results in [Rozgic et al., 2013] and [Li et al., 2015], we did not take familiarity into consideration. We divided the trials into two different classes according to the assigned values. The threshold we chose is 5, and the tasks can be treated as four binary classification problems, namely, high or low valence, arousal, dominance and liking. Among all of the data, 90% samples were used as training data and the rest 10% samples were used as test data."}, {"heading": "3.2 Feature Extraction", "text": "SEED dataset\nPower Spectral Density (PSD) and Differential Entropy (DE) features were extracted from EEG data. Both two kinds of features contain five frequency bands: delta (1\u20134Hz), theta (4\u20138Hz), alpha (8\u201314Hz), beta (14\u201331), and gamma (31\u2013 50Hz). As for eye movement data, we used the same features as in [Lu et al., 2015], which were listed in Table 1. The extracted EEG features and eye movement features were then scaled between 0 and 1 and the scaled features were used as the visible units of BDAE or DAE network.\nDEAP dataset\nInstead of extracting features manually, we used the downloaded preprocessed data directly as the inputs of BDAE network and DAE network to generate shared representations of EEG signals and peripheral physiological signals. First, the EEG signals and peripheral physiological signals were separated and then the signals were segmented into 63 seconds. After segmentation, we combined different channel data of the same time period (one second), forming the input signals of BDAE network. At last, BDAE network generates shared representations."}, {"heading": "3.3 Classification", "text": "The shared representations generated by BDAE network or DAE network were used to train a classifier. In this paper, linear SVM was used. Inspired by [Guo and Guo, 2005], we performed experiments on the following three kinds of emotion recognition tasks to examine the efficiency of our proposed method.\n(1) For unimodal enhancement task, we built a unimodal DAE network for EEG features or eye movement features to reconstruct both modalities. The mid-layer shared representations were extracted to train a classifier. In this task, only SEED dataset was used.\n(2) In multimodal facilitation task, both modalities were needed. The shared representations generated by BDAE network were fed into linear SVM to train a classifier. Both SEED dataset and DEAP dataset were used.\n(3) For cross-modal learning task, we built two unimodal DAEs for EEG features and eye movement features. The mid-layer outputs were extracted as shared representations. Then we used extracted features of one modality as training samples and extracted features of the other modality as testing samples. In this task, only SEED dataset was used."}, {"heading": "4 Results", "text": ""}, {"heading": "4.1 Unimodal enhancement", "text": "In unimodal enhancement task, we used EEG signals to reconstruct information of two modalities. Once the DAE network was trained, we could use it as a feature selector to generate shared representations, even if only one modality information is available. For eye movement data, the process was the same as when only EEG signals were available.\nFigure 2 is the summary of all unimodal enhancement results. We can see from Figure 2 that the DAE model performed best on both EEG features and eye movement features.\nFor EEG-based unimodal enhancement experiments, we constructed an affective model using EEG features of different frequency bands. The experimental results are shown in Table 2. After that an EEG-based DAE network was built to reconstruct both EEG and eye movement features and the shared representations were used as new features to classify emotions. The EEG-only DAE results are shown in Table 3.\nFor eye-based unimodal enhancement experiments, the processes were the same. We carried out experiments using eye movement features. We linked all eye movement features listed in Table 1 together to classify different emotions and the recognition accuracy of 79.64% is achieved. Then, eye movement features were used to train the DAE network to reconstruct both EEG and eye movement features. Emotion recognition accuracies, as shown in Table 4, were got with shared representations.\nWhen only EEG features were used, we can see from Tables 2 and 3, the DAE network increased the recognition rate from 77.64% to 81.19% and the standard deviation for the best accuracy is 13.82, which is smaller than 17.19. We also compared our results with [Lu et al., 2015].\nIn [Lu et al., 2015], the best result achieved when only EEG signal used was 78.51% and the standard deviation for its best accuracy was 14.32. It is clear that the DAE network is supe-\nrior to the state-of-the-art approach. When only eye movement data were available, the DAE network achieved the highest accuracy of 82.11% (in Table 4) in comparison with the state-of-the-art approach [Lu et al., 2015] (77.80%) and directly using eye movement features (79.46%)."}, {"heading": "4.2 Multimodal Facilitation", "text": "We performed two kinds of different experiments to compare our BDAE network with other models.\n(1) Only single modality is available.\n(2) When both modalities are available, the shared representations are obtained by linking the features directly.\nSEED results Figure 3 shows the summary of multimodal facilitation experiment results. We can see from Figure 3 that our BDAE model has the best performance (91.01%). Besides, the standard deviation of our BDAE model is also the smallest. This indicates that the BDAE model has a good robustness. Table 5 shows the results when we linked the features extracted from EEG signals and eye movement data directly. The last column of Table 5 means that we linked both five frequency bands of EEG signals and eye movement data features directly.\nCompared with Table 2, we can see that when linking different modalities together, the emotion recognition accuracy increased in almost all frequency bands, and the standard deviation becomes smaller.\nThe experimental results using the BDAE model are shown in Table 6. We examined the BDAE model three times and the recognition accuracies shown in Table 6 were average. We can see that the BDAE model achieved the best accuracy of 91.01%, which is higher than those of single modality and directly linking strategy. And the standard deviation of the BDAE model is 8.91, which is the smallest among three different approaches.\nIn [Lu et al., 2015], the authors employed fuzzy integral method to fuse different modalities. The classification accuracy is 87.59% and the deviation is 19.87%. Compared with [Lu et al., 2015], the BDAE model enhanced the performance of affective model significantly.\nDEAP results In previous papers, Rozgic et al. treated the EEG signals as a sequence of overlapping segments and a novel non-parametric nearest neighbor model was employed to extract response-level feature from these segments [Rozgic et al., 2013]. Li et al. used Deep Belief Network (DBN) to automatically extract high-level features from raw EEG signals [Li et al., 2015].\nThe experimental results on the DEAP dataset are shown in Table 7. We compared the BDAE results with results in [Li et al., 2015] and [Rozgic et al., 2013]. As can be seen from Table 7, the BDAE model improved recognition accuracies in all classification tasks.\nFrom the experimental results on the SEED and DEAP datasets, we have demonstrated that the BDAE network can be used to extract shared representations from different modalities and the extracted features have better performance than other features."}, {"heading": "4.3 Cross-modal learning", "text": "The key point of both DAE model and BDAE model is the shared representation. In this section, the cross-modal experiments are carried out to examine whether the shared representations can learn common information between two different modalities.\nIn traditional machine learning framework, a classifier trained by EEG features are usually considered to generate bad results when testing it on eye movement features.\nHowever, things are different when we use the DAE model. The DAE network is thought to to able to learn something in common between different modalities. We can test this by using shared representations generated by EEG features as training samples and shared representations generated by eye features as testing samples, and vice versa.\nBoth settings are examined, and the results are shown in Tables 8 and 9. We first trained a classifier with shared representations generated from EEG fed DAE network, and then tested the classifier with eye movement features generated shared representations. The results are shown in Table 8. As we can see from Table 8, both PSD features and DE features in all frequency bands achieved accuracies more than 60%, and the best performance is 66.23%, This is much higher than 33% of random classification of three emotional states.\nThen the other experiment setting was tested. We used shared representations generated by eye movement features to train the classifier and the EEG-based shared representations were used as testing samples. Table 9 shows the results. Similar to Table 8, all accuracies are larger than 60%, and the best result is 66.45%. From Tables 8 and 9, we can see that the DAE models are able to learn common features between EEG features and eye movement features. Though we do not know what kind of shared representations they really are, we can take advantage of this to improve emotion recognition accuracy.\nIn the last, we analyzed the confusing matrices. Table 10 shows the confusing matrices based on the experiment results for each individual task. For convenient, we only listed the confusing matrices on the SEED dataset. From Table 10, we can see that negative emotions are the hardest to recognize and positive emotions are easiest to recognize. This might indicate that when people are happy or exciting, brain activities have some common patterns while when people are sad, the patterns are not so obvious or the patterns are changing with time."}, {"heading": "5 Discussion", "text": "All of three kinds of emotion recognition mentioned tasks above are important for HMI systems in practice. The multimodal facilitation task allows us using different modalities so that HMI systems could have a better recognition accuracy. Besides, the experiments results have indicated that when both modalities were used, the standard deviation became smaller than before. This phenomenon indicates that our system becomes more reliable. Unimodal enhancement results have shown that if we train the DAE network with two modalities, it is feasible to use only one modal in practice. Inspired by this results, EEG signals might be not needed in practice and only some easily-collected signals are used. In the last, cross-modal learning tries to find out the common features between EEG signals and eye movement data. The experiment results have demonstrated that our shared representations do extract some common features between EEG features and eye movement features."}, {"heading": "6 Conclusions and Future Work", "text": "This paper has shown that by fusing EEG features and other features with bimodal deep autoencoders (BDAE), the shared representations are good features to discriminate different emotions. For the SEED dataset, compared with other feature merging strategies, the BDAE model is better than others with the best accuracy of 89.94%. In order to avoid intricacies during acquiring EEG signals, we have adopted unimodal deep autoencoder model (DAE) to extract shared representations even there was only one modality available. The experimental results on the unimodal enhancement task have shown that the DAE model (82.11%) performs better than using single modality directly (78.51% in [Lu et al., 2015]). In addition, the experimental results on the cross-modal learning task demonstrated that the shared representations contain higher level common features between EEG signals and eye movement features. The affective models with the shared representation performed much better than random classification (33.33%) and achieved the best accuracy of 66.45%.\nAs future work, we will focus on the following issues that we have not covered in this paper. First, we will explore the relationship between unimodal features and shared representations, so that we may find a clear explanation of confusing matrices. Second, we want to go deeper with the performance of the DAE and BDAE networks when parameters change. Besides, more experiments are needed in order to study the stability of the DAE and BDAE networks."}, {"heading": "Acknowledgment", "text": "This work was supported in part by the grants from the National Natural Science Foundation of China (Grant No.61272248) and the National Basic Research Program of China (Grant No.2013CB329401)."}], "references": [{"title": "emotion", "author": ["Margaret M Bradley", "Peter J Lang. Memory"], "venue": "and pupil diameter: Repetition of natural scenes. Psychophysiology, 52(9):1186\u201393,", "citeRegEx": "Bradley and Lang. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "negative", "author": ["Felipe Bravo-Marquez", "Eibe Frank", "Bernhard Pfahringer. Positive"], "venue": "or neutral: Learning an expanded opinion lexicon from emoticon-annotated tweets. In IJCAI\u201915, pages 1229\u2013 1235. AAAI Press,", "citeRegEx": "Bravo.Marquez et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In 2013 6th International IEEE/EMBS Conference on Neural Engineering", "author": ["Ruo-Nan Duan", "Jia-Yi Zhu", "BaoLiang Lu. Differential entropy feature for eeg-based emotion classification"], "venue": "pages 81\u201384. IEEE,", "citeRegEx": "Duan et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Science", "author": ["Jianzeng Guo", "Aike Guo. Crossmodal interactions between olfactory", "visual learning in drosophila"], "venue": "309(5732):307\u2013310,", "citeRegEx": "Guo and Guo. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural Computation", "author": ["Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence"], "venue": "14(8):1771\u20131800,", "citeRegEx": "Hinton. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Deap: A database for emotion analysis", "author": ["Koelstra et al", "2012] Sander Koelstra", "Christian M\u00fchl", "Mohammad Soleymani", "Jong-Seok Lee", "Ashkan Yazdani", "Touradj Ebrahimi", "Thierry Pun", "Anton Nijholt", "Ioannis Patras"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "In SIGIR2015 Workshop on Neuro-Physiological Methods in IR Research", "author": ["Xiang Li", "Peng Zhang", "Dawei Song", "Guangliang Yu", "Yuexian Hou", "Bin Hu. EEG based emotion identification using unsupervised deep feature learning"], "venue": "August", "citeRegEx": "Li et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In 2010 International Conference on Cyberworlds", "author": ["Yisi Liu", "Olga Sourina", "Minh Khoa Nguyen. Real-time EEG-based human emotion recognition", "visualization"], "venue": "pages 262\u2013269. IEEE,", "citeRegEx": "Liu et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In IJCAI\u201915", "author": ["Yifei Lu", "Wei-Long Zheng", "Binbin Li", "Bao-Liang Lu. Combining eye movements", "EEG to enhance emotion recognition"], "venue": "pages 1170\u2013 1176,", "citeRegEx": "Lu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["Murugappan Murugappan", "Nagarajan Ramachandran", "Yaacob Sazali"], "venue": "Classification of human emotion from EEG using discrete wavelet transform. Journal of Biomedical Science and Engineering, 3(04):390\u2013396,", "citeRegEx": "Murugappan et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Distinguishing the roles of trait and state anxiety on the nature of anxiety-related attentional biases to threat using a free viewing eye movement", "author": ["Nelson et al", "2015] Andrea L Nelson", "Christine Purdon", "Leanne Quigley", "Jonathan Carriere", "Daniel Smilek"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "In ICML\u201911", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng. Multimodal deep learning"], "venue": "pages 689\u2013696,", "citeRegEx": "Ngiam et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In 2013 IEEE International Conference on Acoustics", "author": ["Viktor Rozgic", "Shiv N Vitaladevuni", "Ranga Prasad. Robust EEG emotion classification using segment level decision fusion"], "venue": "Speech and Signal Processing, pages 1286\u20131290. IEEE,", "citeRegEx": "Rozgic et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The Journal of Machine Learning Research", "author": ["Nitish Srivastava", "Ruslan Salakhutdinov. Multimodal learning with deep boltzmann machines"], "venue": "15(1):2949\u20132980,", "citeRegEx": "Srivastava and Salakhutdinov. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In ICML\u201908", "author": ["Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient"], "venue": "pages 1064\u20131071. ACM,", "citeRegEx": "Tieleman. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals", "author": ["Gyanendra K Verma", "Uma Shanker Tiwary"], "venue": "NeuroImage, 102:162\u2013172,", "citeRegEx": "Verma and Tiwary. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on human emotion recognition approaches", "author": ["C Vinola", "K Vimaladevi"], "venue": "databases and applications. ELCVIA: electronic letters on computer vision and image analysis, pages 24\u201344", "citeRegEx": "Vinola and Vimaladevi. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting emotions in social media: A constrained optimization approach", "author": ["Yichen Wang", "Aditya Pal"], "venue": "IJCAI\u201915, pages 996\u20131002. AAAI Press,", "citeRegEx": "Wang and Pal. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In IJCAI\u201915", "author": ["Yang Yang", "Han-Jia Ye", "De-Chuan Zhan", "Yuan Jiang. Auxiliary information regularized machine for multiple modality feature learning"], "venue": "pages 1033\u20131039. AAAI Press,", "citeRegEx": "Yang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-modality tracker aggregation: from generative to discriminative", "author": ["Xiaoqin Zhang", "Wei Li", "Mingyu Fan", "Di Wang", "Xiuzi Ye"], "venue": "IJCAI\u201915, pages 1937\u20131943. AAAI Press,", "citeRegEx": "Zhang et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE Transactions on Autonomous Mental Development", "author": ["Wei-Long Zheng", "Bao-Liang Lu. Investigating critical frequency bands", "channels for eeg-based emotion recognition with deep neural networks"], "venue": "7(3):162\u2013175,", "citeRegEx": "Zheng and Lu. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "learned an expanded opinion (positive, neutral and negative) lexicon from emoticon annotated tweets [Bravo-Marquez et al., 2015].", "startOffset": 100, "endOffset": 128}, {"referenceID": 17, "context": "Wang and Pal used constraint optimization framework to discover user\u2019 emotions from social media content [Wang and Pal, 2015].", "startOffset": 105, "endOffset": 125}, {"referenceID": 7, "context": "used fractal dimension based algorithm to recognize and visualize emotions in real time [Liu et al., 2010].", "startOffset": 88, "endOffset": 106}, {"referenceID": 9, "context": "employed discrete wavelet transform to extract frequency features from EEG signals and two classifiers are used to classify the features [Murugappan et al., 2010].", "startOffset": 137, "endOffset": 162}, {"referenceID": 2, "context": "found that differential entropy features are more suited for emotion recognition tasks [Duan et al., 2013].", "startOffset": 87, "endOffset": 106}, {"referenceID": 20, "context": "\u2217 Corresponding author deep neural network to classify EEG signals and examined critical bands and channels of EEG for emotion recognition [Zheng and Lu, 2015].", "startOffset": 139, "endOffset": 159}, {"referenceID": 0, "context": "Bradley and Lang recorded eye movement signals to study the relationship between memory, emotion and pupil diameters [Bradley and Lang, 2015].", "startOffset": 117, "endOffset": 141}, {"referenceID": 18, "context": "proposed an auxiliary information regularized machine which treats different modalities with different strategies [Yang et al., 2015].", "startOffset": 114, "endOffset": 133}, {"referenceID": 19, "context": "proposed a multimodal ranking aggregation framework for fusion of multiple visual tracking algorithms [Zhang et al., 2015].", "startOffset": 102, "endOffset": 122}, {"referenceID": 11, "context": "In [Ngiam et al., 2011], the authors built a single modal deep autoencoder and a bimodal deep autoencoder to generate shared representations of images and audios.", "startOffset": 3, "endOffset": 23}, {"referenceID": 11, "context": "Srivastava and Salakhutdinov extended the methods developed by [Ngiam et al., 2011] to bimodal deep Boltzmann machines to handle multimodal deep learning problems [Srivastava and Salakhutdinov, 2014].", "startOffset": 63, "endOffset": 83}, {"referenceID": 13, "context": ", 2011] to bimodal deep Boltzmann machines to handle multimodal deep learning problems [Srivastava and Salakhutdinov, 2014].", "startOffset": 87, "endOffset": 123}, {"referenceID": 15, "context": "As for multimodal emotion recognition, Verma and Tiwary carried out emotion classification experiments with EEG singals and peripheral physiological signals [Verma and Tiwary, 2014].", "startOffset": 157, "endOffset": 181}, {"referenceID": 8, "context": "used two different fusion strategies for combining EEG and eye movement data: feature level fusion and decision level fusion [Lu et al., 2015].", "startOffset": 125, "endOffset": 142}, {"referenceID": 16, "context": "Vinola and Vimaladevi gave a detailed survey on human emotion recognition and listed many other multimodal datasets and methods [Vinola and Vimaladevi, 2015].", "startOffset": 128, "endOffset": 157}, {"referenceID": 4, "context": "In this paper, we use Contrastive Divergence (CD) algorithm [Hinton, 2002] or Persistent CD algorithm [Tieleman, 2008] to train a RBM.", "startOffset": 60, "endOffset": 74}, {"referenceID": 14, "context": "In this paper, we use Contrastive Divergence (CD) algorithm [Hinton, 2002] or Persistent CD algorithm [Tieleman, 2008] to train a RBM.", "startOffset": 102, "endOffset": 118}, {"referenceID": 20, "context": "The SEED dataset was first introduced in [Zheng and Lu, 2015].", "startOffset": 41, "endOffset": 61}, {"referenceID": 8, "context": "In order to compare our proposed method with [Lu et al., 2015], we use the same data", "startOffset": 45, "endOffset": 62}, {"referenceID": 8, "context": "as in [Lu et al., 2015], that is, 27 data files from 9 subjects.", "startOffset": 6, "endOffset": 23}, {"referenceID": 12, "context": "In order to compare the performance of our proposed method with previous results in [Rozgic et al., 2013] and [Li et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 6, "context": ", 2013] and [Li et al., 2015], we did not take familiarity into consideration.", "startOffset": 12, "endOffset": 29}, {"referenceID": 8, "context": "As for eye movement data, we used the same features as in [Lu et al., 2015], which were listed in Table 1.", "startOffset": 58, "endOffset": 75}, {"referenceID": 3, "context": "Inspired by [Guo and Guo, 2005], we performed experiments on the following three kinds of emotion recognition tasks to examine the efficiency of our proposed method.", "startOffset": 12, "endOffset": 31}, {"referenceID": 8, "context": "We also compared our results with [Lu et al., 2015].", "startOffset": 34, "endOffset": 51}, {"referenceID": 8, "context": "The middle bars show the results in [Lu et al., 2015], and the right two bars are the results of our DAE model.", "startOffset": 36, "endOffset": 53}, {"referenceID": 8, "context": "In [Lu et al., 2015], the best result achieved when only EEG signal used was 78.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "The fourth Fuzzy bar denotes the best result in [Lu et al., 2015].", "startOffset": 48, "endOffset": 65}, {"referenceID": 8, "context": "11% (in Table 4) in comparison with the state-of-the-art approach [Lu et al., 2015] (77.", "startOffset": 66, "endOffset": 83}, {"referenceID": 8, "context": "In [Lu et al., 2015], the authors employed fuzzy integral method to fuse different modalities.", "startOffset": 3, "endOffset": 20}, {"referenceID": 8, "context": "Compared with [Lu et al., 2015], the BDAE model enhanced the performance of affective model significantly.", "startOffset": 14, "endOffset": 31}, {"referenceID": 12, "context": "treated the EEG signals as a sequence of overlapping segments and a novel non-parametric nearest neighbor model was employed to extract response-level feature from these segments [Rozgic et al., 2013].", "startOffset": 179, "endOffset": 200}, {"referenceID": 6, "context": "used Deep Belief Network (DBN) to automatically extract high-level features from raw EEG signals [Li et al., 2015].", "startOffset": 97, "endOffset": 114}, {"referenceID": 6, "context": "We compared the BDAE results with results in [Li et al., 2015] and [Rozgic et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 12, "context": ", 2015] and [Rozgic et al., 2013].", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "Valence Arousal Dominance liking [Rozgic et al., 2013] 76.", "startOffset": 33, "endOffset": 54}, {"referenceID": 6, "context": "3 [Li et al., 2015] 58.", "startOffset": 2, "endOffset": 19}, {"referenceID": 8, "context": "51% in [Lu et al., 2015]).", "startOffset": 7, "endOffset": 24}], "year": 2016, "abstractText": "To enhance the performance of affective models and reduce the cost of acquiring physiological signals for real-world applications, we adopt multimodal deep learning approach to construct affective models from multiple physiological signals. For unimodal enhancement task, we indicate that the best recognition accuracy of 82.11% on SEED dataset is achieved with shared representations generated by Deep AutoEncoder (DAE) model. For multimodal facilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE) achieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets, respectively, which are much superior to the state-of-theart approaches. For cross-modal learning task, our experimental results demonstrate that the mean accuracy of 66.34% is achieved on SEED dataset through shared representations generated by EEGbased DAE as training samples and shared representations generated by eye-based DAE as testing sample, and vice versa.", "creator": "LaTeX with hyperref package"}}}