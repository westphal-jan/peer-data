{"id": "1602.08715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2016", "title": "Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus", "abstract": "we accordingly propose a method for efficiently finding all parallel passages appearing in a coherent large corpus, measuring even if the passages are not quite identical due to rephrasing and orthographic variation. the key ideas are the representation of each word in the corpus marked by its two most infrequent letters, finding matched unique pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. once using this method, over 4600 parallel pairs of passages were identified in the babylonian talmud, a hebrew - central aramaic corpus of over 1. 8 million words, totaling in intervals just over 30 seconds. empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods.", "histories": [["v1", "Sun, 28 Feb 2016 13:43:33 GMT  (131kb)", "http://arxiv.org/abs/1602.08715v1", "Submission to the Journal of Data Mining and Digital Humanities (Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages)"]], "COMMENTS": "Submission to the Journal of Data Mining and Digital Humanities (Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["avi shmidman", "moshe koppel", "ely porat"], "accepted": false, "id": "1602.08715"}, "pdf": {"name": "1602.08715.pdf", "metadata": {"source": "CRF", "title": "Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus", "authors": ["Avi Shmidman", "Moshe Koppel", "Ely Porat"], "emails": [], "sections": [{"heading": null, "text": "1 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\n1 The standard edition of the Talmud is the Vilna edition, printed by the Romm Publishing House towards the end of the 19th century. All of the experiments and citations in this paper are based upon this standard edition, available in digital format at http://www.sefaria.org/. Many variant texts of the Talmud exist in manuscripts and earlier printings, and it would certainly be worthwhile to rerun the experiments detailed herein upon those variant texts as well.\n2 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\nIt should be emphasized that very few of the parallel passages within the Talmud are identical letter-for-letter parallels. First of all, any given word may be replaced by a synonymous word or phrase, and many of the passages contain interpolations of additional material. Additionally, there is much variation in the use of grammatical particles and prepositions. Finally, on the level of the individual word, orthographic variations abound. Classical Jewish texts are not at all consistent with regard to the use of matres lectionis; for example, any given instance of the vowel \u201co\u201d may or may not be represented by the letter vuv (\u05d5), and any given instance of the vowel \u201ci\u201d may or may not be represented by the letter yod (\u05d9). Additional spelling variations include alternations between final mem (\u05dd) and final nun (\u05df), alternations between the letter aleph (\u05d0) and the letter yod (\u05d9), and many others. Thus, even if an entire passage were to recur word for word, a standard search algorithm would generally still miss it, due to the alterations in orthography. As a typical example, consider the following a passage that occurs in Tractate Shabbat and then again in Tractate Hagigah:\nWithin this 28/29 word parallel, we can point to five different discrepancies:\n On the first line, the Shabbat passage starts simply \u05e8\u05de\u05d0\u05d5, while the Hagigah passage adds the interrogative particle \u05d4, arriving at the form \u05e8\u05de\u05d0\u05d4\u05d5.\n On the second line, the Hagigah passage ends with the preposition \u05d3\u05e2, while the Shabbat passages substitutes \u05dc\u05d9\u05d1\u05e9\u05d1 \u05d0\u05dc\u05d0 instead.\n On the third line, we find a synonym alternation: the Shabbat passage writes \u05d9\u05e9\u05e0\u05d0, while the Hagigah passage writes \u05d9\u05dc\u05e2\u05d1, although the effective meaning is identical.\n On the fourth line, while quoting a verse from Jeremiah 5:1 the Shabbat passage writes out the word \u05dd\u05d9\u05dc\u05e9\u05d5\u05e8\u05d9 with its full plene spelling, while the Hagigah passage adopts the defective spelling found in Scripture.\n On the sixth line, the Hagigah passage quotes the Jeremiah verse verbatim, while the Shabbat passage omits the redundant words \u05e9\u05d9 \u05dd\u05d0 from the verse.\nA traditional approach to the problem of finding approximate matches of this nature is the use of an edit-distance calculation, such as the Levenshtein distance. The edit distance between two strings is a measure of the number of transformations necessary to mutate the first string into the second. If the edit-distance is beneath a pre-defined threshold, the pair of strings can be considered a match. However, in the situation we are dealing with here \u2013 in which we want to compare all passages against all other passages \u2013 the cost of the calculation of edit-distance is prohibitive. As noted, the Babylonian Talmud contains approximately 1.8 million words. An all-against-all comparison of 20-word pairs within this corpus would require approximately 1.6 trillion calculations. According to our measurements on an Intel Core i7 processor running at 3.47 gigahertz, the computation of the Levenshtein distance for two 20-\n3 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\nword passages requires an average of 380 microseconds. Thus, the computation of all 1.6 trillion pairs would require nearly 20 years.2 In this study, we present an efficient and scalable method of identifying approximate matches within Hebrew and Aramaic texts. Our method allows for variations between the passages both on the word level (orthographic differences), as well as on the phrase level (interchanged words, interpolations). Our method computes in linear time, returning all of the parallel passages throughout a corpus of arbitrary size."}, {"heading": "I Description of the Algorithm", "text": ""}, {"heading": "1.1 Per-word Preprocessing", "text": "As noted above, matching words in parallel Hebrew segments often differ in their orthography. In order to allow our algorithm to focus on the core of the words, while eliding the unstable parts of the words, we perform the following preprocessing procedure:\n We calculate the frequency of the Hebrew letters throughout the input corpus.  For every word, we identify the two most infrequent letters, and we represent the word\nvia those two letters (maintaining the order in which the two letters appear within the word).\nNaturally, the letters representing prefixes and matres lectionis are among the most common the language; thus, by eliminating all but the two most infrequent letters, we effectively eliminate most prefixes and matres lectionis. As an example, here is the frequency chart for Hebrew letters in the Babylonian Talmud, in order of frequency (final letters are counted together with their corresponding regular forms):\nConsider the example of \u05d0\u05de\u05dc\u05d9\u05d3 and \u05d0\u05de\u05dc\u05d3, orthographic variants of the same word. In this case, the \u05d9 ,\u05d0, and \u05de letters are eliminated, leaving only \u05dc\u05d3. The two words are thus represented by 2 To be sure, there are methods of speeding up the Levenshtein calculation. For an optimization using dynamic programming and parallel processing, see [Klein et al, 2014]. Another possible optimization would be to use a bag-of-letters measure as a pre-filter, automatically rejecting all pairs that exceed a certain threshold of discrepancy when comparing their bag-of-letter counts.\n4 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\nthe same two letters, in the same order, allowing the algorithm to easily identify the match. The same occurs with almost all such variant pairs. Occasionally, however, the letter distribution is such that words will not be equated as expected. For instance, in the case of \u05d0\u05d9\u05e0\u05ea\u05d3 and \u05d0\u05d9\u05e0\u05ea\u05d3\u05db\u05dc, the first is reduced to \u05ea\u05d3, and the second to \u05d3\u05db. Nevertheless, these cases are a minority, and they will be handled by the next step, which allows for word alternations and interpolations. Of course, many different Talmudic words will be reduced to the same two letters, and the reader may well wonder whether this might lead to too many false positives. Nevertheless, because our goal is to find sequences of matching two-letter codes, rather than just individual words, the false positives will naturally fall by the wayside. This is demonstrated in the following chart, which details the results of examining the entire text of the Babylonian Talmud against itself based upon two-letter reductions. Every match was run against a Levenshtein filter to evaluate its validity; if the Levenshtein distance totaled greater than 20 percent of the length of the first string, then the match was considered a false positive.\nAs demonstrated here, once the passage length is sufficiently high, the two-letter-reduction method provides a very high degree of accuracy."}, {"heading": "1.2 N-grams and Skip-grams", "text": "The basic unit of comparison within our algorithm is an n-gram of length 4; that is, sequences of four words. Indeed, many previous studies utilize n-grams for the identification of repeated text. However, in addition to allowing for orthographic variations within word sequences, we also need to allow for word alternations and interpolations, in which the parallel word sequences contain a word in the middle which is completely different, or which exists in one sequence but not in the other. Therefore, instead of limiting our algorithm to traditional ngrams, we use a set of noncontiguous n-grams, each of which omits one word from the text. These non-contiguous n-grams are termed \u201cskip-grams\u201d. The effectiveness of skip-grams in identifying parallel textual segments has been demonstrated in recent studies; see for instance [Guthrie et al, 2006]. Identifying skip-grams in a text is special case of the k-mismatch problem in approximate pattern matching for which surprisingly efficient algorithms have been found [Porat and Porat, 2009]. For every 5-word sequence in the text, we extract all combinations of length 4, allowing for any of the last four words to be omitted (we don\u2019t need to include the case in which the first word is omitted, because that case is covered within the skip-gram set of the subsequent\n5 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\nstarting position). Thus, for every starting position \u2018x\u2019 within the corpus, we extract 4 skipgrams:\n x, x+2, x+3, x+4 [Skipping x+1]  x, x+1, x+3, x+4 [Skipping x+2]  x, x+1, x+2, x+4 [Skipping x+3]  x, x+1, x+2, x+3 [Skipping x+4; effectively a contiguous 4-gram]\nTo take a real example, in Tractate Avodah Zarah we find the following sequence: \u05ea\u05e2\u05d3 \u05e2\u05d3\u05d9\u05d5 \u05d5\u05ea\u05de\u05d4\u05d1 \u05ea\u05e2\u05d3 \u05e8\u05e9\u05e4\u05d0 \u05df\u05d5\u05d9\u05dc\u05e2. A parallel passage in Tractate Berakhot states: \u05d0\u05ea\u05e9\u05d4 \u05df\u05d5\u05d9\u05dc\u05e2 \u05ea\u05e2\u05d3 \u05e2\u05d3\u05d5\u05d9\u05d5 \u05d5\u05ea\u05de\u05d4\u05d1 \u05ea\u05e2\u05d3. The respective first words of the passages \u2013 \u05e2\u05d3\u05d5\u05d9\u05d5 and \u05e2\u05d3\u05d9\u05d5 \u2013 are orthographic variants equalized by the two-letter reduction. However, the fourth word of the passage is completely different (even though the import of the two words is effectively the same): \u05d0\u05ea\u05e9\u05d4 instead of \u05e8\u05e9\u05e4\u05d0. Nevertheless, the two passages will match up due to the third skip-gram, which elides the fourth word:\nThe differing words can also be in disparate places within the two passages. Similarly, this method will successfully match any pair of segments, one of length 4 and one of length 5, in which an extra word appears in the latter but not in the former."}, {"heading": "1.3 Indexing the Skip-grams", "text": "As a general rule, n-gram-based algorithms would, at this point, proceed to compute a RabinKarp numerical hash of each n-gram [Karp and Rabin, 1987], in order to index them for quick retrieval. The use of such hashes can, however, slow down performance due to potential collisions. Fortunately, we can leverage our two-letter reduction method (detailed above, section 1.1) in order to efficiently index our skip-grams without any hash collisions. As noted in the previous section, every skip-gram consists of four words, each represented by two letters. There are a total of 22 letters in the Hebrew and Aramaic languages (we count final letters as regular forms), and, since each skip-gram is effectively a permutation of 8 such letters, there are a total of 228 possibilities \u2013 just under 55 billion. We can thus uniquely represent each skip-gram with a single 64-bit integer. For efficiency, however, we maintain a set of 484 skip-gram indexes \u2013 corresponding with the 484 (=222) possibilities for the first word of the skip-gram; the 226 possibilities for the remaining six letters can be uniquely represented via 32-bit integers. The limited number of possibilities for each part of the skip-grams also allows us to utilize precomputed tables in calculating their numeric representations. Thus, the entire set of skip-grams for our corpus \u2013 over 7 million in all \u2013 can be converted quickly and efficiently into an indexed numeric structure, allowing immediate access to all matches for any given skip-gram, without any need to check for hash collisions.\nOnce the indexes have been prepared, we can identify all matching skip-grams for any given position in the text via the following steps:\n6 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\n1. Retrieve the numeric representation of the two-letter code for the word at this position (a number between 0 and 483). This number serves as an offset into our set of indexes, delineating the index that we use in step 3 below.\n2. For each of the four skip-grams at this position, retrieve the numerical representation of the remaining 6 letters.\n3. Using the index identified in the first step, perform a lookup of the numerical representations of each of the four 6-letter patterns. Retrieve all of the skip-grams listed for each of the patterns (ignoring any that are in the immediate vicinity of the current word position).\nThis procedure provides the set of all relevant matching skip-grams."}, {"heading": "1.4 Identifying clusters of skip-grams", "text": "Of course, a skip-gram match alone does not necessarily indicate a parallel passage. As we saw above, our skip-grams are only 4 words long, and, furthermore, they are based upon twoletter reductions of the words. What we really need to find, in order to validate a given match, is a cluster of matching skip-grams [Grozea et al, 2009]. For our purposes, we define a cluster as follows: a set of i or more matching skip-grams with gaps of no more than j words in between the skip-grams, stretching across a total of at least k words from the start of the first skip-gram to the end of the last one. For this paper, we use the values i=3, j=8, k=20.\nOne important benefit of this method is that we will succeed in matching passages in which phrases up to 8 words in length are interpolated in one or the others of the passages, or in which variant strings of up to 8 words appear in both of the passages.\nOur algorithm searches for these clusters using a nested loop, examining each matching skipgram as follows:\n Let pos1 be the end position of the first occurrence of the skip-gram, and let pos2 be the end position of the other occurrence of the skip-gram.\n Move pos1 forward one word at a time. If pos1 is more than 8 places beyond the most\nrecent skip-gram, then terminate.  For each new position of pos1, look up each of its skip-grams to see if there any\nmatches within 8 words of pos2. If there is one, add that skip-gram to the cluster, and start again with the first step. If not, resume with the previous step.\nAt the end of this step, we will have a collection of valid parallel passages within the input corpus."}, {"heading": "II Results", "text": "Our algorithm, running in a single thread, takes just over 30 seconds to complete its analysis of the entire Babylonian Talmud, including the preprocessing, the indexing and skip-gram matching, and the identification of clusters. This is a drastic improvement over the many thousands of hours that would have been required to arrive at the same results via editdistance calculations.\n7 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\nOverall, our algorithm identified a total of 4602 pairs of parallel passages in the Talmud of length 20 words or more, containing a total of 130,242 words (counting each pair of parallel passages a single time). This works out to an average of 3.3 parallel passages for each folio page of the Talmud.\nIn order to verify the accuracy of our results, we compared the parallel passages identified by our algorithm with those found by a standard edit-distance algorithm which computes the Levenshtein distance for every pair of passages. To be sure, as noted above, it would be impractical to run an edit-distance algorithm across the entire Babylonian Talmud. Instead, we ran the edit-distance algorithm on a subset of the corpus, aiming to match passages in Tractate Bava Batra (89,000+ words) with passages in Tractate Gittin (61,000+ words), where the passages are both at least 20 words in length. Altogether, the Levenshtein-based algorithm identified 41 parallel passages between these two tractates (where passages were considered a match if their Levensthein distance was within 30% of the length of the first passage).\nAll 41 of these passages were correctly identified by our algorithm. Moreover, our algorithm identified an addition 5 valid parallels that were missed by the Levenshtein-based algorithm. These extra passages were cases in which interpolations of 4 or 5 words within one or the other of the passages caused the Levenshtein distance to rise above the threshold. In contrast, our algorithm is able to handle multi-word interpolations while still matching the skip-grams on either side."}, {"heading": "III Auto-generating a Thesaurus", "text": "As we have seen, our algorithm allows for word alternations and interpolations within the parallel passages. In such cases, we not only gain knowledge of the words that are similar to one another; rather, we also gain useful information with regard to the words that do not match each other. Specifically, in these cases, our algorithm serves to isolate pairs of disparate words that tend to be used in parallel with one another.\nFor instance, in a case where two parallel passages use two completely different words at one given position, our skip-grams will skip over the disparate words, matching the words beforehand and afterward. The discrepancy that remains \u2013 the length of a single word \u2013 contains a pair of words that potentially correlate with one another. A case in point would be the passage cited in the introduction to this paper, in which we found that the phrase \u05d5\u05e7\u05e1\u05e4\u05e9 \u05e8\u05de\u05d0\u05e0\u05e9 \u05d4\u05e0\u05de\u05d0 \u05d9\u05dc\u05e2\u05d1 \u05d4\u05e0\u05de\u05de in Tractate Hagigah is parallel to the phrase \u05d4\u05e0\u05de\u05d0 \u05d9\u05e9\u05e0\u05d0 \u05d4\u05e0\u05de\u05de \u05d5\u05e7\u05e1\u05e4\u05e9 \u05e8\u05de\u05d0\u05e0\u05e9 in Tractate Shabbat. These two phrases will line up via two skip-grams that each skip the third word. The match of these two skip-grams isolates the two omitted words, \u05d9\u05e9\u05e0\u05d0 and \u05d9\u05dc\u05e2\u05d1, as a potential pair.\nIn such cases, there is a reasonable chance that the pair of words that comprise the discrepancy are words that are semantically similar, which should be treated as identical when searching for parallel passages. Although not all cases will qualify as such, it stands to reason that the recurrence of a given discrepancy in multiple passages within the corpus reflects the existence of an inherent connection between the two words. Thus, our algorithm tallies up a count for all one-word discrepancies, and then generates a thesaurus containing all of the word pairs whose frequency is above a predetermined threshold.\nIt is reasonable to assume that if these word equivalencies had been known to us during the initial stages of our analysis, we would have been able to widen our identification of parallel\n8 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\npassages within the corpus. Furthermore, upon that basis, we would presumably be able to identify additional word equivalencies.\nTherefore, upon generating the thesaurus of word equivalencies, our algorithm reruns the entire procedure starting from step 1 above. This time, during step 1, while calculating the two-letter reductions for each word in the input corpus, the algorithm checks whether the word in question appears in the thesaurus, and, if so, the algorithm stores a second alternate two-letter reduction for the given word, based upon the equivalent word in the thesaurus. Subsequently, when collecting the skip-grams for the corpus, the algorithm examines each group of 5 words to determine whether one or more of the words bears an alternate two-letter reduction (as per the thesaurus). If it does, the algorithm generates two separate sets of skipgrams for that group of words: one as usual, and one based upon the alternate two-letter reductions. Upon completing the full procedure, the resulting set of parallel passages is examined once again for one-word discrepancies. The thesaurus is updated accordingly, and the algorithm repeats again from step 1. This loop continues until we no longer see significant gains from one loop to the next.\nBelow are the results of running our algorithm on the Babylonian Talmud, after implementing the iterative thesaurus addition:\nAs expected, the most significant gain was accrued during the second iteration, in which the total number of matched words rose over ten percent, and the total number of parallel passages rose nearly fifteen percent. The third iteration produced an additional minor gain of 43 words. The slight reduction in total passage count indicates that the extra words served to join parallel passages that would have otherwise been considered separate, effectively lowering the passage count while raising the total length. A fourth iteration provided no further gain."}, {"heading": "IV Conclusions", "text": "In this paper we presented an effective method for identifying text reuse across large corpora of Hebrew and Aramaic texts \u2013 a task that was previously considered to be a formidable challenge fraught with difficulty.\nBecause our algorithm builds its list of potential matches from an index created via a singlepass preprocessing step, it can process texts of any size in O(N) time, allowing it to perform well on text with many millions of words. This provides a significant performance increase over methods that rely on edit-distance calculation, in which the edit-distance needs to be computed separately for every pair of passages, resulting in O(N2) time. To be sure, the identification of clusters step in our algorithm is not linear, but since it is performed only on the set of potential matches, its impact upon the overall run time is relatively limited. Nevertheless, one can imagine bizarre cases in which this step could slow down the algorithm\n9 Journal of Data Mining and Digital Humanities http://jdmdh.episciences.org ISSN 2416-5999, an open-access journal\nsignificantly. In future versions of the algorithm, we plan to improve the efficiency of this step, using heuristics to eliminate the need to test cases in which the probability of clustering is very low.\nThe use of n-gram matching followed by the identification of clusters of such matches is well attested in the literature [Grozea et al 2009]. The novelty here is in casting a wide net initially (matching only the two rarest letters and using skip-grams) and then tightening the requirements by seeking clusters of such matches. The use of 8-letter skip-grams simultaneously increases recall and allows efficient indexing.\nThe iterative generation of a thesaurus is of interest in its own right and improves results considerably. Of course, the identification of synonyms based on similarity of usage in corpora is a venerable idea [Harris, 1954]; for our purposes, we propose using substitutability in parallel texts rather than more general distributional similarity, as has been typical in previous work [Lee, 1999; Lowe, 2001].\nThe efficiency of the algorithm presented here means that when preparing Hebrew and Aramaic databases for use, queries regarding parallel passages do not have to be performed offline and stored for later use; rather, it is now practical to generate results dynamically on demand.\nAlthough the examples within this paper all focus upon the corpus of the Babylonian Talmud, the same method can be applied to other corpora of Hebrew and Aramaic texts as well, whether in an attempt to find all parallel passages throughout a given corpus, or in an attempt to find all parallel passages between two given corpora. Furthermore, given appropriate adjustments to the length of the skip-grams and the parameters of the clustering step, the algorithm should be applicable to other languages as well."}], "references": [{"title": "A Closer Look at Skip-gram Modelling", "author": ["D. Guthrie", "B. Allison", "W. Liu", "L. Guthrie", "Y. Wilks"], "venue": "Proceedings of the 5th International Conference on Language Resources and Evaluation", "citeRegEx": "Guthrie et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Guthrie et al\\.", "year": 2006}, {"title": "ENCOPLOT: Pairwise Sequence Matching in Linear Time Applied to Plagiarism Detection. 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse", "author": ["C. Grozea", "C. Gehl", "M. Popescu"], "venue": "Harris Z., Distributional structure", "citeRegEx": "Grozea et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grozea et al\\.", "year": 2009}, {"title": "Efficient randomized pattern-matching algorithms", "author": ["R. Karp", "M.O. Rabin"], "venue": "IBM Journal of Research and Development", "citeRegEx": "Karp and Rabin,? \\Q1987\\E", "shortCiteRegEx": "Karp and Rabin", "year": 1987}, {"title": "Finding Inexact Quotations Within a Tibetan Buddhist Corpus", "author": ["E. Klein", "N. Dershowitz", "L. Wolf", "O. Almogi", "D. Wangchuk"], "venue": "Digital Humanities", "citeRegEx": "Klein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2014}, {"title": "Measures of distributional similarity. 37th Annual Meeting of the Association for Computational Linguistics", "author": ["L. Lee"], "venue": null, "citeRegEx": "Lee,? \\Q1999\\E", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "Towards a theory of semantic space", "author": ["W. Lowe"], "venue": "Proceedings of the 23rd Annual Meeting of the Cognitive Science Society", "citeRegEx": "Lowe,? \\Q2001\\E", "shortCiteRegEx": "Lowe", "year": 2001}, {"title": "Exact and Approximate Pattern Matching in the Streaming Model", "author": ["B Porat", "E. Porat"], "venue": null, "citeRegEx": "Porat and Porat,? \\Q2009\\E", "shortCiteRegEx": "Porat and Porat", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Identifying skip-grams in a text is special case of the k-mismatch problem in approximate pattern matching for which surprisingly efficient algorithms have been found [Porat and Porat, 2009].", "startOffset": 167, "endOffset": 190}, {"referenceID": 2, "context": "As a general rule, n-gram-based algorithms would, at this point, proceed to compute a RabinKarp numerical hash of each n-gram [Karp and Rabin, 1987], in order to index them for quick retrieval.", "startOffset": 126, "endOffset": 148}, {"referenceID": 4, "context": "Of course, the identification of synonyms based on similarity of usage in corpora is a venerable idea [Harris, 1954]; for our purposes, we propose using substitutability in parallel texts rather than more general distributional similarity, as has been typical in previous work [Lee, 1999; Lowe, 2001].", "startOffset": 277, "endOffset": 300}, {"referenceID": 5, "context": "Of course, the identification of synonyms based on similarity of usage in corpora is a venerable idea [Harris, 1954]; for our purposes, we propose using substitutability in parallel texts rather than more general distributional similarity, as has been typical in previous work [Lee, 1999; Lowe, 2001].", "startOffset": 277, "endOffset": 300}], "year": 2016, "abstractText": "We propose a method for efficiently finding all parallel passages in a large corpus, even if the passages are not quite identical due to rephrasing and orthographic variation. The key ideas are the representation of each word in the corpus by its two most infrequent letters, finding matched pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. Using this method, over 4600 parallel pairs of passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of over 1.8 million words, in just over 30 seconds. Empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods.", "creator": "PScript5.dll Version 5.2.2"}}}