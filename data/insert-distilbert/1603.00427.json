{"id": "1603.00427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2016", "title": "A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear Functionals", "abstract": "nonlinear adaptive filtering allows for modeling of some additional aspects of a general system method and usually always relies basically on highly complex algorithms, such as those based on only the volterra series. through the use of the kronecker product and some basic facts models of tensor algebra, we propose a simple model of residual nonlinearity, one that can be interpreted as a product of the outputs of k fir linear filters, and compute its cost function together with its posterior gradient, which allows for some analysis of the optimization problem. we use these results it in a stochastic gradient framework, from which we effectively derive an elementary lms - like algorithm and investigate the problems arises of multi - modality in the mean - square error surface and the choice of three adequate initial conditions. its computational complexity is calculated. the new algorithm property is tested in a system identification compiler setup and is often compared nicely with other polynomial algorithms from the literature, presenting favorable convergence outcomes and / or supporting computational complexity.", "histories": [["v1", "Tue, 1 Mar 2016 19:53:02 GMT  (214kb,D)", "http://arxiv.org/abs/1603.00427v1", "5 pages, one of references, plus extra page attached"]], "COMMENTS": "5 pages, one of references, plus extra page attached", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["felipe c pinheiro", "c\\'assio g lopes"], "accepted": false, "id": "1603.00427"}, "pdf": {"name": "1603.00427.pdf", "metadata": {"source": "CRF", "title": "A Nonlinear Adaptive Filter Based on the Model of Simple Multilinear Functionals", "authors": ["Felipe C. Pinheiro", "C\u00e1ssio G. Lopes"], "emails": ["felipe.chaud.pinheiro@usp.br;", "cassio@lps.usp.br)."], "sections": [{"heading": null, "text": "Index Terms\u2014Adaptive filters, nonlinear filters, least mean squares methods, multilinear algebra, cost function.\nI. INTRODUCTION\nTHE collective of nonlinear signal processing techniquescome into play whenever nonlinear effects, such as harmonic distortion, saturation and general polynomial behavior, start to become noticeable enough to degrade the performance of conventional linear techniques. A common problem with nonlinear filters is the computational complexity they demand. Some of the most complex filters are based on the Volterra and Wiener models [1]\u2013[3]. It is common to restrict these models in order to reduce their complexity [4]\u2013[6]. We follow this approach.\nWe start by proposing a polynomial technique that could be thought of as a subclass of the Volterra model. It is based on representing the input regressor as an iterated Kronecker product, an idea that has already been used for the estimation of higher order statistics of the regressor [7]\u2014a related problem\u2014and other Volterra approaches [4]\u2013[6], but we also embed this Kronecker structure within the filter itself. This formalism allows us to promptly derive a cost function in explicit form. We also compute its gradient and use it in the derivation of a nonlinear, low-complexity, LMS-like algorithm with good mean-square performance."}, {"heading": "II. SIMPLE MULTILINEAR MODEL", "text": "The general K-th order homogeneous Volterra kernel is a function h(i1, . . . , iK) used to produce the input/output\nThe authors are with the Department of Electronic Systems Engineering, Escola Polit\u00e9cnica, University of S\u00e3o Paulo, S\u00e3o Paulo, Brazil (e-mails: felipe.chaud.pinheiro@usp.br; cassio@lps.usp.br).\nThe first author was supported by a scholarship from CNPq No. 132625/2015-6\nThe second author was supported by a grant from CNPq No. 311031/2013-7\nrelationship in (1). There, u(i) is the input signal and M is the length of the filter.\ny(i) = \u2211\n0\u2264i1,...,iK<M\nh(i1, . . . , iK)u(i\u2212 i1) \u00b7 \u00b7 \u00b7u(i\u2212 iK) (1)\nEvaluating (1) is a highly complex task. One would take O(MK) operations per iteration. This can be slightly reduced should the symmetry in the kernel be explored, but this approach could at most decrease the number of operations to something proportional to ( M\u2212K+1\nK\n) , which still increases\nrapidly. It is necessary to suppose some extra structure in the kernel itself if one wants a significant reduction in complexity.\nThe structure we propose is kernel separability. Explicitly, we suppose that there exists functions hs(is), 1 \u2264 s \u2264 K, such that\nh(i1, . . . , iK) = h1(i1)h2(i2) \u00b7 \u00b7 \u00b7hK(iK). (2)\nWhen we use (2) in (1), we get y(i) = \u2211\n0\u2264i1,...,iK<M\nh1(i1) \u00b7 \u00b7 \u00b7hK(iK)u(i\u2212 i1) \u00b7 \u00b7 \u00b7u(i\u2212 iK)\n= M\u22121\u2211 i1=0 h1(i1)u(i\u2212 i1) \u00b7 \u00b7 \u00b7 M\u22121\u2211 iK=0 hK(iK)u(i\u2212 iK). (3)\nEach factor yos(i) , \u2211M\nis=0 hs(ij)u(i \u2212 is) could be seen\nas the output of a linear FIR filter. As such, we can represent them in vector form. We collect the input signals in a 1\u00d7M row vector1 ui = [u(i)u(i\u2212 1) \u00b7 \u00b7 \u00b7u(i\u2212M +1)] and a set of K vectors of size M \u00d7 1 represented by {w1, . . . , wK}. We also say that, for each s, we have\nws = [hs(0)hs(1) \u00b7 \u00b7 \u00b7hs(M \u2212 1)]T . (4)\nNow, we have yos(i) = uiws for every s, so that\ny(i) = (uiw1)(uiw2) \u00b7 \u00b7 \u00b7 (uiwK). (5)\nThis can be represented by Fig. 1. Granted, this implies a loss of generality from the original Volterra model. Not every kernel is separable. But this simplification allows us to compute the output of the system in O(KM) operations per iteration\u2014a exponential reduction in complexity. Moreover, this separable kernel allows a convenient formalism for the derivation of the cost function that also gives us algebraic insight on the model.\n1This notation is due to [8]. Scalars are represented as x(i), vectors as xi, matrices and constants as capital letters (either is clear from context) and a boldface font for random quantities.\nar X\niv :1\n60 3.\n00 42\n7v 1\n[ cs\n.S Y\n] 1\nM ar\n2 01\n6\n2 u(i) w2 \u00d7 w1\n... wK\ny(i)\nFig. 1. Block diagram of the Simple Multilinear Model."}, {"heading": "A. Kronecker Representation", "text": "Using the identity (A \u2297 B)(C \u2297 D) = (AC) \u2297 (BD) for the Kronocker product [9], it follows that (5) is equivalent to (6), because the Kronecker product of scalars is their ordinary multiplication.\ny(i) = (uiw1) \u00b7 \u00b7 \u00b7 (uiwK) = (uiw1)\u2297 \u00b7 \u00b7 \u00b7 \u2297 (uiwK) = (ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui)\ufe38 \ufe37\ufe37 \ufe38\nK times\n(w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK) (6)\nThe Kronecker product is analogous to the tensor product [10], therefore we can think of the two factors (w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK) and (ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui) in (6) as tensors. As such, to better manipulate and have access to their elements, one can even index them as\n(ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui)j1,...,jK = u(i\u2212 j1 + 1) \u00b7 \u00b7 \u00b7u(i\u2212 jK + 1) (7)\nand\n(w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK)i1,...,iK = h1(i1) \u00b7 \u00b7 \u00b7hK(iK). (8)\nThis should be so that, if we sum the product of (7) and (8) over the indexes is = js for every s, we get the output of the system: 2\u2211 i1,...,iK (ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui)i1,...,iK (w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK)i1,...,iK = y(i).\n(9) Actually, these objects are a very specific kind of tensor: they have rank one and are also called decomposable or simple. One could think of substituting w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK for a general tensor and produce much more diverse nonlinearities. However, such procedure would lead back to the full Volterra model. This fact sheds algebraical reasoning on what can be represented by the a product of FIR systems.\nAdditionally, one can think of the \u201cw\u201d tensor as a K-linear form [11]\u2014or functional\u2014acting on K copies of the vector ui. In this case, they are rank one multilinear functionals, also called simple multilinear functionals, from which we take the name of our model: the simple multilinear model (SML)."}, {"heading": "III. THE MEAN SQUARE ERROR OVER THE SML", "text": "As it is usual in most adaptive schemes, we pose a problem related to optimization\u2014in particular, the minimization of a certain metric, the most common being the mean-square error.\nGiven a desired random signal d, a random 1\u00d7M vector u and a set of M \u00d7 1 column vectors {w1, . . . , wK}, the output estimation error is defined as e , d \u2212 u\u2297Kw. Under our notation, w , w1\u2297\u00b7 \u00b7 \u00b7\u2297wK (KM\u00d71), and u\u2297K , u\u2297\u00b7 \u00b7 \u00b7\u2297u\n2This fact is used in Appendix A.\n(1 \u00d7KM ), the Kronecker product of u with itself K times. The mean-square error (MSE) is defined as\nMSE , E|e|2 = E [ [d\u2212 u\u2297Kw]\u2217[d\u2212 u\u2297Kw] ] = E|d|2 \u2212 w\u2217E[du\u2297K\u2217]\u2212 E[d\u2217u\u2297K ]w (10) + w\u2217E[u\u2297K\u2217u\u2297K ]w.\nUnder the hypothesis of stationarity on both u and d, we can define the following constants:\nRuK = E[u\u2297K\u2217u\u2297K ], RuKd = E[u\u2297Kd\u2217] = R\u2217duK (11) Rd = E|d|2. (12)\nThen, the mean square error takes the familiar form in (13).\nMSE(w1, . . . , wK) = Rd \u2212 w\u2217R\u2217uKd \u2212RuKdw + w \u2217RuKw. (13)\nAlthough the form is familiar, our notation embeds some aspects of this function. For example, it does not describe a quadratic surface. Instead, it has degree 2K, when we look at the individual vectors w1, . . . , wK .\nNote that (13) could be seen as a function of a vector wC built from the stacking of the individual w1, . . . , wK vectors. We now introduce \u2207ws , the gradient with respect to the coordinates of the vector ws. The complete gradient vector, \u2207MSE, over wC , would be the stacking of these gradients. It is possible to show that (see Appendix A)\n\u2207wsMSE = [\u2212RuKd + w\u2217RuK ](w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK), (14)\nwhere w\u0302s implies that ws has been substituted for the identity matrix IM of order M in the product.\nIn addition, the critical MSE is reached when the gradient is 0. This implies the trivial solution wp = wq = 0, for some p 6= q\u2014which makes w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK = 0 for each s\u2014or the equation RuKwo = RduK . Usually we are not interested in the first, therefore we will focus on the second. This equation is reminiscent of the normal equations in linear estimation. As such, we can reorganize it in the form\nE[u\u2297Ku\u2297K\u2217]wo = E[u\u2297K\u2217d] E[u\u2297K\u2217(d\u2212 u\u2297Kwo)] = 0\nE[u\u2297K\u2217eo] = 0, (15)\nthat is, a form of the orthogonality principle, with the optimal output error eo being orthogonal to u\u2297K . One can interpret this as eo being uncorrelated to all of the \u201cdegree K\u201d products of the input.\nDue to the structure of RuK (repeated rows), it will always be singular for K > 1, but under certain conditions we can guarantee the existence of the non trivial solution. Assume for d the model in (16),\nd = u\u2297Kho + n, (16)\nwhere n is some zero-mean noise uncorrelated with u\u2297K (serving as a model for eo), and ho = ho1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 hoK , for\n3 some set of vectors {ho1, . . . , hoK}. Then, one can verify that ho is a solution to the equation RuKwo = RduK in wo:\nRduK = E[u \u2297K\u2217d] = E[u\u2297K\u2217u\u2297K ]ho + E[u \u2297K\u2217n]\n= RuKho. (17)"}, {"heading": "IV. AN LMS INSPIRED ALGORITHM", "text": "We take the total gradient \u2207MSE over wC , as previously defined. Then, we use it to form the equation of the steepest descent.\nwC [i] = wC [i\u2212 1]\u2212 \u00b5[\u2207MSE]\u2217. (18)\nAs it is usually done for stochastic algorithms, we use the realizations ui and d(i) to estimate (11).\nR\u0303uK = u \u2297K\u2217 i u \u2297K i , R\u0303uKd = u \u2297K i d(i) \u2217 (19)\nand compute the gradient accondingly from (14). We define ys(i) , (uiw1) \u00b7 \u00b7 \u00b7 (\u0302uiws) \u00b7 \u00b7 \u00b7 (uiwK), where (\u0302uiws) denotes the absence of the factor (uiws) in the product. Then\n\u2207\u0303wsMSE = [\u2212d(i)\u2217u\u2297Ki + w \u2217u\u2297K\u2217i u \u2297K i ]\n\u00b7 (w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK) = \u2212[d(i)\u2212 u\u2297Ki w]\n\u2217(ui \u2297 \u00b7 \u00b7 \u00b7 \u2297 ui) \u00b7 (w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 IM \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK)\n= \u2212e(i)\u2217(uiw1) \u00b7 \u00b7 \u00b7 (uiIM ) \u00b7 \u00b7 \u00b7 (uiwK)\n= \u2212(uiw1) \u0302\u00b7 \u00b7 \u00b7 (uiws) \u00b7 \u00b7 \u00b7(uiwK)uie(i)\u2217 = \u2212ys(i)e(i)\u2217ui. (20)\nIt follows, then, by using (20) in (18), the update law in (21), that should be performed for every s, 1 \u2264 s \u2264 K.\nws[i] = ws[i\u2212 1] + \u00b5e(i)ys(i)\u2217u\u2217i (21)\nThe algorithm still needs to initialize its variables\u2014and this is an important point in the implementation. As it can be seen from the MSE surface, w1 = \u00b7 \u00b7 \u00b7 = wK = 0 is a critical point. Correspondingly, if every wj is initialized at 0, (21) shows that they will always be 0. In spite of these problems, we have empirically found one satisfactory initial condition for use in high numeric precision environments. And that is: using w1[0] = [1 0 \u00b7 \u00b7 \u00b7 0 0]T , w2[0] = [2\u22121 0 \u00b7 \u00b7 \u00b7 0 0]T , or generally wj [0] = [21\u2212j 0 \u00b7 \u00b7 \u00b7 0 1]T until j = K \u2212 1 and then wK [0] = 0. This is all summarized in the Algorithm Table 1.\nTo calculate the computational complexity for algorithm (21), focus on the multiplications present in Algorithm Table 1. The total number of them, on a given iteration, are, for real data and K \u2265 2, given by MK+K2\u2212K+2M +2. In other words, this an O(MK +K2) algorithm."}, {"heading": "V. SIMULATIONS", "text": "A set of two simulations, Cases I and II, on the problem of system identification, with K = 2 and M = 10, were run. The signal d(i) was an SML plant plus additive zeromean Gaussian noise with variance \u03c32n (see (16)). The input signal was drawn from a zero-mean Gaussian distribuition with unitary variance, collected in the vector ui with a delay-line structure. In Case I, we chose \u03c32n = 10\n\u22123 and in Case II, \u03c32n = 10 \u22126. In each case, the algorithm was run through\nAlgorithm 1 LMS-like algorithm Initialization for j = 1 to K \u2212 1 do wj [0] = [2\n1\u2212j 0 \u00b7 \u00b7 \u00b7 0 0]T end for wK [0] = [0 0 \u00b7 \u00b7 \u00b7 0 0]T Iteration for i = 1 to END do\nfor s = 1 to K do Compute yos(i) = uiws[i\u2212 1] end for for s = 1 to K do\nCompute ys(i) = yo1(i) \u0302\u00b7 \u00b7 \u00b7 yos(i) \u00b7 \u00b7 \u00b7yoK(i) end for Compute y(i) = yK(i)yoK(i) Compute e(i) = d(i)\u2212 y(i) Compute fi = \u00b5e(i)u\u2217i for s = 1 to K do\nCompute ws[i] = ws[i\u2212 1] + fiys(i)\u2217 end for\nend for\n1000 2000 3000 4000 5000\n\u221240\n\u221220\n0\nIterations\ndB\nExcess Mean Square Error (estimated)\nVolterra\nSML\nWiener\n(a) Case I: Algorithms identifying an order 2 SML plant.\n0 2000 4000 6000 8000 \u221280\n\u221260\n\u221240\n\u221220\n0\n20\nIterations\ndB\nExcess Mean Square Error (estimated)\nVolterra\nWiener\nSML\n(b) Case II: Same as Case I; higher SNR.\n0 2000 4000 6000 8000 \u221280\n\u221260\n\u221240\n\u221220\n0\n20\nIterations\ndB\nExcess Mean Square Error (estimated)\nSML\nVolterra\nWiener\n(c) Case III: Algorithms identifying a different order 2 SML plant.\n0 0.5 1 1.5 2\nx 10 5\n\u221260\n\u221240\n\u221220\n0\n20\nIterations dB\nExcess Mean Square Error (estimated)\nVolterra\nWiener\nSML\n(d) Case IV: Algorithms identifying an order 3 SML plant.\n0 5000 10000 \u221230\n\u221220\n\u221210\n0\nMean Square Error (estimated)\nIterations\ndB\nSV\nV\nSML IV\nPF\n(e) Case V: Various filters identifying a smooth plant.\n0 5 10\nx 10 4\n\u221240\n\u221220\n0\nExcess Mean Square Error (estimated)\nIterations\ndB\nCF SML\n(f) Case VI: Parallel cascade and SML.\nFig. 2. Curves of the MSE and ESME showing the performance of the SML algorithm against other polynomial algorithms.\n7000 iterations and the curves were averaged through 1000 realizations. The resulting Excess Mean Square Error (EMSE) curves (calculated as E|u\u22972(h1\u2297h2\u2212w[i\u22121])|2) are present in Figs. 2a and 2b. Plotted together on the graph are the curves for the Volterra-LMS and the Wiener-LMS, designed for the Gaussian vector u, as described in [1]. In those algorithms, we chose3 \u00b5 so to get an approximately equal steady-state EMSE.\nFor Case III we have used a different SML plant. We had\n3Stability bounds require a more sophisticated analysis.\n4 \u03c32n = 10 \u22126. \u00b5 was again chosen to equalize the steady-state EMSE. The rest of the parameters remained the same. The algorithm was also made to run with K = 3, which makes Case IV. We had M = 10 and 200,000 iterations, averaged through 10,000 realizations. We compare the results with the Volterra-LMS and the Wiener-LMS with parameters that equalize the steady-state EMSE. The resulting curves are presented in Fig. 2d. The variance of the additive noise was chosen as \u03c32n = 10\n\u22123. Case V compares the SML-LMS with some relevant LMSlike polynomial algorithms from the literature. Fig. 2e shows the Power Filter (PF) [12], Simplified Volterra (SV) [13], [14], Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra filter and the SML. The task was to identify a smooth, nonSML, order 2 Volterra plant with M = 21. This was averaged through 1000 realizations.\nCase VI shows a simulation against another filter, the Parallel Cascade Filter (CF) [15]\u2014single branch and LMS version. This algorithm is similar to the SML for K \u2264 2, so we simulated them in K = 3, where they are clearly dissimilar. We identify the same plant as Case IV, with M = 10, through 1000 realizations. The results are on Fig. 2f.\nFor both Cases V and VI the additive noise variance was 10\u22123. The parameters were adjusted to provide approximately the same convergence rate.\nA description of all the plants is available with the on-line materials in the file plants.pdf."}, {"heading": "VI. DISCUSSION", "text": "Cases I and II clearly show the system converging to an EMSE of approximately \u221240 dB and \u221270 dB, respectively. On both Figs. 2a and 2b, we can see the SML-LMS running slower through the first 500 iterations, then it speeds up and catches up with\u2014and eventually overtake\u2014the others. Both the Volterra and Wiener models assume O (( M+K\u22121\nK )) operations per iteration, while the SML only O(KM +K2). Although it is natural that the SML algorithm is better at identifying SML plants, it does it faster and with far fewer arithmetic operations.\nNote that the SML algorithm presents richer dynamics compared to the other algorithms\u2014something that will be studied in following publications. Case III, in Fig. 2c, for example, shows that its behavior does not depend only on statistics of the regressor ui\u2014in fact, it is also influenced by the plant to be identified. A source of these effects can be hypothesised as due to the ys(i) factors in (21).\nCase IV, in Fig. 2d, shows that the algorithm also converges for K = 3 and that it has the aforementioned initial irregular convergence rate, here appearing in a more evident way.\nCase V shows that the algorithm is competitive, even in a non-SML plant\u2014as long as this plant is correlated enough. The PF used 21 coefficients, the SV used 66, the IV used 60, Volterra used 231 and SML 42. This is another result that shows the computational simplicity of the SML.\nCase VI shows two similar filters, in the sense that they are formed through the product of others. SML is a product of three linear filters, with a total of 30 coefficients, and CF\nis a product of a linear filter with a second order Volterra filter, with 66 coefficients. In the task of identifying an SML plant, they show similar behavior, with the SML being slightly better\u2014at half the complexity."}, {"heading": "VII. CONCLUSION", "text": "Through the use of a Kronecker representation, we have developed the cost function for the SML model. From this function its gradient was derived motivating a new LMS-like nonlinear adaptive algorithm. It has very low computational complexity\u2014an exponential reduction when compared to te Volterra series\u2014and a competitive mean-square performance in a variety of cases, but also some other unusual behavior.\nFinally, the SML algorithm, just like the Parallel Cascade Filter [15], allows the extension for models of greater rank. This path will be pursued in future publications."}, {"heading": "APPENDIX A PROOF OF THE GRADIENT FORMULA", "text": "Through (7) and (8), one can interpret (14) as a tensor equation, in the sense that we can index the matrices as\n(RuK ) i1,...,iK j1,...,jK = E(u\u2297K\u2217u\u2297K)i1,...,iKj1,...,jK and (RuKd)j1,...,jK = E(u\u2297Kd)j1,...,jK\nso to write RuKdw = \u2211\nj1,...,jK (RuKd)j1,...,jK \u220f ` (w`) j` ,\nwhere (w`)j` is the j`-th coordinate of w`, and w\u2217RuKw = \u2211\ni1,...,iK j1,...,jK\n\u220f p (wp) ip\u2217(RuK ) i1,...,iK j1,...,jK \u220f ` (w`) j` .\nThe other terms from (14) involve only the conjugates of the entries of w, so they became to zero [8].\nThe gradient over the vector ws is given by the derivatives over each of its components:\n\u2202(RuKd)w\n\u2202(ws)jq = \u2211 j1,...,jK (RuKd)j1,...,jK \u220f ` 6=s (w`) j`\u03b4jsjq .\n\u03b4ij is the Kronecker delta and also the representation of the identity matrix. In the above expression, the delta\u2014the identity\u2014takes the place of the factor (ws)js . This observation leads directly to\n\u2207ws(RuKdw) = RuKd(w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK). For w\u2217RuKw, we have, remembering we do not derivate\nthe conjugates [8], \u2202(w\u2217RuKw)\n\u2202(ws)jq = \u2211 i1,...,iK j1,...,jK \u220f p (w\u2217p)ip(RuK ) i1,...,iK j1,...,jK \u220f 6\u0300=s (w`) j`\u03b4jsjq .\nUnder the same argument,\n\u2207ws(wRuKw\u2217) = w\u2217RuK (w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK). Therefore, we combine those two terms to get\n\u2207wsMSE = \u2207ws(Rd \u2212 w\u2217R\u2217uKd \u2212RuKdw + w \u2217RuKw)\n= \u2212\u2207ws(RuKdw) +\u2207ws(w\u2217RuKw) = [\u2212RuKd + w\u2217RuK ](w1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 w\u0302s \u2297 \u00b7 \u00b7 \u00b7 \u2297 wK)."}], "references": [{"title": "Adaptive Nonlinear System Indentification: The Volterra and Wiener Model Approaches", "author": ["T. Ogunfunmi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Adaptive polynomial filters", "author": ["V. Mathews"], "venue": "Signal Processing Magazine, IEEE, vol. 8, no. 3, pp. 10\u201326, July 1991.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Adaptive polynomial filters", "author": ["S. Boyd", "L.O. Chua", "C.A. Desoer"], "venue": "IMA Journal of Mathematical Control and Information, Oxford University Press, vol. 1, no. 3, pp. 243\u2013282, 1984.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1984}, {"title": "Tensor product basis approximations for volterra filters", "author": ["R. Nowak", "B. Van Veen"], "venue": "Signal Processing, IEEE Transactions on, vol. 44, no. 1, pp. 36\u201350, Jan 1996.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "A fully lms adaptive interpolated volterra structure", "author": ["E. Batista", "O.J. Tobias", "R. Seara"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on, March 2008, pp. 3613\u20133616.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "A sparse-interpolated scheme for implementing adaptive volterra filters", "author": ["E. Batista", "O. Tobias", "R. Seara"], "venue": "Signal Processing, IEEE Transactions on, vol. 58, no. 4, pp. 2022\u20132035, April 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2022}, {"title": "Low rank estimation of higher order statistics", "author": ["T. Andre", "R. Nowak", "B. Van Veen"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on, vol. 5, May 1996, pp. 3026\u2013308a vol. 5.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Adaptive Filters", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Kronecker products and matrix calculus in system theory", "author": ["J. Brewer"], "venue": "Circuits and Systems, IEEE Transactions on, vol. 25, no. 9, pp. 772\u2013781, Sep 1978.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1978}, {"title": "Multilinear Algebra, ser. Universitext", "author": ["W. Greub"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Nonlinear acoustic echo cancellation using adaptive orthogonalized power filters", "author": ["F. Kuech", "A. Mitnacht", "W. Kellermann"], "venue": "Acoustics, Speech, and Signal Processing, 2005. Proceedings. (ICASSP \u201905). IEEE International Conference on, vol. 3, March 2005, pp. iii/105\u2013iii/108 Vol. 3.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Low-complexity nonlinear adaptive filters for acoustic echo cancellation in gsm handset receivers", "author": ["A. Fermo", "A. Carini", "G.L. Sicuranza"], "venue": "European Transactions on Telecommunications, vol. 14, no. 2, pp. 161\u2013169, 2003. [Online]. Available: http://dx.doi.org/10.1002/ett.908", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Simplified volterra filters for acoustic echo cancellation in gsm receivers", "author": ["\u2014\u2014"], "venue": "Signal Processing Conference, 2000 10th European, Sept 2000, pp. 1\u20134.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "Some of the most complex filters are based on the Volterra and Wiener models [1]\u2013[3].", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "Some of the most complex filters are based on the Volterra and Wiener models [1]\u2013[3].", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "It is common to restrict these models in order to reduce their complexity [4]\u2013[6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "It is common to restrict these models in order to reduce their complexity [4]\u2013[6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "It is based on representing the input regressor as an iterated Kronecker product, an idea that has already been used for the estimation of higher order statistics of the regressor [7]\u2014a related problem\u2014and other Volterra approaches [4]\u2013[6], but we also embed this Kronecker structure within the filter itself.", "startOffset": 180, "endOffset": 183}, {"referenceID": 3, "context": "It is based on representing the input regressor as an iterated Kronecker product, an idea that has already been used for the estimation of higher order statistics of the regressor [7]\u2014a related problem\u2014and other Volterra approaches [4]\u2013[6], but we also embed this Kronecker structure within the filter itself.", "startOffset": 232, "endOffset": 235}, {"referenceID": 5, "context": "It is based on representing the input regressor as an iterated Kronecker product, an idea that has already been used for the estimation of higher order statistics of the regressor [7]\u2014a related problem\u2014and other Volterra approaches [4]\u2013[6], but we also embed this Kronecker structure within the filter itself.", "startOffset": 236, "endOffset": 239}, {"referenceID": 7, "context": "1This notation is due to [8].", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "Kronecker Representation Using the identity (A \u2297 B)(C \u2297 D) = (AC) \u2297 (BD) for the Kronocker product [9], it follows that (5) is equivalent to (6), because the Kronecker product of scalars is their ordinary multiplication.", "startOffset": 99, "endOffset": 102}, {"referenceID": 9, "context": "Additionally, one can think of the \u201cw\u201d tensor as a K-linear form [11]\u2014or functional\u2014acting on K copies of the vector ui.", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "Plotted together on the graph are the curves for the Volterra-LMS and the Wiener-LMS, designed for the Gaussian vector u, as described in [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "2e shows the Power Filter (PF) [12], Simplified Volterra (SV) [13], [14], Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra filter and the SML.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "2e shows the Power Filter (PF) [12], Simplified Volterra (SV) [13], [14], Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra filter and the SML.", "startOffset": 62, "endOffset": 66}, {"referenceID": 12, "context": "2e shows the Power Filter (PF) [12], Simplified Volterra (SV) [13], [14], Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra filter and the SML.", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "2e shows the Power Filter (PF) [12], Simplified Volterra (SV) [13], [14], Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra filter and the SML.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "2e shows the Power Filter (PF) [12], Simplified Volterra (SV) [13], [14], Sparse Interpolated Volterra (IV) [5], [6], the regular Volterra filter and the SML.", "startOffset": 113, "endOffset": 116}, {"referenceID": 7, "context": "The other terms from (14) involve only the conjugates of the entries of w, so they became to zero [8].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "For wRuKw, we have, remembering we do not derivate the conjugates [8], \u2202(wRuKw) \u2202(ws)q = \u2211", "startOffset": 66, "endOffset": 69}], "year": 2016, "abstractText": "Nonlinear adaptive filtering allows for modeling of some additional aspects of a general system and usually relies on highly complex algorithms, such as those based on the Volterra series. Through the use of the Kronecker product and some basic facts of tensor algebra, we propose a simple model of nonlinearity, one that can be interpreted as a product of the outputs of K FIR linear filters, and compute its cost function together with its gradient, which allows for some analysis of the optimization problem. We use these results it in a stochastic gradient framework, from which we derive an LMS-like algorithm and investigate the problems of multi-modality in the mean-square error surface and the choice of adequate initial conditions. Its computational complexity is calculated. The new algorithm is tested in a system identification setup and is compared with other polynomial algorithms from the literature, presenting favorable convergence and/or computational complexity.", "creator": "LaTeX with hyperref package"}}}