{"id": "1603.00968", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "abstract": "we introduce a novel, simple abstract convolution neural network ( cnn ) integrated architecture - multi - group norm constraint cnn ( mgnc - cnn ) programming that capitalizes on multiple sets of word embeddings for sentence classification. mgnc - cnn extracts features from input embedding sets independently and then joins these at the penultimate layer in the network bundle to form a final feature vector. we then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. this straightforward model choice is much simpler than comparable alternative architectures and overall requires substantially less training time. if furthermore, it is flexible in that it does not require two input word embeddings to be of the same dimensionality. we show similarly that mgnc - cnn consistently outperforms baseline models.", "histories": [["v1", "Thu, 3 Mar 2016 04:12:02 GMT  (200kb,D)", "http://arxiv.org/abs/1603.00968v1", "This paper got accepted by NAACL 2016"], ["v2", "Sun, 27 Mar 2016 03:28:31 GMT  (44kb,D)", "http://arxiv.org/abs/1603.00968v2", "This paper got accepted by NAACL 2016"]], "COMMENTS": "This paper got accepted by NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ye zhang", "stephen roller", "byron wallace"], "accepted": true, "id": "1603.00968"}, "pdf": {"name": "1603.00968.pdf", "metadata": {"source": "CRF", "title": "MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification", "authors": ["Ye Zhang", "Byron Wallace"], "emails": ["yezhang@cs.utexas.edu", "roller@cs.utexas.edu", "byron.wallace@utexas.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural models have recently gained popularity for Natural Language Processing (NLP) tasks (Goldberg, 2015; Collobert and Weston, 2008; Cho, 2015). For sentence classification, in particular, Convolution Neural Networks (CNN) have realized impressive performance (Kim, 2014; Zhang and Wallace, 2015). These models operate over word embeddings, i.e., dense, low dimensional vector representations of words that aim to capture salient semantic and syntactic properties (Collobert and Weston, 2008).\nAn important consideration for such models is the specification of the word embeddings. Sev-\neral options exist. For example, Kalchbrenner et al. (2014) initialize word vectors to random lowdimensional vectors to be fit during training, while Johnson and Zhang (2014) use fixed, one-hot encodings for each word. By contrast, Kim (2014) initializes word vectors to those estimated via the word2vec model trained on 100 billion words of Google News (Mikolov et al., 2013); these are then updated during training. Initializing embeddings to pre-trained word vectors is intuitively appealing because it allows transfer of learned distributional semantics. This has allowed a relatively simple CNN architecture to achieve remarkably strong results.\nMany pre-trained word embeddings are now readily available on the web, induced using different models, corpora, and processing steps. Different embeddings may encode different aspects of language (Pado\u0301 and Lapata, 2007; Erk and Pado\u0301, 2008; Levy and Goldberg, 2014): those based on bag-ofwords (BoW) statistics tend to capture associations (doctor and hospital), while embeddings based on dependency-parses encode similarity in terms of use (doctor and surgeon). It is natural to consider how these embeddings might be combined to improve NLP models in general and CNNs in particular.\nContributions. We propose MGNC-CNN, a novel, simple, scalable CNN architecture that can accommodate multiple off-the-shelf embeddings of variable sizes. Our model treats different word embeddings as distinct groups, and applies CNNs independently to each, thus generating corresponding feature vectors (one per embedding) which are then concatenated at the classification layer. Inspired by prior work exploiting regularization to encode struc-\nar X\niv :1\n60 3.\n00 96\n8v 1\n[ cs\n.C L\n] 3\nM ar\nture for NLP tasks (Yogatama and Smith, 2014; Wallace et al., 2015), we impose different regularization penalties on weights for features generated from the respective word embedding sets.\nOur approach enjoys the following advantages compared to the only existing comparable model (Yin and Schu\u0308tze, 2015): (i) It can leverage diverse, readily available word embeddings with different dimensions, thus providing flexibility. (ii) It is comparatively simple, and does not, for example, require mutual learning or pre-training. (iii) It is an order of magnitude more efficient in terms of training time."}, {"heading": "2 Related Work", "text": "Prior work has considered combining latent representations of words that capture syntactic and semantic properties (Van de Cruys et al., 2011), and inducing multi-modal embeddings (Bruni et al., 2012) for general NLP tasks. And recently, Luo et al. (2014) proposed a framework that combines multiple word embeddings to measure text similarity, however their focus was not on classification.\nMore similar to our work, Yin and Schu\u0308tze (2015) proposed MVCNN for sentence classification. This CNN-based architecture accepts multiple word embeddings as inputs. These are then treated as separate \u2018channels\u2019, analogous to RGB channels in images. Filters consider all channels simultaneously. MVCNN achieved state-of-the-art performance on multiple sentence classification tasks. However, this model has practical drawbacks. (i) MVCNN requires that input word embeddings have the same dimensionality. Thus to incorporate a second set of word vectors trained on a corpus (or using a model) of interest, one needs to either find embeddings that happen to have a set number of dimensions or to estimate embeddings from scratch. (ii) The model is complex, both in terms of implementation and runtime. Indeed, this model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour (and is easy to implement)."}, {"heading": "3 Model Description", "text": "We first review standard one-layer CNN (which exploits a single set of embeddings) for sentence classification (Kim, 2014), and then propose our augmentations, which exploit multiple embedding sets.\nBasic CNN. In this model we first replace each word in a sentence with its vector representation, resulting in a sentence matrix A \u2208 Rs\u00d7d, where s is the (zero-padded) sentence length, and d is the dimensionality of the embeddings. We apply a convolution operation between linear filters with parameters w1,w2, ...,wk and the sentence matrix. For each wi \u2208 Rh\u00d7d, where h denotes \u2018height\u2019, we slide filter i across A, considering \u2018local regions\u2019 of h adjacent rows at a time. At each local region, we perform element-wise multiplication and then take the element-wise sum between the filter and the (flattened) sub-matrix of A, producing a scalar. We do this for each sub-region of A that the filter spans, resulting in a feature map vector ci \u2208 R(s\u2212h+1)\u00d71. We can use multiple filter sizes with different heights, and for each filter size we can have multiple filters. Thus the model comprises k weight vectors w1,w2, ...wk, each of which is associated with an instantiation of a specific filter size. These in turn generate corresponding feature maps c1, c2, ...ck, with dimensions varying with filter size. A 1-max pooling operation is applied to each feature map, extracting the largest number oi from each feature map i. Finally, we combine all oi together to form a feature vector o \u2208 Rk to be fed through a softmax function for classification. We regularize weights at this level in two ways. (1) Dropout, in which we randomly set elements in o to zero during the training phase with probability p, and multiply p with the parameters trained in o at test time. (2) An l2 norm penalty, in which we set a threshold \u03bb for the l2 norm of o during training; if this is exceeded, we rescale the vector accordingly.\nMG-CNN. Assuming we have m word embeddings with corresponding dimensions d1, d2, ...dm, we can simply treat each word embedding independently. In this case, the input to the CNN comprises multiple sentence matrices A1,A2, ...Am, where each Al \u2208 Rs\u00d7dl may have its own width dl. We then apply different groups of filters {w1}, {w2}, ...{wm} independently to each Al, where {wl} denotes the set of filters for Al. As in basic CNN, {wl}may have multiple filter sizes, and multiple filters of each size may be introduced. At the classification layer we then obtain a feature vector ol for each embedding set, and we can simply concatenate these together to form the final feature\nvector o to feed into the softmax function, where o = o1 \u2295 o2... \u2295 om. This representation contains feature vectors generated from all sets of embeddings under consideration. We call this method multiple group CNN (MG-CNN). Here groups refer to the features generated from different embeddings. Note that this differs from \u2018multi-channel\u2019 models because at the convolution layer we use different filters on each word embedding matrix independently, whereas in a standard multi-channel approach, each filter would consider all channels simultaneously and generate a scalar from all channels at each local region. As above, we impose a max l2 norm constraint on the final feature vector o for regularization. Figure 1 illustrates this approach.\nMGNC-CNN. We propose an augmentation of MG-CNN, Multi-Group Norm Constraint CNN (MGNC-CNN), which differs in its regularization strategy. Specifically, in this variant we impose grouped regularization constraints, independently regularizing subcomponents ol derived from the respective embeddings, i.e., we impose separate max norm constraints \u03bbl for each ol (where l again in-\ndexes embedding sets); these \u03bbl hyper-parameters are to be tuned on a validation set. Intuitively, this method aims to better capitalize on features derived from word embeddings that capture discriminative properties of text for the task at hand by penalizing larger weight estimates for features derived from less discriminative embeddings."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "Stanford Sentiment Treebank Stanford Sentiment Treebank (SST) (Socher et al., 2013). This concerns predicting movie review sentiment. Two datasets are derived from this corpus: (1) SST-1, containing five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set.1 Subj (Pang and Lee, 2004). The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each. TREC (Li and Roth, 2002). A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances."}, {"heading": "4.2 Pre-trained Word Embeddings", "text": "We consider three sets of word embeddings for our experiments: (i) word2vec2 is trained on 100 billion tokens of Google News dataset; (ii) GloVe (Pennington et al., 2014)3 is trained on aggregated global word-word co-occurrence statistics from Common Crawl (840B tokens); and (iii) syntactic word embedding trained on dependency-parsed corpora. These three embedding sets happen to all be 300- dimensional, but our model could accommodate arbitrary and variable sizes.\nWe pre-trained our own syntactic embeddings following (Levy and Goldberg, 2014). We parsed the ukWaC corpus (Baroni et al., 2009) using the Stanford Dependency Parser v3.5.2 with Stanford Dependencies (Chen and Manning, 2014) and extracted (word, relation+context) pairs from parse trees. We \u201ccollapsed\u201d nodes with prepositions\n1As in (Kim, 2014). 2https://code.google.com/p/word2vec/ 3http://nlp.stanford.edu/projects/glove/\nand notated inverse relations separately, e.g., \u201cdog barks\u201d emits two tuples: (barks, nsubj dog) and (dog, nsubj\u22121 barks). We filter words and contexts that appear fewer than 100 times, resulting in\u223c173k words and 1M contexts. We trained 300d vectors using word2vecf4 with default parameters."}, {"heading": "4.3 Setup", "text": "We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings (Kim, 2014). We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic. For all models, we tuned the l2 norm constraint \u03bb over the range {13 , 1, 3, 9, 81, 243} on a validation set. For MGNCCNN, we tuned both \u03bb1, and \u03bb2.\nWe used standard train/test splits for those datasets that had them. Otherwise, we performed 10-fold cross validation, creating nested development sets with which to tune hyperparameters. For all experiments we used filters sizes of 3, 4 and 5 and we created 100 feature maps for each filter size. We applied 1 max-pooling and dropout (rate: 0.5) at the classification layer. For training we used backpropagation in mini-batches and used AdaDelta as the stochastic gradient descent (SGD) update rule. In all our experiments, we only tuned the max norm constraint(s), fixing all other hyperparameters."}, {"heading": "4.4 Results and Discussion", "text": "We repeated each experiment 10 times and report the mean and ranges across these. This replication is important because training is stochastic and thus\n4https://bitbucket.org/yoavgo/word2vecf/\nintroduces variance in performance (Zhang and Wallace, 2015). Results are shown in Table 1. We also show results on Subj, SST-1 and SST-2 achieved by the more complex model of (Yin and Schu\u0308tze, 2015) for comparison; this represents the state-of-the-art on the three datasets other than TREC.\nWe can see that MGNC-CNN and MG-CNN always outperform baseline methods (including CCNN), and MGNC-CNN is always better than MG-CNN. And on the Subj dataset, our model actually achieves slightly better results than (Yin and Schu\u0308tze, 2015), with far less complexity and required training time. On the TREC dataset, the bestever accuracy ever we are aware of is 96.0% (Mou et al., 2015); our results are comparable to this.\nOn SST-1 and SST-2, our model performs slightly worse than (Yin and Schu\u0308tze, 2015). However, we again note that their performance is achieved using a much more complex model which involves pretraining and mutual-learning steps. This model takes days to train, whereas our model requires on the order of an hour."}, {"heading": "5 Conclusions", "text": "We have proposed MGNC-CNN: a simple, flexible CNN architecture for sentence classification that can exploit multiple, variable sized word embeddings. We demonstrated that this consistently achieves better results than a baseline architecture that exploits only a single set of word embeddings, and also a naive concatenation approach to capitalizing on multiple embeddings. Furthermore, our results are comparable to those achieved with a recently proposed model (Yin and Schu\u0308tze, 2015) that is much more complex. However, our simple model is easy to implement and requires an order of magnitude less training time. Furthermore, our model is much more flexible than previous approaches, because it can accommodate variable-size word embeddings."}], "references": [{"title": "The wacky wide web: a collection of very large linguistically processed web-crawled corpora", "author": ["Baroni et al.2009] Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta"], "venue": "Language resources and evaluation,", "citeRegEx": "Baroni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2009}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Natural language understanding with distributed representation. arXiv preprint arXiv:1511.07916", "author": ["Kyunghyun Cho"], "venue": null, "citeRegEx": "Cho.,? \\Q2015\\E", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "A structured vector space model for word meaning in context", "author": ["Erk", "Pad\u00f32008] Katrin Erk", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Erk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2008}, {"title": "A primer on neural network models for natural language processing. arXiv preprint arXiv:1510.00726", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Effective use of word order for text categorization with convolutional neural networks. arXiv preprint arXiv:1412.1058", "author": ["Johnson", "Zhang2014] Rie Johnson", "Tong Zhang"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Learning question classifiers", "author": ["Li", "Roth2002] Xin Li", "Dan Roth"], "venue": "In Proceedings of the 19th international conference on Computational linguisticsVolume", "citeRegEx": "Li et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Li et al\\.", "year": 2002}, {"title": "Pre-trained multi-view word embedding using two-side neural network", "author": ["Luo et al.2014] Yong Luo", "Jian Tang", "Jun Yan", "Chao Xu", "Zheng Chen"], "venue": "In TwentyEighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Luo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative neural sentence modeling by tree-based convolution", "author": ["Mou et al.2015] Lili Mou", "Hao Peng", "Ge Li", "Yan Xu", "Lu Zhang", "Zhi Jin"], "venue": "Unpublished manuscript: http://arxiv. org/abs/1504. 01106v5. Version,", "citeRegEx": "Mou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2015}, {"title": "Dependency-based construction of semantic space models", "author": ["Pad\u00f3", "Lapata2007] Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2007}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Pang", "Lee2004] Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "Proceedings of the conference", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Latent vector weighting for word meaning in context", "author": ["Thierry Poibeau", "Anna Korhonen"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cruys et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cruys et al\\.", "year": 2011}, {"title": "Sparse, contextually informed models for irony detection: Exploiting user communities, entities and sentiment", "author": ["Do Kook Choe", "Eugene Charniak"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Wallace et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2015}, {"title": "Multichannel variable-size convolution for sentence classification", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Making the most of bag of words: Sentence regularization with alternating direction method of multipliers", "author": ["Yogatama", "Smith2014] Dani Yogatama", "Noah Smith"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Yogatama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2014}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Zhang", "Wallace2015] Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Natural Language Processing (NLP) tasks (Goldberg, 2015; Collobert and Weston, 2008; Cho, 2015).", "startOffset": 40, "endOffset": 95}, {"referenceID": 3, "context": "Natural Language Processing (NLP) tasks (Goldberg, 2015; Collobert and Weston, 2008; Cho, 2015).", "startOffset": 40, "endOffset": 95}, {"referenceID": 8, "context": "For example, Kalchbrenner et al. (2014) initialize word vectors to random lowdimensional vectors to be fit during training, while", "startOffset": 13, "endOffset": 40}, {"referenceID": 13, "context": "By contrast, Kim (2014) initializes word vectors to those estimated via the word2vec model trained on 100 billion words of Google News (Mikolov et al., 2013); these are then updated during training.", "startOffset": 135, "endOffset": 157}, {"referenceID": 9, "context": "By contrast, Kim (2014) initializes word vectors to those estimated via the word2vec model trained on 100 billion words of Google News (Mikolov et al.", "startOffset": 13, "endOffset": 24}, {"referenceID": 20, "context": "ture for NLP tasks (Yogatama and Smith, 2014; Wallace et al., 2015), we impose different regularization penalties on weights for features generated from the respective word embedding sets.", "startOffset": 19, "endOffset": 67}, {"referenceID": 1, "context": ", 2011), and inducing multi-modal embeddings (Bruni et al., 2012) for general NLP tasks.", "startOffset": 45, "endOffset": 65}, {"referenceID": 1, "context": ", 2011), and inducing multi-modal embeddings (Bruni et al., 2012) for general NLP tasks. And recently, Luo et al. (2014) proposed a framework that combines mul-", "startOffset": 46, "endOffset": 121}, {"referenceID": 9, "context": "sification (Kim, 2014), and then propose our augmentations, which exploit multiple embedding sets.", "startOffset": 11, "endOffset": 22}, {"referenceID": 18, "context": "Stanford Sentiment Treebank Stanford Sentiment Treebank (SST) (Socher et al., 2013).", "startOffset": 62, "endOffset": 83}, {"referenceID": 17, "context": "We consider three sets of word embeddings for our experiments: (i) word2vec2 is trained on 100 billion tokens of Google News dataset; (ii) GloVe (Pennington et al., 2014)3 is trained on aggregated global word-word co-occurrence statistics from Common Crawl (840B tokens); and (iii) syntactic word embedding trained on dependency-parsed corpora.", "startOffset": 145, "endOffset": 170}, {"referenceID": 0, "context": "We parsed the ukWaC corpus (Baroni et al., 2009) using the Stanford Dependency Parser v3.", "startOffset": 27, "endOffset": 48}, {"referenceID": 9, "context": "As in (Kim, 2014).", "startOffset": 6, "endOffset": 17}, {"referenceID": 9, "context": "We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings (Kim, 2014).", "startOffset": 100, "endOffset": 111}], "year": 2017, "abstractText": "We introduce a novel, simple convolution neural network (CNN) architecture \u2013 multi-group norm constraint CNN (MGNC-CNN) \u2013 that capitalizes on multiple sets of word embeddings for sentence classification. MGNCCNN extracts features from input embedding sets independently and then joins these at the penultimate layer in the network to form a final feature vector. We then adopt a group regularization strategy that differentially penalizes weights associated with the subcomponents generated from the respective embedding sets. This model is much simpler than comparable alternative architectures and requires substantially less training time. Furthermore, it is flexible in that it does not require input word embeddings to be of the same dimensionality. We show that MGNC-CNN consistently outperforms baseline models.", "creator": "LaTeX with hyperref package"}}}