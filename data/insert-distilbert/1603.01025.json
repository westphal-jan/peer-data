{"id": "1603.01025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "abstract": "recent advances in convolutional neural networks have considered model complexity and hardware efficiency mechanisms to thereby enable deployment onto embedded systems and mobile devices. for example, it is now well - known that the arithmetic operations of deep networks can be encoded down to 8 - bit fixed - point without significant deterioration in performance. however, adding further reduction in precision down to as low db as 3 - bit fixed - point results in globally significant losses in performance. in this paper we propose a new data representation that enables state - of - the - art networks to be encoded to 3 bits with negligible computational loss in classification performance. to perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non - uniform distributions. using non - uniform, base - 2 ) logarithmic representation to encode weights, communicate activations, and perform discrete dot - products enables networks to gain 1 ) physically achieve higher classification accuracies than fixed - point at the same resolution and 2 ) eliminate bulky digital multipliers. finally, we propose an end - to - end training procedure that uses log representation at 5 - bits, which achieves higher final test accuracy than linear at 5 - bits.", "histories": [["v1", "Thu, 3 Mar 2016 08:51:52 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01025v1", "10 pages, 7 figures"], ["v2", "Thu, 17 Mar 2016 03:32:30 GMT  (2526kb,D)", "http://arxiv.org/abs/1603.01025v2", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["daisuke miyashita", "edward h lee", "boris murmann"], "accepted": false, "id": "1603.01025"}, "pdf": {"name": "1603.01025.pdf", "metadata": {"source": "META", "title": "Convolutional Neural Networks using Logarithmic Data Representation", "authors": ["Daisuke Miyashita", "Edward H. Lee", "Boris Murmann"], "emails": ["DAISUKEM@STANFORD.EDU", "EDHLEE@STANFORD.EDU", "MURMANN@STANFORD.EDU"], "sections": [{"heading": "1. Introduction", "text": "Deep convolutional neural networks (CNN) have demonstrated state-of-the-art performance in image classification\n(Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015) but have steadily grown in computational complexity. For example, the Deep Residual Learning (He et al., 2015) set a new record in image classification accuracy at the expense of 11.3 billion floating-point multiplyand-add operations per forward-pass of an image and 230 MB of memory to store the weights in its 152-layer network.\nIn order for these large networks to run in real-time applications such as for mobile or embedded platforms, it is often necessary to use low-precision arithmetic and apply compression techniques. Recently, many researchers have successfully deployed networks that compute using 8-bit fixed-point representation (Vanhoucke et al., 2011; Abadi et al., 2015) and have successfully trained networks with 16-bit fixed point (Gupta et al., 2015). This work in particular is built upon the idea that algorithm-level noise tolerance of the network can motivate simplifications in hardware complexity.\nInteresting directions point towards matrix factorization (Denton et al., 2014) and tensorification (Novikov et al., 2015) by leveraging structure of the fully-connected (FC) layers. Another promising area is to prune the FC layer before mapping this to sparse matrix-matrix routines in GPUs (Han et al., 2015b). However, many of these inventions aim at systems that meet some required and specific criteria such as networks that have many, large FC layers or accelerators that handle efficient sparse matrix-matrix arithmetic. And with network architectures currently pushing towards increasing the depth of convolutional layers by settling for fewer dense FC layers (He et al., 2015; Szegedy et al., 2015), there are potential problems in motivating a one-size-fits-all solution to handle these computational and memory demands.\nWe propose a general method of representing and comput-\nar X\niv :1\n60 3.\n01 02\n5v 1\n[ cs\n.N E\n] 3\nM ar\ning the dot products in a network that can allow networks with minimal constraint on the layer properties to run more efficiently in digital hardware. In this paper we explore the use of communicating activations, storing weights, and computing the atomic dot-products in the binary logarithmic (base-2 logarithmic) domain for both inference and training. The motivations for moving to this domain are the following:\n\u2022 Training networks with weight decay leads to final weights that are distributed non-uniformly around 0.\n\u2022 Similarly, activations are also highly concentrated near 0. Our work uses rectified Linear Units (ReLU) as the non-linearity.\n\u2022 Logarithmic representations can encode data with very large dynamic range in fewer bits than can fixedpoint representation (Gautschi et al., 2016).\n\u2022 Data representation in log-domain is naturally encoded in digital hardware (as shown in Section 4.3).\nOur contributions are listed:\n\u2022 we show that networks obtain higher classification accuracies with logarithmic quantization than linear quantization using traditional fixed-point at equivalent resolutions.\n\u2022 we show that activations are more robust to quantization than weights. This is because the number of activations tend to be larger than the number of weights which are reused during convolutions.\n\u2022 we apply our logarithmic data representation on stateof-the-art networks, allowing activations and weights to use only 3b with almost no loss in classification performance.\n\u2022 we generalize base-2 arithmetic to handle different base. In particular, we show that a base\u221a 2 enables\nthe ability to capture large dynamic ranges of weights and activations but also finer precisions across the encoded range of values as well.\n\u2022 we develop logarithmic backpropagation for efficient training."}, {"heading": "2. Related work", "text": "Reduced-precision computation. (Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference. For example, (Han et al., 2015b) shows that convolutional layers in AlexNet (Krizhevsky et al., 2012) can be\nencoded to as little as 5 bits without a significant accuracy penalty. There has also been recent work in training using low precision arithmetic. (Gupta et al., 2015) propose a stochastic rounding scheme to help train networks using 16-bit fixed-point. (Lin et al., 2015) propose quantized back-propagation and ternary connect. This method reduces the number of floating-point multiplications by casting these operations into powers-of-two multiplies, which are easily realized with bitshifts in digital hardware. They apply this technique on MNIST and CIFAR10 with little loss in performance. However, their method does not completely eliminate all multiplications end-to-end. During test-time the network uses the learned full resolution weights for forward propagation. Training with reduced precision is motivated by the idea that high-precision gradient updates is unnecessary for the stochastic optimization of networks (Bottou & Bousquet, 2007; Bishop, 1995; Audhkhasi et al., 2013). In fact, there are some studies that show that gradient noise helps convergence. For example, (Neelakantan et al., 2015) empirically finds that gradient noise can also encourage faster exploration and annealing of optimization space, which can help network generalization performance.\nHardware implementations. There have been a few but significant advances in the development of specialized hardware of large networks. For example (Farabet et al., 2010) developed Field-Programmable Gate Arrays (FPGA) to perform real-time forward propagation. These groups have also performed a comprehensive study of classification performance and energy efficiency as function of resolution. (Zhang et al., 2015) have also explored the design of convolutions in the context of memory versus compute management under the RoofLine model. Other works focus on specialized, optimized kernels for general purpose GPUs (Chetlur et al., 2014)."}, {"heading": "3. Concept and Motivation", "text": "Each convolutional and fully-connected layer of a network performs matrix operations that distills down to dot products y = wTx, where x \u2208 Rn is the input, w \u2208 Rn the weights, and y the activations before being transformed by the non-linearity (e.g. ReLU). Using conventional digital hardware, this operation is performed using n multiplyand-add operations using floating or fixed point representation as shown in Figure 1(a). However, this dot product can also be computed in the log-domain as shown in Figure 1(b,c)."}, {"heading": "3.1. Proposed Method 1.", "text": "The first proposed method as shown in Figure 1(b) is to transform one operand to its log representation, convert the resulting transformation back to the linear domain, and\nmultiply this by the other operand. This is simply\nwTx ' n\u2211\ni=1\nwi \u00d7 2x\u0303i\n= n\u2211 i=1 Bitshift(wi, x\u0303i), (1)\nwhere x\u0303i = Quantize(log2(xi)), Quantize(\u2022) quantizes \u2022 to an integer, and Bitshift(a, b) is the function that bitshifts a value a by an integer b in fixed-point arithmetic. In floating-point, this operation is simply an addition of b with the exponent part of a. Taking advantage of the Bitshift(a, b) operator to perform multiplication obviates the need for expensive digital multipliers.\nQuantizing the activations and weights in the log-domain (log2(x) and log2(w)) instead of x and w is also motivated by leveraging structure of the non-uniform distributions of x and w. A detailed treatment is shown in the next section. In order to quantize, we propose two hardware-friendly flavors. The first option is to simply floor the input. This method computes blog2(w)c by returning the position of the first 1 bit seen from the most significant bit (MSB). The second option is to round to the nearest integer, which is more precise than the first option. With the latter option, after computing the integer part, the fractional part is computed in order to assert the rounding direction. This method of rounding is summarized as follows. Pick m bits followed by the leftmost 1 and consider it as a fixed point number F with 0 integer bit and m fractional bits. Then, if F \u2265 \u221a 2 \u2212 1, round F up to the nearest integer and otherwise round it down to the nearest integer."}, {"heading": "3.2. Proposed Method 2.", "text": "The second proposed method as shown in Figure 1(c) is to extend the first method to compute dot products in the log-domain for both operands. Additions in linear-domain map to sums of exponentials in the log-domain and multiplications in linear become log-addition. The resulting dot-product is\nwTx ' n\u2211\ni=1\n2Quantize(log2(wi))+Quantize(log2(xi))\n= n\u2211 i=1 Bitshift(1, w\u0303i + x\u0303i), (2)\nwhere the log-domain weights are w\u0303i = Quantize(log2(wi)) and log-domain inputs are x\u0303i = Quantize(log2(xi)).\nBy transforming both the weights and inputs, we compute the original dot product by bitshifting 1 by an integer result w\u0303i + x\u0303i and summing over all i.\n3.3. Accumulation in log domain\nAlthough Fig. 1(b,c) indicates a logarithm-to-linear converter between layers where the actual accumulation is performed in the linear domain, this accumulation is able to be performed in the log-domain using the approximation log2(1 + x) ' x for 0 \u2264 x < 1. When n = 2,\ns\u03032 = log2\n( 2\u2211\ni=1\nBitshift (1, p\u0303i) ) ' max (p\u03031, p\u03032) + Bitshift (1,\u2212|p\u03031 \u2212 p\u03032|) , (3)\nand when n = n,\ns\u0303n = log2\n( n\u2211\ni=1\nBitshift (1, p\u0303i) ) ' max (s\u0303n\u22121, p\u0303n) + Bitshift (1,\u2212|bs\u0303n\u22121c \u2212 p\u0303n|) . (4)\nNote that s\u0303i preserves fractional part during accumulation and then it is possible to make the error due to the approximation less than that due to quantization that is applied after completion of accumulation.\nBoth accumulation in linear domain and accumulation in log domain have its pros and cons. Accumulation in linear domain is simpler but requires larger bit widths in order to handle large dynamic range numbers. Accumulation in log domain requires slightly complicated computations in Equations 3 and 4, but the required bit width is smaller."}, {"heading": "4. Experiments of Proposed Methods", "text": "Here we evaluate our methods as detailed in Sections 3.1 and 3.2 on the classification task of ILSVRC-2012 (Deng et al., 2009) using Chainer (Tokui et al., 2015). We evaluate method 1 (Section 3.1) on inference (forward pass) in Section 4.1. Similarly, we evaluate method 2 (Section 3.2) on inference in Sections 4.2 and 4.3. For those experiments, we use published models (AlexNet (Krizhevsky et al., 2012), VGG16 (Simonyan & Zisserman, 2014)) from the caffe model zoo ((Jia et al., 2014)) without any fine tuning (or extra retraining). Finally, we evaluate method 2 on training in Section 4.4."}, {"heading": "4.1. Logarithmic Representation of Activations", "text": "This experiment evaluates the classification accuracy using logarithmic activations and floating point 32b for the weights. In similar spirit to that of (Gupta et al., 2015), we describe the logarithmic quantization layer LogQuant that performs the element-wise operation as follows:\nLogQuant(x,bitwidth,FSR) = { 0 x = 0, 2x\u0303 otherwise, (5)\nwhere x\u0303 = Clip ( Round(log2(|x|)),FSR\u2212 2bitwidth,FSR ) , (6)\nClip(x,min,max) =  0 x \u2264 min,max\u2212 1 x \u2265 max, x otherwise. (7)\nThese layers perform the logarithmic quantization and computation as detailed in Section 3.1. Tables 1 and 2 illustrate the addition of these layers to the models. The quantizer has a specified full scale range, and this range in linear scale is 2FSR, where we express this as simply FSR throughout this paper for notational convenience. The FSR values for each layer are shown in Tables 1 and 2; they show fsr added by an offset parameter. This offset parameter is chosen to properly handle the variation of activation ranges from layer to layer using 100 images from the training set. The fsr is a parameter which is global to the network and is tuned to perform the experiments to measure the effect of FSR on classification accuracy. The bitwidth is the number of bits required to represent a number after\nquantization. Note that since we assume applying quantization after ReLU function, x is 0 or positive and then we use unsigned format without sign bit for activations.\nIn order to evaluate our logarithmic representation, we detail an equivalent linear quantization layer described as\nLinearQuant(x, bitwidth,FSR)\n= Clip ( Round ( x\nstep\n) \u00d7 step, 0, 2FSR ) (8)\nand where\nstep = 2FSR\u2212bitwidth. (9)\nFigure 2 illustrates the effect of the quantizer on activations following the conv2 2 layer used in VGG16. The prequantized distribution tends to 0 exponentially, and the logquantized distribution illustrates how the log-encoded activations are uniformly equalized across many output bins which is not prevalent in the linear case. Many smaller activation values are more finely represented by log quantization compared to linear quantization. The total quantization error 1N ||Quantize(x)\u2212x||1, where Quantize(\u2022) is LogQuant(\u2022) or LinearQuant(\u2022), x is the vectorized activations of size N , is less for the log-quantized case than for linear. This result is illustrated in Figure 3. Using linear quantization with step size of 1024, we obtain a distribution of quantization errors that are highly concentrated in the region where |LinearQuant(x) \u2212 x| < 512. However, log quantization with the bitwidth as linear results in a significantly lower number of quantization errors in the region 128 < |LogQuant(x) \u2212 x| < 512. This comes at the expense of a slight increase in errors in the region 512 < |LogQuant(x) \u2212 x|. Nonetheless, the quantization errors 1N ||LogQuant(x) \u2212 x||1 = 34.19 for log and 1 N ||LogQuant(x)\u2212 x||1 = 102.89 for linear.\nWe run the models as described in Tables 1 and 2 and test on the validation set without data augmentation. We evaluate it with variable bitwidths and FSRs for both quantizer layers.\nFigure 4 illustrates the results of AlexNet. Using only 3 bits to represent the activations for both logarithmic and linear quantizations, the top-5 accuracy is still very close to that of the original, unquantized model encoded at floating-point 32b. However, logarithmic representations tolerate a large dynamic range of FSRs. For example, using 4b log, we can obtain 3 order of magnitude variations in the full scale without a significant loss of top-5 accuracy. We see similar results for VGG16 as shown in Figure 5. Table 3 lists the classification accuracies with the optimal FSRs for each case. There are some interesting observations. First, 3b log performs 0.2% worse than 3b linear for AlexNet but 6.2% better for VGG16, which is a higher capacity network than AlexNet. Second, by encoding the activations in 3b log, we\nachieve the same top-5 accuracy compared to that achieved by 4b linear for VGG16. Third, with 4b log, there is no loss in top-5 accuracy from the original float32 representation."}, {"heading": "4.2. Logarithmic Representation of Weights of Fully Connected Layers", "text": "The FC weights are quantized using the same strategies as those in Section 4.1, except that they have sign bit. We evaluate the classification performance using log data representation for both FC weights and activations jointly using method 2 in Section 3.2. For comparison, we use linear for FC weights and log for activations as reference. For both methods, we use optimal 4b log for activations that were computed in Section 4.1.\nTable 4 compares the mentioned approaches along with floating point. We observe a small 0.4% win for log over linear for AlexNet but a 0.2% decrease for VGG16. Nonetheless, log computation is performed without the use of multipliers.\nAn added benefit to quantization is a reduction of the model size. By quantizing down to 4b log including sign bit, we compress the FC weights for free significantly from 1.9 Gb to 0.27 Gb for AlexNet and 4.4 Gb to 0.97 Gb for VGG16. This is because the dense FC layers occupy 98.2% and 89.4% of the total model size for AlexNet and VGG16 respectively."}, {"heading": "4.3. Logarithmic Representation of Weights of Convolutional Layers", "text": "We now represent the convolutional layers using the same procedure. We keep the representation of activations at 4b log and the representation of weights of FC layers at 4b log, and compare our log method with the linear reference and ideal floating point. We also perform the dot products using two different bases: 2, \u221a 2. Note that there is no additional overhead for log base\u221a 2 as it is computed with the same equation shown in Equation 4.\nTable 5 shows the classification results. The results illustrate an approximate 6% drop in performance from floating point down to 5b base-2 but a relatively minor 1.7% drop for 5b base\u221a 2. They includes sign bit. There are also some important observations here.\nWe first observe that the weights of the convolutional layers for AlexNet and VGG16 are more sensitive to quantization than are FC weights. Each FC weight is used only once per image (batch size of 1) whereas convolutional weights are reused many times across the layer\u2019s input activation map. Because of this, the quantization error of each weight now influences the dot products across the entire activation volume. Second, we observe that by moving from 5b base2 to a finer granularity such as 5b base\u221a 2, we allow the\nnetwork to 1) be robust to quantization errors and degradation in classification performance and 2) retain the practical features of log-domain arithmetic.\nThe distributions of quantization errors for both 5b base-2 and 5b base\u221a 2 are shown in Figure 6. The total quantization error on the weights, 1N ||Quantize(x)\u2212x||1, where x is the vectorized weights of size N , is 2\u00d7 smaller for base\u221a 2 than for base-2."}, {"heading": "4.4. Training with Logarithmic Representation", "text": "We incorporate log representation during the training phase. This entire algorithm can be computed using Method 2 in Section 3.2. Table 6 illustrates the networks that we compare. The proposed log and linear networks are trained at the same resolution using 4-bit unsigned activations and 5-bit signed weights and gradients using Algorithm 1 on the CIFAR10 dataset with simple data augmentation described in (He et al., 2015). Note that unlike BinaryNet (Courbariaux & Bengio, 2016), we quantize the backpropagated gradients to train log-net. This enables end-to-end training using logarithmic representation at the 5-bit level. For linear quantization however, we found it necessary to keep the gradients in its unquantized floatingpoint precision form in order to achieve good convergence. Furthermore, we include the training curve for BinaryNet, which uses unquantized gradients.\nFig. 7 illustrates the training results of log, linear, and BinaryNet. Final test accuracies for log-5b, linear-5b, and BinaryNet are 0.9379, 0.9253, 0.8862 respectively where linear-5b and BinaryNet use unquantized gradients. The test results indicate that even with quantized gradients, our proposed network with log representation still outperforms the others that use unquantized gradients.\nAlgorithm 1 Training a CNN with base-2 logarithmic representation. C is the softmax loss for each minibatch. LogQuant(x) quantizes x in base-2 log-domain. The optimization step Update(Wk,gWk ) updates the weights Wk based on backpropagated gradients gWk . We use the SGD with momentum and Adam rule. Require: a minibatch of inputs and targets (a0, a\u2217), previ-\nous weights W . Ensure: updated weights W t+1 {1. Computing the parameters\u2019 gradient:} {1.1. Forward propagation:} for k = 1 to L do W qk \u2190 LogQuant(Wk) ak \u2190 ReLU ( aqk\u22121W b k\n) aqk \u2190 LogQuant(ak)\nend for {1.2. Backward propagation:} Compute gaL = \u2202C \u2202aL knowing aL and a\u2217 for k = L to 1 do gqak \u2190 LogQuant(gak) gak\u22121 \u2190 gqakW q k\ngWk \u2190 gq>ak a q k\u22121\nend for {2. Accumulating the parameters\u2019 gradient:} for k = 1 to L do W t+1k \u2190 Update(Wk, gWk) end for\nFigure 7. Loss curve and test accuracy"}, {"heading": "5. Conclusion", "text": "In this paper, we describe a method to represent the weights and activations with low resolution in the log-domain, which eliminates bulky digital multipliers. This method is also motivated by the non-uniform distributions of weights and activations, making log representation more robust to quantization as compared to linear. We evaluate our methods on the classification task of ILSVRC-2012 using pretrained models (AlexNet and VGG16). We also offer extensions that incorporate end-to-end training using log representation including gradients."}], "references": [{"title": "Noise benefits in backpropagation and deep bidirectional pre-training", "author": ["Audhkhasi", "Kartik", "Osoba", "Osonde", "Kosko", "Bart"], "venue": "In Proceedings of The 2013 International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "Audhkhasi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audhkhasi et al\\.", "year": 2013}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Bishop", "Christopher M"], "venue": "In Neural Computation,", "citeRegEx": "Bishop and M.,? \\Q1995\\E", "shortCiteRegEx": "Bishop and M.", "year": 1995}, {"title": "The tradeoffs of large scale learning", "author": ["Bottou", "L\u00e9on", "Bousquet", "Olivier"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bottou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2007}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Chetlur", "Sharan", "Woolley", "Cliff", "Vandermersch", "Philippe", "Cohen", "Jonathan", "Tran", "John", "Catanzaro", "Bryan", "Shelhamer", "Evan"], "venue": "In Proceedings of Deep Learning and Representation Learning Workshop: NIPS 2014,", "citeRegEx": "Chetlur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. FeiFei"], "venue": "In CVPR09,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton", "Emily", "Zaremba", "Wojciech", "Bruna", "Joan", "LeCun", "Yann", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Hardware accelerated convolutional neural networks for synthetic vision systems", "author": ["Farabet", "Cl\u00e9ment", "Martini", "Berin", "Akselrod", "Polina", "Talay", "Sel\u00e7uk", "LeCun", "Yann", "Culurciello", "Eugenio"], "venue": "In Proceedings of 2010 IEEE International Symposium on Circuits and Systems (IS-", "citeRegEx": "Farabet et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2010}, {"title": "Deep learning with limited numerical precision", "author": ["Gupta", "Suyog", "Agrawal", "Ankur", "Gopalakrishnan", "Kailash", "Narayanan", "Pritish"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Han", "Song", "Mao", "Huizi", "Dally", "William J"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Neural networks with few multiplications", "author": ["Lin", "Zhouhan", "Courbariaux", "Matthieu", "Memisevic", "Roland", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1510.03009,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Neelakantan", "Arvind", "Vilnis", "Luke", "Le", "Quoc V", "Sutskever", "Ilya", "Kaiser", "Lukasz", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Tensorizing neural networks", "author": ["Novikov", "Alexander", "Podoprikhin", "Dmitry", "Osokin", "Anton", "Vetrov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Fixed point performance analysis of recurrent neural networks", "author": ["Shin", "Sungho", "Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Proceedings of The 41st IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP2016)", "citeRegEx": "Shin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:11409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Resiliency of deep neural networks under quantization", "author": ["Sung", "Wonyong", "Shin", "Sungho", "Hwang", "Kyuyeon"], "venue": "arXiv preprint arXiv:1511.06488,", "citeRegEx": "Sung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sung et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vanhoucke", "Vincent", "Senior", "Andrew", "Mao", "Mark Z"], "venue": "In Proceedings of Deep Learning and Unsupervised Feature Learning Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Optimizing FPGA-based accelerator design for deep convolutional neural networks", "author": ["Zhang", "Chen", "Li", "Peng", "Sun", "Guangyu", "Guan", "Yijin", "Xiao", "Bingjun", "Cong", "Jason"], "venue": "In Proceedings of 23rd International Symposium on Field-Programmable Gate Arrays (FPGA2015),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Deep convolutional neural networks (CNN) have demonstrated state-of-the-art performance in image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015) but have steadily grown in computational complexity.", "startOffset": 112, "endOffset": 182}, {"referenceID": 11, "context": "Deep convolutional neural networks (CNN) have demonstrated state-of-the-art performance in image classification (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015) but have steadily grown in computational complexity.", "startOffset": 112, "endOffset": 182}, {"referenceID": 11, "context": "For example, the Deep Residual Learning (He et al., 2015) set a new record in image classification accuracy at the expense of 11.", "startOffset": 40, "endOffset": 57}, {"referenceID": 20, "context": "Recently, many researchers have successfully deployed networks that compute using 8-bit fixed-point representation (Vanhoucke et al., 2011; Abadi et al., 2015) and have successfully trained networks with 16-bit fixed point (Gupta et al.", "startOffset": 115, "endOffset": 159}, {"referenceID": 8, "context": ", 2015) and have successfully trained networks with 16-bit fixed point (Gupta et al., 2015).", "startOffset": 71, "endOffset": 91}, {"referenceID": 6, "context": "Interesting directions point towards matrix factorization (Denton et al., 2014) and tensorification (Novikov et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 15, "context": ", 2014) and tensorification (Novikov et al., 2015) by leveraging structure of the fully-connected (FC) layers.", "startOffset": 28, "endOffset": 50}, {"referenceID": 11, "context": "And with network architectures currently pushing towards increasing the depth of convolutional layers by settling for fewer dense FC layers (He et al., 2015; Szegedy et al., 2015), there are potential problems in motivating a one-size-fits-all solution to handle these computational and memory demands.", "startOffset": 140, "endOffset": 179}, {"referenceID": 19, "context": "And with network architectures currently pushing towards increasing the depth of convolutional layers by settling for fewer dense FC layers (He et al., 2015; Szegedy et al., 2015), there are potential problems in motivating a one-size-fits-all solution to handle these computational and memory demands.", "startOffset": 140, "endOffset": 179}, {"referenceID": 16, "context": "(Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference.", "startOffset": 0, "endOffset": 81}, {"referenceID": 18, "context": "(Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference.", "startOffset": 0, "endOffset": 81}, {"referenceID": 20, "context": "(Shin et al., 2016; Sung et al., 2015; Vanhoucke et al., 2011; Han et al., 2015a) analyzed the effects of quantizing the trained weights for inference.", "startOffset": 0, "endOffset": 81}, {"referenceID": 12, "context": ", 2015b) shows that convolutional layers in AlexNet (Krizhevsky et al., 2012) can be encoded to as little as 5 bits without a significant accuracy penalty.", "startOffset": 52, "endOffset": 77}, {"referenceID": 8, "context": "(Gupta et al., 2015) propose a stochastic rounding scheme to help train networks using 16-bit fixed-point.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "(Lin et al., 2015) propose quantized back-propagation and ternary connect.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Training with reduced precision is motivated by the idea that high-precision gradient updates is unnecessary for the stochastic optimization of networks (Bottou & Bousquet, 2007; Bishop, 1995; Audhkhasi et al., 2013).", "startOffset": 153, "endOffset": 216}, {"referenceID": 14, "context": "For example, (Neelakantan et al., 2015) empirically finds that gradient noise can also encourage faster exploration and annealing of optimization space, which can help network generalization performance.", "startOffset": 13, "endOffset": 39}, {"referenceID": 7, "context": "For example (Farabet et al., 2010) developed Field-Programmable Gate Arrays (FPGA) to perform real-time forward propagation.", "startOffset": 12, "endOffset": 34}, {"referenceID": 21, "context": "(Zhang et al., 2015) have also explored the design of convolutions in the context of memory versus compute management under the RoofLine model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Other works focus on specialized, optimized kernels for general purpose GPUs (Chetlur et al., 2014).", "startOffset": 77, "endOffset": 99}, {"referenceID": 12, "context": "Structure of AlexNet(Krizhevsky et al., 2012) with quan-", "startOffset": 20, "endOffset": 45}, {"referenceID": 5, "context": "2 on the classification task of ILSVRC-2012 (Deng et al., 2009) using Chainer (Tokui et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 12, "context": "For those experiments, we use published models (AlexNet (Krizhevsky et al., 2012), VGG16 (Simonyan & Zisserman, 2014)) from the caffe model zoo ((Jia et al.", "startOffset": 56, "endOffset": 81}, {"referenceID": 8, "context": "In similar spirit to that of (Gupta et al., 2015), we describe the logarithmic quantization layer LogQuant that performs the element-wise operation as follows:", "startOffset": 29, "endOffset": 49}, {"referenceID": 11, "context": "The proposed log and linear networks are trained at the same resolution using 4-bit unsigned activations and 5-bit signed weights and gradients using Algorithm 1 on the CIFAR10 dataset with simple data augmentation described in (He et al., 2015).", "startOffset": 228, "endOffset": 245}], "year": 2017, "abstractText": "Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.", "creator": "LaTeX with hyperref package"}}}