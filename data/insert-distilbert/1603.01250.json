{"id": "1603.01250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Decision Forests, Convolutional Networks and the Models in-Between", "abstract": "this paper investigates the connections between two state of magnitude the high art classifiers : decision forests ( dfs, including decision jungles ) and convolutional neural networks ( conditional cnns ). decision forests are actually computationally efficient thanks to their conditional computation property ( computation is confined to only a small specified region because of exactly the tree, the nodes along a single branch ). cnns achieve state of the art accuracy, thanks to their representation learning capabilities. we present a systematic analysis of how to fuse conditional probability computation with probability representation learning and achieve a continuum of hybrid models scaled with roughly different ratios of accuracy vs. efficiency. we call this new family of hybrid models conditional networks. conditional networks can thus be thought of as : i ) decision trees augmented with data queue transformation operators, or ii ) symmetric cnns, with hierarchical block - diagonal sparse weight matrices, and explicit data routing functions. experimental validation is performed on the common task of image classification on both the cifar and naive imagenet datasets. compared to state of the art cnns, our hybrid computational models potentially yield the same computing accuracy with a fraction of the compute cost generated and much hence smaller number of parameters.", "histories": [["v1", "Thu, 3 Mar 2016 20:41:47 GMT  (4095kb,D)", "http://arxiv.org/abs/1603.01250v1", "Microsoft Research Technical Report"]], "COMMENTS": "Microsoft Research Technical Report", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yani ioannou", "duncan robertson", "darko zikic", "peter kontschieder", "jamie shotton", "matthew brown", "antonio criminisi"], "accepted": false, "id": "1603.01250"}, "pdf": {"name": "1603.01250.pdf", "metadata": {"source": "CRF", "title": "Decision Forests, Convolutional Networks and the Models in-Between", "authors": ["Yani Ioannou", "Duncan Robertson", "Darko Zikic", "Peter Kontschieder", "Jamie Shotton", "Matthew Brown", "Antonio Criminisi"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Machine learning has enjoyed much success in recent years for both academic and commercial scenarios. Two learning approaches have gained particular attention: (i) random forests [1, 4, 5, 22], as used e.g. in Microsoft Kinect [15]; and (ii) deep neural networks (DNNs) [13, 20], as used for speech recognition [31] and image classification [9], among other applications. Decision trees are characterized by a routed behavior: conditioned on some learned routing function, the data is sent either to one child or another. This conditional computation means that at test time only a small fraction of all the nodes are visited, thus achieving high efficiency. Convolutional neural networks repeatedly transform their input through several (learned)\nnon-linear transformations. Typically, at each layer all units need to perform computation. CNNs achieve state-of-theart accuracy in many tasks, but decision trees have the potential to be more efficient. This paper investigates the connections between these two popular models, highlighting differences and similarities in theory and practice. Related work. Decision forests were introduced in [1, 4] as efficient models for classification and regression. Forests were extended to density estimation, manifold learning and semi-supervised learning in [5]. The decision jungle variant [22] replaces trees with DAGs (directed acyclical graphs) to reduce memory consumption.\nConvolutional networks were introduced for the task of digit recognition in [6]. More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].\nIn general, decision trees and neural networks are perceived to be very different models. However, the work in [21, 30] demonstrates how any decision tree or DAG can be represented as a two-layer perceptron with a special pattern of sparsity in the weight matrices. Some recent papers have addressed the issue of mixing properties of trees and convolutional networks together. For example, the tworouted CNN architecture in [13] is a stump (a tree with only two branches). GoogLeNet [27] is another example of a (imbalanced) tree-like CNN architecture.\nThe work in [26, 28] combines multiple \u201cexpert\u201d CNNs into one, manually designed DAG architecture. Each component CNN is trained on a specific task (e.g. detecting an object part), using a part-specific loss. In contrast, here we investigate training a single, tree-shaped CNN model by minimizing one global training loss. In our model the various branches are not explicitly trained to recognize parts (though they may do so if this minimizes the overall loss).\nThe work in [33] is a cascade [29] of CNN classifiers, each trained at a different level of recognition difficulty. Their model does not consider tree-based architectures. Finally, the work in [11] achieves state of the art classification accuracy by replacing the fully-connected layers of a CNN with a forest. This model is at least as expensive as the orig-\n1\nar X\niv :1\n60 3.\n01 25\n0v 1\n[ cs\n.C V\n] 3\nM ar\n2 01\ninal CNN since the convolutional layers (where most of the computation is) are not split into different branches. Contributions. The contributions of this paper are as follows: i) We show how DAG-based CNN architectures (namely conditional networks) with a rich hierarchical structure (e.g. high number of branches, more balanced trees) produce classification accuracy which is at par with state of the art, but with much lower compute and memory requirements. ii) We demonstrate how conditional networks are still differentiable despite the presence of explicit data routing functions. iii) We show how conditional networks can be used to fuse the output of CNN ensembles in a data driven way, yielding higher accuracy for fixed compute. Validation is run on the task of image-level classification, on both the CIFAR and Imagenet datasets."}, {"heading": "2. Structured Sparsity and Data Routing", "text": "The seminal work in [13] demonstrated how introducing rectified linear unit activations (ReLUs) allows deep CNNs to be trained effectively. Given a scalar input vj , its ReLU activation is \u03c3(vj) = max(0, vj). Thus, this type of nonlinearity switches off a large number of feature responses within a CNN. ReLU activations induce a data-dependent sparsity; but this sparsity does not tend to have much structure in it. Enforcing a special type of structured sparsity is at the basis of the efficiency gain attained by conditional networks. We illustrate this concept with a toy example.\nThe output of the exemplar multi-layer perceptron (MLP) of Fig. 1a is computed as v2 = \u03c3(P12v1) = \u03c3(P12\u03c3(P01v0)). Given a trained MLP we can look at the average correlation of activations between pairs of units in two successive layers, over all training data. For example,\nthe matrix \u039b12 (Fig. 1b) shows the joint correlations of activations in layers 1 and 2 in a perceptron trained on the Imagenet classification task.1 Here we use the final two layers of the deep CNN model of [23] with a reduced number of features (250) and classes (350) to aid visualization.\nThanks to the ReLUs, the correlation matrix \u039b12 has many zero-valued elements (in white in Fig. 1b), and these are distributed in an unstructured way. Reordering the rows and columns of \u039b12 reveals an underlying, noisy blockdiagonal pattern (Fig. 1c). This operation corresponds to finding groups of layer-1 features which are highly active for certain subsets of classes (indexed in layer-2). Thus, the darker blocks in Fig. 1c correspond to three super-classes (sets of \u2018related\u2019 classes). Zeroing out the off-diagonal elements (Fig. 1e) corresponds to removing connections between corresponding unit pairs. This yields the sparse architecture in Fig. 1d, where selected subsets of the layer-1 features are sent (after transformation) to the corresponding subsets of layer-2 units; thus giving rise to data routing.\nWe have shown how imposing a block-diagonal pattern of sparsity to the joint activation correlation in a neural network corresponds to equipping the network with a tree-like, routed architecture. Next section will formalize this intuition further and show the benefits of sparse architectures."}, {"heading": "3. Conditional Networks: Trees or Nets?", "text": "This section introduces the conditional networks model, in comparison to trees and CNNs, and discusses their efficiency and training. For clarity, we first introduce a compact graphical notation for representating both trees and CNNs.\nRepresenting CNNs. Figure 2a shows the conventional way to represent an MLP, with its units (circles) connected via edges (for weights). Our new notation is shown in Fig. 2b, where the symbol Pij o denotes the popular nonlinear transformation vj = \u03c3(Pvi) between two consecutive layers i and j. The linear projection matrix is denoted \u2018P\u2019, and \u2018o\u2019 indicates a non-linear function \u03c3(.) (e.g. a sigmoid or ReLU). In the case of CNNs the function \u03c3 could also incorporate e.g. max-pooling and drop-out. Deep CNNs are long concatenations of the structure in Fig. 2b.\nRepresenting trees and DAGs. The same graphical language can also represent trees and DAGs (Fig. 3). Usually, in a tree the data is moved from one node to another untransformed.2 In our notation this is achieved via the identity matrix I (i.e. vj = Ivi). Additionally, Selecting a subset of features v\u2032 from a longer vector v is achieved as v\u2032 = Sv, with S non-square matrix with only one element per row equal to 1, and 0 everywhere else. Identity and selection transforms are special instances of linear projections.\n1The correlation matrix \u039b12 is not the same as the weight matrix P12. 2This is in contrast to representation learning approaches which esti-\nmate optimal data transformation processes. Exceptions are [16, 17].\nA key operator of trees which is not present in CNNs is data routing. Routers send the incoming data to a selected sub-branch and enable conditional computation. Routers (red nodes in Fig. 3) are represented here as perceptrons, though other choices are possible. In general, a router outputs real-valued weights, which may be used to select a single best route, multiple routes (multi-way routing), or send the data fractionally to all children (soft routing).\nA conditional network exhibits both data routing and non-linear data transformation within a highly branched architecture (Fig. 4)."}, {"heading": "3.1. Computational Efficiency", "text": "Efficiency through explicit data routing. Split nodes can have explicit routers where data is conditionally sent to the children according to the output of a routing function (e.g. node 2 in Fig. 4), or have implicit routers where the data is unconditionally but selectively sent to the children using\nselection matrices S (e.g. node 1). If the routing is explicit and hard (like in trees), then successive operations will be applied to ever smaller subsets of incoming data, with the associated compute savings. Next we show how implicit conditional networks can also yield efficiency. Efficiency of implicit routed networks. Figures 5 compares a standard CNN with a 2-routed architecture. The total numbers of filters at each layer is fixed for both to c1, c2 and c3. The number of multiplications necessary in the first convolution is c2 \u00d7 c1kxkyWH , with W,H the size of the feature map and kx, ky the kernel size (for simplicity here we ignore max-pooling operations). This is the same for both architectures. However, due to routing, the depth of the second set of filters is different between the two architectures. Therefore, for the conventional CNN the cost of the second convolution is c3\u00d7 c2kxkyWH , while for the branched architecture the cost is c3 \u00d7 ( c2 2 ) kxkyWH , i.e. half the cost of the standard CNN. The increased efficiency is due only to the fact that shallower kernels are convolved with shallower feature maps. Simultaneous processing of parallel routes may yield additional time savings.3"}, {"heading": "3.2. Back-propagation Training", "text": "Implicitly-routed conditional networks can be trained with the standard back-propagation algorithm [13, 27]. The selection functions S become extra parameters to optimize over, and their gradients can be derived straightforwardly. Now we show that explicitly-routed networks can also be trained using back-propagation. To do so we need to compute partial derivatives with respect to the router\u2019s parameters (all other differentiation operations are as in conventional CNNs). We illustrate this using the small network in Fig. 6. Here subscripts index layers and superscripts index\n3Feature not yet implemented in Caffe [28].\nroutes (instead, in Fig. 4 the subscripts indexed the input and output nodes). The training loss to be minimized is\nL(\u03b8) = 1\n2 (v2(\u03b8)\u2212 v\u22172) > (v2(\u03b8)\u2212 v\u22172) , (1)\nwith \u03b8 = {{Pj}, PR} denoting the parameters of the network, and v\u22172 the ground-truth assignments to the output units. We define this energy for a single training data point, though the extension to a full dataset is a trivial outer summation. The network\u2019s forward mapping is\nvj1 = \u03c3 ( Pjv0 ) and v2(\u03b8) = r(\u03b8) V1(\u03b8), (2)\nwith r = \u03c3 (PRv0) the output of the router. In general: i) the routing weights r are continuous, r(i) \u2208 [0, 1], and ii) multiple routes can be \u201con\u201d at the same time. V1 is a matrix whose j-th row is (vj1)\n>. The update rule is \u2206\u03b8t+1 := \u2212\u03c1 \u2202E\u2202\u03b8 \u2223\u2223 t , with t indexing iterations. We compute the partial derivatives through the chain rule as follows:\n\u2202L \u2202\u03b8 = \u2202L\n\u2202v2\n\u2202v2 \u2202\u03b8 = \u2202L\n\u2202v2\n \u2202r \u2202PR V1 + R\u2211 j=1 r(j) \u2202vj1 \u2202\u03c6j \u2202\u03c6j \u2202Pj  , (3)\nwith \u03c6j := Pjv0, and R the number of routes. Equation (3) shows the influence of the soft routing weights on the backpropagated gradients, for each route. Thus, explicit routers can be trained as part of the overall back-propagation procedure. Since trees and DAGs are special instances of conditional networks, now we have a recipe for training them via back-propagation (c.f . [11, 19, 25]).\nIn summary, conditional networks may be thought of as: i) Decision trees/DAGs which have been enriched with\n(learned) data transformation operations, or as ii) CNNs with rich, DAG-shaped architectures and trainable data routing functions. Next, we show efficiency advantages of such branched models with comparative experiments."}, {"heading": "4. Experiments and Comparisons", "text": "Conditional networks generalize decision trees, DAGs and CNNs, and thus could be used in all tasks where those are successful. Here we compare those models with one another on the popular task of image-level classification. We explore the effect of different \u201cbranched\u201d architectures on a joint measure of: i) classification accuracy, ii) test-time compute cost and iii) model size."}, {"heading": "4.1. Conditional Sparsification of a Perceptron", "text": "We begin with a toy experiment, designed to illustrate potential advandages of using explicit routes within a neural network. We take a perceptron (the last layer of \u201cVGG11\u201d [23]) and train it on the 1,000 Imagenet classes, with no scale or relighting augmentation [10]. Then we turn the perceptron into a small tree, with R routes and an additional, compact perceptron as a router (see fig. 7a). The router PR8 and the projection matrices P i 8 are trained to minimize the overall classification loss (Sec. 3.2). Interpolating between trees and CNNs. Given a test image we apply the convolutional layers until the beginning of the tree. Then we apply the router, and its R outputs are soft-max normalized and treated as probabilities for deciding which route/s to send the image to. We can send the image only to the highest probability route only (as done in trees) or we could send it to multiple routes, e.g. the \u03c4 most probable ones. For \u03c4 = 1 we reproduce the behaviour of a tree. This corresponds to the left-most point in the curves in fig. 7b (lowest cost and higher error). Setting \u03c4 = R corresponds to sending the image to all routes. The latter reproduces the same behaviour as the CNN, with nearly the same cost (lowest error and highest compute cost point in the curves). Different values of \u03c4 \u2208 {1, . . . , R} correspond to different points along the error-cost curves.\nDynamic accuracy-efficiency trade-off. The ability to select the desired accuracy-efficiency operating point at runtime allows e.g. better battery management in mobile applications. In contrast, a CNN corresponds to a single point in the accuracy-efficiency space (see the black point in fig. 7b). The pronounced sub-linear behaviour of the curves in fig. 7b suggests that we can increase the efficiency considerably with little accuracy reduction (in the figure a 4-fold efficiency increase yields an increase in error of less than 1%). Why care about the amount of computation? Modern parallel architectures (such as GPUs) yield high classification accuracy in little time. But parallelism is not the only way of increasing efficiency. Here we focus on reducing the total amount of computations while maintaining high accuracy. Computation affects power consumption, which is of huge practical importance in mobile applications (to increase battery life on a smartphone) as well as in cloud services (the biggest costs in data centres are due to their cooling). Next we extend conditional processing also to the expensive convolutional layers of a deep CNN."}, {"heading": "4.2. Comparing Various Architectures on Imagenet", "text": "Here we validate the use of conditional networks for image classification in the ILSVRC2012 dataset [18]. The dataset consists of \u223c1.2M training images for 1000 classes,\nand 50K validation images. We base our experiments on the VGG network [23] on which the current best models are also based [9]. Specifically, we focus on the VGG11 model as it is deep (11 layers) and relatively memory efficient (trains with Caffe [10] on a single Nvidia K40 GPU). Global max-pooling. We found that using global maxpooling, after the last convolutional layer is effective in reducing the number of parameters while maintaining the same accuracy. We trained a new network (\u2018VGG11-GMP\u2019) with such pooling, and achieved lower top-5 error than the baseline VGG11 network (13.3% vs. 13.8%), with a decrease in the number of parameters of over 72%. Designing an efficient conditional architecture. Then we designed the conditional network in Fig. 8 by starting with the unrouted VGG11-GMP and splitting the convolutional layers (the most computationally expensive layers) into a DAG-like, routed architecture. The hypothesis is that each filter should only need to be applied to a small number of channels in the input feature map. Data routing is implemented via filter groups [13]. Thus, at the n-th convolutional level (with n = 3 . . . 5) the filters of VGG11-GMP are divided into 2(n\u22122) groups. Each group depends only on the results of 128 previous filters. The feature maps of the last convolutional layer are concatenated together, and globally max-pooled before the single-routed, fully-connected layers, which remain the same as those in VGG11-GMP. Training. We trained the architecture in Fig. 8 from scratch, with the same parameters as in [23], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt)\n\u22121, where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3]. When the validation accuracy levelled out, the learning rate was decreased by a factor 10, twice. Our architecture took twice as many epochs to train than VGG11, but\nthanks to higher efficiency it took roughly the same time. Results: accuracy vs. compute vs. size. In order to compare different network architectures as fairly as possible, here we did not use any training augmentation aside from that supported by Caffe (mirroring and random crops). Similarly, we report test-time accuracy based only on centrecropped images, without potentially expensive data oversampling. This reduces the overall accuracy (w.r.t. to state of the art), but constitutes a fairer test bed for teasing out the effects of different architectures. Applying the same oversampling to all networks produced a similar accuracy improvement in all models, without changing their ranking.\nFigure 9 shows top-5 error as a function of test-time compute cost and model size, for various architectures. Compute cost is measured as the number of multiplyaccumulate operations. We chose this measure of efficiency because it is directly related to the theoretical complexity on the testing (run-time) algorithm, and it is machine/implementation independent. Later we will also show how in our parallel implementation this measure of efficiency correlates well with measured timings on both CPU and GPU. Model size is defined here as the total number of parameters (network weights) and it relates to memory efficiency. Larger model sizes tend to yield overfitting (for fixed accuracy). Architectures closest to the axes origin are both more accurate and more efficient.\nThe conditional network of Fig. 8 corresponds to the bright green circle in Fig. 9. It achieves a top-5 error of \u223c13.8%, identical to that of the VGG11 network (yellow diamond) it is based upon. However, our conditional network requires less than half the compute (45%), and almost one-fifth (21%) of the parameters. Our conditional architecture is the second closest to the origin after GoogLeNet [27] (in purple). Both [27] and [13] obtain efficiency by sending data to different branches of the network. Although they do not use \u201chighly branched\u201d tree structures they can still\nbe thought as special instances of (implicit) conditional networks. GoogLeNet achieves the best results in our joint three-way metric, probably thanks to their use of: i) multiple intermediate training losses, ii) learnt low-dimensional embeddings, and iii) better tuning of the architecture to the specific image dataset. Finally, the best accuracy is achieved by [9], but even their most efficient model uses 1.9E+10 flops, and thus falls outside the plot."}, {"heading": "Do fewer operations correspond to faster execution?", "text": "Figure 10 reports a layer-wise comparison between the predicted test-time compute cost (measured as number of multiply-accumulate operations in the model) and the actual measured timings (both on CPU and GPU) for the network architecture in Fig. 8. There is a strong correlation between the number of floating-point operations and the actual measured times. In the GPU case, the correlation is a slightly less strong, due to data moving overheads. This confirms that, indeed, fewer operations do correspond to faster execution, by roughly the same ratio. As discussed in Section 3.1 this extra speed (compared to conventional CNNs) comes\nfrom the fact that in branched architectures successive layers need to run convolutions with smaller shorter kernels, on ever smaller feature maps. All architectures tested in our experiments are implemented in the same Caffe framework and enjoy the same two levels of parallelism: i) parallel matrix multiplications (thanks to BLAS4), and ii) data parallelism, thanks to the use of mini-batches. Although highly-branched conditional networks could in theory benefit from model parallelism (computing different branches on different GPUs, simultaneously), this feature is not yet implemented in Caffe [28]."}, {"heading": "4.3. Comparing Various Architectures on CIFAR", "text": "We further validate our hybrid model on the task of classifying images in the CIFAR10 [12] dataset. The dataset contains 60K images of 10 classes, typically divided into 50K training images and 10K test images. We take the state of the art Network in Network (NiN) model as a reference [14], and we build a conditional version of it. This time the optimal conditional architecture (in Fig. 11) is constructed automatically, by using Bayesian search [24] on a parametrized family of architectures. Designing a family of conditional networks. The NiN model has a large number (192) of filters in the first convolutional layer, representing a sizable amount of the overall compute.5 We build a variant (\u2018NiN-64\u2019) that prepends a layer of 64 filters to the NiN model. While this variant is more complex than NiN, when routed (as described later) it allows us to split the larger layers into many routes and increase the efficiency. By changing the number of routes at each level of the NiN-64 model (from conv2) we can generate a whole family of possible conditional architectures. Learning the optimal network architecture. Next we search this parametrized space of routed architectures by using Bayesian optimization [24]. In the optimization we maximized the size-normalized accuracy \u03b1 =\n4http://www.netlib.org/blas/ 5Most Imagenet networks typically use 64\u2212 96 conv1 filters.\nclassification accuracy model size with respect to the parameters Rl = 2i, {i \u2208 N : 0 \u2264 i \u2264 5}, where Rl is the number of nodes at layer l in the conditional network. Fig. 11 shows the resulting architecture. It turns out to be a DAG with 10 layers.\nFor a fair comparison, we use Bayesian optimization on the NiN architecture too. We reduce the complexity of the unrouted NiN-64 network by learning a reduction in the number of per-layer filters. i.e. we maximize \u03b1 over Fl = Forig/2\ni, {i \u2208 N : 0 \u2264 i \u2264 4}, where Forig is the number of filters in layer l. All networks were trained with the same parameters as [14], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt)\u22121, where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3]. Training was run for 400 epochs (max), or until the validation accuracy had not changed in 10K iterations. We split the original training set into 40K training images and 10K validation images. The remaining 10K images are used for testing. Results: accuracy vs. compute vs. size. Fig. 12 shows test errors with respect to test-time cost and model size for multiple architectrues. Diamonds denote unrouted networks and circles denote conditional networks. The original NiN is shown in red, and samples of unrouted, filterreduced versions explored during the Bayesian optimization are shown in pink. A sample of 300 conditional variants are shown as grey circles. The green circle denotes one such conditional architecture close to the origin of the 3D space (test \u2212 error, test \u2212 cost,model \u2212 size). Most of the conditional networks proposed by the optimization are distributed near a 3D surface with either low error, low size, low compute cost, or all of them. The conditional samples are in average closer to the origin than the unrouted counterparts. The accuracy of the best conditional network is almost identical to that of the NiN model, but it is about 5 times faster and 6 times smaller."}, {"heading": "4.4. Conditional Ensembles of CNNs", "text": "A key difference between CNNs and conditional networks is that the latter may include (trainable) data routers. Here we use an explicitly-routed architecture to create an ensemble of CNNs where the data traverses only selected, component CNNs (and not necessarily all of them), thus saving computation.\nAs an example, the branched network in Fig. 13 is applied to the ILSVRC2012 image classification task. The network has R = 2 routes, each of which is itself a deep CNN. Here, we use GoogLeNet [27] as the basis of each component route, although other architectures may be used. Generalizing to R > 2 is straightforward. The routes have different compute cost (denoted by different-sized rectangles), arising from differing degrees of test-time oversampling. We use no oversampling for the first route and 10X oversampling for the second route. The router determines\nwhich image should be sent to which route (or both). The router is trained together with the rest of the network via back-propagation (Section 3.2) to predict the accuracy of each route for each image. The router is itself a deep CNN, based on CNN1; This allows computation reuse for extra efficiency. At test time, a (dynamic) trade off can be made between predicted accuracy and computational cost.\nFigure 14 shows the resulting error-cost curve. All costs, including the cost of applying the router are taken into consideration here. Given our trained conditional network, we use dynamic, multi-way data routing (Section 4.1) to generate a curve in the error-compute space. Each point on the curve shows the top-5 error on the validation set at a given compute cost, which is an amortized average over the validation set. The dashed line corresponds to the trivial error vs. compute trade-off that could be made by selecting one or other base network at random, with a probability chosen so as to achieve a required average compute cost. The fact that the green curve lies significantly below this straight line confirms the much improved trade-off achieved by the conditional network. In the operating point indicated by the green circle we achieve nearly the same accuracy as the 10\u00d7 oversampled GoogLeNet with less than half its compute cost. A conventional CNN ensemble would incur a higher cost since all routes are used for all images."}, {"heading": "5. Discussion and Conclusion", "text": "This paper has investigated similarities and differences between decision trees/forests and convolutional networks. This has led us to introduce a hybrid model (namely conditional network) which can be thought both as: i) trees which have been augmented with representation learning capabilities, and ii) CNNs which have been augmented with explicit data routers and a rich, branched architecture.\nExperiments on image classification have shown that highly branched architectures yield improved accuracyefficiency trade-off as compared to trees or CNNs. The desired accuracy-efficiency ratio can be selected at run time, without the need to train a new network. Finally, we have shown how explicit routers can improve the efficiency of ensembles of CNNs, without loss of accuracy. We hope these findings will help pave the way to a more systematic exploration of efficient architectures for deep learning at scale."}], "references": [{"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural Computation, 9(7),", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Do Deep Nets Really Need to be Deep", "author": ["L.J. Ba", "R. Caruana"], "venue": "Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic Gradient Descent Tricks", "author": ["L. Bottou"], "venue": "G. Montavon, G. B. Orr, and K.-R. M\u00fcller, editors, Neural Networks: Tricks of the Trade (2nd ed.), volume 7700 of Lecture Notes in Computer Science, pages 421\u2013436. Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Decision Forests for Computer Vision and Medical Image Analysis", "author": ["A. Criminisi", "J. Shotton"], "venue": "Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Hand-written digit recognition with a back-propagation network", "author": ["Y.L. Cun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Predicting Parameters in Deep Learning", "author": ["M. Denil", "B. Shakibi", "L. Dinh", "M.A. Ranzato", "N. de- Freitas"], "venue": "In Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation", "author": ["E. Denton", "W. Zaremba", "J. Bruna", "Y. LeCun", "R.Fergus"], "venue": "In Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "author": ["K. He", "X. Zhang", "J. Sun"], "venue": "eprint arXiv:1502.01852v1,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proc. of the ACM Intl Conf. on Multimedia,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Bulo.\u0301 Deep neural decision forests", "author": ["P. Kontschieder", "M. Fiterau", "A. Criminisi", "S. R"], "venue": "Proc. IEEE Intl Conf. on Computer Vision (ICCV),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, Univ. Toronto,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Entangled decision forests and their application for semantic segmentation of CT images", "author": ["A. Montillo", "J. Shotton", "J. Winn", "J. Iglesias", "D. Metaxas", "A. Criminisi"], "venue": "Proc. Information Processing in Medical Imaging (IPMI),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural decision forests for semantic image labelling", "author": ["S. Rota Bul\u00f2", "P. Kontschieder"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), June", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Alternating decision forests", "author": ["S. Schulter", "P. Wohlhart", "C. Leistner", "A. Saffari", "P.M. Roth", "H. Bischof"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "OverFeat: Integrated Recognition, localization and Detection using Convolutional Networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "Proc. Intl Conf. on Learning Representations (ICLR),", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Entropy nets: from decision trees to neural networks", "author": ["I.K. Sethi"], "venue": "Technical report, Dept. of Computer Science, Wayne State Univ., Detroit, MI,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Decision jungles: Compact and rich models for classification", "author": ["J. Shotton", "T. Sharp", "P. Kohli", "S. Nowozin", "J. Winn", "A. Criminisi"], "venue": "Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Very Deep Convolutional networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proc. Intl Conf. on Learning Representations (ICLR),", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Proc. Neural Information Processing Systems (NIPS),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Globally optimal fuzzy decision trees for classification and regression", "author": ["A. Su\u00e1rez", "J.F. Lutsko"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 21(12), Dec.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Deep Convolutional Network Cascade for Facial Point Detection", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["A. Toshev", "C. Szegedy"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "Intl Journal on Computer Vision (IJCV), 57(2),", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Casting Random Forests as Artificial Neural Networks (and Profiting from It)", "author": ["J. Welbl"], "venue": "Proc. German Conference on Pattern Recognition (GCPR),", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic Speech Recognition: A Deep Learning Approach", "author": ["D. Yu", "L. Deng"], "venue": "Springer,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient and Accurate Approximations of Nonlinear Convolutional Networks", "author": ["X. Zhang", "J. Zou", "X. Ming", "K. He", "J. Sun"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-Stage Contextual Deep Learning for Pedestrian Detection", "author": ["X. Zheng", "W.Ouyang", "X. Wang"], "venue": "In Proc. IEEE Intl Conf. on Computer Vision (ICCV),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Two learning approaches have gained particular attention: (i) random forests [1, 4, 5, 22], as used e.", "startOffset": 77, "endOffset": 90}, {"referenceID": 3, "context": "Two learning approaches have gained particular attention: (i) random forests [1, 4, 5, 22], as used e.", "startOffset": 77, "endOffset": 90}, {"referenceID": 4, "context": "Two learning approaches have gained particular attention: (i) random forests [1, 4, 5, 22], as used e.", "startOffset": 77, "endOffset": 90}, {"referenceID": 19, "context": "Two learning approaches have gained particular attention: (i) random forests [1, 4, 5, 22], as used e.", "startOffset": 77, "endOffset": 90}, {"referenceID": 12, "context": "in Microsoft Kinect [15]; and (ii) deep neural networks (DNNs) [13, 20], as used for speech recognition [31] and image classification [9], among other applications.", "startOffset": 63, "endOffset": 71}, {"referenceID": 17, "context": "in Microsoft Kinect [15]; and (ii) deep neural networks (DNNs) [13, 20], as used for speech recognition [31] and image classification [9], among other applications.", "startOffset": 63, "endOffset": 71}, {"referenceID": 28, "context": "in Microsoft Kinect [15]; and (ii) deep neural networks (DNNs) [13, 20], as used for speech recognition [31] and image classification [9], among other applications.", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "in Microsoft Kinect [15]; and (ii) deep neural networks (DNNs) [13, 20], as used for speech recognition [31] and image classification [9], among other applications.", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "Decision forests were introduced in [1, 4] as efficient models for classification and regression.", "startOffset": 36, "endOffset": 42}, {"referenceID": 3, "context": "Decision forests were introduced in [1, 4] as efficient models for classification and regression.", "startOffset": 36, "endOffset": 42}, {"referenceID": 4, "context": "Forests were extended to density estimation, manifold learning and semi-supervised learning in [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "The decision jungle variant [22] replaces trees with DAGs (directed acyclical graphs) to reduce memory consumption.", "startOffset": 28, "endOffset": 32}, {"referenceID": 5, "context": "Convolutional networks were introduced for the task of digit recognition in [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 6, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 7, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 8, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 12, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 17, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 20, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 24, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 29, "context": "More recently they have been applied with great success to the task of image classification over 1,000 classes [2, 7, 8, 9, 13, 14, 20, 23, 27, 32].", "startOffset": 111, "endOffset": 147}, {"referenceID": 18, "context": "However, the work in [21, 30] demonstrates how any decision tree or DAG can be represented as a two-layer perceptron with a special pattern of sparsity in the weight matrices.", "startOffset": 21, "endOffset": 29}, {"referenceID": 27, "context": "However, the work in [21, 30] demonstrates how any decision tree or DAG can be represented as a two-layer perceptron with a special pattern of sparsity in the weight matrices.", "startOffset": 21, "endOffset": 29}, {"referenceID": 12, "context": "For example, the tworouted CNN architecture in [13] is a stump (a tree with only two branches).", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "GoogLeNet [27] is another example of a (imbalanced) tree-like CNN architecture.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "The work in [26, 28] combines multiple \u201cexpert\u201d CNNs into one, manually designed DAG architecture.", "startOffset": 12, "endOffset": 20}, {"referenceID": 25, "context": "The work in [26, 28] combines multiple \u201cexpert\u201d CNNs into one, manually designed DAG architecture.", "startOffset": 12, "endOffset": 20}, {"referenceID": 30, "context": "The work in [33] is a cascade [29] of CNN classifiers, each trained at a different level of recognition difficulty.", "startOffset": 12, "endOffset": 16}, {"referenceID": 26, "context": "The work in [33] is a cascade [29] of CNN classifiers, each trained at a different level of recognition difficulty.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "Finally, the work in [11] achieves state of the art classification accuracy by replacing the fully-connected layers of a CNN with a forest.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "This is a portion of the \u2018VGG\u2019 model [23] trained on Imagenet.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "The seminal work in [13] demonstrated how introducing rectified linear unit activations (ReLUs) allows deep CNNs to be trained effectively.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "1 Here we use the final two layers of the deep CNN model of [23] with a reduced number of features (250) and classes (350) to aid visualization.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Exceptions are [16, 17].", "startOffset": 15, "endOffset": 23}, {"referenceID": 14, "context": "Exceptions are [16, 17].", "startOffset": 15, "endOffset": 23}, {"referenceID": 12, "context": "Implicitly-routed conditional networks can be trained with the standard back-propagation algorithm [13, 27].", "startOffset": 99, "endOffset": 107}, {"referenceID": 24, "context": "Implicitly-routed conditional networks can be trained with the standard back-propagation algorithm [13, 27].", "startOffset": 99, "endOffset": 107}, {"referenceID": 25, "context": "3Feature not yet implemented in Caffe [28].", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "In general: i) the routing weights r are continuous, r(i) \u2208 [0, 1], and ii) multiple routes can be \u201con\u201d at the same time.", "startOffset": 60, "endOffset": 66}, {"referenceID": 10, "context": "[11, 19, 25]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[11, 19, 25]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[11, 19, 25]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "We take a perceptron (the last layer of \u201cVGG11\u201d [23]) and train it on the 1,000 Imagenet classes, with no scale or relighting augmentation [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "We take a perceptron (the last layer of \u201cVGG11\u201d [23]) and train it on the 1,000 Imagenet classes, with no scale or relighting augmentation [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "(a) We take the deep CNN model in [23] (\u2018VGG11\u2019) and turn the last fully connected layer (size 4095\u00d71000) into a tree with R routes (R = 4 shown in figure).", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "Here we validate the use of conditional networks for image classification in the ILSVRC2012 dataset [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 20, "context": "We base our experiments on the VGG network [23] on which the current best models are also based [9].", "startOffset": 43, "endOffset": 47}, {"referenceID": 8, "context": "We base our experiments on the VGG network [23] on which the current best models are also based [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 9, "context": "Specifically, we focus on the VGG11 model as it is deep (11 layers) and relatively memory efficient (trains with Caffe [10] on a single Nvidia K40 GPU).", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Data routing is implemented via filter groups [13].", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "8 from scratch, with the same parameters as in [23], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt) \u22121, where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3].", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "8 from scratch, with the same parameters as in [23], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt) \u22121, where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "8 from scratch, with the same parameters as in [23], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt) \u22121, where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3].", "startOffset": 257, "endOffset": 260}, {"referenceID": 24, "context": "Our conditional architecture is the second closest to the origin after GoogLeNet [27] (in purple).", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "Both [27] and [13] obtain efficiency by sending data to different branches of the network.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "Both [27] and [13] obtain efficiency by sending data to different branches of the network.", "startOffset": 14, "endOffset": 18}, {"referenceID": 8, "context": "Finally, the best accuracy is achieved by [9], but even their most efficient model uses 1.", "startOffset": 42, "endOffset": 45}, {"referenceID": 25, "context": "Although highly-branched conditional networks could in theory benefit from model parallelism (computing different branches on different GPUs, simultaneously), this feature is not yet implemented in Caffe [28].", "startOffset": 204, "endOffset": 208}, {"referenceID": 11, "context": "We further validate our hybrid model on the task of classifying images in the CIFAR10 [12] dataset.", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "11) is constructed automatically, by using Bayesian search [24] on a parametrized family of architectures.", "startOffset": 59, "endOffset": 63}, {"referenceID": 21, "context": "Next we search this parametrized space of routed architectures by using Bayesian optimization [24].", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "All networks were trained with the same parameters as [14], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt), where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "All networks were trained with the same parameters as [14], except for using the initialization of [9], and a learning schedule of \u03b3t = \u03b30(1 + \u03b30\u03bbt), where \u03b30, \u03b3t and \u03bb are the initial learning rate, learning rate at iteration t, and weight decay, respectively [3].", "startOffset": 261, "endOffset": 264}, {"referenceID": 24, "context": "Here, we use GoogLeNet [27] as the basis of each component route, although other architectures may be used.", "startOffset": 23, "endOffset": 27}], "year": 2016, "abstractText": "This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters.", "creator": "LaTeX with hyperref package"}}}