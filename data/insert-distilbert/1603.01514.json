{"id": "1603.01514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "abstract": "we propose a bayesian model of unsupervised indirect semantic role induction in multiple languages, and use it to explore the usefulness required of parallel corpora for this task. our joint bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. because it is a generative bayesian model, we can do evaluations in a variety of scenarios just by precisely varying the inference procedure, without changing the model, thereby comparing the scenarios directly. we compare using only binary monolingual data, measuring using a parallel corpus, using a parallel corpus with annotations in the other language, and using small amounts of annotation in the target language. we find that the biggest impact of adding a parallel corpus to training is actually the intrinsic increase detected in mono - lingual data, with the alignments to another language eventually resulting in so small quantitative improvements, even with labeled data for the other language.", "histories": [["v1", "Fri, 4 Mar 2016 16:03:53 GMT  (413kb,D)", "http://arxiv.org/abs/1603.01514v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nikhil garg", "james henderson"], "accepted": false, "id": "1603.01514"}, "pdf": {"name": "1603.01514.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Model of Multilingual Unsupervised Semantic Role Induction", "authors": ["Nikhil Garg", "James Henderson"], "emails": ["nikgarg@gmail.com", "james.henderson@unige.ch"], "sections": [{"heading": "1 Introduction", "text": "Semantic Role Labeling (SRL) has emerged as an important task in Natural Language Processing (NLP) due to its applicability in information extraction, question answering, and other NLP tasks. SRL is the problem of finding predicate-argument structure in a sentence, as illustrated below:\n[A0 Mike ] has [PRED written ][A1 a book ] (S1)\nHere, the predicate WRITE has two arguments: \u2018Mike\u2019 as A0 or the writer, and \u2018a book\u2019 as A1 or the thing written. The labels A0 and A1 correspond to the PropBank annotations (Palmer et al., 2005).\nAs the need for SRL arises in different domains and languages, the existing manually annotated\ncorpora become insufficient to build supervised systems. This has motivated work on unsupervised SRL (Lang and Lapata, 2011b; Titov and Klementiev, 2012a; Garg and Henderson, 2012). Previous work has indicated that unsupervised systems could benefit from the word alignment information in parallel text in two or more languages (Naseem et al., 2009; Snyder et al., 2009; Titov and Klementiev, 2012b). For example, consider the German translation of sentence S1: [A0Mike] hat [A1ein Buch][PREDgeschrieben] (S2)\nIf sentences S1 and S2 have the word alignments: Mike-Mike, written-geschrieben, and book-Buch, the system might be able to predict A1 for Buch, even if there is insufficient information in the monolingual German data to learn this assignment. Thus, in languages where the resources are sparse or not good enough, or the distributions are not informative, SRL systems could be made more accurate by using parallel data with resource rich or more amenable languages.\nIn this paper, we propose a joint Bayesian model for unsupervised semantic role induction in multiple languages. The model consists of individual Bayesian models for each language (Garg and Henderson, 2012), and crosslingual latent variables to incorporate soft role agreement between aligned constituents. This latent variable approach has been demonstrated to increase the performance in a multilingual unsupervised part-of-speech tagging model based on HMMs (Naseem et al., 2009). We investigate the application of this approach to unsupervised SRL, presenting the performance improvements obtained in different settings involving labeled and unlabeled data, and analyzing the annotation effort required to obtain similar gains using labeled data.\nWe begin by briefly describing the unsupervised SRL pipeline and the monolingual semantic role induction model we use, and then describe our multilingual model.\nar X\niv :1\n60 3.\n01 51\n4v 1\n[ cs\n.C L\n] 4\nM ar\n2 01\n6"}, {"heading": "2 Unsupervised SRL Pipeline", "text": "As established in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2005), we use a standard unsupervised SRL setup, consisting of the following steps:\n1. Syntactic Parsing Off-the-shelf parsers can be used to syntactically parse a given sentence. We use a dependency parse because of its simplicity and easier comparison with the previous work in unsupervised SRL.\n2. Predicate Identification We select all the nonauxiliary verbs in a sentence as predicates.\n3. Argument Identification For a given predicate, this step classifies each constituent of the parse tree as a semantic argument or a nonargument. Heuristics based on syntactic features such as the dependency relation of a constituent to its head, path from the constituent to the predicate, etc. have been used in unsupervised SRL.\n4. Argument Classification Without access to semantic role labels, unsupervised SRL systems cast the problem as a clustering problem. Arguments of a predicate in all the sentences are divided into clusters such that each cluster corresponds to a single semantic role. The better this clustering is, the easier it becomes for a human to give it an actual semantic role label like A0, A1, etc. Our model assigns a role variable to every identified argument. This variable can take any value from 1 toN , whereN is the number of semantic roles that we want to induce.\nThe task we model, unsupervised semantic role induction, is the step 4 of this pipeline."}, {"heading": "3 Monolingual Model", "text": "We use the Bayesian model of Garg and Henderson (2012) as our base monolingual model. The semantic roles are predicate-specific. To model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary roles as follows:\nPrimary Role (PR) Let there be a total of N roles (or clusters) for each predicate. Assign K of them as PRs {P1, P2, ..., PK}. Further, create 3 additional PRs: START denoting the start of the role sequence, END denoting its end, and PRED denoting the predicate. These (K + 3) PRs are not allowed to repeat in a frame and their ordering defines the global role ordering.\nSecondary Role (SR) The rest of the (N \u2212 K) roles are called SRs {S1, S2, ..., SN\u2212K}. Unlike PRs, they are not constrained to occur only once and only their ordering w.r.t. PRs is used in the probability model.\nFor example, the complete role sequence in a frame could be: (START , P3, S1, S1, PRED, P2, S5, END). The ordering is defined as the sequence of PRs, (START , P3, PRED, P2, END). Each pair of consecutive PRs in an ordering is called an interval. Thus, (P3, PRED) is an interval that contains two SRs, S1 and S1. An interval could also be empty, for instance (START, P3) contains no SRs. When we evaluate, these roles get mapped to gold roles. For instance, the PR P2 could get mapped to a core role like A0, A1, etc. or to a modifier role like AM\u2212TMP , AM\u2212MOD, etc. Garg and Henderson (2012) reported that, in practice, PRs mostly get mapped to core roles and SRs to modifier roles, which conforms to the linguistic motivations for this distinction.\nFigure 4 illustrates two copies of the monolingual model, on either side of the crosslingual latent variables. The generative process is as follows:\n1. Predicate, Voice The predicate p and its voice vc are treated as top-level visible variables.\n2. Ordering (Generate PRs) Select an ordered set of PRs from a multinomial distribution. o \u223cMultinomial(\u03b8orderp,vc )\n3. Generate SRs For each interval in the ordering o, a sequence of SRs is generated as:\nfor each interval I \u2208 o: draw an indicator s \u223c Binomial(\u03b8STOPp,I,0 ) while s 6= STOP :\nchoose a SR r \u223cMultinomial(\u03b8SRp,I) draw an indicator s \u223c Binomial(\u03b8STOPp,I,1 )\n4. Generate Features For each PR and SR, the features for that constituent are generated independently. To keep the model simple and comparable to previous unsupervised work, we only use three features: (i) dependency relation of the argument to its head, (ii) head word of the argument, and (iii) POS tag of the head word:\nfor each generated role r: for each feature type f : choose a value vf \u223cMultinomial(\u03b8Fp,r,f )\nAll the multinomial and binomial distributions have symmetric Dirichlet and beta priors respectively. Figure 1a gives the probability equations\nP (r, f |p, vc) = P (o|p, vc)\ufe38 \ufe37\ufe37 \ufe38 o=ordering(r)\n\u220f ri\u2208r\u2229PR\nP (fi|ri, p)\ufe38 \ufe37\ufe37 \ufe38 Primary Roles\n\u220f I\u2208o\nP (r(I), f(I)|I, p)\ufe38 \ufe37\ufe37 \ufe38 Intervals\n(1)\nwhere P (r(I), f(I)|I, p) = \u220f\nri\u2208r(I) P (\u00acstop|I, p, adj)\ufe38 \ufe37\ufe37 \ufe38 generate indicator P (ri|I, p)\ufe38 \ufe37\ufe37 \ufe38 generate SR P (fi|ri, p)\ufe38 \ufe37\ufe37 \ufe38 generate features P (stop|I, p, adj)\ufe38 \ufe37\ufe37 \ufe38 end of the interval\n(2)\nand P (fi|ri, p) = T\u220f t=1 P (fi,t|ri, p) (3)\nP (f |p, vc) = \u2211 r P (r, f |p, vc) (4) (a) Probability equations for the monolingual model. Bold-faced variables denote a sequence of values. r denotes the complete sequence of roles, and f denotes the complete sequence of features. p and vc denote the predicate and its voice respectively. o denotes the ordering of PRs in the sequence r and ordering(r) is a function for computing this ordering. ri and fi denote the role and features at position i respectively, and r(I) and f(I) respectively denote the SR sequence and feature sequence in interval I . fi,t denotes the value of feature t at position i. adj = 0 for generating the first SR, and 1 for a subsequent one. Equation 1 gives the joint probability of the model and equation 4 gives the marginal probability of the observed features.\nP (rl1, f l1, rl2, f l2, z|pl1, vcl1, pl2, vcl2) = P (z) \u220f\nl\u2208{l1,l2}\nP (rl, f l|z, pl, vcl) (5)\n\u2248 P (z) \u220f\nl\u2208{l1,l2}\nP (rl, f l|pl, vcl) \u220f\ni,k:zk\u2192rli\nP (rli|zk) (6)\n(a) Probability equations for the multilingual model. The superscript l denotes the variable for language l. z denotes the common crosslingual latent variables for both languages. zk \u2192 rli denotes that the argument at position i in language l is connected to the crosslingual latent variable #k.\nFigure 3: Probability equations for the (a) monolingual and (b) multilingual model.\nfor the monolingual model. This formulation models the global role ordering and repetition preferences using PRs, and limited context for SRs using intervals. Ordering and repetition information was found to be helpful in supervised SRL as well (Punyakanok et al., 2004; Pradhan et al., 2005; Toutanova et al., 2008). More details, including the motivations behind this model, are in (Garg and Henderson, 2012)."}, {"heading": "4 Multilingual Model", "text": "The multilingual model uses word alignments between sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each language and add additional crosslingual latent variables (CLVs) to couple the monolingual models, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two corresponding role variables. Figure 4 illustrates this model. The generative process, as explained below, remains the same as the monolingual model for the most part, with the exception of aligned\nroles which are now generated by both the monolingual process as well as the CLV.\n1. Monolingual Data Given a parallel frame with the predicate pair p1, p2, generate two separate monolingual frames as in section 3.\n2. Aligned Arguments For each aligned argument, first generate a crosslingual latent variable from a Chinese Restaurant Process (CRP). Then generate the two aligned roles:\nfor aligned arguments i, j: draw a crosslingual latent variable: z \u223c CRP (\u03b1CRPp1,p2)\ndraw role for language l1: ri \u223cMultinomial(\u03b8alignp1,p2,z,l1) draw role for language l2: rj \u223cMultinomial(\u03b8alignp1,p2,z,l2)\nEvery predicate-tuple has its own inventory of CLVs specific to that tuple. Each CLV z is a multivalued variable where each value defines a distribution over role labels for each language (denoted by \u03b8alignp1,p2,z,l above). These distributions over labels are trained to be peaky, so that each value c for a CLV represents a correlation between the labels that c predicts in the two languages. For ex-\nample, a value c for the CLV z might give high probabilities to S3 and S8 in language 1, and to S1 in language 2. If c is the only value for z that gives high probability to S3 in language 1, and the monolingual model in language 1 decides to assign S3 to the role for z, then z will predict S1 in language 2, with high probability. We generate the CLVs via a Chinese Restaurant Process (Pitman, 2002), a non-parametric Bayesian model, which allows us to induce the number of CLVs for every predicate-tuple from the data. We continue to train on the non-parallel sentences using the respective monolingual models.\nThe multilingual model is deficient, since the aligned roles are being generated twice. Ideally, we would like to add the CLV as additional conditioning variables in the monolingual models. The new joint probability can be written as equation 5 (Figure 2a), which can be further decomposed following the decomposition of the monolingual model in Figure 1a. However, having this additional conditioning variable breaks the Dirichletmultinomial conjugacy, which makes it intractable to marginalize out the parameters during inference. Hence, we use an approximation where we treat each of the aligned roles as being generated twice, once by the monolingual model and once by the corresponding CLV (equation 6).\nThis is the first work to incorporate the coupling of aligned arguments directly in a Bayesian SRL model. This makes it easier to see how to extend this model in a principled way to incorporate additional sources of information. First, the model scales gracefully to more than two languages. If there are a total of n languages, and there is an aligned argument in m of them, the\nmultilingual latent variable is connected to only those m aligned arguments.\nSecond, having one joint Bayesian model allows us to use the same model in various semisupervised learning settings, just by fixing the annotated variables during training. Section 6.6 evaluates a setting where we have some labeled data in one language (called source), while no labeled data in the second language (called target). Note that this is different from a classic annotation projection setting (e.g. (Pado\u0301 and Lapata, 2009)), where the role labels are mapped from source constituents to aligned target constituents."}, {"heading": "5 Inference and Training", "text": "The inference problem consists of predicting the role labels and CLVs (the hidden variables) given the predicate, its voice, and syntactic features of all the identified arguments (the visible variables). We use a collapsed Gibbs-sampling based approach to generate samples for the hidden variables (model parameters are integrated out). The sample counts and the priors are then used to calculate the MAP estimate of the model parameters.\nFor the monolingual model, the role at a given position is sampled as:\nP (ri|r\u2212i,f ,p,vc,D\u2212)\u221dP (ri,r\u2212i,f |p,vc,D\u2212)\n= \u222b P (ri,r\u2212i,f |\u03b8,p,vc)P (\u03b8|D\u2212)d\u03b8\nwhere the subscript \u2212i refers to all the variables except at position i, D\u2212 refers to the variables in all the training instances except the current one, and \u03b8 refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy.\nFor sampling roles in the multilingual model, we also need to consider the probabilities of roles being generated by the CLVs:\nP (ri|r\u2212i,f ,p,vc,z,D\u2212)\u221dP (ri,r\u2212i,f |z,p,vc,D\u2212)\n= \u222b P (ri,r\u2212i,f |\u03b8,z,p,vc)P (\u03b8|D\u2212)d\u03b8\n= \u222b P (ri,r\u2212i,f |\u03b8,p,vc)( \u220f P (rj |\u03b8,zk)\nj,k:zk\u2192rj )P (\u03b8|D\u2212)d\u03b8\nFor sampling CLVs, we need to consider three factors: two corresponding to probabilities of generating the aligned roles, and the third one corresponding to selecting the CLV according to CRP.\nP (zk|rl1i ,rl2j ,D\u2212,k)\u221dP (rl1i |zk,D\u2212,k)P (rl2j |zk,D\u2212,k)P (zk|D\u2212,k)\nwhere the aligned roles rl1i and r l2 j are connected to zk, and D\u2212,k refers to all the variables except zk, rl1i , and r l2 j .\nWe use the trained parameters to parse the monolingual data using the monolingual model. The crosslingual parameters are ignored even if they were used during training. Thus, the information coming from the CLVs acts as a regularizer for the monolingual models."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Evaluation", "text": "Following the setting of Titov and Klementiev (2012b), we evaluate only on the arguments that were correctly identified, as the incorrectly identified arguments do not have any gold semantic labels. Evaluation is done using the metric proposed by Lang and Lapata (2011a), which has 3 components: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO. For each predicate, let N denote the total number of argument instances, Ci the instances in the induced cluster i, and Gj the instances having label j in gold annotations. PU= 1\nN\n\u2211 i maxj |Ci\u2229Gj | ,\nCO= 1 N \u2211 j maxi|Ci\u2229Gj | , and F1= 2\u00b7PU\u00b7COPU+CO . The score for each predicate is weighted by the number of its argument instances, and a weighted average is computed over all the predicates."}, {"heading": "6.2 Baseline", "text": "We use the same baseline as used by Lang and Lapata (2011a) which has been shown to be difficult to outperform. This baseline assigns a semantic\nrole to a constituent based on its syntactic function, i.e. the dependency relation to its head. If there is a total of N clusters, (N \u2212 1) most frequent syntactic functions get a cluster each, and the rest are assigned to the N th cluster."}, {"heading": "6.3 Closest Previous Work", "text": "This work is closely related to the cross-lingual unsupervised SRL work of Titov and Klementiev (2012b). Their model has separate monolingual models for each language and an extra penalty term which tries to maximize P (rl2|rl1) and P (rl1|rl2) i.e. for all the aligned arguments with role label rl1 in language 1, it tries to find a role label rl2 in language 2 such that the given proportion is maximized and vice verse. However, there is no efficient way to optimize the objective with this penalty term and the authors used an inference method similar to annotation projection. Further, the method does not scale naturally to more than two languages. Their algorithm first does monolingual inference in one language ignoring the penalty and then does the inference in the second language taking into account the penalty term. In contrast, our model adds the latent variables as a part of the model itself, and not an external penalty, which enables us to use the standard Bayesian learning methods such as sampling.\nThe monolingual model we use (Garg and Henderson, 2012) also has two main advantages over Titov and Klementiev (2012b). First, the former incorporates a global role ordering probability that is missing in the latter. Secondly, the latter defines argument-keys as a tuple of four syntactic features and all the arguments having the same argumentkeys are assigned the same role. This kind of hard clustering is avoided in the former model where two constituents having the same set of features might get assigned different roles if they appear in different contexts."}, {"heading": "6.4 Data", "text": "Following Titov and Klementiev (2012b), we run our experiments on the English (EN) and German (DE) sections of the CoNLL 2009 corpus (Hajic\u030c et al., 2009), and EN-DE section of the Europarl corpus (Koehn, 2005). We get about 40k EN and 36k DE sentences from the CoNLL 2009 training set, and about 1.5M parallel EN-DE sentences from Europarl. For appropriate comparison, we keep the same setting as in (Titov and Klementiev, 2012b) for automatic parses and ar-\ngument identification, which we briefly describe here. The EN sentences are parsed syntactically using MaltParser (Nivre et al., 2007) and DE using LTH parser (Johansson and Nugues, 2008). All the non-auxiliary verbs are selected as predicates. In CoNLL data, this gives us about 3k EN and 500 DE predicates. The total number of predicate instances are 3.4M in EN (89k CoNLL + 3.3M Europarl) and 2.62M in DE (17k CoNLL + 2.6M Europarl). The arguments for EN are identified using the heuristics proposed by Lang and Lapata (2011a). However, we get an F1 score of 85.1% for argument identification on CoNLL 2009 EN data as opposed to 80.7% reported by Titov and Klementiev (2012b). This could be due to implementation differences, which unfortunately makes our EN results incomparable. For DE, the arguments are identified using the LTH system (Johansson and Nugues, 2008), which gives an F1 score of 86.5% on the CoNLL 2009 DE data. The word alignments for the EN-DE parallel Europarl corpus are computed using GIZA++ (Och and Ney, 2003). For high-precision, only the intersecting alignments in the two directions are kept. We define two semantic arguments as aligned if their head-words are aligned. In total we get 9.3M arguments for EN (240k CoNLL + 9.1M Europarl) and 4.43M for DE (32k CoNLL + 4.4M Europarl). Out of these, 0.76M arguments are aligned."}, {"heading": "6.5 Main Results", "text": "Since the CoNLL annotations have 21 semantic roles in total, we use 21 roles in our model as well as the baseline. Following Garg and Henderson (2012), we set the number of PRs to 2 (excluding START , END and PRED), and SRs to 21-2=19. Table 1 shows the results.\nIn the first setting (Line 1), we train and test the monolingual model on the CoNLL data. We observe significant improvements in F1 score over the Baseline (Line 0) in both languages. Using the CoNLL 2009 dataset alone, Titov and Klementiev (2012b) report an F1 score of 80.9% (PU=86.8%, CO=75.7%) for German. Thus, our monolingual model outperforms their monolingual model in German. For English, they report an F1 score of 83.6% (PU=87.5%, CO=80.1%), but note that our English results are not directly comparable to theirs due to differences argument identification, as discussed in section 6.4. As their argument identification score is lower, perhaps their system\nis discarding \u201cdifficult\u201d arguments which leads to a higher clustering score.\nIn the second setting (Line 2), we use the additional monolingual Europarl (EP) data for training. We get equivalent results in English and a significant improvement in German compared to our previous setting (Line 1). The German dataset in CoNLL is quite small and benefits from the additional EP training data. In contrast, the English model is already quite good due to a relatively big dataset from CoNLL, and good accuracy syntactic parsers. Unfortunately, Titov and Klementiev (2012b) do not report results with this setting.\nThe third setting (Line 3) gives the results of our multilingual model, which adds the word alignments in the EP data. Comparing with Line 2, we get non-significant improvements in both languages. Titov and Klementiev (2012b) obtain an F1 score of 82.7% (PU=85.0%, CO=80.6%) for German, and 83.7% (PU=86.8%, CO=80.7%) for English. Thus, for German, our multilingual Bayesian model is able to capture the cross-lingual patterns at least as well as the external penalty term in (Titov and Klementiev, 2012b). We cannot compare the English results unfortunately due to differences in argument identification.\nWe also compared monolingual and bilingual training data using a setting that emulates the standard supervised setup of separate training and test data sets. We train only on the EP dataset and test on the CoNLL dataset. Lines 4 and 5 of Table 1 give the results. The multilingual model obtains small improvements in both languages, which confirms the results from the standard unsupervised setup, comparing lines 2 to 3.\nThese results indicate that little information can be learned about semantic roles from this parallel data setup. One possible explanation for this result is that the setup itself is inadequate. Given the definition of aligned arguments, only 8% of English arguments and 17% of German arguments are aligned. This plus our experiments suggest that improving the alignment model is a necessary step to making effective use of parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration to future work."}, {"heading": "6.6 Multilingual Training with Labeled Data for One Language", "text": "Another motivation for jointly modeling SRL in multiple languages is the transfer of information\nfrom a resource rich language to a resource poor language. We evaluated our model in a very general annotation transfer scenario, where we have a small labeled dataset for one language (source), and a large parallel unlabeled dataset for the source and another (target) language. We investigate whether this setting improves the parameter estimates for the target language. To this end, we clamp the role annotations of the source language in the CoNLL dataset using a predefined mapping1, and do not sample them during training. This data gives us good parameters for the source language, which are used to sample the roles of the source language in the unlabeled Europarl data. The CLVs aim to capture this improvement and thereby improve sampling and parameter estimates for the target language. Table 2 shows the results of this experiment. We obtain small improvements in the target languages. As in the unsupervised setting, the small percentage of aligned roles probably limits the impact of the cross-lingual information."}, {"heading": "6.7 Labeled Data in Monolingual Model", "text": "We explored the improvement in the monolingual model in a semi-supervised setting. To this end, we randomly selected S% of the sentences in the CoNLL dataset as \u201csupervised sentences\u201d and the rest (100\u2212S)% were kept unsupervised. Next, we clamped the role labels of the supervised sentences\n1A0 was mapped to the primary role P1, A1 to P2, and the rest were mapped to the secondary roles (S1, ..., S19) in the order of their decreasing frequency.\nusing the predefined mapping from Section 6.6. Sampling was done on the unsupervised sentences as usual. We then measured the clustering performance using the trained parameters.2\nTo access the contribution of partial supervision better, we constructed a \u201csupervised baseline\u201d as follows. For predicates seen in the supervised sentences, a MAP estimate of the parameters was calculated using the predefined mapping. For the unseen predicates, the standard baseline was used.\nFigures 5a and 5b show the performance varia-\n2To account for the randomness in selecting the supervised sentences, the experiment was repeated 10 times and average of the performance numbers was taken.\ntion with S. We make the following observations:\n\u2022 In both languages, at around S = 10, the supervised baseline starts outperforming the semisupervised model, which suggests that manually labeling about 10% of the sentences is a good enough alternative to our training procedure. Note that 10% amounts to about 3.6k sentences in German and 4k in English. We noticed that the proportion of seen predicates increases dramatically as we increase the proportion of supervised sentences. At 10% supervised sentences, the model has already seen 63% of predicates in German and 44% in English. This explains to some extent why only 10% labeled sentences are enough.\n\u2022 For German, it takes about 3.5% or 1260 supervised sentences to have the same performance increase as 1.5M unlabeled sentences (Line 1 to Line 2 in Table 1). Adding about 180 more supervised sentences also covers the benefit obtained by alignments in the multilingual model (Line 2 to Line 3 in Table 1). There is no noticeable performance difference in English.\nWe also evaluated the performance variation on a completely unseen CoNLL test set. Since the test set is very small compared to the training set, the clustering evaluation is not as reliable. Nonetheless, we broadly obtained the same pattern."}, {"heading": "7 Related Work", "text": "As discussed in section 6.3, our work is closely related to the crosslingual unsupervised SRL work of Titov and Klementiev (2012b). The idea of using superlingual latent variables to capture crosslingual information was proposed for POS tagging by Naseem et al. (2009), which we use here for SRL. In a semi-supervised setting, Pado\u0301 and Lapata (2009) used a graph based approach to transfer semantic role annotations from English to German. Fu\u0308rstenau and Lapata (2009) used a graph alignment method to measure the semantic and syntactic similarity between dependency tree arguments of known and unknown verbs.\nFor monolingual unsupervised SRL, Swier and Stevenson (2004) presented the first work on a domain-general corpus, the British National Corpus, using 54 verbs taken from VerbNet. Garg and Henderson (2012) proposed a Bayesian model for this problem that we use here. Titov and\nKlementiev (2012a) also proposed a closely related Bayesian model. Grenager and Manning (2006) proposed a generative model but their parameter space consisted of all possible linkings of syntactic constituents and semantic roles, which made unsupervised learning difficult and a separate language-specific rule based method had to be used to constrain this space. Other proposed models include an iterative split-merge algorithm (Lang and Lapata, 2011a) and a graphpartitioning based approach (Lang and Lapata, 2011b). Ma\u0300rquez et al. (2008) provide a good overview of the supervised SRL systems."}, {"heading": "8 Conclusions", "text": "We propose a Bayesian model of semantic role induction (SRI) that uses crosslingual latent variables to capture role alignments in parallel corpora. The crosslingual latent variables capture correlations between roles in different languages, and regularize the parameter estimates of the monolingual models. Because this is a joint Bayesian model of multilingual SRI, we can apply the same model to a variety of training scenarios just by changing the inference procedure appropriately. We evaluate monolingual SRI with a large unlabeled dataset, bilingual SRI with a parallel corpus, bilingual SRI with annotations available for the source language, and monolingual SRI with a small labeled dataset. Increasing the amount of monolingual unlabeled data significantly improves SRI in German but not in English. Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora.\nFuture work includes training on different language pairs, on more than two languages, and with more inclusive models of role alignment."}, {"heading": "Acknowledgments", "text": "This work was funded by the Swiss NSF grant 200021 125137 and EC FP7 grant PARLANCE."}], "references": [{"title": "Graph alignment for semi-supervised semantic role labeling", "author": ["F\u00fcrstenau", "Lapata2009] H. F\u00fcrstenau", "M. Lapata"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume", "citeRegEx": "F\u00fcrstenau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "F\u00fcrstenau et al\\.", "year": 2009}, {"title": "Unsupervised semantic role induction with global role ordering", "author": ["Garg", "Henderson2012] N. Garg", "J. Henderson"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Garg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Garg et al\\.", "year": 2012}, {"title": "Automatic labeling of semantic roles", "author": ["Gildea", "Jurafsky2002] D. Gildea", "D. Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "Gildea et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 2002}, {"title": "Unsupervised discovery of a statistical verb lexicon", "author": ["Grenager", "Manning2006] T. Grenager", "C.D. Manning"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Grenager et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grenager et al\\.", "year": 2006}, {"title": "The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages", "author": ["Haji\u010d et al.2009] J. Haji\u010d", "M. Ciaramita", "R. Johansson", "D. Kawahara", "M.A. Mart\u0131", "L. M\u00e0rquez", "A. Meyers", "J. Nivre", "S. Pad\u00f3", "J. \u0160t\u011bp\u00e1nek"], "venue": null, "citeRegEx": "Haji\u010d et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Haji\u010d et al\\.", "year": 2009}, {"title": "Dependency-based semantic role labeling of propbank", "author": ["Johansson", "Nugues2008] R. Johansson", "P. Nugues"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Johansson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2008}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Unsupervised semantic role induction via splitmerge clustering", "author": ["Lang", "Lapata2011a] J. Lang", "M. Lapata"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Lang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lang et al\\.", "year": 2011}, {"title": "Unsupervised semantic role induction with graph partitioning", "author": ["Lang", "Lapata2011b] J. Lang", "M. Lapata"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Lang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lang et al\\.", "year": 2011}, {"title": "Semantic role labeling: an introduction to the special issue", "author": ["M\u00e0rquez et al.2008] L. M\u00e0rquez", "X. Carreras", "K.C. Litkowski", "S. Stevenson"], "venue": null, "citeRegEx": "M\u00e0rquez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "M\u00e0rquez et al\\.", "year": 2008}, {"title": "Multilingual part-of-speech tagging: Two unsupervised approaches", "author": ["T. Naseem", "B. Snyder", "J. Eisenstein", "R. Barzilay"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Naseem et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2009}, {"title": "Maltparser: A languageindependent system for data-driven dependency parsing", "author": ["J. Nivre", "J. Hall", "J. Nilsson", "A. Chanev", "G. Eryigit", "S. Kubler", "S. Marinov", "E. Marsi"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] F.J. Och", "H. Ney"], "venue": "Computational linguistics,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["Pad\u00f3", "Lapata2009] S. Pad\u00f3", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2009}, {"title": "The proposition bank: An annotated corpus of semantic roles", "author": ["M. Palmer", "D. Gildea", "P. Kingsbury"], "venue": null, "citeRegEx": "Palmer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Combinatorial stochastic processes", "author": ["J. Pitman"], "venue": "Technical report, Technical Report 621, Dept. Statistics, UC Berkeley,", "citeRegEx": "Pitman.,? \\Q2002\\E", "shortCiteRegEx": "Pitman.", "year": 2002}, {"title": "Support vector learning for semantic argument classification", "author": ["Pradhan et al.2005] S. Pradhan", "K. Hacioglu", "V. Krugler", "W. Ward", "J.H. Martin", "D. Jurafsky"], "venue": "Machine Learning,", "citeRegEx": "Pradhan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2005}, {"title": "Semantic role labeling via integer linear programming inference", "author": ["D. Roth", "W. Yih", "D. Zimak"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Punyakanok et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2004}, {"title": "Unsupervised multilingual grammar induction", "author": ["Snyder et al.2009] B. Snyder", "T. Naseem", "R. Barzilay"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan-", "citeRegEx": "Snyder et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Snyder et al\\.", "year": 2009}, {"title": "Unsupervised semantic role labelling", "author": ["Swier", "Stevenson2004] R. Swier", "S. Stevenson"], "venue": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Swier et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Swier et al\\.", "year": 2004}, {"title": "A bayesian approach to unsupervised semantic role induction", "author": ["Titov", "Klementiev2012a] I. Titov", "A. Klementiev"], "venue": "In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Titov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2012}, {"title": "Crosslingual induction of semantic roles", "author": ["Titov", "Klementiev2012b] I. Titov", "A. Klementiev"], "venue": "In Proceedings of the 50th Annual Meeting of the Association", "citeRegEx": "Titov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2012}, {"title": "A global joint model for semantic role labeling", "author": ["A. Haghighi", "C.D. Manning"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "The labels A0 and A1 correspond to the PropBank annotations (Palmer et al., 2005).", "startOffset": 60, "endOffset": 81}, {"referenceID": 10, "context": "Previous work has indicated that unsupervised systems could benefit from the word alignment information in parallel text in two or more languages (Naseem et al., 2009; Snyder et al., 2009; Titov and Klementiev, 2012b).", "startOffset": 146, "endOffset": 217}, {"referenceID": 18, "context": "Previous work has indicated that unsupervised systems could benefit from the word alignment information in parallel text in two or more languages (Naseem et al., 2009; Snyder et al., 2009; Titov and Klementiev, 2012b).", "startOffset": 146, "endOffset": 217}, {"referenceID": 10, "context": "This latent variable approach has been demonstrated to increase the performance in a multilingual unsupervised part-of-speech tagging model based on HMMs (Naseem et al., 2009).", "startOffset": 154, "endOffset": 175}, {"referenceID": 16, "context": "As established in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2005), we use a standard unsupervised SRL setup, consisting of the following steps:", "startOffset": 32, "endOffset": 81}, {"referenceID": 17, "context": "Ordering and repetition information was found to be helpful in supervised SRL as well (Punyakanok et al., 2004; Pradhan et al., 2005; Toutanova et al., 2008).", "startOffset": 86, "endOffset": 157}, {"referenceID": 16, "context": "Ordering and repetition information was found to be helpful in supervised SRL as well (Punyakanok et al., 2004; Pradhan et al., 2005; Toutanova et al., 2008).", "startOffset": 86, "endOffset": 157}, {"referenceID": 22, "context": "Ordering and repetition information was found to be helpful in supervised SRL as well (Punyakanok et al., 2004; Pradhan et al., 2005; Toutanova et al., 2008).", "startOffset": 86, "endOffset": 157}, {"referenceID": 15, "context": "We generate the CLVs via a Chinese Restaurant Process (Pitman, 2002), a non-parametric Bayesian model, which allows us to induce the number of CLVs for every predicate-tuple from the data.", "startOffset": 54, "endOffset": 68}, {"referenceID": 4, "context": "Following Titov and Klementiev (2012b), we run our experiments on the English (EN) and German (DE) sections of the CoNLL 2009 corpus (Haji\u010d et al., 2009), and EN-DE section of the Europarl corpus (Koehn, 2005).", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": ", 2009), and EN-DE section of the Europarl corpus (Koehn, 2005).", "startOffset": 50, "endOffset": 63}, {"referenceID": 11, "context": "The EN sentences are parsed syntactically using MaltParser (Nivre et al., 2007) and DE using LTH parser (Johansson and Nugues, 2008).", "startOffset": 59, "endOffset": 79}, {"referenceID": 11, "context": "The EN sentences are parsed syntactically using MaltParser (Nivre et al., 2007) and DE using LTH parser (Johansson and Nugues, 2008). All the non-auxiliary verbs are selected as predicates. In CoNLL data, this gives us about 3k EN and 500 DE predicates. The total number of predicate instances are 3.4M in EN (89k CoNLL + 3.3M Europarl) and 2.62M in DE (17k CoNLL + 2.6M Europarl). The arguments for EN are identified using the heuristics proposed by Lang and Lapata (2011a). However, we get an F1 score of 85.", "startOffset": 60, "endOffset": 475}, {"referenceID": 11, "context": "The EN sentences are parsed syntactically using MaltParser (Nivre et al., 2007) and DE using LTH parser (Johansson and Nugues, 2008). All the non-auxiliary verbs are selected as predicates. In CoNLL data, this gives us about 3k EN and 500 DE predicates. The total number of predicate instances are 3.4M in EN (89k CoNLL + 3.3M Europarl) and 2.62M in DE (17k CoNLL + 2.6M Europarl). The arguments for EN are identified using the heuristics proposed by Lang and Lapata (2011a). However, we get an F1 score of 85.1% for argument identification on CoNLL 2009 EN data as opposed to 80.7% reported by Titov and Klementiev (2012b). This could be due to implementation differences, which unfortunately makes our EN results incomparable.", "startOffset": 60, "endOffset": 624}, {"referenceID": 10, "context": "The idea of using superlingual latent variables to capture crosslingual information was proposed for POS tagging by Naseem et al. (2009), which we use here for SRL.", "startOffset": 116, "endOffset": 137}, {"referenceID": 10, "context": "The idea of using superlingual latent variables to capture crosslingual information was proposed for POS tagging by Naseem et al. (2009), which we use here for SRL. In a semi-supervised setting, Pad\u00f3 and Lapata (2009) used a graph based approach to transfer semantic role annotations from English to German.", "startOffset": 116, "endOffset": 218}, {"referenceID": 10, "context": "The idea of using superlingual latent variables to capture crosslingual information was proposed for POS tagging by Naseem et al. (2009), which we use here for SRL. In a semi-supervised setting, Pad\u00f3 and Lapata (2009) used a graph based approach to transfer semantic role annotations from English to German. F\u00fcrstenau and Lapata (2009) used a graph alignment method to measure the semantic and syntactic similarity between dependency tree arguments of known and unknown verbs.", "startOffset": 116, "endOffset": 336}, {"referenceID": 9, "context": "M\u00e0rquez et al. (2008) provide a good overview of the supervised SRL systems.", "startOffset": 0, "endOffset": 22}], "year": 2016, "abstractText": "We propose a Bayesian model of unsupervised semantic role induction in multiple languages, and use it to explore the usefulness of parallel corpora for this task. Our joint Bayesian model consists of individual models for each language plus additional latent variables that capture alignments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of scenarios just by varying the inference procedure, without changing the model, thereby comparing the scenarios directly. We compare using only monolingual data, using a parallel corpus, using a parallel corpus with annotations in the other language, and using small amounts of annotation in the target language. We find that the biggest impact of adding a parallel corpus to training is actually the increase in mono-lingual data, with the alignments to another language resulting in small improvements, even with labeled data for the other language.", "creator": "LaTeX with hyperref package"}}}