{"id": "1603.01987", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "A matter of words: NLP for quality evaluation of Wikipedia medical articles", "abstract": "automatic quality evaluation of web information networks is a task with many fields of applications and of great relevance, especially in critical domains like the medical one. we move from the intuition that the quality of content of medical web documents is affected periodically by features related with the specific domain. first, the usage of a specific vocabulary ( domain informativeness ) ; then, the adoption of specific codes ( like those used in the infoboxes of old wikipedia articles ) and the type of document ( e. g., historical and technical ones ). in this paper, we propose to leverage more specific domain features to improve the results of the evaluation of wikipedia medical articles. in literature particular, we evaluate the articles adopting an \" actionable \" model, whose features are related to the content of the articles, so that the model can also directly suggest strategies for improving ensuring a given article quality. we rely on natural language processing ( nlp ) and dictionaries - based techniques in order to extract the bio - medical concepts in a text. we prove the effectiveness of our approach by classifying the medical articles of the wikipedia medicine portal, which have been continually previously manually labeled by the wiki publishing project team. the results of our comprehensive experiments confirm that, by considering domain - oriented features, it is possible to obtain sensible improvements altogether with respect to existing solutions, mainly for those articles that other approaches have less correctly classified. other than being interesting by their own, the results call for further research in the area of domain specific features suitable for web data quality assessment.", "histories": [["v1", "Mon, 7 Mar 2016 09:54:11 GMT  (816kb,D)", "http://arxiv.org/abs/1603.01987v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["vittoria cozza", "marinella petrocchi", "angelo spognardi"], "accepted": false, "id": "1603.01987"}, "pdf": {"name": "1603.01987.pdf", "metadata": {"source": "CRF", "title": "A matter of words: NLP for quality evaluation of Wikipedia medical articles", "authors": ["Vittoria Cozza", "Marinella Petrocchi", "Angelo Spognardi"], "emails": ["m.petrocchi}@iit.cnr.it", "angsp@dtu.dk"], "sections": [{"heading": "1 Introduction", "text": "As observed by a recent article of Nature News [16], \u201cWikipedia is among the most frequently visited websites in the world and one of the most popular places to tap into the world\u2019s scientific and medical information\u201d. Despite the huge amount of consultations, open issues still threaten a fully confident fruition of the popular online open encyclopedia.\nA first issue relates to the reliability of the information available: since Wikipedia can be edited by anyone, regardless of their level of expertise, this\nar X\niv :1\n60 3.\n01 98\n7v 1\n[ cs\n.I R\n] 7\nM ar\ntends to erode the average reputation of the sources, and, consequently, the trustworthiness of the contents posted by those sources. In an attempt to fix this shortcoming, Wikipedia has recently enlisted the help of scientists to actively support the editing on Wikipedia [16]. Furthermore, lack of control may lead to the publication of fake Wikipedia pages, which distort the information by inserting, e.g., promotional articles and promotional external links. Fighting vandalism is one of the main goals of the Wikimedia Foundation, the nonprofit organization that supports Wikipedia: machine learning techniques have been considered to offer a service to \u201cjudge whether an edit was made in good faith or not\u201d [23]. Nonetheless, in the past recent time, malicious organisations have acted disruptively with purposes of extortion - see, e.g., the recent news on the uncovering of a blackmail network of accounts, which threatened celebrities with the menace of inserting offending information on their Wikipedia pages3.\nSecondly, articles may suffer from readability issues: achieving a syntactical accuracy that helps the reader with a fluid reading experience is \u2014quite obviously\u2014 a property which articles should fulfill. Traditionally, the literature has widely adopted well known criteria, as the \u201cFlesch-Kincaid\u201d measure\u201d [17], to automatically assess readability in textual documents. More recently, new techniques have been proposed too, for assessing the readability of natural languages (see, e.g., [13] for the Italian use case, [24] for the Swedish one, [27] for English).\nIn this paper, we face the quest for quality assessment of a Wikipedia article, in an automatic way that comprehends not only readability and reliability criteria, but also additional parameters testifying completeness of information and coherence with the content one expects from an article dealing with specific topics, plus sufficient insights for the reader to elaborate further on some argument. The notion of data quality we deal with in the paper is coherent with the one suggested by recent contributions (see, e.g., [20]), which points out like the quality of Web information is strictly connected to the scope for which one needs such information.\nOur intuition is that groups of articles related to a specific topic and falling within specific scopes are intrinsically different from other groups on different topics within different scopes. We approach the article evaluation through machine learning techniques. Such techniques are not new to be employed for automatic evaluation of articles quality. As an example, the work in [28] exploits classification techniques based on structural and linguistic features of an article. Here, we enrich that model with novel features that are domain-specific. As a running scenario, we focus on the Wikipedia medical portal. Indeed, facing the problems of information quality and ensuring high and correct levels of informativeness is even more demanding when health aspects are involved. Recent statistics report that Internet users are increasingly searching the Web for health information, by consulting search engines, social networks, and specialised health portals, like that of Wikipedia. As pointed out by the 2014 Eurobarometer survey\n3 https://en.wikipedia.org/wiki/Wikipedia:Long-term_abuse/Orangemoody\non European citizens\u2019 digital health literacy4, around six out of ten respondents have used the Internet to search for health-related information. This means that, although the trend in digital health literacy is growing, there is also a demand for a qualified source where people can ask and find medical information which, to an extent, can provide the same level of familiarity and guarantees as those given by a doctor or a health professional.\nWe anticipate here that leveraging new domain-specific features is in line with this demand of articles quality. Moreover, as the outcomes of our experiments show, they effectively improve the classification results in the hard task of multiclass assessment, especially for those classes that other automatic approaches worst classify. Remarkably, our proposal is general enough to be easily extended to other domains, in addition to the medical one.\nSection 2 first describes the structure of the articles present in the medical portal. Then, it gives details on the real data used in the experiments, which are indeed articles extracted from the medical portal and labeled according to the manual assessment by the Wikimedia project. Section 3 briefly presents the actionable model in [28]: we adopt it as the baseline for our analysis. In Section 4, we present the domain-specific, medical model we newly adopt in this paper as an extension of the baseline. The extended model includes features specifically extracted from the medical domain. One novel feature is based on the article textual content. Section 5 presents the process which its extraction relies on, with a non trivial analysis of natural language and domain knowledge. Section 6 presents experiments and results, with a comparison of the baseline model with the new one. In Section 7, we survey related work in the area and in Section 8 we conclude the paper."}, {"heading": "2 Dataset", "text": "We consider the dataset consisting of the entire collection of articles of the Wikipedia Medicine Portal, updated at the end of 2014. Wikipedia articles are written according to the Media Wiki markup language, a HTML-like language. Among the structural elements of one page, which differs from standard HTML pages, there are i) the internal links, i.e., links to other Wikipedia pages, different from links to external resources); ii) categories, which represent the Media Wiki categories a page belongs to: they are encoded in the part of text within the Media Wiki \u201ccategories\u201d tag in the page source, and iii) informative boxes, so called \u201cinfoboxes\u201d, which summarize in a structured manner some peculiar pieces of information related the topic of the article. The category values for the articles in the medical portal span over the ones listed at https://en.wikipedia.org/wiki/Portal:Medicine. Examples of categories, which appear at the bottom of each Wikipedia page, are in Fig. 1.\nInfoboxes of the medical portal feature medical content and standard coding. As an example, Fig. 2 shows the infobox in the Alzheimer\u2019s disease page of\n4 http://ec.europa.eu/public_opinion/flash/fl_404_sum_en.pdf\nthe portal. The infobox contains explanatory figures and text denoting peculiar characteristics of the disease and the value for the standard code of such disease (ICD9, as for the international classification of the disease5).\nThanks to WikiProject Medicine6, the dataset of articles we collected from the Wikipedia Medicine Portal has been manually labeled into seven quality classes. They are ordered as Stub, Start, C, B, A, Good Article (GA), Featured Article (FA). The Featured and Good article classes are the highest ones: to have those labels, an article requires a community consensus and an official review by selected editors, while the other labels can be achieved with reviews from a larger, even controlled, set of editors. Actually, none of the articles in the dataset is labeled as A, thus, in the following, we do not consider that class, restricting the investigation to six classes.\n5 http://www.who.int/classifications/icd/en/ 6 https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Medicine/Assessment\nAt the date of our study, we were able to gather 24,362 rated documents. Remarkably, only a small percentage of them (1%) is labeled as GA and FA. Indeed, the distribution of the articles among the classes is highly skewed. There are very few (201) articles for the highest quality classes (FA and GA), while the vast majority (19,108) belongs to the lowest quality ones (Stub and Start). This holds not only for the medical portal. Indeed, it is common in all Wikipedia, where, on average, only one article in every thousand is a Featured one.\nIn Section 6, we will adopt a set of machine-learning classifiers to automatically label the articles into the quality classes. Dealing with imbalanced classes is a common situation in many real applications of classification learning: healthy patients over the population, fraudulent actions over daily genuine transactions, and so on. Without any countermeasure, common classifiers tend to correctly identify only articles belonging to the majority classes, clearly leading to severe mis-classification of the minority classes, since typical learning algorithms strive to maximize the overall prediction accuracy. To reduce the disequilibrium among the size of the classes, we have first randomly sampled the articles belonging to the most populated classes. Then, we have performed some further elaboration, as shown in the following.\nMany studies have been conducted to improve learning algorithms accuracy in presence of imbalanced data [15]. For the current work, we have considered one of the most popular approaches, namely the Synthetic Sampling with Data Generation, detailed in [9]. It consists in generating synthetic instances from the minority classes, to balance the overall dataset. The approach has been broadly applied to problems relying on NLP features, see, e.g., [10]. In our case, we resampled the input data set by applying the Synthetic Minority Oversampling TEchnique (SMOTE7), with percentage 40% for GA and 180%, for FA. In particular, the steps to oversample are the following:\n\u2013 New instances are generated using as seed real examples from the minority class; \u2013 For each real example, its k (k = 5) nearest neighbours examples are identified; \u2013 Synthetic instances are generated to be at a random point between the seed and the neighbours.\nTable 1 shows the number of articles in the dataset, divided per class, as well as the random samples we have considered for our study. The experiments presented in Section 6 are based on the articles of the right-hand column in the table."}, {"heading": "3 Baseline: the actionable model", "text": "We apply a multi-class classification approach to label the articles of the sampled dataset into the six WikiProject quality classes. In order to have a baseline, we first apply the state of the art model proposed in [28] to the dataset.\n7 Implemented and available in the Weka framework\nThe \u201cactionable model\u201d in [28] focuses on five linguistic and structural features and it weighs them as follows:\n1. Completeness = 0.4*NumBrokenWikilinks + 0.4*NumWikilinks 2. Informativeness = 0.6*InfoNoise + 0.3*NumImages 3. NumHeadings 4. ArticleLength 5. NumReferences/ArticleLength\nwhere\n\u2013 NumWikilinks is the number of links pointing to other Wikipedia pages (whereas NumBrokenWikilinks counts the links that are broken); \u2013 InfoNoise is the proportion of text content remaining in the article after removing MediaWiki markups and cleaning the text with basic NLP operations, such as stopwords removal; \u2013 ArticleLength is the base 10 log of the article length in bytes; \u2013 NumHeadings, NumReferences and NumImages are, quite intuitively, the\nnumber of headers, references and images that an article contains.\nIn order to evaluate such model over our dataset, we have extracted from our the dataset the above mentioned features. We have measured ArticleLength, NumWikilinks and NumBrokenWikilinks as suggested in [12].\nAs shown in Figure 3, the actionable model features have been extracted applying simple scripts mainly based on regular expressions (the \u201cregexp extractor\u201d block), which process the whole HTML code of an article (free text plus media wiki tags). The regexp extractor relies on Python BeautifulSoup8 for extracting HTML structures and excerpts of the textual content within the MediaWiki tags and on nltk libraries9 for basic NLP analysis. In details, nltk has been used for computing the InfoNoise feature, whose computation includes the stopwords removal, following the Porter Stopwords Corpus available through nltk [5].\n8 http://www.crummy.com/software/BeautifulSoup/ 9 http://www.nltk.org/\nThe classification results according to the baseline model are reported in Section 6."}, {"heading": "4 The medical domain model", "text": "Here, we improve the baseline model with novel and specifically crafted features that rely on the medical domain and that capture details on the specific content of an article. As shown in Figure 3, medical model features, the bio-medical entities, have been extracted from the free text only, exploiting advanced NLP techniques and using domain dictionaries.\nIn details, we newly define and extract from the dataset the following novel features:\n1. InfoBoxNormSize: this feature represents the normalised size of an infobox that contains standard medical coding.\n2. Category: the category a page belongs to.\n3. DomainInformativeness: the number of bio-medical entities, which are the domain dependent terms in the article (such as the ones denoting symptoms, diseases, treatments, etc.).\nThe idea of considering infoboxes is not novel: for example, in [28] the authors noticed that the presence of an infobox is a characteristic featured by good articles. However, in the specific case of the Medicine Portal, the presence of an infobox does not seem strictly related to the quality class the article belongs to (according to the manual labelling). Indeed, it is recurrent that articles, spanning all classes, have an infobox, containing a schematic synthesis of the article. In particular, pages with descriptions of diseases usually have an infobox with the medical standard code of the disease (i.e., IDC-9 and IDC-10), as in Figure 2.\nAs done for the baseline, also the first two features of the medical model have been extracted with ad hoc Python scripts, extracting HTML structures and excerpts of the textual content within the MediaWiki tags.\nFor their extraction of the bio-medical entities, we consider the textual part of the article only, obtained after removing the MediaWiki tags, and we apply a NLP analysis, which is presented in Section 5."}, {"heading": "4.1 Infobox-based feature", "text": "We have calculated the Infobox size as the base 10 log of the bytes of data contained within the mediawiki tags that wrap an infobox, and we have normalized it with respect to the ArticleLength, introduced in Section 3."}, {"heading": "4.2 Category-based feature", "text": "We have leveraged the categories assigned to articles in Wikipedia, in particular relating to the medicine topics available at https://en.wikipedia.org/wiki/ Portal:Medicine.\nWe have defined 4 upper level categories of our interest:\n\u2013 A anatomy: an article is about anatomy; \u2013 B biography: an article is a biography of someone or tell the history of some-\nthing; \u2013 D disorder: it is about a disorder; \u2013 F first aid: it reports information for first aid or emergency contacts; \u2013 O other: none of the above.\nWe have matched the article\u2019s text within the MediaWiki categories tag with an approximate list of keywords that are related to our category of interest, as reported in Table 2."}, {"heading": "5 Bio-medical entities", "text": "In the literature, there are several methods available for extracting bio-medical entities from a text (i.e., from medical notes and/or articles). We refer to [19] for an overview of valuable existing techniques. In this work, we have adopted a dictionary-based approach, which exploits lexical features and domain knowledge extracted from the Unified Medical Languages System (UMLS) Metathesaurus [7]. The approach has been proposed for the Italian language in a past work [1]. Since the approach combines the usage of linguistic analysis and domain resources, we were able to conveniently adapt it for the English language, being both the linguistic pipeline and UMLS available for multiple languages (including English and Italian).\nDictionary-based approaches have been proved valid for the task of entities\u2019 extraction, see, for example, another well known, similar approach to the one adopted here, i.e., Metamap10. It is worth noting like, even though dictionarybased approaches could be less precise than Named Entity Recognition [19], in our context even an approximate solution is enough, since we are not annotating medical records. Instead, we are quantifying the mole of inherent information within a text."}, {"heading": "5.1 Reference dictionary", "text": "Several ontologies or taxonomies related to the medical domain are available in English. To build a medical dictionary, we have extracted definitions of medical entities from the Unified Medical Languages System (UMLS) Metathesaurus [7]. UMLS integrates bio-medical resources, such as SNOMED-CT11 that provides the core terminology for electronic health records. In addition, UMLS also provides a semantic network where each entity in the Metathesaurus has an assigned Concept Unique Identifier (CUI) and it is semantically typed.\nFrom UMLS, we have extracted the entries belonging to the following SNOMEDCT semantic groups: Treatment, Sign or Symptom, Disease or Syndrome, Body Parts, Organs, or Organ Components, Pathologic Function, and Mental or Behavioral Dysfunction, for a total of more than one million entries, as shown in Table 3 (where the two last semantic groups have been grouped together, under Disorder). Furthermore, we have extracted common Drugs and Active Ingredients definitions from RxNorm12, accessed by RxTerm13.\nStarting from the entries in Table 3, we have also computed approximate definitions, exploiting syntactic information of the same entries. In details, we have pre-processed the entries by mean of the Tanl pipeline [2], a suite of modules for text analytics and NLP, based on machine learning. Preprocessing has consisted in first dividing the entries into single word forms. Then, for each form, we have\n10 http://metamap.nlm.nih.gov/ 11 http://www.ihtsdo.org/snomed-ct/ 12 https://www.nlm.nih.gov/research/umls/rxnorm/ 13 https://wwwcf.nlm.nih.gov/umlslicense/rxtermApp/rxTerm.cfm\nidentified the lemma (when available) and the part of speech (POS). Thus, we have created an approximate definition that consists in using only the lemma and cleaning the text, excluding punctuation, prepositions and articles. Also, approximate definitions have been normalized by lowercasing each word. As an example, the Disorder entry \u201caneurysm of the vein of galen\u201d has been stored in the dictionary, along with its approximate definition \u201caneurysm vein galen\u201d."}, {"heading": "5.2 Extraction of bio-medical entities", "text": "We have extracted the bio-medical entities present in the Wikipedia medical articles through a n-gram-based technique.\nA pre-processing phase occurs in a similar way as for the dictionary composition. Given a Wikipedia article written in English, we have pre-processed the textual part through the Tanl pipeline. Similar to what described in Section 5.1 for the reference dictionary, we have first divided the text in sentences and the sentences into single word forms. For each form, we have considered the lemma (when available) and the part of speech (POS). For instance, starting from an example sentence extracted from the Wikipedia page on the Alzheimer\u2019s disease: \u201cOther risk factors include a history of head injuries, depression, or hypertension.\u201d, we have obtained the annotation shown in Figure 4. As in the case of the dictionary, each word in the text has been lowercasing.\nAfter pre-processing the text of each article, we have attempted to match each n-gram (with n between 1 and 10) in the corpus with the entries in the extended dictionary. We both attempt an exact match and an approximate match, the latter removing prepositions, punctuations and articles from the n-grams. Approximate matching leads to several advantages. Indeed, exploiting the text pre-processing allows to identify dictionary definitions present in the text, even when the number differs. As an example, the dictionary definition \u201cinjury\u201d will match with \u201cinjuries\u201d, mentioned in the text, because in the approximation one can consider the lemmas. Further, considering the POS allows to identify mentions when interleaved by prepositions, articles, and conjunctions that change the form but do not alter the meaning. As an example, the approximate definition \u201caneurysm vein galen\u201d will match also with the following n-gram: \u201cthe aneurysm and vein of galen\u201d, if present in the text."}, {"heading": "6 Experiments and results", "text": "In this section, we describe the experiments and report the results for the classification of Wikipedia medical articles into the six classes of the Wikipedia Medicine Portal. We compare the results obtained adopting four different classifiers: the actionable model in [28] and three classifiers that leverage the ad-hoc features from the medical domain discussed in the previous sections. All the experiments were realized within the Weka framework [14] and validated through 10 fold cross-validation.\nFor each experiment, we relied on the dataset presented in Section 2, and specifically, on that obtained after sampling the majority classes and oversampling the minority ones (right-hand column in Table 1). The dataset serves both as training and test set for the classifiers.\nMoreover, to take into account the imbalanced data, we have applied several classification algorithms and, for the sake of conciseness, hereafter we report only the best results we have achieved. In particular, we have experimented with bagging, adaptive boosting and random forest and we report the results for the latter only."}, {"heading": "6.1 Classifiers\u2019 features", "text": "In Table 4, we report a summary of the features for each of the considered models: the baseline model in [28] and two new models that employ the medical domain features. In the Medical Domain model, we add to the baseline features the Domain Informativeness, as described in Section 4 and 5. In addition, the Full Medical Domain model also considers the features InfoBoxNormSize and Category.\nFor each of the features, the table also reports the Information Gain, evaluated on the whole dataset (24,362 articles). Information Gain is a well-known metric to evaluate the dependency of one class from a single feature, see, e.g., [11].\nWe can observe how the Domain Informativeness feature has a considerably higher infogain value when compared with Informativeness. We anticipate here that this will lead to a more accurate classification results for the highest classes, as reported in the next section. Leading to a greater accuracy is also true for the other two new features that, despite showing lower values of infogain, are able to further improve the classification results, mainly for the articles belonging to the lowest quality classes (Stub and Start)."}, {"heading": "6.2 Classification results", "text": "Table 5 shows the results of our multi-class classification. For each of the classes, we have computed the ROC Area and F-Measure metrics [21]. The latter, in particular, is usually considered a significant metric in terms of classification, since it combines in one single value all the four indicators that are generally implied for evaluating the classifier performance (i.e., number of True Positives, False Positives, True Negatives and False Negatives). In our scenario, the meaning of the indicators, for each class, are as follows:\n\u2013 True Positives are the articles classified as belonging to a certain class, that indeed belong to that class (according to the quality ratings given by the WikiProject Medicine); \u2013 True Negatives are the articles classified as not belonging to a certain class, that indeed do not belong to that class; \u2013 False Positives are the articles classified as belonging to a certain class, that do not belong to that class; \u2013 False Negatives are the articles classified as not belonging to a certain class, that instead belong to that class.\nAt a first glance, we observe that, across all the models, the articles with the lowest classification values, for both ROC and F-Measure, are those labeled C and GA. Adding the Domain Informativeness feature produces a classification, which is slightly worse for C and FA articles, but better for the other four classes. This is particularly evident for the F-Measure of the articles of the GA class. A\nnoticeable major improvement is obtained with the introduction of the features InfoBoxNormSize and Category in the Medical Domain model. The ROC Area increases for the articles of all the classes within the Full Medical Domain, while the F-Measure is always better than the Baseline and almost always better than the Medical Domain.\nThe size of an article, expressed either as the word count, analyzed in [6], or as the article length, as done here, appears a very strong feature, able to discriminate the articles belonging to the highest and lowest quality classes. This is testified also by the results achieved exploiting the baseline model of [28], which poorly succeeds in discriminating the articles of the intermediate quality classes, while achieving good results for Stub and FA. Here, the newly introduced features have a predominant effect on the articles of the highest classes. This could be justified by the fact that those articles contain, on average, more text and, then, NLP-based features can exploit more words belonging to a specific domain.\nThen, we observe that the ROC Area and the F-Measure are not tightly coupled (namely: high values for the first metric can correspond to low values for the second one, see for example C and GA): this is due to the nature of the ROC Area, that is affected by the different sizes of the considered classes. As an example, we can observe that the baseline model has the same ROC Area value for the articles of both class B and class GA, while the F-Measure of articles of class B is 0.282 higher than that of class GA.\nFinally, the results confirm that the adoption of domain-based features and, in general, of features that leverage NLP, help to distinguish between articles in the lowest classes and articles in the highest classes, as highlighted in bold in Table 5. We notice also that exploiting the full medical domain leads us to the achievement of the best results.\nEven if preliminary, we believe that the results are promising and call both for features\u2019 further refinement and novel features, able to discriminate among the intermediate classes too."}, {"heading": "7 Related work", "text": "Automatic quality evaluation of Wikipedia articles has been addressed in previous works with both unsupervised and supervised learning approaches. The common idea of most of the existing work is to identify a feature set, having as a starting point the Wikipedia project guidelines, to be exploited with the objective in mind to distinguish Featured Articles. In [26], Stvilia et al. identify a relevant set of features, including lingual, structural, historical and reputational aspects of each article. They show the effectiveness of their metrics by applying both clustering and classification. As a result, more than 90% of FA are correctly identified.\nBlumenstock [6] inspects the relevance of the word-count feature at each quality stage, showing that it can play a very important role in the quality assessment of Wikipedia articles. Only using this feature, the author achieves a F-measure of 0.902 in the task of classifying featured articles and 0.983 in the task of classifying non featured articles. The best results of the investigation are achieved with a classifier based on a neural network implemented with a multi-layer perceptron.\nIn [30], the authors try to analyze the factors affecting the quality of Wikipedia articles, with respect to their quality class. The authors evaluate a set of 28 features, over a random sample of 500 Wikipedia articles, by weighing each metric in different stages using neural networks. Findings are that linguistic features weigh more in the lowest quality classes, and structural features, along with historical ones, become more important as the article quality improves. Their results indicate that the information quality is mainly affected by completeness, and to be \u201cwell-written\u201d is a basic requirement in the initial stage. Instead, reputation of authors or editors is not so important in Wikipedia because of its horizontal structure. In [29], the authors consider the quality of the data in the infoboxes of Wikipedia, finding a correlation between the quality of information in the infobox and the article itself.\nIn [28], the authors deal with the problem of discriminating between two large classes, namely NeedWork, GoodEnough (including in GoodEnough both GA and FA), in order to identify which articles need further revisions for being featured. They also introduce new composite features, those that we have referred to as an \u201cactionable model\u201d in Section 3. They obtain good classification results, with a F-measure of 0.876 in their best configuration. They also try classification for all the seven quality classes, as done in this work, using a random forest classifier with 100 trees, with a reduced set of features. The poor results (an average F-measure of 0.425) highlights the hardness of this fine-grained classification. In this paper, we address this last task in a novel way, by introducing\ndomain features, specially dealing with the medical domain. The results of the investigation are promising.\nRecent studies specifically address the quality of medical information (in Wikipedia as well as in other resources): in [3] and [4], the authors debate if Wikipedia is a reliable learning resource for medical students, evaluating articles on respiratory topics and cardiovascular diseases. The evaluation is carried out by exploiting DISCERN14, a tool evaluating readability of articles. In [18] the authors provide novel solutions for measure the quality of medical information in Wikipedia, by adopting an unsupervised approach based on the Analytic Hierachy Process, a multi-criteria decision making technique [22]. The work in [8] aims to provide the web surfers a numerical indication of Quality of Medical Web Sites. In particular in [8] the author proposes an index to make IQ judgment of the content and of its reliability, to give the so called \u201csurface markers\u201d and \u201ctrust indicator\u201d. A similar measurement is considered in [25]. where the authors present an empirical analysis that suggests the need to define genre-specific templates for quality evaluation and to develop models for an automatic genre-based classification of health information Web pages. In addition, the study shows that consumers may lack the motivation or literacy skills to evaluate the information quality of health Web pages. Clearly, this further highlights the cruciality to develop accessible automatic information quality evaluation tools and ontologies. Our work moves towards the goal, by specifically considering domain-relevant features and featuring an automatic classification task spanning over more than two classes."}, {"heading": "8 Conclusions", "text": "In this work, we aimed to provide a fine grained classification mechanism for all the quality classes of the articles of the Wikipedia Medical Portal. The idea was to propose an automatic instrument for helping the reviewers to understand which articles are the less work-demanding papers to pass to next quality stage. We focused on an actionable model, namely whose features are related to the content of the articles, so that they can also directly suggest strategies for improving a given article. An important and novel aspect of our classifier, with respect to previous works, is the leveraging of features extracted from the specific, medical domain, with the help of Natural Language Processing techniques. As the results of our experiments confirm, considering specific domain-based features, like Domain Informativeness and Category, can eventually help and improve the automatic classification results. Since the results are encouraging, as future work we will evaluate other features based on the specific medical domain. Moreover, we are planning to extend our idea, to include and compare also other non medical articles (thus, extending the work to include other domains), in order to further validate our approach. Acknowledgments. The research leading to these results has been partially funded by the Registro.it project My Information Bubble MIB.\n14 http://www.discern.org.uk/"}], "references": [{"title": "Adapting linguistic tools for the analysis of italian medical records", "author": ["G. Attardi", "V. Cozza", "D. Sartiano"], "venue": "The First Italian Conference on Computational Linguistics CLiC-it 2014, page 17,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The TANL pipeline", "author": ["G. Attardi", "S.D. Rossi", "M. Simi"], "venue": "Web Services and Processing Pipelines in HLT: Tool Evaluation, LR Production and Validation (LREC:WSSP),", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Is Wikipedia a reliable learning resource for medical students? Evaluating respiratory topics", "author": ["S.A. Azer"], "venue": "Advances in Physiology Education, 39(1):5\u201314,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Accuracy of cardiuvascular articles on Wikipedia: Are they reliable learning resources for medical students", "author": ["S.A. Azer", "N. AlSwaidan", "L. AlSwairikh", "J. AlShammari"], "venue": "BMJ Open,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Natural Language Processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Size matters: Word count as a measure of quality on Wikipedia", "author": ["J.E. Blumenstock"], "venue": "In Proceedings of the 17th International Conference on World Wide Web, WWW \u201908, pages 1095\u20131096, New York, NY, USA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploring semantic groups through visual approaches", "author": ["O. Bodenreider", "A.T. McCray"], "venue": "Journal of biomedical informatics, 36(6):414\u2013432,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "An information reliability index as a simple consumer-oriented indication of quality of medical web sites", "author": ["F. Cabitza"], "venue": "In Quality Issues in the Management of Web Information, volume 50 of Intelligent Systems Reference Library, pages 159\u2013177. Springer Berlin Heidelberg,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "SMOTE: Synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research, 16:321\u2013357,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Editorial: Special issue on learning from imbalanced data sets", "author": ["N.V. Chawla", "N. Japkowicz", "A. Kotcz"], "venue": "SIGKDD Explor. Newsl., 6(1):1\u20136, June", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "On measuring the quality of Wikipedia articles", "author": ["G. De la Calzada", "A. Dekhtyar"], "venue": "In Proceedings of the 4th Workshop on Information Credibility,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Assessing document and sentence readability in less resourced languages and across textual genres", "author": ["F. Dell\u2019Orletta", "S. Montemagni", "G. Venturi"], "venue": "International Journal of Applied Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD explorations newsletter, 11(1):10\u201318,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning from Imbalanced Data", "author": ["H. He", "E. Garcia"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 21(9):1263\u20131284, Sept", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Wikipedians reach out to academics", "author": ["R. Hodson"], "venue": "Nature News, Sept.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Derivation of new readability formulas for navy enlisted personnel", "author": ["J.P. Kincaid", "R.P. Fishburne", "R.L. Rogers", "B.S. Chissom"], "venue": "National Technical Information Service: Research Report, pages 8\u201375,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1975}, {"title": "Improved automatic maturity assessment of Wikipedia medical articles", "author": ["E. Marzini", "A. Spognardi", "I. Matteucci", "P. Mori", "M. Petrocchi", "R. Conti"], "venue": "In On the Move to Meaningful Internet Systems: OTM 2014 Conferences, volume 8841 of Lecture Notes in Computer Science, pages 612\u2013622. Springer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "editors", "author": ["P. Nakov", "T. Zesch"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Association for Computational Linguistics and Dublin City University, Dublin, Ireland, August", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "An introduction to quality issues in the management of web information", "author": ["G. Pasi", "G. Bordogna", "L. Jain"], "venue": "In G. Pasi, G. Bordogna, and L. C. Jain, editors, Quality Issues in the Management of Web Information, volume 50 of Intelligent Systems Reference Library, pages 1\u20133. Springer Berlin Heidelberg,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation", "author": ["D.M.W. Powers"], "venue": "International Journal of Machine Learning Technologies, 2(1):37\u201363,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "How to make a decision: The Analytic Hierarchy Process", "author": ["T.L. Saaty"], "venue": "European Journal of Operational Research, 48(1),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1990}, {"title": "Artificial intelligence aims to make Wikipedia friendlier and better", "author": ["T. Simonite"], "venue": "In MIT Technology Review, November", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Probability as readability: A new machine learning approach to readability assessment for written Swedish", "author": ["J. Sj\u00f6holm"], "venue": "Master\u2019s thesis, Link\u00f6ping University,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "A model for online consumer health information quality", "author": ["B. Stvilia", "L. Mon", "Y.J. Yi"], "venue": "Journal of the American Society for Information Science and Technology, 60(9):1781\u20131791,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Assessing information quality of a community-based encyclopedia", "author": ["B. Stvilia", "M.B. Twidale", "L.C. Smith", "L. Gasser"], "venue": "In Proceedings of the 2005 International Conference on Information Quality, pages 442\u2013454, Cambridge, MA,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Readability assessment for text simplification: From analysing documents to identifying sentential simplifications", "author": ["S. Vajjala", "D. Meurers"], "venue": "International Journal of Applied Linguistics, 165(2):194\u2013222,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Tell me more: An actionable quality model for Wikipedia", "author": ["M. Warncke-Wang", "D. Cosley", "J. Riedl"], "venue": "In Proceedings of the 9th International Symposium on Open Collaboration, WikiSym \u201913, pages 8:1\u20138:10, New York, NY, USA,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Modelling the quality of attributes in Wikipedia infoboxes", "author": ["K. Wecel", "W. Lewoniewski"], "venue": "In Business Information Systems Workshops, volume 228 of Business Information Processing, pages 308\u2013320. Springer International Publishing,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Mining the factors affecting the quality of wikipedia articles", "author": ["K. Wu", "Q. Zhu", "Y. Zhao", "H. Zheng"], "venue": "Information Science and Management Engineering (ISME), 2010 International Conference of, 1:343\u2013346, Aug", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "As observed by a recent article of Nature News [16], \u201cWikipedia is among the most frequently visited websites in the world and one of the most popular places to tap into the world\u2019s scientific and medical information\u201d.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "In an attempt to fix this shortcoming, Wikipedia has recently enlisted the help of scientists to actively support the editing on Wikipedia [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "Fighting vandalism is one of the main goals of the Wikimedia Foundation, the nonprofit organization that supports Wikipedia: machine learning techniques have been considered to offer a service to \u201cjudge whether an edit was made in good faith or not\u201d [23].", "startOffset": 250, "endOffset": 254}, {"referenceID": 16, "context": "Traditionally, the literature has widely adopted well known criteria, as the \u201cFlesch-Kincaid\u201d measure\u201d [17], to automatically assess readability in textual documents.", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": ", [13] for the Italian use case, [24] for the Swedish one, [27] for English).", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": ", [13] for the Italian use case, [24] for the Swedish one, [27] for English).", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": ", [13] for the Italian use case, [24] for the Swedish one, [27] for English).", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": ", [20]), which points out like the quality of Web information is strictly connected to the scope for which one needs such information.", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "As an example, the work in [28] exploits classification techniques based on structural and linguistic features of an article.", "startOffset": 27, "endOffset": 31}, {"referenceID": 27, "context": "Section 3 briefly presents the actionable model in [28]: we adopt it as the baseline for our analysis.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Many studies have been conducted to improve learning algorithms accuracy in presence of imbalanced data [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "For the current work, we have considered one of the most popular approaches, namely the Synthetic Sampling with Data Generation, detailed in [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": ", [10].", "startOffset": 2, "endOffset": 6}, {"referenceID": 27, "context": "In order to have a baseline, we first apply the state of the art model proposed in [28] to the dataset.", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "The \u201cactionable model\u201d in [28] focuses on five linguistic and structural features and it weighs them as follows:", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "We have measured ArticleLength, NumWikilinks and NumBrokenWikilinks as suggested in [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "In details, nltk has been used for computing the InfoNoise feature, whose computation includes the stopwords removal, following the Porter Stopwords Corpus available through nltk [5].", "startOffset": 179, "endOffset": 182}, {"referenceID": 27, "context": "The idea of considering infoboxes is not novel: for example, in [28] the authors noticed that the presence of an infobox is a characteristic featured by good articles.", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "We refer to [19] for an overview of valuable existing techniques.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "In this work, we have adopted a dictionary-based approach, which exploits lexical features and domain knowledge extracted from the Unified Medical Languages System (UMLS) Metathesaurus [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "The approach has been proposed for the Italian language in a past work [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "It is worth noting like, even though dictionarybased approaches could be less precise than Named Entity Recognition [19], in our context even an approximate solution is enough, since we are not annotating medical records.", "startOffset": 116, "endOffset": 120}, {"referenceID": 6, "context": "To build a medical dictionary, we have extracted definitions of medical entities from the Unified Medical Languages System (UMLS) Metathesaurus [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "In details, we have pre-processed the entries by mean of the Tanl pipeline [2], a suite of modules for text analytics and NLP, based on machine learning.", "startOffset": 75, "endOffset": 78}, {"referenceID": 27, "context": "We compare the results obtained adopting four different classifiers: the actionable model in [28] and three classifiers that leverage the ad-hoc features from the medical domain discussed in the previous sections.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "All the experiments were realized within the Weka framework [14] and validated through 10 fold cross-validation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "In Table 4, we report a summary of the features for each of the considered models: the baseline model in [28] and two new models that employ the medical domain features.", "startOffset": 105, "endOffset": 109}, {"referenceID": 10, "context": ", [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": "For each of the classes, we have computed the ROC Area and F-Measure metrics [21].", "startOffset": 77, "endOffset": 81}, {"referenceID": 5, "context": "The size of an article, expressed either as the word count, analyzed in [6], or as the article length, as done here, appears a very strong feature, able to discriminate the articles belonging to the highest and lowest quality classes.", "startOffset": 72, "endOffset": 75}, {"referenceID": 27, "context": "This is testified also by the results achieved exploiting the baseline model of [28], which poorly succeeds in discriminating the articles of the intermediate quality classes, while achieving good results for Stub and FA.", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "In [26], Stvilia et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Blumenstock [6] inspects the relevance of the word-count feature at each quality stage, showing that it can play a very important role in the quality assessment of Wikipedia articles.", "startOffset": 12, "endOffset": 15}, {"referenceID": 29, "context": "In [30], the authors try to analyze the factors affecting the quality of Wikipedia articles, with respect to their quality class.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [29], the authors consider the quality of the data in the infoboxes of Wikipedia, finding a correlation between the quality of information in the infobox and the article itself.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [28], the authors deal with the problem of discriminating between two large classes, namely NeedWork, GoodEnough (including in GoodEnough both GA and FA), in order to identify which articles need further revisions for being featured.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "Recent studies specifically address the quality of medical information (in Wikipedia as well as in other resources): in [3] and [4], the authors debate if Wikipedia is a reliable learning resource for medical students, evaluating articles on respiratory topics and cardiovascular diseases.", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Recent studies specifically address the quality of medical information (in Wikipedia as well as in other resources): in [3] and [4], the authors debate if Wikipedia is a reliable learning resource for medical students, evaluating articles on respiratory topics and cardiovascular diseases.", "startOffset": 128, "endOffset": 131}, {"referenceID": 17, "context": "In [18] the authors provide novel solutions for measure the quality of medical information in Wikipedia, by adopting an unsupervised approach based on the Analytic Hierachy Process, a multi-criteria decision making technique [22].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [18] the authors provide novel solutions for measure the quality of medical information in Wikipedia, by adopting an unsupervised approach based on the Analytic Hierachy Process, a multi-criteria decision making technique [22].", "startOffset": 225, "endOffset": 229}, {"referenceID": 7, "context": "The work in [8] aims to provide the web surfers a numerical indication of Quality of Medical Web Sites.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "In particular in [8] the author proposes an index to make IQ judgment of the content and of its reliability, to give the so called \u201csurface markers\u201d and \u201ctrust indicator\u201d.", "startOffset": 17, "endOffset": 20}, {"referenceID": 24, "context": "A similar measurement is considered in [25].", "startOffset": 39, "endOffset": 43}], "year": 2016, "abstractText": "Automatic quality evaluation of Web information is a task with many fields of applications and of great relevance, especially in critical domains like the medical one. We move from the intuition that the quality of content of medical Web documents is affected by features related with the specific domain. First, the usage of a specific vocabulary (Domain Informativeness); then, the adoption of specific codes (like those used in the infoboxes of Wikipedia articles) and the type of document (e.g., historical and technical ones). In this paper, we propose to leverage specific domain features to improve the results of the evaluation of Wikipedia medical articles. In particular, we evaluate the articles adopting an \u201cactionable\u201d model, whose features are related to the content of the articles, so that the model can also directly suggest strategies for improving a given article quality. We rely on Natural Language Processing (NLP) and dictionaries-based techniques in order to extract the biomedical concepts in a text. We prove the effectiveness of our approach by classifying the medical articles of the Wikipedia Medicine Portal, which have been previously manually labeled by the Wiki Project team. The results of our experiments confirm that, by considering domain-oriented features, it is possible to obtain sensible improvements with respect to existing solutions, mainly for those articles that other approaches have less correctly classified. Other than being interesting by their own, the results call for further research in the area of domain specific features suitable for Web data quality assessment.", "creator": "LaTeX with hyperref package"}}}