{"id": "1603.02194", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Gaussian Process Regression for Out-of-Sample Extension", "abstract": "manifold learning methods are useful for high threshold dimensional data analysis. many of the existing methods produce a low dimensional representation that attempts purely to describe the intrinsic geometric structure of the original data. typically, this process is computationally expensive and the produced embedding is limited to the training data. in many real life scenarios, the ability to produce embedding of unseen samples is essential. in this accompanying paper we propose a bayesian non - parametric approach for out - of - sample extension. the method is based on gaussian process regression and independent of the manifold learning algorithm. additionally, so the method naturally provides a measure for the degree of abnormality for a newly arrived data point that did not participate in the training process. afterwards we derive the mathematical connection between the proposed method and the nystrom extension routine and show that the delayed latter is a possible special case instance of the former. we present extensive experimental results that demonstrate the performance variability of the proposed method and compare it to other existing out - of - sample extension methods.", "histories": [["v1", "Mon, 7 Mar 2016 18:35:51 GMT  (1695kb)", "http://arxiv.org/abs/1603.02194v1", null], ["v2", "Sun, 5 Jun 2016 16:56:21 GMT  (1695kb)", "http://arxiv.org/abs/1603.02194v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["oren barkan", "jonathan weill", "amir averbuch"], "accepted": false, "id": "1603.02194"}, "pdf": {"name": "1603.02194.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Dimensionality reduction methods are widely used in the machine learning community for high dimensional data analysis. Manifold learning is a subclass of nonlinear dimensionality reduction algorithms. These algorithms attempt to discover the low dimensional manifold that the data points have been sampled from [1]. Many manifold learning algorithms produce an embedding of high dimensional data points in a low dimensional space. In this space, the Euclidean distance indicates the affinity between the original data points with respect to the manifold geometric structure. Typically, the embedding is produced only for the training data points with no extension for out-of-sample points. Moreover, the process of computing the embedding usually involves expensive computational operations such as Singular Values Decomposition\n(SVD). As a result, the application of manifold learning algorithms to massive datasets or data which is accumulated over time becomes impractical. Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].\nIn this paper, we propose a general framework for OOSE which is based on Gaussian Process Regression (GPR) [8]. The method is independent of the manifold learning algorithm and provides a measure of abnormality for a given test instance with respect to the training instances. The outline of the method is as follows: Given a training data and a manifold learning algorithm, we first apply the algorithm to the training data and compute the corresponding embeddings. Then, we learn the hyperparameters for a GPR model using the training data and the embeddings. Finally, given an unseen test instance and the trained GPR model, we produce a predictive distribution and set the embedding value to the distribution mode. Furthermore, the variance of the predictive distribution quantifies the degree of abnormality in the test instance. We analyze the mathematical connection between the proposed method and the Nystrom extension [9] and show that the latter is a special case of the former.\nWe evaluate the proposed method on several wellknown manifold learning algorithms and various synthetic and real world datasets. We demonstrate its performance and show it manages to achieve competitive results when compared with other OOSE methods.\nThe rest of the paper is organized as follows: Section 2 overviews related work. In Section 3 we overview Gaussian Processes and GPR. Section 4 describes the proposed method and discusses its connection to the Nystrom extension [9]. In Section 5 we present experimental results."}, {"heading": "2. Related work", "text": "OOSE for manifold learning is an active research field. Bengio et al. [2] proposes extensions for several well\nknown manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13]. The extensions are based on the Nystrom extension [9], which has been widely used for manifold learning algorithms. In [3] the authors propose to use the Nystrom extension of eigenfunctions of the kernel, however in order to maintain numerical stability, they used only the significant eigenvalues. As a result, the method might suffer from inconsistencies with the insample data. Bermanis et al. [4] suggested to alleviate the aforementioned problem by introducing a method for extending functions using a coarse-to-fine hierarchy of the multiscale decomposition of a Gaussian kernel. The method has been shown to overcome some limitations of Nystrom extension. Recently, Aizenbud et al. [5] suggested an extension for a new data point which is based on local Principal Component Analysis (PCA).\nAdditional attempts to establish a solution for the OOSE problem have been taken. Fernandez et al. [6] proposes an extension of Laplacian Pyramids model that incorporates a modified Leave One Out Cross Validation (LOOCV), but avoids the large computational cost of the standard one. In [7], the authors proposed to extend the embedding to unseen samples by finding a rotation and scale transformations of the sample\u2019s nearest neighbors. Then, the embedding is computed by applying these transformations to the unseen samples. Yang et al. [20] introduced a manifold learning technique that enables OOSE by using regularization.\nIn the context of Bayesian statistics, Lawrence et al. [21] showed how Gaussian Process Latent Variable models can be generalized through back-constraints (GPLVMBC) to preserve local geometries. However, GPLVMBC is not designed to extend a given mapping, but to produce a new one, which is different from the original one. Moreover, the GPLVMBC requires specific derivation per objective. This is in contrast to our proposed method which learns the original mapping and hence it is independent of the manifold learning algorithm.\nWilson et al. [14] introduces a new kernel that can be used with Gaussian Processes in order to discover patterns to enable extrapolation. The new kernel was found to outperform other existing kernels. Contrary to [14], in this paper we stick to the traditional squared exponential covariance function that sometimes referred as Radial Basis Function (RBF) kernel. In our experiments, we did not observe any significant improvement when other kernels are used."}, {"heading": "3. Gaussian Process Regression", "text": "Given a training set\n{( , ) | , , 1,..., }Ni i i iD y y i m= \u2208 \u2208 =x x \u211d \u211d ,\nwhich consists of pairs of input vector and noisy predictions ( , )\ni i yx , Bayesian regression deals with\ncomputing a predictive distribution of *y for a new test\ninstance *x . Typically, the noise is assumed to be additive, independent and Gaussian such that the relation between the input to the output is given by\n( ) i i i y f \u03b5= +x , 2~ (0, )i N\u03b5 \u03c3 , (1)\nwhere f is a function that comes to model the noise\nfree relation between i x and i y and ( , )N a b stands for the normal distribution with a mean a and a variance b .\nA Gaussian Process (GP) is a stochastic process such that any finite subcollection of random variables has a multivariate Gaussian distribution. Gaussian Process Regression (GPR) is a non-parametric Bayesian regression model that assumes prior distribution of the function values such that 1:( | ) ( , )mp N K= fff x 0 where\n1[ ,..., ] T mf f=f ( ( )i if f= x ) is a vector whose entries are the function values. Note that these function values are treated as random variables. m mK \u00d7\u2208ff \u211d is a covariance matrix whose entries are computed by the covariance function [ ] cov( , ) ( , )i j i jijK f f k= =ff x x .\nThen, for a given test vector *x the predictive\ndistribution of *y can be computed by marginalizing out the function values f\n1 * * *( | ) ( , | ) ( ) ( | ) ( , )p f p f d p p p f d \u2212= =\u222b \u222by f y f y y f f f , (2)\nwhere the last transition in Eq.(2) follows Bayes rule and the fact that y is conditionally independent of *f given f . Since both factors in the last integral in Eq.(2) have the following Gaussian distributions\n*\n* * *\n*( , ) , f\nf f f\nK K p f N\nK K     =       ff f f f 0 , 2( | ) ( , )p N \u03c3=y f f I ,\na closed form expression [8] exists for the predictive distribution\n( )2* * *( | ) ,p f N \u00b5 \u03c3=y , ** fK\u00b5 = f Ay ,\n* * * *\n2 * f f f fK K K\u03c3 = \u2212 f fA , ( ) 12 K \u03c3 \u2212 = + ff A I . (3)\nTherefore, training a GPR model amounts to the computation of A and Ay . The computational\ncomplexity of the training procedure is dominated by a matrix inversion which is 3( )O n . Then, the prediction\nfor a new test instance *x is given by the mode of\n*( | )p f y , which is the mean *\u00b5 in case of Gaussian distribution. The variance 2*\u03c3 serves as a measure of the prediction uncertainty."}, {"heading": "4. GPR based OOSE", "text": "Given a manifold learning algorithm M and a training set 1{ } m N i i== \u2282X x \u211d , we apply M to X and compute the corresponding low dimensional embedding\n1{ } ( ) m d i i d n== \u2282Y y \u211d \u226a . Then, for each dimension\n1 j d\u2264 \u2264 , independently, we form a new training set\n{( , ) | , , 1,..., }Nj i ij i ijD y y i m= \u2208 \u2208 =x x \u211d \u211d and train a\nseparate GPR model. Then, given an unseen test example *x , we predict by Eq. (3) its embedding and the measure of uncertainty in the predictions by\n[ ]* * *1 *,..., T d \u00b5 \u00b5= =y \u03bc and 2 2 2* *1 *, ..., T d \u03c3 \u03c3 =  \u03c3 ,\nrespectively. As the variance increases, our confidence in the prediction decreases and *x might be considered as anomaly with respect to training set X . In this work we use the squared exponential covariance function (kernel) 22\n2 cov( , ) ( , ) exp( )i j i j i jf f k \u03c4\n\u2212= = \u2212 \u2212x x x x ,\nwhere \u03c4 is a hyperparameter that determines the width of the kernel. In our experiments, we evaluated several other kernels and they did not produce any significant improvement. An additional hyperparameter is the noise variance 2\u03c3 in Eq. (1). The hyperparameters can be optimized with respect to\njD (note that the\noptimization is done for each GPR model j ,\nseparately). One option is to compute type II Maximum Likelihood (ML) estimates for the hyperparameters with respect to\njD . In the literature, this method is\nnamed as marginal likelihood [8]. Another approach is to apply cross validation. Fortunately, a close form expressions for LOOCV and its gradients exist [8] and the hyperparameters can be optimized with the Conjugate Gradient method. In this work, we use LOOCV for hyperparameter optimization. The main reason we chose this approach is that the marginal likelihood method is more prone to overfitting [8]. The algorithm is summarized in Fig.1."}, {"heading": "4.1 The connection between GPR and the Nystrom extension", "text": "Many manifolds learning methods are cast in the same framework [2], where the computation of the embedding of the training data points is obtained by eigendecomposition of a (normalized) kernel matrix. Therefore, for a given training set 1{ } m N i i== \u2282X x \u211d the kernel matrix is computed by ( , )ij i jK k= x x (this might\nbe followed by a subsequent normalization). Then, the eigendecomposition of K is carried out to form the\nGPR based OOSE\nTraining phase\nInput:\nM - manifold learning algorithm\n1{ } m i i ==X x - training set\nd - target dimensionality\nK - kernel function\nOutput:\n1{ } d i i G G == - set of trained GPR models for each target\ndimension. 1. Compute the embedding 1{ } m i i==Y y using M and X . 2. For 1j \u2190 to d 2.1. {( , ) | 1,..., }\nj i ij D y i m\u2190 =x\n2.2. Update ( )jK and 2\u03c3 (using LOOCV [8]).\n2.3. 1[ ,..., ] T j mj y y\u2190v\n2.4. ( ) 1( ) 2j j j K \u03c3 \u2212 \u2190 + ff A I (Eq. (3))\n2.5. j j\u2190w A v 2.6. ( ){ , , }j j j j G K\u2190 A w\nTest phase\nInput:\n* \u2209x X - test instance.\n1{ } d i iG G == - set of trained GPR models for each target\ndimension.\nOutput:\n*y - the prediction for *x\n*\u03c3 - measure of uncertainty for *y \u2019s entries. 1. For 1j \u2190 to d\nfollowing relation\nTK = Y\u039bY , (4)\nwhere \u039b and Y are a diagonal matrix with the n eigenvalues on its diagonal and their corresponding column eigenvectors, respectively. Note that K is a real symmetric matrix and hence 1T \u2212=Y Y . Finally, the embedding of\ni x is obtained by the i -th row of Y . We\ncan rewrite Eq.(4) as\n1 1\n1\n( , ) n\nij j i j j i z zj\nz\nK k\u03bb \u03bb\u2212 \u2212\n=\n= = \u2211XY y x x Y , (5)\nwhere jy is the j -th column eigenvector in Y and iK X\nis the i -th row in K . In other words, the embedding for each data point in the training set is determined by a linear combination of the embeddings of all the other training data points multiplied by the inverse of the corresponding eigenvalue. The linear coefficients are the scaled kernel values. For the sake of simplicity, we limit the discussion to a single dimensional embedding, the generalization for multidimensional embedding is straightforward.\nThe Nystrom extension proposes to compute the embedding *y for a new test instance *x by\n1 1* * * 1\n( , ) n\nj j j j z zj\nz\nK k\u03bb \u03bb\u2212 \u2212\n=\n= = \u2211Xy y x x Y (6)\nwhich amounts to the application of the kernel for each data point in the training set X with respect to *x ,\nfollowed by a dot product with j y .\nAssuming a noise free GPR model with an identical kernel, the following relation holds: 1\ni i y K K \u2212= X y and\nthe prediction in Eq.(3) reduces to\n1* * *j jK K\u00b5 \u2212= = X y y . (7)\nBy using Eq. (4) we have\n( ) 11 1T TK \u2212 \u2212 \u2212== Y\u039bY Y\u039b Y . (8)\nBy combining Eqs. (7) and (8) we get\n1 1 1* * * * T j j j j j K K K\u03bb\u2212 \u2212 \u2212= = = X X X y Y\u039b Y y Y\u039b e y . (9)\nwhere the second transition is due to the fact that Y \u2019s columns are orthonormal and\nj e is the standard basis\nvector j . Notice that the predictions in Eqs. (6) and\n(9) are identical. Hence, the Nystrom extension is equivalent to a noise free GPR model with no hyperparamters optimization."}, {"heading": "5. Experimental results", "text": "In this section we present experimental results that demonstrate the performance of our proposed method and compare it to other existing OOSE methods."}, {"heading": "5.1 The experimental workflow", "text": "The workflow of the experiments is as follows: Given a manifold learning algorithm M , OOSE method O and a dataset X , we apply M to X and derive corresponding embeddings Y . Then, we randomly divide ( , )X Y to training and test sets\n( , ) train train R X Y= and ( , ) test test Q X Y= , respectively. The\ndivision is done according to a specific portion \u03c1 ( \u03c1\nis the fraction of data points assigned to R the rest are assigned to Q ). Then, by using O , R and testX , we\nproduce the embeddings testY \u0276 . Finally, we measure the accuracy of the extension by the Root Mean Squared Error (RMSE) measure\n1/2 2\n1\n1 ( , )\nn\ni i\ni\nRMSE Y Y y y n\n\u2212\n=   = \u2212    \u2211\u0276 \u0276 .\nWe repeat the above procedure ten times (for different random divisions, R and Q ) to produce a series of\nRMSE scores and determine the final RMSE as the series average. Note that our evaluation is similar to the other previous OOSE works [4]-[7], except for the fact we add the parameter \u03c1 that challenges the evaluated\nmethods with variable training set sizes. We will use the notations defined here throughout this section."}, {"heading": "5.2 OOSE methods", "text": "We compare our proposed method to the Nystrom extension method and several OOSE methods that were recently developed and shown to overcome some of the limitations of the Nystrom extension. The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5]. All of the methods provide an OOSE scheme which is independent of the manifold learning algorithm."}, {"heading": "5.3 Manifold learning algorithms", "text": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13]. We set the methods hyperparameters according to the evaluated dataset: For all datasets we applied ISOMAP, LE, LLE and MDS with the same nearest neighbor value 8k = . For the DM method, we adjust the neighborhood value according to the median of the\nsquared Euclidean distances between the data points. For 3-dimensional datasets, the target dimensionality for the manifold learning methods was set to 2. For high dimensional datasets the target dimension was chosen separately, according to the spectral decay for each manifold learning algorithm."}, {"heading": "5.4 Datasets", "text": "We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18]. Each dataset poses a different challenge for the manifold learning algorithm. All datasets were fed into the manifold learning algorithms and OOSE schemes as are, without any further preprocessing. From the MNIST and USPS datasets we randomly drew a collection of 4000 and 1000 images, respectively of the same digit. For each synthetic dataset we generated a set of 1000 data points."}, {"heading": "5.5 Experiment 1", "text": "Our first experiment is designed to visualize the error obtained by a GPR based OOSE. To this end, we created a Swiss roll [11] with 1000 data points (Fig.(4) left bottom corner) and produced a corresponding embedding using each of the manifold learning\nalgorithms, separately. Then, we created R and Q sets with 0.1\u03c1 = as described in Section 5.1 and trained a GPR model for each of the manifold learning algorithms using R . Figure 2 presents the true embedding\ntest Y and the predictions testY \u0276 that were\nproduced for test X using GPR. As we can see, even for a small value of \u03c1 , the predictions managed to preserve a small amount of noise and follow the same structure of the true embeddings\ntest Y ."}, {"heading": "5.6 Experiment 2", "text": "This experiment is designed to evaluate the OOSE methods, each time on a specific pair of a manifold learning algorithm and a dataset. To this end, given a pair ( , )M X we generate R and Q and compute average RMSE values for each O (see Section 5.1 for notations and further explanation). We repeat the experiment for increasing values of \u03c1 starting from 0.05 to 0.8. Then for each O we plot a graph of the log RMSE as a function of \u03c1 . We used the parameters that were specified in Section 5.3. A Gaussian noise was added to all of the synthetic datasets.\nThe results are presented in Fig. 3. (we did not add labels to the axis, since y values are measured relatively to the competitor methods, rather than their\nabsolute values and axis x is \u03c1 value which is clear from the context). Figure 3 is a graph table in which the ( , )i j entry corresponds to a specific pair ( , )M X . The pairs are clear from the row and columns labels. As we can see GPR produces the lowest RMSE values for most of the configurations followed by POOS as the second best method. The ALP seems to perform the worse, we conclude that it is due overfitting (in the ALP algorithm, a parameter is learnt from the training data and then used in test phase [6]). The reader might notice that some of the RMSE graphs are increasing in certain late intervals (mainly for real datasets), this might be explained by outliers or instability of the manifold learning algorithm: sometimes few points in the embedding are disconnected from the rest. Therefore, as \u03c1 increases, the probability of these points to be included in R increases as well."}, {"heading": "5.7 Experiment 3", "text": "As explained in Section 4 the GPR model produces a distribution of the prediction, with high variance 2*\u03c3 implies that *x is anomaly. In this experiment we evaluate the GPR model as anomaly detector on\nsynthetic datasets. We trained a GPR models for the Swiss Roll and Toroidial Helix datasets that were produced by the Diffusion Maps method (note we repeat the same experiment for the other manifold learning methods and got the same result). Then, we preserved the 2D view of the first two principal dimensions (by fixing the third dimension) and bounded it by a rectangle to form a test set, for each dataset, respectively. Figures 4 top right and bottom right show heatmaps that were produced using\n2 2 * *\n1\n( ) d\ni\ni H \u03c3 = = \u2212\u2211\u03c3 for the Toroidial Helix and Swiss\nRoll, respectively. As we can see, for both of the datasets, the heatmaps represent the geometric structure well and anomalous points are assigned with low H values."}, {"heading": "5.8 Experiment 4", "text": "We experimented with using our proposed model for an anomaly detection task on the DARPA Intrusion Detection Evaluation Data Set [19]. Each instance in this dataset has 14 features based on network traffic. Every instance is associated with standard network activity or a network attack and labeled accordingly.\nswiss hole 0.2 0.4 0.6 corner planes 0.2 0.4 0.6 punctured sphere 0.2 0.4 0.6 twin peaks 0.2 0.4 0.6\n3d clusters\n0.2 0.4 0.6\ntoroidal helix\n0.2 0.4 0.6\nface data 0.2 0.4 0.6\nmnist\n0.2 0.4 0.6\nusps\n0.2 0.4 0.6\n0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6\n0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6\n0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6\n0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6 0.2 0.4 0.6\ntraining). Every plot was generated for a different combination of dataset and manifold learning algorithm. See Section 5.6 for details.\nFirst, each dimension in the training set was mapped to [0, 1] using a constant scale factor. The scale factors were saved to apply the same scaling to the test instances. Next, the training data was reduced to 2 dimensions using diffusion maps with a Gaussian kernel with a neighborhood parameter \u03b5 = 0.7 (which was computed using the median of the average k=5 nearest neighbor distances). To decide whether a test instance is an anomaly, we use our proposed OOSE method to obtain a distribution over the 2-dimensional diffusion space of a lower dimensional vector corresponding to the test instance. The final decision is made by comparing the variance of this Gaussian distribution to a threshold. The threshold was learned by holding out 20% of the training set and optimizing the prediction accuracy on the hold out set. Using this approach, we obtained an accuracy of over 99%.\nIt is important to clarify that the ability to detect anomalies is a byproduct of the proposed OOSE method. We treat this capability as a side contribution of the paper, hence a survey of other anomaly detection methods and comparisons between them to the presented method is out of scope of this paper. This is on par with previous OOSE works such as [4]-[6]. Furthermore, though we show that our proposed OOSE method is able to achieve state-of-the-art results, we do not claim to achieve the state-of-the-art performance for anomaly detection tasks, but merely to show how to apply anomaly detection using our proposed OOSE method and validate these capabilities on both synthetic and real world datasets."}, {"heading": "6. Conclusion", "text": "In this paper, we proposed a non-parametric Bayesian approach for OOSE. The method is based on GPR. We analyzed the relation between the Nystrom extension and GPR and showed that the former is a special case of the latter. We validated our proposed method in a series of experiments that demonstrated its performance and compared it to other OOSE methods. Furthermore, we showed how to apply anomaly detection using a trained GPR model and presented experimental results on both synthetic and real world datasets.\nIn future, we plan to investigate advanced models such as Student t processes [8] for robust Bayesian regression. We also plan to explore the performance of Relevance Vector Machines (RVMs) [24] for sparse Bayesian regression and understand whether accurate predictions can be achieved using a minimal subset of the entire training set. This might increase the computational complexity of the training phase, but substantially reduce the test runtimes. Last but not least, we plan to conduct a comparison between parametric models (e.g. neural networks) and nonparametric models for OOSE."}], "references": [{"title": "Dimensionality reduction: A comparative review.", "author": ["van der Maaten", "Laurens JP", "Eric O. Postma", "H. Jaap van den Herik"], "venue": "Journal of Machine Learning Research", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Plots of the toroidal helix (top) and swiss roll (bottom) views along with their corresponding heatmaps visualizing the negative variance of predictions", "author": ["Bengio", "Yoshua", "Jean-Fran\u00e7ois Paiement", "Pascal Vincent"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Diffusion maps and geometric harmonics.", "author": ["Lafon", "St\u00e9phane S"], "venue": "PhD diss., Yale University,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Multiscale data sampling and function extension.", "author": ["Bermanis", "Amit", "Amir Averbuch", "Ronald R. Coifman"], "venue": "Applied and Computational Harmonic Analysis 34,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "PCA-Based Out-of-Sample Extension for Dimensionality Reduction.", "author": ["Aizenbud", "Yariv", "Amit Bermanis", "Amir Averbuch"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Auto-adaptative Laplacian Pyramids for High-dimensional Data Analysis.", "author": ["Fern\u00e1ndez", "\u00c1ngela", "Neta Rabin", "Dalia Fishelov", "Jos\u00e9 R. Dorronsoro"], "venue": "arXiv preprint arXiv:1311.6594", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "A generalised solution to the out-of-sample extension problem in manifold learning.", "author": ["Strange", "Harry", "Reyer Zwiggelaar"], "venue": "In Twenty-Fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Gaussian processes for machine learning.", "author": ["Rasmussen", "Carl Edward"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines.", "author": ["Williams", "Christopher", "Matthias Seeger"], "venue": "In Proceedings of the 14th Annual Conference on Neural Information Processing Systems, no", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering.", "author": ["Belkin", "Mikhail", "Partha Niyogi"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "A global geometric framework for nonlinear dimensionality reduction.", "author": ["Tenenbaum", "Joshua B", "Vin De Silva", "John C. Langford"], "venue": "Science 290,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding.", "author": ["Roweis", "Sam T", "Lawrence K. Saul"], "venue": "Science 290,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Gaussian process kernels for pattern discovery and extrapolation.", "author": ["Wilson", "Andrew Gordon", "Ryan Prescott Adams"], "venue": "arXiv preprint arXiv:1302.4245", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Hessian eigenmaps: Locally linear embedding techniques for highdimensional data.", "author": ["Donoho", "David L", "Carrie Grimes"], "venue": "Proceedings of the National Academy of Sciences 100, no", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Gradient-based learning applied to document recognition.", "author": ["LeCun", "Yann", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE 86, no", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Statistical learning theory", "author": ["Vapnik", "Vladimir Naumovich", "Vlamimir Vapnik"], "venue": "New York: Wiley,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Evaluating intrusion detection systems: The 1998 DARPA off-line intrusion detection evaluation.", "author": ["Lippmann", "Richard P", "David J. Fried", "Isaac Graf", "Joshua W. Haines", "Kristopher R. Kendall", "David McClung", "Dan Weber"], "venue": "In DARPA Information Survivability Conference and Exposition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Local and Global Regressive Mapping for Manifold Learning with Out-of-Sample Extrapolation.", "author": ["Yang", "Yi", "Feiping Nie", "Shiming Xiang", "Yueting Zhuang", "Wenhua Wang"], "venue": "In Twenty-Fourth AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Qui\u00f1onero-Candela. \"Local distance preservation in the GP-LVM through back constraints.", "author": ["Lawrence", "Neil D", "Joaquin"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Scalable sparse subspace clustering.", "author": ["Peng", "Xi", "Lei Zhang", "Zhang Yi"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Connecting the out-of-sample and pre-image problems in kernel methods.", "author": ["Arias", "Pablo", "Gregory Randall", "Guillermo Sapiro"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Sparse Bayesian learning and the relevance vector machine.\" The journal of machine learning research", "author": ["Tipping", "Michael E"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "These algorithms attempt to discover the low dimensional manifold that the data points have been sampled from [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 2, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 3, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 4, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 5, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 6, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 19, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 20, "context": "Therefore, the out-of-sample extension (OOSE) problem is a major concern for manifold learning algorithms and over the years many methods has been proposed to alleviate this problem [2, 3, 4 ,5 ,6 ,7, 22, 23].", "startOffset": 182, "endOffset": 208}, {"referenceID": 7, "context": "In this paper, we propose a general framework for OOSE which is based on Gaussian Process Regression (GPR) [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 8, "context": "We analyze the mathematical connection between the proposed method and the Nystrom extension [9] and show that the latter is a special case of the former.", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Section 4 describes the proposed method and discusses its connection to the Nystrom extension [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "[2] proposes extensions for several well Gaussian Process Regression for Out-of-Sample Extension", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "2 known manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "2 known manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "2 known manifold learning algorithms: Laplacian Eigenmaps (LE) [10], ISOMAP [11], Locally Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "The extensions are based on the Nystrom extension [9], which has been widely used for manifold learning algorithms.", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "In [3] the authors propose to use the Nystrom extension of eigenfunctions of the kernel, however in order to maintain numerical stability, they used only the significant eigenvalues.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "[4] suggested to alleviate the aforementioned problem by introducing a method for extending functions using a coarse-to-fine hierarchy of the multiscale decomposition of a Gaussian kernel.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] suggested an extension for a new data point which is based on local Principal Component Analysis (PCA).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] proposes an extension of Laplacian Pyramids model that incorporates a modified Leave One Out Cross Validation (LOOCV), but avoids the large computational cost of the standard one.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In [7], the authors proposed to extend the embedding to unseen samples by finding a rotation and scale transformations of the sample\u2019s nearest neighbors.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "[20] introduced a manifold learning technique that enables OOSE by using regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] showed how Gaussian Process Latent Variable models can be generalized through back-constraints (GPLVMBC) to preserve local geometries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] introduces a new kernel that can be used with Gaussian Processes in order to discover patterns to enable extrapolation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Contrary to [14], in this paper we stick to the traditional squared exponential covariance function that sometimes referred as Radial Basis Function (RBF) kernel.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "a closed form expression [8] exists for the predictive distribution", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "In the literature, this method is named as marginal likelihood [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Fortunately, a close form expressions for LOOCV and its gradients exist [8] and the hyperparameters can be optimized with the Conjugate Gradient method.", "startOffset": 72, "endOffset": 75}, {"referenceID": 7, "context": "The main reason we chose this approach is that the marginal likelihood method is more prone to overfitting [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "Many manifolds learning methods are cast in the same framework [2], where the computation of the embedding of the training data points is obtained by eigendecomposition of a (normalized) kernel matrix.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Update ( ) j K and 2 \u03c3 (using LOOCV [8]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "Note that our evaluation is similar to the other previous OOSE works [4]-[7], except for the fact we add the parameter \u03c1 that challenges the evaluated methods with variable training set sizes.", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "Note that our evaluation is similar to the other previous OOSE works [4]-[7], except for the fact we add the parameter \u03c1 that challenges the evaluated methods with variable training set sizes.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "The methods are: Multiscale extension [4], Adaptive Laplacian Pyramids (ALP) [6] and PCA based OOSE (POOS) [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 11, "context": "We evaluate the performance of the OOSE methods for several well-known manifold learning algorithms: Diffusion Maps (DM) [3], ISOMAP [11], Laplacian Eigenmaps (LE) [10], Local Linear Embeddings (LLE) [12] and Multidimensional Scaling (MDS) [13].", "startOffset": 200, "endOffset": 204}, {"referenceID": 10, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 2, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 188, "endOffset": 191}, {"referenceID": 10, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "4 Datasets We use various synthetic and real world datasets: Swiss roll [11], Swiss hole [15], Corner planes [16], Punctured sphere [12], Twin peaks [12], 3D Clusters [16], Toroidal Helix [3], Faces [11], MNIST [17] and USPS [18].", "startOffset": 225, "endOffset": 229}, {"referenceID": 10, "context": "To this end, we created a Swiss roll [11] with 1000 data points (Fig.", "startOffset": 37, "endOffset": 41}, {"referenceID": 5, "context": "The ALP seems to perform the worse, we conclude that it is due overfitting (in the ALP algorithm, a parameter is learnt from the training data and then used in test phase [6]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 16, "context": "8 Experiment 4 We experimented with using our proposed model for an anomaly detection task on the DARPA Intrusion Detection Evaluation Data Set [19].", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": "7 First, each dimension in the training set was mapped to [0, 1] using a constant scale factor.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "This is on par with previous OOSE works such as [4]-[6].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "This is on par with previous OOSE works such as [4]-[6].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "In future, we plan to investigate advanced models such as Student t processes [8] for robust Bayesian regression.", "startOffset": 78, "endOffset": 81}, {"referenceID": 21, "context": "We also plan to explore the performance of Relevance Vector Machines (RVMs) [24] for sparse Bayesian regression and understand whether accurate predictions can be achieved using a minimal subset of the entire training set.", "startOffset": 76, "endOffset": 80}], "year": 2016, "abstractText": "Manifold learning methods are useful for high dimensional data analysis. Many of the existing methods produce a low dimensional representation that attempts to describe the intrinsic geometric structure of the original data. Typically, this process is computationally expensive and the produced embedding is limited to the training data. In many real life scenarios, the ability to produce embedding of unseen samples is essential. In this paper we propose a Bayesian non-parametric approach for out-of-sample extension. The method is based on Gaussian Process Regression and independent of the manifold learning algorithm. Additionally, the method naturally provides a measure for the degree of abnormality for a newly arrived data point that did not participate in the training process. We derive the mathematical connection between the proposed method and the Nystrom extension and show that the latter is a special case of the former. We present extensive experimental results that demonstrate the performance of the proposed method and compare it to other existing out-of-sample extension methods.", "creator": "PScript5.dll Version 5.2.2"}}}