{"id": "1603.02618", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2016", "title": "The red one!: On learning to refer to things based on their discriminative properties", "abstract": "as precisely a first step towards agents learning to communicate about their visual similarity environment, we propose a system that, given visual representations of a referent ( cat ) and with a context ( sofa ), inherently identifies their discriminative attributes, i. e., properties that distinguish near them ( has _ tail ). and moreover, despite the strong lack of direct supervision at the behavioral attribute level, the model learns to assign plausible attributes to objects ( sofa - compliant has _ cushion ). finally, first we present a preliminary experiment confirming the intrinsic referential success of the predicted discriminative attributes.", "histories": [["v1", "Tue, 8 Mar 2016 18:39:46 GMT  (1896kb,D)", "http://arxiv.org/abs/1603.02618v1", null], ["v2", "Mon, 23 May 2016 17:04:15 GMT  (2006kb,D)", "http://arxiv.org/abs/1603.02618v2", "Accepted as an ACL-short sumbmission"]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["angeliki lazaridou", "nghia the pham", "marco baroni"], "accepted": true, "id": "1603.02618"}, "pdf": {"name": "1603.02618.pdf", "metadata": {"source": "CRF", "title": "\u201cThe red one!\u201d: On learning to refer to things based on their discriminative properties", "authors": ["Angeliki Lazaridou", "Marco Baroni"], "emails": ["angeliki.lazaridou@unitn.it", "thenghia.pham@unitn.it", "marco.baroni@unitn.it"], "sections": [{"heading": "1 Introduction", "text": "Recent advances in end-to-end-trained neural network architectures have renewed interest in developing systems capable of genuine language understanding (Hermann et al., 2015; Hill et al., 2015). In this perspective, it is important to think of an appropriate general framework for teaching language to machines. Since we use language primarily for communication, a reasonable approach is to develop systems within a genuine communicative setup (Steels, 2003; Mikolov et al., 2015). In this perspective, our long-term goal is to develop communities of computational agents that learn how to use language efficiently in order to achieve communicative success (Vogel et al., 2013; Foerster et al., 2016).\nWithin this general picture, one fundamental aspect of meaning where communication is indeed crucial is the act of reference (Searle, 1969; Abbott, 2010), the ability to successfully talk to others about things in the external world. A specific instantiation of reference studied in this paper is that of referring expression generation (Dale and\nReiter, 1995; Mitchell et al., 2010; Kazemzadeh et al., 2014). A necessary condition for achieving successful reference is that referring expressions (REs) accurately distinguish the intended referent from any other object in the context (Dale and Haddock, 1991). Along these lines, we present here a model that, given an intended referent and a context object, predicts the attributes that discriminate between the two. Some examples of the behaviour of the model are presented in Figure 1.\nImportantly, and distinguishing our work from earlier literature on generating REs (Krahmer and Van Deemter, 2012): (i) the input objects are represented by natural images, so that the agent must learn to extract relevant attributes from realistic data; and (ii) no direct supervision on the attributes is provided: the training signal concerns their discriminativeness for a given object pair. We use this \u201cpragmatic\u201d signal since it could later be replaced by a measure of success in actual communication between two agents (e.g., whether a second agent was able to pick the correct referent given a RE)."}, {"heading": "2 Discriminative Attribute Dataset", "text": "We generated the Discriminative Attribute Dataset, consisting of pairs of (intended) referents and contexts, with respect to which the referents should be identified by their distinctive attributes.\nOur starting point is the Visual Attributes for Concepts Dataset (ViSA) (Silberer et al., 2013), which contains per-concept (as opposed to perimage) attributes for 500 concrete concepts (CAT,\nar X\niv :1\n60 3.\n02 61\n8v 1\n[ cs\n.C L\n] 8\nM ar\n2 01\n6\nSOFA, MILK) spanning across different categories (MAMMALS, FURNITURE), annotated with 636 general attributes. We disregarded ambiguous concepts (e.g., bat), thus reducing our working set of concepts C to 462 and the number of attributes V to 573, as we eliminated any attribute that did not occur with concepts in C. We extracted on average 100 images annotated with each of these concepts from ImageNet (Deng et al., 2009). Finally, each image i of concept c was associated to a visual instance vector, by feeding the image to the VGG-19 ConvNet (Simonyan and Zisserman, 2014), as implemented in the MatConvNet toolkit (Vedaldi and Lenc, 2015), and extracting the fc7 layer as its 4096-dimensional visual representation vci .\nWe split the target concepts into training, validation and test sets, containing 80%, 10% and 10% of the concepts in each category, respectively. This ensures that (i) the intersection between train and test concepts is empty, thus allowing us to test the generalization of the model across different objects, but (ii) there are instances of all categories in each set, so that it is possible to generalize across training and testing objects. Finally we build all possible combinations of concepts in the training split to form pairs of referents and contexts \u3008cr, cc\u3009 and obtain their (binary) attribute vectors pcr and pcc from ViSA, resulting in 70K training pairs. From the latter, we derive, for each pair, a concept-level \u201cdiscriminativeness\u201d vector by computing the symmetric difference dcr,cc = (pcr \u2212 pcc) \u222a (pcc \u2212 pcr). The latter will contain 1s for discriminative attributes, 0s elsewhere. On average, each pair is associated with 20 discriminative attributes. The final training data are triples of the form \u3008cr, cc,dcr,cc\u3009 (the model never observes the attribute vectors of specific concepts), to be associated with visual instances of the two concepts. Table 1 presents some examples.\nNote that ViSA contain concept-level attributes, but images contain specific instances of concepts\nfor which a general attribute might not hold. This introduces a small amount of noise. For example, is_green would in general be a discriminative attribute for apples and cats, but it is not such for the second sample in Table 1. Datasets with perimage attribute annotations solve this issue. However, the currently available ones only cover specific classes of concepts (e.g., only clothes, or animals, or scenes, etc.). Thus, taken separately, they are not general enough for our purposes, and we cannot merge them, since their concepts live in different attribute spaces."}, {"heading": "3 Discriminative Attribute Network", "text": "The proposed Discriminative Attribute Network (DAN) learns to predict the discriminative attributes of referent object cr and context cc without direct supervision at the attribute level, but relying only on discriminativeness information (e.g., for the objects in Figure 2, the gold vector would contain 1 for has_tail, but 0 for both is_green and has_legs). Still, the model is implicitly encouraged to embed objects into an attribute space, to generalize across the discriminativeness vectors of different training pairs, so it also effectively learns to annotate objects with visual attributes.\nFigure 2 presents a schematic view of DAN, focusing on a single attribute. The model is presented with two concepts (\u3008CAT, SOFA\u3009), and randomly samples a visual instance of each. The instance visual vectors v (i.e., ConvNet fc7 layers) are mapped into attribute vectors of dimensionality |V | (cardinality of all available attributes), using shared weights Ma \u2208 R4096\u00d7|V | . Intuitively, this layer should learn whether an attribute is active for a specific object, as this is crucial for determining whether the attribute is discriminative for an object pair. In Section 5, we present experimental evidence corroborating this hypothesis.\nIn order to capture the pairwise interactions between attribute vectors, the model proceeds by concatenating the two units associated with the same visual attribute v across the two objects (e.g., the units encoding information about has_tail) and pass them as input to the discriminative layer. The discriminative layer processes the two units by applying a linear transformation with weights Md \u2208 R2\u00d7h, followed by a sigmoid activation function, finally deriving a single value by another linear transformation with weights MD \u2208 Rh\u00d71. The output d\u0302v encodes the predicted degree of\ndiscriminativeness of attribute v for the specific reference-context pair. The same process, with the same shared weights Md and MD, is applied to all attributes v \u2208 V , to derive the estimated discriminativeness vector d\u0302.\nTo learn the parameters \u03b8 of the model (i.e. Ma, Md and MD), given training data \u3008cr, cc,dcr,cc\u3009, we minimizing MSE between the gold vector dcr,cc and model-estimated d\u0302cr,cc . We trained the model with rmsprop and with a batch size of 32. All hyperparameters (e.g., size h of hidden layer) were tuned to maximize performance on the validation set."}, {"heading": "4 Predicting Discriminativeness", "text": "We evaluate the ability of the model to predict attributes that discriminate the intended referent from the context. Precisely, we ask the model to return all discriminative attributes for a pair, independently of whether they are positive for the referent or for the context (given images of a cat and a building, both +is_furry and \u2212made_ of_bricks are discriminative of the cat).\nTest stimuli We derive our test stimuli from the VisA test split (see Section 2), containing 2000 pairs. Unlike in training, where the model was presented with specific visual instances (i.e., single images), for evaluation we use visual concepts (CAT, BED), which we derive by averaging the vectors of all images associated to an object (i.e., deriving CAT from all images of cats), due to lack of gold information on per-image attributes.\nResults We compare DAN against a random baseline based on per-attribute discriminativeness probabilities estimated from the training data and an ablation model without attribute layer. For the\ntwo trainable models, we use a 0.5 threshold for an attribute discriminative, without tuning. The results in Table 2 confirm that, with appropriate supervision, DAN performs discriminativeness prediction reasonably well. Interestingly, allowing the model to embed visual representations into an intermediate attribute space has a strong positive effect on performance, Intuitively, since DAN is evaluated on novel concepts, the mediating attribute layer provides more high-level semantic information helping generalization."}, {"heading": "5 Predicting Attributes", "text": "Attribute learning is typically studied in supervised setups (Ferrari and Zisserman, 2007; Farhadi et al., 2009; Russakovsky and Fei-Fei, 2010). Our model learns to embed visual objects in an attribute space through indirect supervision about attribute discriminativeness for specific <referent, context> pairs. Concept attributes are never explicitly observed during training. The question arises of whether discriminativeness pushes the model to learn plausible concept attributes. Note that the idea that the semantics of attributes arises from their distinctive function within a communication system is fully in line with the classic structuralist view of linguistic meaning (Geeraerts, 2009).\nTo test our hypothesis, we feed DAN the same test stimuli (visual concept vectors) as in the previous experiment, but now look at activations in the attribute layer. Since these activations are real numbers whereas gold values (i.e., the visual attributes in the ViSA dataset) are binary, we use the validation set to learn the threshold to deem an attribute active. The resulting binary attribute vector p\u0302c for concept c is compared against the corresponding gold attribute vector pc.\nResults We compare DAN to the random baseline and to a neural network with one hidden layer, trained as a separate classifier for each target attribute (thus relying on supervision at the attribute level). We report moreover the best F1 score of Silberer et al. (2013), who learn a SVM for each\nvisual attribute based on HOG visual features. Unlike in our setup, in theirs, images for the same concept are used both for training and to derive visual attributes (our setup is \u201czero-shot\u201d at the concept level, i.e., we predict attributes of concepts not seen in training). We thus take their results to be an upper bound on ours.\nDAN reaches, and indeed surpasses, the performance of the model with direct supervision at the attribute level, confirming the power of discriminativeness as a driving force in building semantic representations. The comparison with Silberer\u2019s model suggests that there is room for improvement, although the noise inherent in concept-level annotation imposes a relatively low bound on realistic performance."}, {"heading": "6 Evaluating Referential Success", "text": "We finally ran a pilot study testing whether DAN\u2019s ability to predict discriminative attributes at the concept level translates into producing features that would be useful in constructing successful referential expressions for specific object instances.\nTest stimuli Our starting point is the ReferIt dataset (Kazemzadeh et al., 2014), consisting of REs denoting objects (delimited by bounding boxes) in natural images. We filter out any \u3008RE, bounding box\u3009 pair whose RE does not overlap with our attribute set V and annotate the remaining ones with the overlapping attribute, deriving data of the form \u3008RE, bounding box, attribute\u3009. For each intended referent of this type, we sample as context another \u3008RE, bounding box\u3009 pair such that (i) the context RE does not contain the referent attribute, so that the latter is a likely discriminative feature; (ii) referent and context come from different images, so that their bounding boxes do not accidentally overlap; (iii) there is maximum word overlap between referent and contexts REs, creating a realistic referential ambiguity setup (e.g., two cars, two objects in similar environments). Finally we sample maximally 20 \u3008referent, context\u3009 pairs per attribute, resulting in 790 test items. For each referent and context\nwe extract CNN visual vectors from their bounding boxes, and feed them to DAN to obtain their discriminative attributes. Note that we used the ViSA-trained DAN for this experiment as well.\nResults For 12% of the test \u3008referent, context\u3009 pairs, the discriminative attribute is contained in the set of discriminative attributes predicted by DAN. A random baseline estimated from the distribution of attributes in the ViSA dataset would score 15% recall. This baseline however on average predicts 20 discriminative attributes, whereas DAN activates, only 4. Thus, the baseline has a trivial recall advantage.\nIn order to evaluate whether in general the discriminative attributes activated by DAN would lead to referential success, we further sampled a subset of 100 \u3008referent,context\u3009 test pairs. We presented them separately to two subjects (one a co-author of this study) together with the attribute that the model activated with the largest score (see Figure 1 for examples). Subjects were asked to identify the intended referent based on the attribute. If both agreed on the same referent, we achieved referential success, since the modelpredicted attribute sufficed to coherently discriminate between the two images. Encouragingly, the subjects agreed on 78% of the pairs (p<0.001 when comparing against chance guessing, according to a 2-tailed binomial test). In cases of disagreement, the predicted attribute was either too generic or very salient in both objects, a behaviour observed especially in same-category pairs (e.g., is_round in Figure 1)."}, {"heading": "7 Conclusion", "text": "We presented DAN, a model that, given a referent and a context, learns to predict their discriminative features, while also inferring visual attributes of concepts as a by-product of its training regime. While the predicted discriminative attributes can result in referential success, DAN is currently lacking all other properties of reference (Grice, 1975) (salience, linguistic and pragmatic felicity, etc). We are currently working towards adding communication (thus simulating a speaker-listener scenario (Golland et al., 2010)) and natural language to the picture."}], "references": [{"title": "Content determination in the generation of referring expressions", "author": ["Dale", "Haddock1991] Robert Dale", "Nicholas Haddock"], "venue": "Computational Intelligence,", "citeRegEx": "Dale et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dale et al\\.", "year": 1991}, {"title": "Computational interpretations of the gricean maxims in the generation of referring expressions", "author": ["Dale", "Reiter1995] Robert Dale", "Ehud Reiter"], "venue": "Cognitive science,", "citeRegEx": "Dale et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dale et al\\.", "year": 1995}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Lia-Ji Li", "Li Fei-Fei"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Describing objects by their attributes", "author": ["Farhadi et al.2009] Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Learning visual attributes", "author": ["Ferrari", "Zisserman2007] Vittorio Ferrari", "Andrew Zisserman"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Ferrari et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ferrari et al\\.", "year": 2007}, {"title": "Learning to communicate to solve riddles with deep distributed recurrent q-networks", "author": ["Yannis M. Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "Technical Report arXiv:1602.02672", "citeRegEx": "Foerster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foerster et al\\.", "year": 2016}, {"title": "Theories of lexical semantics", "author": ["Dirk Geeraerts"], "venue": null, "citeRegEx": "Geeraerts.,? \\Q2009\\E", "shortCiteRegEx": "Geeraerts.", "year": 2009}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["Golland et al.2010] Dave Golland", "Percy Liang", "Dan Klein"], "venue": "In Proceedings of the 2010 conference on empirical methods in natural language processing,", "citeRegEx": "Golland et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golland et al\\.", "year": 2010}, {"title": "Teaching machines to read and comprehend", "author": ["Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations. http://arxiv.org/ abs/1511.02301", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["Vicente Ordonez", "Mark Matten", "Tamara L Berg"], "venue": "In EMNLP,", "citeRegEx": "Kazemzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Computational generation of referring expressions: A survey", "author": ["Krahmer", "Van Deemter2012] Emiel Krahmer", "Kees Van Deemter"], "venue": "Computational Linguistics,", "citeRegEx": "Krahmer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krahmer et al\\.", "year": 2012}, {"title": "A roadmap towards machine intelligence", "author": ["Armand Joulin", "Marco Baroni"], "venue": "arXiv preprint arXiv:1511.08130", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Natural reference to objects in a visual domain", "author": ["Kees van Deemter", "Ehud Reiter"], "venue": "In Proceedings of the 6th international natural language generation conference,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Attribute learning in large-scale datasets", "author": ["Russakovsky", "Fei-Fei2010] Olga Russakovsky", "Li Fei-Fei"], "venue": "In Proceedings of ECCV,", "citeRegEx": "Russakovsky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2010}, {"title": "Speech Acts: An Essay in the Philosophy of Language", "author": ["John R. Searle"], "venue": null, "citeRegEx": "Searle.,? \\Q1969\\E", "shortCiteRegEx": "Searle.", "year": 1969}, {"title": "Models of semantic representation with visual attributes", "author": ["Vittorio Ferrari", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Social language learning", "author": ["Luc Steels"], "venue": "The Future of Learning,", "citeRegEx": "Steels.,? \\Q2003\\E", "shortCiteRegEx": "Steels.", "year": 2003}, {"title": "MatConvNet \u2013 Convolutional Neural Networks for MATLAB", "author": ["Vedaldi", "Lenc2015] Andrea Vedaldi", "Karel Lenc"], "venue": "Proceeding of the ACM Int. Conf. on Multimedia", "citeRegEx": "Vedaldi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedaldi et al\\.", "year": 2015}, {"title": "Emergence of gricean maxims from multi-agent decision theory", "author": ["Vogel et al.2013] Adam Vogel", "Max Bodoia", "Christopher Potts", "Daniel Jurafsky"], "venue": "In HLT-NAACL,", "citeRegEx": "Vogel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "Recent advances in end-to-end-trained neural network architectures have renewed interest in developing systems capable of genuine language understanding (Hermann et al., 2015; Hill et al., 2015).", "startOffset": 153, "endOffset": 194}, {"referenceID": 9, "context": "Recent advances in end-to-end-trained neural network architectures have renewed interest in developing systems capable of genuine language understanding (Hermann et al., 2015; Hill et al., 2015).", "startOffset": 153, "endOffset": 194}, {"referenceID": 18, "context": "setup (Steels, 2003; Mikolov et al., 2015).", "startOffset": 6, "endOffset": 42}, {"referenceID": 12, "context": "setup (Steels, 2003; Mikolov et al., 2015).", "startOffset": 6, "endOffset": 42}, {"referenceID": 15, "context": "Within this general picture, one fundamental aspect of meaning where communication is indeed crucial is the act of reference (Searle, 1969; Abbott, 2010), the ability to successfully talk to others about things in the external world.", "startOffset": 125, "endOffset": 153}, {"referenceID": 16, "context": "Our starting point is the Visual Attributes for Concepts Dataset (ViSA) (Silberer et al., 2013), which contains per-concept (as opposed to perimage) attributes for 500 concrete concepts (CAT, ar X iv :1 60 3.", "startOffset": 72, "endOffset": 95}, {"referenceID": 2, "context": "We extracted on average 100 images annotated with each of these concepts from ImageNet (Deng et al., 2009).", "startOffset": 87, "endOffset": 106}, {"referenceID": 3, "context": "pervised setups (Ferrari and Zisserman, 2007; Farhadi et al., 2009; Russakovsky and Fei-Fei, 2010).", "startOffset": 16, "endOffset": 98}, {"referenceID": 6, "context": "tics of attributes arises from their distinctive function within a communication system is fully in line with the classic structuralist view of linguistic meaning (Geeraerts, 2009).", "startOffset": 163, "endOffset": 180}, {"referenceID": 16, "context": "We report moreover the best F1 score of Silberer et al. (2013), who learn a SVM for each", "startOffset": 40, "endOffset": 63}, {"referenceID": 10, "context": "Test stimuli Our starting point is the ReferIt dataset (Kazemzadeh et al., 2014), consisting of REs denoting objects (delimited by bounding boxes) in natural images.", "startOffset": 55, "endOffset": 80}, {"referenceID": 7, "context": "We are currently working towards adding communication (thus simulating a speaker-listener scenario (Golland et al., 2010)) and natural language to the picture.", "startOffset": 99, "endOffset": 121}], "year": 2017, "abstractText": "As a first step towards agents learning to communicate about their visual environment, we propose a system that, given visual representations of a referent (cat) and a context (sofa), identifies their discriminative attributes, i.e., properties that distinguish them (has_tail). Moreover, despite the lack of direct supervision at the attribute level, the model learns to assign plausible attributes to objects (sofa-has_cushion). Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes.", "creator": "LaTeX with hyperref package"}}}