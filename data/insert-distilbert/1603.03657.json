{"id": "1603.03657", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2016", "title": "Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting", "abstract": "essentially when a convolutional neural network is used for on - the - fly evaluation of continuously updating adaptive time - sequences, many redundant convolution operations are performed. we propose the method of deep shifting, ai which often remembers previously calculated results of convolution correction operations in order to minimize the number of conflicting calculations. the reduction in complexity is at least a constant integral and in the best case quadratic. we demonstrate that this method does indeed save significant computation time periods in a practical mesh implementation, especially ones when the networks subsequently receives a large number of time - frames.", "histories": [["v1", "Fri, 11 Mar 2016 15:16:09 GMT  (912kb,D)", "http://arxiv.org/abs/1603.03657v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["koen groenland", "sander bohte"], "accepted": false, "id": "1603.03657"}, "pdf": {"name": "1603.03657.pdf", "metadata": {"source": "CRF", "title": "Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting", "authors": ["Koen Groenland", "Sander Bohte"], "emails": ["groenlan@cwi.nl"], "sections": [{"heading": null, "text": "Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting\nKoen Groenland, Sander Bohte Centrum Wiskunde en Informatica\u2217\n(Dated: March 14, 2016)\nWhen a Convolutional Neural Network is used for on-the-fly evaluation of continuously updating time-sequences, many redundant convolution operations are performed. We propose the method of Deep Shifting, which remembers previously calculated results of convolution operations in order to minimize the number of calculations. The reduction in complexity is at least a constant and in the best case quadratic. We demonstrate that this method does indeed save significant computation time in a practical implementation, especially when the networks receives a large number of time-frames.\nI. INTRODUCTION\nConvolutional Neural Networks (CNNs) have proven extremely succesful in finding structure in highdimensional data, including time-sequences such as audio and video [1\u20133]. Some examples of promising real-world applications are speech- and video recognition, and automatic translation [4]. Challenges for software development include training of the networks using large GPU clusters and gathering huge, labeled datasets [5]. Practical user-side evaluation faces completely different challenges, including efficient and fast performance, low resource consumption, and responsiveness, such that the software responds to recognized events as quickly as possible [6]. Earlier work focusing on achieving these challenges include using less-parameter convolution filters [7], pruning obsolete weights [6], and using spiking networks [8]. This paper deals with optimizing convolution of timeseries, as used for example in 3D convolutional neural networks as they are applied in human action recognition [2].\nWe observe that when forward propagating continuously updating time-sequences through a neural network that applies convolution in the time-dimension, many redundant calculations are made. In order to avoid these calculations, to save CPU resources and potentially battery life on mobile devices, we propose Deep Shifting, which copies results of convolution operations from earlier time steps, rather than re-calculating these over and over. This can save substantial calculation time, especially when the CNN looks at a large number of timeframes. This paper is organized as follows: Section 2 shows how Deep Shifting performs CNN operations on time-sequences without performing redundant calculations. Sections 3 and 4 examine the theoretical and practical benefits of Deep Shifting. Section 5 investigates the possibilities of training a network using a minimal number of neurons and operations, and the paper finishes with a discussion and conclusion.\n\u2217Electronic address: groenlan@cwi.nl"}, {"heading": "II. DEEP SHIFTING", "text": "Let x be an input with a matrix shape, having a timeaxis and a context axis. The context axis holds what is called \u201cchannels\u201d in image convolutions. A convolutional auto-encoder, for which we label the layers x, h and y respectively, applies the following encoding operation with convolution over the time axis:\nht = \u03c3 ( w\u2211 \u03c4=0 W\u03c4 \u00b7 xt+\u03c4 + b ) . (1)\nHere, w is the size of the convolution window, W is a set of w weight matrices, t denotes the time step of the input sequence, and \u03c4 labels the time axis of the weights. \u03c3 denotes an activation function, which we choose to be tanh in our computational part. A schematic view is given in figure 1.\nIf the input consists of time-evolving data, we typically want to perform the encoding every time new information becomes available, e.g. at every new time step. However, due to the nature of convolution, many redundant calculations are made. Let the actual time at which we perform an encoding be denoted by T , and denote the neuronal activation at a specific time step by xt(T ) and ht(T ). Moreover, let the indices t be defined relative to T , such that the \u2018leftmost\u2019 input neuron is xT . If at time T = 5, the network in figure 2 received input time steps 0 through 5 initially, it has calculated h0(5) up to h3(5)\nar X\niv :1\n60 3.\n03 65\n7v 1\n[ cs\n.L G\n] 1\n1 M\nar 2\n01 6\n2 t=0t=3 t=2\nw2 w1 w0\nt=1t=4t=5 x\nh\ny\nFIG. 2: A graphical view of an auto-encoder which applies convolution on the time-axis of its input.\nFIG. 3: A comparison between a regular convolution network (left) and the pruned version version using Deep Shifting (right).\nthrough formula 1. When we give the same network time steps 1 through 6 at a later time T = 6, it will calculate h1(6) through h4(6). By formula 1, h1(5) = h1(6), and the same goes for the other time steps h2 and h3. By the natural flow of time, the activations in the network will have shifted through the network. Therefore, we could have just as well stored these values, and copied them to the neighbouring neurons, without performing the the convolution calculation all over again. Let th denote the highest time step a hidden neuron can have at the time of encoding, e.g. th = T \u2212w+1. Using the Deep Shifting method, the neural activations are calculated by:\nhth = \u03c3 ( w\u2211 \u03c4=0 W\u03c4 \u00b7 xth+\u03c4 + b ) , (2) ht(T ) = ht+1(T \u2212 1) (t < th). (3)\nThe same reasoning holds for an arbitrary number of layers, can be used to save calculations in down-sampling, and is easily extended to convolution in multiple dimensions, such as 3D convolutions that are often used on video data [2]. Moreover, it can avoid obsolete reconstruction operations: Layer y in figure 2 will have y3 fixed after T = 5, such that this value could also in principle be stored and shifted (assuming the weights will not have changed significantly in the meantime). A graphical comparison is given in figure 3."}, {"heading": "III. THEORETICAL BENEFIT", "text": "How much exactly did we prune from the original network? Let one \u201cconvolution operation\u201d be the calculation of one time-frame in the hidden layer, e.g. one full vector ht for some t. If the layers x, h, y would normally span tx, th and ty time steps, we don\u2019t have to store tx\u2212w, th\u2212w and ty\u2212w neurons in the three layers, and we saved ourselves calculating th \u2212 1 hidden convolution operations and ty \u2212 w output convolutions.\nRecall that for most practical applications, convolutional auto-encoders are stacked in an hourglass shape, where the shallow layers span many time steps, whilst the deeper layers have increasingly smaller time axes. Assume we start with the deepest hidden layer of a CAE, with a time-axis of size thn , on which we will stack some number n of shallower layers. Any previous layers should then have a time-axis of size th + w \u2212 1 if a convolution window of size w is used. The number of convolution operations C performed in a forward pass through this network can be calculated to be:\nCnormal = thn\n+ thn + w \u2212 1 + thn + w \u2212 1 + w \u2212 1 + thn + w \u2212 1 + w \u2212 1 + w \u2212 1 + . . .\n+ thn + (n\u2212 1)(w \u2212 1)\n= nthn + 1\n2 (n\u2212 1)(n\u2212 2)(w \u2212 1). (4)\nIn the last line, we used \u2211n i=1 i = 1 2n(n\u22121), where in our case the sum runs up to (n \u2212 1) because the shallowest layer\u2019s neurons are not calculated through convolution. Clearly, due to the increasing size of the time-axes, the number of computations in a regular time-encoding CNN scales quadratically with the number of layers, assuming fixed thn . This case assumes the same w in every layer, but this quadratic scaling holds even with w varying per layer, as long as the windows have sizes of at least 2. On the other hand, a Deep Shifting architecture scales linearly with the number of layers:\nCdeep shifting = n. (5)\nThe situation is slightly different if we consider the case where the time-axis of the input tx is fixed, and we add increasing numbers of deeper layers, which have smaller time axes. In this case, the total number of convolutions\n3 upon encoding equals\nCnormal = tx \u2212 w + 1 + tx \u2212 w + 1\u2212 w + 1 + tx \u2212 w + 1\u2212 w + 1\u2212 w + 1 + . . .\n+ tx \u2212 n(w \u2212 1)\n= ntx \u2212 1\n2 n(n\u2212 1)(w \u2212 1). (6)\nUnder these assumptions, Deep Shifting uses a number of convolution operations still equaling the number of layers. In this situation, we obtain a linear gain in performance: the average number of convolutions per layer is tx \u2212 n+12 (w \u2212 1), compared to just 1 for Deep Shifting. Therefore, Deep Shifting requires tx \u2212 n+12 (w \u2212 1) times less convolution operations in the whole encoding process."}, {"heading": "IV. EMPIRICAL BENEFIT", "text": "We compare our own Matlab implementation of a Deep Shifting network with the CNN from the \u2018Deep Learn Toolbox\u2019 by Rasmus Bergpalm [9]. As the original version applies convolution in 2 dimensions for images specifically, we modified the code to apply convolution in only 1 dimension, representing time. We consider two networks: the first using the conventional CNN architecture (formula 1), the second exploiting Deep Shifting (formula 2 and 3). Both networks receive a dataset representing part of a long time-sequence, and each subsequent input is the previous sequence shifted by one \u2018neuron\u2019 in the time dimension. The average time required for forward propagation using various context axis sizes and convolution windows are displayed in figure 4.\nAs expected, the computation time on the regular CNN increases with increasing time-window input tx, whereas it remains constant using Deep Shifting. The shifting operation as implemented however requires a higher base computation time, and whether Deep Shifting does save computation time is then dependent on the parameters of the CNN. For realistic situations, such as a context size of 40 such as used in large-scale speech tasks [3], Deep Shifting becomes beneficial when more than roughly 20 time-frames are given as input to the CNN. Moreover, the advantage of Deep Shifting becomes more noticeable when the size of the context axis increases."}, {"heading": "V. TRAINING WITH MINIMAL CONNECTIVITY", "text": "Can we also use the sparse connectivity structure sufficient for Deep Shifting in the context of learning? This section compares the training behaviour of Deep Shifting and a regular convolutional network. We train the networks as a Convolutional Auto-Encoder (CAE). Given an\ninput x, the hidden layer ht is calculated using formula 1 (normal CAE) or formulas 2 and 3 (Deep Shifting), using \u2018valid convolution\u2019 (the number of time steps in the hidden layer is th = tx\u2212w+1). Next, the network performs a reconstruction step, applying the adjoint of the earlier convolution to find the reconstruction x\u0303. This involves \u2018full convolution\u2019, where the number of output time steps equals tx\u0303 = th + w \u2212 1 = tx. The network weight and bias parameters are set to minimize the reconstruction error E = \u2211 x\u2208Dataset |x \u2212 x\u0303|2 by using gradient descent for 100 epochs at a learning rate of 10\u22124. Our test involves 2 datesets. The first is the Spoken Digit dataset, as used in Ref. [11], consisting of the number \u2018zero\u2019 to \u2018nine\u2019 pronounced 10 times by 5 different speakers. The second dataset is the Auslan dataset, a time-series representing hand-movements of the Australian deaf-community sign language, obtained from Ref. [12]. We test whether our networks can classify the right digit or sign, respectively. First, an MLP with 30 hidden units is trained to properly classify these on the training set, after which the percentage of correct classifications on a separate test dataset is measured.\nFigure 5 shows the results on the Spoken Digit dataset. The network denoted by \u2018ShiftNet\u2019, which employed Deep Shifting, shows very competitive results compared to a regular CNN. Note that both networks manage to achieve a very low percentage of erroneous classifications, indi-\n4 cating that the networks encode all important information in the dataset into a small number of hidden units. Similarly to the approach of Ref. [11], we used a random 60% of the 500 data points for training, and the others for testing.\nFigure 6 shows the results on the Auslan dataset. The parameter \u2018len\u2019 indicates the number of time-frames to which the input was scaled, and \u2018nh\u2019 indicates the number of hidden units. On this much harder dataset, a regular CNN is able to perform much better than the ShiftNet, although the latter is still able to achieve very reasonable results despite the very limited connectivity. For this test, 10-fold cross validation is used, where 10% of the data is taken as test data, and another 10% used as validation data to limit overfitting by the MLP.\nIn general, we observe that Deep Shifting is able to achieve results comparable to normal CNNs when the tests are \u2018easy\u2019, for example, when the classification errors are low, or when many hidden units are used. When classification errors get large, or the number of hidden units is small compared to the data size, Deep Shifting generally does not reach the performance of a regular CNN."}, {"heading": "VI. DISCUSSION AND CONCLUSION", "text": "We described the method of Deep Shifting, which can speed up the forward propagation of continuously updating inputs in convolutional auto-encoders. Using both a\ntheoretical and empirical analysis, we showed that Deep Shifting requires less convolution operations and computation time than a regular convolutional network when the number of input time-frames exceeds some threshold. For common practical applications, we found this threshold to lie around roughly 16 time-frames for 40 to 80 dimensional inputs with windows of size 10. Deep Shifting is only relevant when time-sequences need to be continuously evaluated, and when the number of timeframes to be considered is larger than the size of the convolution window. The latter will always be the case if multiple layers are used.\nOur analysis did not consider the use of graphical processing units: We are not sure how the speed of the copy operations of Deep Shifting compare against the largescale parallel operations that GPU\u2019s are capable of, although we wonder if these can be used in practical userend implementations.\nWe also considered training a network with the Deep Shifting architecture. Our tests show that a regular neural network layout is preferred on hard training tasks. In general, training a Deep Shifting architecture has very few applications, since networks are often trained on readily stored datasets, limiting the need for on-the-fly evaluation.\nStill, Deep Shifting is an easily implementable trick to optimize deep neural networks working with timeevolving data, when speed or power consumption is relevant. This may be of benefit for mobile devices that aim to interpret sound, video, or other sensor data.\n[1] M. Baccouche, F. Mamalet, C Wolf, C. Garcia, A. Baskurt, Sequential Deep Learning for Human Action Recognition, 2nd International Workshop on Human Behavior Understanding (HBU), 29-39, 2011, [2] S. Ji, W. Xu, M. Yang, K. Yu, 3D Convolutional Neural Networks for Human Action Recognition. IEEE Trans. Pattern Anal. Mach. Intell. 35(1), 221-231, 2013 [3] T. N. Sainath, A. Mohamed, B. Kingsbury and B. Ramabhadran, Deep Convolutional Neural Networks for LVCSR, Proc. ICASSP, 2013 [4] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature 521, 7553, 436-444, 2015 [5] Y. Bengio, Practical recommendations for gradient-based training of deep architectures, Neural networks: Tricks of the trade, 437-478, 2012 [6] S. Han, J. Pool, J. Tran, W. Dally, Learning both Weights and Connections for Efficient Neural Network, Advances in Neural Information Processing Systems 28, 1135\u20131143, 2015\n[7] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the Inception Architecture for Computer Vision, arXiv:1512.00567v3, 2015 [8] S. Bohte, Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model, Advances in Neural Information Processing Systems 25, 1835\u20131843, 2012 [9] R. B. Palm, Prediction as a candidate for learning deep hierarchical models of data, 2012 [10] F. Bastien, P. Lamblin, R. Pascanu et al., Theano: new features and speed improvements, NIPS, 2012 [11] D. Verstraeten, B. Schrauwen, and D. Stroobandt, Isolated word recognition using a liquid state machine, Proceedings of the 13th european symposium on artificial neural networks (esann), 435-440, 2005 [12] M. Lichman, UCI Machine Learning Repository, 2013 [13] M. Waleed Kadous and C. Sammut, Classification of mul-\ntivariate time series and structured data using constructive induction, Machine Learning 58, 179-216, 2005\n5"}], "references": [{"title": "Sequential Deep Learning for Human Action Recognition", "author": ["M. Baccouche", "F. Mamalet", "C Wolf", "C. Garcia", "A. Baskurt"], "venue": "2nd International Workshop on Human Behavior Understanding (HBU),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "3D Convolutional Neural Networks for Human Action Recognition", "author": ["S. Ji", "W. Xu", "M. Yang", "K. Yu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep Convolutional Neural Networks for LVCSR", "author": ["T.N. Sainath", "A. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Proc. ICASSP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Y. Bengio"], "venue": "Neural networks: Tricks of the trade,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Learning both Weights and Connections for Efficient Neural Network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Rethinking the Inception Architecture for Computer Vision, arXiv:1512.00567v3", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model, Advances in Neural Information", "author": ["S. Bohte"], "venue": "Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Isolated word recognition using a liquid state machine", "author": ["D. Verstraeten", "B. Schrauwen", "D. Stroobandt"], "venue": "Proceedings of the 13th european symposium on artificial neural networks (esann),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Classification of multivariate time series and structured data using constructive induction, Machine Learning", "author": ["M. Waleed Kadous", "C. Sammut"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional Neural Networks (CNNs) have proven extremely succesful in finding structure in highdimensional data, including time-sequences such as audio and video [1\u20133].", "startOffset": 164, "endOffset": 169}, {"referenceID": 1, "context": "Convolutional Neural Networks (CNNs) have proven extremely succesful in finding structure in highdimensional data, including time-sequences such as audio and video [1\u20133].", "startOffset": 164, "endOffset": 169}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) have proven extremely succesful in finding structure in highdimensional data, including time-sequences such as audio and video [1\u20133].", "startOffset": 164, "endOffset": 169}, {"referenceID": 3, "context": "Challenges for software development include training of the networks using large GPU clusters and gathering huge, labeled datasets [5].", "startOffset": 131, "endOffset": 134}, {"referenceID": 4, "context": "Practical user-side evaluation faces completely different challenges, including efficient and fast performance, low resource consumption, and responsiveness, such that the software responds to recognized events as quickly as possible [6].", "startOffset": 234, "endOffset": 237}, {"referenceID": 5, "context": "Earlier work focusing on achieving these challenges include using less-parameter convolution filters [7], pruning obsolete weights [6], and using spiking networks [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Earlier work focusing on achieving these challenges include using less-parameter convolution filters [7], pruning obsolete weights [6], and using spiking networks [8].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Earlier work focusing on achieving these challenges include using less-parameter convolution filters [7], pruning obsolete weights [6], and using spiking networks [8].", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "This paper deals with optimizing convolution of timeseries, as used for example in 3D convolutional neural networks as they are applied in human action recognition [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "The same reasoning holds for an arbitrary number of layers, can be used to save calculations in down-sampling, and is easily extended to convolution in multiple dimensions, such as 3D convolutions that are often used on video data [2].", "startOffset": 231, "endOffset": 234}, {"referenceID": 7, "context": "We compare our own Matlab implementation of a Deep Shifting network with the CNN from the \u2018Deep Learn Toolbox\u2019 by Rasmus Bergpalm [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "For realistic situations, such as a context size of 40 such as used in large-scale speech tasks [3], Deep Shifting becomes beneficial when more than roughly 20 time-frames are given as input to the CNN.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "[11], consisting of the number \u2018zero\u2019 to \u2018nine\u2019 pronounced 10 times by 5 different speakers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11], we used a random 60% of the 500 data points for training, and the others for testing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "35(1), 221-231, 2013 [3] T.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "Hinton, Deep learning, Nature 521, 7553, 436-444, 2015 [5] Y.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "Bengio, Practical recommendations for gradient-based training of deep architectures, Neural networks: Tricks of the trade, 437-478, 2012 [6] S.", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "1135\u20131143, 2015 [7] C.", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": "00567v3, 2015 [8] S.", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "Bohte, Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model, Advances in Neural Information Processing Systems 25, 1835\u20131843, 2012 [9] R.", "startOffset": 158, "endOffset": 161}, {"referenceID": 8, "context": "[11] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Lichman, UCI Machine Learning Repository, 2013 [13] M.", "startOffset": 47, "endOffset": 51}], "year": 2016, "abstractText": "When a Convolutional Neural Network is used for on-the-fly evaluation of continuously updating time-sequences, many redundant convolution operations are performed. We propose the method of Deep Shifting, which remembers previously calculated results of convolution operations in order to minimize the number of calculations. The reduction in complexity is at least a constant and in the best case quadratic. We demonstrate that this method does indeed save significant computation time in a practical implementation, especially when the networks receives a large number of time-frames.", "creator": "LaTeX with hyperref package"}}}