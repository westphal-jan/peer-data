{"id": "1603.03833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2016", "title": "Learning real manipulation tasks from virtual demonstrations using LSTM", "abstract": "robots assisting disabled or elderly people in the performance performance of activities of daily living need to perform complex manipulation tasks which are highly dependent on the environment and preferences of nearby the human user. in addition, these task environments and users are not suitable for the collection of massive amounts of training data, as the manipulated objects can be fragile, portable and the wheelchair - bound advanced users might have physical difficulty recovering from a failed manipulation task.", "histories": [["v1", "Sat, 12 Mar 2016 00:47:38 GMT  (1949kb,D)", "http://arxiv.org/abs/1603.03833v1", null], ["v2", "Thu, 15 Sep 2016 23:56:19 GMT  (2964kb,D)", "http://arxiv.org/abs/1603.03833v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["rouhollah rahmatizadeh", "pooya abolghasemi", "aman behal", "ladislau b\\\"ol\\\"oni"], "accepted": false, "id": "1603.03833"}, "pdf": {"name": "1603.03833.pdf", "metadata": {"source": "CRF", "title": "Learning Manipulation Trajectories Using Recurrent Neural Networks", "authors": ["Rouhollah Rahmatizadeh", "Pooya Abolghasemi", "Ladislau B\u00f6l\u00f6ni"], "emails": ["lboloni}@eecs.ucf.edu"], "sections": [{"heading": "1 Introduction", "text": "Assistive robotics, whether in the form of wheelchair mounted robotic arms or mobile robots with manipulators, promise to improve the independence and quality of life of persons with disabilities or the elderly. A typical use of such robots is to help users in the performance of Activities of Daily Living (ADLs) such as self-feeding, dressing, grooming, personal hygiene and leisure activities such as reading. Almost all ADLs involve some type of object manipulation.\nCurrent assistive systems rely on remote control, but increasing efforts had been put into systems that exhibit some degree of autonomy in the identification of the objects, grasping and manipulation. Most of these systems are positioned as solving problems of object identification combined with problems in planning and control theory [Endres et al., 2013; Miller et al., 2012; Bollini et al., 2011].\nOne of the challenges of this approach is the very wide variability of the home environment, the physical objects involved in the ADL as well as the preferences of the user. For a practical deployment, it would be desirable for the assistive robot to learn the appropriate trajectories adapted to the user and the environment - similarly to the way a human caretaker would learn the needs and preferences of the disabled person, as well as the specific circumstances of her home.\nIn this work, we propose to use recurrent neural networks to learn and generalize patterns in demonstrated trajectories in order for the robot to learn to perform complex tasks autonomously. Recurrent neural networks, and especially Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] with gating mechanism, had been proved to be effective in many sequence learning tasks. One of the most interesting advantages of LSTMs is that they are really good at capturing the long term patterns in the sequence. For instance, in character-level text generation [Karpathy et al., 2015], they can learn to close a parenthesis that had been opened earlier. This pattern is similar to opening the robot\u2019s gripper to release the held object when it is closed somewhere before to grasp it.\nOne of the most successful approaches to teach the robot to perform tasks is Learning from Demonstrations (LfD). In LfD the human assists the robot in performing the task (for instance, by guiding its arm or by teleoperation). One of the challenges in utilizing neural networks in LfD is that neural networks require a comparatively large amount of training data. For applications in assistive robotics, this is further complicated by the fact that these environments and users are not suitable for the collection of massive amounts of training data. For instance, the manipulated objects and the environment can be fragile and the wheelchair-bound users might have difficulty recovering from a failed manipulation task, such as a dropped cup. Furthermore, the users who might have motor or cognitive disabilities, might not be able to consistently provide high quality demonstrations. The approach we propose in this paper involves the user performing the demonstrations in a virtual environment closely emulating the home environment that will be used for the physical robot. The \u201cgamified\u201d data collection model, and the ability to recover from failed demonstrations with minimal impact allows us to gather a significantly higher number of demonstrations than what would be possible with the physical robot. We are ar X iv :1 60 3.\n03 83\n3v 1\n[ cs\n.R O\n] 1\n2 M\nar 2\n01 6\nconsidering two manipulation tasks frequently encountered in ADLs: (a) pushing objects to desired positions on the desk and (b) picking up and placing objects to desired positions. The collected demonstrations were used to train several models of neural networks: recurrent networks LSTM and Gated Recurrent (GRU) with different configurations, as well as a feed-forward network. The objective was to obtain a network that can perform the manipulation task on an arbitrary example scenario.\nThe reminder of this paper is organized as follows. Section 2 describes related work. In Section 3 we explain the proposed method for learning trajectories using recurrent neural networks. Section 4 describes the experiments and the method for collecting demonstration trajectories. Section 5 presents the results on the performance of different neural networks in learning the specified tasks. We discuss the findings of this research in Section 6 and conclude in Section 7."}, {"heading": "2 Related work", "text": "Recurrent Neural Networks. Recurrent Neural Networks (RNNs) are proved to be an effective way to model and reproduce patterns in sequential data. Some of successful applications include handwriting recognition [Graves et al., 2009], language modeling [Karpathy et al., 2015], machine translation [Sutskever et al., 2014], speech recognition [Graves et al., 2013], visual recognition [Donahue et al., 2014], and image captioning [Vinyals et al., 2015].\nAlthough recurrent neural networks had been proposed as early as the 1980s, primary versions suffered from the difficulty of training over sequential data, due to the difficulty to account for events occurring at different times in the training data sequences (the \u201cvanishing error gradient\u201d problem). Newer RNN models feature explicit gating mechanism [Hochreiter and Schmidhuber, 1997] that helped in storing and retrieving information over long time periods. In recent years, similar mechanisms were proposed such as Gated Recurrent Units (GRU) [Cho et al., 2014].\nAutonomous trajectory execution. One of the most effective approaches for teaching robots to execute a desired task is Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings [Argall et al., 2009]. In this method, sample trajectories for performing a task is demonstrated by the user. The challenge is to extend these demonstrations to unseen situations. Examples of successful application include autonomous helicopter maneuvers [Abbeel et al., 2010], playing table tennis [Kober et al., 2011][Calinon et al., 2010], object manipulation [Pastor et al., 2009], and making coffee [Sung et al., 2015]. To gather enough data for the task learning, some researchers proposed to use cloud-based and crowdsourced data collection techniques [Kehoe et al., 2013], [Forbes et al., 2014], [Crick et al., 2011]. Such approaches can successfully multiply the training data for manipulation tasks where there is a userindependent solution, but cannot be used to train a robot to adapt to the user\u2019s preferences.\nRecently, deep neural networks were applied to complex robotic tasks and achieved interesting results. For instance, [Levine et al., 2015] utilized feed-forward neural net-\nworks to map robot\u2019s visual input to control commands. In this approach, the visual input is processed using a convolutional neural network (CNN) to extract 2D feature points, then it is aggregated with the robot\u2019s current joint configuration, and fed into a feed-forward neural network. The neural network then will predict the next joint configuration of the robot.\nUsing a feed-forward neural network and only giving the current configuration of the robot without considering the history of the executed trajectory would not be effective in learning complex tasks. For instance, consider two different trajectories with different aims. One of the waypoints (i.e. the configuration of the robot and also the visual input) in the first trajectory might be the same as one of the waypoints in the second trajectory. In this case, which occurs often in complex tasks, the network forgets which one of the trajectories was being executed; therefore, it will be confused. Recurrent neural networks can resolve this problem by storing the history of the sequence (trajectory waypoints) and using this information for predicting the next trajectory waypoint."}, {"heading": "3 Method", "text": ""}, {"heading": "3.1 Trajectory representation", "text": "Let us assume that N demonstrations D = {d1 . . . di . . . dN} of performing a task are available. Each demonstration di = {E,Q, P} consists of Q = {q1 . . . qt . . . qT } in which qt is the state of environment at time step t = [1 . . . T ], E = [e1 . . . et . . . eT ] in which et is the end-effector pose augmented with gripper status (open/close) at time t, and P = {p1 . . . pt . . . pT } is the set of user preferences at each time step. The duration of execution of a demonstrated trajectory T may be different for each demonstration. In this paper, we consider the state of the environment Q as the sequence of poses of objects involved in a manipulation task. Therefore, we assume qt = {o1 . . . oj . . . oM} in which oj is the pose of object j at time step t, and M is the total number of objects involved in the manipulation. Pose ot = [px, py, pz, rx, ry, rz, rw] is the vector containing the position and rotation quaternion of an object with respect to the origin.\nWhen a user demonstrates a task to the robot, the user will choose one of the many possible trajectories based on some hidden user preferences. For instance, the user might choose a trajectory where the robot arm slows down where it approaches the object for grasping, but moves faster after it accomplished the grasp. Naturally, it is also possible that the user might fail in a certain demonstration.\nIn the following parts, a generative model is proposed to perform a manipulation task based on user preferences."}, {"heading": "3.2 The input and output of the neural network", "text": "Similar to feed-forward neural networks, RNNs take a fixedsize vector of inputs and produce a fixed-size vector of outputs. However, what makes the RNNs special is the feedback connection from the output of each layer to its input. This structure helps them to store a memory of the past and use this information while generating the output. We take advantage of this feature of RNNs to model a trajectory since the\ncurrent waypoint in the trajectory depends on the previous waypoints. It is possible to simply feed a waypoint containing et, the end-effector pose at time t, and ask the network to predict what will be et+1, the next end-effector pose at time t+1. In this scenario, however, the network learns to execute an average of demonstrated trajectories without considering the state of the environment. Since a manipulation trajectory also depends on the pose of all the object involved in the task, the state of the environment qt containing the pose of every involved object is fed into the network. In addition, the user preferences are fed to the network as a control signal to generate trajectory based on user preferences. Figure 1 shows the input and the output of RNN at each time step."}, {"heading": "3.3 The structure of the neural network", "text": "The goal of the network is to learn a model of the trajectory by estimating the mean of the probability distribution of the next end-effector waypoint in the trajectory given all previous waypoints containing end-effector, environment state, and user preferences:\nP (e1, . . . , eT ) = T\u220f t=1 P (et|{e, q, p}1, . . . , {e, q, p}t\u22121) (1)\nFor this purpose, RNNs learn a fixed-size representation of the input and use it to predict the output. This hidden representation can be stored in one or more layers. If we show the state of layer l = 1 . . . L at time step t = 1 . . . T by hlt, the layer\u2019s input will be the current output of the previous layer hl\u22121t , and the output of the same layer from the previous time step hlt\u22121. The first layer h 1 t will use the input xt = {et, qt, pt} and h1t\u22121, and the output of last layer hLt will predict yt = et+1. By calculating the recurrence form (hlt\u22121, h l\u22121 t ) \u2192 hlt, we can generate waypoints one at a time to create a trajectory for the end-effector of the robot arm. The recurrent networks we use in this paper are LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho\net al., 2014] that follow the general formulation explained above. For the precise mathematics of these models, the reader is referred to the original papers. Training. The network is unrolled for 50 time steps and at each timestep the loss function is defined as the squared error between the predicted and actual value for each element of the end-effector pose and also its status (1 for open and 0 for close). All the parameters are initialized uniformly between -0.08 to 0.08 following the recommendation by [Sutskever et al., 2014]. Stochastic gradient descent with mini-batches of size 10 is used to train the network. RMSProp [Tieleman and Hinton, 2012] with initial learning rate of 0.002 and decay of 0.95-0.99 (based on number of examples) is used to divide the gradients by a running average of their recent magnitude. In order to overcome the exploding gradients problem, the gradients are clipped in the range [-1, 1]. We use 90% of the data for training and keep the remaining 10% for validation."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Simulation environment", "text": "In order to gather the data required for training the network and test the execution of the task, we designed a virtual environment in the Unity3D game engine. Unity3D simulates the basic physics of the real world including gravity, collision between objects and friction. For experiments in a typical environment where ADLs take place, we created a model of a desk that holds a variety of objects. As the scenario we are assuming involves a wheelchair mounted robotic arm (such as an InMotion MANUS or Kinova Jaco), the virtual environment contains a simple two finger gripper that can be opened and closed to grasp and carry an object. The user can move the robot\u2019s end-effector in the 3D Cartesian space using the mouse or joystick. It is also possible to rotate the endeffector using the keyboard or joystick. In addition, opening and closing the gripper at any time is possible, thus 7 degrees of freedom are provided to the user to accomplish a task. The reach of the robotic arm had been limited to match the possibilities of a wheelchair mounted robotic arm whose base is positioned at the side of the user, and thus close to the desk. Robot-specific considerations need to be made so that the end-effector trajectory can be converted to a sequence of joint configurations to be executed by the robot. For instance, we assumed the width of the table to be 2 meters, thus the robot needs to be mobile in order to reach the entire tabletop area. If a stationary robot is being used, the reach area of the gripper needs to be more limited.\nDuring the demonstration, the pose of the gripper and all the objects involved in a manipulation task augmented with the gripper status (open/close) is recorded at a frequency of 33Hz and stored in a file. The sequence of recorded waypoints forms the data that will be used to train the network."}, {"heading": "4.2 Manipulation tasks for ADLs", "text": "The performance of ADLs such as self-feeding, dressing, grooming and personal hygiene normally involve manipulation tasks interrupted by pauses and cognitive decisions. Unless the users have severe cognitive disabilities, they would normally retain the cognitive decisions (such as deciding for\nwhich food item to reach next). Thus, for this paper we have focused on two manipulation tasks that are constituents of many ADLs:\nPushing. The first task is to push a big box which is not graspable to somewhere close to the shelf in order to organize the desk. The robot starts from a random gripper position, moves the gripper close to the box, and gradually pushes the box towards the shelf. When the box reaches the desired area close to the shelf with 5cm tolerance error, the box will reappear in a random position on the desk with a random orientation. The gripper needs to circle around the box without colliding with it to repeat the task. Pick and place. The second task is to pick up a small box located on top of the desk, and place it into the shelf. The robot needs to move the gripper from a random position to a point close to the box, open the gripper, position the fingers around the box, close the gripper, move towards the shelf, orient properly to not collide with the shelf, enter the shelf, and finally open the gripper to release the box. Once the box is released, it will reappear on top of the desk in a random position, and the robot needs to repeat the task starting from the current position of the gripper.\nBoth tasks need planning and precision to be completed successfully. There are many important features that need to be learned by the algorithm out of the raw trajectory data. For instance, the algorithm should find out where to open and close the gripper, where to move when the box is gripped and when it is not, and so on.\nAutonomously performing these tasks is non-trivial since the simulation environment is realistically non-deterministic. For instance, while grasping the box, it might slip and not be grasped successfully. Therefore, the robot should understand if it is carrying the box or not and plan accordingly to try the grasp one more time or proceed towards the shelf."}, {"heading": "4.3 User preferences", "text": "There are different ways to accomplish a task. For instance, when the goal is to put the box into the shelf, any trajectory that leads the box to be placed inside the shelf is acceptable. However, these trajectories may be different in their path, speed, smoothness, etc. In order to demonstrate how well our model can generate the trajectory based on user preferences, we consider two preferences in the pick and place task.\nPosition of the box inside the shelf. In the pick and place task, the user can place the box in an arbitrary location inside the shelf. It is possible to extract this preference from the recorded data and use it to control this behavior. For this purpose, the end point of each task execution (i.e. when the box is inside the shelf) is considered and the box\u2019s position in that moment is extracted. The z element of the position which is in the same direction as the shelf\u2019s width is added as an input to the network for all the time steps of that particular task execution. Therefore, during the test by giving an arbitrary z position to that input at each time step, the network should generate a trajectory to reach the desired z position inside the shelf. To evaluate how well the network can generate a trajectory to reach the desired position inside the shelf, we compare\nthe position of the box when the task is accomplished with the desired position.\nTrajectory duration. Another user preference we consider is the duration of the trajectory. Sometimes the user wants the trajectory to be executed very fast and sometimes he wants a slow and cautious movement. In order to capture this preference, we measure how long the task execution takes and give this as an input to the network. The network needs to adjust the speed of the trajectory so that the task ends at the expected time. In order to evaluate the performance of the network in capturing this preference, we compare the duration of the trajectory with the expected duration."}, {"heading": "4.4 Data", "text": "We have collected a dataset in which the pushing task was demonstrated 473 times while the pick and place task 290 times. During the demonstration, the demonstrating user occasionally made mistakes, thus, the data is noisy. Since the difference between the consecutive waypoints when the task is recorded at 33Hz is very small, the network cannot capture long range dependencies. Therefore, we keep only 1 waypoint from each 4 consecutive points to make the resulting trajectory rate 8Hz. By doing so, we ended up with 12908 waypoints for training the network on the first task.\nIn the second task, in order to properly capture user preferences, we need more data. To save user\u2019s time while generating data and avoid overfitting, we added to the dataset synthetic trajectories obtained by transferring the whole trajectory in space and time. For space data augmentation, we shifted all the trajectory waypoints and the position of objects involved in the manipulation in 3D space. Since in this task the trajectory is still valid when everything is transferred on the z axis (parallel to the shelf\u2019s width), we shift the trajectory in that direction between -40 to 40cm to create a new set of trajectories at every 10cm. We choose a desk wide enough so that the shifted trajectories do not cause the robot to drop the object out of the workspace. However, the box still can be dropped if the robot does strange movements. We ask the network to place the box at a particular position on z axis inside the shelf (which is 2 meters wide).\nIn order to learn to generate trajectories with different durations, the network needs to see a variety of demonstrations with different durations. To capture user preference of trajectory duration in the pick and place task, we shifted the data in time. We had removed 3 consecutive waypoints from each 4 to make the rate of the trajectory approximately 8Hz. We also removed 4 consecutive waypoints from each 5 to make a trajectory with rate of 6Hz and concatenate these trajectories to the previous ones. By doing this, the resulting trajectories of the second task take between 1 to 15 seconds to finish.\nHowever, it is not practical to ask the network to finish the task in 1 second regardless of where the box and the gripper are located at the beginning of the task. In addition, the number of examples when the duration is very short or very long is limited. Therefore, we ask the network to generate trajectories with durations between 3 to 10 seconds. After shifting the trajectories of pick and place task in time and space, we ended up with 219555 waypoints available for training the network."}, {"heading": "4.5 Network architectures", "text": "For our experimental study we trained four different neural networks of the following architectures and parametrization:\n\u2022 LSTM-2: 2 layers of LSTM with 512 memory states in each layer\n\u2022 LSTM-4: 4 layers of LSTM with total parameters close to the LSTM-2 network\n\u2022 GRU-2: 2 layers of GRU with enough memory states to make the total number of parameters close to the previous networks\n\u2022 FeedForward-4: a fully connected feedforward network with 4 layers and number of parameters close to the LSTM and GRU networks. The input is the current gripper\u2019s pose and status augmented with the current pose of the objects involved in the manipulation and the output is the next predicted pose of the gripper and its status.\nAll the networks have approximately 4.2 million parameters."}, {"heading": "5 Results", "text": "The sequence of images in the upper row of the Figure 2 shows an autonomous execution of the pushing task while bottom row shows the same for the pick and place task. The complete video can be found online1.\nIn order to quantitatively evaluate the performance of our model, we let the robot perform the task 20 times. If it can not complete the task in a limited time (10 seconds for the first task and 30 seconds for the second one), or if the box falls down from the table, we count the try as a failure. Table 1 shows the difference in performance of compared models in the pushing task. The columns are:\n\u2022 loss: training error in predicting the next pose and status (distance in meters; orientation and status(open/close) normalized between 0 and 1)\n\u2022 success: the rate of successfully executing the task. \u2022 time: average time it takes for the robot to accomplish\nthe task.\nThe results of the pick and place task is illustrated in Table 2. The user preference columns are:\n1https://youtu.be/yzH8gu_bEW0\n\u2022 duration error (s): the average error in duration of execution of the trajectory in seconds. \u2022 position error (cm): the average difference between the\ndesired position of the box inside the shelf and the position resulting from the autonomous execution of the task in centimeters. \u2022 position error (%): The percentile position error which\nis the position error divided by the width of the table (200cm).\nFeedforward network. The feedforward network with a Markov assumption was not able to complete the tasks even once. In the pick and place task, it learns to follow the box and sometimes close the gripper, however, it stops there and does not continue towards the shelf. The reason might be that the user usually pauses after closing the gripper to see if the grasp is successful or not. Since the gripper does not move for a few waypoints and then continues to move, the memoryless network fails to determine when the gripper should stop and when it should continue towards the shelf.\nRecurrent networks. The LSTM-2 network learns to successfully perform the tasks in most of the cases. It also learns to recover from failure; for instance, when the grasp fails in the pick and place task, the gripper does not continue towards the shelf without the box, instead it tries the grasp one more time. Similarly, in the pushing task, when the force to the big box was not enough to put it into the desired location, it tries again. These behaviors are learned by the model based on the performance of the user in similar situations.\nThe performance of LSTM-4 and GRU-2 networks are worse than the LSTM-2 network. They could not learn the pushing task, however, they learned the pick and place task. Note that the difference in the loss among the networks is minor, however, the performance is very different. This is because the error in generating one waypoint will be propagated to the future waypoints.\nWe found the LSTM-2 network to be especially good at generating trajectories based on user preferences. The gripper moves towards the specific location inside the shelf and does not release the box even when it is already inside the shelf until it reaches the desired location. In addition, to finish the task on the expected time, the gripper waits for a couple of seconds and makes mistakes to finish later, or performs the trajectory as fast as possible to finish sooner.\nOne of the reasons behind the better performance of the networks in the second task compared to the first task might be shifting the data in time and space in the second task that mitigates overfitting. This indicates that the networks are capable of modeling complex tasks, however, enough data is necessary to properly learn the task. This is the case especially when the network needs to learn everything from the scratch including the geometry of the space, speed, etc."}, {"heading": "6 Discussion", "text": "Deterministic nature of the model. The proposed model is deterministic, which means that at each state, only one action can be taken. However, in some tasks there are different solutions from one state. As a simplified example consider the\ntask of pushing a box to northeast, there are two solutions: 1) first pushing it to north and then east 2) first pushing it to east and then north. Therefore, the model might take the mean of these paths and push the corner of the box to northeast which is not a right solution. In these kind of tasks, the proposed model fails to learn properly to accomplish the task. Such failures point to the necessity to integrate higher level decision making into the process as the tasks become more complex. Extension to the real world environment. The proposed model can be trained using the data gathered from the simulation. For using the pre-trained method in real world, however, the pose of objects involved in the manipulation need to be captured by a vision module and fed into the model. Object pose estimation is a well-studied problem in computer vision and beyond the scope of this paper.\nWe preferred to control the end-effector instead of the arm to make the learned trajectory transferable to many robots with two finger grippers. In addition, we limited the movement range of the gripper to account for the limitations of wheelchair mounted robotic arms. Moreover, controlling the end-effector in 3D space is more intuitive and convenient for a human compared to controlling the joints of a high degrees of freedom robotic arm. The end-effector trajectory can be converted to joint space of the real robot for execution (e.g. [Pastor et al., 2009]) Capturing long range dependencies while maintaining\nprecision. Although recurrent neural networks are capable of capturing long range patterns in the sequence, this ability is not unlimited. According to our tests, if we record the data at a large rate (e.g. 30Hz), the network cannot learn the task efficiently. This is mainly because the dependencies in the sequence occur too far away apart from each other; therefore, the network cannot capture the pattern. In order to overcome this problem, we need to skip the recorded waypoints or use a smaller recording rate to make it easier for the network to learn the task. However, this will be achieved at the expense of reduction in the precision; thus, the generated trajectory might not be as smooth as the demonstration."}, {"heading": "7 Conclusions", "text": "In this paper we described an approach through which an assistive robot can learn how to perform autonomously certain manipulation trajectories encountered in ADLs. The approach is based on teaching a neural network to generate the trajectory using demonstration sequences. As the networks require a relatively large number of demonstrations, we developed a virtual reality environment in which the demonstrations can be done more quickly and safely. We found that recurrent neural networks were able to learn the two tasks (push object and pick-and-place respectively) with a 2-layer LTSM network performing the best. On the other hand, we found that a feed-forward neural network, with its implicit Markov assumption is unable to learn any of these tasks."}], "references": [{"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["Pieter Abbeel", "Adam Coates", "Andrew Y Ng"], "venue": "International Journal of Robotics Research (IJRR),", "citeRegEx": "Abbeel et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Robotics and autonomous systems", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning. A survey of robot learning from demonstration"], "venue": "57(5):469\u2013483,", "citeRegEx": "Argall et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Bakebot: Baking cookies with the PR2", "author": ["Mario Bollini", "Jennifer Barry", "Daniela Rus"], "venue": "IROS PR2 workshop: results, challenges and lessons learned in advancing robots with a common platform,", "citeRegEx": "Bollini et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Florent D\u2019halluin", "author": ["Sylvain Calinon"], "venue": "Eric L Sauser, Darwin G Caldwell, and Aude G Billard. Learning and reproduction of gestures by imitation. IEEE Robotics & Automation Magazine, 17(2):44\u201354,", "citeRegEx": "Calinon et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 339\u2013346", "author": ["Christopher Crick", "Sarah Osentoski", "Graylin Jay", "Odest Chadwicke Jenkins. Human", "robot perception in large-scale learning from demonstration. In International conference on Human-robot interaction"], "venue": "ACM,", "citeRegEx": "Crick et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al", "2014] Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "author": ["Felix Endres", "Jeff Trinkle", "Wolfram Burgard. Learning the dynamics of doors for robotic manipulation"], "venue": "pages 3543\u20133549,", "citeRegEx": "Endres et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Robot programming by demonstration with crowdsourced action fixes", "author": ["Maxwell Forbes", "Michael Jae-Yoon Chung", "Maya Cakmak", "Rajesh PN Rao"], "venue": "Second AAAI Conference on Human Computation and Crowdsourcing,", "citeRegEx": "Forbes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves et al", "2009] Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Speech and Signal Processing (ICASSP)", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In IEEE International Conference on Acoustics"], "venue": "pages 6645\u20136649,", "citeRegEx": "Graves et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li"], "venue": "arXiv preprint arXiv:1506.02078,", "citeRegEx": "Karpathy et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In IEEE International Conference on Robotics and Automation (ICRA)", "author": ["Ben Kehoe", "Akihiro Matsukawa", "Sal Candido", "James Kuffner", "Ken Goldberg. Cloud-based robot grasping with the Google object recognition engine"], "venue": "pages 4263\u20134270,", "citeRegEx": "Kehoe et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In International Joint Conference on Artificial Intelligence (IJCAI)", "author": ["Jens Kober", "Erhan Oztop", "Jan Peters. Reinforcement learning to adjust robot movements to new situations"], "venue": "volume 22, page 2650,", "citeRegEx": "Kober et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "The International Journal of Robotics Research", "author": ["Stephen Miller", "Jur Van Den Berg", "Mario Fritz", "Trevor Darrell", "Ken Goldberg", "Pieter Abbeel. A geometric approach to robotic laundry folding"], "venue": "31(2):249\u2013267,", "citeRegEx": "Miller et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In IEEE International Conference on Robotics and Automation (ICRA)", "author": ["Peter Pastor", "Heiko Hoffmann", "Tamim Asfour", "Stefan Schaal. Learning", "generalization of motor skills by learning from demonstration"], "venue": "pages 763\u2013768,", "citeRegEx": "Pastor et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Robobarista: Object part-based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["Jaeyong Sung", "Seok Hyun Jin", "Ashutosh Saxena"], "venue": "International Symposium on Robotics Research (ISRR),", "citeRegEx": "Sung et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems (NIPS)", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le. Sequence to sequence learning with neural networks"], "venue": "pages 3104\u20133112,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton", "2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "as solving problems of object identification combined with problems in planning and control theory [Endres et al., 2013; Miller et al., 2012; Bollini et al., 2011].", "startOffset": 99, "endOffset": 163}, {"referenceID": 16, "context": "as solving problems of object identification combined with problems in planning and control theory [Endres et al., 2013; Miller et al., 2012; Bollini et al., 2011].", "startOffset": 99, "endOffset": 163}, {"referenceID": 2, "context": "as solving problems of object identification combined with problems in planning and control theory [Endres et al., 2013; Miller et al., 2012; Bollini et al., 2011].", "startOffset": 99, "endOffset": 163}, {"referenceID": 11, "context": "Recurrent neural networks, and especially Long Short Term Memory (LSTM) [Hochreiter and Schmidhuber, 1997] with gating mechanism, had been proved to be effective in many sequence learning tasks.", "startOffset": 72, "endOffset": 106}, {"referenceID": 12, "context": "For instance, in character-level text generation [Karpathy et al., 2015], they can learn to close a parenthesis that had been opened earlier.", "startOffset": 49, "endOffset": 72}, {"referenceID": 12, "context": ", 2009], language modeling [Karpathy et al., 2015], machine translation [Sutskever et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 19, "context": ", 2015], machine translation [Sutskever et al., 2014], speech recognition [Graves et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 10, "context": ", 2014], speech recognition [Graves et al., 2013], visual recognition [Donahue et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 21, "context": ", 2014], and image captioning [Vinyals et al., 2015].", "startOffset": 30, "endOffset": 52}, {"referenceID": 11, "context": "Newer RNN models feature explicit gating mechanism [Hochreiter and Schmidhuber, 1997] that helped in storing and retrieving information over long time periods.", "startOffset": 51, "endOffset": 85}, {"referenceID": 4, "context": "In recent years, similar mechanisms were proposed such as Gated Recurrent Units (GRU) [Cho et al., 2014].", "startOffset": 86, "endOffset": 104}, {"referenceID": 1, "context": "One of the most effective approaches for teaching robots to execute a desired task is Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings [Argall et al., 2009].", "startOffset": 194, "endOffset": 215}, {"referenceID": 0, "context": "of successful application include autonomous helicopter maneuvers [Abbeel et al., 2010], playing table tennis [Kober et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 14, "context": ", 2010], playing table tennis [Kober et al., 2011][Calinon et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 3, "context": ", 2011][Calinon et al., 2010], object manipulation [Pastor et al.", "startOffset": 7, "endOffset": 29}, {"referenceID": 17, "context": ", 2010], object manipulation [Pastor et al., 2009], and making coffee [Sung et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 18, "context": ", 2009], and making coffee [Sung et al., 2015].", "startOffset": 27, "endOffset": 46}, {"referenceID": 13, "context": "enough data for the task learning, some researchers proposed to use cloud-based and crowdsourced data collection techniques [Kehoe et al., 2013], [Forbes et al.", "startOffset": 124, "endOffset": 144}, {"referenceID": 8, "context": ", 2013], [Forbes et al., 2014], [Crick et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 5, "context": ", 2014], [Crick et al., 2011].", "startOffset": 9, "endOffset": 29}, {"referenceID": 15, "context": "For instance, [Levine et al., 2015] utilized feed-forward neural networks to map robot\u2019s visual input to control commands.", "startOffset": 14, "endOffset": 35}, {"referenceID": 11, "context": "The recurrent networks we use in this paper are LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al.", "startOffset": 53, "endOffset": 87}, {"referenceID": 4, "context": "The recurrent networks we use in this paper are LSTM [Hochreiter and Schmidhuber, 1997] and GRU [Cho et al., 2014] that follow the general formulation explained above.", "startOffset": 96, "endOffset": 114}, {"referenceID": 19, "context": "08 following the recommendation by [Sutskever et al., 2014].", "startOffset": 35, "endOffset": 59}, {"referenceID": 17, "context": "[Pastor et al., 2009])", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Robots assisting disabled or elderly people in the performance of activities of daily living need to perform complex manipulation tasks which are highly dependent on the environment and preferences of the user. In addition, these environments and users are not suitable for the collection of massive amounts of training data, as the manipulated objects can be fragile, and the wheelchair-bound users might have difficulty recovering from a failed manipulation task. In this paper, we propose an end-to-end learning mechanism for the type of complex robot arm trajectories used in manipulation tasks for assistive robots. The trajectory is learned using a recurrent neural network that can generate the trajectory in real-time based on the current situation of the end-effector, the objects in the environment and the preferences of the user. The learning data is acquired from a simulation environment where the human can demonstrate the task in a simulation closely modeling his or her own environment. Experiments using two different manipulation tasks show that the robot can learn the manipulation planning as well the ability to recover from failure.", "creator": "LaTeX with hyperref package"}}}