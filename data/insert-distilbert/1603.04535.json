{"id": "1603.04535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Learning Domain-Invariant Subspace using Domain Features and Independence Maximization", "abstract": "when the distributions of filtering the source and using the target domains are different, domain adaptation techniques are needed. for example, in the field of sensors and measurement, discrete and relatively continuous distributional change methods often readily exist in data because of instrumental kernel variation and time - varying sensor drift. in this paper, we propose maximum independence domain adaptation ( hmm mida ) mechanism to basically address this problem. domain features are first defined to describe the local background - information of a sample, such precisely as the device label and actual acquisition time. then, mida learns specified features which have maximal independence interactions with the domain features, so as to reduce the inter - domain discrepancy in sampled distributions. a feature augmentation strategy is designed so that the learned projection is neither background - specific. robust semi - supervised sampling mida ( smida ) extends mida by exploiting the label information. the proposed estimation methods can handle completely not only discrete domains in traditional domain length adaptation problems but also continuous distributional change such as the time - varying drift. in addition, they are naturally applicable in supervised / semi - supervised / unsupervised classification or regression problems with multiple domains. this flexibility brings potential for a wide range of applications. the effectiveness of our empirical approaches is verified by experiments designed on synthetic datasets and four real - world ones on sensors, measurement, and augmented computer vision.", "histories": [["v1", "Tue, 15 Mar 2016 02:56:22 GMT  (717kb,D)", "http://arxiv.org/abs/1603.04535v1", "13 pages, 9 figures, 6 tables"], ["v2", "Thu, 22 Jun 2017 01:39:22 GMT  (2544kb,D)", "http://arxiv.org/abs/1603.04535v2", "Accepted"]], "COMMENTS": "13 pages, 9 figures, 6 tables", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ke yan", "lu kou", "david zhang"], "accepted": false, "id": "1603.04535"}, "pdf": {"name": "1603.04535.pdf", "metadata": {"source": "CRF", "title": "Domain Adaptation via Maximum Independence of Domain Features", "authors": ["Ke Yan", "Lu Kou"], "emails": ["yank10@mails.tsinghua.edu.cn).", "cslkou@comp.polyu.edu.hk).", "csdzhang@comp.polyu.edu.hk)."], "sections": [{"heading": null, "text": "Index Terms\u2014Dimensionality reduction, domain adaptation, drift correction, Hilbert-Schmidt independence criterion, machine olfaction, transfer learning\nI. INTRODUCTION\nDOMAIN adaptation techniques are useful when the la-beled training data are from a source domain and the test ones are from a target domain. Samples of the two domains are collected under different conditions, thus have different distributions. Labeling samples in the target domain to develop new prediction models is often labor-intensive and time-consuming. Therefore, domain adaptation or transfer learning is needed to improve the performance in the target domain by leveraging unlabeled (and maybe a few labeled) target samples [1]. This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6]. It is also\nThe work is partially supported by the GRF fund from the HKSAR Government, the central fund from Hong Kong Polytechnic University, the NSFC fund (61332011, 61272292, 61271344), Shenzhen Fundamental Research fund (JCYJ20150403161923528, JCYJ20140508160910917), and Key Laboratory of Network Oriented Intelligent Computation, Shenzhen, China.\nK. Yan is with the Department of Electronic Engineering, Graduate School at Shenzhen, Tsinghua University, Shenzhen 518055, China (e-mail: yank10@mails.tsinghua.edu.cn).\nL. Kou is with the Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong (e-mail: cslkou@comp.polyu.edu.hk).\nD. Zhang is with the Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen 518055, China, and also with the Department of Computing, Biometrics Research Centre, The Hong Kong Polytechnic University, Kowloon, Hong Kong (e-mail: csdzhang@comp.polyu.edu.hk).\nimportant in the field of sensors and measurement. Because of the variations in the fabrication of sensors and devices, the responses to the same signal source may not be identical for different instruments. Furthermore, the sensing characteristics of the sensors, the operating condition, or even the signal source itself, can change over time. As a result, the prediction model trained with the samples from the initial device in an earlier time period (source domain) is not suitable for new devices or in a latter time (target domains).\nA typical application plagued by this problem is machine olfaction, which uses electronic noses (e-noses) and pattern recognition algorithms to predict the type and concentration of odors [7]. The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11]. However, owing to the nature of chemical sensors, many e-noses are prone to instrumental variation and time-varying drift mentioned above [12], [13], which greatly hamper their usage in real-world applications. Traditional methods for calibration transfer (compensating instrumental variation) and drift correction (compensating time-varying drift) require a set of predefined gas samples as transfer samples. They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15]. Then, a widely-used method is to map the features in the target domain to the source domain with regression algorithms [10], [14]. Nevertheless, collecting transfer samples is demanding for non-professional e-nose users because standard gases need to be prepared and much effort has to be made.\nIn such cases, domain adaptation techniques with unlabeled target samples are desirable. An intuitive idea is to reduce the difference of distributions among domains in the feature level, i.e. to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19]. For example, Pan et al. [5] proposed transfer component analysis (TCA), which finds a latent feature space that minimizes the difference of distributions between two domains in the sense of maximum mean discrepancy. More related methods will be introduced in Section II. When applied to calibration transfer and drift correction, however, existing domain adaptation algorithms are faced with two difficulties. First, they are designed to handle discrete source and target domains. In time-varying drift, samples come in a stream, so the change in data distribution is often continuous. One solution is to split data into several batches, but it will lost the temporal order information. Second, because of the variation in the sensitivity of chemical sensors, the same signal in different conditions may indicate different ar X\niv :1\n60 3.\n04 53\n5v 1\n[ cs\n.C V\n] 1\n5 M\nar 2\n01 6\nconcepts. In other words, the conditional probability P (Y |X) may change for samples with different backgrounds, where \u201cbackground\u201d means when and with which device a sample was collected. Methods like TCA project all samples to a common subspace, hence the samples with similar appearance but different concepts cannot be distinguished.\nIn this paper, we present a simple yet effective algorithm called maximum independence domain adaptation (MIDA). The algorithm first defines \u201cdomain features\u201d for each sample to describe its background. Then, it finds a latent feature space in which the samples and their domain features are maximally independent in the sense of Hilbert-Schmidt independence criterion (HSIC) [20]. Thus, the discrete and continuous change in distribution can be handled uniformly. In order to project samples according to their backgrounds, feature augmentation is performed by concatenating the original feature vector with the domain features. We also propose semi-supervised MIDA (SMIDA) to exploit the label information with HSIC. MIDA and SMIDA are both very flexible. (1) They can be applied in situations with single or multiple source or target domains thanks to the use of domain features. In fact, the notion \u201cdomain\u201d has been extended to \u201cbackground\u201d which can carry more information about the background of a sample. (2) Although they are designed for unsupervised domain adaptation problems (no labeled sample in target domains), the proposed methods naturally allow both unlabeled and labeled samples in any domains, thus can be applied in semi-supervised (both unlabeled and labeled samples in target domains) and supervised (only labeled samples in target domains) problems as well. (3) The label information can be either discrete (binaryor multi-class classification) or continuous (regression). This advantage is inherited from HSIC.\nTo illustrate the effect of our algorithms, we first evaluate them on several synthetic datasets. Then, calibration transfer and drift correction experiments are performed on two e-nose datasets and one spectroscopy dataset. Note that spectrometers suffer the same instrumental variation problem as e-noses [21]. Finally, a domain adaptation experiment is conducted on a well-known object recognition benchmark [22]. Results confirm the effectiveness of the proposed algorithms. The rest of the paper is organized as follows. Related work on unsupervised domain adaptation and HSIC is briefly reviewed in Section II. Section III describes domain features, MIDA, and SMIDA in detail. The experimental configurations and results are presented in Section IV, along with some discussions. Section V concludes the paper."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Unsupervised domain adaptation", "text": "Two good surveys on domain adaptation can be found in [1] and [2]. In this section, we focus on typical methods that extract domain-invariant features. Compared to modellevel adaptation methods, feature-level ones are easier to use because the extracted features can be applied to various prediction models. In order to reduce the difference of distributions among domains while preserving useful information, researchers have developed many strategies. Some algorithms\nproject all samples to a common latent space [5], [16], [19]. Transfer component analysis (TCA) [5] tries to learn transfer components across domains in a reproducing kernel Hilbert space (RKHS) using maximum mean discrepancy. It is further extended to semi-supervised TCA (SSTCA) to encode label information and preserve local geometry of the manifold. Shi et al. [16] measured domain difference by the mutual information between all samples and their binary domain labels, which can be viewed as a primitive version of the domain features used in this paper. They also minimized the negated mutual information between the target samples and their cluster labels to reduce the expected classification error. The low-rank transfer subspace learning (LTSL) algorithm presented in [19] is a reconstruction guided knowledge transfer method. It aligns source and target data by representing each target sample with some local combination of source samples in the projected subspace. The label and geometry information can be retained by embedding different subspace learning methods into LTSL.\nAnother class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3]. Fernando et al. [17] utilized a transformation matrix to map the source subspace to the target one, where a subspace was represented by eigenvectors of PCA. The geodesic flow kernel (GFK) method [22] measures the geometric distance between two different domains in a Grassmann manifold by constructing a geodesic flow. An infinite number of subspaces are combined along the flow in order to model a smooth change from the source to the target domain. Liu et al. [23] adapted GFK for drift correction of enoses. A sample stream is first split into batches according to the acquisition time. The first and the last batches (domains) are then connected through every intermediate batch using GFK. Another improvement of GFK is domain adaptation by shifting covariance (DASC) [3]. Observing that modeling one domain as a subspace is not sufficient to represent the difference of distributions, DASC characterizes domains as covariance matrices and interpolates them along the geodesic to bridge the domains."}, {"heading": "B. Hilbert-Schmidt independence criterion (HSIC)", "text": "HSIC is used as a convenient method to measure the dependence between two sample sets X and Y . Let kx and ky be two kernel functions associated with RKHSs F and G, respectively. pxy is the joint distribution. HSIC is defined as the square of the Hilbert-Schmidt norm of the cross-covariance operator Cxy [20]:\nHSIC(pxy,F ,G) = \u2016Cxy\u20162HS =Exx\u2032yy\u2032 [kx(x, x \u2032)ky(y, y \u2032)] +Exx\u2032 [kx(x, x \u2032)]Eyy\u2032 [ky(y, y \u2032)]\n\u2212 2Exy[Ex\u2032 [kx(x, x\u2032)]Ey\u2032 [ky(y, y\u2032)]].\nHere Exx\u2032yy\u2032 is the expectation over independent pairs (x, y) and (x\u2032, y\u2032) drawn from pxy . It can be proved that with characteristic kernels kx and ky , HSIC(pxy,F ,G) is zero if and only if x and y are independent [24]. A large HSIC suggests strong dependence with respect to the choice of kernels. HSIC has a biased empirical estimate. Suppose Z =\nX \u00d7 Y = {(x1, y1), . . . , (xn, yn)}, Kx,Ky \u2208 Rn\u00d7n are the kernel matrices of X and Y , respectively, then [20]:\nHSIC(Z,F ,G) = (n\u2212 1)\u22122tr(KxHKyH), (1)\nwhere H = I \u2212 n\u221211n1Tn \u2208 Rn\u00d7n is the centering matrix. Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24]. Researchers typically use it to maximize the dependence between the extracted/selected features and the label. However, to our knowledge, it has not been utilized in domain adaptation to reduce the dependence between the extracted features and the domain features."}, {"heading": "III. PROPOSED METHOD", "text": ""}, {"heading": "A. Domain Feature", "text": "We aim to reduce the dependence between the extracted features and the background information. A sample\u2019s background information should (1) naturally exists, thus can be easily obtained; (2) has different distributions in training and test samples; (3) correlates with the distribution of the original features. The domain label (which domain a sample belongs) in common domain adaptation problems is an example of such information. According to these characteristics, the information clearly interferes the testing performance of a prediction model. Thus, minimizing the aforementioned dependence is desirable. First, a group of new features need to be designed to describe the background information. The features are called \u201cdomain features\u201d. From the perspective of calibration transfer and drift correction, there are two main types of background information: the device label (with which device the sample was collected) and the acquisition time (when the sample was collected). We can actually encode more information such as the place of collection, the operation condition, and so on, which will be useful in other domain adaptation problems.\nFormally, if we only consider the instrumental variation, an one-hot coding scheme can be used. Suppose there are ndev devices, which result in ndev different but related domains. The domain feature vector is thus d \u2208 Rndev , where dp = 1 if the sample is from the pth device and 0 otherwise. This scheme also applies to traditional domain adaptation problems with several discrete domains. If the time-varying drift is also considered, the acquisition time can be further added. If a sample was collected from the pth device at time t, then d \u2208 R2ndev , where\ndq =  1, q = 2p\u2212 1, t, q = 2p,\n0, otherwise. (2)\nAccording to (1), the kernel matrix Kd of the domain features needs to be computed for HSIC. We apply the linear kernel. Suppose D = [d1, . . . ,dn] \u2208 Rmd\u00d7n, md is the dimension of a domain feature vector. Then\nKd = D TD. (3)\n(Kd)ij is 0 if samples i and j are from different devices; otherwise, it is 1 when time-varying drift is not considered, or 1 + titj when it is considered."}, {"heading": "B. Feature Augmentation", "text": "Feature augmentation is used in this paper to learn background-specific subspaces. In [27], the author proposed a feature augmentation strategy for domain adaptation: if a sample x \u2208 Rm is from the source domain, then its augmented\nfeature vector is x\u0302 =  xx 0m  \u2208 R3m; If it is from the target domain, then x\u0302 =\n x0m x  \u2208 R3m. The augmented labeled source and target samples are then used jointly to train one prediction model. In this way, the learned model can be viewed as two different models for the two domains. Meanwhile, the two models share a common component [27]. However, this strategy requires that data lie in discrete domains and cannot deal with time-varying drift. We propose a more general and efficient feature augmentation strategy: concatenating the original features and the domain features, i.e.\nx\u0302 = [ x d ] \u2208 Rm+md . (4)\nThe role of this strategy can be demonstrated through a linear dimensionality reduction example. Suppose a projection matrix W \u2208 R(m+md)\u00d7h has been learned for the augmented feature vector. h is the dimension of the subspace. W has two\nparts: W = [ Wx Wd ] ,Wx \u2208 Rm\u00d7h,Wd \u2208 Rmd\u00d7h. The embedding of x\u0302 can be expressed as WTx\u0302 =WTx x+W T d d \u2208 Rh, which means that a background-specific bias (WTd d)i has been added to each dimension i of the embedding. One may argue that a bias may not always be enough for alignment. It may be better to have domain-specific affine transformation matrices. However, with the absence of target labels, learning such matrices can be prone to overfitting. From another perspective, the feature augmentation strategy maps the samples to an augmented space with higher dimension before projecting them to a subspace. It will be easier to find a projection direction in the augmented space to align the samples well in the subspace. The effect of feature augmentation will be illustrated on several synthetic datasets in Section IV-A.\nTake machine olfaction for example, there are situations when the conditional probability P (Y |X) changes along with the background. For instance, the sensitivity of chemical sensors often decays over time. A signal that indicates low concentration in an earlier time actually suggests high concentration in a later time. In such cases, feature augmentation is important, because it allows samples with similar appearance but different concepts to be treated differently by the background-specific bias. The strategy also helps to align the domains better in each projected dimension."}, {"heading": "C. Maximum Independence Domain Adaptation (MIDA)", "text": "In this section, we introduce the formulation of MIDA in detail. Suppose X \u2208 Rm\u00d7n is the matrix of n samples. The training and the test samples are pooled together. More importantly, we do not have to explicitly differentiate which domain a sample is from. The feature vectors has been\naugmented, but we use the notations X and m instead of X\u0302 and m + md for brevity. A linear or nonlinear mapping function \u03a6 can be used to map X to a new space. Based on the kernel trick, we need not know the exact form of \u03a6, but the inner product of \u03a6(X) can be represented by the kernel matrix Kx = \u03a6(X)T\u03a6(X). Then, a projection matrix W\u0303 is applied to project \u03a6(X) to a subspace with dimension h, leading to the projected samples Z = W\u0303T\u03a6(X) \u2208 Rh\u00d7n. Similar to other kernel dimensionality reduction algorithms [28], [29], the key idea is to express each projection direction as a linear combination of all samples in the space, namely W\u0303 = \u03a6(X)W . W \u2208 Rn\u00d7h is the projection matrix to be actually learned. Thus, the projected samples are\nZ =WT\u03a6(X)T\u03a6(X) =WTKx (5)\nwith the kernel matrix\nKz = KxWW TKx. (6)\nIntuitively, if the projected features are independent of the domain features, then we cannot distinguish the background of a sample by its projected features, suggesting that the difference of distributions among domains is diminished in the subspace. Therefore, by substituting (6) into the empirical HSIC (1) and omit the scaling factor, we get the expression to be minimized: tr(KzHKdH) = tr(KxWWTKxHKdH).\nIn domain adaptation, the goal is not only minimizing the difference of distributions, but also preserving important properties of data, such as the variance [5]. It can be achieved by maximizing the trace of the covariance matrix of the project samples. The covariance matrix is\ncov(Z) = cov(WTKx)\n= 1\nn (WTKx \u2212\n1 n WTKx1n1 T n )(W TKx \u2212 1 n WTKx1n1 T n ) T\n=WTKxHKxW, (7)\nwhere H = I \u2212 n\u221211n1Tn is the same as that in (1). An orthonormal constraint is further added on W . The learning problem then becomes\nmax W\n\u2212 tr(WTKxHKdHKxW ) + \u00b5 tr(WTKxHKxW ),\ns.t. WTW = I, (8)\nwhere \u00b5 > 0 is a trade-off hyper-parameter. To solve (8), we can use its Lagrangian:\ntr(WTKx(\u2212HKdH + \u00b5H)KxW )\u2212 tr((WTW \u2212 I)\u039b), (9)\nwhere \u039b is a matrix containing the Lagrange multipliers. Setting the derivative of (9) with respect to W to zero, we get\nKx(\u2212HKdH + \u00b5H)KxW =W\u039b. (10)\nConsequently, W is the eigenvectors of Kx(\u2212HKdH + \u00b5H)Kx corresponding to the h largest eigenvalues. Note that a conventional constraint is requiring W\u0303 to be orthonormal as in [26], which will lead to a generalized eigenvector problem. However, we find that this strategy is inferior to the proposed\none in both adaptation accuracy and training speed in practice, so it is not used.\nWhen computing Kx, a proper kernel function needs to be selected. Common kernel functions include linear (k(x,y) = xyT), polynomial (k(x,y) = (\u03c3xyT + 1)d), Gaussian radial basis function (RBF, k(x,y) = exp(\u2016x\u2212y\u2016 2\n2\u03c32 )), and so on. Different kernels indicate different assumptions on the type of dependence in using HSIC [24]. According to [24], the polynomial and RBF kernels map the original features to a higher or infinite dimensional space, thus are able to detect more types of dependence. However, choosing a suitable kernel width parameter (\u03c3) is also important for these more powerful kernels [24]. When looking for a linear projection matrix to align the domains, first mapping the original features to a higher dimensional space may also be helpful, see the experiment on a synthetic dataset (Section IV-A, Fig. 4) for an example.\nThe maximum mean discrepancy (MMD) criterion is used in TCA [5] to measure the difference between two distributions. Song et al. [24] showed that when HSIC and MMD are both applied to measure the dependence of features and labels in a binary-class classification problem, they are identical up to a constant factor if the label kernel matrix in HSIC is properly designed. However, TCA is feasible only when there are two discrete domains. On the other hand, MIDA can deal with a variety of situations including multiple domains and continuous distributional change. The stationary subspace analysis (SSA) algorithm [30] is able to identify temporally stationary components in multivariate time series. However, SSA only ensures that the mean and covariance of the components are stationary, while they may not be suitable for preserving important properties in data. Concept drift adaptation algorithms [31] are able to correct continuous time-varying drift. However, most of them rely on newly arrived labeled data to update the prediction models, while MIDA works unsupervisedly."}, {"heading": "D. Semi-supervised MIDA (SMIDA)", "text": "MIDA aligns samples with different backgrounds without considering the label information. However, if the labels of some samples are known, they can be incorporated into the subspace learning process, which may be beneficial for prediction. Therefore, we extend MIDA to semi-supervised MIDA (SMIDA). Since we do not explicitly differentiate the domain labels of the samples, both unlabeled and labeled samples can exist in any domain. Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels. The biggest advantage of this strategy is that all types of labels can be exploited, such as the discrete labels in classification and the continuous ones in regression.\nThe label matrix Y is defined as follows. For c-class classification problems, the one-hot coding scheme can be used, i.e. Y \u2208 Rn\u00d7c, yi,j = 1 if xi is labeled and belongs to the jth class; 0 otherwise. For regression problems, the target values can be centered first. Then, Y \u2208 Rn, yi equals\nto the target value of xi if it is labeled; 0 otherwise. The linear kernel function is chosen for the label kernel matrix, i.e.\nKy = Y Y T. (11)\nThe objective of SMIDA is\nmax W\ntr(WTKx(\u2212HKdH + \u00b5H + \u03b3HKyH)KxW ),\ns.t. WTW = I, (12)\nwhere \u03b3 > 0 is a trade-off hyper-parameter. Its solution is the eigenvectors of Kx(\u2212HKdH + \u00b5H + \u03b3HKyH)Kx corresponding to the h largest eigenvalues. The outline of MIDA and SMIDA is summarized in Algorithm III.1. The statements in brackets correspond to those specialized for SMIDA.\nAlgorithm III.1 MIDA [or SMIDA] Input: The matrix of all samples X and their background\ninformation; [the labels of some samples]; the kernel function for X; h, \u00b5, [and \u03b3]. Output: The projected samples Z. 1: Construct the domain features according to the back-\nground information, e.g. Section III-A. 2: Augment the original features with the domain features\n(4). 3: Compute the kernel matrices Kx,Kd (3), [and Ky (11)]. 4: Obtain W , namely the eigenvectors of Kx(\u2212HKdH + \u00b5H)Kx [or Kx(\u2212HKdH + \u00b5H + \u03b3HKyH)Kx] corresponding to the h largest eigenvalues. 5: Z =WTKx.\nBesides variance and label dependence, another useful property of data is the geometry structure, which can be preserved by manifold regularization (MR) [32]. The manifold structure is modeled by a data adjacency graph. MR can be conveniently incorporated into SMIDA by adding a regularizer \u2212\u03bb tr(WTKxLKxW ) into (12), where L is the graph Laplacian matrix [32], \u03bb > 0 is a trade-off hyperparameter. In our experiments, adding MR generally increases the accuracy slightly. However, It also brings three more hyperparameters, including \u03bb, the number of nearest neighbors, and the kernel width when computing the data adjacency graph. Consequently, the experimental results in the next section were obtained without MR. It can still be an option in applications where geometry structure is important."}, {"heading": "IV. EXPERIMENTS", "text": "In this section, we first conduct experiments on some synthetic datasets to verify the effect of the proposed methods. Then, calibration transfer and drift correction experiments are performed on two e-nose datasets and a spectroscopy dataset. To show the universality of the proposed methods, we further evaluate them on a visual object recognition dataset. Comparison is made between them and recent unsupervised feature-level domain adaptation algorithms."}, {"heading": "A. Synthetic Dataset", "text": "In Fig. 1, TCA [5] and MIDA are compared on a 2D dataset with two discrete domains. For both methods, the linear kernel was used on the original features and the hyper-parameter \u00b5 was set to 1. In order to quantitatively assessing the effect of domain adaptation, logistic regression models were trained on the labeled source data and tested on the target data. The accuracies are displayed in the caption, showing that the order of performance is MIDA > TCA > original feature. TCA aligns the two domains only on the first projected dimension. However, we can find that the two classes have large overlap on that dimension. This is because the direction for alignment is different from that for discrimination. Incorporating the label information of the source domain (SSTCA) did no help. On the contrary, MIDA can align the two domains well in both projected dimensions, in which the domain-specific bias on the second dimension brought by feature augmentation played a key role. Thus, good accuracy can be obtained by using the two dimensions for classification.\nIn Fig. 2, SSA [30] and MIDA are compared on a 2D dataset with continuous distributional change, which resembles time-varying drift in machine olfaction. The chronological order of a sample is indicated by color ranging from blue to red. Samples in both classes drift to the upper right. The parameter setting of MIDA was the same with those in Fig. 1, whereas the number of stationary components in SSA was set to 1. The classification accuracies were obtained by training a logistic regression model on the first halves of the data in both classes, and testing them on the last halves. SSA succeeds in finding a direction (z1) that is free from timevarying drift (z2). However, the two classes cannot be well separated in that direction. In plot (c), the randomly scattered colors suggest that the time-varying drift is totally removed in the subspace. MIDA first mapped the 2D data into a 3D space with the third dimension being time. Then, the augmented data were projected to a 2D plane that is orthogonal to the direction of drift in the 3D space. The projection direction was decided so that the independence of the projected data and time is maximized, meanwhile class separation was achieved by properly exploiting the background information.\nNo label information was used in the last two experiments. If keeping the label dependence in the subspace is a priority, SMIDA can be adopted instead of MIDA. In the 3D synthetic dataset in Fig. 3, the best direction to align the two domains (x3) also mixes the two classes, which results in the output of MIDA in plot (b). For SMIDA, the weights for variance (\u00b5) and label dependence (\u03b3) were both set to 1. The labels in the source domain were used when learning the subspace. From plot (c), we can observe that the classes are separated. In fact, class separation can still be found in the third dimension of the space learned by MIDA. However, for the purpose of dimensionality reduction, we generally hope to keep the important information in the first few dimensions.\nNonlinear kernels are often applied in machine learning algorithms when data is not linearly separable. Besides, they are also useful in domain adaptation when domains are not linearly \u201calignable\u201d, as shown in Fig. 4. As can be found in\nplot (a), the inter-domain changes in distributions are different for the two classes. Hence, it is difficult to find a linear projection direction to align the two domains, even with the domain-specific biases of MIDA. Actually, domain-specific rotation matrices are needed. Since the target labels are not available, the rotation matrices cannot be obtained accurately. However, a nonlinear kernel can be used to map the original features to a space with higher dimensions, in which the domains may be linearly alignable. We applied an RBF kernel with width \u03c3 = 10. Although the domains are not perfectly aligned in plot (c), the classification model trained in the source domain can be better adapted to the target domain."}, {"heading": "B. Gas Sensor Array Drift Dataset", "text": "The gas sensor array drift dataset1 collected by Vergara et al. [33] is dedicated to research in drift correction. A total of 13910 samples were collected by an e-nose with 16 gas sensors over a course of 36 months. There are six different kinds of gases (ammonia, acetaldehyde, acetone, ethylene, ethanol, and toluene) at different concentrations. They were split into 10 batches by the authors according to their acquisition time. Table I shows the period of collection and the number of samples of different type of gases in each batch. We aim to classify the type of gases, despite their concentrations.\n1 http://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset+at+Different+Concentrations\nA strategy used in previous literatures [33], [23] was also adopted in this paper so that the performance of the domain adaptation algorithms can be evaluated. We took the samples in batch 1 as labeled training samples, whereas those in batches 2\u201310 are unlabeled test ones. This evaluation strategy also resembles the situation in real-world applications. In the dataset, each sample is represented by 128 features extracted from the sensors\u2019 response curves [33]. The original features have quite different dynamic ranges, which will interfere the learning process. Therefore, each feature was first normalized to have zero mean and unit variance within each batch. The time-varying drift of the preprocessed features across batches can be visually inspected in Fig. 5. It is obvious that samples in different batches have different distributions. Next, the labeled samples in batch 1 were adopted as the source domain and the unlabeled ones in batch b (b = 2, . . . , 10) as the target domain. The proposed algorithms together with several recent ones were used to learn domain-invariant features based on these samples. Then, a logistic regression model was trained on the source domain and tested on each target one. For multiclass classification, the one-vs-all strategy was utilized.\nAs displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16]. For all methods, the hyperparameters were tuned for the best accuracy. In KPCA, TCA, SSTCA, and the proposed MIDA and SMIDA, the polynomial kernel with degree 2 was used. KPCA learned a subspace\nbased on the union of source and target data. In TCA, SSTCA, MIDA, and SMIDA, eigenvalue decomposition needs to be done on kernel matrices. If the number of samples is too large, this step can be time-consuming. In order to reduce the computational burden, we randomly chose at most nt samples in each target domain when using these methods, with nt being twice the number of the samples in the source domain. GFK\nused PCA to generate the subspaces in both source and target domains. The subspace dimension of GFK was determined according to the subspace disagreement measure in [22]. The results of ML-comGFK are copied from [23].\nWe also compared several variants of our methods. In Table II, the notation \u201c(discrete)\u201d means that two discrete domains (source and target) were used in MIDA and SMIDA, which is similar to other compared methods. The domain feature vector of a sample was thus [1, 0]T if it is from the source domain and [0, 1]T if it is from the target. However, this strategy cannot make use of the samples in intermediate batches. An intuitive assumption is that the distributions of adjacent batches should be similar. When adapting the information from batch 1 to b, taking samples from batches 2 to b \u2212 1 into consideration may improve the generalization ability of the learned subspace. Concretely, nt samples were randomly selected from batches 2 to b instead of batch b alone. For each sample, the domain feature was defined as its batch index, which can be viewed as a proxy of its acquisition time. MIDA and SMIDA then maximized the independence between the learned subspace and the batch indices. The results are labeled as \u201c(continuous)\u201d in Table II. Besides, the accuracies of continuous SMIDA without feature augmentation (no aug.) are also shown.\nFrom Table II, we can find that as the batch index increases, the accuracies of all methods generally degrade, which confirms the influence of the time-varying drift. Continuous SMIDA achieves the best average domain adaptation accuracy. The continuous versions of MIDA and SMIDA outperform the discrete versions, proving that the proposed methods can effectively exploit the chronological information of the samples. They also surpass ML-comGFK which uses the samples in intermediate batches to build connections between the source and the target batches. Feature augmentation is important in this dataset, since removing it in continuous SMIDA causes a drop of four percentage points in average accuracy. In Fig. 6, the average classification accuracies with varying subspace dimension are shown. MIDA and SMIDA are better than other methods when more than 30 features are extracted."}, {"heading": "C. Breath Analysis Dataset", "text": "As a noninvasive approach, disease screening and monitoring with e-noses is attracting more and more attention [8], [11]. The concentration of some biomarkers in breath has been proved to be related with certain diseases, which makes it possible to analyze a person\u2019s health state with an e-nose conveniently. For example, the concentration of acetone in diabetics\u2019 breath is often higher than that in healthy people [11]. However, the instrumental variation and time-varying drift of e-noses hinder the popularization of this technology in real-world applications. Most traditional calibration transfer and drift correction algorithms require transfer samples to be collected in each new device and time period, which is a demanding job, especially for non-professional e-nose users. When the devices are used for breath sample collection in communities for disease screening, breath samples are easy to acquire, but not their labels. Therefore, unsupervised domain adaptation algorithms are necessary.\nWe have collected a breath analysis dataset in years 2014\u2013 2015 using two e-noses of the same model [11]. In this paper, samples of five diseases were selected for experiments, including diabetes, chronical kidney disease (CKD), cardiopathy, lung cancer, and breast cancer. They have been proved to be related with certain breath biomarkers. We performed five binary-class classification tasks to distinguish samples with one disease from the healthy samples. Each sample was represented by the steady state responses of nine gas sensors in the e-nose. When a gas sensor is used to sense a gas sample, its response will reach a steady state in a few minutes. The steady state response has a close relationship with the concentration of the measured gas. Therefore, the 9D feature vector contains most information needed for disease screening.\nTo show the instrumental variation and time-varying drift in the dataset, we draw the steady state responses of two sensors of the CKD samples in Fig. 7. Each data point indicates a breath sample. In plot (a), the sensitivity of the sensor in both devices gradually decayed as time elapsed. In plot (b), the aging effect was so significant that we had to replace the sensors in the two devices with new ones on about day 200. In this case, a signal at 0.3 V will suggest low concentration on day 0 and 250 but high concentration in day 150 and 450. In addition, the responses in different devices are different (e.g. plot (b), after day 200).\nThe numbers of samples in the six classes (healthy and the five diseases mentioned above) are 125, 431, 340, 97, 156, and 215, respectively. We chose the first 50 samples collected with device 1 in each class as labeled training samples. Among the other samples, 10 samples were randomly selected in each class for validation, the rest for testing. The hyper-parameters were tuned in the validation sets. Logistic regression was adopted as the classifier. Because of the difference in the number of samples in each class, the F-score was used as the accuracy criterion. Results are compared in Table III.\nIn KPCA, TCA, SSTCA, MIDA, and SMIDA, the RBF kernel was used. Because methods other than stationary subspace analysis (SSA) [30], MIDA, and SMIDA are not capable of handling the chronological information, we simply regarded each device as a discrete domain and learned device-invariant features with them. The same strategy was used in discrete MIDA and SMIDA. In continuous MIDA and SMIDA, the domain features were defined according to (4), where t was the exact acquisition time converted to years and the number of devices ndev = 2. SSA naturally considers the chronological information by treating the sample stream as a multivariate time series and identifying temporally stationary components. However, SSA cannot deal with time series with multiple sources, such as the multi-device case in this dataset. Thus,\nthe samples were arranged in chronological order despite their device labels.\nFrom Table III, we can find that the performance of the original features is not promising, which is caused by the instrumental variation and time-varying drift in the dataset. The domain adaptation algorithms can improve the accuracy. The improvement made by SSA is little, possibly because the stationary criterion is not suitable for preserving important properties in data. For example, the noise in data can also be stationary [5]. MIDA and SMIDA achieved obviously better results than other methods. They can address both instrumental variation and time-varying drift. With the background-specific bias brought by feature augmentation, they can compensate for the change in conditional probability in this dataset. Similar to the gas sensor array drift dataset, it can be seen that the continuous MIDA and SMIDA that utilize the time information are better than the discrete ones. Feature augmentation can\nimprove continuous SMIDA by six percentage points. SMIDA is better than MIDA because the label information of the first 50 samples in each class was better kept."}, {"heading": "D. Corn Dataset", "text": "Similar to e-noses, data collected with spectrometers are one-dimensional signals indicating the concentration of the analytes. Instrumental variation is also a problem for them [21]. In this section, we test out methods on the corn dataset 2. It is a spectroscopy dataset collected with three nearinfrared spectrometers designated as m5, mp5, and mp6. The moisture, oil, protein, and starch contents of 80 corn samples were measured by each device, with ranges of the measured values as [9.377, 10.993], [3.088, 3.832], [7.654, 9.711], and [62.826, 66.472], respectively. Each sample is represented by a spectrum with 700 features. One can observe the discrepancy among the three devices from Fig. 8. This dataset resembles traditional domain adaptation datasets because there is no time-varying drift. Three discrete domains can be defined based on the three devices. We adopt m5 as the source domain, mp5 and mp6 as the target ones. In each domain, samples 4, 8, . . . , 76, 80 were assigned as the test set, the rest as the training set. For hyper-parameter tuning, we applied a threefold cross-validation on the training sets of the three domains. After the best hyper-parameters were determined for each algorithm, a regression model was trained on the training set from the source domain and applied on the test set from the target domains. The regression algorithm was ridge regression with the L2 regularization parameter \u03bb = 1.\nTable IV displays the root mean square error (RMSE) of the four prediction tasks and their average on the two target domains. We also plot the overall average RMSE of the two domains with respect to the subspace dimension h in Fig. 9. ITL was not investigated because it is only applicable in classification problems. In KPCA, TCA, SSTCA, MIDA, and SMIDA, the RBF kernel was used. For the semi-supervised methods SSTCA and SMIDA, the target values were normalized to zero mean and unit variance before subspace learning.\n2http://www.eigenvector.com/data/Corn/\nWe can find that when no domain adaptation was done, the prediction error is large. All domain adaptation algorithms managed to significantly reduce the error. KPCA also has good performance, which is probably because the source and the target domains have similar principal directions, which also contain the most discriminative information. Therefore, source regression models can fit the target samples well. In this dataset, different domains have identical data composition. As a result, corresponding data can be aligned by subspaces alignment, which explains the small error of SA. However, this condition may not hold in other datasets.\nMIDA and SMIDA obtained the lowest average errors in both target domains. Aiming at exploring the prediction accuracy when there is no instrument variation, we further trained regression models on the training set of the two target domains and tested on the same domain. The results are listed as \u201ctrain on target\u201d in Table IV. It can be found that SMIDA outperforms these results. This could be attributed to three reasons: (1) The discrepancy in distributions in this dataset is relatively easy to correct; (2) The use of RBF kernel in SMIDA improves the accuracy; (3) SMIDA learned the subspace on the basis of both training and test samples. Although the test samples were unlabeled, they can provide some information about the distribution of the samples to make the learned subspace generalize better, which can be viewed as the merit of semi-supervised learning. To testify this assumption, we conducted another experiment with multiple target domains. The training samples from the source domain and the test ones from both target domains were leveraged together for subspace learning in MIDA and SMIDA. The average RMSE for the two target domains are 0.209 and 0.217 for MIDA, and 0.208 and 0.218 for SMIDA. Compared with the results in Table IV with single target domain, the results have been further improved, showing that incorporating more unlabeled samples from target domains can be beneficial.\nE. Visual Object Recognition Dataset\nIn [22], Gong et al. evaluated domain adaptation algorithms on four visual object recognition datasets, namely Amazon (A), Caltech-256 (C), DSLR (D), and Webcam (W). Ten common classes were selected from them, with 8 to 151 samples per class per domain, and 2533 images in total. Each image was encoded with an 800-bin histogram using SURF features. The normalized histograms were z-scored to have zero mean and unit variance in each dimension. Following the experimental setting provided in the sample code from the authors of [22], experiments were conducted in 20 random trials for each pair of domains. For each unsupervised trail, 20 (for A, C, W) or 8 (for D) labeled samples per class were randomly chosen from the source domain as the training set (other samples were used unsupervisedly for domain adaptation), while all unlabeled samples in the target domain made up the test set. In semi-supervised trails, three labeled samples per class in the target domain were also assumed to be labeled. Averaged accuracies on each pair of domains as well as standard errors are listed in Tables V and VI.\nFor GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3]. For other methods tested, the hyper-parameters were tuned for the best accuracy. Logistic regression was adopted as the classifier. The polynomial kernel with degree 2 was used in KPCA, TCA, SSTCA, MIDA, and SMIDA. MIDA and SMIDA achieve the best average accuracies in both unsupervised and semi-supervised visual object recognition experiments. We observe that TCA and SSTCA have comparable performance with MIDA and SMIDA, which may be explained by the fact that the HSIC criterion used in MIDA and MMD used in TCA are identical under certain conditions when there are one source and one target domain [24]. Besides, the feature augmentation strategy in MIDA is not crucial in this dataset because there is no change in conditional probability. On the other hand, TCA and SSTCA can only handle one source and one target domains. SSTCA uses the manifold regularization strategy to preserve local geometry information, hence introduces three more hyper-parameters than SMIDA. Moreover, computing the data adjacency graph in SSTCA and\nthe matrix inversion operation in TCA and SSTCA make them slower than MIDA and SMIDA. We compared their speed on the domain adaptation experiment C\u2192A. They were run on a server with Intel Xeon 2.00 GHz CPU and 128 GB RAM. No parallel computing was used. The codes of the algorithms were written in Matlab R2014a. On average, the running times of each trial of MIDA, SMIDA, TCA, and SSTCA were 2.4 s, 2.5 s, 3.0 s, and 10.2 s, respectively. Therefore, MIDA and SMIDA are more practical to use than TCA and SSTCA. Besides, they were initially designed for calibration transfer and drift correction. This dataset is used to show their universality."}, {"heading": "V. CONCLUSION", "text": "In this paper, we introduced maximum independence domain adaptation (MIDA) to learn domain-invariant features. The main idea of MIDA is to reduce the discrepancy among domains by maximizing the independence between the learned features and the domain features of the samples. The domain features describe the background information of each sample, such as the domain label in traditional domain adaptation problems. In the field of sensors and measurement, the device label and acquisition time of the each collected sample can be expressed by the domain features, so that unsupervised calibration transfer and drift correction can be achieved by using MIDA. The feature augmentation strategy proposed in this paper adds domain-specific biases to the learned features, which helps MIDA to align domains. It is also useful when there is a change in conditional probability. Finally, to incorporate the label information, semi-supervised MIDA (SMIDA) is further presented.\nMIDA and SMIDA are flexible algorithms. With the design of the domain features and the use of the HSIC criterion, they can be applied in all kinds of domain adaptation problems, including discrete or continuous distributional change, supervised/semi-supervised/unsupervised, multiple domains, classification or regression, etc. Thus, they have a wide range of potential applications. They are also easy to implement and fast, requiring to solve only one eigenvalue decomposition problem. Experimental results on various types of datasets proved their effectiveness. Although initially designed for calibration transfer and drift correction in machine olfaction,\nthey also performed well in spectroscopy and vision datasets. Future directions may include further extending the definition of the domain features for other applications. Besides, this paper mainly focus on reducing the difference of the marginal distribution P (X) among domains, while it may be better if the conditional distribution P (Y |X) among domains can also be approximately aligned [16]."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the providers of the datasets\nused in this paper."}], "references": [{"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345\u20131359, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual domain adaptation: A survey of recent advances", "author": ["V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Signal Processing Magazine, IEEE, vol. 32, no. 3, pp. 53\u201369, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Flowing on riemannian manifold: Domain adaptation by shifting covariance", "author": ["Z. Cui", "W. Li", "D. Xu", "S. Shan", "X. Chen", "X. Li"], "venue": "IEEE Trans. Cybern., vol. 44, no. 12, pp. 2264\u20132273, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Cross-domain human action recognition", "author": ["W. Bian", "D. Tao", "Y. Rui"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 42, no. 2, pp. 298\u2013307, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain adaptation via transfer component analysis", "author": ["S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 2, pp. 199\u2013210, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Combating negative transfer from predictive distribution differences", "author": ["C.-W. Seah", "Y.-S. Ong", "I.W. Tsang"], "venue": "IEEE Trans. Cybern., vol. 43, no. 4, pp. 1153\u20131165, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A brief history of electronic noses", "author": ["J.W. Gardner", "P.N. Bartlett"], "venue": "Sens. Actuators B: Chem., vol. 18, no. 1, pp. 210\u2013211, 1994.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1994}, {"title": "Electronic nose: current status and future trends", "author": ["F. R\u00f6ck", "N. Barsan", "U. Weimar"], "venue": "Chem. Rev., vol. 108, no. 2, pp. 705\u2013725, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Gas distribution mapping of multiple odour sources using a mobile robot", "author": ["A. Loutfi", "S. Coradeschi", "A.J. Lilienthal", "J. Gonzalez"], "venue": "Robotica, vol. 27, no. 02, pp. 311\u2013319, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "On-line sensor calibration transfer among electronic nose instruments for monitoring volatile organic chemicals in indoor air quality", "author": ["L. Zhang", "F. Tian", "C. Kadri", "B. Xiao", "H. Li", "L. Pan", "H. Zhou"], "venue": "Sens. Actuators: B. Chem., vol. 160, no. 1, pp. 899\u2013909, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of a breath analysis system for diabetes screening and blood glucose level prediction", "author": ["K. Yan", "D. Zhang", "D. Wu", "H. Wei", "G. Lu"], "venue": "IEEE Trans. Biomed. Eng., vol. 61, no. 11, pp. 2787\u20132795, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Signal and data processing for machine olfaction and chemical sensing: a review", "author": ["S. Marco", "A. Guti\u00e9rrez-G\u00e1lvez"], "venue": "IEEE Sens. J., vol. 12, no. 11, pp. 3189\u20133214, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Drift correction methods for gas chemical sensors in artificial olfaction systems: techniques and challenges", "author": ["S. Di Carlo", "M. Falasconi"], "venue": "InTech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Improving the transfer ability of prediction models for electronic noses", "author": ["K. Yan", "D. Zhang"], "venue": "Sens. Actuators B: Chem., vol. 220, pp. 115\u2013124, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Calibration transfer and drift compensation of e-noses via coupled task learning", "author": ["\u2014\u2014"], "venue": "Sens. Actuators B: Chem., vol. 225, pp. 288\u2013297, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Information-theoretical learning of discriminative clusters for unsupervised domain adaptation", "author": ["Y. Shi", "F. Sha"], "venue": "Proceedings of the Intl. Conf. on Machine Learning (ICML), 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised visual domain adaptation using subspace alignment", "author": ["B. Fernando", "A. Habrard", "M. Sebban", "T. Tuytelaars"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2013, pp. 2960\u2013 2967.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning kernels for unsupervised  SUBMITTED TO IEEE TRANSACTIONS ON CYBERNETICS  13 domain adaptation with applications to visual object recognition", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "International Journal of Computer Vision, vol. 109, no. 1-2, pp. 3\u201327, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Generalized transfer subspace learning through low-rank constraint", "author": ["M. Shao", "D. Kit", "Y. Fu"], "venue": "International Journal of Computer Vision, vol. 109, no. 1-2, pp. 74\u201393, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring statistical dependence with hilbert-schmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Schlkopf"], "venue": "Algorithmic learning theory. Springer, 2005, pp. 63\u201377.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Transfer of multivariate calibration models: a review", "author": ["R.N. Feudale", "N.A. Woody", "H. Tan", "A.J. Myles", "S.D. Brown", "J. Ferr\u00e9"], "venue": "Chemometr. Intell. Lab., vol. 64, no. 2, pp. 181\u2013192, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Geodesic flow kernel for unsupervised domain adaptation", "author": ["B. Gong", "Y. Shi", "F. Sha", "K. Grauman"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 2012, pp. 2066\u2013 2073.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Drift compensation for electronic nose by semi-supervised domain adaption", "author": ["Q. Liu", "X. Li", "M. Ye", "S.S. Ge", "X. Du"], "venue": "IEEE Sens. J., vol. 14, no. 3, pp. 657\u2013665, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Feature selection via dependence maximization", "author": ["L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt"], "venue": "J. Mach. Learn. Res., vol. 13, no. 1, pp. 1393\u20131434, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Colored maximum variance unfolding", "author": ["L. Song", "A. Gretton", "K.M. Borgwardt", "A.J. Smola"], "venue": "Advances in neural information processing systems, 2007, pp. 1385\u20131392.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Supervised principal component analysis: Visualization, classification and regression on subspaces and submanifolds", "author": ["E. Barshan", "A. Ghodsi", "Z. Azimifar", "M.Z. Jahromi"], "venue": "Pattern Recogn., vol. 44, no. 7, pp. 1357\u20131371, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daum"], "venue": "Proc. 45th Ann. Meeting of the Assoc. for Computational Linguistics, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Schlkopf", "A. Smola", "K.-R. Mller"], "venue": "Neural computation, vol. 10, no. 5, pp. 1299\u20131319, 1998.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Fisher discriminant analysis with kernels", "author": ["B. Scholkopft", "K.-R. Mullert"], "venue": "Neural networks for signal processing, vol. IX, pp. 41\u201348, 1999.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Finding stationary subspaces in multivariate time series", "author": ["P. Von B\u00fcnau", "F.C. Meinecke", "F.C. Kir\u00e1ly", "K.-R. M\u00fcller"], "venue": "Physical review letters, vol. 103, no. 21, p. 214101, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Computing Surveys (CSUR), vol. 46, no. 4, p. 44, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Chemical gas sensor drift compensation using classifier ensembles", "author": ["A. Vergara", "S. Vembu", "T. Ayhan", "M.A. Ryan", "M.L. Homer", "R. Huerta"], "venue": "Sens. Actuators B: Chem., vol. 166, pp. 320\u2013329, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, domain adaptation or transfer learning is needed to improve the performance in the target domain by leveraging unlabeled (and maybe a few labeled) target samples [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "This topic is receiving increasing attention in recent years due to its broad applications such as computer vision [2], [3], [4] and text classification [5], [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "A typical application plagued by this problem is machine olfaction, which uses electronic noses (e-noses) and pattern recognition algorithms to predict the type and concentration of odors [7].", "startOffset": 188, "endOffset": 191}, {"referenceID": 7, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 147, "endOffset": 150}, {"referenceID": 9, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "The applications of machine olfaction range from agriculture and food to environmental monitoring, robotics, biometrics, and disease analysis [8], [9], [10], [11].", "startOffset": 158, "endOffset": 162}, {"referenceID": 11, "context": "However, owing to the nature of chemical sensors, many e-noses are prone to instrumental variation and time-varying drift mentioned above [12], [13], which greatly hamper their usage in real-world applications.", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "However, owing to the nature of chemical sensors, many e-noses are prone to instrumental variation and time-varying drift mentioned above [12], [13], which greatly hamper their usage in real-world applications.", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 13, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "They should be collected with each device and in each time period so as to provide mapping information between the source and the target domains [12], [10], [14], [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "Then, a widely-used method is to map the features in the target domain to the source domain with regression algorithms [10], [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Then, a widely-used method is to map the features in the target domain to the source domain with regression algorithms [10], [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 49, "endOffset": 52}, {"referenceID": 15, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 2, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 66, "endOffset": 69}, {"referenceID": 17, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 71, "endOffset": 75}, {"referenceID": 18, "context": "to learn domain-invariant feature representation [5], [16], [17], [3], [18], [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "[5] proposed transfer component analysis (TCA), which finds a latent feature space that minimizes the difference of distributions between two domains in the sense of maximum mean discrepancy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Then, it finds a latent feature space in which the samples and their domain features are maximally independent in the sense of Hilbert-Schmidt independence criterion (HSIC) [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 20, "context": "Note that spectrometers suffer the same instrumental variation problem as e-noses [21].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Finally, a domain adaptation experiment is conducted on a well-known object recognition benchmark [22].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Two good surveys on domain adaptation can be found in [1] and [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Two good surveys on domain adaptation can be found in [1] and [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Some algorithms project all samples to a common latent space [5], [16], [19].", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "Some algorithms project all samples to a common latent space [5], [16], [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Some algorithms project all samples to a common latent space [5], [16], [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "Transfer component analysis (TCA) [5] tries to learn transfer components across domains in a reproducing kernel Hilbert space (RKHS) using maximum mean discrepancy.", "startOffset": 34, "endOffset": 37}, {"referenceID": 15, "context": "[16] measured domain difference by the mutual information between all samples and their binary domain labels, which can be viewed as a primitive version of the domain features used in this paper.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The low-rank transfer subspace learning (LTSL) algorithm presented in [19] is a reconstruction guided knowledge transfer method.", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 147, "endOffset": 151}, {"referenceID": 2, "context": "Another class of methods first project the source and the target data into separate subspaces, and then build connections between them [17], [22], [23], [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 16, "context": "[17] utilized a transformation matrix to map the source subspace to the target one, where a subspace was represented by eigenvectors of PCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The geodesic flow kernel (GFK) method [22] measures the geometric distance between two different domains in a Grassmann manifold by constructing a geodesic flow.", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "[23] adapted GFK for drift correction of enoses.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Another improvement of GFK is domain adaptation by shifting covariance (DASC) [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "HSIC is defined as the square of the Hilbert-Schmidt norm of the cross-covariance operator Cxy [20]:", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "It can be proved that with characteristic kernels kx and ky , HSIC(pxy,F ,G) is zero if and only if x and y are independent [24].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": ", (xn, yn)}, Kx,Ky \u2208 Rn\u00d7n are the kernel matrices of X and Y , respectively, then [20]:", "startOffset": 82, "endOffset": 86}, {"referenceID": 24, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 84, "endOffset": 87}, {"referenceID": 25, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "Due to its simplicity and power, HSIC has been adopted for feature extraction [25], [5], [26] and feature selection [24].", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "In [27], the author proposed a feature augmentation strategy for domain adaptation: if a sample x \u2208 R is from the source domain, then its augmented feature vector is x\u0302 = \uf8ee\uf8f0 xx", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Meanwhile, the two models share a common component [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "Similar to other kernel dimensionality reduction algorithms [28], [29], the key idea is to express each projection direction as a linear combination of all samples in the space, namely W\u0303 = \u03a6(X)W .", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Similar to other kernel dimensionality reduction algorithms [28], [29], the key idea is to express each projection direction as a linear combination of all samples in the space, namely W\u0303 = \u03a6(X)W .", "startOffset": 66, "endOffset": 70}, {"referenceID": 4, "context": "In domain adaptation, the goal is not only minimizing the difference of distributions, but also preserving important properties of data, such as the variance [5].", "startOffset": 158, "endOffset": 161}, {"referenceID": 25, "context": "Note that a conventional constraint is requiring W\u0303 to be orthonormal as in [26], which will lead to a generalized eigenvector problem.", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Different kernels indicate different assumptions on the type of dependence in using HSIC [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 23, "context": "According to [24], the polynomial and RBF kernels map the original features to a higher or infinite dimensional space, thus are able to detect more types of dependence.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "However, choosing a suitable kernel width parameter (\u03c3) is also important for these more powerful kernels [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "The maximum mean discrepancy (MMD) criterion is used in TCA [5] to measure the difference between two distributions.", "startOffset": 60, "endOffset": 63}, {"referenceID": 23, "context": "[24] showed that when HSIC and MMD are both applied to measure the dependence of features and labels in a binary-class classification problem, they are identical up to a constant factor if the label kernel matrix in HSIC is properly designed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The stationary subspace analysis (SSA) algorithm [30] is able to identify temporally stationary components in multivariate time series.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "Concept drift adaptation algorithms [31] are able to correct continuous time-varying drift.", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 17, "endOffset": 20}, {"referenceID": 25, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "Similar to [25], [5], [26], [24], HSIC is adopted to maximize the dependence between the projected features and the labels.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "Besides variance and label dependence, another useful property of data is the geometry structure, which can be preserved by manifold regularization (MR) [32].", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "MR can be conveniently incorporated into SMIDA by adding a regularizer \u2212\u03bb tr(WKxLKxW ) into (12), where L is the graph Laplacian matrix [32], \u03bb > 0 is a trade-off hyperparameter.", "startOffset": 136, "endOffset": 140}, {"referenceID": 4, "context": "1, TCA [5] and MIDA are compared on a 2D dataset with two discrete domains.", "startOffset": 7, "endOffset": 10}, {"referenceID": 29, "context": "2, SSA [30] and MIDA are compared on a 2D dataset with continuous distributional change, which resembles time-varying drift in machine olfaction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "[33] is dedicated to research in drift correction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "TABLE I PERIOD OF COLLECTION AND NUMBER OF SAMPLES IN THE GAS SENSOR ARRAY DRIFT DATASET [33].", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "A strategy used in previous literatures [33], [23] was also adopted in this paper so that the performance of the domain adaptation algorithms can be evaluated.", "startOffset": 40, "endOffset": 44}, {"referenceID": 22, "context": "A strategy used in previous literatures [33], [23] was also adopted in this paper so that the performance of the domain adaptation algorithms can be evaluated.", "startOffset": 46, "endOffset": 50}, {"referenceID": 32, "context": "In the dataset, each sample is represented by 128 features extracted from the sensors\u2019 response curves [33].", "startOffset": 103, "endOffset": 107}, {"referenceID": 4, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 137, "endOffset": 140}, {"referenceID": 16, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 21, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 262, "endOffset": 266}, {"referenceID": 15, "context": "As displayed in Table II, the compared methods include kernel PCA (KPCA), transfer component analysis (TCA), semi-supervised TCA (SSTCA) [5], subspace alignment (SA) [17], geodesic flow kernel (GFK) [22], manifold regularization with combination GFK (ML-comGFK) [23], and informationtheoretical learning (ITL) [16].", "startOffset": 310, "endOffset": 314}, {"referenceID": 21, "context": "The subspace dimension of GFK was determined according to the subspace disagreement measure in [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 22, "context": "The results of ML-comGFK are copied from [23].", "startOffset": 41, "endOffset": 45}, {"referenceID": 0, "context": "The domain feature vector of a sample was thus [1, 0] if it is from the source domain and [0, 1] if it is from the target.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "The domain feature vector of a sample was thus [1, 0] if it is from the source domain and [0, 1] if it is from the target.", "startOffset": 90, "endOffset": 96}, {"referenceID": 7, "context": "As a noninvasive approach, disease screening and monitoring with e-noses is attracting more and more attention [8], [11].", "startOffset": 111, "endOffset": 114}, {"referenceID": 10, "context": "As a noninvasive approach, disease screening and monitoring with e-noses is attracting more and more attention [8], [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "For example, the concentration of acetone in diabetics\u2019 breath is often higher than that in healthy people [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "We have collected a breath analysis dataset in years 2014\u2013 2015 using two e-noses of the same model [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 4, "context": "TCA [5] 82.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 84.", "startOffset": 6, "endOffset": 9}, {"referenceID": 16, "context": "SA [17] 80.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GFK [22] 77.", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "ML-comGFK [23] 80.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "ITL [16] 76.", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "Because methods other than stationary subspace analysis (SSA) [30], MIDA, and SMIDA are not capable of handling the chronological information, we simply regarded each device as a discrete domain and learned device-invariant features with them.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "TCA [5] 67.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 67.", "startOffset": 6, "endOffset": 9}, {"referenceID": 16, "context": "SA [17] 29.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GFK [22] 41.", "startOffset": 4, "endOffset": 8}, {"referenceID": 15, "context": "ITL [16] 68.", "startOffset": 4, "endOffset": 8}, {"referenceID": 29, "context": "SSA [30] 49.", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "For example, the noise in data can also be stationary [5].", "startOffset": 54, "endOffset": 57}, {"referenceID": 20, "context": "Instrumental variation is also a problem for them [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "TCA [5] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 16, "context": "SA [17] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "GFK [22] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "In [22], Gong et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "Following the experimental setting provided in the sample code from the authors of [22], experiments were conducted in 20 random trials for each pair of domains.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "For GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "For GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3].", "startOffset": 171, "endOffset": 175}, {"referenceID": 2, "context": "For GFK, low-rank transfer subspace learning (LTSL), and domain adaptation by shifting covariance (DASC), we copied the best results reported in the original papers [18], [19], [3].", "startOffset": 177, "endOffset": 180}, {"referenceID": 23, "context": "We observe that TCA and SSTCA have comparable performance with MIDA and SMIDA, which may be explained by the fact that the HSIC criterion used in MIDA and MMD used in TCA are identical under certain conditions when there are one source and one target domain [24].", "startOffset": 258, "endOffset": 262}, {"referenceID": 4, "context": "TCA [5] 49.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 50.", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "ITL [16] 41.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "SA [17] 48.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "GFK [18] 40.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "LTSL [19] 50.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "DASC [3] 39.", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "TCA [5] 55.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "SSTCA [5] 55.", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "ITL [16] 51.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "SA [17] 51.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "GFK [18] 46.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "LTSL [19] 50.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Besides, this paper mainly focus on reducing the difference of the marginal distribution P (X) among domains, while it may be better if the conditional distribution P (Y |X) among domains can also be approximately aligned [16].", "startOffset": 222, "endOffset": 226}], "year": 2016, "abstractText": "When the distributions of the source and the target domains are different, domain adaptation techniques are needed. For example, in the field of sensors and measurement, discrete and continuous distributional change often exist in data because of instrumental variation and time-varying sensor drift. In this paper, we propose maximum independence domain adaptation (MIDA) to address this problem. Domain features are first defined to describe the background information of a sample, such as the device label and acquisition time. Then, MIDA learns features which have maximal independence with the domain features, so as to reduce the inter-domain discrepancy in distributions. A feature augmentation strategy is designed so that the learned projection is background-specific. Semi-supervised MIDA (SMIDA) extends MIDA by exploiting the label information. The proposed methods can handle not only discrete domains in traditional domain adaptation problems but also continuous distributional change such as the time-varying drift. In addition, they are naturally applicable in supervised/semi-supervised/unsupervised classification or regression problems with multiple domains. This flexibility brings potential for a wide range of applications. The effectiveness of our approaches is verified by experiments on synthetic datasets and four real-world ones on sensors, measurement, and computer vision.", "creator": "LaTeX with hyperref package"}}}