{"id": "1603.04553", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Unsupervised Ranking Model for Entity Coreference Resolution", "abstract": "coreference distance resolution is one of the first stages in deep language understanding and its importance has previously been well recognized in the natural language processing community. in this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. our combined unsupervised system achieves 58. 44 % f1 score of supporting the standard conll metric on the emerging english content data from the conll - 2012 shared task ( pradhan et und al., 2010 2012 ), outperforming the aforementioned stanford deterministic system ( lee et al., 2013 ) by 3. 01 %.", "histories": [["v1", "Tue, 15 Mar 2016 04:39:15 GMT  (55kb)", "http://arxiv.org/abs/1603.04553v1", "Accepted by NAACL 2016"]], "COMMENTS": "Accepted by NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xuezhe ma", "zhengzhong liu", "eduard h hovy"], "accepted": true, "id": "1603.04553"}, "pdf": {"name": "1603.04553.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["liu}@cs.cmu.edu,", "ehovy@cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n04 55\n3v 1\n[ cs\n.C L\n] 1\n5 M\nCoreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%."}, {"heading": "1 Introduction", "text": "Entity coreference resolution has become a critical component for many Natural Language Processing (NLP) tasks. Systems requiring deep language understanding, such as information extraction (Wellner et al., 2004), semantic event learning (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and named entity linking (Durrett and Klein, 2014; Ji et al., 2014) all benefit from entity coreference information.\nEntity coreference resolution is the task of identifying mentions (i.e., noun phrases) in a text or dialogue that refer to the same real-world entities. In recent years, several supervised entity coreference resolution systems have been proposed, which, according to Ng (2010), can be categorized into three classes \u2014 mentionpair models (McCarthy and Lehnert, 1995), entity-mention models (Yang et al., 2008a;\nHaghighi and Klein, 2010; Lee et al., 2011) and ranking models (Yang et al., 2008b; Durrett and Klein, 2013; Fernandes et al., 2014) \u2014 among which ranking models recently obtained state-of-the-art performance. However, the manually annotated corpora that these systems rely on are highly expensive to create, in particular when we want to build data for resourcepoor languages (Ma and Xia, 2014). That makes unsupervised approaches, which only require unannotated text for training, a desirable solution to this problem.\nSeveral unsupervised learning algorithms have been applied to coreference resolution. Haghighi and Klein (2007) presented a mentionpair nonparametric fully-generative Bayesian model for unsupervised coreference resolution. Based on this model, Ng (2008) probabilistically induced coreference partitions via EM clustering. Poon and Domingos (2008) proposed an entity-mention model that is able to perform joint inference across mentions by using Markov Logic. Unfortunately, these unsupervised systems\u2019 performance on accuracy significantly falls behind those of supervised systems, and are even worse than the deterministic rule-based systems. Furthermore, there is no previous work exploring the possibility of developing an unsupervised ranking model which achieved state-of-the-art performance under supervised settings for entity coreference resolution.\nIn this paper, we propose an unsupervised generative ranking model for entity coreference resolution. Our experimental results on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012)\nshow that our unsupervised system outperforms the Stanford deterministic system (Lee et al., 2013) by 3.01% absolute on the CoNLL official metric. The contributions of this work are (i) proposing the first unsupervised ranking model for entity coreference resolution. (ii) giving empirical evaluations of this model on benchmark data sets. (iii) considerably narrowing the gap to supervised coreference resolution accuracy."}, {"heading": "2 Unsupervised Ranking Model", "text": ""}, {"heading": "2.1 Notations and Definitions", "text": "In the following, D = {m0,m1, . . . ,mn} represents a generic input document which is a sequence of coreference mentions, including the artificial root mention (denoted by m0). The method to detect and extract these mentions is discussed later in Section 2.6. Let C = {c1, c2, . . . , cn} denote the coreference assignment of a given document, where each mention mi has an associated random variable ci taking values in the set {0, i, . . . , i \u2212 1}; this variable specifies mi\u2019s selected antecedent (ci \u2208 {1, 2, . . . , i \u2212 1}), or indicates that it begins a new coreference chain (ci = 0)."}, {"heading": "2.2 Generative Ranking Model", "text": "The following is a straightforward way to build a generative model for coreference:\nP (D,C) = P (D|C)P (C)\n= n \u220f\nj=1 P (mj|mcj )\nn \u220f\nj=1 P (cj |j)\n(1)\nwhere we factorize the probabilities P (D|C) and P (C) into each position j by adopting appropriate independence assumptions that given the coreference assignment cj and corresponding coreferent mention mcj , the mention mj is independent with other mentions in front of it. This independent assumption is similar to that in the IBM 1 model on machine translation (Brown et al., 1993), where it assumes that given the corresponding English word, the aligned foreign word is independent with other English and foreign words. We do not make any independent assumptions among different features (see Section 2.4 for details).\nInference in this model is efficient, because we\ncan compute cj separately for each mention:\nc\u2217j = argmax cj P (mj|mcj )P (cj |j)\nThe model is a so-called ranking model because it is able to identify the most probable candidate antecedent given a mention to be resolved."}, {"heading": "2.3 Resolution Mode Variables", "text": "According to previous work (Haghighi and Klein, 2009; Ratinov and Roth, 2012; Lee et al., 2013), antecedents are resolved by different categories of information for different mentions. For example, the Stanford system (Lee et al., 2013) uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables \u03a0 = {\u03c01, . . . , \u03c0n}, where for each mention j the variable \u03c0j \u2208 {str, prec, attr} indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (prec), and attribute-matching (attr) \u2014 and \u03a0 is deterministic when D is given (i.e. P (\u03a0|D) is a point distribution). We determine \u03c0j for each mention mj in the following way:\n\u2022 \u03c0j = str, if there exists a mention mi, i < j such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system (Lee et al., 2013).\n\u2022 \u03c0j = prec, if there exists a mention mi, i < j such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\u2022 \u03c0j = attr, if there is no mention mi, i < j satisfies the above two conditions.\nNow, we can extend the generative model in Eq. 1 to:\nP (D,C) = P (D,C,\u03a0)\n= n \u220f\nj=1 P (mj|mcj , \u03c0j)P (cj |\u03c0j, j)P (\u03c0j |j)\nwhere we define P (\u03c0j |j) to be uniform distribution. We model P (mj |mcj , \u03c0j) and P (cj |\u03c0j, j) in the following way:\nP (mj |mcj , \u03c0j) = t(mj|mcj , \u03c0j)\nP (cj |\u03c0j , j) =\n{\nq(cj |\u03c0j , j) \u03c0j = attr 1 j otherwise\nwhere \u03b8 = {t, q} are parameters of our model. Note that in the attribute-matching mode (\u03c0j = attr) we model P (cj |\u03c0j, j) with parameter q, while in the other two modes, we use the uniform distribution. It makes sense because the position information is important for coreference resolved by matching attributes of two mentions such as resolving pronoun coreference, but not that important for those resolved by matching text or special relations like two mentions referring the same person and matching by the name."}, {"heading": "2.4 Features", "text": "In this section, we describe the features we use to represent mentions. Specifically, as shown in Table 1, we use different features under different resolution modes. It should be noted that only the Distance feature is designed for parameter q, all other features are designed for parameter t."}, {"heading": "2.5 Model Learning", "text": "For model learning, we run EM algorithm (Dempster et al., 1977) on our Model, treating D as observed data and C as latent variables. We run EM with 10 iterations and select the parameters achieving the best performance on the development data. Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters\nappear at around the 5th iteration, according to our experiments.The detailed derivation of the learning algorithm is shown in Appendix A. The pseudo-code is shown is Algorithm 1. We use uniform initialization for all the parameters in our model.\nSeveral previous work has attempted to use EM for entity coreference resolution. Cherry and Bergsma (2005) and Charniak and Elsner (2009) applied EM for\npronoun anaphora resolution. Ng (2008) probabilistically induced coreference partitions via EM clustering. Recently, Moosavi and Strube (2014) proposed an unsupervised model utilizing the most informative relations and achieved competitive performance with the Stanford system."}, {"heading": "2.6 Mention Detection", "text": "The basic rules we used to detect mentions are similar to those of Lee et al. (2013), except that their system uses a set of filtering rules designed to discard instances of pleonastic it, partitives, certain quantified noun phrases and other spurious mentions. Our system keeps partitives, quantified noun phrases and bare NP mentions, but discards pleonastic it and other spurious mentions."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Setup", "text": "Datasets. Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010) (Parker et al., 2011) to train the model. Following previous work (Chambers and Jurafsky, 2008), we remove duplicated documents and the documents which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes cor-\npus (Hovy et al., 2006). The corpora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entitybased CEAF (CEAFe) (Luo, 2005). In addition, we also report results on another two popular metrics: Mention-based CEAF (CEAFm) and BLANC (Recasens and Hovy, 2011). All the results are given by the latest version of CoNLL-2012 scorer 1"}, {"heading": "3.2 Results and Comparison", "text": "Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system (Lee et al., 2011) and Multigraph: the unsupervised multigraph system (Martschat, 2013), and one unsupervised system, namely MIR: the unsupervised system using most informative relations (Moosavi and Strube, 2014). Our model outperforms the three baseline systems on all the evaluation metrics. Specifically, our model achieves improvements of 2.93% and 3.01% on CoNLL F1\n1http://conll.cemantix.org/2012/software.html\nscore over the Stanford system, the winner of the CoNLL 2011 shared task, on the CoNLL 2012 development and test sets, respectively. The improvements on CoNLL F1 score over the Multigraph model are 1.41% and 1.77% on the development and test sets, respectively. Comparing with the MIR model, we obtain significant improvements of 2.62% and 3.02% on CoNLL F1 score.\nTo make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task (Bjo\u0308rkelund and Farkas, 2012); Latent-Tree: the latent tree model (Fernandes et al., 2012) obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set (Durrett and Klein, 2013); LaSO: the structured perceptron system with nonlocal features (Bjo\u0308rkelund and Kuhn, 2014); Latent-Strc: the latent structure system (Martschat and Strube, 2015); Model-Stack: the entity-centric system with model stacking (Clark and Manning, 2015); and Non-Linear: the non-linear mention-ranking model with feature representations (Wiseman et al., 2015). Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3."}, {"heading": "4 Conclusion", "text": "We proposed a new generative, unsupervised ranking model for entity coreference resolution into which we introduced resolution mode variables to distinguish mentions resolved by different categories of information. Experimental results on the data from CoNLL-2012 shared task show that our system significantly improves the accuracy on different evaluation metrics over the baseline systems.\nOne possible direction for future work is to differentiate more resolution modes. Another one is to add more precise or even event-based features to improve the model\u2019s performance."}, {"heading": "Acknowledgements", "text": "This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA."}, {"heading": "Appendix A. Derivation of Model Learning", "text": "Formally, we iteratively estimate the model parameters \u03b8, employing the following EM algorithm: E-step: Compute the posterior probabilities of C , P (C|D; \u03b8), based on the current \u03b8. M-step: Calculate the new \u03b8\u2032 that maximizes the expected complete log likelihood, EP (C|D;\u03b8)[log P (D,C; \u03b8 \u2032)] For simplicity, we denote: P (C|D; \u03b8) = P\u0303 (C|D) P (C|D; \u03b8\u2032) = P (C|D) In addition, we use \u03c4(\u03c0j |j) to denote the probability P (\u03c0j |j) which is uniform distribution in our model. Moreover, we use the following notation for convenience: \u03b8(mj,mk, j, k, \u03c0j) = t(mj|mk, \u03c0j)q(k|\u03c0j , j)\u03c4(\u03c0j |j) Then, we have E P\u0303 (c|D)[logP (D,C)] = \u2211 C P\u0303 (C|D) logP (D,C) = \u2211 C P\u0303 (C|D) ( n \u2211 j=1 log t(mj |mcj , \u03c0j) + log q(cj |\u03c0j , j) + log \u03c4(\u03c0j |j) ) = n \u2211 j=1 j\u22121 \u2211 k=0 Ljk ( log t(mj |mk, \u03c0j) + log q(k|\u03c0j , j) + log \u03c4(\u03c0j |j) ) Then the parameters t and q that maximize EP\u0303 (c|D)[log P (D,C)] satisfy that t(mj |mk, \u03c0j) = Ljk n\u2211 i=1 Lik q(k|\u03c0j , j) = Ljk j\u22121\u2211\ni=0\nLji\nwhere Ljk can be calculated by\nLjk = \u2211\nC,cj=k\nP\u0303 (C|D) =\n\u2211\nC,cj=k\nP\u0303 (C,D)\n\u2211\nC\nP\u0303 (C,D)\n=\n\u2211\nC,cj=k\nn\u220f\ni=1\n\u03b8\u0303(mi,mci ,ci,i,\u03c0i)\n\u2211\nC\nn\u220f\ni=1\n\u03b8\u0303(mi,mci ,ci,i,\u03c0i)\n= \u03b8\u0303(mj ,mk,k,j,\u03c0j)\n\u2211\nC(\u2212j)\nP\u0303 (C(\u2212j)|D)\nj\u22121\u2211 i=0 \u03b8\u0303(mj ,mi,i,j,\u03c0j) \u2211 C(\u2212j) P\u0303 (C(\u2212j)|D)\n= \u03b8\u0303(mj ,mk,k,j,\u03c0j)\nj\u22121\u2211\ni=0\n\u03b8\u0303(mj ,mi,i,j,\u03c0j)\n= t\u0303(mj|mk,\u03c0j)q\u0303(k|\u03c0j ,j)\u03c4\u0303(\u03c0j |j)\nj\u22121\u2211\ni=0\nt\u0303(mj |mi,\u03c0j)q\u0303(i|\u03c0j ,j)\u03c4\u0303(\u03c0j |j)\n= t\u0303(mj|mk,\u03c0j)q\u0303(k|\u03c0j ,j)\nj\u22121\u2211\ni=0\nt\u0303(mj |mi,\u03c0j)q\u0303(i|\u03c0j ,j)\nwhere C(\u2212j) = {c1, . . . , cj\u22121, cj+1, . . . , cn}. The above derivations correspond to the learning algorithm in Algorithm 1."}], "references": [{"title": "Algorithms for scoring coreference chains", "author": ["Bagga", "Baldwin1998] Amit Bagga", "Breck Baldwin"], "venue": "In The first international conference on language resources and evaluation workshop on linguistics coreference,", "citeRegEx": "Bagga et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bagga et al\\.", "year": 1998}, {"title": "Bootstrapping path-based pronoun resolution", "author": ["Bergsma", "Lin2006] Shane Bergsma", "Dekang Lin"], "venue": "In Proceedings of ACL-2006,", "citeRegEx": "Bergsma et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2006}, {"title": "Data-driven multilingual coreference resolution using resolver stacking", "author": ["Bj\u00f6rkelund", "Farkas2012] Anders Bj\u00f6rkelund", "Rich\u00e1rd Farkas"], "venue": "In Proceedings of EMNLP-CoNLL-2012 - Shared Task,", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2012}, {"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": "In Proceedings of ACL-2014,", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Unsupervised learning of narrative event chains", "author": ["Chambers", "Jurafsky2008] Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of ACL-2008: HLT,", "citeRegEx": "Chambers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["Chambers", "Jurafsky2009] Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of ACL-2009,", "citeRegEx": "Chambers et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2009}, {"title": "EM works for pronoun anaphora resolution", "author": ["Charniak", "Elsner2009] Eugene Charniak", "Micha Elsner"], "venue": "In Proceedings of EACL", "citeRegEx": "Charniak et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Charniak et al\\.", "year": 2009}, {"title": "An Expectation Maximization approach to pronoun resolution", "author": ["Cherry", "Bergsma2005] Colin Cherry", "Shane Bergsma"], "venue": "In Proceedings of CoNLL-2005,", "citeRegEx": "Cherry et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cherry et al\\.", "year": 2005}, {"title": "Entity-centric coreference resolution with model stacking", "author": ["Clark", "Manning2015] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of ACL-IJCNLP-2015,", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Nan M Laird", "Donald B Rubin"], "venue": "Journal of the royal statistical society. Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Durrett", "Klein2013] Greg Durrett", "Dan Klein"], "venue": "In Proceedings of EMNLP-2013,", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the Transactions of the Association for Computational Linguistics", "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Latent structure perceptron with feature induction for unrestricted coreference resolution", "author": ["C\u0131\u0301cero dos Santos", "Ruy Milidi\u00fa"], "venue": "In Proceedings of EMNLP-CoNLL-2012 - Shared Task,", "citeRegEx": "Fernandes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2012}, {"title": "Latent trees for coreference resolution", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Ruy Luiz Milidi\u00fa"], "venue": null, "citeRegEx": "Fernandes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2014}, {"title": "Unsupervised coreference resolution in a nonparametric bayesian model", "author": ["Haghighi", "Klein2007] Aria Haghighi", "Dan Klein"], "venue": "In Proceedings of ACL2007,", "citeRegEx": "Haghighi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2007}, {"title": "Simple coreference resolution with rich syntactic and semantic features", "author": ["Haghighi", "Klein2009] Aria Haghighi", "Dan Klein"], "venue": "In Proceedings of EMNLP2009,", "citeRegEx": "Haghighi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2009}, {"title": "Coreference resolution in a modular, entitycentered model", "author": ["Haghighi", "Klein2010] Aria Haghighi", "Dan Klein"], "venue": "In Proceedings of NAACL-2010,", "citeRegEx": "Haghighi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2010}, {"title": "Ontonotes: The 90% solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of NAACL-2006,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Gender and animacy knowledge discovery from web-scale ngrams for unsupervised person mention detection", "author": ["Ji", "Lin2009] Heng Ji", "Dekang Lin"], "venue": "In Proceedings of PACLIC-2009,", "citeRegEx": "Ji et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2009}, {"title": "Overview of tac-kbp2014 entity discovery and linking tasks", "author": ["Ji et al.2014] Heng Ji", "HT Dang", "J Nothman", "B Hachey"], "venue": "In Proc. Text Analysis Conference", "citeRegEx": "Ji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2014}, {"title": "Stanford\u2019s multi-pass sieve coreference resolution system at the conll-2011 shared task", "author": ["Lee et al.2011] Heeyoung Lee", "Yves Peirsman", "Angel Chang", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky"], "venue": "In Proceedings of CoNLL-2011: Shared Task,", "citeRegEx": "Lee et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Deterministic coreference resolution based on entity-centric", "author": ["Lee et al.2013] Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky"], "venue": "precision-ranked rules. Comput. Linguist.,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "On coreference resolution performance metrics", "author": ["Xiaoqiang Luo"], "venue": "In Proceedings of EMNLP2005,", "citeRegEx": "Luo.,? \\Q2005\\E", "shortCiteRegEx": "Luo.", "year": 2005}, {"title": "Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization", "author": ["Ma", "Xia2014] Xuezhe Ma", "Fei Xia"], "venue": "In Proceedings of ACL-2014,", "citeRegEx": "Ma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2014}, {"title": "Latent structures for coreference resolution. Transactions of the Association for Computational Linguistics, 3:405\u2013418", "author": ["Martschat", "Strube2015] Sebastian Martschat", "Michael Strube"], "venue": null, "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Multigraph clustering for unsupervised coreference resolution", "author": ["Sebastian Martschat"], "venue": "In ACL-2013: Student Research Workshop,", "citeRegEx": "Martschat.,? \\Q2013\\E", "shortCiteRegEx": "Martschat.", "year": 2013}, {"title": "Using decision trees for conference resolution", "author": ["McCarthy", "Lehnert1995] Joseph F McCarthy", "Wendy G Lehnert"], "venue": "In Proceedings of IJCAI-1995,", "citeRegEx": "McCarthy et al\\.,? \\Q1995\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 1995}, {"title": "Unsupervised coreference resolution by utilizing the most informative relations", "author": ["Moosavi", "Strube2014] Nafise Sadat Moosavi", "Michael Strube"], "venue": "In Proceedings of COLING-2014,", "citeRegEx": "Moosavi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moosavi et al\\.", "year": 2014}, {"title": "Unsupervised models for coreference resolution", "author": ["Vincent Ng"], "venue": "In Proceedings of EMNLP2008,", "citeRegEx": "Ng.,? \\Q2008\\E", "shortCiteRegEx": "Ng.", "year": 2008}, {"title": "Supervised noun phrase coreference research: The first fifteen years", "author": ["Vincent Ng"], "venue": "In Proceedings of ACL-2010,", "citeRegEx": "Ng.,? \\Q2010\\E", "shortCiteRegEx": "Ng.", "year": 2010}, {"title": "English gigaword fifth edition", "author": ["Parker et al.2011] Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Parker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2011}, {"title": "Joint unsupervised coreference resolution with Markov Logic", "author": ["Poon", "Domingos2008] Hoifung Poon", "Pedro Domingos"], "venue": "In Proceedings of EMNLP-2008,", "citeRegEx": "Poon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2008}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang"], "venue": "In Proceedings of EMNLP-CoNLL-2012 - Shared Task,", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Learning-based multi-sieve co-reference resolution with knowledge", "author": ["Ratinov", "Roth2012] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of EMNLPCoNLL-2012,", "citeRegEx": "Ratinov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2012}, {"title": "Blanc: Implementing the rand index for coreference evaluation", "author": ["Recasens", "Hovy2011] Marta Recasens", "Eduard Hovy"], "venue": "Natural Language Engineering,", "citeRegEx": "Recasens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recasens et al\\.", "year": 2011}, {"title": "A machine learning approach to coreference resolution of noun phrases", "author": ["Soon et al.2001] Wee Meng Soon", "Hwee Tou Ng", "Daniel Chung Yong Lim"], "venue": null, "citeRegEx": "Soon et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "A model-theoretic coreference scoring scheme", "author": ["Vilain et al.1995] Marc Vilain", "John Burger", "John Aberdeen", "Dennis Connolly", "Lynette Hirschman"], "venue": "In Proceedings of the 6th conference on Message understanding,", "citeRegEx": "Vilain et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Vilain et al\\.", "year": 1995}, {"title": "An integrated, conditional model of information extraction and coreference with application to citation matching", "author": ["Wellner et al.2004] Ben Wellner", "Andrew McCallum", "Fuchun Peng", "Michael Hay"], "venue": "In Proceedings of the 20th conference on Uncertainty", "citeRegEx": "Wellner et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Wellner et al\\.", "year": 2004}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Wiseman et al.2015] Sam Wiseman", "Alexander M. Rush", "Stuart Shieber", "Jason Weston"], "venue": "In Proceedings of ACL-IJCNLP2015,", "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "An entity-mention model for coreference resolution with inductive logic programming", "author": ["Yang et al.2008a] Xiaofeng Yang", "Jian Su", "Jun Lang", "Chew Lim Tan", "Ting Liu", "Sheng Li"], "venue": "In Proceedings of ACL2008,", "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}, {"title": "A twin-candidate model for learning-based anaphora resolution. Computational Linguistics, 34(3):327\u2013356", "author": ["Yang et al.2008b] Xiaofeng Yang", "Jian Su", "Chew Lim Tan"], "venue": "Appendix A. Derivation of Model Learning", "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 33, "context": "44% F1 score of the CoNLL metric on the English data from the CoNLL2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 22, "context": ", 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.", "startOffset": 57, "endOffset": 75}, {"referenceID": 38, "context": "Systems requiring deep language understanding, such as information extraction (Wellner et al., 2004), semantic event learning (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and named entity linking (Durrett and Klein, 2014; Ji et al.", "startOffset": 78, "endOffset": 100}, {"referenceID": 20, "context": ", 2004), semantic event learning (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and named entity linking (Durrett and Klein, 2014; Ji et al., 2014) all benefit from entity coreference information.", "startOffset": 118, "endOffset": 160}, {"referenceID": 21, "context": "In recent years, several supervised entity coreference resolution systems have been proposed, which, according to Ng (2010), can be categorized into three classes \u2014 mentionpair models (McCarthy and Lehnert, 1995), entity-mention models (Yang et al., 2008a; Haghighi and Klein, 2010; Lee et al., 2011) and ranking models (Yang et al.", "startOffset": 236, "endOffset": 300}, {"referenceID": 14, "context": ", 2011) and ranking models (Yang et al., 2008b; Durrett and Klein, 2013; Fernandes et al., 2014) \u2014 among which ranking models recently obtained state-of-the-art performance.", "startOffset": 27, "endOffset": 96}, {"referenceID": 17, "context": ", 2004), semantic event learning (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), and named entity linking (Durrett and Klein, 2014; Ji et al., 2014) all benefit from entity coreference information. Entity coreference resolution is the task of identifying mentions (i.e., noun phrases) in a text or dialogue that refer to the same real-world entities. In recent years, several supervised entity coreference resolution systems have been proposed, which, according to Ng (2010), can be categorized into three classes \u2014 mentionpair models (McCarthy and Lehnert, 1995), entity-mention models (Yang et al.", "startOffset": 144, "endOffset": 487}, {"referenceID": 29, "context": "Based on this model, Ng (2008) probabilistically induced coreference partitions via EM clustering.", "startOffset": 21, "endOffset": 31}, {"referenceID": 29, "context": "Based on this model, Ng (2008) probabilistically induced coreference partitions via EM clustering. Poon and Domingos (2008) proposed an entity-mention model that is able to perform joint inference across mentions by using Markov Logic.", "startOffset": 21, "endOffset": 124}, {"referenceID": 33, "context": "Our experimental results on the English data from the CoNLL-2012 shared task (Pradhan et al., 2012)", "startOffset": 77, "endOffset": 99}, {"referenceID": 22, "context": "show that our unsupervised system outperforms the Stanford deterministic system (Lee et al., 2013) by 3.", "startOffset": 80, "endOffset": 98}, {"referenceID": 4, "context": "This independent assumption is similar to that in the IBM 1 model on machine translation (Brown et al., 1993), where it assumes that given the corresponding English word, the aligned foreign word is independent with other English and foreign words.", "startOffset": 89, "endOffset": 109}, {"referenceID": 22, "context": "According to previous work (Haghighi and Klein, 2009; Ratinov and Roth, 2012; Lee et al., 2013), antecedents are resolved by different categories of information for different mentions.", "startOffset": 27, "endOffset": 95}, {"referenceID": 22, "context": "For example, the Stanford system (Lee et al., 2013) uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym.", "startOffset": 33, "endOffset": 51}, {"referenceID": 22, "context": "\u2022 \u03c0j = str, if there exists a mention mi, i < j such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system (Lee et al., 2013).", "startOffset": 207, "endOffset": 225}, {"referenceID": 36, "context": "Semantic Class semantic classes derived from WordNet (Soon et al., 2001).", "startOffset": 53, "endOffset": 72}, {"referenceID": 21, "context": "Number the number of a mention similarly derived from Lee et al. (2013). Gender the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009).", "startOffset": 54, "endOffset": 72}, {"referenceID": 21, "context": "Number the number of a mention similarly derived from Lee et al. (2013). Gender the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009).", "startOffset": 54, "endOffset": 132}, {"referenceID": 21, "context": "Number the number of a mention similarly derived from Lee et al. (2013). Gender the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009). Person the person attribute from Lee et al.", "startOffset": 54, "endOffset": 154}, {"referenceID": 21, "context": "Number the number of a mention similarly derived from Lee et al. (2013). Gender the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009). Person the person attribute from Lee et al. (2013). We assign person attributes to all mentions, not only pronouns.", "startOffset": 54, "endOffset": 206}, {"referenceID": 21, "context": "Number the number of a mention similarly derived from Lee et al. (2013). Gender the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009). Person the person attribute from Lee et al. (2013). We assign person attributes to all mentions, not only pronouns. Animacy the animacy attribute same as Lee et al. (2013). Semantic Class semantic classes derived from WordNet (Soon et al.", "startOffset": 54, "endOffset": 327}, {"referenceID": 10, "context": "For model learning, we run EM algorithm (Dempster et al., 1977) on our Model, treating D as observed data and C as latent variables.", "startOffset": 40, "endOffset": 63}, {"referenceID": 29, "context": "Ng (2008) probabilistically induced coreference partitions via EM clustering.", "startOffset": 0, "endOffset": 10}, {"referenceID": 29, "context": "Ng (2008) probabilistically induced coreference partitions via EM clustering. Recently, Moosavi and Strube (2014) proposed an unsupervised model utilizing the most informative relations and achieved competitive performance with the Stanford system.", "startOffset": 0, "endOffset": 114}, {"referenceID": 21, "context": "The basic rules we used to detect mentions are similar to those of Lee et al. (2013), except that their system uses a set of filtering rules designed to discard instances of pleonastic it, partitives, certain quantified noun phrases and other spurious mentions.", "startOffset": 67, "endOffset": 85}, {"referenceID": 31, "context": "Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010) (Parker et al., 2011) to train the model.", "startOffset": 120, "endOffset": 141}, {"referenceID": 33, "context": "The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 18, "context": ", 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006).", "startOffset": 52, "endOffset": 71}, {"referenceID": 37, "context": "We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entitybased CEAF (CEAFe) (Luo, 2005).", "startOffset": 75, "endOffset": 96}, {"referenceID": 23, "context": ", 1995), B3 (Bagga and Baldwin, 1998), and Entitybased CEAF (CEAFe) (Luo, 2005).", "startOffset": 68, "endOffset": 79}, {"referenceID": 21, "context": "Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system (Lee et al., 2011) and Multigraph: the unsupervised multigraph system (Martschat, 2013), and one unsupervised system, namely MIR: the unsupervised system using most informative relations (Moosavi and Strube, 2014).", "startOffset": 135, "endOffset": 153}, {"referenceID": 26, "context": ", 2011) and Multigraph: the unsupervised multigraph system (Martschat, 2013), and one unsupervised system, namely MIR: the unsupervised system using most informative relations (Moosavi and Strube, 2014).", "startOffset": 59, "endOffset": 76}, {"referenceID": 13, "context": "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task (Bj\u00f6rkelund and Farkas, 2012); Latent-Tree: the latent tree model (Fernandes et al., 2012) obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set (Durrett and Klein, 2013); LaSO: the structured perceptron system with nonlocal features (Bj\u00f6rkelund and Kuhn, 2014); Latent-Strc: the latent structure system (Martschat and Strube, 2015); Model-Stack: the entity-centric system with model stacking (Clark and Manning, 2015); and Non-Linear: the non-linear mention-ranking model with feature representations (Wiseman et al.", "startOffset": 311, "endOffset": 335}, {"referenceID": 39, "context": ", 2012) obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set (Durrett and Klein, 2013); LaSO: the structured perceptron system with nonlocal features (Bj\u00f6rkelund and Kuhn, 2014); Latent-Strc: the latent structure system (Martschat and Strube, 2015); Model-Stack: the entity-centric system with model stacking (Clark and Manning, 2015); and Non-Linear: the non-linear mention-ranking model with feature representations (Wiseman et al., 2015).", "startOffset": 469, "endOffset": 491}], "year": 2016, "abstractText": "Coreference resolution is one of the first stages in deep language understanding and its importance has been well recognized in the natural language processing community. In this paper, we propose a generative, unsupervised ranking model for entity coreference resolution by introducing resolution mode variables. Our unsupervised system achieves 58.44% F1 score of the CoNLL metric on the English data from the CoNLL2012 shared task (Pradhan et al., 2012), outperforming the Stanford deterministic system (Lee et al., 2013) by 3.01%.", "creator": "LaTeX with hyperref package"}}}