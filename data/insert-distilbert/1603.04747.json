{"id": "1603.04747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Topic Modeling Using Distributed Word Embeddings", "abstract": "we propose a new algorithm for topic modeling, formerly vec2topic, that identifies practically the main query topics contained in a corpus using semantic information captured via high - dimensional distributed word file embeddings. basically our technique is unsupervised and generates a list of topics usually ranked with respect to publication importance. we clearly find that ultimately it works better than existing topic modeling techniques such as latent dirichlet layer allocation for identifying key topics in user - generated variable content, such as emails, chats, etc., where topics are diffused across the corpus. we also find that vec2topic works equally well for non - user generated content, such as topic papers, reports, etc., authors and for small corpora such as a single - document.", "histories": [["v1", "Tue, 15 Mar 2016 16:21:58 GMT  (3675kb)", "http://arxiv.org/abs/1603.04747v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ramandeep s randhawa", "parag jain", "gagan madan"], "accepted": false, "id": "1603.04747"}, "pdf": {"name": "1603.04747.pdf", "metadata": {"source": "CRF", "title": "Topic Modeling Using Distributed Word Embeddings", "authors": ["Ramandeep S. Randhawa", "Gagan Madan"], "emails": ["ramandeep.randhawa@marshall.usc.edu", "paragjain78@gmail.com", "gagan.madan1@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n04 74\n7v 1\n[ cs\n.C L\n] 1\n5 M\nar 2"}, {"heading": "1 Introduction", "text": "Understanding an individual\u2019s key topics of interest to meet their customized needs is an important challenge for personalization and information filtering applications, such as recommender systems. With the proliferation of diverse types of applications and smart devices, users are generating a large amount of text through their emails, chats, and on social media, such as Twitter and Facebook. This data typically contains many words that reflect their interests: for example, if we consider the work emails of someone who works on datacenter servers, they will likely have several semanticallysimilar words such as processors, cloud computing, virtualization etc. Individual emails, however, tend to be more about the context in which these words are used, for instance, meetings, status updates, follow-ups, and so on. The topics themselves are diffused across all the emails \u2014 our goal in this paper is to build an unsupervised algorithm that automatically infers these key topics of interest.\nState-of-the-art topic modeling algorithms, such as Latent Dirichlet Allocation (LDA, [6, 2]), have been successfully applied to discover the main topics across a large collection of documents for some time now, and are a natural candidate for solving the problem at hand. The typical goal of these methods has been to organize this collection according to the discovered topics \u2014 they work well for news articles, papers in a journal, etc., where the document is about the key topics, and the same words show up several times. User-generated content, however, is usually about the context: actual keywords show up infrequently, and are surrounded by several contextual words such as meetings, status, email, thanks, etc. Because of this difference in structure of corpus, LDA\n\u2217Marshall School of Business, e-mail: ramandeep.randhawa@marshall.usc.edu \u2020e-mail: paragjain78@gmail.com \u2021e-mail: gagan.madan1@gmail.com\ntends to cluster topics with similar contexts, and for each topic tends to capture the most frequently used words. It, however, fails to capture the key topics across the entire corpus.\nIn this paper, we propose a new technique of topic modeling, Vec2Topic, that is aimed toward capturing the key topics across user-generated content. Our algorithm consists of two main ideas. The first idea is that a key topic in a corpus should contain a large number of semantically-similar words that relate to that topic. We capture this notion using a depth measure of a word, that helps identify clusters with the highest density of semantically-similar words. We leverage word semantics as captured by high-dimensional distributed word embeddings. Specifically, we perform agglomerative clustering on these distributed word vectors over the vocabulary, and we use the resulting dendrogram to compute the depth score for a word as the number of links between the word and the root node. Clusters that contain words with the highest depth scores reflect the user\u2019s key topics of interest. Our second idea is aimed at deriving good labels that best describe these key topics \u2014 the idea is that such keywords not only have high depth, but also show up in the context of a large number of different words. We capture this using a degree measure of a word, which we define as the count of the number of unique words that co-occur with the word within a context window in the corpus. We then combine the depth and degree measures into a single word score. We use this score to rank all the topics in the corpus.\nTo illustrate this concept, we ran Vec2Topic on Enron\u2019s publicly available email corpus. Table 1 shows the results on former Enron CEO Kenneth Lay\u2019s emails. Topics 1-3 are the top-3 topics identified by Vec2Topic in decreasing order of importance; each topic is listed with the ten highest scoring constituent words. We can see that the most important topic is about the corporate functions of a CEO (business, development, market, policy, etc.), the next topic reflects that Enron is an energy company (power, supply, demand, electricity, etc.), and the third topic is more about its operating aspects (product, strategy, marketing, budget, etc.). We then contrast it with LDA1, which tends to cluster emails with similar context into topics. In Table 1, we notice that LDA\u2019s Topic A captures words like meeting, email, time, thanks, etc., that show up frequently in the context of his emails; Topic C seems to capture a specific theme on the Libya conflict. While these are themes in Mr. Lay\u2019s emails, they do not capture the overall essence of who Mr. Lay is, what\n1We ran LDA with K = 10, 25, and 50 and picked the topics that seemed most relevant.\nhe works on, and what his key interests are across all his communication. Vec2Topic does a much better job in capturing this insight, and in a sense is answering a different question than LDA does. Table 2 shows similar results on Enron\u2019s Former Managing Director of Research, V. Kaminski\u2019s emails \u2014 again, we see that it captures key topics such as modeling and analysis, investment, and government-related topics, which are fairly reflective of his work portfolio.\nTo summarize, we propose a new architecture for topic modeling that is geared for extracting key topics of interest from user generated text content. We show that it works better than existing topic modeling techniques such as LDA. We also study its performance on non-user generated content, and find that it works equally well on such datasets. In particular, we show results on the set of all the accepted papers from NIPS 2015 conference. We also demonstrate its efficacy when run on a single-document by running it on the annual financial report of Apple."}, {"heading": "2 Background", "text": "In 2003, LDA was introduced as a generative probabilistic topic modeling model to manage large document archives, [6]. The goal was to discover the main themes across a large collection of documents, and organize the collection according to the discovered themes. The intuition behind LDA is that documents have a latent topic structure; each document is a probability distribution over topics, and each topic in turn is a distribution over a fixed vocabulary. Words in the vocabulary are represented by 1-of-V encoding, where V is the size of the vocabulary \u2014 there is no notion of semantic similarity between words. The central problem is to use the observed documents to infer the hidden topic structure.\nSince then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].\nOur algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9]. In this formulation, each word is represented by a vector that is concatenated or averaged with other word vectors in a context, and the resulting vector is used to predict other words in the context. The outcome is that after the model is trained, the word vectors are mapped into a vector space such that semantically-similar words have similar word representations (e.g. \u201cstrong\u201d is close to \u201cpowerful\u201d). [1] used a feedforward neural network with a linear projection layer and a non-linear hidden layer to jointly learn the word vector representations and a statistical language model. [8] and [9] leveraged distributed word vectors to show that neural network based models match or outperform feature-engineered systems for standard Natural Language Processing (NLP) tasks that include part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. [10] introduced a technique to learn better word embeddings by incorporating both local and global document context, and account for homonymy and polysemy by learning multiple embeddings per word.\n[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data. The model avoids non-linear transformations and therefore makes training extremely efficient. This enables learning of highdimensional word vectors from huge datasets with billions of words, and millions of words in the vocabulary. High-dimensional word vectors can capture subtle semantic relationships between words. For example, word vectors can be used to answer analogy questions using simple vector algebra: vKing \u2212 vman + vwoman = vQueen, where vx denotes the vector for the word x. [17] later introduced GloVe, Global Vectors for Word Representation, which combines word-word global co-occurrence statistics from a corpus, and context based learning similar to Word2Vec to deliver an improved word vector representation. Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others."}, {"heading": "3 Methodology", "text": "In this section, we describe our algorithm for identifying the key topics underlying a corpus. Our approach is three-fold. First, we build distributed word embeddings for the vocabulary of the corpus. Second, we cluster the word embeddings usingK-means to yield K clusters of semanticallyrelated words. We implement K-means using the standard euclidean distance metric, but with all word vectors normalized so that their norm is unity. We identify each of the K clusters obtained through K-means as a topic. Third, we score the importance of each topic and label the keywords that best describe the topic.\nThe core of Vec2Topic lies in the third step, i.e., scoring the importance of topics. So, we will focus on describing this next in Section 3.1. For ease of exposition, we proceed by assuming that the first and second steps have been completed. That is, we are in possession of good distributed word embeddings for all nouns and nouns phrases in the corpus, and that these have been clustered using K-means. We formally discuss how to build the word embeddings in Section 3.2. Algorithm 1 describes all steps of Vec2Topic."}, {"heading": "3.1 Scoring importance of topics", "text": "We put forth the basic idea that: a core topic in a corpus should contain a large number of words that relate to that topic, and further, these words, though distinct, should have similar meanings. For instance, we expect the email corpus of Mr. Kaminski (who was Enron\u2019s Managing Director of Research) to contain many modeling-related words such as \u201cprobability\u201d, \u201cstandard deviation,\u201d \u201ccovariance\u201d, etc. Further, when considered in the vector space of the word embeddings, these words should form a tight cluster with a large number of words that are near each other. We do expect additional clusters to form, for instance, time-related words such as days of the week and names of months would have vector representations that would be close to each other. However, such a time-related cluster would be much less dense than the modeling-related one because modeling would consist of a much larger number of words that are closer in the high-dimensional space. In this fashion, identifying the densest clusters can help us identify the core topics.\nTo formally capture this notion of cluster density, we use the notion of depth of a word. In particular, we take all the words in our vocabulary and perform hierarchical (agglomerative) clustering on the corresponding word vectors. This is an iterative method that starts by placing all words in their own cluster, and then the proceeds by merging the two closest clusters in each step, until a single cluster that includes all words is obtained. We use the typical cosine distance measure for this clustering approach, i.e., for any two word vectors wx and wy, we define the distance\nd(wx, wy) = 1\u2212 wx\n\u2016 wx \u20162 \u00b7\nwy\n\u2016 wy \u20162 , (1)\nwhere \u2016 \u00b7 \u20162 denotes the Euclidean norm. As an output of this clustering method, we obtain a dendrogram, which is a tree with each word represented by a leaf node and every non-leaf node representing a cluster that contains the words corresponding to its leaf nodes. The root node of the dendrogram reflects the cluster of all words. We modify this dendrogram by normalizing the length of all links between parent and children nodes to unity. We then define the depth of a word as the (minimum) distance of the (leaf) node that corresponds to the word from the root node on this modified tree. Thus, the depth of a word can also be understood as the minimum number of links that need to be traversed to travel from the word node to the root node. That is,\ndepth(w) = No. of links between root node and w. (2)\nTable 3 lists the ten deepest words for Mr. Kaminski\u2019s email corpora. We next explain this measure in more detail by focusing on Mr. Kaminski\u2019s email corpus. Figure 1 (left) displays the modified dendrogram corresponding to the agglomerative clustering over the vocabulary. This dendrogram preserves the clusters of the agglomerative clustering approach, but creates links with unit length at each level. At the top of the dendrogram is the root node; words that cluster close to the root do not have many semantically-similar words close to them in vector space, and hence have low depth. The words with the highest depth are the ones that have many semantically-similar words that are close to them, and therefore, are farthest away from the root. The topic cluster that the deepest words belong to reflects the core topic for the corpus.\nFigure 1 (right) displays our word embeddings in two-dimensional space. We clearly see that words with the highest depth are very close to each other in vector space, and the topic they belong to is the core topic. Our word embeddings are 325-dimensional vectors (which will be described in more detail in Section 3.2) so visualizing them in two-dimensions perfectly is not possible. However,\nwe use the t-SNE technique [24], which does a remarkable job of dimension-reduction by trying to ensure that the distances in two dimensions are reflective of those in the higher-dimensional space.\nWe notice that the depth measure helps identify the core topics, however, the deepest words are by themselves not sufficient to understand or label the overall topic to which they pertain. For instance, Mr. Kaminski\u2019s deepest words are \u201capproximation, probability, covariance,\u201d which are quite specialized and do not explicitly reveal their overall topic.\nTo identify words that provide good labels for the key topics, we next put forth our second idea: topical words that are indicative of core topics would be used in the context of a large number of different words. For instance, a modeling-related topic in Mr. Kaminski\u2019s email corpus should contain \u201cmodel\u201d or a similar word as a topical word, and this word should be used in the context of many words, which would indicate that Mr. Kaminski is communicating about different types of models, with different people, etc. We use the degree of a word as another independent measure that quantifies this notion of \u201ctopicality.\u201d We compute the degree of the words as follows: we build a graph of the corpus by dividing the corpus into individual sentences; we create a link between any two words of the vocabulary V that co-occur in the same sentence; then, we define the number of neighbors of each word in this graph as the degree of that word. That is,\ndegree(w) = No. of unique words co-occurring with w. (3)\nWe next combine the depth and degree measures in (2) and (3) to create an overall score for each word. We do so by normalizing depth and degree measures by their maximum values across the vocabulary. The degree measure tends to be quite skewed, so we take a logarithmic transformation before scaling by the maximum. Formally, the score is defined as: for v \u2208 V ,\nScore(v) =\n(\ndepth(v)\nmaxu\u2208V depth(u)\n)\u03b1\n\u00d7\n(\nlog(1 + degree(v))\nlog(1 + maxu\u2208V degree(u))\n)\u03b2\n,\n(4)\nwhere \u03b1 and \u03b2 are normalization parameters; in all our experiments we set \u03b1 = \u03b2 = 1 and we discuss a generalization of this in Section 4.4. Intuitively, the deepest words pertain to core topics but can be too specialized to indicate the topic. So, to identify the topics, we need to identify words that are similar in meaning to the deep words but are also used in various contexts. That is, these words should have both high depth and high degree. The score in (4) reflects this intuition by formally multiplying the two measures, after scaling them appropriately. Note that because we take the logarithmic transformation of degree, we consider (1 + degree(v)) to avoid the argument of the logarithm from taking the value zero.\nTable 3 displays the top 10 words based on this score for Mr. Kaminski\u2019s corpus. We believe that this list of words seems quite consistent with what one would expect to be his important words and provides labels for the core topics appropriately. For instance, \u201canalysis\u201d and \u201cmodel\u201d are the top words, which represent the core topic of modeling quite well.\nOnce the words have been scored, we compute a score for each topic by averaging the score of the words that comprise the corresponding cluster (based on K-means clustering). That is,\nScore of Topici =\n\u2211\nv\u2208Topici Score(v)\n|Topici| . (5)\nThus, we obtain a sorted list of topics, with higher scoring topics identified as being more important than lower scoring ones. Tables 1 and 2 depict the topics with the highest three scores from Mr. Lay\u2019s and Mr. Kaminski\u2019s corpora, respectively. In both cases, the topics are numbered in decreasing order of the score; Topic 1 has the highest score. Notice that within each topic, the constituent words are also ranked based on the word score. In this manner, each topic is represented by the words most relevant to that topic. Figure 2 visualizes K = 10 topics using the vocabulary of Mr. Kaminski\u2019s emails. The ten clusters are labeled in different colors; the top-3 topics are numbered as in Table 2 and their top-3 words are also displayed."}, {"heading": "3.2 Building distributed word embeddings", "text": "A natural way to build distributed word embeddings is to apply a standard technique such as skip-gram [12] on the given corpus. This approach however assumes that the training corpus is large enough, and words show up in several different contexts to develop semantically accurate word representations; trained models have been built on corpuses that have billions of tokens and millions of words in the vocabulary [12, 13]. User generated data, however, tends to have much smaller corpus and vocabulary size, which does not allow understanding the relative meaning of words sufficiently; nevertheless, training this model on the data does capture how these words are used in the user\u2019s context.\nTo overcome this limitation, we take a two step approach: we learn \u201cglobal\u201d word embeddings using the skip-gram technique on a knowledge-base \u2014 these word embeddings tend to capture the generic meaning of words in widely used contexts. We then augment these with \u201clocal\u201d word embeddings learned from the user\u2019s data to capture their context to generate our desired word vectors. Denoting the knowledge-base vectors by k : V \u2192 R\u03ba, where V is the knowledge-base vocabulary, and our local word vectors by \u2113 : V \u2192 R\u03bb, we use the concatenated word vectors\nAlgorithm 1 Vec2Topic Input: Text corpus Knowledge-base k of word embeddings Number of topics to extract, K Build word embeddings: 1: Extract vocabulary of nouns and noun phrases V from corpus 2: Learn distributed word representations on corpus, \u2113v for v \u2208 V 3: Compute word vectors wv = [kv; \u2113v] for v \u2208 V Identify topics: 4: Perform K-Means clustering to obtain Topici for i = 1, . . . ,K Score topics: 5: Perform agglomerative clustering on {wv : v \u2208 V } 6: Compute depth(v) for v \u2208 V using (2) 7: Compute the degree of each word, degree(v) for v \u2208 V , using the sentence-level co-occurrence\ngraph as in (3). 8: For v \u2208 V , compute:\nScore(v) =\n(\ndepth(v)\nmaxu\u2208V depth(u)\n)\u03b1( log(1 + degree(v))\nlog(1 +maxu\u2208V degree(u))\n)\u03b2\n,\nwhere \u03b1, \u03b2 are normalization parameters; all our experiments use \u03b1 = \u03b2 = 1. 9: Compute average score of each topic as the average score of constituent words using (5)\nOutput: Score(v) for v \u2208 V : the score for each word Topic[i] for i = 1, . . . ,K: the ranked list of topics\nw = [k(v); \u2113(v)] for v \u2208 V , with w(v) \u2208 Rd, and d := \u03ba + \u03bb. For our experiments, we first use the skip-gram technique on the Wikipedia corpus that contains about three million unique words, and about six billion total tokens. We set \u03ba = 300 and \u03bb = 25 to obtain 325-dimensional word vectors after combining the knowledge-base and local word vectors.\nIt is educational to consider the cases in which the augmentation is not performed and the word vectors are derived either solely based on the given corpus, or solely based on the knowledgebase. Table 4 presents the top four topics obtained for both of these cases on Mr. Kaminski\u2019s email corpus. Comparing the two cases, we see that when using the local word vectors alone, word semantics are not captured that well, and thus the topics obtained are more diffuse. When using knowledge-base word vectors, the semantics dominate the topic definition with similar meaning words pulled into topics from all over the corpus, without consideration for the context. When one augments both these word vectors, we obtain a good balance that yields the sharper results of Table 2."}, {"heading": "4 Discussion", "text": "In this section, we discuss various aspects of Vec2Topic. First, we discuss its speed in Section 4.1, then, in Section 4.2, we discuss its robustness to the number of topics to extract. In Section 4.3 we demonstrate its performance when implemented on: a non-user generated dataset (NIPS 2015 papers) and a small dataset (a single-document, Apple\u2019s 10-K financial report) . Finally, in Section 4.4, we discuss some additional considerations in implementing the algorithm."}, {"heading": "4.1 Algorithm complexity", "text": "To understand the complexity of the algorithm, we consider each of its key components. We use V\u0304 to denote the entire vocabulary of the corpus (recall that |V | is the vocabulary of nouns and noun phrases).\n1. Hierarchical (agglomerative) clustering: we use the fastcluster method, [16], which has a complexity of \u0398(|V |2).\nIn all our experiments, the algorithm does not take more than a few minutes to run. Table 5 provides details on the sizes of all the datasets considered in this paper and the runtime of the algorithm. The slowest component of Vec2Topic is building word vectors, which took majority of the run time. However, this method is quite scalable and has been used to build word vectors on a Google news dataset of 100 billion tokens in a couple of days. Indeed, we trained our knowledgebase vectors on Wikipedia corpus, which contains about 6 billion tokens and about 2.8 million unique words in about 6 hours.\nThe K-means clustering has an efficient implementation as well, and scales with sizes well. Further, computing the degree measure can also be done very efficiently as it involves hashing as well as sparse matrix operations. So, as the scale of the corpus grows, we believe the critical component of the algorithm from a run time perspective is hierarchical clustering. For |V | = 5, 000 with d = 325, this component completes in about 2.5 seconds. This scales up quadratically so that |V | = 10, 000 takes 10 seconds, and |V | = 50, 000 takes slightly more than four hours. Given that V only comprises nouns and noun phrases, we believe that its size will be an order smaller than the total unique words in the document, and thus overall, the algorithm has the ability to scale well with data size.\n2Based on a 4-core 4GHz Intel processor with 32GB RAM. Python 2.7 was used on a Ubuntu machine with the libraries Numpy, Scipy, Scikit-learn (for K-means clustering), Fastcluster (for agglomerative clustering), and Gensim (for extracting bigrams and running Skip-gram); many components were not parallelized. The runtime is the total time after text corpus is presented as input and includes time for any pre-processing. Our pre-processing included running a lemmatizer on the corpus, and in the case of an email corpus, removing any names that were included in to, from, cc fields. Note: runtime excludes the time taken to load the knowledge-base vectors into memory, which was about 40 seconds."}, {"heading": "4.2 Effect of changing K", "text": "Similar to other topic modeling methods, our algorithm takes as input the number of topics to be extracted, K. For the results described so far, we fixed K = 10. Table 6 displays the top-four topics for Mr. Kaminski when the number of topics K is set to 5 and 50. Notice that in both cases, the overall word score remains unaffected by the change in K, the only change is the clustering of topics. Observe that the top topic is identical for these two cases, and for the case K = 10 of Table 2. Even the second-ranked topic is quite similar. This illustrates the robustness of our approach in capturing the key topics from a corpus.\nNotice that the case K = 5 is quite crude in the sense that all words are clubbed into 5 clusters and hence we observe a diffuse topic such as Topic 4. On the other hand K = 50 is much more pointed and picks up a large number of small clusters, which leads to identifying many specialized topics. In our extensive experiments, we found that moderate K values, such as K = 10 or 20, work well in identifying key topics."}, {"heading": "4.3 Other datasets", "text": "NIPS 2015 Dataset We next consider a dataset consisting of the full-text of all papers accepted at the NIPS 2015 conference.3 Table 7 lists the top-3 topics obtain from applying Vec2Topic compared with three relevant LDA-based topics (we ran LDA with K = 10 to be consistent with our approach). We note that in this case LDA does a very good job of capturing topics. In fact, the table provides a good insight into how Vec2Topic differs from LDA. Our algorithm focuses on concepts across the documents in the dataset, whereas LDA appears to focus on concepts within each document in the dataset. Consider Topic 1 and Topic A, which contain two common words (\u201cgraph\u201d and \u201cmatrix\u201d). Topic 1 from Vec2Topic places these words along with others that tend to be used in similar themes, such as graph theory. Although Topic A also appears to relate to the topic of graph theory, it is more focused on the context of each paper, and contains broader words such as \u201cdata\u201d and \u201cproblem.\u201d Again Topic 2 and Topic B appear to be about inference, but Topic 2 contains terms that relate to inference such as \u201calgorithm\u201d, \u201cMCMC\u201d, whereas Topic B seems to be more about the context in which inference is done in the papers with\n3We used Andrej Karpathy\u2019s scripts available at https://github.com/karpathy/nipspreview.git to scrape this information. As part of pre-processing we removed the References section of each paper; we discuss the value of doing so in Section 4.4.\nwords: \u201csubmodular\u201d, \u201cfunction\u201d, \u201cgreedy\u201d, etc. Finally, Topic 3 is about general concepts such as \u201cproblem\u201d, \u201ccomplexity\u201d, \u201cefficiency\u201d, etc., that is once again a common topic across all the papers, whereas Topic C appears to be about learning algorithms.\nApple\u2019s 2015 10-K Financial Report We next apply Vec2Topic to a single document. We consider Apple\u2019s financial report, the 10-K document for 2015. We apply Vec2Topic with K = 10. Table 8 displays the top-4 topics from the report along with the top-10 highest scoring words. We notice that Topic 1 corresponds to the financial accounting terminology that one expects to be central to the financial report (asset, tax, income, etc.). Topic 2 is about the nature of the company (produce, software, hardware, etc.); Topic 4 provides specifics on these (iOS, iPhone, iPad, etc.). Topic 3 pertains to financial considerations of the company (cost, risk, volatility, etc.). Notice that the list of top words goes across the different topics and contains both financial and product-related words (asset, product, software, income, etc.). We would like to point out that LDA does not directly apply to a single document so we do not provide the comparison."}, {"heading": "4.4 Other implementation considerations", "text": "Normalization parameters, \u03b1 and \u03b2 The word score computed in (4) uses the normalization parameters \u03b1 and \u03b2, which were fixed as \u03b1 = \u03b2 = 1 in the experiments. We briefly discuss how these may be modified to improve the performance of the algorithm in some cases. The score combines two different types of measures: depth and degree, which are distributed quite differently over the vocabulary. We need to ensure that the role both these measures play in identifying the core topics is \u201cbalanced.\u201d In most documents, setting \u03b1 = \u03b2 = 1 works because degree is normalized by the logarithmic transformation. However, we find that a more robust score can be obtained by choosing \u03b1 and \u03b2 so that after this power transformation the depth and degree measures both have median values equal to 1/2. Formally, we choose \u03b1 so that we have\nMedianv\u2208V\n(\ndepth(v)\nmaxu\u2208V depth(u)\n)\u03b1\n= 1\n2 . (6)\nThe value for \u03b2 can be computed in an analogous fashion. For datasets in this paper, the values of \u03b1 and \u03b2 so computed are quite close to one; incorporating this into the algorithm leaves all the results in the paper qualitatively unchanged. However, in our extensive experiments we did find some datasets for which using this normalization method improved performance.\nPerformance In this paper, we have demonstrated that Vec2Topic works well both for usergenerated and non-user generated datasets and even for single documents. The ability of the algorithm to capture core topics depends on how well-defined these topics are. For documents with a clear core topic, the algorithm can extract it even when the size of the document is quite small. In our extensive experiments, we found it to perform well even for documents with a couple of hundred tokens in total, such as a news article. However, in that case, we did not build local word embeddings and instead used the knowledge-base word vectors alone. The reason this works is because a document such as a news article is written around a central theme or topic, and hence the context of usage is not that consequential for identifying the topic.\nWe would like to point out that though Vec2Topic does not explicitly use a probabilistic approach, its output is not deterministic because it depends on building local word embeddings. In our experiments, we used the skip-gram approach, which results in slightly different word embeddings each time it is run. This effect is more pronounced with smaller datasets, so while the degree score remains constant across runs, the depth score may vary slightly. Nevertheless, the overall results and the top core topics are qualitatively quite robust to this effect.\nThe performance of the algorithm is also limited by the quality of the knowledge-base vectors. In our experiments, we used the English Wikipedia for our knowledge-base. One of the limitations therein was with respect to foreign words. This knowledge-base contains many foreign words (for instance, movie names) without sufficient context. This implies that the learned word representations of these words are not semantically accurate. To alleviate this issue, we removed such words (this issue came up with Mr. Kaminski\u2019s emails). Another related issue is that of people names. The word vectors for these tend to be quite close to each other, and if there are a large number of names, then this affects the depth measure. Clearly, people names would not make for key topics. So, to resolve this issue, in the email corpora, we remove names of people listed in the address fields. In the NIPS dataset, we remove the References section of the paper. Another way to resolve this issue is to run the dataset through a Named Entity Recognition algorithm."}, {"heading": "5 Conclusions", "text": "In this paper we propose a novel technique for topic modeling that leverages understanding of word semantics using high-dimensional word vectors. We showed that Vec2Topic works well to extract a user\u2019s key topics of interest across their own generated content \u2014 it also ranks these topics, and identifies keywords that best describe it. We contrasted it with the state-of-the-art topic modeling algorithm LDA, and observed that it works much better when the topic keywords are spread across the various documents, and are surrounded by several contextual words that are generic in nature. Further we observe that the technique is not limited to user generated content; it works equally well on more structured documents such as scientific papers, news articles, blogs, web pages, etc. It is also fairly robust to the corpus size \u2014 it can scale from a single document to a large collection.\nOne of our ongoing efforts is focused on extending the algorithm to identify phrases \u2014 sequences of keywords that together capture the user\u2019s key interests. For example, turning to the example of a professional who works on datacenter servers, the phrase \u201ccloud server virtualization\u201d conveys a lot more topical context than cloud, server and virtualization individually. Each of these words may show up in several contexts in the user\u2019s data \u2014 we therefore need a way to rank such phrases. Also, these phrases differ from bigram/trigram phrases in the sense that their words may not show up consecutively in the user\u2019s data. We believe extending topic modeling in this direction is an interesting avenue for future study."}, {"heading": "6 Acknowledgments", "text": "The authors would like to thank Achal Bassamboo for many useful discussions."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Probabilistic topic models", "author": ["D.M. Blei"], "venue": "Commun. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "The nested Chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "Journal of the ACM (JACM), 57(2):7", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 113\u2013120. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "A correlated topic model of science", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "The Annals of Applied Statistics, pages 17\u201335", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Hierarchical relational models for document networks", "author": ["J. Chang", "D.M. Blei"], "venue": "The Annals of Applied Statistics, pages 124\u2013150", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 160\u2013167, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL \u201912, pages 873\u2013882, Stroudsburg, PA, USA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "arXiv preprint arXiv:1506.07285", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, abs/1301.3781", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111\u20133119. Curran Associates, Inc.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["T. Mikolov", "W.-T. Yih", "G. Zweig"], "venue": "HLT-NAACL, pages 746\u2013751", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1081\u20131088", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "fastcluster: Fast hierarchical", "author": ["D. M\u00fcllner"], "venue": "agglomerative clustering routines for r and python. Journal of Statistical Software, 53(9):1\u201318", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic pooling and unfolding recursive auto-encoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "C. Manning", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 129\u2013136", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised recursive auto-encoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151\u2013161. Association for Computational Linguistics", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the conference on empirical methods in natural language processing (EMNLP), volume 1631, page 1642. Citeseer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Hierarchical dirichlet processes. Journal of the American Statistical Association, 101:1566\u20131581", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384\u2013394. Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Topic modeling: beyond bag-of-words", "author": ["H.M. Wallach"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 977\u2013984. ACM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "State-of-the-art topic modeling algorithms, such as Latent Dirichlet Allocation (LDA, [6, 2]), have been successfully applied to discover the main topics across a large collection of documents for some time now, and are a natural candidate for solving the problem at hand.", "startOffset": 86, "endOffset": 92}, {"referenceID": 1, "context": "State-of-the-art topic modeling algorithms, such as Latent Dirichlet Allocation (LDA, [6, 2]), have been successfully applied to discover the main topics across a large collection of documents for some time now, and are a natural candidate for solving the problem at hand.", "startOffset": 86, "endOffset": 92}, {"referenceID": 5, "context": "In 2003, LDA was introduced as a generative probabilistic topic modeling model to manage large document archives, [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 24, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 244, "endOffset": 248}, {"referenceID": 3, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 310, "endOffset": 313}, {"referenceID": 2, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 402, "endOffset": 409}, {"referenceID": 21, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 402, "endOffset": 409}, {"referenceID": 4, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 476, "endOffset": 479}, {"referenceID": 6, "context": "Since then, a huge body of work has been done to relax and extend the statistical assumptions made in LDA to uncover more sophisticated structure in the text: Topic models that assume that topics generate words conditional on the previous word [25]; dynamic topic models that respect the ordering of documents [4]; Bayesian non-parametric topic models that generate a hierarchy of topics from the data [3, 22]; Correlated topic models that allow topics to exhibit correlation [5]; Relational topic models that capture both the topic model and a network model for the documents [7].", "startOffset": 577, "endOffset": 580}, {"referenceID": 0, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 7, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 14, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 22, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 8, "context": "Our algorithm is inspired by the recent work in learning word vector representations using neural networks [1, 8, 15, 23, 9].", "startOffset": 107, "endOffset": 124}, {"referenceID": 0, "context": "[1] used a feedforward neural network with a linear projection layer and a non-linear hidden layer to jointly learn the word vector representations and a statistical language model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] and [9] leveraged distributed word vectors to show that neural network based models match or outperform feature-engineered systems for standard Natural Language Processing (NLP) tasks that include part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8] and [9] leveraged distributed word vectors to show that neural network based models match or outperform feature-engineered systems for standard Natural Language Processing (NLP) tasks that include part-of-speech tagging, chunking, named entity recognition, and semantic role labeling.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "[10] introduced a technique to learn better word embeddings by incorporating both local and global document context, and account for homonymy and polysemy by learning multiple embeddings per word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[12, 13, 14] introduced Word2Vec and the Skip-gram model, a very simple method for learning word vectors from large amounts of unstructured text data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[17] later introduced GloVe, Global Vectors for Word Representation, which combines word-word global co-occurrence statistics from a corpus, and context based learning similar to Word2Vec to deliver an improved word vector representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 18, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 19, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 20, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 150, "endOffset": 166}, {"referenceID": 10, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 197, "endOffset": 205}, {"referenceID": 25, "context": "Word vectors are an attractive building block and are being used as input for many neural net based natural language tasks such as sentiment analysis [18, 19, 20, 21], question and answer systems, [11, 26], and others.", "startOffset": 197, "endOffset": 205}, {"referenceID": 23, "context": "we use the t-SNE technique [24], which does a remarkable job of dimension-reduction by trying to ensure that the distances in two dimensions are reflective of those in the higher-dimensional space.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "A natural way to build distributed word embeddings is to apply a standard technique such as skip-gram [12] on the given corpus.", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "This approach however assumes that the training corpus is large enough, and words show up in several different contexts to develop semantically accurate word representations; trained models have been built on corpuses that have billions of tokens and millions of words in the vocabulary [12, 13].", "startOffset": 287, "endOffset": 295}, {"referenceID": 12, "context": "This approach however assumes that the training corpus is large enough, and words show up in several different contexts to develop semantically accurate word representations; trained models have been built on corpuses that have billions of tokens and millions of words in the vocabulary [12, 13].", "startOffset": 287, "endOffset": 295}, {"referenceID": 15, "context": "Hierarchical (agglomerative) clustering: we use the fastcluster method, [16], which has a complexity of \u0398(|V |2).", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "Building word vectors: we use the skip-gram model [12], which has a running complexity of E \u00d7 T \u00d7Q, where E is the number of iterations, which is typically 5 \u2212 50, T is the total number of words or tokens in the corpus, and Q = c\u00d7 (x+x log2(|V\u0304 |)), where c is the context size (in all our experiments we set c = 5) and x \u2208 {\u03ba, \u03bb} denotes the dimensionality of the word vectors used in training.", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.", "creator": "LaTeX with hyperref package"}}}