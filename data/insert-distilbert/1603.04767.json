{"id": "1603.04767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2016", "title": "Evaluating the word-expert approach for Named-Entity Disambiguation", "abstract": "named entity disambiguation ( ned ) is the task of safely linking a named - entity mention function to an instance in a knowledge - base, typically wikipedia. this assessment task is closely related to word - relevant sense disambiguation ( wsd ), where the recently supervised word - expert approach has prevailed. in this work. we present the results of the word - expert approach to ned, noting where one classifier is built for each target entity at mention string. considering the resources necessary precisely to build the system, a dictionary and a set of training instances, have been deemed automatically derived from wikipedia. we provide empirical evidence of the value of this approach, significantly as well as a study of the differences between different wsd and ned, these including ambiguity and synonymy statistics.", "histories": [["v1", "Tue, 15 Mar 2016 17:16:02 GMT  (776kb,D)", "http://arxiv.org/abs/1603.04767v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["angel x chang", "valentin i spitkovsky", "christopher d manning", "eneko agirre"], "accepted": false, "id": "1603.04767"}, "pdf": {"name": "1603.04767.pdf", "metadata": {"source": "CRF", "title": "Evaluating the word-expert approach for Named-Entity Disambiguation", "authors": ["Angel X. Chang", "Valentin I. Spitkovsky", "Christopher D. Manning", "Eneko Agirre"], "emails": ["angelx@cs.stanford.edu,", "vals@cs.stanford.edu,", "manning@cs.stanford.edu,", "e.agirre@ehu.eus"], "sections": [{"heading": "1 Introduction", "text": "Construction of formal representations from snippets of free-form text is a long-soughtafter goal in natural language processing (NLP). Grounding written language with respect to background knowledge about real-life entities and world events is important for building such representations. It also has many applications in its own right: text mining, information retrieval and the semantic web [Weikum and Theobald, 2010]. Wikipedia and related repositories of structured data (e.g. WikiData or DBpedia) already provide extensive inventories of named entities, including people, organizations and geo-political entities.\nAn individual named-entity string may refer to multiple entities and the process of resolving the appropriate meaning in context is called entity linking (EL) or named entity disambiguation (NED). The former terminology (EL) stresses the importance of linking a mention to an actual instance in the given knowledge-base [McNamee and Dang, 2009]. We prefer the latter term (NED), which focuses on the potential ambiguity among several possible instances. It highlights the connection to the closely\nar X\niv :1\n60 3.\n04 76\n7v 1\n[ cs\n.C L\nrelated problem of word-sense disambiguation (WSD), and was used in some of the earliest works [Bunescu and Pasca, 2006, Cucerzan, 2007].\nAs our first example of NED, consider the following sentence: \u201cChampion triple jumper Jonathan Edwards has spoken of the impact losing his faith has had on his life.\u201d Jonathan Edwards may refer to several people \u2014 Wikipedia lists more than ten,1 including the intended athlete2 \u2014 as well as a residential college at Yale and a music record. Figure 1 shows a disambiguation page for the string, and Figure 2 an excerpt from the athlete\u2019s Wikipedia article.\nIn addition to disambiguation pages that list possible entities to which a canonical string like \u201cJonathan Edwards\u201d may refer, many naturally-occurring entity mentions in regular Wikipedia articles are also cross-referenced. For instance, the first sentence of the entry for Jonathan Edwards (the athlete) includes hyperlinks (shown in blue) to \u201cCBE,\u201d \u201cBritish\u201d and \u201ctriple jumper.\u201d The first link references an article on the Order of the British Empire.3 Anchor-text (words in blue) often exposes alternate ways of referring to entities: e.g., a member of the Order of the British Empire can be called \u201cCBE.\u201d Figure 3 shows five sentences that link to Jonathan Edwards (the athlete), and Figure 4 shows three sentences with hyperlinks to other people called Jonathan Edwards. Anchors present a rich source of disambiguation information. Aggregating over all occurrences of hyper-text \u201cJonathan Edwards,\u201d we can compute that, most of the time, it refers to the theologian. Yet by analyzing the context of each named entity\u2019s occurrence, we could conclude that a span like \u201c. . . triple jumper Jonathan Edwards . . . \u201d is more similar to mentions of the athlete. These linked spans from Wikipedia can also be\n1http://en.wikipedia.org/wiki/Jonathan_Edwards_(disambiguation) 2http://en.wikipedia.org/wiki/Jonathan_Edwards_(athlete) 3http://en.wikipedia.org/wiki/Order_of_the_British_Empire\nused to obtain alternative terms that refer to the same entity. Figure 5 shows a variety of ways in which Wikipedia refers to Bill Clinton.\nAs online encyclopedias grow in size, entities and ideas that are of interest to even small communities of users may get their own Wikipedia pages with relevant descriptions.4 Thanks to the hyper-linked nature of the web, many online mentions have already been annotated with pointers to corresponding articles, both within Wikipedia and from external sites. As a result, large quantities of freely available information \u2014 suitable for supervised machine learning algorithms \u2014 already exist, obviating the need for costly manual annotations that are typically associated with the training of traditional NLP systems in general and WSD in particular.\nNED is a disambiguation task that is closely related to WSD, where the goal is to disambiguate open-class words (i.e., common nouns, adjectives, verbs and adverbs) with respect to a sense inventory, such as WordNet [Fellbaum, 1998]. The extensive WSD literature [Agirre and Edmonds, 2006, Navigli, 2009] has shown that building a supervised classifier for each target lemma \u2014 the so-called word-expert approach \u2014 outperforms other techniques [Zhong and Ng, 2010].\nIn this work we propose an architecture for NED following the word-expert approach, where we build a classifier for each named-entity mention, with two main modules: (1) A candidate generation module which, given a string, lists all potentially relevant entities. This module is based on a static dictionary, which also lists the popularity of each of the entities, and can thus serve as a standalone context-independent\n4http://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia\nWikipedia article Example sentence Target Wikipedia article Mission (Christian) In North America, missionaries\nto the native Americans included Jonathan Edwards, the well known preacher of the Great Awakening , who in his later years retired from the very public life of his early career.\nJonathan Edwards (theologian)\nOne Day Closer One Day Closer is the ninth studio album (eleventh total album) released by singer songwriter Jonathan Edwards and features many ballads and love songs. Jonathan Edwards (musician)\nBuildings of Jesus College, Oxford Jonathan Edwards (principal from 1686 to 1712) is reported to have spent \u00a31,000 during his lifetime on the interior of the chapel. Jonathan Edwards (academic)\nFigure 4: Example sentences from Wikipedia where the referent of \u201cJonathan Edwards\u201d is not the athlete.\ndisambiguation module. (2) A context-sensitive supervised classifier that selects the entity which is most suited for the context of the mention. The classifiers use the kind of features routinely used in a WSD classifier. To our knowledge, this is the first time a NED system following the word-expert approach is reported, although early work on Wikification already hinted at its usefulness [Csomai and Mihalcea, 2008].\nOur system is based on two main resources: (i) a dictionary listing all candidate entities for each surface string, together with their frequencies;5 and (ii) a set of training instances for each target mention. The bulk of the information comes from Wikipedia but we have further complemented the dictionary with web counts from a subset of a 2011 Google crawl. The dictionary\u2019s release was documented in a short conference paper [Spitkovsky and Chang, 2012], which we extend with additional explanations and analyses here.\n5http://www-nlp.stanford.edu/pubs/crosswikis-data.tar.bz2\nWe present a detailed analysis of the performance of the components and variations of our NED system, as applied to the entity linking task of the NIST Text Analysis Conference\u2019s (TAC)6 knowledge-base population (KBP) track [Ji and Grishman, 2011]. The task focuses on several target entity mentions, which makes it well suited for our word-expert approach. In the future we would like to explore other datasets which include all mentions in full documents [Hoffart et al., 2011]. Our final results are quite strong despite the simplicity of the techniques used, with the dictionary\u2019s raw frequencies already performing extremely well. We focus on the candidate generation and disambiguation modules, leaving aside mention detection and the task of NIL detection, where mentions which refer to entities not listed in the knowledge-base have to be detected [Durrett and Klein, 2014].\nIn addition, we will study differences between the closely related worlds of WSD and NED. In WSD, an exhaustive dictionary is provided, while in NED, one has to generate all candidate entities for a target string \u2014 a step that has been shown to be critical to success [Hachey et al., 2012]. In WSD very few occurrences correspond to senses missing in the dictionary, but in NED this problem is quite prevalent. We will also show that ambiguity is larger for NED. On the bright side, there is a lot of potential training data for NED, for instance, in the form of human-generated anchortexts. This article shows that an architecture based on WSD methods can work well for NED, and that it is feasible to model candidate generation with a static dictionary. We will also compare ambiguity, synonymy and inter-annotator agreement statistics of both problems.\nNote that the authors participated in the TAC-KBP Entity Linking tracks with preliminary versions of the system reported in this article. Those systems, alongside all participant systems7, were reported in publicly available proceedings [Agirre et al., 2009, Chang et al., 2010, 2011].\nThe paper is organized as follows: we first present related work, followed by the architecture of our system (Section 3). The dictionary for candidate generation is pre-\n6http://www.nist.gov/tac/ 7There was no peer-review, and all papers were accepted.\nsented next (Section 4), followed by methods that produce the training data and build the supervised classifier (Section 5). Section 6 introduces the TAC-KBP dataset used for evaluation, followed by the adaptation of our system (Section 7). Section 8 explores several alternatives to our approach and analyzes their performance on development data, as well as the final results and discussion. We then compare our system to related work, draw conclusions and propose future directions."}, {"heading": "2 Related Work", "text": "We will now review several NLP problems that are closely related to NED, including wikification and WSD, as well as previous work on NED. For clearer exposition, we group the previous work into three sections: first, the earlier and more influential contributions, followed by work on wikification, and finally NED systems. We briefly touch on state-of-the-art techniques used in NED, specifically, handling of candidate generation (i.e., our dictionary), and disambiguation. A full comparison of our results to those of the latest state-of-the-art systems can be found in Section 8.7."}, {"heading": "2.1 Related Problems", "text": "NED is related to several problems in NLP. For instance, it presupposes that mentions of named entities have already been identified in text, building up from the named entity recognition (NER) task [Marsh and Perzanowski, 1998, Tjong Kim Sang and De Meulder, 2003]. Each mention may be further labeled with a broad semantic category \u2014 such as names of persons, organizations or locations \u2014 via named entity classification (NEC). This last task is often performed by using gazetteers to cover many known entities, in addition to training a single supervised classifier that outputs category types for input mentions given their specific contexts. Overall, we view NED as a specific instantiation of the record linkage problem, in which the task is to find records referring to the same entity across different data sources, such as data files, databases, books or websites.\nThe term record linkage was first used by Dunn Halbert [1946], in reference to resolving person names across official records held by a government. More recently, Bagga and Baldwin [1998] focused on cross-document coreference of people by first identifying coreference chains within documents and then comparing the found chains\u2019 contexts across documents. Similar problems arise in citation databases, where it is necessary to decide which mentions of authors in bibliographic records refer to the same person [Bhattacharya and Getoor, 2007]. Other typical applications that lack a predefined inventory of entities include resolving names in e-mails and web people search (WePS). In the WePS task, starting from a set of web-pages that mention a name (e.g., John Doe), the goal is to decide how many John Does there are, and who is mentioned where [Artiles et al., 2008, 2009]. The e-mails task can be tackled by assuming that each address corresponds to a distinct person and that people\u2019s identifying information can be deduced from what they write [Elsayed et al., 2008].\nThree important aspects differentiate NED from record linkage and other cross-\ndocument entity coreference tasks, such as the exercise studied at ACE 20088 and clustering of documents that mention the same entity [Mann and Yarowsky, 2003, Gooi and Allan, 2004]. These differences hinge on the existence of (1) a knowledge-base (e.g., Wikipedia) that lists gold-standard entities; (2) rich textual information describing each entity (i.e., its Wikipedia page); and (3) many explicitly disambiguated mentions of entities (i.e., incoming hyper-links furnished by Wikipedia\u2019s contributors and other, external web publishers). A more closely related task is wikification [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008, Milne and Witten, 2008, Kulkarni et al., 2009], which involves first deciding which keywords or concepts are relevant in a given text and then disambiguating them by linking to the correct Wikipedia article. Although the overall thrust of that task is different, since wikification systems target common nouns as well as named entities, its disambiguation techniques are relevant to NED (see Section 2.4)."}, {"heading": "2.2 Word Sense Disambiguation", "text": "In WSD, the task is to determine which sense of an open class content word \u2014 i.e., an adverb, verb, adjective or noun that isn\u2019t a named entity \u2014 applies to a particular occurrence of that word [Agirre and Edmonds, 2006, Navigli, 2009]. Typically, the sense inventory is taken from a dictionary, such as WordNet [Fellbaum, 1998], and fixed in advance. Dictionaries are comprehensive, covering nearly all uses of a word. Consequently, WSD systems tend to return a sense for every occurrence. For instance, the Senseval-3 lexical sample dataset [Mihalcea et al., 2004] contains 2,945 manually annotated occurrences, 98.3% of which have been assigned a dictionary sense: only 1.7% are problematic and have senses not found in the dictionary. By contrast, in NED, a significant portion of mentions are out-of-inventory (around 50% in our dataset). However, in WSD target senses are often open to interpretation, as reflected by low inter-annotator agreement, e.g., 72.5% in the Senseval-3 all-words task [Snyder and Palmer, 2004]. We will show that NED poses a better-defined problem, with less dispute about what constitutes a correct target entity, as indicated by significantly higher inter-annotator agreement.\nWSD and NED differ also in two other key properties: (i) polysemy, the expected number of senses a word might have; and (ii) synonymy, the expected number of different words that may be used to lexicalize a given concept. These statistics can be used to characterize the difficulty of an evaluation set. We compared average polysemy and synonymy values for WSD and NED, computed using gold standards (by counting senses listed in dictionaries versus those actually occurring in test data). NED scored substantially higher on both metrics, relative to WSD (see Section 6.4).\nThe best-performing WSD systems are currently based on supervised machine learning, judging by public evaluation exercises [Snyder and Palmer, 2004, Pradhan et al., 2007]. Typically, the problem is modeled using multi-class classifiers [Marquez et al., 2006], with one classifier trained for each target word (a.k.a. the word expert approach). Training examples are represented by feature vectors (see Section 5) and labeled with gold senses. At test time, inputs are processed and represented in the same\n8http://www.itl.nist.gov/iad/mig//tests/ace/2008/doc/\nway, as vectors of features, with appropriate classifiers predicting intended senses. Our proposed system architecture for NED follows this design (see Section 3).\nWSD systems can perform well when training data are plentiful. On the Senseval3 lexical sample task [Mihalcea et al., 2004], accuracies can reach as high as 72.9% with fine-grained senses and 79.3% with coarser-grained senses . Both performance numbers are well above the most-frequent sense (MFS) baselines (55.2 and 64.5%, respectively). The test set used there comprised 57 target words, each backed by at least 100 manually-annotated examples. But many words found in running text lack sufficient training instances, leading to lower performance in evaluations when all words are considered. The best accuracy reported for the Senseval-3 all words task, testing all open-class words occurring in three texts (editorial, news and fiction), was 65.1%, only slightly higher than the MFS baseline\u2019s 62.4% [Snyder and Palmer, 2004]. Unfortunately, it is expensive to produce thorough hand-tagged training data for WSD and people do not typically annotate their words with the exact sense they meant.\nIn contrast with WSD, where training data is scarce, the upside for NED is that labeled instances for many mentions are already available in large numbers, annotated by Wikipedia volunteers, since the contexts and targets of hyper-links could be used as supervision. In other respects, we find many similarities between WSD and NED: For each mention that could be used to refer to a named entity, the knowledge base may list all possible targets, as the dictionary does for WSD. E.g., for a string like \u201cJohn Edwards\u201d a disambiguation page may list all relevant entities (see Figure 1). Naturally, there are also differences: A big downside for NED is that the inventory of meanings is not explicit and, for most strings, will be badly incomplete, unlike dictionaries used in WSD.\nIn Sections 6 and 8 we will explore the relation between WSD and NED further, showing that (1) ambiguity, synonymy and incidence of dictionary misses are all higher for NED than for WSD; (2) the NED task appears better-defined, as signaled by higher inter-annotator agreement than in WSD; (3) the skew of frequencies is more extreme for NED, with MFS consequently presenting an even stronger baseline there than in WSD; (4) the high number of training instances available to NED enables better supervised training, allowing NED systems to follow the same architecture as WSD systems, using analogous preprocessing, feature sets, and classifiers; and (5) the high ambiguity of mentions encountered by NED makes a typical word expert approach more computationally expensive, but still feasible. Lastly, (6) we will discuss the feasibility of constructing a comprehensive dictionary for NED."}, {"heading": "2.3 Early Work on NED", "text": "The earliest work on NED using Wikipedia is by Bunescu and Pasca [2006], who used article titles, redirects and disambiguation pages to generate candidate entities. Similarity between a mention\u2019s context and article text provided the rankings, according to tf-idf and cosine similarity. Each article\u2019s term vector was further enriched using words from other articles in the same category. Disambiguation of mentions was local, i.e., performed separately for each one.\nCucerzan [2007] followed an overall similar design, but using context vectors that consisted of key words and short phrases extracted from Wikipedia. He disambiguated\nall named entities in text simultaneously, adding a global constraint that required target Wikipedia articles to come from the same category. Candidate lists were augmented with link information whenever a given anchor-text mentioned the same target entity from at least two different Wikipedia pages. His approach was later reimplemented (see Section 8.7) by Hachey et al. [2012], who also reimplemented the earlier system of Bunescu and Pasca [2006].\nFader et al. [2009] also generated candidates as did Cucerzan [2007]. They introduced prior probabilities \u2014 estimated from the numbers of anchors that refer to entities \u2014 in addition to considering the overlap between the context of a mention and the text of the target articles."}, {"heading": "2.4 Wikification", "text": "Most research on wikification obviates candidate generation and focuses on disambiguation. In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.\nZhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents. They used a ranker to select best-fitting entities. Although anchor context was used to compute similarity between article texts, it was not tapped for features during classification. Evaluation against their own dataset showed that improvements over local disambiguation were small. Guo et al. [2011] use direct hyperlinks between the target entity and the mentions in the context, using directly the number of such links."}, {"heading": "2.5 Current NED Systems", "text": "Given the popularity of NED, we focus this review on systems that evaluate against TAC-KBP 2009 and 2010 data \u2014 two of the most cited NED datasets \u2014 to which we will compare our own results (see Section 8.7).\nIn 2009, Varma et al. [2009] obtained the best results, using anchors from Wikipedia to train one classifier per target string, combined with querying of mention contexts against an online search engine. For candidate generation they used a complex mixture of redirect links, disambiguation pages and bold text in leading paragraphs of articles, metaphones to capture possible spelling variations and errors, and a separate module\nthat tried to find acronym expansions. In addition, they used a dynamic algorithm that returned entities matching some (but not all) tokens of a target string.\nIn 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier. McNamee [2010] had the highest-scoring submission among systems that did not use the text in target Wikipedia articles, focusing on the provided KB alone for building the list of candidate entities. A closely related system by Dredze et al. [2010] included finite-state transducers that had been trained to recognize common abbreviations, such as \u201cJ Miller\u201d for \u201cJennifer Miller.\u201d\nZheng et al. [2010] also use a number of features that resemble those of McNamee [2010]. They evaluated several learning-to-rank systems, with ListNet yielding best results. Unfortunately their systems were developed, trained and tested on the same corpus, making it unclear whether their results are comparable to those of other systems evaluated in TAC-KBP 2009. A similar system [Zhang et al., 2010] made use of a synthetic corpus, with unambiguous occurrences of strings replaced by ambiguous synonyms. Manufactured training data was then combined with Wikipedia. Unfortunately, since the overall system was developed and evaluated on the same (2009) corpus, it may have been overfitted.\nHan and Sun [2011] proposed a generative model with three components: popularities of entities, probabilities of strings lexicalizing an entity, and probabilities of entire documents given a seed entity, estimated from anchor data. They included a translation model, learning string-entity pairs from thousands of training examples. Although their results are highest for TAC-KBP 2009 published to date, they may be overstated, since some parameters were tuned using cross-validation over test data.\nRecently, Hachey et al. [2012] reimplemented three well-known NED systems [Bunescu and Pasca, 2006, Cucerzan, 2007, Varma et al., 2009], combined them, and carefully analyzed the performance of candidate generation and disambiguation components for each. They studied contributions from a variety of available candidate sources, including titles (of articles, redirects and disambiguation pages) and link anchors, as well as two additional heuristics: bold in first paragraph and hatnote templates from popular entities to corresponding disambiguation pages.\nNone of the published NED systems, except Csomai and Mihalcea [2008], uses the context of anchors in Wikipedia to train a classifier for each mention, as we do. In fact, among other relevant systems which have been tested against data sets other than TAC-KBP, all use other kinds of techniques. Given that those other systems have been evaluated on other datasets, we will skip their description here. As mentioned in the introduction, most of the papers on NED describe complex systems with many components and data sources but few ablation studies that might help understand the contribution of each.9 In contrast, our system relies solely on static dictionary look-ups and supervised classification of mentions.\n9Again, with the notable exception of work by Hachey et al. [2012], which we will discuss in detail (see Section 8.7)."}, {"heading": "3 System Architecture", "text": "Figure 6 summarizes our approach to NED, which is inspired by the word-expert technique, popular in mainstream WSD systems. We take a surface text form (mention), with the context of its occurrence, and determine an appropriate entity (meaning) to which the input string may refer. In this example, our system disambiguates the mention \u201cAbbott\u201d in the sentence \u201cThe voice was provided by Candy Candido, who briefly became Abbott\u2019s partner after Costello had died.\u201d First, a context-independent component (the dictionary) expands the string to a set of potentially-referent candidate entities, ordered by popularity; next, a context-sensitive component (supervised classifier) selects a candidate that seems most appropriate for the context.\nThe context-independent dictionary maps strings to ordered lists of possible Wikipedia titles; it also provides scores \u2014 which are indicative of conditional probabilities of each article given the string \u2014 that determine the ranking. This (untuned) component is constructed, primarily, using anchor-texts \u2014 of (internal) hyper-links between English Wikipedia articles and (external) web-links into the greater Wikipedia\u2019s pages, covering many languages (see Section 4 for a detailed description of the dictionary and its variants). According to our dictionary, the most probable entity for the mention \u201cAbbott\u201d \u2014 ignoring context \u2014 is Abbott_Laboratories.\nFrom the set of candidate Wikipedia titles, a classifier can select more appropriate entities, using not only the mention but also surrounding text, in the relevant document. This (optimized) component is trained on contexts around inter-Wikipedia links, i.e., pointers at entities mentioned in Wikipedia articles. We train a separate classifier for\nevery string: its job is to return the entity which best fits the shared mention\u2019s context (this and other variants of our supervised classifers are described in Section 5). In light of context, our system prefers the entity Bud_Abbott.\nOne could view these two components as performing (i) candidate generation, listing all possible entities for a mention; and (ii) candidate selection [Hachey et al., 2012], although our generation module also scores and ranks entities (by popularity). Following the WSD convention, we will refer to these subsystems as (i) the dictionary; and (ii) the disambiguation component."}, {"heading": "4 The Dictionary", "text": "The dictionary is a cornerstone component of our system, serving two objectives. Its primary goal is to provide a short list of candidate referent Wikipedia articles for any string that could name an entity. In addition, it provides a score that quantifies the string\u2019s affinity for each candidate entity.\nIf the dictionary fails to recognize that a given string could refer to a particular entity, then our system will not be able to return that entity. Thus, the dictionary introduces a performance bottleneck. We will use this fact to measure an upper bound on our system\u2019s performance, by means of an oracle whose choices are restricted to just the entities proposed by the dictionary.\nThe dictionary represents a set of weighted pairs \u2014 an exhaustive enumeration of all possible string-entity combinations, with corresponding strengths of association. We constructed this resource from all English Wikipedia pages (as of the March 6th, 2009 data dump) and many references to Wikipedia from the greater web (based on a subset of a 2011 Google crawl). Individual string-entity co-occurrences were mined from several sources:\n\u2022 Article titles that name entities, e.g., \u201cRobert Redford\u201d from http://en. wikipedia.org/wiki/Robert_Redford.10 Many title strings had to be processed, e.g., separating trailing parentheticals, like \u201c(athlete)\u201d in Figure 2, and underscores, from names proper.\n\u2022 Titles of pages that redirect to other Wikipedia pages, e.g., \u201cStanford\u201d for the article Stanford_University, redirected from the page Stanford.\n\u2022 Titles of disambiguation pages that fan out to many similarly-named articles, e.g., linking \u201cStanford\u201d to Stanford_University, Aaron_Stanford or Stanford,_Bedfordshire, among other possibilities, via Stanford_ (disambiguation).\n\u2022 Anchor text, e.g, we interpret the fact that Wikipedia editors linked the two strings \u201cCharles Robert Redford\u201d and \u201cRobert Redford Jr.\u201d to the article Robert_\n10In the remainder of this article, we will use the following conventions: \u201cstring\u201d for a string that can be used to name an entity (e.g., \u201cRobert Redford\u201d) and sufixes of English Wikipedia URLs, without the prefix http://en.wikipedia.org/wiki/, as short-hand for corresponding articles (e.g., Robert_ Redford).\nRedford as a strong indication that both could refer to \u201cRobert Redford.\u201d We use the number of links connecting a particular string with a specific entity as a measure of the strength of association.\nNote that our dictionary spans not just named entities but also many general topics for which there are Wikipedia articles. Further, it transcends Wikipedia by including anchors (i) from the greater web; and (ii) to Wikipedia pages that may not (yet) exist. For the purposes of NED, it could make sense to discard all but the articles that correspond to named entities. We keep everything, however, since not all articles have a known entity type, and because we would like to construct a resource that is generally useful for disambiguating concepts. Our dictionary can disambiguate mentions directly, simply by returning the highest-scoring entry for a given string. Next, we describe the method used to create several different variants of the dictionary."}, {"heading": "4.1 Redirects and Canonical Pages", "text": "One of many hassles involved in building a Wikipedia-based dictionary stems from redirects, as it is important to separate articles that are actual entries in Wikipedia from other place-holder pages that redirect to them. We will use the term URL in this context to refer indiscriminately to any article page (redirect or not), e.g., both http://en. wikipedia.org/wiki/Stanford and http://en.wikipedia.org/wiki/Stanford_ University, where it is forwarded.\nTo collect all titles and URLs that refer to the same article, we first map all such strings to URLs using Wikipedia\u2019s canonicalization algorithm.11 We then connect any two URLs that either appear together as an official redirection (in a Wikipedia dump) or get redirected at crawl-time (via HTTP status codes 3xx). For each title and URL, we then extract a connected component, represented by a single canonical article, with preference given to (1) non-redirect pages from the Wikipedia dump; followed by (2) official redirect pages; and finally (3) pages that did not appear in any official dump.12 Within each preference category, we resolved ties lexicographically. Figure 7 shows a simple example clustering of URLs, with a canonical article.\n11http://en.wikipedia.org/wiki/Wikipedia:Canonicalization 12This process can be particularly complicated when reconciling snapshots of Wikipedia taken at different times (i.e., an official dump and a web crawl), since sources and targets of redirects can switch roles over time, yielding cycles."}, {"heading": "4.2 The Core and Other Dictionaries", "text": "We created a core dictionary by extracting strings from titles of canonical articles, redirects and disambiguation pages, as well as the referring anchor text encountered in both Wikipedia and the Google crawl. In all cases, we paired a string with the canonical article of the respective cluster.\nOur core dictionary maps strings to sorted lists of Wikipedia articles, with associated scores. These scores are computed from the occurrence frequencies of the anchor texts. For a given string-article pair, where the string has been observed as the anchor-text of a total of y inter-Wikipedia \u2014 and v external \u2014 links, of which x (and, respectively, u) pointed to a page that is represented by the canonical article in the pair, we set the pair\u2019s score to be (x + u)/(y + v).\nWe call this dictionary exact (EXCT), as it matches precisely the raw strings found using the methods outlined above. For example, Figure 8 shows all eight articles that have been referred to by the string \u201cHank Williams.\u201d Note that this dictionary does no filtering: removal of undesired target Wikipedia pages (such as disambiguations) will be done at a later stage (see Section 4.3)."}, {"heading": "4.2.1 Aggregating String Variants", "text": "In addition to exact string lookups, we also adopt two less strict views of the core dictionary. They capture string variants whose lower-cased normalized forms are either the same (the LNRM dictionary) or close (the FUZZ dictionary) by Levenshtein editdistance to that of the string being queried. In both cases, an incoming string now matches a set of keys (strings) in the dictionary, whose lists of scored articles are then merged, as follows: given n articles, with scores ai/bi, their aggregate score is also a ratio, ( \u2211n i=1 ai) / ( \u2211n i=1 bi).\nWe form the lower-cased normalized variant l(s) of a string s by canonicalizing Unicode characters, eliminating diacritics, lower-casing and discarding any resulting ASCII-range characters that are not alpha-numeric. If what remains is the empty string, then s maps to no keys; otherwise, s matches all keys k such that l(s) = l(k), with the exception of k = s, to exclude the original key (which is already covered by EXCT). Figure 9 shows a subset of the LNRM dictionary, with combined contributions from\nstrings k that are similar to (but different from) s. We define the FUZZ dictionary via a metric, d(s, s\u2032): the byte-level UTF-8 Levenshtein edit-distance between strings s and s\u2032. If l(s) is empty, then s maps to no keys once again; otherwise, s matches all keys k, whose l(k) is also not empty, that minimize d(l(s), l(k)) > 0. This approach excludes not only k = s but also l(k) = l(s), already covered by LNRM. For example, for the string Hank Williams, there exist keys whose signature is exactly one byte away, including Tank Williams, Hanks Williams, hankwilliam, and so forth. Figure 10 shows the three articles \u2014 two of them already discovered by both EXCT and LNRM dictionaries \u2014 found via this fuzzy match."}, {"heading": "4.2.2 Querying Search Engines", "text": "In addition to constructing our own custom dictionaries, we also experimented with emulating a lookup by (twice) querying the Google search engine,13 which we call the GOOG dictionary, on the fly. Our query pairs consisted of the raw target string, <str> \u2014 and also a separately-issued phrase-query, \u201c<str>\u201d \u2014 combined with a restriction, site:en.wikipedia.org. We kept only the returned URLs that begin with http://en.wikipedia.org/wiki/, using their (sums of) inverse ranks as scores. Although this method cannot be used effectively to find all strings that may name an entity, given any string, the dictionary portion for that specific string can be filled in dynamically, on demand."}, {"heading": "4.3 Disambiguating with Dictionaries", "text": "For any input string, one could simply use a dictionary to select a highest-scoring entity. Our core dictionary\u2019s scores already capture the popularity of each entity given a string, approximated by the frequency of their pairing in anchor texts. The Google dictionary may, in addition, reflect relatedness to the text of the corresponding Wikipedia\n13http://www.google.com/\narticle, according to more sophisticated ranking algorithms. Such approaches would be closely related to the MFS heuristic in WSD and could also be viewed as context-free disambiguation, since they do not take into account the context of the mention, always returning the same article, regardless of the surrounding text.\nA set of dictionaries, like ours, can be abstracted away behind a single dictionary interface, to be used by downstream components. Internally, dictionaries could be combined using a variety of strategies. We suggest a backoff approach that checks the dictionaries in order of precision, and also a heuristic that discards very unlikely candidates. Other strategies, such as using decision trees or a weighted combination of the dictionaries, are also possible. But many of these strategies would involve parameter tuning (e.g., learning the weights). A thorough exploration of more optimal techniques for combining dictionaries lies outside of the present paper\u2019s scope.\nOur first strategy is a cascade of dictionaries: for a given string, it provides a ranked list of entities by consulting the dictionaries in order of their precision. It starts from EXCT, which is most precise, backing off to the rest. We consider two specific cascades (in remaining sections, all references to the LNRM dictionary correspond to the LNRM cascade, and similarly for FUZZ): \u2022 the LNRM cascade first checks the EXCT dictionary, returning its associated\nentities and scores if the string has an entry there, and defaulting to LNRM dictionary\u2019s results if not; \u2022 the FUZZ cascade also first checks the EXCT dictionary, backs off to the LNRM dictionary in case of a miss, but then finally defaults to the contents from the FUZZ dictionary.\nOur second strategy is a heuristic combination (HEUR): it combines suggestions from the original (EXCT, LNRM, and FUZZ) dictionaries while also filtering out some of the noise. Its goal is to retain good suggestions without drowning in obviously bad choices: since many titles suggested by the FUZZ dictionary are noisy, we only include those for which there is additional evidence of relevance (for instance, if the suggestion is an acronym for the string). Similarly, if a string has been used to link to an article only a few times, the connection between them may not be reliable, calling for more evidence before the entity could be accepted as a possible referent for the string. Naturally, we also discard articles that are clearly not real entities \u2014 such as disambiguation pages, \u201clist of\u201d pages, and pages of dates \u2014 using additional features collected with the core dictionary. Table 1 summarizes our complete list of heuristics, which was finalized by consulting the news subset of the 2010 TAC-KBP development dataset (see Section 6.1).\nHeuristic combinations can yield dictionaries that are considerably smaller than full cascades. For example, for the string ABC, the EXCT dictionary offers 191 options, the LNRM cascade 253, and the FUZZ cascade 3,527. But with the above-mentioned filters, the number of candidate Wikipedia titles can be reduced to just 110. Although our heuristics could be applied to any dictionary or cascade, in the remainder of the paper we will use the short-hand HEUR to refer to the larger FUZZ cascade, followed by the application of these heuristic filtering rules.\nInevitably, HEUR will sometimes discard a correct mapping in favor of inferior choices. For instance, the feasible suggestion Angela Dorothea Kasner \u2192 Angela_ Merkel gets dropped, but the undesirable association Angela Merkel \u2192 German_\nfederal_election,_2005 is kept. Nevertheless, with a reduced set of candidates, it will be easier for supervised classifiers to narrow down remaining options based on a document\u2019s context, down the road (see Section 5)."}, {"heading": "5 Supervised Disambiguation", "text": "The large number of naturally-occurring links pointing at entities in Wikipedia makes it possible to gather richly-annotated data automatically, to augment context-free disambiguation provided by the raw frequency information the dictionaries. In this respect, NED differs from many other applications in natural language processing and information retrieval, where most available input data is not already pre-labeled, and substantial resources are devoted to manual annotation efforts."}, {"heading": "5.1 Core Method", "text": "We followed a mainstream WSD approach (word-expert), training supervised classifiers for all target strings, as follows: For every string in the dictionary, we first identify the entities to which it may refer. We then gather all example spans from Wikipedia articles that contain links to any of these entities. To ensure that our training data is natural language (and not, e.g., lists or tables), we only include text marked as paragraphs (i.e., enclosed between HTML tags <P> and </P>). The relevant training subset for a target string then consists of example contexts with anchor texts containing the string.17\n14I.e., either (a) the string is an acronym for the title; or (b) the title is an acronym for the string. 15I.e., either (a) the strings are the same; or (b) both strings have length less than or equal to six, with an edit distance exactly equal to one; or (c) the ratio between edit distance and string length is less than or equal to 0.1.\n16I.e., if (a) the number of total links to the page (both inside Wikipedia and from the external web) is no more than ten; or (b) the number of times the string links to the page is no more than one; or (c) the score is no more than 0.001.\n17The target string is a substring of the anchor text after case normalization.\nWe take spans of up to 100 tokens to the left \u2014 and another 100 to the right \u2014 of a link to be the contexts. Figure 11 shows one such sample training instance in detail.\nSince the EXCT dictionary often provides too few examples, we used the LNRM cascade as our source dictionary (with remapped canonical articles \u2014 see Section 4.1). Given this training data, we applied standard machine learning techniques to perform supervised disambiguation of entities. We trained a multi-class classifier for each target string. Then, given a mention of the target string in the test data, we applied its classifier to the context of the mention, and returned the corresponding article. We did not construct classifiers for strings whose training data maps to a unique entity. Instead, in those cases, a default classifier falls back to LNRM cascade\u2019s output.\nFrom each context, we extracted features (see Figure 11) commonly used for supervised classification in the WSD setting [Agirre and Lopez de Lacalle, 2007, Zhong and Ng, 2010]: \u2022 the anchor text; \u2022 the unordered set of lemmas in the span; \u2022 lemma for noun/verb/adjective in a four-token window around the anchor text; \u2022 lemma/word for noun/verb/adjective before and after the anchor text; \u2022 word/lemma/part-of-speech bigram and trigrams including the anchor text."}, {"heading": "5.2 Variations", "text": "Over the course of developing our system, we tested several variations of the core algorithm: Classifier: We tried maximum entropy models (MAXENT) and support vector machines (SVM). Dictionary: A dictionary influences supervised classification in two places. First, when building the training data, to filter example spans selected for training. And second, as a backup ranker, for cases when a classifier is not trained, due to a lack of examples. In both the filtering stage and the back-off stage, we compared using the HEUR dictionary in place of the LNRM cascade. Span: In addition to training with contexts of (up to) 100 tokens to the left and right of a string, we also tried single-sentence and full-paragraph spans (the 100, SENT and PARA variants). Match: When gathering examples for a target string, we made sure that the anchor text contains this string (the LEX default). Alternatively, we could allow additional examples, ignoring anchor text mismatch (the SENSE variant): given the entities that a dictionary lists for the target string, we include as training examples all contexts that apply to these entities, regardless of their anchor text. In this variant, the target string is simply treated as another feature by the classifier. If a test example\u2019s string does not match any of the anchor text seen in training, then features that include the target string (i.e., its unigram, bigram, and trigram features) will not fire. Classification will then depend on features describing the rest of the context: a classifier could still give a high score, but only if surrounding words of a span carry a strong enough signal for an entity. This approach may allow us to classify aliases for which there isn\u2019t exact training data, provided that our filtering dictionary yields a precise list of potential entities corresponding to a target string."}, {"heading": "6 Datasets for NED", "text": "Evaluation of NED systems requires manually annotated data. Although many corpora have been introduced in various papers, we decided to focus on the earlier datasets developped for the entity linking task of the knowledge-base population (KBP) track at Text Analysis Conferences (TAC),18 which have been running annually each year since 2009 [McNamee and Dang, 2009, Ji et al., 2010, Ji and Grishman, 2011].\nThe TAC-KBP evaluation focuses on three main types of named entities: persons (PER), organizations (ORG) and geo-political entities (GPE). Given a set of handselected mentions of entities \u2014 and documents containing these strings \u2014 the task is to determine which knowledge-base instance, if any, corresponds to each named entity string. The knowledge-base (KB) is derived from a subset of Wikipedia. Mentions are chosen among occurrences in a collection of 1,286,609 newswire documents and 490,596 web-pages. The tasks\u2019 organizers have released substantial amounts of development and test data, as well as standardized evaluation software.\nThere are several reasons why we chose to evaluate against TAC-KBP data: (i) it consists of named entity mentions from two genres (news articles and pages crawled off the web); (ii) it focuses on several taget entity mentions, making it well suited to our word-expert approach; and (iii) its high number of participating systems \u2014 and subsequent publications \u2014 provide an informative setting for comparing state-of-theart techniques. Among TAC-KBP datasets, those from 2009 and 2010 attract the largest number of papers [Varma et al., 2009, McNamee, 2010, Lehmann et al., 2010, Zheng et al., 2010, Dredze et al., 2010, Zhang et al., 2010, Han and Sun, 2011, Ploch, 2011, Chen and Ji, 2011, Gottipati and Jiang, 2011, Hachey et al., 2012, Han and Sun, 2012]. In the future we would like to extend our work to other datasets which include all mentions in full documents [Hoffart et al., 2011]."}, {"heading": "6.1 The TAC-KBP Dataset", "text": "The TAC-KBP exercise provides an inventory of target named entities, based on a subset of Wikipedia articles that had info-boxes in October of 2008. This KB contains 818,741 entities (a.k.a. KB instances), each marked with (i) its name (a string); (ii) the assigned entity type (one of PER, ORG or GPE); (iii) a KB instance ID (a unique identifier, such as E001); (iv) the set of info-box slot names and values from the corresponding Wikipedia page; and (v) the text of that Wikipedia page. Figure 12 shows a sample KB entry for a person, whose entity derives from the Wikipedia article identified by the URL Mike_Quigley_(footballer).\nGiven a query that consists of a string and a document ID (see Figure 13), the task is to determine the knowledge-base entity to which that document\u2019s string refers (or to establish that the entity is not present in the reference KB). The document provides context which may be useful in disambiguating the string. A referent entity will generally occur in multiple TAC-KBP queries, under different surface name variants and in different documents. Because of possible auto-correlations, official rules stipulate that queries must be processed independently of one another. As expected, some entities share confusable names (e.g., the string Stanford refers to a university, its town and founder, among many other possibilities). For each query, a system must return a KB ID (or NIL when there is no corresponding KB-instance). All of the queries had been tagged by a team of annotators. Inter-annotator agreement is high for organizations (ORG: 92.98%) and people (PER: 91.53%), and somewhat lower for remaining entities (GPE: 87.5%).\nTAC-KBP has been running and releasing development and test data each year since 2009. It attracted 13 teams in the first year and 16 teams in 2010. We use the same data split as in the 2010 task. For the dev-set, all news examples come from the 2009 test-set, and web examples are exclusively from the 2010 training set; the 2010 test-set contains both types. Table 2 gives a break-down of our development and evaluation sets by the three entity types (PER, ORG, GPE). Note that news samples in the dev-set are especially skewed toward ORG, with approximately five times as many examples as either PER or GPE; the rest of the data is perfectly balanced.\nSince the inventory of entities used in TAC-KBP is incomplete, it is possible for queries to refer to unlisted entities. In such cases, human annotators tagged the mentions as NILs. Note that dthe development and test datasets a large number of NIL mentions, 49% and 55%, respectively. We evaluate on non-NIL mentions.\n18http://www.nist.gov/tac/"}, {"heading": "6.2 Ambiguity", "text": "Some simple measurements can give an idea as to the difficulty of a problem or dataset. In our case, one such quantity is a mention\u2019s ambiguity: a lower bound on the number of different KB-instances to which its string can refer. In WSD, an equivalent notion would be the concept of polysemy. In our dictionary, for example, the string ICNC covers four distinct entities in Wikipedia.\nWe estimated ambiguity as follows: For each target string in a dataset, we counted the number of different KB-instances associated to it by human annotators. Table 3 reports ambiguities in both development and evaluation sets. The section of the devset, for instance, comprises 1,162 unique target strings (types). Of those, 462 were tagged with NIL, as no referent entity was deemed appropriate in the KB; 488 strings had all their mentions tagged with the same entity; and only 112 had been tagged with multiple KB-instances \u2014 an average of 2.34 entities per ambiguous string. Of course, this calculation grossly underestimates actual polysemy, as the number of potential articles to which a string could refer may far exceed those found in our collection by annotators. As mentioned in the previous section, populating a complete list of entities that could have been invoked is part of the task, and as such, still an open question. However, many target strings tend to be highly skewed, in the sense that they usually refer to one specific entity. This fact is reflected by the large number of strings which refer to a single entity in the gold standard sample.\nWe will show that actual polysemy is much higher (e.g., according to our dictionaries \u2014 see Section 6.4), since many of the possible entities do not appear in standard datasets. This fact further differentiates NED from typical WSD settings, where most senses can be found in a gold standard. Table 4 shows polysemy as average number\nof senses for a WSD set-up (Senseval-3), with respect to both gold-standard data and a dictionary, which happen to be fairly similar. Yet another point of contrast is that many mentions in NED refer to only a single entity, whereas in WSD all 57 target words are polysemous (with multiple senses attested by the gold standard)."}, {"heading": "6.3 Synonymy", "text": "Another quantity that sheds light on the complexity of a disambiguation task, in a manner that is complementary to ambiguity, is the number of different strings that can be used to name a particular entity: its synonymy, in WSD terms. As before, tallying all unique strings that may refer to a given entity is an open problem. Table 5 tabulates the gold standard\u2019s synonymy (i.e., the number of strings found in the document collection by the annotators) for each entity in the KB. Most entities are associated with only a single string, especially in the evaluation sets. This could be an artifact of how the organizers constructed the test data, since their procedure was to first choose ambiguous strings and then find documents containing different meanings (entities) for these mentions \u2014 as opposed to first choosing entities and then querying string variants of names. The annotators thus did not search for alternative lexicalizations (synonyms) to be used as query strings. As was the case with ambiguity, actual synonymy is much higher (see Section 6.4, in which we compute similar figures to analyze some of the dictionaries used by our systems).\nTable 6 shows average synonymy for a WSD dataset (Senseval-3 lexical sample). The gold standard contains 278 senses (for 57 target words), of which 85 have a unique lexicalization (and 193 have multiple), with 3.66 synonyms on average. Average synonymy in this WSD setting is slightly higher than that of TAC-KBP\u2019s gold standard, similarly to polysemy (though Section 6.4 will show that, in practice, synonymy for\nNED can be much higher, according to a dictionary ). To summarize, the number of cases where annotators could not assign an entity (NILs) is significantly higher in NED than in WSD (around 50% compared to just 1.7%). And ambiguity and synonymy, according to gold standards, are substantially lower than in WSD (with average ambiguities of around 2 vs. 5, and average polysemies of 2 vs. 4). These statistics are misleading, however, since TAC-KBP\u2019s actual ambiguity and synonymy are more extreme, according to our dictionaries (discussed in Section 6.4). Finally, inter-annotator agreement for TAC-KBP is higher \u2014 ranging between 87% and 93%, depending on the entity type \u2014 compared to 72% in WSD."}, {"heading": "6.4 Ambiguity and Synonymy in Dictionaries", "text": "We now consider polysemy and synonymy in our dictionaries. Table 7 reports ambiguities for the LNRM and HEUR dictionaries on development data. For instance, the dev-set comprises 1,162 target strings, of which LNRM has no suggestions for 111 (94 with HEUR). Among the remaining strings, 331 yield a unique suggestion from Wikipedia (331 with HEUR). The average ambiguity for the 765 strings with multiple candidates is 86.14 (only 22.26 with HEUR). The distribution of the ambiguity between news and web datasets is distinct, 50.90 and 103.24, respectively. Thus, HEUR covers more strings and has fewer monosemous entries yet lower polysemy than LNRM. Thus, HEUR retains more references from Wikipedia and simultaneously substantially reduces ambiguity. Although we don\u2019t tabulate figures for other dictionaries, we note here that FUZZ has even higher polysemy than LNRM. The two lower rows in the the table also shows ambiguities faced by our supervised classifiers,\nwhich discard dictionary suggestions for which there is no training data (e.g., entities that do not occur as targets of an anchor link \u2014 see Section 5). The lower half of the table focuses on the HEUR dictionary in combination with the LEX and SENSE strategies for gathering training instances. As expected, resulting ambiguities are lower than for full dictionaries, with more cases of zero or one candidates and fewer strings mapping to multiple entities (also with lower ambiguities); the decrease is smaller for the more conservative filtering strategy (SENSE), which is again consistent with our expectations.\nAmbiguity in dictionaries is much higher than according to the gold standard (compare to Table 3). This comes as no surprise, since the gold standard severely underestimates true ambiguity by focusing exclusively on entities that are mentioned in a target dataset. Since many target strings can, in fact, refer to dozens of possible external entities, our estimates of ambiguity are also much higher than in WSD settings (see Table 4). In contrast to WSD, NED\u2019s true ambiguity usually remains unknown, as its determination would require a laborious inspection of all strings and entities, and an exhaustive search for examples of usage in actual text. Although our dictionaries have good quality and come close to covering all entities in the gold standard (see Section 8.6), a manual inspection showed that they also contain incorrect entries. Therefore, we suspect that actual ambiguity maybe be slightly lower than our estimate obtained with the HEUR dictionary.\nTable 8 shows synonymy figures for both the raw EXCT dictionary and also after applying heuristic rules (EXCT-HEUR). Entities corresponding to NILs were not covered by the dictionary (tallied under the No Strings heading), and all of the remaining entities were lexicalized by a large number of strings (none by just one). The EXCT dictionary had, on average, 210 strings, which is reduced to 46 with heuristics. Although high estimates reflect the comprehesive coverage afforded by our dictionaries, they do not reveal true levels of synonymy, which would require hand-checking all en-\ntries, as before. Nevertheless, these figures illustrate, at a high level, another important difference between NED and WSD, where synonymy tends to be much lower (e.g., around 3.5, according to one popular dictionary \u2014 see Table 9). Extending the table to include LNRM and FUZZ dictionaries would require performing more complicated calculations, but the resulting synonymy estimates would only be higher for these omitted cascades."}, {"heading": "7 A NED System for TAC-KBP", "text": "Evaluating our system on the TAC-KBP exercise required several adaptations. Figure 14 shows the updated architecture. In it, a new module looks up top-ranked articles in the KB. If a top-scorer has a corresponding entry in the KB, the module returns its KB ID. Since all KB-instances came from Wikipedia, there is a direct mapping from titles to KB IDs. To guarantee matches, we altered our dictionary construction process slightly, making sure to include all Wikipedia titles explicitly referenced by the official KB (see Figure 12). When deciding which pages are canonical (see Section 4.1), we preferred entries listed in the KB over all others (from a superset of Wikipedia data that included the TAC-KBP dump of October 2008)."}, {"heading": "8 Experimental Results and Performance Analyses", "text": "We now evaluate different variants of dictionaries and supervised classifiers on development data. These experiments allow us to tune settings, to be used with the blind test set (see Section 8.7)."}, {"heading": "8.1 Evaluation Setup for TAC-KBP", "text": "We follow a standard evaluation procedure, using scripts from the 2009\u201310 TAC-KBP exercise. The metric is (micro-averaged) accuracy: given a reference set with N queries \u2014 and a corresponding set of guesses, where C are correctly disambiguated (i.e., a system\u2019s output label equals the gold standard\u2019s entity ID string) \u2014 the score is simply C/N . Since our main goal is to disambiguate among entities, we will focus on evaluation of the dictionaries and supervised classifiers restricted to entities present in the KB (i.e., ignoring gold standard examples tagged with NILs)."}, {"heading": "8.2 Performance of Top Dictionary Entries", "text": "Table 10 shows performance on the news subset of development data for the EXCT and GOOG dictionaries (69.4 and 69.6%), the LNMR and FUZZ cascades (69.5 and 71.3%), and the heuristic combination, HEUR (72.1%). It confirms our intuition (see Section 4): HEUR improves over the FUZZ cascade, offering cleaner suggestions (since it yields many fewer candidates). Results from GOOG are competitive with cascades and may be a good alternative in situations when full dictionaries are unavailable.19 The table also shows that using counts from Wikipedia alone is worse than relying on counts from the rest of the web \u2014 and that merging the two sets of counts (see Section 4) works best \u2014 for the LNRM cascade; we use merged counts in all remaining experiments. Hovering at 70%, MFS heuristics have higher accuracies here than even in typical WSD settings, where they are known to be strong (e.g., around\n19However, this may require querying the search engine just-in-time, using Google\u2019s API, with consequent limitations.\n55% \u2014 see Section 2.2), which shows that choosing most popular entries makes for powerful methods in NED as well."}, {"heading": "8.3 Precision/Recall Curves for Dictionaries", "text": "Although performance of top candidates is indicative of a dictionary\u2019s overall quality, it tells us little about the less popular choices. We use dictionaries to expand queries into pools of possible candidates, to be disambiguated by a supervised system. Therefore, it is important to understand how close a dictionary might come to capturing all \u2014 even low-ranking \u2014 entities that may be relevant to a gold standard. Figure 15 shows precision/recall curves that evaluate our dictionaries beyond just top-ranking entries (the left-most points of each curve, corresponding to Table 10).20\nThese curves show that the FUZZ cascade generates more entities than other dictionaries, with higher recall at similar levels of precision; unfortunately it draws on far too many candidates. The LNRM and EXCT dictionaries perform similarly at all recall levels; HEUR performs better at high precision, slightly better at high recall, and worse in-between. The GOOG dictionary performs worse everywhere except at the highest recall, where it dominates (at close to 99%); its generally-low performance indicates that although GOOG manages to generate candidates for nearly all strings, its ranking function is less-well suited to the NED task than are our methods.\n20 Precision is the number of correct entities divided by the total number of entities returned; recall is the number of correct entities divided by the total number of gold standard entities). The P/R curve is obtained taking the top K candidates with highest probability."}, {"heading": "8.4 Performance of Supervised Classifiers", "text": "Table 11 shows performance on the news subset of development data for several variants of our supervised classifier (described in Section 5). The first row corresponds to default parameters; the rest represent a greedy exploration of the space of alternatives. Each additional row specifies a setting that differs from the first row; dashes (-) indicate that all other parameters are the same. Classifier: The second and third rows correspond to the accuracies of a multi-class classifier, based on SVMmulticlass [Tsochantaridis et al., 2004], and a one-versusall approach, using SVMlight binary classifiers [Joachims, 1999]. Both alternatives perform worse than our default classification algorithm (maximum entropy with `2- regularization), MAXENT [Manning and Klein, 2003]. We did not tune any of the available parameters, since we were interested in out-of-the-box performance of all methods (for SVMs, we used a linear kernel with a cost of 0.01). Dictionary: The fourth row shows that the LNRM cascade generates worse training examples than our default dictionary combination, HEUR. We do not show results for other dictionaries, which yield too many candidates (without improving precision). The fifth row shows that HEUR also performs better than LNRM when used as a back-off dictionary, improving over the default. Span: Rows six and seven show that supervised classification performs equally well using either sentences or paragraphs (but that the best results are obtained using left and right 100 tokens). One reason for similar performance is that most paragraphs are not marked correctly, in Wikipedia, often comprising a single sentence. A fixed span of tokens to each side of a mention may extend beyond sentence boundaries, providing more context to help with disambiguation. Match: The last row shows that using all examples that refer to an entity (SENSE) improves over the default approach, which uses only a subset of examples that contain the target string (LEX)."}, {"heading": "8.5 Extending Analyses to Web Data", "text": "The best results for the news portion of the development data were obtained with the MAXENT classifier, the HEUR dictionary for both filtering training examples and backing off, 100-token spans, and the SENSE match strategy. On web data, our dictionary and supervised components yield somewhat different results, most of them congruent with conclusions based on news data.\nTable 12 shows a subset of the variants with qualitatively different outcomes. For instance, the LNRM dictionary, our default for back-off, is the better option with web data. As for plain dictionary look-ups, HEUR is still the best overall choice, although GOOG performs slightly better on web data. This disparity may stem from search engines being developed primarily with web-pages in mind, whereas our heuristics came about from analyzing examples of news articles in the development set; LNRM may be the more robust back-off dictionary for similar reasons. Furthermore, our classifiers fall through to a back-off dictionary only when there aren\u2019t enough training instances, which tends to be the case for rare entities. Since LNRM has higher recall than HEUR, it may be generally more useful for obscure references. Overall, performance differences between dictionaries are smaller for web data, and average accuracies are substantially higher than with the news portion. Evaluated against the combined news+web development data, ranks of dictionaries remain the same as for news alone.\nAll variations of the supervised system with good performance on news worked better still for web data,21 scoring well above the dictionaries. The reduction in error going from news to web is remarkable, with accuracies well over 90%. Manual inspection showed that mentions in web data are more heavily skewed towards most popular entities, compared to news data, which may explain the much higher accuraces of both supervised classifiers and the underlying dictionaries. In fact, web data referenced, for the most part, extremely well-known entities (e.g., European Union), along with some that are relatively unknown (e.g., CCC \u2014 the Cincinnati Cycle Club). Entities in news data, on the other hand, tended to be of substantially more ambiguous nature.\nThe second (and final) variant that does better on news but did not pan out with web data is the SENSE strategy for gathering training instances. Although it has higher coverage (see Section 6.4), additional matches tend to be less precise, as these examples aren\u2019t required to contain the target string. Since preliminary results indicate that relative performance is sensitive to the type of data, we decided to stick with the simpler, more conservative strategy, LEX. We conclude that our default settings may already be optimal for unseen data, and will thus evaluate this set-up against the TAC-KBP 2010 test data, to compare our system with the state-of-the-art."}, {"heading": "8.6 Upper Bounds for Supervised Classification with Our Dictionaries", "text": "An important function of the filtering dictionary is to provide sets of plausible entities, which determine construction of (distantly) supervised training data. Given an ideal\n21Table 12 omits results for variations that did worse than the default on news, as they also do worse with web data.\nclassifier, we would prefer to use comprehensive dictionaries that might contain the correct entity for all strings in the evaluation set, even if that meant dragging in many incorrect candidates too. Table 13 shows the skyline results that could be attained by an oracle, choosing the best possible entity available to each system. For LNRM and HEUR cascades, gold standard entities are among the dictionaries\u2019 suggestions about 92% of the time (98% for web data). Supervised classification with the LEX strategy lowers these bounds slightly, because in some cases there are no training examples available for the gold entity, which precludes our classifiers from returning the correct result; by expanding the pool of training instances, the SENSE strategy restores this bound to nearly what it was for HEUR. Naturally, higher bounds aren\u2019t necessarily superior: for example, FUZZ yields the overall largest number of possibilities, but has lower realized performance than HEUR."}, {"heading": "8.7 Final Results and Comparison to the State-of-the-Art", "text": "Following the development phase, we tested our best dictionary and supervised classifier on the hidden (2010 test) dataset. Table 14 shows both results, also broken down by data type. This final evaluation confirmed that the heuristic dictionary already performs quite well (at 75%) overall, and that trained classifiers can tap into further improvements (scoring close to 85%). The improvement is larger on news, which is again more challenging that the web dataset. The results are similar to those of development, except for web, where results have dropped around 5 points.\nTable 15 shows performance of recent NED systems that report results for either of the 2009/2010 TAC-KBP test sets (see Section 2 for their descriptions), including our dictionary and classifier.\nThe dictionary performs well on the KB-only subset of the 2009 test set, beating some of the more complex systems. Since we used the 2009 data to choose bests of several system variants, evaluation on this test set is not blind and may overstate our results, as well as those of many other published systems that did the same [Zhang\net al., 2010, Han and Sun, 2011, Hachey et al., 2012]; our supervised classifier scores 77% on the KB-only subset, behind two systems [Zhang et al., 2010, Han and Sun, 2011] scoring 79%, both of which were also developed using 2009 test data.\nFor the 2010 test data, our evaluation was completely blind. Here the dictionary performs better than the next best \u201cno context\u201d system [McNamee, 2010] by a large margin: 75% vs. 65%. The supervised classifier also scores highest \u2014 85% vs. 81% for the next best system [Lehmann et al., 2010]. The entry for Hachey et al. [2012] represents the best of several systems chosen on the test set; their best variant according to development data was, in fact, the system of Varma et al. [2009], which scores 7% lower22"}, {"heading": "9 Discussion", "text": "We have shown that it is possible to construct an effective dictionary for NED, covering between 92% and 98% of the manually annotated string-entity pairs (for development data). Although the ambiguity in such dictionaries varies, it tends to be higher than for the gold standard in NED and also for a typical WSD dataset. Since a similar phenomenon is also observed with synonymy, one might expect NED to pose a more difficult problem than WSD; nevertheless, we observed the opposite effect in practice. Our popularity-based dictionary heuristic performs even more strongly than the MFS baseline in WSD (75% in our blind NED evaluation, compared to 55%); supervised system variants also score much higher (84% vs. 73%). Of course, comparing evaluation numbers across tasks requires extreme caution. Nonetheless, we suspect that qualitatively large differences in performance here indicate that NED has larger numbers of training data, compared to WSD.\nHigh ambiguity and synonymy, together with the large volumes of text data, make NED computationally more demanding than WSD, both in the scope of regular memory and disk storage capacities, as well as speed and efficient processing requirements. Our approach in particular could invoke training of potentially millions of classifiers, requiring significant engineering effort. But since each classifier can be trained independently, parallelization is simple and easy.\nWe found that more comprehensive dictionaries that provide better coverage aren\u2019t always optimal when it comes to training supervised classifiers, since it is also important to have enough examples for each candidate entity being suggested. Instead, a dictionary that contains the most precise and common choices may work better, as demonstrated by our cascades of dictionaries.\nAll in all, our systems fare well, compared to the state-of-the-art. The dictionary beats all systems not using context in the 2010 TAC-KBP task by a large margin. And the supervised system, despite its simplicity, outperforms other systems as well. More detailed comparisons are difficult, because many recent papers lack ablative analyses: even when performance of individual components is reported, interactions with NILs\n22Note that, since Hachey et al. [2012] reimplemented three well-known systems [Bunescu and Pasca, 2006, Cucerzan, 2007, Varma et al., 2009], our results also compare favorably to the other two.\nmake proper comparisons challenging.23,24\nAs mentioned previously, our dictionaries could be expanded by drawing on additional sources of data. This fact should not be overlooked, since Hachey et al. [2012] found that candidate generation accounted for most of the performance variation in the systems that they reimplemented; in particular, acronym handling, using coreference resolution to find longer mentions, led to substantial improvements. We could augment our dictionaries via various techniques for mining acronyms [Varma et al., 2009, McNamee, 2010, Lehmann et al., 2010], metaphones [Varma et al., 2009], as well as bolded words in first paragraphs and hatnote templates [Hachey et al., 2012]. Moving beyond the static dictionary model, it is also possible to exploit dynamic methods for proposing candidates. Online techniques can make use of partial matches between tokens in query strings and entities [Varma et al., 2009, McNamee, 2010, Lehmann et al., 2010], finite-state transducers [Dredze et al., 2010], matches with longer mentions [Lehmann et al., 2010], automatic spelling correction [Zheng et al., 2010], Wikipedia search engines and the \u201cDid you mean...\u201d functionality [Zhang et al., 2010]. Storing as much information as possible in static dictionaries will make it easier to debug, replicate and share resources. But some dynamic lookups could also be batched to enrich a dictionary just-in-time. A thorough study of the overlap and contribution of various enhanced candidate generation methods may make for a fruitful research direction.\nWith respect to disambiguation, our approach closely followed that of typical supervised WSD systems, which train a classifier for each target string. We used anchor texts in Wikipedia articles to train logistic regressions, showing that this method is also appropriate and computationally feasible for NED. Given their popularity in WSD research, we expected straight-forward classification techniques to be more prevalent in NED, at least as baselines. Methods such as ours may also prove to be useful in combination with other disambiguation systems which tap on different knowledge sources, e.g. those using the hyperlink structure [Milne and Witten, 2008, Moro et al., 2014].\nIn summary, our NED system can be easily replicated because it uses an offline dictionary. Its simplicity allows for a clearer understanding and ablative analyses, compared with systems that rely on dynamic candidate generation methods. As a result, it may make a good platform for testing and incorporating various modular extensions to NED, such as NIL classifiers, candidate generators, similarity-based techniques, global coherence or coreference resolution. We designed the system to work with the entire space of Wikipedia articles and strings: no thresholds or other kinds of parameters were fine-tuned to the test data. Given the limited number, scope and complexity of decisions made even in development, we expect our system to be robust. Naturally,\n23E.g., Han and Sun [2011] report end-to-end performance of their popularity and name model (roughly equivalent to our dictionary), combined with a NIL detection system, excluding KB- or NIL-only results. Their score on the 2009 test set is far below ours (50% vs. 72%), but it is not possible to identify the main reason behind this discrepancy.\n24An exception, Hachey et al. [2012] provide precision and recall of their candidate generation component for KB queries (56.3 and 87.8, respectively) on the 2009 test data (the news subset of our development set), which can be compared to oracle and realized performance of HEUR (91.88 and 72.12); they also report an ambiguity of 7.2, obtained by dividing the number of candidate entities by the total number of query strings (our corresponding figure for HEUR would be 9.08, if calculated in the same fashion, i.e., differently from the numbers listed in Table 3).\nperformance could be further optimized to fit a target corpus or genre, if desired."}, {"heading": "10 Conclusions and Future Work", "text": "We presented a system for NED based on word-experts, which is a well-understood technique from the WSD literature. Our system comprises two components: (1) a context-independent module, based on frequencies of entities, which returns most popular candidates; and (2) a context-sensitive classifier for each named-entity string that selects entities that are best suited to a mention\u2019s surrounding text. We show that such a word-expert provides results which are competitive to the state-of-the-art, as measured on the 2009 and 2010 TAC datasets.\nIn the future we would like to extend our work to other datasets, which will provide further points of comparison to the state of the art. We would also like to include our classifiers in more complex NED systems, where complementary information like link structure [Moro et al., 2014], similarities between article texts and mention contexts [Hoffart et al., 2012], as well as global optimization techniques [Ratinov et al., 2011] could further improve the results.\nWe highlighted many connections between WSD and NED. In WSD settings, dictionaries are provided, but NED involves constructing possible mappings from strings to entities \u2014 a step that Hachey et al. [2012] showed to be key to success, which we also confirmed experimentally. The resulting dictionaries exhibit very high synonymy and ambiguity (polysemy) yet still do not cover many occurrences that ought to be tagged by a NED system, making the task appear more difficult, in theory, compared to WSD. But in practice, the opposite seems to be the case, due to actual mentions being more heavily skewed towards popular entities than in WSD, a plethora of available training data in the form of human-entered anchor-texts of hyperlinks on the web, and higher inter-annotator agreement, which indicates more crisp differences between possible shades of meanings than in WSD. As a result, both popularity-based dictionary lookups (MFS heuristics) and supervised classifiers, which are traditional WSD architectures, perform better for NED than for WSD. In the future, we would like to extend our study using datasets which include all mentions in full documents [Hoffart et al., 2011]."}, {"heading": "Acknowledgements", "text": "We thank Oier Lopez de Lacalle and David Martinez, for the script to extract features, as well as Daniel Jurafsky and Eric Yeh, for their contributions to our earliest participation in TAC-KBP.\nParts of this work were carried out while Eneko Agirre was visiting Stanford University, with a grant from the Ministry of Science; Angel X. Chang has been supported by a SAP Stanford Graduate Fellowship; Valentin I. Spitkovsky has been partially supported by NSF grants IIS-0811974 and IIS-1216875 and by the Fannie & John Hertz Foundation Fellowship. We gratefully acknowledge the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Re-\nsearch Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government."}], "references": [{"title": "Word Sense Disambiguation: Algorithms and Applications, volume 33 of Text, Speech and Language", "author": ["Eneko Agirre", "Philip Edmonds", "editors"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2006}, {"title": "UBC-ALM: Combining k-NN with SVD for WSD", "author": ["Eneko Agirre", "Oier Lopez de Lacalle"], "venue": "In SemEval,", "citeRegEx": "Agirre and Lacalle.,? \\Q2007\\E", "shortCiteRegEx": "Agirre and Lacalle.", "year": 2007}, {"title": "Stanford-UBC at TAC-KBP", "author": ["Eneko Agirre", "Angel X. Chang", "Dan S. Jurafsky", "Christopher D. Manning", "Valentin I. Spitkovsky", "Eric Yeh"], "venue": "In Proceedings of the Text Analysis Conference,", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Web people search: results of the first evaluation and the plan for the second", "author": ["Javier Artiles", "Satoshi Sekine", "Julio Gonzalo"], "venue": "In WWW,", "citeRegEx": "Artiles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2008}, {"title": "WePS 2 evaluation campaign: overview of the web people search clustering task", "author": ["Javier Artiles", "Julio Gonzalo", "Satoshi Sekine"], "venue": "In WePS,", "citeRegEx": "Artiles et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Artiles et al\\.", "year": 2009}, {"title": "Entity-based cross-document coreferencing using the vector space model", "author": ["Amit Bagga", "Breck Baldwin"], "venue": "In COLING-ACL,", "citeRegEx": "Bagga and Baldwin.,? \\Q1998\\E", "shortCiteRegEx": "Bagga and Baldwin.", "year": 1998}, {"title": "Collective entity resolution in relational data", "author": ["Indrajit Bhattacharya", "Lise Getoor"], "venue": "ACM TKDD,", "citeRegEx": "Bhattacharya and Getoor.,? \\Q2007\\E", "shortCiteRegEx": "Bhattacharya and Getoor.", "year": 2007}, {"title": "Using encyclopedic knowledge for named entity disambiguation", "author": ["Razvan C. Bunescu", "Marius Pasca"], "venue": "In EACL,", "citeRegEx": "Bunescu and Pasca.,? \\Q2006\\E", "shortCiteRegEx": "Bunescu and Pasca.", "year": 2006}, {"title": "Stanford-UBC entity linking at TAC-KBP", "author": ["Angel X. Chang", "Valentin I. Spitkovsky", "Christopher D. Manning", "Eneko Agirre"], "venue": "In Proceedings of the Text Analysis Conference,", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Stanford-UBC entity linking at TAC-KBP, again", "author": ["Angel X. Chang", "Valentin I. Spitkovsky", "Christopher D. Manning", "Eneko Agirre"], "venue": "In Proceedings of the Text Analysis Conference,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Collaborative ranking: A case study on entity linking", "author": ["Zheng Chen", "Heng Ji"], "venue": "In EMNLP,", "citeRegEx": "Chen and Ji.,? \\Q2011\\E", "shortCiteRegEx": "Chen and Ji.", "year": 2011}, {"title": "Linking documents to encyclopedic knowledge", "author": ["Andras Csomai", "Rada Mihalcea"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Csomai and Mihalcea.,? \\Q2008\\E", "shortCiteRegEx": "Csomai and Mihalcea.", "year": 2008}, {"title": "Large-scale named entity disambiguation based on Wikipedia data", "author": ["Silviu Cucerzan"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Cucerzan.,? \\Q2007\\E", "shortCiteRegEx": "Cucerzan.", "year": 2007}, {"title": "Entity disambiguation for knowledge base population", "author": ["Mark Dredze", "Paul McNamee", "Delip Rao", "Adam Gerber", "Tim Finin"], "venue": "In COLING,", "citeRegEx": "Dredze et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2010}, {"title": "A joint model for entity analysis: Coreference, typing, and linking", "author": ["Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett and Klein.,? \\Q2014\\E", "shortCiteRegEx": "Durrett and Klein.", "year": 2014}, {"title": "Resolving personal names in email using context expansion", "author": ["Tamer Elsayed", "Doug Oard", "Galileo Mark Namata"], "venue": "In ACL,", "citeRegEx": "Elsayed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Elsayed et al\\.", "year": 2008}, {"title": "Scaling Wikipedia-based named entity disambiguation to arbitrary web text", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In WikiAI,", "citeRegEx": "Fader et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2009}, {"title": "WordNet: An Electronic Database", "author": ["Christiane Fellbaum", "editor"], "venue": null, "citeRegEx": "Fellbaum and editor.,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum and editor.", "year": 1998}, {"title": "Cross-document coreference on a large scale corpus", "author": ["Chung Heong Gooi", "James Allan"], "venue": null, "citeRegEx": "Gooi and Allan.,? \\Q2004\\E", "shortCiteRegEx": "Gooi and Allan.", "year": 2004}, {"title": "Linking entities to a knowledge base with query expansion", "author": ["Swapna Gottipati", "Jing Jiang"], "venue": "In EMNLP,", "citeRegEx": "Gottipati and Jiang.,? \\Q2011\\E", "shortCiteRegEx": "Gottipati and Jiang.", "year": 2011}, {"title": "A graph-based method for entity linking", "author": ["Y. Guo", "W. Che", "T. Liu", "S. Li"], "venue": "In Proceedings of 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Guo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2011}, {"title": "Evaluating Entity Linking with Wikipedia", "author": ["B. Hachey", "W. Radford", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Hachey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hachey et al\\.", "year": 2012}, {"title": "A generative entity-mention model for linking entities with knowledge base", "author": ["Xianpei Han", "Le Sun"], "venue": "In ACL HLT,", "citeRegEx": "Han and Sun.,? \\Q2011\\E", "shortCiteRegEx": "Han and Sun.", "year": 2011}, {"title": "An entity-topic model for entity linking", "author": ["Xianpei Han", "Le Sun"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "Han and Sun.,? \\Q2012\\E", "shortCiteRegEx": "Han and Sun.", "year": 2012}, {"title": "Robust disambiguation of named entities in text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": "In EMNLP,", "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Kore: Keyphrase overlap relatedness for entity disambiguation", "author": ["Johannes Hoffart", "Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of the 21st ACM international conference on Information and knowledge management,", "citeRegEx": "Hoffart et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2012}, {"title": "Knowledge base population: Successful approaches and challenges", "author": ["Heng Ji", "Ralph Grishman"], "venue": "In ACL HLT,", "citeRegEx": "Ji and Grishman.,? \\Q2011\\E", "shortCiteRegEx": "Ji and Grishman.", "year": 2011}, {"title": "Overview of the TAC 2010 Knowledge Base Population track", "author": ["Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis"], "venue": "In TAC,", "citeRegEx": "Ji et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Making large-scale SVM learning practical", "author": ["Thorsten Joachims"], "venue": "Advances in Kernel Methods - Support Vector Learning", "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Collective annotation of Wikipedia entities in web text", "author": ["Sayali Kulkarni", "Amit Singh", "Ganesh Ramakrishnan", "Soumen Chakrabarti"], "venue": "In KDD,", "citeRegEx": "Kulkarni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2009}, {"title": "LCC approaches to knowledge base population at TAC", "author": ["John Lehmann", "Sean Monahan", "Luke Nezda", "Arnold Jung", "Ying Shi"], "venue": "In TAC,", "citeRegEx": "Lehmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2010}, {"title": "Unsupervised personal name disambiguation", "author": ["Gideon S. Mann", "David Yarowsky"], "venue": "In CoNLL,", "citeRegEx": "Mann and Yarowsky.,? \\Q2003\\E", "shortCiteRegEx": "Mann and Yarowsky.", "year": 2003}, {"title": "Optimization, maxent models, and conditional estimation without magic", "author": ["Christopher D. Manning", "Dan Klein"], "venue": "In HLT-NAACL,", "citeRegEx": "Manning and Klein.,? \\Q2003\\E", "shortCiteRegEx": "Manning and Klein.", "year": 2003}, {"title": "Supervised corpus-based methods for WSD", "author": ["Lluis Marquez", "Gerard Escudero", "David Martinez", "German Rigau"], "venue": "Word Sense Disambiguation: Algorithms and Applications,", "citeRegEx": "Marquez et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marquez et al\\.", "year": 2006}, {"title": "MUC-7 evaluation of IE technology: Overview of results", "author": ["Elaine Marsh", "Dennis Perzanowski"], "venue": "In MUC,", "citeRegEx": "Marsh and Perzanowski.,? \\Q1998\\E", "shortCiteRegEx": "Marsh and Perzanowski.", "year": 1998}, {"title": "HLTCOE efforts in entity linking at TAC KBP", "author": ["Paul McNamee"], "venue": "In TAC,", "citeRegEx": "McNamee.,? \\Q2010\\E", "shortCiteRegEx": "McNamee.", "year": 2010}, {"title": "Overview of the TAC 2009 Knowledge Base Population track", "author": ["Paul McNamee", "Hoa Dang"], "venue": "In TAC,", "citeRegEx": "McNamee and Dang.,? \\Q2009\\E", "shortCiteRegEx": "McNamee and Dang.", "year": 2009}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["Rada Mihalcea", "Andras Csomai"], "venue": "In CIKM,", "citeRegEx": "Mihalcea and Csomai.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea and Csomai.", "year": 2007}, {"title": "The Senseval-3 English lexical sample task", "author": ["Rada Mihalcea", "Timothy Chklovski", "Adam Kilgarriff"], "venue": "In Senseval,", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Learning to link with Wikipedia", "author": ["David Milne", "Ian H. Witten"], "venue": "In CIKM,", "citeRegEx": "Milne and Witten.,? \\Q2008\\E", "shortCiteRegEx": "Milne and Witten.", "year": 2008}, {"title": "Entity linking meets word sense disambiguation: a unied approach", "author": ["Andrea Moro", "Alessandro Raganato", "Roberto Navigli"], "venue": "Transactions of the Association of Computational Linguistics,", "citeRegEx": "Moro et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moro et al\\.", "year": 2014}, {"title": "Word sense disambiguation: A survey", "author": ["Roberto Navigli"], "venue": "ACM Computing Surveys,", "citeRegEx": "Navigli.,? \\Q2009\\E", "shortCiteRegEx": "Navigli.", "year": 2009}, {"title": "Exploring entity relations for named entity disambiguation", "author": ["Danuta Ploch"], "venue": "In ACL HLT: Student Session,", "citeRegEx": "Ploch.,? \\Q2011\\E", "shortCiteRegEx": "Ploch.", "year": 2011}, {"title": "SemEval2007 Task-17: English lexical sample SRL and all words", "author": ["Sameer S. Pradhan", "Edward Loper", "Dmitriy Dligach", "Martha Palmer"], "venue": "In SemEval,", "citeRegEx": "Pradhan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2007}, {"title": "Local and global algorithms for disambiguation to Wikipedia", "author": ["Lev-Arie Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson"], "venue": "In ACL,", "citeRegEx": "Ratinov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "The English all-words task", "author": ["Benjamin Snyder", "Martha Palmer"], "venue": "In Senseval,", "citeRegEx": "Snyder and Palmer.,? \\Q2004\\E", "shortCiteRegEx": "Snyder and Palmer.", "year": 2004}, {"title": "A cross-lingual dictionary for English Wikipedia concepts", "author": ["Valentin I. Spitkovsky", "Angel X. Chang"], "venue": "In LREC,", "citeRegEx": "Spitkovsky and Chang.,? \\Q2012\\E", "shortCiteRegEx": "Spitkovsky and Chang.", "year": 2012}, {"title": "Introduction to the CoNLL-2003 shared task: language-independent named entity recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder"], "venue": "In CoNLL,", "citeRegEx": "Sang and Meulder.,? \\Q2003\\E", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["Ioannis Tsochantaridis", "Thomas Hofmann", "Thorsten Joachims", "Yasemin Altun"], "venue": "In ICML,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2004}, {"title": "IIIT Hyderabad at TAC", "author": ["Vasudeva Varma", "Vijay Bharath Reddy", "Sudheer Kovelamudi", "Praveen Bysani", "Santhosh Gsk", "Kiran Kumar", "Kranthi Reddy", "Karuna Kumar", "Nithin Maganti"], "venue": "In TAC,", "citeRegEx": "Varma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Varma et al\\.", "year": 2009}, {"title": "From information to knowledge: Harvesting entities and relationships from web sources", "author": ["Gerhard Weikum", "Martin Theobald"], "venue": "In PODS,", "citeRegEx": "Weikum and Theobald.,? \\Q2010\\E", "shortCiteRegEx": "Weikum and Theobald.", "year": 2010}, {"title": "Entity linking leveraging automatically generated annotation", "author": ["Wei Zhang", "Jian Su", "Chew Lim Tan", "Wen Ting Wang"], "venue": "In COLING,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Learning to link entities with knowledge base", "author": ["Zhicheng Zheng", "Fangtao Li", "Minlie Huang", "Xiaoyan Zhu"], "venue": "In NAACL HLT,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng"], "venue": "In ACL: System Demonstrations,", "citeRegEx": "Zhong and Ng.,? \\Q2010\\E", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}, {"title": "Resolving surface forms to Wikipedia topics", "author": ["Yiping Zhou", "Lan Nie", "Omid Rouhani-Kalleh", "Flavian Vasile", "Scott Gaffney"], "venue": "In COLING,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 50, "context": "It also has many applications in its own right: text mining, information retrieval and the semantic web [Weikum and Theobald, 2010].", "startOffset": 104, "endOffset": 131}, {"referenceID": 36, "context": "The former terminology (EL) stresses the importance of linking a mention to an actual instance in the given knowledge-base [McNamee and Dang, 2009].", "startOffset": 123, "endOffset": 147}, {"referenceID": 53, "context": "The extensive WSD literature [Agirre and Edmonds, 2006, Navigli, 2009] has shown that building a supervised classifier for each target lemma \u2014 the so-called word-expert approach \u2014 outperforms other techniques [Zhong and Ng, 2010].", "startOffset": 209, "endOffset": 229}, {"referenceID": 11, "context": "To our knowledge, this is the first time a NED system following the word-expert approach is reported, although early work on Wikification already hinted at its usefulness [Csomai and Mihalcea, 2008].", "startOffset": 171, "endOffset": 198}, {"referenceID": 46, "context": "The dictionary\u2019s release was documented in a short conference paper [Spitkovsky and Chang, 2012], which we extend with additional explanations and analyses here.", "startOffset": 68, "endOffset": 96}, {"referenceID": 26, "context": "We present a detailed analysis of the performance of the components and variations of our NED system, as applied to the entity linking task of the NIST Text Analysis Conference\u2019s (TAC)6 knowledge-base population (KBP) track [Ji and Grishman, 2011].", "startOffset": 224, "endOffset": 247}, {"referenceID": 24, "context": "In the future we would like to explore other datasets which include all mentions in full documents [Hoffart et al., 2011].", "startOffset": 99, "endOffset": 121}, {"referenceID": 14, "context": "We focus on the candidate generation and disambiguation modules, leaving aside mention detection and the task of NIL detection, where mentions which refer to entities not listed in the knowledge-base have to be detected [Durrett and Klein, 2014].", "startOffset": 220, "endOffset": 245}, {"referenceID": 21, "context": "In WSD, an exhaustive dictionary is provided, while in NED, one has to generate all candidate entities for a target string \u2014 a step that has been shown to be critical to success [Hachey et al., 2012].", "startOffset": 178, "endOffset": 199}, {"referenceID": 6, "context": "Similar problems arise in citation databases, where it is necessary to decide which mentions of authors in bibliographic records refer to the same person [Bhattacharya and Getoor, 2007].", "startOffset": 154, "endOffset": 185}, {"referenceID": 15, "context": "The e-mails task can be tackled by assuming that each address corresponds to a distinct person and that people\u2019s identifying information can be deduced from what they write [Elsayed et al., 2008].", "startOffset": 173, "endOffset": 195}, {"referenceID": 29, "context": "For instance, it presupposes that mentions of named entities have already been identified in text, building up from the named entity recognition (NER) task [Marsh and Perzanowski, 1998, Tjong Kim Sang and De Meulder, 2003]. Each mention may be further labeled with a broad semantic category \u2014 such as names of persons, organizations or locations \u2014 via named entity classification (NEC). This last task is often performed by using gazetteers to cover many known entities, in addition to training a single supervised classifier that outputs category types for input mentions given their specific contexts. Overall, we view NED as a specific instantiation of the record linkage problem, in which the task is to find records referring to the same entity across different data sources, such as data files, databases, books or websites. The term record linkage was first used by Dunn Halbert [1946], in reference to resolving person names across official records held by a government.", "startOffset": 157, "endOffset": 893}, {"referenceID": 3, "context": "More recently, Bagga and Baldwin [1998] focused on cross-document coreference of people by first identifying coreference chains within documents and then comparing the found chains\u2019 contexts across documents.", "startOffset": 15, "endOffset": 40}, {"referenceID": 38, "context": "For instance, the Senseval-3 lexical sample dataset [Mihalcea et al., 2004] contains 2,945 manually annotated occurrences, 98.", "startOffset": 52, "endOffset": 75}, {"referenceID": 45, "context": "5% in the Senseval-3 all-words task [Snyder and Palmer, 2004].", "startOffset": 36, "endOffset": 61}, {"referenceID": 33, "context": "Typically, the problem is modeled using multi-class classifiers [Marquez et al., 2006], with one classifier trained for each target word (a.", "startOffset": 64, "endOffset": 86}, {"referenceID": 38, "context": "On the Senseval3 lexical sample task [Mihalcea et al., 2004], accuracies can reach as high as 72.", "startOffset": 37, "endOffset": 60}, {"referenceID": 45, "context": "4% [Snyder and Palmer, 2004].", "startOffset": 3, "endOffset": 28}, {"referenceID": 7, "context": "The earliest work on NED using Wikipedia is by Bunescu and Pasca [2006], who used article titles, redirects and disambiguation pages to generate candidate entities.", "startOffset": 47, "endOffset": 72}, {"referenceID": 7, "context": "The earliest work on NED using Wikipedia is by Bunescu and Pasca [2006], who used article titles, redirects and disambiguation pages to generate candidate entities. Similarity between a mention\u2019s context and article text provided the rankings, according to tf-idf and cosine similarity. Each article\u2019s term vector was further enriched using words from other articles in the same category. Disambiguation of mentions was local, i.e., performed separately for each one. Cucerzan [2007] followed an overall similar design, but using context vectors that consisted of key words and short phrases extracted from Wikipedia.", "startOffset": 47, "endOffset": 484}, {"referenceID": 18, "context": "7) by Hachey et al. [2012], who also reimplemented the earlier system of Bunescu and Pasca [2006].", "startOffset": 6, "endOffset": 27}, {"referenceID": 7, "context": "[2012], who also reimplemented the earlier system of Bunescu and Pasca [2006]. Fader et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 7, "context": "[2012], who also reimplemented the earlier system of Bunescu and Pasca [2006]. Fader et al. [2009] also generated candidates as did Cucerzan [2007].", "startOffset": 53, "endOffset": 99}, {"referenceID": 7, "context": "[2012], who also reimplemented the earlier system of Bunescu and Pasca [2006]. Fader et al. [2009] also generated candidates as did Cucerzan [2007]. They introduced prior probabilities \u2014 estimated from the numbers of anchors that refer to entities \u2014 in addition to considering the overlap between the context of a mention and the text of the target articles.", "startOffset": 53, "endOffset": 148}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms.", "startOffset": 44, "endOffset": 387}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.", "startOffset": 44, "endOffset": 638}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.", "startOffset": 44, "endOffset": 797}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification.", "startOffset": 44, "endOffset": 817}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis).", "startOffset": 44, "endOffset": 885}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis).", "startOffset": 44, "endOffset": 932}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents.", "startOffset": 44, "endOffset": 1135}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents.", "startOffset": 44, "endOffset": 1202}, {"referenceID": 11, "context": "In seminal work [Mihalcea and Csomai, 2007, Csomai and Mihalcea, 2008], the authors used mentions in anchors to train a supervised (na\u0131\u0308ve Bayes) classifier. This is the work most similar to ours. However, they did not address the problems of building a dictionary or different methods to collect training data (cf. Section 5.2). Wikification work continued with Milne and Witten [2008], who combined popularity and relatedness (computed as the number of inlinks shared between the context and target articles), using several machine learning algorithms. They were the first to use the link structure of Wikipedia. Kulkarni et al. [2009] later proposed a method that collectively wikified an entire document, by solving a global optimization problem, using ideas from both Milne and Witten [2008] and Cucerzan [2007], but now applied in the context of wikification. Zhou et al. [2010] also built on ideas of Milne and Witten [2008] and included, for the first time, relatedness between entities (based on search engine log analysis). They tested two classifiers (binary and learning-to-rank), with mixed results. Ratinov et al. [2011] utilized link structure \u2014 again, following Milne and Witten [2008] \u2014 to arrive at coherent sets of disambiguations for input documents. They used a ranker to select best-fitting entities. Although anchor context was used to compute similarity between article texts, it was not tapped for features during classification. Evaluation against their own dataset showed that improvements over local disambiguation were small. Guo et al. [2011] use direct hyperlinks between the target entity and the mentions in the context, using directly the number of such links.", "startOffset": 44, "endOffset": 1573}, {"referenceID": 49, "context": "In 2009, Varma et al. [2009] obtained the best results, using anchors from Wikipedia to train one classifier per target string, combined with querying of mention contexts against an online search engine.", "startOffset": 9, "endOffset": 29}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best.", "startOffset": 9, "endOffset": 31}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier.", "startOffset": 9, "endOffset": 269}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier.", "startOffset": 9, "endOffset": 297}, {"referenceID": 29, "context": "In 2010, Lehmann et al. [2010] did best. They used titles, redirects, anchors and disambiguation pages, as well as Google searches, dynamic generation of acronym expansions, longer mentions and inexact matching. Their system pulled in features from both McNamee [2010] and Milne and Witten [2008] to train a binary classifier. McNamee [2010] had the highest-scoring submission among systems that did not use the text in target Wikipedia articles, focusing on the provided KB alone for building the list of candidate entities.", "startOffset": 9, "endOffset": 342}, {"referenceID": 13, "context": "A closely related system by Dredze et al. [2010] included finite-state transducers that had been trained to recognize common abbreviations, such as \u201cJ Miller\u201d for \u201cJennifer Miller.", "startOffset": 28, "endOffset": 49}, {"referenceID": 51, "context": "A similar system [Zhang et al., 2010] made use of a synthetic corpus, with unambiguous occurrences of strings replaced by ambiguous synonyms.", "startOffset": 17, "endOffset": 37}, {"referenceID": 35, "context": "[2010] also use a number of features that resemble those of McNamee [2010]. They evaluated several learning-to-rank systems, with ListNet yielding best results.", "startOffset": 60, "endOffset": 75}, {"referenceID": 35, "context": "[2010] also use a number of features that resemble those of McNamee [2010]. They evaluated several learning-to-rank systems, with ListNet yielding best results. Unfortunately their systems were developed, trained and tested on the same corpus, making it unclear whether their results are comparable to those of other systems evaluated in TAC-KBP 2009. A similar system [Zhang et al., 2010] made use of a synthetic corpus, with unambiguous occurrences of strings replaced by ambiguous synonyms. Manufactured training data was then combined with Wikipedia. Unfortunately, since the overall system was developed and evaluated on the same (2009) corpus, it may have been overfitted.", "startOffset": 60, "endOffset": 642}, {"referenceID": 19, "context": "Recently, Hachey et al. [2012] reimplemented three well-known NED systems [Bunescu and Pasca, 2006, Cucerzan, 2007, Varma et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "None of the published NED systems, except Csomai and Mihalcea [2008], uses the context of anchors in Wikipedia to train a classifier for each mention, as we do.", "startOffset": 42, "endOffset": 69}, {"referenceID": 21, "context": "9Again, with the notable exception of work by Hachey et al. [2012], which we will discuss in detail (see Section 8.", "startOffset": 46, "endOffset": 67}, {"referenceID": 21, "context": "One could view these two components as performing (i) candidate generation, listing all possible entities for a mention; and (ii) candidate selection [Hachey et al., 2012], although our generation module also scores and ranks entities (by popularity).", "startOffset": 150, "endOffset": 171}, {"referenceID": 24, "context": "In the future we would like to extend our work to other datasets which include all mentions in full documents [Hoffart et al., 2011].", "startOffset": 110, "endOffset": 132}, {"referenceID": 48, "context": "Classifier: The second and third rows correspond to the accuracies of a multi-class classifier, based on SVMmulticlass [Tsochantaridis et al., 2004], and a one-versusall approach, using SVMlight binary classifiers [Joachims, 1999].", "startOffset": 119, "endOffset": 148}, {"referenceID": 28, "context": ", 2004], and a one-versusall approach, using SVMlight binary classifiers [Joachims, 1999].", "startOffset": 73, "endOffset": 89}, {"referenceID": 32, "context": "Both alternatives perform worse than our default classification algorithm (maximum entropy with `2regularization), MAXENT [Manning and Klein, 2003].", "startOffset": 122, "endOffset": 147}, {"referenceID": 13, "context": "System KB-only [Dredze et al., 2010] 0.", "startOffset": 15, "endOffset": 36}, {"referenceID": 21, "context": "7063 [Hachey et al., 2012] \u22170.", "startOffset": 5, "endOffset": 26}, {"referenceID": 49, "context": "723 [Varma et al., 2009] 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 51, "context": "[Zhang et al., 2010] \u22170.", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "792 [Han and Sun, 2011] \u22170.", "startOffset": 4, "endOffset": 23}, {"referenceID": 20, "context": "79 [Guo et al., 2011] 0.", "startOffset": 3, "endOffset": 21}, {"referenceID": 35, "context": "2010 [McNamee, 2010] 0.", "startOffset": 5, "endOffset": 20}, {"referenceID": 49, "context": "[Varma et al., 2009] 0.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "[Guo et al., 2011] 0.", "startOffset": 0, "endOffset": 18}, {"referenceID": 21, "context": "741 [Hachey et al., 2012] \u22170.", "startOffset": 4, "endOffset": 25}, {"referenceID": 30, "context": "784 [Lehmann et al., 2010] 0.", "startOffset": 4, "endOffset": 26}, {"referenceID": 20, "context": "705 Reported by Hachey et al. [2012]. [Guo et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 35, "context": "Here the dictionary performs better than the next best \u201cno context\u201d system [McNamee, 2010] by a large margin: 75% vs.", "startOffset": 75, "endOffset": 90}, {"referenceID": 30, "context": "81% for the next best system [Lehmann et al., 2010].", "startOffset": 29, "endOffset": 51}, {"referenceID": 21, "context": "The entry for Hachey et al. [2012] represents the best of several systems chosen on the test set; their best variant according to development data was, in fact, the system of Varma et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 21, "context": "The entry for Hachey et al. [2012] represents the best of several systems chosen on the test set; their best variant according to development data was, in fact, the system of Varma et al. [2009], which scores 7% lower22", "startOffset": 14, "endOffset": 195}, {"referenceID": 19, "context": "22Note that, since Hachey et al. [2012] reimplemented three well-known systems [Bunescu and Pasca, 2006, Cucerzan, 2007, Varma et al.", "startOffset": 19, "endOffset": 40}, {"referenceID": 49, "context": ", 2010], metaphones [Varma et al., 2009], as well as bolded words in first paragraphs and hatnote templates [Hachey et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 21, "context": ", 2009], as well as bolded words in first paragraphs and hatnote templates [Hachey et al., 2012].", "startOffset": 75, "endOffset": 96}, {"referenceID": 13, "context": ", 2010], finite-state transducers [Dredze et al., 2010], matches with longer mentions [Lehmann et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 30, "context": ", 2010], matches with longer mentions [Lehmann et al., 2010], automatic spelling correction [Zheng et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 52, "context": ", 2010], automatic spelling correction [Zheng et al., 2010], Wikipedia search engines and the \u201cDid you mean.", "startOffset": 39, "endOffset": 59}, {"referenceID": 51, "context": "\u201d functionality [Zhang et al., 2010].", "startOffset": 16, "endOffset": 36}, {"referenceID": 20, "context": "This fact should not be overlooked, since Hachey et al. [2012] found that candidate generation accounted for most of the performance variation in the systems that they reimplemented; in particular, acronym handling, using coreference resolution to find longer mentions, led to substantial improvements.", "startOffset": 42, "endOffset": 63}, {"referenceID": 21, "context": ", Han and Sun [2011] report end-to-end performance of their popularity and name model (roughly equivalent to our dictionary), combined with a NIL detection system, excluding KB- or NIL-only results.", "startOffset": 2, "endOffset": 21}, {"referenceID": 21, "context": "24An exception, Hachey et al. [2012] provide precision and recall of their candidate generation component for KB queries (56.", "startOffset": 16, "endOffset": 37}, {"referenceID": 40, "context": "We would also like to include our classifiers in more complex NED systems, where complementary information like link structure [Moro et al., 2014], similarities between article texts and mention contexts [Hoffart et al.", "startOffset": 127, "endOffset": 146}, {"referenceID": 25, "context": ", 2014], similarities between article texts and mention contexts [Hoffart et al., 2012], as well as global optimization techniques [Ratinov et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 44, "context": ", 2012], as well as global optimization techniques [Ratinov et al., 2011] could further improve the results.", "startOffset": 51, "endOffset": 73}, {"referenceID": 24, "context": "In the future, we would like to extend our study using datasets which include all mentions in full documents [Hoffart et al., 2011].", "startOffset": 109, "endOffset": 131}, {"referenceID": 21, "context": "In WSD settings, dictionaries are provided, but NED involves constructing possible mappings from strings to entities \u2014 a step that Hachey et al. [2012] showed to be key to success, which we also confirmed experimentally.", "startOffset": 131, "endOffset": 152}], "year": 2016, "abstractText": "Named Entity Disambiguation (NED) is the task of linking a named-entity mention to an instance in a knowledge-base, typically Wikipedia. This task is closely related to word-sense disambiguation (WSD), where the supervised word-expert approach has prevailed. In this work we present, for the first time, the results of the word-expert approach to NED, where one classifier is built for each target entity mention string. The resources necessary to build the system, a dictionary and a set of training instances, have been automatically derived from Wikipedia. We provide empirical evidence of the value of this approach, as well as a study of the differences between WSD and NED, including ambiguity and synonymy statistics.", "creator": "LaTeX with hyperref package"}}}