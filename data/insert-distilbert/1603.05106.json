{"id": "1603.05106", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "One-Shot Generalization in Deep Generative Models", "abstract": "also humans have an impressive remarkable ability to reason publicly about new concepts and experiences from just a single example. in particular, humans have an intuitive ability possessed for initiating one - shot generalization : an ability to accidentally encounter a new concept, understand fully its structure, and then inevitably be able to generate compelling alternative variations of the concept. simultaneously we develop machine learning systems with this important capacity by developing new deep reasoning generative models, models that combine arguably the representational power of deep learning with the inferential power of bayesian mathematical reasoning. we would develop a class complex of composite sequential generative models that are built on the principles of feedback and attention. these two characteristics lead to generative models that are among the state - of - the art in density functional estimation and digital image generation. we demonstrate the one - \u2010 shot generalization ability necessary of our models using three tasks : unconditional sample sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. in all cases our models are able to generate compelling and diverse samples - - - having seen distinctly new examples just once - - - providing an important class of general - purpose models for one - shot machine learning.", "histories": [["v1", "Wed, 16 Mar 2016 14:10:00 GMT  (6267kb,D)", "http://arxiv.org/abs/1603.05106v1", null], ["v2", "Wed, 25 May 2016 12:57:19 GMT  (77030kb,D)", "http://arxiv.org/abs/1603.05106v2", "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd International Conference on Machine Learning, JMLR: W&amp;CP volume 48, 2016"]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["danilo jimenez rezende", "shakir mohamed", "ivo danihelka", "karol gregor", "daan wierstra"], "accepted": true, "id": "1603.05106"}, "pdf": {"name": "1603.05106.pdf", "metadata": {"source": "META", "title": "One-Shot Generalization in Deep Generative Models", "authors": ["Danilo J. Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "emails": ["DANILOR@GOOGLE.COM", "SHAKIR@GOOGLE.COM", "DANIHELKA@GOOGLE.COM", "KAROLG@GOOGLE.COM", "WIERSTRA@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "Consider the images in the red box to the left in figure 1. We see each of these new concepts just once, understand their structure, and are then able to imagine and generate compelling alternative variations of each concept, similar to those drawn in the rows beneath the red box. This is an\nability that humans have for one-shot generalization: an ability to generalize to new concepts given just one or a few examples. In this paper, we develop new models that possess this capacity for one-shot generalization\u2014models that allow for one-shot reasoning from the data streams we are likely to encounter in practice, that use only limited forms of domain-specific knowledge, and that can be applied to diverse sets of problems.\nThere are few approaches for one-shot generalization. Salakhutdinov et al. (2013) developed a probabilistic model that combines a deep Boltzmann machine with a hierarchical Dirichlet process to learn hierarchies of concept categories as well as provide a powerful generative model. Recently, Lake et al. (2015) presented a compelling demonstration of the ability of probabilistic models to perform one-shot generalization, using Bayesian program learning, which is able to learn a hierarchical, non-parametric generative model of handwritten characters. Their approach incorporates specific knowledge of how strokes are formed and the ways in which they are combined to produce characters of different types, exploiting similar strategies used by humans to perform this task. Lake et al. (2015) see the capacity for one-shot generalization demonstrated by Bayesian programming learning \u2018as a challenge for neural models\u2019. By combining the representational power of deep neural networks embedded within hierarchical latent variable models, with the inferential power of approximate Bayesian reasoning, we show that this is a challenge that can be overcome. The resulting deep generative models are general-purpose image models that are accurate and scalable, among the state-of-the-art, and possess the important capacity for one-shot generalization.\nDeep generative models are a rich class of models for density estimation that specify a generative process for observed data using a hierarchy of latent variables. Models that are directed graphical models have recently become popular in machine learning and include discrete latent variable models such as sigmoid belief networks and deep auto-regressive networks (Saul et al., 1996; Gregor et al., 2014), or continuous latent variable models such as non-linear Gaussian belief networks and deep latent Gaus-\nar X\niv :1\n60 3.\n05 10\n6v 1\n[ st\nat .M\nL ]\n1 6\nM ar\n2 01\nsian models (Rezende et al., 2014; Kingma & Welling, 2014). These models use deep networks in the specification of their conditional probability distributions to allow rich non-linear structure to be learned. Such models have been shown to have a number of desirable properties: inference of the latent variables allows us to provide a causal explanation for the data that can be used to explore its underlying factors of variation, and for exploratory analysis; analogical reasoning between two related concepts, e.g., styles and identities of images, is naturally possible; any missing data can be imputed by treating them as additional latent variables, capturing the the full range of correlation between missing entries under any missingness mechanism; these models embody minimum description length principles and can be used for compression; these models can be used to learn environment-simulators enabling a wide range of approaches for simulation-based planning.\nTwo principles are central to our approach: feedback and attention. These principles allow the models we develop to reflect the principles of analysis-by-synthesis, in which the analysis of observed information is continually integrated with constructed interpretations of it (Yuille & Kersten, 2006; Erdogan et al., 2015; Nair et al., 2008). Analysis is realized by attentional mechanisms that allow us to selectively process and route information from the observed data into the model. Interpretations of the data are then obtained by sets of latent variables that are inferred sequentially to evaluate the probability of the data. The aim of such a construction is to introduce internal feedback into the model that allows for a \u2018thinking time\u2019 during which information can be extracted from each data point more effectively, leading to improved inference, generation and generalization. We shall refer to such models as sequential generative models. Models such as DRAW (Gregor et al., 2015) and composited variational auto-encoders (Huang & Murphy, 2015) are existing models in this class, and we will develop a general class of sequential generative models that incorporates these and other latent variable models and variational auto-encoders. Our contributions are: \u2022 We develop sequential generative models that provide a\ngeneralization of existing approaches, allowing for sequential generation and inference, multi-modal posterior approximations, and a rich new class of deep generative models. \u2022 We demonstrate the clear improvement that the combination of attentional mechanisms in more powerful models and inference has in advancing the state-of-the-art in deep generative models. \u2022 Importantly, we show that our generative models have the ability to perform one-shot generalization. We explore three generalization tasks and show that our models can imagine and generate compelling alternative variations of images after having seen them just once."}, {"heading": "2. Varieties of Attention", "text": "Attending to parts of a scene, ignoring others, analyzing the parts that we focus on, and sequentially building up an interpretation and understanding of a scene: these are natural parts of human cognition. This is so successful a strategy for reasoning that it is now also an important part of many machine learning systems, and this repeated process of attention and interpretation, analysis and synthesis, is an important component of the generative models we develop.\nIn its most general form, any mechanism that allows us to selectively route information from one part of our model to another can be regarded as an attentional mechanism. Attention allows for a wide range of invariances to be incorporated, with few additional parameters and low computational cost. Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015). The attention used in discriminative tasks is a \u2018reading\u2019 attention, where we work, not on the image itself, but on a transformation of the image (into a canonical coordinate space that is typically lower dimensional), with the parameters controlling the attention being learned by gradient descent.\nAttention in unsupervised learning is much more recent (Tang et al., 2014; Gregor et al., 2015). In latent variable models, we have two processes, inference and generation, that can both use attention, though in slightly different ways. The generative process makes use of a \u2018writing\u2019 or generative attention, which implements a selective updating of the output variables, e.g., updating only a small part of the image. The inference process makes use of reading attention, like that used in classification. Although conceptually different, both these forms of attention can be implemented with the same computational tools. We focus on image modelling and make use of spatial attention. Two other types of attention, randomized and error-based, are discussed in appendix B.\nSpatially-transformed attention. Rather than simply selecting a patch of an image (taking glimpses) as other methods do, a more powerful approach is to use a mechanism that provides invariance to shape and size of objects in the images (general affine transformations). Tang et al. (2014) take such an approach and use 2D similarity transforms to provide basic affine invariance. Spatial transformers (Jaderberg et al., 2015) are a more general method for providing such invariance, and is our preferred attentional mechanism. Spatial transformers (ST) process an input image x, using parameters h, and generate an output y(x,h):\ny(x,h) = [\u03bah(h)\u2297 \u03baw(h)] \u2217 x,\nwhere \u03bah and \u03baw are 1-dimensional kernels, \u2297 indicates the tensor outer-product of the two kernels and \u2217 indicates a convolution. Huang & Murphy (2015) develop occlusionaware generative models that make use of spatial transformers in this way. When used for reading attention, spatial transformers allow the model to observe the input image in a canonical form, providing the desired invariance. When used for writing attention, it allows the generative model to independently handle position, scale and rotation of parts of the generated image, as well as their content. An simple extension is to use multiple attention windows simultaneously (see appendix)."}, {"heading": "3. Iterative and Attentive Generative Models", "text": ""}, {"heading": "3.1. Latent Variable Models and Variational Inference", "text": "Generative models with latent variables describe the probabilistic process by which an observed data point can be generated. The simplest formulations such as PCA and factor analysis use Gaussian latent variables z that are combined linearly to generate Gaussian distributed data points x. In more complex models, the probabilistic description consists of a hierarchy of L layers of latent variables, where each layer depends on the layer above in a non-linear way (Rezende et al., 2014) \u2014for deep generative models, we specify this non-linear dependency using deep neural networks. To compute the marginal probability of the data, we must integrate over any unobserved variables:\np(x) = \u222b p\u03b8(x|z)p(z)dz (1)\nIn deep latent Gaussian models, the prior distribution p(z) is a Gaussian distribution and the likelihood function p\u03b8(x|z) is any distribution that is appropriate for the observed data, such as a Gaussian, Bernoulli, categorical or other distribution, and that is dependent in a non-linear way on the latent variables. For most models, the marginal likelihood (1) is intractable and we must instead approximate it. One popular approximation technique is based on variational inference (Jordan et al., 1999), which transforms the difficult integration into an optimization problem that is typically more scalable and easier to solve. Using variational inference we can approximate the marginal likelihood by a lower bound, which is the objective function we use for optimization:\nF = Eq(z|x)[log p\u03b8(x|z)]\u2212 KL[q\u03c6(z|x)\u2016p(z)] (2)\nThe objective function (2) is the negative free energy, which allows us to trade-off the reconstruction ability of the model (first term) against the complexity of the posterior distribution (second term). Variational inference approximates the true posterior distribution by a known family of approximating posteriors q\u03c6(z|x) with variational parameters \u03c6. Learning now involves optimization of the variational parameters \u03c6 and model parameters \u03b8.\nInstead of optimization by the variational EM algorithm,\nwe take an amortized inference approach and represent the distribution q(z|x) as a recognition or inference model, which we also parameterize using a deep neural network. Inference models amortize the cost of posterior inference and makes it more efficient by allowing for generalization across the inference computations using a set of global variational parameters \u03c6. In this framework, we can think of the generative model as a decoder of the latent variables, and the inference network as its inverse, an encoder of the observed data into the latent description. As a result, this specific combination of deep latent variable model (typically latent Gaussian) with variational inference that is implemented using an inference network is referred to as a variational auto-encoder (VAE). VAEs allow for a single computational graph to be constructed and straightforward gradient computations: when the latent variables are continuous, gradient estimators based on pathwise derivative estimators are used (Rezende et al., 2014; Kingma & Welling, 2014; Burda et al., 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al., 2014; Mansimov et al., 2015)."}, {"heading": "3.2. Sequential Generative Models", "text": "The generative models as we have described them thus far can be characterized as single-step models, since they are models of i.i.d data that evaluate their likelihood functions by transforming the latent variables using a nonlinear, feed-forward transformation. A sequential generative model is a natural extension of the latent variable models used in VAEs. They combine both stochastic and deterministic computations to form a multi-step generative process that uses recursive transformations of the latent variables, i.e. uses an internal state-space model."}, {"heading": "3.2.1. GENERATIVE MODEL", "text": "In their most general form, sequential generative models describe the observed data over T time steps using a set of latent variables zt at each step. The generative model is shown in the stochastic computational graph of figure 2(a), and described by:\nLatent variables zt \u223c N (zt|0,1) t = 1, . . . , T (3) Hidden state ht = fh(ht\u22121, zt; \u03b8h) (4)\nHidden Canvas ct = fc(ct\u22121,ht; \u03b8c) (5) Observation x \u223c p(x|fo(cT ; \u03b8o)) (6)\nEach step generates an independent set of K-dimensional latent variables zt (equation (3)). A deterministic transition function fh introduces the sequential dependency between each of the latent variables (equation (4)). This allows any transition mechanism to be used and our transition is specified as a long short-term memory network (LSTM, Hochreiter & Schmidhuber (1997). We explicitly represent the creation of a set of hidden variables ct that represents a hidden canvas of the model equation (5). The canvas\nfunction fc allows for many different transformations, and it is here where generative (writing) attention is used; we describe a number of choices for this function in section 3.2.3. The condition (6) is computed using an observation function fo(c; \u03b8o) that maps the last hidden canvas cT to the parameters of the observation model, e.g., probability of being on or off for a Bernoulli distribution. We denote the set of all parameters of this generative model as \u03b8 = {\u03b8h, \u03b8c, \u03b8o}."}, {"heading": "3.2.2. FREE ENERGY OBJECTIVE", "text": "Given the probabilistic model (3)-(6) we can obtain an objective function for inference and parameter learning using variational inference. By applying the variational principle, we obtain the free energy objective:\nlog p(x) = log \u222b p(x|z1,...,T )p(z1...T )dz1...T \u2265 F\nF = Eq(z1,...,T )[log p\u03b8(x|z1,...,T )] \u2212\u2211Tt=1 KL[q\u03c6(zt|z<tx)\u2016p(zt)], (7)\nwhere z<t indicates the collection of all latent variables from iteration 1 to t \u2212 1. We can now optimize this objective function for the variational parameters \u03c6 and the model parameters \u03b8, by stochastic gradient descent using a mini-batch of data. As with other VAEs, we use a single sample of the latent variables generated from q\u03c6(z|x) when computing the Monte Carlo gradient. To complete our specification, we now describe specific forms of the hidden-canvas functions fc and the approximate posterior distribution q\u03c6(zt)."}, {"heading": "3.2.3. HIDDEN CANVAS FUNCTIONS", "text": "The canvas transition function fc(ct\u22121,ht; \u03b8c) (5) updates the hidden canvas by first non-linearly transforming the current hidden state of the LSTM ht (using a function fw) and fuses the result with the existing canvas ct\u22121. In this work we use hidden canvases that have the same size as the original images, though they could be either larger or smaller in size and can have any number of channels (four in this paper). We consider two ways with which to update the hidden canvas:\nAdditive Canvas. As the name implies, an additive canvas updates the canvas by simply adding a transformation of the hidden state fw(ht; \u03b8c) to the previous canvas state ct\u22121. This is a simple, yet effective (see results) update rule:\nfc(ct\u22121,ht; \u03b8c) = ct\u22121 + fw(ht; \u03b8c), (8)\nGated Recurrent Canvas. The canvas function can be updated using a convolutional gated recurrent unit (CGRU) architecture (Kaiser & Sutskever, 2015), which provides a non-linear and recursive updating mechanism for the canvas and are simplified versions of convolutional LSTMs (further details of the CGRU are given in appendix B). The canvas update is:\nfc(ct\u22121,ht; \u03b8c) = CGRU(ct\u22121 + fw(ht; \u03b8c)) (9)\nIn both cases, the function fw(ht; \u03b8w) is a writing function that is used by the canvas function to transform the LSTM hidden state into the coordinate system of the hidden canvas. This can be any operation, such as a fully- or locally-connected mapping. We use a writing or generative attention function. The writing attention will be fixed to use a spatial transformer throughout.\nThe final phase of the generative process transforms the hidden canvas at the last time step cT into the parameters of the likelihood function using the output function fo(c; \u03b8o). Since we use a hidden canvas that is the same size as the original images but that have a different number of filters, we implement the output function as a 1\u00d7 1 convolution. When the hidden canvas has a different size, a convolutional network is used instead."}, {"heading": "3.2.4. DEPENDENT POSTERIOR INFERENCE", "text": "We use a structured posterior approximation that has an auto-regressive form, i.e. q(zt|z<t,x). We implement this distribution as an inference network parameterized by a deep network. The specific form we use is:\nSprite rt = fr(x,ht\u22121;\u03c6r) (10) Sample zt\u223cN (zt|\u00b5(st,ht\u22121;\u03c6\u00b5),\u03c3(rt,ht\u22121;\u03c6\u03c3)) (11)\nAt every step of computation, we form a low-dimensional representation rt of the input image using a non-linear\ntransformation fr of the input image and the hidden state of the model. This function is a reading function and is the counterpart to the writing function from the previous section. The reading function allows the input image to be transformed into a new coordinate space that allows for easier inference computations. Like the writing function, reading can be implemented as a fully- or locallyconnected network (and we compare to a fully-connected function). Better inference is obtained using a reading or recognition attention. The result of reading is a sprite rt that is then combined with the previous state ht\u22121 through a further non-linear function to produce the mean \u00b5t and variance \u03c3t of a K-dimensional diagonal Gaussian distribution. We denote all the parameters of the inference model by \u03c6 = {\u03c6r, \u03c6\u00b5, \u03c6\u03c3}. Although the conditional distributions q(zt|z<t) are Gaussian, the joint posterior posterior is non-Gaussian and can be multi-modal, and thus provides more accurate inference."}, {"heading": "3.2.5. MODEL PROPERTIES AND COMPLEXITY", "text": "The above sequential generative model and inference is a generalization of existing models such as DRAW (Gregor et al., 2015) and composited VAEs (Huang & Murphy, 2015). This generalization has a number of differences and important properties. One of the largest deviations is the introduction of the hidden canvas into the generative model that provides an important richness to the model, since it allows a pre-image to be constructed in a hidden space before a final corrective transformation, using the function fo, is used. The generative process has an important property that allows the model be sampled without feeding-back the results of the canvas ct to the hidden state ht\u2014such a connection is not needed and provides more efficiency by reducing the number of model parameters. The inference network in our framework is also similarly simplified. We do not use a separate recurrent function within the inference network (like DRAW), but instead share parameters of the LSTM from the prior\u2014the removal of this additional recursive function has no effect on performance.\nAnother important difference between our framework and existing frameworks is the type of attention that is used. Gregor et al. (2015) use a generative attention based on Gaussian convolutions parameterized by a location and scale, and Tang et al. (2014) use 2D similarity transformations. We use a much more powerful and general attention mechanism based on spatial transformers (Jaderberg et al., 2015; Huang & Murphy, 2015).\nThe overall complexity of the algorithm described matches the typical complexity of widely-used methods in deep learning. For images of size I \u00d7 I , the spatial transformer has a complexity that is linear in the number of pixels of the attention window. For a J \u00d7 J attention window, with J \u2264 I , the spatial transformer has a complexity of\nO(NTJ2), for T sequential steps and N data points. All other components have the standard quadratic complexity in the layer size, hence for L layers with average size D, this gives a complexity of O(NLD2)."}, {"heading": "4. Image Generation and Analysis", "text": "We first show that our models are state-of-the-art, obtaining highly competitive likelihoods, and are able to generate high-quality samples across a wide range of data sets with different characteristics.\nFor all our experiments, our data consists of binary images and we use use a Bernoulli likelihood to model the probability of the pixels. In all models we use 400 LSTM hidden units. We use 12 \u00d7 12 kernels for the spatial transformer, whether used for recognition or generative attention. The latent variable zt are 4-dimensional Gaussian distributions and we use a number of steps that vary from 20-80. The hidden canvas has dimensions that are the size of the images with four channels. We present the main results here and any additional results in Appendix A. All the models were trained for approximatively 800K iterations with mini-batches of size 24. The reported likelihood bounds for the training set are computed by averaging the last 1K iterations during training. The reported likelihood bounds for the test set were computed by averaging the bound for 24000 random samples (sampled with replacement) and the reported error bars are the standard-deviations of the mean."}, {"heading": "4.1. MNIST and Multi-MNIST", "text": "We highlight the behaviour of the models using two data sets based on the MNIST benchmark. The first experiment uses the binarized MNIST data set of Salakhutdinov & Murray (2008), that consists of 28 \u00d7 28 binary images with 50,000 training and 10,000 test images. Table 1 compares the log-likelihoods obtained on MNIST using existing models, as well as the models discussed here (with variances of our estimates in parentheses). The sequential generative model that uses the spatially-transformed attention with the CGRU hidden canvas provides the best performance among existing work on this data set. We show samples from the model in figure 3.\nWe form a multi-MNIST data set of 64 \u00d7 64 images that consists of two MNIST digits placed at random locations in the image (having adapted the cluttered MNIST generator from Mnih et al. (2014) to procedurally generate the data). We compare the performance in table 2 and show samples from this model in figure 3. This data set is much harder than MNIST to learn, with much slower convergence. The additive canvas with spatially-transformed attention provides a reliable way to learn from this data.\nImportance of each step These results also indicate that longer sequences can lead to better performance. Every step taken by the model adds\na term to the objective function (2) corresponding to the KL-divergence between the prior distribution and the contribution to the approximate posterior distribution at that step. Figure 4 shows the KL-divergence for each iteration for two models on MNIST up to 20 steps. The KLdivergence decays towards the end of the sequence, indicating that the latent variables zt have diminishing contribution to the model as the number of steps grow. Unlike VAEs where we often find that there are many dimensions which contribute little to the likelihood bound, the sequential property allows us to more efficiently allocate and decide on the number of latent variables to use and means of deciding when to terminate the sequential computation."}, {"heading": "4.2. Omniglot", "text": "Unlike MNIST, which has a small number of classes with many images of each class and a large amount of data, the omniglot data set (Lake et al., 2015) consists of 105\u00d7 105 binary images across 1628 classes with just 20 images per class. This data set allows us to demonstrate that attentional mechanisms and better generative models allow us to perform well even in regimes with larger images and limited amounts of data.\nThere are two versions of the omniglot data that have been previously used for the evaluation of generative models. One data set used by Burda et al. (2015) consists of 28\u00d728\n\u25cf\n\u25cf\n\u25cf \u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n\u25cf\n1\n2\n3\n4\n5 10 15 20 Steps\nK LD\n( na\nts )\n\u25cf \u25cf ST+CGRU ST+Additive\nFigure 4. Per-step KL contribution on MNIST.\n125\n130\n135\n140\n145\n150\n155\n30x20 40x10 45x5 Data split\nB ou\nnd (\nna ts\n) train test\nFigure 5. Gap between train and test bound on omniglot.\nimages, but is different to that of Lake et al. (2015). We compare the available methods on the dataset from Burda et al. (2015) in table 3 and find that the sequential models perform better than all competing approaches, further establishing the effectiveness of these models. Our second evaluation uses the dataset of Lake et al. (2015), which we downsampled to 52 \u00d7 52 using a 2 \u00d7 2 max-pooling. We compare different sequential models in table 4 and again find that spatially-transformed attention is a powerful general purpose attention and that the additive hidden canvas performs best."}, {"heading": "4.3. Multi-PIE", "text": "The Multi-PIE dataset (Gross et al., 2010) consists of 48 \u00d7 48 RGB face images from various viewpoints. We have converted the images to grayscale and trained our model on a subset comprising of all 15-viewpoints but only 3 out of the 19 illumination conditions. Our simplification results in 93, 130 training samples and 10, 000 test samples. Samples from this model are shown in figure 7 and are highly compelling, showing faces in different orientations, different genders and are representative of the data. The model was trained using the logit-normal density for the pixels as in Rezende & Mohamed (2015)."}, {"heading": "5. One-Shot Generalization", "text": "Lake et al. (2015) introduce three tasks to evaluate one-shot generalization, testing weaker to stronger forms of generalization. The three tasks are: (1) unconditional (free) generation, (2) generation of novel variations of a given exemplar, and (3) generation of representative samples from a novel alphabet. Lake et al. (2015) conduct human evaluations as part of their assessment, which is important in contrasting the performance of models against the cognitive ability of humans; we do not conduct human benchmarks in this paper (human evaluation will form part of our follow-up work). Our focus is on the machine learning of one-shot generalization and the computational challenges\nassociated with this task."}, {"heading": "1. Unconditional Generation.", "text": "This is the same generation task reported for the data sets in the previous section. Figure 8 shows samples that reflect the characteristics of the omniglot data, showing a variety of styles including rounded patterns, line segments, thick and thin strokes that are representative of the data set. The likelihoods reported in tables 3 and 4 quantitatively establish this model as among the state-of-the-art."}, {"heading": "2. Novel variations of a given exemplar.", "text": "This task corresponds to figure 5 in Lake et al. (2015)). At test time, the model is presented with a character of a type it has never seen before (was not part of its training set), and asked to generate novel variations of this character. To do this, a simple modification of the model is made, and shown in figure 2(b), in which it is conditioned on an external context. The context is the image that we wish the model to generate new exemplars of. To expose the boundaries of our approach, we test this under weak and strong one-shot generalization tests:\na) We use a data set whose training data consists of all available alphabets, but for which three character types from each alphabet have been removed to form the test set (3000 characters). This is a weak one-shot generalization test where, although the model has never seen the test set characters, it has seen related characters from the same alphabet and is expected to transfer that knowledge to this generation task. b) We use exactly the data split used by Lake et al. (2015), which consists of 30 alphabets as the training set and the remaining 20 alphabets as the test set. This is a strong one-shot generalization test, since the model has seen neither the test character nor any alphabets from its family. This is a hard test for our model, since this split provides limited training data, making overfitting\nWe show the model\u2019s performance on the weak generalization test in figure 9, where the first row shows the exemplar image, and the subsequent rows show the variations of that image generated by the model. We show generations for the strong generalization test in figure 10. Our model also generates visually similar and reasonable variations of the image in this case. Unlike the model of Lake et al. (2015), which uses human stroke information and a model structured around the way in which humans draw images, our model is applicable to any image data, with the only domain specific information that is used being that the data is spatially arranged (which is exploited by the convolution and attention). This test also exposes the difficulty that the model has in coping with small amounts of data. We compare the difference between train and test log-likelihoods for the various data splits in figure 5. We see that there is a small gap between the training and test likelihoods in the regime where we have more data (45-5 split) indicating no overfitting. There is a large gap for the other splits, hence a greater tendency for overfitting in the low data regime. An interesting observation is that even for the cases where there is a large gap between train and test ( figure 5 ) likelihood bounds, the examples generated by the model (figure 10, left and middle) still generalize to unseen character classes. Data-efficiency is an important challenge for the large parametric models that we use and one we hope to address in future."}, {"heading": "3. Representative samples from a novel alphabet.", "text": "This task corresponds to figure 7 in Lake et al. (2015), and conditions the model on anywhere between 1 to 10 samples of a novel alphabet and asks the model to generate new characters consistent with this novel alphabet. We show here the hardest form of this test, using only 1 context image. This test is highly subjective, but the model generations in figure 11 show that it is able to pick up common features and use them in the generations.\nWe have emphasized the usefulness of deep generative models as scalable, general-purpose tools for probabilistic reasoning that have the important property of one-shot generalization. But, these models do have limitations. We have already pointed to the need for reasonable amounts of data. Another important consideration is that, while our models can perform one-shot generalization, they do not perform one-shot learning. One-shot learning requires that a model is updated after the presentation of each new input, e.g., like the non-parametric models used by Lake et al. (2015) or Salakhutdinov et al. (2013). Parametric models such as ours require a gradient update of the parameters, which we do not do. Instead, our model performs a type of one-shot inference that during test time can perform inferential tasks on new data points, such as missing data completion, new exemplar generation, or analogical sampling, but does not learn from these points. This distinction between one-shot learning and inference is important and affects how such models can be used. We aim to extend our approach to the online and one-shot learning setting in future."}, {"heading": "30-20 40-10 45-5", "text": ""}, {"heading": "6. Conclusion", "text": "We have developed a new class of general-purpose models that have the ability to perform one-shot generalization, emulating an important characteristic of human cognition. Sequential generative models are natural extensions of variational auto-encoders and provide state-of-the-art models for deep density estimation and image generation. The models specify a sequential process over groups of latent variables that allows it to compute the probability of data points over a number of steps, using the principles of feedback and attention. The use of spatial attention mechanisms substantially improves the ability of the model to generalize. The spatial transformer is a highly flexible attention mechanism for both reading and writing, and is now our default mechanism for attention in generative models. We highlighted the one-shot generalization ability of the model over a range of tasks that showed that the model is able to generate compelling and diverse samples, having seen new examples just once. However there are limitations of this approach, e.g. still needing a reasonable amount of data to avoid overfitting, which we hope to address in future work."}, {"heading": "Acknowledgements", "text": "We thank Brenden Lake and Josh Tenenbaum for insightful discussions. We are grateful to Theophane Weber, Ali Eslami, Peter Battaglia and David Barrett for their valuable feedback."}, {"heading": "A. Additional Results", "text": "A.1. SVHN\nThe SVHN dataset (Netzer et al., 2011) consists of 32\u00d7 32 RGB images from house numbers."}, {"heading": "B. Other types of attention", "text": "Randomized attention. The simplest attention randomly selects patches from the input image, which is the simplest way of implementing a sparse selection mechanism. Applying dropout regularisation to the input layer of deep models would effectively implement this type of attention (a hard attention that has no learning). In data sets like MNIST this attention allows for competitive learning of the generative model if the model is allowed to attend to a large number of patches; see this video https://www.youtube. com/watch?v=W0R394wEUqQ.\nError-based attention. One of the difficulties with attention mechanisms is that for large and sparse images, there can be little gradient information available, which can cause the attentional selection to become stuck. To address this issue, previous approaches have used particle methods (Tang et al., 2014) and exploration techniques from reinforcement learning (Mnih et al., 2014) to infer the latent variables that control the attentional, and allow it to jump more easily to relevant parts of the input. A simple way of realizing this, is to decide where to attend to by jumping o places where the model has made the largest reconstruction errors. To do this, we convert the element-wise reconstruction error at every step into a probability map of locations to attend to at the next iteration:\np(at = k|x, x\u0302t\u22121) \u221d exp ( \u2212\u03b2| k \u2212 \u0304 \u03ba | )\nwhere k = xk \u2212 x\u0302t\u22121,k is the reconstruction error of the kth pixel, x\u0302t\u22121 is the reconstructed image at iteration t\u22121, x is the current target image, \u0304 is the spatial average of k, and \u03ba is the spatial standard deviation of k. This attention is suited to models of sparse images ; see this video https://www.youtube.com/watch?v=qb2-73OHuWA for an example of a\nmodel with this attention mechanism. In this type of hardattention, a policy does not need to be learned, since a new one is obtained after every step based on the reconstruction error and effectively allows every step to work more efficiently towards reducing the reconstruction error. It also overcomes the problem of limited gradient information in large, sparse images, since this form of attention will have a saccadic behaviour since it will be able to jump to any part of the image that has high error.\nMultiple spatial attention. A simple generalization of using a single spatial transformer is to have multiple STs that are additively combined:\ny(v) = K\u2211 i=1 [\u03bah(hi(v))\u2297 \u03baw(hi(v))] \u2217 xi(v),\nwhere v is a context that conditions all STs. This module allows the generative model to write or read at multiple locations simultaneously."}, {"heading": "C. Other model details", "text": "The CGRU of Kaiser & Sutskever (2015) has the following form:\nfc(ct\u22121,ht; \u03b8c) = CGRU(ct\u22121 + fw(ht; \u03b8c)), (12) CGRU(c) = u c + (1\u2212 u) tanh(U \u2217 (r c) + B), u = \u03c3(U\u2032 \u2217 c + B\u2032), r = \u03c3(U\u2032\u2032 \u2217 c + B\u2032\u2032)\nwhere the symbols indicates the element-wise product, \u2217 a size-preserving convolution with stride of 1 \u00d7 1, and \u03c3(\u00b7) is the sigmoid function. The matrices U , U \u2032 and U \u2032\u2032 are 3\u00d7 3 kernels. The number of filters used for the hidden canvas c is specified on section 4."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["Ba", "Jimmy", "Salakhutdinov", "Ruslan R", "Grosse", "Roger B", "Frey", "Brendan J"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Importance weighted autoencoders", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1509.00519,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "What and where: A bayesian inference theory of attention", "author": ["Chikkerur", "Sharat", "Serre", "Thomas", "Tan", "Cheston", "Poggio", "Tomaso"], "venue": "Vision research,", "citeRegEx": "Chikkerur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chikkerur et al\\.", "year": 2010}, {"title": "An analysisby-synthesis approach to multisensory object shape perception", "author": ["G. Erdogan", "I. Yildirim", "R.A. Jacobs"], "venue": "In NIPS,", "citeRegEx": "Erdogan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erdogan et al\\.", "year": 2015}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Jimenez Rezende", "Danilo", "Wierstra", "Daan"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Efficient inference in occlusion-aware generative models of images", "author": ["Huang", "Jonathan", "Murphy", "Kevin"], "venue": "arXiv preprint arXiv:1511.06362,", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Spatial transformer networks", "author": ["Jaderberg", "Max", "Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Advances in Neural Information Processing Systems, pp. 2008\u20132016,", "citeRegEx": "Jaderberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2015}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Neural gpus learn algorithms", "author": ["Kaiser", "\u0141ukasz", "Sutskever", "Ilya"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Generating images from captions with attention", "author": ["Mansimov", "Elman", "Parisotto", "Emilio", "Ba", "Jimmy Lei", "Salakhutdinov", "Ruslan"], "venue": "arXiv preprint arXiv:1511.02793,", "citeRegEx": "Mansimov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mansimov et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Analysis-by-synthesis by learning to invert generative black boxes", "author": ["Nair", "Vinod", "Susskind", "Josh", "Hinton", "Geoffrey E"], "venue": "In Artificial Neural Networks-ICANN", "citeRegEx": "Nair et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2008}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Black box variational inference", "author": ["Ranganath", "Rajesh", "Gerrish", "Sean", "Blei", "David M"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "arXiv preprint arXiv:1505.05770,", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Salakhutdinov", "Ruslan", "Murray", "Iain"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Learning with hierarchical-deep models", "author": ["Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B", "Torralba", "Antonio"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q1958\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 1958}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Saul", "Lawrence K", "Jaakkola", "Tommi", "Jordan", "Michael I"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}, {"title": "Learning generative models with visual attention", "author": ["Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan R"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Vision as bayesian inference: analysis by synthesis", "author": ["Yuille", "Alan", "Kersten", "Daniel"], "venue": "Trends in cognitive sciences,", "citeRegEx": "Yuille et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 2006}, {"title": "SVHN The SVHN dataset (Netzer et al., 2011) consists of 32\u00d7 32 RGB images from house numbers. B. Other types of attention", "author": ["A. Additional Results A"], "venue": null, "citeRegEx": "A.1.,? \\Q2011\\E", "shortCiteRegEx": "A.1.", "year": 2011}], "referenceMentions": [{"referenceID": 21, "context": "Salakhutdinov et al. (2013) developed a probabilistic model that combines a deep Boltzmann machine with a hierarchical Dirichlet process to learn hierarchies of concept categories as well as provide a powerful generative model.", "startOffset": 0, "endOffset": 28}, {"referenceID": 12, "context": "Recently, Lake et al. (2015) presented a compelling demonstration of the ability of probabilistic models to perform one-shot generalization, using Bayesian program learning, which is able to learn a hierarchical, non-parametric generative model of handwritten characters.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "Recently, Lake et al. (2015) presented a compelling demonstration of the ability of probabilistic models to perform one-shot generalization, using Bayesian program learning, which is able to learn a hierarchical, non-parametric generative model of handwritten characters. Their approach incorporates specific knowledge of how strokes are formed and the ways in which they are combined to produce characters of different types, exploiting similar strategies used by humans to perform this task. Lake et al. (2015) see the capacity for one-shot generalization demonstrated by Bayesian programming learning \u2018as a challenge for neural models\u2019.", "startOffset": 10, "endOffset": 513}, {"referenceID": 24, "context": "Models that are directed graphical models have recently become popular in machine learning and include discrete latent variable models such as sigmoid belief networks and deep auto-regressive networks (Saul et al., 1996; Gregor et al., 2014), or continuous latent variable models such as non-linear Gaussian belief networks and deep latent Gausar X iv :1 60 3.", "startOffset": 201, "endOffset": 241}, {"referenceID": 4, "context": "Models that are directed graphical models have recently become popular in machine learning and include discrete latent variable models such as sigmoid belief networks and deep auto-regressive networks (Saul et al., 1996; Gregor et al., 2014), or continuous latent variable models such as non-linear Gaussian belief networks and deep latent Gausar X iv :1 60 3.", "startOffset": 201, "endOffset": 241}, {"referenceID": 20, "context": "sian models (Rezende et al., 2014; Kingma & Welling, 2014).", "startOffset": 12, "endOffset": 58}, {"referenceID": 3, "context": "These principles allow the models we develop to reflect the principles of analysis-by-synthesis, in which the analysis of observed information is continually integrated with constructed interpretations of it (Yuille & Kersten, 2006; Erdogan et al., 2015; Nair et al., 2008).", "startOffset": 208, "endOffset": 273}, {"referenceID": 17, "context": "These principles allow the models we develop to reflect the principles of analysis-by-synthesis, in which the analysis of observed information is continually integrated with constructed interpretations of it (Yuille & Kersten, 2006; Erdogan et al., 2015; Nair et al., 2008).", "startOffset": 208, "endOffset": 273}, {"referenceID": 5, "context": "Models such as DRAW (Gregor et al., 2015) and composited variational auto-encoders (Huang & Murphy, 2015) are existing models in this class, and we will develop a general class of sequential generative models that incorporates these and other latent variable models and variational auto-encoders.", "startOffset": 20, "endOffset": 41}, {"referenceID": 2, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 26, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 8, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 16, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 0, "context": "Attention has been most widely used for classification tasks, having been shown to improve both scalability and generalization (Larochelle & Hinton, 2010; Chikkerur et al., 2010; Xu et al., 2015; Jaderberg et al., 2015; Mnih et al., 2014; Ba et al., 2015).", "startOffset": 127, "endOffset": 255}, {"referenceID": 25, "context": "Attention in unsupervised learning is much more recent (Tang et al., 2014; Gregor et al., 2015).", "startOffset": 55, "endOffset": 95}, {"referenceID": 5, "context": "Attention in unsupervised learning is much more recent (Tang et al., 2014; Gregor et al., 2015).", "startOffset": 55, "endOffset": 95}, {"referenceID": 8, "context": "Spatial transformers (Jaderberg et al., 2015) are a more general method for providing such invariance, and is our preferred attentional mechanism.", "startOffset": 21, "endOffset": 45}, {"referenceID": 24, "context": "Tang et al. (2014) take such an approach and use 2D similarity transforms to provide basic affine invariance.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "In more complex models, the probabilistic description consists of a hierarchy of L layers of latent variables, where each layer depends on the layer above in a non-linear way (Rezende et al., 2014) \u2014for deep generative models, we specify this non-linear dependency using deep neural networks.", "startOffset": 175, "endOffset": 197}, {"referenceID": 9, "context": "One popular approximation technique is based on variational inference (Jordan et al., 1999), which transforms the difficult integration into an optimization problem that is typically more scalable and easier to solve.", "startOffset": 70, "endOffset": 91}, {"referenceID": 20, "context": "VAEs allow for a single computational graph to be constructed and straightforward gradient computations: when the latent variables are continuous, gradient estimators based on pathwise derivative estimators are used (Rezende et al., 2014; Kingma & Welling, 2014; Burda et al., 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al.", "startOffset": 216, "endOffset": 282}, {"referenceID": 1, "context": "VAEs allow for a single computational graph to be constructed and straightforward gradient computations: when the latent variables are continuous, gradient estimators based on pathwise derivative estimators are used (Rezende et al., 2014; Kingma & Welling, 2014; Burda et al., 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al.", "startOffset": 216, "endOffset": 282}, {"referenceID": 19, "context": ", 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al., 2014; Mansimov et al., 2015).", "startOffset": 71, "endOffset": 139}, {"referenceID": 14, "context": ", 2015) and when they are discrete, score function estimators are used (Mnih & Gregor, 2014; Ranganath et al., 2014; Mansimov et al., 2015).", "startOffset": 71, "endOffset": 139}, {"referenceID": 5, "context": "The above sequential generative model and inference is a generalization of existing models such as DRAW (Gregor et al., 2015) and composited VAEs (Huang & Murphy, 2015).", "startOffset": 104, "endOffset": 125}, {"referenceID": 8, "context": "We use a much more powerful and general attention mechanism based on spatial transformers (Jaderberg et al., 2015; Huang & Murphy, 2015).", "startOffset": 90, "endOffset": 136}, {"referenceID": 4, "context": "Gregor et al. (2015) use a generative attention based on Gaussian convolutions parameterized by a location and scale, and Tang et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Gregor et al. (2015) use a generative attention based on Gaussian convolutions parameterized by a location and scale, and Tang et al. (2014) use 2D similarity transformations.", "startOffset": 0, "endOffset": 141}, {"referenceID": 16, "context": "We form a multi-MNIST data set of 64 \u00d7 64 images that consists of two MNIST digits placed at random locations in the image (having adapted the cluttered MNIST generator from Mnih et al. (2014) to procedurally generate the data).", "startOffset": 174, "endOffset": 193}, {"referenceID": 3, "context": "From Gregor et al. (2015) and Burda et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 1, "context": "(2015) and Burda et al. (2015)", "startOffset": 11, "endOffset": 31}, {"referenceID": 12, "context": "Unlike MNIST, which has a small number of classes with many images of each class and a large amount of data, the omniglot data set (Lake et al., 2015) consists of 105\u00d7 105 binary images across 1628 classes with just 20 images per class.", "startOffset": 131, "endOffset": 150}, {"referenceID": 1, "context": "One data set used by Burda et al. (2015) consists of 28\u00d728 \u25cf", "startOffset": 21, "endOffset": 41}, {"referenceID": 11, "context": "images, but is different to that of Lake et al. (2015). We compare the available methods on the dataset from Burda et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 1, "context": "We compare the available methods on the dataset from Burda et al. (2015) in table 3 and find that the sequential models perform better than all competing approaches, further establishing the effectiveness of these models.", "startOffset": 53, "endOffset": 73}, {"referenceID": 1, "context": "We compare the available methods on the dataset from Burda et al. (2015) in table 3 and find that the sequential models perform better than all competing approaches, further establishing the effectiveness of these models. Our second evaluation uses the dataset of Lake et al. (2015), which we downsampled to 52 \u00d7 52 using a 2 \u00d7 2 max-pooling.", "startOffset": 53, "endOffset": 283}, {"referenceID": 1, "context": "From Burda et al. (2015)", "startOffset": 5, "endOffset": 25}, {"referenceID": 12, "context": "This task corresponds to figure 5 in Lake et al. (2015)).", "startOffset": 37, "endOffset": 56}, {"referenceID": 12, "context": "b) We use exactly the data split used by Lake et al. (2015), which consists of 30 alphabets as the training set and the remaining 20 alphabets as the test set.", "startOffset": 41, "endOffset": 60}, {"referenceID": 12, "context": "Unlike the model of Lake et al. (2015), which uses human stroke information and a model structured around the way in which humans draw images, our model is applicable to any image data, with the only domain specific information that is used being that the data is spatially arranged (which is exploited by the convolution and attention).", "startOffset": 20, "endOffset": 39}, {"referenceID": 12, "context": "This task corresponds to figure 7 in Lake et al. (2015), and conditions the model on anywhere between 1 to 10 samples of a novel alphabet and asks the model to generate new characters consistent with this novel alphabet.", "startOffset": 37, "endOffset": 56}, {"referenceID": 12, "context": ", like the non-parametric models used by Lake et al. (2015) or Salakhutdinov et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 12, "context": ", like the non-parametric models used by Lake et al. (2015) or Salakhutdinov et al. (2013). Parametric models such as ours require a gradient update of the parameters, which we do not do.", "startOffset": 41, "endOffset": 91}], "year": 2016, "abstractText": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples\u2014 having seen new examples just once\u2014providing an important class of general-purpose models for one-shot machine learning.", "creator": "LaTeX with hyperref package"}}}