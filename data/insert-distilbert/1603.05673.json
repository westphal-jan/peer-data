{"id": "1603.05673", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Predicting health inspection results from online restaurant reviews", "abstract": "informatics around ethical public health are increasingly shifting from the professional to the public spheres. in this work, we apply linguistic analytics to restaurant reviews, from yelp, in attempting order to automatically uniformly predict official health healthcare inspection reports. we consider two types of feature sets, i. e., keyword detection and topic model features, and use these latter in several classification methods. our recent empirical semantic analysis shows that these extracted features trees can predict improved public health inspection reports with over 90 % accuracy using simple support vector machines.", "histories": [["v1", "Thu, 17 Mar 2016 20:20:32 GMT  (348kb,D)", "http://arxiv.org/abs/1603.05673v1", "7 pages, 2 figures, 2 tables"]], "COMMENTS": "7 pages, 2 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["samantha wong", "hamidreza chinaei", "frank rudzicz"], "accepted": false, "id": "1603.05673"}, "pdf": {"name": "1603.05673.pdf", "metadata": {"source": "CRF", "title": "Predicting health inspection results from online restaurant reviews", "authors": ["Samantha Wong", "Hamidreza Chinaei", "Frank Rudzicz", "Raymond Chang"], "emails": ["samantha.y.wong@ryerson.ca,", "hrchinaei@ryerson.ca", "frank@cs.toronto.edu"], "sections": [{"heading": "1 Introduction", "text": "Typically, official Health and Safety inspections of restaurants are only conducted one to three times a year, and involve fixed checklists related to food safety3. Meanwhile, consumer reviews of these restaurants are posted publicly on sites like Yelp and TripAdvisor much more frequently. Naturally, this leads to the question of whether restaurant reviews can support the public health inspection process by flagging potentially high risk restaurants that may need more frequent inspections.\nTo address this question, we compare official restaurant health inspection results from the Kitchener-Waterloo region against text comments found in Yelp reviews of the relevant restaurants. In particular, we extract series of keywords and topic model features and apply these to a few classification methods. Our extracted features can predict the public health inspection with 90% accuracy when used with a support vector machine (SVM)."}, {"heading": "1.1 Related Work", "text": "Kang et al. [2013] matched keywords and phrases (unigram and bigram) from text restaurant reviews and other Yelp metadata for to results from the Department of Health\u2019s research results, with the goal of allocating inspectors more\n3 http://chd.region.waterloo.on.ca/en/healthylivinghealthprotection/\nresources/sample_restaurant_inspection.pdf\nar X\niv :1\n60 3.\n05 67\n3v 1\n[ cs\n.C L\n] 1\n7 M\nefficiently. That study used liblinear\u2019s SVM with L1 regularization and 10-fold cross-validation, which was able to achieve 82.68% accuracy when classifying text reviews with problematic restaurants. Kang et al. suggested that the highest accuracy can be achieved using only contentful unigrams and bigrams. They also presented cue features for different topics/categories such as Hygienic (e.g., gross, mess, sticky, smell, dirty), Cuisines (Vietnamese, Dim Sum, Thai), and Sentiment (cheap, never). Our work extends theirs in a few ways, including our use of topic modelling.\nBlei et al. [2003] proposed a Bayesian topic modeling approach called latent dirichlet allocation (LDA) which is used for discovering hidden topics of documents. In this model, a document can be represented as a mixture of the learned hidden topics, where each hidden topic is represented by a distribution over words occurred in the document. The authors also applied their LDA method to text classification on Reuters-21578 dataset. They trained a SVM on the topic model representations applied by LDA and compared this SVM to an SVM trained on all the word features. They concluded that the former SVM can potentially produce better accuracy results. In this work, we use both topic model features and keyword features in an SVM.\nSahami et al. [1998] studied junk mail filtering using na\u0308\u0131ve Bayes and message text, along with several other non-textual features. They also described building a corpus of keywords from the text of junk emails, and how some of those keywords were removed as part of feature selection based loosely on Zipf\u2019s law. They were able to achieve 97.1% precision and 94.3% recall for classifying junk mail. In our work, we use also na\u0308\u0131ve Bayes and text reviews, but for a different purpose.\nNsoesie et al. [2014] addressed the spread of food-borne illness through Yelp reviews using keyword selection (according to symptoms) and clustering to predict the geographical location where a food-borne outbreak originated. They also highlighted limitations in using Yelp data to support health-related studies, specifically with regards to review bias, and issues related to timing. Nonetheless, Nsoesie et al. confirmed that it is possible to crowd-source food and health reports, given careful analysis.\nOther work has applied analysis of textual signals in social media to public health surveillance more generally, especially with regards to the spread of disease [Aramaki et al., 2011; Sadilek et al., 2012, 2013; Lamb et al., 2013; Dredze et al., 2013; Von Etter et al., 2010]. The increase of recent work in this area suggests at a shift towards \u2018crowd-sourced\u2019 public health informatics."}, {"heading": "2 Data", "text": "In our study, we use Yelp4 and Waterloo Public Health (WPH)5 data. We extract facility information from WPH, as well as official inspection results (date, type, and recommended actions). From Yelp, we extract business information (matched with WPH facility information) and tokenize text reviews. These tokens are used in a document/term matrix, and act as features in this work. The other set of features are learned topic models, i.e., the distribution of topics for each learned topic. We use binary prediction labels: \u201caction\u201d indicating noncompliance or other major problem with the restaurant, and \u201cno action\u201d if the restaurant passes health inspection."}, {"heading": "3 Approach", "text": "Figure 2 summarizes our approach, summarized here.\nImport row data First, we filter business data to those in the Kitchener-WaterlooCambridge area. We then extract the data for the region from WPH. Finally, we merge the two datasets, resulting in a list of 126 restaurants, their Yelp Business\n4 https://www.yelp.ca/dataset_challenge/dataset 5 http://www.regionofwaterloo.ca/en/regionalGovernment/\nFoodPremiseDataset.asp\nIDs, and WPH Facility IDs. This allows us to merge WPH health inspection results and textual Yelp reviews. A preliminary qualitative analysis of documents labeled with \u201caction\u201d suggested that relatively few of the associated reviews included negative terms (e.g., dirty, sick, bad). Interestingly, many of those reviews focused on poor service, food temperature, or the restaurant environment generally, rather than direct references to public health issues.\nBuild a document-term matrix (DTM) We build a tf-idf weighted documentterm matrix using all text reviews. First, we create a regular document-term matrix with plain text (stripped of whitespaces, punctuation, stopwords, and stems). Then, we weigh each term with tf-idf. Specifically, we calculate term frequencies for each word using a function based on:\nTF(t,d) = Number of times term t appears in a document d\nTotal number of terms in document d .\nWe then calculate the inverse document frequencies using:\nIDF(t,D) = log\n( Total number of documents in set D\nNumber of documents in D containing term t\n) .\nFinally, the overall score for term-document pair (t, d) is TF (t, d) \u00b7 IDF (t,D). We use the built-in functionality of the tm package in R6, which includes further optimizations for tf-idf, and we use the resulting matrix for all of our calculations below.\nApplying na\u0308\u0131ve Bayes (NB) classification to the DTM We apply NB to the full DTM created above but obtained poor results. NB had difficulty classifying \u201cNo Action\u201d reviews, with only 11.78% accuracy and 10% sensitivity.\nTherefore, we reduce the number of features in the DTM by using only the N most frequent few contentful terms, identified by sums of the weighted values for each column of the DTM. Optimizing this process gives N = 300, with which we achieve an accuracy of 47.13% and a sensitivity of 13.22%, which is still sub-optimal.\nIn an effort to improve the results, we add latent Dirichlet analysis (LDA) [Blei et al., 2003] to perform topic modelling, and incorporate these topics into a SVM classifier.\nBuild a topic model using LDA First, we apply term frequency weighting to the original document term matrix (as opposed to tf-idf), prior to LDA. We empirically select 20 topics and, similar to previous work [Gruber et al., 2007], we set the Dirichlet prior on the per-document topic distributions to \u03b1 = 1+50/k (where k is the number of topics), giving \u03b1 = 3.5. Topics appeared to be mostly divided by the type of cuisine (e.g., Chinese, wings, burgers, tea/coffee, breakfast).\nWe then extract the posterior topic distribution for each document, along with the document term matrix, to perform SVM classification. Table 1 shows a sample of learned topics.\n6 http://www.inside-r.org/packages/cran/tm/docs/weightTfIdf\nCombining DTM and LDA topics in SVM classification As discussed above, we create the DTM with TF weighting, and bind the LDA posterior topic distributions, as well as document classes (\u2018Action\u2019 or \u2018No Action\u2019). As before, we optimize the number of features from the DTM within an SVM classifier. To avoid issues of imbalanced classes, we use SMOTE [Chawla et al., 2002] to oversample the minority class, resulting in approximately 900 samples for each class.\nThe performance of different methods, in a 10-fold cross-fold validation, are shown in Table 2."}, {"heading": "4 Conclusion", "text": "Textual analysis of Yelp reviews can clearly be used to predict official health inspection results, as evidenced by over 90% accuracy in predicting official WPH actions using learned keywords and topic model features in an SVM classifier. In both the NB and SVM approaches, reducing the total number of keywords to an optimized subset provides a significant improvement in performance, most likely to possible overfitting in the full set.\nGiven that both sensitivity and sensitivity are very high in our optimum system, it is strongly suggested that this tool, or one very much like it, can be used by governments to optimize their resources. Specifically, inspection schedules should be automatically generated from crowd-sourced data, as in this work, and standard regulated procedures. The derivation of such schedules is the subject of future work. Other possible avenues of exploration include interactive online databases combining official health inspector reports and crowd-sourced information. Artificial intelligence is helping to shift public health informatics to the public sphere, and it will be increasingly important for that shift to incorporate new approaches to machine learning."}], "references": [{"title": "Twitter catches the flu", "author": ["E. Aramaki", "S. Maskawa", "M. Morita"], "venue": null, "citeRegEx": "Aramaki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aramaki et al\\.", "year": 2011}, {"title": "Carmen: A Twitter", "author": ["M. Dredze", "M.J. Paul", "S. Bergsma", "H. Tran"], "venue": "Intelligence Research,", "citeRegEx": "Dredze et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2013}, {"title": "Separating fact from fear", "author": ["A. Lamb", "M.J. Paul", "M. Dredze"], "venue": "Journal of Statistical Software,", "citeRegEx": "Lamb et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2013}, {"title": "Online reports", "author": ["Atlanta", "Georgia", "E.O. USA. Nsoesie", "S.A. Kluberg", "J.S. Brownstein"], "venue": null, "citeRegEx": "Atlanta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Atlanta et al\\.", "year": 2014}, {"title": "Modeling spread of disease", "author": ["CA", "A. USA. Sadilek", "H.A. Kautz", "V. Silenzio"], "venue": null, "citeRegEx": "CA et al\\.,? \\Q2012\\E", "shortCiteRegEx": "CA et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Other work has applied analysis of textual signals in social media to public health surveillance more generally, especially with regards to the spread of disease [Aramaki et al., 2011; Sadilek et al., 2012, 2013; Lamb et al., 2013; Dredze et al., 2013; Von Etter et al., 2010].", "startOffset": 162, "endOffset": 276}, {"referenceID": 2, "context": "Other work has applied analysis of textual signals in social media to public health surveillance more generally, especially with regards to the spread of disease [Aramaki et al., 2011; Sadilek et al., 2012, 2013; Lamb et al., 2013; Dredze et al., 2013; Von Etter et al., 2010].", "startOffset": 162, "endOffset": 276}, {"referenceID": 1, "context": "Other work has applied analysis of textual signals in social media to public health surveillance more generally, especially with regards to the spread of disease [Aramaki et al., 2011; Sadilek et al., 2012, 2013; Lamb et al., 2013; Dredze et al., 2013; Von Etter et al., 2010].", "startOffset": 162, "endOffset": 276}], "year": 2016, "abstractText": "Informatics around public health are increasingly shifting from the professional to the public spheres. In this work, we apply linguistic analytics to restaurant reviews, from Yelp, in order to automatically predict official health inspection reports. We consider two types of feature sets, i.e., keyword detection and topic model features, and use these in several classification methods. Our empirical analysis shows that these extracted features can predict public health inspection reports with over 90% accuracy using simple support vector machines.", "creator": "LaTeX with hyperref package"}}}