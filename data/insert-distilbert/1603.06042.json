{"id": "1603.06042", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Globally Normalized Transition-Based Neural Networks", "abstract": "we introduce a globally normalized transition - method based neural network modelling model that achieves state - of - the - art part - end of - speech tagging, dependency parsing and sentence compression results. our model is a simple feed - forward neural network function that operates on a task - specific transition system, yet achieves comparable variance or better accuracies than recurrent models. the key insight is directly based on a novel proof illustrating the product label bias problem and algorithms showing that universal globally normalized mapping models can be strictly more expressive than locally normalized models.", "histories": [["v1", "Sat, 19 Mar 2016 03:56:03 GMT  (38kb)", "http://arxiv.org/abs/1603.06042v1", null], ["v2", "Wed, 8 Jun 2016 13:43:30 GMT  (39kb)", "http://arxiv.org/abs/1603.06042v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["daniel andor", "chris alberti", "david weiss", "aliaksei severyn", "alessandro presta", "kuzman ganchev", "slav petrov", "michael collins"], "accepted": true, "id": "1603.06042"}, "pdf": {"name": "1603.06042.pdf", "metadata": {"source": "CRF", "title": "Globally Normalized Transition-Based Neural Networks", "authors": ["Daniel Andor", "Chris Alberti", "David Weiss"], "emails": ["andor@google.com", "chrisalberti@google.com", "djweiss@google.com", "severyn@google.com", "apresta@google.com", "kuzman@google.com", "slav@google.com", "mjcollins@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 04\n2v 1\n[ cs\n.C L\n] 1\n9 M\nar 2\n01 6"}, {"heading": "1 Introduction", "text": "Neural network approaches have taken the field of natural language processing (NLP) by storm. In particular, variants of long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have produced impressive results on some of the classic NLP tasks such as part-of-speech tagging (Ling et al., 2015), syntactic parsing (Vinyals et al., 2015) and semantic role labeling (Zhou and Xu, 2015). One might speculate that it is the recurrent nature of these models that enables these results.\nIn this work we demonstrate that simple feed-forward networks without any recurrence can achieve comparable or better accuracies than LSTMs, as long as they are globally normalized. Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and intro-\nduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001) to overcome the label bias problem that locally normalized models suffer from. Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015). We compute gradients based on this approximate global normalization and perform full backpropagation training of all neural network parameters based on the CRF loss.\nWe revisit the label bias problem in Section 3 and provide a novel proof that globally normalized models are strictly more expressive than locally normalized models. Lookahead features can partially mitigate this discrepancy, but cannot fully compensate for it\u2014a point to which we return later. To empirically demonstrate the effectiveness of global normalization, we evaluate our model on part-of-speech tagging, syntactic dependency parsing and sentence compression (Section 4). Our model achieves state-of-the-art accuracy on all of these tasks, matching or outperforming LSTMs while being significantly faster. In particular for dependency parsing on the Wall Street Journal we achieve the best-ever published unlabeled attachment score of 94.41%.\nAs discussed in more detail in Section 5, we also outperform previous structured training approaches used for neural network transitionbased parsing. Our ablation experiments show that we outperform Weiss et al. (2015) and Alberti et al. (2015) because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model. We also outperform Zhou et al. (2015) despite using a smaller beam. To shed additional light on the label bias problem in practice, we provide a sentence\ncompression example where the local model completely fails. We then demonstrate that a globally normalized parsing model without any lookahead features is almost as accurate as our best model, while a locally normalized model loses more than 10% absolute in accuracy because it cannot effectively incorporate evidence as it becomes available."}, {"heading": "2 Model", "text": "At its core, our model is an incremental transitionbased parser (Nivre, 2006). To apply it to different tasks we only need to adjust the transition system and the input features."}, {"heading": "2.1 Transition System", "text": "Given an input x, most often a sentence, we define:\n\u2022 A set of states S . \u2022 A special start state s\u2020 \u2208 S . \u2022 A set of allowed decisions A(s) for all s \u2208 S . \u2022 A transition function t(s, d) returning a new\nstate s\u2032 for any decision d \u2208 A(s).\nWe drop the dependence on x for brevity. We will use a function \u03c1(s, d; \u03b8) to compute the score of decision d in state s. The vector \u03b8 contains the model parameters and we assume that \u03c1(s, d; \u03b8) is differentiable with respect to \u03b8.\nThroughout this work we will use transition systems in which all complete structures for the same input x have the same number of decisions n(x) (or n for brevity). In dependency parsing for example, this is true for both the arc-standard and arc-eager transition systems (Nivre, 2006), where for a sentence x of length m, the number of decisions for any complete parse is n(x) = 2 \u00d7 m.1 A complete structure is then a sequence of decision/state pairs (s1, d1) . . . (sn, dn) such that s1 = s\u2020, di \u2208 S(si) for i = 1 . . . n, and si+1 = t(si, di). We use the notation d1:j to refer to a decision sequence d1 . . . dj .\nWe assume that there is a one-to-one mapping between decision sequences d1:j and states sj : that is, we essentially assume that a state encodes the entire history of decisions. Thus, each state can be reached by a unique decision sequence from s\u2020.2 We will use decision sequences d1:j and states interchangeably: in a slight abuse of notation, we\n1Note that this is not true for the swap transition system defined in Nivre (2009).\n2It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences.\ndefine \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j .\nThe scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al. (2015), we define it via a feedforward neural network as\n\u03c1(s, d; \u03b8) = \u03c6(s; \u03b8(l)) \u00b7 \u03b8(d).\nHere \u03b8(l) are the parameters of the neural network, excluding the parameters at the final layer. \u03b8(d) are the final layer parameters for decision d. \u03c6(s; \u03b8(l)) is the representation for state s computed by the neural network under parameters \u03b8(l). Note that the score is linear in the parameters \u03b8(d). We next describe how softmax-style normalization can be performed at the local or global level."}, {"heading": "2.2 Global vs. Local Normalization", "text": "In the Chen and Manning (2014) style of greedy neural network parsing, the conditional probability distribution over decisions dj given context d1:j\u22121 is defined as\np(dj |d1:j\u22121; \u03b8) = exp \u03c1(d1:j\u22121, dj ; \u03b8)\nZL(d1:j\u22121; \u03b8) , (1)\nwhere\nZL(d1:j\u22121; \u03b8) = \u2211\nd\u2032\u2208A(d1:j\u22121)\nexp \u03c1(d1:j\u22121, d \u2032; \u03b8).\nEach ZL(d1:j\u22121; \u03b8) is a local normalization term. The probability of a sequence of decisions d1:n is\npL(d1:n) = n\u220f\nj=1\np(dj|d1:j\u22121; \u03b8)\n= exp \u2211n j=1 \u03c1(d1:j\u22121, dj ; \u03b8)\u220fn\nj=1 ZL(d1:j\u22121; \u03b8) . (2)\nBeam search can be used to attempt to find the maximum of (2) with respect to d1:n.\nIn contrast, a Conditional Random Field (CRF) defines a distribution pG(d1:n) as follows:\npG(d1:n) = exp\n\u2211n j=1 \u03c1(d1:j\u22121, dj ; \u03b8)\nZG(\u03b8) , (3)\nwhere\nZG(\u03b8) = \u2211\nd\u2032 1:n\u2208Dn\nexp\nn\u2211\nj=1\n\u03c1(d\u20321:j\u22121, d \u2032 j ; \u03b8)\nand Dn is the set of all valid sequences of decisions of length n. ZG(\u03b8) is a global normalization term. The inference problem is now to find\nargmax d1:n\u2208Dn pG(d1:n) = argmax d1:n\u2208Dn\nn\u2211\nj=1\n\u03c1(d1:j\u22121, dj ; \u03b8).\nBeam search can again be used to approximately find the argmax."}, {"heading": "2.3 Training", "text": "Training data consists of inputs x paired with gold decision sequences d\u22171:n. We use stochastic gradient descent on the negative log-likelihood of the data under the model. Under a locally normalized model, the negative log-likelihood is\nLlocal(d \u2217 1:n; \u03b8) = \u2212 ln pL(d \u2217 1:n; \u03b8) = (4)\n\u2212\nn\u2211\nj=1\n\u03c1(d\u22171:j\u22121, d \u2217 j ; \u03b8) +\nn\u2211\nj=1\nlnZL(d \u2217 1:j\u22121; \u03b8),\nwhereas under a globally normalized model it is\nLglobal(d \u2217 1:n; \u03b8) = \u2212 ln pG(d \u2217 1:n; \u03b8) =\n\u2212\nn\u2211\nj=1\n\u03c1(d\u22171:j\u22121, d \u2217 j ; \u03b8) + lnZG(\u03b8). (5)\nA significant practical advantange of the locally normalized cost (4) is that it factorizes into n independent terms, each of which can be computed exactly and minimized separately. By contrast, the ZG term in (5) contains a sum over d\u20321:n \u2208 Dn that is in many cases intractable.\nTo make learning tractable with the globally normalized model, we use beam search and early updates (Collins and Roark, 2004; Zhou et al., 2015). As the training sequence is being decoded, we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step j, a stochastic gradient step is taken on the following objective:\nLglobal\u2212beam(d \u2217 1:j ; \u03b8) =\n\u2212\nj\u2211\ni=1\n\u03c1(d\u22171:i\u22121, d \u2217 i ; \u03b8) + ln\n\u2211\nd\u2032 1:j\u2208Bj\nexp\nj\u2211\ni=1\n\u03c1(d\u20321:i\u22121, d \u2032 i; \u03b8).(6)\nHere the set Bj contains all paths in the beam at step j, together with the gold path prefix d\u22171:j . It is straightforward to derive gradients of the loss in (6) and to back-propagate gradients to all levels of a neural network defining the score \u03c1(s, d; \u03b8). If the gold path remains in the beam throughout decoding, a gradient step is performed using Bn, the beam at the end of decoding."}, {"heading": "3 The Label Bias Problem", "text": "Intuitively, we would like the model to be able to revise an earlier decision made during search, when later evidence becomes available that rules out the earlier decision as incorrect. At first glance, it might appear that a locally normalized model used in conjunction with beam search or exact search is able to revise earlier decisions. However the label bias problem (see Lafferty et al. (2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.\nThis section gives a more formal perspective on the label bias problem than in previous work, through a proof that globally normalized models are strictly more expressive than locally normalized models. The proof makes use of an example that gives an illustration of the label bias problem.\nGlobal Models can be Strictly More Expressive than Local Models Consider a tagging problem where the task is to map an input sequence x1:n to a decision sequence d1:n. First, consider a locally normalized model where we restrict the scoring function to access only the first i input symbols x1:i when scoring decision di. We will return to this restriction soon. The scoring function \u03c1 can be an otherwise arbitrary function of the tuple \u3008d1:i\u22121, di, x1:i\u3009:\npL(d1:n|x1:n) =\nn\u220f\ni=1\npL(di|d1:i\u22121, x1:i)\n= exp \u2211n i=1 \u03c1(d1:i\u22121, di, x1:i)\u220fn\ni=1 ZL(d1:i\u22121, x1:i) .\nSecond, consider a globally normalized model\npG(d1:n|x1:n) = exp\n\u2211n i=1 \u03c1(d1:i\u22121, di, x1:i)\nZG(x1:n) .\nThis model again makes use of a scoring function \u03c1(d1:i\u22121, di, x1:i) restricted to the first i input symbols when scoring decision di.\nDefine PL to be the set of all possible distributions pL(d1:n|x1:n) under the local model obtained as the scores \u03c1 vary. Similarly, define PG to be the set of all possible distributions pG(d1:n|x1:n) under the global model. Here a \u201cdistribution\u201d is a function from a pair (x1:n, d1:n) to a probability p(d1:n|x1:n). Our main result is the following:\nTheorem 3.1 PL is a strict subset of PG, that is PL ( PG.\nTo prove this we will first prove that PL \u2286 PG. This step is straightforward. We then show that PG * PL; that is, there are distributions in PG that are not in PL. The proof that PG * PL gives a clear illustration of the label bias problem.\nProof that PL \u2286 PG: We need to show that for any locally normalized distribution pL, we can construct a globally normalized model pG such that pG = pL. Consider a locally normalized model with scores \u03c1(d1:i\u22121, di, x1:i). Define a global model pG with scores\n\u03c1\u2032(d1:i\u22121, di, x1:i) = log pL(di|d1:i\u22121, x1:i).\nThen it is easily verified that\npG(d1:n|x1:n) = pL(d1:n|x1:n)\nfor all x1:n, d1:n. In proving PG * PL we will use a simple problem where every example seen in training or test data is one of the following two tagged sentences:\nx1x2x3 = a b c, d1d2d3 = A B C\nx1x2x3 = a b e, d1d2d3 = A D E (7)\nNote that the input x2 = b is ambiguous: it can take tags B or D. This ambiguity is resolved when the next input symbol, c or e, is observed.\nNow consider a globally normalized model, where the scores \u03c1(d1:i\u22121, di, x1:i) are defined as follows. Define T as the set {(A,B), (B,C), (A,D), (D,E)} of bigram tag transitions seen in the data. Similarly, define E as the set {(a,A), (b,B), (c, C), (b,D), (e,E)} of (word, tag) pairs seen in the data. We define\n\u03c1(d1:i\u22121, di, x1:i) (8)\n= \u03b1\u00d7 J(di\u22121, di) \u2208 T K + \u03b1\u00d7 J(xi, di) \u2208 EK\nwhere \u03b1 is the single scalar parameter of the model, and J\u03c0K = 1 if \u03c0 is true, 0 otherwise.\nProof that PG * PL: We will construct a globally normalized model pG such that there is no locally normalized model such that pL = pG.\nUnder the definition in (8), it is straightforward to show that\nlim \u03b1\u2192\u221e pG(A B C|a b c) = lim \u03b1\u2192\u221e pG(A D E|a b e) = 1.\nIn contrast, under any definition for \u03c1(d1:i\u22121, di, x1:i), we must have\npL(A B C|a b c) + pL(A D E|a b e) \u2264 1 (9)\nThis follows because pL(A B C|a b c) = pL(A|a) \u00d7 pL(B|A, a b) \u00d7 pL(C|A B, a b c) and pL(A D E|a b e) = pL(A|a) \u00d7 pL(D|A, a b) \u00d7 pL(E|A D, a b e). The inequality pL(B|A, a b) + pL(D|A, a b) \u2264 1 then immediately implies (9).\nIt follows that for sufficiently large values of \u03b1, we have pG(A B C|a b c)+ pG(A D E|a b e) > 1, and given (9) it is impossible to define a locally normalized model with pL(A B C|a b c) = pG(A B C|a b c) and pL(A D E|a b e) = pG(A D E|a b e).\nUnder the restriction that scores \u03c1(d1:i\u22121, di, x1:i) depend only on the first i input symbols, the globally normalized model is still able to model the data in (7), while the locally normalized model fails (see Eq. 9). The ambiguity at input symbol b is naturally resolved when the next symbol (c or e) is observed, but the locally normalized model is not able to revise its prediction.\nIt is easy to fix the locally normalized model for the example in (7) by allowing scores \u03c1(d1:i\u22121, di, x1:i+1) that take into account the input symbol xi+1. Such lookahead is common in practice, but insufficient in general. For every amount of lookahead k, we can construct examples that cannot be modeled with a locally normalized model by duplicating the middle input b in (7) k + 1 times. Only a local model with scores \u03c1(d1:i\u22121, di, x1:n) that considers the entire input can capture any distribution p(d1:n|x1:n): in this case the decomposition pL(d1:n|x1:n) =\u220fn\ni=1 pL(di|d1:i\u22121, x1:n) makes no independence assumptions.\nHowever, increasing the amount of context used as input comes at a cost, requiring more powerful learning algorithms, and potentially more training data. For a detailed analysis of the tradeoffs between structural features in CRFs and more powerful local classifiers without structural constraints, see Liang et al. (2008); in these experiments local classifiers are unable to reach the performance of CRFs on problems such as parsing and named entity recognition where structural constraints are important. Note that there is nothing to preclude an approach that makes use of both global normalization and more powerful scoring functions \u03c1(d1:i\u22121, di, x1:n), obtaining the best of both worlds. The experiments that follow make use of both."}, {"heading": "4 Experiments", "text": "To demonstrate the flexibility and modeling power of our approach, we provide experimental results on a diverse set of structured prediction tasks. We first direct our attention to POS tagging, then to syntactic dependency parsing and finally to sentence compression.\nWhile directly optimizing the global model (5) works well, we found that training the model in two steps achieves the same precision much faster: we first pretrain the network using the local objective (4), and then perform additional training steps using the global objective (6). We pretrain all layers except the softmax layer in this way. We purposefully abstain from complicated hand engineering of input features, which might improve performance further (Durrett and Klein, 2015)."}, {"heading": "4.1 Part of Speech Tagging", "text": "Part of speech (POS) tagging is a classic NLP task, where modeling the structure of the output is important for achieving state-of-the-art performance.\nData & Evaluation. We conducted experiments on a number of different datasets: (1) English Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006) with identical setup to Weiss et al. (2015); and (3) CoNLL \u201909 multilingual shared task (Hajic\u030c et al., 2009).\nModel Configuration. Inspired by the integrated POS tagging and parsing transition system of Bohnet and Nivre (2012), we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack. We extract the\nfollowing features on a window \u00b13 tokens centered at the current focus token: word, cluster, character n-gram up to length 3. We also extract the tag predicted for the previous 4 tokens. The network in these experiments has a single hidden layer with 256 units on WSJ and Treebank Union and 64 on CoNLL\u201909.\nResults. In Table 1 we compare our model to a linear CRF and to the compositional characterto-word LSTM model of Ling et al. (2015). The CRF is a first-order linear model with exact inference and the same emission features as our model. It additionally also has transition features of the word, cluster and character n-gram up to length 3 on both endpoints of the transition. The results for Ling et al. (2015) were solicited from the authors.\nOur local model already compares favorably against these methods on average. Using beam search with a locally normalized model does not help, but with global normalization it leads to a 7% reduction in relative error, empirically demonstrating the effect of label bias. It is also interesting to note that the set of character ngrams feature is very important, increasing average accuracy on the CoNLL\u201909 datasets by about 0.5% absolute. This shows that character-level modeling can also be done with a simple feed-forward netowork without recurrence."}, {"heading": "4.2 Dependency Parsing", "text": "In dependency parsing the goal is to produce a directed tree representing the syntactic structure of the input sentence.\nData & Evaluation. We use the same corpora as in our POS tagging experiments, except that we use the standard parsing splits of the WSJ. We convert the English constituency trees to Stanford style dependencies (De Marneffe et al., 2006) using version 3.3.0 of the converter. For English, we use predicted POS tags (the same POS tags are used for all models) and exclude punctua-\ntion from the evaluation, as is standard. For the CoNLL \u201909 datasets we follow standard practice and include all punctuation in the evaluation. We follow Alberti et al. (2015) and use our own predicted POS tags so that we can include a k-best tag feature (see below) but use the supplied predicted morphological features. We report unlabeled and labeled attachment scores (UAS/LAS).\nModel Configuration. Our model configuration is basically the same as the one originally proposed by Chen and Manning (2014) and then refined by Weiss et al. (2015). In particular, we use the arc-standard transition system and extract the same set of features as prior work: words, part of speech tags, and dependency arcs and labels in the surrounding context of the state, as well as k-best tags as proposed by Alberti et al. (2015). We use two hidden layers of 1,024 dimensions each.\nResults. Tables 2 and Table 3 show our final parsing results and a comparison to the best systems from the literature. We obtain the best ever published results on almost all datasets, including the WSJ. The results in Table 2 are without tritraining. When we use tri-training, our WSJ accuracy improves to 94.61/92.78 (UAS/LAS), which compares favorably to the 94.26/92.41 reported by Weiss et al. (2015) with tri-training. As we\nshow in Section 5, these gains can be attributed to the full backpropagation training that differentiates our approach from that of Weiss et al. (2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al. (2015) and Ballesteros et al. (2015)."}, {"heading": "4.3 Sentence Compression", "text": "Our final structured prediction task is extractive sentence compression.\nData & Evaluation. We follow Filippova et al. (2015), where a large news collection is used to heuristically generate compression instances. Our final corpus contains about 2.3M compression instances: we use 2M examples for training, 130k for development and 160k for the final test. We report per-token F1 score and per-sentence accuracy (A), i.e. percentage of instances that fully match the golden compressions. Following Filippova et al. (2015) we also run a human evaluation on 200 sentences where we ask the raters to score compressions for readability (read) and informativeness (info) on a scale from 0 to 5.\nModel Configuration. The transition system for sentence compression is similar to POS tagging: we scan sentences from left-to-right and la-\nGenerated corpus Human eval Method A F1 read info\nFilippova et al. (2015) 35.36 82.83 4.66 4.03 Automatic - - 4.31 3.77\nOur Local (B=1) 30.51 78.72 4.58 4.03 Our Local (B=8) 31.19 75.69 - - Our Global (B=8) 35.16 81.41 4.67 4.07\nTable 4: Sentence compression results on News data. Automatic refers to application of the same automatic extraction rules used to generate the News training corpus.\nbel each token as keep or drop. We extract features from words, POS tags, and dependency labels from a window of tokens centered on the input, as well as features from the history of predictions. We use a single hidden layer of size 400.\nResults. Table 4 shows our sentence compression results. Our globally normalized model again significantly outperforms the local model. Beam search with a locally normalized model suffers from severe label bias issues that we discuss on a concrete example in Section 5. We also compare to the best sentence compression system from Filippova et al. (2015), a 3-layer stacked LSTM which uses dependency label information. The LSTM and our global model perform on par on both the automatic evaluation as well as the human ratings, but our model is roughly 100\u00d7 faster. All compressions kept approximately 42% of the tokens on average and all the models are significantly better than the automatic extractions (p < 0.05)."}, {"heading": "5 Discussion", "text": "We derived a proof for the label bias problem and the advantages of global models. We then emprirically verified this theoretical superiority by demonstrating state-of-the-art performance on three different tasks. Our experiments showed consistent improvements in accuracy for globally normalized models over locally normalized models with beam search. In this section we situate and compare our model to previous work and provide two examples of the label bias problem in practice."}, {"heading": "5.1 Related Neural CRF Work", "text": "Neural network models have been been combined with conditional random fields and globally normalized models before. Bottou et al. (1997) and Le Cun et al. (1998) describe global train-\nMethod UAS LAS\nLocal (B=1) 92.85 90.59 Local (B=16) 93.32 91.09\nGlobal (B=16) {\u03b8(d)} 93.45 91.21 Global (B=16) {W2, \u03b8(d)} 94.01 91.77 Global (B=16) {W1,W2, \u03b8(d)} 94.09 91.81 Global (B=16) (full) 94.38 92.17\nTable 5: WSJ dev set scores for successively deeper levels of backpropagation. The full parameter set corresponds to backpropagation all the way to the embeddings. Wi: hidden layer i weights.\ning of neural network models for structured prediction problems. Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures. Yao et al. (2014) and Zheng et al. (2015) introduce recurrence into the model and Huang et al. (2015) finally combine CRFs and LSTMs. These neural CRF models are limited to sequence labeling tasks where exact inference is possible, while our model works well when exact inference is intractable."}, {"heading": "5.2 Related Transition-Based Parsing Work", "text": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014). Empirically, Weiss et al. (2015) achieves the best performance, even though their model keeps the parameters of the locally normalized neural network fixed and only trains a perceptron that uses the activations as features. Their model is therefore limited in its ability to revise the predictions of the locally normalized model. In Table 5 we show that full backpropagation training all the way to the word embeddings is very important and significantly contributes to the performance of our model. We also compared training under the CRF objective with a Perceptron-like hinge loss between the gold and best elements of the beam. When we limited the backpropagation depth to training only the top layer \u03b8(d), we found negligible differences in accuracy: 93.20% and 93.28% for the CRF objective and hinge loss respectively. However,\nwhen training with full backpropagation the CRF accuracy is 0.2% higher and training converged more than 4\u00d7 faster.\nZhou et al. (2015) perform full backpropagation training like us, but even with a much larger beam, their performance is significantly lower than ours. We also apply our model to two additional tasks, while they experiment only with dependency parsing. Finally, Watanabe and Sumita (2015) introduce recurrent components and additional techniques like maxviolation updates for a corresponding constituency parsing model. In contrast, our model does not require any recurrence or specialized training."}, {"heading": "5.3 Label Bias in Practice", "text": "We observed several instances of severe label bias in the sentence compression task. Although using beam search with the local model outperforms greedy inference on average, beam search leads the local model to occasionally produce empty compressions (Table 6). It is important to note that these are not search errors: the empty compression has higher probability under pL than the prediction from greedy inference. However, the more expressive globally normalized model does not suffer from this limitation, and correctly gives the empty compression almost zero probability.\nWe also present some empirical evidence that the label bias problem is severe in parsing. We trained models where the scoring functions in parsing at position i in the sentence are limited to considering only tokens x1:i; hence unlike the full parsing model, there is no ability to look ahead in the sentence when making a decision.3 The result for a greedy model under this constraint is 76.96% UAS; for a locally normalized model with beam search is 81.35%; and for a globally normalized model is 93.60%. Thus the globally normalized model gets very close to the perfor-\n3This setting may be important in some applications, where for example parse structures for sentence prefixes are required, or where the input is received one word at a time and online processing is beneficial.\nmance of a model with full lookahead, while the locally normalized model with a beam gives dramatically lower performance. In our final experiments with full lookahead, the globally normalized model achieves 94.01% accuracy, compared to 93.07% accuracy for a local model with beam search. Thus adding lookahead allows the local model to close the gap in performance to the global model; however there is still a significant difference in accuracy, which may in large part be due to the label bias problem.\nA number of authors have considered modified training procedures for greedy models, or for locally normalized models. Daume\u0301 III et al. (2009) introduce Searn, an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions. Goldberg and Nivre (2013) describe improvements to a greedy parsing approach that makes use of methods from imitation learning (Ross et al., 2011) to augment the training set. Note that these methods are focused on greedy models: they are unlikely to solve the label bias problem when used in conjunction with beam search, given that the problem is one of expressivity of the underlying model. More recent work (Yazdani and Henderson, 2015; Vaswani and Sagae, 2016) has augmented locally normalized models with correctness probabilities or error states, effectively adding a step after every decision where the probability of correctness of the resulting structure is evaluated. This gives considerable gains over a locally normalized model, although performance is lower than our full globally normalized approach."}, {"heading": "6 Conclusions", "text": "We presented a simple and yet powerful model architecture that produces state-of-the-art results for POS tagging, dependency parsing and sentence compression. Our model combines the flexibility of transition-based algorithms and the modeling power of neural networks. Our results demon-\nstrate that feed-forward network without recurrence can outperform recurrent models such as LSTMs when they are trained with global normalization. We further support our empirical findings with a proof showing that global normalization helps the model overcome the label bias problem from which locally normalized models suffer."}, {"heading": "Acknowledgements", "text": "We would like to thank Ling Wang for training his C2W part-of-speech tagger on our setup, and Emily Pitler, Ryan McDonald, Greg Coppola and Fernando Pereira for tremendously helpful discussions. Finally, we are grateful to all members of the Google Parsing Team."}], "references": [{"title": "Improved transition-based parsing and tagging with neural networks", "author": ["David Weiss", "Greg Coppola", "Slav Petrov"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Alberti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alberti et al\\.", "year": 2015}, {"title": "Improved transitionbased parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["Bohnet", "Nivre2012] Bernd Bohnet", "Joakim Nivre"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural", "citeRegEx": "Bohnet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bohnet et al\\.", "year": 2012}, {"title": "Graph transformer networks for image recognition. Bulletin of the International Statistical Institute (ISI)", "author": ["Bottou", "LeCun2005] L\u00e9on Bottou", "Yann LeCun"], "venue": null, "citeRegEx": "Bottou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2005}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["Bottou et al.1997] L\u00e9on Bottou", "Yann Le Cun", "Yoshua Bengio"], "venue": "In Proceedings of Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "Une approche th\u00e9orique de lapprentissage connexionniste: Applications \u00e0 la reconnaissance de la parole", "author": ["L\u00e9on Bottou"], "venue": "Ph.D. thesis, Doctoral dissertation, Universite de Paris XI", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Collins", "Roark2004] Michael Collins", "Brian Roark"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics", "citeRegEx": "Collins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2004}, {"title": "Search-based structured prediction. Machine Learning Journal (MLJ)", "author": ["John Langford", "Daniel Marcu"], "venue": null, "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of Fifth International Conference on Language Resources", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Neural conditional random fields", "author": ["Do", "Artires2010] Trinh Minh Tri Do", "Thierry Artires"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Do et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Do et al\\.", "year": 2010}, {"title": "Neural crf parsing", "author": ["Durrett", "Klein2015] Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Durrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "\u0141ukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "Training deterministic parsers with non-deterministic oracles. Transactions of the Association for Computational Linguistics, 1:403\u2013414", "author": ["Goldberg", "Nivre2013] Yoav Goldberg", "Joakim Nivre"], "venue": null, "citeRegEx": "Goldberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2013}, {"title": "The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages", "author": ["Yi Zhang"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task,", "citeRegEx": "Zhang.,? \\Q2009\\E", "shortCiteRegEx": "Zhang.", "year": 2009}, {"title": "Inducing history representations for broad coverage statistical parsing", "author": ["James Henderson"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association", "citeRegEx": "Henderson.,? \\Q2003\\E", "shortCiteRegEx": "Henderson.", "year": 2003}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson"], "venue": "In Proceedings of the 42nd Meeting of the Association for Computational Linguistics", "citeRegEx": "Henderson.,? \\Q2004\\E", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: The 90% solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Questionbank: Creating a corpus of parse-annotated questions", "author": ["Judge et al.2006] John Judge", "Aoife Cahill", "Josef van Genabith"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Associa-", "citeRegEx": "Judge et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Judge et al\\.", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the Eighteenth International Conference on Machine Learn-", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Gradient based learning applied to document recognition", "author": ["Le Cun et al.1998] Yann Le Cun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of IEEE,", "citeRegEx": "Cun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cun et al\\.", "year": 1998}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["Lei et al.2014] Tao Lei", "Yu Xin", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Lei et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2014}, {"title": "Structure compilation: Trading structure for features", "author": ["Liang et al.2008] Percy Liang", "III Hal Daum\u00e9", "Dan Klein"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Beatrice Santorini", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Miguel Almeida", "Noah A. Smith"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Inductive Dependency Parsing", "author": ["Joakim Nivre"], "venue": null, "citeRegEx": "Nivre.,? \\Q2006\\E", "shortCiteRegEx": "Nivre.", "year": 2006}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "Conditional neural fields", "author": ["Peng et al.2009] Jian Peng", "Liefeng Bo", "Jinbo Xu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Peng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2009}, {"title": "Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL)", "author": ["Petrov", "McDonald2012] Slav Petrov", "Ryan McDonald"], "venue": null, "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "No-regret reductions for imitation learning and structured prediction. AISTATS", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Efficient structured inference for transition-based parsing with neural networks and error states", "author": ["Vaswani", "Sagae2016] Ashish Vaswani", "Kenji Sagae"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Vaswani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Grammar as a foreign language", "author": ["\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Transition-based neural constituent parsing", "author": ["Watanabe", "Sumita2015] Taro Watanabe", "Eiichiro Sumita"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Watanabe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 2015}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Recurrent conditional random field for language understanding", "author": ["Yao et al.2014] Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Dong Yu", "Xiaolong Li", "Feng Gao"], "venue": "In IEEE International Conference on Acoustics,", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Incremental recurrent neural network dependency parser with searchbased discriminative training", "author": ["Yazdani", "Henderson2015] Majid Yazdani", "James Henderson"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural", "citeRegEx": "Yazdani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yazdani et al\\.", "year": 2015}, {"title": "Enforcing structural diversity in cube-pruned dependency parsing", "author": ["Zhang", "McDonald2014] Hao Zhang", "Ryan McDonald"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Conditional random fields", "author": ["Zheng et al.2015] Shuai Zheng", "Sadeep Jayasumana", "Bernardino Romera-Paredes", "Vibhav Vineet", "Zhizhong Su", "Dalong Du", "Chang Huang", "Philip H.S. Torr"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Zhou et al.2015] Hao Zhou", "Yue Zhang", "Jiajun Chen"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 26, "context": "In particular, variants of long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have produced impressive results on some of the classic NLP tasks such as part-of-speech tagging (Ling et al., 2015), syntactic parsing (Vinyals et al.", "startOffset": 198, "endOffset": 217}, {"referenceID": 35, "context": ", 2015), syntactic parsing (Vinyals et al., 2015) and semantic role labeling (Zhou and Xu, 2015).", "startOffset": 27, "endOffset": 49}, {"referenceID": 29, "context": "Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014).", "startOffset": 70, "endOffset": 83}, {"referenceID": 4, "context": "We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001) to overcome the label bias problem that locally normalized models suffer from.", "startOffset": 173, "endOffset": 238}, {"referenceID": 22, "context": "We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al., 1997; Le Cun et al., 1998; Lafferty et al., 2001) to overcome the label bias problem that locally normalized models suffer from.", "startOffset": 173, "endOffset": 238}, {"referenceID": 42, "context": "Since we use beam inference, we approximate the partition function by summing over the elements in the beam, and use early updates (Collins and Roark, 2004; Zhou et al., 2015).", "startOffset": 131, "endOffset": 175}, {"referenceID": 24, "context": "Our model, described in detail in Section 2, uses a transition system (Nivre, 2006) and feature embeddings as introduced by Chen and Manning (2014). We do not use any recurrence, but perform beam search for maintaining multiple hypotheses and introduce global normalization with a conditional random field (CRF) objective (Bottou et al.", "startOffset": 71, "endOffset": 148}, {"referenceID": 36, "context": "Our ablation experiments show that we outperform Weiss et al. (2015) and Alberti et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015) because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model.", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015) because we do global backpropagation training of all model parameters, while they fix the neural network parameters when training the global part of their model. We also outperform Zhou et al. (2015) despite using a smaller beam.", "startOffset": 11, "endOffset": 233}, {"referenceID": 29, "context": "At its core, our model is an incremental transitionbased parser (Nivre, 2006).", "startOffset": 64, "endOffset": 77}, {"referenceID": 29, "context": "In dependency parsing for example, this is true for both the arc-standard and arc-eager transition systems (Nivre, 2006), where for a sentence x of length m, the number of decisions for any complete parse is n(x) = 2 \u00d7 m.", "startOffset": 107, "endOffset": 120}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences.", "startOffset": 69, "endOffset": 82}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. define \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j . The scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al.", "startOffset": 69, "endOffset": 456}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. define \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j . The scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al.", "startOffset": 69, "endOffset": 477}, {"referenceID": 29, "context": "Note that this is not true for the swap transition system defined in Nivre (2009). It is straightforward to extend the approach to make use of dynamic programming in the case where the same state can be reached by multiple decision sequences. define \u03c1(d1:j , d; \u03b8) to be equal to \u03c1(s, d; \u03b8) where s is the state reached by decisions d1:j . The scoring function \u03c1(s, d; \u03b8) can be defined in a number of ways. In this work, following Chen and Manning (2014), Weiss et al. (2015), and Zhou et al. (2015), we define it via a feedforward neural network as", "startOffset": 69, "endOffset": 501}, {"referenceID": 42, "context": "To make learning tractable with the globally normalized model, we use beam search and early updates (Collins and Roark, 2004; Zhou et al., 2015).", "startOffset": 100, "endOffset": 144}, {"referenceID": 21, "context": "However the label bias problem (see Lafferty et al. (2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": "(2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.", "startOffset": 8, "endOffset": 22}, {"referenceID": 5, "context": "(2001), Bottou (1991), Bottou and LeCun (2005)) means that locally normalized models often have a very weak ability to revise earlier decisions.", "startOffset": 8, "endOffset": 47}, {"referenceID": 25, "context": "For a detailed analysis of the tradeoffs between structural features in CRFs and more powerful local classifiers without structural constraints, see Liang et al. (2008); in these experiments local classifiers are unable to reach the performance of CRFs on problems such as parsing and named entity recognition where structural constraints are important.", "startOffset": 149, "endOffset": 169}, {"referenceID": 26, "context": "17 Ling et al. (2015) 97.", "startOffset": 3, "endOffset": 22}, {"referenceID": 27, "context": "We conducted experiments on a number of different datasets: (1) English Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al.", "startOffset": 124, "endOffset": 145}, {"referenceID": 19, "context": ", 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 21, "context": ", 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006) with identical setup to Weiss et al.", "startOffset": 111, "endOffset": 131}, {"referenceID": 19, "context": ", 1993) with standard POS tagging splits; (2) English \u201cTreebank Union\u201d multi-domain corpus containing data from the OntoNotes corpus version 5 (Hovy et al., 2006), the English Web Treebank (Petrov and McDonald, 2012), and the updated and corrected Question Treebank (Judge et al., 2006) with identical setup to Weiss et al. (2015); and (3) CoNLL \u201909 multilingual shared task (Haji\u010d et al.", "startOffset": 144, "endOffset": 331}, {"referenceID": 29, "context": "Inspired by the integrated POS tagging and parsing transition system of Bohnet and Nivre (2012), we employ a simple transition system that uses only a SHIFT action and predicts the POS tag of the current word on the buffer as it gets shifted to the stack.", "startOffset": 83, "endOffset": 96}, {"referenceID": 26, "context": "In Table 1 we compare our model to a linear CRF and to the compositional characterto-word LSTM model of Ling et al. (2015). The CRF is a first-order linear model with exact inference and the same emission features as our model.", "startOffset": 104, "endOffset": 123}, {"referenceID": 26, "context": "In Table 1 we compare our model to a linear CRF and to the compositional characterto-word LSTM model of Ling et al. (2015). The CRF is a first-order linear model with exact inference and the same emission features as our model. It additionally also has transition features of the word, cluster and character n-gram up to length 3 on both endpoints of the transition. The results for Ling et al. (2015) were solicited from the authors.", "startOffset": 104, "endOffset": 402}, {"referenceID": 0, "context": "06 Alberti et al. (2015) 94.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "60 Alberti et al. (2015) 92.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "We follow Alberti et al. (2015) and use our own predicted POS tags so that we can include a k-best tag feature (see below) but use the supplied predicted morphological features.", "startOffset": 10, "endOffset": 32}, {"referenceID": 36, "context": "Our model configuration is basically the same as the one originally proposed by Chen and Manning (2014) and then refined by Weiss et al. (2015). In particular, we use the arc-standard transition system and extract the same set of features as prior work: words, part of speech tags, and dependency arcs and labels in the surrounding context of the state, as well as k-best tags as proposed by Alberti et al.", "startOffset": 124, "endOffset": 144}, {"referenceID": 0, "context": "In particular, we use the arc-standard transition system and extract the same set of features as prior work: words, part of speech tags, and dependency arcs and labels in the surrounding context of the state, as well as k-best tags as proposed by Alberti et al. (2015). We use two hidden layers of 1,024 dimensions each.", "startOffset": 247, "endOffset": 269}, {"referenceID": 34, "context": "41 reported by Weiss et al. (2015) with tri-training.", "startOffset": 15, "endOffset": 35}, {"referenceID": 34, "context": "41 reported by Weiss et al. (2015) with tri-training. As we show in Section 5, these gains can be attributed to the full backpropagation training that differentiates our approach from that of Weiss et al. (2015) and Alberti et al.", "startOffset": 15, "endOffset": 212}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al.", "startOffset": 11, "endOffset": 33}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al. (2015) and Ballesteros et al.", "startOffset": 11, "endOffset": 124}, {"referenceID": 0, "context": "(2015) and Alberti et al. (2015). Our results also significantly outperform the LSTM-based approaches of Dyer et al. (2015) and Ballesteros et al. (2015).", "startOffset": 11, "endOffset": 154}, {"referenceID": 13, "context": "We follow Filippova et al. (2015), where a large news collection is used to heuristically generate compression instances.", "startOffset": 10, "endOffset": 34}, {"referenceID": 13, "context": "We follow Filippova et al. (2015), where a large news collection is used to heuristically generate compression instances. Our final corpus contains about 2.3M compression instances: we use 2M examples for training, 130k for development and 160k for the final test. We report per-token F1 score and per-sentence accuracy (A), i.e. percentage of instances that fully match the golden compressions. Following Filippova et al. (2015) we also run a human evaluation on 200 sentences where we ask the raters to score compressions for readability (read) and informativeness (info) on a scale from 0 to 5.", "startOffset": 10, "endOffset": 430}, {"referenceID": 13, "context": "We also compare to the best sentence compression system from Filippova et al. (2015), a 3-layer stacked LSTM which uses dependency label information.", "startOffset": 61, "endOffset": 85}, {"referenceID": 3, "context": "Bottou et al. (1997) and Le Cun et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bottou et al. (1997) and Le Cun et al. (1998) describe global trainMethod UAS LAS", "startOffset": 0, "endOffset": 46}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures.", "startOffset": 0, "endOffset": 19}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures.", "startOffset": 0, "endOffset": 104}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures. Yao et al. (2014) and Zheng et al.", "startOffset": 0, "endOffset": 190}, {"referenceID": 30, "context": "Peng et al. (2009) add a non-linear neural network layer to a linearchain CRF and Do and Artires (2010) apply a similar approach to more general Markov network structures. Yao et al. (2014) and Zheng et al. (2015) introduce recurrence into the model and Huang et al.", "startOffset": 0, "endOffset": 214}, {"referenceID": 20, "context": "(2015) introduce recurrence into the model and Huang et al. (2015) finally combine CRFs and LSTMs.", "startOffset": 47, "endOffset": 67}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al.", "startOffset": 68, "endOffset": 147}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014).", "startOffset": 68, "endOffset": 167}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014).", "startOffset": 68, "endOffset": 198}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014). Empirically, Weiss et al.", "startOffset": 68, "endOffset": 295}, {"referenceID": 16, "context": "For early work on neural-networks for transition-based parsing, see Henderson (2003; 2004). Our work is closest to the work of Weiss et al. (2015), Zhou et al. (2015) and Watanabe and Sumita (2015); in these approaches global normalization is added to the local model of Chen and Manning (2014). Empirically, Weiss et al. (2015) achieves the best performance, even though their model keeps the parameters of the locally normalized neural network fixed and only trains a perceptron that uses the activations as features.", "startOffset": 68, "endOffset": 329}, {"referenceID": 33, "context": "Goldberg and Nivre (2013) describe improvements to a greedy parsing approach that makes use of methods from imitation learning (Ross et al., 2011) to augment the training set.", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "Daum\u00e9 III et al. (2009) introduce Searn, an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions.", "startOffset": 6, "endOffset": 24}, {"referenceID": 8, "context": "Daum\u00e9 III et al. (2009) introduce Searn, an algorithm that allows a classifier making greedy decisions to become more robust to errors made in previous decisions. Goldberg and Nivre (2013) describe improvements to a greedy parsing approach that makes use of methods from imitation learning (Ross et al.", "startOffset": 6, "endOffset": 189}], "year": 2016, "abstractText": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-ofspeech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.", "creator": "LaTeX with hyperref package"}}}