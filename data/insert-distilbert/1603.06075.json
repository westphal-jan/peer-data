{"id": "1603.06075", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Tree-to-Sequence Attentional Neural Machine Translation", "abstract": "most of the existing neural machine translation ( nmt ) models focus on the conversion of sequential data and don't directly take syntax concepts into consideration. we propose a novel syntax end - to - task end branching syntactic phonetic nmt model, extending above a sequence - to - sequence model with the source - side phrase structure. our model has an attention mechanism that enables permitting the decoder to generate a translated word while softly aligning it with phrases as as tightly well as words of the source sentence. experimental experimental results on the wat'15 english - hungarian to - japanese dataset size demonstrate that our proposed model outperforms sequence - to - sequence attentional nmt models and compares favorably with most the state - word of - the - art tree - to - string smt system.", "histories": [["v1", "Sat, 19 Mar 2016 10:08:40 GMT  (437kb)", "https://arxiv.org/abs/1603.06075v1", null], ["v2", "Tue, 22 Mar 2016 09:55:39 GMT  (340kb,D)", "http://arxiv.org/abs/1603.06075v2", null], ["v3", "Wed, 8 Jun 2016 08:39:11 GMT  (371kb,D)", "http://arxiv.org/abs/1603.06075v3", "Accepted as a full paper at the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akiko eriguchi", "kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": true, "id": "1603.06075"}, "pdf": {"name": "1603.06075.pdf", "metadata": {"source": "CRF", "title": "Tree-to-Sequence Attentional Neural Machine Translation", "authors": ["Akiko Eriguchi", "Kazuma Hashimoto"], "emails": ["tsuruoka}@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Machine Translation (MT) has traditionally been one of the most complex language processing problems, but recent advances of Neural Machine Translation (NMT) make it possible to perform translation using a simple end-to-end architecture. In the Encoder-Decoder model (Cho et al., 2014b; Sutskever et al., 2014), a Recurrent Neural Network (RNN) called the encoder reads the whole sequence of source words to produce a fixedlength vector, and then another RNN called the decoder generates the target words from the vector. The Encoder-Decoder model has been extended with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), which allows the model to jointly learn the soft alignment between the source language and the target language. NMT models have achieved state-of-the-art results in English-to-French and English-to-German trans-\nlation tasks (Luong et al., 2015b; Luong et al., 2015a). However, it is yet to be seen whether NMT is competitive with traditional Statistical Machine Translation (SMT) approaches in translation tasks for structurally distant language pairs such as English-to-Japanese.\nFigure 1 shows a pair of parallel sentences in English and Japanese. English and Japanese are linguistically distant in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units. In this example, the Japanese word \u201c\u7dd1\u8336\u201d is aligned with the English words \u201cgreen\u201d and \u201ctea\u201d, and the English word sequence \u201ca cup of\u201d is aligned with a special symbol \u201cnull\u201d, which is not explicitly translated into any Japanese words. One way to solve this mismatch problem is to consider the phrase structure of the English sentence and align the phrase \u201ca cup of green tea\u201d with \u201c\u7dd1\u8336\u201d. In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment (Yamada and Knight, 2001) and translation accuracy (Liu et al., 2006; Neubig and Duh, 2014). However, the existing NMT models do not allow us to perform this kind of alignment.\nIn this paper, we propose a novel attentional NMT model to take advantage of syntactic infor-\nar X\niv :1\n60 3.\n06 07\n5v 3\n[ cs\n.C L\n] 8\nJ un\n2 01\n6\nmation. Following the phrase structure of a source sentence, we encode the sentence recursively in a bottom-up fashion to produce a vector representation of the sentence and decode it while aligning the input phrases and words with the output. Our experimental results on the WAT\u201915 English-toJapanese translation task show that our proposed model achieves state-of-the-art translation accuracy."}, {"heading": "2 Neural Machine Translation", "text": ""}, {"heading": "2.1 Encoder-Decoder Model", "text": "NMT is an end-to-end approach to data-driven machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). In other words, the NMT models directly estimate the conditional probability p(y|x) given a large collection of source and target sentence pairs (x,y). An NMT model consists of an encoder process and a decoder process, and hence they are often called Encoder-Decoder models. In the Encoder-Decoder models, a sentence is treated as a sequence of words. In the encoder process, the encoder embeds each of the source words x = (x1, x2, \u00b7 \u00b7 \u00b7 , xn) into a d-dimensional vector space. The decoder then outputs a word sequence y = (y1, y2, \u00b7 \u00b7 \u00b7 , ym) in the target language given the information on the source sentence provided by the encoder. Here, n and m are the lengths of the source and target sentences, respectively. RNNs allow one to effectively embed sequential data into the vector space.\nIn the RNN encoder, the i-th hidden unit hi \u2208 Rd\u00d71 is calculated given the i-th input xi and the previous hidden unit hi\u22121 \u2208 Rd\u00d71,\nhi = fenc(xi,hi\u22121), (1)\nwhere fenc is a non-linear function, and the initial hidden unit h0 is usually set to zeros. The encoding function fenc is recursively applied until the nth hidden unit hn is obtained. The RNN EncoderDecoder models assume that hn represents a vector of the meaning of the input sequence up to the n-th word.\nAfter encoding the whole input sentence into the vector space, we decode it in a similar way. The initial decoder unit s1 is initialized with the input sentence vector (s1 = hn). Given the previous target word and the j-th hidden unit of the decoder, the conditional probability that the j-th\ntarget word is generated is calculated as follows:\np(yj |y<j ,x) = g(sj), (2)\nwhere g is a non-linear function. The j-th hidden unit of the decoder is calculated by using another non-linear function fdec as follows:\nsj = fdec(yj\u22121, sj\u22121). (3)\nWe employ Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) in place of vanilla RNN units. The tth LSTM unit consists of several gates and two different types of states: a hidden unit ht \u2208 Rd\u00d71 and a memory cell ct \u2208 Rd\u00d71,\nit = \u03c3(W (i)xt +U (i)ht\u22121 + b (i)), ft = \u03c3(W (f)xt +U (f)ht\u22121 + b (f)), ot = \u03c3(W (o)xt +U (o)ht\u22121 + b (o)), c\u0303t = tanh(W (c\u0303)xt +U (c\u0303)ht\u22121 + b (c\u0303)),\nct = it c\u0303t + ft ct\u22121, ht = ot tanh(ct), (4)\nwhere each of it, ft, ot and c\u0303t \u2208 Rd\u00d71 denotes an input gate, a forget gate, an output gate, and a state for updating the memory cell, respectively. W (\u00b7) \u2208 Rd\u00d7d and U (\u00b7) \u2208 Rd\u00d7d are weight matrices, b(\u00b7) \u2208 Rd\u00d71 is a bias vector, and xt \u2208 Rd\u00d71 is the word embedding of the t-th input word. \u03c3(\u00b7) is the logistic function, and the operator denotes element-wise multiplication between vectors."}, {"heading": "2.2 Attentional Encoder-Decoder Model", "text": "The NMT models with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a) have been proposed to softly align each decoder state with the encoder states. The attention mechanism allows the NMT models to explicitly quantify how much each encoder state contributes to the word prediction at each time step.\nIn the attentional NMT model in Luong et al. (2015a), at the j-th step of the decoder process, the attention score \u03b1j(i) between the i-th source hidden unit hi and the j-th target hidden unit sj is calculated as follows:\n\u03b1j(i) = exp(hi \u00b7 sj)\u2211n\nk=1 exp(hk \u00b7 sj) , (5)\nwhere hi \u00b7 sj is the inner product of hi and sj , which is used to directly calculate the similarity score between hi and sj . The j-th context vector\ndj is calculated as the summation vector weighted by \u03b1j(i):\ndj = n\u2211\ni=1\n\u03b1j(i)hi. (6)\nTo incorporate the attention mechanism into the decoding process, the context vector is used for the the j-th word prediction by putting an additional hidden layer s\u0303j :\ns\u0303j = tanh(Wd[sj ; dj ] + bd), (7)\nwhere [sj ;dj ] \u2208 R2d\u00d71 is the concatenation of sj and dj , and Wd \u2208 Rd\u00d72d and bd \u2208 Rd\u00d71 are a weight matrix and a bias vector, respectively. The model predicts the j-th word by using the softmax function:\np(yj |y<j ,x) = softmax(Wss\u0303j + bs), (8)\nwhereWs \u2208 R|V |\u00d7d and bs \u2208 R|V |\u00d71 are a weight matrix and a bias vector, respectively. |V | stands for the size of the vocabulary of the target language. Figure 2 shows an example of the NMT model with the attention mechanism."}, {"heading": "2.3 Objective Function of NMT Models", "text": "The objective function to train the NMT models is the sum of the log-likelihoods of the translation pairs in the training data:\nJ(\u03b8) = 1 |D| \u2211\n(x,y)\u2208D\nlog p(y|x), (9)\nwhere D denotes a set of parallel sentence pairs. The model parameters \u03b8 are learned through Stochastic Gradient Descent (SGD)."}, {"heading": "3 Attentional Tree-to-Sequence Model", "text": ""}, {"heading": "3.1 Tree-based Encoder + Sequential Encoder", "text": "The exsiting NMT models treat a sentence as a sequence of words and neglect the structure of\na sentence inherent in language. We propose a novel tree-based encoder in order to explicitly take the syntactic structure into consideration in the NMT model. We focus on the phrase structure of a sentence and construct a sentence vector from phrase vectors in a bottom-up fashion. The sentence vector in the tree-based encoder is therefore composed of the structural information rather than the sequential data. Figure 3 shows our proposed model, which we call a tree-to-sequence attentional NMT model.\nIn Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003), a sentence is composed of multiple phrase units and represented as a binary tree as shown in Figure 1. Following the structure of the sentence, we construct a tree-based encoder on top of the standard sequential encoder. The k-th parent hidden unit h(phr)k for the k-th phrase is calculated using the left and right child hidden units hlk and h r k as follows:\nh (phr) k = ftree(h l k,h r k), (10)\nwhere ftree is a non-linear function.\nWe construct a tree-based encoder with LSTM units, where each node in the binary tree is represented with an LSTM unit. When initializing the leaf units of the tree-based encoder, we employ the sequential LSTM units described in Section 2.1. Each non-leaf node is also represented with an LSTM unit, and we employ Tree-LSTM (Tai et al., 2015) to calculate the LSTM unit of the parent node which has two child LSTM units. The hidden unit h(phr)k \u2208 R\nd\u00d71 and the memory cell c (phr) k \u2208 R d\u00d71 for the k-th parent node are calcu-\nlated as follows:\nik = \u03c3(U (i) l h l k +U (i) r h r k + b (i)), f lk = \u03c3(U (fl) l h l k +U (fl) r h r k + b (fl)), f rk = \u03c3(U (fr) l h l k +U (fr) r h r k + b (fr)), ok = \u03c3(U (o) l h l k +U (o) r h r k + b (o)), c\u0303k = tanh(U (c\u0303) l h l k +U (c\u0303) r h r k + b (c\u0303)),\nc (phr) k = ik c\u0303k + f l k clk + f rk crk, h (phr) k = ok tanh(c (phr) k ), (11)\nwhere ik, f lk, f r k , oj , c\u0303j \u2208 Rd\u00d71 are an input gate, the forget gates for left and right child units, an output gate, and a state for updating the memory cell, respectively. clk and c r k are the memory cells for the left and right child units, respectively. U (\u00b7) \u2208 Rd\u00d7d denotes a weight matrix, and b(\u00b7) \u2208 Rd\u00d71 represents a bias vector.\nOur proposed tree-based encoder is a natural extension of the conventional sequential encoder, since Tree-LSTM is a generalization of chainstructured LSTM (Tai et al., 2015). Our encoder differs from the original Tree-LSTM in the calculation of the LSTM units for the leaf nodes. The motivation is to construct the phrase nodes in a context-sensitive way, which, for example, allows the model to compute different representations for multiple occurrences of the same word in a sentence because the sequential LSTMs are calculated in the context of the previous units. This ability contrasts with the original Tree-LSTM, in which the leaves are composed only of the word embeddings without any contextual information."}, {"heading": "3.2 Initial Decoder Setting", "text": "We now have two different sentence vectors: one is from the sequence encoder and the other from the tree-based encoder. As shown in Figure 3, we provide another Tree-LSTM unit which has the final sequential encoder unit (hn) and the tree-based encoder unit (h(phr)root ) as two child units and set it as the initial decoder s1 as follows:\ns1 = gtree(hn,h (phr) root ), (12)\nwhere gtree is the same function as ftree with another set of Tree-LSTM parameters. This initialization allows the decoder to capture information from both the sequential data and phrase structures. Zoph and Knight (2016) proposed a similar method using a Tree-LSTM for initializing the\ndecoder, with which they translate multiple source languages to one target language. When the syntactic parser fails to output a parse tree for a sentence, we encode the sentence with the sequential encoder by setting h(phr)root = 0. Our proposed treebased encoder therefore works with any sentences."}, {"heading": "3.3 Attention Mechanism in Our Model", "text": "We adopt the attention mechanism into our treeto-sequence model in a novel way. Our model gives attention not only to sequential hidden units but also to phrase hidden units. This attention mechanism tells us which words or phrases in the source sentence are important when the model decodes a target word. The j-th context vector dj is composed of the sequential and phrase vectors weighted by the attention score \u03b1j(i):\ndj = n\u2211\ni=1 \u03b1j(i)hi + 2n\u22121\u2211 i=n+1 \u03b1j(i)h (phr) i . (13)\nNote that a binary tree has n \u2212 1 phrase nodes if the tree has n leaves. We set a final decoder s\u0303j in the same way as Equation (7).\nIn addition, we adopt the input-feeding method (Luong et al., 2015a) in our model, which is a method for feeding s\u0303j\u22121, the previous unit to predict the word yj\u22121, into the current target hidden unit sj ,\nsj = fdec(yj\u22121, [sj\u22121; s\u0303j\u22121]), (14)\nwhere [sj\u22121; s\u0303j\u22121] is the concatenation of sj\u22121 and s\u0303j\u22121. The input-feeding approach contributes to the enrichment in the calculation of the decoder, because s\u0303j\u22121 is an informative unit which can be used to predict the output word as well as to be compacted with attentional context vectors. Luong et al. (2015a) showed that the input-feeding approach improves BLEU scores. We also observed the same improvement in our preliminary experiments."}, {"heading": "3.4 Sampling-Based Approximation to the NMT Models", "text": "The biggest computational bottleneck of training the NMT models is in the calculation of the softmax layer described in Equation (8), because its computational cost increases linearly with the size of the vocabulary. The speedup technique with GPUs has proven useful for sequence-based NMT models (Sutskever et al., 2014; Luong et al.,\n2015a) but it is not easily applicable when dealing with tree-structured data. In order to reduce the training cost of the NMT models at the softmax layer, we employ BlackOut (Ji et al., 2016), a sampling-based approximation method. BlackOut has been shown to be effective in RNN Language Models (RNNLMs) and allows a model to run reasonably fast even with a million word vocabulary with CPUs.\nAt each word prediction step in the training, BlackOut estimates the conditional probability in Equation (2) for the target word and K negative samples using a weighted softmax function. The negative samples are drawn from the unigram distribution raised to the power \u03b2 \u2208 [0, 1] (Mikolov et al., 2013). The unigram distribution is estimated using the training data and \u03b2 is a hyperparameter. BlackOut is closely related to Noise Contrastive Estimation (NCE) (Gutmann and Hyva\u0308rinen, 2012) and achieves better perplexity than the original softmax and NCE in RNNLMs. The advantages of Blackout over the other methods are discussed in Ji et al. (2016). Note that BlackOut can be used as the original softmax once the training is finished."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Training Data", "text": "We applied the proposed model to the English-toJapanese translation dataset of the ASPEC corpus given in WAT\u201915.1 Following Zhu (2015), we extracted the first 1.5 million translation pairs from the training data. To obtain the phrase structures of the source sentences, i.e., English, we used the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008). We used Enju only to obtain a binary phrase structure for each sentence and did not use any HPSG specific information. For the target language, i.e., Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT\u201915.2 We then filtered out the translation pairs whose sentence lengths are longer than 50 and whose source sentences are not parsed successfully. Table 1 shows the details of the datasets used in our experiments. We carried out two experiments on a small training dataset to investigate\n1http://orchid.kuee.kyoto-u.ac.jp/WAT/ WAT2015/index.html\n2http://orchid.kuee.kyoto-u.ac.jp/WAT/ WAT2015/baseline/dataPreparationJE.html\nthe effectiveness of our proposed model and on a large training dataset to compare our proposed methods with the other systems.\nThe vocabulary consists of words observed in the training data more than or equal to N times. We set N = 2 for the small training dataset and N = 5 for the large training dataset. The out-ofvocabulary words are mapped to the special token \u201cunk\u201d. We added another special symbol \u201ceos\u201d for both languages and inserted it at the end of all the sentences. Table 2 shows the details of each training dataset and its corresponding vocabulary size."}, {"heading": "4.2 Training Details", "text": "The biases, softmax weights, and BlackOut weights are initialized with zeros. The hyperparameter \u03b2 of BlackOut is set to 0.4 as recommended by Ji et al. (2016). Following Jo\u0301zefowicz et al. (2015), we initialize the forget gate biases of LSTM and Tree-LSTM with 1.0. The remaining model parameters in the NMT models in our experiments are uniformly initialized in [\u22120.1, 0.1]. The model parameters are optimized by plain SGD with the mini-batch size of 128. The initial learning rate of SGD is 1.0. We halve the learning rate when the development loss becomes worse. Gradient norms are clipped to 3.0 to avoid exploding gradient problems (Pascanu et al., 2012).\nSmall Training Dataset We conduct experiments with our proposed model and the sequential attentional NMT model with the input-feeding approach. Each model has 256-dimensional hidden units and word embeddings. The number of negative samples K of BlackOut is set to 500 or 2000.\nLarge Training Dataset Our proposed model has 512-dimensional word embeddings and ddimensional hidden units (d \u2208 {512, 768, 1024}). K is set to 2500.\nOur code3 is implemented in C++ using the Eigen library,4 a template library for linear algebra, and we run all of the experiments on multicore CPUs.5 It takes about a week to train a model on the large training dataset with d = 512."}, {"heading": "4.3 Decoding process", "text": "We use beam search to decode a target sentence for an input sentence x and calculate the sum of the log-likelihoods of the target sentence y = (y1, \u00b7 \u00b7 \u00b7 , ym) as the beam score:\nscore(x,y) = m\u2211 j=1 log p(yj |y<j ,x). (15)\nDecoding in the NMT models is a generative process and depends on the target language model given a source sentence. The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014). In our preliminary experiments, the beam search with the length normalization in Cho et al. (2014a) was not effective in English-to-Japanese translation. The method in Pouget-Abadie et al. (2014) needs to estimate the conditional probability p(x|y) using another NMT model and thus is not suitable for our work.\nIn this paper, we use statistics on sentence lengths in beam search. Assuming that the length of a target sentence correlates with the length of a source sentence, we redefine the score of each candidate as follows:\nscore(x,y) = Lx,y + m\u2211 j=1 log p(yj |y<j ,x),(16)\nLx,y = log p(len(y)|len(x)), (17)\nwhere Lx,y is the penalty for the conditional probability of the target sentence length len(y) given the source sentence length len(x). It allows the model to decode a sentence by considering the length of the target sentence. In our experiments, we computed the conditional probability\n3https://github.com/tempra28/tree2seq 4http://eigen.tuxfamily.org/index.php 516 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @\n3.20GHz\np(len(y)|len(x)) in advance following the statistics collected in the first one million pairs of the training dataset. We allow the decoder to generate up to 100 words."}, {"heading": "4.4 Evaluation", "text": "We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al., 2002) following WAT\u201915. We used the KyTea-based evaluation script for the translation results.6 The RIBES score is a metric based on rank correlation coefficients with word precision, and the BLEU score is based on n-gram word precision and a Brevity Penalty (BP) for outputs shorter than the references. RIBES is known to have stronger correlation with human judgements than BLEU in translation between English and Japanese as discussed in Isozaki et al. (2010)."}, {"heading": "5 Results and Discussion", "text": ""}, {"heading": "5.1 Small Training Dataset", "text": "Table 3 shows the perplexity, BLEU, RIBES, and the training time on the development data with the Attentional NMT (ANMT) models trained on the small dataset. We conducted the experiments with our proposed method using BlackOut and softmax. We decoded a translation by our proposed beam search with a beam size of 20.\nAs shown in Table 3, the results of our proposed model with BlackOut improve as the number of negative samplesK increases. Although the result of softmax is better than those of BlackOut (K = 500, 2000), the training time of softmax per epoch is about three times longer than that of BlackOut even with the small dataset.\nAs to the results of the ANMT model, reversing the word order in the input sentence decreases the scores in English-to-Japanese translation, which contrasts with the results of other language pairs reported in previous work (Sutskever et al., 2014; Luong et al., 2015a). By taking syntactic information into consideration, our proposed model improves the scores, compared to the sequential attention-based approach.\nWe found that better perplexity does not always lead to better translation scores with BlackOut as shown in Table 3. One of the possible reasons is that BlackOut distorts the target word distribution\n6http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/automatic_evaluation_systems/ automaticEvaluationJA.html\nby the modified unigram-based negative sampling where frequent words can be treated as the negative samples multiple times at each training step.\nEffects of the proposed beam search Table 4 shows the results on the development data of proposed method with BlackOut (K = 2000) by the simple beam search and our proposed beam search. The beam size is set to 6 or 20 in the simple beam search, and to 20 in our proposed search. We can see that our proposed search outperforms the simple beam search in both scores. Unlike RIBES, the BLEU score is sensitive to the beam size and becomes lower as the beam size increases. We found that the BP had a relatively large impact on the BLEU score in the simple beam search as the beam size increased. Our search method works better than the simple beam search by keeping long sentences in the candidates with a large beam size.\nEffects of the sequential LSTM units We also investigated the effects of the sequential LSTMs at the leaf nodes in our proposed tree-based encoder. Table 5 shows the result on the development data of our proposed encoder and that of an attentional tree-based encoder without sequential LSTMs with BlackOut (K = 2000).7 The results show that our proposed encoder considerably out-\n7For this evaluation, we used the 1,789 sentences that were successfully parsed by Enju because the encoder without sequential LSTMs always requires a parse tree.\nperforms the encoder without sequential LSTMs, suggesting that the sequential LSTMs at the leaf nodes contribute to the context-aware construction of the phrase representations in the tree."}, {"heading": "5.2 Large Training Dataset", "text": "Table 6 shows the experimental results of RIBES and BLEU scores achieved by the trained models on the large dataset. We decoded the target sentences by our proposed beam search with the beam size of 20.8 The results of the other systems are the ones reported in Nakazawa et al. (2015).\nAll of our proposed models show similar performance regardless of the value of d. Our ensemble model is composed of the three models with d = 512, 768, and 1024, and it shows the best RIBES score among all systems.9\nAs for the time required for training, our implementation needs about one day to perform one epoch on the large training dataset with d = 512. It would take about 11 days without using the BlackOut sampling.\nComparison with the NMT models The model of Zhu (2015) is an ANMT model (Bahdanau et al., 2015) with a bi-directional LSTM encoder, and uses 1024-dimensional hidden units and 1000-\n8We found two sentences which ends without eos with d = 512, and then we decoded it again with the beam size of 1000 following Zhu (2015).\n9Our ensemble model yields a METEOR (Denkowski and Lavie, 2014) score of 53.6 with language option \u201c-l other\u201d.\ndimensional word embeddings. The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200- dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)\u2019s best system which is a hybrid of the ANMT and SMT models by +1.54 RIBES and by +0.74 BLEU scores and Lee et al. (2015)\u2019s ANMT system with special character-based decoding by +1.30 RIBES and +1.20 BLEU scores.\nComparison with the SMT models PB, HPB and T2S are the baseline SMT systems in WAT\u201915: a phrase-based model, a hierarchical phrase-based model, and a tree-to-string model, respectively (Nakazawa et al., 2015). The best model in WAT\u201915 is Neubig et al. (2015)\u2019s treeto-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder. Our proposed end-to-end NMT model compares favorably with Neubig et al. (2015)."}, {"heading": "5.3 Qualitative Analysis", "text": "We illustrate the translations of test data by our model with d = 512 and several attentional relations when decoding a sentence. In Figures 4 and 5, an English sentence represented as a binary tree is translated into Japanese, and several attentional relations between English words or phrases and\nJapanese word are shown with the highest attention score \u03b1. The additional attentional relations are also illustrated for comparison. We can see the target words softly aligned with source words and phrases.\nIn Figure 4, the Japanese word \u201c\u6db2\u6676\u201d means \u201cliquid crystal\u201d, and it has a high attention score (\u03b1 = 0.41) with the English phrase \u201cliquid crystal for active matrix\u201d. This is because the j-th target hidden unit sj has the contextual information about the previous words y<j including \u201c\u6d3b\u6027\u30de \u30c8\u30ea\u30c3\u30af\u30b9\u306e\u201d (\u201cfor active matrix\u201d in English). The Japanese word \u201c\u30bb\u30eb\u201d is softly aligned with the phrase \u201cthe cells\u201d with the highest attention score (\u03b1 = 0.35). In Japanese, there is no definite article like \u201cthe\u201d in English, and it is usually aligned with null described as Section 1.\nIn Figure 5, in the case of the Japanese word \u201c\u793a\u201d (\u201cshowed\u201d in English), the attention score with the English phrase \u201cshowed excellent performance\u201d (\u03b1 = 0.25) is higher than that with the English word \u201cshowed\u201d (\u03b1 = 0.01). The Japanese word \u201c\u306e\u201d (\u201cof\u201d in English) is softly aligned with the phrase \u201cof Si dot MOS capacitor\u201d with the highest attention score (\u03b1 = 0.30). It is because our attention mechanism takes each previous context of the Japanese phrases \u201c\u512a\u308c\u305f\u6027\u80fd\u201d (\u201cexcellent performance\u201d in English) and \u201c\uff33\uff49\u30c9\u30c3 \u30c8 \uff2d\uff2f\uff33 \u30b3\u30f3\u30c7\u30f3\u30b5\u201d (\u201cSi dot MOS capacitor\u201d in English) into account and softly aligned the target words with the whole phrase when translating the English verb \u201cshowed\u201d and the preposition \u201cof\u201d. Our proposed model can thus flexibly learn the attentional relations between English and Japanese.\nWe observed that our model translated the word \u201cactive\u201d into \u201c\u6d3b\u6027\u201d, a synonym of the reference word \u201c\u30a2\u30af\u30c6\u30a3\u30d6\u201d. We also found similar ex-\namples in other sentences, where our model outputs synonyms of the reference words, e.g. \u201c\u5973\u201d and \u201c\u5973\u6027\u201d (\u201cfemale\u201d in English) and \u201cNASA\u201d and \u201c\u822a\u7a7a\u5b87\u5b99\u5c40\u201d (\u201cNational Aeronautics and Space Administration\u201d in English). These translations are penalized in terms of BLEU scores, but they do not necessarily mean that the translations were wrong. This point may be supported by the fact that the NMT models were highly evaluated in WAT\u201915 by crowd sourcing (Nakazawa et al., 2015)."}, {"heading": "6 Related Work", "text": "Kalchbrenner and Blunsom (2013) were the first to propose an end-to-end NMT model using Convolutional Neural Networks (CNNs) as the source encoder and using RNNs as the target decoder. The Encoder-Decoder model can be seen as an extension of their model, and it replaces the CNNs with RNNs using GRUs (Cho et al., 2014b) or LSTMs (Sutskever et al., 2014).\nSutskever et al. (2014) have shown that making the input sequences reversed is effective in a French-to-English translation task, and the technique has also proven effective in translation tasks between other European language pairs (Luong et al., 2015a). All of the NMT models mentioned above are based on sequential encoders. To incorporate structural information into the NMT models, Cho et al. (2014a) proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation performance. These studies motivated us to investigate the role of syntactic structures explicitly given by existing syntactic parsers in the NMT models.\nThe attention mechanism (Bahdanau et al., 2015) has promoted NMT onto the next stage. It\nenables the NMT models to translate while aligning the target with the source. Luong et al. (2015a) refined the attention model so that it can dynamically focus on local windows rather than the entire sentence. They also proposed a more effective attentional path in the calculation of ANMT models. Subsequently, several ANMT models have been proposed (Cheng et al., 2016; Cohn et al., 2016); however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages."}, {"heading": "7 Conclusion", "text": "In this paper, we propose a novel syntactic approach that extends attentional NMT models. We focus on the phrase structure of the input sentence and build a tree-based encoder following the parsed tree. Our proposed tree-based encoder is a natural extension of the sequential encoder model, where the leaf units of the tree-LSTM in the encoder can work together with the original sequential LSTM encoder. Moreover, the attention mechanism allows the tree-based encoder to align not only the input words but also input phrases with the output words. Experimental results on the WAT\u201915 English-to-Japanese translation dataset demonstrate that our proposed model achieves the best RIBES score and outperforms the sequential attentional NMT model."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their constructive comments and suggestions. This work was supported by CREST, JST, and JSPS KAKENHI Grant Number 15J12597."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation", "author": ["Cheng et al.2016] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "In Proceedings of the 25th International Joint Con-", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Cho et al.2014a] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "In Proceedings of Eighth Workshop on Syntax,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Transla", "author": ["Cho et al.2014b] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["Cohn et al.2016] Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": null, "citeRegEx": "Cohn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association", "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Learning to Forget: Continual Prediction with LSTM", "author": ["Gers et al.2000] Felix A. Gers", "J\u00fcrgen Schmidhuber", "Fred A. Cummins"], "venue": "Neural Computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Automatic Evaluation of Translation Quality", "author": ["Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada"], "venue": null, "citeRegEx": "Isozaki et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies", "author": ["Ji et al.2016] Shihao Ji", "S.V.N. Vishwanathan", "Nadathur Satish", "Michael J. Anderson", "Pradeep Dubey"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "An Empirical Exploration of Recurrent Network Architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "NAVER Machine Translation System for WAT 2015", "author": ["Lee et al.2015] Hyoung-Gyu Lee", "JaeSong Lee", "JunSeok Kim", "Chang-Ki Lee"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Liu et al.2006] Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["Luong et al.2015a] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Luong et al.2015b] Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computa-", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Feature Forest Models for Probabilistic HPSG Parsing", "author": ["Miyao", "Tsujii2008] Yusuke Miyao", "Jun\u2019ichi Tsujii"], "venue": "Computational Linguistics,", "citeRegEx": "Miyao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Miyao et al\\.", "year": 2008}, {"title": "On the elements of an accurate treeto-string machine translation system", "author": ["Neubig", "Duh2014] Graham Neubig", "Kevin Duh"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Neubig et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2014}, {"title": "Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis", "author": ["Neubig et al.2011] Graham Neubig", "Yosuke Nakata", "Shinsuke Mori"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational", "citeRegEx": "Neubig et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2011}, {"title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Understanding the exploding gradient problem", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Overcoming the curse of sentence length for neural machine translation using automatic segmentation", "author": ["Dzmitry Bahdanau", "Bart van Merrienboer", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pouget.Abadie et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pouget.Abadie et al\\.", "year": 2014}, {"title": "Syntactic Theory: A Formal Introduction. Center for the Study of Language and Information, Stanford, 2nd edition", "author": ["Sag et al.2003] Ivan A. Sag", "Thomas Wasow", "Emily Bender"], "venue": null, "citeRegEx": "Sag et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sag et al\\.", "year": 2003}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long ShortTerm Memory Networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Compu-", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "A syntax-based statistical translation model", "author": ["Yamada", "Knight2001] Kenji Yamada", "Kevin Knight"], "venue": "In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Yamada et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2001}, {"title": "Evaluating Neural Machine Translation in English-Japanese Task", "author": ["Zhongyuan Zhu"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Zhu.", "year": 2015}, {"title": "Multi-Source Neural Translation", "author": ["Zoph", "Knight2016] Barret Zoph", "Kevin Knight"], "venue": "In Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "In the Encoder-Decoder model (Cho et al., 2014b; Sutskever et al., 2014), a Recurrent Neural Network (RNN) called the encoder reads the whole sequence of source words to produce a fixedlength vector, and then another RNN called the decoder generates the target words from the vector.", "startOffset": 29, "endOffset": 72}, {"referenceID": 0, "context": "The Encoder-Decoder model has been extended with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a), which allows the model to jointly learn the soft alignment between the source language and the target language.", "startOffset": 72, "endOffset": 116}, {"referenceID": 14, "context": "In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment (Yamada and Knight, 2001) and translation accuracy (Liu et al., 2006; Neubig and Duh, 2014).", "startOffset": 176, "endOffset": 216}, {"referenceID": 26, "context": "NMT is an end-to-end approach to data-driven machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 65, "endOffset": 144}, {"referenceID": 0, "context": "NMT is an end-to-end approach to data-driven machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 65, "endOffset": 144}, {"referenceID": 6, "context": "We employ Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) in place of vanilla RNN units.", "startOffset": 46, "endOffset": 99}, {"referenceID": 0, "context": "The NMT models with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a) have been proposed to softly align each decoder state with the encoder states.", "startOffset": 43, "endOffset": 87}, {"referenceID": 0, "context": "The NMT models with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a) have been proposed to softly align each decoder state with the encoder states. The attention mechanism allows the NMT models to explicitly quantify how much each encoder state contributes to the word prediction at each time step. In the attentional NMT model in Luong et al. (2015a), at the j-th step of the decoder process, the attention score \u03b1j(i) between the i-th source hidden unit hi and the j-th target hidden unit sj is calculated as follows:", "startOffset": 44, "endOffset": 371}, {"referenceID": 25, "context": "In Head-driven Phrase Structure Grammar (HPSG) (Sag et al., 2003), a sentence is composed of multiple phrase units and represented as a binary tree as shown in Figure 1.", "startOffset": 47, "endOffset": 65}, {"referenceID": 27, "context": "Each non-leaf node is also represented with an LSTM unit, and we employ Tree-LSTM (Tai et al., 2015) to calculate the LSTM unit of the parent node which has two child LSTM units.", "startOffset": 82, "endOffset": 100}, {"referenceID": 27, "context": "Our proposed tree-based encoder is a natural extension of the conventional sequential encoder, since Tree-LSTM is a generalization of chainstructured LSTM (Tai et al., 2015).", "startOffset": 155, "endOffset": 173}, {"referenceID": 15, "context": "Luong et al. (2015a) showed that the input-feeding approach improves BLEU scores.", "startOffset": 0, "endOffset": 21}, {"referenceID": 10, "context": "In order to reduce the training cost of the NMT models at the softmax layer, we employ BlackOut (Ji et al., 2016), a sampling-based approximation method.", "startOffset": 96, "endOffset": 113}, {"referenceID": 17, "context": "The negative samples are drawn from the unigram distribution raised to the power \u03b2 \u2208 [0, 1] (Mikolov et al., 2013).", "startOffset": 92, "endOffset": 114}, {"referenceID": 10, "context": "The advantages of Blackout over the other methods are discussed in Ji et al. (2016). Note that BlackOut can be used as the original softmax once the training is finished.", "startOffset": 67, "endOffset": 84}, {"referenceID": 20, "context": ", Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT\u201915.", "startOffset": 26, "endOffset": 47}, {"referenceID": 26, "context": "1 Following Zhu (2015), we extracted the first 1.", "startOffset": 12, "endOffset": 23}, {"referenceID": 23, "context": "0 to avoid exploding gradient problems (Pascanu et al., 2012).", "startOffset": 39, "endOffset": 61}, {"referenceID": 10, "context": "4 as recommended by Ji et al. (2016). Following J\u00f3zefowicz et al.", "startOffset": 20, "endOffset": 37}, {"referenceID": 10, "context": "4 as recommended by Ji et al. (2016). Following J\u00f3zefowicz et al. (2015), we initialize the forget gate biases of LSTM and Tree-LSTM with 1.", "startOffset": 20, "endOffset": 73}, {"referenceID": 24, "context": "The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014).", "startOffset": 146, "endOffset": 193}, {"referenceID": 2, "context": "The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014). In our preliminary experiments, the beam search with the length normalization in Cho et al. (2014a) was not effective in English-to-Japanese translation.", "startOffset": 147, "endOffset": 295}, {"referenceID": 2, "context": "The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence (Cho et al., 2014a; Pouget-Abadie et al., 2014). In our preliminary experiments, the beam search with the length normalization in Cho et al. (2014a) was not effective in English-to-Japanese translation. The method in Pouget-Abadie et al. (2014) needs to estimate the conditional probability p(x|y) using another NMT model and thus is not suitable for our work.", "startOffset": 147, "endOffset": 391}, {"referenceID": 9, "context": "We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al.", "startOffset": 67, "endOffset": 89}, {"referenceID": 22, "context": ", 2010) and BLEU (Papineni et al., 2002) following WAT\u201915.", "startOffset": 17, "endOffset": 40}, {"referenceID": 9, "context": "We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al., 2002) following WAT\u201915. We used the KyTea-based evaluation script for the translation results.6 The RIBES score is a metric based on rank correlation coefficients with word precision, and the BLEU score is based on n-gram word precision and a Brevity Penalty (BP) for outputs shorter than the references. RIBES is known to have stronger correlation with human judgements than BLEU in translation between English and Japanese as discussed in Isozaki et al. (2010).", "startOffset": 68, "endOffset": 580}, {"referenceID": 26, "context": "As to the results of the ANMT model, reversing the word order in the input sentence decreases the scores in English-to-Japanese translation, which contrasts with the results of other language pairs reported in previous work (Sutskever et al., 2014; Luong et al., 2015a).", "startOffset": 224, "endOffset": 269}, {"referenceID": 0, "context": "Comparison with the NMT models The model of Zhu (2015) is an ANMT model (Bahdanau et al., 2015) with a bi-directional LSTM encoder, and uses 1024-dimensional hidden units and 1000-", "startOffset": 72, "endOffset": 95}, {"referenceID": 28, "context": "Comparison with the NMT models The model of Zhu (2015) is an ANMT model (Bahdanau et al.", "startOffset": 44, "endOffset": 55}, {"referenceID": 29, "context": "We found two sentences which ends without eos with d = 512, and then we decoded it again with the beam size of 1000 following Zhu (2015). Our ensemble model yields a METEOR (Denkowski and Lavie, 2014) score of 53.", "startOffset": 126, "endOffset": 137}, {"referenceID": 29, "context": "ANMT with LSTMs (Zhu, 2015) 79.", "startOffset": 16, "endOffset": 27}, {"referenceID": 13, "context": "21 3 pre-reordered ensembles ANMT with GRUs (Lee et al., 2015) 81.", "startOffset": 44, "endOffset": 62}, {"referenceID": 21, "context": "58 + ANMT Rerank (Neubig et al., 2015) 81.", "startOffset": 17, "endOffset": 38}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings.", "startOffset": 13, "endOffset": 31}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.", "startOffset": 13, "endOffset": 306}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)\u2019s best system which is a hybrid of the ANMT and SMT models by +1.", "startOffset": 13, "endOffset": 508}, {"referenceID": 13, "context": "The model of Lee et al. (2015) is also an ANMT model with a bidirectional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)\u2019s end-to-end NMT model with ensemble and unknown replacement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)\u2019s best system which is a hybrid of the ANMT and SMT models by +1.54 RIBES and by +0.74 BLEU scores and Lee et al. (2015)\u2019s ANMT system with special character-based decoding by +1.", "startOffset": 13, "endOffset": 629}, {"referenceID": 19, "context": "The best model in WAT\u201915 is Neubig et al. (2015)\u2019s treeto-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder.", "startOffset": 28, "endOffset": 49}, {"referenceID": 19, "context": "The best model in WAT\u201915 is Neubig et al. (2015)\u2019s treeto-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder. Our proposed end-to-end NMT model compares favorably with Neubig et al. (2015).", "startOffset": 28, "endOffset": 223}, {"referenceID": 26, "context": ", 2014b) or LSTMs (Sutskever et al., 2014).", "startOffset": 18, "endOffset": 42}, {"referenceID": 0, "context": "The attention mechanism (Bahdanau et al., 2015) has promoted NMT onto the next stage.", "startOffset": 24, "endOffset": 47}, {"referenceID": 1, "context": "Subsequently, several ANMT models have been proposed (Cheng et al., 2016; Cohn et al., 2016); however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages.", "startOffset": 53, "endOffset": 92}, {"referenceID": 4, "context": "Subsequently, several ANMT models have been proposed (Cheng et al., 2016; Cohn et al., 2016); however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages.", "startOffset": 53, "endOffset": 92}, {"referenceID": 0, "context": "The Encoder-Decoder model can be seen as an extension of their model, and it replaces the CNNs with RNNs using GRUs (Cho et al., 2014b) or LSTMs (Sutskever et al., 2014). Sutskever et al. (2014) have shown that making the input sequences reversed is effective in a French-to-English translation task, and the technique has also proven effective in translation tasks between other European language pairs (Luong et al.", "startOffset": 117, "endOffset": 195}, {"referenceID": 0, "context": "The Encoder-Decoder model can be seen as an extension of their model, and it replaces the CNNs with RNNs using GRUs (Cho et al., 2014b) or LSTMs (Sutskever et al., 2014). Sutskever et al. (2014) have shown that making the input sequences reversed is effective in a French-to-English translation task, and the technique has also proven effective in translation tasks between other European language pairs (Luong et al., 2015a). All of the NMT models mentioned above are based on sequential encoders. To incorporate structural information into the NMT models, Cho et al. (2014a) proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation performance.", "startOffset": 117, "endOffset": 577}, {"referenceID": 0, "context": "The attention mechanism (Bahdanau et al., 2015) has promoted NMT onto the next stage. It enables the NMT models to translate while aligning the target with the source. Luong et al. (2015a) refined the attention model so that it can dynamically focus on local windows rather than the entire sentence.", "startOffset": 25, "endOffset": 189}], "year": 2016, "abstractText": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT\u201915 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.", "creator": "LaTeX with hyperref package"}}}