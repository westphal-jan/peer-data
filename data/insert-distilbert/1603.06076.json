{"id": "1603.06076", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", "abstract": "detecting hypernymy relations is likely a key task in nlp, which is addressed in the literature using two complementary approaches. distributional methods, whose supervised variants are possibly the current best performers, and path - based methods who receive less research attention. we suggest an improved path - based filter algorithm, in which the longest dependency paths are encoded using a recurrent neural network, and achieve results comparable to distributional methods. we may then extend the approach to significantly integrate both path - based tasks and local distributional memory signals, similarly significantly improving the state - of - the - art on computing this task.", "histories": [["v1", "Sat, 19 Mar 2016 10:09:53 GMT  (250kb,D)", "http://arxiv.org/abs/1603.06076v1", null], ["v2", "Tue, 24 May 2016 16:07:40 GMT  (200kb,D)", "http://arxiv.org/abs/1603.06076v2", "ACL 2016"], ["v3", "Tue, 7 Jun 2016 10:09:43 GMT  (200kb,D)", "http://arxiv.org/abs/1603.06076v3", "ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vered shwartz", "yoav goldberg", "ido dagan"], "accepted": true, "id": "1603.06076"}, "pdf": {"name": "1603.06076.pdf", "metadata": {"source": "CRF", "title": "Improving Hypernymy Detection with an Integrated Path-based and Distributional Method", "authors": ["Vered Shwartz", "Yoav Goldberg", "Ido Dagan"], "emails": ["vered1986@gmail.com", "yoav.goldberg@gmail.com", "dagan@cs.biu.ac.il"], "sections": [{"heading": null, "text": "Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods who receive less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, and achieve results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving the state-of-the-art on this task."}, {"heading": "1 Introduction", "text": "Hypernymy is an important lexical-semantic relation for NLP tasks. For instance, knowing that Tom Cruise is an actor can help a question answering system answer the question \u201cwhich actors are involved in Scientology?\u201d. While semantic taxonomies, like WordNet (Fellbaum, 1998), define hypernymy relations between word types, they are limited in scope and domain. Therefore, automated methods have been developed to identify, for a given term-pair (x, y), whether y is an hypernym of x, based on their occurrences in a large corpus.\nFor a couple of decades, this task has been addressed by two types of approaches: distributional, and path-based. In distributional methods, the decision whether y is a hypernym of x is based on the distributional representations of these terms. Lately, with the popularity of word embeddings (Mikolov et al., 2013), most focus has shifted towards supervised distributional methods, in which\neach (x, y) term-pair is represented using some combination of the terms\u2019 embedding vectors.\nIn contrast to distributional methods, in which the decision is based on the separate contexts of x and y, path-based methods base the decision on the lexico-syntactic paths connecting the joint occurrences of x and y in a corpus. Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features.\nUsing individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, \u201cSpelt is a species of wheat\u201d and \u201cFantasy is a genre of fiction\u201d yield two different paths: X be species of Y and X be genre of Y, while both indicating that X is-a Y. A possible solution is to generalize paths by replacing words along the path with their part-of-speech tags or with wild cards, as done in PATTY (Nakashole et al., 2012).\nOverall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based methods: they require that the terms of the pair occur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hypernymy, and perform best on detecting broad semantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008).\nIn this paper, we propose an integrated pathbased and distributional method for hypernymy detection. Inspired by recent progress in relation\nar X\niv :1\n60 3.\n06 07\n6v 1\n[ cs\n.C L\n] 1\n9 M\nar 2\n01 6\nclassification, we use a long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) to encode dependency paths. In order to create enough training data for our network, we followed previous methodology of constructing a dataset based on knowledge resources.\nWe first show that our path-based approach, on its own, substantially improves performance over prior path-based methods, yielding performance comparable to state-of-the-art distributional methods. Our analysis suggests that the neural path representation enables better generalizations. While coarse-grained generalizations, such as replacing a word by its POS tag, capture mostly syntactic similarities between paths, our method captures also semantic similarities.\nWe then show that we can easily integrate distributional signals in the network. The integration results confirm that the distributional and pathbased signals indeed provide complementary information, with the combined model yielding an improvement of up to 14 F1 points over each individual model."}, {"heading": "2 Background", "text": "We introduce the two main approaches for hypernymy detection: distributional (section 2.1), and path-based (section 2.2). We then discuss the recent use of recurrent neural networks in the related task of relation classification (section 2.3)."}, {"heading": "2.1 Distributional Methods", "text": "Hypernymy detection is commonly addressed using distributional methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs separately in the corpus.\nEarlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010). This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hyponyms.\nMore recently, the focus of the distributional ap-\nproach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term\u2019s embeddings vector: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y. Based on neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014)."}, {"heading": "2.2 Path-based Methods", "text": "A different approach to detecting hypernymy between a pair of terms (x, y) considers the lexicosyntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.g. Y such as X, X and other Y).\nIn a later work, Snow et al. (2004) learned to detect hypernymy. Rather than searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths.\nPaths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010).\nA major limitation in relying on lexicosyntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lexical level, generalizing such variations into more abstract paths can increase recall. The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxonomy of term relations from free text. For each path, they added generalized versions in which a\nsubset of words along the path were replaced by either their POS tags, their ontological types or wild-cards. This generalization increased recall while maintaining the same level of precision."}, {"heading": "2.3 RNNs for Relation Classification", "text": "Relation classification is a related task whose goal is to classify the relation that is expressed between two target terms in a given sentence, to one of predefined relation classes. To illustrate, consider the following sentence, from the SemEval-2010 relation classification task dataset (Hendrickx et al., 2009): \u201cThe [apples]e1 are in the [basket]e2\u201d. Here, the relation expressed between the target entities is Content\u2212 Container(e1, e2).\nThe shortest dependency paths between the target entities were shown to be informative for this task (Fundel et al., 2007). Recently, deep learning techniques showed good performance in capturing the indicative information in such paths.\nIn particular, several papers show improved performance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. Xu et al. (2015; 2016) apply a separate long shortterm memory (LSTM) network to each sequence of words, POS tags, dependency labels and WordNet hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the input of a network that predicts the classification. Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015).\nWhile relation classification and hypernymy detection are both concerned with identifying semantic relations that hold for pairs of terms, they differ in a major respect. In relation classification, the relation should be expressed in the given text, while in hypernymy detection, the goal is to recognize a generic lexical-semantic relation between terms, that holds in many contexts. Accordingly, in relation classification, a term-pair is represented by a single dependency path, while in hypernymy detection, it is represented by the multiset of all\ndependency paths in which they co-occur in the corpus."}, {"heading": "3 LSTM-based Hypernymy Detection", "text": "We present an LSTM-based method for hypernymy detection. We first focus on improving path representation (section 3.1), and then integrate distributional signals into our network, creating a combined method (section 3.2)."}, {"heading": "3.1 Path-based Network", "text": "Similarly to prior work, we represent each dependency path as a sequence of edges that leads from x to y in the dependency tree.1 Each edge contains the lemma and part-of-speech tag of the source node, the dependency label, and the edge direction between two subsequent nodes. For readability, we denote each edge as lemma/POS/dep, placing the direction signs between two subsequent edges. See figure 1 for an illustration.\nRather than treating an entire dependency path as a single feature, we encode the sequence of edges using a long short-term memory (LSTM) network. The vectors obtained for the different paths of a given (x, y) pair are pooled, and the resulting vector is used for classification. Figure 2 depicts the overall network structure, which is described below.\nEdge Representation We represent each edge by the concatenation of its components\u2019 vectors:\n~ve = [~vl, ~vpos, ~vdep, ~vdir]\nwhere ~vl, ~vpos, ~vdep, ~vdir represent the embedding vectors of the lemma, part-of-speech, dependency label and dependency direction (along the path from x to y), respectively.\nPath Representation For a path p composed of edges e1, ..., ek, the edge vectors ~ve1 , ..., ~vek are fed in order to an LSTM encoder, resulting in a vector ~op representing the entire path p. The LSTM architecture is effective at capturing temporal patterns in sequences. We expect the training procedure to drive the LSTM encoder to focus on parts of the path that are more informative for the classification task while ignoring others.\n1Like Snow et al. (2004), we added for each path, additional paths containing single daughters of x or y not already contained in the path, to include paths such as Such Y as X.\nTerm-Pair Classification Each (x, y) term-pair is represented by the multiset of lexico-syntactic paths that connected x and y in the corpus, denoted as paths(x, y), while the supervision is given for the term pairs. We represent each (x, y) term-pair as the weighted-average of its path vectors, by applying average pooling on its path vectors, as follows:\n~vxy = ~vpaths(x,y) = \u2211 p\u2208paths(x,y) fp,(x,y)\u00b7 ~op\u2211 p\u2208paths(x,y) fp(x,y) (1)\nwhere fp,(x,y) is the frequency of p in paths(x, y). We then feed this path vector to a single-layer network that performs binary classification to decide whether y is a hypernym of x.\nc = softmax(W \u00b7 ~vxy) (2)\nc is a 2-dimensional vector whose components sum to 1, and we classify a pair as positive if c[1] > 0.5.\nImplementation Details To train the network, we used PyCNN2. We minimize the cross entropy loss using gradient-based optimization, with mini-batches of size 10 and the Adam update rule (Kingma and Ba, 2014). Regularization is applied by a dropout on each of the components\u2019 embeddings. We tuned the hyper-parameters (learning rate and dropout rate) on the validation set. Table 3 displays the chosen hyper-parameters values.\nWe initialized the lemma embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on\n2https://github.com/clab/cnn\nWikipedia. The other embeddings, as well as outof-vocabulary lemmas, are initialized randomly. We update all embedding vectors during training."}, {"heading": "3.2 Integrated Network", "text": "The network presented in section 3.1 classifies each (x, y) term-pair based on the paths that connect x and y in the corpus. Our goal was to improve upon previous path-based methods for hypernymy detection, and we show in section 6 that our network indeed outperforms them. Yet, as path-based and distributional methods are considered complementary, we present a simple way to integrate distributional features in the network, yielding improved performance.\nWe extended the network to take into account distributional information on each term. Inspired by the supervised distributional concatenation method (Baroni et al., 2012), we simply concatenate x and y word embeddings to the (x, y) feature vector, redefining ~vxy:\n~vxy = [ ~vwx , ~vpaths(x,y), ~vwy ] (3)\nwhere ~vwx and ~vwy are x and y\u2019s word embeddings, respectively, and ~vpaths(x,y) is the averaged path vector defined in equation 1. This way, each (x, y) pair is represented using both the distributional features of x and y, and their path-based features."}, {"heading": "4 Dataset", "text": ""}, {"heading": "4.1 Creating Instances", "text": "Neural networks typically require a large amount of training data, whereas the existing hypernymy datasets, like BLESS (Baroni and Lenci, 2011),\nare relatively small. Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrandec\u030cic\u0301, 2012) and Yago (Suchanek et al., 2007).\nAll instances in our dataset, both positive and negative, are pairs of terms that are directly related in at least one of the resources. These resources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015).\nTerm-pairs related by other relations (including hyponymy), are considered as negative instances. Using related rather than random term-pairs as negative instances tests our method\u2019s ability to distinguish between hypernymy and other kinds of semantic relatedness. We maintain a ratio of 1:4 positives to negatives in the dataset.\nLike Snow et al. (2004), we include only termpairs that have joint occurrences in the corpus, requiring at least two different dependency paths for each pair."}, {"heading": "4.2 Random and Lexical Dataset Splits", "text": "As our primary dataset, we perform standard random splitting, with 70% train, 25% test and 5% validation sets.\nAs pointed out by Levy et al. (2015), supervised distributional lexical inference methods tend to perform \u201clexical memorization\u201d, i.e., instead of learning a relation between the two terms, they\nmostly learn an independent property of a single term in the pair: whether it is a \u201cprototypical hypernym\u201d or not. For instance, if the training set contains term-pairs such as (dog, animal), (cat, animal), and (cow, animal), all annotated as positive examples, the algorithm may learn that animal is a prototypical hypernym, classifying any new (x, animal) pair as positive, regardless of the relation between x and animal. Levy et al. (2015) suggested to split the train and test sets such that each will contain a distinct vocabulary (\u201clexical split\u201d), in order to prevent the model from overfitting by lexical memorization.\nTo investigate such behaviors, we present results also for a lexical split of our dataset. In this case, we split the train, test and validation sets such that each contains a distinct vocabulary. We note that this differs from Levy et al. (2015), who split only the train and the test sets, and dedicated a subset of the train for validation. We chose to deviate from Levy et al. (2015) because we noticed that when the validation set contains terms from the train set, the model is rewarded of lexical memorization when tuning the hyper-parameters, consequently yielding suboptimal performance on the lexically-distinct test set. When each set has a distinct vocabulary, the hyper-parameters are tuned to avoid lexical memorization and are likely to perform better on the test set. We tried to keep roughly the same 70/25/5 ratio in our lexical split.3 The sizes of the two datasets are shown in Table 2. Indeed, training a model on a lexically split dataset may result in a more general model, that can better handle pairs consisting of two unseen terms during inference. However, we argue that in the common applied scenario, the inference involves an unseen pair (x, y), in which x and/or y have already been observed separately. Models trained on a random split may introduce the model with a term\u2019s \u201cprior probability\u201d of being a hypernym or a hyponym, and this information can be exploited beneficially at inference time."}, {"heading": "5 Baselines", "text": "We compare our method with several state-of-theart methods for hypernymy detection, as described in section 2: path-based methods (section 5.1), and distributional methods (section 5.2). Due to different works using different datasets and corpora, we\n3The lexical split discards many pairs consisting of crossset terms.\nreplicated the baselines rather than comparing to the reported results.\nWe use the Wikipedia dump from May 2015 as the underlying corpus of all the methods, and parse it using spaCy4. We perform model selection on the validation set to tune the hyper-parameters of each method.5 The best hyper-parameters are reported in Table 3."}, {"heading": "5.1 Path-based Methods", "text": "Snow We follow the original paper, and extract all shortest paths of four edges or less between terms in a dependency tree. Like Snow et al. (2004), we add paths with \u201csatellite edges\u201d, i.e., single words not already contained in the dependency path, which are connected to either X or Y, allowing paths like such Y as X. The number of distinct paths was 324,578. We apply \u03c72 feature selection to keep only the 100,000 most informative paths and train a logistic regression classifier.\nGeneralization We also compare our method to a baseline that uses generalized dependency paths. Following PATTY\u2019s (Nakashole et al., 2012) approach to generalizing paths, we replace edges with their part-of-speech tags as well as with wild cards. We generate the powerset of all possible generalizations, including the original paths. See Table 4 for examples. The number of features after generalization went up to 2,093,220. Similarly to the first baseline, we apply feature selection, this time keeping the 1,000,000 most informative paths, and train a logistic regression classifier over the generalized paths.6\n4https://spacy.io/ 5We applied grid search for a range of values, and picked the ones that yield the highest F1 score on the validation set. 6We also tried keeping the 100,000 most informative paths, but the performance was worse."}, {"heading": "5.2 Distributional Methods", "text": "Unsupervised SLQS (Santus et al., 2014) is an entropy-based measure for hypernymy detection, reported to outperform previous state-of-the-art unsupervised methods (Weeds and Weir, 2003; Kotlerman et al., 2010). Following the original paper, we set the number of each term\u2019s most associated contexts to N=50. We use the validation set to tune the threshold for classifying a pair as positive. As our low results suggest, while this method is state-of-the-art for unsupervised hypernymy detection, it is basically designed for classifying specificity level of related terms, rather than hypernymy in particular.\nSupervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y. We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al. (2015) to perform best in this setting. We perform model selection on the validation set to select the best word vectors, method and regularization factor (see Table 3)."}, {"heading": "6 Results", "text": "Table 5 displays performance scores of our method and the baselines. LSTM is our path-based recurrent neural network model (section 3.1) and LSTM-Integrated is our combined method (section 3.2). Comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of precision, reassessing the behavior found in Nakashole et al. (2012). Our LSTM-based method outperforms both path-based baselines by a significant improvement in recall and with slightly lower precision. The recall boost is due to better path gen-\neralization, as demonstrated in section 7.1. Regarding distributional methods, the unsupervised SLQS baseline performed worse on our dataset. The low precision stems from its inability to distinguish between hypernyms and meronyms, which are common in our dataset, causing many false positive pairs such as (zabrze, poland) and (kibbutz, israel). Out of a sample of 50 false positive pairs, 38% were holonym-meronym pairs.\nIn accordance with previously reported results, the supervised embedding-based method is the best performing baseline on our dataset as well. Our purely path-based method performs slightly better, achieving state-of-the-art results. Adding distributional features to our method shows that these two approaches are indeed complementary. On both dataset splits, the performance differences between the integrated method and our pure pathbased method, as well as the supervised distributional method, are substantial, and statistically significant with p-value of 1% (paired t-test).\nWe also reassess that indeed supervised distributional methods perform worse on a lexical split (Levy et al., 2015). We further observe a similar reduction when using the LSTM-based methods, which is not a result of lexical memorization, but rather stems from over-generalization (section 7.1)."}, {"heading": "7 Analysis", "text": ""}, {"heading": "7.1 Qualitative Analysis of Learned Paths", "text": "We analyze our method\u2019s ability to generalize over path structures, by comparing prominent indicative paths which were learned by each of the pathbased methods. We do so by finding high-scoring paths that contributed to the classification of truepositive pairs in the dataset. In the path-based baselines, these are the highest-weighted features as learned by the logistic regression classifier. In the LSTM-based method, it is less straightforward to identify the most indicative paths. We assess the contribution of a certain path p to classification by\nregarding it as the only path that appeared for the term-pair, and compute its TRUE label score from the class distribution: softmax(W \u00b7 ~vxy)[1], setting ~vxy = [~0, ~op,~0].\nA notable pattern is that Snow\u2019s method learns specific paths, like X is Y from (e.g. Megadeth is an American thrash metal band from Los Angeles). While Snow\u2019s method can only rely on verbatim paths, limiting its recall, the generalized version of Snow often makes coarse generalizations, such as X VERB Y from. Clearly, such a path is too general, and almost any verb assigned to it results in a non-indicative path (e.g. X take Y from). Efforts by the learning method to avoid such generalization, again, lower the recall. Our method provides a better midpoint, making finegrained generalizations by learning additional semantically similar paths such as X become Y from and X remain Y from. See table 6 for additional example paths which illustrate these behaviors.\nWe also noticed that while on the random split our model learns a range of specific paths such as X is Y published (learned for e.g. Y=magazine) and X is Y produced (Y=film), in the lexical split it only learns the general X is Y path for these relations. We note that X is Y is a rather \u201cnoisy\u201d path, which may occur in ad-hoc contexts without indicating generic hypernymy relations (e.g. chocolate is a big problem in the context of children\u2019s health). While such a model may identify hypernymy relations between unseen terms, based on general paths, it is prone to over-generalization, hurting its performance, as seen in Table 5. As discussed in section 4.2, we suspect that this scenario, in which both terms are unseen, is usually not common enough to justify this limiting training setup."}, {"heading": "7.2 Error Analysis", "text": "False Positive We categorized the false positive pairs on the random split according to the relation holding between each pair of terms in the re-\nsources used to construct the dataset. We grouped several semantic relations from different resources to broad categories, e.g. synonym includes also alias and Wikipedia redirection. Table 7 displays the distribution of semantic relations among false positive pairs.\nMore than 20% of the errors stem from confusing synonymy with hypernymy, which are known to be difficult to distinguish. An additional 30% of the term-pairs are reversed hypernym-hyponym pairs (y is a hyponym of x). Examining a sample of these pairs suggests that they are usually nearsynonyms, i.e., it is not that clear whether one term is truely more general than the other or not. For instance, fiction is annotated in WordNet as a hypernym of story, while our method classified fiction as its hyponym.\nA possible future research direction might be to quite simply extend our network to classify term-pairs simultaneously to multiple semantic relations, as in Pavlick et al. (2015). Such a multiclass model can hopefully better distinguish between these similar semantic relations.\nAnother notable category is hypernymy-like relations: these are other relations in the resources that could also be considered as hypernymy (e.g. occupation), but were annotated as negative due to our restrictive selection of only indisputable hypernymy relations from the resources (see section 4.1).\nLastly, other errors made by the model often correspond to term-pairs that co-occur very few times in the corpus, e.g. xebec, a studio producing Anime, was falsely classified as a hyponym of anime.\nFalse Negative We sampled 50 term-pairs that were falsely annotated as negative, and analyzed the major (overlapping) types of errors (Table 8).\nMost of these pairs had only few co-occurrences in the corpus. This is often either due to infrequent terms (cbc.ca), or a rare sense of x in which the hypernymy relation holds ((night, play)). Such a term-pair may have too few hypernymy-indicating paths, leading to classifying it as negative."}, {"heading": "8 Conclusion", "text": "We presented a neural-networks-based method for hypernymy detection. First, we focused on improving path representation using LSTM, suggesting a path-based model that performs significantly better than prior path-based methods, and matches the previously superior distributional methods. We demonstrated that the increase in recall is a result of generalizing semantically-similar paths, in contrast to prior methods, which either make no generalizations or over-generalize paths.\nWe then extended our network by integrating distributional signals, yielding an improvement of additional 14F1 points, and demonstrating that the path-based and the distributional approaches are indeed complementary.\nFinally, our architecture seems straightforwardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "How we blessed distributional semantic evaluation", "author": ["Baroni", "Lenci2011] Marco Baroni", "Alessandro Lenci"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In EACL,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Evolutionary algorithms for definition extraction", "author": ["Borg et al.2009] Claudia Borg", "Mike Rosner", "Gordon Pace"], "venue": "In Proceedings of the 1st Workshop on Definition Extraction,", "citeRegEx": "Borg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Borg et al\\.", "year": 2009}, {"title": "Toward an architecture for never-ending language learning", "author": ["Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr.", "Tom M Mitchell"], "venue": "In AAAI,", "citeRegEx": "Carlson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Learning semantic hierarchies via word embeddings", "author": ["Fu et al.2014] Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In ACL,", "citeRegEx": "Fu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2014}, {"title": "Relexrelation extraction using dependency parse", "author": ["Fundel et al.2007] Katrin Fundel", "Robert K\u00fcffner", "Ralf Zimmer"], "venue": "trees. Bioinformatics,", "citeRegEx": "Fundel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fundel et al\\.", "year": 2007}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A Hearst"], "venue": "In ACL,", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Using hidden markov random fields to combine distributional and pattern-based word clustering", "author": ["Kaji", "Kitsuregawa2008] Nobuhiro Kaji", "Masaru Kitsuregawa"], "venue": "In COLING,", "citeRegEx": "Kaji et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kaji et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Directional distributional similarity for lexical inference", "author": ["Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet"], "venue": null, "citeRegEx": "Kotlerman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "A semi-supervised method to learn and construct taxonomies using the web", "author": ["Kozareva", "Hovy2010] Zornitsa Kozareva", "Eduard Hovy"], "venue": "In EMNLP,", "citeRegEx": "Kozareva et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kozareva et al\\.", "year": 2010}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "An informationtheoretic definition of similarity", "author": ["Dekang Lin"], "venue": "In ICML,", "citeRegEx": "Lin.,? \\Q1998\\E", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "A dependency-based neural network for relation classification", "author": ["Liu et al.2015] Yang Liu", "Furu Wei", "Sujian Li", "Heng Ji", "Ming Zhou", "Houfeng Wang"], "venue": "arXiv preprint arXiv:1507.04646", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating pattern-based and distributional similarity methods for lexical entailment acquisition", "author": ["Ido Dagan", "Maayan Geffet"], "venue": "In COLING and ACL,", "citeRegEx": "Mirkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mirkin et al\\.", "year": 2006}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In EMNLP and CoNLL,", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Learning word-class lattices for definition and hypernym extraction", "author": ["Navigli", "Velardi2010] Roberto Navigli", "Paola Velardi"], "venue": "In ACL,", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Combining neural networks and log-linear models to improve relation extraction", "author": ["Nguyen", "Grishman2015] Thien Huu Nguyen", "Ralph Grishman"], "venue": "arXiv preprint arXiv:1511.05926", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Adding semantics to data-driven paraphrasing", "author": ["Johan Bos", "Malvina Nissim", "Charley Beller", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Distributional lexical entailment by topic coherence", "author": ["Laura Rimell"], "venue": "In EACL,", "citeRegEx": "Rimell.,? \\Q2014\\E", "shortCiteRegEx": "Rimell.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Katrin Erk", "Gemma Boleda"], "venue": "In COLING,", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Santus et al.2014] Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte Im Walde"], "venue": "In EACL,", "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Learning to exploit structured resources for lexical inference", "author": ["Omer Levy", "Ido Dagan", "Jacob Goldberger"], "venue": "In CoNLL,", "citeRegEx": "Shwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2015}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Semantic taxonomy induction from heterogenous evidence", "author": ["Snow et al.2006] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "In ACL,", "citeRegEx": "Snow et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2006}, {"title": "Yago: a core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In WWW,", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "A general framework for distributional similarity", "author": ["Weeds", "Weir2003] Julie Weeds", "David Weir"], "venue": "In EMLP,", "citeRegEx": "Weeds et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2003}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "author": ["Weeds et al.2014] Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller"], "venue": "In COLING,", "citeRegEx": "Weeds et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Xu et al.2015] Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Improved relation classification by deep recurrent neural networks with data augmentation", "author": ["Xu et al.2016] Yan Xu", "Ran Jia", "Lili Mou", "Ge Li", "Yunchuan Chen", "Yangyang Lu", "Zhi Jin"], "venue": "arXiv preprint arXiv:1601.03651", "citeRegEx": "Xu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "Lately, with the popularity of word embeddings (Mikolov et al., 2013), most focus has shifted towards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms\u2019 embedding vectors.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "Hearst (1992) identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. Snow et al. (2004) represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hypernymy, based on these features.", "startOffset": 0, "endOffset": 117}, {"referenceID": 19, "context": "tags or with wild cards, as done in PATTY (Nakashole et al., 2012).", "startOffset": 42, "endOffset": 66}, {"referenceID": 18, "context": "Though these approaches seem complementary, there has been rather little work on integrating them (Mirkin et al., 2006; Kaji and Kitsuregawa, 2008).", "startOffset": 98, "endOffset": 147}, {"referenceID": 15, "context": "Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al.", "startOffset": 107, "endOffset": 118}, {"referenceID": 12, "context": "Earlier methods developed unsupervised measures for hypernymy, starting with symmetric similarity measures (Lin, 1998), and followed by directional measures based on the distributional inclusion hypothesis (Weeds and Weir, 2003; Kotlerman et al., 2010).", "startOffset": 206, "endOffset": 252}, {"referenceID": 27, "context": "More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hyponyms.", "startOffset": 17, "endOffset": 52}, {"referenceID": 25, "context": "More recent work (Santus et al., 2014; Rimell, 2014) introduce new measures, based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hyponyms.", "startOffset": 17, "endOffset": 52}, {"referenceID": 2, "context": "Several methods are used to represent term-pairs as a combination of each term\u2019s embeddings vector: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al.", "startOffset": 120, "endOffset": 141}, {"referenceID": 26, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 5, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 33, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7 ~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 17, "context": "Based on neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al.", "startOffset": 32, "endOffset": 79}, {"referenceID": 23, "context": "Based on neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014), these methods are easy to apply, and show good results (Baroni et al.", "startOffset": 32, "endOffset": 79}, {"referenceID": 2, "context": ", 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014).", "startOffset": 64, "endOffset": 106}, {"referenceID": 26, "context": ", 2014), these methods are easy to apply, and show good results (Baroni et al., 2012; Roller et al., 2014).", "startOffset": 64, "endOffset": 106}, {"referenceID": 7, "context": "Automatic acquisition of hypernyms from free text, based on such paths, was first proposed by Hearst (1992), who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.", "startOffset": 94, "endOffset": 108}, {"referenceID": 29, "context": "In a later work, Snow et al. (2004) learned to detect hypernymy.", "startOffset": 17, "endOffset": 36}, {"referenceID": 30, "context": "\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al.", "startOffset": 72, "endOffset": 159}, {"referenceID": 4, "context": "\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al.", "startOffset": 72, "endOffset": 159}, {"referenceID": 24, "context": "\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al., 2006; Kozareva and Hovy, 2010; Carlson et al., 2010; Riedel et al., 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al.", "startOffset": 72, "endOffset": 159}, {"referenceID": 3, "context": ", 2013), analogy identification (Turney, 2006), and definition extraction (Borg et al., 2009; Navigli and Velardi, 2010).", "startOffset": 74, "endOffset": 120}, {"referenceID": 5, "context": "The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance.", "startOffset": 73, "endOffset": 87}, {"referenceID": 5, "context": "The paths identified by this method were shown to subsume those found by Hearst (1992), yielding improved performance. Variations of Snow et al.\u2019s (2004) method were later used in tasks such as taxonomy construction (Snow et al.", "startOffset": 73, "endOffset": 154}, {"referenceID": 19, "context": "The PATTY algorithm (Nakashole et al., 2012) applied such generalizations for the purpose of acquiring a taxonomy of term relations from free text.", "startOffset": 20, "endOffset": 44}, {"referenceID": 8, "context": "To illustrate, consider the following sentence, from the SemEval-2010 relation classification task dataset (Hendrickx et al., 2009): \u201cThe [apples]e1 are in the [basket]e2\u201d.", "startOffset": 107, "endOffset": 131}, {"referenceID": 6, "context": "The shortest dependency paths between the target entities were shown to be informative for this task (Fundel et al., 2007).", "startOffset": 101, "endOffset": 122}, {"referenceID": 16, "context": "Other papers suggest incorporating additional network architectures to further improve performance (Nguyen and Grishman, 2015; Liu et al., 2015).", "startOffset": 99, "endOffset": 144}, {"referenceID": 29, "context": "Like Snow et al. (2004), we added for each path, additional paths containing single daughters of x or y not already contained in the path, to include paths such as Such Y as X.", "startOffset": 5, "endOffset": 24}, {"referenceID": 23, "context": "We initialized the lemma embeddings with the 50-dimensional pre-trained GloVe word embeddings (Pennington et al., 2014), trained on", "startOffset": 94, "endOffset": 119}, {"referenceID": 2, "context": "Inspired by the supervised distributional concatenation method (Baroni et al., 2012), we simply concatenate x and y word embeddings to the (x, y) feature vector, redefining ~ vxy:", "startOffset": 63, "endOffset": 84}, {"referenceID": 29, "context": "Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013).", "startOffset": 119, "endOffset": 159}, {"referenceID": 24, "context": "Therefore, we followed the common methodology of creating a dataset using distant supervision from knowledge resources (Snow et al., 2004; Riedel et al., 2013).", "startOffset": 119, "endOffset": 159}, {"referenceID": 0, "context": "(2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al., 2007), Wikidata (Vrande\u010di\u0107, 2012) and Yago (Suchanek et al.", "startOffset": 194, "endOffset": 213}, {"referenceID": 31, "context": ", 2007), Wikidata (Vrande\u010di\u0107, 2012) and Yago (Suchanek et al., 2007).", "startOffset": 45, "endOffset": 68}, {"referenceID": 23, "context": ", 2004; Riedel et al., 2013). Following Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fellbaum, 1998), DBPedia (Auer et al.", "startOffset": 8, "endOffset": 59}, {"referenceID": 28, "context": "To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations (Table 1), which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015).", "startOffset": 216, "endOffset": 238}, {"referenceID": 29, "context": "Like Snow et al. (2004), we include only termpairs that have joint occurrences in the corpus, requiring at least two different dependency paths for each pair.", "startOffset": 5, "endOffset": 24}, {"referenceID": 14, "context": "As pointed out by Levy et al. (2015), supervised distributional lexical inference methods tend to perform \u201clexical memorization\u201d, i.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": "As pointed out by Levy et al. (2015), supervised distributional lexical inference methods tend to perform \u201clexical memorization\u201d, i.e., instead of learning a relation between the two terms, they mostly learn an independent property of a single term in the pair: whether it is a \u201cprototypical hypernym\u201d or not. For instance, if the training set contains term-pairs such as (dog, animal), (cat, animal), and (cow, animal), all annotated as positive examples, the algorithm may learn that animal is a prototypical hypernym, classifying any new (x, animal) pair as positive, regardless of the relation between x and animal. Levy et al. (2015) suggested to split the train and test sets such that each will contain a distinct vocabulary (\u201clexical split\u201d), in order to prevent the model from overfitting by lexical memorization.", "startOffset": 18, "endOffset": 639}, {"referenceID": 14, "context": "We note that this differs from Levy et al. (2015), who split only the train and the test sets, and dedicated a subset of the train for validation.", "startOffset": 31, "endOffset": 50}, {"referenceID": 14, "context": "We note that this differs from Levy et al. (2015), who split only the train and the test sets, and dedicated a subset of the train for validation. We chose to deviate from Levy et al. (2015) because we noticed that when the validation set contains terms from the train set, the model is rewarded of lexical memorization when tuning the hyper-parameters, consequently yielding suboptimal performance on the lexically-distinct test set.", "startOffset": 31, "endOffset": 191}, {"referenceID": 27, "context": "5 Distributional SLQS (Santus et al., 2014) N = 50, threshold = 0.", "startOffset": 22, "endOffset": 43}, {"referenceID": 29, "context": "Like Snow et al. (2004), we add paths with \u201csatellite edges\u201d, i.", "startOffset": 5, "endOffset": 24}, {"referenceID": 19, "context": "Following PATTY\u2019s (Nakashole et al., 2012) approach to generalizing paths, we replace edges with their part-of-speech tags as well as with wild cards.", "startOffset": 18, "endOffset": 42}, {"referenceID": 27, "context": "Unsupervised SLQS (Santus et al., 2014) is an entropy-based measure for hypernymy detection, reported to outperform previous state-of-the-art unsupervised methods (Weeds and Weir, 2003; Kotlerman et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 12, "context": ", 2014) is an entropy-based measure for hypernymy detection, reported to outperform previous state-of-the-art unsupervised methods (Weeds and Weir, 2003; Kotlerman et al., 2010).", "startOffset": 131, "endOffset": 177}, {"referenceID": 2, "context": "Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al.", "startOffset": 128, "endOffset": 149}, {"referenceID": 26, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 5, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 33, "context": ", 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y.", "startOffset": 26, "endOffset": 84}, {"referenceID": 17, "context": "We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al.", "startOffset": 45, "endOffset": 92}, {"referenceID": 23, "context": "We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al.", "startOffset": 45, "endOffset": 92}, {"referenceID": 1, "context": "Supervised To represent term-pairs with distributional features, we tried several state-of-the-art methods: concatenation ~x\u2295~y (Baroni et al., 2012), difference ~y\u2212~x (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity ~x \u00b7~y. We downloaded several pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was reported by Levy et al. (2015) to perform best in this setting.", "startOffset": 129, "endOffset": 493}, {"referenceID": 19, "context": "Comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of precision, reassessing the behavior found in Nakashole et al. (2012). Our LSTM-based method outperforms both path-based baselines by a significant improvement in recall and with slightly lower precision.", "startOffset": 160, "endOffset": 184}, {"referenceID": 27, "context": "Distributional SLQS (Santus et al., 2014) 0.", "startOffset": 20, "endOffset": 41}, {"referenceID": 14, "context": "We also reassess that indeed supervised distributional methods perform worse on a lexical split (Levy et al., 2015).", "startOffset": 96, "endOffset": 115}, {"referenceID": 22, "context": "A possible future research direction might be to quite simply extend our network to classify term-pairs simultaneously to multiple semantic relations, as in Pavlick et al. (2015). Such a multiclass model can hopefully better distinguish between these similar semantic relations.", "startOffset": 157, "endOffset": 179}], "year": 2016, "abstractText": "Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods who receive less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, and achieve results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving the state-of-the-art on this task.", "creator": "LaTeX with hyperref package"}}}