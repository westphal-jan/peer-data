{"id": "1603.06653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2016", "title": "Information Theoretic-Learning Auto-Encoder", "abstract": "we formally propose intermediate information theoretic - learning ( itl ) divergence measures for variational regularization of neural domain networks. we propose also explore itl - regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders domains and generative adversarial networks for randomly impulse generating sample data without explicitly defining a partition function. this paper also openly formalizes, generative moment independent matching networks under the itl spectrum framework.", "histories": [["v1", "Tue, 22 Mar 2016 01:05:47 GMT  (441kb)", "http://arxiv.org/abs/1603.06653v1", "8 pages, 4 figures"]], "COMMENTS": "8 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eder santana", "matthew emigh", "jose c principe"], "accepted": false, "id": "1603.06653"}, "pdf": {"name": "1603.06653.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n06 65\n3v 1\n[ cs\n.L G\n] 2\n2 M"}, {"heading": "1 Introduction", "text": "Deep, regularized neural networks work better in practice than shallow unconstrained neural networks [1]. This regularization takes classic forms such as L2-norm ridge regression, L1-norm LASSO, architectural constraints such as convolutional layers [2], but also uses modern techniques such as dropout [3]. Recently, especially in the subfield of autoencoding neural networks, regularization has been accomplished with variational methods [4][5]. In this paper we propose Information Theoretic-Learning [6] divergence measures for variational regularization.\nIn deep learning, variational regularization forces the function implemented by a neural network to be as close as possible to an imposed prior, which is a stronger restriction than that imposed by point-wise regularization methods such as L1 or L2 norms. Variational methods for deep learning were popularized by the variational autoencoder (VAE) framework proposed by [4] and [5] which also brought the attention of deep learning researchers to the reparametrization trick. The Gaussian reparametrization trick works as follows: the encoder (deep) network outputs a mean \u00b5 and a standard deviation \u03c3, from which we sample a latent factor z = \u00b5+\u03c3 \u00b7\u01eb, where \u01eb \u223c N(0, 1). This latent factor is then fed forward to the decoder network and the parameters \u00b5 and \u03c3 are regularized using the KL-divergence KL(N(\u00b5, \u03c3)\u2016N(0, 1)) between the inferred distribution and the imposed prior, which has a simple form [4]. After training, one can generate data from a VAE by first sampling from the Gaussian prior distribution and feeding it to the VAE\u2019s decoder. This is an approach similar to the inverse cumulative distribution method and does not involve estimation of the partition function, rejection sampling, or other complicated approaches [7]. VAE\u2019s methodology has been successfully extended to convolutional autoencoders [8] and more elaborate architectures such as Laplacian pyramids for image generation [9].\nUnfortunately, VAE cannot be used when there does not exist a simple closed form solution for the KL-divergence. To cope with that, generative adversarial networks (GAN) were proposed [10]. GAN uses two neural networks that are trained competitively\u2014a generator network G for sampling data and a discriminator network D for discerning the outputs of G from real data. Unfortunately, training G to match a high dimensional dataset distribution using only D\u2019s binary \u201cfake\u201d or \u201clegit\u201d outputs is not a stable or simple process.\n\u2217The companion source code of this paper can be found online: https://github.com/cnel/itl-ae\nMakhzani et. al. proposed adversarial autoencoders [11] which use an adversarial discriminator D to tell the low dimensional codes in the output of the encoder from data sampled from a desired distribution. In this way adversarial autoencoders can approximate variational regularization as long as it is possible to sample from the desired distribution. We note that although this partially solves the problem of generalized functional regularization for neural networks 1, adversarial autoencoders require us to train a third network, the discriminator, in addition to the encoder and decoder already being trained.\nHere we observe that, assuming we can sample from the desired distribution, we can use empirical distribution divergence measures proposed by Information Theoretic-Learning (ITL) as a measure of how close the function implemented by an encoder network is to a desired prior distribution. Thus, we propose Information Theoretic-Learning Autoencoders (ITL-AE).\nIn the next section of this paper we review ITL\u2019s Euclidean and Cauchy-Schwarz divergence measures [12]. In Section 3 we propose the ITL-AE and run experiments to illustrate the proposed method in Section 4. We conclude the paper afterwards."}, {"heading": "2 Information Theoretic-Learning", "text": "Information-theoretic learning (ITL) is a field at the intersection of machine learning and information theory [6] which encompasses a family of algorithms that compute and optimize informationtheoretic descriptors such as entropy, divergence, and mutual information. ITL objectives are computed directly from samples (non-parametrically) using Parzen windowing and Renyi\u2019s entropy [13]."}, {"heading": "2.1 Parzen density estimation", "text": "Parzen density estimation is a nonparametric method for estimating the pdf of a distribution empirically from data. For samples xi drawn from a distribution p, the parzen window estimate of p can be computed nonparametrically as\np\u0302(x) = 1\nN\nN \u2211\ni=1\nG\u03c3(x\u2212 xi). (1)\n1We still have a problem when we cannot sample from the desired distribution.\nIntuitively, as shown in Fig 1, parzen estimation corresponds to centering a Gaussian kernel at each sample xi drawn from p, and then summing to estimate the pdf. The optimal kernel size [14] depends on the density of samples, which approaches zero as the number of samples approaches infinity."}, {"heading": "2.2 ITL descriptors", "text": "Renyi\u2019s \u03b1-order entropy for probability density function (pdf) p is given by:\nH\u03b1(X) = 1\n1\u2212 \u03b1 log\n\u222b\np\u03b1(x)dx (2)\nwhere p \u2208 L\u03b1. Renyi\u2019s \u03b1-order entropy can be considered a generalization of Shannon entropy since lim\u03b1\u21921 H\u03b1 = \u222b\n\u2212 log p(x)dx, which is Shannon entropy. For the case of \u03b1 = 2, equation (2) simplifies to H2 = \u2212 log \u222b p2(x)dx which is known as Renyi\u2019s quadratic entropy.\nPlugging (1) into (2), for \u03b1 = 2, we obtain:\nH\u03022(X) = \u2212 log\n\u222b\n(\n1\nN\nN \u2211\ni=1\nG\u03c3(x\u2212 xi)\n)2\ndx (3)\n= \u2212 log\n\n\n1\nN2\nN \u2211\ni=1\nN \u2211\nj=1\nG\u03c3 \u221a 2 (xj \u2212 xi)\n\n (4)\nwhere G\u03c3(x, y) = 1\u221a 2\u03c0\u03c3\nexp ( \u2016x\u2212y\u20162 2\u03c32 ) is the Gaussian kernel, and \u03c3 is the kernel size. The argu-\nment of the logarithm in equation (4) is called the information potential by analogy with potential fields from physics and is denoted by V\u0302\u03c3(X).\nAnother important ITL descriptor is Renyi\u2019s cross-entropy which is given by:\nH2(X,Y ) = \u2212 log\n\u222b\npX(z)pY (z)dz (5)\nSimilarly to equation (2), cross-entropy can be estimated by\nH\u03022(X,Y ) = \u2212 log 1\nNXNY\nNX \u2211\ni=1\nNY \u2211\nj=1\nG\u221a 2\u03c3(xi \u2212 yj) (6)\nThe argument of the logarithm in equation (5) is called cross-information potential and is denoted V\u0302\u03c3(X,Y ). Cross-information potential can be viewed as the average sum of interactions of samples drawn from pX with the estimated pdf p\u0302Y (or vice-versa).\nITL has also described a number of divergences connected with Renyi\u2019s entropy. In particular, the Euclidean and Cauchy-Schwarz divergences are given by:\nDED(pX\u2016pY ) =\n\u222b\n(pX(z)\u2212 pY (z)) 2 dz (7)\n=\n\u222b\np2X(z)dz +\n\u222b\np2Y (z)dz \u2212 2\n\u222b\npX(z)pY (z)dz (8)\nand\nDCS(pX\u2016pY ) = \u2212 log\n(\u222b pX(z)pY (z)dz )2\n\u222b p2X(z)dz \u222b p2Y (z)dz , (9)\nrespectively. Equations (7) and (9) can be put in terms of information potentials:\nDED(pX ||pY ) = V (X) + V (Y )\u2212 2V (X,Y ) (10)\nDCS(pX ||pY ) = log V (X)V (Y )\nV 2(X,Y ) (11)\nEuclidean divergence is so named because it is equivalent to the Euclidean distance between pdfs. Furthermore, it is equivalent to maximum mean discrepancy [15] (MMD) statistical test. CauchySchwarz divergence is named for the Cauchy-Schwarz inequality, which guarantees the divergence\nis only equal to zero when the pdfs are equal almost everywhere. DCS is symmetric but, unlike DED, does not obey the triangle equality.\nMinimizing either divergence over pX , i.e., minpX D(pX\u2016pY ), is a tradeoff between minimizing the information potential (maximizing entropy) of pX and maximizing the cross-information potential (minimizing cross-entropy) of pX with respect to pY . Intuitively, minimizing the information potential encourages samples from pX to spread out, while maximizing the cross-information potential encourages samples from pX to move toward samples from pY ."}, {"heading": "3 ITL Autoencoders", "text": "Let us define autoencoders as a 4-tuple AE = {E,D,L,R}. Where E and D are the encoder and the decoder functions, here parameterized as neural networks. L is the reconstruction cost function that measures the difference between original data samples x and their respective reconstructions x\u0303 = D(E(x)). A typical reconstruction cost is mean-squared error. R is a functional regularization. Here this functional regularization will only be applied to the encoder E. Nonetheless, although we are only regularizing the encoder E, the interested investigator could also regularize another intermediate layer of the autoencoder 2.\nThe general cost function for the ITL-AE can be summarized by the following equation:\ncost = L (x, x\u0303) + \u03bbR(E,P ), (12)\nwhere the strength of regularization is controlled by the scale parameter \u03bb, and P is the imposed prior.\nThe functional regularization costs investigated in this paper are the ITL Euclidean and CauchySchwarz divergences. Both types of divergence encourage a smooth manifold similar to the imposed prior. That is, maximizing latent code distribution entropy encourages code samples to spread out, while minimizing the cross-entropy between the latent code and the prior distributions encourages code samples to be similar to prior samples.\nNote that if the data dimensionality is too high, ITL divergence measures require larger batch sizes to be reliably estimated, this explains why Li et. al. [16] used batches of 1000 samples in their experiments and also why they reduced the data dimensionality with autoencoders. In our own experiments (not shown), the Cauchy-Schwarz divergence worked better than Euclidean for highdimensional data."}, {"heading": "4 Relation to other work", "text": "Generative Moment Matching Networks (GMMNs) [16] correspond to the specific case where the input of the decoder D comes from a multidimensional uniform distribution and the reconstruction\n2For those interested in such investigation we recommend modifying the companion code of this paper. In our method adding more regularization does not increase the number of adaptive weights.\nfunction L is given by the Euclidean divergence measure. GMMNs could be applied to generate samples from the original input space itself or from a lower dimensional previously trained stacked autoencoder (SCA) [17] hidden space. An advantage of our approach compared to GMMNs is that we can train all the elements in the 4-tuple AE together without the elaborate process of training layerwise stacked autoencoders for dimensionality reduction.\nVariational Autoencoders (VAE) [4] adapt a lower bound of the variational regularization, R, using parametric, closed form solutions for the KL-divergence. That divergence can be defined using Shannon\u2019s entropy or H\u03b1=1. Thus, we can also interpret ITL-AE as nonparametric variational autoencoders, where the likelihood of the latent distribution is estimated empirically using Parzen windows. Note that since we can estimate that distribution directly, we do not use the reparametrization trick. Here the reparametrization trick could possibly be used for imposing extra regularization, just like how adding dropout noise regularizes neural networks.\nAdversarial autoencoders (AA) [11] have the architecture that inspired our method the most. Instead of using the adversarial trick to impose regularization on the encoder, we defined that regularization from first principles, which allowed us to train a competing method with much fewer trainable parameters. Our most recent experiments show that AA scales better than ITL-AE for high dimensional latent codes. We leave investigation into high dimensional ITL-AE for future work."}, {"heading": "5 Experiments", "text": "In this section we show experiments using the ITL-AE architecture described in the previous section. First, for a visual interpretation of the effects of variational regularization we trained autoencoders with 2-dimensional latent codes. We used as desired priors a Gaussian, a Laplacian, and a 2D swissroll distribution. The resulting codes are shown in Fig. 3. Note that in all of these examples the autoencoder was trained in a completely unsupervised manner. However, given the simplicity of the data and the imposed reconstruction cost, some of the numbers were clustered in separate regions of the latent space. Fig. 4 shows some images obtained by sampling from a linear path on the swiss-roll and random samples from the Gaussian manifold.\nFor easier comparisons and to avoid extensive hyperparameter search, we constrained our encoder and decoders, E and D, to have the same architecture as those used in [11] i.e., each network is a two hidden layer fully connected network with 1000 hidden neurons. Thus, the only hyperparameters investigated in this paper were kernel size \u03c3 and scale parameter \u03bb. For the MNIST dataset, the Euclidean distance worked better with smaller kernels, such as \u03c3 = 1 or \u03c3 = 5, while the Cauchy-Schwarz divergence required larger kernel, \u03c3 = 10 for example. Nevertheless, here we will focus in regularizing the low dimensional latent codes and leave experiments using Cauchy-Schwarz divergence for future work.\nOur best results for small batch sizes common in deep learning had 3-dimensional latent codes in the output of the encoder, euclidean divergence as the regularization R and mean-squared error as the reconstruction cost L. As we will show in the next section, we were able to obtain competitive results and reproduce behaviors obtained by methods trained with larger networks or extra adversarial networks."}, {"heading": "5.1 Log-likelihood analysis", "text": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19]. After training ITL-AE on the training set, we generated 104 images by inputting 104 samples from a N(0, 5) distribution to the decoder. Those generated MNIST images were used estimate a distribution using Parzen windows on the high dimensional image space3. We calculated the log-likelihood of separate 10k samples from the test set and report the results on Table 1. The kernel size of that Parzen estimator was chose using the best results on a held-out cross-validation dataset. That kernel size was \u03c3 = 0.16. Note that our method obtained the second best results between all the compared fully connected generative models. Remember that this was obtained with about 106 less adaptive parameters than the best method Adversarial autoencoders."}, {"heading": "6 Conclusions", "text": "Here we derived and validated the Information Theoretic-Learning Autoencoders, a non-parametric and (optionally) deterministic alternative to Variational Autoencoders. We also revisited ITL for neural networks, but this time, instead of focusing on nonparametric cost functions for non-Gaussian signal processing, we focused on distribution divergences for regularizing deep architectures.\nOur results using relatively small, for deep learning standards, 4 layer networks with 3-dimensional latent codes obtained competitive results on the log-likelihood analysis of the MNIST dataset.\nAlthough our results were competitive for fully connected architectures, future work should address the scalability of the ITL estimators for large dimensional latent spaces, which is common on large neural networks and convolutional architectures as well."}], "references": [{"title": "Deep learning", "author": ["Ian Goodfellow", "Aaron Courville", "Yoshua Bengio"], "venue": "Book in preparation for MIT Press.(Cited on page 159), 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1929}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1401.4082, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Information theoretic learning: Renyi\u2019s entropy and kernel perspectives", "author": ["Jose C Principe"], "venue": "Springer Science & Business Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W Keith Hastings"], "venue": "Biometrika, vol. 57, no. 1, pp. 97\u2013109, 1970.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1970}, {"title": "Deep convolutional inverse graphics network", "author": ["Tejas D Kulkarni", "Will Whitney", "Pushmeet Kohli", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1503.03167, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep generative image models using a? laplacian pyramid of adversarial networks", "author": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1486\u20131494.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u20132680.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Adversarial autoencoders", "author": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow"], "venue": "arXiv preprint arXiv:1511.05644, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Information theoretic learning", "author": ["Jose C Principe", "Dongxin Xu", "John Fisher"], "venue": "Unsupervised adaptive filtering, vol. 1, pp. 265\u2013319, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "On measures of entropy and information", "author": ["ALFRPED Rrnyi"], "venue": "Fourth Berkeley symposium on mathematical statistics and probability, 1961, vol. 1, pp. 547\u2013561.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1961}, {"title": "Density estimation for statistics and data analysis", "author": ["Bernard W Silverman"], "venue": "CRC press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "A kernel method for the two-sample-problem", "author": ["Arthur Gretton", "Karsten M Borgwardt", "Malte Rasch", "Bernhard Sch\u00f6lkopf", "Alex J Smola"], "venue": "Advances in neural information processing systems, 2006, pp. 513\u2013520.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard Zemel"], "venue": "arXiv preprint arXiv:1502.02761, 2015. 7", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gr\u00e9goire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "arXiv preprint arXiv:1207.4404, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep generative stochastic networks trainable by backprop", "author": ["Yoshua Bengio", "Eric Thibodeau-Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": "arXiv preprint arXiv:1306.1091, 2013. 8", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Deep, regularized neural networks work better in practice than shallow unconstrained neural networks [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "This regularization takes classic forms such as L2-norm ridge regression, L1-norm LASSO, architectural constraints such as convolutional layers [2], but also uses modern techniques such as dropout [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "This regularization takes classic forms such as L2-norm ridge regression, L1-norm LASSO, architectural constraints such as convolutional layers [2], but also uses modern techniques such as dropout [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 3, "context": "Recently, especially in the subfield of autoencoding neural networks, regularization has been accomplished with variational methods [4][5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Recently, especially in the subfield of autoencoding neural networks, regularization has been accomplished with variational methods [4][5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "In this paper we propose Information Theoretic-Learning [6] divergence measures for variational regularization.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Variational methods for deep learning were popularized by the variational autoencoder (VAE) framework proposed by [4] and [5] which also brought the attention of deep learning researchers to the reparametrization trick.", "startOffset": 114, "endOffset": 117}, {"referenceID": 4, "context": "Variational methods for deep learning were popularized by the variational autoencoder (VAE) framework proposed by [4] and [5] which also brought the attention of deep learning researchers to the reparametrization trick.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "This latent factor is then fed forward to the decoder network and the parameters \u03bc and \u03c3 are regularized using the KL-divergence KL(N(\u03bc, \u03c3)\u2016N(0, 1)) between the inferred distribution and the imposed prior, which has a simple form [4].", "startOffset": 230, "endOffset": 233}, {"referenceID": 6, "context": "This is an approach similar to the inverse cumulative distribution method and does not involve estimation of the partition function, rejection sampling, or other complicated approaches [7].", "startOffset": 185, "endOffset": 188}, {"referenceID": 7, "context": "VAE\u2019s methodology has been successfully extended to convolutional autoencoders [8] and more elaborate architectures such as Laplacian pyramids for image generation [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 8, "context": "VAE\u2019s methodology has been successfully extended to convolutional autoencoders [8] and more elaborate architectures such as Laplacian pyramids for image generation [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 9, "context": "To cope with that, generative adversarial networks (GAN) were proposed [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "proposed adversarial autoencoders [11] which use an adversarial discriminator D to tell the low dimensional codes in the output of the encoder from data sampled from a desired distribution.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "In the next section of this paper we review ITL\u2019s Euclidean and Cauchy-Schwarz divergence measures [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Information-theoretic learning (ITL) is a field at the intersection of machine learning and information theory [6] which encompasses a family of algorithms that compute and optimize informationtheoretic descriptors such as entropy, divergence, and mutual information.", "startOffset": 111, "endOffset": 114}, {"referenceID": 12, "context": "ITL objectives are computed directly from samples (non-parametrically) using Parzen windowing and Renyi\u2019s entropy [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "The optimal kernel size [14] depends on the density of samples, which approaches zero as the number of samples approaches infinity.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Furthermore, it is equivalent to maximum mean discrepancy [15] (MMD) statistical test.", "startOffset": 58, "endOffset": 62}, {"referenceID": 15, "context": "[16] used batches of 1000 samples in their experiments and also why they reduced the data dimensionality with autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "4 Relation to other work Generative Moment Matching Networks (GMMNs) [16] correspond to the specific case where the input of the decoder D comes from a multidimensional uniform distribution and the reconstruction For those interested in such investigation we recommend modifying the companion code of this paper.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "GMMNs could be applied to generate samples from the original input space itself or from a lower dimensional previously trained stacked autoencoder (SCA) [17] hidden space.", "startOffset": 153, "endOffset": 157}, {"referenceID": 3, "context": "Variational Autoencoders (VAE) [4] adapt a lower bound of the variational regularization, R, using parametric, closed form solutions for the KL-divergence.", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "Adversarial autoencoders (AA) [11] have the architecture that inspired our method the most.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "For easier comparisons and to avoid extensive hyperparameter search, we constrained our encoder and decoders, E and D, to have the same architecture as those used in [11] i.", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "We followed the log-likelihood analysis on the MNIST dataset reported on the literature [18][17][19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "Methods Log-likelihood Stacked CAE [17] 121\u00b1 1.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "6 DBN [18] 138\u00b1 2 Deep GSN [19] 214\u00b1 1.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "6 DBN [18] 138\u00b1 2 Deep GSN [19] 214\u00b1 1.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "1 GAN [10] 225\u00b1 2 GMMN + AE [16] 282\u00b1 2 ITL-AE\u2217 300\u00b1 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "1 GAN [10] 225\u00b1 2 GMMN + AE [16] 282\u00b1 2 ITL-AE\u2217 300\u00b1 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "5 Adversarial Autoencoder [11] 340\u00b1 2 \u2217 Proposed method.", "startOffset": 26, "endOffset": 30}], "year": 2016, "abstractText": "We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks. We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function. This paper also formalizes, generative moment matching networks under the ITL framework.", "creator": "LaTeX with hyperref package"}}}