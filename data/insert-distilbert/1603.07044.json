{"id": "1603.07044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2016", "title": "Recurrent Neural Network Encoder with Attention for Community Question Answering", "abstract": "we apply a detailed general recurrent neural network ( rnn ) encoder framework to community question answering ( cqa ) tasks. our approach does not rely on simply any linguistic processing, and can be applied to different languages used or domains. further improvements are observed when we might extend the rnn encoders with a neural - attention support mechanism that encourages reasoning over entire verb sequences. to deal with practical issues such as data sparsity and imbalanced encoding labels, we apply various techniques such as transfer layer learning and multitask learning. our experiments on the semeval - 2016 cqa task show 10 % improvement on a functional map score compared to an information retrieval - based approach, and achieve comparable performance to a strong handcrafted feature - based method.", "histories": [["v1", "Wed, 23 Mar 2016 01:52:54 GMT  (1272kb,D)", "http://arxiv.org/abs/1603.07044v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["wei-ning hsu", "yu zhang", "james glass"], "accepted": false, "id": "1603.07044"}, "pdf": {"name": "1603.07044.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Network Encoder with Attention for Community Question Answering", "authors": ["Wei-Ning Hsu"], "emails": ["wnhsu@csail.mit.edu", "yzhang87@csail.mit.edu", "jrg@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Community question answering (cQA) is a paradigm that provides forums for users to ask or answer questions on any topic with barely any restrictions. In the past decade, these websites have attracted a great number of users, and have accumulated a large collection of question-comment threads generated by these users. However, the low restriction results in a high variation in answer quality, which makes it time-consuming to search for useful information from the existing content. It would therefore be valuable to automate the procedure of ranking related questions and comments for users with a new question, or when looking for solutions from comments of an existing question.\nAutomation of cQA forums can be divided into three tasks: question-comment relevance (Task A), question-question relevance (Task B), and questionexternal comment relevance (Task C). One might think that classic retrieval models like language models for information retrieval (Zhai and Lafferty, 2004) could solve these tasks. However, a big challenge for cQA tasks is that users are used to expressing similar meanings with different words, which creates gaps when matching questions based on common words. Other challenges include informal usage of language, highly diverse content of comments, and variation in the length of both questions and comments.\nTo overcome these issues, most previous work (e.g. SemEval 2015 (Nakov et al., 2015)) relied heavily on additional features and reasoning capabilities. In (Rockta\u0308schel et al., 2015), a neural attention-based model was proposed for automatically recognizing entailment relations between pairs of natural language sentences. In this study, we first modify this model for all three cQA tasks. We also extend this framework into a jointly trained model when the external resources are available, i.e. selecting an external comment when we know the question that the external comment answers (Task C).\nOur ultimate objective is to classify relevant questions and comments without complicated handcrafted features. By applying RNN-based encoders, we avoid heavily engineered features and learn the representation automatically. In addition, an attention mechanism augments encoders with the ability to attend to past outputs directly. This becomes helpful when encoding longer sequences, since we no\nar X\niv :1\n60 3.\n07 04\n4v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\nlonger need to compress all information into a fixedlength vector representation.\nIn our view, existing annotated cQA corpora are generally too small to properly train an end-to-end neural network. To address this, we investigate transfer learning by pretraining the recurrent systems on other corpora, and also generating additional instances from existing cQA corpus."}, {"heading": "2 Related Work", "text": "Earlier work of community question answering relied heavily on feature engineering, linguistic tools, and external resource. (Jeon et al., 2006) and (Shah and Pomerantz, 2010) utilized rich non-textual features such as answer\u2019s profile. (Grundstro\u0308m and Nugues, 2014) syntactically analyzed the question and extracted name entity features. (Harabagiu and Hickl, 2006) demonstrated a textual entailment system can enhance cQA task by casting question answering to logical entailment.\nMore recent work incorporated word vector into their feature extraction system and based on it designed different distance metric for question and answer (Tran et al., 2015) (Belinkov et al., 2015). While these approaches showed effectiveness, it is difficult to generalize them to common cQA tasks since linguistic tools and external resource may be restrictive in other languages and features are highly customized for each cQA task.\nVery recent work on answer selection also involved the use of neural networks. (Wang and Nyberg, 2015) used LSTM to construct a joint vector based on both the question and the answer and then converted it into a learning to rank problem. (Feng et al., 2015) proposed several convolutional neural network (CNN) architectures for cQA. Our method differs in that RNN encoder is applied here and by adding attention mechanism we jointly learn which words in question to focus and hence available to conduct qualitative analysis. During classification, we feed the extracted vector into a feed-forward neural network directly instead of using mean/max pooling on top of each time steps."}, {"heading": "3 Method", "text": "In this section, we first discuss long short-term memory (LSTM) units and an associated attention mech-\nanism. Next, we explain how we can encode a pair of sentences into a dense vector for predicting relationships using an LSTM with an attention mechanism. Finally, we apply these models to predict question-question similarity, question-comment similarity, and question-external comment similarity."}, {"heading": "3.1 LSTM Models", "text": "LSTMs have shown great success in many different fields. An LSTM unit contains a memory cell with self-connections, as well as three multiplicative gates to control information flow. Given input vector xt, previous hidden outputs ht\u22121, and previous cell state ct\u22121, LSTM units operate as follows:\nX =\n[ xt\nht\u22121\n] (1)\nit = \u03c3(WiXX +Wicct\u22121 + bi) (2)\nft = \u03c3(WfXX +Wfcct\u22121 + bf ) (3)\not = \u03c3(WoXX +Wocct\u22121 + bo) (4)\nct = ft ct\u22121 + it tanh(WcXX + bc) (5) ht = ot tanh(ct) (6)\nwhere it, ft, ot are input, forget, and output gates, respectively. The sigmoid function \u03c3() is a soft gate function controlling the amount of information flow. W s and bs are model parameters to learn."}, {"heading": "3.2 Neural Attention", "text": "A traditional RNN encoder-decoder approach (Sutskever et al., 2014) first encodes an arbitrary length input sequence into a fixed-length dense vector that can be used as input to subsequent classification models, or to initialize the hidden state of a secondary decoder. However, the requirement to compress all necessary information into a single fixed length vector can be problematic. A neural attention model (Bahdanau et al., 2014) (Cho et al., 2014) has been recently proposed to alleviate this issue by enabling the network to attend to past outputs when decoding. Thus, the encoder no longer needs to represent an entire sequence with one vector; instead, it encodes information into a sequence of vectors, and adaptively chooses a subset of the vectors when decoding."}, {"heading": "3.3 Predicting Relationships of Object Pairs with an Attention Model", "text": "In our cQA tasks, the pair of objects are (question, question) or (question, comment), and the relationship is relevant/irrelevant. The left side of Figure 1 shows one intuitive way to predict relationships using RNNs. Parallel LSTMs encode two objects independently, and then concatenate their outputs as an input to a feed-forward neural network (FNN) with a softmax output layer for classification.\nThe representations of the two objects are generated independently in this manner. However, we are more interested in the relationship instead of the object representations themselves. Therefore, we consider a serialized LSTM-encoder model in the right side of Figure 1 that is similar to that in (Rockta\u0308schel et al., 2015), but also allows an augmented feature input to the FNN classifier.\nFigure 2 illustrates our attention framework in more detail. The first LSTM reads one object, and passes information through hidden units to the second LSTM. The second LSTM then reads the other\nobject and generates the representation of this pair after the entire sequence is processed. We build another FNN that takes this representation as input to classify the relationship of this pair.\nBy adding an attention mechanism to the encoder, we allow the second LSTM to attend to the sequence of output vectors from the first LSTM, and hence generate a weighted representation of first object according to both objects. Let hN be the last output of second LSTM and M = [h1, h2, \u00b7 \u00b7 \u00b7 , hL] be the sequence of output vectors of the first object. The weighted representation of the first object is\nh\u2032 =\nL\u2211\ni=1\n\u03b1ihi (7)\nThe weight is computed by\n\u03b1i = exp(a(hi, hN ))\u2211L j=1 exp(a(hj , hN ))\n(8)\nwhere a() is the importance model that produces a higher score for (hi, hN ) if hi is useful to determine\nthe object pair\u2019s relationship. We parametrize this model using another FNN. Note that in our framework, we also allow other augmented features (e.g., the ranking score from the IR system) to enhance the classifier. So the final input to the classifier will be hN , h\u2032, as well as augmented features."}, {"heading": "3.4 Modeling Question-External Comments", "text": "For task C, in addition to an original question (oriQ) and an external comment (relC), the question which relC commented on is also given (relQ). To incorporate this extra information, we consider a multitask learning framework which jointly learns to predict the relationships of the three pairs (oriQ/relQ, oriQ/relC, relQ/relC).\nFigure 3 shows our framework: the three lower models are separate serialized LSTM-encoders for the three respective object pairs, whereas the upper model is an FNN that takes as input the concatenation of the outputs of three encoders, and predicts the relationships for all three pairs. More specifically, the output layer consists of three softmax layers where each one is intended to predict the relationship of one particular pair.\nFor the overall loss function, we combine three separate loss functions using a heuristic weight vector \u03b2 that allocates a higher weight to the main task (oriQ-relC relationship prediction) as follows:\nL = \u03b21L1 + \u03b22L2 + \u03b23L3 (9)\nBy doing so, we hypothesize that the related tasks can improve the main task by leveraging commonality among all tasks."}, {"heading": "4 Experiments", "text": "We evaluate our approach on all three cQA tasks. We use the cQA datasets provided by the Semeval 2016 task 1. The cQA data is organized as follows: there are 267 original questions, each question has 10 related question, and each related question has 10 comments. Therefore, for task A, there are a total number of 26,700 question-comment pairs. For task B, there are 2,670 question-question pairs. For task C, there are 26,700 question-comment pairs. The test dataset includes 50 questions, 500 related questions and 5,000 comments which do not overlap with\n1http://alt.qcri.org/semeval2016/task3\nthe training set. To evaluate the performance, we use mean average precision (MAP) and F1 score.\nBaseline System: Figure 4 illustrates our baseline systems. The IR-based system is scored by the Google search engine. For each question-comment pair, or question-question pair, we use Google\u2019s rank to calculate the MAP. While there is no training on the target data, we expect that Google used many external resources to produce these ranks. The featurerich system is that proposed by (Belinkov et al., 2015) in SemEval-2015. In this approach, they compute text-based, vector-based, metadata-based and rank-based features from the pre-processed data. The features are used by a linear SVM for comment selection. This system includes traditional handcrafted features, and some RNN-based features (word vectors). It also includes the information from the IR system (ranked-based). So we believe it is a strong baseline to compare with our model.\nRNN encoder: Our system is based on Theano (Bastien et al., 2012; Bergstra et al., 2010). Table 1 gives a list of hyper-parameters we considered. As suggested by (Greff et al., 2015), the hyper-parameters for LSTMs can be tuned independently. We tuned each parameter separately on a development set (split from the training set) and simply picked the best setting. Our experiments show that using word embeddings from Google-News provides modest improvements, but fixing the embedding degrades performance a lot. Also, using separate parameters for LSTMs is better than sharing. For the optimization method, AdaDelta converged faster, but AdaGrad gives better performance. Note that all the parameters were tuned on Task A, and we simply applied them to Task B and C. This is for saving computation, and also because Task A is more well-defined compared to B and C in terms of dataset size and label balance."}, {"heading": "4.1 Preliminary Results", "text": "Table 2 shows the initial results using the RNN encoder for different tasks. We observe that the attention model always gets better results than the RNN without attention, especially for task C. However, the RNN model achieves a very low F1 score. For task B, it is even worse than the random baseline.\nWe believe the reason is because for task B, there are only 2,670 pairs for training which is very limited training for a reasonable neural network. For task C, we believe the problem is highly imbalanced data. Since the related comments did not directly comment on the original question, more than 90% of the comments are labeled as irrelevant to the original\nquestion. The low F1 (with high precision and low recall) means our system tends to label most comments as irrelevant. In the following section, we investigate methods to address these issues."}, {"heading": "4.2 Robust Parameter Initialization", "text": "One way to improve models trained on limited data is to use external data to pretrain the neural network. We therefore considered two different datasets for this task.\n\u2022 Cross-domain: The Stanford natural language inference (SNLI) corpus (Bowman et al., 2015) has a huge amount of cleaned premise and hypothesis pairs. Unfortunately the pairs are for a different task. The relationship between the premise and hypothesis may be similar to the relation between questions and comments, but may also be different.\n\u2022 In-domain: since task A seems has reason-\nable performance, and the network is also welltrained, we could use it directly to initialize task B.\nTo utilize the data, we first trained the model on each auxiliary data (SNLI or Task A) and then removed the softmax layer. After that, we retrain the network using the target data with a softmax layer that was randomly initialized.\nFor task A, the SNLI cannot improve MAP or F1 scores. Actually it slightly hurts the performance. We surmise that it is probably because the domain is different. Further investigation is needed: for example, we could only use the parameter for embedding layers etc. For task B, the SNLI yields a slight improvement on MAP (0.2%), and Task A could give (1.2%) on top of that. No improvement was observed on F1. For task C, pretraining by task A is also better than using SNLI (task A is 1% better than the baseline, while SNLI is almost the same).\nIn summary, the in-domain pretraining seems better, but overall, the improvement is less than we expected, especially for task B, which only has very limited target data. We will not make a conclusion here since more investigation is needed."}, {"heading": "4.3 Multitask Learning", "text": "As mentioned in Section 3.4, we also explored a multitask learning framework that jointly learns to predict the relationships of all three tasks. We set 0.8 for the main task (task C) and 0.1 for the other auxiliary tasks. The MAP score did not improve, but F1 increases to 0.1617. We believe this is because other tasks have more balanced labels, which improves the shared parameters for task C."}, {"heading": "4.4 Augmented data", "text": "There are many sources of external question-answer pairs that could be used in our tasks. For example: WebQuestion (was introduced by the authors of SEMPRE system (Berant et al., 2013)) and The SimpleQuestions dataset 2. All of them are positive examples for our task and we can easily create negative examples from it. Initial experiments indicate that it is very easy to overfit these obvious negative examples. We believe this is because our negative\n2http://fb.ai/babi.\nexamples are non-informative for our task and just introduce noise.\nSince the external data seems to hurt the performance, we try to use the in-domain pairs to enhance task B and task C. For task B, if relative question 1 (rel1) and relative question 2 (rel2) are both relevant to the original question, then we add a positive sample (rel1, rel2, 1). If either rel1 and rel2 is irrelevant and the other is relevant, we add a negative sample (rel1, rel2, 0). After doing this, the samples of task B increase from 2, 670 to 11, 810. By applying this method, the MAP score increased slightly from 0.5723 to 0.5789 but the F1 score improved from 0.4334 to 0.5860.\nFor task C, we used task A\u2019s data directly. The results are very similar with a slight improvement on MAP, but large improvement on F1 score from 0.1449 to 0.2064."}, {"heading": "4.5 Augmented features", "text": "To further enhance the system, we incorporate a one hot vector of the original IR ranking as an additional feature into the FNN classifier. Table 3 shows the results. In comparing the models with and without augmented features, we can see large improvement for task B and C. The F1 score for task A degrades slightly but MAP improves. This might be because task A already had a substantial amount of training data."}, {"heading": "4.6 Comparison with Other Systems", "text": "Table 4 gives the final comparison between different models (we only list the MAP score because it is the official score for the challenge). Since the two baseline models did not use any additional data, in this table our system was also restricted to the provided training data. For task A, we can see that if there is enough training data our single system already performs better than a very strong feature-rich based system. For task B, since only limited training data is given, both feature-rich based system and our system are worse than the IR system. For task C, our system also got comparable results with the feature-rich based system. If we do a simple system combination (average the rank score) between our system and the IR system, the combined system will\ngive large gains on tasks B and C3. This implies that our system is complimentary with the IR system."}, {"heading": "5 Analysis of Attention Mechanism", "text": "In addition to quantitative analysis, it is natural to qualitatively evaluate the performance of the attention mechanism by visualizing the weight distribution of each instance. We randomly picked several instances from the test set in task A, for which the sentence lengths are more moderate for demonstration. These examples are shown in Figure 5, and categorized into short, long, and noisy sentences for discussion. A darker blue patch refers to a larger weight relative to other words in the same sentence."}, {"heading": "5.1 Short Sentences", "text": "Figure 5a illustrates two cQA examples whose questions are relatively short. The comments corresponding to these questions are \u201c...snorkeling two days ago off the coast of dukhan...\u201d and \u201cthe doha international airport...\u201d. We can observe that our model successfully learns to focus on the most representative part of the question pertaining to classifying the relationship, which is \u201dplace for snorkeling\u201d for the first example and \u201cplace can ... visited in qatar\u201d for the second example.\n3The feature-rich based system was already combined with the IR system)"}, {"heading": "5.2 Long Sentences", "text": "In Figure 5b, we investigate two examples with longer questions, which both contain 63 words. Interestingly, the distribution of weights does not become more uniform; the model still focuses attention on a small number of hot words, for example, \u201cpuppy dog for ... mall\u201d and \u201chectic driving in doha ... car insurance ... quite costly\u201d. Additionally, some words that appear frequently but carry little information for classification are assigned very small weights, such as I/we/my, is/am, like, and to."}, {"heading": "5.3 Noisy Sentence", "text": "Due to the open nature of cQA forums, some content is noisy. Figure 5c is an example with excessive usage of question marks. Again, our model exhibits its robustness by allocating very low weights to the noise symbols and therefore excludes the noninformative content."}, {"heading": "6 Conclusion", "text": "In this paper, we demonstrate that a general RNN encoder framework can be applied to community question answering tasks. By adding a neural attention mechanism, we showed quantitatively and qualitatively that attention can improve the RNN encoder framework. To deal with a more realistic scenario, we expanded the framework to incorporate metadata as augmented inputs to a FNN classifier, and pretrained models on larger datasets, increasing both stability and performance. Our model is consistently better than or comparable to a strong feature-rich baseline system, and is superior to an IR-based system when there is a reasonable amount of training data.\nOur model is complimentary with an IR-based system that uses vast amounts of external resources but trained for general purposes. By combining the two systems, it exceeds the feature-rich and IRbased system in all three tasks.\nMoreover, our approach is also language independent. We have also performed preliminary experiments on the Arabic portion of the SemEval-2016 cQA task. The results are competitive with a handtuned strong baseline from SemEval-2015.\nFuture work could proceed in two directions: first, we can enrich the existing system by incorporating available metadata and preprocessing data with morphological normalization and out-of-vocabulary mappings; second, we can reinforce our model by carrying out word-by-word and history-aware at-\ntention mechanisms instead of attending only when reading the last word."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Vectorslu: A continuous word vector approach to answer selection in community question answering systems", "author": ["Mitra Mohtarami", "Scott Cyphers", "James Glass"], "venue": "In Proceedings of the 9th International", "citeRegEx": "Belinkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Berant et al.2013] J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David WardeFarley", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman et al.2015] S. Bowman", "G. Angeli", "C. Potts", "C. Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Applying deep learning to answer selection: A study and an open task. arXiv preprint arXiv:1508.01585", "author": ["Feng et al.2015] Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "LSTM: A search space odyssey", "author": ["Greff et al.2015] Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R. Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2015}, {"title": "Using syntactic features in answer reranking", "author": ["Grundstr\u00f6m", "Nugues2014] Jakob Grundstr\u00f6m", "Pierre Nugues"], "venue": "In AAAI 2014 Workshop on Cognitive Computing for Augmented Human Intelligence,", "citeRegEx": "Grundstr\u00f6m et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Grundstr\u00f6m et al\\.", "year": 2014}, {"title": "Methods for using textual entailment in open-domain question answering", "author": ["Harabagiu", "Hickl2006] Sanda Harabagiu", "Andrew Hickl"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting", "citeRegEx": "Harabagiu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Harabagiu et al\\.", "year": 2006}, {"title": "A framework to predict the quality of answers with non-textual features", "author": ["Jeon et al.2006] Jiwoon Jeon", "W Bruce Croft", "Joon Ho Lee", "Soyeon Park"], "venue": "In Proceedings of the 29th annual international ACM", "citeRegEx": "Jeon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2006}, {"title": "Semeval-2015 task 3: Answer selection in community question answering", "author": ["Nakov et al.2015] R. Nakov", "L. Marquez", "W. Magdy", "A. Moschitti", "J. Glass"], "venue": "In Proc. SamEval,", "citeRegEx": "Nakov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Evaluating and predicting answer quality in community qa", "author": ["Shah", "Pomerantz2010] Chirag Shah", "Jefferey Pomerantz"], "venue": "In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Shah et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Jaist: Combining multiple features for answer selection in community question answering", "author": ["Tran et al.2015] Quan Hung Tran", "Vu Tran", "Tu Vu", "Minh Le Nguyen", "Son Bao Pham"], "venue": "In Proceedings of the 9th International Workshop on Semantic Evaluation,", "citeRegEx": "Tran et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2015}, {"title": "A long short-term memory model for answer sentence selection in question answering", "author": ["Wang", "Nyberg2015] Di Wang", "Eric Nyberg"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["Zhai", "Lafferty2004] C. Zhai", "J. Lafferty"], "venue": "In ACM Trans. Inf. Syst", "citeRegEx": "Zhai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "SemEval 2015 (Nakov et al., 2015)) relied heavily on additional features and reasoning capabilities.", "startOffset": 13, "endOffset": 33}, {"referenceID": 12, "context": "In (Rockt\u00e4schel et al., 2015), a neural attention-based model was proposed for automatically recognizing entailment relations between pairs of natural language sentences.", "startOffset": 3, "endOffset": 29}, {"referenceID": 10, "context": "(Jeon et al., 2006) and (Shah and Pomerantz, 2010) utilized rich non-textual features such as answer\u2019s profile.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "More recent work incorporated word vector into their feature extraction system and based on it designed different distance metric for question and answer (Tran et al., 2015) (Belinkov et al.", "startOffset": 154, "endOffset": 173}, {"referenceID": 1, "context": ", 2015) (Belinkov et al., 2015).", "startOffset": 8, "endOffset": 31}, {"referenceID": 6, "context": "(Feng et al., 2015) proposed several convolutional neural network (CNN) architectures for cQA.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "A traditional RNN encoder-decoder approach (Sutskever et al., 2014) first encodes an arbitrary length input sequence into a fixed-length dense vector that can be used as input to subsequent classification models, or to initialize the hidden state of a secondary decoder.", "startOffset": 43, "endOffset": 67}, {"referenceID": 0, "context": "A neural attention model (Bahdanau et al., 2014) (Cho et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": ", 2014) (Cho et al., 2014) has been recently proposed to alleviate this issue by enabling the network to attend to past outputs when decoding.", "startOffset": 8, "endOffset": 26}, {"referenceID": 12, "context": "Therefore, we consider a serialized LSTM-encoder model in the right side of Figure 1 that is similar to that in (Rockt\u00e4schel et al., 2015), but also allows an augmented feature input to the FNN classifier.", "startOffset": 112, "endOffset": 138}, {"referenceID": 1, "context": "The featurerich system is that proposed by (Belinkov et al., 2015) in SemEval-2015.", "startOffset": 43, "endOffset": 66}, {"referenceID": 3, "context": "RNN encoder: Our system is based on Theano (Bastien et al., 2012; Bergstra et al., 2010).", "startOffset": 43, "endOffset": 88}, {"referenceID": 7, "context": "As suggested by (Greff et al., 2015), the hyper-parameters for LSTMs can be tuned independently.", "startOffset": 16, "endOffset": 36}, {"referenceID": 4, "context": "\u2022 Cross-domain: The Stanford natural language inference (SNLI) corpus (Bowman et al., 2015) has a huge amount of cleaned premise and hypothesis pairs.", "startOffset": 70, "endOffset": 91}, {"referenceID": 2, "context": "For example: WebQuestion (was introduced by the authors of SEMPRE system (Berant et al., 2013)) and The SimpleQuestions dataset 2.", "startOffset": 73, "endOffset": 94}], "year": 2016, "abstractText": "We apply a general recurrent neural network (RNN) encoder framework to community question answering (cQA) tasks. Our approach does not rely on any linguistic processing, and can be applied to different languages or domains. Further improvements are observed when we extend the RNN encoders with a neural attention mechanism that encourages reasoning over entire sequences. To deal with practical issues such as data sparsity and imbalanced labels, we apply various techniques such as transfer learning and multitask learning. Our experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score compared to an information retrieval-based approach, and achieve comparable performance to a strong handcrafted feature-based method.", "creator": "LaTeX with hyperref package"}}}