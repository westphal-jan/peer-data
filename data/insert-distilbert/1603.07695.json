{"id": "1603.07695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Part-of-Speech Relevance Weights for Learning Word Embeddings", "abstract": "this paper proposes a model to learn word embeddings with weighted contexts based on part - of - speech ( pos ) relevance weights. pos is a familiar fundamental element in natural language. however, state - proof of - the - art word type embedding models fail to consider it. only this paper later proposes to use position - dependent pos relevance weighting matrices to model roughly the inherent syntactic scaling relationship among words assembled within a context window. we utilize the pos relevance squared weights to model each word - context pairs during the word embedding training process. because the model proposed in this paper paper jointly optimizes word vectors and the pos fixed relevance matrices. experiments conducted on popular word analogy and word network similarity tasks all demonstrated, the effectiveness of the effectively proposed method.", "histories": [["v1", "Thu, 24 Mar 2016 18:22:39 GMT  (507kb,D)", "http://arxiv.org/abs/1603.07695v1", "Word embeddings"]], "COMMENTS": "Word embeddings", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["quan liu", "zhen-hua ling", "hui jiang", "yu hu"], "accepted": false, "id": "1603.07695"}, "pdf": {"name": "1603.07695.pdf", "metadata": {"source": "CRF", "title": "Part-of-Speech Relevance Weights for Learning Word Embeddings", "authors": ["Quan Liu", "Zhen-Hua Ling", "Hui Jiang", "Yu Hu"], "emails": ["quanliu@mail.ustc.edu.cn,", "zhling@ustc.edu.cn", "hj@cse.yorku.ca,", "yuhu@iflytek.com"], "sections": [{"heading": "1 Introduction", "text": "Word embedding that represents words into continuous vector space based on distributional hypothesis is an important research topic in the natural language processing community (Hinton et al., 1986; Turian et al., 2010; Mikolov et al., 2013a). State-of-theart word embedding models include the neural language models (Bengio et al., 2003; Mikolov et al., 2010), the C&W model (Collobert et al., 2011), the continuous bag-of-word (CBOW) and Skip-gram word2vec models (Mikolov et al., 2013a), and the GloVe model (Pennington et al., 2014). Word embedding techniques have been widely applied to various natural language processing tasks, including machine translation (Devlin et al., 2014; Wu\net al., 2014), sequence labelling (Collobert et al., 2011) and antonym selection (Chen et al., 2015). Under the framework of current word embedding models, word vectors are estimated based on the distributional hypothesis (Harris, 1954). It has been found that the learned vectors could explicitly encode many linguistic regularities and patterns (Mikolov et al., 2013b). However, it is still inadequate to learn high-quality representations just relying on word level distributional information collected from text corpora (Faruqui et al., 2015; Liu et al., 2015).\nTo improve the representation of word embeddings, some recent works have been proposed to incorporate various kinds of additional resources into the word representation learning framework. Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015). In Levy and Goldberg (2014), they investigated to use syntactic contexts that were derived from automatically produced dependency parse-trees for word representation training. Meanwhile, some people attempted to utilize multilingual parallel corpora to guide the word vector training process (Zhang et al., 2014; Hermann and Blunsom, 2014; Lu et al., 2015). Nevertheless, all those works failed to consider the basic POS information in the training corpus.\nTo exploit the effectiveness of POS information for word representations, this paper proposes a model to incorporate it into the training process of word embeddings. POS tags capture syntac-\nar X\niv :1\n60 3.\n07 69\n5v 1\n[ cs\n.C L\n] 2\n4 M\nar 2\ntic roles of words and ignore much of their lexical information (Jelinek, 1990). This paper considers that, for a word-context pair, the collocation of their POS tags encodes their inherent syntactic relationships. This paper proposes to model the syntactic relationships of word-context pairs with a set of position-dependent POS relevance weighting matrices. After that, all word-context pairs used for learning word embeddings are weighted by the POS relevance weights. Finally, this paper proposes to learn the relevance weights and word embeddings jointly using the stochastic gradient descend (SGD) algorithm. Experiments conducted on the popular word analogy and word similarity tasks have demonstrated that, by introducing POS relevance weight as a discriminative factor for context weighting, the quality of the learned word vectors is significantly improved comparing to the baseline model."}, {"heading": "2 Related Work", "text": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al., 2008) and name entity recognition (Turian et al., 2010). However, very few of them investigated to use POS information for distributed word representation. The most related one was the method proposed by Qiu et al. (2014) which trained one representation vector for each POS tag of a word and did not try to improve the representation ability of word embeddings. In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning."}, {"heading": "3 The proposed model", "text": "In this section, we present the proposed model that employs POS information for learning word embeddings, called PWE hereafter. Given a large training corpus, the POS information can be obtained efficiently by a state-of-the-art POS tagger. Consider a sequence of words S = {w1, ..., wN} with N word tokens. After conducting POS tagging, each word token wi is labelled as a specific POS tag zi. The corresponding word-POS pairs are denoted as\n\u3008wi, zi\u3009."}, {"heading": "3.1 Main framework", "text": "This paper proposes to incorporate POS information for learning word embeddings based on the CBOW model (Mikolov et al., 2013a). We firstly review the objective function of a typical CBOW model, which is to maximize the log likelihood of each token given its contexts:\nQcbow = 1\nT T\u2211 t=1 log p(wt|wt\u2212ct+c) (1)\nwhere c specifics the context window size, T is the token number of the training corpus. Meanwhile, the word prediction probability is\np(wt|wt\u2212ct+c) = exp\n( w\n(2) t \u00b7 v t+c t\u2212c ) \u2211V\nk=1 exp ( w (2) k \u00b7 v t+c t\u2212c ) (2) where vt+ct\u2212c is the context representation, w (2) t is the output vector of word wt, V is the vocabulary size. In the CBOW model, the context representation is calculated by bag-of-word averaging\nvt+ct\u2212c = \u2211\n\u2212c\u2264i\u2264c,i 6=0 w\n(1) t+i (3)\nwhere w(1)t+i is the word embedding for word wt+i. The CBOW model treats word-context pairs equally and fails to utilize the discrimination information encoded in their inherent syntactic relationships. Motivated by this deficiency, this paper proposes to use POS relevance weighting matrices to model each word-context pair. As a result, the context word vector for predicting the central word wt is calculated as a weighted sum rather than a simple average operation:\nvt+ct\u2212c = \u2211\n\u2212c\u2264i\u2264c,i 6=0 \u03a6i(zt+i, zi)w\n(1) t+i (4)\nwhere \u03a6i(zt+i, zi) is a core weighting factor that represents the relevance weight from POS tag zt+i to zt. The subscript i indicates the position distance between word wt+i and wt within the specific training context window. Therefore, the POS based weighting factors are position-dependent.\nThe main framework of the proposed POS based word embedding model is depicted in Figure 1. In this framework, we propose to model the wordcontext pair based on the relevance weights between the corresponding two POS tags.\nFor example, if we consider the central word wt and its context word wt\u22121, we define a POS relevance weighting matrix \u03a6\u22121 from which \u03a6\u22121(zt\u22121, zt) is used as a weighting factor based on the corresponding POS tags zt\u22121 and zt. Meanwhile, considering word wt and its context word wt+2, since the distance between them is 2, we define another POS relevance weighting matrix \u03a62 where again we could extract a factor \u03a62(zt+2, zt) for context weighting."}, {"heading": "3.2 Optimization algorithm", "text": "In the PWE framework, all the POS relevance weights are updated jointly with the word representations during training. This work proposes to apply the stochastic gradient descend (SGD) algorithm to parameter learning. The key issue for training PWE model is to calculate the derivatives of the POS relevance weighting matrices. The partial derivatives of the objective function with respect to word embeddings are\n\u2202Qcbow \u2202w\n(1) t+i\n= \u03a6i(zt+i, zt) \u2202Qcbow \u2202vt+ct\u2212c\n(5)\nwhere \u2202Qcbow \u2202vt+ct\u2212c could be calculated efficiently under the typical word embedding framework. Meanwhile, the partial derivative with respect to the POS relevance weight can be computed as\n\u2202Qcbow \u2202\u03a6i(zt+i, zt) = \u2202Qcbow \u2202vt+ct\u2212c \u00b7w(1)t+i (6)\nFor efficiency purpose, this paper uses the negative sampling technique during training process while the POS tags of the negative samples is set to be the same as the tags of the training tokens. The baseline models are trained based on dynamic context window using the popular word2vec toolkit (Mikolov et al., 2013a)1. In the next section, we will report our experimental results without using dynamic context window and prove the effectiveness of the proposed model."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experimental setup", "text": "In this section, we present the experimental setup, which including the training corpus, POS tagger and parameter settings for all experiments.\nTraining Corpus: This paper used the April 2010 snapshot of the Wikipedia corpus (Shaoul and Westbury, 2010) with a total of about 2 million articles and 1 billion tokens. The Wikipedia corpus was preprocessed using the perl script from the Matt Mahoney\u2019s page2. After normalization, we constructed a vocabulary with 212,300 distinct words by discarding words that occur less than 50 times.\nPart-of-speech Tagger: To obtain the POS tags for each training token, we used the OpenNLP toolkit3 for part-of-speech tagging. The tag set is the Penn Treebank POS tag set (Marcus et al., 1993), which consists of 36 common POS tags and 6 symbol tags.\nExperiment Setting: As for word embedding training, we set the vector dimensionality to 300 and the negative sample number to 5. The learning rate was set to 0.025 and then decreased with the training process (Mikolov et al., 2013a). The context window size was set to 5 for all models. All the weights in the POS relevance weighting matrices were initialized uniformly. For all the experimental results reported in this paper, we used just one training epoch for comparison. Meanwhile, all the experimental results were obtained based on repeated running and averaging.\n1https://code.google.com/p/word2vec/ 2http://mattmahoney.net/dc/textdata.html 3http://opennlp.apache.org/"}, {"heading": "4.2 Qualitative analysis", "text": "We firstly plot the top 500 words in vocabulary based on word vectors learned by the CBOW and the proposed model in Figure 2 and Figure 3. For simplified visulization, all main POS tags are divided into 5 coarse-grained groups {N, V, J, R, Other} as the work of Qiu et al. (2014). The corresponding colors in the visualization figures are {red, canary, green, blue, pink, red} respectively. We find the proposed model could group words with similar POS category. We also test our model based on unsupervised POS induction for the top 500 words (Christodoulopoulos et al., 2010; Yatbaz et al., 2012). By using the k-means clustering algorithm, we map each cluster to the gold standard tag that is most common for the words in that cluster. The cluster purity is improved from 53.9% to 74.3% when comparing the baseline CBOW model and the proposed model. The qualitative analysis indicates that the proposed model has the ability to model the syntactic relationships of word meanings."}, {"heading": "4.3 Syntactic word analogy task", "text": "Here we use two syntactic word analogical reasoning datasets for experiments. The first one named as MSR is a dataset that contains 8000 morphosyntactic analogy questions (Mikolov et al., 2013c). The other one named as SYN is a dataset proposed in Mikolov et al. (2013a), which contains 10675 syntactic questions. To answer those analogy questions, we firstly remove all out-of-vocabulary words4 and then use the typical similarity multiplication method to find the correct answers from the entire vocabulary (Levy et al., 2014). Our experimental results are shown in Table 1.\nThe experimental results on the two syntactic\n4This removed 1574 instances from the MSR dataset and 66 instances from the SYN dataset.\nword analogy tasks have shown that, the proposed PWE model achieves significant improvements on the syntactic word analogy tasks. Using POS relevance weights for context weighting improves the representation of syntactic relationships in the learned word vectors."}, {"heading": "4.4 Word similarity tasks", "text": "We test our models on WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2012), and MC (Miller and Charles, 1991) tasks of word similarity. All the experimental results are given in Table 2.\nExperimental results shown in Table 1 indicate that the proposed PWE model achieves consistent improvements in three word similarity tasks. This proves that the incorporation of POS relevance weights for discriminatively context weighting is useful for better modeling the word-context patterns, so as to word embeddings."}, {"heading": "5 Conclusion", "text": "This paper has proposed a model for learning distributed word representations with context weighting based on POS relevance weights. This paper designs position-dependent POS relevance weighting matrices for weighting word-context pairs during the training process of word embeddings. In the proposed model, word embeddings and the POS relevance weighting matrices are jointly learned using the stochastic gradient descend algorithm. Experiments conducted on the popular word analogy and word similarity tasks have all demonstrated the effectiveness of the proposed method."}], "references": [{"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["Brown et al.1992] Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Revisiting word embedding for contrasting meaning", "author": ["Chen et al.2015] Zhigang Chen", "Wei Lin", "Qian Chen", "Xiaoping Chen", "Si Wei", "Xiaodan Zhu", "Hui Jiang"], "venue": "Proceedings of ACL", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Two decades of unsupervised pos induction: How far have we come", "author": ["Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proceedings of ACL,", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "Proceedings of NAACL", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of WWW,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Pos tagging versus classes in language modeling", "author": ["Peter A Heeman"], "venue": "In Proceedings of the 6th Workshop on Very Large Corpora,", "citeRegEx": "Heeman.,? \\Q1998\\E", "shortCiteRegEx": "Heeman.", "year": 1998}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Blunsom2014] Karl Moritz Hermann", "Phil Blunsom"], "venue": "In Proceedings of the ACL,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Distributed representations. In Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["James L McClelland", "David E Rumelhart"], "venue": "Volume 1: Foundations,", "citeRegEx": "Hinton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Self-organized language modeling for speech recognition", "author": ["Fred Jelinek"], "venue": "Readings in speech recognition,", "citeRegEx": "Jelinek.,? \\Q1990\\E", "shortCiteRegEx": "Jelinek.", "year": 1990}, {"title": "Simple semi-supervised dependency parsing", "author": ["Koo et al.2008] Terry Koo", "Xavier Carreras P\u00e9rez", "Michael Collins"], "venue": null, "citeRegEx": "Koo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Dependencybased word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of ACL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy et al.2014] Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Liu et al.2015] Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu"], "venue": "Proceedings of ACL", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu et al.2015] Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of NAACL-HLT,", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991] George A Miller", "Walter G Charles"], "venue": "Language and cognitive processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning word representation considering proximity and ambiguity", "author": ["Qiu et al.2014] Lin Qiu", "Yong Cao", "Zaiqing Nie", "Yong Rui"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Qiu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta", "author": ["Shaoul", "Westbury2010] Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul et al\\.", "year": 2010}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Improve statistical machine translation with context-sensitive bilingual semantic embedding model", "author": ["Wu et al.2014] Haiyang Wu", "Daxiang Dong", "Xiaoguang Hu", "Dianhai Yu", "Wei He", "Hua Wu", "Haifeng Wang", "Ting Liu"], "venue": "Proceedings of EMNLP", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "Rc-net: A general framework for incorporating knowledge into word representations", "author": ["Xu et al.2014] Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu"], "venue": "In Proceedings of CIKM,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Learning syntactic categories using paradigmatic representations of word context", "author": ["Enis Sert", "Deniz Yuret"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods", "citeRegEx": "Yatbaz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2012}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In Proceedings of ACL,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Bilinguallyconstrained phrase embeddings for machine translation", "author": ["Zhang et al.2014] Jiajun Zhang", "Shujie Liu", "Mu Li", "Ming Zhou", "Chengqing Zong"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "Word embedding that represents words into continuous vector space based on distributional hypothesis is an important research topic in the natural language processing community (Hinton et al., 1986; Turian et al., 2010; Mikolov et al., 2013a).", "startOffset": 177, "endOffset": 242}, {"referenceID": 27, "context": "Word embedding that represents words into continuous vector space based on distributional hypothesis is an important research topic in the natural language processing community (Hinton et al., 1986; Turian et al., 2010; Mikolov et al., 2013a).", "startOffset": 177, "endOffset": 242}, {"referenceID": 0, "context": "State-of-theart word embedding models include the neural language models (Bengio et al., 2003; Mikolov et al., 2010), the C&W model (Collobert et al.", "startOffset": 73, "endOffset": 116}, {"referenceID": 19, "context": "State-of-theart word embedding models include the neural language models (Bengio et al., 2003; Mikolov et al., 2010), the C&W model (Collobert et al.", "startOffset": 73, "endOffset": 116}, {"referenceID": 5, "context": ", 2010), the C&W model (Collobert et al., 2011), the continuous bag-of-word (CBOW) and Skip-gram word2vec models (Mikolov et al.", "startOffset": 23, "endOffset": 47}, {"referenceID": 24, "context": ", 2013a), and the GloVe model (Pennington et al., 2014).", "startOffset": 30, "endOffset": 55}, {"referenceID": 6, "context": "Word embedding techniques have been widely applied to various natural language processing tasks, including machine translation (Devlin et al., 2014; Wu et al., 2014), sequence labelling (Collobert et al.", "startOffset": 127, "endOffset": 165}, {"referenceID": 28, "context": "Word embedding techniques have been widely applied to various natural language processing tasks, including machine translation (Devlin et al., 2014; Wu et al., 2014), sequence labelling (Collobert et al.", "startOffset": 127, "endOffset": 165}, {"referenceID": 5, "context": ", 2014), sequence labelling (Collobert et al., 2011) and antonym selection (Chen et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 3, "context": ", 2011) and antonym selection (Chen et al., 2015).", "startOffset": 30, "endOffset": 49}, {"referenceID": 7, "context": "However, it is still inadequate to learn high-quality representations just relying on word level distributional information collected from text corpora (Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 152, "endOffset": 192}, {"referenceID": 16, "context": "However, it is still inadequate to learn high-quality representations just relying on word level distributional information collected from text corpora (Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 152, "endOffset": 192}, {"referenceID": 29, "context": "Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 155, "endOffset": 233}, {"referenceID": 7, "context": "Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 155, "endOffset": 233}, {"referenceID": 16, "context": "Typically, some knowledge enhanced word embedding models tried to exploit lexical knowledge resources as semantic constraints for learning word embeddings (Yu and Dredze, 2014; Xu et al., 2014; Faruqui et al., 2015; Liu et al., 2015).", "startOffset": 155, "endOffset": 233}, {"referenceID": 32, "context": "Meanwhile, some people attempted to utilize multilingual parallel corpora to guide the word vector training process (Zhang et al., 2014; Hermann and Blunsom, 2014; Lu et al., 2015).", "startOffset": 116, "endOffset": 180}, {"referenceID": 17, "context": "Meanwhile, some people attempted to utilize multilingual parallel corpora to guide the word vector training process (Zhang et al., 2014; Hermann and Blunsom, 2014; Lu et al., 2015).", "startOffset": 116, "endOffset": 180}, {"referenceID": 7, "context": ", 2014; Faruqui et al., 2015; Liu et al., 2015). In Levy and Goldberg (2014), they investigated to use syntactic contexts that were derived from automatically produced dependency parse-trees for word representation training.", "startOffset": 8, "endOffset": 77}, {"referenceID": 12, "context": "tic roles of words and ignore much of their lexical information (Jelinek, 1990).", "startOffset": 64, "endOffset": 79}, {"referenceID": 9, "context": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al.", "startOffset": 91, "endOffset": 105}, {"referenceID": 13, "context": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al., 2008) and name entity recognition (Turian et al.", "startOffset": 126, "endOffset": 144}, {"referenceID": 27, "context": ", 2008) and name entity recognition (Turian et al., 2010).", "startOffset": 36, "endOffset": 57}, {"referenceID": 12, "context": "In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning.", "startOffset": 91, "endOffset": 140}, {"referenceID": 1, "context": "In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning.", "startOffset": 91, "endOffset": 140}, {"referenceID": 9, "context": "In order to exploit POS information, this paper borrows the idea of language modeling from (Jelinek, 1990; Brown et al., 1992; Heeman, 1998) and proposes to use POS relevance weighting matrices to model each word-context pair for word embedding learning.", "startOffset": 91, "endOffset": 140}, {"referenceID": 8, "context": "POS has been used in various natural language processing tasks such like language modeling (Heeman, 1998), dependency parsing (Koo et al., 2008) and name entity recognition (Turian et al., 2010). However, very few of them investigated to use POS information for distributed word representation. The most related one was the method proposed by Qiu et al. (2014) which trained one representation vector for each POS tag of a word and did not try to improve the representation ability of word embeddings.", "startOffset": 92, "endOffset": 361}, {"referenceID": 18, "context": "The tag set is the Penn Treebank POS tag set (Marcus et al., 1993), which consists of 36 common POS tags and 6 symbol tags.", "startOffset": 45, "endOffset": 66}, {"referenceID": 4, "context": "We also test our model based on unsupervised POS induction for the top 500 words (Christodoulopoulos et al., 2010; Yatbaz et al., 2012).", "startOffset": 81, "endOffset": 135}, {"referenceID": 30, "context": "We also test our model based on unsupervised POS induction for the top 500 words (Christodoulopoulos et al., 2010; Yatbaz et al., 2012).", "startOffset": 81, "endOffset": 135}, {"referenceID": 24, "context": "For simplified visulization, all main POS tags are divided into 5 coarse-grained groups {N, V, J, R, Other} as the work of Qiu et al. (2014). The corresponding colors in the visualization figures are {red, canary, green, blue, pink, red} respectively.", "startOffset": 123, "endOffset": 141}, {"referenceID": 14, "context": "To answer those analogy questions, we firstly remove all out-of-vocabulary words4 and then use the typical similarity multiplication method to find the correct answers from the entire vocabulary (Levy et al., 2014).", "startOffset": 195, "endOffset": 214}, {"referenceID": 17, "context": "The first one named as MSR is a dataset that contains 8000 morphosyntactic analogy questions (Mikolov et al., 2013c). The other one named as SYN is a dataset proposed in Mikolov et al. (2013a), which contains 10675 syntactic questions.", "startOffset": 94, "endOffset": 193}, {"referenceID": 8, "context": "We test our models on WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 2, "context": ", 2001), MEN (Bruni et al., 2012), and MC (Miller and Charles, 1991) tasks of word similarity.", "startOffset": 13, "endOffset": 33}], "year": 2016, "abstractText": "This paper proposes a model to learn word embeddings with weighted contexts based on part-of-speech (POS) relevance weights. POS is a fundamental element in natural language. However, state-of-the-art word embedding models fail to consider it. This paper proposes to use position-dependent POS relevance weighting matrices to model the inherent syntactic relationship among words within a context window. We utilize the POS relevance weights to model each word-context pairs during the word embedding training process. The model proposed in this paper paper jointly optimizes word vectors and the POS relevance matrices. Experiments conducted on popular word analogy and word similarity tasks all demonstrated the effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}