{"id": "1603.07954", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Mar-2016", "title": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning", "abstract": "initially in traditional formulations, information extraction control systems operate on a fixed collection of documents. in this work, we explore the systemic task of acquiring and quickly incorporating external evidence to improve extraction accuracy. this process entails query reformulation for search, extraction from new sources and reconciliation of hidden extracted values, periods which are repeated until sufficient structured evidence is collected. we approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. we employ a deep q - network, trained to locally optimize a reward function that reflects extraction accuracy while penalizing extra effort. our experiments on a publicly available database capable of shooting incidents demonstrate that our system outperforms traditional extractors satisfaction by 7. 2 % on average.", "histories": [["v1", "Fri, 25 Mar 2016 16:38:54 GMT  (1314kb,D)", "http://arxiv.org/abs/1603.07954v1", "10 pages"], ["v2", "Tue, 14 Jun 2016 03:24:37 GMT  (1175kb,D)", "http://arxiv.org/abs/1603.07954v2", "Updated with additional experiments, 10 pages"], ["v3", "Tue, 27 Sep 2016 23:33:28 GMT  (1181kb,D)", "http://arxiv.org/abs/1603.07954v3", "Appearing in EMNLP 2016 (12 pages incl. supplementary material)"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karthik narasimhan", "adam yala", "regina barzilay"], "accepted": true, "id": "1603.07954"}, "pdf": {"name": "1603.07954.pdf", "metadata": {"source": "CRF", "title": "Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning", "authors": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay"], "emails": ["karthikn@csail.mit.edu", "adamyala@mit.edu", "regina@csail.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Despite a great deal of advancement, Information Extraction (IE) is still far from a solved problem. The task is challenging since the same relation can be expressed in many different ways, all of which may not appear in the training data. Consider an IE system that aims to identify entities such as the perpetrator and the number of people killed in a shooting incident (Figure 1). The document does not explicitly mention the shooter (Scott Westerhuis), but instead refers to him as a suicide victim. Extraction of the number of fatally shot victims is similarly difficult, as the system needs to infer that \"A couple and four children\" means six people. One way to handle such difficult cases\n1Code and data will be made available.\nis to augment information extraction systems with elaborate semantic inference (Chai and Biermann, 1997; Mooney, 1999; Poon and Domingos, 2007; Hogenboom et al., 2013).\nIn this paper, we explore an alternative approach to boost extraction accuracy \u2013 using external information sources. Specifically, our strategy is to find other documents that contain the information sought, expressed in a form that a basic extractor can \"understand\". For instance, Figure 2 shows two other articles describing the same event, wherein the entities of interest \u2013 the number of people killed and the name of the shooter \u2013 are expressed explicitly. Processing such stereotypical phrasing is easier for most extraction systems, compared to analyzing the original source document. This approach is particularly suitable for extracting information from news where a typical event is covered by multiple news outlets.\nThe challenges, however, lie in (1) performing event coreference (i.e. retrieving suitable articles describing the same incident) and (2) reconciling\nar X\niv :1\n60 3.\n07 95\n4v 1\n[ cs\n.C L\n] 2\n5 M\nar 2\n01 6\nthe entities extracted from these different documents. Querying the web (using the source article\u2019s title for instance) often retrieves documents about other incidents with a tangential relation to the original story. For example, the query \u201c4 adults, 1 teenager shot in west Baltimore 3 april 2015\u201d yields only two relevant articles among the top twenty results on Bing search, while returning other shooting events at the same location. Moreover, the values extracted from these different sources require resolution since some of them might be inaccurate.\nOne solution to this problem would be to perform a single search to retrieve articles on the same event and then reconcile values extracted from them (say, using a meta-classifier). However, if the confidence of the new set of values is still low, we might wish to perform further queries. Thus, the problem is inherently sequential, requiring alternating phases of querying to retrieve articles and integrating the extracted values.\nWe address these challenges using a Reinforcement Learning (RL) approach that combines query reformulation, extraction from new sources, and value reconciliation. To effectively select among possible actions, our state representation encodes information about the current and new entity values along with the similarity between the source article and the newly retrieved document. The model learns to select good actions for both article retrieval and value reconciliation in order to optimize the reward function, which reflects extraction accuracy and includes penalties for extra moves. We train the RL agent using a Deep Q-Network\n(DQN) (Mnih et al., 2015) that is used to predict both querying and reconciliation choices simultaneously. While we use a maximum entropy model as the base extractor, this framework can be inherently applied to other extraction algorithms.\nWe evaluate our system using a publicly available database of mass shootings in the United States. The database is populated by volunteers and includes the source articles. Each incident consists of four entities \u2013 ShooterName, NumKilled, NumWounded and City. Our experiments demonstrate that the final RL model outperforms basic extractors as well as a meta-classifier baseline across all the relations. For instance, the average accuracy improvement over the metaclassifier is 7%, with a maximum gain of 10.3% achieved on the City entity."}, {"heading": "2 Related Work", "text": "Open Information Extraction Existing work in open IE has used external sources from the web to improve extraction accuracy and coverage (Agichtein and Gravano, 2000; Etzioni et al., 2011; Fader et al., 2011; Wu and Weld, 2010). Such research has focused on identifying multiple instances of the same relation, independent of the context in which this information appears. In contrast, our goal is to extract information from additional sources about a specific event described in a source article. Therefore, the novel challenge of our task resides in performing event coreference (Lee et al., 2012; Bejan and Harabagiu, 2014) (i.e identifying other sources describing the same event) while simultaneously reconciling extracted information. Moreover, relations typically considered by open IE systems have significantly higher coverage in online documents than a specific incident described in a few news sources. Hence, we require a different mechanism for finding and reconciling online information.\nEntity linking, multi-document extraction and event coreference Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input documents (Mann and Yarowsky, 2005; Han et al., 2011; Durrett and Klein, 2014). Since this setup already includes multiple input documents, the model is not required to look for additional sources or decide on their relevance. Also, while the set of input documents overlap in terms of entities mentioned, they\ndo not necessarily describe the same event. Given these differences in setup, the challenges and opportunities of the two tasks are distinct.\nKnowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015). Though our work also aims at increasing extraction recall for a database, traditional KBC approaches do not require searching for additional sources of information. West et al. (2014) explore query reformulation in the context of KBC. Using existing search logs, they learn how to formulate effective queries for different types of database entries. Once query learning is completed, the model employs several selected queries, and then aggregates the results based on retrieval ranking. As we demonstrate in our experiments, a similar approach does not work well for our task \u2013 since many retrieved articles do not describe the same event as the original article, ranking-based aggregation introduces noise. Our model handles this noise by learning to select event-related articles.\nKanani and McCallum (2012) also combine search and information extraction. In their task of faculty directory completion, the system has to find documents from which to extract desired information. They employ reinforcement learning to address computational bottlenecks, by minimizing the number of queries, document downloads and extraction action. The extraction accuracy is not part of this optimization, since the baseline IE system achieves high performance on the relations of interest. Hence, given different design goals, the two RL formulations are very different. Our approach is also close in spirit to the AskMSR system (Banko et al., 2002) which aims at using information redundancy on the web to better answer questions. Though our goal is similar, we learn to query and consolidate the different sources of information instead of using pre-defined rules."}, {"heading": "3 Framework", "text": "We model the information extraction task as a Markov Decision Process (MDP), where the model learns to utilize external sources to improve upon extractions from a source article (see Figure 3). The MDP framework allows us to dynami-\ncally incorporate entity predictions while also providing flexibility to choose the type of articles to extract from. At each step, the system has to reconcile extracted values from a related article with the current set of values, and decide on the next query for retrieving more articles.\nWe represent the MDP as a tuple \u3008S,A, T,R\u3009, where S = {s} is the space of all possible states, A = {a = (d, q)} is the set of all actions, R(s, a) is the reward function, and T (s\u2032|s, a) is the transition function. We describe these in detail below.\nStates The state s in our MDP consists of the extractor\u2019s confidence in predicted entity values, the context from which the values are extracted and the similarity between the new document and the original one. We represent the state as a continuous real-valued vector (Figure 3) incorporating these pieces of information: 1. Confidence scores of current and newly ex-\ntracted entity values. 2. One-hot encoding of matches between current\nand new values. 3. Unigram/tf-idf counts2 of context words. These\nare words that occur in the neighborhood of the entity values in a document (e.g. the words which, left, people and wounded in the phrase \u201cwhich left 5 people wounded\u201d). 4. tf-idf similarity between the original article and the new article.\nActions At each step, the agent is required to take two actions - a reconciliation decision d and a query choice q. The decision d on the newly extracted values can be one of the following: (1) accept a specific entity\u2019s value, (2) accept all entity values, (3) reject all values. The current values and confidence scores are simply updated with the accepted values and the corresponding confidences.3 The choice q is used to either stop the episode or continue to query using a set of automatically generated alternatives (details below) in order to retrieve the next article.\nRewards The reward function is chosen to maximize the final extraction accuracy while minimizing the number of queries. The accuracy component is calculated using the difference between the accuracy of the current and the previous set of en-\n2Counts are computed on the documents used to train the basic extraction system.\n3We also experiment with other forms of value reconciliation. See Section 5 for details.\ntity values: R(s, a) = \u2211 j Acc(ejcur)\u2212 Acc(ejprev)\nThere is a negative reward per step to penalize the agent for longer episodes.\nQueries The queries are based on automatically generated templates, created using the title of an article along with words most likely to co-occur with each entity type in the training data.4 We use a search engine to query the web for articles on the same event as the source article and retrieve the top k links per query.5 Documents that are more than a month older than the original article are filtered out of the search results.\nTransitions Each episode starts off with a single source article xi from which an initial set of entity values are extracted. The subsequent steps in the episode involve the extra articles, downloaded using different types of query formulations based on the source article. A single step in the episode consists of the agent being given the state s containing information about the current and new set of values (extracted from a single article) using which the next action a = (d, q) is chosen. The transition function T (s\u2032|s, a) incorporates the reconciliation decision d from the agent in state s along with the values from the next article retrieved using query q and produces the next state s\u2032. Algorithm 1 details the entire MDP framework.\n4Stop words, numeric terms and proper nouns are filtered out.\n5k=20 in our experiments.\nAlgorithm 1 MDP framework for Information Extraction\n1: Initialize set of original articles X 2: for xi \u2208 X do 3: for each query template T q do 4: Download articles with query T q(xi) 5: Queue retrieved articles in Y qi 6: for epoch = 1,M do 7: for i = 1, |X| do //episode 8: Extract entities e0 from xi 9: ecur \u2190 e0\n10: q\u2190 0, r\u2190 0 //query type, reward 11: while Y qi not empty do 12: Pop next article y from Y qi 13: Extract entities enew from y 14: Compute tf-idf similarity Z(xi, y) 15: Compute context vector C(y) 16: Form state s using ecur, enew, Z(xi, y) and C(y) 17: Send (s, r) to agent 18: Get decision d, query q from agent 19: if q == \u201cend_episode\u201d then break 20: eprev \u2190 ecur 21: ecur \u2190 Reconcile(ecur, enew, d) 22: r\u2190 \u2211 j Acc(e j cur) - Acc(e j prev) 23: Send (send, r) to agent"}, {"heading": "4 Reinforcement Learning for Information Extraction", "text": "In order to learn a good policy for an agent, we utilize the paradigm of Reinforcement Learning (RL). The MDP described in the previous section\ncan be viewed in terms of a sequence of transitions (s, a, r, s\u2032). The agent typically utilizes a state-action value function Q(s, a) to determine which action a to perform in state s. A commonly used technique for learning an optimal value function is Q-learning (Watkins and Dayan, 1992), in which the agent iteratively updates Q(s, a) using the rewards obtained from episodes. The updates are derived from the Bellman equation (Sutton and Barto, 1998) which provides a recursive equation for the optimal Q:\nQi+1(s, a) = E[r + \u03b3max a\u2032\nQi(s \u2032, a\u2032) | s, a]\nHere, \u03b3 is a factor discounting the value of future rewards and the expectation is taken over all transitions involving state s and action a.\nSince our problem involves a continuous state space S, we need a function approximator for Q(s, a), for which we use a Deep Q-Network (DQN) (Mnih et al., 2015). The DQN, in which the Q-function is approximated using a deep neural network, has been shown to learn better value functions than linear approximators (Narasimhan et al., 2015; He et al., 2015) and can capture nonlinear interactions between the different pieces of information in our state.\nWe use a neural network with several linear and non-linear layers. The network takes the continuous state vector s as input and predictsQ(s, d) and Q(s, q) for reconciliation decisions d and query choices q simultaneously using different output layers (Figure 4).\nParameter Learning The parameters \u03b8 of the DQN are learnt using stochastic gradient descent with RMSprop (Tieleman and Hinton, 2012).\nEach parameter update aims to close the gap between the Q(st, at; \u03b8) predicted by the DQN and the expected Q-value rt + \u03b3maxaQ(st+1, a; \u03b8). Following Mnih et al. (2015), we make use of a (separate) target Q-network to calculate the expected Q-value, in order to have stable updates. The target Q-network is periodically updated with the parameters from the continuously evolving Qnetwork. We also make use of an experience replay memory D to store transitions. To perform updates, we sample a batch of transitions (s\u0302, a\u0302, s\u0302\u2032, r) at random fromD and aim to minimize the loss function:\nL(\u03b8) = Es\u0302,a\u0302[(y \u2212Q(s\u0302, a\u0302; \u03b8))2]\nwhere y = r + \u03b3maxa\u2032 Q(s\u0302\u2032, a\u2032; \u03b8target) is the target Q-value. The learning updates are made every training step using the following gradients:\n\u2207\u03b8iLi(\u03b8i) = Es\u0302,a\u0302[2(yi \u2212Q(s\u0302, a\u0302; \u03b8i))\u2207\u03b8iQ(s\u0302, a\u0302; \u03b8i)]\nAlgorithm 2 details the entire training procedure."}, {"heading": "5 Experimental Setup", "text": "Data We perform experiments with data collected from the Gun Violence archive,6 a website tracking shootings in the United States. The data contains a news article on each shooting and annotations for (1) the name of the shooter, (2) the number of people killed, (3) the number of people wounded, and (4) the city where the incident took place. We consider these as the entities of interest, to be extracted from the articles. Table 1 provides details on the data.\nFor each article, we download extra articles using the Bing Search API7 with different automatically generated queries (Table 2). We use only the source articles from the train portion to learn the parameters of the basic extractor. The entire train set with downloaded articles is used to train the DQN agent and the meta-classifier (described below). The entire test set (source + downloaded articles) is used to evaluate all the models.\nExtraction model We use a 5-way maximum entropy classifier as the base extraction system. The classifier is used to tag each word in a document with one of the following tags: {ShooterName, NumKilled, NumWounded, City, Other}.\n6http://www.shootingtracker.com/Main_ Page\n7http://www.bing.com/toolbox/ bingsearchapi\nAlgorithm 2 Training Procedure for DQN agent with -greedy exploration 1: Initialize experience memory D 2: Initialize parameters \u03b8 randomly 3: for episode = 1,M do 4: Initialize environment and get start state s1 5: for t = 1, N do 6: if random() < then 7: Select a random action at 8: else 9: Compute Q(st, a) for all actions a 10: Select at = argmax Q(st, a) 11: Execute action at and observe reward rt and new state st+1 12: Store transition (st, at, rt, st+1) in D 13: Sample random mini batch of transitions (sj , aj , rj , sj+1) from D\n14: Set yj = { rj if sj+1 is terminal rj + \u03b3 maxa\u2032 Q(sj+1, a\u2032; \u03b8) if sj+1 is non-terminal 15: Perform gradient descent step on the loss L(\u03b8) = (yj \u2212Q(sj , aj ; \u03b8))2\nThen, for each tag except Other, we choose the mode of the values to obtain the set of extractions from the article. Table 3 shows features used in the classifier. We use a context window c = 4 to calculate features on the neighboring words. We also experimented with a Conditional Random Field (CRF) (with the same features) for the sequence tagging but obtained worse empirical performance (see Section 6).\nEvaluation We evaluate the extracted entity values against the gold annotations and report the\ncorpus-level average accuracy on each entity type. For shooter name, the annotations (and the news articles) often contain multiple names (first and last) in various combinations, so we consider retrieving either name as a successful extraction. For all other entities, we look for exact matches.\nBaselines We explore 4 types of baselines: Basic extractors: We use the CRF and the 5- way Maxent classifier mentioned previously. Aggregation systems: We examine two systems that perform different types of value reconciliation. The first model (Confidence) chooses entity values with the highest confidence score assigned by the base extractor. The second system (Majority) takes a majority vote over all values extracted from these articles. Both methods filter new entity values by using a threshold \u03c4 on the cosine similarity over the tf-idf representation of the source article and the new article.\nMeta-classifer: To demonstrate the importance of modeling the problem in the RL framework, we\nconsider a meta-classifier baseline. The classifier operates over the same input state space and produces the same set of reconciliation decisions {d} as the DQN. We train this classifier by using the original source article for each event along with a related downloaded article to compute the state. If the downloaded article has the correct value and the original one does not, we label it as a positive example for that entity class. If multiple such entity classes exist, we create several training instances with appropriate labels, and if none exist, we use the label corresponding to the reject all action. For each test event, the classifier is used to provide decisions for all the downloaded articles and the final extraction is performed by aggregating the value predictions using the Confidencebased scheme described above.\nOracle: Finally, we also have an ORACLE score which is computed assuming perfect reconciliation and querying decisions on top of the Maxent base extractor. This helps us analyze the contribution of the RL system in isolation of the inherent limitations of the base extractor.\nRL models We perform experiments using two variants of RL agents \u2013 (1) RL-Basic, which performs only reconciliation decisions8, and (2) RLExtract, our full system incorporating both reconciliation and query decisions. We train both models for 10000 steps every epoch using the Maxent classifier as the base extractor, and evaluate on the entire test set every epoch. The final accuracy scores reported are averaged over three independent runs; each run\u2019s score is averaged over 20 epochs after 100 epochs of training. The penalty reward per step is set to -0.001. For the DQN, we used a replay memory D of size 500k, and a discount (\u03b3) of 0.8. We set the learning rate to 2.5 \u2217 10\u22125. The in -greedy exploration is annealed from 1 to 0.1 over 500k transitions. The target-Q network is updated every 5000 steps."}, {"heading": "6 Results", "text": "Performance Table 4 demonstrates that our system (RL-Extract) obtains a substantial gain in accuracy over the basic extractors on all entity types. For instance, RL-Extract is 11.4% more accurate than the basic Maxent extractor on City and 7.1% better on NumKilled, while also achieving gains of more than 5% on the other entities. We can also\n8Articles are presented to the agent in a round-robin fashion from the different query lists.\nobserve that simple aggregation schemes like the Confidence and Majority baselines9 don\u2019t handle the complexity of the task well. RL-Extract outperforms these by 7.2% averaged over all entities. Further, the importance of sequential decisionmaking is established by RL-Extract performing significantly better than the meta-classifier (7.0% on average over all entities). Finally, we see the advantage of enabling the RL system to select queries as our full model RL-Extract obtains a 6.8% average improvement over RL-Basic.\nFigure 5 shows the learning curve of the agent by measuring reward on the test set after each epoch of training. We observe that the reward improves gradually and the accuracy on each entity increases simultaneously.\nAnalysis We also analyze the importance of different reconciliation schemes, rewards and\n9Results shown with best \u03c4 (=0.8).\ncontext-vectors in RL-Extract (Table 5). In addition to simple replacement (Replace), we also experiment with using Confidence and Majoritybased reconciliation schemes for RL-Extract. We observe that the Replace scheme performs much better than the others (2-6% on all entities) and believe this is because it provides the agent with more flexibility in choosing the final values.\nFrom the same table, we also see that using the tf-idf counts of context words as part of the state provides better performance than using no context or using simple unigram counts. In terms of reward structure, providing rewards after each step is empirically found to be significantly better (>10% on average) compared to a single delayed reward per episode.\nThe last column shows the average number of steps per episode \u2013 the values range from 6.8 to 10.0 steps for the different schemes. The best system (RL-Extract with Replace, tf-idf and stepbased rewards) uses 9.4 steps per episode.\nTable 6 provides some examples where our model is able to extract the right values when the baseline fails. One can see that in most cases\nthis is due to the model making use of articles with prototypical language or articles containing the entities in readily extractable form."}, {"heading": "7 Conclusions", "text": "In this paper, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy. This process comprises query reformulation for search, extraction from new sources and reconciliation of extracted values, repeated until sufficient evidence is obtained. We use a Reinforcement Learning framework and learn optimal action sequences to maximize extraction accuracy while penalizing extra effort. We show that our model, trained as a Deep Q-Network, outperforms traditional extractors by 7.2% on average. We also demonstrate the importance of sequential decision making by comparing our model to a meta-classifier operating on the same space, obtaining a 7% gain in accuracy."}, {"heading": "Acknowledgements", "text": "We thank Tao Lei, David Alvarez and Ramya Ramakrishnan for helpful discussions and feedback.\nWe also thank the members of the MIT NLP group for insightful comments."}], "references": [{"title": "Snowball: Extracting relations from large plain-text collections", "author": ["Agichtein", "Gravano2000] Eugene Agichtein", "Luis Gravano"], "venue": "In Proceedings of the fifth ACM conference on Digital libraries,", "citeRegEx": "Agichtein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Agichtein et al\\.", "year": 2000}, {"title": "Askmsr: Question answering using the worldwide web", "author": ["Banko et al.2002] Michele Banko", "Eric Brill", "Susan Dumais", "Jimmy Lin"], "venue": "In Proceedings of 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases,", "citeRegEx": "Banko et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2002}, {"title": "Unsupervised event coreference resolution", "author": ["Bejan", "Sanda Harabagiu"], "venue": "Computational Linguistics,", "citeRegEx": "Bejan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bejan et al\\.", "year": 2014}, {"title": "The use of lexical semantics in information extraction", "author": ["Chai", "Biermann1997] Joyce Yue Chai", "Alan Biermann"], "venue": "In Proceedings of the ACL Workshop on Natural Language Learning. Citeseer", "citeRegEx": "Chai et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chai et al\\.", "year": 1997}, {"title": "A joint model for entity analysis: Coreference", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": "typing, and linking. Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Open information extraction: The second generation", "author": ["Etzioni et al.2011] Oren Etzioni", "Anthony Fader", "Janara Christensen", "Stephen Soderland", "Mausam Mausam"], "venue": "In IJCAI,", "citeRegEx": "Etzioni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Etzioni et al\\.", "year": 2011}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "author": ["Gardner et al.2014] Matt Gardner", "Partha Talukdar", "Jayant Krishnamurthy", "Tom Mitchell"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Nat-", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Traversing knowledge graphs in vector space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Collective entity linking in web text: a graphbased method", "author": ["Han et al.2011] Xianpei Han", "Le Sun", "Jun Zhao"], "venue": "In Proceedings of the 34th interna-", "citeRegEx": "Han et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Han et al\\.", "year": 2011}, {"title": "Deep reinforcement learning with an action space defined by natural language. arXiv preprint arXiv:1511.04636", "author": ["He et al.2015] Ji He", "Jianshu Chen", "Xiaodong He", "Jianfeng Gao", "Lihong Li", "Li Deng", "Mari Ostendorf"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Semantics-based information extraction for detecting economic events", "author": ["Frederik Hogenboom", "Flavius Frasincar", "Kim Schouten", "Otto van der Meer"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Hogenboom et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hogenboom et al\\.", "year": 2013}, {"title": "Selecting actions for resource-bounded information extraction using reinforcement learning", "author": ["Kanani", "McCallum2012] Pallika H Kanani", "Andrew K McCallum"], "venue": "In Proceedings of the fifth ACM international conference on Web search and", "citeRegEx": "Kanani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanani et al\\.", "year": 2012}, {"title": "Joint entity and event coreference resolution across documents", "author": ["Lee et al.2012] Heeyoung Lee", "Marta Recasens", "Angel Chang", "Mihai Surdeanu", "Dan Jurafsky"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language", "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "Multi-field information extraction and cross-document fusion", "author": ["Mann", "Yarowsky2005] Gideon S Mann", "David Yarowsky"], "venue": "In Proceedings of the 43rd annual meeting on association for computational linguistics,", "citeRegEx": "Mann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2005}, {"title": "Human-level control through deep reinforcement learning", "author": ["Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis."], "venue": "Nature, 518(7540):529\u2013533, 02.", "citeRegEx": "Antonoglou et al\\.,? 2015", "shortCiteRegEx": "Antonoglou et al\\.", "year": 2015}, {"title": "Relational learning of pattern-match rules for information extraction", "author": ["R Mooney"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence,", "citeRegEx": "Mooney.,? \\Q1999\\E", "shortCiteRegEx": "Mooney.", "year": 1999}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["Tejas Kulkarni", "Regina Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language", "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "Compositional vector space models for knowledge", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Joint inference in information extraction", "author": ["Poon", "Domingos2007] Hoifung Poon", "Pedro Domingos"], "venue": "In AAAI,", "citeRegEx": "Poon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2007}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Barto1998] Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Hinton2012] Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Knowledge base completion via search-based question answering", "author": ["West et al.2014] Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin"], "venue": "In Proceedings of the 23rd international conference on World", "citeRegEx": "West et al\\.,? \\Q2014\\E", "shortCiteRegEx": "West et al\\.", "year": 2014}, {"title": "Open information extraction using wikipedia", "author": ["Wu", "Weld2010] Fei Wu", "Daniel S Weld"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2014] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "arXiv preprint arXiv:1412.6575", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "is to augment information extraction systems with elaborate semantic inference (Chai and Biermann, 1997; Mooney, 1999; Poon and Domingos, 2007; Hogenboom et al., 2013).", "startOffset": 79, "endOffset": 167}, {"referenceID": 11, "context": "is to augment information extraction systems with elaborate semantic inference (Chai and Biermann, 1997; Mooney, 1999; Poon and Domingos, 2007; Hogenboom et al., 2013).", "startOffset": 79, "endOffset": 167}, {"referenceID": 5, "context": "Open Information Extraction Existing work in open IE has used external sources from the web to improve extraction accuracy and coverage (Agichtein and Gravano, 2000; Etzioni et al., 2011; Fader et al., 2011; Wu and Weld, 2010).", "startOffset": 136, "endOffset": 226}, {"referenceID": 6, "context": "Open Information Extraction Existing work in open IE has used external sources from the web to improve extraction accuracy and coverage (Agichtein and Gravano, 2000; Etzioni et al., 2011; Fader et al., 2011; Wu and Weld, 2010).", "startOffset": 136, "endOffset": 226}, {"referenceID": 13, "context": "Therefore, the novel challenge of our task resides in performing event coreference (Lee et al., 2012; Bejan and Harabagiu, 2014) (i.", "startOffset": 83, "endOffset": 128}, {"referenceID": 9, "context": "Entity linking, multi-document extraction and event coreference Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input documents (Mann and Yarowsky, 2005; Han et al., 2011; Durrett and Klein, 2014).", "startOffset": 230, "endOffset": 298}, {"referenceID": 20, "context": "Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015).", "startOffset": 179, "endOffset": 285}, {"referenceID": 25, "context": "Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015).", "startOffset": 179, "endOffset": 285}, {"referenceID": 7, "context": "Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015).", "startOffset": 179, "endOffset": 285}, {"referenceID": 18, "context": "Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015).", "startOffset": 179, "endOffset": 285}, {"referenceID": 8, "context": "Knowledge Base Completion and Online Search Recent work has explored several techniques to perform Knowledge Base Completion (KBC) such as vector space models and graph traversal (Socher et al., 2013; Yang et al., 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015).", "startOffset": 179, "endOffset": 285}, {"referenceID": 7, "context": ", 2014; Gardner et al., 2014; Neelakantan et al., 2015; Guu et al., 2015). Though our work also aims at increasing extraction recall for a database, traditional KBC approaches do not require searching for additional sources of information. West et al. (2014) explore query reformulation in the context of KBC.", "startOffset": 8, "endOffset": 259}, {"referenceID": 1, "context": "Our approach is also close in spirit to the AskMSR system (Banko et al., 2002) which aims at using information redundancy on the web to better answer questions.", "startOffset": 58, "endOffset": 78}, {"referenceID": 17, "context": "The DQN, in which the Q-function is approximated using a deep neural network, has been shown to learn better value functions than linear approximators (Narasimhan et al., 2015; He et al., 2015) and can capture nonlinear interactions between the different pieces of information in our state.", "startOffset": 151, "endOffset": 193}, {"referenceID": 10, "context": "The DQN, in which the Q-function is approximated using a deep neural network, has been shown to learn better value functions than linear approximators (Narasimhan et al., 2015; He et al., 2015) and can capture nonlinear interactions between the different pieces of information in our state.", "startOffset": 151, "endOffset": 193}], "year": 2016, "abstractText": "In traditional formulations, information extraction systems operate on a fixed collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy. This process entails query reformulation for search, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on a publicly available database of shooting incidents demonstrate that our system outperforms traditional extractors by 7.2% on average.1", "creator": "LaTeX with hyperref package"}}}