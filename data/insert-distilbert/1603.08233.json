{"id": "1603.08233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2016", "title": "Evolution of active categorical image classification via saccadic eye movement", "abstract": "pattern domain recognition and classification is a central concern essential for modern information processing systems. in geographic particular, one key challenge devoted to image recognition and video classification has been that the computational cost of image sequences processing scales linearly with the number of intermediate pixels in the image encoding or video. here we present an appropriate intelligent machine ( the \" active categorical classifier, \" or acc ) that is inspired fundamentally by the saccadic movements of the eye, and is capable of classifying images remotely by selectively scanning only a portion of the image. we harness evolutionary computation to optimize the acc on the mnist hand - written digit classification task, and provide a proof - of - concept that the acc library works on noisy multi - class data. we subsequently further analyze the acc and demonstrate its enormous ability to classify entire images after viewing only a fraction of the pixels, and fully provide insight on future research paths to further improve upon the acc process presented here.", "histories": [["v1", "Sun, 27 Mar 2016 16:36:43 GMT  (585kb,D)", "http://arxiv.org/abs/1603.08233v1", "10 pages, 5 figures, submitted to PPSN 2016 conference"], ["v2", "Thu, 16 Jun 2016 21:00:53 GMT  (292kb,D)", "http://arxiv.org/abs/1603.08233v2", "10 pages, 5 figures, to appear in PPSN 2016 conference proceedings"]], "COMMENTS": "10 pages, 5 figures, submitted to PPSN 2016 conference", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["randal s olson", "jason h moore", "christoph adami"], "accepted": false, "id": "1603.08233"}, "pdf": {"name": "1603.08233.pdf", "metadata": {"source": "CRF", "title": "Evolution of active categorical image classification via saccadic eye movement", "authors": ["Randal S. Olson", "Jason H. Moore", "Christoph Adami"], "emails": ["olsonran@upenn.edu", "adami@msu.edu"], "sections": [{"heading": null, "text": "Keywords: active categorical perception, attention-based processing, evolutionary computation, machine learning, supervised classification"}, {"heading": "1 Introduction", "text": "Pattern recognition and classification is one of the most challenging ongoing problems in computer science in which we seek to classify objects within an image into categories, typically with considerable variation among the objects within each category. With invariant pattern recognition, we seek to develop a model of each category that captures the essence of the class while compressing inessential variations. In this manner, invariant pattern recognition can tolerate (sometimes drastic) variations within a class, while at the same time recognizing differences across classes that can be minute but salient. One means of achieving this goal is through invariant feature extraction [1], where the image is transformed into feature vectors that may be invariant with respect to a set of transformations, such as displacement, rotation, scaling, skewing, and lighting changes. This method can also be used in a hierarchical setting, where subsequent layers extract compound features from features already extracted in lower levels, such that the last layer extracts features that are essentially the classes themselves [2]. Most of these existing methods have one thing in common: they ar X iv :1 60 3.\n08 23\n3v 1\n[ cs\n.C V\n] 2\n7 M\nar 2\n01 6\nachieve invariance either by applying transformations to the image when searching for the best match, or by mapping the image to a representation that is itself invariant to such transformations.\nIn contrast to these \u201cpassive\u201d methods where transformations are applied to the image, we propose an active, attention-based method, where a virtual camera roams over and focuses on particular portions of the image, similar to how our own brain controls the focus of our attention [3]. In this case, the camera\u2019s actions are guided by what the camera finds in the image itself: In essence, the camera searches the image to discover features that it recognizes, creating in the process a time series of experiences that guides further movements and eventually allows the camera to classify the image. We call this camera an \u201cactive categorical classifier,\u201d or ACC for short.\nBroadly speaking, the problem of classifying a spatial pattern is transformed into one of detecting differences between time series, namely the temporal sequence that the virtual camera generates in its sensors as it navigates the image. The method we propose here is inspired by models of visual attention [4], where attention to \u201csalient\u201d elements of an image or scene is guided by the image itself, such that only a small part of the incoming sensory information reaches short-term memory and visual awareness. Thus, focused attention overcomes the information-processing bottleneck imposed by massive sensory input (which can easily be 107 \u2212 108 bits per second in parallel at the optic nerve [4]), and serializes this stream to achieve near-real-time processing with limited computational requirements.\nIn previous work, we have shown that it is possible to evolve robust controllers that navigate arbitrary mazes with near-perfect accuracy [5] and simulate realistic animal behavior [6]. Independently, we have shown that we can evolve simple spatial classifiers for hand-written numerals in the MNIST data set [7]. Here we use the same technology to evolve active categorical classifiers that \u201cforage\u201d on images and respond to queries about what they saw in the image without needing to examine the image again."}, {"heading": "2 Methods", "text": "In this section, we describe the methods used to evolve the active categorical classifiers (ACCs). We begin by describing the simulation environment in which the ACC scans and classifies the images. Next, we outline the structure and underlying neural architecture of an ACC. Finally, we provide details on the evolutionary process that we used to evolve the ACCs and the experiments that we conducted to evaluate them."}, {"heading": "2.1 Simulation Environment", "text": "We evaluate the ACC on the MNIST data set, which is a well-known set of handwritten digits commonly used in supervised image classification research [8]. The MNIST data set contains 28x28 pixel images of hand-written digits\u2014all\nwith corresponding labels indicating what digit the image represents (0\u20139)\u2014 and comes in two predefined sets of training and testing data (60,000 and 10,000 images, respectively). In this project, we binarize the images such that any pixels with a grayscale value > 127 are assigned a value of 1, and all others are assigned a value of 0.\nWhen we evaluate an ACC, we place it at a random starting point in the 28x28 image and provide it a maximum of 40 steps to scan the image and assign a classification. Every simulation step, the ACC decides 1) what direction to move, 2) what class(es) it currently classifies the image as, and 3) whether it has made its final classification and is ready to terminate the simulation early. The ACC is evaluated only on its final classification for each image in the training set, with a \u201cfitness\u201d score (Find) assigned as:\nFind = 1\n1000 \u00d7 1000\u2211 i=1 CorrectClassi NumClassesGuessedi\n(1)\nwhere i is the index of an individual image in the training set, CorrectClassi = 1 if the correct class is among the NumClassesGuessedi guesses that the ACC offers (it is allowed to guess more than one), and CorrectClassi = 0 otherwise. Thus, an ACC can achieve a minimum fitness of 0.1 by guessing all classes for all images, but only achieves a maximum fitness of 1.0 by guessing the correct class only for every image. We note that due to computational limitations, we subset the MNIST training set to the first 100 images of each digit, such that we use only 1,000 training images in total (1/60th of the total set)."}, {"heading": "2.2 Active Categorical Classifier (ACC)", "text": "We show in Fig. 1 the ACC in its natural habitat, roaming a digitized MNIST numeral. Each ACC has a brain that consists of 64 Markov neurons (\u201cstates\u201d) that either fire (state = 1) or are quiescent (state = 0), and represent sensory input from the image, internal memory, and decisions about how to interact with the image. The ACC uses nine of these states to view nine pixels of the image in a 3x3 square, and four of the states to probe for activated pixels outside of its field of view with four raycast sensors that project across the image from the 0\u25e6, 90\u25e6, 180\u25e6, and 270\u25e6 angles of the 3x3 square (green squares in Fig. 1). The raycast sensors allow the ACC to find the numeral even if its starting position is far from it.\nWe also provide the ACC two actuator states (\u201cmotor neurons\u201d) that allow it to \u201csaccade\u201d three pixels up/down and left/right, or any combination thereof (red rectangles denoted as wheels in Fig. 1). In addition, the ACC has 20 states dedicated to classifying the image: 10 states that can be activated to guess each digit class (blue squares), and 10 states to veto an activated guess for each digit class (purple squares), e.g., \u201cthis is definitely not a 4.\u201d This configuration allows the ACC to guess multiple classes at once, and combine its internal logic to veto any of those guesses if it believes them to be incorrect. Finally, the ACC has a \u201cdone\u201d state (orange triangle), which allows it to end the simulation early if it has already decided on its final guess(es) for the current image. The remaining 28 neurons are \u201cmemory\u201d states (black circles) used to process and store information, and integrate that information over time.\nThe \u201cartificial brain\u201d for the ACC in these experiments is a Markov Network (MN, see, e.g., [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1. The combination of output states and sensory inputs from time t + 1 are then used to determine the output states for the ACC at time t+ 2, and so on. Every MN must therefore usefully combine the information provided over time in the 64 states to decide where to move, classify the image, and finally to decide when it has gathered enough information to make an accurate classification. Making all these decisions at once requires complex logic that is difficult to design."}, {"heading": "2.3 Optimization Process", "text": "In order to create the complex logic embodied by a Markov Network, we evolve the MNs to maximize classification accuracy on the training images. We use a standard Genetic Algorithm (GA) to stochastically optimize a population of byte strings [10], which deterministically map to the MNs that function as the ACC\u2019s \u201cartificial brains\u201d in the simulation described above. Due to space limitations, we cannot describe MNs in full detail here; a detailed description of MNs and how they are evolved can be found in [11].\nIn our experiments, the GA maintains a population of 100 byte strings (\u201ccandidates\u201d) of variable length (maximum = 10,000 bytes) and evaluates them according to the fitness function in Equation 1. The GA selects the candidates\nto reproduce into the next generation\u2019s population via tournament selection, where it shuffles the population and competes every byte string against only one other byte string. In each tournament, the byte string with the highest fitness produces one exact copy of itself as well as one mutated copy of itself into the next generation, while the \u201closer\u201d produces no offspring. We note that the GA applies only mutations to the offspring (no crossover, that is, recombination), with a per-byte mutation rate of 0.05%, a gene duplication rate of 5%, and a gene deletion rate of 2%."}, {"heading": "2.4 Experiments", "text": "According to the evolutionary optimization process, the GA selects ACCs that are capable of spatio-temporal classification of MNIST digits. We first ran 30 replicates of the GA with random starting populations and distinct random seeds and allowed these replicates to run for 168 hours on a high-performance compute cluster. From those 30 replicates, we identified the highest-fitness ACC (the \u201celite\u201d), and seeded another set of 30 replicates with mutants of the elite ACC. We allowed this second set of replicates to run for another 168 hours. In the following section, we report on the results of these experiments."}, {"heading": "3 Results", "text": "At the completion of the second set of replicates, the remaining active categorical classifiers (ACCs) had been optimized for 336 hours and roughly 250,000 generations. Shown in Fig. 2, the ACCs experienced the majority of their improvements within the first 150,000 generations, and minimal improvements occurred in the second set of replicates, indicating that the ACCs had reached a plateau\u2013either because the scan pattern required to improve was too complex, or else because improving the classification accuracy on poorly classified digits compromised the ability to classify those digits the ACC was already proficient at. Such trade-offs are likely due to insufficient brain size, and investigations with larger brains are currently underway.\nInstead of continuing the optimization process for a third set of replicates, we identified the highest-fitness ACC from replicate set 2 (highlighted in blue, Fig. 2) and analyzed its spatio-temporal classification behavior to gain insights into its functionality. For the remainder of this section, we focus on the best ACC evolved in replicate set 2, which we will simply call \u201cthe ACC.\u201d Shown in Fig. 3, the ACC achieved respectable but not state-of-the-art performance on the MNIST testing set: It managed to classify most of the 0s and 1s correctly for example, but failed to classify any of the 2s. Overall, the ACC achieved a macro-averaged accuracy of 76%, which provides a proof-of-concept that the ACC works, but still has room for improvement on noisy multi-class data sets. We note that we have optimized ACCs on a set of hand-designed, non-noisy digits, where they managed to achieve 100% accuracy. Thus, it is clear that the ACC architecture requires additional experimentation to fully adapt to noisy\ndata, much like other methods currently in use. In Fig. 4B, we analyze the movement patterns of the ACC by counting how many times each pixel is viewed in the ACC\u2019s 3x3 visual grid when classifying the MNIST data set. Even though the ACC always starts at a random location in the image, we find that it follows a stereotypical scanning patterns of the digits: the ACC lines itself up to the top-left of the digit, then executes an L-shaped scanning pattern.\nIn contrast, Fig. 4A depicts the most informative pixels (according to Gini importance [12]) for differentiating the classes in the MNIST data set with a Random Forest classifier as implemented in scikit-learn [13]. Here, we find that the most informative pixels exist in the center of the images, with several lessinformative pixels on the image edges. Importantly, we note that the ACC never scans some of the most informative pixels in the lower half of the MNIST images (Fig. 4A vs. Fig. 4B). We believe that this behavior is the reason that the ACC is unable to classify any of the 2s, for example, because some of the most critical pixels for differentiating 2s from the rest of the digits are never visited.\nWe provide examples of the ACC scanning patterns in Fig. 5. Shown again is the stereotypical L-shaped scanning pattern starting at the upper-left corner of every digit. (We note that we trimmed the agent paths to only the final scanning pattern because the initial phase of ACC movements are simply lining up to the upper-left corner of the digit.) Interestingly, the ACC scans only a fraction of the available pixels to make each classification, and appears to be integrating information about the digit over space and time to identify distinctive subfeatures of the digits. Furthermore, the ACC completes the majority of its scans within 5\u201310 steps and then immediately activates the \u201cdone\u201d state, indicating that the ACC also learned when it knows the correct digit.\nThe arrows indicate the direction that the ACC followed, whereas the dark grey areas indicate the pixels that it scanned. Although the ACC starts all evaluations at random spots in the grid, it aligns itself to the digit to a common starting point and executes and L-shaped scan of the digit. We note that we excluded an example of digit 2 because the ACC never classifies it correctly, although it follows a similar trajectory as with the other digits."}, {"heading": "4 Discussion", "text": "The results that we display here show that it is possible to optimize an active categorical classifier (ACC) that scans a small portion of an image, integrates that information over space and time, and proceeds to perform an accurate classification of the image. Although the ACC does not achieve competitive accuracy on the MNIST data set compared to many modern techniques (76% testing accuracy, Fig. 3), we believe that this result is due to the lack of training data rather than any particular limitation of ACCs: Due to computational limitations, we were only able to use 1,000 training images (100 of each class) to optimize the ACCs, while modern techniques use much larger training sets that even include additional variations of the training images [14]. Indeed, when we trained a scikitlearn Random Forest with 500 decision trees [13] on the same limited training set of 1,000 images, it achieves only 88.5% accuracy on the MNIST testing set\nas compared to 97.5% when it is trained on the full training set. Thus, in future work we will focus on integrating methods that expose the ACCs to all training images in an efficient manner.\nFrom the point of view of evolved artificial intelligence, that the ACC succeeded at all is uite remarkable. For one, these experiments challenged a single artificial brain to simultaneously perform several complex tasks, including to line itself up to a consistent starting point regardless of where it randomly starts in the image, decide where it needs to move to complete the scan based on limited information about the image, determine what pixels are important to consider, and integrate that information over space and time to classify the image into 1 of 10 classes. We furthermore challenged the ACC to evolve basic \u201ctheory of mind\u201d such that it knows when it has guessed the correct class for the image and to end the simulation early. In future work, it will be illuminating to analyze the underlying neural architecture of the evolved ACCs to provide insight into the fundamentals of active categorical perception [15].\nUnlike many modern image classification techniques that must analyze an entire static image to determine an image\u2019s class, the ACC performs the same classification by integrating information from a small subset of the pixels over space and time. This method naturally lends itself to video classification, where feature compression will play a crucial role in overcoming the massive data size challenge for real-time classification of moving objects [3].\nLastly, recent work has shown that modern deep learning-based image classification techniques tend to be easily fooled because they are trained in a supervised, discriminative manner: they establish decision boundaries that appropriately separate the data they encounter in the training phase, but these decision boundaries also include (and thus mis-classify) many inappropriate data points never encountered during training [16]. Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.\nDue to their nature, deep neural networks are highly dependent on the training data, and only generalize to new challenges if the aforementioned challenges are similar to those accounted for in the training data [17]. In contrast, heuristicbased machines such as the ACC learn simple, generalizable heuristics for classifying images that encode the conceptual representation [9] of the objects in question, and should not be so easily fooled. As such, even if the ACC in the present work does not achieve competitive classification accuracy when compared to modern deep learning image classification techniques, we believe that further development of heuristic-based spatio-temporal image classification machines such as the ACC will lead to robust classifiers that will eventually surpass deep neural networks in generalizability without the need for adversarial training images."}, {"heading": "5 Acknowledgments", "text": "We thank David B. Knoester, Arend Hintze, and Jeff Clune for their valuable input during the development of this project. We also thank the Michigan State University High Performance Computing Center for the use of their computing resources. This work was supported in part by the National Science Foundation BEACON Center under Cooperative Agreement DBI-0939454, and in part by National Institutes of Health grants LM009012, LM010098, and EY022300."}], "references": [{"title": "Feature extraction methods for character recognition - A survey", "author": ["O.D. Trier", "A.K. Jain", "T. Taxt"], "venue": "Pattern Recognition 29", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "Denker", "J. S"], "venue": "Neural Computation 1", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Recurrent Models of Visual Attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems. NIPS \u201909", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational modelling of visual attention", "author": ["L. Itti", "C. Koch"], "venue": "Nat Rev Neurosci 2", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Integrated information increases with fitness in the evolution of animats", "author": ["J. Edlund", "N. Chaumont", "A Hintze"], "venue": "PLoS Comput. Biol. 7", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Predator confusion is sufficient to evolve swarming behaviour", "author": ["R. Olson", "A. Hintze", "F. Dyer", "D. Knoester", "C. Adami"], "venue": "J. Roy. Soc. Interface 10", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Evolution of an artificial visual cortex for image recognition", "author": ["S. Chapman", "D. Knoester", "A. Hintze", "C. Adami"], "venue": "In P. Li\u00f2 et al., ed.: Advances in Artificial Life (ECAL 2013), MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Cognitive systems evolve complex representations for adaptive behavior", "author": ["L. Marstaller", "A. Hintze", "C. Adami"], "venue": "Neural Computation 25", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Introduction to Evolutionary Computing", "author": ["A. Eiben", "J. Smith"], "venue": "Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Evolution of swarming behavior is shaped by how predators attack", "author": ["R.S. Olson", "D.B. Knoester", "C. Adami"], "venue": "arXiv e-print 1310.6012", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Random forests - classification description (March 2016) http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm", "author": ["L. Breiman", "A. Cutler"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A Gramfort"], "venue": "Journal of Machine Learning Research 12", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Regularization of neural networks using DropConnect", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus"], "venue": "Proceedings of the 30th International Conference on Machine Learning. ICML \u201913", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "The dynamics of active categorical perception in an evolved model agent", "author": ["R.D. Beer"], "venue": "Adaptive Behavior 11", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Computer Vision and Pattern Recognition (CVPR \u201915). IEEE Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv eprint 1412.6572v3", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I Sutskever"], "venue": "arXiv eprint 1312.6199v4", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One means of achieving this goal is through invariant feature extraction [1], where the image is transformed into feature vectors that may be invariant with respect to a set of transformations, such as displacement, rotation, scaling, skewing, and lighting changes.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "This method can also be used in a hierarchical setting, where subsequent layers extract compound features from features already extracted in lower levels, such that the last layer extracts features that are essentially the classes themselves [2].", "startOffset": 242, "endOffset": 245}, {"referenceID": 2, "context": "In contrast to these \u201cpassive\u201d methods where transformations are applied to the image, we propose an active, attention-based method, where a virtual camera roams over and focuses on particular portions of the image, similar to how our own brain controls the focus of our attention [3].", "startOffset": 281, "endOffset": 284}, {"referenceID": 3, "context": "The method we propose here is inspired by models of visual attention [4], where attention to \u201csalient\u201d elements of an image or scene is guided by the image itself, such that only a small part of the incoming sensory information reaches short-term memory and visual awareness.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "Thus, focused attention overcomes the information-processing bottleneck imposed by massive sensory input (which can easily be 10 \u2212 10 bits per second in parallel at the optic nerve [4]), and serializes this stream to achieve near-real-time processing with limited computational requirements.", "startOffset": 181, "endOffset": 184}, {"referenceID": 4, "context": "In previous work, we have shown that it is possible to evolve robust controllers that navigate arbitrary mazes with near-perfect accuracy [5] and simulate realistic animal behavior [6].", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "In previous work, we have shown that it is possible to evolve robust controllers that navigate arbitrary mazes with near-perfect accuracy [5] and simulate realistic animal behavior [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "Independently, we have shown that we can evolve simple spatial classifiers for hand-written numerals in the MNIST data set [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "We evaluate the ACC on the MNIST data set, which is a well-known set of handwritten digits commonly used in supervised image classification research [8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": ", [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1.", "startOffset": 2, "endOffset": 11}, {"referenceID": 6, "context": ", [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1.", "startOffset": 2, "endOffset": 11}, {"referenceID": 8, "context": ", [5, 7, 9]) that deterministically maps the 64 states (described above) at time t to a corresponding series of output states that we interpret to determine the ACC\u2019s movement actions and classifications at time t + 1.", "startOffset": 2, "endOffset": 11}, {"referenceID": 9, "context": "We use a standard Genetic Algorithm (GA) to stochastically optimize a population of byte strings [10], which deterministically map to the MNs that function as the ACC\u2019s \u201cartificial brains\u201d in the simulation described above.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Due to space limitations, we cannot describe MNs in full detail here; a detailed description of MNs and how they are evolved can be found in [11].", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": ", Gini importance [12]), whereas Panel B shows the pixels that the best active categorical classifier visited most frequently when classifying the MNIST data set.", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "4A depicts the most informative pixels (according to Gini importance [12]) for differentiating the classes in the MNIST data set with a Random Forest classifier as implemented in scikit-learn [13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "4A depicts the most informative pixels (according to Gini importance [12]) for differentiating the classes in the MNIST data set with a Random Forest classifier as implemented in scikit-learn [13].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "3), we believe that this result is due to the lack of training data rather than any particular limitation of ACCs: Due to computational limitations, we were only able to use 1,000 training images (100 of each class) to optimize the ACCs, while modern techniques use much larger training sets that even include additional variations of the training images [14].", "startOffset": 355, "endOffset": 359}, {"referenceID": 12, "context": "Indeed, when we trained a scikitlearn Random Forest with 500 decision trees [13] on the same limited training set of 1,000 images, it achieves only 88.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "In future work, it will be illuminating to analyze the underlying neural architecture of the evolved ACCs to provide insight into the fundamentals of active categorical perception [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 2, "context": "This method naturally lends itself to video classification, where feature compression will play a crucial role in overcoming the massive data size challenge for real-time classification of moving objects [3].", "startOffset": 204, "endOffset": 207}, {"referenceID": 15, "context": "Lastly, recent work has shown that modern deep learning-based image classification techniques tend to be easily fooled because they are trained in a supervised, discriminative manner: they establish decision boundaries that appropriately separate the data they encounter in the training phase, but these decision boundaries also include (and thus mis-classify) many inappropriate data points never encountered during training [16].", "startOffset": 426, "endOffset": 430}, {"referenceID": 16, "context": "Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.", "startOffset": 164, "endOffset": 168}, {"referenceID": 15, "context": "Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.", "startOffset": 202, "endOffset": 206}, {"referenceID": 17, "context": "Although most deep learning researchers respond to this challenge by creating additional \u201cadversarial\u201d training images with which to train the deep neural networks [17], we believe that the findings in [16] highlight a critical weakness in deep learning: the resulting neural networks are trained to precisely map inputs to corresponding target outputs, but they do not generalize far beyond the training data they are exposed to [18], in contrast to humans.", "startOffset": 430, "endOffset": 434}, {"referenceID": 16, "context": "Due to their nature, deep neural networks are highly dependent on the training data, and only generalize to new challenges if the aforementioned challenges are similar to those accounted for in the training data [17].", "startOffset": 212, "endOffset": 216}, {"referenceID": 8, "context": "In contrast, heuristicbased machines such as the ACC learn simple, generalizable heuristics for classifying images that encode the conceptual representation [9] of the objects in question, and should not be so easily fooled.", "startOffset": 157, "endOffset": 160}], "year": 2017, "abstractText": "Pattern recognition and classification is a central concern for modern information processing systems. In particular, one key challenge to image and video classification has been that the computational cost of image processing scales linearly with the number of pixels in the image or video. Here we present an intelligent machine (the \u201cactive categorical classifier,\u201d or ACC) that is inspired by the saccadic movements of the eye, and is capable of classifying images by selectively scanning only a portion of the image. We harness evolutionary computation to optimize the ACC on the MNIST hand-written digit classification task, and provide a proof-of-concept that the ACC works on noisy multi-class data. We further analyze the ACC and demonstrate its ability to classify images after viewing only a fraction of the pixels, and provide insight on future research paths to further improve upon the ACC presented here.", "creator": "LaTeX with hyperref package"}}}