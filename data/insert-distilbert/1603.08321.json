{"id": "1603.08321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2016", "title": "Audio Visual Emotion Recognition with Temporal Alignment and Perception Attention", "abstract": "this accompanying paper focuses on two key problems for audio - perception visual emotion recognition behaviour in the video. one is the audio and visual streams temporal alignment for feature level fusion. the other one is locating and re - weighting the peripheral perception attentions in the whole audio - visual stream for better recognition. the long short term memory - recurrent neural network ( lstm - rnn ) is employed as the main classification architecture. firstly, soft attention mechanism aligns the audio and visual streams. secondly, specifically seven linear emotion embedding vectors, which are corresponding to each perceived classification emotion type, are mainly added to locate the perception attentions. the locating and re - weighting process is also based on assessing the soft attention mechanism. the experiment results on emotiw2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques.", "histories": [["v1", "Mon, 28 Mar 2016 06:06:10 GMT  (469kb)", "http://arxiv.org/abs/1603.08321v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["linlin chao", "jianhua tao", "minghao yang", "ya li", "zhengqi wen"], "accepted": false, "id": "1603.08321"}, "pdf": {"name": "1603.08321.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "audio-visual emotion recognition in the video. One is the audio and visual streams temporal alignment for feature level fusion. The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition. The Long Short Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main classification architecture. Firstly, soft attention mechanism aligns the audio and visual streams. Secondly, seven emotion embedding vectors, which are corresponding to each classification emotion type, are added to locate the perception attentions. The locating and re-weighting process is also based on the soft attention mechanism. The experiment results on EmotiW2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques."}, {"heading": "1 Introduction", "text": "Emotion recognition plays an important role in human machine interaction. Early researches mainly focus on utterance level speech emotion recognition or static image level facial expression recognition. However, emotion is a temporally dynamic event which can be better inferred from both audio and video feature sequences. This point of view is proved by cognitive researchers, where they argue that the dynamics of human behaviors are crucial for their interpretation [Sander et al., 2005]. Moreover, a number of recent studies [Chao et al., 2014; Liu et al., 2014] in affective computing demonstrate this point of view.\nMeanwhile, human emotions are expressed in a multimodal way. Psychological study such as [Russell and Fern\u00e1ndezdols, 2003], has highlighted the importance of using multiple modalities to strengthen the accuracy of the emotion analysis. In [Busso et al. 2004], the authors analyzes the strengths and weaknesses of vision-only and audio-only based expression analysis systems. They also outline\napproaches for fusing the two modalities, and it is shown that when these two modalities are fused, the performance and the robustness of the emotion recognition system improve measurably.\nAlthough combining audio and visual modalities improve the recognition accuracy, the audio visual fusion is still a problem. Three fusion strategies are widely utilized. Currently, most of the works combines the two modalities in decision level [Liu et al., 2014; Kahou et al., 2015]. In the decision-level fusion, the inputs coming from different modalities are modeled independently, and these single-modal recognition results are combined in the end. Since humans display audio and visual expressions in a complementary redundant manner, the assumption of conditional independence between audio and visual data streams in decision-level fusion is incorrect and results in the loss of information of mutual correlation between the two modalities [Zeng et al., 2005].\nFeature level fusion is another way utilized in audio visual emotion recognition. [Sikka et al., 2013] combines visual descriptors with audio features using Multiple Kernel Learning and the audio-video clips are classified by SVM classifier. A more common way is extracting audio and visual features separately, pooling these features to single vectors for each feature sets and then concatenating these vectors into one single feature vector for classification [Busso et al., 2004; Chao et al., 2016]. These feature level fusion methods do not consider the temporal coupling of the audio and visual streams.\nTo address this problem, model-level fusion is proposed to\nmake use of the correlation between audio and visual streams. In particular, multiple stream Hidden Markov Models (HMM) [Song et al., 2004; Zeng et al., 2005], Artificial Neural Network-based fusion [Fragopanagos and Taylor, 2005] are proposed. However, these models all require different modalities have strong synchronization. While the audio and visual signals always have different frame rates, temporal alignment before audio and visual features fed into these models is necessary, which is often manually operated.\nIn this paper, we utilize LSTM-RNN [Hochreiter and Schmidhuber, 1997] to model the audio and visual streams. Particularly, soft attention mechanism [Mnih et al., 2014; Xu et al., 2015; Bahdanau et al., 2014] is employed for audio and visual streams alignment. This mechanism enables the neural network to learn to align audio and visual streams and predict emotion type jointly. Without manually temporal alignment, we believe this model can have less information loss and better recognition results.\nWhen the RNN models are utilized for sequence classification, every time step outputs a hidden representation, which encodes the input information from start to the current time step. The final classification result is often calculated by the hidden reprensetation of the last time step (last-time encoding) [Kahou et al. 2015]. Previous study [Chao et al., 2016] shows that average all the hidden representations from different time steps (average encoding) can have better results. However, are the last-time encoding and average encoding the optimal choices? We believe that during the perception process for a special audio-video clip, people\u2019s attention will focus on several key sub-clips, which is more emotionally salient. These sub-clips can provide better clues and more attention should be paid for better emotion perception. Studies in the video emotion recognition filed can also prove this point of view. For example, [Kayaoglu and Eroglu Erdem, 2015] select the key frames in the video clip to make the final classification, which also shows competitive performance. The key frames can be seen as the emotional salient part.\nInspired by the above findings, we utilize the soft attention mechanism to re-weight and combine the hidden representations of all time steps in RNN for final\nclassification. In order to locate the emotional salient parts, emotion embedding vectors, which are corresponding to each emotion classification types, are added to the proposed model. Each emotion embedding vector works as an anchor to choose and increase the weights of the salient parts of specific emotion type. After locating, re-weighting and combining, each emotion type will have a unique hidden representation for final classification. These emotion embedding vectors are jointly learned by the neural network with other parameters."}, {"heading": "2 Method", "text": ""}, {"heading": "2.1 LSTM model and audio visual alignment", "text": "We use the implementation discussed in [Xu et al., 2015]:\n( ) (\n) (\n* (1)\n(2) (3)\nwhere is the input gate, is the forget gate, is the output gate and is calculated by Eq.1. is the cell state, is the hidden state and represents the input to the LSTM at time step t. M: is the affine transformation consisting of trainable parameters with and , where d is the dimensionality of all of , , , , and and D is the dimensionality of the . In the proposed architecture, given the audio input feature\nsequence { }, a LSTM layer (Audio LSTM)\nfirst learns the dynamics of the audio sequence and encodes it\ninto hidden representation { } . The visual feature sequence is represented by { }. and T represent the length of the audio feature sequence and visual feature sequence separately. The visual representation dynamics and audio visual coupling are encoded together by another LSTM layer (Audio-Visual LSTM). In this layer, soft attention mechanism is utilized to align the audio and visual streams. During alignment, window technique is applied. At each time step t, the soft attention mechanism considers a sub-sequence { } of the whole sequence , where w is a\npredefined window width and is the median of the alignment. Given and T, we can calculate the coarse alignment of the two streams, which is for each time step. Adding a window can utilize this prior knowledge and also result in a lower complexity. The more accurate alignment\ncan be calculated as the follow ways:\n( )\n\u2211 ( )\n(\n)\nwhere are the weights mapping to the element of the softmax, is the score of aligned to and these scores are normalized by softmax (Eq. 4). The softmax can be thought of as the probability with which our model believes the corresponding frames in the are temporal aligned to . After calculating these probabilities, the soft attention mechanism computes the expected value of the at every time step by taking the expectation over :\n\u2211\nThen and the corresponding are fed into the audio-visual LSTM, which learns the correlation and dynamic of the audio and visual streams (see Fig.1a.).\nAt every time step t, hidden representation of the audio-visual LSTM is calculated, which encodes the input features from start to time step t. Normally, there are two ways to get the final classification results. The first one (last-time encoding) is based on , which is the last hidden representation. The classification results via last time encoding can be represented as:\n| |\nThe other one is the average encoding, which is calculated based on the average of the whole . Classification results via average-pool can also be represented as:\n| \u2211 |\nFor the last-time encoding, the , a fixed-length vector, may not fully contain all the necessary information from the whole audio-visual stream. This problem also exists in RNN based machine translation [Bahdanau et al., 2014]. Previous study [Chao et al., 2016] has proved the average encoding is better than the last time encoding. However, average encoding encodes as a classifier fusion way with equal weights given to the sub-parts of the whole sequence. As perception attention exists when perceiving the audio-visual clips, given equal weights to each sub-parts is not an optimal way. Thus, better encoding way should be explored. Find the perception attentions and increase the weights of perception attentions is a solution."}, {"heading": "2.3 Perception attention", "text": "We hypothesis different emotion types have different perception attentions. To locate these perception attentions, we add emotion embedding vectors { } to the model. N equals to the number of emotion types which needs to be classified. Each emotion embedding vector works as an anchor to select the emotional salient parts from the whole audio-visual stream. Based on the soft attention mechanism, the attention scores { } for emotion type n are calculated as follows:\n( )\n\u2211 ( )\nwhere is the mapping matrix for from the orignal dimension to the dimension of represents emotion type n\u2019s attention distribution of . Then we can get the hidden representation \u2211 , which is specially\ncalculated for emotion type n. The classification score , which is the audio-visual clip is classified to emotion type n, is calculated as:\n\u2211\nwhere and are assigned to emotion type n, and : with a equals to the dimension of and b equals to one. The classification scores of all the emotion types are then normalized by softmax function,\n|\n\u2211\nwhere y represents the predicted emotion type.\nBy adding embedding vectors, the model can learn the perception attentions for each emotion type. Then output the\nclassification score based on the weighted combination of perception attentions. Compared to average encoding or last-time encoding, the hidden representations of LSTM-RNN are utilized more efficiently and the information loss is decreased.\nIn the implementation of the proposed model, we encode the audio and visual streams two times with similar architecture. The first time encodes to hidden representation , which are utilized to locate the attentions and obtain in Eq.9, with a smaller network. The second time encodes to . Then\nand combines for further processing (see Fig.1b). When we calculate and separately, better performance can be obtained compared to compute based on the same . We believe the separate encoding way can decrease the correlation among parameters. Thus it is easier to optimize."}, {"heading": "3 Dataset and Feature Set", "text": "The EmotiW2015 [Dhall et al., 2015] provides the common benchmarks for emotion recognition researchers, which mimics real-world conditions. There are two sub-challenges: audio-visual based emotion recognition challenge (AFEW) and image based static facial expression recognition challenge (SFEW). AFEW sub-challenge is to assign a single emotion label to the video clip from the six universal emotions (Anger, Disgust, Fear, Happiness, Sadness and Surprise) and Neutral. The databases (AFEW and SFEW) are divided into three sets for the challenge: training, validation and testing. The training and validation sets are utilized to train the emotion recognizer. Prediction results on testing set are utilized to rank participants. The sample rate for audio data in AFEW is 44kHz. The video data in AFEW has 25 frames per second."}, {"heading": "3.1 Face shape feature", "text": "For video features, we mainly focus on the face part. As the face shape provides import clues for facial expression, we use the landmarks\u2019 location of the face as face shape feature. After feature normalization for each clip, these features can also reflect the head movement and head pose. The 49 landmarks\u2019 locations are then PCA whitened [Bengio, 2012], with the final 20 dimensions are kept."}, {"heading": "3.2 Face appearance feature", "text": "For face appearance feature, we utilize the features extracted from a convolutional neural network (CNN) [LeCun et al., 1998] model. Previous work [Liu et al. 2014] utilizes the CNN model trained via face recognition dataset to extract face representation. And this representation can be generalized to facial expression recognition problem. We employ the same strategy to train a CNN model from Celebrity Faces in the Wild (CFW) [Zhang et al., 2012] and Facescurb [Ng and Winkle, 2014] dataset, which are designed for face recognition tasks. Over 110,000 face images from 1032 people are used for training and the labels\nare their identities. The architecture is the same with [Krizhevsky et al., 2012]. There are three fully connected layers and five convolutional layers. Compared to the three fully connected layers, convolutional layers have better generation performance [Girshick et al., 2013]. The deeper layers extract more abstract features [Zeiler and Fergus, 2013]. Thus, we extract the feature from the 5th pooling layer (pool5) as appearance feature. While the dimension number of the features from pool5 is 9216. Meanwhile, the training data is relatively small. Thus we employ random forest algorithm implemented by scikit-learn 1 for feature selection and 1024 features are kept for the appearance feature set. The random forest classifier is trained via the SFEW database, where one of the seven emotion labels is assigned to a single static face image."}, {"heading": "3.3 Audio feature", "text": "We utilize the YAAFE toolbox 2 to extract audio features. All the 27 features of the toolbox are extracted. The audio data is resampled to 16KHz and default parameters of each feature is utilized. Finally, 939 dimensions features are extracted for each frame and the frame length is 1024. The audio features are then PCA whitened, with the final 50 dimensions are kept."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experiments setup", "text": "We follow the challenge criterion of EmotiW2015 to utilize training set, validation set and testing set. We utilize the landmarks provided by the organizers for the shape feature. Caffe [Jia et al., 2015] implementation of CNN is utilized to extract face appearance features, where the cropped face image is provided by the organizers.\nFor the verification of perception alignment, we compare the average encoding or last-time encoding with the proposed model. Thus, there are mainly two architectures for comparison. The first one is the average encoding or last-time encoding for the audio visual inputs. In this architecture, there are 64 memory cells utilized for both Audio LSTM and Audio-Visual LSTM. The dimensions of all the hidden layers before LSTM layers are equal to the dimension of LSTM layers. The audio feature sequence is fed into a hidden layer first and then fed into the Audio LSTM. For visual feature sequences, both face shape feature and face appearance feature are fed into one hidden layer with 64 nodes separately. The hidden representations of the two feature sets are then concatenated together and fed into another hidden layer. The fused hidden representation of face shape and appearance features is utilized to align the audio stream and represent the visual representation for Audio-Visual LSTM. The second one is the proposed model with both temporal alignment module and perception\n1 http://scikit-learn.org/stable/ 2 http://yaafe.sourceforge.net/\nattention module. The main difference compared with the first architecture exists in the perception attention module (Section 2.2) is added. The same architecture with the first\none is utilized to get and , with the dimensions are 32 and 8. The dimension of the seven emotion embedding vectors is also 8. Thus, the second model have smaller parameters (In the experiments, we find the model works best when the memory cell number of LSTMs for equals to 32).\nFor the verification of audio visual alignment, the performance of the average encoding with single face appearance feature and the audio visual inputs with temporal alignment module only are compared. The model for appearance feature has one hidden layer and one LSTM layer with 64 memory cells.\nAll the models are trained via Adadelta [Zeiler, 2012]. The maximum training epoch is 50 with dropout regularization technique utilized in all layers except the LSTM layer. The drop rate is 0.5. Weight decay in all the layers with the parameter 0.0005 is applied to prevent over fitting. Early stopping technique is also employed. The best results for testing set are chosen by the best prediction accuracy in the validation set."}, {"heading": "4.2 Qualitative analysis", "text": "In Fig.2, we show how the alignments between audio and visual streams change with the increase of time step. Looking at the overall, the examples show a clear shift of the attention focuses in the given window as time step increases (shift at 45 degree visually and shift from one location in an window to next location in next window). Between the obvious attention shift time steps, the middle time steps show no clear change mode. This is because as time step increases, the attentions will shift from start to end of the window and shift\nreverse from end to the start happens in these middle time steps. The changing mode happens in almost all samples no matter how many time steps of each sample (Fig. 4a-4d). The first half of Fig. 4b also shows different attention distribution, which is similar to even distribution. The reason for this is mainly from the dataset. As the EmotiW dataset is collected from the movies, which are in a wild environment and the background can be very noise [Dhall et al., 2015]. When the audio modality has no sound of human, the alignment tends to become even distribution. The perception attention visualization shows in Fig.3. As the perception attentions have various distributions for different samples, we randomly pick several samples to represent all the samples. This figure shows that perception attentions can locate different parts in the whole audio-visual streams, from the start (Fig.3e), close to middle (Fig.3f) to the end (Fig.3g). This figure also shows that the perception can locate in multiple locations (mainly in the first half (Fig.3a and Fig.3c), in the second half (Fig. 3d) and random distribution in the whole stream (Fig.3b)). In the picking process, we also find that a relative large proportion of samples have the same distribution with Fig.3e, where the perception attention mainly locates in the first frames. Two reasons may explain this result. The first one is that the neural network fails to learn the right distribution totally. The second one may come from the dataset collection process. During the annotators label the audio-visual recording, the clip begins when they find the emotional salient part. Thus,\nFigure 2: Examples of temporal alignment for audio and visual streams. The gray values of the bars indicate the scores of the alignment from the audio frames in an given window to a particular visual frame. Each row is normalized to have maximum value of 1. The rows of each sample are corresponding to the four locations in the given window and the columns are corresponding to the time steps. The bottom row of each sample is the first location in the given window.\nFigure 3: Examples of perception attentions for each emotion type to the audio visual stream. The gray values of the bars indicate the attention scores. Each row is normalized to have maximum value of 1. The rows of each sample are corresponding to the seven emotion types, which are angry, disgust, fear, happy, neutral, sad and surprise. The columns are corresponding to the time steps of each sample.\nthe start of the audio-visual clip can be the most emotional salient frame. In this context, seven emotion embedding vectors are added. We can see the seven emotion type almost focus on the same emotional salient sub-parts with relative little differences in the attention distribution. This suggests that the emotional salient parts attract our attention to judge the emotion types. Fig.4 shows the projection of the emotion embedding vectors from random initialization before model training (Fig.4a) to the final values when the model training finished (Fig.4b). The relative positions of these vectors change totally. However, there are not clear patterns among the relative positions of each vector when jointly observing with the confusion matrix (Fig.5).\nMore details of the best submitted result are shown in Fig.5. The confusion matrix shows that angry, happy, neutral and sad are easier to classify. The surprise and fear are easy to be misclassified to angry. Disgust is easy to confuse with happy. The reason may lie in the data set distribution is not balance. Fine grained classification among angry, fear and surprise needs more effort."}, {"heading": "4.3 Quantitative analysis", "text": "Table 1 reports accuracy of the comparison experiments. The results show that the performance is slightly better when combine the audio visual modalities in feature level. The proposed model also works better than the average encoding and last-time encoding model. Table 2 also shows the performance comparisons with several state-of-the-art models. The first three results are the top three performers on EmotiW 2015. There are significant gaps compared with these leading models. The top 2 performers all focus in designing better features. The work of [Kahou et al., 2015] is more close to our work. However, their model utilizes decision level fusion to improve the performance significantly. In fact, their RNN model with appearance feature behaves similar to ours. We can conclude that decision level model works better than feature level fusion on this dataset and better features are needed for our model."}, {"heading": "5 Conclusion", "text": "In this paper we utilize the soft attention mechanism to temporally align the audio and visual streams and fuse these streams in the feature level. We also add the emotion embedding vectors and the soft attention mechanism in the output layer of RNN to locate and re-weight the perception attentions in the audio visual stream. Compared to the widely utilized average encoding or last-time encoding, our model decrease the information loss in the output layer of RNN and utilize the output of RNN more efficiently. Besides, both the qualitative analysis and quantitative analysis show the effectiveness of the proposed techniques. We also think the proposed model, especially for the perception attention technique, can be utilized to other sequence classification tasks. In the future, we plan to explore better features for emotion classification task since there is still large space to improve compared with the state-of-the-art models.\nFigure 4: The projection of the emotion embedding vectors. All the vectors are projected to a 2-D space via PCA dimension reduction and the top two eigenvalues are kept for each vector. (4a): the projection of the initialized embedding vectors before the model training. (4b): the projection of the learned embedding vectors."}, {"heading": "Acknowledgments", "text": "This work is supported by the National High-Tech Research and Development Program of China (863 Program) (No.2015AA016305), the National Natural Science Foundation of China (NSFC) (No.61425017, No.61332017, No.61375027, No.61305003, No.61203258), the Major Program for the National Social Science Fund of China (13&ZD189) and the Strategic Priority Research Program of the CAS (Grant XDB02080006)."}], "references": [{"title": "A systems approach to appraisal mechanisms in emotion", "author": ["al. Sander et", "D 2005] Sander", "D Grandjean", "R. Scherer K"], "venue": "Neural Networks the Official Journal of the International Neural Network Society,", "citeRegEx": "et et al\\.,? \\Q2005\\E", "shortCiteRegEx": "et et al\\.", "year": 2005}, {"title": "Multi-scale Temporal Modeling for Dimensional Emotion Recognition in Video", "author": ["Chao et al", "2014] L. Chao", "J. Tao", "M. Yang", "Y. Li", "Z. Wen"], "venue": "AVEC@MM", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild", "author": ["M. Liu", "R. Wang", "S. Li", "S. Shan", "Z. Huang", "X. Chen"], "venue": "Proceedings of the 16th International Conference on Multimodal Interaction.", "citeRegEx": "Liu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Annual Review of Psychology", "author": ["Russell J A", "Bachorowski J A", "Fern\u00e1ndezdols J. Facial", "vocal expressions of emotion"], "venue": "54(1):329-349,", "citeRegEx": "Russell and Fern\u00e1ndezdols. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Yildirim S", "author": ["C Busso", "Z Deng"], "venue": "et al.", "citeRegEx": "Busso et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "et al", "author": ["S Ebrahimi Kahou", "V Michalski", "K Konda"], "venue": "Recurrent Neural Networks for Emotion Recognition in Video. ICMI. ACM,", "citeRegEx": "Kahou et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["K Sikka", "K Dykstra", "S Sathyanarayana"], "venue": "Multiple kernel learning for emotion recognition in the wild. ICMI ACM, 2013:517-524,", "citeRegEx": "Sikka et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short term memory recurrent neural network based encoding method for emotion recognition", "author": ["al. Chao et", "L 2016] Chao", "J Tao", "M Yang"], "venue": "in video. ICASSP,", "citeRegEx": "et et al\\.,? \\Q2016\\E", "shortCiteRegEx": "et et al\\.", "year": 2016}, {"title": "Audio-visual based emotion recognition - a new approach", "author": ["al. Song et", "M 2004] Song", "J Bu", "C Chen"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition", "citeRegEx": "et et al\\.,? \\Q2004\\E", "shortCiteRegEx": "et et al\\.", "year": 2004}, {"title": "Audio-visual affect recognition through multi-stream fused HMM for HCI", "author": ["al. Zeng et", "Z 2005] Zeng", "J Tu", "M Pianfetti B"], "venue": "IEEE Conference on Computer Vision & Pattern Recognition", "citeRegEx": "et et al\\.,? \\Q2005\\E", "shortCiteRegEx": "et et al\\.", "year": 2005}, {"title": "Emotion recognition in human-computer interaction", "author": ["Fragopanagos", "Taylor", "2005] Fragopanagos N", "Taylor J G"], "venue": "Neural Networks,", "citeRegEx": "Fragopanagos et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fragopanagos et al\\.", "year": 2005}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Schmidhuber", "S. 1997] Hochreiter"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent Models of Visual Attention[J", "author": ["al. Mnih et", "V 2014] Mnih", "N Heess", "A Graves"], "venue": "Eprint Arxiv,", "citeRegEx": "et et al\\.,? \\Q2014\\E", "shortCiteRegEx": "et et al\\.", "year": 2014}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention[J", "author": ["al. Xu et", "K 2015] Xu", "J Ba", "R Kiros"], "venue": "Mccarthy,", "citeRegEx": "et et al\\.,? \\Q2057\\E", "shortCiteRegEx": "et et al\\.", "year": 2057}, {"title": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction", "author": ["Kayaoglu M", "Eroglu Erdem C. Affect Recognition using Key Frame Selection based on Minimum Sparse Reconstruction"], "venue": "ACM,", "citeRegEx": "Kayaoglu and Eroglu Erdem. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Video and Image based Emotion Recognition Challenges in the Wild: EmotiW 2015", "author": ["Abhinav Dhall", "O.V. Ramana Murthy", "Roland Goecke", "Jyoti Joshi", "Tom Gedeon"], "venue": "ACM ICMI", "citeRegEx": "Dhall et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun et al", "Y. 1998] LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "al. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "al. et al\\.", "year": 1998}, {"title": "Finding celebrities in billions of web images", "author": ["X. Zhang", "L. Zhang", "X.-J. Wang", "H.-Y. Shum"], "venue": "Multimedia, IEEE Transactions on, 14 (4):995\u20131007", "citeRegEx": "Zhang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "A data-driven approach to cleaning large face datasets", "author": ["H.-W. Ng", "S. Winkler"], "venue": "Proc. IEEE International Conference on Image Processing (ICIP), Paris, France, Oct. 27-30", "citeRegEx": "Ng and Winkle. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton. G."], "venue": "NIPS", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "arXiv preprint arXiv:1311.2524", "citeRegEx": "Girshick et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate[J", "author": ["D Bahdanau", "K Cho", "Y Bengio"], "venue": "Eprint Arxiv,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Jia", "Yet al.", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv: 1408. 5093,", "citeRegEx": "Jia et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "2012] Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2012}, {"title": "et al", "author": ["A Yao", "J Shao", "N Ma"], "venue": "Capturing AU-Aware Facial Features and Their Latent Relations for Emotion Recognition in the Wild. ICMI ACM,", "citeRegEx": "Yao et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["H Kaya", "F Rpinar", "S Afshar"], "venue": "Contrasting and Combining Least Squares Based Learners for Emotion Recognition in the Wild. ICMI ACM,", "citeRegEx": "Kaya et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Moreover, a number of recent studies [Chao et al., 2014; Liu et al., 2014] in affective computing demonstrate this point of view.", "startOffset": 37, "endOffset": 74}, {"referenceID": 3, "context": "Psychological study such as [Russell and Fern\u00e1ndezdols, 2003], has highlighted the importance of using multiple modalities to strengthen the accuracy of the emotion analysis.", "startOffset": 28, "endOffset": 61}, {"referenceID": 4, "context": "In [Busso et al. 2004], the authors analyzes the strengths and weaknesses of vision-only and audio-only based expression analysis systems.", "startOffset": 3, "endOffset": 22}, {"referenceID": 2, "context": "Currently, most of the works combines the two modalities in decision level [Liu et al., 2014; Kahou et al., 2015].", "startOffset": 75, "endOffset": 113}, {"referenceID": 6, "context": "[Sikka et al., 2013] combines visual descriptors with audio features using Multiple Kernel Learning and the audio-video clips are classified by SVM classifier.", "startOffset": 0, "endOffset": 20}, {"referenceID": 21, "context": "Particularly, soft attention mechanism [Mnih et al., 2014; Xu et al., 2015; Bahdanau et al., 2014] is employed for audio and visual streams alignment.", "startOffset": 39, "endOffset": 98}, {"referenceID": 5, "context": "The final classification result is often calculated by the hidden reprensetation of the last time step (last-time encoding) [Kahou et al. 2015].", "startOffset": 124, "endOffset": 143}, {"referenceID": 14, "context": "For example, [Kayaoglu and Eroglu Erdem, 2015] select the key frames in the video clip to make the final classification, which also shows competitive performance.", "startOffset": 13, "endOffset": 46}, {"referenceID": 21, "context": "This problem also exists in RNN based machine translation [Bahdanau et al., 2014].", "startOffset": 58, "endOffset": 81}, {"referenceID": 15, "context": "The EmotiW2015 [Dhall et al., 2015] provides the common benchmarks for emotion recognition researchers, which mimics real-world conditions.", "startOffset": 15, "endOffset": 35}, {"referenceID": 17, "context": "We employ the same strategy to train a CNN model from Celebrity Faces in the Wild (CFW) [Zhang et al., 2012] and Facescurb [Ng and Winkle, 2014] dataset, which are designed for face recognition tasks.", "startOffset": 88, "endOffset": 108}, {"referenceID": 18, "context": ", 2012] and Facescurb [Ng and Winkle, 2014] dataset, which are designed for face recognition tasks.", "startOffset": 22, "endOffset": 43}, {"referenceID": 19, "context": "The architecture is the same with [Krizhevsky et al., 2012].", "startOffset": 34, "endOffset": 59}, {"referenceID": 20, "context": "Compared to the three fully connected layers, convolutional layers have better generation performance [Girshick et al., 2013].", "startOffset": 102, "endOffset": 125}, {"referenceID": 22, "context": "Caffe [Jia et al., 2015] implementation of CNN is utilized to extract face appearance features, where the cropped face image is provided by the organizers.", "startOffset": 6, "endOffset": 24}, {"referenceID": 15, "context": "As the EmotiW dataset is collected from the movies, which are in a wild environment and the background can be very noise [Dhall et al., 2015].", "startOffset": 121, "endOffset": 141}], "year": 2016, "abstractText": "This paper focuses on two key problems for audio-visual emotion recognition in the video. One is the audio and visual streams temporal alignment for feature level fusion. The other one is locating and re-weighting the perception attentions in the whole audio-visual stream for better recognition. The Long Short Term Memory Recurrent Neural Network (LSTM-RNN) is employed as the main classification architecture. Firstly, soft attention mechanism aligns the audio and visual streams. Secondly, seven emotion embedding vectors, which are corresponding to each classification emotion type, are added to locate the perception attentions. The locating and re-weighting process is also based on the soft attention mechanism. The experiment results on EmotiW2015 dataset and the qualitative analysis show the efficiency of the proposed two techniques.", "creator": "Microsoft\u00ae Word 2010"}}}