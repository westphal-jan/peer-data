{"id": "1603.08831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Towards Understanding Sparse Filtering: A Theoretical Perspective", "abstract": "in this paper we present solely our study on a recent and effective algorithm for unsupervised learning, that area is, sparse filtering. the aim of this research is not to show whether or how adaptive well sparse texture filtering works, extensively but strive to understand why and when sparse filtering does work. we first provide a thorough study essentially of introducing this algorithm through a conceptual evaluation of feature distribution learning, a theoretical analysis of the properties of sparse filtering, and an experimental software validation of our conclusions. we fully argue that sparse filtering works by explicitly maximizing the informativeness of exploring the learned representation relation through the error maximization of the proxy of sparsity, and sometimes by implicitly preserving information conveyed by the distribution of the original data through the constraint of structure preservation. in particular, afterwards we prove that sparse filtering preserves the cosine neighborhoodness of the data. we validate our statements on artificial and real data sets by applying our theoretical understanding to the explanation of the success of sparse filtering on real - world problems. using our present work provides a strong theoretical framework for understanding sparse filtering, it highlights assumptions and conditions for achieving success behind the algorithm, and it provides a fresh concrete insight into developing new fuzzy feature distribution learning performance algorithms.", "histories": [["v1", "Tue, 29 Mar 2016 16:23:32 GMT  (869kb,D)", "http://arxiv.org/abs/1603.08831v1", "47 pages, 10 figures"], ["v2", "Tue, 13 Dec 2016 21:34:44 GMT  (664kb,D)", "http://arxiv.org/abs/1603.08831v2", "49 pages, 9 figures"], ["v3", "Thu, 10 Aug 2017 22:57:16 GMT  (659kb,D)", "http://arxiv.org/abs/1603.08831v3", "54 pages, 9 figures"]], "COMMENTS": "47 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fabio massimo zennaro", "ke chen"], "accepted": false, "id": "1603.08831"}, "pdf": {"name": "1603.08831.pdf", "metadata": {"source": "CRF", "title": "Towards Understanding Sparse Filtering: A Theoretical Perspective", "authors": ["Fabio Massimo Zennaro", "Ke Chen"], "emails": ["zennarof@cs.manchester.ac.uk", "chen@cs.manchester.ac.uk"], "sections": [{"heading": null, "text": "Keywords: sparse filtering, feature distribution learning, soft clustering, information preservation, cosine metric"}, {"heading": "1. Introduction", "text": "Unsupervised learning can be conceptualized as the area of research within machine learning concerned with the problem of modeling data. This problem is stated as the problem of learning a transformation which maps data in a given representation onto a new representation. Contrasted with supervised learning, where we are provided labels and we learn a (logic, causal or stochastic) relationship between the data and the labels, unsupervised learning can not rely on any external semantics in the form of labels. Thus, in order to direct learning, unsupervised learning must rely on the specification of assumptions and constraints. These assumptions and constraints translate our very understanding of the problem of modeling data and our intuition on how useful or meaningful mappings may be learned (for example, if we judge that a useful representation of the data would be provided by grouping together the instances according to a specific metric, then we may employ a clustering algorithm to\nar X\niv :1\n60 3.\n08 83\n1v 1\n[ cs\n.L G\n] 2\n9 M\ngenerate one-hot representations of the data).\nOften, the tacit aim of unsupervised learning is to generate representations of the data that may next simplify the problem of learning meaningful relationships through supervised learning. Coates et al. (2011) clearly showed that very simple unsupervised learning algorithms (such as k-means clustering), when properly tuned, can generate representations of the data that allow even basic supervised classifiers, such as support vector machines, to achieve state-of-the-art performances.\nOne common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al. (2006). Several different algorithms have been developed or have been adapted to learn sparse representations; for a recent survey of these algorithms we refer the reader to Zhang et al. (2015)."}, {"heading": "1.1 Sparse Filtering and Related Work", "text": "In 2011, Ngiam et al. (2011) proposed an original unsupervised learning framework for generating sparse representation.\nFirst, they put forward the idea and the intuitive definition of an alternative way to perform unsupervised learning. Most of the successful unsupervised learning algorithms may be described as data distribution learning algorithms, that is, algorithms that try to learn new representations which better model the underlying probability distribution that generated the data. In contrast, they proposed the possibility of developing feature distribution learning algorithms, that is, algorithms which try to learn a new representation having desirable properties, without taking into account the problem of modeling the distribution of the data.\nConsistent with the feature distribution learning framework, they defined a concrete algorithm named sparse filtering. Sparse filtering ignores the problem of learning the data distribution and instead focuses only on optimizing the sparsity of the learned representations. Sparse filtering proved to be an excellent algorithm for unsupervised learning: it is extremely simple to tune, since it is \u201cessentially hyperparameter-free\u201d, having only a single hyper-parameter to select; it scales very well with the dimension of the input; it is easy to implement; and, more importantly, it was shown to achieve state-of-the-art performance.\nBecause of its simplicity and its performance, the sparse filtering algorithm has been studied and adopted by other researchers. In the original paper, Ngiam et al. (2011) showed that sparse filtering was able to provide state-of-the-art performance on image recognition and phone classification. Thaler (see Goodfellow et al., 2013) and Romaszko (2013) used sparse filtering in their machine learning systems while taking part into the Kaggle Black Box Learning Challenge, achieving respectively the first and the sixth best positions. These results spurred interest in sparse filtering.\nIndeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al. (2014) modified the original sparse filtering algorithm by introducing a penalty on the weight matrix. These studies highlight a clear interest in the refinement and improvement of the original algorithm.\nAt the same time, on the practical side, the simplicity of the sparse filtering algorithm favored its adoption in many real-world applications: Raja et al. (2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al. (2016) implemented sparse filtering in their system for hyper-spectral anomaly detection; and, Ryman et al. (2016) integrated it in their system for modeling the olfactive temporal response in arrays of chemically diverse sensors. All of these applications bear witness to the usefulness of sparse filtering and to the simplicity of integrating it and using it in very different machine learning tasks.\nSome studies have also provided sparse filtering with some biological support. Bruce et al. (2016) analyzed different biologically-grounded principles for representation learning of images, using sparse filtering as a starting point for the definition of new learning algorithms. More interestingly, Kozlov and Gentner (2016) used sparse filtering to model the receptive fields of high-level auditory neurons in a songbird; relying on the idea that sparseness and normalization are canonical neural processing operations (Carandini and Heeger, 2012), their results show that sparse filtering can reproduce the receptive fields of the European starling\nand provides further confirmation of the general hypothesis that sparsity and normalization are general principles of neural computation in the sensory system of animals. Sparse filtering may then be of interest not only for practical machine learning applications, but also for modeling and understanding biological systems."}, {"heading": "1.2 Problem Statement", "text": "So far, the sparse filtering algorithm has been successfully applied to many scenarios, and its usefulness is beyond doubt. However, a clear theoretical explanation of the algorithm is still lacking. Ngiam et al. (2011) drew connections between sparse filtering, divisive normalization, independent component analysis, and sparse coding, while Lederer and Guadarrama (2014) provided a deeper analysis of the normalization steps inside the sparse filtering algorithm. Despite this, the reasons why sparse filtering, in particular, and feature distribution learning, in general, work are left unexplored. In this research, we do not aim to show whether or how well sparse filtering works; its state-of-the-art performance has already been reported and reproduced. Instead, we aim at understanding why and when sparse filtering works well.\nWe begin by arguing that any unsupervised learning algorithm, in order to work properly, has to deal with the problem of preserving information conveyed by the probability distribution of the data. Now, given that feature distribution learning ignores the problem of learning the data distribution, a natural question arises: how is the information conveyed by the data distribution preserved in feature distribution learning and in sparse filtering?\nWe first tackle this question about information preservation in the generic case of feature distribution learning. We believe that addressing this question in the abstract is important, as it will give insights on how we could develop new feature distribution learning algorithms. The first step in order to discuss information preservation in feature distribution learning algorithms is to have a clear understanding of what feature distribution algorithms are. We then state the problem: how can we distinguish between data distribution learning algorithms and feature distribution learning algorithms?\nOnce we have this understanding, we can tackle the actual problem of information preservation. We then consider: what does it mean to ignore learning the data distribution and to what degree can we actually ignore it?\nNext, we turn to the question about information preservation in the concrete case of sparse filtering. The practical success of sparse filtering suggests that the algorithm is indeed able to preserve relevant information conveyed in the distribution of the data. However, no explanation for this behavior has been given. To understand how sparse filtering may preserve information, we focus on the transformations of the data defined by the algorithm. We then investigate the properties of these transformations in order to answer the following question: is there any sort of data structure that is preserved by the processing in sparse filtering?\nIf we can prove that sparse filtering does indeed preserve information, the subsequent questions concern whether our formal analysis can help us explain the success or the failure of sparse filtering in practical applications. In particular we consider: can we explain the\nfailure of alternative forms of sparse filtering on the grounds of information preservation? Can we explain the success of sparse filtering in terms of the type of structure preserved? Can we identify scenarios in which sparse filtering is likely to be helpful and other scenarios in which it is likely not to be useful?"}, {"heading": "1.3 Contributions", "text": "We summarize the contributions made in this study as follows:\n\u2022 We present a theoretical framework that contributes to a better understanding of feature distribution learning, provides a way to distinguish it from data distribution learning, and explains the role of information preservation.\n\u2022 We provide a deep theoretical and practical understanding of how sparse filtering works, by describing its dynamics and its properties, by proving that it preserves information through data structure preservation, and by showing which sort of data structure it preserves.\n\u2022 We apply our understanding to concrete scenarios, thus offering a justification of the real-world success of sparse filtering, providing an explanation for the failure of alternative forms of sparse filtering, and suggesting a set of conditions under which we can expect sparse filtering to be useful.\nUltimately, we show that sparse filtering, through its implicit constraints, encodes a precise assumption about the data that necessarily makes it more suitable for certain scenarios."}, {"heading": "1.4 Organization", "text": "The rest of this paper is organized as follows. We first review the concepts and ideas forming the foundations of our work (Section 2). Next, we propose a more rigorous conceptualization of the idea of feature distribution learning (Section 3). This conceptual study drives the following theoretical analysis of the sparse filtering algorithm (Section 4). The theoretical results we achieved inform the ensuing experimental simulations (Section 5). We then discuss the results we collected, in light of our analysis of sparse filtering, in particular, and feature distribution learning, in general (Section 6). Finally, we draw conclusions by summarizing our contributions and highlighting future developments (Section 7).\nThe mathematical notation is introduced progressively through the text, but in Table 1 we offer a quick reference that summarizes the conventions we adopted."}, {"heading": "2. Foundations", "text": "In this section we review basic concepts underlying our study. We provide a rigorous description of unsupervised learning, we relate it to distribution learning, we formalize the property of sparsity, and, finally, we bring all these concepts together in the definition of the sparse filtering algorithm."}, {"heading": "2.1 Unsupervised Learning", "text": "Let X = {x(i) \u2208 RO}Ni=1 be a set of N samples or data points represented as vectors in an O-dimensional space. We will refer to the given representation of a sample x(i) in the space RO as the original representation of the sample x(i) and to RO as the original space. From an algebraic point of view, we can represent the data set as a matrix X of dimensions (O \u00d7 N); from a probabilistic point of view, we can model the data points x(i) as i.i.d. samples from a multivariate random variable X = (X1, X2, . . . , XO) with pdf p (X).\nUnsupervised learning discovers a transformation f : RO \u2192 RL mapping the set X = {x(i) \u2208 RO}Ni=1 from an O-dimensional space to the set Z = {z(i) \u2208 RL}Ni=1 in an Ldimensional space. We will refer to the transformed representation z(i) in the space RL as the learned representation of the sample x(i) and to RL as the learned space. Again, from an algebraic point of view, we can represent the transformed data set as a matrix Z of dimensions (L \u00d7 N); from a probabilistic point of view, we can model the data points z(i) as i.i.d. samples from a multivariate random variable Z = (Z1, Z2, . . . , ZL) with pdf p (Z).\nUnsupervised learning is often used for learning better representations for ensuing supervised tasks. Suppose that we are given a set Y = {y(i) \u2208 R}Ni=1 of N labels, such that the ith label in Y is associated to the ith sample in X . From an algebraic point of view, we can represent the labels as a vector Y of dimensions (1\u00d7N); from a probabilistic point of view, we can model the labels y(i) as i.i.d. samples from a random variable Y with pdf p (Y ). Let us now consider the new data set (X ,Y) = {( x(i), y(i) ) \u2208 RO \u00d7 R }N i=1\n. In this scenario, the aim of unsupervised learning is to learn representations z(i) such that modeling the relationship f : z(i) 7\u2192 y(i) is easier than modeling the relationship f : x(i) 7\u2192 y(i).\nClustering. A specific form of unsupervised learning is clustering. In general, clustering algorithms can be categorized as hard or soft. Hard clustering discovers a transformation f : RO \u2192 RL mapping the original samples x(i) onto one-hot representations z(i), where the single non-null component of z(i) encodes the assignment of the original sample to a cluster. Soft clustering discovers a transformation f : RO \u2192 RL mapping the original samples x(i) onto representations z(i), where the value of each component of z(i) encodes the degree of membership of the original sample to each cluster.\nSoft clustering algorithms may be used for learning representations z(i) that simplify the problem of modeling the relationship f : z(i) 7\u2192 y(i). A standard soft clustering algorithm is grounded on the following assumptions. (i) Data points are generated by a true stochastic process with pdf p (X\u2217) and then corrupted by some form of random noise; the final samples x(i) with pdf p(X) are therefore corrupted versions of the true samples and they are expected to have a weaker correlation to the labels y(i) than the true samples. (ii) The true pdf p (X\u2217) may be approximated through a mixture model. (iii) Relationships of neighborhoodness under a chosen metric in the original space RO allows us to recover the original pdf p (X\u2217); data points x(i) that happen to be next to each other in the original space under the chosen metric are taken to be similar. Under these assumptions, soft clustering algorithms instantiate a set of C clusters (each one describing one component of the mixture model) and group into the clusters nearby data points. Two data points x(1) and x(2) falling in the same clusters are represented by the same exemplar x\u0304, assuming that such an exemplar contains all the relevant information carried by x(1) and x(2), and that the information\ncontained in the difference between x(1) or x(2) and the exemplar x\u0304 amounts to noise. If all the assumptions are correct, a soft clustering algorithm will learn new representations z(i) whose pdf p(Z) is closer to the true pdf p (X\u2217) than the original pdf p(X); and, therefore, it will be easier to learn f : z(i) 7\u2192 y(i) or p(Y |Z) than learning f : x(i) 7\u2192 y(i) or p(Y |X)."}, {"heading": "2.2 Distribution Learning", "text": "Distribution learning is a form of unsupervised learning focused either on modeling the true pdf p (X\u2217) or on shaping a useful pdf p(Z).\nData Distribution Learning. Intuitively, we define data distribution learning as any unsupervised learning algorithm that is concerned with learning a pdf p (Z) that closely matches the true pdf p (X\u2217). Data distribution learning algorithms aim at estimating p (X\u2217) from the available data, and the learned representation z(i) is meant to encode the factors that explain the original data x(i). Examples of data distribution learning algorithms include denoising auto-encoders (DAE), restricted Boltzmann machines (RBM), and independent component analysis (ICA) (Ngiam et al., 2011).\nIn the context of representation learning for supervised tasks, learning a pdf p(Z) that approximates the true pdf p (X\u2217) is meaningful under the assumption that the labels we are given are strongly correlated with the true distribution of the data. If p(Z) is a good estimation of the true pdf p (X\u2217), we can reasonably expect that learning p(Y |Z) or p(Z, Y ) will be simplified.\nFeature Distribution Learning. In contrast, we intuitively define feature distribution learning as any unsupervised learning algorithm that is concerned with learning a pdf p (Z) which has a set of desirable properties. Feature distribution learning algorithms overlook the problem of estimating the true distribution p (X\u2217) and focus instead on shaping the learned pdf p (Z) according to chosen criteria. The most representative algorithm of this family is sparse filtering (SF) (Ngiam et al., 2011).\nIn the context of representation learning for supervised tasks, learning a pdf p(Z) with specific properties is meaningful if we know a priori that certain properties will be useful for supervised learning. Properties like sparsity or smoothness are well known to be useful properties when modeling p(Y |Z) or p(Z, Y ) and, therefore, they can be imposed during feature distribution learning."}, {"heading": "2.3 Sparsity", "text": "Definition (Sparsity). Given a generic vector v in an N -dimensional space, we say that v is sparse if a small number of components of the vector accounts for most of the energy1 of the vector (Hurley and Rickard, 2009). Practically, we say that the vector v is sparse if n N components of the vector v are active (that is, have a value different from zero) while the remaining N \u2212 n components are inactive (that is, have the value zero). A vector v is k-sparse if exactly k components are active. By analogy, we may define sparsity for matrices (with reference to their components)\n1. We refer here to the meaning of energy from signal processing, that is: energy(v) = \u2211N\ni=1 |vi| 2.\nand for random variables (with reference to their realizations).\nMeasures of sparsity. Several measures of sparsity have been proposed in the literature; Hurley and Rickard (2009) offer a review of different measures of sparsity and their properties. Here, we pay attention only to a specific measure from the family of `p-norm measures.\nOne of the most common measures of sparsity is the `1-norm. Given a generic vector v in an N -dimensional space, its `1-norm is defined as: `1(v) = \u2211N i=1 |vi|. The `1-norm is an approximation of the `0-norm that has a continuous derivative (except in the origin), and thus is easily optimized with standard derivative-based techniques. In the sparse filtering literature, the `1-norm is often referred to as activation. Given a representation z(i), we will quantify its sparsity by computing `1 ( z(i) ) or activation ( z(i) ) . Minimizing the activation of the learned representation z(i) will maximize the sparsity of z(i)."}, {"heading": "2.4 Sparse Filtering", "text": "Sparse filtering is the most representative example of feature distribution learning algorithms (Ngiam et al., 2011). Its aim is learning a pdf p(Z) which maximizes the sparsity of the learned representations z(i).\nEnforcement of sparsity in sparse filtering. A sparse learned representation z(i) is achieved by enforcing three constraints on the matrix of learned representations Z:\n\u2022 Population sparsity : each sample z(i), 1 \u2264 i \u2264 N , is required to be sparse, that is, described only by a few features. The sparsity of a sample z(i) is computed as its activation: `1 ( z(i) ) = \u2211L\nj=1 \u2223\u2223\u2223z(i)j \u2223\u2223\u2223. \u2022 Lifetime sparsity : each feature zj , 1 \u2264 j \u2264 L, is required to be sparse, that is, to\ndescribe only a few samples. Lifetime sparsity is often referred to as selectivity (Goh et al., 2012). The sparsity of a feature zj is computed as its activation: `1 (zj) =\u2211N\ni=1 \u2223\u2223\u2223z(i)j \u2223\u2223\u2223. \u2022 High dispersal : all the features zj , 1 \u2264 j \u2264 L, are required to have approximately the\nsame activation. The dispersal of the features zj is computed as the variance of their activation: V ar [activation(zj)]. Lower variance corresponds to higher dispersal.\nThe enforcement of these three properties translates into learning non-degenerate sparse representation.\nImplementation of sparse filtering. Sparse filtering is implemented as a simple algorithm in six steps (refer to Figure 1 for an illustration of this decomposition and to Figure 2 for an illustration of the transformations on a two-dimensional data set):\nA0. Initialization of the weights: the weight matrix W with dimensions (L\u00d7O) is initialized by sampling each component from a normal distribution N (0, 1).\nA1. Linear projection of the original data: fA1(X) = WX. The weight matrix W can be interpreted as a dictionary (Denil and de Freitas, 2012) or as a filter bank (Dong et al., 2015), where each row is a codeword or a filter applied to every sample in the columns of X. Refer to Figure 2(a) and 2(b) for an illustration of this transformation.\nA2. Non-linear transformation: F = fA2 (WX), where fA2(\u00b7) : R \u2192 R is an element-wise non-linear function. Although this non-linear function can, in principle, be arbitrarily chosen, all the implementations known to the authors used an element-wise absolutevalue function f(x) = |x|. For practical reasons, this non-linearity is implemented as a soft absolute-value f(x) = \u221a x2 + , where is a small negligible value (for instance,\n= 10\u22128). Refer to Figure 2(b) and 2(c) for an illustration of this transformation.\nA3. `2-normalization along the features (or along the rows): F\u0303 = fA3 (F) = F (i) j\u221a\u2211N\ni=1\n( F\n(i) j )2 . In this step, each feature is normalized so that its squared activation is one, that is,\u2211N\ni=1\n( F\u0303\n(i) j\n)2 = 1. Refer to Figure 2(c) and 2(d) for an illustration of this transfor-\nmation. A4. `2-normalization along the samples (or along the columns): Z = F\u0302 = fA4 ( F\u0303 ) =\nF\u0303 (i) j\u221a\u2211L\nj=1\n( F\u0303\n(i) j )2 . In this step, each sample is normalized so that its squared activation is one, that is, \u2211L j=1 ( F\u0302 (i) j )2 = 1. Refer to Figure 2(d) and 2(e) for an illustration of this transformation.\nA5. `1-minimization: min \u2211 ij F\u0302 (i) j . This minimization is the objective of sparse filtering;\nby minimizing the overall activation of the matrix F\u0302, we maximize the sparsity of the learned representations.\nNotice that step A0 is executed only during the initialization of the algorithm, while step A5 is executed only during learning. After learning, new data X\u2032 is processed through step A1 to A4, that is, Z\u2032 = fA1:A4 (X\u2032) = fA4 (fA3 (fA2 (WX\u2032)))\nAs explained by Ngiam et al. (2011), the combination of the `1-minimization with the two `2-normalizations guarantees the learning of a representation with the properties of population sparsity, lifetime sparsity, and high dispersal."}, {"heading": "3. Conceptual Analysis of Feature Distribution Learning", "text": "We believe that the intuitive definition of feature distribution learning reviewed in Section 2.2 is unsatisfactory. On one side, it can not really be used to categorize algorithms that both try to learn the true distribution of the data p (X\u2217) and to model a useful distribution of the features p(Z) (for example, sparse RBM, Ranzato et al., 2007). On the other hand, it is not clear what is implied by the fact that feature distribution learning ignores learning the true data distribution p (X\u2217); \u201cignoring\u201d may mean anything ranging from the extreme option of \u201ccompletely disregarding the problem of learning the data distribution\u201d to the more moderate option of \u201cnot caring about optimizing the learning of the data distribution.\u201d\nA better understanding of feature distribution learning is needed to properly study and analyze concrete instances of feature distribution learning, such as sparse filtering. Here we suggest a more comprehensive definition of distribution learning which is grounded on information-theoretic concepts and which relies on ideas already discussed by the machine learning community."}, {"heading": "3.1 Infomax Principle and Informativeness Principle", "text": "Vincent et al. (2010) argued that an unsupervised learning algorithm can generate good representations by satisfying two requirements: (i) retaining information about the input, and (ii) applying constraints that lead to the extraction of useful information from noise.\nIn more general terms, we may state that a good unsupervised representation may be obtained by satisfying the two following information-theoretic requirements: (i) maximizing the mutual information between input and output, and (ii) maximizing a measure of information of the output (against noise). The first requirement is the same as the one stated by Vincent et al. (2010), and it corresponds to the infomax principle (Linsker, 1989). The second requirement, for a lack of a better term, will be referred to as informativeness principle.\nAccording to this understanding, the aim of feature distribution learning may be expressed as a pure optimization of the informativeness principle. That is, we try to learn a transformation f such that the pdf p(Z) of the new representations z(i) has maximal informativeness. Maximizing the informativeness may be simply expressed as the minimization\n(a) Original representation of the data X in R2. (b) Linear projection of the data onto the intermediate representation WX. (c) Non-linear projection of WX using an absolute-value function onto the intermediate representation F. (d) `2- normalization of the data F along the features, yielding the intermediate representation F\u0303. (e) `2-normalization of the data F\u0303 along the samples, yielding the final learned representation F\u0302 = Z. Notice that the colors of the data points x(i) do not have any meanings. A random color has been assigned to each point in order to allow the tracking of the location of the points through the different transformations applied by sparse filtering.\nof the entropy H [Z] or the maximization of the relative entropy between the learned pdf p (Z) and the entropy-maximizing pdf q:\nmax D [p(Z) \u2016 q] ,\nwhere D [\u00b7] is a measure of distance or divergence between the pdfs, such as the KullbackLeibler divergence (MacKay, 2003).\nHowever, without other requirements, the objective of maximizing the information is not sufficient to lead to any useful or meaningful learning. The optimal solution of the problem of maximizing D [p(Z) \u2016 q] is trivially learning a pdf with the shape of a Dirac delta function. If we map all the original samples x(i) onto an arbitrary representation z\u0304, then the pdf p(Z) will have the shape of a Dirac delta function centered on z\u0304. This pdf has minimal entropy and, therefore, maximal informativeness. However, it is clear that arbitrarily mapping all the samples x(i) to a constant representation z\u0304 has no meaning. We maximize the information in p (Z) but we lose all the information carried by p (X) about p (X\u2217).\nIt is necessary, therefore, to take into consideration the infomax principle as well. This translates into the maximization of the mutual information I [X;Z], or, equivalently, into the maximization of the relative entropy between p(X,Z) and p(X)p(Z):\nmax D [p(X,Z) \u2016 p(X)p(Z)] ,\nwhere, again, D [\u00b7] is a measure of distance or divergence between the pdfs.\nLike any unsupervised learning algorithm, feature distribution learning has to somehow negotiate the trade-off between the infomax principle and the informativeness principle:\nmax D [p(X,Z) \u2016 p(X)p(Z)]\ufe38 \ufe37\ufe37 \ufe38 infomax principle + max D [p(Z) \u2016 q]\ufe38 \ufe37\ufe37 \ufe38 informativeness principle . (1)\nEven if the definition of feature distribution learning makes no reference to the infomax principle, we argue that it must taken into account in some way.\nThe learning objective defined in Equation 1 remains mainly theoretical. Informationtheoretic quantities, like relative entropy, are extremely hard to estimate and therefore we have to rely on approximations or heuristics. Moreover, the optimization of a dual-objective function is often very challenging; operational research suggests that a simpler approach would be to translate one objective into a constraint and explicitly optimize the remaining one. Indeed, this approach seems to provide a better way to understand the difference between data distribution learning and feature distribution learning.\nData distribution learning. In relation to Equation 1, we can define data distribution learning as any unsupervised learning in which the main objective is maximizing the infomax principle (first term in Equation 1), while the maximization of the informativeness principle (second term in Equation 1) is accounted for through priors or constraints.\nFor instance, DAEs approximate the maximization of the mutual information I [X;Z] through the minimization of the reconstruction error; Vincent et al. (2010) showed that minimizing the reconstruction error in a DAE is indeed equivalent to maximizing a lower bound on the mutual information. Similarly, RBMs approximate the maximization of the mutual information I [X;Z] through the minimization of the divergence between the distribution of the data and the learned distribution via the maximization of the log probability of the data (Hinton et al., 2006). These algorithms do not tackle the problem of maximizing the relative entropy D [p(Z) \u2016 q] directly, but they often address it by using constraints, such as bottleneck architectures (Tishby et al., 2000), or by imposing priors, such as adding a sparsity penalty to the learning objective (Vincent et al., 2010).\nFeature distribution learning. Again, in relation to Equation 1, we can define feature distribution learning as any unsupervised learning in which the main objective is maximizing the informativeness principle (second term in Equation 1), while the maximization of the infomax principle (first term in Equation 1) is accounted through priors or constraints."}, {"heading": "4. Theoretical Analysis of Sparse Filtering", "text": "The previous conceptual analysis of feature distribution learning provides the bedrock for studying and understanding sparse filtering. Relying on the fact that any unsupervised algorithm (including feature distribution learning algorithms) must somehow satisfy the infomax principle and the informativeness principle, we will deploy a set of conceptual tools, definitions, proofs, and lemmas to demonstrate the following thesis:\nSparse filtering does satisfy the informativeness principle through the maximization of the proxy of sparsity and it satisfies the infomax principle through the constraint of preservation of the structure of cosine neighborhoodness of the data."}, {"heading": "4.1 Informativeness Principle", "text": "Showing that sparse filtering satisfies the informativeness principle is straightforward. Since the explicit maximization of the relative entropy D [p(Z) \u2016 q] is computationally hard, the sparse filtering algorithm adopts the standard proxy of sparsity. Increasing the sparsity of the representations z(i) increases the entropy of the learned distribution p (Z) by concentrating the mass of the pdf p(Z) around zero. Thus, minimizing the `1-norm of the learned representations z(i) in the objective function accounts for the maximization of the informativeness of the learned representation z(i)."}, {"heading": "4.2 Infomax Principle", "text": "Showing that sparse filtering satisfies the informativeness principle is more complex. By definition, as a feature distribution learning algorithm, sparse filtering does not address the problem of modeling the data distribution. However, by virtue of the fact that sparse filtering works and its learned representations z(i) allow the achievement of state-of-the-art performance when learning p (Y |Z), it must be that the algorithm preserves information contained in the original representations x(i). If it were not so, sparse filtering could simply\nsolve its optimization problem by mapping the original data matrix X onto a pre-computed sparse representation matrix Z\u0304 with a minimal computational complexity of O(1); however, these representations z(i) would clearly be useless because of the independence between representations and labels, from which follows that p(Y |Z) = p(Y ).\nSince sparse filtering does not try to explicitly model p (X\u2217), we hypothesize that it must implicitly preserve information about p (X\u2217) conveyed by the pdf p(X). In particular, we hypothesize that sparse filtering preserves the information conveyed by the pdf p(X) through the proxy of the preservation of data structure. The geometric structure of the data in the original space RO constitutes a set of realizations of the random variable X through which we can estimate the pdf p(X). Preserving relationships of neighborhoodness (under a given metric) allows us to preserve information conveyed by the pdf p(X): regions of high density and low density in the domain of p(X) can be maintained by preserving relationships of neighborhoodness. Thus, preservation of the geometric structure under a chosen metric may act as a proxy for the maximization of mutual information I[X;Z]."}, {"heading": "4.3 Non-preservation of Euclidean Distances", "text": "The preservation of absolute or relative distances under the Euclidean metric is the most common way to preserve the structure of the data. However, it can be easily ruled out that sparse filtering preserves this type of structure.\nProposition. Let { x(i) \u2208 RO }N i=1\nbe a set of points in the original space RO. Then, the transformations from A1 to A4 do not preserve the structure of the data described by the Euclidean metric.\nProof. Considering transformation A1, a generic linear transformation does not provide specific guarantees on the preservation of Euclidean distances from the original space RO to the learned space RL. Specific instances of linear transformation may preserve Euclidean absolute distances (rotations and reflections) or relative distances (scaling). The JonhsonLindestrauss lemma (Dasgupta and Gupta, 2003; Johnson and Lindenstrauss, 1984) sets bounds on the preservation of the distances by random projections; however, we have no clear guarantees that the randomly initialized weight matrix W would implement such a transformation or that, during training, sparse filtering would learn a transformation that preserves Euclidean distances.\nConcerning transformation A2, the absolute-value function does not preserve, in general, absolute or relative Euclidean distances. Absolute distances are preserved only in the particular case in which the input points belong to the same orthant in the domain RL. This effect is particularly obvious when we interpret the application of absolute-value as a folding of the space RL onto the positive orthant (Montufar et al., 2014).\nConcerning transformation A3, the normalization along the features is a transformation that preserves relative Euclidean distances from the domain to the codomain space. This can be intuitively understood by observing that the normalization along the features merely corresponds to a rescaling of the axes in the feature space. A formal proof may be given showing that the transformation A3 can be encoded as a linear transformation with an\nassociated positive diagonal matrix. By definition, a positive diagonal matrix preserves relative distances among the points.\nConcerning transformation A4, the normalization along the samples is a transformation with no guarantees of preserving absolute or relative distances from the domain to the codomain space. As all the samples are projected from the positive orthant RL\u22650 to the surface of the unit hyper-sphere, relationships of relative distance are not preserved.\nSince more than one of the transformations from A1 to A4 do not preserve Euclidean distances, we can conclude that the overall transformation fA1:A4(\u00b7) does not preserve Euclidean distances."}, {"heading": "4.4 Preservation of Collinearity", "text": "Having ascertained that sparse filtering can not preserve the data structure defined by the Euclidean metric, we investigate other properties of the algorithm that may lead us to discover the preservation of alternative data structures. A first relevant observation is that sparse filtering preserves collinearity.\nProposition. Let x(1),x(2) \u2208 RO be collinear points in the original space RO. Then, the outputs of transformations from A1 to A4, that is fA1:A4 ( x(1) ) , fA1:A4 ( x(2) ) \u2208 RL, are collinear. Proof. Concerning transformation A1, linear transformations preserve collinearity as proved in appendix A.1. Concerning transformation A2, the absolute-value function preserves collinearity by rigidly folding all the orthants on the first one. A formal proof is given in appendix A.2. Concerning transformation A3, the normalization along the features preserves collinearity as well, by acting simply as a rescaling of the axes. A formal proof is given in appendix A.3.\nConcerning transformation A4, the normalization along the samples also preserves collinearity. A formal proof is given in appendix A.4.\nSince all the transformations from A1 to A4 preserve collinearity, we can conclude that the overall transformation fA1:A4(\u00b7) preserves collinearity."}, {"heading": "4.5 Homo-representation of Collinear Points", "text": "An immediate consequence of the previous proposition is the following theorem which states that all the collinear points in the original representation space are mapped to an identical representation. This result is significant as it gives us a first understanding of the principle and the type of metric that sparse filtering uses to map original samples x(i) onto their representations z(i).\nTheorem. Let x \u2208 RO be a point in the the original space RO. Then there is a set of infinite points x(i) \u2208 RO such that fA1:A4(x) = fA1:A4 ( x(i) ) . This set is the set of the points collinear with x. Proof. This theorem follows from the proposition of preservation of collinearity."}, {"heading": "4.6 Homo-representation of Points with Same Moduli", "text": "A further analysis of sparse filtering reveals that not only collinear points are mapped to the same representation, but also points in the learned representation space having the same moduli (that is, the same absolute value for their components) are mapped to identical representations. Again, this result is relevant since it sheds light on the type of structure preserved by sparse filtering.\nTheorem. Let z \u2208 RL be a point in the codomain of the function WX. It holds that for a z in the first orthant, there are at least 2L points z(i) \u2208 RL such that fA2:A4(z) = fA2:A4 ( z(i) ) .\nProof. By definition, z = [ z(1) z(2) . . . z(L) ] is an L-dimensional vector whose\ncomponents belong to the first orthant and are therefore positive. Now, the application of the absolute-value fA2(\u00b7) maps z to itself:\nfA2 (z) = [ z(1) z(2) . . . z(L) ] = z.\nHowever, all the vectors z(i) in the codomain of the function WX with the following form are mapped to z by the absolute-value fA2(\u00b7) :\nz(i) = [ \u00b1z(1) \u00b1z(2) . . . \u00b1z(L) ] .\nBy combinatorial analysis, we know that there are 2L possible ways of picking the values of z(i), thus defining 2L points in RL that are mapped to the same value z by the the absolutevalue fA2(\u00b7). Since all the points z(i) are mapped to the same point z at the end of step A2, the application of the remaining deterministic functions in the following steps of sparse filtering will map them to the same representation, so that fA2:A4(z) = fA2:A4 ( z(i) ) ."}, {"heading": "4.7 Poles and Pole Pursuit", "text": "To facilitate our formal description, we now introduce two concepts that will be useful in our analysis.\nDefinition (Poles). In an L-dimensional space RL, a pole is a point identified by a vector p such that \u2203! i, 1 \u2264 i \u2264 L for which pi = 1, and \u2200j, j 6= i, 1 \u2264 j \u2264 L then pj = 0.\nPoles are then binary 1-sparse vectors having a single component set to one. The following properties derive easily from the definition: (i) in an L-dimensional space there are exactly L poles; and, (ii) mapping a set of original representations x(i) \u2208 RO to the poles of RL produces an optimal solution for the sparse filtering algorithm, as the poles have a minimal `1-norm in RL under the constraint of sparse filtering.\nDefinition (Pole Pursuit). Given a set of original representations x(i) \u2208 RO, we define the search for an optimal solution as pole pursuit. Through gradient descent, the representations z(i) \u2208 RL are progressively pushed towards the poles of RL.\nHowever, in general, notice that sparse filtering is not guaranteed to find a solution in which all the original representations x(i) are mapped onto poles. The achievement of such\nan optimal solution depends on the original data set X , on the dimensionality of the learned space L, and on the random initialization of the weight matrix W. Gradient descent may lead sparse filtering to settle into a sub-optimal solution where the original representations x(i) are not mapped onto poles but onto k-sparse (k > 1) representations in RL."}, {"heading": "4.8 Representation Cones", "text": "The propositions we proved above and the concept of pole pursuit allow us to introduce a last conceptual tool that gives us a better insight into the properties and the dynamics of sparse filtering.\nFrom the theorem of homo-representation of collinear points we learned that sparse filtering identifies sets of collinear points in the original space that are mapped onto poles; from the theorem of homo-representation of points with the same moduli we can deduce that there must a symmetric structure around lines of collinear points. Putting together these two intuitions, we can conclude that sparse filtering defines precise maps in the original representation space RO. More precisely, we state that sparse filtering defines representation cones2 in the original representation space RO.\nDefinition (Representation Cone). A representation cone Rp(j) is a function Rp(j) : RO \u2192 R\u22650 mapping points in the original representation space RO to their (Euclidean) distance from the pole p(j).\nPlotting a representation cone Rp(j) in the original representation space RO defines a region of space having a conical (or hyper-conical in higher dimensions) shape, such that all the points on the line of its height are mapped onto the pole Rp(j) , and all the points in its volume are mapped into the neighborhood of the pole Rp(j) . The volume of a representation cone is loosely defined and it changes according to the chosen size for the pole neighborhood.\nMoreover, given a point x(i) \u2208 RO, we say that the representation cone Rp (j)\nx(i) is centered\non x(i) if the point x(i) lies on the line of the height of the representation cone Rp (j)\nx(i) . There-\nfore, the representation cone Rp (j)\nx(i) maps the point x(i) and all the points collinear with x(i)\nto the pole p(j).\nProperties. Several interesting and useful properties immediately follow from the definition of representation cones:\nAssociation with a pole Each representation cone is associated with a single pole in the learned representation space RL.\nExistence of L representation cones Given the learned representation space RL, the sparse filtering algorithm defines exactly L representation cones. This statement easily follows from the fact that in an L-dimensional space RL we have exactly L poles.\nO-dimensionality of the representation cones Given the original representation space RO, the sparse filtering algorithm defines O-dimensional representation cones in the\n2. Cones suggest the idea of two-dimensional shapes; this is not strictly true as representation cones may live in higher dimensions; the nomenclature cones remains, though, as we first developed this idea while working in two-dimensions.\noriginal representation space. Thus, if the original space is two-dimensional, the representation cone is an actual cone; in higher dimensions the representation cone is a hyper-cone.\nBounds of representation cones Given a point x(i) in the original representation space RO, then 0 \u2264 Rp(j) ( x(i) ) \u2264 \u221a\n2 \u2200j, 0 \u2264 j \u2264 L. Since each point x(i) is mapped to a point z(i) on the surface of the unit hyper-sphere in the positive orthant, the distance from any pole of z(i) is bounded between 0 and \u221a 2.\nCloseness to the poles The representation cones comply with a simple rule of inverse proportionality: the closer a point x(i) approaches a pole p(j), the further it moves away from all other poles p(k).\nComplementarity of the representation poles Inspecting a plot of the representation cones gives us a rapid intuitive idea of the quality of the solution: points on the line of height of a representation cone Rp(j)are mapped onto a perfect 1-sparse representation (pole); points within the volume of a representation cone Rp(j)are mapped in the neighborhood of a pole; points far from any representation cone are mapped onto sub-optimal k-sparse representations, with 1 < k \u2264 L.\nLearning. After initialization, the representation cones are randomly placed in the original representation space RO. This leads to an unsatisfactory solution, as random points far from our samples x(i) may be mapped to poles, while the samples x(i) may be left far from the poles. Pole pursuit consists in moving around the representation cones so that at the end they are centered on the samples x(i).\nThe optimization process of sparse filtering can be interpreted as the search for an optimal location of the representation cones: representation cones are rotated in a continuous way in the original representation space, until their placement provides an optimal solution in terms of sparsity of the learned representations. Indeed, the optimal solution to the problem of minimizing the `1-norm of the learned representations z(i) is equivalent to the optimal solution to the problem of minimizing the Rp(j) distances of the original representations x(i). In other words, instead of minimizing the sparsity of the learned representations we can minimize the distances computed by the representation cones.\nLemma (Learning). The optimal solution for the minimization of \u2211N i=1 ` ( z(i) ) , under\nthe constraint `2 ( z(i) ) = 1, is the same as the optimal solution for \u2211N\ni=1 \u2211L j=1R p(j) ( x(i) ) ,\nunder the same constraint. Proof. A proof of this lemma is given in appendix A.5."}, {"heading": "4.9 Preservation of Cosine Neighborhoodness", "text": "Through representation cones we have shown that sparse filtering maps points having the same angles to the same representation and points having similar angles to similar representation. The structure that sparse filtering tries to preserve is therefore the structure defined by the cosine metric in the original space RO:\nDC\n( x(i),x(j) ) = 1\u2212 cos \u03b8,\nwhere DC ( x(i),x(j) ) is the cosine distance between the vectors x(i) and x(j), and \u03b8 is the angle between the vectors x(i) and x(j). Sparse filtering preserves cosine distances only in an approximate way: collinear or close-by points will be mapped next to each other, but points far apart are not necessarily kept separated. This is a consequence of the fact that the linear transformation in step A1 preserves collinearity but does not preserve the cosine metric, and that the non-linearity in step A2 may collapse together points radially far apart. In general, points with small cosine distances DC ( x(i),x(j) ) in the original space RO have\nsmall Euclidean distances DE ( z(i), z(j) ) in the learned space RL.\nRecall that, given a sample x(i) in the original space RO, this sample can be described by O Cartesian coordinates, or by one radial coordinate \u03c1 and O\u2212 1 angular coordinates \u03b8i. Thus, sparse filtering tries to preserve the information about the O\u2212 1 angular coordinates \u03b8i, discarding the information about the radial coordinate \u03c1."}, {"heading": "4.10 Non-preservation of Cosine Neighborhoodness in Alternative Implementations of Sparse Filtering", "text": "The choice of the non-linearity applied in step A2 is crucial for guaranteeing the preservation of cosine neighborhoodness. Indeed, the absolute-value non-linearity is a suitable non-linear function for sparse filtering precisely because of its property of preservation of collinearity.\nNgiam et al. (2011) suggest that the original absolute-value non-linearity may be substituted by other non-linear functions; for instance, standard non-linear functions from the neural networks literature, such as the sigmoid non-linearity or the rectified linear unit (ReLU), may be used. Despite this possibility, all the implementations of sparse filtering so far have relied on the absolute-value non-linearity. An unpublished technical report by Thaler3, after the Kaggle Black Box Learning Challenge, states that sparse filtering with alternative non-linearities (ReLU and quadratic non-linearity) did not perform as well as the absolute-value non-linearity, but does not clarify the reasons of this lack of success. Therefore, for experimental reasons, the absolute-value has always been recommended as the best non-linearity for sparse filtering.\nHere, we argue that one theoretical reason for the limited success of alternative implementations of sparse filtering is due to the fact that they can not provide strong guarantees of preservation of data structure, as the standard implementation of sparse filtering does. If we replace the absolute-value non-linearity with another non-linearity, such as sigmoid or ReLU, we lose the property of preservation of collinearity. Indeed, non-linearities such as sigmoid or ReLU do not induce in the original space representation cones with a regular conical shape, but they define arbitrary regions of space to be mapped onto a pole. Some non-linearities may preserve other structures, such as sigmoid non-linearity which preserves relative Euclidean distances; however, this property is not very useful since other steps in sparse filtering (steps A1 and A4) do not preserve Euclidean distances. From a theoretical perspective, the absolute-value non-linearity is then an optimal choice for the sparse filtering algorithm, in that it preserves the property of collinearity which is also preserved by all the other steps of the algorithm, and it thus guarantees the preservation of cosine neighborhoodness.\n3. https://www.kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge/ forums/t/4717/1st-place-entry"}, {"heading": "4.11 Preservation of Euclidean Neighborhoodness in the Limit Case", "text": "Interestingly, beyond preserving cosine neighborhoodness, we can also prove that, in the limit of an infinite dimensionality (O \u2192\u221e), representation cones defined by sparse filtering not only preserve cosine neighborhoodness, but are likely to preserve relations of Euclidean neighborhoodness.\nTheorem. Let x(i) \u2208 RO be a point in the original space RO and let Rp (k)\nx(i) be a\nrepresentation cone centered on x(i) that maps x(i) onto the pole p(k). Let us now consider a point x(j) in the same representation cone x(j) \u2208 Rp (k)\nx(i) , that is, a point x(j) which is\nmapped by Rp (k)\nx(i) into the neighborhood of p(k).\nLet us assume that: (i) points x(m) distribute on a limited region of space; and, (ii) points x(m) distribute uniformly on this limited region of space.\nThen, if we consider a point x(j), for O \u2192\u221e, P\n( x(j)\u2208neighbourhood(x(i))|x(j)\u2208Rp (k)\nx(i) ) P ( x(j) /\u2208neighbourhood(x(i))|x(j)\u2208Rp (k)\nx(i) ) = 1. Proof. Let us consider x(i) \u2208 RO and let us define its neighborhood as the set of points whose distance from x(i) is at most r. The neighborhood of x(i) is therefore a hyper-sphere of radius r.\nLet us consider now the representation cone Rp (k)\nx(i) centered on x(i) and define its area.\nAs this representation cone is inscribing the neighborhood of x(i), we will take its width around x(i) to be 2r. We also define the length of the representation cone Rp (k)\nx(i) as l. We\ndo not care about the absolute or relative value of l; we only care that it has a finite value, which is guaranteed by the first assumption. In conclusion, the representation cone Rp (k)\nx(i) is\na hyper-cone with a hyper-spherical base of radius 2r and height l. For illustration, refer to the schema in Figure 3, where we represented this setup in the case O = 2. In two dimensions, the neighborhood of x(i) is a circle of radius r, while the representation cone Rp (k)\nx(i) is a triangle with base 2r and height l.\nGiven this setup, we can now compute the volume contained in the neighborhood of x(i)\nand the volume contained by the representation cone Rp (k)\nx(i) . Without loss of generality, we\nwill restrict our attention to the representation cone Rp (k)\nx(i) of height l. We could consider a\nbigger representation cone of height l\u2032 and bigger radius 2r\u2032; however, it will be shown that the ratio between the volumes depends on the dimensionality O, and choosing a different value for the height or the radius does not affect the result.\nLet us consider the neighborhood of x(i), defined as a hyper-sphere of radius r in O dimensions. Its volume can be computed as:\nVsphere(O, r) = VOrO,\nwhere VO is the following function:\nVn = \u03c0\nn 2 \u0393 ( n 2 + 1 ) ,\nx(i) in two-dimensional space.\nand \u0393(n) is the gamma function.\nLet us now consider the representation cone Rp (k)\nx(i) , defined as a hyper-cone with base\nradius r and height l in O dimensions. The volume of the hyper-cone depends on the volume of the lower-dimensional hyper-sphere in the base (Ball, 1997) and it can be computed as:\nVcone(O, r, l) = 1\nO \u00b7 l \u00b7 Vsphere(O \u2212 1, r) =\n1 O \u00b7 l \u00b7 VO\u22121 \u00b7 rO\u22121,\nwhere VO is the function defined above.\nLet us now consider the ratio of the volume of the hyper-sphere and the volume of the hyper-cone as a function of the dimensions O:\nVsphere(O, r) Vcone(O, r, l) = VO VO\u22121 \u00b7 Or l .\nThe behavior of this quantity as a function of the number of dimensions O may be studied by taking the limit:\nlim O\u2192\u221e\nVsphere(O, r) Vcone(O, r, l) = lim O\u2192\u221e O \u00b7 VO VO\u22121 = lim O\u2192\u221e\nO \u00b7 \u0393 ( O+1 2 ) \u0393 ( O+2\n2 ) . Let us rewrite the argument of the gamma function as follows:\nlim O\u2192\u221e\nO \u00b7 \u0393 ( O 2 + 1 2 ) \u0393 ( O 2 + 1 2 + 1 2 ) ,\nand let us substitute x = O2 + 1 2 and, consequently, O = 2x\u2212 1:\nlim x\u2192\u221e x \u00b7 \u0393 (x) \u0393 ( x+ 12 ) . Now, by lemma A.6, we can solve the ratio of the gamma functions:\nlim x\u2192\u221e x \u00b7 1\u221a x = lim x\u2192\u221e \u221a x.\nRe-substituting, to make the dependence from the dimensionality O explicit, we get:\nlim O\u2192\u221e\n\u221a O\n2 +\n1\n2\nlim O\u2192\u221e\n\u221a O =\u221e.\nTherefore, as the dimensionality O of the original space increases, the ratio between the volume of the hyper-sphere and the volume of the hyper-cone tends sub-linearly to infinity.\nAs the dimensionality O increases, the proportion of space accounted for by the volume in the neighborhood of x(i) increases with respect to the total amount of space accounted for by the whole representation cone Rp (k)\nx(i) . Therefore, if we consider a random point x(j) \u2208 Rp\n(k)\nx(i)\nsampled inside the representation cone Rp (k)\nx(i) , then the probability of this point falling in\nthe neighborhood of x(i) grows with the dimensionality O. Then, in the limit, for O \u2192\u221e, P ( x(j)\u2208neighbourhood(x(i))|x(j)\u2208Rp (k)\nx(i) ) P ( x(j) /\u2208neighbourhood(x(i))|x(j)\u2208Rp (k)\nx(i) ) = 1. Notice that this proof is based on two simplified assumptions. First, the region of the original space in which a point x(j) can fall is limited; this assumption is reasonable because, practically, the range of any feature is always bounded, and, technically, we often rescale or normalize features within bounded intervals. Second, a point x(j) has a uniform probability of falling anywhere within the area defined by the representation cone Rx(i) ; this is clearly a simplified assumption because the pdf of the data p(X) may have a very irregular distribution within the area defined by the representation cone Rx(i) ; however, since such a pdf varies from case to case, assuming a uniform distribution, which is a distribution that maximizes our uncertainty, seems a reasonable choice.\nIf we accept these assumptions, we can then conclude that, in the case of a highdimensional space, sparse filtering provides a strong guarantee for the preservation of the cosine neighborhood structure of the data and a weaker asymptotic guarantee for the preservation of the Euclidean neighborhood structure."}, {"heading": "4.12 Sparse Filtering for Representation Learning", "text": "Given the above results, we may now interpret sparse filtering as a soft clustering algorithm for representation learning.\nIndeed, we may state that sparse filtering implicitly makes all the assumptions made by traditional soft clustering algorithms: (i) it aims at discovering less noisy representations\nwhose pdf may automatically be closer to a true stochastic generating process with pdf p (X\u2217); (ii) it models the true pdf p (X\u2217) with a mixture model whose components are related to the poles p(j); and, (iii) it relies on the cosine metric to evaluate relationships of neighborhoodness in the original space RO. From this perspective, we can interpret the dimensionality of the learned space as the number of clusters for soft clustering, the poles as the cluster centroids in a space described by the cosine metric, the pole pursuit process as the sequential process of update of the location of the centroids, and the learned representation z(i) as the (stochastic) degree of membership of the original data samples x(i) to each cluster.\nGiven this interpretation, we can align and meaningfully compare sparse filtering with other soft clustering algorithms for representation learning that use different metrics. The choice of an appropriate metric is critical for a distance-based clustering algorithm (Xing et al., 2003), and it expresses our understanding on which spatial directions encode relevant changes (Simard et al., 1998). It is natural then to compare sparse filtering with other standard algorithms which adopt the Euclidean metric to explain the data. Preserving the relationships of neighborhoodness under the Euclidean metric means preserving the information conveyed by the pdf p(X) in the representation space defined by the Cartesian product of the random variables X1, X2, . . . , XO. Preserving this information for representation learning makes sense if we expect that the structure of the data (with respect to a set of labels) is better explained by an Euclidean structure. In contrast, preserving the relationships of neighborhoodness under the cosine metric means preserving information conveyed by the pdf p(X) in the representation space defined by the projection into polar (or hyper-spherical) coordinates of the random variables X1, X2, . . . , XO. Preserving such information for representation learning makes sense if we expect that the structure of the data (with respect to a set of labels) is better explained by a radial structure."}, {"heading": "5. Empirical Validation", "text": "Based on the theoretical analysis provided in the previous section, we conduct a set of simulations aimed at verifying our theoretical results empirically. As a general rule, in order to make our results visualizable and easily understandable, we favor simple simulations in low dimensions; experiments in higher dimensions generalize our results but they do not add anything conceptually new to our conclusions."}, {"heading": "5.1 Properties of Sparse Filtering", "text": "First, we run simulations on elaborately designed toy data sets in order to validate our basic understanding of sparse filtering. These simulations aim at verifying: (i) the property of homo-representation of collinear points (see Section 4.5); (ii) the property of homorepresentation of points with the same moduli (see Section 4.6); (iii) the usefulness of representation cones (see Section 4.8); and, (iv) the dynamics of pole pursuit (see Section 4.8).\nWe generate a random set of data X of three samples (N = 3) in two-dimensional space (O = 2). Each point is generated using spherical coordinates: the radial distance \u03c1 is sampled from a uniform distribution Unif (\u22125, 5); the angular coordinate \u03b8 is set to \u03c03 for the first two points and sampled from a uniform distribution Unif (0, \u03c0) for the third point. A sparse filtering module is trained on this data set in order to learn a new representation of the data in two dimensions (L = 2). After training, we create a dense mesh of points x\nin the original representation space RO; we project each point x to its representation z in the learned representation space RL, and we compute the distance from each pole p(j) in RL. The plot of each representation cone is then shown as a two-dimensional contour plot in the original space RO.\nFigure 4 shows the state of sparse filtering before training. From the plots 4(b) and 4(d) we can immediately verify the property of homo-representation of collinear points; indeed, in the learned space RL the collinear points occupy the same location and their matrix representation is the same. From the plots 4(e) and 4(f) we can verify the existence of representation cones in the original space RO and appreciate several of the properties discussed above (existence of L representation cones; O-dimensionality of each representation cone; bounds of representation cones; and, complementarity of the representation cones). At the same time, the symmetric structure in the same plots 4(e) and 4(f) validate the properties of homo-representation of points with the same moduli. Notice that, at this point, after the random initialization of the weight matrix W, the quality of the representations generated by the untrained sparse filtering module is far from satisfactory.\nFigure 5 shows the state of sparse filtering at the end of training. From the plots 4(b) and 4(d) we can see that the trained sparse filtering module has found an optimal solution that maps all the points to poles; as expected, the collinear points are mapped to the same pole, while the third point is mapped to the remaining pole. From the plots 4(e) and 4(f) we can validate our intuition about pole pursuit; indeed, training corresponded to a rotation of the representation poles in order to center them on the available samples. Moreover, the same plots 4(e) and 4(f) also confirm the last properties of representation cones which we could not evaluate at the beginning of our simulation (association to the pole; closeness to a pole)."}, {"heading": "5.2 Preservation of Cosine Neighborhoodness", "text": "Next, we run more simulations on other toy data sets in order to validate the properties of data structure preservation in sparse filtering. These simulations aim at verifying: (i) that sparse filtering preserves a structure defined by cosine neighborhoodness; and, (ii) that the absolute-value non-linearity is crucial in preserving structure and substituting it with other non-linearities (such as, sigmoid or ReLU) negates this property.\nWe generated a random set of data X of nine samples (N = 9) in two-dimensional space (O = 2). Each point is generated using spherical coordinates. The first three points have a radial distance \u03c1 sampled from a uniform distribution Unif (\u22122, 0) and an angular coordinate \u03b8 sampled from a uniform distribution Unif ( \u03c0 9 \u2212 \u03b7, \u03c0 9 + \u03b7 ) ; the second three points have a radial distance \u03c1 sampled from a uniform distribution Unif (0, 3) and an angular coordinate \u03b8 sampled from a uniform distribution Unif ( 2\u03c0 9 \u2212 \u03b7, 2\u03c0 9 + \u03b7 ) ; the last three points have a radial distance \u03c1 sampled from a uniform distribution Unif (2, 4) and an angular coordinate \u03b8 sampled from a uniform distribution Unif ( 4\u03c0 9 \u2212 \u03b7, 4\u03c0 9 + \u03b7 ) . The parameter \u03b7 is meant to represent a form of noise and its value is set to \u03b7 = \u03c045 . In this way, we generate three clusters of points, such that the cosine distances among the points belonging to the same cluster are small, while the distances among points belonging to different clusters are large. Three implementations of sparse filtering with different non-\nData is generated as explained in the text (the blue dots represent collinear points). (a) Data in the original representation space RO; (b) data in the learned representation space RL; (c) matrix plot of the original data X; (d) matrix plot of the learned representations Z; (e) plot of first representation cone showing distances from the pole p(1) = [0, 1]T ; (f) plot of the second representation cone showing distances from the pole p(2) = [1, 0]T .\nlinearities (absolute-value, sigmoid, and ReLU4) are used to learn a new representation of the data in three dimensions (L = 3).\nFigure 6 shows the state of the modules of the three implementations of sparse filtering at the end of the training. From the plots 4(a)-4(c) we can immediately verify that sparse filtering with an absolute-value non-linearity preserves cosine neighborhoodness. The representation-cone plots show that points with similar angular coordinates fall within the same representation cones. The matrix plot shows that points with similar angular coordinates are projected onto very similar representations; in other words, points that originally had a small cosine distance are projected onto almost identical representations. On the other hand, from plots 4(d)-4(i) we can easily see that sparse filtering with an alternative non-linearity does not preserve cosine neighborhoodness. The representation-cone plots show that the sigmoid and the ReLU non-linearity do not induce representation cones, but, instead define large regions of the original space to be mapped onto a pole. Since these regions are not rigidly bounded (as in the case of the absolute-value non-linearity) several points are indistinctly mapped onto a pole. The matrix plots show that the representations computed by these alternative sparse filtering modules are not related to the original cosine distances anymore; points originally belonging to the same cluster are mapped to opposite representations, and, vice versa, points originally belonging to different clusters are mapped to identical representations."}, {"heading": "5.3 Sparse Filtering for Representation Learning", "text": "In the following set of simulations, we compare sparse filtering against another unsupervised algorithm, the soft k-means algorithm (MacKay, 2003), in order to show under which conditions sparse filtering is a good choice for processing data. These simulations aim at verifying the following intuitive implication: if the structure of the data (with respect to a specific set of labels) is better explained by the cosine metric, then sparse filtering is likely to be a good option for unsupervised learning.\nIn our comparison, we measure sparse filtering against the soft k-means algorithm. We choose this algorithm for the following reason: (i) like sparse filtering, the soft k-means algorithm is a soft clustering algorithm producing sparse representations; (ii) the algorithm is based on the Euclidean metric, thus providing a different interpretation of the data from sparse filtering; and, (iii) it is a well-known and easy-to-interpret algorithm (even if, analogous results may be obtained by other algorithms, such as Gaussian mixture models or sparse auto-encoders).\nTo validate our hypothesis, we generate two data sets, XEuclid and Xcosine. The data set XEuclid contains data explained by the Euclidean metric. It is composed of nine samples (N = 9) in two dimensions (O = 2). The first three points are sampled from a multivariate\nnormal distribution N ([\n1 1\n] , [ .05 0 0 .05 ]) ; the second three points are sampled from\na multivariate normal distribution N ([\n2 \u22121\n] , [ .05 0 0 .05 ]) ; the last three points are\n4. ReLU has been implemented in a soft version, like the absolute-value: softReLU(x) =\n{ x if x > 0\notherwise\nfor = 10\u22128\nsampled from a multivariate normal distribution N ([ \u22121 \u22121 ] , [ .05 0 0 .05 ]) . The data set Xcosine contains data explained by the cosine metric. The data is generated following the same protocol used in the simulation in Section 5.2. Sparse filtering is used to learn a new representation of the data in three dimensions (L = 3).\nFrom Figure 7, we can see that our understanding of sparse filtering is correct: if the data we are processing are better explained by the cosine metric, then sparse filtering produces a good representation; otherwise, if the data are better explained by the Euclidean metric, then it is better to opt for a different unsupervised learning algorithm, such as soft k-means. In the case of the data set with Euclidean structure, plot 7(b) shows that sparse filtering is not able to preserve the identity of the generating clusters, and indeed it maps samples from the first and the third clusters onto the same representation (because of their collinearity); instead, plot 7(c) shows that soft k-means algorithm maps points from different clusters to different representations. In contrast, in the case of the data set with cosine structure, plot 7(e) shows that sparse filtering preserves the identity of the generating clusters, while plot\n7(f) shows that the soft k-means algorithm is unable to map samples from different clusters onto consistent representations."}, {"heading": "5.4 Sparse Filtering on Real Data Sets", "text": "In this last set of simulations we apply our discoveries about sparse filtering to real-world data sets to further verify our results. Once again, these experiments aim at validating the connection between the radial structure of the data and the success of sparse filtering. In the first simulation, we extend the result that we proved in Section 5.3 for toy data sets to real data sets; that is, we verify the direct implication: if the structure of the data (with respect to a specific set of labels) is better explained by the cosine metric, then sparse filtering is likely to be a good option for unsupervised learning. In the second simulation, we validate, instead, the reverse implication: if sparse filtering happens to be a good option for unsupervised learning, then the structure of the data (with respect to a specific set of labels) is likely to be better explained by the cosine metric.\nNotice that when dealing with real data sets, it is very challenging to assess the structure of the data. In low dimensions, with few samples and with the simplified assumption that all the data belonging to a given class are generated by a single highly localized cluster (as in the previous simulations), a simple visualization of the data is enough to understand which metric is underlying the data. Thanks to these simplified assumptions, a straight computation of distances among samples belonging to the same class is sufficient to decide which metric best describes the data. However, when we consider real data sets, we have to deal with samples in high dimensions, with a large number of samples, and with the fact that samples belonging to the same class may be generated by different clusters spread throughout the space; in this case, we can not rely on visualization anymore. In order to explore high-dimensional data, we decided to rely on the k-nearest neighbors algorithm (KNN). We implemented two versions of KNN, one selecting k neighbors according to the Euclidean distance and one selecting k neighbors according to the cosine distance5. If the structure of the data (with respect to a set of labels) is better explained by the Euclidean distance, we expect KNN with the Euclidean metric to provide better results; alternatively, if the structure of the data (with respect to a set of labels) is better explained by the cosine distance, we expect KNN with the cosine metric to provide better results.\nBerlin Emotional data set. The Berlin Emotional (EMODB) data set is a well-known audio data set in the emotion recognition community (Burkhardt et al., 2005); it contains recordings of ten German actors expressing seven different types of emotions. We opted for this data set to validate the direct implication between data structure and effectiveness of sparse filtering for the following reasons. (i) Samples in EMODB naturally lend themselves to alternative labellings; in particular, the same data may be used both for speaker recognition (using subject labels) and for emotion recognition (using emotional labels). (ii) The same set of Mel-frequency spectrum (Childers et al., 1977) coefficient (MFCC) features may\n5. The KNN using cosine distance has been implemented relying on the \u201ctrick\u201d that the cosine distance between vectors v,u is the same as the Euclidean metric on the `2-normalized vectors. Therefore, we perform an `2-normalization of each data sample and then we run KNN with Euclidean distance, re-using off-the-shelf KNN code optimized for the Euclidean metric.\nreasonably be used both for speaker recognition and for emotion recognition; indeed, MFCC features were primarily designed for speaker recognition, but they proved to be relevant for emotion recognition as well (Wu et al., 2010; Schuller et al., 2011). Using the same features we explore the data under different labeling.\nWe first explore structure of the data with respect to the two different labeling systems in order to evaluate whether the Euclidean distance or the cosine distance better explains the structure of the data. We run the KNN algorithm with different values of neighbors (k = {2, 3, 5, 7, 10, 15, 20, 25, 50, 75, 100}); for each configuration of KNN, fifty simulations are executed; in each simulation the data set is randomly partitioned into a training data set (900 samples) and a test data set (311 samples); KNN is then trained and tested using one of the two available metrics.\nAfter this analysis, we use both an Euclidean-based unsupervised learning algorithm, Gaussian mixture model (Bishop, 2007), and a cosine-based unsupervised learning algorithm, sparse filtering, to project the data into an L-dimensional space. We opted for the Gaussian mixture of models (GMM) algorithm because it is based on the Euclidean metric and yields better performance than the soft k-means algorithm. After processing the data, we then run a simple linear SVM classifier on the processed data and we analyze how our observations on the structure of the data relate with the actual classification performance. We consider several values of dimensionality (L = {2, 3, ..., 40}); for each configuration, fifty simulations are executed; as before, in each simulation the data set is randomly partitioned into a training data set (900 samples) and in a test data set (311 samples).\nFigure 8(a) shows that the structure of EMODB data with respect to emotional labels is better explained by the Euclidean distance. This result is further confirmed by the classification with the linear SVM module in Figure 8(b). Classification using the GMMprocessed data with low learned dimensionality (L \u2264 15) returns an accuracy that is significantly better than using sparse filtering-processed data (Wilcoxon signed-rank test, p-value P = 5\u00b710\u221285); however, in higher dimensions the classification with sparse filtering-processed data approaches and overtakes the accuracy obtained using GMM-processed data. In general, in low dimensions, the Euclidean structure assumed by GMM explains the data better; in high dimensions, sparse filtering provides good results (most likely thanks to the property of sparsity) but the gap between the accuracy provided by the two representations remains limited. On the other hand, Figure 9(a) shows that the structure of EMODB data with respect to the speaker identity labels is better explained by the cosine distance. This result is further confirmed by the classification with the linear SVM module in Figure 9(b). Classification using the sparse filtering-processed data returns, for all learned dimensionality, an accuracy that is significantly better than GMM-processed data (Wilcoxon signed-rank test, p-value P = 4 \u00b7 10\u2212307). The assumption of the cosine metric allows sparse filtering to explain the data much better, as is evident from the large gap between the accuracy provided by the two representations.\nThese results confirm a connection between the radial structure of the data (with respect to a set of labels) and the usefulness of sparse filtering.\nKaggle Black Box Learning Challenge data set. The Kaggle Black Box Learning Challenge (KBBLC) data set is a visual data set made up of obfuscated images of house numbers; the original images are taken from the well-known Street View House Numbers\nClassification is performed as explained in the text. (a) Exploration of the data via KNN with Euclidean metric (green line) and with cosine metric (blue line); (b) Classification using a linear SVM after processing with a GMM algorithm (green line) and with sparse filtering (blue line). The plot shows the average accuracy and the standard error of SVM (over fifty simulations).\nThe meaning of the subplots is the same as in Figure 8.\n(SVHN) data set (Netzer et al., 2011). Each sample in the KBBLC data set contains a single obfuscated digit and it is accompanied by a label specifying the value of the digit. We opted to validate the reverse implication between data structure and effectiveness of sparse filtering on this data set for the following reasons. (i) Sparse filtering provided state-of-theart performance in the competitive KBBLC contest, thus showing that sparse filtering was a particularly suitable choice for this data set. (ii) The KBBLC data set is available with labels. During the challenge the authors provided obfuscated data without labels; however, after the challenge they revealed the original source of the data6 and they released the code they used for obfuscation7. Thanks to this information, we were able to retrieve a large amount of data and obfuscate it, and thus recreate the original conditions of the challenge. However, differently from the challenge, we retain the labels in order to explore the structure of the data. (iii) During the challenge, samples from the data sets were processed without pre-processing. Since sparse filtering was directly applied to the samples, we can analyze the structure of the samples straightforwardly. This condition is not always true. If we consider other data sets on which sparse filtering provided good results, such as CIFAR-10 or STL-10, sparse filtering was not applied to the original samples but to random patches extracted from the images; in this case, we should not analyze the data structure of the\n6. http://ufldl.stanford.edu/housenumbers/ 7. https://www.kaggle.com/c/challenges-in-representation-learning-the-black-box-learning-challenge/\nforums/t/5167/the-data\nThe KNN with Euclidean metric (green line) and with cosine metric (blue line) has been used to explore the structure of the data. The plot shows the average accuracy and the standard error of KNN (over five simulations).\noriginal samples, but the data structure of the patches. However, patches are not labeled, which hinders our ability to carry out an analysis of the data structure.\nIn exploring the structure of the data (with respect to the digit labels), we aim at evaluating whether the Euclidean distance or the cosine distance better explains the structure of the data. We run the KNN with the same settings as in the previous experiment. In each simulation a random subset of 10000 samples from the data set was selected and then partitioned into a training data set (9000 samples) and a test data set (1000 samples). KNN was then trained and tested using one of the two available metrics.\nFigure 10 confirms our intuition. For all the different values of k we considered, the cosine distance proved to be a better metric to explain the structure of the data in the Kaggle Black Box Learning Challenge. This provides an explanation why sparse filtering proved so useful with the KBBLC data, when compared to other standard unsupervised learning algorithms, especially those based on the Euclidean metric. This result agrees with the fact that the Euclidean metric is not a suitable metric for measuring distances among samples of digits represented in the pixel space; other distances less sensitive to irrelevant transformations, such as tangent distance (Simard et al., 1998), are known to be better choices."}, {"heading": "6. Discussion", "text": "The theoretical analysis and the empirical verification we performed allow us to conclude that our thesis is correct: sparse filtering satisfies the informativeness principle through the maximization of the proxy of sparsity, and it satisfies the infomax principle through the constraint of preservation of the radial structure of the data. In particular, sparse filtering\nis able to implicitly preserve the mutual information between the original representations x(i) and the learned representations z(i) through the preservation of the structure defined by cosine neighborhoodness.\nIn our experiments, we showed that sparse filtering operates as an unsupervised soft clustering algorithm based on the cosine metric. This allowed us to contrast the results of sparse filtering with other standard algorithms for clustering based on the Euclidean metric (for instance, soft k-means or Gaussian mixture models). Sparse filtering, thus, does not provide a better processing of the data in absolute terms, but instead provides an alternative interpretation of the data based on a different metric.\nConsequently, we have been able to highlight the conditions under which sparse filtering may be expected to perform significantly better than the standard Euclidean-based alternatives. We showed that whenever the structure of the data (with respect to a set of labels) can be explained through cosine distances, sparse filtering is able to provide cutting-edge performance. Indeed, sparse filtering may be seen as an algorithm approximately transforming cosine distances in the original space into Euclidean distances in the representation space; if cosine distances are meaningful (with respect to a set of labels), then sparse filtering will provide a representation that is especially useful for the large set of standard classifiers that rely on the Euclidean metric in their learning process.\nThe ideal scenario in which to employ sparse filtering is one in which relevant information is brought by the radial structure of the data. It is normally assumed that the data points x(i) are best explained as samples from a multivariate random variable X = (X1, X2, . . . , XO), where each random variable Xj describes a component x (i) j . However, given the data points x(i), it is possible to assume that the generating process is better described by a multivariate random variable X \u2032 = ( X \u2032 1, X \u2032 2, . . . , X \u2032 O\u22121 ) , where each random variable describes an angular coordinate \u03b8j of x (i) j . In such a case, sparse filtering is a very reasonable choice for unsupervised representation learning.\nIn our experiments, we were aware a priori of the metric (either Euclidean or cosine) underlying a synthetic data set. However, in a real-world setting, such knowledge may not be available. Sparse filtering, though, is a scalable and efficient algorithm and it may be possible to test it on the data even without knowing a priori the metric underlying it. The usefulness of polar coordinates in several scientific fields and physical applications may suggest that interpreting data according to cosine distance could be a sensible choice. More rigorously, it is possible to run a simple exploratory analysis of the data (using, for instance KNN) to assess which metric seems better suited for the data set.\nAdditionally, we proved that, even if we believe that the structure of the data is better explained in terms of Euclidean distances in Cartesian coordinates, in high dimensions, sparse filtering may still provide good results. This is justified by the fact that, under the assumptions we made, as we increase the number of dimensions, the probability that unrelated points with high Euclidean distance will have the same angular coordinates \u03b8i will necessarily decrease (Section 4.11)."}, {"heading": "7. Concluding Remarks", "text": "In this paper, we have explained why sparse filtering works (by proving its property of preservation of cosine neighborhoodness) and when it should be expected to provide useful representations (by considering the data structure of the samples).\nAt the foundation of our analysis lies the understanding that sparse filtering must preserve some information carried by the pdf p(X) about the true pdf p (X\u2217). Despite sparse filtering ignoring the problem of explicitly modeling the true pdf p (X\u2217), the algorithm is hard-coded with an implicit constraint that guarantees the preservation of some data structure. This is clearly a specific conclusion about the particular algorithm of sparse filtering, but we can expect that this principle will be applicable to the whole class of feature distribution learning algorithms. We might expect that any feature distribution learning algorithm, in order to be successful, must take into account, through constraints or priors, the problem of preserving the mutual information between the original representations x(i) and the learned representations z(i). Being aware of this requirement could be of help in designing and analyzing new feature distribution learning algorithms; it may, for instance, help us to avoid solutions (such as sparse filtering with a sigmoid or ReLU non-linearity) that, being unable to preserve any structure of the data, are bound to produce unsatisfactory representations.\nOur theoretical analysis and simulations were not designed to show that sparse filtering is able to provide state-of-the-art performance against other algorithms, but, instead, to show how the implicit assumptions and constraints of sparse filtering make it better suited for certain scenarios instead of others. In particular, we showed that the success conditions of sparse filtering are tied to the structure underlying the data. Consistently with the nofree lunch theorem (Wolpert and Macready, 1997), we reached the conclusion that sparse filtering is not a better algorithm than other Euclidean-based clustering algorithms, but that there is a specific set of problems (in which the data structure is explained by the cosine metric) where the performance of sparse filtering is excellent, balanced by a set of problems (in which the data structure is explained by the Euclidean metric) where its performance is less outstanding. This led us to interpret the representation of sparse filtering as a \u201cview\u201d of the data according to the cosine metric. Whenever the underlying structure of the data is unknown, it may also be possible to combine this \u201ccosine view\u201d of the data provided with a more standard \u201cEuclidean view\u201d of the data. Combining these two different views could provide representations with more discriminative power.\nA promising avenue in our ongoing research is the extension of sparse filtering to semisupervised learning. Indeed, the paradigm of feature distribution learning seems perfectly suited for the scenario in which we are provided with few labeled samples and many unlabeled samples: following the approach of feature distribution learning we may exploit the information carried by the labeled samples to better shape the feature distribution p(Z), without addressing the problem of estimating the true pdf p (X\u2217); at the same time, the constraint of sparsity would help us to not overfit, and the constraint of structure preservation would help us to preserve the information conveyed by p(X). In particular, assuming\nsome regularity in the original representation space, we hypothesize that we could use the information in the labeled samples to address the problem of covariate shift (Sugiyama and Kawanabe, 2012) in a semi-supervised learning scenario."}, {"heading": "Appendix A. Additional Proofs", "text": "We here provide the proofs of propositions and lemmas in the main text.\nA.1 Preservation of Collinearity under Linear Transformation\nLemma. Let us consider v,u \u2208 RO, two generic collinear vectors, and let f : RO \u2192 RL be a linear transformation encoded by the matrix W. Then Wv,Wu \u2208 RL are also collinear.\nProof. Let us consider the two collinear vectors v = [ v1 v2 v3 ... vo ]T and u = [ u1 u2 u3 ... uo\n]T in RO. Since the two vectors are collinear, they share the same angular coordinates \u03c6i to the origin. We can therefore rewrite the vectors using spherical coordinates:\nv =  \u03c1v cos (\u03c61) \u03c1v sin (\u03c61) cos (\u03c62) \u03c1v sin (\u03c61) sin (\u03c62) cos (\u03c63)\n... \u03c1v sin (\u03c61) ... sin (\u03c6O\u22122) cos (\u03c6O\u22121)\n ; u = \n\u03c1u cos (\u03c61) \u03c1u sin (\u03c61) cos (\u03c62)\n\u03c1u sin (\u03c61) sin (\u03c62) cos (\u03c63) ...\n\u03c1u sin (\u03c61) ... sin (\u03c6O\u22122) cos (\u03c6O\u22121)\n .\nSince both the radial coordinate \u03c1v and \u03c1u are real numbers, we can definitely find \u03b1 \u2208 R such that \u03b1 = \u03c1u\u03c1v . Now, substituting \u03c1u with \u03b1\u03c1v we can rewrite:\nu = \u03b1  \u03c1v cos (\u03c61) \u03c1v sin (\u03c61) cos (\u03c62) \u03c1v sin (\u03c61) sin (\u03c62) cos (\u03c63)\n... \u03c1v sin (\u03c61) ... sin (\u03c6O\u22122) cos (\u03c6O\u22121)\n .\nLet us now consider the linear transformation described by W \u2208 RL\u00d7O:\nW =  W11 W12 \u00b7 \u00b7 \u00b7 W1O W21 W22 \u00b7 \u00b7 \u00b7 W2O \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 WL1 WL2 \u00b7 \u00b7 \u00b7 WLO  . The projection Wv can be computed as:\nWv =  W11\u03c1v cos (\u03c61) +W12\u03c1v sin (\u03c61) cos (\u03c62) + \u00b7 \u00b7 \u00b7+W1O\u03c1v sin (\u03c61) ... sin (\u03c6O\u22122) cos (\u03c6O\u22121) W21\u03c1v cos (\u03c61) +W22\u03c1v sin (\u03c61) cos (\u03c62) + \u00b7 \u00b7 \u00b7+W2O\u03c1v sin (\u03c61) ... sin (\u03c6O\u22122) cos (\u03c6O\u22121)\nWL1\u03c1v cos (\u03c61) +WL2\u03c1v sin (\u03c61) cos (\u03c62) + \u00b7 \u00b7 \u00b7+WLO\u03c1v sin (\u03c61) ... sin (\u03c6O\u22122) cos (\u03c6O\u22121)\n .\nLet us call the new components of Wv as ai:\nWv = [ a1 a2 a3 ... aL ]T .\nIf we want to find the new radial coordinates \u03bei of Wv we could resort to the standard formula:\n\u03bei = arccos ai\u221a\na2L + a 2 L\u22121 + \u00b7 \u00b7 \u00b7+ a2i\n.\nThe projection Wu can be computed as:\nWu = W (\u03b1v),\nand by linearity:\nW (\u03b1v) = \u03b1 (Wv) = \u03b1 [ a1 a2 a3 ... aL ]T .\nIf we want to find the new radial coordinates \u03b6i of Wu we could resort to the same formula used above:\n\u03b6i = arccos \u03b1ai\u221a\n(\u03b1aL) 2 + (\u03b1aL\u22121) 2 + \u00b7 \u00b7 \u00b7+ (\u03b1ai)2\n= arccos ai\u221a(\na2L + a 2 L\u22121 + \u00b7 \u00b7 \u00b7+ a2i ) = \u03bei.\nTherefore the radial coordinates \u03bei of Wv and the radial coordinates \u03b6i of Wu are the same. Hence Wv and Wu are collinear.\nA.2 Preservation of Collinearity under Absolute-Value\nLemma. Let us consider v,u \u2208 RL, two generic collinear vectors, and let f : RL \u2192 RL be the element-wise absolute-value function. Then f (v) , f(u) \u2208 RL are also collinear.\nProof. Let us consider the two collinear vectors v = [ v1 v2 v3 ... vL ]T and u = [ u1 u2 u3 ... uL\n]T in RL. Since the two vectors are collinear, they share the same angular coordinates \u03c6i to the origin. We can therefore rewrite the vectors using spherical coordinates:\nv =  \u03c1v cos (\u03c61) \u03c1v sin (\u03c61) cos (\u03c62) \u03c1v sin (\u03c61) sin (\u03c62) cos (\u03c63)\n... \u03c1v sin (\u03c61) ... sin (\u03c6L\u22122) cos (\u03c6L\u22121)\n ; u = \n\u03c1u cos (\u03c61) \u03c1u sin (\u03c61) cos (\u03c62)\n\u03c1u sin (\u03c61) sin (\u03c62) cos (\u03c63) ...\n\u03c1u sin (\u03c61) ... sin (\u03c6L\u22122) cos (\u03c6L\u22121)\n .\nApplying the absolute-value to v and decomposing, we have:\nf (v) = |\u03c1v|  |cos (\u03c61)| |sin (\u03c61)| |cos (\u03c62)| |sin (\u03c61)| |sin (\u03c62)| |cos (\u03c63)|\n... |sin (\u03c61)| ... |sin (\u03c6L\u22122)| |cos (\u03c6L\u22121)|\n .\nAnalogously, computing the application of absolute-value to u we have:\nf (u) = |\u03c1u|  |cos (\u03c61)| |sin (\u03c61)| |cos (\u03c62)| |sin (\u03c61)| |sin (\u03c62)| |cos (\u03c63)|\n... |sin (\u03c61)| ... |sin (\u03c6L\u22122)| |cos (\u03c6L\u22121)|\n .\nNow, we can solve the absolute values of the trigonometric functions according to the following formulas:\n|cos (\u03c6)| =  cos (\u03c6) 0 \u2264 \u03c6 < \u03c02 cos (\u03c0 \u2212 \u03c6) \u03c02 \u2264 \u03c6 < \u03c0 cos (\u03c6\u2212 \u03c0) \u03c0 \u2264 \u03c6 < 34\u03c0 cos (\u03c0 \u2212 \u03c6) 34\u03c0 \u2264 \u03c6 < 2\u03c0 ; |sin (\u03c6)| =  sin (\u03c6) 0 \u2264 \u03c6 < \u03c02 sin (\u03c0 \u2212 \u03c6) \u03c02 \u2264 \u03c6 < \u03c0 sin (\u03c6\u2212 \u03c0) \u03c0 \u2264 \u03c6 < 34\u03c0 sin (\u03c0 \u2212 \u03c6) 34\u03c0 \u2264 \u03c6 < 2\u03c0 .\nWe can therefore remove the absolute value by substituting each angle \u03c6i with the angle \u03c6\u2217i given by the identities above:\nf (v) = |\u03c1v|  cos (\u03c6\u22171) sin (\u03c6\u22171) cos (\u03c6 \u2217 2) sin (\u03c6\u22171) sin (\u03c6 \u2217 2) cos (\u03c6 \u2217 3)\n... sin (\u03c6\u22171) ... sin ( \u03c6\u2217L\u22122 ) cos ( \u03c6\u2217L\u22121 )\n ;\nf (u) = |\u03c1u|  cos (\u03c6\u22171) sin (\u03c6\u22171) cos (\u03c6 \u2217 2) sin (\u03c6\u22171) sin (\u03c6 \u2217 2) cos (\u03c6 \u2217 3)\n... sin (\u03c6\u22171) ... sin ( \u03c6\u2217L\u22122 ) cos ( \u03c6\u2217L\u22121 )\n .\nBut then, again, f (v) and f (u) have the same angular coordinates. Therefore f (v) and f (u) are collinear.\nA.3 Preservation of Collinearity under Normalization Along the Features\nLemma. Let us consider v,u \u2208 RL, two positive8 collinear vectors, and let f : RL \u2192 RL be the `2-normalization along the features. Then f (v) , f(u) \u2208 RL are also collinear.\nProof. Let us consider the two positive collinear vectors v = [ v1 v2 v3 ... vL ]T and u = [ u1 u2 u3 ... uL\n]T in RL, where vi \u2265 0 and ui \u2265 0. Since the two vectors are collinear, they share the same angular coordinates \u03c6i to the origin:\n\u03c6i = arccos vi\u221a\nv2L + v 2 L\u22121 + . . .+ v 2 i\n= arccos ui\u221a\nu2L + u 2 L\u22121 + . . .+ u 2 i\n.\nNormalizing along the features means dividing each component i of a vector by a constant ki equal to the `2-norm of the component i across all the available vectors. The\n`2-normalized vectors are then f(v) = v\u0303 = [\nv1 k1 v2 k2 v3 k3 ... vLkL ]T and f(u) = u\u0303 =[\nu1 k1 u2 k2 u3 k3 ... uLkL\n]T . Now, given that each component is divided by the same con-\nstant ki, the ratios defining the angular coordinate \u03c6i are still equivalent:\narccos vi ki\u221a(\nvL kL )2 + ( vL\u22121 kL\u22121 )2 + . . .+ ( vi ki )2 = = arccos ui ki\u221a(\nuL kL )2 + ( uL\u22121 kL\u22121 )2 + . . .+ ( ui ki )2 = \u03c6\u0303i. The angular coordinates of f(v) and f(u) are still the same. Therefore f(v) and f(u) are collinear.\nA.4 Preservation of Collinearity under Normalization Along the Samples\nLemma. Let us consider v,u \u2208 RL, two positive9 collinear vectors, and let f : RL \u2192 RL be the `2-normalization along the samples. Then f (v) , f(u) \u2208 RL are also collinear.\nProof. Let us consider the two positive collinear vectors v = [ v1 v2 v3 ... vL ]T and u = [ u1 u2 u3 ... uL\n]T in RL, where vi \u2265 0 and ui \u2265 0. Since the two vectors are collinear, they share the same angular coordinates \u03c6i to the origin:\n\u03c6i = arccos vi\u221a\nv2L + v 2 L\u22121 + . . .+ v 2 i\n= arccos ui\u221a\nu2L + u 2 L\u22121 + . . .+ u 2 i\n.\n8. Notice that we can safely make the assumption of positivity in sparse filtering since v and u are the output of an absolute-value function. 9. Notice that we can safely make the assumption of positivity in sparse filtering since v and u are the output of the normalization along the feature which preserves the positivity.\nNormalizing along the samples means dividing each vector by a constant given by the `2- norm of the vector itself. The `2-normalized vectors can then be computed as f(v) =\nv\u0302 = [\nv1 `2(v) v2 `2(v) v3 `2(v) ... vL`2(v)\n]T and f(u) = u\u0302 = [ u1 `2(u) u2 `2(u) u3 `2(u) ... uL`2(u) ]T .\nNow, dividing each component of a vector by the same constant does not affect the value of the angular coordinates \u03c6i :\narccos vi `2(v)\u221a(\nvL `2(v) )2 + ( vL\u22121 `2(v) )2 + . . .+ ( vi `2(v) )2 = = arccos vi `2(v)\u221a\n1 (`2(v)) 2\n( v2L + v 2 L\u22121 + . . .+ v 2 i ) = \u03c6i. Analogously, this holds for f(u), too. The angular coordinates of f(v) and f(u) are unchanged. Therefore f(v) and f(u) are collinear.\nA.5 Equivalence of Minimizing `1-norm and Maximizing R(pj)\nLemma. Let x(i) \u2208 RO be an original representation and z(i) \u2208 RL be a learned representation computed through sparse filtering, that is z(i) = fA1:A4 ( x(i) ) . Let Rp(j) : RO \u2192 R\u22650 the representation-cone function mapping points in the original representation space RO to their (Euclidean) distance from the pole p(j). Then the optimal solution for the minimization of \u2211N i=1 `1 ( z(i) ) , under the constraint `2 ( z(i) )\n= 1, is the same as the optimal solution for \u2211N\ni=1 \u2211L j=1R p(j) ( x(i) ) , under the same constraint.\nProof. Recall that, by definition, the `1-norm of z(i) is:\nmin N\u2211 i=1 `1 ( z(i) ) = min N\u2211 i=1 L\u2211 j=1 ( z (i) j ) .\nWe already know that, given the constraint `2 ( z(i) )\n= 1, the optimal solution for this optimization problem is given by the set of poles p(j) in RL (Ngiam et al., 2011). Now, let us consider the minimization of the representation-cone function and let us make explicit its argument:\nmin N\u2211 i=1 L\u2211 j=1 Rp (j) ( x(i) ) = min N\u2211 i=1 L\u2211 j=1 \u2225\u2225\u2225p(j) \u2212 fA1:A4 (x(i))\u2225\u2225\u2225 L2\n= min N\u2211 i=1 L\u2211 j=1 \u2225\u2225\u2225p(j) \u2212 z(i)\u2225\u2225\u2225 L2 .\nGiven the constraint `2 ( z(i) )\n= 1, the distances between a representation z(i) and the poles p(j) are minimized when the representations z(i) are identified with the poles. If a representation z(i) lies anywhere else on the surface of the hyper-sphere defined by the constraint,\nthen, by the triangle inequality, the distance between z(i) and the poles p(j) is greater. Therefore, both the optimization problems have an optimal solution in having the representations z(i) mapped to the poles p(j).\nA.6 Limit of the Ratio of Gamma Functions\nLemma. Let us consider x \u2208 R. Then limx\u2192\u221e \u0393(x)\u0393(x+ 12) = 1\u221a x .\nProof. This statement follows from an application of the Stirling\u2019s formula to the gamma function. Given the following limit proven in Equation 2.3.19 in Freeden and Gutting (2013):\nlim x\u2192\u221e\n\u0393(x+ a)\nxa\u0393 (x) = 1,\nlet us substitute a with 12 :\nlim x\u2192\u221e\n\u0393(x+ 12)\nx 1 2 \u0393 (x)\n= 1.\nReordering:\nlim x\u2192\u221e \u0393(x+ 12)\u221a x\u0393 (x) = 1\nlim x\u2192\u221e\n\u0393 (x) \u0393 ( x+ 12 ) = 1\u221a x ,\nwhich is the formula we wanted to prove."}], "references": [{"title": "Representation learning: a review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Pattern recognition and machine", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2007\\E", "shortCiteRegEx": "Bishop.", "year": 2007}, {"title": "Sparse coding in early visual representation: from specific properties to general principles", "author": ["Neil D.B. Bruce", "Shafin Rahman", "Diana Carrier"], "venue": null, "citeRegEx": "Bruce et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bruce et al\\.", "year": 2016}, {"title": "A database of German emotional speech", "author": ["Felix Burkhardt", "Astrid Paeschke", "Miriam Rolfes", "Walter F. Sendlmeier", "Benjamin Weiss"], "venue": "In Interspeech,", "citeRegEx": "Burkhardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Burkhardt et al\\.", "year": 2005}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["Emmanuel J. Candes", "Justin K. Romberg", "Terence Tao"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Normalization as a canonical neural computation", "author": ["Matteo Carandini", "David J. Heeger"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "Carandini and Heeger.,? \\Q2012\\E", "shortCiteRegEx": "Carandini and Heeger.", "year": 2012}, {"title": "The cepstrum: a guide to processing", "author": ["Donald G. Childers", "David P. Skinner", "Robert C. Kemerait"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Childers et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Childers et al\\.", "year": 1977}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Adam Coates", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Coates and Ng.,? \\Q2011\\E", "shortCiteRegEx": "Coates and Ng.", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Andrew Y. Ng", "Honglak Lee"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "An elementary proof of a theorem of Johnson and Lindenstrauss", "author": ["Sanjoy Dasgupta", "Anupam Gupta"], "venue": "Random structures and algorithms,", "citeRegEx": "Dasgupta and Gupta.,? \\Q2003\\E", "shortCiteRegEx": "Dasgupta and Gupta.", "year": 2003}, {"title": "Recklessly approximate sparse coding", "author": ["Misha Denil", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1208.0959,", "citeRegEx": "Denil and Freitas.,? \\Q2012\\E", "shortCiteRegEx": "Denil and Freitas.", "year": 2012}, {"title": "Vehicle type classification using unsupervised convolutional neural network", "author": ["Zhen Dong", "Mingtao Pei", "Yang He", "Ting Liu", "Yanmei Dong", "Yunde Jia"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Vehicle type classification using a semisupervised convolutional neural network", "author": ["Zhen Dong", "Yuwei Wu", "Mingtao Pei", "Yunde Jia"], "venue": "Intelligent Transportation Systems, IEEE Transactions on,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Sparse coding in the primate cortex", "author": ["Peter F\u00f6ldi\u00e1k", "Malcom P. Young"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "F\u00f6ldi\u00e1k and Young.,? \\Q1995\\E", "shortCiteRegEx": "F\u00f6ldi\u00e1k and Young.", "year": 1995}, {"title": "Special functions of mathematical (geo-) physics", "author": ["Willi Freeden", "Martin Gutting"], "venue": "Springer Science & Business Media,", "citeRegEx": "Freeden and Gutting.,? \\Q2013\\E", "shortCiteRegEx": "Freeden and Gutting.", "year": 2013}, {"title": "Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis", "author": ["Surya Ganguli", "Haim Sompolinsky"], "venue": "Annual review of neuroscience,", "citeRegEx": "Ganguli and Sompolinsky.,? \\Q2012\\E", "shortCiteRegEx": "Ganguli and Sompolinsky.", "year": 2012}, {"title": "Unsupervised and supervised visual codes with restricted Boltzmann machines", "author": ["Hanlin Goh", "Nicolas Thome", "Matthieu Cord", "Joo-Hwee Lim"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Goh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2012}, {"title": "Learning deep hierarchical visual feature coding", "author": ["Hanlin Goh", "Nicolas Thome", "Matthieu Cord", "Joo-Hwee Lim"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on,", "citeRegEx": "Goh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2014}, {"title": "Challenges in representation learning: a report on three machine learning contests", "author": ["Yoshua Bengio"], "venue": "In Neural information processing,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Learning quality-aware filters for no-reference image quality assessment", "author": ["Zhongyi Gu", "Lin Zhang", "Xiaoxu Liu", "Hongyu Li", "Jianwei Lu"], "venue": "In Multimedia and Expo (ICME),", "citeRegEx": "Gu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2014}, {"title": "Deep learning human actions from video via sparse filtering and locally competitive algorithms", "author": ["William Edward Hahn", "Stephanie Lewkowitz", "Daniel C. Lacombe Jr.", "Elan Barenholtz"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Hahn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hahn et al\\.", "year": 2015}, {"title": "Hierarchical approach to detect common mistakes of beginner flute players", "author": ["Yoonchang Han", "Kyogu Lee"], "venue": "In ISMIR,", "citeRegEx": "Han and Lee.,? \\Q2014\\E", "shortCiteRegEx": "Han and Lee.", "year": 2014}, {"title": "Detecting fingering of overblown flute sound using sparse feature learning", "author": ["Yoonchang Han", "Kyogu Lee"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing,", "citeRegEx": "Han and Lee.,? \\Q2016\\E", "shortCiteRegEx": "Han and Lee.", "year": 2016}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Comparing measures of sparsity", "author": ["Niall Hurley", "Scott Rickard"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Hurley and Rickard.,? \\Q2009\\E", "shortCiteRegEx": "Hurley and Rickard.", "year": 2009}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["William B Johnson", "Joram Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Central auditory neurons have composite receptive fields", "author": ["Andrei S. Kozlov", "Timothy Q. Gentner"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Kozlov and Gentner.,? \\Q2016\\E", "shortCiteRegEx": "Kozlov and Gentner.", "year": 2016}, {"title": "Compute less to get more: using ORC to improve sparse filtering", "author": ["Johannes Lederer", "Sergio Guadarrama"], "venue": "arXiv preprint arXiv:1409.4689,", "citeRegEx": "Lederer and Guadarrama.,? \\Q2014\\E", "shortCiteRegEx": "Lederer and Guadarrama.", "year": 2014}, {"title": "An intelligent fault diagnosis method using unsupervised feature learning towards mechanical big data", "author": ["Yaguo Lei", "Feng Jia", "Jing Lin", "Saibo Xing", "Steven Ding"], "venue": "Industrial Electronics, IEEE Transactions on,", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "An application of the principle of maximum information preservation to linear systems", "author": ["Ralph Linsker"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Linsker.,? \\Q1989\\E", "shortCiteRegEx": "Linsker.", "year": 1989}, {"title": "Information theory, inference, and learning algorithms, volume 7", "author": ["David J.C. MacKay"], "venue": null, "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F. Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y. Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["Bruno A. Olshausen", "David J. Field"], "venue": "Vision research,", "citeRegEx": "Olshausen and Field.,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field.", "year": 1997}, {"title": "Smartphone based visible iris recognition using deep sparse filtering", "author": ["Kiran B. Raja", "R. Raghavendra", "Vinay Krishna Vemuri", "Christoph Busch"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Raja et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Raja et al\\.", "year": 2015}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Marc\u2019Aurelio Ranzato", "Cristopher Poultney", "Sumit Chopra", "Yann LeCun"], "venue": "In Proceedings of Neural Information Processing Systems,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Sparse feature learning for deep belief networks", "author": ["Marc\u2019Aurelio Ranzato", "Y-Lan Boureau", "Yann LeCun"], "venue": "In Proceedings of Neural Information Processing Systems,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "The relative advantages of sparse versus distributed encoding for associative neuronal networks in the brain. Network: computation", "author": ["Edmund T. Rolls", "Alessandro Treves"], "venue": "in neural systems,", "citeRegEx": "Rolls and Treves.,? \\Q1990\\E", "shortCiteRegEx": "Rolls and Treves.", "year": 1990}, {"title": "A deep learning approach with an ensemble-based neural network classifier for black box ICML 2013 contest", "author": ["Lukasz Romaszko"], "venue": "In Workshop on Challenges in Representation Learning,", "citeRegEx": "Romaszko.,? \\Q2013\\E", "shortCiteRegEx": "Romaszko.", "year": 2013}, {"title": "No more meta-parameter tuning in unsupervised sparse feature learning", "author": ["Adriana Romero", "Petia Radeva", "Carlo Gatta"], "venue": "arXiv preprint arXiv:1402.5766,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Temporal responses of chemically diverse sensor arrays for machine olfaction using artificial intelligence", "author": ["Shaun K. Ryman", "Neil D.B. Bruce", "Michael S. Freund"], "venue": "Sensors and Actuators B: Chemical,", "citeRegEx": "Ryman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ryman et al\\.", "year": 2016}, {"title": "Recognising realistic emotions and affect in speech: state of the art and lessons learnt from the first challenge", "author": ["Bj\u00f6rn Schuller", "Anton Batliner", "Stefan Steidl", "Dino Seppi"], "venue": "Speech Communication,", "citeRegEx": "Schuller et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Schuller et al\\.", "year": 2011}, {"title": "Edge directed single image super resolution through the learning based gradient regression estimation", "author": ["Dandan Si", "Yuanyuan Hu", "Zongliang Gan", "Ziguan Cui", "Feng Liu"], "venue": "In Image and Graphics,", "citeRegEx": "Si et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Si et al\\.", "year": 2015}, {"title": "Transformation invariance in pattern recognition - tangent distance and tangent propagation", "author": ["Patrice Y. Simard", "Yann A. LeCun", "John S. Denker", "Bernard Victorri"], "venue": "In Neural networks: tricks of the trade,", "citeRegEx": "Simard et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Simard et al\\.", "year": 1998}, {"title": "Machine learning in non-stationary environments: introduction to covariate shift adaptation", "author": ["Masashi Sugiyama", "Motoaki Kawanabe"], "venue": null, "citeRegEx": "Sugiyama and Kawanabe.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama and Kawanabe.", "year": 2012}, {"title": "Accumulating pyramid spatial-spectral collaborative coding divergence for hyperspectral anomaly detection", "author": ["Hao Sun", "Huanxin Zou", "Shilin Zhou"], "venue": "In 2015 ISPRS International Conference on Computer Vision in Remote Sensing, pages 99010J\u201399010J. International Society for Optics and Photonics,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "The information bottleneck method", "author": ["Naftali Tishby", "Fernando C. Pereira", "William Bialek"], "venue": "arXiv preprint physics/0004057,", "citeRegEx": "Tishby et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tishby et al\\.", "year": 2000}, {"title": "Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "No free lunch theorems for optimization", "author": ["David H. Wolpert", "William G. Macready"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "Wolpert and Macready.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}, {"title": "Acoustic feature analysis in speech emotion primitives estimation", "author": ["Dongrui Wu", "Thomas D. Parsons", "Shrikanth S. Narayanan"], "venue": "In INTERSPEECH,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Driving posture recognition by convolutional neural networks", "author": ["Chao Yan", "Frans Coenen", "Bailing Zhang"], "venue": "IET Computer Vision,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}, {"title": "Single-layer unsupervised feature learning with l2 regularized sparse filtering", "author": ["Zhao Yang", "Lianwen Jin", "Dapeng Tao", "Shuye Zhang", "Xin Zhang"], "venue": "In Signal and Information Processing (ChinaSIP),", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Performance evaluation of typical unsupervised feature learning algorithms for visual object recognition", "author": ["Shaohua Zhang", "Hua Yang", "Zhouping Yin"], "venue": "In Intelligent Control and Automation (WCICA),", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "A survey of sparse representation: algorithms and applications", "author": ["Zheng Zhang", "Yong Xu", "Jian Yang", "Xuelong Li", "David Zhang"], "venue": "Access, IEEE,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Coates et al. (2011) clearly showed that very simple unsupervised learning algorithms (such as k-means clustering), when properly tuned, can generate representations of the data that allow even basic supervised classifiers, such as support vector machines, to achieve state-of-the-art performances.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013).", "startOffset": 89, "endOffset": 110}, {"referenceID": 0, "context": "From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al.", "startOffset": 276, "endOffset": 297}, {"referenceID": 0, "context": ", 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 17, "context": ", 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 15, "context": ", 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al.", "startOffset": 65, "endOffset": 96}, {"referenceID": 4, "context": ", 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006).", "startOffset": 186, "endOffset": 207}, {"referenceID": 37, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 144, "endOffset": 168}, {"referenceID": 15, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 260, "endOffset": 291}, {"referenceID": 33, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 311, "endOffset": 338}, {"referenceID": 13, "context": "From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014).", "startOffset": 371, "endOffset": 396}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al.", "startOffset": 90, "endOffset": 1760}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al.", "startOffset": 90, "endOffset": 1782}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al. (2006). Several different algorithms have been developed or have been adapted to learn sparse representations; for a recent survey of these algorithms we refer the reader to Zhang et al.", "startOffset": 90, "endOffset": 1808}, {"referenceID": 0, "context": "One common assumption hard-wired in several unsupervised learning algorithms is sparsity (Bengio et al., 2013). Sparse representation learning aims at learning a mapping that produces new representations where few of the components of each new representation are active while all of the others are reduced to zero. The adoption of sparsity relies both on theoretical justifications and on biological analogies. From a formal perspective, sparse representations provide a good trade-off between robustness, maximized by one-hot representations, and representative power, maximized by dense representations (Bengio, 2009); they may be insensitive to irrelevant perturbations of the inputs (Bengio et al., 2013); they may deal with explaining-away phenomena (Bengio et al., 2013); they may improve pattern discrimination (Goh et al., 2014); they may help tackling the curse of high dimensionality (Ganguli and Sompolinsky, 2012); and, sparsity allows high levels of compression within the compressed sensing framework (Candes et al., 2006). From a physical and biological perspective, sparse representations are energy efficient; they are parsimonious and can enhance storage capacity (Rolls and Treves, 1990); several physical phenomena may be encoded with a sparse representation in a proper domain (Ganguli and Sompolinsky, 2012); the visual cortex (Olshausen and Field, 1997) and the brain cortex in general (F\u00f6ldi\u00e1k and Young, 1995) seem to rely on sparse codes; and, sparse codes make it possible to control the variability and the overlap of input stimuli to the brain (Babadi and Sompolinsky, 2014). Beyond these rationales, the usefulness of sparsity has been confirmed through numerous practical implementations; see, for instance, Bengio et al. (2013), Coates and Ng (2011) and Ranzato et al. (2006). Several different algorithms have been developed or have been adapted to learn sparse representations; for a recent survey of these algorithms we refer the reader to Zhang et al. (2015).", "startOffset": 90, "endOffset": 1995}, {"referenceID": 5, "context": "More interestingly, Kozlov and Gentner (2016) used sparse filtering to model the receptive fields of high-level auditory neurons in a songbird; relying on the idea that sparseness and normalization are canonical neural processing operations (Carandini and Heeger, 2012), their results show that sparse filtering can reproduce the receptive fields of the European starling", "startOffset": 241, "endOffset": 269}, {"referenceID": 26, "context": ", 2013) and Romaszko (2013) used sparse filtering in their machine learning systems while taking part into the Kaggle Black Box Learning Challenge, achieving respectively the first and the sixth best positions.", "startOffset": 12, "endOffset": 28}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al.", "startOffset": 66, "endOffset": 96}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al.", "startOffset": 66, "endOffset": 246}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al.", "startOffset": 66, "endOffset": 365}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al. (2014) modified the original sparse filtering algorithm by introducing a penalty on the weight matrix.", "startOffset": 66, "endOffset": 469}, {"referenceID": 18, "context": "Indeed, more theoretical and experimental studies were published: Lederer and Guadarrama (2014) proposed an improved stopping criterion for sparse filtering when processing images relying on results from random matrix theory; Zhang et al. (2014) published an experimental comparison of six sparse coding algorithms, including sparse filtering; Romero et al. (2014) introduced a new algorithm inspired by sparse filtering with no meta-parameters; and Yang et al. (2014) modified the original sparse filtering algorithm by introducing a penalty on the weight matrix. These studies highlight a clear interest in the refinement and improvement of the original algorithm. At the same time, on the practical side, the simplicity of the sparse filtering algorithm favored its adoption in many real-world applications: Raja et al. (2015) deployed it in a system for iris recognition on smartphones; Dong et al.", "startOffset": 66, "endOffset": 830}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al.", "startOffset": 68, "endOffset": 110}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al.", "startOffset": 68, "endOffset": 182}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al.", "startOffset": 68, "endOffset": 263}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al.", "startOffset": 68, "endOffset": 343}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al.", "startOffset": 68, "endOffset": 469}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al.", "startOffset": 68, "endOffset": 566}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al.", "startOffset": 68, "endOffset": 589}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al.", "startOffset": 68, "endOffset": 683}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al. (2016) implemented sparse filtering in their system for hyper-spectral anomaly detection; and, Ryman et al.", "startOffset": 68, "endOffset": 797}, {"referenceID": 9, "context": "(2015) deployed it in a system for iris recognition on smartphones; Dong et al. (2014) and Dong et al. (2015) adopted it in a system for vehicle type recognition; Hahn et al. (2015) used sparse filtering for detecting human actions from videos; Yan et al. (2015) implemented it in their system for detecting driving posture; Lei et al. (2015) integrated it in a system for intelligent fault diagnosis from big data collected from mechanical apparatus; Gu et al. (2014) implemented sparse filtering in a system for the assessment of image quality; Han and Lee (2014) and Han and Lee (2016) used it in a system for detecting mistakes and overblowing in flute playing; Si et al. (2015) introduced it in their system for estimating high resolution images from low resolution images; Sun et al. (2016) implemented sparse filtering in their system for hyper-spectral anomaly detection; and, Ryman et al. (2016) integrated it in their system for modeling the olfactive temporal response in arrays of chemically diverse sensors.", "startOffset": 68, "endOffset": 905}, {"referenceID": 2, "context": "Bruce et al. (2016) analyzed different biologically-grounded principles for representation learning of images, using sparse filtering as a starting point for the definition of new learning algorithms.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Bruce et al. (2016) analyzed different biologically-grounded principles for representation learning of images, using sparse filtering as a starting point for the definition of new learning algorithms. More interestingly, Kozlov and Gentner (2016) used sparse filtering to model the receptive fields of high-level auditory neurons in a songbird; relying on the idea that sparseness and normalization are canonical neural processing operations (Carandini and Heeger, 2012), their results show that sparse filtering can reproduce the receptive fields of the European starling", "startOffset": 0, "endOffset": 247}, {"referenceID": 27, "context": "(2011) drew connections between sparse filtering, divisive normalization, independent component analysis, and sparse coding, while Lederer and Guadarrama (2014) provided a deeper analysis of the normalization steps inside the sparse filtering algorithm.", "startOffset": 131, "endOffset": 161}, {"referenceID": 24, "context": "Given a generic vector v in an N -dimensional space, we say that v is sparse if a small number of components of the vector accounts for most of the energy1 of the vector (Hurley and Rickard, 2009).", "startOffset": 170, "endOffset": 196}, {"referenceID": 24, "context": "Several measures of sparsity have been proposed in the literature; Hurley and Rickard (2009) offer a review of different measures of sparsity and their properties.", "startOffset": 67, "endOffset": 93}, {"referenceID": 16, "context": "Lifetime sparsity is often referred to as selectivity (Goh et al., 2012).", "startOffset": 54, "endOffset": 72}, {"referenceID": 12, "context": "The weight matrix W can be interpreted as a dictionary (Denil and de Freitas, 2012) or as a filter bank (Dong et al., 2015), where each row is a codeword or a filter applied to every sample in the columns of X.", "startOffset": 104, "endOffset": 123}, {"referenceID": 29, "context": "(2010), and it corresponds to the infomax principle (Linsker, 1989).", "startOffset": 52, "endOffset": 67}, {"referenceID": 30, "context": "where D [\u00b7] is a measure of distance or divergence between the pdfs, such as the KullbackLeibler divergence (MacKay, 2003).", "startOffset": 108, "endOffset": 122}, {"referenceID": 23, "context": "Similarly, RBMs approximate the maximization of the mutual information I [X;Z] through the minimization of the divergence between the distribution of the data and the learned distribution via the maximization of the log probability of the data (Hinton et al., 2006).", "startOffset": 244, "endOffset": 265}, {"referenceID": 46, "context": "These algorithms do not tackle the problem of maximizing the relative entropy D [p(Z) \u2016 q] directly, but they often address it by using constraints, such as bottleneck architectures (Tishby et al., 2000), or by imposing priors, such as adding a sparsity penalty to the learning objective (Vincent et al.", "startOffset": 182, "endOffset": 203}, {"referenceID": 47, "context": ", 2000), or by imposing priors, such as adding a sparsity penalty to the learning objective (Vincent et al., 2010).", "startOffset": 92, "endOffset": 114}, {"referenceID": 45, "context": "For instance, DAEs approximate the maximization of the mutual information I [X;Z] through the minimization of the reconstruction error; Vincent et al. (2010) showed that minimizing the reconstruction error in a DAE is indeed equivalent to maximizing a lower bound on the mutual information.", "startOffset": 136, "endOffset": 158}, {"referenceID": 9, "context": "The JonhsonLindestrauss lemma (Dasgupta and Gupta, 2003; Johnson and Lindenstrauss, 1984) sets bounds on the preservation of the distances by random projections; however, we have no clear guarantees that the randomly initialized weight matrix W would implement such a transformation or that, during training, sparse filtering would learn a transformation that preserves Euclidean distances.", "startOffset": 30, "endOffset": 89}, {"referenceID": 25, "context": "The JonhsonLindestrauss lemma (Dasgupta and Gupta, 2003; Johnson and Lindenstrauss, 1984) sets bounds on the preservation of the distances by random projections; however, we have no clear guarantees that the randomly initialized weight matrix W would implement such a transformation or that, during training, sparse filtering would learn a transformation that preserves Euclidean distances.", "startOffset": 30, "endOffset": 89}, {"referenceID": 31, "context": "This effect is particularly obvious when we interpret the application of absolute-value as a folding of the space RL onto the positive orthant (Montufar et al., 2014).", "startOffset": 143, "endOffset": 166}, {"referenceID": 50, "context": "The choice of an appropriate metric is critical for a distance-based clustering algorithm (Xing et al., 2003), and it expresses our understanding on which spatial directions encode relevant changes (Simard et al.", "startOffset": 90, "endOffset": 109}, {"referenceID": 43, "context": ", 2003), and it expresses our understanding on which spatial directions encode relevant changes (Simard et al., 1998).", "startOffset": 96, "endOffset": 117}, {"referenceID": 30, "context": "In the following set of simulations, we compare sparse filtering against another unsupervised algorithm, the soft k-means algorithm (MacKay, 2003), in order to show under which conditions sparse filtering is a good choice for processing data.", "startOffset": 132, "endOffset": 146}, {"referenceID": 3, "context": "The Berlin Emotional (EMODB) data set is a well-known audio data set in the emotion recognition community (Burkhardt et al., 2005); it contains recordings of ten German actors expressing seven different types of emotions.", "startOffset": 106, "endOffset": 130}, {"referenceID": 6, "context": "(ii) The same set of Mel-frequency spectrum (Childers et al., 1977) coefficient (MFCC) features may", "startOffset": 44, "endOffset": 67}, {"referenceID": 49, "context": "reasonably be used both for speaker recognition and for emotion recognition; indeed, MFCC features were primarily designed for speaker recognition, but they proved to be relevant for emotion recognition as well (Wu et al., 2010; Schuller et al., 2011).", "startOffset": 211, "endOffset": 251}, {"referenceID": 41, "context": "reasonably be used both for speaker recognition and for emotion recognition; indeed, MFCC features were primarily designed for speaker recognition, but they proved to be relevant for emotion recognition as well (Wu et al., 2010; Schuller et al., 2011).", "startOffset": 211, "endOffset": 251}, {"referenceID": 1, "context": "After this analysis, we use both an Euclidean-based unsupervised learning algorithm, Gaussian mixture model (Bishop, 2007), and a cosine-based unsupervised learning algorithm, sparse filtering, to project the data into an L-dimensional space.", "startOffset": 108, "endOffset": 122}, {"referenceID": 32, "context": "(SVHN) data set (Netzer et al., 2011).", "startOffset": 16, "endOffset": 37}, {"referenceID": 43, "context": "This result agrees with the fact that the Euclidean metric is not a suitable metric for measuring distances among samples of digits represented in the pixel space; other distances less sensitive to irrelevant transformations, such as tangent distance (Simard et al., 1998), are known to be better choices.", "startOffset": 251, "endOffset": 272}, {"referenceID": 48, "context": "Consistently with the nofree lunch theorem (Wolpert and Macready, 1997), we reached the conclusion that sparse filtering is not a better algorithm than other Euclidean-based clustering algorithms, but that there is a specific set of problems (in which the data structure is explained by the cosine metric) where the performance of sparse filtering is excellent, balanced by a set of problems (in which the data structure is explained by the Euclidean metric) where its performance is less outstanding.", "startOffset": 43, "endOffset": 71}, {"referenceID": 44, "context": "some regularity in the original representation space, we hypothesize that we could use the information in the labeled samples to address the problem of covariate shift (Sugiyama and Kawanabe, 2012) in a semi-supervised learning scenario.", "startOffset": 168, "endOffset": 197}, {"referenceID": 14, "context": "19 in Freeden and Gutting (2013):", "startOffset": 6, "endOffset": 33}], "year": 2017, "abstractText": "In this paper we present our study on a recent and effective algorithm for unsupervised learning, that is, sparse filtering. The aim of this research is not to show whether or how well sparse filtering works, but to understand why and when sparse filtering does work. We provide a thorough study of this algorithm through a conceptual evaluation of feature distribution learning, a theoretical analysis of the properties of sparse filtering, and an experimental validation of our conclusions. We argue that sparse filtering works by explicitly maximizing the informativeness of the learned representation through the maximization of the proxy of sparsity, and by implicitly preserving information conveyed by the distribution of the original data through the constraint of structure preservation. In particular, we prove that sparse filtering preserves the cosine neighborhoodness of the data. We validate our statements on artificial and real data sets by applying our theoretical understanding to the explanation of the success of sparse filtering on real-world problems. Our work provides a strong theoretical framework for understanding sparse filtering, it highlights assumptions and conditions for success behind the algorithm, and it provides a fresh insight into developing new feature distribution learning algorithms.", "creator": "LaTeX with hyperref package"}}}