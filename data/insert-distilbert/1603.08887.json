{"id": "1603.08887", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2016", "title": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints", "abstract": "we present a discriminative model for single - document summarization that integrally combines compression and anaphoricity constraints. our model selects textual units to include in the summary based on a rich curated set of sparse features whose weights are learned on a large corpus. we furthermore allow for the logical deletion of new content within a sentence when that deletion is licensed by consistency compression rules ; in our framework, usually these are implemented practically as dependencies between subsentential units of text. anaphoricity constraints then improve their cross - sentence coherence by guaranteeing that, for that each pronoun included in the summary, the pronoun's antecedent is included as well or the pronoun is rewritten as a full mention. when trained end - to - end, our final system outperforms prior work on both rouge as well as expenditure on human judgments of linguistic quality.", "histories": [["v1", "Tue, 29 Mar 2016 18:58:42 GMT  (839kb,D)", "http://arxiv.org/abs/1603.08887v1", null], ["v2", "Wed, 8 Jun 2016 05:39:10 GMT  (859kb,D)", "http://arxiv.org/abs/1603.08887v2", "ACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["greg durrett", "taylor berg-kirkpatrick", "dan klein"], "accepted": true, "id": "1603.08887"}, "pdf": {"name": "1603.08887.pdf", "metadata": {"source": "CRF", "title": "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints", "authors": ["Greg Durrett", "Taylor Berg-Kirkpatrick", "Dan Klein"], "emails": ["gdurrett@cs.berkeley.edu", "tberg@cs.cmu.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult. Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008). In this work, we tackle the single-document problem by training an expressive summarization model on a large nat-\n1Available at http://nlp.cs.berkeley.edu\nurally occurring corpus\u2014the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries\u2014learning to select important content with lexical features. This corpus has been explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization.\nTo increase the expressive capacity of our model we allow more aggressive compression of individual sentences by combining two different formalisms\u2014one syntactic and the other discursive. Additionally, we incorporate a model of anaphora resolution and give our system the ability rewrite pronominal mentions, further increasing expressivity. In order to guide the model, we incorporate (1) constraints from coreference ensuring that critical pronoun references are clear in the final summary and (2) constraints from syntactic and discourse parsers ensuring that sentence realizations are well-formed. Despite the complexity of these additional constraints, we demonstrate an efficient inference procedure using an ILPbased approach. By training our full system endto-end on a large-scale dataset, we are able to learn a high-capacity structured model of the summarization process, contrasting with past approaches to the single-document task which have typically been heuristic in nature (Daume\u0301 and Marcu, 2002; Hirao et al., 2013).\nWe focus our evaluation on the New York Times Annotated corpus (Sandhaus, 2008). According to ROUGE, our system outperforms a document prefix baseline, a bigram coverage baseline adapted from a strong multi-document system (Gillick and Favre, 2009), and a discourse-informed method from prior work (Yoshida et al., 2014). Imposing discursive and referential constraints improves human judgments of clarity of both language and referential structure\u2014outperforming the method of\nar X\niv :1\n60 3.\n08 88\n7v 1\n[ cs\n.C L\n] 2\n9 M\nar 2\n01 6\nYoshida et al. (2014) and approaching the clarity of a sentence-extractive baseline\u2014and still achieves substantially higher ROUGE score than either method. These results indicate that our model has the expressive capacity to extract important content, but is sufficiently constrained to ensure fluency is not sacrificed as a result.\nPast work has explored various kinds of structure for summarization. Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011). Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014). Similar to these approaches, we appeal to structures from upstream NLP tasks (syntactic parsing, RST parsing, and coreference) to restrict our model\u2019s capacity to generate. However, we go further by optimizing for ROUGE subject to these constraints with end-to-end learning."}, {"heading": "2 Model", "text": "Our model is shown in Figure 1. Broadly, our ILP takes a set of textual units u = (u1, . . . , un) from a document and finds the highest-scoring extractive summary by optimizing over variables\nxUNIT = xUNIT1 , . . . , x UNIT n , which are binary indicators of whether each unit is included. Textual units are contiguous parts of sentences that serve as the fundamental units of extraction in our model. For a sentence-extractive model, these would be entire sentences, but for our compressive models we will have more fine-grained units, as shown in Figure 2 and described in Section 2.1. Textual units are scored according to features f and model parameters w learned on training data. Finally, the extraction process is subject to a length constraint of k words. This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015). For our model, type-level n-gram scoring only arises when we compute our loss function in maxmargin training (see Section 3).\nIn Section 2.1, we discuss grammaticality constraints, which take the form of introducing dependencies between textual units, as shown in Figure 2. If one textual unit requires another, it cannot be included unless its prerequisite is. We will show that different sets of requirements can capture both syntactic and discourse-based compression schemes.\nFurthermore, we introduce anaphora constraints (Section 2.2) via a new set of variables that capture the process of rewriting pronouns to make them\nexplicit mentions. That is, xREFij = 1 if we should rewrite the jth pronoun in the ith unit with its antecedent. These pronoun rewrites are scored in the objective and introduced into the length constraint to make sure they do not cause our summary to be too long. Finally, constraints on these variables control when they are used and also require the model to include antecedents of pronouns when the model is not confident enough to rewrite them."}, {"heading": "2.1 Grammaticality Constraints", "text": "Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary. During training, our model learns how to take advantage of available compression options and select content to match human generated summaries as closely possible.2 We explore two ways of deriving units for compression: the RST-based compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011).\nRST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001). We show a sentence broken into elemen-\n2The features in our model are actually rich enough to learn a sophisticated compression model, but the data we have (abstractive summaries) does not directly provide examples of correct compressions; past work has gotten around this with multi-task learning (Almeida and Martins, 2013), but we simply treat grammaticality as a constraint from upstream models.\ntary discourse units (EDUs) with RST relations between them. Units marked as SAME-UNIT must both be kept or both be deleted, but other nodes in the tree structure can be deleted as long as we do not delete the parent of an included node. For example, we can delete the ELABORATION clause, but we can delete neither the first nor last EDU. Arrows depict the constraints this gives rise to in the ILP (see Figure 1): u2 requires u1, and u1 and u3 mutually require each other. This is a more constrained form of compression than was used in past work (Hirao et al., 2013), but we find that it improves human judgments of fluency (Section 4.3).\nSyntactic compressions Figure 2b shows two examples of compressions arising from syntactic patterns (Berg-Kirkpatrick et al., 2011): deletion of the second part of a coordinated NP and deletion of a PP modifier to an NP. These patterns were curated to leave sentences as grammatical after being compressed, though perhaps with damaged fluency or semantic content.\nCombined compressions Figure 2c shows the textual units and requirement relations yielded by combining these two types of compression. On this example, the two schemes capture orthogonal compressions, and more generally we find that they stack to give better results for our final system (see Section 4.3). To actually synthesize textual units and the constraints between them, we start from the set of RST textual units and introduce syntactic compressions as new children when they don\u2019t cross existing brackets; because syntactic compressions are typically narrower in scope, they are usually completely contained in EDUs.\nFigure 2d shows an example of this process: the possible deletion of with Aetna is grafted onto the textual unit and appropriate requirement relations are introduced. The net effect is that the textual unit is wholly included, partially included (with Aetna removed), or not at all.\nFormally, we define an RST tree as Trst = (Srst, \u03c0rst) where Srst is a set of EDU spans (i, j) and \u03c0 : S \u2192 2S is a mapping from each EDU span to EDU spans it depends on. Syntactic compressions can be expressed in a similar way with trees Tsyn. These compressions are typically smallerscale than EDU-based compressions, so we use the following modification scheme. Denote by Tsyn(kl) a nontrivial (supports some compression) subtree of Tsyn that is completely contained in an EDU (i, j). We build the following combined compression tree, which we refer to as the augmentation of Trst with Tsyn(kl):\nTcomb = (S \u222a Ssyn(kl) \u222a {(i, k), (l, j)}, \u03c0rst \u222a \u03c0syn(kl)\u222a {(i, k)\u2192 (l, j), (l, j)\u2192 (i, k), (k, l)\u2192 (i, k)})\nThat is, we maintain the existing tree structure except for the EDU (i, j), which is broken into three parts: the outer two depend on each other (is a claims adjuster and . from Figure 2d) and the inner one depends on the others and preserves the tree structure from Tsyn. We augment Trst with all maximal subtrees of Tsyn, i.e. all trees that are not contained in other trees that are used in the augmentation process.\nThis is broadly similar to the combined compression scheme in Kikuchi et al. (2014) but we use a different set of constraints that more strictly enforce grammaticality.3"}, {"heading": "2.2 Anaphora Constraints", "text": "What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010). Cases of pronouns being \u201corphaned\u201d during extraction (their antecedents are deleted) are\n3We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014). We experimented with these and found no improvement from using them, possibly because we have a featurebased model rather than a heuristic content selection procedure, and possibly because automatic discourse parsers are less good at recovering cross-sentence relations.\nThis hasn\u2019t been Kellogg\u2019s year .\nReplacement (2.2.1): If :\nThe oat-bran craze has cost it market share.\nu1\nu2\np1 p2\np3\nAllow pronoun replacement with the predicted antecedent and add the following constraint:\nKelloggit yearit\nNo replacement necessary\nmax(p1, p2, p3) > \u21b5\nrelatively common: they occur in roughly 60% of examples produced by our summarizer when no anaphora constraints are enforced. This kind of error is particularly concerning for summary interpretation and impedes the ability of summaries to convey information effectively (Grice, 1975). Our solution is to explicitly impose constraints on the model based on pronoun anaphora resolution.4\nFigure 3 shows an example of a problem case. If we extract only the second textual unit shown, the pronoun it will lose its antecedent, which in this case is Kellogg. We explore two types of constraints for dealing with this: rewriting the pronoun explicitly, or constraining the summary to include the pronoun\u2019s antecedent."}, {"heading": "2.2.1 Pronoun Replacement", "text": "One way of dealing with these pronoun reference issues is to explicitly replace the pronoun with what it refers to. This replacement allows us to maintain maximal extraction flexibility, since we\n4We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013).\ncan make an isolated textual unit meaningful even if it contains a pronoun. Figure 3 shows how this process works. We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun. If the coreference system is sufficiently confident in its prediction (i.e. maxi pi > \u03b1 for a specified threshold \u03b1 > 12 ), we allow ourselves to replace the pronoun with the first mention of the entity corresponding to the pronoun\u2019s most likely antecedent. In Figure 3, if the system correctly determines that Kellogg is the correct antecedent with high probability, we enable the first replacement shown there, which is used if u2 is included the summary without u1.5\nAs shown in the ILP in Figure 1, we instantiate corresponding pronoun replacement variables xREF where xREFij = 1 implies that the jth pronoun in the ith sentence should be replaced in the summary. We use a candidate pronoun replacement if and only if the pronoun\u2019s corresponding (predicted) entity hasn\u2019t been mentioned previously in the summary.6 Because we are generally replacing pronouns with longer mentions, we also need to modify the length constraint to take this into account. Finally, we incorporate features on pronoun replacements in the objective, which helps the model learn to prefer pronoun replacements that help it to more closely match the human summaries."}, {"heading": "2.2.2 Pronoun Antecedent Constraints", "text": "Explicitly replacing pronouns is risky: if the coreference system makes an incorrect prediction, the intended meaning of the summary may be damaged. Fortunately, the coreference model\u2019s posterior probabilities have been shown to be wellcalibrated (Nguyen and O\u2019Connor, 2015), meaning that cases where it is likely to make errors are signaled by flatter posterior distributions. In this case, we enable a more conservative set of constraints that include additional content in the summary to make the pronoun reference clear without explicitly replacing it. This is done by requiring the inclusion of any textual unit which contains\n5If the proposed replacement is a proper mention, we replace the pronoun just with the subset of the mention that constitutes a named entity (rather than the whole noun phrase). We control for possessive pronouns by deleting or adding \u2019s as appropriate.\n6Such a previous mention may be a pronoun; however, note that that pronoun would then be targeted for replacement unless its antecedent were included somehow.\npossible pronoun references whose posteriors sum to at least a threshold parameter \u03b2. Figure 3 shows that this constraint can force the inclusion of u1 to provide additional context. Although this could still lead to unclear pronouns if text is stitched together in an ambiguous or even misleading way, in practice we observe that the textual units we force to be added almost always occur very recently before the pronoun, giving enough additional context for a human reader to figure out the pronoun\u2019s antecedent unambiguously."}, {"heading": "2.3 Features", "text": "The features in our model (see Figure 1) consist of a set of surface indicators capturing mostly lexical and configurational information. Their primary role is to identify important document content. The first three types of features fire over textual units, the last over pronoun replacements.\nLexical These include indicator features on nonstopwords in the textual unit that appear at least five times in the training set and analogous POS features. We also use lexical features on the first, last, preceding, and following words for each textual unit. Finally, we conjoin each of these features with an indicator of bucketed position in the document (the index of the sentence containing the textual unit).\nStructural These features include various conjunctions of the position of the textual unit in the document, its length, the length of its corresponding sentence, the index of the paragraph it occurs in, and whether it starts a new paragraph (all values are bucketed).\nCentrality These features capture rough information about the centrality of content: they consist of bucketed word counts conjoined with bucketed sentence index in the document. We also fire features on the number of times of each entity mentioned in the sentence is mentioned in the rest of the document (according to a coreference system), the number of entities mentioned in the sentence, and surface properties of mentions including type and length\nPronoun replacement These target properties of the pronoun replacement such as its length, its sentence distance from the current mention, its type (nominal or proper), and the identity of the pronoun being replaced."}, {"heading": "3 Learning", "text": "We learn weights w for our model by training on a large corpus of documents u paired with reference summaries y. We formulate our learning problem as a standard instance of structured SVM where our loss function is the ROUGE score of the predicted summary with respect to the reference (human) summary. We refer the reader to Smith (2011) for an introduction to structured SVM. We train the model via stochastic subgradient descent on the primal (Ratliff et al., 2007; Kummerfeld et al., 2015). In order to compute the subgradient for a given training example, we need to find the most violated constraint on the given instance through a cost-augmented decode, which generally takes the form argmaxxw\n>f(x)+`(x,y). To use ROUGE as our loss function, we take\n`(xNGRAM,y) = max x\u2217 ROUGE-1(x\u2217,y)\u2212ROUGE-1(xNGRAM,y)\ni.e. the difference between the predicted ROUGE score and the oracle ROUGE score achievable under the model (including constraints). Here xNGRAM are indicator variables that track, for each n-gram type in the reference summary, whether that n-gram is present in the system summary. These are the sufficient statistics for computing ROUGE.\nDuring training, we use an extended version of our ILP in Figure 1 that is augmented to explicitly track type-level n-grams during inference:\nmax xUNIT,xREF,xNGRAM [\u2211 i [ xUNITi (w >f(ui)) ]\n+ \u2211 (i,j) [ xREFij (w >f(rij)) ] \u2212 `(xNGRAM,y)  subject to all constraints from Figure 1 xNGRAMi = 1 iff an included textual unit or replacement contains the ith reference n-gram\nThese kinds of variables and constraints are common in multi-document summarization systems that score bigrams (Gillick and Favre, 2009 inter alia). Note that since ROUGE is only computed over non-stopword n-grams and pronoun replacements only replace pronouns, pronoun replacement can never remove an n-gram that would otherwise be included.\nFor all experiments, we optimize our objective using AdaGrad (Duchi et al., 2011) with `1 regularization (\u03bb = 10\u22128, chosen by grid search), with a step size of 0.1 and a minibatch size of 1. We\ntrain for 10 iterations on the training data, at which point held-out model performance no longer improves. Finally, we set the anaphora thresholds \u03b1 = 0.8 and \u03b2 = 0.6 (see Section 2.2). The values of these and other hyperparameters were determined on a held-out development set from our New York Times training data. All ILPs are solved using GLPK version 4.55."}, {"heading": "4 Experiments", "text": "We primarily evaluate our model on a 3000- document evaluation set from the New York Times Annotated Corpus (Sandhaus, 2008). We also investigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.7 Throughout this section, we set the word budget for our summarizer to be the same length as the reference summaries, following previous work (Hirao et al., 2013; Yoshida et al., 2014)."}, {"heading": "4.1 Preprocessing", "text": "We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution System (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans.\nTo follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style described in Hirao et al. (2013), since their parser is not publicly available. Specifically, we use the first-order projective parsing model of McDonald et al. (2005) and features from Soricut and Marcu (2003), Hernault et al. (2010), and Joty et al. (2013). When using the same head annotation scheme as Yoshida et al. (2014), we outperform their discourse dependency parser on unlabeled dependency accuracy, getting 56% as opposed to 53%.\n7Tasks like DUC and TAC have focused on multidocument summarization since around 2003, hence the lack of more standard datasets for single-document summarization."}, {"heading": "4.2 New York Times Corpus", "text": "We now provide some details about the New York Times Annotated corpus. This dataset contains 110,540 articles with abstractive summaries; we split these into 100,834 training and 9706 test examples, based on date of publication (test is all articles published on January 1, 2007 or later). Examples of two documents from this dataset are shown in Figure 4. The bottom example demonstrates that some summaries are extremely short and formulaic (especially those for obituaries and editorials). To counter this, we filter the raw dataset by removing all documents with summaries that are shorter than 50 words. One benefit of filtering is that the length distribution of our resulting dataset is more in line with standard summarization evaluations like DUC; it also ensures a sufficient number of tokens in the budget to produce nontrivial summaries. This filtered test set, which we call NYT50, includes 3,452 test examples out of the original 9,706."}, {"heading": "4.3 New York Times Results", "text": "We evaluate our system along two axes: first, on content selection, using ROUGE8 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon\n8We use the ROUGE 1.5.5 script with the following command line arguments: -n 2 -x -m -s. All given results are macro-averaged recall values over the test set.\nMechanical Turk. We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale. Furthermore, we ask how many unclear pronouns references there were in the text. The Turkers do not see the original document or the reference summary, and rate each summary in isolation. Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems according to expert judgments.\nTo speed up preprocessing and training time on this corpus, we further restrict our training set to only contain documents with fewer than 100 EDUs. All told, the final system takes roughly 20 hours to make 10 passes through the subsampled training data (22,000 documents) on a single core of an Amazon EC2 r3.4xlarge instance.\nTable 1 shows the results on the NYT50 corpus. We compare several variants of our system and baselines. For baselines, we use two variants of first k: one which must stop on a sentence boundary (which gives better linguistic quality) and one which always consumes k tokens (which gives better ROUGE). We also use a heuristic sentenceextractive baseline that maximizes the document counts (term frequency) of bigrams covered by the summary, similar in spirit to the multi-document method of Gillick and Favre (2009). We also compare to our implementation of the Tree Knapsack\nmethod of Yoshida et al. (2014), which matches their results very closely on the RST Discourse Treebank when discourse trees are controlled for. Finally, we compare several variants of our system: purely extractive systems operating over sentences and EDUs respectively, our grammaticallyconstrained extractive and compressive system, and our full system that further incorporates pronoun constraints and pronoun replacement.\nIn terms of content selection, we see that all of the systems that incorporate end-to-end learning (under \u201cThis work\u201d) substantially outperform our various heuristic baselines. Our full system using the full compression scheme is substantially better on ROUGE than ablations where the syntactic or discourse compressions are removed. These improvements reflect the fact that more compression options give the system more flexibility to include key content words. Removing the anaphora resolution constraints actually causes ROUGE to increase slightly (as a result of granting the model flexibility), but has a negative impact on the linguistic quality metrics.\nOn our linguistic quality metrics, it is no surprise that the sentence prefix baseline performs the best. Our sentence-extractive system also does\nwell on these metrics. Compared to the EDUextractive system with no constraints, our constrained compression method improves substantially on both linguistic quality and reduces the number of unclear pronouns, and adding the pronoun anaphora constraints gives further improvement. Our final system is approaches the sentenceextractive baseline, particularly on unclear pronouns, and achieves substantially higher ROUGE score."}, {"heading": "4.4 RST Treebank", "text": "We also evaluate on the RST Discourse Treebank, of which 30 documents have abstractive summaries. Following Hirao et al. (2013), we use the gold EDU segmentation from the RST corpus but automatic RST trees. We break this into a 10- document development set and a 20-document test set. Table 2 shows the results on the RST corpus. Our system is roughly comparable to Tree Knapsack here, and we note that none of the differences in the table are statistically significant. We also observed significant variation between multiple runs on this corpus, with scores changing by 1-2 ROUGE points for slightly different system variants.9"}, {"heading": "5 Conclusion", "text": "We presented a single-document summarization system trained end-to-end on a large corpus. We integrate a compression model that enforces grammaticality as well as pronoun anaphoricity constraints that enforce coherence. Our system improves substantially over baseline systems on ROUGE while still maintaining good linguistic quality.\n9The system of Yoshida et al. (2014) is unavailable, so we use a reimplementation. Our results differ from theirs due to different discourse trees and high variance on the test set."}], "references": [{"title": "Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning", "author": ["Miguel Almeida", "Andre Martins."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Almeida and Martins.,? 2013", "shortCiteRegEx": "Almeida and Martins.", "year": 2013}, {"title": "Modeling Local Coherence: An Entity-based Approach", "author": ["Regina Barzilay", "Mirella Lapata."], "venue": "Computational Linguistics, 34(1):1\u201334, March.", "citeRegEx": "Barzilay and Lapata.,? 2008", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2008}, {"title": "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization", "author": ["Regina Barzilay", "Lillian Lee."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Barzilay and Lee.,? 2004", "shortCiteRegEx": "Barzilay and Lee.", "year": 2004}, {"title": "Sentence Ordering in Multidocument Summarization", "author": ["Regina Barzilay", "Noemie Elhadad", "Kathleen R. McKeown."], "venue": "Proceedings of the International Conference on Human Language Technology Research.", "citeRegEx": "Barzilay et al\\.,? 2001", "shortCiteRegEx": "Barzilay et al\\.", "year": 2001}, {"title": "Jointly Learning to Extract and Compress", "author": ["Taylor Berg-Kirkpatrick", "Dan Gillick", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2011", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2011}, {"title": "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "author": ["Jaime Carbonell", "Jade Goldstein."], "venue": "Proceedings of the International ACM SIGIR Conference on Research and Development in Information", "citeRegEx": "Carbonell and Goldstein.,? 1998", "shortCiteRegEx": "Carbonell and Goldstein.", "year": 1998}, {"title": "Building a Discourse-tagged Corpus in the Framework of Rhetorical Structure Theory", "author": ["Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski."], "venue": "Proceedings of the Second SIGDIAL Workshop on Discourse and Dialogue.", "citeRegEx": "Carlson et al\\.,? 2001", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "Towards Coherent MultiDocument Summarization", "author": ["Janara Christensen", "Mausam", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Christensen et al\\.,? 2013", "shortCiteRegEx": "Christensen et al\\.", "year": 2013}, {"title": "Global Inference for Sentence Compression an Integer Linear Programming Approach", "author": ["James Clarke", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research, 31(1):399\u2013429, March.", "citeRegEx": "Clarke and Lapata.,? 2008", "shortCiteRegEx": "Clarke and Lapata.", "year": 2008}, {"title": "Discourse Constraints for Document Compression", "author": ["James Clarke", "Mirella Lapata."], "venue": "Computational Linguistics, 36(3):411\u2013441, September.", "citeRegEx": "Clarke and Lapata.,? 2010", "shortCiteRegEx": "Clarke and Lapata.", "year": 2010}, {"title": "A NoisyChannel Model for Document Compression", "author": ["III Hal Daum\u00e9", "Daniel Marcu."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Daum\u00e9 and Marcu.,? 2002", "shortCiteRegEx": "Daum\u00e9 and Marcu.", "year": 2002}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159, July.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A New Entity Salience Task with Millions of Training Examples", "author": ["Jesse Dunietz", "Daniel Gillick."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Dunietz and Gillick.,? 2014", "shortCiteRegEx": "Dunietz and Gillick.", "year": 2014}, {"title": "Easy Victories and Uphill Battles in Coreference Resolution", "author": ["Greg Durrett", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), October.", "citeRegEx": "Durrett and Klein.,? 2013", "shortCiteRegEx": "Durrett and Klein.", "year": 2013}, {"title": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking", "author": ["Greg Durrett", "Dan Klein."], "venue": "Transactions of the Association for Computational Linguistics (TACL).", "citeRegEx": "Durrett and Klein.,? 2014", "shortCiteRegEx": "Durrett and Klein.", "year": 2014}, {"title": "A Scalable Global Model for Summarization", "author": ["Dan Gillick", "Benoit Favre."], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing.", "citeRegEx": "Gillick and Favre.,? 2009", "shortCiteRegEx": "Gillick and Favre.", "year": 2009}, {"title": "Non-Expert Evaluation of Summarization Systems is Risky", "author": ["Dan Gillick", "Yang Liu."], "venue": "Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk.", "citeRegEx": "Gillick and Liu.,? 2010", "shortCiteRegEx": "Gillick and Liu.", "year": 2010}, {"title": "Logic and Conversation", "author": ["H.P. Grice."], "venue": "Syntax and Semantics 3: Speech Acts, pages 41\u201358.", "citeRegEx": "Grice.,? 1975", "shortCiteRegEx": "Grice.", "year": 1975}, {"title": "Centering: A Framework for Modeling the Local Coherence of Discourse", "author": ["Barbara J. Grosz", "Scott Weinstein", "Aravind K. Joshi."], "venue": "Computational Linguistics, 21(2):203\u2013225, June.", "citeRegEx": "Grosz et al\\.,? 1995", "shortCiteRegEx": "Grosz et al\\.", "year": 1995}, {"title": "Sparser, Better, Faster GPU Parsing", "author": ["David Hall", "Taylor Berg-Kirkpatrick", "John Canny", "Dan Klein."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "HILDA: A discourse parser using support vector machine classification", "author": ["Hugo Hernault", "Helmut Prendinger", "David A. Duverle", "Mitsuru Ishizuka", "Tim Paek."], "venue": "Dialogue and Discourse, 1:1\u201333.", "citeRegEx": "Hernault et al\\.,? 2010", "shortCiteRegEx": "Hernault et al\\.", "year": 2010}, {"title": "Single-Document Summarization as a Tree Knapsack Problem", "author": ["Tsutomu Hirao", "Yasuhisa Yoshida", "Masaaki Nishino", "Norihito Yasuda", "Masaaki Nagata."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Hirao et al\\.,? 2013", "shortCiteRegEx": "Hirao et al\\.", "year": 2013}, {"title": "Improving the Estimation of Word Importance for News MultiDocument Summarization", "author": ["Kai Hong", "Ani Nenkova."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "Hong and Nenkova.,? 2014", "shortCiteRegEx": "Hong and Nenkova.", "year": 2014}, {"title": "Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis", "author": ["Shafiq Joty", "Giuseppe Carenini", "Raymond Ng", "Yashar Mehdad."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Joty et al\\.,? 2013", "shortCiteRegEx": "Joty et al\\.", "year": 2013}, {"title": "An Empirical Analysis of Optimization for Max-Margin NLP", "author": ["Jonathan K. Kummerfeld", "Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kummerfeld et al\\.,? 2015", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2015}, {"title": "Using External Resources and Joint Learning for Bigram Weighting in ILP-Based Multi-Document Summarization", "author": ["Chen Li", "Yang Liu", "Lin Zhao."], "venue": "Proceedings of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A Class of Submodular Functions for Document Summarization", "author": ["Hui Lin", "Jeff Bilmes."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Lin and Bilmes.,? 2011", "shortCiteRegEx": "Lin and Bilmes.", "year": 2011}, {"title": "Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Improving Summarization Performance by Sentence Compression: A Pilot Study", "author": ["Chin-Yew Lin."], "venue": "Proceedings of the International Workshop on Information Retrieval with Asian Languages.", "citeRegEx": "Lin.,? 2003", "shortCiteRegEx": "Lin.", "year": 2003}, {"title": "A Coherence Model Based on Syntactic Patterns", "author": ["Annie Louis", "Ani Nenkova."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).", "citeRegEx": "Louis and Nenkova.,? 2012", "shortCiteRegEx": "Louis and Nenkova.", "year": 2012}, {"title": "Discourse Indicators for Content Selection in Summarization", "author": ["Annie Louis", "Aravind Joshi", "Ani Nenkova."], "venue": "Proceedings of the SIGDIAL 2010 Conference.", "citeRegEx": "Louis et al\\.,? 2010", "shortCiteRegEx": "Louis et al\\.", "year": 2010}, {"title": "AutomaticSummarization", "author": ["Inderjeet Mani."], "venue": "John Benjamins Publishing.", "citeRegEx": "Mani.,? 2001", "shortCiteRegEx": "Mani.", "year": 2001}, {"title": "Rhetorical Structure Theory: Toward a Functional Theory of Text Organization", "author": ["William C. Mann", "Sandra A. Thompson."], "venue": "Text, 8(3):243\u2013281.", "citeRegEx": "Mann and Thompson.,? 1988", "shortCiteRegEx": "Mann and Thompson.", "year": 1988}, {"title": "Improving summarization through rhetorical parsing tuning", "author": ["Daniel Marcu."], "venue": "Proceedings of the Workshop on Very Large Corpora.", "citeRegEx": "Marcu.,? 1998", "shortCiteRegEx": "Marcu.", "year": 1998}, {"title": "Summarization with a Joint Model for Sentence Extraction and Compression", "author": ["Andre Martins", "Noah A. Smith."], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing.", "citeRegEx": "Martins and Smith.,? 2009", "shortCiteRegEx": "Martins and Smith.", "year": 2009}, {"title": "Online Large-margin Training of Dependency Parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Discriminative Sentence Compression With Soft Syntactic Evidence", "author": ["Ryan McDonald."], "venue": "Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).", "citeRegEx": "McDonald.,? 2006", "shortCiteRegEx": "McDonald.", "year": 2006}, {"title": "Generating Concise Natural Language Summaries", "author": ["Kathleen McKeown", "Jacques Robin", "Karen Kukich."], "venue": "Information Processing and Management, 31(5):703\u2013733, September.", "citeRegEx": "McKeown et al\\.,? 1995", "shortCiteRegEx": "McKeown et al\\.", "year": 1995}, {"title": "Discourse Structures to Reduce Discourse Incoherence in Blog Summarization", "author": ["Shamima Mithun", "Leila Kosseim."], "venue": "Proceedings of Recent Advances in Natural Language Processing.", "citeRegEx": "Mithun and Kosseim.,? 2011", "shortCiteRegEx": "Mithun and Kosseim.", "year": 2011}, {"title": "Automatic summarization", "author": ["Ani Nenkova", "Kathleen McKeown."], "venue": "Foundations and Trends in Information Retrieval, 5(2?3):103\u2013233.", "citeRegEx": "Nenkova and McKeown.,? 2011", "shortCiteRegEx": "Nenkova and McKeown.", "year": 2011}, {"title": "Posterior Calibration and Exploratory Analysis for Natural Language Processing Models", "author": ["Khanh Nguyen", "Brendan O\u2019Connor"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Nguyen and O.Connor.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen and O.Connor.", "year": 2015}, {"title": "Learning to Generate Coherent Summary with Discriminative Hidden Semi-Markov Model", "author": ["Hitoshi Nishikawa", "Kazuho Arita", "Katsumi Tanaka", "Tsutomu Hirao", "Toshiro Makino", "Yoshihiro Matsuo."], "venue": "Proceedings of the International Confer-", "citeRegEx": "Nishikawa et al\\.,? 2014", "shortCiteRegEx": "Nishikawa et al\\.", "year": 2014}, {"title": "A Critical Reassessment of Evaluation Baselines for Speech Summarization", "author": ["Gerald Penn", "Xiaodan Zhu."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Penn and Zhu.,? 2008", "shortCiteRegEx": "Penn and Zhu.", "year": 2008}, {"title": "Learning Accurate, Compact, and Interpretable Tree Annotation", "author": ["Slav Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein."], "venue": "Proceedings of the Conference on Computational Linguistics and the Association for Computational Linguistics (ACL-", "citeRegEx": "Petrov et al\\.,? 2006", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Modelling Events through Memory-based, Open-IE Patterns for Abstractive Summarization", "author": ["Daniele Pighin", "Marco Cornolti", "Enrique Alfonseca", "Katja Filippova."], "venue": "Proceedings of the Association for Computational Linguistics (ACL).", "citeRegEx": "Pighin et al\\.,? 2014", "shortCiteRegEx": "Pighin et al\\.", "year": 2014}, {"title": "Online) Subgradient Methods for Structured Prediction", "author": ["Nathan J. Ratliff", "Andrew Bagnell", "Martin Zinkevich."], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Ratliff et al\\.,? 2007", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "The New York Times Annotated Corpus", "author": ["Evan Sandhaus."], "venue": "Linguistic Data Consortium.", "citeRegEx": "Sandhaus.,? 2008", "shortCiteRegEx": "Sandhaus.", "year": 2008}, {"title": "Linguistic Structure Prediction", "author": ["Noah A. Smith."], "venue": "Morgan & Claypool Publishers, 1st edition.", "citeRegEx": "Smith.,? 2011", "shortCiteRegEx": "Smith.", "year": 2011}, {"title": "Sentence Level Discourse Parsing Using Syntactic and Lexical Information", "author": ["Radu Soricut", "Daniel Marcu."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "Soricut and Marcu.,? 2003", "shortCiteRegEx": "Soricut and Marcu.", "year": 2003}, {"title": "Supervised Sentence Fusion with Single-Stage Inference", "author": ["Kapil Thadani", "Kathleen McKeown."], "venue": "Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP).", "citeRegEx": "Thadani and McKeown.,? 2013", "shortCiteRegEx": "Thadani and McKeown.", "year": 2013}, {"title": "Multiple Aspect Summarization Using Integer Linear Programming", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Woodsend and Lapata.,? 2012", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2012}, {"title": "Dependency-based Discourse Parser for Single-Document Summarization", "author": ["Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Yoshida et al\\.,? 2014", "shortCiteRegEx": "Yoshida et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al.", "startOffset": 72, "endOffset": 177}, {"referenceID": 15, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al.", "startOffset": 72, "endOffset": 177}, {"referenceID": 26, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al.", "startOffset": 72, "endOffset": 177}, {"referenceID": 39, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al.", "startOffset": 72, "endOffset": 177}, {"referenceID": 37, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult.", "startOffset": 209, "endOffset": 276}, {"referenceID": 33, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult.", "startOffset": 209, "endOffset": 276}, {"referenceID": 31, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult.", "startOffset": 209, "endOffset": 276}, {"referenceID": 21, "context": "While multi-document summarization is wellstudied in the NLP literature (Carbonell and Goldstein, 1998; Gillick and Favre, 2009; Lin and Bilmes, 2011; Nenkova and McKeown, 2011), single-document summarization (McKeown et al., 1995; Marcu, 1998; Mani, 2001; Hirao et al., 2013) has received less attention in recent years and is generally viewed as more difficult.", "startOffset": 209, "endOffset": 276}, {"referenceID": 42, "context": "Content selection is tricky without redundancy across multiple input documents as a guide and simple positional information is often hard to beat (Penn and Zhu, 2008).", "startOffset": 146, "endOffset": 166}, {"referenceID": 46, "context": "edu urally occurring corpus\u2014the New York Times Annotated Corpus (Sandhaus, 2008) which contains around 100,000 news articles with abstractive summaries\u2014learning to select important content with lexical features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 12, "context": "explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization.", "startOffset": 29, "endOffset": 80}, {"referenceID": 22, "context": "explored in related contexts (Dunietz and Gillick, 2014; Hong and Nenkova, 2014), but to our knowledge it has not been directly used for singledocument summarization.", "startOffset": 29, "endOffset": 80}, {"referenceID": 46, "context": "We focus our evaluation on the New York Times Annotated corpus (Sandhaus, 2008).", "startOffset": 63, "endOffset": 79}, {"referenceID": 15, "context": "According to ROUGE, our system outperforms a document prefix baseline, a bigram coverage baseline adapted from a strong multi-document system (Gillick and Favre, 2009), and a discourse-informed method from prior work (Yoshida et al.", "startOffset": 142, "endOffset": 167}, {"referenceID": 51, "context": "According to ROUGE, our system outperforms a document prefix baseline, a bigram coverage baseline adapted from a strong multi-document system (Gillick and Favre, 2009), and a discourse-informed method from prior work (Yoshida et al., 2014).", "startOffset": 217, "endOffset": 239}, {"referenceID": 30, "context": "Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011).", "startOffset": 79, "endOffset": 119}, {"referenceID": 21, "context": "Some work has focused on improving content selection using discourse structure (Louis et al., 2010; Hirao et al., 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011).", "startOffset": 79, "endOffset": 119}, {"referenceID": 2, "context": ", 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011).", "startOffset": 27, "endOffset": 51}, {"referenceID": 38, "context": ", 2013), topical structure (Barzilay and Lee, 2004), or related techniques (Mithun and Kosseim, 2011).", "startOffset": 75, "endOffset": 101}, {"referenceID": 3, "context": "Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al.", "startOffset": 82, "endOffset": 183}, {"referenceID": 1, "context": "Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al.", "startOffset": 82, "endOffset": 183}, {"referenceID": 29, "context": "Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al.", "startOffset": 82, "endOffset": 183}, {"referenceID": 7, "context": "Other work has used structure primarily to reorder summaries and ensure coherence (Barzilay et al., 2001; Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Christensen et al., 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al.", "startOffset": 82, "endOffset": 183}, {"referenceID": 49, "context": ", 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014).", "startOffset": 67, "endOffset": 115}, {"referenceID": 44, "context": ", 2013) or to represent content for sentence fusion or abstraction (Thadani and McKeown, 2013; Pighin et al., 2014).", "startOffset": 67, "endOffset": 115}, {"referenceID": 15, "context": "This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015).", "startOffset": 169, "endOffset": 266}, {"referenceID": 4, "context": "This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015).", "startOffset": 169, "endOffset": 266}, {"referenceID": 22, "context": "This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015).", "startOffset": 169, "endOffset": 266}, {"referenceID": 25, "context": "This approach is similar in spirit to ILP formulations of multi-document summarization systems, though in those systems content is typically modeled in terms of bigrams (Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Hong and Nenkova, 2014; Li et al., 2015).", "startOffset": 169, "endOffset": 266}, {"referenceID": 20, "context": "(a) RST-based compression structure like that in Hirao et al. (2013), where we can delete the ELABORATION clause.", "startOffset": 49, "endOffset": 69}, {"referenceID": 4, "context": "(b) Two syntactic compression options from Berg-Kirkpatrick et al. (2011), namely deletion of a coordinate and deletion of a PP modifier.", "startOffset": 43, "endOffset": 74}, {"referenceID": 36, "context": "Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and", "startOffset": 48, "endOffset": 89}, {"referenceID": 8, "context": "Following work on isolated sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and", "startOffset": 48, "endOffset": 89}, {"referenceID": 28, "context": "compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary.", "startOffset": 26, "endOffset": 147}, {"referenceID": 34, "context": "compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary.", "startOffset": 26, "endOffset": 147}, {"referenceID": 4, "context": "compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary.", "startOffset": 26, "endOffset": 147}, {"referenceID": 50, "context": "compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary.", "startOffset": 26, "endOffset": 147}, {"referenceID": 0, "context": "compressive summarization (Lin, 2003; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013), we wish to be able to compress sentences so we can pack more information into a summary.", "startOffset": 26, "endOffset": 147}, {"referenceID": 20, "context": "compressions of Hirao et al. (2013) and the syntactic compressions of Berg-Kirkpatrick et al.", "startOffset": 16, "endOffset": 36}, {"referenceID": 4, "context": "(2013) and the syntactic compressions of Berg-Kirkpatrick et al. (2011).", "startOffset": 41, "endOffset": 72}, {"referenceID": 32, "context": "RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001).", "startOffset": 93, "endOffset": 140}, {"referenceID": 6, "context": "RST compressions Figure 2a shows how to derive compressions from Rhetorical Structure Theory (Mann and Thompson, 1988; Carlson et al., 2001).", "startOffset": 93, "endOffset": 140}, {"referenceID": 0, "context": "The features in our model are actually rich enough to learn a sophisticated compression model, but the data we have (abstractive summaries) does not directly provide examples of correct compressions; past work has gotten around this with multi-task learning (Almeida and Martins, 2013), but we simply treat grammaticality as a constraint from upstream models.", "startOffset": 258, "endOffset": 285}, {"referenceID": 21, "context": "This is a more constrained form of compression than was used in past work (Hirao et al., 2013), but we find that it improves human judgments of fluency (Section 4.", "startOffset": 74, "endOffset": 94}, {"referenceID": 4, "context": "Syntactic compressions Figure 2b shows two examples of compressions arising from syntactic patterns (Berg-Kirkpatrick et al., 2011): deletion of the second part of a coordinated NP and deletion of a PP modifier to an NP.", "startOffset": 100, "endOffset": 131}, {"referenceID": 18, "context": "What kind of cross-sentential coherence do we need to ensure for the kinds of summaries our system produces? Many notions of coherence are useful, including centering theory (Grosz et al., 1995) and lexical cohesion (Nishikawa et al.", "startOffset": 174, "endOffset": 194}, {"referenceID": 41, "context": ", 1995) and lexical cohesion (Nishikawa et al., 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010).", "startOffset": 29, "endOffset": 53}, {"referenceID": 9, "context": ", 2014), but one of the most pressing phenomena to deal with is pronoun anaphora (Clarke and Lapata, 2010).", "startOffset": 81, "endOffset": 106}, {"referenceID": 21, "context": "We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014).", "startOffset": 84, "endOffset": 126}, {"referenceID": 51, "context": "We also differ from past work in that we do not use crosssentential RST constraints (Hirao et al., 2013; Yoshida et al., 2014).", "startOffset": 84, "endOffset": 126}, {"referenceID": 14, "context": "It, which refers to Kellogg, has several possible antecedents from the standpoint of an automatic coreference system (Durrett and Klein, 2014).", "startOffset": 117, "endOffset": 142}, {"referenceID": 17, "context": "This kind of error is particularly concerning for summary interpretation and impedes the ability of summaries to convey information effectively (Grice, 1975).", "startOffset": 144, "endOffset": 157}, {"referenceID": 13, "context": "We focus on pronoun coreference because it is the most pressing manifestation of this problem and because existing coreference systems perform well on pronouns compared to harder instances of coreference (Durrett and Klein, 2013).", "startOffset": 204, "endOffset": 229}, {"referenceID": 14, "context": "We run the Berkeley Entity Resolution System (Durrett and Klein, 2014) and compute posteriors over possible links for the pronoun.", "startOffset": 45, "endOffset": 70}, {"referenceID": 40, "context": "Fortunately, the coreference model\u2019s posterior probabilities have been shown to be wellcalibrated (Nguyen and O\u2019Connor, 2015), meaning that cases where it is likely to make errors are signaled by flatter posterior distributions.", "startOffset": 98, "endOffset": 125}, {"referenceID": 45, "context": "We train the model via stochastic subgradient descent on the primal (Ratliff et al., 2007; Kummerfeld et al., 2015).", "startOffset": 68, "endOffset": 115}, {"referenceID": 24, "context": "We train the model via stochastic subgradient descent on the primal (Ratliff et al., 2007; Kummerfeld et al., 2015).", "startOffset": 68, "endOffset": 115}, {"referenceID": 45, "context": "We refer the reader to Smith (2011) for an introduction to structured SVM.", "startOffset": 23, "endOffset": 36}, {"referenceID": 11, "context": "For all experiments, we optimize our objective using AdaGrad (Duchi et al., 2011) with `1 regularization (\u03bb = 10\u22128, chosen by grid search), with a step size of 0.", "startOffset": 61, "endOffset": 81}, {"referenceID": 46, "context": "We primarily evaluate our model on a 3000document evaluation set from the New York Times Annotated Corpus (Sandhaus, 2008).", "startOffset": 106, "endOffset": 122}, {"referenceID": 6, "context": "vestigate its performance on the RST Discourse Treebank (Carlson et al., 2001), but because this dataset is only 30 documents it provides much less robust estimates of performance.", "startOffset": 56, "endOffset": 78}, {"referenceID": 21, "context": "summarizer to be the same length as the reference summaries, following previous work (Hirao et al., 2013; Yoshida et al., 2014).", "startOffset": 85, "endOffset": 127}, {"referenceID": 51, "context": "summarizer to be the same length as the reference summaries, following previous work (Hirao et al., 2013; Yoshida et al., 2014).", "startOffset": 85, "endOffset": 127}, {"referenceID": 43, "context": "We preprocess all data using the Berkeley Parser (Petrov et al., 2006), specifically the GPUaccelerated version of the parser from Hall et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 19, "context": ", 2006), specifically the GPUaccelerated version of the parser from Hall et al. (2014), and the Berkeley Entity Resolution Sys-", "startOffset": 68, "endOffset": 87}, {"referenceID": 14, "context": "tem (Durrett and Klein, 2014).", "startOffset": 4, "endOffset": 29}, {"referenceID": 13, "context": "tem (Durrett and Klein, 2014). For RST discourse analysis, we segment text into EDUs using a semiMarkov CRF trained on the RST treebank with features on boundaries similar to those of Hernault et al. (2010), plus novel features on spans including span length and span identity for short spans.", "startOffset": 5, "endOffset": 207}, {"referenceID": 44, "context": "To follow the conditions of Yoshida et al. (2014) as closely as possible, we also build a discourse parser in the style described in Hirao et al.", "startOffset": 28, "endOffset": 50}, {"referenceID": 20, "context": "(2014) as closely as possible, we also build a discourse parser in the style described in Hirao et al. (2013), since their parser is not publicly available.", "startOffset": 90, "endOffset": 110}, {"referenceID": 20, "context": "(2014) as closely as possible, we also build a discourse parser in the style described in Hirao et al. (2013), since their parser is not publicly available. Specifically, we use the first-order projective parsing model of McDonald et al. (2005) and features from Soricut and Marcu (2003), Hernault et al.", "startOffset": 90, "endOffset": 245}, {"referenceID": 20, "context": "(2014) as closely as possible, we also build a discourse parser in the style described in Hirao et al. (2013), since their parser is not publicly available. Specifically, we use the first-order projective parsing model of McDonald et al. (2005) and features from Soricut and Marcu (2003), Hernault et al.", "startOffset": 90, "endOffset": 288}, {"referenceID": 20, "context": "(2005) and features from Soricut and Marcu (2003), Hernault et al. (2010), and Joty et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 20, "context": "(2005) and features from Soricut and Marcu (2003), Hernault et al. (2010), and Joty et al. (2013). When using the same head annotation scheme as Yoshida et al.", "startOffset": 51, "endOffset": 98}, {"referenceID": 20, "context": "(2005) and features from Soricut and Marcu (2003), Hernault et al. (2010), and Joty et al. (2013). When using the same head annotation scheme as Yoshida et al. (2014), we outperform their discourse dependency parser on unlabeled dependency accuracy, getting 56% as opposed to 53%.", "startOffset": 51, "endOffset": 167}, {"referenceID": 27, "context": "We evaluate our system along two axes: first, on content selection, using ROUGE8 (Lin and Hovy, 2003), and second, on clarity of language and referential structure, using annotators from Amazon", "startOffset": 81, "endOffset": 101}, {"referenceID": 16, "context": "We follow the method of Gillick and Liu (2010) for this evaluation and ask Turkers to rate a summary on how grammatical it is using a 10-point Likert scale.", "startOffset": 24, "endOffset": 47}, {"referenceID": 16, "context": "Gillick and Liu (2010) showed that for linguistic quality judgments (as opposed to content judgments), Turkers reproduced the ranking of systems according to expert judgments.", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "method of Gillick and Favre (2009). We also compare to our implementation of the Tree Knapsack", "startOffset": 10, "endOffset": 35}, {"referenceID": 46, "context": "Table 1: Results on the NYT50 test set (documents with summaries of at least 50 tokens) from the New York Times Annotated Corpus (Sandhaus, 2008).", "startOffset": 129, "endOffset": 145}, {"referenceID": 51, "context": "On content selection, our system substantially outperforms all baselines, our implementation of the tree knapsack system (Yoshida et al., 2014), and learned extractive systems with less compression, even an EDU-extractive system that sacrifices grammaticality.", "startOffset": 121, "endOffset": 143}, {"referenceID": 51, "context": "method of Yoshida et al. (2014), which matches", "startOffset": 10, "endOffset": 32}, {"referenceID": 6, "context": "Table 2: Results for RST Discourse Treebank (Carlson et al., 2001).", "startOffset": 44, "endOffset": 66}, {"referenceID": 6, "context": "Table 2: Results for RST Discourse Treebank (Carlson et al., 2001). Differences between our system and the Tree Knapsack system of Yoshida et al. (2014) are not statistically significant, consistent with a high variance in this small (20 document) test set.", "startOffset": 45, "endOffset": 153}, {"referenceID": 21, "context": "Following Hirao et al. (2013), we use the gold EDU segmentation from the RST corpus but automatic RST trees.", "startOffset": 10, "endOffset": 30}, {"referenceID": 51, "context": "The system of Yoshida et al. (2014) is unavailable, so we use a reimplementation.", "startOffset": 14, "endOffset": 36}], "year": 2016, "abstractText": "We present a discriminative model for single-document summarization that integrally combines compression and anaphoricity constraints. Our model selects textual units to include in the summary based on a rich set of sparse features whose weights are learned on a large corpus. We allow for the deletion of content within a sentence when that deletion is licensed by compression rules; in our framework, these are implemented as dependencies between subsentential units of text. Anaphoricity constraints then improve cross-sentence coherence by guaranteeing that, for each pronoun included in the summary, the pronoun\u2019s antecedent is included as well or the pronoun is rewritten as a full mention. When trained end-to-end, our final system1 outperforms prior work on both ROUGE as well as on human judgments of linguistic quality.", "creator": "TeX"}}}