{"id": "1603.09233", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs", "abstract": "we describe and study a model for an automated online recommendation system ( aors ) in which a user's preferences can be time - dependent and can also depend on the history of past recommendations and play - outs. the three key features of viewing the model that makes it more fully realistic compared to existing network models for recommendation systems are ( 1 ) user preference is inherently latent, ( 2 ) current recommendations can affect future preferences, and ( 3 ) it theoretically allows for the development of learning algorithms with provable performance guarantees. the problem is cast as an average - cost restless multi - armed bandit for a given user, with an independent partially constrained observable markov decision process ( pomdp ) for each item of content. we analyze the pomdp for a single arm, describe its structural properties, and characterize its optimal policy. we then develop a thompson sampling - based online reinforcement learning inference algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous cost recommendations. we then generally analyze the performance of the learning algorithm and characterize predicting the regret. illustrative numerical results and directions for extension to the restless fast hidden markov multi - armed bandit problem are also presented.", "histories": [["v1", "Wed, 30 Mar 2016 14:58:32 GMT  (204kb)", "http://arxiv.org/abs/1603.09233v1", "8 pages, submitted to conference"]], "COMMENTS": "8 pages, submitted to conference", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rahul meshram", "aditya gopalan", "d manjunath"], "accepted": false, "id": "1603.09233"}, "pdf": {"name": "1603.09233.pdf", "metadata": {"source": "CRF", "title": "Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs", "authors": ["Rahul Meshram", "Aditya Gopalan"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n09 23\n3v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 6\nI. INTRODUCTION\nAutomated online recommendation (AOR) systems for different types of content aim to adapt to user\u2019s preferences and issue targeted recommendations for content for which the user is estimated to have a higher preference. In the generation of these recommendations, user behavior is typically modeled as a fixed, stochastic response governed by the preference, or taste, of the user for each specific item that has been recommended for consumption. However, in most AOR systems, the dynamic aspects of the response of the user to recommended content are not modeled or investigated. For example, consider an AOR for music. It is reasonable to assume that for some items, there will be short-term user fatigue for a song that has been just been recommended and played out. In this case, the user\u2019s preference for the item drops sharply immediately after consumption and rises with time subsequently. Hence, for such items it is ideal to allow for an interval of time before recommending the item again. It is also possible that the opposite is true\u2014the user\u2019s appetite is whetted with each successive recommendation of a particular item. Capturing such response dynamics could involve assigning a notion of state to the user\u2019s preference for each item at the time of the choosing the recommendations. This state should in turn\nRahul Meshram and D. Manjunath are with the Electrical Engineering Department of IIT Bombay in Mumbai INDIA. Aditya Gopalan is with the Electrical Communication Engineering Department of Indian Institute of Science in Bangalore INDIA. The work of Rahul Meshram and D. Manjunath was carried out in the Bharti Centre for Communications at IIT Bombay. D. Manjunath is also supported by grants from CEFIPRA and DST.\ndepend on the history of the recommendations or play-outs for the item.\nThere are several technical challenges in capturing or estimating the time dependent user preference to an item. Firstly, even for a recommended item, the user\u2019s taste or preference for an item is never directly observed; only a binary response in the form of like/dislike, or play/skip, depending on the preference at that time is available. Thus the actual preference is a latent quantity which needs to be inferred and tracked continuously. This motivates the use of a hidden Markov model for the state of a user with respect to an item\u2014the state captures the instantaneous preference and the response depends in a stochastic manner on the state. The AOR observes the response but not the state. The second challenge is that the act of recommending content to the user itself changes the user\u2019s state of mind (e.g., fatigued, stimulated) which in turn influences the user\u2019s responses to future content. This means that the model should allow for action dependent transitions between the states. A third challenge is adapting to the heterogeneity among users\u2014two users may have not only different propensities towards a content item but also different rates at which they react dynamically to recommendations. This in turn necessitates learning the associated state transition models under uncertainty of not knowing what state induced a response.\nThis paper frames the problem of optimal recommendation under user adaptation and uncertainty as learning a stylized average-cost partially observable Markov decision process (POMDP). For a given user, an independent POMDP is associated with each item. The state of the POMDP expresses either a high (state 1) interest or a low (state 0) interest of the user for the item. In each step, the AORS recommends one item and the user response for the item is determined by the state. This binary response is also available to the AORS. The states of each of the POMDPs changes accordingly as the the item is recommended or not recommended. Thus the AORS can be seen to be a restless hidden Markov multi-armed bandit. In this paper we develop this model and describe a Thompson sampling mechanism to learn the parameters of the model using the response for each recommendation. Specifically, our contributions in this paper are as follows.\n1) Formulate a POMDP-based model for each item in the database of a recommendation system by incorporating user adaptation, hidden state and uncertainty in model. The AOR itself is modeled as an average cost restless hidden Markov multi-armed bandit. 2) Analyze the structure of the single-armed POMDP and show that the optimal policy is of single-threshold type\n2 in the belief. A consequence of the single threshold is that the optimal policy has a cyclic form with a recommendation step followed by k no-recommendation steps. The optimal k is derived.\n3) Devise a natural online learning algorithm based on Thompson sampling (TS) for optimizing reward in the POMDP with no knowledge of the model parameters. 4) Derive what is, to our knowledge, the first known regret bounds for TS for online learning in a class of POMDPs."}, {"heading": "A. Related Work", "text": "Multi-armed bandit models for recommendation systems and for online advertising have been modeled as contextual bandits, e.g., [1]\u2013[3] and the user interests are assumed to be independent of the recommendation history, i.e., they have static models of reward. There are several models for restless multi-armed bandits that use state transitions and state-based rewards with applications in dynamic spectrum access, e.g., [4]\u2013[6]. Such models assume (1) perfect observation of the state when the arm is sampled, and (2) state transitions being independent of/unrelated to actions. There is some work in modeling changing rewards in multi-armed bandits, e.g., [7] but it is again in the fully observable state case, thus circumventing the critical problem of state uncertainty arising in user adaptation. Other approaches towards handling user reactions to recommendations have considered algorithms that use a finite sequence of past user responses as a basis for deciding the current recommendation, e.g., [8]; but these are primarily numerical studies. A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10]. In [10] it was shown that the such a system is approximately Whittle-indexable. The restless bandit that we propose in this paper is a special case of that from [10] for which we obtain much stronger results and also a Thompson sampling-based algorithm to learn the parameters of the arms.\nThe rest of the paper is organized as follows. In the next section we describe the model and set up notation. In Section III we analyze the structural properties of the averagecost POMDP corresponding to a single arm. In Section IV a Thompson sampling based algorithm is described to learn the parameters of the POMDP based on the observed reward. In Section V we analyse the regret as compared to the optimal policy. We conclude with some illustrative numerical results and a discussion on extension to the multi-armed bandit case."}, {"heading": "II. MODEL DESCRIPTION AND PRELIMINARIES", "text": "The AOR system is modeled as a restless multi-armed bandit with arm i representing the state of the user with respect to item i. Each arm is modeled as an independent partially observable Markov decision processes, with two states and two actions. We first describe the model for a single generic arm. S = {0, 1} is the set of states with 0 corresponding to a low interest and 1 corresponding to high interest. A = {0, 1} is the set of actions with 1 for sampling the arm and 0 for not sampling the arm. r : S \u00d7A \u2192 R is the reward function with\n0 1(1\u2212 q)\nq\n1\nReward: \u03bb Reward: \u03bb\nState transitions and rewards when At = 0.\n0 11\n1\nReward: 1 w.p. \u03c1 Reward: 1 w.p. 1\nState transitions and rewards when At = 1.\nFig. 1. Single-arm POMDP. The state transitions and rewards for when the item is recommended and when it is not recommended.\nR = {0, 1, \u03bb} being the set of rewards. Time progresses in discrete steps indexed by t = 1, 2, 3, . . . A step is an instant at which a recommendation can potentially be made by the AORS. Xt denotes the state of the arm at the start of time step t, At is the action played at time t and and Rt is the reward obtained at time t.\nThe reward structure for the POMDP is as follows. A unit reward is obtained with probability 1 if At = Xt = 1. This corresponds to recommending the item when the user\u2019s interest is high. A unit reward is obtained with probability \u03c1 if At = 1 and Xt = 0, i.e., if an item is recommended and the interest is low. A reward \u03bb is obtained independent of Xt for At = 0, i.e., when the item is not recommended.\nRemark 1. A restless multi-armed bandit is typically analyzed by first analyzing the single arm case with a reward of \u03bb for At = 0. \u03bb is called the subsidy for not sampling. Such an analysis is used to determine if the Whittle index based policy can be used to optimally choose the arm at each time step. This analysis is also used to calculate the Whittle-index for an arm.\nIf At = 0, then 0 \u2192 1 transitions occur with probability q and 1 \u2192 0 transitions with probability 0. This corresponds to an increasing interest as the time progresses since the last recommendation. If At = 1, then 0 \u2192 0 and 1 \u2192 0 transitions happen with probability 1 corresponding to a decreased interest after a recommendation. Fig. 1 illustrates the preceding discussion in the form of a state transition diagram.\nRecall that the actual state (0 or 1) is never observed. Let \u03c0t = Pr (Xt = 0) , denote the belief about the state of the arm at the beginning time t. Let Ht denote the history of actions and rewards up to time t. Let \u03c6t : Ht \u2192 {0, 1} be the strategy that determines the action at time t. For a strategy \u03c6 and an initial belief \u03c0 at time t = 0 (i.e., Pr (X0 = 0) = \u03c0), the\n3 expected finite horizon reward function is\nV \u03c6T (\u03c0) = E \u03c6\n[\nT\u22121 \u2211\nt=0\nr(Xt, At)\n\u2223 \u2223 \u2223 \u2223 \u03c00 = \u03c0\n]\n= E\u03c6\n[\nT\u22121 \u2211\nt=0\n[\u03c0t\u03c1+ (1\u2212 \u03c0t)1]\n\u2223 \u2223 \u2223 \u2223 \u03c00 = \u03c0\n]\n.\nThe long term average reward is defined as V \u03c6(\u03c0) = limT\u2192\u221e 1 T V \u03c6T (\u03c0).\nIn the next section we assume that (q, \u03c1), are known and determine the policy \u03c6 that maximizes V \u03c6(\u03c0)."}, {"heading": "III. SINGLE ARM: OPTIMAL POLICY", "text": "We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]\u2014by first considering a discounted reward system and then taking limits as the discount vanishes. The infinite horizon discounted reward under policy \u03c6 and discount \u03b2, 0 < \u03b2 < 1, is\nV \u03c6\u03b2 (\u03c0) := E \u03c6\n[\n\u221e \u2211\nt=1\n\u03b2t\u22121 (\na\u03c6t (\u03c0t\u03c1+ (1 \u2212 \u03c0t))+\n(1\u2212 a\u03c6t )\u03bb )\n\u2223 \u2223 \u2223 \u2223 \u03c00 = \u03c0 ] . (1)\nFrom [10], we can show that the following dynamic program solves (1).\nV\u03b2(\u03c0) = max {\u03bb+ \u03b2V\u03b2((1\u2212 q)\u03c0), 1\u2212 \u03c0(1\u2212 \u03c1) + \u03b2V\u03b2(1)} (2)\nFurther, we can state the following about V\u03b2(\u03c0).\nLemma 1. 1) Equation (2) has a unique solution V\u03b2(\u03c0). Further, V\u03b2(\u03c0) is continuous and bounded.\n2) V\u03b2(\u03c0) is convex non-increasing in \u03c0 and increasing in \u03b2. 3) \u2223 \u2223V\u03b2(\u03c01) \u2212 V\u03b2(\u03c02) \u2223\n\u2223 < (1 \u2212 \u03c1) for all \u03c0 \u2208 [0, 1] and \u03b2 \u2208 [0, 1). 4) The optimal policy is of threshold type with a single threshold for \u03b2 \u2208 [0, 1) and \u03bbL \u2264 \u03bb \u2264 \u03bbH .\nThe first two above follow directly from [10] and the last two are derived in [13].\nDefine V \u03b2 := V\u03b2(\u03c0) \u2212 V\u03b2(1) for \u03c0 \u2208 [0, 1]. From (2), we get\nV \u03b2 + (1\u2212 \u03b2)V\u03b2(1) = max { \u03bb+ \u03b2V \u03b2((1 \u2212 q)\u03c0),\n1\u2212 \u03c0(1\u2212 \u03c1)} (3)\nFrom Lemma 1, V \u03b2(\u03c0) is convex monotone in \u03c0 and by definition V \u03b2(1) = 0. Further, from the lemma we know that there is a constant C < \u221e such that \u2223\n\u2223V\u03b2(\u03c0) \u2212 V\u03b2(1) \u2223\n\u2223 < C. This implies that V \u03b2(\u03c0) is bounded and Lipschitz-continuous. Finally, (1 \u2212 \u03b2)V\u03b2(\u03c0) is also bounded. Hence we can apply the Arzela-Ascoli theorem [14], to find a subsequence (V \u03b2k(\u03c0), (1 \u2212 \u03b2)V\u03b2k(\u03c0)) that converges uniformly to (V (\u03c0), g) as \u03b2k \u2192 1. Thus, as \u03b2k \u2192 1, along an appropriate subsequence, (3) reduces to\nV (\u03c0) + g = max {\u03bb+ V ((1\u2212 q)\u03c0), 1 \u2212 \u03c0(1 \u2212 \u03c1)} , (4)\nfor all \u03c0 \u2208 [0, 1]. (4) is the dynamic programming equation whose solution gives us the optimal value function that maximizes the average reward.\nSince V (\u03c0) inherits the structural properties of V\u03b2(\u03c0), we have, from Lemma 1, that\nLemma 2. 1) V (\u03c0) is monotone non-increasing and convex in \u03c0.\n2) The optimal policy is of threshold type with a single threshold for \u03bbL \u2264 \u03bb \u2264 \u03bbH .\nThis in turn leads us to the following theorem which is a direct analog of Theorem 6.17 in [11].\nTheorem 1. If there exists a bounded function V (\u03c0) for \u03c0 \u2208 [0, 1] and a constant g that satisfies (4), then there exists a stationary policy \u03c6\u2217 such that\ng = max \u03c6 lim T\u2192\u221e\n1 T V \u03c6T (\u03c0) (5)\nfor all \u03c0 \u2208 [0, 1], and moreover, \u03c6\u2217 is the policy for which the RHS of (4) is maximized."}, {"heading": "A. An Equivalent Form for the Optimal Policy", "text": "For the single-armed bandit, the threshold policy of Lemma 2 can be interpreted as follows. Let \u03c0T be the threshold such that the optimal policy is At = 1 if \u03c0t \u2264 \u03c0T and At = 0, if \u03c0t > \u03c0T . We know that if At = 1, then \u03c0t+1 = 1, i.e., if the item is recommended then the state becomes 0. When the item is not recommended, the belief about state 0 decreases by a factor of (1 \u2212 q). Since there is a single threshold and \u03c0t monotonically decreases every time the item is not recommended, the optimal policy will be to wait for k steps before recommending again, where k is the first time that \u03c0t has crossed \u03c0T . This value k is a function of q and \u03c1 and will be denoted by kpt(q, \u03c1).\nWe first consider infinite horizon discounted reward problem. In this case, solving (1) is equivalent to solving following optimization problem.\nk\u03b2,opt(q, \u03c1) = argmax k\u22651 V\u0303\u03b2(k), (6)\nwhere V\u0303\u03b2(k) is the value function obtained by not recommending for k steps between successive recommendations. We can write\nV\u0303\u03b2(k) := { \u03bb+ \u03bb\u03b2 + \u03bb\u03b22 + \u00b7 \u00b7 \u00b7+ \u03bb\u03b2k\u22121\n+\u03b2k [ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]}\n+\u03b2k+1 { \u03bb+ \u03bb\u03b2 + \u03bb\u03b22 + \u00b7 \u00b7 \u00b7+ \u03bb\u03b2k\u22121\n+\u03b2k [ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]}\n+\u03b22(k+1) { \u03bb+ \u03bb\u03b2 + \u03bb\u03b22 + \u00b7 \u00b7 \u00b7+ \u03bb\u03b2k\u22121\n+\u03b2k [ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]} + \u00b7 \u00b7 \u00b7\nLet Ck denote the reward from the first one cycle of k steps with no recommendations for (k \u2212 1) steps and a\n4 recommendation in the k-th step. We can write\nCk := \u03bb+ \u03bb\u03b2 + \u03bb\u03b2 2 + \u00b7 \u00b7 \u00b7+ \u03bb\u03b2k\u22121\n+\u03b2k [ (1 \u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]\n= \u03bb (1\u2212 \u03b2k)\n(1 \u2212 \u03b2) + \u03b2k\n[ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]\nThe first k\u22121 terms above correspond to the reward from not sampling and the kth term denotes the reward from sampling. Thus, V\u0303\u03b2(k) can be rewritten as follows.\nV\u0303\u03b2(k) = Ck [ 1 + \u03b2k+1 + \u03b22(k+1) + \u00b7 \u00b7 \u00b7 ]\n= 1\n1\u2212 \u03b2k+1\n[\n\u03bb (1\u2212 \u03b2k)\n(1\u2212 \u03b2) + \u03b2k\n[ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]\n]\nThe preceding discussion gives us the following result on the value function and the optimal policy for the average reward criterion POMDP.\nTheorem 2. 1) The value function for policy k is\nV\u0303 (k) = lim \u03b2\u21921 (1 \u2212 \u03b2)V\u0303\u03b2(k)\n= 1\nk + 1\n[ \u03bbk + [ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]] .\n2) The optimum policy kopt(q, \u03c1) satisfies\nkopt(q, \u03c1) = argmax k\u22651 V\u0303 (k)\n= argmax k\u22651\n1\nk + 1\n[ \u03bbk + [ (1\u2212 q)k\u03c1+ 1\u2212 (1\u2212 q)k ]] (7)\nThus, for the single armed bandit, given q, \u03c1 and \u03bb, we obtain the optimal policy as the number of steps to wait before recommending the item again. In the next section we describe the Thompson sampling algorithm to learn the parameters based on the reward that is observed. Subsequently, we analyze the regret from the learning process. We remind the reader that the state is never observed in the system and the learning is based only on rewards."}, {"heading": "IV. THOMPSON SAMPLING LEARNING ALGORITHM", "text": "We have seen that the optimal policy for the (singlearm) POMDP described by q and \u03c1 is of threshold type (Section III-A), and corresponds to waiting for kopt(q, \u03c1) steps in between successive recommendations. However, when the parameters q, \u03c1 that describe the Markov chain transition probabilities are unknown a priori1, they must be learnt or inferred from the available feedback in order to attain maximum cumulative reward. This section describes an online algorithm that learns to play the optimal policy using experience, i.e., observations from previously played actions, while at the same time keeping the net reward as high as possible (the exploreexploit problem).\nThe learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes\n1as in an AOR system where user behavior is unknown at start\n(MDPs) [17]\u2013[19] and POMDPs. It works in epochs, where an epoch is defined to be the interval of time from an instant at which the POMDP is sampled (action 1 is played) up until the next instant at which it is sampled again. At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]\u00d7 [0, 1] containing all possible POMDPs parameterized by (q, \u03c1). At the start of each epoch \u2113 \u2265 1, a model (q\u2113, \u03c1\u2113) is randomly and independently sampled according to the current prior over X (this random draw is crucial in inducing exploration over the model space). Then, the optimal policy for this sampled model is computed, which by the previous results corresponds to sampling the chain after an interval of kopt(q\u2113, \u03c1\u2113) time instants3. This policy is now applied for one cycle, i.e., the algorithm waits for the specified interval of time instants, samples the chain at the next time instant, and obtains an observation for the sampled instant (a Bernoulli-distributed reward). The observed reward is used to update the prior over models via Bayes\u2019 rule, the epoch ends, and the next epoch starts with the updated prior. Notation. In Algorithm 1, B(X ) denotes the Borel \u03c3-algebra of X \u2282 Rd. Pr ( R = r \u2223 \u2223 (q, \u03c1), k )\ndenotes the likelihood, under the POMDP model specified by parameters (q, \u03c1), of observing a reward of r \u2208 {0, 1} upon sampling the Markov chain after having not sampled the chain for exactly k previous time instants. Specifically, we have\nAlgorithm 1: Thompson sampling algorithm for learning the optimal policy\nInput: Parameter space X \u2286 [0, 1]2, Policy space K \u2282 {0, 1, 2, . . .}, Observation space R = {0, 1}, Prior probability distribution Z1 over (X ,B(X )) for epoch \u2113 = 1, 2, . . . do\nSample (q\u2113, \u03c1\u2113) \u2208 X according to the probability distribution Z\u2113 Compute the optimal policy k\u2113 = kopt(q\u2113, \u03c1\u2113) for sampled parameters Apply the policy k\u2113 once: wait for the next (k\u2113 \u2212 1) time steps and sample the Markov chain at the k\u2113-th time instant Observe reward on sampling, denote it by R\u2113 \u2208 {0, 1} Update the current prior over (q, \u03c1) to\nZ\u2113+1(B) :=\n\u222b\nB Pr\n( R = R\u2113 \u2223 \u2223 (q, \u03c1), k\u2113 )\nZ\u2113(q, \u03c1)dq d\u03c1 \u222b\nX Pr\n( R = R\u2113 \u2223 \u2223 (q, \u03c1), k\u2113 ) Z\u2113(q, \u03c1)dq d\u03c1\nfor any Borel set B \u2208 B(X ). end for\nPr ( R = r \u2223 \u2223 (q, \u03c1), k ) =\n{\nf(q, \u03c1, k) if r = 1\n1\u2212 f(q, \u03c1, k) if r = 0,\n2Note that the prior used in Thompson sampling is merely a parameter of the algorithm (e.g., the uniform measure over a compact domain), carefully designed to induce random exploration, and is not related to any Bayesian modeling assumptions on the true model as such.\n3This could be carried out using either standard planning methods such as value/policy iteration or exhaustive search over threshold-type policies.\n5 where f(q, \u03c1, k) is simply the probability of observing a reward of 1 after having waited for k time steps since the last sample, when the parameters are (q, \u03c1). It follows that\nf(q, \u03c1, k) = (1\u2212 q)k\u03c1+ [ 1\u2212 (1\u2212 q)k ] ."}, {"heading": "V. MAIN RESULT \u2013 REGRET BOUND", "text": "In this section, we show an analytical performance guarantee for Algorithm 1.\nTo this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21]. The regret of a strategy, for the POMDP described by (q\u2217, \u03c1\u2217), is the difference between the cumulative reward which the optimal policy4 kopt(q\u2217, \u03c1\u2217) earns when run from a fixed initial state for a fixed time horizon T , and that which the strategy earns with the same initial state and time horizon. Formally, the regret for a strategy A is the random variable\nRA(q\u2217,\u03c1\u2217)(T ) :=\nT\u22121 \u2211\nt=0\nr ( Xt, A kopt(q \u2217,\u03c1\u2217) t ) \u2212 T\u22121 \u2211\nt=0\nr ( Xt, A A t ) ,\nwhere Akopt(q \u2217,\u03c1\u2217)\nt (resp. A A t ) represents the action taken by\nkopt(q \u2217, \u03c1\u2217) (resp. A) at time instant t, and it is assumed that the algorithms A and kopt(q\u2217, \u03c1\u2217) are run on independent POMDP instances. The goal is typically to bound the regret of a sequential decision making algorithm as a function of the structure of the POMDP (number of states/actions in the underlying MDP) and show that it grows only sub-linearly with time T (i.e., the per-round regret vanishes), either in expectation or with high probability.\nTowards bounding the regret of the Thompson sampling POMDP algorithm (Algorithm 1), it is convenient to consider a modified version of regret that essentially counts the number of epochs during the operation of the algorithm in which the policy used is not kopt(q\u2217, \u03c1\u2217), or in other words the length of the epoch consisting of no-sampling instants is not kopt(q\u2217, \u03c1\u2217). This corresponds to the quantity\nR\u0303(q\u2217,\u03c1\u2217)(L) :=\nL \u2211\n\u2113=1\n1{k\u2113 6=kopt(q\u2217,\u03c1\u2217)},\ndefined for the first L epochs that the algorithm executes. Note that under the reasonable assumption that an upper bound kmax on kopt(q\u2217, \u03c1\u2217) is available a priori, and if the Thompson sampling algorithm samples at all times parameters (q, \u03c1) for which kopt(q, \u03c1) \u2264 kmax, then the length of each epoch is bounded between 1 and kmax; thus the standard regret R(T ) is bounded in terms of the modified regret R\u0303(L) by a constant factor kmax (note that the maximum possible reward is 1). In order to focus on the order-wise scaling of the regret with time or number of epochs, we henceforth concentrate on bounding the (modified) regret R\u0303(T ), with high probability.\nWe will need the following set of mild assumptions on the structure of the parameter space and the initial prior under which a regret bound holds.\n4We overload notation, when the context is clear, to represent the optimal policy using the optimal waiting time kopt(q\u2217, \u03c1\u2217).\nAssumption 1. (a) The parameter space X \u2286 [\u03b7, 1 \u2212 \u03b7] for some \u03b7 \u2208 (\n0, 12 ) , (b) |X | < \u221e, (c) The true model (q\u2217, \u03c1\u2217) \u2208 X , (d) The prior distribution Z1 over X puts positive mass on the true model, (e) There is a unique (average-reward) optimal policy kopt(q\u2217, \u03c1\u2217) \u2264 kmax for the true model with a known upper bound kmax \u2208 Z.\nTheorem 3 (Main Result \u2013 Thompson sampling regret). Let Assumption 1 hold, and let \u01eb, \u03b4 \u2208 (0, 1). There exists L0 \u2261 L0(\u01eb) such that the following bound holds, for the cumulative regret of Algorithm 1 with initial prior Z1, with probability at least 1\u2212 \u03b4 for all L \u2265 L0:\nR\u0303(q\u2217,\u03c1\u2217)(L) \u2264 B + C(logL),\nwhere B \u2261 B(\u01eb, \u03b4, (q\u2217, \u03c1\u2217),X ) is a problem-dependent constant independent of the number of epochs L, and C \u2261 C(\u03b4, (q\u2217, \u03c1\u2217),X ) is the solution to an optimization problem (P1).\nNote. The optimization problem is described in detail in Appendix A for the sake of clarity. Discussion. Theorem 3 establishes that the regret of Algorithm 1 scales only logarithmically (thus, sub-linearly) with time (or epochs), with high probability, when starting with a \u2018grain-of-truth\u2019 prior that ascribes positive probability to the true model. The algorithm is thus able to achieve a suitable balance between exploring across different sampling policies and exploiting its improving knowledge about the true model (q\u2217, \u03c1\u2217) to keep the regret controlled, in a nontrivial fashion. Moreover, this is achieved in a POMDP model in which the state of the Markov chain is never available at any time instant, but instead only a stochastic reward correlated with the current state is observed that conveys implicit information about the true parameter (q\u2217, \u03c1\u2217). The logarithmic growth of the regret with time is controlled by the quantity C, which depends on Kullback-Leibler (KL) divergences of the distribution of the observations under different models and policies, thus encoding the information structure of the observations.\nThe theorem is proven by following closely the method developed to show [22, Theorem 1 and Proposition 2] and [18, Theorem 1 and 5], namely the strategy of bounding the posterior mass (with high probability) both from below (in a neighborhood of the true model) and from above (outside the neighborhood, for parameters corresponding to suboptimal policies) spelt out in detail in [22, Appendix A]. We describe the derivation of Theorem 3 in Appendix A.\nThe following accompanying result provides more insight into the order-wise scaling of regret. In the following, D(p||q) := p log (\np q\n) + (1 \u2212 p) log (\n1\u2212p 1\u2212q\n)\ndenotes the KL divergence between Bernoulli distributions of parameter p and q, 0 < p, q < 1.\nTheorem 4. Consider L to be large enough so that\nmax (q,\u03c1)\u2208X ,k\u2264kmax\nD(f(q\u2217, \u03c1\u2217, k)||f(q, \u03c1, k)) \u2264 1 + \u01eb\n1\u2212 \u01eb logL.\nThen, there exists \u22062 > 0 such that\nC(logL) \u2264\n(\n1\n\u22062\n)\n2(1 + \u01eb)\n1\u2212 \u01eb logL.\n6 Discussion. Theorem 4 highlights a key property of the regret induced by the information structure of the POMDP problem \u2013 that the regret asymptotically does not scale with the total number kmax of candidate optimal policies, which could be large by itself. This can be contrasted with running a simple multi-armed bandit algorithm such as UCB [23] with the \u2018arms\u2019 being different waiting-duration policies with the duration ranging from 0, 1, . . . , kmax (a total of kmax + 1 arms), and the reward being the reward from applying a single cycle of any such policy while disregarding the POMDP structure entirely. It follows from standard stochastic bandit regret bounds that such an algorithm would achieve regret that scales with the total number of arms, i.e., O(kmax logT ). The advantage of using Thompson sampling with a prior on POMDP structures (q, \u03c1) is that every application of any waiting-time policy provides a non-trivial amount of information (in the sense of the prior-posterior update) about the true POMDP (q\u2217, \u03c1\u2217), and hence about all other policies (in the multi-armed bandit view this is akin to any arm providing reward information about all arms following every pull). The proof of the result is motivated by [22, Proposition 2], and is detailed in the appendix."}, {"heading": "VI. NUMERICAL RESULTS AND DISCUSSION", "text": "We present some numerical results to show the rate of convergence of Algorithm 1 to the optimal policy of the POMDP model, for various configurations of the true model and initial prior. We fix \u03bb = 0.3 in all of the simulations and consider four combinations of the true models (q\u2217, \u03c1\u2217); (1) (0.05,0.25), (2) (0.05,0.15), (3) (0.05,0.35), and (4) (0.15,0.35). For each of these four values of the true parameters, we present two performance measures as a function of the time step\u2014the regret and also probability mass on the true values. These values are average from 300 runs of the simulation. The first set of plots (shown in Fig. 2) are obtained by discretising the parameter space coarsely into a (5 \u00d7 5) grid at (0.05, 0.15, 0.25, 0.35, 0.45) and starting with the uniform a distribution on the 25 points. The second set of plots (shown in Fig. 3) is obtained by using a finer 10 \u00d7 10 grid at (0.05, 0.10, . . . , 0.50).\nWe observe that true model and the initial prior (i.e., supported on the coarse/fine grid) both affect the convergence rate of regret and probability distribution of true model. The effect of the prior on the finer grid is to increase the overall regret and slow down the convergence to the true model. This is presumably due to (a) the fact that imposing a prior over a fairly coarse grid is equivalent to providing a large amount of side information about the true model (i.e., that it must be one of a small set of models), and on a related note, (b) the presence of confounding or competing models that are closer to it than in the coarse grid prior, which must be eliminated to achieve low regret (this is analogous to the phenomenon of smaller \u2018gap\u2019 in multi-armed bandits leading to higher regret)."}, {"heading": "A. Discussion and Directions \u2013 Multi-armed bandit case", "text": "We formulated the problem of optimizing recommendations for a single user, whose taste in a certain item changes\nwith recommendations, as online learning in a two-state, twoparameter POMDP. Using this approach, we developed and analyzed the performance of a natural Thompson-sampling algorithm for learning the optimal policy for a user-item pair. A logical next step in this investigation is to treat the multi-armed bandit version of the problem with multiple independently evolving POMDPs, each representing different users/items, and a resource constraint on which users/items can be activated at any instant, e.g., decide which of several items is to be shown to a user at a certain time, given that the user remembers how far ago she consumed a certain item and may respond accordingly to item recommendations.\nThe Thompson sampling based algorithm proposed in this work could be extended to cover the multi-armed bandit case by jointly sampling parameters for all POMDPs, computing the optimal refresh rate for each of them, and scheduling the recommendations accordingly while at the same time updating its current prior to incorporate observations. This opens up new avenues for analysis of performance for such algorithms, and we plan to pursue it as part of future work."}, {"heading": "A. Proof of Theorem 3", "text": "We sketch how the proof of the result can be adapted from that of [22, Theorem 1]; due to space constraints the reader is referred to [18], [22] for precise estimates and details. We first define the decision regions based on KL-divergence for each policy k. Let Sk := {(q, \u03c1) \u2208 X : kopt(q, \u03c1) = k} be the collection of all models (q, \u03c1) \u2208 X for which the optimal policy is k. Denote k\u2217 := kopt(q\u2217, \u03c1\u2217), let \u01eb > 0, and define the following sub-decision regions \u2200k 6= k\u2217:\nS \u2032 k := S \u2032 k(\u01eb) = {(q, \u03c1) \u2208 Sk : D (f(q \u2217 , \u03c1 \u2217 , k \u2217)||f(q, \u03c1, k\u2217)) \u2264 \u01eb}\nS \u2032\u2032 k := Sk \\ S \u2032 k = {(q, \u03c1) \u2208 Sk : D (f(q \u2217 , \u03c1 \u2217 , k \u2217)||f(q, \u03c1, k\u2217)) > \u01eb} .\nLet Nk(l) = \u2211l\ni=1 1{(qi,\u03c1i)\u2208Sk} be the number of times up to and including epoch l for which the policy employed by Algorithm 1 is k. Also, Nk(l) = \u2211l\ni=1 1{(qi,\u03c1i)\u2208S\u2032k} +\n\u2211l i=1 1{(qi,\u03c1i)\u2208S\u2032\u2032k } . Define N \u2032 k(l) := \u2211l i=1 1{(qi,\u03c1i)\u2208S\u2032k} and\nN \u2032\u2032 k (l) := \u2211l\ni=1 1{(qi,\u03c1i)\u2208S\u2032\u2032k } . Next, it can be shown that\nthe posterior on S \u2032\u2032\nk decays exponentially with t, leading to a negligible, i.e., O(1), regret from S \u2032\u2032\nk . To obtain posterior distribution on S \u2032\nk to be small, we need \u2211kmax\nk Nk(l)D (f(q \u2217, \u03c1\u2217, k)||f(q, \u03c1, k)) \u2248 logL. This mean that suboptimal models are sampled as long as their posterior probability mass is greater than 1\nL . If the posterior probability\nof a model parameter is less than 1 L , then the number of times that parameter sampled up to epoch L is O(1) and this is negligible compared to regret. It is thus enough to bound the maximum amount of time that the posterior probability of any S \u2032\nk, k 6= k \u2217, can stay above 1/L, when non-trivial regret\nis incurred. We now define N \u2032 (l) := ( N \u2032 k(l) ) , at l \u2265 0, N \u2032\n(0) = (0, \u00b7 \u00b7 \u00b7 , 0). The policy k is eliminated when all its model losses exceed logL. Let \u03c41 be the first time when some policy k1 is eliminated, k1 6= k\u2217. The play count of policy k1 fixed at N \u2032\nk1 (\u03c41) for remaining horizon up to L. Next \u03c42 \u2265 \u03c41\nwhen policy k2 /\u2208 {k\u2217, k1} is eliminated and play count of k2 fixed at N \u2032\nk2 (\u03c42). This process goes on until all subop-\ntimal policies eliminated. N \u2032 (\u03c4i) = ( N \u2032 k(\u03c4i) ) {k=1,\u00b7\u00b7\u00b7kmax} is play count vector of all policies at time \u03c4i. Let Yi := N \u2032 (\u03c4i) = ( N \u2032 k(\u03c4i) )\n{k=1,\u00b7\u00b7\u00b7kmax} . Since the play count of\npolicy ki fixed at N \u2032\nki (\u03c4i) for remaining horizon, we have\nconstraints Yi(kj) = Yj(kj), for i \u2265 j. That means plays of policy kj do not occur after time \u03c4j . Let D(f(q, \u03c1)) := (D (f(q\u2217, \u03c1\u2217, k)||f(q, \u03c1, k))){k=1,\u00b7\u00b7\u00b7kmax} is a vector of the marginal Kullback-Leibler divergences for all policies. As the policy ki eliminated at time \u03c4i, this translates into the following problem: min(q,\u03c1)\u2208S\u2032\nki\n\u3008Yi, D(f(q, \u03c1))\u3009 \u2265 1+\u01eb 1\u2212\u01eb logL,\nwhere \u3008x, y\u3009 denotes the standard inner product in Euclidean space. We summarize the discussion on the elimination of suboptimal policies in the following constrained optimization\n8 problem that depends on the marginal KL divergences.\nC(logL) :=\nmax\nkmax\u22121 \u2211\ni=1\nYi(ki)\ns.t. Yi \u2208 R kmax + , i = 1, \u00b7 \u00b7 \u00b7 , kmax \u2212 1\nYi(kmax) = 0, k = 1, \u00b7 \u00b7 \u00b7 , kmax \u2212 1 Yi \u2265 Yj , i \u2265 j, j = 1 \u00b7 \u00b7 \u00b7 , kmax \u2212 1 Yi(j) = Yj(j), i \u2265 j, j = 1, \u00b7 \u00b7 \u00b7 , kmax \u2212 1 \u03c3 : {1, \u00b7 \u00b7 \u00b7 , kmax \u2212 1} \u2192 {1, \u00b7 \u00b7 \u00b7 , kmax} \u2212 {k \u2217} injective\nmin (q,\u03c1)\u2208S \u2032\n\u03c3(i)\n\u3008Yi, D(f(q, \u03c1))\u3009 = 1 + \u01eb\n1\u2212 \u01eb logL,\ni = 1, \u00b7 \u00b7 \u00b7 , kmax \u2212 1. (P1)"}, {"heading": "B. Preliminary Results Towards Proving Theorem 4", "text": "We collect here some useful assertions towards showing the result. We first note that the variation distance provides a lower bound on KL-divergence and it is given as\nD(f(q\u2217, \u03c1\u2217, k)||f(q, \u03c1, k)) \u2265 1\n2 ln 2 d 2(f(q\u2217, \u03c1\u2217, k), f(q, \u03c1, k))\n\u2265 1\nln 2 dk(q, \u03c1) (8)\nHere, d(f(q\u2217, \u03c1\u2217, k), f(q, \u03c1, k)) is variation distance between f(q\u2217, \u03c1\u2217, k) and f(q, \u03c1, k) and this is described as follows.\nd(f(q\u2217, \u03c1\u2217, k), f(q, \u03c1, k)) = 2 \u2223 \u2223f(q\u2217, \u03c1\u2217, k)\u2212 f(q, \u03c1, k) \u2223 \u2223\nWe can rewrite \u2223 \u2223f(q\u2217, \u03c1\u2217, k)\u2212 f(q, \u03c1, k) \u2223 \u2223 2 as follows.\n\u2223 \u2223f(q\u2217, \u03c1\u2217, k)\u2212 f(q, \u03c1, k) \u2223 \u2223 2 =\n[\nqk(\u03c1\u2212 1)\u2212 q\u2217 k (\u03c1\u2217 \u2212 1)\n]2\n,\nwhere q = 1 \u2212 q, and q\u2217 = 1 \u2212 q\u2217. Define dk(q, \u03c1) := [ qk(\u03c1\u2212 1)\u2212 q\u2217 k (\u03c1\u2217 \u2212 1) ]2 , and d(q, \u03c1) :=\n[d1(q, \u03c1), \u00b7 \u00b7 \u00b7 , dkmax(q, \u03c1)] . We need the following series of lemmas to prove Theorem 4.\nLemma 3. For every \u01eb > 0, there exists \u03b4 > 0 such that if (q, \u03c1) and (q\u2217, \u03c1\u2217) sufficiently away and D(f(q\u2217, \u03c1\u2217, k\u2217)||f(q, \u03c1, k\u2217)) \u2264 \u01eb then D(f(q\u2217, \u03c1\u2217, k)||f(q, , \u03c1, k)) \u2265 \u03b4.\nProof: Since (q, \u03c1) is sufficiently away from (q\u2217, \u03c1\u2217), the difference |qk \u2212 q\u2217 k | will be positive for all policies k = 1, 2, \u00b7 \u00b7 \u00b7 , kmax. We set\n\u03b41 := min k\n[\nqk(\u03c1\u2212 1)\u2212 q\u2217 k (\u03c1\u2217 \u2212 1)\n]2\n> 0.\nThen, using inequality in (8), we obtain\nD(f(q\u2217, \u03c1\u2217, k)||f(q, \u03c1, k)) > 1\nln 2 \u03b41 > \u03b4,\nwhere \u03b4 = 1ln 2\u03b41. This completes the proof.\nLemma 4. One can find \u01eb > 0 such that it can not happen that there exists (q, \u03c1) /\u2208 N\u01eb1(q \u2217, \u03c1\u2217) and k, k \u2032 , k 6= k \u2032\nfor which dk(q, \u03c1) \u2264 \u01eb and dk\u2032 (q, \u03c1) \u2264 \u01eb.\nProof: Suppose dk(q, \u03c1) = dk\u2032 (q, \u03c1) = 0, then we will have\nqk\nq\u2217 k\n= 1\u2212 \u03c1\u2217\n1\u2212 \u03c1 =\nqk \u2032\nq\u2217 k \u2032\n(9)\nThis implies that qk\u2212k \u2032 = q\u2217 k\u2212k\n\u2032\nNow observe that when k 6= k \u2032\nand q is not in neighborhood of q\u2217, so equality (9) is not true. This means that our assumption dk(q, \u03c1) = dk\u2032 (q, \u03c1) = 0 is not true. Further, it implies that only one of the following claim is true.\n1) if dk(q, \u03c1) = 0, then dk\u2032 (q, \u03c1) > 0 for k 6= k \u2032 2) if dk\u2032 (q, \u03c1) = 0 then dk(q, \u03c1) > 0 for k 6= k \u2032 .\nIn other word, we can find \u01eb > 0 for which either dk(q, \u03c1) \u2264 \u01eb, dk\u2032 (q, \u03c1) > \u01eb is true, or dk\u2032 (q, \u03c1) \u2264 \u01eb dk(q, \u03c1) > \u01eb is true.\nLemma 5. Consider any parameter (q, \u03c1) 6= (q\u2217, \u03c1\u2217) and (q, \u03c1) /\u2208 N\u01eb1(q \u2217, \u03c1\u2217), where N\u01eb1(q \u2217, \u03c1\u2217) is \u01eb1 neighborhood of (q\u2217, \u03c1\u2217). Then there exists an integer \u03ba \u2208 {1, 2, 3, \u00b7 \u00b7 \u00b7 , kmax\u2212 1} and \u2206 > 0 such that for all (q, \u03c1) /\u2208 N\u01eb1(q\n\u2217, \u03c1\u2217) : \u2223 \u2223{k : dk(q, \u03c1) \u2265 \u2206} \u2223 \u2223 \u2265 \u03ba. (10)\nAlso, for sufficiently small \u2206 > 0, we have \u03ba = kmax \u2212 1.\nProof: From Lemma 4, notice that in d(q, \u03c1), there can be at most one element which can be zero or arbitrary close zero, say, dl(q, \u03c1) \u2264 \u01eb and remaining entries, dk(q, \u03c1) > \u01eb, k 6= l. When (q, \u03c1) /\u2208 N\u01eb1(q\n\u2217, \u03c1\u2217), implies |q \u2212 q\u2217| \u2265 \u01eb1 and |\u03c1\u2212 \u03c1\u2217| \u2265 \u01eb1. Thus we obtain\ndk(q, \u03c1) = [ q k(\u03c1\u2212 1)\u2212 q\u2217 k (\u03c1\u2217 \u2212 1) ]2\n\u2265 min k\n[\n(q\u2217 + \u01eb1) k((\u03c1\u2217 + \u01eb1)\u2212 1)\u2212 q\u2217 k (\u03c1\u2217 \u2212 1)\n]2\n\u2206 := min1\u2264k\u2264kmax\n[\n(q\u2217 + \u01eb1) k((\u03c1\u2217 + \u01eb1)\u2212 1) \u2212 q\u2217 k (\u03c1\u2217 \u2212 1)\n]2\n,\nthis \u2206 > 0. Now, combining Lemma 4, and dk(q, \u03c1) \u2265 \u2206, there exists \u03ba \u2208 {1, 2, 3, \u00b7 \u00b7 \u00b7 , kmax\u22121}, such that for (q, \u03c1) /\u2208 N\u01eb1(q\n\u2217, \u03c1\u2217), we have \u2223\n\u2223{k : dk(q, \u03c1) \u2265 \u2206} \u2223 \u2223 \u2265 \u03ba. (11)\nFurther, for sufficiently small \u01eb > 0, and (q, \u03c1) /\u2208 N\u01eb1(q \u2217, \u03c1\u2217), we have kmax \u2212 1 nonzero entries in vector d(q, \u03c1). In this case, fix \u2206 = \u01eb, we obtain \u2223 \u2223{k : dk(q, \u03c1) > \u2206} \u2223\n\u2223 = kmax \u2212 1. Thus, \u03ba = kmax \u2212 1."}, {"heading": "C. Proof of Theorem 4", "text": "From lemma 5, we know that for sufficiently small \u2206 > 0, we have \u03ba = kmax \u2212 1 and \u2223 \u2223{k : dk(q, \u03c1) \u2265 \u2206} \u2223\n\u2223 = kmax \u2212 1. Thus, we have \u22062 = 1ln 2\u2206 > 0 and \u03ba = kmax \u2212 1 such that \u2223 \u2223{k \u2208 K : k 6= k\u2217, D(f(q\u2217, \u03c1\u2217, k)||f(q, \u03c1, k)) \u2265 \u22062} \u2223 \u2223 = kmax \u2212 1\nFrom Lemmas 3, 5 and eqn. (8), we note that all assumptions in [22, Proposition 2] are satisfied. Note that the upper bound on C(logL) is given in [22, Proposition 2] and it is as follows.\nC(logL) \u2264\n(\nkmax \u2212 \u03ba\n\u22062\n)\n2(1 + \u01eb)\n1\u2212 \u01eb logL\nHere, we substitute \u03ba = kmax \u2212 1, and required upper bound on C(logL) follows."}], "references": [{"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "Advances in Neural Information Processing Systems. NIPS, Dec. 2007, pp. 1\u20138.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Leveraging side observations in stochastic bandits", "author": ["S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat"], "venue": "Arxiv, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "Conference on World Wide Web. ACM, April 2010, pp. 661\u2013670.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Indexability of restless bandit problems and optimality of Whittle index for dynamic multichannel access", "author": ["K. Liu", "Q. Zhao"], "venue": "IEEE Transactions Information Theory, vol. 56, no. 11, pp. 5557\u20135567, November 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Exploiting channel memory for joint estimation and scheduling in downlink networks", "author": ["W. Ouyang", "S. Murugesan", "A. Eyrilmaz", "N. Shroff"], "venue": "Proceedings of IEEE INFOCOM, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Network utility maximization over partially observable Markovian channels", "author": ["C. Li", "M.J. Neely"], "venue": "Performance Evaluation, vol. 70, no. 7\u20138, pp. 528\u2013548, July 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimality of myopic policy for a class of monotone affine restless multi-armed bandits", "author": ["P. Mansourifard", "T. Javidi", "B. Krishnamachari"], "venue": "Proceedings of IEEE CDC, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-aware music recommendation based on latent topic sequential patterns", "author": ["N. Hariri", "B. Mobasher", "R. Burke"], "venue": "Proceedings of the Sixth ACM Conference on Recommender ystems (RecSys \u201912), 2012, pp. 131\u2013138.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A restless bandit with no observable states for recommendation systems and communication link scheduling", "author": ["R. Meshram", "D. Manjunath", "A. Gopalan"], "venue": "Proceedings of IEEE CDC, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "On the Whittle index for restless multi-armed hidden markov bandits", "author": ["R. Meshram", "D. Manjunath", "A. Gopalan"], "venue": "Submitted for Publication. Also available on Arxiv:1603.04739, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Applied Probability Models with Optimization Applications", "author": ["S.M. Ross"], "venue": "Dover Publications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Whittle index policy for crawling ephemeral content", "author": ["K. Avrachenkov", "V.S. Borkar"], "venue": "Tech. Rep. Research Report No. 8702, INRIA, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive playlists from hidden markov bandits", "author": ["R. Meshram", "D. Manjunath", "A. Gopalan"], "venue": "Submitted for Publication. Also available arxiv, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Principles of mathematical analysis, McGraw-Hill Book Co., New York, third edition, 1976, International Series in Pure and Applied Mathematics", "author": ["Walter Rudin"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1976}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R. Thompson"], "venue": "Biometrika, vol. 24, no. 3\u20134, pp. 285\u2013294, 1933.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1933}, {"title": "Model-based reinforcement learning and the eluder dimension", "author": ["Ian Osband", "Benjamin V. Roy"], "venue": "Advances in Neural Information Processing Systems 27, 2014, pp. 1466\u20131474.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Thompson sampling for learning parameterized markov decision processes", "author": ["Aditya Gopalan", "Shie Mannor"], "venue": "Conf. on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, 2015, pp. 861\u2013898.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "Proc. NIPS, 2011, pp. 2312\u20132320.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "JMLR, vol. 11, pp. 1563\u20131600, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Thompson Sampling for Complex Online Problems", "author": ["Aditya Gopalan", "Shie Mannor", "Yishay Mansour"], "venue": "Proc. International Conf. on Machine Learning, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": ", [1]\u2013[3] and the user interests are assumed to be independent of the recommendation history, i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": ", [1]\u2013[3] and the user interests are assumed to be independent of the recommendation history, i.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": ", [4]\u2013[6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": ", [4]\u2013[6].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": ", [7] but it is again in the fully observable state case, thus circumventing the critical problem of state uncertainty arising in user adaptation.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [8]; but these are primarily numerical studies.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10].", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": "A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10].", "startOffset": 140, "endOffset": 144}, {"referenceID": 9, "context": "In [10] it was shown that the such a system is approximately Whittle-indexable.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "The restless bandit that we propose in this paper is a special case of that from [10] for which we obtain much stronger results and also a Thompson sampling-based algorithm to learn the parameters of the arms.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "SINGLE ARM: OPTIMAL POLICY We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]\u2014by first considering a discounted reward system and then taking limits as the discount vanishes.", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "SINGLE ARM: OPTIMAL POLICY We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]\u2014by first considering a discounted reward system and then taking limits as the discount vanishes.", "startOffset": 138, "endOffset": 142}, {"referenceID": 9, "context": "From [10], we can show that the following dynamic program solves (1).", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "\u2223 < (1 \u2212 \u03c1) for all \u03c0 \u2208 [0, 1] and \u03b2 \u2208 [0, 1).", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "The first two above follow directly from [10] and the last two are derived in [13].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "The first two above follow directly from [10] and the last two are derived in [13].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Define V \u03b2 := V\u03b2(\u03c0) \u2212 V\u03b2(1) for \u03c0 \u2208 [0, 1].", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": "Hence we can apply the Arzela-Ascoli theorem [14], to find a subsequence (V \u03b2k(\u03c0), (1 \u2212 \u03b2)V\u03b2k(\u03c0)) that converges uniformly to (V (\u03c0), g) as \u03b2k \u2192 1.", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "Thus, as \u03b2k \u2192 1, along an appropriate subsequence, (3) reduces to V (\u03c0) + g = max {\u03bb+ V ((1\u2212 q)\u03c0), 1 \u2212 \u03c0(1 \u2212 \u03c1)} , (4) for all \u03c0 \u2208 [0, 1].", "startOffset": 131, "endOffset": 137}, {"referenceID": 10, "context": "17 in [11].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "If there exists a bounded function V (\u03c0) for \u03c0 \u2208 [0, 1] and a constant g that satisfies (4), then there exists a stationary policy \u03c6 such that", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "for all \u03c0 \u2208 [0, 1], and moreover, \u03c6 is the policy for which the RHS of (4) is maximized.", "startOffset": 12, "endOffset": 18}, {"referenceID": 14, "context": "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]\u2013[19] and POMDPs.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]\u2013[19] and POMDPs.", "startOffset": 303, "endOffset": 307}, {"referenceID": 17, "context": "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]\u2013[19] and POMDPs.", "startOffset": 308, "endOffset": 312}, {"referenceID": 0, "context": "At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]\u00d7 [0, 1] containing all possible POMDPs parameterized by (q, \u03c1).", "startOffset": 178, "endOffset": 184}, {"referenceID": 0, "context": "At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]\u00d7 [0, 1] containing all possible POMDPs parameterized by (q, \u03c1).", "startOffset": 186, "endOffset": 192}, {"referenceID": 0, "context": "Algorithm 1: Thompson sampling algorithm for learning the optimal policy Input: Parameter space X \u2286 [0, 1], Policy space K \u2282 {0, 1, 2, .", "startOffset": 100, "endOffset": 106}, {"referenceID": 18, "context": "To this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "To this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21].", "startOffset": 115, "endOffset": 119}], "year": 2016, "abstractText": "We describe and study a model for an Automated Online Recommendation System (AORS) in which a user\u2019s preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multiarmed bandit problem are also presented.", "creator": "LaTeX with hyperref package"}}}