{"id": "1603.09495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Reactive Policies with Planning for Action Languages", "abstract": "we describe a representation in a high - context level transition system for policies that barely express adequately a reactive behavior for the agent. we consider selecting a target decision component that figures out what to do next and stores an ( online ) planning capability to compute the plans needed to reach these targets. our representation allows one to roughly analyze the flow of executing the given reactive policy, and manage to determine whether it works as expected. additionally, the theoretical flexibility of the representation opens in a range of possibilities for designing behaviors.", "histories": [["v1", "Thu, 31 Mar 2016 09:05:28 GMT  (56kb,D)", "http://arxiv.org/abs/1603.09495v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["zeynep g saribatur", "thomas eiter"], "accepted": false, "id": "1603.09495"}, "pdf": {"name": "1603.09495.pdf", "metadata": {"source": "META", "title": "Reactive Policies with Planning for Action Languages", "authors": ["Zeynep G. Saribatur", "Thomas Eiter"], "emails": ["zeynep@kr.tuwien.ac.at", "eiter@kr.tuwien.ac.at"], "sections": [{"heading": null, "text": "Autonomous agents are systems that decide for themselves what to do to satisfy their design objectives. These agents have a knowledge base that describes their capabilities, represents facts about the world and helps them in reasoning about their course of actions. A reactive agent interacts with its environment. It perceives the current state of the world through sensors, consults its memory (if there is any), reasons about actions to take and executes them in the environment. A policy for these agents gives guidelines to follow during their interaction with the environment.\nAs autonomous systems become more common in our lives, the issue of verifying that they behave as intended becomes more important. During the operation of an agent, one would want to be sure that by following the designed policy, the agent will achieve the desired results. It would be highly costly, time consuming and sometimes even fatal to realize at runtime that the designed policy of the agent does not provide the expected properties.\nFor example, in search and rescue scenarios, an agent needs to find a missing person in unknown environments. A naive approach would be to directly try to find a plan that achieves the main goal of finding the person. However, this problem easily becomes troublesome, since not knowing the environment causes the planner to consider all possible cases and find a plan that guarantees reaching the goal in all settings. Alternatively, one can describe a reactive policy for the agent that determines its course of actions according to its current knowledge, and guides the agent in the environment towards the main goal. A possible such policy could be \u201calways\n\u2217This work has been supported by Austrian Science Fund (FWF) project W1255-N23. Copyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nmove to the farthest unvisited point in visible distance, until a person is found\u201d. Following this reactive policy, the agent would traverse the environment by choosing its actions to reach the farthest possible point from the current state, and by reiterating the decision process after reaching a new state. The agent may also remember the locations it has been in and gain information (e.g. obstacle locations) through its sensors on the way. Verifying beforehand whether or not the designed policy of the agent satisfies the desired goal (e.g. can the agent always find the person?), in all possible instances of the environment is nontrivial.\nAction languages (Gelfond and Lifschitz 1998) provide a useful framework on defining actions and reasoning about them, by modeling dynamic systems as transition systems. Their declarative property helps in describing the system in an understandable, concise language, and they also address the problems encountered when reasoning about actions. By design, these languages are made to be decidable, which ensures reliable descriptions of dynamic systems. As these languages are closely related with classical logic and answer set programming (ASP) (Lifschitz 2008; 1999), they can be translated into logic programs and queried for computation. The programs produced by such translations can yield sound and complete answers to such queries. There have been various works on action languages (Gelfond and Lifschitz 1998; 1993; Giunchiglia and Lifschitz 1998) and their reasoning systems (Giunchiglia et al. 2004; Gebser, Grote, and Schaub 2010), with underlying mechanisms that rely on SAT and ASP solvers.\nThe shortage of representations that are capable of modeling reactive policies prevents one from verifying such policies using action languages as above before putting them into use. The necessity of such a verification capability motivates us to address this issue. We thus aim for a general model that allows for verifying the reactive behavior of agents in environments with different types in terms of observability and determinism. In that model, we want to use the representation power of the transition systems described by action languages and combine components that are efficient for describing reactivity.\nTowards this aim, we consider in this paper agents with a reactive behavior that decide their course of actions by determining targets to achieve during their interaction with the environment. Such agents come with an (online) planning\nar X\niv :1\n60 3.\n09 49\n5v 1\n[ cs\n.A I]\n3 1\nM ar\n2 01\n6\ncapability that computes plans to reach the targets. This method matches the observe-think-act cycle of Kowalski and Sadri (1999), but involves a planner that considers targets. The flexibility in the two components - target development and external planning - allow for a range of possibilities for designing behaviors. For example, one can use HEX (Eiter et al. 2005) to describe a program that determines a target given the current state of an agent, finds the respective plan and the execution schedule. ACTHEX programs (Fink et al. 2013), in particular, provide the tools to define such reactive behaviors as it allows for iterative evaluation of the logic programs and the ability to observe the outcomes of executing the actions in the environment. Specifically, we make the following contributions:\n(1) We introduce a novel framework for describing the semantics of a policy that follows a reactive behavior, by integrating components of target establishment and online planning. The purpose of this work is not synthesis, but to lay foundations for verification of behaviors of (human-designed) reactive policies. The outsourced planning might also lend itself for modular, hierarchic planning, where macro actions (expressed as targets) are turned into a plan of micro actions. Furthermore, outsourced planning may also be exploited to abstract from correct sub-behaviors (e.g. going always to the farthest point).\n(2) We relate this to action languages and discuss possibilities for policy formulation. In particular, we consider the action language C (Giunchiglia and Lifschitz 1998) to illustrate an application.\nThe remainder of this paper is organized as follows. After some preliminaries, we present a running example and then the general framework for modeling policies with planning. After that, we consider the relation to action languages, and as a particular application we consider (a fragment of) the action language C. We briefly discuss some related work and conclude with some issues for ongoing and future work."}, {"heading": "Preliminaries", "text": "Definition 1. A transition system T is defined as T = \u3008S, S0,A,\u03a6\u3009 where \u2022 S is the set of states. \u2022 S0 \u2286 S is the set of possible initial states. \u2022 A is the set of possible actions. \u2022 \u03a6 : S \u00d7 A \u2192 2S is the transition function, returns the\nset of possible successor states after applying a possible action in the current state. For any states s, s\u2032 \u2208 S, we say that there is a trajectory between s and s\u2032, denoted by s\u2192\u03c3 s\u2032 for some action sequence \u03c3 = a1, . . . , an where n \u2265 0, if there exist s0, . . . , sn \u2208 S such that s = s0, s\u2032 = sn and si+1 \u2208 \u03a6(si, ai+1) for all 0 \u2264 i < n.\nWe will refer to this transition system as the original transition system. The constituents S and A are assumed to be finite in the rest of the paper. Note that, this transition system represents fully observable settings. Large environments\ncause high number of possibilities for states, which cause the transition systems to be large. Especially, if the environment is nondeterministic, the resulting transition system contains high amount of transitions between states, since one needs to consider all possible outcomes of executing an action."}, {"heading": "Action Languages", "text": "Action languages describe a particular type of transition systems that are based on action signatures. An action signature consists of a set V of value names, a set F of fluent names and a set A of action names. Any fluent has a value in any state of the world.\nA transition system of an action signature \u3008V,F,A\u3009 is similar to Defn. 1, where A = A and \u03a6 corresponds to the relationR \u2286 S\u00d7A\u00d7S. In addition, we have a value function V : F\u00d7S \u2192 V, where V (P, s) shows the value of P in state s. A transition system can be thought as a labeled directed graph, where a state s is represented by a vertex labeled with the function P \u2192 V (P, s), that gives the value of the fluents. Every triple \u3008s, a, s\u2032\u3009 \u2208 R is represented by an edge leading from a state s to a state s\u2032 and labeled by a.\nAn action a is executable at a state s, if there is at least one state s\u2032 such that \u3008s, a, s\u2032\u3009 \u2208 R and a is deterministic if there is at most one such state. Concurrent execution of actions can be defined by considering transitions in the form \u3008s,A, s\u2032\u3009 with a set A \u2286 A of actions, where each action a\u2208A is executable at s.\nAn action signature \u3008V,F,A\u3009 is propositional if its value names are truth values: V={f, t}. In this work, we confine to propositional action signatures.\nThe transition system allows one to answer queries about the program. For example, one can find a plan to reach a goal state from an initial state, by searching for a path between the vertices that represent these states in the transition system. One can express properties about the paths of the transition system by using an action query language.\nRunning Example: Search Scenarios Consider a memoryless agent that can sense horizontally and vertically, in an unknown n\u00d7n grid cell environment with obstacles, where a missing person needs to be found. Suppose we are given the action description of the agent with a policy of \u201calways going to the farthest reachable point in visible distance (until a person is found)\u201d. Following this reactive policy, the agent chooses its course of actions to reach the farthest reachable point, referred as target, from its current location with respect to its current knowledge about the environment. After executing the plan and reaching a state that satisfies the target, the decision process is reiterated and a new target, hence a new course of actions, is determined.\nGiven such a policy, one would want to check whether or not the agent can always find the person, in all instances of the environment. Note that we assume that the obstacles are placed in a way that the person is always reachable.\nFigure 1 shows some possible instances for n=3, where the square in a cell represents the agent and the dot represents the missing person. The course of actions determined by the policy in all the instances is to move to (3,1), which is the\nfarthest reachable point, i.e. target. It can be seen that (a) is an instance where the person can be found with the given policy, while in (b) the agent goes in a loop and can\u2019t find the person, since after reaching (3,1) it will decide to move to (1,1) again. In (c), after reaching (3,1) following its policy, the agent has two possible directions to choose, since there are two farthest points. It can either move to (3,3), which would result in seeing the person, or it can move back to (1,3), which would mean that there is a possibility for the agent to go in a loop.\nNotice that our aim is different from finding a policy (i.e. global plan) that satisfies certain properties (i.e. goals) in an unknown environment. On the contrary, we assume that we are given a representation of a system with a certain policy, and we want to check what it is capable (or incapable) of."}, {"heading": "Modeling Policies in Transition Systems", "text": "A reactive policy is described to reach some main goal, by guiding the agent through its interaction with the environment. This guidance can be done by determining the course of actions to bring about targets from the current situation, via externally computed plans. A transition system that models such policies should represent the flow of executing the policy, which is the agent\u2019s actual trajectory in the environment following the policy. This would allow for verifying whether execution of a policy results in reaching the desired main goal, i.e. the policy works.\nWe define such a transition system by clustering the states into groups depending on a profile. A profile is determined by evaluating a set of formulas over a state that informally yield attribute (respectively feature) values; states with the same attribute values are clustered into one. The choice of formulas for determining profiles depends on the given policy or the environment one is considering. Then, the transitions between these clusters are defined according to the policy. The newly defined transitions are able to show the evaluation of the policy by a higher level action from one state to the next state. This next state satisfies the target determined by a target component, and the higher level action corresponds to the execution of an externally computed plan.\nHaving such a classification on states and defining higher level transitions between the states can help in reducing the state space or the number of transitions when compared to the original transition system. Furthermore, it aids in abstraction and allows one to emulate a modular hierarchic approach, in which a higher level (macro) action, expressed by a target, is realized in terms of a sequence of (micro) actions that is compiled by the external planner, which may use different ways (planning on the fly, resorting to scripts etc.)\nState profiles according to the policy We now describe a classification of states, which helps to omit parts of the state that are irrelevant with respect to the environment or the policy. This classification is done by determining profiles, and clustering the states accordingly.\nExample 1. Remember the possible instances from Figure 1 in the running example. Due to partial observability, the agent is unable to distinguish the states that it is in, and the unobservable parts are irrelevant to the policy. Now assume that there are fluents that hold the information of the agent\u2019s location, the locations of the obstacles and the reachable points. One can determine a profile of the form \u201cthe agent is at (1,1), sees an obstacle at (1,3), and is able to reach to points at (1,2), (2,1), (3,1)\u201d by not considering the remaining part of the environment that the agent can not observe. The states that have this profile can be clustered in one group as in Figure 2, where the cells with question marks demonstrate that they are not observable by the agent.\nFor partially observable environments, the notion of indistinguishable states can be used in the classification of states. The states that provide the same observations for the agent are considered as having the same profile. However, in fully observable environments, observability won\u2019t help in reducing the state space. One needs to find other notions to determine profiles.\nWe consider a classification function, h : S \u2192 \u2126h, where \u2126h is the set of possible state clusters. This is a general notion applicable to fully and partially observable cases.\nDefinition 2. An equalized state relative to the classification function h is a state s\u0302 \u2208 \u2126h. The term equalized comes from the fact that the states in the same classification are considered as the same, i.e. equal.\nTo talk about a state s that is clustered into an equalized state s\u0302, we use the notation s\u2208 s\u0302, where we identify s\u0302 with its pre-image (i.e. the set of states that are mapped to s\u0302 according to h).\nDifferent from the work by Son and Baral (2001) where they consider a \u201ccombined-state\u201d which consists of the real state of the world and the states that the agent thinks it may be in, we consider a version where we combine the real states into one state if they provide the same classification (or observation, in case of partial observability) for the agent. The equalization of states allows for omitting the details that are irrelevant to the behavior of the agent.\nTransition systems according to the policy We now define the notion of a transition system that is able to represent the evaluation of the policy on the state clusters.\nDefinition 3. An equalized (higher level) transition system Th, with respect to the classification function h, is defined as Th = \u3008S\u0302, S\u03020, GB ,B,\u03a6B\u3009, where \u2022 S\u0302 is the finite set of equalized states; \u2022 S\u03020 \u2286 S\u0302 is the finite set of initial equalized states, where s\u0302 \u2208 S\u03020 if there is some si \u2208 s\u0302 such that si \u2208 S0 holds; \u2022 GB is the finite set of possible targets relative to the behavior, where a target can be satisfied by more than one equalized state;\n\u2022 B : S\u0302 \u2192 2GB , is the target function that returns the possible targets to achieve from the current equalized state, according to the policy;\n\u2022 \u03a6B : S\u0302 \u2192 2S\u0302 is the transition function according to the policy, referred to as the policy execution function, returns the possible resulting equalized states after applying the policy in the current equalized state.\nThe target function gets the equalized state as input and produces the possible targets to achieve. These targets may be expressed as formulas over the states (in particular, of states that are represented by fluents or state variables), or in some other representation. A target can be considered as a subgoal condition to hold at the follow-up state, depending on the current equalized state. The aim would be to intend to reach a state that satisfies the conditions of the target, without paying attention to the steps taken in between. That\u2019s where the policy execution function comes into the picture.\nThe formal description of the policy execution function is as follows:\n\u03a6B(s\u0302) = {s\u0302\u2032 | s\u0302\u2032 \u2208 Res(s\u0302, \u03c3), \u03c3 \u2208 Reach(s\u0302, gB), gB \u2208 B(s\u0302)},\nwhere Reach is an outsourced function that returns a plan \u03c3 = \u3008a1, . . . , an\u3009, n \u2265 0 needed to reach a state that meets the conditions gB from the current equalized state s\u0302:\nReach(s\u0302, gB) \u2286 {\u03c3 | \u2200s\u0302\u2032 \u2208 Res(s\u0302, \u03c3) : s\u0302\u2032 |= gB} where s\u0302|=gB \u21d4 \u2200s\u2208 s\u0302 : s|=gB , and Res gives the resulting states of executing a sequence of actions at a state s\u0302:\nRes(s\u0302, \u3008a1, . . . , an\u22651\u3009) ={ \u22c3 s\u0302\u2032\u2208\u03a6\u0302(s\u0302,a1) Res(s\u0302\n\u2032, \u3008a2, . . . , an\u3009) \u03a6\u0302(s\u0302, a1)6=\u2205 {s\u0302err} \u03a6\u0302(s\u0302, a1)=\u2205\nRes(s\u0302, \u3008\u3009) = {s\u0302} where the state s\u0302err is an artifact state that does not satisfy any of the targets, and\n\u03a6\u0302(s\u0302, a) = {s\u0302\u2032 | \u2203s\u2032 \u2208 s\u0302\u2032 \u2203s \u2208 s\u0302 : s\u2032 \u2208 \u03a6(s, a)}.\nFigure 3 demonstrates a transition in the equalized transition system. The equalized states may contain more than one state that has the same profile. Depending on the current state, s\u0302, the policy chooses the next target, gB , that should be satisfied. There may be more than one equalized state satisfying the same target. The policy execution function \u03a6B(s\u0302)\nfinds a transition into one of these equalized states, s\u0302\u2032, that is reachable from the current equalized state. The transition \u03a6B is considered as a big jump between states, where the actions taken and the states passed in between are omitted.\nNotice that we assume that the outsourced Reach function is able to return conformant plans that guarantee to reach a state that satisfies the determined targets. In particular, \u03c3 may also contain only one action. For practical reasons, we consider Reach to be able to return a subset of all conformant plans. The maximal possible Reach , where we have equality, is denoted with Reach0.\nConsider the case of uncertainty, where the agent requires to do some action, e.g. checkDoor , in order to get further information about its state. One can define the target function to return as target a dummy fluent to ensure that the action is made, e.g. doorIsChecked , and given this target, the Reach function can return the desired action as the plan. The nondeterminism or partial observability of the environment is modeled through the set of possible successor states returned by Res .\nThe generic definition of the equalized transition system allows for the possibility of representing well-known concepts like purely reactive systems or universal planning (Cimatti, Riveri, and Traverso 1998a). To represent reactive systems, one can describe a policy of \u201cpick some action\u201d. This way one can model reactive systems that do not do reasoning, but immediately react to the environment with an action. As for the exactly opposite case, which is finding a plan that guarantees reaching the goal, one can choose the target as the main goal. Then, the Reach would have the difficult task of finding a universal plan or a conformant plan that reaches the main goal. If however, one is aware of such a plan, then it is possible to mimic the plan by modifying the targets GB and the target function B in a way that at each point in time the next action in the plan is returned by Reach, and the corresponding transition is made. For that, one needs to record information in the states and keep track of the targets.\nAs the function Reach is outsourced, we rely on an implementation that returns conformant plans for achieving transitions in the equalized transition systems. This naturally raises the issue of whether a given such implementation is suitable, and leads to the question of soundness (only correct\nplans are output) and completeness (some plan will be output, if one exists). We next assess how expensive it is to test this, under some assumptions about the representation and computational properties of (equalized) transition systems, which will then also be used for assessing the cost of policy checking.\nAssumptions We have certain assumptions on the representation of the system. We assume that given a state s \u2208 S which is implicitly given using a binary encoding, the cost of evaluating the classification h(s), the (original) transition \u03a6(s, a) for some action a, and recognizing the initial state, say with \u03a6init(s), is polynomial. The cost could also be in NP, if projective (i.e. existentially quantified) variables are allowed. Furthermore, we assume that the size of the representation of a \u201ctarget\u201d in GB is polynomial in size of the state, so that given a string, one can check in polynomial time if it is a correct target description gB . This test can also be relaxed to be in NP by allowing projective variables.\nGiven these assumptions, we have the following two results. These results show the cost of checking whether an implementation of Reach that we have at hand is sound (delivers correct plans) and in case does not skip plans (is complete); we assume here that testing whether \u03c3 \u2208 Reach(s\u0302, gB) is feasible in \u03a0p2 (this is the cost of verifying conformant plans, and we may assume that Reach is no worse than a naive guess and check algorithm).\nTheorem 1 (soundness of Reach). Let Th = \u3008S\u0302, S\u03020, GB , B,\u03a6B\u3009 be a transition system with respect to a classification function h. The problem of checking whether every transition found by the policy execution function \u03a6B induced by a given implementation Reach is correct is in \u03a0p3.\nProof (Sketch). According to the definition of the policy execution function, every transition from a state s\u0302 to some state s\u0302\u2032 corresponds to some plan \u03c3 returned by Reach(s\u0302, gB). Therefore, first one needs to check whether each plan \u03c3 = \u3008a1, a2, . . . , an\u3009 returned by Reach given some s\u0302 and gB is correct. For that we need to check two conditions on the corresponding trajectories of the plan:\n(i) for all partial trajectories s\u03020, s\u03021, . . . , s\u0302i\u22121 it holds that for the upcoming action ai from the plan \u03c3, \u03a6\u0302(s\u0302i\u22121, ai) 6= \u2205 (i.e. the action is applicable)\n(ii) for all trajectories s\u03020, s\u03021, . . . , s\u0302n, s\u0302n |= gB .\nChecking whether these conditions hold is in \u03a0p2. Thus, to decide whether for some state s\u0302 and target gB the function \u03a6B(s\u0302, gB) does not work correctly, we can guess s\u0302 (resp. s\u2208 s\u0302), gB and a plan \u03c3 and verify that \u03c3 \u2208Reach(s\u0302, gB) and that \u03c3 is not correct. As the verification is doable with an oracle for \u03a3p2 in polynomial time, a counterexample for correctness can be found in \u03a3p3; thus the problem is in \u03a0p3.\nThe complexity is lower, if output checking of Reach has lower complexity (in particular, it drops to \u03a0p2 if output checking is in co-NP).\nThe result for soundness of Reach is complemented with another result for completeness with respect to short (polynomial size) conformant plans that are returned by Reach .\nTheorem 2 (completeness of Reach). Let Th = \u3008S\u0302, S\u03020, GB , B,\u03a6B\u3009 be a transition system with respect to a classification function h. Deciding whether for a given implementation Reach , \u03a6B fulfills s\u0302\u2032 \u2208 \u03a6B(s\u0302) whenever a short conformant plan from s\u0302 to s\u0302\u2032 exists in Th, is in \u03a0 p 4.\nProof (Sketch). For a counterexample, we can guess some s\u0302 and s\u0302\u2032 (resp. s\u2208 s\u0302, s\u2032 \u2208 s\u0302\u2032) and some short plan \u03c3 and verify that (i) \u03c3 is a valid conformant plan in Th to reach s\u0302\u2032 from s\u0302, and (ii) that a target gB exists such that Reach(s\u0302, gB) produces some output. We can verify (i) using a \u03a0p2 oracle to check that \u03c3 is a conformant plan, and we can verify (ii) using a \u03a0p3 oracle (for all guesses of targets gB and short plans \u03c3\u2032, either gB is not a target for s\u0302 or \u03c3\u2032 is not produced by Reach(s\u0302, gB)). This establishes membership in \u03a0 p 4.\nAs in the case of soundness, the complexity drops if checking the output of Reach is lower (in particular, to \u03a0p3 if the output checking is in co-NP).\nWe also restrict the plans \u03c3 that are returned by Reach(s\u0302, gB) to have polynomial size. This constraint would not allow for exponentially long conformant plans (even if they exist). Thus, the agent is forced under this restriction to develop targets that it can reach in polynomially many steps, and then to go on from these targets. Informally, this does not limit the capability of the agent in general. The \u201clong\u201d conformant plans can be split into short plans with a modified policy and by encoding specific targets into the states.\nWe denote the main goal that the reactive policy is aiming for by g\u221e. Our aim is to have the capability to check whether following the policy always results in reaching some state that satisfies the main goal. That is, for each run, i.e. sequence s\u03020, s\u03021, . . . such that s\u03020 \u2208 S\u03020 and s\u0302i+1 \u2208 \u03a6B(s\u0302i), for all i \u2265 0, there is some j \u2265 0 such that s\u0302j |= g\u221e. (The behavior could be easily modified to stop or to loop in any state s\u0302 that satisfies the goal.) This way we can say whether the policy works or not. Under the assumptions from above, we obtain the following\nTheorem 3. The problem of determining that the policy works is in PSPACE.\nProof (Sketch). One needs to look at all runs s\u03020, s\u03021, . . . from every initial state s\u03020 in the equalized transition system and check whether each such run has some state s\u0302j that satisfies the main goal g\u221e. Given that states have a representation in terms of fluent or state variables, there are at most exponentially many different states. Thus to find a counterexample, a run of at most exponential length in which g\u221e is not satisfied is sufficient. Such a run can be nondeterministically built in polynomial space; as NPSPACE = PSPACE, the result follows.\nNote that in this formulation, we have tacitly assumed that the main goal can be established in the original system, thus at least some trajectory from some initial state to a state fulfilling the goal exists (this can be checked in PSPACE as\nwell). In a more refined version, we could define the working of a policy relative to the fact that some abstract plan would exist that makes g\u221e true; naturally, thus may impact the complexity of the policy checking.\nAbove, we have been considering arbitrary states, targets and transitions in the equalized transition system. In fact, for the particular behavior, only the states that can be encountered in runs really matter; these are the reachable states defined as follows. Definition 4. A state s\u0302 is reachable from an initial state in the equalized transition system if and only if s \u2208 Ri for some i \u2208 N whereRi is defined as follows.\nR0 = S\u03020 Ri+1 = \u22c3 s\u0302\u2208Ri \u03a6B(s\u0302)\n. . . R\u221e = \u22c3i\u22650Ri.\nUnder the assumptions that apply to the previous results, we can state the following. Theorem 4. The problem of determining whether a state in an equalized transition system is reachable is in PSPACE.\nThe notions of soundness and completeness of an outsourced planning function Reach could be restricted to reachable states; however, this, would not change the cost of testing these properties in general (assuming that s\u0302 \u2208 R is decidable with sufficiently low complexity)."}, {"heading": "Constraining equalization", "text": "The definition of \u03a6\u0302 allows for certain transitions between equalized states that don\u2019t have corresponding concrete transitions in the original transition system. However, the aim of defining such an equalized transition system is not to introduce new features, but to keep the structure of the original transition system and discard the unnecessary parts with respect to the policy. Therefore, one needs to give further restrictions on the transitions of the equalized transition system, in order to obtain the main objective.\nLet us consider the following condition\ns\u0302\u2032 \u2208 \u03a6\u0302(s\u0302, a)\u21d4 \u2200s\u2032 \u2208 s\u0302\u2032, \u2203s \u2208 s\u0302 : s\u2032 \u2208 \u03a6(s, a) (1) This condition ensures that a transition between two states s\u03021, s\u03022 in the equalized transition system represents that any state in s\u03022 has a transition from some state in s\u03021. An equalization is called proper if condition (1) is satisfied.\nTheorem 5. Let Th=\u3008S\u0302, S\u03020, GB ,B,\u03a6B\u3009 be a transition system with respect to a classification function h. Let \u03a6\u0302 be the transition function that the policy execution function \u03a6B is based on. The problem of checking whether \u03a6\u0302 is proper is in \u03a0p2.\nProof (sketch). As a counterexample, one needs to guess s\u0302, a, s\u0302\u2032 \u2208 \u03a6\u0302(s\u0302, a) and s\u2032 \u2208 s\u0302\u2032 such that no s\u2208 s\u0302 has s\u2032 \u2208\u03a6(s, a).\nThe results in Theorems 1-5 are all complemented by lower bounds for realistic realizations of the parameters (notably, for typical action languages such as fragments of C).\nThe following proposition is based on the assumption that the transition function \u03a6\u0302 satisfies condition (1).\nProposition 1 (soundness). Let Th=\u3008S\u0302, S\u03020, GB ,B,\u03a6B\u3009 be a transition system with respect to a classification function h. Let s\u03021, s\u03022 \u2208 S\u0302 be equalized states that are reachable from some initial states, and s\u03022 \u2208 \u03a6B(s\u03021). Then for any concrete state s2 \u2208 s\u03022 there is a concrete state s1 \u2208 s\u03021 such that s1 \u2192\u03c3 s2 for some action sequence \u03c3, in the original transition system.\nProof. For equalized states s\u03021, s\u03022, having s\u03022 \u2208\u03a6B(s\u03021) means that s\u03022 satisfies a goal condition that is determined at s\u03021, and is reachable via executing some plan \u03c3. With the assumption that (1) holds, we can apply backwards tracking from any state s2 \u2208 s\u03022 following the transitions \u03a6 corresponding to the actions in the plan \u03c3 backwards. In the end, we can find a concrete state s1 \u2208 s\u03021 from which one can reach the state s2 \u2208 s\u03022 by applying the plan \u03c3 in the original transition system.\nThus, we can conclude the following corollary, with the requirement of only having initial states clustered into the equalized initial states (i.e. no \u201cnon-initial\u201d state is mapped to an initial equalized state). Technically, it should hold that \u2200s \u2208 S0 : h\u22121(h(s)) \u2286 S0. Corollary 1. If there is a trajectory in the equalized transition system with initial state clustering from an equalized initial state s\u03020 to g\u221e, then it is possible to find a trajectory in the original transition system from some concrete initial state s0 \u2208 s\u03020 to g\u221e.\nWe want to be able to study the reactive policy through the equalized transition system. In case the policy does not work as expected, there should be trajectories that shows the reason of the failure. Knowing that any such trajectory found in the equalized transition system exists in the original transition system is enough to conclude that the policy indeed does not work.\nCurrent assumptions can not avoid the case where a plan \u03c3 returned by Reach on the equalized transition system does not have a corresponding trajectory in the original transition system. Therefore, we consider an additional condition as\ns\u0302\u2032 \u2208 \u03a6\u0302(s\u0302, a)\u21d4 \u2200s \u2208 s\u0302, \u2203s\u2032 \u2208 s\u0302\u2032 : s\u2032 \u2208 \u03a6(s, a) (2) that strengthens the properness condition (1). Under this condition, every plan returned by Reach can be successfully executed in the original transition system and will establish the target gB . However, still we may lose trajectories of the original system as by clustering states they might not turn into conformant plans. Then one would need to modify the description of determining targets, i.e. the set of targets GB and the function B. Example 2. Remember the environment and the policy described in the running example, and consider the scenario shown in Figure 4(a). It shows a part of the equalized transition system constructed according to the policy. The states that are not distinguishable due to the partial observability are clustered into the same state.\nThe policy is applied according to current observations, and the successor states show the possible resulting states. The aim of the policy is to have the agent move to the farthest reachable point, which for s\u03021 is (3, 1). As expected, there can be several states that satisfy the target gB=robotAt(3, 1). The successor states of \u03a6B(s\u03021) is determined by Res(s\u03021, \u03c3) computing the possible resulting states after executing the plan \u03c3 returned by Reach(s\u03021, gB). Considering that the agent will gain knowledge about the environment while moving, there are several possibilities for the resulting state.\nNotice that this notion of a transition system can help in reducing the number of states, due to the fact that it is able to disregard states with information on fluents that does not have any effect on the system\u2019s behavior. For example, Figure 4(b) shows a case where the unknown parts behind the obstacles are not relevant to the agent\u2019s behavior, i.e. the person can be found nonetheless."}, {"heading": "Relation with Action Languages", "text": "In this section, we describe how our definition of a higherlevel transition system that models the behavior can fit into the action languages. Given a program defined by an action language and its respective (original) transition system, we now describe how to model this program following a reactive policy and how to construct the corresponding equalized transition system according to the policy."}, {"heading": "Classifying the state space", "text": "The approach to classify the (original) state space relies on defining a function that classifies the states. There are at least two kinds of such classification; one can classify the states depending on whether they give the same values for certain fluents and omit the knowledge of the values of the remaining fluents, or one can introduce a new set of fluents and classify the states depending on whether they give the same values for the new fluents: \u2022 Type 1: Extend the set of truth values by V\u2032 = V \u222a {u},\nwhere u denotes the value to be unknown. Extend the value\nfunction by V \u2032 : F \u00d7 S \u2192 V\u2032. Then, consider a new set of groups of states, S\u0302 = {s\u03021, . . . , s\u0302n}, where a group state s\u0302i contains all the states s \u2208 S that give the same values for all p \u2208 F, i.e. S\u0302 = {s\u0302 | \u2200d, e \u2208 S, d, e \u2208 s\u0302\u21d0\u21d2 \u2200p \u2208 F : V \u2032(p, d)=V \u2032(p, e) }. The value function for the new group of states is V\u0302 : F\u00d7 S\u0302 \u2192 V\u2032.\n\u2022 Type 2: Consider a new set of (auxiliary) fluent names Fa, where each fluent p \u2208 Fa is related with some fluents of F. The relation can be shown with a mapping \u2206 : 2F\u00d7V \u2192 Fa \u00d7 V. Then, consider a new set of groups of states, S\u0302 = {s\u03021, . . . , s\u0302n}, where a group state s\u0302i contains all the states s \u2208 S that give the same values for all p \u2208 Fa, i.e. S\u0302 = {s\u0302 | \u2200d, e \u2208 S, d, e \u2208 s\u0302\u21d0\u21d2 \u2200p \u2208 Fa : V (p, d)=V (p, e) }. The value function for the new group of states is V\u0302 : Fa \u00d7 S\u0302 \u2192 V. We can consider the states in the same classification to have the same profile, and the classification function h as a membership function that assigns the states into groups.\nRemarks: (1) In Type 1, introducing the value unknown for the fluents allows for describing sensing actions and knowing the true value of a fluent at a later state. Also, one needs to give constraints for a fluent to have the unknown value. e.g. it can\u2019t be the case that a fluent related to a grid cell is unknown while the robot is able to observe it. (2) In Type 2, one needs to modify the action descriptions according to the newly defined fluents and define abstract actions. However, in Type 1, the modification of the action definitions is not necessary, assuming that the actions are defined in a way that the fluents that are used when determining an action always have known values.\nOnce a set of equalized states is constructed according to the classification function, one needs to define the reactive policy to determine the transitions. Next, we describe how a policy can be defined from an abstract point of view, through a target language which figures out the targets and helps in determining the course of actions, and show how the transitions are constructed."}, {"heading": "Defining a target language", "text": "Let F\u0302 denote the set of fluents that the equalized transition system is built upon. Let F(F\u0302) denote the set of formulas in an abstract language that can be constructed over F\u0302.\nWe consider a declarative way of finding targets. Let FB(F\u0302)\u2286F(F\u0302) be the set of formulas that describe target determination. Let FGB (F\u0302)\u2286F(F\u0302) denote the set of possible targets that can be determined via the evaluation of the formulas FB(F\u0302) over the related fluents in the equalized states.\nNotice that separation of the target determining formulas FB(F\u0302) and the targets FGB (F\u0302) is to allow for outsourced planners that are able to understand simple target formulas. These planners do not need to know about the target language in order to find plans. However, if one is able to use planners that are powerful enough, then the target language can be given as input to the planner, so that the planner determines the target and finds the corresponding plan.\nTo define a relation between FB(F\u0302) and FGB (F\u0302), we introduce some placeholder fluents. Let FB(F\u0302) = {f1, . . . , fn} be the set of target formulas. Consider a new set of fluents F\u0302B = {pf1 , . . . , pfn} where each of the formulas in FB is represented by some fluent. The value of a fluent depends on whether its respective formula is satisfied or not, i.e. for a state s, s |= f \u21d0\u21d2 V (pf , s) = t. Now consider a mapping M : 2F\u0302B \u2192 2FGB (F\u0302) where M({pf1 , pf2 , . . . , pfm}) = {g1, . . . , gr},m\u2264n and r\u2265 1\nmeans that if there is a state s such that s|=fi, 1\u2264 i\u2264m and s2 f for the remaining formulas f \u2208FB(F\u0302)\\{f1, . . . , fm}, then in the successor state s\u2032 of s, s\u2032 |= gi for some 1\u2264 i\u2264 r, should hold. We consider the output of M to be a set of targets in order to represent the possibility of nondeterminism in choosing a target."}, {"heading": "Transition between states", "text": "The transition for the equalized transition system can be denoted with R\u0302\u2286S\u0302\u00d7S\u0302, where R\u0302 corresponds to the policy execution function \u03a6B that uses (a) the target language to determine targets, (b) an outsourced planner (corresponding to the function Reach) to find conformant plans and (c) the computation of executing the plans (corresponding to the function Res). The outsourced planner finds a sequence of actions \u03c3 \u2208 2A from an equalized state s\u0302 to one of its determined targets gB . Then the successor equalized states are computed by executing the plan from s\u0302. Transition R\u0302 shows the resulting states after applying the policy. Example 3. Let us consider a simple blocksworld example where a policy (of two phases) is defined as follows: \u2022 if at phase 1 and not all the blocks are on the table, move\none free block on a stack with highest number of blocks to the table. \u2022 if all the blocks are on the table, move to phase 2. \u2022 if at phase 2 and not all the blocks are on top of each other,\nmove one of the free blocks on the table on top of the stack with more than one block (if exists any, otherwise move the block on top of some block). Since the policy does not take blocks\u2019 labels into consideration, a classification can be of the following form for n number of blocks: We introduce an n-tuple \u3008b1, . . . , bn\u3009 to denote equalized states such that for i \u2264 n, bi would represent the number of stacks that have i blocks. For example, for 4 blocks, a state \u30081, 0, 1, 0\u3009 where b1 = 1, b2 = 0, b3 = 1, b4 = 0 would represent all the states in the original transition system with the profile \u201ccontains a stack of 1 block and a stack of 3 blocks\u201d. Notice that in the original transition system for 4 labeled blocks, there are 24 possible states that have this profile and if the blocks need to be in order, then there are 4 possible states.\nFigure 5 demonstrates the corresponding equalized transition system for the case of 4 blocks. The equalized transition system for this example is in the following form:\n\u2022 S\u0302 is the set of equalized states according to the abstraction as described above.\n\u2022 S\u03020 \u2208 S\u0302 is the initial equalized states (all elements of S\u0302 except \u30080, . . . , 0, 1\u3009).\n\u2022 GB = S\u0302, since the policy is related with all the blocks, it can determine targets as the whole states.\n\u2022 B : S\u0302 \u2192 S\u0302 is the target function. \u2022 \u03a6B : S\u0302 \u2192 S\u0302 is the policy execution function, returning the\nresulting successor state after applying one action desired by the behavior, shown as in Figure 5.\nApplication on Action Language C In this section, we describe how one can construct an equalized transition system for a reactive system that is represented using the action language C (Giunchiglia and Lifschitz 1998). First, we give some background information about the language C, then move on to the application of our definitions.\nSyntax A formula is a propositional combination of fluents. Given a propositional action signature \u3008{f, t},F,E}, whose set E of elementary action names is disjoint from F, an action description is a set of expressions of the following forms: \u2022 static laws:\ncaused F if G, (3) where F andG are formulas that do not contain elementary actions;\n\u2022 dynamic laws: caused F if G after U, (4)\nwhere F and G are as above, and U is a formula.\nSemantics The transition system \u3008S, V,R\u3009 described by an action description D is defined as follows:\n(i) S is the set of all interpretations s of F such that, for every static law (3) s satisfies F if s satisfies G,\n(ii) V (P, s) = s(P ), i.e. identify s with V (P, s), (iii) R is the set of all triples \u3008s,A, s\u2032\u3009, A \u2286 E, such that s\u2032\nis the only interpretation of F which satisfies the heads of all \u2022 static laws (3) in D for which s\u2032 satisfies G, and\n\u2022 dynamic laws (4) in D for which s\u2032 satisfies G and s \u222aA satisfies U .\nWe focus on a fragment of the language C where the heads of the static and dynamic laws only consist of literals. This restriction on the laws reduces the cost of evaluating the transitions \u3008s,A, s\u2032\u3009 \u2208R to polynomial time. Thus, we match the conditions on complexity from above. Furthermore, by well-known results on the complexity of action language C (Turner 2002; Eiter et al. 2004) all the results in Theorems 1-5 can be turned into completeness results already for this fragment."}, {"heading": "Defining a policy", "text": "Let F\u0302 be the set of fluents that are relevant to the policy. The target language is defined explicitly via static laws using the fluents in F\u0302, denoted FB(F\u0302), where a target is determined by the evaluation of these formulas in a state. Example 4. An example of a target language for the running example uses causal laws from C:\ncaused target(X1 ,Y1 ) if robotAt(X ,Y ) \u2227 farthest(X ,Y ,X1 ,Y1 ) \u2227 not personDetected . caused personDetected if personDetected(X ,Y ). caused targetPerson(X ,Y ) if personDetected(X ,Y ). caused personFound if personDetected(X ,Y )\n\u2227 robotAt(X ,Y ).\nwhere FGB (F\u0302) consists of all atoms target(X ,Y ) and targetPerson(X ,Y ) for 1\u2264X \u2264n, 1\u2264Y \u2264n.\nThe target of a state according to the policy is computed through joint evaluation of these causal laws over the state with the known fluents about the agent\u2019s location and the reachable points. Then, the outsourced planner may take as input the agent\u2019s current location and the target location, to find a plan to reach the target.\nEqualized transition system The equalized transition system \u3008S\u0302, V\u0302 , R\u0302\u3009 that describes the policy is defined as follows:\n(i) S\u0302 is the set of all interpretations of F\u0302 such that, for every static law (3) s\u0302 satisfies F if s\u0302 satisfies G,\n(ii) V\u0302 (P, s\u0302) = s\u0302(P ), where P \u2208 F\u0302, (iii) R\u0302 \u2286 S\u0302 \u00d7 S\u0302 is the set of all \u3008s\u0302, s\u0302\u2032\u3009 such that\n(a) for every s\u2032 \u2208 s\u0302\u2032 there is a trajectory from some s \u2208 s\u0302 of the form s,A1, s1, . . . , An, s\u2032 in the original transition system;\n(b) for static laws f1, f2, . . . , fm \u2208 FB(F\u0302) for which s\u0302 satisfies the body, it holds that s\u0302\u2032 |= g for some g \u2208M(pf1 , . . . , pfm).\nNotice that in the definition of the transition relation R\u0302 in (iii) there is no description of (a) how a trajectory is computed or (b) how a target is determined. This gives flexibility on the implementation of these components.\nOther languages can be similarly used to describe the equalized transition system, as long as they are powerful enough to express the concepts from the previous section.\nRelated Work There are works being conducted on the verification of GOLOG programs (Levesque et al. 1997), a family of highlevel action programming languages defined on top of action theories expressed in the situation calculus. The method of verifying properties of non-terminal processes are sound, but do not have the guarantee of termination due to the verification problem being undecidable (De Giacomo, Ternovskaia, and Reiter 1997; Cla\u00dfen and Lakemeyer 2008). By resorting to action formalisms based on description logic, decidability can be achieved (Baader and Zarrie\u00df 2013).\nVerifying temporal properties of dynamic systems in the context of data management is studied by (Calvanese et al. 2013) in the presence of description logic knowledge bases. However, target establishment and planning components are not considered in these works, and they do not address real life environment settings.\nThe logical framework for agent theory developed by Rao and Georgeff (1991) is based on beliefs, desires and intentions, in which agents are viewed as being rational and acting in accordance with their beliefs and goals. There are many different agent programming languages and platforms based on the BDI approach. Some works carried out on verifying properties of agents represented in these languages, such as (Bordini et al. 2006; Dennis et al. 2012). These approaches consider very complex architectures that even contain a plan library where plans are matched with the intentions or the agent\u2019s state and manipulate the intentions.\nSynthesizing and Verifying Plans There have been various works on synthesizing plans via symbolic model checking techniques by Cimatti et al. (1998b; 1998a), Bertoli et al. (2006). These approaches are able to solve difficult planning problems like strong planning and strong cyclic planning.\nSon and Baral (2001) extend the action description language by allowing sensing actions and allow to query conditional plans. These conditional plans are general plans that consist of sensing actions and conditional statements.\nThese works address a different problem then ours. When nondeterminism and partial observability are taken into account, finding a plan that satisfies the desired results in the environment is highly demanding. We consider a much less ambitious approach where given a behavior, we aim to check whether or not this behavior gives the desired results in the environment. However, our framework is capable emulating the plans found by these works.\nExecution Monitoring There are logic-based monitoring frameworks that monitor the plan execution and recover the plans in case of failure. The approaches that are studied are replanning (De Giacomo, Reiter, and Soutchanski 1998), backtracking to the point of failure and continuing from there (Soutchanski 2003), or diagnosing the failure and recovering from the failure situation (Fichtner, Gro\u00dfmann, and Thielscher 2003; Eiter et al. 2007).\nThese works consider the execution of a given plan, while we consider a given reactive policy that determines targets and use (online) planning to reach these targets."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we described a high-level representation that models reactive behaviors, and integrates target development and online planning capabilities. Flexibility in these components does not bound one to only use action languages, but allows for the use of other formalizations as well. For future work, one could imagine targets to depend on further parameters or to incorporate learning from experience in the framework. It is also possible to use other plans, e.g. short conditional plans, in the planner component.\nThe long-term goal of this work is to check and verify properties of the reactive policies for action languages. In order to solve these problems practically, it is necessary to use techniques from model checking, such as abstraction, compositional reasoning and parameterization. Also, the use of temporal logic formulas is needed to express complex goals such as properties of the policies. Our main target is to work with action languages, and to incorporate their syntax and semantics with such model checking techniques. The general structure of our framework allows one to focus on action languages, and to investigate how to merge these techniques."}], "references": [{"title": "Verification of Golog programs over description logic actions", "author": ["F. Baader", "B. Zarrie\u00df"], "venue": "Frontiers of Combining Systems 181\u2013196.", "citeRegEx": "Baader and Zarrie\u00df,? 2013", "shortCiteRegEx": "Baader and Zarrie\u00df", "year": 2013}, {"title": "Strong planning under partial observability", "author": ["P. Bertoli", "A. Cimatti", "M. Riveri", "P. Traverso"], "venue": "Artificial Intelligence 170(4):337\u2013384.", "citeRegEx": "Bertoli et al\\.,? 2006", "shortCiteRegEx": "Bertoli et al\\.", "year": 2006}, {"title": "Verifying multi-agent programs by model checking", "author": ["R.H. Bordini", "M. Fisher", "W. Visser", "M. Wooldridge"], "venue": "Autonomous agents and multi-agent systems 12(2):239\u2013256.", "citeRegEx": "Bordini et al\\.,? 2006", "shortCiteRegEx": "Bordini et al\\.", "year": 2006}, {"title": "Verification and synthesis in description logic based dynamic systems", "author": ["D. Calvanese", "G. De Giacomo", "M. Montali", "F. Patrizi"], "venue": "Web Reasoning and Rule Systems. Springer. 50\u201364.", "citeRegEx": "Calvanese et al\\.,? 2013", "shortCiteRegEx": "Calvanese et al\\.", "year": 2013}, {"title": "Automatic OBDD-based generation of universal plans in nondeterministic domains", "author": ["A. Cimatti", "M. Riveri", "P. Traverso"], "venue": "Proc. of AAAI/IAAI, 875\u2013881.", "citeRegEx": "Cimatti et al\\.,? 1998a", "shortCiteRegEx": "Cimatti et al\\.", "year": 1998}, {"title": "Strong planning in non-deterministic domains via model checking", "author": ["A. Cimatti", "M. Riveri", "P. Traverso"], "venue": "AIPS 98:36\u201343.", "citeRegEx": "Cimatti et al\\.,? 1998b", "shortCiteRegEx": "Cimatti et al\\.", "year": 1998}, {"title": "A logic for nonterminating Golog programs", "author": ["J. Cla\u00dfen", "G. Lakemeyer"], "venue": "Proc. of KR, 589\u2013599.", "citeRegEx": "Cla\u00dfen and Lakemeyer,? 2008", "shortCiteRegEx": "Cla\u00dfen and Lakemeyer", "year": 2008}, {"title": "Execution monitoring of high-level robot programs", "author": ["G. De Giacomo", "R. Reiter", "M. Soutchanski"], "venue": "Proc. of KR, 453\u2013465.", "citeRegEx": "Giacomo et al\\.,? 1998", "shortCiteRegEx": "Giacomo et al\\.", "year": 1998}, {"title": "Nonterminating processes in the situation calculus", "author": ["G. De Giacomo", "E. Ternovskaia", "R. Reiter"], "venue": "Working Notes of Robots, Softbots, Immobots: Theories of Action, Planning and Control, AAAI97 Workshop.", "citeRegEx": "Giacomo et al\\.,? 1997", "shortCiteRegEx": "Giacomo et al\\.", "year": 1997}, {"title": "Model checking agent programming languages", "author": ["L.A. Dennis", "M. Fisher", "M.P. Webster", "R.H. Bordini"], "venue": "Automated Software Engineering 19(1):5\u201363.", "citeRegEx": "Dennis et al\\.,? 2012", "shortCiteRegEx": "Dennis et al\\.", "year": 2012}, {"title": "A logic programming approach to knowledge-state planning: Semantics and complexity", "author": ["T. Eiter", "W. Faber", "N. Leone", "G. Pfeifer", "A. Polleres"], "venue": "ACM Trans. Comput. Log. 5(2):206\u2013263.", "citeRegEx": "Eiter et al\\.,? 2004", "shortCiteRegEx": "Eiter et al\\.", "year": 2004}, {"title": "A Uniform Integration of Higher-Order Reasoning and External Evaluations in Answer-Set Programming", "author": ["T. Eiter", "G. Ianni", "R. Schindlauer", "H. Tompits"], "venue": "Proc. of IJCAI, 90\u201396.", "citeRegEx": "Eiter et al\\.,? 2005", "shortCiteRegEx": "Eiter et al\\.", "year": 2005}, {"title": "A logicbased approach to finding explanations for discrepancies in optimistic plan execution", "author": ["T. Eiter", "E. Erdem", "W. Faber", "J. Senko"], "venue": "Fundamenta Informaticae 79(12):25\u201369.", "citeRegEx": "Eiter et al\\.,? 2007", "shortCiteRegEx": "Eiter et al\\.", "year": 2007}, {"title": "Intelligent execution monitoring in dynamic environments", "author": ["M. Fichtner", "A. Gro\u00dfmann", "M. Thielscher"], "venue": "Fundamenta Informaticae 57(2-4):371\u2013392.", "citeRegEx": "Fichtner et al\\.,? 2003", "shortCiteRegEx": "Fichtner et al\\.", "year": 2003}, {"title": "Acthex: Implementing HEX programs with action atoms", "author": ["M. Fink", "S. Germano", "G. Ianni", "C. Redl", "P. Sch\u00fcller"], "venue": "Logic Programming and Nonmonotonic Reasoning 317\u2013322.", "citeRegEx": "Fink et al\\.,? 2013", "shortCiteRegEx": "Fink et al\\.", "year": 2013}, {"title": "Coala: A compiler from action languages to ASP", "author": ["M. Gebser", "T. Grote", "T. Schaub"], "venue": "Proc. of JELIA, 360\u2013364. Springer Heidelberg.", "citeRegEx": "Gebser et al\\.,? 2010", "shortCiteRegEx": "Gebser et al\\.", "year": 2010}, {"title": "Representing action and change by logic programs", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "The Journal of Logic Programming 17(2):301\u2013321.", "citeRegEx": "Gelfond and Lifschitz,? 1993", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1993}, {"title": "Action languages", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "Electronic Transactions on AI 3(16).", "citeRegEx": "Gelfond and Lifschitz,? 1998", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1998}, {"title": "An action language based on causal explanation: Preliminary report", "author": ["E. Giunchiglia", "V. Lifschitz"], "venue": "Proc. of AAAI/IAAI, 623\u2013630.", "citeRegEx": "Giunchiglia and Lifschitz,? 1998", "shortCiteRegEx": "Giunchiglia and Lifschitz", "year": 1998}, {"title": "Nonmonotonic causal theories", "author": ["E. Giunchiglia", "J. Lee", "V. Lifschitz", "N. McCain", "H. Turner"], "venue": "Artificial Intelligence 153(1):49\u2013104.", "citeRegEx": "Giunchiglia et al\\.,? 2004", "shortCiteRegEx": "Giunchiglia et al\\.", "year": 2004}, {"title": "From logic programming towards multi-agent systems", "author": ["R.A. Kowalski", "F. Sadri"], "venue": "Ann. Math. Artif. Intell. 25(34):391\u2013419.", "citeRegEx": "Kowalski and Sadri,? 1999", "shortCiteRegEx": "Kowalski and Sadri", "year": 1999}, {"title": "GOLOG: A logic programming language for dynamic domains", "author": ["H.J. Levesque", "R. Reiter", "Y. Lesperance", "F. Lin", "R.B. Scherl"], "venue": "The Journal of Logic Programming 31(1):59\u201383.", "citeRegEx": "Levesque et al\\.,? 1997", "shortCiteRegEx": "Levesque et al\\.", "year": 1997}, {"title": "Action languages, answer sets and planning", "author": ["V. Lifschitz"], "venue": "The Logic Programming Paradigm: a 25-Year Perspective, 357\u2013373. Springer.", "citeRegEx": "Lifschitz,? 1999", "shortCiteRegEx": "Lifschitz", "year": 1999}, {"title": "What is answer set programming? In Proc", "author": ["V. Lifschitz"], "venue": "of. AAAI, 1594\u20131597.", "citeRegEx": "Lifschitz,? 2008", "shortCiteRegEx": "Lifschitz", "year": 2008}, {"title": "Modeling rational agents within a BDI-architecture", "author": ["A.S. Rao", "M.P. Georgeff"], "venue": "Proc. of KR, 473\u2013484.", "citeRegEx": "Rao and Georgeff,? 1991", "shortCiteRegEx": "Rao and Georgeff", "year": 1991}, {"title": "Formalizing sensing actions \u2013 a transition function based approach", "author": ["T.C. Son", "C. Baral"], "venue": "Artificial Intelligence 125(1):19\u201391.", "citeRegEx": "Son and Baral,? 2001", "shortCiteRegEx": "Son and Baral", "year": 2001}, {"title": "High-level robot programming and program execution", "author": ["M. Soutchanski"], "venue": "Proc. of ICAPS Workshop on Plan Execution.", "citeRegEx": "Soutchanski,? 2003", "shortCiteRegEx": "Soutchanski", "year": 2003}, {"title": "Polynomial-length planning spans the polynomial hierarchy", "author": ["H. Turner"], "venue": "Proc. of JELIA, 111\u2013124. Springer.", "citeRegEx": "Turner,? 2002", "shortCiteRegEx": "Turner", "year": 2002}], "referenceMentions": [{"referenceID": 17, "context": "Action languages (Gelfond and Lifschitz 1998) provide a useful framework on defining actions and reasoning about them, by modeling dynamic systems as transition systems.", "startOffset": 17, "endOffset": 45}, {"referenceID": 23, "context": "As these languages are closely related with classical logic and answer set programming (ASP) (Lifschitz 2008; 1999), they can be translated into logic programs and queried for computation.", "startOffset": 93, "endOffset": 115}, {"referenceID": 17, "context": "There have been various works on action languages (Gelfond and Lifschitz 1998; 1993; Giunchiglia and Lifschitz 1998) and their reasoning systems (Giunchiglia et al.", "startOffset": 50, "endOffset": 116}, {"referenceID": 18, "context": "There have been various works on action languages (Gelfond and Lifschitz 1998; 1993; Giunchiglia and Lifschitz 1998) and their reasoning systems (Giunchiglia et al.", "startOffset": 50, "endOffset": 116}, {"referenceID": 19, "context": "There have been various works on action languages (Gelfond and Lifschitz 1998; 1993; Giunchiglia and Lifschitz 1998) and their reasoning systems (Giunchiglia et al. 2004; Gebser, Grote, and Schaub 2010), with underlying mechanisms that rely on SAT and ASP solvers.", "startOffset": 145, "endOffset": 202}, {"referenceID": 11, "context": "For example, one can use HEX (Eiter et al. 2005) to describe a program that determines a target given the current state of an agent, finds the respective plan and the execution schedule.", "startOffset": 29, "endOffset": 48}, {"referenceID": 14, "context": "ACTHEX programs (Fink et al. 2013), in particular, provide the tools to define such reactive behaviors as it allows for iterative evaluation of the logic programs and the ability to observe the outcomes of executing the actions in the environment.", "startOffset": 16, "endOffset": 34}, {"referenceID": 16, "context": "This method matches the observe-think-act cycle of Kowalski and Sadri (1999), but involves a planner that considers targets.", "startOffset": 51, "endOffset": 77}, {"referenceID": 18, "context": "In particular, we consider the action language C (Giunchiglia and Lifschitz 1998) to illustrate an application.", "startOffset": 49, "endOffset": 81}, {"referenceID": 25, "context": "Different from the work by Son and Baral (2001) where they consider a \u201ccombined-state\u201d which consists of the real state of the world and the states that the agent thinks it may be in, we consider a version where we combine the real states into one state if they provide the same classification (or observation, in case of partial observability) for the agent.", "startOffset": 27, "endOffset": 48}, {"referenceID": 18, "context": "In this section, we describe how one can construct an equalized transition system for a reactive system that is represented using the action language C (Giunchiglia and Lifschitz 1998).", "startOffset": 152, "endOffset": 184}, {"referenceID": 27, "context": "Furthermore, by well-known results on the complexity of action language C (Turner 2002; Eiter et al. 2004) all the results in Theorems 1-5 can be turned into completeness results already for this fragment.", "startOffset": 74, "endOffset": 106}, {"referenceID": 10, "context": "Furthermore, by well-known results on the complexity of action language C (Turner 2002; Eiter et al. 2004) all the results in Theorems 1-5 can be turned into completeness results already for this fragment.", "startOffset": 74, "endOffset": 106}, {"referenceID": 21, "context": "There are works being conducted on the verification of GOLOG programs (Levesque et al. 1997), a family of highlevel action programming languages defined on top of action theories expressed in the situation calculus.", "startOffset": 70, "endOffset": 92}, {"referenceID": 6, "context": "The method of verifying properties of non-terminal processes are sound, but do not have the guarantee of termination due to the verification problem being undecidable (De Giacomo, Ternovskaia, and Reiter 1997; Cla\u00dfen and Lakemeyer 2008).", "startOffset": 167, "endOffset": 236}, {"referenceID": 0, "context": "By resorting to action formalisms based on description logic, decidability can be achieved (Baader and Zarrie\u00df 2013).", "startOffset": 91, "endOffset": 116}, {"referenceID": 3, "context": "Verifying temporal properties of dynamic systems in the context of data management is studied by (Calvanese et al. 2013) in the presence of description logic knowledge bases.", "startOffset": 97, "endOffset": 120}, {"referenceID": 2, "context": "Some works carried out on verifying properties of agents represented in these languages, such as (Bordini et al. 2006; Dennis et al. 2012).", "startOffset": 97, "endOffset": 138}, {"referenceID": 9, "context": "Some works carried out on verifying properties of agents represented in these languages, such as (Bordini et al. 2006; Dennis et al. 2012).", "startOffset": 97, "endOffset": 138}, {"referenceID": 22, "context": "The logical framework for agent theory developed by Rao and Georgeff (1991) is based on beliefs, desires and intentions, in which agents are viewed as being rational and acting in accordance with their beliefs and goals.", "startOffset": 52, "endOffset": 76}, {"referenceID": 1, "context": "(1998b; 1998a), Bertoli et al. (2006). These approaches are able to solve difficult planning problems like strong planning and strong cyclic planning.", "startOffset": 16, "endOffset": 38}, {"referenceID": 26, "context": "The approaches that are studied are replanning (De Giacomo, Reiter, and Soutchanski 1998), backtracking to the point of failure and continuing from there (Soutchanski 2003), or diagnosing the failure and recovering from the failure situation (Fichtner, Gro\u00dfmann, and Thielscher 2003; Eiter et al.", "startOffset": 154, "endOffset": 172}, {"referenceID": 12, "context": "The approaches that are studied are replanning (De Giacomo, Reiter, and Soutchanski 1998), backtracking to the point of failure and continuing from there (Soutchanski 2003), or diagnosing the failure and recovering from the failure situation (Fichtner, Gro\u00dfmann, and Thielscher 2003; Eiter et al. 2007).", "startOffset": 242, "endOffset": 302}], "year": 2016, "abstractText": "We describe a representation in a high-level transition system for policies that express a reactive behavior for the agent. We consider a target decision component that figures out what to do next and an (online) planning capability to compute the plans needed to reach these targets. Our representation allows one to analyze the flow of executing the given reactive policy, and to determine whether it works as expected. Additionally, the flexibility of the representation opens a range of possibilities for designing behaviors. Autonomous agents are systems that decide for themselves what to do to satisfy their design objectives. These agents have a knowledge base that describes their capabilities, represents facts about the world and helps them in reasoning about their course of actions. A reactive agent interacts with its environment. It perceives the current state of the world through sensors, consults its memory (if there is any), reasons about actions to take and executes them in the environment. A policy for these agents gives guidelines to follow during their interaction with the environment. As autonomous systems become more common in our lives, the issue of verifying that they behave as intended becomes more important. During the operation of an agent, one would want to be sure that by following the designed policy, the agent will achieve the desired results. It would be highly costly, time consuming and sometimes even fatal to realize at runtime that the designed policy of the agent does not provide the expected properties. For example, in search and rescue scenarios, an agent needs to find a missing person in unknown environments. A naive approach would be to directly try to find a plan that achieves the main goal of finding the person. However, this problem easily becomes troublesome, since not knowing the environment causes the planner to consider all possible cases and find a plan that guarantees reaching the goal in all settings. Alternatively, one can describe a reactive policy for the agent that determines its course of actions according to its current knowledge, and guides the agent in the environment towards the main goal. A possible such policy could be \u201calways \u2217This work has been supported by Austrian Science Fund (FWF) project W1255-N23. Copyright c \u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. move to the farthest unvisited point in visible distance, until a person is found\u201d. Following this reactive policy, the agent would traverse the environment by choosing its actions to reach the farthest possible point from the current state, and by reiterating the decision process after reaching a new state. The agent may also remember the locations it has been in and gain information (e.g. obstacle locations) through its sensors on the way. Verifying beforehand whether or not the designed policy of the agent satisfies the desired goal (e.g. can the agent always find the person?), in all possible instances of the environment is nontrivial. Action languages (Gelfond and Lifschitz 1998) provide a useful framework on defining actions and reasoning about them, by modeling dynamic systems as transition systems. Their declarative property helps in describing the system in an understandable, concise language, and they also address the problems encountered when reasoning about actions. By design, these languages are made to be decidable, which ensures reliable descriptions of dynamic systems. As these languages are closely related with classical logic and answer set programming (ASP) (Lifschitz 2008; 1999), they can be translated into logic programs and queried for computation. The programs produced by such translations can yield sound and complete answers to such queries. There have been various works on action languages (Gelfond and Lifschitz 1998; 1993; Giunchiglia and Lifschitz 1998) and their reasoning systems (Giunchiglia et al. 2004; Gebser, Grote, and Schaub 2010), with underlying mechanisms that rely on SAT and ASP solvers. The shortage of representations that are capable of modeling reactive policies prevents one from verifying such policies using action languages as above before putting them into use. The necessity of such a verification capability motivates us to address this issue. We thus aim for a general model that allows for verifying the reactive behavior of agents in environments with different types in terms of observability and determinism. In that model, we want to use the representation power of the transition systems described by action languages and combine components that are efficient for describing reactivity. Towards this aim, we consider in this paper agents with a reactive behavior that decide their course of actions by determining targets to achieve during their interaction with the environment. Such agents come with an (online) planning ar X iv :1 60 3. 09 49 5v 1 [ cs .A I] 3 1 M ar 2 01 6 capability that computes plans to reach the targets. This method matches the observe-think-act cycle of Kowalski and Sadri (1999), but involves a planner that considers targets. The flexibility in the two components target development and external planning allow for a range of possibilities for designing behaviors. For example, one can use HEX (Eiter et al. 2005) to describe a program that determines a target given the current state of an agent, finds the respective plan and the execution schedule. ACTHEX programs (Fink et al. 2013), in particular, provide the tools to define such reactive behaviors as it allows for iterative evaluation of the logic programs and the ability to observe the outcomes of executing the actions in the environment. Specifically, we make the following contributions: (1) We introduce a novel framework for describing the semantics of a policy that follows a reactive behavior, by integrating components of target establishment and online planning. The purpose of this work is not synthesis, but to lay foundations for verification of behaviors of (human-designed) reactive policies. The outsourced planning might also lend itself for modular, hierarchic planning, where macro actions (expressed as targets) are turned into a plan of micro actions. Furthermore, outsourced planning may also be exploited to abstract from correct sub-behaviors (e.g. going always to the farthest point). (2) We relate this to action languages and discuss possibilities for policy formulation. In particular, we consider the action language C (Giunchiglia and Lifschitz 1998) to illustrate an application. The remainder of this paper is organized as follows. After some preliminaries, we present a running example and then the general framework for modeling policies with planning. After that, we consider the relation to action languages, and as a particular application we consider (a fragment of) the action language C. We briefly discuss some related work and conclude with some issues for ongoing and future work. Preliminaries Definition 1. A transition system T is defined as T = \u3008S, S0,A,\u03a6\u3009 where \u2022 S is the set of states. \u2022 S0 \u2286 S is the set of possible initial states. \u2022 A is the set of possible actions. \u2022 \u03a6 : S \u00d7 A \u2192 2 is the transition function, returns the set of possible successor states after applying a possible action in the current state. For any states s, s\u2032 \u2208 S, we say that there is a trajectory between s and s\u2032, denoted by s\u2192\u03c3 s\u2032 for some action sequence \u03c3 = a1, . . . , an where n \u2265 0, if there exist s0, . . . , sn \u2208 S such that s = s0, s\u2032 = sn and si+1 \u2208 \u03a6(si, ai+1) for all 0 \u2264 i < n. We will refer to this transition system as the original transition system. The constituents S and A are assumed to be finite in the rest of the paper. Note that, this transition system represents fully observable settings. Large environments cause high number of possibilities for states, which cause the transition systems to be large. Especially, if the environment is nondeterministic, the resulting transition system contains high amount of transitions between states, since one needs to consider all possible outcomes of executing an action.", "creator": "TeX"}}}