{"id": "1604.00077", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2016", "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection", "abstract": "recurrent neural network architectures combining with attention mechanism, attribute or neural attention model, have previously shown potentially promising performance recently for the tasks : including speech track recognition, image caption generation, visual question answering and machine translation. in this paper, neural attention index model is applied on two sequence classification tasks, dialogue act detection and complex key term extraction. in separating the sequence labeling optimization tasks, the model input is a speech sequence, and the output is the label of the input sequence. the major difficulty present of sequence labeling is being that when the input sequence is long, it can include many noisy or irrelevant part. if so the information input in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. the attention mechanism is helpful for creating sequence classification task because it is capable of efficiently highlighting important part among the entire sequence diagram for the classification recording task. the experimental results show for that with the attention mechanism, discernible improvements seen were achieved in the sequence labeling task considered here. the roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.", "histories": [["v1", "Thu, 31 Mar 2016 23:17:46 GMT  (4037kb,D)", "http://arxiv.org/abs/1604.00077v1", "5 pages, 2 figures"]], "COMMENTS": "5 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sheng-syun shen", "hung-yi lee"], "accepted": false, "id": "1604.00077"}, "pdf": {"name": "1604.00077.pdf", "metadata": {"source": "CRF", "title": "Neural Attention Models for Sequence Classification: Analysis and Application to Key Term Extraction and Dialogue Act Detection", "authors": ["Sheng-syun Shen", "Hung-yi Lee"], "emails": ["r03942071@ntu.edu.tw,", "hungyilee@ntu.edu.tw"], "sections": [{"heading": null, "text": "1. Introduction Recently, attention-mechanism has been incorporated with recurrent neural networks, and has shown significant improvement on a great variety of tasks. Attention mechanism is first introduced by Bahdanau et al. [1] in the task of machine translation. They proposed an recurrent neural network (RNN) [2,3] encoder-decoder model for end-to-end translation, and this mechanism is intuitively designed in order to take care about the positions of input elements according to previous output result. Inspired by this work, Chorowski et al. [4] then proposed attention-based models for speech recognition, which are claimed to be robust to long inputs. Kelvin Xu et al. [5] and Huijuan Xu et al. [6] also demonstrated how attention mechanism works while reading a picture. The above works iteratively process their input by selecting relevant content at every step. Attention-mechanism are also useful for tasks other than sequence to sequence learning. Memory Neural Networks (MemNN) which are developed by Weston et al. [7] and Sukhbaatar et al. [8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.\nIn this paper, neural attention model is applied on sequence classification tasks. In a sequence classification task,\nthe input of the model is a sequence, and the model output is the class of the sequence. Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc. One of the major difficulties for sequence classification is that when the input sequence is long, it can include many noisy or irrelevant parts, and without techniques to ignore these parts, they may degrade the classification performance. Attention-mechanism shows the potential of automatically ignoring the unimportant parts in the entire input sequence and highlighting the important parts [7\u20139]. This inspires us to explore the use of attention mechanism on sequence classification.\nIn this paper, we present a novel attention-mechanism long short-term memory (LSTM) [22, 23] network architecture for sequence classification, in which the LSTM network reads the entire input, attention-mechanism highlights the important elements, and the sequence classes are predicted by the highlighted parts. This model is first tested on dialogue act detection in which the model input is the transcriptions of one to several utterances, and the output is the dialogue acts. It is shown that the attention-mechanism is especially helpful with longer input. We further formulate the key term extraction as sequence classification task [18], and apply the proposed model. This methodology shows promising results on key term extraction. Finally, visualization and analysis are also performed to understand how the attention process works."}, {"heading": "2. Neural Attention Model for Sequence Classification", "text": "The overall structure of the proposed method is in Figure 1. The inputs of model would be represented as a dense sequence vector OT , which will be described in section 2.1. With the sequence vector, attention mechanism is then applied to extract related information from input sequence in section 2.2. In section 2.3, the model will predict target according to the selected feature vectors."}, {"heading": "2.1. Sequence Representation", "text": "We use recurrent neural networks (RNN) for encoding. RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years. We select long short-term memory (LSTM) networks, a type of recurrent neural networks with a more complex computational unit, to\nar X\niv :1\n60 4.\n00 07\n7v 1\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 6\nprocesses inputs sequentially. A brief introduction of LSTMs can be found in [22, 23].\nIn the upper part of Figure 1, we demonstrate the encoding procedure to transform input sequences into fixed-length vector representation OT . The set x = (x1, x2, . . . , xT ) denotes the input sequence, where T is the sequence length. Each element in x represents a fixed-length feature vector. For example, it might be a high dimensional 1-of-N encoding unigram vector for the task of text classification. In order to reduce the model complexity, we set an embedding layer, a linear transformation matrix, to turn the inputs into low dimensional dense vectors V = (V1, V2, . . . , VT ), and then they will be sent to the LSTM encoder. In each time step, the LSTM takes one element Vi from feature vector set, and after processing the last element, it then generates an output vector OT , which can be regarded as the summaries of the preceding feature vectors."}, {"heading": "2.2. Attention Mechanism", "text": "When input sequence x is long, the summaries vector OT is likely to contain noisy information from many irrelevant feature vectors Vi, we thus apply attention mechanism to select only relevant frames among the entire sequence. The procedures are shown in the lower part of Figure 1. There is also an embedding layer to transform input sequences into dense vectors, and all the parameters in the embedding layer are shared with the previous one. We then calculate the cosine similarity between the sequence vector OT and word embedding set V :\nei = OT Vi, (1)\nwhere denotes cosine similarity between two vectors. As a result, we have a list of score e = (e1, e2, . . . , eT ). The attention weights \u03b1 = (\u03b11, \u03b12, . . . , \u03b1T ) come from the normalized score list e. Due to some considerations, we normalize the scores in two ways, which is inspired by Chorowski et al. in [4]:\nSharpening: The score list is normalized using softmax activation function:\n\u03b1i = exp(ei)\u2211T i=1 exp(ei) , (2)\nIt has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.\nSmoothing: The sharpening normalization method prefers to mostly focus on only a single feature vector Vi, and might negatively affects the model\u2019s performance. We then apply a new way for the model to aggregate selections from multiple top-scored frames. In this way, more input locations are considered for bringing more diversity to the model. We replace the exponential function in equation (2) with logistic sigmoid function \u03c3 :\n\u03b1i = \u03c3(ei)\u2211T i=1 \u03c3(ei)\n(3)\nVisualization and analysis of the both normalization functions are provided in the experiment section."}, {"heading": "2.3. Target Selection", "text": "The right part of Figure 1 illustrates the target selection procedures. We weighted sum all the feature vectors as \u2211 \u03b1iVi, and sending it to a fully connected layer. Usually, the neurons in this layer are activated by nonlinear functions. The last layer is for target prediction, and the dimension is set to be candidate target numbers."}, {"heading": "3. Experiments", "text": "We conducted two sequence classification tasks in this section. In section 3.1, we describe the definition of dialogue act detection, and also demonstrate the experimental results. In section 3.2, we introduce how to apply the proposed methodology on key term extraction task. The role of attention mechanism during classification procedure will be discussed in section 3.3, and we also show the visualization results."}, {"heading": "3.1. Dialogue Act Detection", "text": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue. This prediction task is still challenging because there are various distinct ways of formulating an intention. In this work, DAs are labeled with one of a number of tags. For example, the tag <OFFER> is related to the situation that someone commands partner to carry out actions, e.g., \u201cYou need to give me your ideas, and then I need to see whether that would sell in the market place.\u201d"}, {"heading": "3.1.1. Experimental setup", "text": "We conducted experiments on Switchboard Dialog Act (SwDA) Corpus [27], which is a corpus of telephone conversations on selected topics. It consists of about 2,500 conversations by 500 speakers from the U.S. The conversations in the corpus are labeled with 43 unique dialogue act tags and split to 1,115 train and 19 test conversations. The training and testing corpus respectively contain 213,543 and 4,514 utterances, having average length of about 8 words."}, {"heading": "3.1.2. Baselines", "text": "We compared the proposed model with the following baselines. Support Vector Machines: SVM is the most common way to be adopted for text classification. Silva et al. [28] chose sentence unigrams as input feature vector, and trained the SVM model. We extracted one-of-N encoding unigram features for every word in the dataset, aggregating them together for each training example. To reduce the number of dimensions, we set\nminimum word counts to 5. The Radial basis function (RBF) [29] kernel was also applied.\nMultiple Layer Perceptron: The work introduced by Ries et. al [30] is the first approach that importing artificial neural networks (ANN) for dialogue act detection. We also extracted unigram features as the model input for experiments. We trained an MLP model with 3 hidden layers. Each hidden layer has 512 neurons. The relu activation function was applied on every hidden layer, and we set rmsprop as the optimizer. The training epoch was set to be 20.\nLong Short-term Memory: In order to examine the use of attention mechanism, we also implemented the original LSTM network. The LSTM model takes one word from the input sequence in each time step. We applied word embedding for unigram features, thus the high dimensional sparse vectors are transformed into dense vectors. The embedding size was 400, and we set the dimension of recurrent layers as 128 and the fully connected layer before output as 500, respectively. To avoid overfitting, we only trained the LSTM network for 10 epochs."}, {"heading": "3.1.3. Experimental results", "text": "We implemented both sharpening-attend and smoothing-attend neural attention model in the experiments. The LSTM part of the proposed model is the same as the original LSTM briefly illustrated in the previous subsection, and the hyper-parameters for model training was also the same. As the previous work stated [31], context information from previous utterances may help for the dialogue act prediction. Therefore, we also appended n previous utterances to the the utterance being classified, and n was set to be 3 in the experiments.\nThe results are reported in Table 1. Rows (a) to (d) are the baseline results, and the results of the proposed approaches are in rows (e) to (h). It is clear that the LSTM networks already outperformed the other baselines (rows (c) vs (a), (b)) because the LSTM networks have better capability of handling sequence information than multiple layer perceptrons and support vectors. Moreover, with context information the LSTM can have higher accuracy than the one without it (rows (d) vs (c)).\nConsidering the case without context information, the proposed approaches show improvements comparing to all the baselines no matter the attention is sharpening or smoothing (rows (e), (f) vs (a), (b), (c)). The neural attention model with sharpening attention is only slightly better than the original LSTM (rows (e) v.s. (c)), but the smoothing attention shows significant improvement (rows (f) v.s. (c)). Besides, we also know that the prediction of sequence classification cannot just rely on the most relevant element, the rest of the relevant part should also be considered. Neural attention model with sharpening attention does not show any improvement after adding context information into the prediction procedure (rows (g) v.s. (e)). This is because the sharpening-attend mechanism only focuses on the most relevant part of the input sequence, adding more candidates would not be helpful. On the other hand, when using smoothing attention, context information became very helpful (rows (h) v.s. (f)). This shows that smoothing attention can better exploit the context information than sharpening attention."}, {"heading": "3.2. Key Term Extraction", "text": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document. Key terms may possibly describe the core concept or summary of a document, which can help users understand, organize, and extract important information efficiently from documents. These terms are\nusually manually labeled by humans according to cognition and domain knowledge, so automatic key term extraction is not an easy task.\nKey term extraction can be regarded as a sequence classification problem [18]. The model input is a document, while the model selects some terms as key terms from a set of candidates. Each term in the set of candidate terms is considered as a class, and the documents containing the same key terms belong to the same class. In our task, chances are that some terms do not exist in the document, but they represent the core concepts of the document. These terms are also regarded as key terms here, which makes this task even more difficult. It is possible that a document has more than one key term, or a document can belong to multiple classes. However, the number of key terms in each testing document is unknown, as a result we consider this task to be a ranking problem. That is, the model assigns a score to each candidate term. Then, the candidate terms are ranked according to the scores. The target of the system is to rank the key terms above the non key terms.\nIn training procedures, each document with n labeled key terms would be mapped into a sparse vector, which is the probability training target. The dimension of this sparse vector is the number of candidate terms. Most of the values are zero, only the indexes corresponding to labeled terms would be assigned to a value 1\nn , and the summation of this vector is 1. For exam-\nple, assuming we have 1,000 term candidates and the number of labeled key terms is 4 in a document, we then have an 1,000- dimension sparse target vector with only 4 elements all assigned with 1\n4 ."}, {"heading": "3.2.1. Experimental setup", "text": "We collected the data from Stack Overflow1 website where serves as a platform for users to ask and answer questions. While users of Stack Overflow post questions on the forum, they are asked to label 2~6 key terms for each post. The dataset we collected includes 290,000 examples in total (250,000 for training and 40,000 for testing), and there are about 24,000 kinds of labeled key term. Each example contains a post and 2~6 key term labels, and the average length of the article is about 120 words. The collected dataset is available for download. 2\nIn practice, to reduce the training complexity, we only selected the 1,000 most frequent key terms in the training set as candidates. These top 1,000 candidates cover over 76% of the key term labels in the training set, so we can still expect to get reasonable results.\n1 http://stackoverflow.com/ 2http://speech.ee.ntu.edu.tw/\u02dcsense/\nstackoverflow_pack.zip"}, {"heading": "3.2.2. Baselines", "text": "We implemented multiple layer perceptrons (MLP) and long short-term memory (LSTM) networks as the baseline models, which have already been described in section 3.1.2.\nTf-idf Sorting is the baseline we also applied. \u201cTf-idf\u201d is the abbreviation of term frequency-inverse document frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. A brief introduction about how Tf-idf Sorting extracts key terms can be found in [32]. We calculated the tf-idf values of a set of candidate key terms according to the dataset, and these candidates were sorted by their values. We then reported the ranking list for evaluation."}, {"heading": "3.2.3. Experimental results", "text": "To examine the prediction result, we chose MAP and P@R as the evaluation methods. The MAP score for a set of documents is the mean of the average precision scores for each document. P@R is defined as the precision after R elements have been selected by the system, where R is also the total number of judged relevant results for the given inputs. Precision is defined as the portion of returned results that are truly belong to the ground truth set.\nThe experimental results are demonstrated in Table 2. Row (a) is the oracle score, which is for reference. Since we only selected 1,000 most frequent key terms as candidates from the training set, we can\u2019t achieve 100% accurate performance. The score of baseline approaches we applied are in rows (b) to (d), and rows (e), (f) are the performance of the proposed neural at-\ntention model. The supervised learning baselines outperformed the Tf-idf Sorting baseline (rows (c), (d) vs (b)). That is because without supervised learning, we may not fit the dataset, and we also can\u2019t predict the key terms which do not exist in the document. Besides, like the experiment we previously conducted, LSTM shows better ability of handling sequence information in comparison to original neural networks (rows (d) vs (c)), so the LSTM network performs better while using both MAP and P@R as evaluation methods. We found that the performance of our neural attention model with sharpening-attend mechanism degraded while comparing to the original LSTM (rows (e) vs (d), but the one with smoothing attention outperformed all the other approaches (rows (f) vs (b), (c), (d), (e)). This result proved that adding more relevant elements into consideration can help solving sequence classification problems."}, {"heading": "3.3. Visualization and Analysis", "text": "Figure 2 demonstrates the visualization of how attentionmechanism works in the sequence classification tasks. The upper row is for dialogue act detection and the lower row is for key term extraction. The darker the color, the higher the weights. We only chose the smoothing-attend mechanism for visualization due to its better performance. According to this figure, we found that attention weights are capable of reducing sentence disfluency problems and filtering out most of the unimportant elements such as function words."}, {"heading": "4. Conclusions", "text": "In this paper, we proposed a neural attention model for sequence classification. In such kinds of task, the input of model is a sequence, and the output is the class of sequence. The major difficulty is that when the input sequence is long, the noisy or irrelevant part may degrade the classification performance. The proposed model can reduce the influences because it is able to highlight important part among the entire sequence. In the experiments, the neural attention model can achieve 72.6% accuracy for dialogue act detection task and 50.5% MAP score for key term extraction task, which shows discernible improvements comparing to the other approaches.\n5. References [1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neu-\nral machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[2] Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211, 1990.\n[3] Michael I Jordan. Serial order: A parallel distributed processing approach. Advances in psychology, 121:471\u2013495, 1997.\n[4] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In Advances in Neural Information Processing Systems, pages 577\u2013585, 2015.\n[5] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.\n[6] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual question answering. arXiv preprint arXiv:1511.05234, 2015.\n[7] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.\n[8] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-toend memory networks. In Advances in Neural Information Processing Systems, pages 2431\u20132439, 2015.\n[9] Wei-Ning Hsu, Yu Zhang, and James Glass. Recurrent neural network encoder with attention for community question answering, 2016.\n[10] Najim Dehak, Reda Dehak, Patrick Kenny, Niko Brummer, Pierre Ouellet, and Pierre Dumouchel. Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification. In INTERSPEECH, 2009.\n[11] Bjorn Schuller, Stefan Steidl, and Anton Batliner. The INTERSPEECH 2009 emotion challenge. In INTERSPEECH, 2009.\n[12] Hung-Yi Lee and Lin-Shan Lee. Enhanced spoken term detection using support vector machines and weighted pseudo examples. Audio, Speech, and Language Processing, IEEE Transactions on, 21(6):1272\u20131284, 2013.\n[13] I.-F. Chen and C.-H. Lee. A hybrid HMM/DNN approach to keyword spotting of short words. In INTERSPEECH, 2013.\n[14] A. Norouzian, A. Jansen, R. Rose, and S. Thomas. Exploiting discriminative point process models for spoken term detection. In INTERSPEECH, 2012.\n[15] Max M Louwerse and Scott A Crossley. Dialog act classification using n-gram algorithms. In FLAIRS Conference, pages 758\u2013763, 2006.\n[16] Kristy Elizabeth Boyer, Joseph F Grafsgaard, Eun Young Ha, Robert Phillips, and James C Lester. An affect-enriched dialogue act classification model for task-oriented dialogue. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 1190\u20131199. Association for Computational Linguistics, 2011.\n[17] Dinoj Surendran and Gina-Anne Levow. Dialog act tagging with support vector machines and hidden markov models. In INTERSPEECH, 2006.\n[18] Kamal Sarkar, Mita Nasipuri, and Suranjan Ghose. A new approach to keyphrase extraction using neural networks. arXiv preprint arXiv:1004.3274, 2010.\n[19] Yun-Nung Chen, Yu Huang, Sheng-Yi Kong, and Lin-Shan Lee. Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features. In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 265\u2013270. IEEE, 2010.\n[20] Hiroshi Nakagawa and Tatsunori Mori. A simple but powerful automatic term extraction method. In COLING-02 on COMPUTERM 2002: second international workshop on computational terminology-Volume 14, pages 1\u20137. Association for Computational Linguistics, 2002.\n[21] Yun-Nung Chen, Wei Yu Wang, and Alexander I Rudnicky. An empirical investigation of sparse log-linear models for improved dialogue act classification. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 8317\u20138321. IEEE, 2013.\n[22] Felix A Gers and Ju\u0308rgen Schmidhuber. Lstm recurrent networks learn simple context-free and context-sensitive languages. Neural Networks, IEEE Transactions on, 12(6):1333\u20131340, 2001.\n[23] Sepp Hochreiter and Ju\u0308rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.\n[24] Kaisheng Yao, Baolin Peng, Yu Zhang, Dong Yu, Geoffrey Zweig, and Yangyang Shi. Spoken language understanding using long short-term memory neural networks. In Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 189\u2013194. IEEE, 2014.\n[25] Kaisheng Yao, Geoffrey Zweig, Mei-Yuh Hwang, Yangyang Shi, and Dong Yu. Recurrent neural networks for language understanding. In INTERSPEECH, pages 2524\u20132528, 2013.\n[26] Gre\u0301goire Mesnil, Yann Dauphin, Kaisheng Yao, Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xiaodong He, Larry Heck, Gokhan Tur, Dong Yu, et al. Using recurrent neural networks for slot filling in spoken language understanding. Audio, Speech, and Language Processing, IEEE/ACM Transactions on, 23(3):530\u2013539, 2015.\n[27] Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca. Switchboard swbd-damsl shallow-discourse-function annotation coders manual. Institute of Cognitive Science Technical Report, pages 97\u2013 102, 1997.\n[28] Joao Silva, Lu\u0131\u0301sa Coheur, Ana Cristina Mendes, and Andreas Wichert. From symbolic to sub-symbolic information in question classification. Artificial Intelligence Review, 35(2):137\u2013154, 2011.\n[29] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Michael Ringgaard, and Chih-Jen Lin. Training and testing low-degree polynomial data mappings via linear svm. The Journal of Machine Learning Research, 11:1471\u20131490, 2010.\n[30] Klaus Ries. Hmm and neural network based speech act detection. In Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on, volume 1, pages 497\u2013 500. IEEE, 1999.\n[31] Euge\u0301nio Ribeiro, Ricardo Ribeiro, and David Martins de Matos. The influence of context on dialogue act recognition. arXiv preprint arXiv:1506.00839, 2015.\n[32] http://stevenloria.com/ finding-important-words-in-a-document-using-tf-idf/."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Serial order: A parallel distributed processing approach", "author": ["Michael I Jordan"], "venue": "Advances in psychology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "arXiv preprint arXiv:1511.05234,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "End-toend memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Recurrent neural network encoder with attention for community question answering, 2016", "author": ["Wei-Ning Hsu", "Yu Zhang", "James Glass"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Support vector machines versus fast scoring in the low-dimensional total variability space for speaker verification", "author": ["Najim Dehak", "Reda Dehak", "Patrick Kenny", "Niko Brummer", "Pierre Ouellet", "Pierre Dumouchel"], "venue": "In INTERSPEECH,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "The INTER- SPEECH 2009 emotion challenge", "author": ["Bjorn Schuller", "Stefan Steidl", "Anton Batliner"], "venue": "In INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Enhanced spoken term detection using support vector machines and weighted pseudo examples. Audio, Speech, and Language Processing", "author": ["Hung-Yi Lee", "Lin-Shan Lee"], "venue": "IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A hybrid HMM/DNN approach to keyword spotting of short words", "author": ["I.-F. Chen", "C.-H. Lee"], "venue": "In INTERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Exploiting discriminative point process models for spoken term detection", "author": ["A. Norouzian", "A. Jansen", "R. Rose", "S. Thomas"], "venue": "In INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Dialog act classification using n-gram algorithms", "author": ["Max M Louwerse", "Scott A Crossley"], "venue": "In FLAIRS Conference,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "An affect-enriched dialogue act classification model for task-oriented dialogue", "author": ["Kristy Elizabeth Boyer", "Joseph F Grafsgaard", "Eun Young Ha", "Robert Phillips", "James C Lester"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Dialog act tagging with support vector machines and hidden markov models", "author": ["Dinoj Surendran", "Gina-Anne Levow"], "venue": "In INTER- SPEECH,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "A new approach to keyphrase extraction using neural networks", "author": ["Kamal Sarkar", "Mita Nasipuri", "Suranjan Ghose"], "venue": "arXiv preprint arXiv:1004.3274,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features", "author": ["Yun-Nung Chen", "Yu Huang", "Sheng-Yi Kong", "Lin-Shan Lee"], "venue": "In Spoken Language Technology Workshop (SLT), 2010 IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "A simple but powerful automatic term extraction method", "author": ["Hiroshi Nakagawa", "Tatsunori Mori"], "venue": "In COLING-02 on COMPUT- ERM 2002: second international workshop on computational terminology-Volume", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "An empirical investigation of sparse log-linear models for improved dialogue act classification", "author": ["Yun-Nung Chen", "Wei Yu Wang", "Alexander I Rudnicky"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Biasca. Switchboard swbd-damsl shallow-discourse-function annotation coders manual", "author": ["Dan Jurafsky", "Elizabeth Shriberg", "Debra"], "venue": "Institute of Cognitive Science Technical Report,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1997}, {"title": "From symbolic to sub-symbolic information in question classification", "author": ["Joao Silva", "Lu\u0131\u0301sa Coheur", "Ana Cristina Mendes", "Andreas Wichert"], "venue": "Artificial Intelligence Review,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Yin-Wen Chang", "Cho-Jui Hsieh", "Kai-Wei Chang", "Michael Ringgaard", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Hmm and neural network based speech act detection", "author": ["Klaus Ries"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1999}, {"title": "The influence of context on dialogue act recognition", "author": ["Eug\u00e9nio Ribeiro", "Ricardo Ribeiro", "David Martins de Matos"], "venue": "arXiv preprint arXiv:1506.00839,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1] in the task of machine translation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "They proposed an recurrent neural network (RNN) [2,3] encoder-decoder model for end-to-end translation, and this mechanism is intuitively designed in order to take care about the positions of input elements according to previous output result.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "They proposed an recurrent neural network (RNN) [2,3] encoder-decoder model for end-to-end translation, and this mechanism is intuitively designed in order to take care about the positions of input elements according to previous output result.", "startOffset": 48, "endOffset": 53}, {"referenceID": 3, "context": "[4] then proposed attention-based models for speech recognition, which are claimed to be robust to long inputs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] and Huijuan Xu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] also demonstrated how attention mechanism works while reading a picture.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.", "startOffset": 47, "endOffset": 52}, {"referenceID": 7, "context": "[8] can deal with question answering (QA) task [7\u20139], and the attention-mechanism plays an important role in the model.", "startOffset": 47, "endOffset": 52}, {"referenceID": 8, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 93, "endOffset": 97}, {"referenceID": 9, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 162, "endOffset": 169}, {"referenceID": 11, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 162, "endOffset": 169}, {"referenceID": 12, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 162, "endOffset": 169}, {"referenceID": 13, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 194, "endOffset": 201}, {"referenceID": 14, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 194, "endOffset": 201}, {"referenceID": 15, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 194, "endOffset": 201}, {"referenceID": 16, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 17, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 18, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 19, "context": "Many common tasks can be formulated as sequence classification including speaker recognition [10], audio emotion classification [11], spoken term detection (STD) [12\u201314], dialogue act detection [15\u201317], key term extraction [18\u201321], etc.", "startOffset": 223, "endOffset": 230}, {"referenceID": 6, "context": "Attention-mechanism shows the potential of automatically ignoring the unimportant parts in the entire input sequence and highlighting the important parts [7\u20139].", "startOffset": 154, "endOffset": 159}, {"referenceID": 7, "context": "Attention-mechanism shows the potential of automatically ignoring the unimportant parts in the entire input sequence and highlighting the important parts [7\u20139].", "startOffset": 154, "endOffset": 159}, {"referenceID": 20, "context": "In this paper, we present a novel attention-mechanism long short-term memory (LSTM) [22, 23] network architecture for sequence classification, in which the LSTM network reads the entire input, attention-mechanism highlights the important elements, and the sequence classes are predicted by the highlighted parts.", "startOffset": 84, "endOffset": 92}, {"referenceID": 21, "context": "In this paper, we present a novel attention-mechanism long short-term memory (LSTM) [22, 23] network architecture for sequence classification, in which the LSTM network reads the entire input, attention-mechanism highlights the important elements, and the sequence classes are predicted by the highlighted parts.", "startOffset": 84, "endOffset": 92}, {"referenceID": 16, "context": "We further formulate the key term extraction as sequence classification task [18], and apply the proposed model.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years.", "startOffset": 151, "endOffset": 158}, {"referenceID": 23, "context": "RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years.", "startOffset": 151, "endOffset": 158}, {"referenceID": 24, "context": "RNNs are capable of handling sequence information over time, so they have demonstrated outstanding performance on natural language understanding tasks [24\u201326] in recent years.", "startOffset": 151, "endOffset": 158}, {"referenceID": 20, "context": "A brief introduction of LSTMs can be found in [22, 23].", "startOffset": 46, "endOffset": 54}, {"referenceID": 21, "context": "A brief introduction of LSTMs can be found in [22, 23].", "startOffset": 46, "endOffset": 54}, {"referenceID": 3, "context": "in [4]: Sharpening: The score list is normalized using softmax activation function:", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 4, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 5, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 7, "context": "It has been widely used in many existing neural attention frameworks [1,5\u20137,9], and is capable of solving the data noisy issue.", "startOffset": 69, "endOffset": 78}, {"referenceID": 13, "context": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue.", "startOffset": 28, "endOffset": 35}, {"referenceID": 14, "context": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue.", "startOffset": 28, "endOffset": 35}, {"referenceID": 15, "context": "Dialogue act (DA) detection [15\u201317] is about categorizing the intention behind the speaker\u2019s move in conversations, and recognition of a speaker\u2019s act may help reason the entire dialogue.", "startOffset": 28, "endOffset": 35}, {"referenceID": 25, "context": "We conducted experiments on Switchboard Dialog Act (SwDA) Corpus [27], which is a corpus of telephone conversations on selected topics.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "[28] chose sentence unigrams as input feature vector, and trained the SVM model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The Radial basis function (RBF) [29] kernel was also applied.", "startOffset": 32, "endOffset": 36}, {"referenceID": 28, "context": "al [30] is the first approach that importing artificial neural networks (ANN) for dialogue act detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "As the previous work stated [31], context information from previous utterances may help for the dialogue act prediction.", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 17, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 18, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 19, "context": "The goal of key term extraction [18\u201321] is to automatically extract relevant terms from a given document.", "startOffset": 32, "endOffset": 39}, {"referenceID": 16, "context": "Key term extraction can be regarded as a sequence classification problem [18].", "startOffset": 73, "endOffset": 77}], "year": 2016, "abstractText": "Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence labeling tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.", "creator": "LaTeX with hyperref package"}}}