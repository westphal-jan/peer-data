{"id": "1604.01518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Simple and Efficient Learning using Privileged Information", "abstract": "the support vector machine using privileged data information ( svm + ) has continuously been proposed to train a classifier to utilize only the considerably additional privileged information that is only available in developing the training phase but not when available in the test phase. in this work, we propose an efficient solution for svm + by simply utilizing lowering the squared hinge loss instead of the unit hinge loss as in the existing svm + formulation, which interestingly leads to a dual form with noticeably less variables and in the same form occurs with the dual of the standard svm. the proposed algorithm is utilized to leverage the additional web knowledge that simulation is only quickly available during practical training for the image categorization tasks. although the extensive experimental results on both caltech101 andwebqueries datasets show that our proposed method can achieve a factor factor of up to hundred times speedup with the comparable design accuracy when compared with the longer existing svm + method.", "histories": [["v1", "Wed, 6 Apr 2016 07:33:55 GMT  (16kb)", "http://arxiv.org/abs/1604.01518v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xinxing xu", "joey tianyi zhou", "ivorw tsang", "zheng qin", "rick siow mong goh", "yong liu"], "accepted": false, "id": "1604.01518"}, "pdf": {"name": "1604.01518.pdf", "metadata": {"source": "CRF", "title": "Simple and Efficient Learning using Privileged Information", "authors": ["Xinxing Xu", "Joey Tianyi Zhou", "Ivor W. Tsang", "Zheng Qin", "Rick Siow Mong Goh", "Yong Liu"], "emails": ["liuyong}@ihpc.a-star.edu.sg,", "ivor.tsang@uts.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n01 51\n8v 1\n[ cs\n.L G\n] 6\nA pr\n2 01"}, {"heading": "1 Introduction", "text": "In traditional machine learning paradigm, the classifiers are trained based on the features of training data, and the test is done using the same type of features. However, in real-world applications, there will be additional information that is only available during training but not available during test. For example, for the image categorization problem, we are usually classifying the images from different categories. Take the Caltech101 data set as an example, there are a total number of 101 object categories in the training set, and each of the object category has its associated descriptions, such as the textual descriptions from Wikipedia. The descriptions about each concept can be associated to each training image simply by using its label information, and hence they are only available at the training stage, but not available during the test phase. Another example is the learning from weakly labeled web images. The images in the Web are usually associated with descriptions from tags that are uploaded by the users. The tag information can be collected during training data collection. However, during test we may only have the test images that do not contain any descriptions.\nThe aforementioned two examples fall into the new learning paradigm of the Learning using Privileged Informa-\ntion (LUPI) [Vapnik and Vashist, 2009], in which the additional information is referred to as the privileged information. The Support Vector Machine using Privileged Information (SVM+) [Vapnik and Vashist, 2009] has been proposed for utilizing the privileged information. As the hinge loss is used in [Vapnik and Vashist, 2009], we refer to the formulation in their work as SVM1+ in the following. It has been proved theoretically that the incorporating of the additional privileged information can improve the convergence rate [Pechyony and Vapnik, 2010]. The SVM+ attracts much attention recently [Vapnik and Izmailov, 2015; Lopez-Paz et al., 2015; Lapin et al., 2014], and has been successfully applied to different applications [Niu et al., 2016; Sharmanska et al., 2013; Fouad et al., 2012]. Following this learning scenario, some recent works proposed to utilize the privileged information for the different learning scenarios such as learning to rank [Sharmanska et al., 2013], Gaussian Process classifier [Herna\u0301ndez-Lobato et al., 2014], clustering [Feyereisl and Aickelin, 2012] and distance metric learning [Fouad et al., 2013; Xu et al., 2015].\nHowever, the proposed optimization method for SVM1+ in [Vapnik and Vashist, 2009] is based on its dual form. If there are a total number of n training data, the corresponding dual problem is a Quadratic Programming (QP) problem with 2n variables, which is 2 times larger than the dual form of the standard SVM [Cortes and Vapnik, 1995] with only n variables. Besides, as the constraints in the dual form of SVM+ are different with those in the dual form of the standard SVM, the efficient implementations and the off-the-shelf solvers for SVM such as LibSVM [Chang and Lin, 2001] and SVMLight [Joachims, 1999] can not be readily utilized, and the additional efforts must be spent to design the specific solvers for optimization of SVM1+ [Pechyony et al., 2010].\nTo overcome the aforementioned problem, in this work, we study the optimization of SVM+. Specifically, we propose a new objective function called Support Vector Machine using Privileged Information with Squared Hinge Loss (SVM2+) by simply replacing the hinge loss in SVM1+ with the squared hinge loss. The dual form of SVM2+ is a Quadratic Programming (QP) problem with only n variables, which significantly reduces the training complexity. What\u2019s more, the dual form of SVM2+ shares the same form with the dual of the standard SVM. Therefore, the existing off-theshelf QP solvers for SVM can be readily applied to solve the\nproposed objective function.\nWe apply our proposed approach to the image categorization tasks by leveraging the additional information from web knowledge. We evaluate our algorithms on Caltech101 dataset with the additional textual descriptions for the training data obtained from Wikipedia, as well as the WebQueries dataset with the additional tag information. The extensive empirical results show that our proposed SVM2+ can achieve 110.6 (resp. 92.5) times speedup on Caltech101 (resp.WebQueries) when compared with the solution for SVM1+ as in [Vapnik and Vashist, 2009] yet with comparable classification accuracies. Hence, the experimental evaluation demonstrates both the efficiency and the effectiveness of our proposed SVM2+ algorithm for utilizing the additional web knowledge as privileged information for the task of image categorization.\nIn the following, we firstly introduce the SVM1+ in Section 2. In Section 3, we propose the objective function and the solution for SVM2+. The experimental results are shown in Section 4 and finally the conclusions are given in Section 5."}, {"heading": "2 Support Vector Machine using Privileged Information with Hinge Loss", "text": "For the classical machine learning paradigm, we are given a set of n independent and identically distributed (IID) training pairs {(x1, y1), . . . , (xn, yn)} with xi \u2208 X \u2282 Rd and yi \u2208 Y \u2282 R that are generated according to a fixed but unknown probability distribution P (x, y). The target of any machine learning algorithm is therefore to learn a function f(x) that can make the risk functional R(f) = 1 2 \u222b\n|y \u2212 f(x)|dP (x, y) minimized. Different from the classical learning paradigm, the Learning using Privileged Information (LUPI) paradigm tackles the learning senario where an additional \u201cteacher\u201d is only available in the training phase but not available during the test phase. Specifically, we are given a set of n independent and identically distributed training triplets {(x1, z1, y1), . . . , (xn, zn, yn)} with xi \u2208 X \u2282 R\nd, zi \u2208 Z \u2282 Rs and yi \u2208 Y \u2282 R that are generated according to a fixed but unknown probability distributionP (x, z, y). In the test phase, the aim is the same with that of the classical learning paradigm, i.e., classifying any test samples xi\u2019s, where i = n+ 1, . . . , n+m. The X is referred to as the decision space as the test is done only in X , while Z is referred to as the correcting space [Pechyony and Vapnik, 2010].\nThe support vector machine using privileged information (SVM+) has been proposed in [Vapnik and Vashist, 2009] for utilizing the privileged information. Specifically, f(x) = w\u2032x + b is the decision function based on the main feature x and g(z) = z\u2032v + \u03c1 is the correcting function based on the privileged information z, and the SVM+ with hinge loss (SVM1+) is proposed to optimize the following objective\nfunction [Vapnik and Vashist, 2009]:\nmin w,v,b,\u03c1,\u03bei\n1 2 ||w||2 + C\nn \u2211\ni=1\n\u03bei + \u03bb\n2 ||v||2\ns.t., z\u2032iv + \u03c1 = \u03bei,\nyi (w \u2032xi + b) \u2265 1\u2212 \u03bei, (1)\n\u03bei \u2265 0.\nThe objective function in (1) is solved in its dual form as in [Vapnik and Vashist, 2009; Vapnik and Izmailov, 2015]. Specifically, by introducing the non-negative multipliers \u03b1 = [\u03b11, . . . , \u03b1n]\n\u2032 and \u03b2 = [\u03b21, . . . , \u03b2n]\u2032, the dual form is given as in the following Quadratic Programming (QP) problem:\nmax \u03b1,\u03b2\n\u03b1\u20321\u2212 1\n2 (\u03b1\u2299 y)\u2032K(\u03b1\u2299 y)\n\u2212 1\n2\u03bb (\u03b1+ \u03b2 \u2212 C1)\u2032K\u0303(\u03b1+ \u03b2 \u2212 C1)\ns.t., n \u2211\ni=1\n(\u03b1i + \u03b2i \u2212 C) = 0, (2)\nn \u2211\ni=1\n\u03b1iyi = 0,\n\u03b1i \u2265 0, \u03b2i \u2265 0,\nwhere y = [y1, . . . , yn]\u2032 \u2208 Rn is the label vector, 1 = [1, . . . , 1]\u2032 \u2208 Rn, \u2299 is the elementwise product between two vectors/matrices, K \u2208 Rn\u00d7n with Kij = x\u2032ixj is the kernel matrix obtained from the main feature x, while K\u0303 \u2208 Rn\u00d7n with K\u0303ij = z\u2032izj is the kernel matrix constructed using the privileged information z. Although it is linear kernel here, any type of non-linear kernel [Xu et al., 2013] can be readily utilized in (2).\nNote that the dual problem in (2) is a standard 2n \u00d7 2n QP problem, and it can be solved by the general QP solver. However, the 2n \u00d7 2n problem in (2) takes more memory and requires more training time than the original SVM dual form in a n\u00d7n QP problem. Moreover, due to the introduced constraints for the \u03b1 and \u03b2, the existing efficient solvers for the SVM (e.g., LibSVM and SVMLight) cannot be readily applied to solve (2). Therefore, additional efforts are required for the development of the efficient solution and the softwares such as [Pechyony et al., 2010].\nIn the following, we propose the SVM+ with squared hinge loss, and we are interested to see that we can obtain a QP problem in size of n\u00d7n that is also in the same form with the standard dual form of SVM and can therefore be solved using any off-the-shelf efficient QP solver developed for SVM."}, {"heading": "3 Support Vector Machine using Privileged Information with Squared Hinge Loss", "text": "In order to solve the SVM+ more efficiently and motivated by the using of squared hinge loss for SVM [Cristianini and Shawe-Taylor, 2000], we propose the Support Vector Machine using Privileged Information with Squared Hinge Loss (SVM2+). We learn the decision function f(x) = w\u2032x + b as well as the correcting function\ng(z) = v\u2032zi 1 by optimizing the following objective function:\nmin w,v,b,\u03bei\n1 2 ||w||2 + C 2\nn \u2211\ni=1\n\u03be2i + \u03bb\n2 ||v||2\ns.t., v\u2032zi = \u03bei, (3)\nyi (w \u2032xi + b) \u2265 1\u2212 \u03bei,\nwhere C and \u03bb are still the two regularization parameters. In the objective function (3), we just simply replace the original hinge loss (i.e., \u2211n\ni=1 \u03bei) in (1) with the squared hinge loss (i.e., \u2211n\ni=1 \u03be 2 i ). The improvement looks quite simple and straightforward, but we can observe the significant differences and the benefits in the dual form as shown in the following proposition 1. Proposition 1. The dual form of the optimization problem in (3) is given as in the following form:\nmin \u03b1\n1 2 (\u03b1\u2299 y)\u2032 (K+Q\u03bb \u2299 (yy \u2032)) (\u03b1\u2299 y) \u2212\u03b1\u20321,\ns.t. \u03b1 \u2265 0,\u03b1\u2032y = 0, (4)\nwhere y = [y1, . . . , yn]\u2032 \u2208 Rn is the label vector, 1 = [1, . . . , 1]\u2032 \u2208 Rn, \u2299 is the elementwise product between any two vectors/matrices, \u03b1 = [\u03b11, . . . , \u03b1n]\u2032 \u2208 Rn is the vector of the Lagrangian multipliers, K \u2208 Rn\u00d7n with Kij = x\u2032ixj is the kernel matrix constructed from x, and Q\u03bb \u2208 Rn\u00d7n is a deformed kernel matrix in the form of\nQ\u03bb = 1\n\u03bb\n(\nK\u0303\u2212 K\u0303\n(\n\u03bb C In + K\u0303\n) \u22121\nK\u0303\n)\n, (5)\nwhere K\u0303 \u2208 Rn\u00d7n with K\u0303ij = z\u2032izj is the kernel matrix constructed from z.\nNote that similarly for SVM1+, we just use the linear kernel as a demonstration, but any type of non-linear kernel can be readily utilized in (4).\nInterestingly, we observe that the optimization problem in (4) is just a n\u00d7n Quadratic Programming (QP) problem. We only need to optimize with respect to the n dual variables \u03b1 rather than the 2n dual variables as in (2). The reduced dual problem can not only save memory for optimization but also reduce the computational complexity. Although there is an additional matrix inversion operation as in (5), it can be done very efficiently when compared with the optimization of the QP problem. We reduce the size of the dual form by utilizing the squared hinge loss instead of the hinge loss.\nMore importantly, the QP problem in (4) also shares the same form with the QP problem of that of the classical SVM [Cortes and Vapnik, 1995]. The difference is the changing of the kernel matrix as in the QP problem. Specifically, we can just replace the kernel matrix K in the original SVM to be K+Q\u03bb \u2299 (yy\u2032). More interestingly, the matrix Q\u03bb is simply a transformation of the kernel matrix K\u0303 constructed on the privileged information by using (5). The QP problem in (4) can be readily solved by any existing solvers (e.g., LibSVM) specifically developed for the standard SVM.\n1We augment an additional 1 to the feature vector z to incorporate the bias term, and thus z should be replaced with [z; 1] \u2208 Rs+1, but we still use z to represent the augmented feature vector for ease of presentation."}, {"heading": "4 Experiments", "text": "In this section, we show the experimental results of our proposed algorithm and the baseline algorithms for image categorization tasks on the Caltech101 [Li et al., 2007] and the WebQueries [Krapac et al., 2010] datasets.\nWe compare our proposed new algorithm SVM2+ with the baseline algorithms, i.e., SVM, SVM2K [Farquhar et al., 2005] and SVM1+. The SVM is trained based only on the visual feature extracted from images, and the SVM2K is a two-view learning algorithm that trains two classifiers on the two views simultaneously, and we only use the view from visual feature for prediction. Therefore, the SVM2K, SVM1+ and SVM2+ all utilized the additional privileged information during training. However, only the images are used for the test for all the algorithms as the privileged information is not available in our learning setting. For all the algorithms, we set all the regularization parameters in the range of {10\u22123, 10\u22122, . . . , 103}. The best result from each algorithm is reported for performance comparisons."}, {"heading": "4.1 Dataset Description and Feature Extraction", "text": "Table 1 summarizes the details of the used two datasets for our experiments, which are described in the following."}, {"heading": "Caltech101", "text": "The Caltech101 dataset2 contains images from 101 object categories (e.g., \u201chelicopter\u201d, \u201celephant\u201d and \u201cchair\u201d etc.) and a background category that contains the images not from the 101 object categories. For each object category, there are about 40 to 800 images, while most classes have about 50 images. The resolution of the image is roughly about 300\u00d7200 pixels. Following the popular settings [Li et al., 2007], we utilize 10 images per class for training and up to 50 images per category for the test. Finally, we get 1020 images in the training set, and 2995 images in the test set."}, {"heading": "WebQueries", "text": "The WebQueries dataset3 is composed of 71,478 images obtained by retrieving a total number of 353 textual web queries (e.g.,\u201ceiffel tower\u201d, \u201cviolin\u201d and \u201cFrance flag\u201d etc.). For each image in the WebQueries dataset, there are corresponding textual descriptions either in English or French. Besides, the relevant labels have been annotated by human manually. In our experiments, the images with English queries and the textual queries with more than 100 images according to the ground truth labels are used as the evaluation queries. In this way, we obtain a total number of 76 queries for the final classification tasks. For each of the 76 queries, we used 10 (resp., 50) images for constructing the training set (resp., test set). Therefore, we have 760 images for training, while we have 3800 images for test."}, {"heading": "Image Feature Extraction", "text": "For the image representation, the deep learning features are extracted from each of the images due to its excellent performance for computer vision tasks [Donahue et al., 2014].\n2http://www.vision.caltech.edu/Image Datasets/Caltech101 3https://lear.inrialpes.fr/\u223ckrapac/webqueries/webqueries.html\nThe MatConvNet [Vedaldi and Lenc, 2015] is used to extract the deep learning features. The vggs model [Chatfield et al., 2014] is used in our work. It is pre-trained on the 1.2 million ImageNet dataset [Deng et al., 2009], and the 4096-dimensional output of the fc6 in the deep Convolutional Neural Network (CNN) model from each image is employed as the visual feature representation. The same type of visual features are extracted for both the Caltech101 and the WebQueries datasets."}, {"heading": "Web Knowledge as Privileged Information", "text": "For the Caltech101, it is difficult to obtain descriptions for each of the image. We therefore discover the web knowledge by searching the category name of each category from Wikipedia. Then we collect the text descriptions of each concept from the webpage of Wikipedia. Obtaining the textual descriptions for all the categories, we further use the term frequency-inverse document frequency (TF-IDF) to convert each textual description into the bag of word frequency feature vector. We finally get a 29,535 dimensional feature vector for each of the object category. During training, each training image is associated with one vector that represents its object category as its privileged information. As we do not have the ground-truth labels for test images, the privileged information is not available during the test phase.\nFor the WebQueries dataset, each image is associated with a tag that contains a short description of the image in the Website. We also collect the textual descriptions from each of the training images, and then we remove the stop-words and calculate the term-frequency (TF) for each of the textual descriptions. The top-2000 words from the whole corpora are used as the vocabulary, and finally each textual description is represented as a 2000-dimensional feature vector."}, {"heading": "4.2 Experimental Results and Discussion", "text": "We use the linear kernel for both the visual feature and the privileged textual feature to train one-vs-others classifiers for each object category or query, and then we assign the labels of the test image to be the one with the highest output from\nclassifiers on all categories or queries. The classification accuracy is used as the performance measurement.\nThe experimental results with the classification accuracies for the different algorithms are shown as in Table 2. We can observe that the additional privileged information does help the classification tasks for both the datasets by using the LUPI framework. Therefore, it is beneficial to utilize the web knowledge for the task of image categorization. Besides, the SVM2K is worse than SVM and the other algorithms, which shows that the training of the two-view classifiers is not effective for the LUPI learning paradigm. We observe that the results from SVM2+ on the two datasets are slightly better than the results from SVM1+, which demonstrates that it is more suitable to utilize the squared hinge loss for the SVM+ on these two applications. Besides, the training speed differs significantly as shown later.\nWe compare the training CPU time of the different algorithms with a Lenovo desktop (3.20GHz CPU with 8GB RAM) and Matlab implementation. The parameters with the best classification accuracy for each algorithm are used to report the training time. As the SVM1+ cannot be fit into the standard QP solver for SVM, we utilize the commercial software Mosek4 with academic license to solve the Quadratic Programming problem. For SVM and SVM2+, we all utilize the LibSVM5 solver. The results in Table 3 show the CPU time of each algorithm on the whole Caltech101 and WebQueries datasets. Table 4 shows the speedup times of our proposed SVM2+ when compared with the SVM1+. We can observe from the table that our proposed SVM2+ achieves 110.6 and 92.5 times speedup respectively on Caltech101 and WebQueries datasets when compared with SVM1+."}, {"heading": "5 Conclusions", "text": "In this work, we propose a simple but effective SVM2+ objective function by utilizing the squared hinge loss instead of the hinge loss as in the traditional SVM1+ method, which leads to up to hundred times speedup for training the SVM+ classifiers on the image categorization datasets. In the future,\n4https://www.mosek.com/ 5https://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nwe would like to study the convergence rate and theoretical bound for our proposed SVM2+."}], "references": [{"title": "LIBSVM:a library for support vector machines", "author": ["Chang", "Lin", "2001] Chih-Chung Chang", "ChihJen Lin"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference", "citeRegEx": "Chatfield et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Machine Learning", "author": ["Corinna Cortes", "Vladimir Vapnik. Support-vector networks"], "venue": "pages 273\u2013297,", "citeRegEx": "Cortes and Vapnik. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["Nello Cristianini", "John Shawe-Taylor"], "venue": "Cambridge University Press,", "citeRegEx": "Cristianini and Shawe.Taylor. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "LiJia Li", "Kai Li", "Fei-Fei Li"], "venue": "CVPR, pages 248\u2013255,", "citeRegEx": "Deng et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "ICML", "citeRegEx": "Donahue et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Two view learning: Svm-2k", "author": ["Jason D.R. Farquhar", "David R. Hardoon", "Hongying Meng", "John Shawe-Taylor", "S\u00e1ndor Szedm\u00e1k"], "venue": "theory and practice. In NIPS, pages 355\u2013362,", "citeRegEx": "Farquhar et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Inf", "author": ["Jan Feyereisl", "Uwe Aickelin. Privileged information for data clustering"], "venue": "Sci., 194:4\u201323,", "citeRegEx": "Feyereisl and Aickelin. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In ICANN", "author": ["Shereen Fouad", "Peter Ti\u00f1o", "Somak Raychaudhury", "Petra Schneider. Learning using privileged information in prototype based models"], "venue": "pages 322\u2013329,", "citeRegEx": "Fouad et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural Netw", "author": ["Shereen Fouad", "Peter Ti\u00f1o", "Somak Raychaudhury", "Petra Schneider. Incorporating privileged information through metric learning. IEEE Trans"], "venue": "Learning Syst., 24(7):1086\u20131098,", "citeRegEx": "Fouad et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Mind the nuisance: Gaussian process classification using privileged noise", "author": ["Daniel Hern\u00e1ndez-Lobato", "Viktoriia Sharmanska", "Kristian Kersting", "Christoph H. Lampert", "Novi Quadrianto"], "venue": "NIPS, pages 837\u2013845,", "citeRegEx": "Hern\u00e1ndez.Lobato et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "B. Sch\u00f6lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, chapter 11, pages 169\u2013184. MIT Press, Cambridge, MA", "citeRegEx": "Joachims. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Improving web image search", "author": ["Krapac et al", "2010] Josip Krapac", "Moray Allan", "Jakob J. Verbeek", "Fr\u00e9d\u00e9ric Jurie"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "al. et al\\.", "year": 2010}, {"title": "Learning using privileged information: SVM+ and weighted SVM", "author": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "venue": "Neural Networks, 53:95\u2013108,", "citeRegEx": "Lapin et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["Fei-Fei Li", "Robert Fergus", "Pietro Perona"], "venue": "Computer Vision and Image Understanding, 106(1):59\u201370,", "citeRegEx": "Li et al.. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "CoRR", "author": ["David Lopez-Paz", "L\u00e9on Bottou", "Bernhard Sch\u00f6lkopf", "Vladimir Vapnik. Unifying distillation", "privileged information"], "venue": "abs/1511.03643,", "citeRegEx": "Lopez.Paz et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Netw", "author": ["Li Niu", "Xinxing Xu", "Lin Chen", "Lixin Duan", "Dong Xu. Action", "event recognition in videos by learning from heterogeneous web sources. IEEE Trans"], "venue": "Learning Syst., March", "citeRegEx": "Niu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In NIPS", "author": ["Dmitry Pechyony", "Vladimir Vapnik. On the theory of learnining with privileged information"], "venue": "pages 1894\u20131902,", "citeRegEx": "Pechyony and Vapnik. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In DMIN", "author": ["Dmitry Pechyony", "Rauf Izmailov", "Akshay Vashist", "Vladimir Vapnik. Smo-style algorithms for learning using privileged information"], "venue": "pages 235\u2013241,", "citeRegEx": "Pechyony et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the 14th International Conference on Computer Vision", "author": ["Viktoriia Sharmanska", "Novi Quadrianto", "Christoph H. Lampert. Learning to rank using privileged information"], "venue": "pages 825\u2013832, Sydney, Australia, Dec.", "citeRegEx": "Sharmanska et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning using privileged information: Similarity control and knowledge transfer", "author": ["Vladimir Vapnik", "Rauf Izmailov"], "venue": "Journal of Machine Learning Research, 16:2023\u20132049,", "citeRegEx": "Vapnik and Izmailov. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["Vladimir Vapnik", "Akshay Vashist"], "venue": "Neural Networks, 22(5-6):544\u2013557,", "citeRegEx": "Vapnik and Vashist. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Matconvnet: Convolutional neural networks for MATLAB", "author": ["Andrea Vedaldi", "Karel Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, MM \u201915, Brisbane, Australia, October 26 - 30, 2015, pages 689\u2013692,", "citeRegEx": "Vedaldi and Lenc. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Netw", "author": ["Xinxing Xu", "Ivor W. Tsang", "Dong Xu. Soft margin multiple kernel learning. IEEE Trans"], "venue": "Learning Syst., 24(5):749\u2013761,", "citeRegEx": "Xu et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural Netw", "author": ["Xinxing Xu", "Wen Li", "Dong Xu. Distance metric learning using privileged information for face verification", "person re-identification. IEEE Trans"], "venue": "Learning Syst., 26(12):3150\u20133162,", "citeRegEx": "Xu et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "The aforementioned two examples fall into the new learning paradigm of the Learning using Privileged Information (LUPI) [Vapnik and Vashist, 2009], in which the additional information is referred to as the privileged information.", "startOffset": 120, "endOffset": 146}, {"referenceID": 21, "context": "The Support Vector Machine using Privileged Information (SVM+) [Vapnik and Vashist, 2009] has been proposed for utilizing the privileged information.", "startOffset": 63, "endOffset": 89}, {"referenceID": 21, "context": "As the hinge loss is used in [Vapnik and Vashist, 2009], we refer to the formulation in their work as SVM1+ in the following.", "startOffset": 29, "endOffset": 55}, {"referenceID": 17, "context": "It has been proved theoretically that the incorporating of the additional privileged information can improve the convergence rate [Pechyony and Vapnik, 2010].", "startOffset": 130, "endOffset": 157}, {"referenceID": 20, "context": "The SVM+ attracts much attention recently [Vapnik and Izmailov, 2015; Lopez-Paz et al., 2015; Lapin et al., 2014], and has been successfully applied to different applications [Niu et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 15, "context": "The SVM+ attracts much attention recently [Vapnik and Izmailov, 2015; Lopez-Paz et al., 2015; Lapin et al., 2014], and has been successfully applied to different applications [Niu et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 13, "context": "The SVM+ attracts much attention recently [Vapnik and Izmailov, 2015; Lopez-Paz et al., 2015; Lapin et al., 2014], and has been successfully applied to different applications [Niu et al.", "startOffset": 42, "endOffset": 113}, {"referenceID": 16, "context": ", 2014], and has been successfully applied to different applications [Niu et al., 2016; Sharmanska et al., 2013; Fouad et al., 2012].", "startOffset": 69, "endOffset": 132}, {"referenceID": 19, "context": ", 2014], and has been successfully applied to different applications [Niu et al., 2016; Sharmanska et al., 2013; Fouad et al., 2012].", "startOffset": 69, "endOffset": 132}, {"referenceID": 8, "context": ", 2014], and has been successfully applied to different applications [Niu et al., 2016; Sharmanska et al., 2013; Fouad et al., 2012].", "startOffset": 69, "endOffset": 132}, {"referenceID": 19, "context": "Following this learning scenario, some recent works proposed to utilize the privileged information for the different learning scenarios such as learning to rank [Sharmanska et al., 2013], Gaussian Process classifier [Hern\u00e1ndez-Lobato et al.", "startOffset": 161, "endOffset": 186}, {"referenceID": 10, "context": ", 2013], Gaussian Process classifier [Hern\u00e1ndez-Lobato et al., 2014], clustering [Feyereisl and Aickelin, 2012] and distance metric learning [Fouad et al.", "startOffset": 37, "endOffset": 68}, {"referenceID": 7, "context": ", 2014], clustering [Feyereisl and Aickelin, 2012] and distance metric learning [Fouad et al.", "startOffset": 20, "endOffset": 50}, {"referenceID": 9, "context": ", 2014], clustering [Feyereisl and Aickelin, 2012] and distance metric learning [Fouad et al., 2013; Xu et al., 2015].", "startOffset": 80, "endOffset": 117}, {"referenceID": 24, "context": ", 2014], clustering [Feyereisl and Aickelin, 2012] and distance metric learning [Fouad et al., 2013; Xu et al., 2015].", "startOffset": 80, "endOffset": 117}, {"referenceID": 21, "context": "However, the proposed optimization method for SVM1+ in [Vapnik and Vashist, 2009] is based on its dual form.", "startOffset": 55, "endOffset": 81}, {"referenceID": 2, "context": "If there are a total number of n training data, the corresponding dual problem is a Quadratic Programming (QP) problem with 2n variables, which is 2 times larger than the dual form of the standard SVM [Cortes and Vapnik, 1995] with only n variables.", "startOffset": 201, "endOffset": 226}, {"referenceID": 11, "context": "Besides, as the constraints in the dual form of SVM+ are different with those in the dual form of the standard SVM, the efficient implementations and the off-the-shelf solvers for SVM such as LibSVM [Chang and Lin, 2001] and SVMLight [Joachims, 1999] can not be readily utilized, and the additional efforts must be spent to design the specific solvers for optimization of SVM1+ [Pechyony et al.", "startOffset": 234, "endOffset": 250}, {"referenceID": 18, "context": "Besides, as the constraints in the dual form of SVM+ are different with those in the dual form of the standard SVM, the efficient implementations and the off-the-shelf solvers for SVM such as LibSVM [Chang and Lin, 2001] and SVMLight [Joachims, 1999] can not be readily utilized, and the additional efforts must be spent to design the specific solvers for optimization of SVM1+ [Pechyony et al., 2010].", "startOffset": 378, "endOffset": 401}, {"referenceID": 21, "context": "WebQueries) when compared with the solution for SVM1+ as in [Vapnik and Vashist, 2009] yet with comparable classification accuracies.", "startOffset": 60, "endOffset": 86}, {"referenceID": 17, "context": "The X is referred to as the decision space as the test is done only in X , while Z is referred to as the correcting space [Pechyony and Vapnik, 2010].", "startOffset": 122, "endOffset": 149}, {"referenceID": 21, "context": "The support vector machine using privileged information (SVM+) has been proposed in [Vapnik and Vashist, 2009] for utilizing the privileged information.", "startOffset": 84, "endOffset": 110}, {"referenceID": 21, "context": "Specifically, f(x) = wx + b is the decision function based on the main feature x and g(z) = zv + \u03c1 is the correcting function based on the privileged information z, and the SVM+ with hinge loss (SVM1+) is proposed to optimize the following objective function [Vapnik and Vashist, 2009]:", "startOffset": 259, "endOffset": 285}, {"referenceID": 21, "context": "The objective function in (1) is solved in its dual form as in [Vapnik and Vashist, 2009; Vapnik and Izmailov, 2015].", "startOffset": 63, "endOffset": 116}, {"referenceID": 20, "context": "The objective function in (1) is solved in its dual form as in [Vapnik and Vashist, 2009; Vapnik and Izmailov, 2015].", "startOffset": 63, "endOffset": 116}, {"referenceID": 23, "context": "Although it is linear kernel here, any type of non-linear kernel [Xu et al., 2013] can be readily utilized in (2).", "startOffset": 65, "endOffset": 82}, {"referenceID": 18, "context": "Therefore, additional efforts are required for the development of the efficient solution and the softwares such as [Pechyony et al., 2010].", "startOffset": 115, "endOffset": 138}, {"referenceID": 3, "context": "In order to solve the SVM+ more efficiently and motivated by the using of squared hinge loss for SVM [Cristianini and Shawe-Taylor, 2000], we propose the Support Vector Machine using Privileged Information with Squared Hinge Loss (SVM2+).", "startOffset": 101, "endOffset": 137}, {"referenceID": 2, "context": "More importantly, the QP problem in (4) also shares the same form with the QP problem of that of the classical SVM [Cortes and Vapnik, 1995].", "startOffset": 115, "endOffset": 140}, {"referenceID": 14, "context": "In this section, we show the experimental results of our proposed algorithm and the baseline algorithms for image categorization tasks on the Caltech101 [Li et al., 2007] and the WebQueries [Krapac et al.", "startOffset": 153, "endOffset": 170}, {"referenceID": 6, "context": ", SVM, SVM2K [Farquhar et al., 2005] and SVM1+.", "startOffset": 13, "endOffset": 36}, {"referenceID": 14, "context": "Following the popular settings [Li et al., 2007], we utilize 10 images per class for training and up to 50 images per category for the test.", "startOffset": 31, "endOffset": 48}, {"referenceID": 5, "context": "For the image representation, the deep learning features are extracted from each of the images due to its excellent performance for computer vision tasks [Donahue et al., 2014].", "startOffset": 154, "endOffset": 176}, {"referenceID": 22, "context": "The MatConvNet [Vedaldi and Lenc, 2015] is used to extract the deep learning features.", "startOffset": 15, "endOffset": 39}, {"referenceID": 1, "context": "The vggs model [Chatfield et al., 2014] is used in our work.", "startOffset": 15, "endOffset": 39}, {"referenceID": 4, "context": "2 million ImageNet dataset [Deng et al., 2009], and the 4096-dimensional output of the fc6 in the deep Convolutional Neural Network (CNN) model from each image is employed as the visual feature representation.", "startOffset": 27, "endOffset": 46}], "year": 2016, "abstractText": "The Support Vector Machine using Privileged Information (SVM+) has been proposed to train a classifier to utilize the additional privileged information that is only available in the training phase but not available in the test phase. In this work, we propose an efficient solution for SVM+ by simply utilizing the squared hinge loss instead of the hinge loss as in the existing SVM+ formulation, which interestingly leads to a dual form with less variables and in the same form with the dual of the standard SVM. The proposed algorithm is utilized to leverage the additional web knowledge that is only available during training for the image categorization tasks. The extensive experimental results on both Caltech101 and WebQueries datasets show that our proposed method can achieve a factor of up to hundred times speedup with the comparable accuracy when compared with the existing SVM+ method.", "creator": "LaTeX with hyperref package"}}}