{"id": "1604.01696", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories", "abstract": "representation and empirical learning of commonsense short knowledge is already one of the foundational problems in the quest history to enable deep language understanding. this issue is particularly challenging for understanding potentially casual and correlational relationships between various events. while this topic has received a lot of interest in the nlp community, research has been hindered by the lack of a modern proper evaluation framework. this paper attempts to address this problem with a new framework for evaluating mature story researchers understanding language and script learning : the'story cloze test '. this test requires hiring a system to choose the correct ending counterpart to a four - sentence story. thereby we created a new robust corpus of ~ 50k five - sentence commonsense stories, rocstories, to enable this evaluation. this corpus is unique in two ways : ( 1 ) it captures a rich set tree of causal and temporal textual commonsense relations between daily events, and ( 2 ) it is a high quality collection of everyday life stories that can also also be used for story generation. experimental evaluation drawing shows that studying a host of baselines and similar state - of - business the - art models based on shallow language understanding struggle to achieve a high score computed on the story cloze test. we discuss these implications for script and story learning, and ourselves offer suggestions offered for deeper language understanding.", "histories": [["v1", "Wed, 6 Apr 2016 17:15:10 GMT  (607kb,D)", "http://arxiv.org/abs/1604.01696v1", "In Proceedings of the 2016 North American Chapter of the ACL (NAACL HLT), 2016"]], "COMMENTS": "In Proceedings of the 2016 North American Chapter of the ACL (NAACL HLT), 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["nasrin mostafazadeh", "nathanael chambers", "xiaodong he", "devi parikh", "dhruv batra", "lucy vanderwende", "pushmeet kohli", "james allen"], "accepted": false, "id": "1604.01696"}, "pdf": {"name": "1604.01696.pdf", "metadata": {"source": "CRF", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "authors": ["Nasrin Mostafazadeh", "Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen"], "emails": ["nasrinm@cs.rochester.edu,", "james@cs.rochester.edu,", "nchamber@usna.edu,", "parikh@vt.edu,", "dbatra@vt.edu,", "xiaohe@microsoft.com", "lucyv@microsoft.com", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000).\nRecently, there has been a renewed interest in story and narrative understanding based on progress made in core NLP tasks. This ranges from generic story telling models to building systems which can compose meaningful stories in collaboration with humans (Swanson and Gordon, 2008). Perhaps the biggest challenge of story understanding (and story generation) is having commonsense knowledge for the interpretation of narrative events. The question is how to provide commonsense knowledge regarding daily events to machines.\nA large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of \u2018narrative chains\u2019 (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swanar X iv :1 60 4. 01 69\n6v 1\n[ cs\n.C L\n] 6\nA pr\nson, 2009; Manshadi et al., 2008), but teasing out useful information from noisy blog entries is a problem of its own. Consider the following snippet from ICWSM 2011 Spinn3r Dataset of Weblog entries (Burton et al., 2009):\n\u201cI had an interesting day in the studio today. It was so interesting that I took pictures along the way to describe it to you. Sometimes I like to read an autobiography/biography to discover how someone got from there to here.....how they started, how they traveled in mind and spirit, what made them who they are now. Well, today, my work was a little like that.\u201d\nThis text is full of discourse complexities. A host of challenging language understanding tasks are required to get at the commonsense knowledge embedded within such text snippets. What is needed is a simplified version of these narratives. This paper introduces a new corpus of such short commonsense stories. With careful prompt design and multiple phases of quality control, we collected 50k high quality five-sentence stories that are full of stereotypical causal and temporal relations between events. The corpus not only serves as a resource for learning commonsense narrative schemas, but is also suitable for training story generation models. We describe this corpus in detail in Section 3.\nThis new corpus also addresses a problem facing script learning over the past few years. Despite the attention scripts have received, progress has been inhibited by the lack of a systematic evaluation framework. A commonly used evaluation is the \u2018Narrative Cloze Test\u2019 (Chambers and Jurafsky, 2008) in which a system predicts a held-out event (a verb and its arguments) given a set of observed events. For example, the following is one such test with a missing event: {X threw, pulled X, told X, ???, X completed}1. As is often the case, several works now optimize to this specific test, achieving higher scores with shallow techniques. This is problematic because the models often are not learning commonsense knowledge, but rather how to beat the shallow test.\nThis paper thus introduces a new evaluation framework called the Story Cloze Test. Instead of predicting an event, the system is tasked with choosing an entire sentence to complete the given story.\n1Narrative cloze tests were not meant to be human solvable.\nWe collected 3,742 doubly verified Story Cloze Test cases. The test is described in detail in Section 4.\nFinally, this paper proposes several models, including the most recent state-of-the-art approaches for the narrative cloze test, for tackling the Story Cloze Test. The results strongly suggest that achieving better than random or constant-choose performance requires richer semantic representation of events together with deeper levels of modeling the semantic space of stories. We believe that switching to the Story Cloze Test as the empirical evaluation framework for story understanding and script learning can help direct the field to a new direction of deeper language understanding."}, {"heading": "2 Related Work", "text": "Several lines of research have recently focused on learning narrative/event representations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the \u2018protagonist\u2019. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned.\nSeveral groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation.\nThere has also been renewed attention toward natural language comprehension and commonsense\nreasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, there is a rich body of work on story plot generation and creative or artistic story telling (Me\u0301ndez et al., 2014; Riedl and Leo\u0301n, 2008). This paper is unique to these in its corpus of short, simple stories with a wide variety of commonsense events. We show these to be useful for learning, but also for enabling a rich evaluation framework for narrative understanding."}, {"heading": "3 A Corpus of Short Commonsense Stories", "text": "We aimed to build a corpus with two goals in mind:\n1. The corpus contains a variety of commonsense causal and temporal relations between everyday events. This enables learning narrative structure across a range of events, as opposed to a single domain or genre.\n2. The corpus is a high quality collection of nonfictional daily short life stories, which can be used for training rich coherent story-telling models.\nIn order to narrow down our focus, we carefully define a narrative or story as follows: \u2018A narrative or story is anything which is told in the form of a causally (logically) linked set of events involving some shared characters\u2019. The classic definition of a story requires having a plot, (e.g., a character following a goal and facing obstacles), however, here we are not concerned with how entertaining or dramatic the stories are. Instead, we are concerned with the essence of actually being a logi-\ncally meaningful story. We follow the notion of \u2018storiness\u2019 (Forster, 1927; Bailey, 1999), which is described as \u201cthe expectations and questions that a reader may have as the story develops\u201d, where expectations are \u2018common-sense logical inferences\u2019 made by the imagined reader of the story.\nWe propose to satisfy our two goals by asking hundreds of workers on Amazon Mechanical Turk (AMT) to write novel five-sentence stories. The fivesentence length gives enough context to the story without allowing room for sidetracks about less important or irrelevant information in the story. In this Section we describe the details about how we collected this corpus, and provide statistical analysis."}, {"heading": "3.1 Data Collection Methodology", "text": "Crowdsourcing this corpus makes the data collection scalable and adds to the diversity of stories. We tested numerous pilots with varying prompts and instructions. We manually checked the submitted stories in each pilot and counted the number of submissions which did not have our desired level of coherency or were specifically fictional or offensive. Three people participated in this task and they iterated over the ratings until everyone agreed with the next pilot\u2019s prompt design. We achieved the best results when we let the workers write about anything they have in mind, as opposed to mandating a prespecified topic. The final crowdsourcing prompt can be found in supplementary material.\nThe key property that we had enforced in our final prompt was the following: the story should read like a coherent story, with a specific beginning and ending, where something happens in between. This constraint resulted in many causal and temporal links between events. Table 1 shows the examples we provided to the workers for instructing them about the constraints. We set a limit of 70 characters to the length of each sentence. This prevented multi-part sentences that include unnecessary details. The workers were also asked to provide a title that best describes their story. Last but not least, we instructed the workers not to use quotations in their sentences and avoid using slang or informal language.\nCollecting high quality stories with these constraints gives us a rich collection of commonsense stories which are full of stereotypical inter-event re-\nX challenge Y Y agree play Y practice Y beat X\nFigure 1: An example narrative chain with characters X and Y.\nlations. For example, from the good story in first row of Table 1, one can extract the narrative chain represented in Figure 1. Developing a better semantic representation for narrative chains which can capture rich inter-event relations in these stories is a topic of future work.\nQuality Control: One issue with crowdsourcing is how to instruct non-expert workers. This task is a type of creative writing, and is trickier than classification and tagging tasks. In order to ensure we get qualified workers, we designed a qualification test on AMT in which the workers had to judge whether or not a given story (total five stories) is an acceptable one. We used five carefully selected stories to be a part of the qualification test. This not only eliminates any potential spammers on AMT, but also provides us with a pool of creative story writers. Furthermore, we qualitatively browsed through the submissions and gave the workers detailed feedback before approving their submissions. We often bonused our top workers, encouraging them to write new stories on a daily basis.\nStatistics: Figure 2 shows the distribution of number of tokens of different sentence positions. The first sentence tends to be shorter, as it usually introduces characters or sets the scene, and the fifth\nsentence is longer, providing more detailed conclusions to the story. Table 2 summarizes the statistics of our crowdsourcing effort. Figure 3 shows the distribution of the most frequent 50 events in the corpus. Here we count event as any hyponym of \u2018event\u2019 or \u2018process\u2019 in WordNet (Miller., 1995). The top two events, \u2018go\u2019 and \u2018get\u2019, each comprise less than 2% of all the events, which illustrates the rich diversity of the corpus.\nFigure 4 visualizes the n-gram distribution of our story titles, where each radial path indicates an n-\ngram sequence. For this analysis we set n=5, where the mean number of tokens in titles is 9.8 and median is 10. The \u2018end\u2019 token distinguishes the actual ending of a title from five-gram cut-off. This figure demonstrates the range of topics that our workers have written about. The full circle reflects on 100% of the title n-grams and the n-gram paths in the faded 3/4 of the circle comprise less than 0.1% of the ngrams. This further demonstrates that the range of topics covered by our corpus is quite diverse. A full dynamic visualization of these n-grams can be found here: http://goo.gl/Qhg60B."}, {"heading": "3.2 Corpus Release", "text": "The corpus is publicly available to the community and can be accessed through http: //cs.rochester.edu/nlp/rocstories, which will be grown even further over the coming years. Given the quality control pipeline and the creativity required from workers, data collection goes slowly.\nWe are also making available semantic parses of these stories. Since these stories are not newswire, off-the-shelf syntactic and shallow semantic parsers for event extraction often fail on the language. To address this issue, we customized search parameters and added a few lexical entries2 to TRIPS broad-coverage semantic parser3, optimizing its performance on our corpus. TRIPS parser (Allen et al., 2008) produces state-of-the-art logical forms for input stories, providing sense disambiguated and ontology-typed rich deep structures which enables event extraction together with semantic roles and coreference chains throughout the five sentences."}, {"heading": "3.3 Temporal Analysis", "text": "Being able to temporally order events in the stories is a pre-requisite for complete narrative understanding. Temporal analysis of the events in our short commonsensical stories is an important topic of further research on its own. In this Section, we summarize two of our analyses regarding the nature of temporal ordering of events in our corpus.\nShuffling Experiment: An open question in any text genre is how text order is related to temporal order. Do the sentences follow the real-world temporal order of events? This experiment shuffles the stories and asks AMT workers to arrange them back to a coherent story. This can shed light on the correlation between the original position of the sentences and the position when another human rearranges them in a commonsensically meaningful way. We set up this experiment as follows: we sampled two sets of 50 stories from our corpus: GoodStories50 and Random-Stories50. Good-Stories504 is sampled from a set of stories written by top workers\n2For example, new informal verbs such as \u2018vape\u2019 or \u2018vlog\u2019 have been added to the lexicon of this semantic parser.\n3http://trips.ihmc.us/parser/cgi/step 4This set can be found here: https://goo.gl/VTnJ9s\nwho have shown shown consistent quality throughout their submissions. Random-Stories505 is a random sampling from all the stories in the corpus. Then we randomly shuffled the sentences in each story and asked five crowd workers on AMT to rearrange the sentences.\nTable 3 summarizes the results of this experiment. The first row shows the result of ordering if we take the absolute majority ordering of the five crowd workers as the final ordering. The second row shows the result of ordering if we consider each of the 250 (50 stories x 5 workers ordering each one) ordering cases independently. As shown, the good stories are perfectly ordered with very high accuracy. It is important to note that this specific set rarely had any linguistic adverbials such as \u2018first\u2019, \u2018then\u2019, etc. to help human infer the ordering, so the main factors at play are the following: (1) the commonsensical temporal and causal relation between events (narrative schemas), e.g., human knows that first someone loses a phone then starts searching; (2) the natural way of narrating a story which starts with introducing the characters and concludes the story at the end. The role of the latter factor is quantified in the misplacement rate of each position reported in Table 3, where the first and last sentences are more often correctly placed than others. The high precision of ordering in sentences 2 up to 4 further verifies the richness of our corpus in terms of logical relation between events.\nTimeML Annotation: TimeML-driven analysis of these stories can give us finer-grained insight about temporal aspect of the events in this corpus. We performed a simplified TimeML-driven (Pustejovsky et al., 2003) expert annotation of a sample of 20 stories6. Among all the temporal links (TLINK) annotated, 62% were \u2018before\u2019 and 10% were \u2018simultaneous\u2019. We were interested to know if the actual text order mirrors real-world order of events. We\n5This set can be found here: https://goo.gl/pgm2KR 6The annotation is available: http://goo.gl/7qdNsb\nfound that sentence order matches TimeML order 55% of the time. A more comprehensive study of temporal and causal aspects of these stories requires defining a specific semantic annotation framework which covers not only temporal but also causal relations between commonsense events. This is captured in a recent work on semantic annotation of ROCStories (Mostafazadeh et al., 2016)."}, {"heading": "4 A New Evaluation Framework", "text": "As described earlier in the introduction, the common evaluation framework for script learning is the \u2018Narrative Cloze Test\u2019 (Chambers and Jurafsky, 2008), where a system generates a ranked list of guesses for a missing event, given some observed events. The original goal of this test was to provide a comparative measure to evaluate narrative knowledge. However, gradually, the community started optimizing towards the performance on the test itself, achieving higher scores without demonstrating narrative knowledge learning. For instance, generating the ranked list according to the event\u2019s corpus frequency (e.g., always predicting \u2018X said\u2019) was shown to be an extremely strong baseline (Pichotta and Mooney, 2014b). Originally, narrative cloze test chains were extracted by hand and verified as gold chains. However, the cloze test chains used in all of the most recent works are not human verified as gold.\nIt is evident that there is a need for a more systematic automatic evaluation framework which is more in line with the original deeper script/story understanding goals. It is important to note that reordering of temporally shuffled stories (Section 3.3) can serve as a framework to evaluate a system\u2019s story understanding. However, reordering can be achieved to a degree by using various surface features such as adverbials, so this cannot be a foolproof story understanding evaluation framework. Our ROCStories corpus enables a brand new framework for evaluating story understanding, called the \u2018Story Cloze Test\u2019."}, {"heading": "4.1 Story Cloze Test", "text": "The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank. We introduce \u2018Story Cloze Test\u2019, in which a system is given a four-sentence \u2018context\u2019 and two alternative endings to the story, called \u2018right ending\u2019 and \u2018wrong ending\u2019. Hence, in this test the fifth sentence is blank. Then the system\u2019s task is to choose the right ending. The \u2018right ending\u2019 can be viewed as \u2018entailing\u2019 hypothesis in a classic Recognizing Textual Entailment (RTE) framework (Giampiccolo et al., 2007), and \u2018wrong\u2019 ending can be seen as the \u2019contradicting\u2019 hypothesis. Table 4 shows three example Story Cloze Test cases.\nStory Cloze Test will serve as a generic story understanding evaluation framework, also applicable to evaluation of story generation models (for instance by computing the log-likelihoods assigned to the two ending alternatives by the story generation model), which does not necessarily imply requirement for explicit narrative knowledge learning. However, it is safe to say that any model that performs well on Story Cloze Test is demonstrating some level of deeper story understanding."}, {"heading": "4.2 Data Collection Methodology", "text": "We randomly sampled 13,500 stories from ROCStories Corpus and presented only the first four sentences of each to AMT workers. For each story, a worker was asked to write a \u2018right ending\u2019 and a \u2018wrong ending\u2019. The workers were prompted to satisfy two conditions: (1) the sentence should follow up the story by sharing at least one of the characters of the story, and (2) the sentence should be entirely realistic and sensible when read in isolation. These conditions make sure that the Story Cloze Test cases are not trivial. More details on this setup is described in the supplementary material.\nQuality Control: The accuracy of the Story Cloze Test can play a crucial role in directing the research community in the right trajectory. We implemented the following two-step quality control:\n1. Qualification Test: We designed a qualification test for this task, where the workers had to choose whether or not a given \u2018right ending\u2019 and \u2018wrong\nending\u2019 satisfy our constraints. At this stage we collected 13,500 cloze test cases.\n2. Human Verification: In order to further validate the cloze test cases, we compiled the 13,500 Story Cloze Test cases into 2\u00d713, 500 = 27, 000 full five-sentence stories. Then for each story we asked three crowd workers to verify whether or not the given sequence of five sentences makes sense as a meaningful and coherent story, rating within {-1, 0, 1}. Then we filtered cloze test cases which had \u2018right ending\u2019 with all ratings 1 and \u2018wrong ending\u2019 with all ratings 0. This process ensures that there are no boundary cases of \u2018right ending\u2019 and \u2018wrong ending\u2019. This resulted in final 3,742 test cases, which was randomly divided into validation and test Story Cloze Test sets. We also made sure to remove the original stories used in the validation and test set from our ROCStories Corpus.\nStatistics: Table 5 summarizes the statistics of our crowdsourcing effort. The Story Cloze Test sets can also be accessed through our website."}, {"heading": "5 Story Cloze Test Models", "text": "In this Section we demonstrate that Story Cloze Test cannot be easily tackled by using shallow techniques, without actually understanding the underlying narrative. Following other natural language inference frameworks such as RTE, we evaluate system performance according to basic accuracy measure, which is defined as #correct#test cases . We present the following baselines and models for tackling Story Cloze Test. All of the models are tested on the validation and test Story Cloze sets, where only the validation set could be used for any tuning purposes. 1. Frequency: Ideally, the Story Cloze Test cases should not be answerable without the context. For example, if for some context the two alternatives are \u2018He was mad after he won\u20197 and \u2018He was cheerful after he won\u2019, the first alternative is simply less probable in real world than the other one. This baseline chooses the alternative with higher search engine8 hits of the main event (verb) together\n7Given our prompt that the \u2018wrong ending\u2019 sentences should make sense in isolation, such cases should be rare in our dataset.\n8https://developers.google.com/ custom-search/\nwith its semantic roles (e.g., \u2018I*poison*flowers\u2019 vs \u2018I*nourish*flowers\u2019). We extract the main verb and its corresponding roles using TRIPS semantic parser. 2. N-gram Overlap: Simply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothesis that matches the sentiment of the last context sentence. 6. Skip-thoughts Model: This model uses Skipthoughts\u2019 Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels.\nThis model is trained on the \u2018BookCorpus\u2019 (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books. We use the skip-thoughts embedding of the alternatives and contexts for making decision the same way as with GenSim model.\n7. Narrative Chains-AP: Implements the standard approach to learning chains of narrative events based on Chambers and Jurafsky (2008). An event is represented as a verb and a typed dependency (e.g., the subject of runs). We computed the PMI between all event pairs in the Associate Press (AP) portion of the English Gigaword Corpus that occur at least 2 times. We run coreference over the given story, and choose the hypothesis whose coreferring entity has the highest average PMI score with the entity\u2019s chain in the story. If no entity corefers in both hypotheses, it randomly chooses one of the hypotheses.\n8. Narrative Chains-Stories: The same model as above, but trained on ROCStories.\n9. Deep Structured Semantic Model (DSSM): This model (Huang et al., 2013) is trained to project the four-sentences context and the fifth sentence into the same vector space. It consists of two separate deep neural networks for learning jointly the embedding of the four-sentences context and the fifth sentence, respectively. As suggested in Huang et al. (2013), the input of the DSSM is based on contextdependent characters, e.g., the distribution count of letter-trigrams in the context and in the fifth sentence, respectively. The hyper parameters of the DSSM is determined on the validation set, while the model\u2019s parameters are trained on the ROCStories corpus. In our experiment, each of the two neural networks in the DSSM has two layers: the dimen-\nsion of the hidden layer is 1000, and the dimension of the embedding vector is 300. At runtime, this model picks the candidate with the largest cosine similarity between its vector representation and the context\u2019s vector representation.\nThe results of evaluating these models on the Story Cloze validation and test sets are shown in Table 6. The constant-choose-first (51%) and human performance (100%) is also provided for comparison. Note that these sets were doubly verified by human, hence it does not have any boundary cases, resulting in 100% human performance. The DSSM model achieves the highest accuracy, but only 7.2 points higher than constant-choose-first. Error analysis on the narrative chains model shows why this and other event-based language models are not sufficient for the task: often, the final sentences of our stories contain complex events beyond the main verb, such as \u2018Bill was highly unprepared\u2019 or \u2018He had to go to a homeless shelter\u2019. Event language models only look at the verb and syntactic relation like \u2018was-object\u2019 and \u2018go-to\u2019. In that sense, going to a homeless shelter is the same as going to the beach. This suggests the requirement of having richer semantic representation for events in narratives. Our proposed Story Cloze Test offers a new challenge to the community."}, {"heading": "6 Discussion", "text": "There are three core contributions in this paper: (1) a new corpus of commonsense stories, called ROCStories, (2) a new evaluation framework to evaluate script/story learners, called Story Cloze Test, and (3) a host of first approaches to tackle this new test framework. ROCStories Corpus is the first crowdsourced corpus of its kind for the community. We have released about 50k stories, as well as valida-\ntion and test sets for Story Cloze Test. This dataset will eventually grow to 100k stories, which will be released through our website. In order to continue making meaningful progress on this task, although it is possible to keep increasing the size of the training data, we expect the community to develop models that will learn to generalize to unseen commonsense concepts and situations.\nThe Story Cloze Test proved to be a challenge to all of the models we tested. We believe it will serve as an effective evaluation for both story understanding and script knowledge learners. We encourage the community to benchmark their progress by reporting their results on Story Cloze test set. Compared to the previous Narrative Cloze Test, we found that one of the early models for that task actually performs worse than random guessing. We can conclude that Narrative Cloze test spurred interest in script learning, however, it ultimately does not evaluate deeper knowledge and language understanding."}, {"heading": "Acknowledgments", "text": "We would like to thank the amazing crowd workers whose endless hours of daily story writing made this research possible. We thank William de Beaumont and Choh Man Teng for their work on TRIPS parser. We thank Alyson Grealish for her great help in the quality control of our corpus. This work was supported in part by Grant W911NF-15-1-0542 with the US Defense Advanced Research Projects Agency (DARPA), the Army Research Office (ARO) and the Office of Naval Research (ONR). Our data collection effort was sponsored by Nuance Foundation."}], "references": [{"title": "Deep semantic analysis of text", "author": ["James F. Allen", "Mary Swift", "Will de Beaumont."], "venue": "Proceedings", "citeRegEx": "Allen et al\\.,? 2008", "shortCiteRegEx": "Allen et al\\.", "year": 2008}, {"title": "Searching for storiness: Storygeneration from a reader\u2019s perspective", "author": ["Paul Bailey."], "venue": "AAAI Fall Symposium on Narrative Intelligence.", "citeRegEx": "Bailey.,? 1999", "shortCiteRegEx": "Bailey.", "year": 1999}, {"title": "Generating coherent event schemas at scale", "author": ["Niranjan Balasubramanian", "Stephen Soderland", "Oren Etzioni Mausam", "Oren Etzioni."], "venue": "EMNLP, pages 1721\u20131731.", "citeRegEx": "Balasubramanian et al\\.,? 2013", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2013}, {"title": "Learning latent personas of film characters", "author": ["David Bamman", "Brendan OConnor", "Noah Smith."], "venue": "ACL.", "citeRegEx": "Bamman et al\\.,? 2013", "shortCiteRegEx": "Bamman et al\\.", "year": 2013}, {"title": "Learning natural language inference from a large annotated corpus", "author": ["Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642,", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "The icwsm 2009 spinn3r dataset", "author": ["K. Burton", "A. Java", "I. Soboroff"], "venue": "Proceedings of the Third Annual Conference on Weblogs and Social Media (ICWSM", "citeRegEx": "Burton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Burton et al\\.", "year": 2009}, {"title": "Unsupervised learning of narrative event chains", "author": ["Nathanael Chambers", "Daniel Jurafsky."], "venue": "Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors, ACL, pages 789\u2013797. The Association for Computer Linguistics.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Process-", "citeRegEx": "Chambers and Jurafsky.,? 2009", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2009}, {"title": "Event schema induction with a probabilistic entity-driven model", "author": ["Nathanael Chambers."], "venue": "EMNLP, volume 13, pages 1797\u20131807.", "citeRegEx": "Chambers.,? 2013", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Toward a model of children\u2019s story comprehension", "author": ["Eugene Charniak."], "venue": "December.", "citeRegEx": "Charniak.,? 1972", "shortCiteRegEx": "Charniak.", "year": 1972}, {"title": "Probabilistic frame induction", "author": ["Jackie Cheung", "Hoifung Poon", "Lucy Vanderwende."], "venue": "ACL.", "citeRegEx": "Cheung et al\\.,? 2013", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "Aspects of the Novel", "author": ["E.M. Forster."], "venue": "Edward Arnold, London.", "citeRegEx": "Forster.,? 1927", "shortCiteRegEx": "Forster.", "year": 1927}, {"title": "The third pascal recognizing textual entailment challenge", "author": ["Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan."], "venue": "Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, RTE \u201907, pages 1\u20139, Stroudsburg, PA, USA.", "citeRegEx": "Giampiccolo et al\\.,? 2007", "shortCiteRegEx": "Giampiccolo et al\\.", "year": 2007}, {"title": "Identifying Personal Stories in Millions of Weblog Entries", "author": ["Andrew S. Gordon", "Reid Swanson."], "venue": "Third International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA, May.", "citeRegEx": "Gordon and Swanson.,? 2009", "shortCiteRegEx": "Gordon and Swanson.", "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of the 22Nd ACM International Conference on Information & Knowl-", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Skip n-grams and ranking functions for predicting script events", "author": ["Bram Jans", "Steven Bethard", "Ivan Vuli\u0107", "Marie Francine Moens."], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguis-", "citeRegEx": "Jans et al\\.,? 2012", "shortCiteRegEx": "Jans et al\\.", "year": 2012}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "NIPS.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "The winograd schema challenge", "author": ["Hector J. Levesque."], "venue": "AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning. AAAI.", "citeRegEx": "Levesque.,? 2011", "shortCiteRegEx": "Levesque.", "year": 2011}, {"title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics", "author": ["Chin-Yew Lin", "Franz Josef Och."], "venue": "Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL \u201904, Strouds-", "citeRegEx": "Lin and Och.,? 2004", "shortCiteRegEx": "Lin and Och.", "year": 2004}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics:", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Learning a Probabilistic Model of Event Sequences From Internet Weblog Stories", "author": ["Mehdi Manshadi", "Reid Swanson", "Andrew S. Gordon."], "venue": "21st Conference of the Florida AI Society, Applied Natural Language Processing Track, Coconut Grove, FL, May.", "citeRegEx": "Manshadi et al\\.,? 2008", "shortCiteRegEx": "Manshadi et al\\.", "year": 2008}, {"title": "Learning to tell tales: A data-driven approach to story generation", "author": ["Neil McIntyre", "Mirella Lapata."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International", "citeRegEx": "McIntyre and Lapata.,? 2009", "shortCiteRegEx": "McIntyre and Lapata.", "year": 2009}, {"title": "A model of character affinity for agent-based story generation", "author": ["Gonzalo M\u00e9ndez", "Pablo Gerv\u00e1s", "Carlos Le\u00f3n."], "venue": "9th International Conference on Knowledge, Information and Creativity Support Systems, Limassol, Cyprus, 11/2014. Springer-Verlag,", "citeRegEx": "M\u00e9ndez et al\\.,? 2014", "shortCiteRegEx": "M\u00e9ndez et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neu-", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["G. Miller."], "venue": "In Communications of the ACM.", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Semantic annotation of event structures in commonsense stories", "author": ["Nasrin Mostafazadeh", "Alyson Grealish", "Nathanael Chambers", "James F. Allen", "Lucy Vanderwende."], "venue": "Proceedings of the The 4th Workshop on EVENTS: Definition, Detection, Coreference,", "citeRegEx": "Mostafazadeh et al\\.,? 2016", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Understanding script-based stories using commonsense reasoning", "author": ["Erik T. Mueller."], "venue": "Cognitive Systems Research, 5:2004.", "citeRegEx": "Mueller.,? 2002", "shortCiteRegEx": "Mueller.", "year": 2002}, {"title": "Modeling space and time in narratives about restaurants", "author": ["Erik T. Mueller."], "venue": "LLC, 22(1):67\u201384.", "citeRegEx": "Mueller.,? 2007", "shortCiteRegEx": "Mueller.", "year": 2007}, {"title": "Generative event schema induction with entity disambiguation", "author": ["Kiem-Hieu Nguyen", "Xavier Tannier", "Olivier Ferret", "Romaric Besan\u00e7on."], "venue": "Proceedings of the 53rd annual meeting of the Association for Computational Linguistics (ACL-15).", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Statistical script learning with multi-argument events", "author": ["Karl Pichotta", "Raymond J Mooney."], "venue": "EACL 2014, page 220.", "citeRegEx": "Pichotta and Mooney.,? 2014a", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2014}, {"title": "Statistical script learning with multi-argument events", "author": ["Karl Pichotta", "Raymond J. Mooney."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014), Gothenburg, Sweden, April.", "citeRegEx": "Pichotta and Mooney.,? 2014b", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2014}, {"title": "Timeml: Robust specification of event and temporal expressions in text", "author": ["James Pustejovsky", "Jos Castao", "Robert Ingria", "Roser Saur", "Robert Gaizauskas", "Andrea Setzer", "Graham Katz."], "venue": "in Fifth International Workshop on Computational Semantics (IWCS-", "citeRegEx": "Pustejovsky et al\\.,? 2003", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Learning script knowledge with web experiments", "author": ["Michaela Regneri", "Alexander Koller", "Manfred Pinkal."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Regneri et al\\.,? 2010", "shortCiteRegEx": "Regneri et al\\.", "year": 2010}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw."], "venue": "EMNLP, pages 193\u2013203. ACL.", "citeRegEx": "Richardson et al\\.,? 2013", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Toward vignette-based story generation for drama management systems", "author": ["M. Riedl", "Carlos Le\u00f3n."], "venue": "Workshop on Integrating Technologies for Interactive Stories - 2nd International Conference on INtelligent TEchnologies for interactive enterTAINment, 8-10/1.", "citeRegEx": "Riedl and Le\u00f3n.,? 2008", "shortCiteRegEx": "Riedl and Le\u00f3n.", "year": 2008}, {"title": "Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning", "author": ["Melissa Roemmele", "Cosmin Adrian Bejan", "Andrew S. Gordon."], "venue": "AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning, Stanford Univer-", "citeRegEx": "Roemmele et al\\.,? 2011", "shortCiteRegEx": "Roemmele et al\\.", "year": 2011}, {"title": "Script induction as language modeling", "author": ["Rachel Rudinger", "Pushpendre Rastogi", "Francis Ferraro", "Benjamin Van Durme."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP-15).", "citeRegEx": "Rudinger et al\\.,? 2015", "shortCiteRegEx": "Rudinger et al\\.", "year": 2015}, {"title": "Scripts, Plans, Goals and Understanding: an Inquiry into Human Knowledge Structures", "author": ["Roger C. Schank", "Robert P. Abelson."], "venue": "L. Erlbaum, Hillsdale, NJ.", "citeRegEx": "Schank and Abelson.,? 1977", "shortCiteRegEx": "Schank and Abelson.", "year": 1977}, {"title": "Episodic logic meets little red riding hood: A comprehensive, natural representation for language understanding", "author": ["Lenhart K. Schubert", "Chung Hee Hwang."], "venue": "Natural Language Processing and Knowledge Representation: Language for Knowledge", "citeRegEx": "Schubert and Hwang.,? 2000", "shortCiteRegEx": "Schubert and Hwang.", "year": 2000}, {"title": "Say Anything: A Massively collaborative Open Domain Story Writing Companion", "author": ["Reid Swanson", "Andrew S. Gordon."], "venue": "First International Conference on Interactive Digital Storytelling, Erfurt, Germany, November.", "citeRegEx": "Swanson and Gordon.,? 2008", "shortCiteRegEx": "Swanson and Gordon.", "year": 2008}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor."], "venue": "Journalism quarterly.", "citeRegEx": "Taylor.,? 1953", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "The creative process: A computer model of storytelling", "author": ["Scott R. Turner."], "venue": "Hillsdale: Lawrence Erlbaum.", "citeRegEx": "Turner.,? 1994", "shortCiteRegEx": "Turner.", "year": 1994}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov."], "venue": "CoRR, abs/1502.05698.", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Understanding Natural Language", "author": ["Terry Winograd."], "venue": "Academic Press, Inc., Orlando, FL, USA.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Richard Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "arXiv preprint", "citeRegEx": "Zhu et al\\.,? 2015", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 9, "context": "Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000).", "startOffset": 120, "endOffset": 192}, {"referenceID": 44, "context": "Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000).", "startOffset": 120, "endOffset": 192}, {"referenceID": 42, "context": "Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000).", "startOffset": 120, "endOffset": 192}, {"referenceID": 39, "context": "Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000).", "startOffset": 120, "endOffset": 192}, {"referenceID": 40, "context": "This ranges from generic story telling models to building systems which can compose meaningful stories in collaboration with humans (Swanson and Gordon, 2008).", "startOffset": 132, "endOffset": 158}, {"referenceID": 38, "context": "A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977).", "startOffset": 76, "endOffset": 102}, {"referenceID": 6, "context": "Most relevant to this issue is work on unsupervised learning of \u2018narrative chains\u2019 (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al.", "startOffset": 83, "endOffset": 112}, {"referenceID": 7, "context": "Most relevant to this issue is work on unsupervised learning of \u2018narrative chains\u2019 (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015).", "startOffset": 131, "endOffset": 232}, {"referenceID": 2, "context": "Most relevant to this issue is work on unsupervised learning of \u2018narrative chains\u2019 (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015).", "startOffset": 131, "endOffset": 232}, {"referenceID": 10, "context": "Most relevant to this issue is work on unsupervised learning of \u2018narrative chains\u2019 (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015).", "startOffset": 131, "endOffset": 232}, {"referenceID": 29, "context": "Most relevant to this issue is work on unsupervised learning of \u2018narrative chains\u2019 (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015).", "startOffset": 131, "endOffset": 232}, {"referenceID": 5, "context": "Consider the following snippet from ICWSM 2011 Spinn3r Dataset of Weblog entries (Burton et al., 2009):", "startOffset": 81, "endOffset": 102}, {"referenceID": 6, "context": "A commonly used evaluation is the \u2018Narrative Cloze Test\u2019 (Chambers and Jurafsky, 2008) in which a system predicts a held-out event (a verb and its arguments) given a set of observed events.", "startOffset": 57, "endOffset": 86}, {"referenceID": 6, "context": "Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the \u2018protagonist\u2019.", "startOffset": 54, "endOffset": 83}, {"referenceID": 7, "context": "Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al.", "startOffset": 73, "endOffset": 102}, {"referenceID": 33, "context": "Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al.", "startOffset": 121, "endOffset": 143}, {"referenceID": 2, "context": ", 2010), and relgrams (Balasubramanian et al., 2013).", "startOffset": 22, "endOffset": 52}, {"referenceID": 10, "context": "Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015).", "startOffset": 86, "endOffset": 165}, {"referenceID": 3, "context": "Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015).", "startOffset": 86, "endOffset": 165}, {"referenceID": 8, "context": "Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015).", "startOffset": 86, "endOffset": 165}, {"referenceID": 29, "context": "Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015).", "startOffset": 86, "endOffset": 165}, {"referenceID": 16, "context": "(Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008).", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": ", 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008).", "startOffset": 116, "endOffset": 145}, {"referenceID": 30, "context": "Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015).", "startOffset": 72, "endOffset": 123}, {"referenceID": 37, "context": "Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015).", "startOffset": 72, "endOffset": 123}, {"referenceID": 18, "context": "reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015).", "startOffset": 10, "endOffset": 70}, {"referenceID": 36, "context": "reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015).", "startOffset": 10, "endOffset": 70}, {"referenceID": 4, "context": "reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015).", "startOffset": 10, "endOffset": 70}, {"referenceID": 14, "context": "There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al.", "startOffset": 72, "endOffset": 115}, {"referenceID": 43, "context": "There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al.", "startOffset": 72, "endOffset": 115}, {"referenceID": 34, "context": ", 2015), including the MCTest (Richardson et al., 2013) as a notable one.", "startOffset": 30, "endOffset": 55}, {"referenceID": 27, "context": "This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009).", "startOffset": 77, "endOffset": 92}, {"referenceID": 28, "context": "This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009).", "startOffset": 207, "endOffset": 222}, {"referenceID": 22, "context": "This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009).", "startOffset": 264, "endOffset": 291}, {"referenceID": 23, "context": "Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M\u00e9ndez et al., 2014; Riedl and Le\u00f3n, 2008).", "startOffset": 102, "endOffset": 145}, {"referenceID": 35, "context": "Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M\u00e9ndez et al., 2014; Riedl and Le\u00f3n, 2008).", "startOffset": 102, "endOffset": 145}, {"referenceID": 11, "context": "We follow the notion of \u2018storiness\u2019 (Forster, 1927; Bailey, 1999), which is described as \u201cthe expectations and questions that a reader may have as the story develops\u201d, where expectations are \u2018common-sense logical inferences\u2019 made by the imagined reader of the story.", "startOffset": 36, "endOffset": 65}, {"referenceID": 1, "context": "We follow the notion of \u2018storiness\u2019 (Forster, 1927; Bailey, 1999), which is described as \u201cthe expectations and questions that a reader may have as the story develops\u201d, where expectations are \u2018common-sense logical inferences\u2019 made by the imagined reader of the story.", "startOffset": 36, "endOffset": 65}, {"referenceID": 25, "context": "Here we count event as any hyponym of \u2018event\u2019 or \u2018process\u2019 in WordNet (Miller., 1995).", "startOffset": 70, "endOffset": 85}, {"referenceID": 0, "context": "TRIPS parser (Allen et al., 2008) produces state-of-the-art logical forms for input stories, providing sense disambiguated and ontology-typed rich deep structures which enables event extraction together with semantic roles and coreference chains throughout the five sentences.", "startOffset": 13, "endOffset": 33}, {"referenceID": 32, "context": "We performed a simplified TimeML-driven (Pustejovsky et al., 2003) expert annotation of a sample of 20 stories6.", "startOffset": 40, "endOffset": 66}, {"referenceID": 26, "context": "This is captured in a recent work on semantic annotation of ROCStories (Mostafazadeh et al., 2016).", "startOffset": 71, "endOffset": 98}, {"referenceID": 6, "context": "As described earlier in the introduction, the common evaluation framework for script learning is the \u2018Narrative Cloze Test\u2019 (Chambers and Jurafsky, 2008), where a system generates a ranked list of guesses for a missing event, given some observed events.", "startOffset": 124, "endOffset": 153}, {"referenceID": 31, "context": ", always predicting \u2018X said\u2019) was shown to be an extremely strong baseline (Pichotta and Mooney, 2014b).", "startOffset": 75, "endOffset": 103}, {"referenceID": 41, "context": "The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank.", "startOffset": 15, "endOffset": 29}, {"referenceID": 12, "context": "The \u2018right ending\u2019 can be viewed as \u2018entailing\u2019 hypothesis in a classic Recognizing Textual Entailment (RTE) framework (Giampiccolo et al., 2007), and \u2018wrong\u2019 ending can be seen as the \u2019contradicting\u2019 hypothesis.", "startOffset": 119, "endOffset": 145}, {"referenceID": 19, "context": "We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context.", "startOffset": 25, "endOffset": 44}, {"referenceID": 24, "context": "GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context.", "startOffset": 77, "endOffset": 99}, {"referenceID": 20, "context": "We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence.", "startOffset": 53, "endOffset": 75}, {"referenceID": 17, "context": "Skip-thoughts Model: This model uses Skipthoughts\u2019 Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels.", "startOffset": 74, "endOffset": 94}, {"referenceID": 45, "context": "This model is trained on the \u2018BookCorpus\u2019 (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books.", "startOffset": 42, "endOffset": 60}, {"referenceID": 6, "context": "Narrative Chains-AP: Implements the standard approach to learning chains of narrative events based on Chambers and Jurafsky (2008). An event is represented as a verb and a typed dependency (e.", "startOffset": 102, "endOffset": 131}, {"referenceID": 15, "context": "Deep Structured Semantic Model (DSSM): This model (Huang et al., 2013) is trained to project the four-sentences context and the fifth sentence into the same vector space.", "startOffset": 50, "endOffset": 70}, {"referenceID": 15, "context": "Deep Structured Semantic Model (DSSM): This model (Huang et al., 2013) is trained to project the four-sentences context and the fifth sentence into the same vector space. It consists of two separate deep neural networks for learning jointly the embedding of the four-sentences context and the fifth sentence, respectively. As suggested in Huang et al. (2013), the input of the DSSM is based on contextdependent characters, e.", "startOffset": 51, "endOffset": 359}], "year": 2016, "abstractText": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the \u2018Story Cloze Test\u2019. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of 50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "creator": "LaTeX with hyperref package"}}}