{"id": "1604.01792", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR", "abstract": "very effective deep cnns with small 3x3 kernels have recently yet been shown ability to remarkably achieve twice very strong memory performance as acoustic models in hybrid interactive nn - hmm speech recognition systems. in this paper, we demonstrate that the accuracy gains of these deep cnns are retained both on larger scale data, and after sequence training. we show this by carrying out simulated sequence training tested on both the 300h switchboard - 1 and the 2000h switchboard sensor dataset. furthermore, we effectively investigate how pooling and padding in time influences performance, both in terms of data word sequence error - rate and computational cost. we argue that designing cnns complete without timepadding and without timepooling, though slightly suboptimal for accuracy, has two significant consequences. firstly, the proposed design allows tools for sufficiently efficient evaluation measures at sequence training and test ( deployment ) time. only secondly, this design principle correctly allows for batch normalization to be adopted to cnns on sequence data. our defining very deep cnn model sequence trained on the 2000h microsoft switchboard dataset obtains 9. 4 word error rate on the hub5 test - set, matching with a single model the performance of 2015 ibm system combination, which was the official previous best published result.", "histories": [["v1", "Wed, 6 Apr 2016 20:07:52 GMT  (2578kb,D)", "http://arxiv.org/abs/1604.01792v1", null], ["v2", "Sat, 25 Jun 2016 00:27:19 GMT  (3546kb,D)", "http://arxiv.org/abs/1604.01792v2", "Proc. Interspeech 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["tom sercu", "vaibhava goel"], "accepted": false, "id": "1604.01792"}, "pdf": {"name": "1604.01792.pdf", "metadata": {"source": "CRF", "title": "Advances in Very Deep Convolutional Neural Networks for LVCSR", "authors": ["Tom Sercu", "Vaibhava Goel"], "emails": ["tsercu@us.ibm.com,", "vgoel@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "We present advances and results on using very deep VGG-style convolutional networks as acoustic model for Large Vocabulary Speech Recognition, extending our earlier work [1].\nIn [1], we introduced very deep convolutional network architectures to LVCSR, presenting strong results on babel and the 300-hour switchboard-1 dataset (SWB-1) after crossentropy training only. The very deep convolutional networks are inspired by the \u201cVGG Net\u201d architecture introduced in [2] for the 2014 ImageNet classification challenge. The central idea of VGG networks is to replace layers with large convolutional kernels by a stack of layers with small 3\u00d7 3 kernels. This way, the same receptive field is created with less parameters and more nonlinearity. Furthermore, by applying zero-padding throughout and only reducing spatial resolution through pooling, the networks in [2] were simple and elegant. We followed this design in the acoustic model CNNs [1].\nThe VGG Net-inspired networks from [1] achieved 11.8% WER, a 1.4% absolute (or 10.6% relative) improvement over the baseline 2-layer sigmoid CNN from [3] after cross-entropy training on 300-hour switchboard. However, the questions remained whether the improvements hold up after sequence training, and with training on more data.\nSecondly, an important design choice remained unexplored: how to deal with time-pooling and time-padding (i.e. zeropadding at the borders along the time dimension). The networks from [1] pool in time with stride 2 on the higher layers of the\nnetwork, and applied time-padding throughout. This allowed for the elegant design analogous to the VGG networks for computer vision. In this paper, we explore alternatives to that design choice. Most importantly, we focus on designs that allow for efficient evaluation during sequcence training and test time (or deployment in a production system).\nThe contributions of this paper are: \u2022 We show that the very deep networks\u2019 gains are pre-\nserved after sequence training, with a Hub5 WER of 10.5 after ST on SWB-1 (300h), a 1.3 WER gain over the classical CNN. \u2022 We present results of the very deep networks\u2019 performance after training on the full SWB (2000h) corpus, achieving a 9.4 WER (1.0 WER better than the classical CNN). \u2022 We empirically investigate whether or not pooling in time on the higher layers of the network is appropriate. At the one hand, we show that an equivalent architecture obtains better results when downsampled in time (section 2). At the other hand, we discuss why the design from [1] with time-pooling and time-padding is problematic for efficient evaluation of a full utterance (section 3). \u2022 To address the efficient evaluation concern, we discuss an architectural constraint on deep CNNs: by only using convolutional layers without padding or pooling in time one can evaluate an utterance at once, without redundant computation. However, performance of the networks following this design principle are slightly inferior to the original design. \u2022 We demonstrate the merit of batch normalization (BN) [4] for CNN acoustic models (section 4). BN is a technique to accelerate training and improve generalization by normalizing the internal representations inside the network. We show that in order to use batch normalization for CNNs during sequence training, it is important to train on several utterances at the same time. Since this is feasible for the efficient evaluation architectures only, Batch Normalization gives us essentially a way to compensate the lost performance from following the no time-padding design principle from section 3."}, {"heading": "2. Time-pooling on top layers", "text": "Pooling with stride is an essential element of CNNs in computer vision, as the downsampling reduces the grid size while building invariance to local geometric transformations. In acoustic CNNs, pooling can be readily applied along the frequency dimension, which can help build invariance against spectral variation. In our deepest 10-layer CNN in [1], we reduce the frequency-dimension size from 40 to 20, 10, 4, 2 through pooling after the second, fourth, seventh and tenth convolutional layer, while the convolutions are zero-padded in order to pre-\nar X\niv :1\n60 4.\n01 79\n2v 1\n[ cs\n.C L\n] 6\nA pr\n2 01\n6\nserve size in the frequency dimension. Along the time dimension the application of pooling is less straightforward. It was argued in [7] that downsampling in time should be avoided, rather pooling with stride 1 was used, which does not downsample in time. However, in [1] we did pool with stride 2. This fits in the design of our CNNs, where zeropadding is applied both along the time and frequency directions, so pooling is the only operation which reduces the context window from its original size (e.g. 16) to the final size (e.g. 4). This design is directly analogous to VGG networks in vision.\nApart from this practical reason, we did not justify this design choice in [1]. We hypothesize that downsampling in time has both an advantage and a disadvantage. The advantage is that higher layers in the network are able to access more context and can learn useful invariants in time. As argued in [8], once a feature is detected in the lower layers, its exact location does not\nmatter that much anymore and can be blurred out, as long as its approximate relative position is conserved. The disadvantage is that the resolution is reduced with which neighboring but different CD states can be distinguished, which could possibly hurt performance. In this section we empirically investigate whether pooling in time is justified.\nFigure 1 summarizes three variations of the 10-layer architecture, (a) being the original version of [1], Figure 1 (b) shows an alternative to pooling in time. To reduce the context from its original size to the size we want to absorb in the fully connected layer, we simply omit time-padding on top layers as needed to achieve the desired reduction.\nIn table 1 row (a) and (b) we compare results with and without timepooling. We see that architecture (a) with time-pooling outperforms architecture (b) after sequence training and training on 2000 hours. The result after 2000 hours and sequence training matches the system combination of a classical CNN, DNN and RNN from [5]. Also note that the CE number on SWB is far better than the baselines, but the gains from ST are less. This can be explained by the fact that we do stochastic rather than HF sequence training (see section 5), which leaves an opportunity for improvement.\nFrom comparing results for model variants (a) against (b), we conclude that pooling in time improves performance over the same architecture with reducing the time dimension through unpadded convolution."}, {"heading": "3. Efficient evaluation", "text": "In the previous section we showed how time padding and pooling allowed for the design which was presented in [1]. In this section, we address an important issue: timepadding makes the\nCNN feature maps dependent on the location in the sequence. This does not matter for cross-entropy training, but destroys the desirable property of efficient full utterance evaluation at sequence training and deployment time.\nFigure 2 shows the two different ways of processing an utterance with a convolutional net. In the efficient evaluation setting (b), the computational cost of evaluating an additional neigboring frame is small: on each convolutional layer we add just (1\u00d7 freqsize) spatial size (depicted with the light green frame neighboring the red window). In contrast, evaluating an additional neighboring frame in setting (a) means adding a sample to the minibatch of size (context_size \u00d7 freqsize).\nSo under what conditions does a cross-entropy trained network allow for efficient evaluation as in figure 2 B? The requirement is that the output values of each layer are identical when shifting to the next timestep. This property holds for convolution without padding in time, pointwise nonlinearity, and pooling in the frequency dimension. However, this requirement is not fulfilled by convolutions with padding in time, and pooling in time.\nThis is illustrated in figure 1 (b). The light red edges are zero-padding in time. The red dashed line indicates which output values in the CNN are modified by the timepadding, as compared to the same network without timepadding. Note that on the first layer, only one outer frame at each side is modified, on the second layer the outer two frames, etc. The modification travels inwards deeper in the network. Everything outside the dashed lines is modified by the timepadding which is specific to the center frame. Note that, as we evalute the next window (shifted by one frame to the right), the location of the zero padding changes. Therefore all values outside the dashed lines can not be re-used when evaluating the next frame.\nPooling in time has a similar issue, since the downsampling in time makes it problematic to do efficient evaluation.\nTo remediate the problem above, we propose the design from figure 1 (c), which does not have any timepadding and pooling, and therefore looks at a larger context window. For (c) the increased context on the lower layers gives a slightly increased computational cost during CE training (table 2, left column). However architecture (c) is the only architecture that\nallows for efficient sequence training and deployment (table 2, right column). The WER results of network (c) is in the bottom row of table 1. The results of (c) are not significantly different from (b) after 300h-ST and 2000h.\nAs we will discuss in the next section, next to computational efficiency, another big advantage to architecture (c) is the fact that this architecture allows for a modified version of batch normalization at sequence training time."}, {"heading": "4. Batch Normalization", "text": "Batch normalization (BN) [4] is a technique to accelerate training and improve generalization which gained a lot of traction in the deep learning community. The idea of BN is to standardize the internal representations inside the network (i.e. the layer outputs), which helps the network to converge faster and generalize better, inspired by the way whitening the network input improves performance. BN is implemented by standardizing the output of a layer before applying the nonlinearity, using the local mean and variance computed over the minibatch, then correcting with a learned variance and bias term (\u03b3 and \u03b2 resp):\nBN(x) = \u03b3 x\u2212 E[x]\n(Var[x] + )1/2 + \u03b2 (1)\nThe mean and variance (computed over the minibatch) are a cheap and simple-to-implement stochastic approximation of the data statistics at this specific layer, for the current network weights. Since at test time we want to be able to do inference without presence of a minibatch, the prescribed method is to accumulate a running average of the mean and variance during training to be used at test time. For CNNs, the mean and variance is computed over samples and spatial location.\nThe standard formulation of BN for CNNs can be readily applied to cross-entropy training during which the minibatch contains samples from different utterances, different targets and different speakers. However, during sequence training, spliced evaluation as in figure 2 A is problematic. If we construct a minibatch from consecutive windows of the same utterance, the minibatch mean and variance will be poor for two reasons:\n\u2022 The consecutive samples are identical except the shift and border frame. Thus the different samples in the minibatch will be highly correlated. \u2022 The GPU memory will limit the number of samples in the batch (to around 512 samples on our system). Therefore the mean and variance can typically only be computed over one utterance or even just a chunk of an utterance. Both these reasons cause the mean and variance estimate to be a poor approximation of the true data statistics, and to fluctuate strongly between minibatches.\nWe can drastically improve the mean and variance if we have an architecture that allows for efficient evaluation like figure 2 B. In this case both of the issues are solved: firstly there is\nno duplication from splicing. Secondly, several utterances can be processed in one minibatch, since now they fit in GPU memory. We compute the mean and variance over utterances and along the sequence dimension.\nWe aim to maximize the number of frames being processed in a minibatch, in order for the mean and variance to become a better estimate. We achieve this by matching the number of utterances in a minibatch with the utterance length, such that (number of utterances) \u00d7 (max utterance length) = (constant number of frames). The algorithm for batch assembly can be expressed in pseudo-code as:\n\u2022 choose numFrames to maximize GPU usage \u2022 while (training):\n\u2013 targUttLen\u2190 sample from p(uttLen) \u223c f(uttLen) \u00d7 uttLen \u2013 numUtts\u2190 floor(numFrames / uttLen) \u2013 minibatch \u2190 sample (numUtts) utterances with\nlength close to targUttLen Within our implementation of the 10-layer network of figure 1 (c), and a 12 GB Nvidia Tesla K40 GPU, we found numFrames = 6000 to be optimal, taking up about 11GB of memory on the device.\nTable 3 shows the results of the architectural variants (b) and (c) with and without BN. As expected, for architecture (b) with batch normalization we do not obtain good performance from sequence training, since we have to resort to spliced (inefficient) evaluation. The performance is worse with than without BN. In contrast, using the efficient evaluation with architecture (c), using batch normalization improves performance from 10.8 to 10.5 on SWB-1 (300h), matching the performance of the superior architecture (a). On SWB (2000h) adding BN brings the WER down to 9.5, almost matching the result of (a)."}, {"heading": "5. Training details", "text": "To deal with class imbalance, we adopt the balanced sampling from [1], by sampling from context dependent state CDi with probability pi =\nf \u03b3 i\u2211 j f \u03b3 j . We keep \u03b3 = 0.8 throughout the\nexperiments during cross-entropy training. During CE training, we found SGD to work best for networks without BN, and nesterov accelerated gradient (NAG) with momentum 0.99 for networks with BN.\nWe found two elements essential to make the sequence training work well in the stochastic setting:\n\u2022 NAG with momentum 0.99, which we dropped to 0.95 during training (as recommended by [9]). \u2022 Regularization of ST by adding the gradient of crossentropy loss, as proposed in [10]."}, {"heading": "6. Related Work", "text": "The \u201cIntroduction\u201d section of [1] contains an overview of the application of CNNs to Speech Recognition, and the relation of\nthis line of work with other domains. Pooling in time has been applied in the context of Time Delay NNs (TDNNs) [11], and further explored for CNNs in [12]. However, in that paper, the premise was to keep the spatial resolution intact, and pooling in time was found to not matter for the explored architectures. In contrast, we do not follow that premise, subsample in time, and show superior performance over networks without pooling in time. What we call \u201cefficient evaluation\u201d is exactly how CNNs have been applied to sequences since the nineties [8, 13], however there is no mention of the influence of padding and pooling in these works.\nSequence training was introduced to neural network training in [14]. We performed stochastic sequence training with the MPE criterion as opposed to Hessian-Free sequence training [15] which was used in our baselines [3, 5]. As mentioned in section 5, we smoothed the ST loss with CE loss as in [10], and used Nesterov Accelerated Gradient (nag) as optimization method, which was reformulated as a modification to classical momentum in stochastic training of deep neural networks in [9].\nBatch normalization (BN) was introduced in [4], and is closely related to prior work aimed at whitening the activations inside the network [16]. BN was shown to improve the Imagenet classification performance in the GoogLeNet architecture [17] and residual networks [18], which were the top two submissions in 2015 to the ImageNet classification competition.\nWhen applying batch normalization to sequence data, the way of processing multiple utterances as one batch for computing the mean and variance stastistics, is identical to how BN was applied to recurrent neural networks in [19, 20]."}, {"heading": "7. Discussion", "text": "In this paper we demonstrated the strength of very deep convolutional networks applied to speech recognition in the hybrid NN-HMM framework. We obtain a WER of 9.4 after sequence training on the 2000 hour switchboard dataset, which as a single model matches the performance of the state of the art model combination DNN+RNN+CNN from [5]. This model, when combined with a state of the art RNN acoustic model and better language models, obtains significantly better performance on Hub5 than any other published model, see [6].\nWe compared three model variants, and discussed the importance of time-padding and time-pooling.\n\u2022 Architecture (a) with pooling performs better then (b) and (c) without pooling. \u2022 Architecture (c) without padding or pooling is prefered after the cross-entropy stage since it allows for efficient evaluation and batch normalization.\nThis naturally rises the question whether we can combine the best of both: pooling like in architecture (a) and efficient full-utterance processing from architecture (c). This hybrid would mean: removing time-padding in all layers, then apply pooling in higher layers. The downsampling effect from pooling could be solved in two ways: (1) By duplicating the utterances right before pooling, one of the duplications shifted. The result would be equivalent to spliced evaluation. (2) By working with a downsampled utterance in the HMM stage (downsampling has been shown not to hurt in the end-to-end setting [20]). We leave this for future work.\nWe expect additonal gains from using Hessian Free optimization for sequence training."}, {"heading": "8. References", "text": "[1] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, \u201cVery\ndeep multilingual convolutional neural networks for lvcsr,\u201d Proc. ICASSP, 2016.\n[2] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d CoRR arXiv:1409.1556, 2014.\n[3] H. Soltau, G. Saon, and T. N. Sainath, \u201cJoint training of convolutional and non-convolutional neural networks,\u201d to Proc. ICASSP, 2014.\n[4] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d Proc. ICML, 2015.\n[5] G. Saon, H.-K. J. Kuo, S. Rennie, and M. Picheny, \u201cThe ibm 2015 english conversational telephone speech recognition system,\u201d Proc. Interspeech, 2015.\n[6] G. Saon, T. Sercu, S. Rennie, and H.-K. J. Kuo, \u201cThe ibm 2016 english conversational telephone speech recognition system,\u201d -, 2016.\n[7] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhadran, \u201cDeep convolutional neural networks for lvcsr,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8614\u20138618.\n[8] Y. LeCun and Y. Bengio, \u201cConvolutional networks for images, speech, and time series,\u201d The handbook of brain theory and neural networks, vol. 3361, no. 10, p. 1995, 1995.\n[9] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, \u201cOn the importance of initialization and momentum in deep learning,\u201d in Proc. ICML, 2013, pp. 1139\u20131147.\n[10] H. Su, G. Li, D. Yu, and F. Seide, \u201cError back propagation for sequence training of context-dependent deep networks for conversational speech transcription,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6664\u20136668.\n[11] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang, \u201cPhoneme recognition using time-delay neural networks,\u201d Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 37, no. 3, pp. 328\u2013339, 1989.\n[12] T. N. Sainath, B. Kingsbury, G. Saon, H. Soltau, A.-r. Mohamed, G. Dahl, and B. Ramabhadran, \u201cDeep convolutional neural networks for large-scale speech tasks,\u201d Neural Networks, 2014.\n[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n[14] B. Kingsbury, \u201cLattice-based optimization of sequence classification criteria for neural-network acoustic modeling,\u201d in Proc. ICASSP. IEEE, 2009, pp. 3761\u20133764.\n[15] B. Kingsbury, T. N. Sainath, and H. Soltau, \u201cScalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization,\u201d in Thirteenth Annual Conference of the International Speech Communication Association, 2012.\n[16] S. Wiesler, A. Richard, R. Schluter, and H. Ney, \u201cMeannormalized stochastic gradient for large-scale deep learning,\u201d in proc. ICASSP. IEEE, 2014, pp. 180\u2013184.\n[17] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \u201cRethinking the inception architecture for computer vision,\u201d CoRR arXiv:1512.00567, 2015.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d CoRR arXiv:1512.03385, 2015.\n[19] C. Laurent, G. Pereyra, P. Brakel, Y. Zhang, and Y. Bengio, \u201cBatch normalized recurrent neural networks,\u201d Proc. ICASSP, 2016.\n[20] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos et al., \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d CoRR arXiv:1512.02595, 2015."}], "references": [{"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR arXiv:1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["H. Soltau", "G. Saon", "T.N. Sainath"], "venue": "to Proc. ICASSP, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proc. ICML, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The ibm 2015 english conversational telephone speech recognition system", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "The ibm 2016 english conversational telephone speech recognition system", "author": ["G. Saon", "T. Sercu", "S. Rennie", "H.-K.J. Kuo"], "venue": "-, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["T.N. Sainath", "A.-r. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8614\u20138618.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, no. 10, p. 1995, 1995.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1995}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton"], "venue": "Proc. ICML, 2013, pp. 1139\u20131147.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["H. Su", "G. Li", "D. Yu", "F. Seide"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6664\u20136668.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 37, no. 3, pp. 328\u2013339, 1989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Deep convolutional neural networks for large-scale speech tasks", "author": ["T.N. Sainath", "B. Kingsbury", "G. Saon", "H. Soltau", "A.-r. Mohamed", "G. Dahl", "B. Ramabhadran"], "venue": "Neural Networks, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1998}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B. Kingsbury"], "venue": "Proc. ICASSP. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable minimum bayes risk training of deep neural network acoustic models using distributed hessian-free optimization", "author": ["B. Kingsbury", "T.N. Sainath", "H. Soltau"], "venue": "Thirteenth Annual Conference of the International Speech Communication Association, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Meannormalized stochastic gradient for large-scale deep learning", "author": ["S. Wiesler", "A. Richard", "R. Schluter", "H. Ney"], "venue": "proc. ICASSP. IEEE, 2014, pp. 180\u2013184.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "CoRR arXiv:1512.00567, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR arXiv:1512.03385, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalized recurrent neural networks", "author": ["C. Laurent", "G. Pereyra", "P. Brakel", "Y. Zhang", "Y. Bengio"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "CoRR arXiv:1512.02595, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "We present advances and results on using very deep VGG-style convolutional networks as acoustic model for Large Vocabulary Speech Recognition, extending our earlier work [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 0, "context": "In [1], we introduced very deep convolutional network architectures to LVCSR, presenting strong results on babel and the 300-hour switchboard-1 dataset (SWB-1) after crossentropy training only.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "The very deep convolutional networks are inspired by the \u201cVGG Net\u201d architecture introduced in [2] for the 2014 ImageNet classification challenge.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "Furthermore, by applying zero-padding throughout and only reducing spatial resolution through pooling, the networks in [2] were simple and elegant.", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "We followed this design in the acoustic model CNNs [1].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "The VGG Net-inspired networks from [1] achieved 11.", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "6% relative) improvement over the baseline 2-layer sigmoid CNN from [3] after cross-entropy training on 300-hour switchboard.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "The networks from [1] pool in time with stride 2 on the higher layers of the network, and applied time-padding throughout.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "At the other hand, we discuss why the design from [1] with time-pooling and time-padding is problematic for efficient evaluation of a full utterance (section 3).", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "\u2022 We demonstrate the merit of batch normalization (BN) [4] for CNN acoustic models (section 4).", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "In our deepest 10-layer CNN in [1], we reduce the frequency-dimension size from 40 to 20, 10, 4, 2 through pooling after the second, fourth, seventh and tenth convolutional layer, while the convolutions are zero-padded in order to prear X iv :1 60 4.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "Figure 1: Different versions of the 10-layer CNN from [1].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "(a) This is the original (WDX) architecture from [1], starting from a 16-frame window.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Classic 512 [3] 13.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "4 Classic+AD+Maxout [5] 12.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "9* DNN+RNN+CNN [5] - 11.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "*New results [6].", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "It was argued in [7] that downsampling in time should be avoided, rather pooling with stride 1 was used, which does not downsample in time.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "However, in [1] we did pool with stride 2.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "Apart from this practical reason, we did not justify this design choice in [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "As argued in [8], once a feature is detected in the lower layers, its exact location does not matter that much anymore and can be blurred out, as long as its approximate relative position is conserved.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Figure 1 summarizes three variations of the 10-layer architecture, (a) being the original version of [1], Figure 1 (b) shows an alternative to pooling in time.", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "The result after 2000 hours and sequence training matches the system combination of a classical CNN, DNN and RNN from [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "In the previous section we showed how time padding and pooling allowed for the design which was presented in [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "Batch normalization (BN) [4] is a technique to accelerate training and improve generalization which gained a lot of traction in the deep learning community.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "To deal with class imbalance, we adopt the balanced sampling from [1], by sampling from context dependent state CDi with", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "95 during training (as recommended by [9]).", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "\u2022 Regularization of ST by adding the gradient of crossentropy loss, as proposed in [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "The \u201cIntroduction\u201d section of [1] contains an overview of the application of CNNs to Speech Recognition, and the relation of this line of work with other domains.", "startOffset": 30, "endOffset": 33}, {"referenceID": 10, "context": "Pooling in time has been applied in the context of Time Delay NNs (TDNNs) [11], and further explored for CNNs in [12].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "Pooling in time has been applied in the context of Time Delay NNs (TDNNs) [11], and further explored for CNNs in [12].", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "What we call \u201cefficient evaluation\u201d is exactly how CNNs have been applied to sequences since the nineties [8, 13], however there is no mention of the influence of padding and pooling in these works.", "startOffset": 106, "endOffset": 113}, {"referenceID": 12, "context": "What we call \u201cefficient evaluation\u201d is exactly how CNNs have been applied to sequences since the nineties [8, 13], however there is no mention of the influence of padding and pooling in these works.", "startOffset": 106, "endOffset": 113}, {"referenceID": 13, "context": "Sequence training was introduced to neural network training in [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "We performed stochastic sequence training with the MPE criterion as opposed to Hessian-Free sequence training [15] which was used in our baselines [3, 5].", "startOffset": 110, "endOffset": 114}, {"referenceID": 2, "context": "We performed stochastic sequence training with the MPE criterion as opposed to Hessian-Free sequence training [15] which was used in our baselines [3, 5].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "We performed stochastic sequence training with the MPE criterion as opposed to Hessian-Free sequence training [15] which was used in our baselines [3, 5].", "startOffset": 147, "endOffset": 153}, {"referenceID": 9, "context": "As mentioned in section 5, we smoothed the ST loss with CE loss as in [10], and used Nesterov Accelerated Gradient (nag) as optimization method, which was reformulated as a modification to classical momentum in stochastic training of deep neural networks in [9].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "As mentioned in section 5, we smoothed the ST loss with CE loss as in [10], and used Nesterov Accelerated Gradient (nag) as optimization method, which was reformulated as a modification to classical momentum in stochastic training of deep neural networks in [9].", "startOffset": 258, "endOffset": 261}, {"referenceID": 3, "context": "Batch normalization (BN) was introduced in [4], and is closely related to prior work aimed at whitening the activations inside the network [16].", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "Batch normalization (BN) was introduced in [4], and is closely related to prior work aimed at whitening the activations inside the network [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "BN was shown to improve the Imagenet classification performance in the GoogLeNet architecture [17] and residual networks [18], which were the top two submissions in 2015 to the ImageNet classification competition.", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "BN was shown to improve the Imagenet classification performance in the GoogLeNet architecture [17] and residual networks [18], which were the top two submissions in 2015 to the ImageNet classification competition.", "startOffset": 121, "endOffset": 125}, {"referenceID": 18, "context": "When applying batch normalization to sequence data, the way of processing multiple utterances as one batch for computing the mean and variance stastistics, is identical to how BN was applied to recurrent neural networks in [19, 20].", "startOffset": 223, "endOffset": 231}, {"referenceID": 19, "context": "When applying batch normalization to sequence data, the way of processing multiple utterances as one batch for computing the mean and variance stastistics, is identical to how BN was applied to recurrent neural networks in [19, 20].", "startOffset": 223, "endOffset": 231}, {"referenceID": 4, "context": "4 after sequence training on the 2000 hour switchboard dataset, which as a single model matches the performance of the state of the art model combination DNN+RNN+CNN from [5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 5, "context": "This model, when combined with a state of the art RNN acoustic model and better language models, obtains significantly better performance on Hub5 than any other published model, see [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 19, "context": "(2) By working with a downsampled utterance in the HMM stage (downsampling has been shown not to hurt in the end-to-end setting [20]).", "startOffset": 128, "endOffset": 132}], "year": 2017, "abstractText": "Very deep CNNs with small 3 \u00d7 3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper, we demonstrate that the accuracy gains of these deep CNNs are retained both on larger scale data, and after sequence training. We show this by carrying out sequence training on both the 300h switchboard-1 and the 2000h switchboard dataset. Furthermore, we investigate how pooling and padding in time influences performance, both in terms of word error rate and computational cost. We argue that designing CNNs without timepadding and without timepooling, though slightly suboptimal for accuracy, has two significant consequences. Firstly, the proposed design allows for efficient evaluation at sequence training and test (deployment) time. Secondly, this design principle allows for batch normalization to be adopted to CNNs on sequence data. Our very deep CNN model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching with a single model the performance of 2015 IBM system combination, which was the previous best published result.", "creator": "LaTeX with hyperref package"}}}