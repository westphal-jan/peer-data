{"id": "1604.03058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Binarized Neural Networks on the ImageNet Classification Task", "abstract": "we trained binarized neural networks ( bnns ) on the high resolution imagenet lsvrc - 2102 dataset classification task and achieved progressively a good performance. with a moderate size network of 10 layers, we obtained top - tech 5 classification accuracy rate of 81 percent on validation set which is much better than previous published results. we expect training vector networks of educating a firm much better performance through increase network depth would be straight forward supplemented by analysis following our current strategies. a detailed discussion on strategies primarily used in upgrading the network training is included as well well as preliminary analysis.", "histories": [["v1", "Mon, 11 Apr 2016 18:39:33 GMT  (471kb)", "http://arxiv.org/abs/1604.03058v1", null], ["v2", "Wed, 13 Apr 2016 03:22:37 GMT  (0kb,I)", "http://arxiv.org/abs/1604.03058v2", "More testing and verification is needed"], ["v3", "Mon, 24 Oct 2016 15:25:06 GMT  (430kb)", "http://arxiv.org/abs/1604.03058v3", "Updated with new results"], ["v4", "Tue, 8 Nov 2016 00:38:03 GMT  (512kb)", "http://arxiv.org/abs/1604.03058v4", null], ["v5", "Sat, 19 Nov 2016 01:37:40 GMT  (512kb)", "http://arxiv.org/abs/1604.03058v5", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["xundong wu", "yong wu", "yong zhao"], "accepted": false, "id": "1604.03058"}, "pdf": {"name": "1604.03058.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["wuxundong@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n03 05\n8v 1\n[ cs\n.C V"}, {"heading": "1 Introduction", "text": "Recent wave of deep learning research has brought the deep neural network to the frontier of AI applications. Current implementations of deep neural networks in wide variety of fields heavily rely on high computing power, energy hungry hardware such as GPUs, which limits implementation of those neural networks in embedded environment such as mobile phones or wearable hardware. Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).\nRecently, (Rastegari et al., 2016; Courbariaux et al., 2016, 2015) have been trying to reduce synapse weight and/or intermediate signal representation to binary form. As been shown binarification of neural network can dramatically reduce the computing need, and also the amount of memory needed for weight and intermediate results storage. These approaches are especially useful for memory constrained environment such as the FPGA implementation.\nIn this work, we designed a network architecture based on Binarized Neural Networks (BNN), originally proposed by (Courbariaux et al., 2016), and trained networks on the ImageNet classification task. We identified an issue that caused slow learning with original BNN when it is combined with large dimension Softmax output function. An efficient solution is proposed that greatly speed up the network training.\nWe also analyzed the behavior of weights adaptation during SGD training and use the observation to guide the setting of proper learning rate used in the network training. This also leads to significant gain in the learning speed of the network.\nWith a moderate size network an 81 percent top 5 accuracy on validation set of ImageNet classification task is achieved. We expect training of better performing networks will be quite straight-forward as we observed steady performance gain with more layers added to the network."}, {"heading": "2 Results", "text": ""}, {"heading": "2.1 Softmax Input Rescaling", "text": "A pure BNN layer has an output of large range of magnitude due to the binary weights and binary inputs from previous layer. At the initial state of the network for a standard dense fully connected layer, if we assume the layer input is of 50% density, i.e. half of the inputs have activation of +1, the rest have activation of -1, and the synapse weights are also initialized to be 50% density. The mean of unit output of this layer will be 0, while the \u03c3 will be \u221a\nN , here N is the number of inputs the layer receives which is typical quite large for the last layer of a network used for tasks such as ImageNet data. With a typical N = 4096 used in this study, which give us \u03c3 value of 64. This won\u2019t be a problem if the output of a BNN layer is followed by a batch normalization layer as in (Courbariaux et al., 2016), since batch normalization bring the activation into right range. In (Courbariaux et al., 2016) case, the network output has small dimensionality, for example in MNIST case the output dimension is 10. For ImageNet data we need to handle output of 1000 different categories thus the activation of output units are forced to be very sparse. Which means for a moderate batch size, for example 256, used in our SGD learning batch normalization will be difficult. For this reason, we cannot use batch normalization at last layer before the last layer non-linearity in this case.\nHere we used log-loss with Softmax function on the last layer of the network. That is: with the last BNN layer output X of N = 1, 000 dimension, and ground truth training target Y , we pass X through Softmax function, which gave us Pj = e\nxj\u2211 k exk and\nfrom there we got loss function l = \u22121/N \u2211N\ni Yilog(Pi). Due to the combination of strong non-linearity with the large output variation range of BNN layer, we found that learning can be quite slow especially at the early stage of the training.\nWe propose an efficient solution to overcome the learning difficulty described above. Here we insert a scaling layer between the last BNN layer and the Softmax nonlinearity. In this way the input to the Softmax function will be in the proper range thus avoid the learning difficulty. In most of our experiments we used a scaling layer with an initial scale factor shared by all units to re-scale input of Softmax to standard deviation of 1 for fast learning. As shown in Figure1, indeed such re-scaling significantly speed up the network training. We also tried a fixed-value scaling layer with 1/\u03c3 scale factor, small scale experiment shows it performs slight worse than a layer with a trainable scaling factor."}, {"heading": "2.2 First layer architecture", "text": "In the original BNN layer designed for CIFAR-10 dataset as in (Courbariaux et al., 2016), all layers of network are binary. The filters used in the first layer of that network have small kernel size of 3 and small stride step of 1. Such design is suitable for CIFAR data since it comes with image size of 32x32, which is very small when compared with the typical input image size used in ImageNet data of 224x224. To reduce the computing need for the network, typically a large convolutional kernel size and big stride steps are used for the first layer of in most of state of art networks trained on ImageNet data. For this reason, most networks implemented in this work also start with large kernel sizes (7 or 11) and big stride steps (2 or 4) on first layer and directly followed by a max-pooling layer to reduce first convolutional layer output size. Show in Figure 2, when a real value weight layer is used in the first layer the network performance is slightly better than pure binary network. Therefore for the rest of this study all networks trained in this study we use real-value weight in the first layer of network.\nSince the first layer input only have 3 input channels, this won\u2019t significantly increase computing load for the whole network. Due to the bottle-neck effect of a binary representation, a BNN might benefit from extra communication channels, it is possible with smaller convolutional kernel size and stride steps on the first layer we might achieve similar or even better performance with binary weight first layer. However, limited by the available computing resource, currently we are not able to test this hypothesis."}, {"heading": "2.3 Weight adaptation behavior analysis and learning rate selection", "text": "In a BNN layer, at training stage real-valued version of synaptic weights are used during SGD training. This extra resolution is removed at the stage of computing layer output. It is believed that real-valued weights are needed for smooth out the noise in the SGD learning (Courbariaux et al., 2016). Here we show indeed this is the case. When a high learning rate is used for training as suggested in (Rastegari et al., 2016), after one epoch of training, we observed most weight value stay around -1 and +1 (Figure 3a) because of weight clipping. Under such condition weights quickly jump between edge values of -1 and +1 with little accumulation process. Correspondingly a rather slow loss decline process is observed (Figure 3c). When we lower learning rate to 0.001, we observed a weight distribution more toward uniform or concentrated around 0, indicating weights are taking accumulated steps to travel between -1 and +1 (Figure 3b). Correspondingly, a much faster loss decline was observed during the training process (Figure 3c). We also show the result of 20 full epoch training with initial learning rate of 0.1 and 0.001 decay at fix rate in Figure 3d."}, {"heading": "3 Discussion", "text": "Our results show that an ImageNet scale of BNN can be trained to a good performance with a good network architecture and training strategies. At moment the best network we trained is a 10 layers network with 81 percent top 5 accuracy rate, which is much better than the result shown in (Rastegari et al., 2016).\nLimited by the available computing resource, we mostly tested networks with real value weight first layer of big filters and big stride. Judge from some of our small scale experiments and also results shown by (Courbariaux et al., 2016), it is very possible a purely binary network can achieve state of art level of performance with small filter size and smaller stride step. A pure binary network is also desirable since it further reduce and simplify the network design in constrained environment."}], "references": [{"title": "Compressing neural networks with the hashing trick", "author": ["Chen", "Wenlin", "Wilson", "James T", "Tyree", "Stephen", "Weinberger", "Kilian Q", "Yixin"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Courbariaux", "Matthieu", "Bengio", "Yoshua", "David", "Jean-Pierre"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Courbariaux", "Matthieu", "Hubara", "Itay", "Soudry", "Daniel", "El-Yanivand", "Ran", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "Courbariaux et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Han", "Song", "Pool", "Jeff", "Tran", "John", "Dally", "William"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Jaderberg", "Max", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1405.3866,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Xnornet: Imagenet classification using binary convolutional neural networks", "author": ["Rastegari", "Mohammad", "Ordonez", "Vicente", "Redmon", "Joseph", "Farhadi", "Ali"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).", "startOffset": 90, "endOffset": 151}, {"referenceID": 0, "context": "Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).", "startOffset": 90, "endOffset": 151}, {"referenceID": 4, "context": "Much efforts have been put to reduce computing need of deep neural networks, for example, (Han et al., 2015; Chen et al., 2015; Jaderberg et al., 2014).", "startOffset": 90, "endOffset": 151}, {"referenceID": 5, "context": "Recently, (Rastegari et al., 2016; Courbariaux et al., 2016, 2015) have been trying to reduce synapse weight and/or intermediate signal representation to binary form.", "startOffset": 10, "endOffset": 66}, {"referenceID": 2, "context": "In this work, we designed a network architecture based on Binarized Neural Networks (BNN), originally proposed by (Courbariaux et al., 2016), and trained networks on the ImageNet classification task.", "startOffset": 114, "endOffset": 140}, {"referenceID": 2, "context": "This won\u2019t be a problem if the output of a BNN layer is followed by a batch normalization layer as in (Courbariaux et al., 2016), since batch normalization bring the activation into right range.", "startOffset": 102, "endOffset": 128}, {"referenceID": 2, "context": "In (Courbariaux et al., 2016) case, the network output has small dimensionality, for example in MNIST case the output dimension is 10.", "startOffset": 3, "endOffset": 29}, {"referenceID": 2, "context": "In the original BNN layer designed for CIFAR-10 dataset as in (Courbariaux et al., 2016), all layers of network are binary.", "startOffset": 62, "endOffset": 88}, {"referenceID": 2, "context": "It is believed that real-valued weights are needed for smooth out the noise in the SGD learning (Courbariaux et al., 2016).", "startOffset": 96, "endOffset": 122}, {"referenceID": 5, "context": "When a high learning rate is used for training as suggested in (Rastegari et al., 2016), after one epoch of training, we observed most weight value stay around -1 and +1 (Figure 3a) because of weight clipping.", "startOffset": 63, "endOffset": 87}, {"referenceID": 5, "context": "At moment the best network we trained is a 10 layers network with 81 percent top 5 accuracy rate, which is much better than the result shown in (Rastegari et al., 2016).", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "Judge from some of our small scale experiments and also results shown by (Courbariaux et al., 2016), it is very possible a purely binary network can achieve state of art level of performance with small filter size and smaller stride step.", "startOffset": 73, "endOffset": 99}], "year": 2016, "abstractText": "We trained Binarized Neural Networks (BNNs) on the high resolution ImageNet LSVRC-2102 dataset classification task and achieved a good performance. With a moderate size network of 10 layers, we obtained top-5 classification accuracy rate of 81 percent on validation set which is much better than previous published results. We expect training networks of a much better performance through increase network depth would be straight forward by following our current strategies. A detailed discussion on strategies used in the network training is included as well as preliminary analysis.", "creator": "LaTeX with hyperref package"}}}