{"id": "1604.03099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Symbolic Knowledge Extraction using {\\L}ukasiewicz Logics", "abstract": "this work describes both a methodology that combines integrated logic - based systems grammar and connectionist systems. our approach uses finite truth - valued { \\ l } ukasiewicz logic, wherein every connective can be defined by a neuron in an existing artificial network. this allowed the injection of first - tree order formulas into a network architecture, and also simplified symbolic rule extraction. for that we trained a neural string networks using the levenderg - hoffmann marquardt algorithm, allowing where we restricted the knowledge dissemination in the network structure. this procedure reduces neural network plasticity without drastically damaging the learning performance, with thus making the descriptive power of produced neural networks similar to the descriptive power of { \\ l } ukasiewicz logic language and simplifying the translation between symbolic and connectionist structures. we used this numerical method for reverse engineering truth table and in extraction of formulas from real data sets.", "histories": [["v1", "Mon, 11 Apr 2016 05:17:09 GMT  (16kb)", "http://arxiv.org/abs/1604.03099v1", "15 pages. arXiv admin note: substantial text overlap witharXiv:1604.02780,arXiv:1604.02774"]], "COMMENTS": "15 pages. arXiv admin note: substantial text overlap witharXiv:1604.02780,arXiv:1604.02774", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["carlos leandro"], "accepted": false, "id": "1604.03099"}, "pdf": {"name": "1604.03099.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["miguel.melro.leandro@gmail.com"], "sections": null, "references": [{"title": "The logic of neural networks", "author": ["J. Castro", "E. Trillas"], "venue": "Mathware and Soft Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Logic programs and connectionist networks", "author": ["P. Hitzler", "S. H\u00f6lldobler", "A. Seda"], "venue": "Journal of Applied Logic,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Challenge problems for the integration of logic and connectionist systems", "author": ["S. H\u00f6lldobler"], "venue": "Proceedings 14. Workshop Logische Programmierung, GMD Report", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Knowledge-based connectionism from revising domain theories", "author": ["L. Fu"], "venue": "IEEE Trans. Syst. Man. Cybern,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1993}, {"title": "Knowledge-based artificial neural networks", "author": ["G. Towell", "J. Shavlik"], "venue": "Artif. Intell.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Connectionist expert systems", "author": ["S. Gallant"], "venue": "Commun. ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "Neural Network Learning and Expert Systems", "author": ["S. Gallant"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Extracting refined rules from knowledge-based neural networks", "author": ["G. Towell", "J. Shavlik"], "venue": "Mach. Learn.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Training feed-forward networks with marquardt algorithm", "author": ["M. Hagan", "M. Menhaj"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Optimal brain surgeon and general network pruning", "author": ["B. Hassibi", "D. Stork", "G. Wolf"], "venue": "IEEE International Conference on Neural Network,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Fuzzy logic from the logical point of view", "author": ["P. H\u00e1jek"], "venue": "In Proceedings SOFSEM\u201995,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Neural networks and rational lukasiewicz logic", "author": ["P. Amato", "A. Nola", "B. Gerla"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Semantics of architectural connectors", "author": ["J. Fiadeiro", "A. Lopes"], "venue": "TAPSOFT\u201997 LNCS, v.1214,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "A modified regression algorithm for fast one layer neural network training", "author": ["T. Andersen", "B. Wilamowski"], "venue": "World Congress of Neural Networks, Washington DC, USA,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1995}, {"title": "Frist- and second-order methods for learning between steepest descent and newton\u2019s method", "author": ["R. Battiti"], "venue": "Neural Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Neural Network Design", "author": ["M. Hagan", "H. Demuth", "M. Beal"], "venue": "PWS Publishing Company,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Our approach uses finite truth-valued Lukasiewicz logic, wherein every connective can be defined by a neuron in an artificial network [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Such neuro-symbolic models are currently a very active area of research: for the extraction of logic programs from trained networks see [2] [3].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Such neuro-symbolic models are currently a very active area of research: for the extraction of logic programs from trained networks see [2] [3].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "This is done on knowledge-based networks [4] [5], to generate the initial network architecture from crude symbolic domain knowledge.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "This is done on knowledge-based networks [4] [5], to generate the initial network architecture from crude symbolic domain knowledge.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "However in [6] [7] [8] this processes is used by identifing the most significant determinants of decision or classification.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "However in [6] [7] [8] this processes is used by identifing the most significant determinants of decision or classification.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "However in [6] [7] [8] this processes is used by identifing the most significant determinants of decision or classification.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Every logic connective can be defined by a neuron in an artificial network having, by activation function, the identity truncated to zero and one [1].", "startOffset": 146, "endOffset": 149}, {"referenceID": 8, "context": "Multilayer feedforward NN, having this type of activation function, can be trained efficiently using the Levenderg-Marquardt (LM) algorithm [9], and the generated network can be simplified quickly using the \u201dOptimal Brain Surgeon\u201d algorithm proposed by B.", "startOffset": 140, "endOffset": 143}, {"referenceID": 9, "context": "Stork [10].", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "A many-valued logic having [0, 1] as set of truth values is called a fuzzy logic.", "startOffset": 27, "endOffset": 33}, {"referenceID": 10, "context": "The fuzzy logic defined using Lukasiewicz t-norm is called Lukasiewicz logic ( Llogic) and the corresponding propositional calculus has a nice complete axiomatization [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 11, "context": "2 Processing units As mentioned in [12] there is a lack of a deep investigation of the relationships between logics and NNs.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "In [1] it is shown how, by taking as activation function, \u03c8, the identity truncated to zero and one, \u03c8(x)=min(1,max(x,0)), it is possible to represent the corresponding NN as a combination of propositions of Lukasiewicz calculus and viceversa [12].", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "In [1] it is shown how, by taking as activation function, \u03c8, the identity truncated to zero and one, \u03c8(x)=min(1,max(x,0)), it is possible to represent the corresponding NN as a combination of propositions of Lukasiewicz calculus and viceversa [12].", "startOffset": 243, "endOffset": 247}, {"referenceID": 12, "context": "This task of construct complex structures based on simplest ones can be formalized using generalized programming [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "A truth table f\u03c6 for a formula \u03c6, in a fuzzy logic, is a map f\u03c6 : [0, 1] m \u2192 [0, 1], where m is the number of propositional variables used in \u03c6.", "startOffset": 66, "endOffset": 72}, {"referenceID": 0, "context": "A truth table f\u03c6 for a formula \u03c6, in a fuzzy logic, is a map f\u03c6 : [0, 1] m \u2192 [0, 1], where m is the number of propositional variables used in \u03c6.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "Each n > 0, defines a sub-table for f\u03c6 defined by f (n) \u03c6 : (Sn) m \u2192 [0, 1], given by f (n) \u03c6 (v\u0304) = f\u03c6(v\u0304), and called the \u03c6 (n+1)-valued truth sub-table.", "startOffset": 69, "endOffset": 75}, {"referenceID": 0, "context": "Since, for every network N and n > 0, \u2206(N) \u2265 \u2206(\u03a5n(N)), we have: Proposition 7 Given a NNs N with weights in the interval [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 9, "context": "Stork in [10].", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "The Levenberg-Marquardt (LM) algorithm [9] [14] ensued from the development of EBP algorithm-dependent methods.", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "The Levenberg-Marquardt (LM) algorithm [9] [14] ensued from the development of EBP algorithm-dependent methods.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "It gives a good exchange between the speed of the Newton algorithm and the stability of the steepest descent method [15].", "startOffset": 116, "endOffset": 120}, {"referenceID": 15, "context": "In this way, the performance function is always reduced at each iteration of the algorithm [16].", "startOffset": 91, "endOffset": 95}], "year": 2016, "abstractText": "This work describes a methodology that combines logic-based systems and connectionist systems. Our approach uses finite truth-valued Lukasiewicz logic, wherein every connective can be defined by a neuron in an artificial network [1]. This allowed the injection of first-order formulas into a network architecture, and also simplified symbolic rule extraction. For that we trained a neural networks using the Levenderg-Marquardt algorithm, where we restricted the knowledge dissemination in the network structure. This procedure reduces neural network plasticity without drastically damaging the learning performance, thus making the descriptive power of produced neural networks similar to the descriptive power of Lukasiewicz logic language and simplifying the translation between symbolic and connectionist structures. We used this method for reverse engineering truth table and in extraction of formulas from real data sets. \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-", "creator": "LaTeX with hyperref package"}}}