{"id": "1604.03200", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Efficient Classification of Multi-Labelled Text Streams by Clashing", "abstract": "we present a method for the classification of multi - labelled text documents explicitly designed for data message stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. traditionally our method is composed of an online procedure used to efficiently map text into a low - dimensional transparent feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multiple multi - label text annotations. documents contents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using taking the statistics computed from the labelled instances colliding in the same region. this approach is referred to as clashing. we illustrate the method in real - world text data, comparing the results with those obtained using other text classifiers. in addition, we provide an analysis about the general effect of the representation space dimensionality on the predictive performance of the system. our results show that the online cache embedding ability indeed approximates the geometry of the full corpus - wise tf and tf - idf space. the model obtains competitive f measures with respect to the most accurate methods, using significantly fewer computational resources. in addition, the method achieves a higher macro - averaged f measure than methods developed with similar running time. furthermore, the system is able to learn faster than the other methods from partially labelled streams.", "histories": [["v1", "Tue, 12 Apr 2016 01:52:38 GMT  (1073kb,D)", "http://arxiv.org/abs/1604.03200v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["ricardo \\~nanculef", "ilias flaounas", "nello cristianini"], "accepted": false, "id": "1604.03200"}, "pdf": {"name": "1604.03200.pdf", "metadata": {"source": "META", "title": "Efficient Classification of Multi-Labelled Text Streams by Clashing", "authors": ["Ricardo \u00d1anculefa", "Ilias Flaounasb", "Nello Cristianinib"], "emails": ["jnancu@inf.utfsm.cl", "ilias.flaounas@bristol.ac.uk", "nello.cristianini@bristol.ac.uk"], "sections": [{"heading": null, "text": "We present a method for the classification of multi-labelled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time.\nOur method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labelled instances colliding in the same region. This approach is referred to as clashing.\nWe illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labelled streams.\nKeywords: Text Classification, Data Streams, Multi-label Classification, Feature Hashing, Massive Data Mining"}, {"heading": "1. INTRODUCTION", "text": "The efficient analysis of massive datasets is one of the main challenges in modern machine learning and data mining applications (Rajaraman and Ullman, 2012; Hand, 2013; Wu et al., 2014). Usually in these scenarios, data is being generated continuously, arriving to the system in the form of a fast and virtually infinite data stream (Aggarwal, 2007; Bifet, 2013). Examples include the stream of messages exchanged on a social\nEmail addresses: jnancu@inf.utfsm.cl (Ricardo \u00d1anculef), ilias.flaounas@bristol.ac.uk (Ilias Flaounas), nello.cristianini@bristol.ac.uk (Nello Cristianini)\nPreprint submitted to Elsevier\nar X\niv :1\n60 4.\n03 20\n0v 1\n[ cs\n.A I]\n1 2\nA pr\n2 01\nnetwork or the stream of daily stories generated by different news outlets. The challenge for a mining system designed to work under this setting is being ready to predict at any time, learning constantly from new observations, but using limited computational resources i.e. bounded memory and constant processing time.\nIn this paper, we present a simple and efficient method to classify large data streams of documents: one of the most common type of user generated data. Document classification is one of the most frequent and important problems in textual data analysis, with applications from information retrieval and spam filtering to content personalization and natural language processing. The task is that of learning a mechanism from data to automatically annotate documents with thematic categories or labels from a given set (Sebastiani, 2002; Aggarwal and Zhai, 2012). Since documents can be associated with multiple non-exclusive categories (e.g. politics, economics and international affairs) simultaneously, this task is one of the most common examples of multi-label classification (Tsoumakas et al., 2010).\nThe novelty and contribution of this work is in addressing explicitly and simultaneously text representation and multi-label classification in streaming environments with limited computational resources. As we explain below, this is a challenging setting because of the large number of possible features arising in textual domains and the traditional batch setting for dimensionality reduction and multi-label classifier design (Read et al., 2012)."}, {"heading": "1.1. Context of this Research", "text": "In this section we motivate the settings of this research, discussing current work in the field, and stressing the novel aspects of our approach."}, {"heading": "1.1.1. Text Representation", "text": "Different methods to classify documents have been investigated in the last years (Aggarwal and Zhai, 2012; Sebastiani, 2002). A fundamental component of these systems is the way to represent text into an amenable form for machine learning algorithms. This representation is commonly obtained by selecting a set of indexing terms suitable to capture document content (the vocabulary) and a weighting scheme assigning values to the dimensions of the feature vector spanned them (Joachims, 2002; Zhang et al., 2011). The bag of words (BOW) indexing model is the most widely used text representation method in current research (Lan et al., 2009; Ren and Sohrab, 2013). In this model, each possible word in the set of known texts corresponds to a dimension of the feature space used to embedd documents. Along with BOW, the TF-IDF weighting scheme is usually applied to obtain the final representation of a document (Lan et al., 2009; Aggarwal and Zhai, 2012). TF-IDF is proportional to the number of times a particular word appeared in a document and inversely proportional to the number of documents containing the word. Despite its widespread acceptance among practitioners, this approach for text representation has some drawbacks that recently have started to be addressed by researchers in the field. First, documents are treated as collections of unordered words. A number of authors have thus investigated the use of longer indexing units, linguistically or statistically meaningful for content identification (Zhang et al., 2011). They include k-grams (Caropreso et al., 2001), frequent word sequences (Li et al., 2008) and frequent word sets (Zhang et al., 2010, 2011). Unfortunately, in text categorization problems,\nthese methods have shown improvements somewhat disappointing (Li et al., 2011; Zhang et al., 2011). Second, TF-IDF does not exploit the co-occurrence of terms and categories in the weighting process. Therefore, methods capable to exploit information about the different distribution of terms among the documents of each class have been focus of increasing interest in the last years (Lan et al., 2009; Guan et al., 2009; Luo et al., 2011; Ren and Sohrab, 2013; Wang and Zhang, 2013) and is shaping up as an important direction of research. However, as regards the scalability of text classification systems, the most important drawback of BOW and TF-IDF is the high-dimensionality of the resulting representation space.\nThe dimensionality of TF-IDF matches the size of the vocabulary i.e. the number of distinct terms across the entire dataset. This \u201ccurse of dimensionality\u201d in traditional text representation brings about huge memory requirements and huge computation since most classification models scale linearly or super linearly in the dimensionality of the feature set size. In data stream scenarios, the problem is still worse, because the word distribution (required to compute TF and IDF) is not known beforehand and both the vocabulary and the corpus is constantly growing. Recently, various feature selection techniques to reduce dimensionality have been studied and compared in text domains (Forman, 2003; Fragoudis et al., 2005; Yang et al., 2012; Spola\u00f4r and Tsoumakas, 2013). Most of these methods correspond to filter approaches, that is, methods selecting features from general characteristics of the training data regardless of the learning algorithm (Spola\u00f4r and Tsoumakas, 2013). Although wrapping methods, using the classifier to determine the quality of selected features, usually outperform filter methods, they tend to be prohibitively expensive on large-scale datasets (Yang et al., 2012; Wang et al., 2013b; Spola\u00f4r and Tsoumakas, 2013) and thus more simple and efficient methods such as Information Gain, Chi-Squared and Bi-normal Separation (Forman, 2003) are preferred in practice. Unfortunately, despite the increasing importance of data stream scenarios, most studies on feature selection are restricted to the batch setting, that is, the selection task is conducted off-line and all the features and training instances are supposed to be known a priori. Thus, weighting schemes like TF-IDF and feature selection methods used to reduce dimensionality, need to be updated at a corpus level which demands lots of computation and requires the storage of a large amount of training data. A fully online feature selection method has been recently presented in (Wang et al., 2013b). However, it is focused on a binary single label classification with a perceptron classifier. Indeed, research on multi-label feature selection is still very scarce. Feature selection for text datasets often applies traditional filter methods focusing on a single label and then uses some aggregation strategy to obtain a decision. It is well known that this approach can neglect strongly predictive features for unfrequent labels in unbalanced categorization problems. Therefore, both multi label and online feature selection are still topics that need to be studied in the field.\nIn this paper we investigate a fully online embedding method for approximating TFIDF using constant memory and time. In contrast to most dimensionality reduction approaches, we do not apply a reduction method on the original TF-IDF representation but directly on the sequence of words contained in a document. The method is built on the ideas of count min sketching (Cormode, 2012) and feature hashing (Shi et al., 2009b,a), methods introduced to estimate data stream distributions and high dimensional dot products respectively. Therefore, our method is related also to other data-oblivious embedding techniques like random projections (RP) (Achlioptas, 2003) and fast Johnson\nLindenstrauss transforms (FJL) (Ailon and Chazelle, 2010), for which there has been significant interest in the community. In contrast to these techniques, the tools we use do not rely on the application of cumbersome projection matrices to data but rely on simple hashing functions which can be directly applied to words. Random projections for text representation have been applied to text representation in (Lin and Gunopulos, 2003) and (DeBarr andWechsler, 2012). Up to our knowledge, the computationally efficient versions of RP described in (Ailon and Chazelle, 2010) still have not been explicitly studied in text domains. Formerly, Baena-Garcia et al. (2011) has proposed using the count min sketch to allow the efficient computation of IDF for massive streams of documents, studying the similarity between the ranking of the exact TF-IDF values and that of the approximate values obtained from approximate IDF. However, this algorithm works with exact TF and authors do not assess (theoretically or empirically) the effects of this approximation on document classification tasks. Approximating TF using with fewer dimensions is important in text categorization problems, because classification algorithms usually rely on the computation of metrics which scale linearly in the number of dimensions of the representation space. In this work, we study the approximate computation of both TF and IDF studying both theoretically and empirically the quality of the approximation. Recently, a generic approach for mining massive data using sketches has been suggested in (Gupta et al., 2013). However it relies on low rank matrix multiplications and does not focus on text representation or text categorization in online environments."}, {"heading": "1.1.2. Multi-Label Text Classification", "text": "Most classification methods studied in machine learning are devised to deal with single label assignments i.e. a data item belongs to one and only one class of the set of possible categories. Therefore, multi-label classification methods for problems arising in areas like text categorization, image annotation and protein function classification are of increasing research interest in the last years (Tsoumakas et al., 2010; Madjarov et al., 2012; Jiang et al., 2012; Yu et al., 2014; Monta\u00f1es et al., 2014). Multi-label classification is usually approached using either a problem transformation approach, where the problem is decomposed into several classic classification tasks, or by directly designing a method to predict multiple classes at once (Madjarov et al., 2012; Yu et al., 2014). In the first category, the Binary Relevance model is the most widely used approach in practice (Tsoumakas et al., 2010). Other popular methods in text categorization include the Binary Pairwise model, Boostexter (based on the Adaboost algorithm) and BRkNN (based on kNN and the BR framework) (Madjarov et al., 2012). Recently, several papers have shown that classification accuracy can improve by taking into account the possible correlations among labels (Dembczy\u0144ski et al., 2012a; Monta\u00f1es et al., 2014). One of the first methods addressing this issue is the label powerset method that considers each distinct combination of labels that exist in the dataset as a different class value. Clearly, the computational complexity of this method is worst case and therefore several efforts have been directed towards making it more efficient (Read et al., 2008; Tsoumakas et al., 2011). The classifier chain method (CC) is another ingenious way to model label dependencies (Read et al., 2011) but still enjoying the advantages of BR. This method selects an order on the label set and trains a binary classifier for each label in this chain. Dependencies are included by extending the feature space of each classifier to include the label associations of all previous classifiers. Variants of this method have been recently proposed in (Dembczy\u0144ski et al., 2012b), (Senge et al., 2013) and (Monta\u00f1es et al., 2014).\nUnfortunately, even if the problem of multi-label classification has seen considerable development in recent years, few authors have looked at this task in a data stream context (Read et al., 2012). Methods mentioned above suppose that both features and data are known beforehand. A straighTForward adaptation of them for online environments may thus require to periodically recompute a model to learn from new observations, an approach that require unbounded memory and time to store and process the full data stream. One possible solution at this level is exploiting problem transformation strategies and borrowing single label data stream classification techniques to obtain multi-labelled assignments (Read et al., 2011, 2012). However, this approach substantially limits the predictive performance and speed of the classification system due to the high imbalance the obtained subproblems which leads to overwhelm the class with more samples, the possible correlation between different labels and the large number of sub-models that needs to be updated in problem transformation schemes(Zhou et al., 2012). Up to our knowledge, the first method explicitly devised for multi-label classification of data streams has been recently proposed in (Read et al., 2012). This approach is based on the Very Fast Decision Tree (VFDT) introduced by Domingos and Hulten (2000) and extended several times in the literature (see e.g. Liang et al., 2012). A drawback of this method is that for numerical attributes VFDT\u2019s runtime is O(Zd) where d is the number of attributes and Z the set of possible splitting points. If Z is large (e.g. d), complexity becomes prohibitive, but if Z is small accuracy can significantly decrease.\nIn order to efficiently predict multi-label classifications, we propose to organize the feature space into a set of regions where documents with similar low dimensional representations collide by way of certain mapping process. Documents colliding in the same region are said to clash. Then, we implement a conditional naive bayesian approach. Labels are assumed to be independent given that the clashing region is known. This allows to model partial dependencies among labels and still keep the system efficient (Dembczy\u0144ski et al., 2012b). A simple way to implement a tessellation of the feature space is by adopting a prototype based method. In particular, we focus on centroids as they have been successfully used in the text categorization literature and are generating a renewed interest in the last years due to their computational efficiency (Tan et al., 2011; Pang and Jiang, 2013; Wang et al., 2013a; Borodin et al., 2013). Tan (2008) reports significant improvements on naive Bayes and KNN methods using adaptive centroid classifiers in text categorization tasks. An online extension of this method has been formerly presented in (Tan et al., 2011) but applied to classic train/test problems where the method slightly outperforms SVMs. Recently, a similar technique was presented in (Borodin et al., 2013) for text classification in data stream environments. Unfortunately it focuses on single label classification and documents are represented using batch TF-IDF."}, {"heading": "1.2. Novelty and Contributions", "text": "The main contribution of this paper is a simple and effective method for document classification where both components, text representation and classifier adaptation, are explicitly designed for large-scale multi-label data stream scenarios. The combined approach for representing and classifying text is referred to as clashing. This system efficiently extracts and keeps sufficient statistics for online text representation and multilabel classification, ensuring a truly bounded resources solution, where every operation is performed in constant time. As we explained in section 1.1.1, our setting and approach for text representation is novel and builds on recent ideas for mining massive data streams.\nNovelty is here in the fact that both TF and IDF approximation are made entirely online and guarantee constant time and space. As we explain in section 1.1.2, our approach to obtain multi-label annotations is also novel and attempts to exploit and keep the efficiency of our online embedding method. It can be regarded as a multi-label centroid based classifier (CC) where multi-label annotations are obtained by adding (conditional) naive bayesian models at each region where documents clash. Novelty is here in the way to extend CC to multi-label data stream tasks. Below we provide some highlights about this work:\n\u2022 We provide a document classification system where both text representation and multi-label annotation are online and guarantee constant processing time.\n\u2022 We show (theoretically and empirically) that the online text representation system approximates the TF-IDF space without requiring corpus wise computations.\n\u2022 We analyze the effects of the approximate representation on the classification system, taking exact TF-IDF as the baseline.\n\u2022 We show that the proposed method is better or comparable, in terms of macroand micro- averaged F measures, to a periodically recomputed SVM, but uses much fewer computational resources."}, {"heading": "1.3. Practical Implications of this Research", "text": "The problem of classifying documents using finite resources is an important challenge in the domain of mining textual streams. Fast flowing streams of text are generated by online news, social media and endless other applications, and the need to automatically and adaptively sort them into sub-streams is a crucial one. The insistence on only making use of bounded resources is a consequence of the size of the streams: both time and memory need to be kept under control. We focused on this problem as part of our ongoing efforts in the analysis of web news, and the algorithm we have developed will be incorporated into our pipeline devoted to Computational Social Sciences (Flaounas et al., 2011)."}, {"heading": "1.4. Organization of this paper", "text": "The rest of this article is organized as follows. In section 2, we briefly introduce the problem of document classification and the data stream scenario. In this section we also present the hashing technique that will be used to obtain text representation in online domains. The clashing approach is described in section 3 with two variants to learn a partitioning of that space based on the available labels. In section 4 we present a theoretical analysis of the representation obtained by hashing compared to the geometry of the full word count space. Additional related work, not discussed in this introduction, is presented in section 5. In section 6, we present several experiments performed on the Reuters RCV1 corpus and additional scalability tests carried out in the New York Times dataset. The main conclusions and contributions of this paper are summarized in section 7. The appendix at the end of this manuscript provides the proofs of all the theoretical claims presented in section 5."}, {"heading": "2. PROBLEM STATEMENT AND BACKGROUND", "text": "We denote by D the set of all possible documents and use x(d) \u2208 X to denote a vector representation d \u2208 D obtained by using a text representation model x : D \u2192 X . We use x as a shorthand of x(d), and xi as a shorthand of x(di), when this is clear from the context. A textual dataset or corpus is a collection of documents D containing words w1, w2, . . . from a set W."}, {"heading": "2.1. Document Classification", "text": "Given a finite and non-empty set of nt labels T = {\u03c41, \u03c42, . . . , \u03c4nt}, the task of text classification is that of extracting from a dataset D a decision mechanism f : D \u2192 2T that describes how documents ought be classified (Sebastiani, 2002), that is, how to annotate documents with labels which are relevant for them. Text classification can be seen as a kind of information retrieval task for each instance, where the labels play the role of the documents to retrieve (Quevedo et al., 2012). In contrast to single-label pattern classification tasks, text classification is a multi-label problem, that is, classes are not mutually exclusive: a document can be simultaneously assigned with several labels. In this paper, we assume that each document di \u2208 D has been annotated with class labels Ti \u2282 T , that is, we adopt a supervised approach (Joachims, 2002), building f both from D and the known annotations T1, T2, . . ..\nDifferent methods for text classification have been explored in the last years, including linear models, nearest neighbor approaches, support vector machines (SVMs), neural networks, classifier ensembles and various generative methods (Sebastiani, 2002). Traditionally, previous work has focused on improving classification performance using classic train/test settings, which allows to employ sophisticated feature selection approaches to optimize text representation and data intensive training algorithms to obtain an accurate classifier. For a comprehensible discussion of these methods please refer to (Sebastiani, 2002) and the recent survey presented in (Aggarwal and Zhai, 2012). Below we introduce the data stream setting for extracting a model from data."}, {"heading": "2.2. The Data Stream Setting", "text": "In this paper, we address text categorization under a data stream setting, i.e, we assume that documents arrive as a continuous and virtually infinite sequence of observations d1, d2, . . . , dt, . . ., , dt \u2208 D. In contrast to the traditional setting for learning a text classifier, the set of documents and the set of words (features) contained in those documents are both unknown beforehand, i.e., D andW are being continuously updated. In agreement with recent papers on the topic (see e.g. Read et al., 2012; Mena Torres and Aguilar Ruiz, 2014) we recognize original requirements of systems devised to efficiently operate in this setting: (1) process an instance at a time and inspect it at most once; (2) be ready to predict at any point; (3) data may be evolving over time; and (4) expect an infinite stream, but process it under finite resources (time and memory).\nAn appropriate framework for learning from data under this setting is online learning (Pavlidis et al., 2011; Wang et al., 2012). Online learning takes place in a sequence of consecutive rounds t = 1, 2, . . .. At each round t, the system is asked to predict the set of tags T\u0302t \u2282 T corresponding to a new observation dt \u2208 D. To this end, the system stores a prediction mechanism ft : D \u2192 2T . Only after providing an output T\u0302t = ft(dt), and if available, the system is feed with the set of correct labels Tt. Using this information,\nthe learner can estimate its current performance and may activate a learning mechanism to get an improved decision mechanism ft+1. In contrast to batch settings, in which the system knows all the correct labels in advance, an online system is being tested continuously as long as correct labels are provided. Each cycle corresponds to a testing and training step. Thus, online classification systems can be monitored by updating the so called cumulative loss, LT = \u2211T t=1 `(Tt, T\u0302t), where ` : T \u00d7 T \u2192 R is a loss function measuring the cost of a difference between T\u0302t and Tt. In section 5, we discuss some related work on the task of classifying data streams. Two important issues in adapting these techniques for document classification are the requirements of multi-label annotations and the large dimensionality arising from the traditional way to represent text."}, {"heading": "2.3. Text Representation", "text": "Text representation is the task of transforming text documents into elements of a formally defined space X amenable for pattern analysis tasks. Because of its simplicity and effectiveness, the vector space model (VSM) is quite the most used text representation approach in document retrieval and text mining applications (Joachims, 2002; Zhang et al., 2011). In this model, text is represented as a vector x = (x1, . . . , xd) whose components correspond to the weights of a set of linguistic units V used to capture the content of a document. Most frequently, V is constructed by extracting the words contained in the documents and the weights are determined by using count statistics ignoring both sequence and position in the text. For instance, given a document d and word wi observed Fi(d) times in the document, the TF representation of a text document can be obtained by setting xi = TF(wi, d), where,\nTF(wi, d) = Fi(d)/F (d) , (1)\nand F (d) is a suitable normalization factor such as the document length in words. Usually, the word extraction process involves some pre-processing steps carried out to increase the quality of the representation. Typical pre-processing steps include (1) lower-casing, (2) removal of punctuation, (3) stop-word removal and (4) stemming (for details see e.g. Joachims, 2002). More sophisticated feature selection and dimensionality reduction methods can also be used to increase the quality of the vocabulary (Forman, 2003).\nIn addition to individual words, a number of authors have investigated the use of longer indexing units, linguistically or statistically meaningful for identification purposes, as they may have a smaller degree of ambiguity and become closer to expressing structured concepts (Zhang et al., 2011). Similarly, a number of more sophisticated weighting schemes have been investigated to enhance text classification (Lan et al., 2009). TF-IDF is probably the popular method (Joachims, 2002; Sebastiani, 2002). The basic idea is that a term which occurs in many documents is not a good discriminator and should be given less weight than one which occurs in few documents. This idea can be quantified using the so called inverse document frequency (IDF), which is commonly computed as\nIDF(wi) = log (n/ni) , (2)\nwhere ni is the number of documents containing the term wi. Combining IDF with the TF weight of Eqn.(1), leads to the TF-IDF weighting scheme\nTF-IDF(wi, d) = TF(wi, d)\u00d7 IDF(wi) . (3)"}, {"heading": "2.4. The Hashing Trick", "text": "The hashing trick is a method for speeding up the inner product computations required by some learning methods and improving their storage requirements. This was first presented by Shi et al. (2009b) and then extended in (Shi et al., 2009a) and (Attenberg et al., 2009). Here we briefly present the essential definitions and results that we require for the analysis of our technique.\nSuppose data has been represented in a d-dimensional feature space X indexed by [d] = {1, 2, . . . , d} and let h : [d] \u2192 M := {1, 2, . . . ,m} with M d be a hash function drawn from a family of pairwise independent hash functions. Then, for a given vector x \u2208 X , the hashing trick maps the original representation x to a new representation \u03c6(x) in the low dimensional space indexed by M. The coordinates of \u03c6(x) are defined as follows\n\u03c6(x)i := \u2211\nj:h(j)=i\n\u03be(j)xj , (4)\nwhere \u03be : N\u2192 {\u00b11} is an auxiliary Rademacher hash function (Shi et al., 2009a). This \u201ccompressed\u201d representation can then be used to approximate \u3008x1,x2\u3009 by computing\n\u3008x1,x2\u3009\u03c6 := \u3008\u03c6(x1),\u03c6(x2)\u3009 . (5)\nApproximating \u3008x1,x2\u3009 by (8) reduces the time required to compute a single inner product and the space required to store a single training instance from O(d) to O(m). As demonstrated by Shi et al. (2009a), \u3008x1,x2\u3009\u03c6 is an unbiased estimator of \u3008x1,x2\u3009, i.e., E\u03c6 (\u3008x1,x2\u3009\u03c6) = \u3008x1,x2\u3009. Furthermore, exponential tail bounds for the approximate preservation of norms and inner products were recently proved by Dasgupta et al. (2010). We state the main result below,\nTheorem 1. For any \u2208 (0, 1), any \u03b4 < 0.1 and any x satisfying the sparseness condition \u03b72 = \u2016x\u20162\u221e \u2264 log(1/\u03b4)\u22121 log(m/\u03b4)\u22122/16 , (6) one has with probability at least 1\u2212 3\u03b4 the following property\n(1\u2212 )\u2016x\u201622 \u2264 \u2016x\u20162\u03c6 \u2264 (1 + )\u2016x\u201622 , (7)\nprovided m \u2265 12 \u22122 log(1/\u03b4). Rephrasing, under mild conditions on the original representation space, the probability of a distortion in the inner-products \u3008x1,x2\u3009\u03c6 with respect to the original inner product \u3008x1,x2\u3009 can be bounded as\n\u03b4 = Pr ( \u3008x1,x2\u3009\u03b3 \u2212 \u3008x1,x2\u3009 \u3008x1,x2\u3009 \u2265 ) \u2264 6 exp ( \u2212m 2 12 ) , (8)\nthat is, the probability of a large deviation of \u3008x1,x2\u3009\u03b3 from \u3008x1,x2\u3009 decays exponentially fast with the dimensionality m of the embedding (4).\nA similar result holds for the preservation of norms. A technique to obtain property (7) without explicitly requiring condition (6) was studied in (Weinberger et al., 2009). Essentially, this method pre-condition data by using a densification matrix P. This increases the computational cost of computing the embedding by a factor of c = O(1/ )O(log(1/\u03b4)), but leads to a result valid for any type of data."}, {"heading": "2.5. The Count-Min (CM) Sketch", "text": "The hashing trick presented above has its roots in the so-called count-min sketch proposed by Cormode and Muthukrishnan (2005); a probabilistic data structure designed to efficiently track the observed frequency distribution f(\u00b7) of a stream of elements D from a domain N . We will use this technique to obtain an online approximation of the IDF weighting scheme.\nLet f(j) denote the number of elements in D having a value j \u2208 N . A count-min sketch to approximate f(j) within a precision and a confidence \u03b4, is based a twodimensional array C of L\u00d7m counters. For each row `, a hash function h`(\u00b7) maps the input domain N uniformly into the rangeM = {1, 2, . . . ,m}. If, at a given time, element j is observed mj times, each row of the sketch is updated as follows\nC`,h`(j) = f(h`(j)) +mj , (9)\nthat is, the counts for the element j are stored in the position h`(j) of each row `. Of course, position h`(j) may also store the counts for other elements k such that h`(k) = h`(j). If, at a given time, an estimation of f(j) is required, the sketch outputs\nf\u0302(j) = min ` (C`,h`(j)) . (10)\nCormode and Muthukrishnan (2005) proved the following theorem about the the estimate f\u0302(j).\nTheorem 2. Given parameters > 0, \u03b4 \u2208 (0, 1), set m = dexp(1)/ e and L = dln (1/\u03b4)e. Thus, the estimate f\u0302(j) in Eqn.(10) satisfies: f(j) \u2264 f\u0302(j), and with probability at least 1\u2212 \u03b4, f\u0302(j) \u2264 f(j) + \u2016f\u20161 , (11) where \u2016f\u20161 = \u2211 j f(j)."}, {"heading": "3. THE CLASHING SYSTEM", "text": "A clashing system is composed of the following main components: (1) a document wise hashing function \u03c6 : D \u2192 Rm, used to get a low-dimensional vector representation of a document from the set of terms (words) it contains; (2) a partition R = {R1, . . . ,Rnp} of the low-dimensional feature space \u03c6(D); (3) a map function, used to automatically accommodate a document in a region of the partition; (4) a prediction rule used to output a set of labels according to the region; (5) a learning rule, used to update the prediction rule from labelled instances. Algorithm 1 depicts the general operation of the system.\nIn a nutshell, the principle underlying clashing is that similar documents will collide in the same region with high probability and thus classification can be performed by storing a set of simple statistics about the label distribution. In the next paragraphs we present each component in detail.\nAlgorithm 1: Overview of the Clashing System. Input: A stream of documents d1, d2, . . . with tag sets T1, T2, . . ..\n1 Initialize each component of the system. 2 for d1, d2, . . . do 3 Hash the document: \u03c6(dt)\u2190 doc-hash(dt). 4 Map to a region: j\u2217 \u2190 map (\u03c6(dt)). 5 Predict labels: T\u2217 \u2190 predict (j\u2217). 6 Update the prediction mechanism: If Ti 6= \u2205, learn (dt, T\u2217, Ti)."}, {"heading": "3.1. Document Hashing", "text": "We have shown in the previous section that any vector in a metric space X can be quickly embedded in a low-dimensional space \u03c6(D) such that the norms and inner products are approximately preserved. Thus, if x(d) represents the TF representation of d, the hashed representation \u03c6(x) in Eqn.(4) approximately preserves the geometry of the word count space. A direct application of this \u201chashing trick\u201d in a batch setting would thus proceed as follows:\n1. Create a vocabulary V = {w1, w2, . . .}, containing the words in the corpus, and assign an arbitrary index j \u2208 N to each of them. 2. Build the TF representation x(d) for each document d using V and Eqn. (1). 3. Use the hashing map of Eqn.(4) with d = |V| to reduce dimensionality. However, in data stream scenarios the vocabulary V is unknown in advance and is indeed continuously growing. An important advantage of the hashing method is that \u03c6(x) can be computed by simply scanning the words in d one by one, without explicitly building the vocabulary. This procedure is presented as Algorithm 2.\nAlgorithm 2: doc-Hash(d) for preserving the TF geometry. Input: A document d with words w1, w2, . . . Output: \u03c6(x) where x is the TF representation of d. Initialization: No required.\n1 \u03c6(d)\u2190 0 2 for Each word wi in the document do 3 k \u2190 h(wi). 4 \u03c6(d)k \u2190 \u03c6(d)k + \u03be(wi). 5 Output \u03c6(d).\nNote that here, the hashing functions h :W \u2192M := {1, 2, . . . ,m} and \u03be :W \u2192 {\u00b11} operate directly on strings. It is easy to see that after the loop in Algorithm 2, to process all the words w1, w2, . . . in a document d, the hashed representation \u03c6(d) has components \u03c6(d)i = \u2211 w:h(w)=i \u03be(w)x(w), where x(w) is the number of occurrences of the word w in the document, i.e, the j-th coordinate in the TF representation of d, assuming that j is the index assigned to the word w in the vocabulary. Since the indexing in V is arbitrary, applying Algorithm 2 to each document in the corpus is equivalent to performing the batch steps.\nAs we have previously discussed, a term weighting scheme \u03c9 : V \u2192 R+0 is usually performed on the TF representation in order to enhance classification performance. In particular, IDF weights have demonstrated to be highly effective in practice (Sebastiani, 2002; Joachims, 2002). Here we provide an efficient online method to preserve the geometry of the word count space corresponding to the TF-IDF representation. In batch scenarios we may explicitly scale the TF representation of documents, using an IDF dictionary computed in an offline fashion from the corpus, and then applying the hashed feature map of Eqn.(4) to the obtained feature vector in order to reduce dimensionality. However, in online scenarios, this approach forces a periodic recomputation of the IDF weights for a probably large vocabulary V. Algorithm (3) provides a method to perform this computation online.\nAlgorithm 3: doc-Hash(d) for preserving the TF-IDF geometry. Input: A document d. Output: A low dimensional approximation to the TF-IDF representation of d (counters\nC \u2208 Rm, n \u2208 R are updated and stored for the next round). Initialization: n\u2190 0, C \u2190 0\n1 \u03c6\u0302(d)\u2190 0, C\u0304 \u2190 0. 2 for Each word wi in the document do 3 k \u2190 h(wi). 4 \u03c6\u0302(d)k \u2190 \u03c6\u0302(d)k + \u03be(wi). 5 If C\u0304k = 0, then C\u0304k \u2190 1. 6 C \u2190 C + C\u0304. 7 n\u2190 n+ 1. 8 for Each component k in \u03c6\u0304(d)k do 9 IDF(k)\u2190 log(n/Ck)\n10 \u03c6\u0302(d)k \u2190 IDF(k)\u03c6\u0302(d)k. 11 Output \u03c6(d) := \u03c6\u0302(d).\nEssentially, Algorithm 3 differs from Algorithm 2 in the scaling performed in step 10. This scaling is aimed to approximate the IDF weighting scheme and it is performed by storing a hashed representation of the vector f containing the true document frequencies of words among the n documents observed up to a given round. This approximation is represented by the array C. Indeed, is not hard to see that after observing n documents, C has components Ck = \u2211 j:h(i)=k f(i), where f(i) is the document frequency of the word wi among the n documents, that is, the number of documents containing at least once the term. Thus, array C in Alg.3 is a direct implementation, with L = 1, of the method presented by Cormode and Muthukrishnan (2005) to approximate f(i) and thus enjoys the properties given in Theorem 2. For a given wi, the exact IDF computed on set of documents observed by the system would proceed by computing IDF(wi)\u2190 log(n/f(i)). Our method, instead, computes IDF(k) \u2190 log(n/Ck) for any word colliding in the kth coordinate of C. In the next section, we use the bound of Eqn.(11) to bound the probability that log(n/Ck) be different than the true IDF weight log(n/f(i)) of any word wi such that h((wi) = k.\nAlgorithm 4: (The Mapper) map(d). Input: A document d represented as \u03c6(d). Output: The index j\u2217 of the region for clashing d. Initialization: No required.\n1 Find the Nearest Prototype: j\u2217 = arg maxi s(\u03c6(d),pi). 2 Output j\u2217."}, {"heading": "3.2. The Mapper", "text": "The classification model underlying the clashing system can be represented by a partition R = {R1, . . . ,Rnp} of the low-dimensional space \u03c6(D) in which documents are embed. A function called Mapper is aimed to actively make similar documents to collide in the same regions. A simple way to achieve this goal is by using a set of prototypes p1,p2, . . . ,pnp \u2208 \u03c6(D) and a Voronoi rule of the form\nRi := {\u03c6(d) : i = arg minj \u2016\u03c6(d)\u2212 pj\u20162} . (12)\nBy Eqn.(12), the documents assigned to the same regions have the same nearest prototype. If denote by 2\u03b7 the maximum distance from a point in \u03c6(D) to its corresponding prototype (the extreme points of Ri), we have that two documents d1, d2 assigned to the same region are similar by at least s(\u03c6(d1),\u03c6(d2)) \u2265 \u03b7. Since \u03c6(d) preserves similarity with a high probability, d1, d2 are likely to be close in the TF or TF-IDF space. The Mapper is then be implemented as in Alg. 4."}, {"heading": "3.3. The Prediction Rule", "text": "Each region Ri is associated with a probability distribution on T , denoted Fi = {Fi,1, . . . , Fi,nt}, which models the probability of observing a given tag \u03c4j in a document assigned to that region, that is, Fi,j = Pr(\u03c4j |Ri) or better, Fi,j = Pr(\u03c4j \u2208 T (d)|\u03c6(d) \u2208 Ri), where T (d) denotes the set of tags for a document d. Formally, the classification hypothesis f : D \u2192 2T is implemented as\nf(d) = {\u03c4i \u2208 T : F\u0302i,j\u2217 > \u03b8} , (13)\nwhere F\u0302i,j is an estimator of Fi,j and \u03b8 is a parameter that may be used to control the precision/recall tradeoff of the system. That is, a label is predicted by the system if and only if a fraction at least \u03b8 of the documents colliding in the region j\u2217 contains that tag. We adopt \u03b8 = 0.5 in this paper."}, {"heading": "3.4. The Learning Engine", "text": "The learning engine is the component of the system determining the way to construct a suitable partition and estimate the label distributions from data. Considering the decisions made before, the first question translates into how to construct the prototypes from a set of examples D = {(d1, T1), . . . (dn, Tn)}. As to keep the model simple and efficient, we consider linear transformations of subsets of training data \u03c6(D) = {\u03c6(d1), . . . ,\u03c6(dn)}, i.e., if \u03c6D denotes the matrix with the examples arranged in the columns, pi := \u03c6D\u03b1, where \u03b1 is a set of parameters. In addition, we restrict our analysis to recursive update rules of the form\npt+1i = (1\u2212 \u03bbti)pti + \u03bbti\u03c6(dt) , (14)\nwhere dt is the t-th document observed by the system. Parameter \u03bbti \u2208 [0, 1] allows us to track the set of examples colliding together in region Ri. Different schemes to automatically set \u03bbti \u2208 [0, 1] from empirical data can be devised. In this paper, we restrict the analysis to the following simple approaches:\n\u2022 Mode 1. We associate one prototype pi to each class label \u03c4i and compute pi as the centroid of all the documents containing this tag. That is, at a given round t, \u03bbti = 0 if and only if Tt contains \u03c4i. Otherwise \u03bbti = 1/(nti + 1), where ni is the number of documents containing \u03c4i up to round t. Algorithm 5 summarizes this procedure.\n\u2022 Mode 2. We associate one prototype pi to each class label \u03c4i but compute pi only from the examples leading to a classification mistake. This is essentially the principle behind the perceptron rule. Here we devise a multi-label version. Given a training document dt with labels Tt, the system is trained as follows. If a label \u03c4i \u2208 Tt is not predicted by the system (false negative), we feed the example to pi. In this case, \u03bbti = 1/(nti + 1). This searches to increase the recall of the system. If one label \u03c4i \u2208 Tt is incorrectly assigned to dt, we apply the rule explained in the item 1. Algorithm 6 summarizes this learning rule.\nTo estimate the probabilities Fi, we adopt a frequentist approach, computing\nF\u0302i,j := |\u03c6(D)i \u2229 \u03c6(D)j | |\u03c6(D)i| = ni,j ni , (15)\nwhere ni,j is the number of training documents containing tags \u03c4i and \u03c4j . This criterion can be implemented in online learning using the following simple recursion\nF\u0302 t+1i,j = { (1\u2212 \u03bbti)F\u0302 ti,j + \u03bbti if \u03c4i \u2208 Tt and \u03c4j \u2208 Tt (1\u2212 \u03bbti)F\u0302 ti,j otherwise\n(16)\nNote that setting \u03bbti = 1/nti easily leads to Eqn.(15). Definition of Eqn. (16) may be used to set more general schemes to adapt F\u0302i,j from labelled examples. Algorithms 5 and 6 incorporate the estimation of the local statistics.\nAlgorithm 5: The Learning Engine - Mode 1. Input: \u03c6(dt), predicted tags T\u2217, and correct tags Tt. Output: Updated partition R and local statistics Fi. Initialization: Create one prototype for each class, setting pi \u2190 0, \u2200i, F\u0302i,j \u2190 0, \u2200i, j\n1 for Each label \u03c4i contained in Tt do 2 Set ni \u2190 ni + 1 and \u03bbi \u2190 1/ni 3 Update pi \u2190 (1\u2212 \u03bbi)pi + \u03bbi\u03c6(dt) 4 for Each F\u0302i,j in F\u0302i do 5 If \u03c4j \u2208 Tt, F\u0302i,j \u2190 (1\u2212 \u03bbi)F\u0302i,j + \u03bbi // Local Statistics 6 Else F\u0302i,j \u2190 (1\u2212 \u03bbi)F\u0302i,j\nAlgorithm 6: The Learning Engine - Mode 2. Input: \u03c6(dt), predicted tags T\u2217, and correct tags Tt. Output: Updated partition R and local statistics Fi. Initialization: Create one prototype for each class, setting pi \u2190 0, \u2200i, F\u0302i,j \u2190 0, \u2200i, j\n1 Fn = Tt \u2212 T\u2217 // False Negatives Labels 2 Fp = T\u2217 \u2212 Tt // False Positive Labels 3 for Each label \u03c4i contained in Fn do 4 Set ni \u2190 ni + 1 and \u03bbi \u2190 1/ni 5 Update pi \u2190 (1\u2212 \u03bbti)pi + \u03bbti\u03c6(dt) 6 for Each F\u0302i,j in F\u0302i do 7 If \u03c4j \u2208 Tt, F\u0302i,j \u2190 (1\u2212 \u03bbi)F\u0302i,j + \u03bbi // Local Statistics 8 Else F\u0302i,j \u2190 (1\u2212 \u03bbi)F\u0302i,j\n9 If Fp 6= \u2205, call Learning Engine - Mode 1."}, {"heading": "3.5. Complexities", "text": "The system needs to store a fixed set of np prototypes. In this paper, np = nt, that is, we have one prototype for each possible label. Classification and training require a search among the prototypes, using a similarity function linear in the compressed space dimensionality. Thanks to the hashing based embedding, the space and running time complexities are independent of the potential number of attributes (words in text). Space complexity amounts to O(npm)+O(npnt) because we store np m-dimensional prototypes and nt counters for each region. Test complexity amounts to O(npm) due to the nearest prototype search. Update time is O(|Tt|(npm + nt)) where |Tt| is the number of labels contained in the document dt. Note that both storage and running time are independent of the number of observed documents. Parallelization of prototypes storage and search (in applications with a huge number of labels) is straightforward."}, {"heading": "3.6. Beyond TF-IDF", "text": "Considering the increasing interest on weighting methods capable to improve the accuracy of text classifiers in the last years (Lertnattee and Theeramunkong, 2004; Lan et al., 2009; Guan et al., 2009; Altincay and Erenel, 2010; Luo et al., 2011; Erenel and Altincay, 2012; Ren and Sohrab, 2013; Wang and Zhang, 2013), it may be relevant to ask if the online method provided here to approximate TF-IDF can be extended to approximate other weighting schemas. For instance, several papers have recently investigated methods capable to exploit information about the different distribution of terms among documents of each class. Other works have studied variants of TF-IDF based on feature selection scores (filter methods) originally devised to reduce dimensionality (Azam and Yao, 2012; Sebastiani, 2002; Lan et al., 2009; Altincay and Erenel, 2010).\nOur online embedding procedure is based on two probabilistic methods: one for approximating TF and one for approximating IDF. These two components interact as depicted in Algorithm 3. The guarantees for the probably approximately correct approximation of IDF are based on the properties of the count-min sketch presented in section 2.5. Using this data structure, we can approximate the document frequency \u2211 t I(ti \u2208 dt) (DF) of a term ti in the stream with high accuracy. However, the transformation applied on a DF estimate to obtain an IDF estimate has an impact on the final accuracy of the\napproximation. Similarly, our guarantees for the approximately correct computation of TF are based on the properties of the hashing map studied in section 2.4. Functions of TF computed on our estimator can be more or less accurate depending on the specific transformation involved. If the weighting method is based on a \u201csimple\u201d transformation of DF, IDF and/or TF, as defined in our manuscript, an extension of our approach is likely to be painless. However, if the transformation applied by the term weighting method is more \u201ccomplex\u201d, for instance it is not continuous, it is highly non linear and/or dependent on several other counts that we are not currently computing/estimating, a more acute analysis is required. Specifically we would need to check if Lemma 1 and Proposition 1 can be adapted for the specific weighting scheme at hand.\nIn order to provide a more specific answer to the question, we can focus on some specific approaches. Consider for instance the class-based weighting method proposed by Ren and Sohrab (2013), which consistently outperformed other term weighting approaches using SVM and centroid-based classifiers. This method works by incorporating in TF-IDF a novel component which depends on the class conditional document frequency\u2211 t I(ti \u2208 dt\u2227dt \u2208 Ck) (let us say CDF) of a term ti in the corpus. The sketch we use to approximate DF can be easily adapted to approximate these class conditional document frequencies from a data stream. It would be enough to store K sketches C1, C2, . . . , CK as those used by Algorithm 3, one for each category in the problem, and update Cj from the flow of terms coming from documents containing label j. Therefore, if we can accept an slight additional cost in terms of storage and computation, our method may be adapted to approximate CDF with about the same accuracy as we currently approximate DF. The precision obtained for the final weighting scheme depends on the function mapping CDF to weights.\nThe first scheme proposed in (Ren and Sohrab, 2013) weights the standard TF-IDF using a component defined in terms of the average of the class-conditional document frequencies,\nw(ti) = 1 + log\n( C\nCS\u03b4(ti)\n) , CS\u03b4(ti) = \u2211 ck nck(ti) Nck ,\nwhere nck(ti) denotes the number of documents that include the term ti and are a member of the category ck, and Nck denotes the total number of documents in a certain category ck. Since ck coincides with our definition of CDF, the sum in CS\u03b4(ti) is a convex function, and the transformation \u03c9(\u00b7) = 1 + log(C/\u00b7) employed in this scheme is essentially the same that the transformation applied on DF to compute IDF, it is easy to adapt Lemma 1 and Proposition 1 in order to provide guarantees for the new weighting schema.\nThe second approach proposed in Ren and Sohrab (2013) is based on Inverse Category Frequency, an idea explored also in (Wang and Zhang, 2013; Lertnattee and Theeramunkong, 2006) and (Lertnattee and Theeramunkong, 2004). It scales TF-IDF by the additional factor\n\u03c9(ti) = 1 + log\n( C\nc(ti)\n) ,\nwhere C is the number of classes and c(ti) is the number of categories in which the term ti occurs at least once. It is clear that C can be computed exactly. In order to approximate c(ti) for each possible ti, a straightforward method is to rely on theK CountMin Sketches C1, C2, . . . , CK we have described above. Given the hashing function h(\u00b7)\nand a term ti such that h(ti) = j, c(ti) may be approximated as c\u0304(ti) = \u2211 j I(C k j ), where I(x) is 1 if x > 0 and 0 otherwise. Since Ckj approximates nck(ti), c\u0304(ti) should approximate c(ti) reasonably well. However, in order to preserve the guarantees provided by Lemma 1 and Proposition 1, we would need to examine the effect of the discontinuity in the function I(x). Clearly, we should also focus on designing a more efficient sketch to directly approximate c(ti) without storing K different data structures C1, C2, . . . , CK .\nAnother approach for more accurate term weighting in text categorization is based on feature selection metrics such as chi-squared, Gini, information gain, odds ratio, etc. (Azam and Yao, 2012; Lan et al., 2009; Altincay and Erenel, 2010). Given a term ti these metrics can be defined using the following counts\n1. Ak[ti]: number of documents that contain ti and belong to class k. 2. Bk[ti]: number of documents that do not contain ti and belong to class k. 3. Ck[ti]: number of documents that contain ti and do not belong to class k. 4. Dk[ti]: number of documents that do not contain ti and do not belong to class k.\nClearly, Ak[ti] coincides with our definition of CDF for class j and term ti. In addition, Bj [ti] = Nck \u2212 Ak[ti]. Therefore, it is not hard to see from our previous discussion that we can adapt our method to approximate Ak[ti] and Bk[ti] with high accuracy at a slight increase in computational cost. On the other hand, Ck[ti] = \u2211 t I(ti \u2208 dt \u2227 dt \u2208 Ck) \u2212 Ak[ti]. Both terms in the subtraction can be accurately approximated. Finally, Dk[ti] = N \u2212Ak[ti]\u2212Bk[ti]\u2212Ck[ti]. Thus, we can approximate all these counters using the K class-based sketches described above. This is a good start for an extension of our method to weighting schemes using feature selection metrics. However, metrics can depend on Ak[ti], Bk[ti], Ck[ti], Dk[ti] in very complex way (see e.g. Forman, 2003; Lan et al., 2009). Therefore, we would need to proceed case by case, considering the relevant metrics and studying a way to produce accurate final estimations."}, {"heading": "4. THEORETICAL ANALYSIS", "text": "We present an analysis of the online embedding procedure used by the clasher aimed to theoretically characterize the effect of the dimensionality reduction on the classification performance. We compare the predictive performance of the model in the hash space \u03c6(D) to the performance of the equivalent model in the word count space X . If X denotes the TF representation of the document space, we know from the discussion in the previous section that \u03c6(X ) approximately preserves the geometry (norms and inner products) of the space X . This is a direct consequence of Theorem 1. However if X denotes the TF-IDF representation of the document space, Theorem 1 does not suffice because we need to account for the effect of the approximate IDF weighting used in Alg. 3."}, {"heading": "4.1. Preservation of the TF-IDF Geometry", "text": "Our first result is that approximate IDF weights computed in Alg. 3 enjoy essentially the same guarantees that Theorem 2 provides for the sketch C. i.e., the operation log(n/\u00b7) does not essentially change the theoretical bounds on the difference between the true and the estimated quantities. The proof of this lemma is provided in the appendix.\nLemma 1. At a given round t, let IDF(wi) be the exact IDF for a word wi computed from the documents observed so far, and h(wi) = k. Thus, with constant probability, the IDF estimate IDF(wi) = log(n/Ck) computed by Alg. 3 satisfies IDF(wi) \u2264 IDF(wi) and IDF(wi) \u2265 IDF(wi) \u2212 \u2016f\u2016, for any > 0, provided m \u2265 dexp(1)/ e. Here, \u2016f\u2016 is defined as in Theorem 2, i.e., it corresponds to the sum of all the document frequencies.\nThe next proposition puts together the guarantees about the approximate IDF weights and the guarantees for the hashed TF representation, in order to show that Alg. 3 computes a mapping that approximately preserves the exact TF-IDF geometry for any desired precision \u0304 provided the dimensionality of the hash function is large enough. The proof of this proposition is provided in the appendix.\nProposition 1. Under the conditions of Theorem 1, the representation \u03c6(dt) computed by Alg. 3 at round t approximates the exact TF-IDF representation xt of dt, i.e., we have with probability at least (1\u2212 e\u22121)(1\u2212 3\u03b4) that the following property holds\n(1\u2212 \u0304)\u2016xt\u201622 \u2264 \u2016\u03c6(x)\u20162 \u2264 (1 + \u03042)\u2016xt\u201622 , (17)\nprovided m \u2265 max ( 588\u2016f\u20164\u0304\u22122 log(1/\u03b4), d7\u2016f\u20162 exp(1)/\u0304e ) , for any \u0304 \u2208 (0, 1) and any \u03b4 < 0.1."}, {"heading": "4.2. Effect on the Classification Performance", "text": "We analyze the cumulated performance of the system at a given instant t compared to the performance of an equivalent system operating directly in the word-count space. Thanks to the Theorem 1, the word count space can correspond to either the TF representation or the TF-IDF representation, with slightly different conditions on the dimensionality required to obtain the guarantees. For simplicity we analyze the case the TF representation using Theorem 1. For the TF-IDF the procedure is analogous using Theorem 1. For brevity we also assume that the clashing system is operating with the learning rule of Alg. 5.\nThe following result shows that the chance of achieving the same performance depends on the following notion of margin\n\u03b7(x) = minj 6=i\u2217 \u2016p\u0304j \u2212 x\u20162 \u2212 \u2016p\u0304i\u2217 \u2212 x\u20162 (18)\nLemma 2. Suppose that x is classified with margin at least \u03b7 in the original data space. For any \u03b4\u0304 < 0.1, set \u03b7\u0304 \u2264 \u03b7/maxj \u2016p\u0304j \u2212 x\u20162, m \u2265 48\u03b7\u0304\u22122 log(3nt/\u03b4\u0304) and c = 32\u03b7\u0304\u22121 log(3nt/\u03b4\u0304) log 2(3mnt/\u03b4\u0304). Thus, if x satisfies \u2016x\u2016\u221e \u2264 1/ \u221a c, the probability that the hash embedding preserves the original decision is at least 1\u2212 \u03b4\u0304.\nThe proof of the latter lemma and the next proposition are provided in the appendix.\nProposition 2. Let Dt = {d1, d2, . . . dnt} be a test set such that x(Dt) = {xt1, . . . ,xtnt} is contained in a ball of diameter D \u2208 R. Let Pr(Err) be the test error incurred by the clasher in the hash space and Pr(Err) the test error in the original data space. For any \u03b7\u2217 > 0 and \u03b4\u0304 < 0.1, set \u03b7\u0304 = \u03b7\u2217/D2 and keep the parameters as in Lemma 2. Therefore, if \u2200k \u2208 [nt], \u2016xtk\u2016\u221e \u2264 1/ \u221a c, we have\nPr(Err) \u2264 Pr(Err) + (1\u2212 \u03b4\u03b7\u2217)\u03b4\u0304 + \u03b4\u03b7\u2217 , (19)\nwhere \u03b4\u03b7\u2217 is the fraction of test instances classified with margin \u03b7(x) < \u03b7\u2217. Indeed, if all the data \u03c6(D) = {x1, . . . ,xnt} is correctly classified with margin \u03b7\u2217 > 0 in the original space,\nPr(Err) \u2264 \u03b4\u0304 \u2264 3nt exp ( \u2212m\u03b7\u2217 2\n48D4\n) . (20)"}, {"heading": "5. ADDITIONAL RELATED WORK", "text": "In this paper, we have been interested in constructing a very efficient system to classify documents coming from a large and high-speed data stream using bounded resources for every training and testing round. Therefore, this work intersects several issues in the field of data mining: data streams, text representation and multi-label classification. As we have previously discussed in section 1, few works approach text representation and multi-label classification in data stream settings. On the other hand, our solution to the problem relates to recent advances in data stream sketching, hashing and centroid based text classification. Here, we combine these ideas in a novel and original way to solve the problem at hand. In this section, we comment some additional related work to the proposal.\nData Stream Methods. Efficient data stream classification is currently approached in one of the following two ways. One is enabling a traditional machine learning technique with an online or incremental learning rule which allows the method to continuously learn from new observations. The VFDT (very fast decision tree) proposed by Domingos and Hulten (2000) is one of the most popular algorithms in this category. It extracts and keeps an anytime decision tree classifier using bounded computational resources with a performance similar to that of a batch implementation. Several extensions to deal with continuous attributes has been proposed (see e.g. Gama et al., 2003). Another example of this approach is the family of methods based on the perceptron algorithm presented by Crammer et al. (2006) to approximate SVMs in online learning settings. Law and Zaniolo (2005) presented an adaptation of the nearest neighbor algorithm to adaptively determine a suitable neighborhood in single-label data stream scenarios. The other mainstream approach for data stream classification consists in decomposing the stream into batches, training a different classifier on each batch and using a voting scheme to implement an ensemble decision function. The method developed by Oza (2005) is a baseline in this category. It extends bagging for online data streams and accepts any type of online classifier as base learner. An online version of boosting is also proposed but it is significantly more expensive computationally. Recently Bifet et al. (2009) has extended the work of Oza (2005) to deal with concept drift by designing a change detector to decide when to discard underperforming learners. This provides a general way to extend models supporting online learning (such as those studied in this paper) to drifting environments.\nMulti-label Classifiers. Methods allowing multi-label text annotations have largely relied on the Binary Relevance (BR) decomposition method and its variants. Multi-label classification is achieved by training several binary classifiers on atomic tags, using the documents containing them as positive examples and the rest a negative examples. An advantage of this approach is that it makes possible to employ accurate techniques, such\nas support vector machines (SVMs), to solve the obtained sub-problems. Applying BR to data streams is straighforward by adopting online or incremental binary base models (see for instance Chai et al., 2002). The BR approach however has been largely criticized in the literature because it fails to take into account possible correlations among labels during the classification process. On the other hand, the obtained sub problems tend to be highly assymetric and it has been observed as a consequence that the approach usually overwhelm the class with more samples. Following Tsoumakas et al. (2010), BR can be classified as a problem transformation method where the representation of the dataset is changed in such a way that single-label classification methods can be directly applied. Other methods in this category are those using binary classifiers to predict label sets instead of atomic labels Read et al. (2011), or the pairwise classification method, where binary models are used for every possible pair of labels. Several adaptation methods where multi-label data is modeled at once has also been investigated, including extensions of boosting Schapire and Singer (2000) and neural networks Zhang and Zhou (2006). Most of this work focus on classic test/train settings and thus it is not well known how these approaches can work in data stream environments. A important contribution to provide baselines in this context is the work of Read et al. (2012) where several multi label methods are studied under a data stream settings. Here the authors propose an extension of the VFDT method Domingos and Hulten (2000) to handle multi-label predictions by incorporating multi-label classifiers at the leaves. This method was also combined with the ensemble methods of Bifet et al. (2009) and Oza (2005) in order to handle concept drift.\nLow Dimensional Text Representation. The online text representation method we investigate in this paper is based on the technique presented by Shi et al. (2009b) to deal with high-dimensional data. Theorem (1) essentially shows that the hashing based feature map of Eqn. (4) can be used to implement the Johnson-Lindenstrauss (JL) lemma (Matou\u0161ek, 2008). In the last years, random projections (RP) (Achlioptas, 2003) have emerged as a popular way to achieve the same goal, leading to an efficient dimensionality reduction approach that has been used to attack several data mining problems. A number of methods to improve the computational cost of performing RP have been recently investigated, leading to the so called fast Johnson-Lindenstrauss transforms (Ailon and Chazelle, 2009). We can identify two key advantages in using a hashing based representation instead of RP for data stream applications. The first is that RP needs to know in advance the dimensionality of the original space. In some applications, such as those involving text, constraining in advance the set of potential attributes (e.g. words in text) prevent the system to fully exploit new data. The second is that RP needs to explicitly store the projection matrices that lead, via inner product computations, to the compressed representation. This leads to an additional storage cost and prevents to enjoy truly constant memory requirements in applications where the number of features is allowed to grow. In (Shi et al., 2009a), the idea of using a low dimensional sketch (Cormode and Muthukrishnan, 2005) to approximate the TF-IDF representation was applied to large-scale corpora but it was not explored in data stream settings. Recently, Baena-Garcia et al. (2011) extended this method to allow efficient representation of massive streams of documents but the effects of this approximation on classification tasks was not analyzed.\nPrototype-based Classification. The idea of using a set the prototypes and a similarity function to implement a text classifier has a long tradition in information retrieval (Joachims, 2002; Sebastiani, 2002). It is also a popular method to reduce the complexity of nearest neighbor approaches (Garcia et al., 2012). In particular, centroid based text classification (CC), where class dependent centroids are used as prototypes, has been recently revisited by several authors because of the computational efficiency that can be achieved in large scale applications. Tan (2008) reports significant improvements on naive Bayes and KNN methods using an adaptive centroid classifier for text categorization tasks. An online extension of this procedure has been presented in (Tan et al., 2011) and applied to large train/test problems where the method slightly outperforms SVMs. A similar technique was presented in (Borodin et al., 2013) for text classification in stationary and non-stationary environments. Wang et al. (2013a) propose to filter out instances far from the boundary to enhance the predictive power of CC and Pang and Jiang (2013) integrate it with a clustering algorithm to obtain a lightweight approximation of nearest neighbors. Unfortunately, all these contributions focus on single-label classification. Multi-label annotations are indeed filtered out for experimental purposes and thus it is not yet well known how to adapt CC to multi-label scenarios. In this paper, we explicitly devise a multi-label classification method to annotate documents, using centroids to learn a partition of the representation space where documents has been embedded using an online representation method."}, {"heading": "6. EXPERIMENTS", "text": "This section presents experiments performed to evaluate the performance of the clashing system and other text classification methods, explicitly devised or adapted for data stream settings.\nWe start our analysis studying the ability of the online embedding procedure studied in this paper to effectively preserve the geometry of the word count space. In particular, we seek to determine how the performance of the clashing approach depends on the dimensionality of the low-dimensional representation space. Next, we perform experiments to compare the performance of system with other text classifiers. The performance measures used in all the experiments of this section are detailed below."}, {"heading": "6.1. Performance Measures", "text": "Widespread metrics to assess performance text classification are precision and recall. Precision can be defined as the probability that a retrieved instance is relevant to a given query and Recall as the probability to retrieve a relevant instance (Joachims, 2002). Given a class label \u03c4j they be respectively estimated as follows\np\u0302j = n j ++/(n j ++ + n j +\u2212) , r\u0302j = n j ++/(n j ++ + n j \u2212+) ,\nwhere nj+\u2212 is the number of documents for which \u03c4j \u2208 f(xi) but \u03c4j /\u2208 Ti (false positives), nj++ is the number of documents for which \u03c4j \u2208 f(xi) and actually \u03c4j /\u2208 Ti (true positives), nj\u2212+ is the number of documents for which \u03c4j \u2208 Ti but \u03c4j /\u2208 f(xi) (false negatives). Usually, a tradeoff between precision and recall is unavoidable (Joachims, 2002). For this\nreason, these scores are usually combined into a single performance measure, called the F1 measure\nF j1 = 2pjrj/(pj + rj) = 2n j ++/(2n j ++ + n j +\u2212 + n j \u2212+) . (21)\nIn multi label classification, the goal is to obtain a good performance among all the possible class labels, including those with less samples. Two different methods are typically used to assess such multi-label performance: macro- and micro- averaging. In macro-averaging, performance is measured by averaging the values of F j1 among the different labels, Fmacro1 = nt\u22121 \u2211 j F j 1 . In micro-averaging in contrast, the different types of errors are first computed as a whole, and just then they are processed to compute a value Fmicro1 = 2n++/(2n+++n+\u2212+n\u2212+), where n+\u2212 = \u2211 j n j +\u2212, n++ = \u2211 j n j ++ and n\u2212+ = \u2211 j n j \u2212+. These two methods may give quite different results. Micro-averaging tends to overwhelm labels with less samples among data. Macro-averaged measures are usually more difficult to optimize because they give equal importance to all the class labels, including those which are under-represented with respect other labels (Tsoumakas et al., 2010)."}, {"heading": "6.2. Preservation of Metric Information", "text": "In this experiment we study the preservation of the metric information on the Reuters RCV1 Volume I Corpus (RCV1). This an archive of over 800.000 newswire stories, collected and manually categorized by Reuters, extensively used to assess text mining algorithms (for details see Lewis et al., 2004). We use the Topics category set consisting of 103 tags which capture the major subjects of a story. For this corpus, we compute two n\u00d7 n Gram matrices Mx and M\u03c6 composed of the inner products between documents in the original vector space X and the low dimensional representation space \u03c6(X ) respectively; that is, Mxij = \u3008xi,xj\u3009 and M\u03c6ij = \u3008\u03c6(xi),\u03c6(xj)\u3009. As we have discussed before, a document classification system based on TF-IDF vector space representation is typically more accurate than a system based only on TF. However, direct embedding of TF-IDF requires precomputing a entire vocabulary for the corpus beforehand, thus preventing a fully online operation of the proposed system. This experiment is also aimed to determine if the IDF correction scheme we have described in previous sections allow us to get correlation back.\nFigure 1 shows the linear correlation coefficient \u03c1 as a function of the number of the number of dimensions (in logarithmic scale) obtained by sampling 2 \u00d7 106 pairs of points from the Mx and M\u03c6, for the different vector space representations. As it may be expected, the linear correlation between TF and the hashed TF representation (red, circled, solid curve) converges to 1 as the number of dimensions increases. After m = 214 dimensions, the linear correlation is practically optimal, and even m = 212 = 4096 dimensions provides a quite satisfactory level of approximation (\u03c1 = 0.9240). The correlation between TF-IDF and hashed TF representations (green, dashed, circled curve) in contrast, is convergent to approximately 0.95, demonstrating that IDF indeed changes the geometry of the TF vector space. However, after applying the IDF correction scheme to the hashed TF representations (orange, diamond, dotted curve) the linear correlation is convergent to 1 again, as desired, thus confirming that the TF-IDF vector space can be approximated by hashing without storing a vocabulary."}, {"heading": "6.3. Effect of Dimensionality on the Classification Performance", "text": "Continuing with the previous experiment, we study the predictive power of the clashing models as a function of the dimensionality of the hashed space. With this aim, we use all the documents dated in odd days as an independent test set and the rest are used as training instances. The training instances are processed in chronological order, following the data stream setting introduced in section 2.2, that is, each document is classified and used as a training instance only once. Figure 2 shows the Macro a Micro F1 measures on the test set after processing the training documents and considering all the 103 tags in the Reuters Corpus (Topics collection).\nAfter m = 210 dimensions, we observe a fast convergence to a stable performance. Note that this is coherent with the results obtained in the previous section in which we observed a fast convergence to linear correlation \u03c1 = 1 after m = 212 or m = 214 dimensions. Indeed, these results suggest that in order to obtain a competitive performance, the system does not need to preserve all the geometric information of the original space and it is instead robust to slight distortions in the inner products computed in the compressed space."}, {"heading": "6.4. Batch Sanity Checks", "text": "We present a classic train-test experiment aimed to compare the predictive performance of the models analyzed in this paper with some results available in (Lewis et al., 2004), in which the authors thoroughly study the Reuters corpus. Table 1 shows accuracy, macro and micro averaged F measure, precision and recall, computed on the test set, for different classifiers, after processing the full labelled data stream. For kNN, we set k = 1 and predicted all the labels contained in the nearest neighbor of a test document. Binary models (perceptron and SVM) were trained using a binary relevance approach (Sebastiani, 2002; Lewis et al., 2004). The perceptron was trained sequentially on the dataset with a learning rate of 0.1. SVMs were trained using the LIBLINEAR library for large-scale classification (Fan et al., 2008), using a classic L1-loss, L2-regularized formulation with default parameters. Following Lewis et al. (2004), a first SVM (SVM-Lewis)\nwas trained using the first 22.000 training examples in chronological order. SVM-Full was trained using the full training stream (400K examples).\nWe conclude that the performance of kNN and SVM models is similar or slightly better than those reported in (Lewis et al., 2004). The SVM-Full is clearly the best classifier in this setting, followed by kNN. However, these models require significantly more computational resources than a clashing system. In particular, kNN has a training complexity of O(Dn2), where D is the dimensionality of the representation space and n is the size of the training set. Similarly, SVMs exhibit a training time between linear and quadratic in n even using modern solvers.\nUsing few computational resources, the clashing models achieve macro averaged scores competitive with those of kNN. The Clasher-M2 shows a better macro averaged recall than kNN, as expected from its design. In terms of macro F measure, both Clashers improve on the SVM trained with the amount of data used in (Lewis et al., 2004) to\nmake training tractable. The online perceptron achieves a lower macro F measure due to a lower macro averaged precision. This scenario is similar in terms on micro averaged measures, except for the perceptron and SVM-Lewis which become significantly more competitive in terms of precision and F measure. This improvement with respect to macro averaged measures suggests that these models are better for learning classes observed more frequently among documents, underwhelming classes with less samples."}, {"heading": "6.5. Data Stream Experiments", "text": "In this section, we analyze the cumulated performance of the clashing models and alternative classification methods in the data stream setting described in section 2.2. At each round t, the system is given with a new document dt \u2208 D and it is asked to predict the set of labels T\u0302t \u2282 T that should be assigned to it. After providing a prediction T\u0302t = ft\u22121(dt), the system is feed with the set of correct labels Tt and this information is used to update the model. The cumulative performance achieved by the models is monitored by updating counters for false positives, true positives, false negatives and true negatives, which are then used to compute precision, recall, and F1 measures at any desired time. Since we are interested in a multi-label evaluation of the classifiers, we compute micro and macro averages among all the 103 tags of the Reuters corpus. That is, for a micro average we use global errors counts. For obtaining a macro average, we first compute the measure of performance for each possible tag and then average the results.\nIn addition to the Reuters data, we present here scalability experiments performed in the full New York times annotated corpus (Sandhaus, 2008). This is a collection of 1.855.658 articles obtained from the historical archive of The New York Times, covering a period of more than twenty years, between January 1987 and June 2007. According to Crammer et al. (2009), this is possibly the largest collection of publicly released annotated news text, and therefore an ideal benchmark to test large-scale text analysis tools. In this paper we select the General Online Descriptors family of labels to illustrate the methods (1622 tags).\nFigure 3 displays the cumulative performance measures for the different models as they process the Reuters corpus and Figure 5 shows the corresponding results using the New York Times dataset. We compare the average running time (secs.) required by the models to process one document, including testing and training operations. This time was measured every 5000 rounds. The second and third panels show the macroaveraged and micro-averaged F1measure respectively. These scores were computed every 500 rounds for the first 20.000 rounds and every 5000 rounds after that. Additionally we show in Figure 4 a comparison of the clashing system, operating with the simplest of the two learning criteria investigated (Mode-1), against the online Hoeffding tree models proposed in (Read et al., 2012) to deal with multi-label data streams.\nClashing versus kNN and SVMs. . For this experiment, a SVM was periodically retrained using batches of increasing size extracted from the stream. An initial model was computed using the first 4000 observations and re-computed every 2000 examples during the first 20.000 test/train rounds. Then, we increase the training period to 20.000 examples. As for the previous experiment, we employ a L1-loss, L2-regularized formulation, solved using the large-scale coordinate ascent method provided in the LIBLINEAR\nlibrary (Fan et al., 2008), using default parameters. kNN was used by setting k = 1 and storing all the labelled documents in memory as they arrive to the system.\nIn Figure 3 (Reuters data), we observe that all the models quickly converge to a micro averaged F1 measure close to the test performance observed in the previous experiment. The SVM is significantly superior in terms of this score, followed by the kNN approach. However, in terms of macro averaged F1 measure (second panel of Figure 3 ), the picture is very different. The clashing models provide superior predictive performance uniformly in the first half of the stream. kNN and the SVM need to observe significantly more observations in order to converge to performances comparable to those observed in the batch setting. This result confirms that our models are more effective for modeling classes underrepresented in terms of samples. By comparing the macro averaged and micro averaged F1 measures, we conclude that kNN and the SVM overwhelm more easily the popular labels at expenses of tags with less samples. The different implementations of the learning rule for the clashing system do not show significant differences.\nFrom Figure 5 (New York Times data), we observe that the clashing models obtain significantly better performance in terms of macro averaged F1 measure than the SVM. The SVM performance indeed starts to decrease at n \u2248 200K documents and only after n \u2248 800K it starts to slowly increase again. Studying the learning curves corresponding to micro averaged F1 measure, we note that the performance of all the models start to decrease around n \u2248 200K documents. This trend starts to be slowly reversed around n \u2248 800K documents for the SVM. We explain this result by hypothesizing a concept drift between n \u2248 200K and n \u2248 800K for the classes of the problem which are dominant in terms of number of samples. We confirm this hypothesis by studying the performance of the clashing models on a random permutation of the data stream. From Figure 5, we observe, as expected, that the performance of the model is practically constant on the randomly shuffled data.\nSince the micro averaged F1 measure is highly biased towards the performance obtained in the prediction of over represented labels, all the models suffer the effect of the concept drift if we measure performance according to this score. After the drift stops, the SVM is able to more quickly increase its micro averaged score because it is more easily affected by the classes with more samples. The clashing models do not recover easily after the drift stops but their performance decreases more slowly after it starts, in such a way that they exhibit a performance comparable to the SVM at the end. Since the clashing models are not biased towards the most popular labels, they obtain monotonically increasing macro averaged F1 measures, in contrast to the SVM which suffer the drift also in terms of the micro averaged F1 score.\nThe first panel of Figure 3 illustrates that the running time complexity of the clashing models is significantly better than the time required by kNN and the SVM to process examples. kNN scales linearly with the data stream size because it stores each new document presented to the system. Even using a modern solver, running time of the the SVM seems to scale linearly or super-linearly in the number of processed documents. The same conclusion is obtained from Figure 5: processing times for the SVM seems to scale linearly in the stream size. Our methods in contrast guarantee constant processing time in both cases.\nClashing versus Online Perceptron. . This comparison is interesting because, as the proposed methods, the perceptron is a truly online method.\nIn Figure 3 (Reuters data), we note that the micro averaged F1 measure achieved by the perceptron is similar to that of the clashing techniques with an advantage at the end. The perceptron is slightly worse than the proposed methods on the first part of the stream. However, the predictive power of the clashers increase significantly faster than the perceptron\u2019s learning curve when we analyze the macro averaged F1 measure. Our models keep a significant advantage most of the time. This result suggests again that the clashing approach is better to predict tags which are underrepresented in terms of samples as compared to more frequently observed tags. Underwhelming classes with a high number of negatives with respect to positives (i.e. documents containing the label) is a common problem in multi-label applications, in such a way that obtaining high macro averaged scores is usually hard in practice.\nIn Figure 5, we observe that the trend of the perceptron\u2019s learning curve on the New York Times data is similar to form the SVM\u2019s learning curve. However, most of the time, the perceptron achieves an F1 measure lower than the SVM\u2019s and thus lower than the clashing models\u2019. Our previous conclusions about a likely concept drift between n \u2248 200K and n \u2248 800K hold as well. The perceptron is more readily affected by the drift and it\u2019s able to more quickly recover from it. We explain this result by the tendency of the perceptron to overwhelm classes with a high number of samples, i.e., with a large number of positive examples.\nFrom Figure 3 (Reuters data), we note that, at the beginning, the average processing time for this technique is higher and more variable because the probability of incurring a classification loss is higher. This suggests that in applications with more complex, e.g. drifting, concepts, the efficiency of this technique may decrease. The running time of the clashing models is instead always constant, independently of the current performance. In Figure 5, we observe that the processing times of the the clashing models on the New York Times data are significantly better than the perceptron\u2019s times. This behavior can be explained by the low performance of the perceptron in terms of averaged F1 measures which translate into a high number of mistakes triggering updates, and the large number of sub-models the perceptron needs to handle in this dataset (1622 tags).\nClashing versus Online Hoeffding Trees. . In figure 4, we show the results achieved on the Reuters data stream of 4 techniques investigated in (Read et al., 2012) to deal with multi-label data streams. Online Hoeffding trees with Majority Label classifiers at the leaves are denoted as HT-ML. Online Hoeffding trees with Pruned Label Sets at the leaves are denoted as HT-PS. As stressed by Read et al. (2012) these methods may be significantly enhanced by using them in ensemble methods like (Bifet et al., 2009) and (Oza, 2005) designed for data streams. Here we show the results obtained by using the method in (Bifet et al., 2009), but the results are similar to those obtained by using (Bifet et al., 2009). Ensemble versions of HT-ML and HT-PS are denoted EA-HTML and EA-HT-PS. We used the implementation provided by the authors in the last version of the MOA software (Bifet et al., 2010). Parameters were set as default, e.g., we used adaptive thresholding and M = 10 base learners. Results show that the clashing system is significantly more accurate than all these techniques both in terms of Macro F measure and Micro F measure. Running times are low for all the techniques with a slight advantage for the method investigated in this paper."}, {"heading": "6.6. Partially Labelled Streams", "text": "From the results presented so far, we hypothesize that the clashing system can be advantageous learning from a partially labelled stream because of its ability to reach a competitive performance more quickly. To confirm this hypothesis we conducted an experiment on the full Reuters dataset, where the classifiers are trained using different fractions (p) of labelled data. We present the 800.000 examples corresponding to this dataset in a series of rounds t = 1, 2, . . .. Every document is used as testing instance to compute the cumulative performance measures. However, after testing, a document is used as a training instance with probability p.\nFigure 6 displays the results obtained by setting, p = 0.25, p = 0.125, p = 0.0625 and p = 0.03125. These results confirm that the clashing approach is able to learn faster than the other methods from a few labelled instances. This advantage is more clear when we focus on macro averaged F measure."}, {"heading": "7. CONCLUSIONS", "text": "In this paper, we presented a simple and efficient method to classify large data streams of documents, referred to as clashing.\nContributions. The novelty of the proposed approach is in addressing both text representation and document classification in an online fashion, using limited computational resources. This is a challenging setting because: (1) the computation of standard text representation requires to known in advance the documents and words to be processed, (2) an exact incremental text representation based on words requires unbounded resources, (3) usually text annotations are non-exclusive but most multi-label classifiers have been studied in batch scenarios.\nWe solved the problem of text representation by devising a novel method to approximate the geometry of the full TF-IDF space using only online computations. This approach combines recent ideas of data stream sketching and feature hashing. Up to our knowledge, this is the first paper showing that the online computation of both TF and IDF, without corpus-wise periodical computations, is possible and accurate. To efficiently predict multi-label classifications, we devised a method to organize the feature space into a set of regions where documents with similar low dimensional representations collide by way of a winner-takes-all mapping process. Then, we implemented a conditional naive bayesian approach where labels are assumed to be independent given the clashing region. This method allows to keep the high efficiency of the text representation engine. Up to our knowledge, this is the first extension of centroid based classification to multi-label data and data stream text classification. We showed that this method can be simple, scalable and accurate.\nThe clashing system showed constant processing time independently of the data stream size and the number of possible attributes (words). We performed experiments using the Reuters RCV1 and New York Times data. They suggest that learning from data streams using this system is efficient and more robust to unbalanced classes than methods with comparable running times, yielding better macro averaged predictive scores. Finally, the system arrives faster to give forth competitive predictions even when it has observed few labelled instances. These properties can be useful in settings where the\nclassifier needs to learn from partially labelled streams or in drifting scenarios, where components of the model need to quickly learn from new observations to fit the new data configuration.\nPractical Implications and Future Work. This work was motivated by a real world problem in which we need to annotate and analyze large streams of web content. The algorithm we have developed will be incorporated into our pipeline devoted to Computational\nSocial Sciences, NOAM (Flaounas et al., 2011). The insistence on only making use of bounded resources is a consequence of the size of the streams: both time and memory need to be kept under control. This problem comes up often in practice. Fast flowing streams of text are generated by online news, social media and endless other applications, and the need to automatically annotate and adaptively sort them into sub-streams is a\ncrucial one. In the future, we plan to extend this work in the following directions. (1) To incorporate in the system the ability to explicitly detect and manage concept drift. We\nbelieve that the system is particularly suitable to support this feature by incorporating adaptive forgetting factors in the engines used to approximate TF-IDF and to obtain\nmulti-label annotations. (2) To use our online text representation system in the solution of unsupervised text mining problems. (3) To extend the online embedding procedure to approximate other weighting schemas. In particular class-based term weighting for text classification has been focus of increasing interest in the last years (Lertnattee and Theeramunkong, 2004; Lan et al., 2009; Guan et al., 2009; Luo et al., 2011; Ren and Sohrab, 2013; Wang and Zhang, 2013). Unfortunately, latest contributions typically adopt a batch setting for text representation. Our preliminary analysis suggests that some approaches, e.g. (Ren and Sohrab, 2013), may be efficiently and accurately approximated using data sketches without significantly increasing storage and processing time."}, {"heading": "Acknowledgments", "text": "Nello Cristianini and Ilias Flaounas were supported by EU project COMPLACS. Ricardo \u00d1anculef was partially funded by the National Commission for Scientific and Technological Research of Chile, Grant Fondecyt 11130122."}, {"heading": "APPENDIX (PROOFS)", "text": "Proof. (Lemma 1 ). We have shown in 3.1, that the array Ck computed by Alg. 3 corresponds to an implementation, with L = 1, of the Cormode and Muthukrishna sketch, described in 2.5, to approximate the document frequencies (f(wi)) of the words observed till round t. Therefore, for any word wi such that h(wi) = k, we have from Theorem 2, f(wi) \u2264 Ck \u2264 f(wi) + \u2016f\u2016, with probability (1\u2212 e\u22121), provided m \u2265 dexp(1)/ e. Now, since log(\u00b7) is a monotonically increasing function.\nf(wi) \u2264 Ck \u2264 f(wi) + \u2016f\u2016 \u21d2 log (f(wi)) \u2264 log (Ck) \u2264 log (f(wi)) + log ( f(wi)+ \u2016f\u2016\nf(wi)\n)\n\u21d2 log (f(wi)) \u2264 log (Ck) \u2264 log (f(wi)) + log ( 1 + \u2016f\u2016f(wi) ) . (22)\nThe left hand side implies\nlog\n( n\nf(wi)\n) = log(n)\u2212 log f(wi) \u2265 log(n)\u2212 log (Ck) = log ( n\nCk\n) . (23)\nThus IDF(wi) \u2265 IDF(wi). Similarly, the left hand side of (22) implies (we assume f(wi) > 1 for simplicity)\nlog\n( n\nCk\n) \u2265 log ( n\nf(wi)\n) \u2212 log (1 + \u2016f\u2016) . (24)\nThus IDF(wi) \u2265 IDF(wi) \u2212 \u0304\u2016f\u2016, with \u0304 := log (1 + \u2016f\u2016) /\u2016f\u2016. Solving for yields = (exp(\u0304\u2016f\u2016)\u2212 1)/\u2016f\u2016. Using the inequality exp(x) \u2265 x+ 1 which holds for any x < 1, we have = (exp(\u0304\u2016f\u2016)\u2212 1)/\u2016f\u2016 \u2265 ((\u0304\u2016f\u2016) + 1\u2212 1)/\u2016f\u2016 = \u0304. Thus \u2265 \u0304 > 0. Therefore, IDF(wi) \u2265 IDF(wi)\u2212 \u0304\u2016f\u2016 implies IDF(wi) \u2265 IDF(wi)\u2212 \u2016f\u2016.\nProof. (Lemma 2). Let X = {x1, . . . ,xn} the representation of the documents seen so far in the word-count space X . Let Xi := {xk \u2208 X : \u03c4i \u2208 Tk} and Xi the matrix with columns corresponding to the elements of Xi. Let S\u03c6 and Sx denote the clashing systems operated on \u03c6(D) and X respectively. Let p\u03041, . . . , p\u0304nt , with p\u0304i = Xi\u03b1, represent the prototypes of Sx and C\u0304i,j = |Xi \u2229 Xj |/|Xi| its probability estimates. Here, \u03b1 is the set of coefficients used by the system to linearly generate the prototypes. Note that the Mapper in Sx computes j\u2217 = arg mini \u2016x \u2212 p\u0304i\u20162 and the prediction rule given by Eqn. (13) still holds by using the new definition of j\u2217. Note now that due to the linearity of the hashed feature map,\npi = \u03c6Xi\u03b1 = \u03c6(Xi\u03b1) = \u03c6(p\u0304i) . (25)\nThis allows us to use the bounds for preservation of norms in X given in the previous section. Suppose, x is assigned to the region i\u2217 in the original data space. For any \u03b4 < 0.1, let \u03b7j(x) = \u2016p\u0304j \u2212 x\u20162 \u2212 \u2016p\u0304i\u2217 \u2212 x\u20162, \u03b7(x) = minj 6=i\u2217 \u03b7j(x) and j = \u03b7(x)2\u2016p\u0304j\u2212x\u20162 . Consider the event A\u03c6(x) composed of the \u03c6(x) such that (simultaneously)\n\u2016pi\u2217 \u2212 \u03c6(x)\u20162 = \u2016\u03c6(p\u0304i\u2217)\u2212 \u03c6(x)\u20162 \u2264 \u2016p\u0304i\u2217 \u2212 x\u20162 + i\u2217\u2016p\u0304i\u2217 \u2212 x\u20162 (26) \u2016pj \u2212 \u03c6(x)\u20162 = \u2016\u03c6(p\u0304j)\u2212 \u03c6(x)\u20162 \u2265 \u2016p\u0304j \u2212 x\u20162 \u2212 j\u2016p\u0304j \u2212 x\u20162,\u2200j 6= i\u2217 .\nFor any j 6= i\u2217 and for any x \u2208 A\u03c6(x) we obtain, by definition of j , \u03b7j(x) and \u03b7(x)\n\u2016pi\u2217 \u2212 \u03c6(x)\u20162 \u2264 \u2016p\u0304i\u2217 \u2212 x\u20162 + \u03b7(x)/2 \u2264 \u2016p\u0304i\u2217 \u2212 x\u20162 + \u03b7j(x)/2 (27) \u2016pj \u2212 \u03c6(x)\u20162 \u2265 \u2016p\u0304j \u2212 x\u20162 \u2212 \u03b7(x)/2 \u2265 \u2016p\u0304j \u2212 x\u20162 \u2212 \u03b7j(x)/2 ,\nand thus,\n\u2016pi\u2217 \u2212 \u03c6(x)\u20162 \u2264 (\u2016p\u0304i\u2217 \u2212 x\u20162 + \u2016p\u0304j \u2212 x\u20162)/2 (28) \u2016pj \u2212 \u03c6(x)\u20162 \u2265 (\u2016p\u0304i\u2217 \u2212 x\u20162 + \u2016p\u0304j \u2212 x\u20162)/2 ,\nwhich finally leads \u2016pi\u2217 \u2212 \u03c6(x)\u20162 \u2264 \u2016pj \u2212 \u03c6(x)\u20162, that is if x \u2208 A\u03c6(x), the hashed decision for x is identical to the decision in the original data space. We just need to bound Pr(A\u03c6(x)). For any \u03b4 < 0.1, set = mini( i), m = 12 \u22122 log(1/\u03b4) and c = 16 \u22121 log(1/\u03b4) log2(m/\u03b4). Note that this setting of parameters makes the conditions of Theorem 1 hold for any tuple (\u03b4, i,m, c). Then, a simple union bound ensures that Pr(A\u0304\u03c6(x)) \u2264 3\u03b4nt. Rephrasing, the probability that the embedding keeps the decision made in the original data space is at least 1 \u2212 3\u03b4nt. To conclude, note that to have a global confidence \u03b4\u0303 it is enough to choose \u03b4 = \u03b4\u0303/nt and that\n\u03b7\u0304(x) = \u03b7(x)\nmaxj \u2016p\u0304j \u2212 x\u20162 = 2 . (29)\nProof. Proposition 1) Let x be the exact TF-IDF representation of a document d and z the its exact TF representation. If IDF denotes the vector of exact IDF weights corresponding to the coordinates of x, we have x = IDF z, where denotes the Hadamard product (component by component). Alg. 3 computes an approximation z\u0304\nof the TF representation z, an approximation IDF of IDF and finally approximates the TF-IDF representation as x\u0304 := \u03c6(d) = IDF z\u0304. Note that z\u0304 is just the hashed representation of z (identical to the embedding computed by Alg.2). In order to bound |\u2016x\u201622 \u2212 \u2016x\u0304\u201622|, we can write,\n\u2016x\u201622 \u2212 \u2016x\u0304\u201622 = ( \u2016x\u201622 \u2212 \u2016\u03c6(IDF z)\u201622 ) + ( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016x\u0304\u201622 ) , (30)\nwhere \u03c6(IDF z) is the hashed approximation of the true TF-IDF representation computed using Eqn.(4). Since x = IDF z, the first term is\n( \u2016x\u201622 \u2212 \u2016\u03c6(IDF z)\u201622 ) = ( \u2016IDF z\u201622 \u2212 \u2016\u03c6(IDF z)\u201622 ) . (31)\nUsing Theorem 1, we have with probability 1\u2212 3\u03b4 \u2223\u2223(\u2016x\u201622 \u2212 \u2016\u03c6(IDF z)\u201622 )\u2223\u2223 \u2264 \u2016x\u201622 . (32)\nThe second term is ( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016x\u0304\u201622 ) = ( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016IDF \u03c6(z)\u201622 ) . (33)\nExpanding\n\u2016\u03c6(IDF z)\u201622 = \u2211 k (\u2211 i:h(wi)=k IDFizi )2 \u2016IDF \u03c6(z)\u201622 = \u2211 k IDF 2 k (\u2211 i:h(wi)=k zi )2 . (34)\nFrom lemma 1 we have with probability at least (1 \u2212 e\u22121), IDF(wi) \u2265 IDF(wi) \u2265 IDF(wi) \u2212 \u2016f\u2016, provided m \u2265 dexp(1)/ e. Here IDF(wi) = IDFk for any wi such that h(wi) = k. Fix i and the corresponding k. We have IDFi \u2265 IDFk \u2265 IDFi \u2212 \u2016f\u2016. Substituting IDFi \u2265 IDFk leads to ( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016x\u0304\u201622 ) > 0. Thus \u2016x\u201622 \u2212 \u2016x\u0304\u201622 >\n\u2016x\u201622 with probability at least (1\u2212 e\u22121)(1\u2212 3\u03b4). Using IDFi \u2264 IDFk + \u2016f\u2016 lead to\n( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016x\u0304\u201622 ) \u2264 \u2211\nk\n( 2 \u2016f\u2016IDFk + 2\u2016f\u20162 )(\u2211 i:h(wi)=k zi )2 . (35)\nSince \u2016f\u2016 is the sum of all the document frequencies and IDFi \u2264 1, we can assume that IDFk \u2264 \u2016f\u2016. Using also that 2 \u2264 for any \u2208 (0, 1), we obtain,\n( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016x\u0304\u201622 ) \u2264 3 \u2016f\u20162 \u2211\nk\n(\u2211 i:h(wi)=k zi )2 = 3 \u2016f\u20162\u2016\u03c6(z)\u201622 . (36)\nUsing (32) and 2 \u2264 again we have ( \u2016\u03c6(IDF z)\u201622 \u2212 \u2016x\u0304\u201622 ) \u2264 6 \u2016f\u20162\u2016x\u201622 . (37)\nCombining with (32) and since \u2016f\u2016 > 1 (at least one word in one document was observed), we have that with probability at least (1\u2212 e\u22121)(1\u2212 3\u03b4), \u2016x\u201622 \u2212 \u2016x\u0304\u201622 < 7 \u2016f\u20162\u2016x\u201622. To obtain the result, set \u0304 = 7 \u2016f\u20162 and substitute in Theorem 1. Note that can be arbitrarily small and so \u0304.\nProof. (Proposition 2) Clearly,\nPr(Err) \u2264 Pr(Err) + Prx,\u03c6 (fX (x) 6= f\u03c6(x)) , (38)\nwhere Prx,\u03c6 (fX (x) 6= f\u03c6(x)) is the probability that the embedding changes a decision made in the original space. This probability is taken both with respect to the randomness of \u03c6(\u00b7) and x. Let I(\u03be) be the indicator function for the event \u03be, that is I(\u03be) = 1 if \u03be is verified and 0 otherwise.\nPrx,\u03c6 (fX (d) 6= f\u03c6(d)) = Ex,\u03c6 [I (fX (x) 6= f\u03c6(x))] (39) = ExE\u03c6|xI (fX (x) 6= f\u03c6(x)) = ExPr\u03c6|x(fX (xi) 6= f\u03c6(xi)) ,\nwhere Ex denotes the mean among the test instances. Now we can average among the instances satisfying \u03b7(x) \u2265 \u03b7\u2217 and the instances violating \u03b7(x) \u2265 \u03b7\u2217,\nExPr\u03c6|x (fX (x) 6= f\u03c6(x)) =Ex,\u03b7(x)\u2265\u03b7\u2217Pr\u03c6|x (fX (x) 6= f\u03c6(x)) (40) +Ex,\u03b7(x)<\u03b7\u2217Pr\u03c6|x (fX (x) 6= f\u03c6(x)) .\nClearly,\nEx,\u03b7(x)<\u03b7\u2217Pr\u03c6|x (fX (x) 6= f\u03c6(x)) \u2264 Ex,\u03b7(x)<\u03b7\u22171 = \u03b4\u03b7\u2217 . (41)\nOn the other hand, for the instances satisfying \u03b7(x) \u2265 \u03b7\u2217, the model parameters \u03b7\u0304\u2217 = \u03b7\u2217/D2, m \u2265 48\u03b7\u0304\u22122 log(3nt/\u03b4\u0304) and c = 32\u03b7\u0304\u22121 log(3nt/\u03b4\u0304) log2(3mnt/\u03b4\u0304) are enough to apply Lemma 2. Thus,\nEx,\u03b7(x)\u2265\u03b7\u2217Pr\u03c6|x (fX (x) 6= f\u03c6(x)) \u2264 Ex,\u03b7(x)\u2265\u03b7\u2217 [ \u03b4\u0304 ] = (1\u2212 \u03b4\u03b7\u2217)\u03b4\u0304 .\nIf all the data \u03c6(D) = {x1, . . . ,xnt} is correctly classified with margin \u03b7\u2217 > 0 in the original space, Pr(Err) = 0 and \u03b4\u03b7\u2217 = 0. By construction, m \u2265 48\u03b7\u0304\u22122 log(3nt/\u03b4\u0304). Solving for \u03b4\u0304 gives,\n\u03b4\u0304 \u2264 3nt exp ( \u2212m\u03b7\u2217 2\n48D4\n) . (42)"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We present a method for the classification of multi-labelled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labelled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labelled streams.", "creator": "LaTeX with hyperref package"}}}