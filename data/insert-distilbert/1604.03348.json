{"id": "1604.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2016", "title": "Optimal Margin Distribution Machine", "abstract": "support vector machine ( svm ) has been one of the most popular robust learning algorithms, with becoming the central experimental idea of maximizing the minimum margin, i. e., the smallest overall distance compared from the instances to the classification boundary. recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. based on on this concept idea, we propose a very new method, named total optimal margin distribution machine ( odm ), below which tries to achieve a better net generalization performance by optimizing the margin distribution. commonly we characterize the margin cost distribution by the first - and second - order statistics, i. e., the margin sample mean and variance. the proposed graph method is a general learning approach which otherwise can be used in any place where pure svm can be applied, yet and their superiority is verified both theoretically and empirically in this paper.", "histories": [["v1", "Tue, 12 Apr 2016 11:39:16 GMT  (58kb)", "http://arxiv.org/abs/1604.03348v1", "arXiv admin note: substantial text overlap witharXiv:1311.0989"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1311.0989", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["teng zhang", "zhi-hua zhou"], "accepted": false, "id": "1604.03348"}, "pdf": {"name": "1604.03348.pdf", "metadata": {"source": "CRF", "title": "Optimal Margin Distribution Machine", "authors": ["Teng Zhang", "Zhi-Hua Zhou"], "emails": ["zhouzh@lamda.nju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4.\n03 34\n8v 1\n[ cs\n.L G\n] 1\n2 A\nSupport vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be used in any place where SVM can be applied, and their superiority is verified both theoretically and empirically in this paper.\nKeywords: margin, margin distribution, minimum margin, classification"}, {"heading": "1. Introduction", "text": "Support Vector Machine (SVM) (Cortes and Vapnik, 1995; Vapnik, 1995) has always been one of the most successful learning algorithms. The basic idea is to identify a classification boundary having a large margin for all the training examples, and the resultant optimization can be accomplished by a quadratic programming (QP) problem. Although SVMs have a long history of literatures, there are still great efforts (Lacoste-julien et al., 2013;\n\u2217Email: zhouzh@lamda.nju.edu.cn\nPreprint submitted to Artificial Intelligence Journal April 13, 2016\nCotter et al., 2013; Takac et al., 2013; Jose et al., 2013; Do and Alexandre, 2013) on improving SVMs.\nIt is well known that SVMs can be viewed as a learning approach trying to maximize the minimum margin of training examples, i.e., the smallest distance from the examples to the classification boundary, and the margin theory (Vapnik, 1995) provided a good support to the generalization performance of SVMs. It is noteworthy that the margin theory not only plays an important role for SVMs, but also has been extended to interpret the good generalization of many other learning approaches, such as AdaBoost (Freund and Schapire, 1995), a major representative of ensemble methods (Zhou, 2012). Specifically, Schapire et al. (Schapire et al., 1998) first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting; soon after, Breiman (Breiman, 1999) indicated that the minimum margin is crucial and developed a boosting-style algorithm, named Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance. Later, Reyzin et al. (Reyzin and Schapire, 2006) found that although Arc-gv tends to produce larger minimum margin, it suffers from a poor margin distribution; they conjectured that the margin distribution, rather than the minimum margin, is more crucial to the generalization performance. Such a conjecture has been theoretically studied (Wang et al., 2011; Gao and Zhou, 2013), and it was recently proven by Gao and Zhou (Gao and Zhou, 2013). Moreover, it was disclosed that rather than simply considering a single-point margin, both the margin mean and variance are important (Gao and Zhou, 2013; Zhou, 2014). All these theoretical studies, however, focused on boosting-style algorithms, whereas the influence of the margin distribution for SVMs in practice has not been well exploited.\nIn this paper, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve strong generalization performance by optimizing the margin distribution. Inspired by the recent theoretical result (Gao and Zhou, 2013), we characterize the margin distribution by the first- and second-order statistics, and try to maximize the margin mean and minimize the margin variance simultaneously. For optimization, we propose a dual coordinate descent method for kernel ODM, and a stochastic gradient descent with variance reduction for large scale linear kernel ODM. Comprehensive experiments on thirty two regular scale data sets and ten large scale data sets show the superiority of our method to SVM and some other state-of-the-art methods, verifying that the margin distribution is more\ncrucial for SVM-style learning approaches than minimum margin. A preliminary version of this work appeared in a conference paper (Zhang and Zhou, 2014). Compared with the original version, the new approach has a simpler formulation and is more comprehensible. In addition, it avoids the operation of matrix inverse, so it can be more efficient when nonlinear kernels are applied. We also give a new theoretical analysis for the proposed algorithm, and present better empirical performance. The preliminary version was called LDM (large margin distribution machine), but it is not proper to call a better margin distribution as a \u201clarger\u201d one; thus we now call it ODM (optimal margin distribution learning machine), and the algorithm described in (Zhang and Zhou, 2014) is called as ODML in this paper.\nThe rest of this paper is organized as follows. Section 2 introduces some preliminaries. Section 3 and 4 present the formulation and optimization of ODML and ODM respectively. Section 5 present the theoretical analysis. Section 6 reports on our experiments. Section 7 discusses about some related works. Finally, Section 8 concludes."}, {"heading": "2. Preliminaries", "text": "We denote by X \u2208 Rd the instance space and Y = {+1,\u22121} the label set. Let D be an unknown (underlying) distribution over X \u00d7Y . A training set of size m\nS = {(x1, y1), (x2, y2), . . . , (xm, ym)},\nis drawn identically and independently (i.i.d.) according to the distribution D. Our goal is to learn a function which is used to predict the labels for future unseen instances.\nFor SVMs, f is regarded as a linear model, i.e., f(x) = w\u22a4\u03c6(x) where w is a linear predictor, \u03c6(x) is a feature mapping of x induced by some kernel k, i.e., k(xi,xj) = \u03c6(xi)\n\u22a4\u03c6(xj). According to (Cortes and Vapnik, 1995; Vapnik, 1995), the margin of instance (xi, yi) is formulated as\n\u03b3i = yiw \u22a4\u03c6(xi), \u2200i = 1, . . . , m. (1)\nIt can be found that in separable cases where the training examples can be separated with the zero error, all the \u03b3i will be non-negative. By scaling it with 1/\u2016w\u2016, we can get the geometric distance from (xi, yi) to w \u22a4\u03c6(x),\ni.e.,\n\u03b3\u0302i = yi w\u22a4\n\u2016w\u2016 \u03c6(xi), \u2200i = 1, . . . , m.\nFrom (Cristianini and Shawe-Taylor, 2000), it is shown that SVM with hardmargin (or Hard-margin SVM) is regarded as the maximization of the minimum distance,\nmax w \u03b3\u0302\ns.t. yi w\u22a4\n\u2016w\u2016 \u03c6(xi) \u2265 \u03b3\u0302, i = 1, . . . , m.\nIt can be rewritten as\nmax w\n\u03b3\n\u2016w\u2016\ns.t. yiw \u22a4\u03c6(xi) \u2265 \u03b3, i = 1, . . . , m.\nSince the value of \u03b3 doesn\u2019t have influence on the optimization, we can simply set it as 1. Note that maximizing 1/\u2016w\u2016 is equivalent to minimizing \u2016w\u20162/2, we can get the classic formulation of Hard-margin SVM as follows:\nmin w\n1 2 w\u22a4w\ns.t. yiw \u22a4\u03c6(xi) \u2265 1, i = 1, . . . , m.\nIn non-separable cases where the training examples cannot be separated with the zero error, SVM with soft-margin (or Soft-margin SVM) is posed,\nmin w,\u03bei\n1 2 w\u22a4w + C m\nm \u2211\ni=1\n\u03bei\ns.t. yiw \u22a4\u03c6(xi) \u2265 1\u2212 \u03bei,\n\u03bei \u2265 0, i = 1, . . . , m.\n(2)\nwhere \u03bei = [\u03be1, . . . , \u03bem] \u22a4 are slack variables which measure the losses of instances, and C is a trading-off parameter. There exists a constant C\u0304 such that (2) can be equivalently reformulated as,\nmax w\n\u03b30 \u2212 C\u0304\nm\n\u2211m\ni=1 \u03bei\ns.t. \u03b3i \u2265 \u03b30 \u2212 \u03bei,\n\u03bei \u2265 0, i = 1, . . . , m.\nwhere \u03b30 is a relaxed minimum margin, and C\u0304 is the trading-off parameter. Note that \u03b30 indeed characterizes the top-pminimum margin (Gao and Zhou, 2013); hence, SVMs (with both hard-margin and soft-margin) consider only a single-point margin and have not exploited the whole margin distribution."}, {"heading": "3. Formulation", "text": "The two most straightforward statistics for characterizing the margin distribution are the first- and second-order statistics, that is, the mean and the variance of the margin. Formally, denote X as the matrix whose i-th column is \u03c6(xi), i.e., X = [\u03c6(x1), . . . , \u03c6(xm)], y = [y1, . . . , ym]\n\u22a4 is a column vector, and Y is a m\u00d7m diagonal matrix with y1, . . . , ym as the diagonal elements. According to the definition in (1), the margin mean is\n\u03b3\u0304 = 1\nm\nm \u2211\ni=1\nyiw \u22a4\u03c6(xi) =\n1\nm (Xy)\u22a4w, (3)\nand the margin variance is\n\u03b3\u0302 = 1\nm\nm \u2211\ni=1\n(yiw \u22a4\u03c6(xi)\u2212 \u03b3\u0304) 2\n= 1\nm w\u22a4\nm \u2211\ni=1\n\u03c6(xi)\u03c6(xi) \u22a4w \u2212\n2\nm\nm \u2211\ni=1\nyiw \u22a4\u03c6(xi)\u03b3\u0304 + \u03b3\u0304 2\n= 1\nm w\u22a4XX\u22a4w \u2212\n1\nm2 w\u22a4Xyy\u22a4X\u22a4w\n= w\u22a4X mI \u2212 yy\u22a4\nm2 X\u22a4w\n(4)\nwhere I is the identity matrix. Inspired by the recent theoretical result (Gao and Zhou, 2013), we attempt to maximize the margin mean and minimize the margin variance simultaneously.\n3.1. ODML\nFirst consider a simpler way, i.e., adding the margin mean \u03b3\u0304 and the margin variance \u03b3\u0302 to the objective function of SVM explicitly. Then in the separable cases where the training examples can be separated with the zero\nerror, the maximization of the margin mean and the minimization of the margin variance leads to the following hard-margin ODML,\nmin w\n1 2 w\u22a4w + \u03bb1\u03b3\u0302 \u2212 \u03bb2\u03b3\u0304\ns.t. yiw \u22a4\u03c6(xi) \u2265 1, i = 1, . . . , m,\nwhere \u03bb1 and \u03bb2 are the parameters for trading-off the margin variance, the margin mean and the model complexity. It\u2019s evident that the hard-margin ODML subsumes the hard-margin SVM when \u03bb1 and \u03bb2 equal 0.\nFor the non-separable cases, similar to soft-margin SVM, the soft-margin ODML leads to\nmin w,\u03bei\n1 2 w\u22a4w + \u03bb1\u03b3\u0302 \u2212 \u03bb2\u03b3\u0304 + C m\nm \u2211\ni=1\n\u03bei\ns.t. yiw \u22a4\u03c6(xi) \u2265 1\u2212 \u03bei,\n\u03bei \u2265 0, i = 1, . . . , m.\n(5)\nSimilarly, soft-margin ODML subsumes the soft-margin SVM if \u03bb1 and \u03bb2 both equal 0. Because the soft-margin SVM often performs much better than the hard-margin one, in the following we will focus on soft-margin ODML and if without clarification, ODML is referred to the soft-margin ODML."}, {"heading": "3.2. ODM", "text": "The idea of ODML is quite straightforward, however, the final formulation is a little complex. In this section, we try to propose a simpler one.\nNote that SVM set the minimum margin as 1 by scaling \u2016w\u2016, following the similar way, we can also fix the margin mean as 1. Then the deviation of the margin of (xi, yi) to the margin mean is |yiw\n\u22a4\u03c6(xi)\u2212 1|. By minimizing the margin variance, we arrive at the following formulation,\nmin w,\u03bei,\u01ebi\n1 2 w\u22a4w + C m\nm \u2211\ni=1\n(\u03be2i + \u01eb 2 i )\ns.t. yiw \u22a4\u03c6(xi) \u2265 1\u2212 \u03bei,\nyiw \u22a4\u03c6(xi) \u2264 1 + \u01ebi, i = 1, . . . , m.\n(6)\nSince the margin of (xi, yi) is either smaller or greater than the margin mean, so at most one of \u03bei and \u01ebi can be positive. In addition, if one is positive, the\nother must be zero (otherwise if it\u2019s negative, we can set it as zero without violating any constraint but decrease the objective function value), so the second term of the objective function is the margin variance.\nThe hyperplane yiw \u22a4\u03c6(xi) = 1 divides the space into two subspaces. For each example, no matter which space it lies in, it will suffer a loss which is quadratic with the deviation. However, the examples lie in the space corresponding to yiw\n\u22a4\u03c6(xi) < 1 are much easier to be misclassified than the other. So it is more reasonable to set different weights for the loss of examples in different spaces, i.e., the second term of (6) can be modified as\n1\nm\nm \u2211\ni=1\n(C1\u03be 2 i + C2\u01eb 2 i ),\nwhere C1 and C2 are the trading-off parameters. According to the representer theorem (Scho\u0308lkopf and Smola, 2001), the optimal solution will be spanned by the support vectors. Unfortunately, for ODM, almost all training examples are support vectors. To make the solution sparse, we introduce a D-insensitive loss like SVR, i.e., the examples whose deviation is smaller than D are tolerated and only those whose deviation is larger than D will suffer a loss. Finally, we obtain the formulation of ODM,\nmin w,\u03bei,\u01ebi\n1 2 w\u22a4w + 1 m\nm \u2211\ni=1\n(C1\u03be 2 i + C2\u01eb 2 i )\ns.t. yiw \u22a4\u03c6(xi) \u2265 1\u2212D \u2212 \u03bei,\nyiw \u22a4\u03c6(xi) \u2264 1 +D + \u01ebi, i = 1, . . . , m.\n(7)\nwhere C1 and C2 are described previously, D is a parameter for controling the number of support vectors (sparsity of the solution)."}, {"heading": "4. Optimization", "text": "We first propose a dual coordinate descent method for kernel ODML and ODM, and then propose a stochastic gradient descent with variance reduction for large scale linear kernel ODML and ODM.\n4.1. ODMdcd In this section we show that the dual of kernel ODML and ODM are both convex quadratic optimization with only simple decoupled box constraints, and then present a dual coordinate descent method ODMdcd to solve them uniformly.\n4.1.1. Kernel ODML\nBy substituting (3)-(4), (5) leads to the following quadratic programming problem,\nmin w,\u03be\n1 2 w\u22a4w +w\u22a4X\n\u03bb1(mI \u2212 yy \u22a4)\nm2 X\u22a4w \u2212 \u03bb2 m (Xy)\u22a4w + C m e\u22a4\u03be\ns.t. Y X\u22a4w \u2265 e\u2212 \u03be,\n\u03be \u2265 0.\n(8)\nwhere e stands for the all-one vector. Introduce the lagrange multipliers \u03b1 \u2265 0 and \u03b2 \u2265 0 for the first and the second constraints respectively, the Lagrangian of (8) leads to\nL(w, \u03be,\u03b1,\u03b2) = 1\n2 w\u22a4w +w\u22a4X\n\u03bb1(mI \u2212 yy \u22a4)\nm2 X\u22a4w \u2212 \u03bb2 m (Xy)\u22a4w\n+ C\nm e\u22a4\u03be \u2212\u03b1\u22a4(Y X\u22a4w \u2212 e+ \u03be)\u2212 \u03b2\u22a4\u03be\n= 1\n2 w\u22a4Qw \u2212w\u22a4XY\n(\n\u03bb2 m e+\u03b1\n)\n+\u03b1\u22a4e\n+ \u03be\u22a4 ( C\nm e\u2212\u03b1\u2212 \u03b2\n) (9)\nwhereQ = I+X 2\u03bb1(mI\u2212yy \u22a4)\nm2 X\u22a4. By setting the partial derivations of {w, \u03be}\nto zero, we have\n\u2202L \u2202w = Qw \u2212XY\n(\n\u03bb2 m e+\u03b1\n)\n=\u21d2 w = Q\u22121XY\n(\n\u03bb2 m e+\u03b1\n)\n, (10)\n\u2202L \u2202\u03be = C m e\u2212\u03b1\u2212 \u03b2 =\u21d2 0 \u2264 \u03b1 \u2264 C m e. (11)\nBy substituting (10) and (11) into (9), the dual of (8) can be cast as:\nmin \u03b1\nf(\u03b1) = 1\n2\n(\n\u03bb2 m e+\u03b1\n)\u22a4\nY X\u22a4Q\u22121XY\n(\n\u03bb2 m e+\u03b1\n)\n\u2212 e\u22a4\u03b1\ns.t. 0 \u2264 \u03b1 \u2264 C\nm e.\n(12)\nNote that the dimension of Q depends on the feature mapping \u03c6(x) and may not be calculated if \u03c6(x) maps an instance into an infinite dimension space. Fortunately, X\u22a4Q\u22121X is a m\u00d7m square matrix, next we show how to calculate this matrix.\nLemma 1. For any matrix X and A, it holds that (I + XAX\u22a4)\u22121 = I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4.\nProof. By multiplying the right side with I +XAX\u22a4, we have\n(I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4)(I +XAX\u22a4)\n= I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4 +XAX\u22a4 \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4XAX\u22a4\n= I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4 +XAX\u22a4\n\u2212X(A\u22121 +X\u22a4X)\u22121(X\u22a4X +A\u22121 \u2212A\u22121)AX\u22a4\n= I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4 +XAX\u22a4 \u2212XAX\u22a4\n+X(A\u22121 +X\u22a4X)\u22121X\u22a4\n= I\nIt is shown that\n(I +XAX\u22a4)\u22121 = I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4.\nAccording to Lemma 1, we have\nQ\u22121X = (I \u2212X(A\u22121 +X\u22a4X)\u22121X\u22a4)X\n= X(I \u2212 (A\u22121 +G)\u22121G) = X(I \u2212 (A\u22121 +G)\u22121(A\u22121 +G\u2212A\u22121)) = X(I \u2212 I + (A\u22121 +G)\u22121A\u22121) = X(I +AG)\u22121\n(13)\nwhere A = 2\u03bb1(mI\u2212yy \u22a4)\nm2 and G = X\u22a4X is the kernel matrix. Then\nX\u22a4Q\u22121X = G(I +AG)\u22121.\nBy denoting H = Y G(I +AG)\u22121Y , the objective function of (12) can be written as\nf(\u03b1) = 1\n2\n(\n\u03bb2 m e+\u03b1\n)\u22a4\nH\n(\n\u03bb2 m e+\u03b1\n)\n\u2212 e\u22a4\u03b1\n= 1\n2 \u03b1\u22a4H\u03b1+ \u03bb2 m e\u22a4H\u03b1\u2212 e\u22a4\u03b1+ const\n= 1\n2 \u03b1\u22a4H\u03b1+\n(\n\u03bb2 m He\u2212 e\n)\u22a4\n\u03b1+ const\nNegelect the const term which doesn\u2019t have influence on the optimization, we arrive at the final formulation of the dual kernel ODML,\nmin \u03b1\n1 2 \u03b1\u22a4H\u03b1+\n(\n\u03bb2 m He\u2212 e\n)\u22a4\n\u03b1\ns.t. 0 \u2264 \u03b1 \u2264 C\nm e.\n(14)\nFor prediction, according to (10) and (13), one can obtain the coefficients w from the optimal \u03b1\u2217 as\nw = X(I +AG)\u22121Y\n(\n\u03bb2 m e+\u03b1\u2217 ) = X\u03b8,\nwhere \u03b8 = (I +AG)\u22121Y (\u03bb2 m e + \u03b1\u2217). Hence for testing instance z, its label can be obtained by\nsgn ( w\u22a4\u03c6(z) ) = sgn\n(\nm \u2211\ni=1\n\u03b8ik(xi, z)\n)\n."}, {"heading": "4.1.2. Kernel ODM", "text": "Introduce the lagrange multipliers \u03b6 \u2265 0 and \u03b2 \u2265 0 for the two constraints respectively, the Lagrangian of (7) leads to\nL(w, \u03be, \u01eb, \u03b6,\u03b2) = 1\n2 w\u22a4w + C1 m \u03be\u22a4\u03be + C2 m \u01eb\u22a4\u01eb\u2212 \u03b6\u22a4(Y X\u22a4w \u2212 (1\u2212D)e+ \u03be)\n+ \u03b2\u22a4(Y X\u22a4w \u2212 (1 +D)e\u2212 \u03be).\n(15)\nBy setting the partial derivations of w, \u03be, \u01eb to zero, we have\n\u2202L \u2202w = w \u2212XY \u03b6 +XY \u03b2 =\u21d2 w = XY (\u03b6 \u2212 \u03b2) (16)\n\u2202L \u2202\u03be = 2C1 m \u03be \u2212 \u03b6 =\u21d2 \u03be = m 2C1 \u03b6 (17) \u2202L \u2202\u01eb = 2C2 m \u01eb\u2212 \u03b2 =\u21d2 \u01eb = m 2C2 \u03b2 (18)\nBy substituting (16), (17) and (18) into (15), we have\nL(\u03b6,\u03b2) = \u2212 1\n2 (\u03b6 \u2212 \u03b2)\u22a4Y X\u22a4XY (\u03b6 \u2212 \u03b2) + C1 m\nm2\n4C21 \u03b6\u22a4\u03b6 + C2 m\nm2\n4C22 \u03b2\u22a4\u03b2\n\u2212 m\n2C1 \u03b6\u22a4\u03b6 \u2212\nm\n2C2 \u03b2\u22a4\u03b2 + (1\u2212D)\u03b6\u22a4e\u2212 (1 +D)\u03b2\u22a4e\n= \u2212 1\n2 (\u03b6 \u2212 \u03b2)\u22a4Q(\u03b6 \u2212 \u03b2)\u2212\nm\n4C1 \u03b6\u22a4\u03b6 \u2212\nm\n4C2 \u03b2\u22a4\u03b2\n+ (1\u2212D)\u03b6\u22a4e\u2212 (1 +D)\u03b2\u22a4e\nwhere Q = Y X\u22a4XY . Denote \u03b1\u22a4 = [\u03b6\u22a4,\u03b2\u22a4], then \u03b6 = [I, 0]\u03b1, \u03b2 = [0, I]\u03b1 and \u03b6 \u2212 \u03b2 = [I,\u2212I]\u03b1. The Lagrangian can be rewritten as\nL(\u03b6,\u03b2) = \u2212 1\n2 \u03b1\u22a4[I,\u2212I]\u22a4Q[I,\u2212I]\u03b1\u2212\nm\n4C1 \u03b1\u22a4[I, 0]\u22a4[I, 0]\u03b1\n\u2212 m\n4C2 \u03b1\u22a4[0, I]\u22a4[0, I]\u03b1+ (1\u2212D)e\u22a4[I, 0]\u03b1\u2212 (1 +D)e\u22a4[0, I]\u03b1\n= \u2212 1 2 \u03b1\u22a4 [ Q \u2212Q \u2212Q Q ] \u03b1\u2212 1 2 \u03b1\u22a4 [ m 2C1 I 0\n0 m 2C2 I\n]\n\u03b1+\n[\n(1\u2212D)e \u2212(1 +D)e\n]\u22a4\n\u03b1\nwhere e stands for the all-one vector. Thus the dual of (7) can be cast as:\nmin \u03b1\n1 2 \u03b1\u22a4 [ Q+ m 2C1 I \u2212Q\n\u2212Q Q+ m 2C2 I\n]\n\u03b1+\n[\n(D \u2212 1)e (D + 1)e\n]\u22a4\n\u03b1\ns.t. \u03b1 \u2265 0.\n(19)\nFor prediction, according to (16), one can obtain the coefficients w from the optimal \u03b1\u2217 as\nw = XY (\u03b6 \u2212 \u03b2) = XY [I,\u2212I]\u03b1\u2217 = X\u03b8,\nwhere \u03b8 = Y [I,\u2212I]\u03b1\u2217. Hence for testing instance z, its label can be obtained by\nsgn ( w\u22a4\u03c6(z) ) = sgn\n(\nm \u2211\ni=1\n\u03b8ik(xi, z)\n)\n."}, {"heading": "4.1.3. Dual Coordinate Descent", "text": "Note that (14) and (19) are both the special cases of the following form, which has convex quadratic objective function and simple decoupled box constraints,\nmin \u03b1\nf(\u03b1) = 1\n2 \u03b1\u22a4H\u03b1+ q\u22a4\u03b1\ns.t. 0 \u2264 \u03b1 \u2264 u.\nwhere u = \u221e for ODM. As suggested by (Yuan et al., 2012), it can be efficiently solved by the dual coordinate descent method. In dual coordinate descent method (Hsieh et al., 2008), one of the variables is selected to minimize while the other variables are kept as constants at each iteration, and a closed-form solution can be achieved at each iteration. Specifically, to minimize \u03b1i by keeping the other \u03b1j 6=i\u2019s as constants, one needs to solve the following subproblem,\nmin t f(\u03b1+ tei)\ns.t. 0 \u2264 \u03b1i + t \u2264 ui, (20)\nwhere ei denotes the vector with 1 in the i-th coordinate and 0\u2019s elsewhere. Let H = [hij ]i,j=1,...,m, we have\nf(\u03b1+ tei) = 1\n2 hiit\n2 + [\u2207f(\u03b1)]it + f(\u03b1),\nwhere [\u2207f(\u03b1)]i is the i-th component of the gradient \u2207f(\u03b1). Note that f(\u03b1) is independent of t and thus can be dropped. Considering that f(\u03b1 + tei) is a simple quadratic function of t, and further note the box constraint 0 \u2264 \u03b1i \u2264 ui, the minimizer of (20) leads to a closed-form solution,\n\u03b1newi = min\n(\nmax\n(\n\u03b1i \u2212 [\u2207f(\u03b1)]i\nhii , 0\n)\n, ui\n)\n.\nAlgorithm 1 summarizes the pseudo-code of ODMdcd for kernel ODM L and ODM.\n4.2. ODMsvrg In section 4.1.3, the proposed method can efficiently deal with kernel ODML and ODM. However, the inherent computational cost for the kernel matrix takes O(m2) time, which might be computational prohibitive for\nAlgorithm 1 ODMdcd Input: Data set X. Output: \u03b1. Initialize \u03b1 = 0, calculate H and q. while \u03b1 not converge do for i = 1, . . .m do [\u2207f(\u03b1)]i \u2190 [H\u03b1+ q]i.\n\u03b1i \u2190 min ( max ( \u03b1i \u2212 [\u2207f(\u03b1)]i hii , 0 ) , ui ) .\nend for\nend while\nlarge scale problems. To make them more useful, in the following, we present a fast linear kernel ODML and ODM for large scale problems by adopting the stochastic gradient descent with variance reduction (SVRG) (Polyak and Juditsky, 1992; Johnson and Zhang, 2013).\nFor linear kernel ODML, (5) can be reformulated as the following form,\nmin w\nfL(w) = 1\n2 w\u22a4w + \u03bb1 m w\u22a4XX\u22a4w \u2212 \u03bb1 m2 w\u22a4Xyy\u22a4X\u22a4w\n\u2212 \u03bb2 m (Xy)\u22a4w + C m\nm \u2211\ni=1\nmax{0, 1\u2212 yiw \u22a4xi},\n(21)\nwhere X = [x1, . . . ,xm], y = [y1, . . . , ym] \u22a4 is a column vector. For linear kernel ODM, (7) can be reformulated as the following form,\nmin w\nfO(w) = 1\n2 w\u22a4w + C1 m\nm \u2211\ni=1\nmax{0, 1\u2212D \u2212 yiw \u22a4xi} 2\n+ C2 m\nm \u2211\ni=1\nmax{0, yiw \u22a4xi \u2212 1\u2212D} 2.\n(22)\nFor large scale problems, computing the gradient of (21) and (22) is expensive because its computation involves all the training examples. Stochastic gradient descent (SGD) works by computing a noisy unbiased estimation of the gradient via sampling a subset of the training examples. Theoretically, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution (Kushner and Yin, 2003; Bottou, 2010). During the past decade, SGD has been applied to various\nmachine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).\nThe following theorem presents an approach to obtain an unbiased estimation of the gradient \u2207fL(w) and \u2207fO(w).\nTheorem 1. If two examples (xi, yi) and (xj, yj) are randomly sampled from the training set independently, then\n\u2207fL(w,xi,xj) = w + 2\u03bb1xix \u22a4 i w \u2212 2\u03bb1yiyjxix \u22a4 j w \u2212 \u03bb2yixi\n\u2212 CyixiI(i \u2208 I1) (23)\nand\n\u2207fO(w,xi) = w + 2C1(yiw \u22a4xi +D \u2212 1)yixiI(i \u2208 I2)\n+ 2C2(yiw \u22a4xi \u2212D \u2212 1)yixiI(i \u2208 I3)\n(24)\nare the unbiased estimation of \u2207fL(w) and \u2207fO(w) respectively, where I(\u00b7) is the indicator function that returns 1 when the argument holds, and 0 otherwise. I1, I2, I3 are the index sets defined as\nI1 \u2261 {i | yiw \u22a4xi < 1}, I2 \u2261 {i | yiw \u22a4xi < 1\u2212D}, I3 \u2261 {i | yiw \u22a4xi > 1 +D}.\nProof. Note that the gradient of fL(w) is\n\u2207fL(w) = w + 2\u03bb1 m XX\u22a4w \u2212 2\u03bb1 m2 Xyy\u22a4X\u22a4w \u2212 \u03bb2 m Xy\n\u2212 C\nm\nm \u2211\ni=1\nyixiI(i \u2208 I1).\nFurther note that\nE[xix \u22a4 i ] =\n1\nm\nm \u2211\ni=1\nxix \u22a4 i =\n1\nm XX\u22a4, E[yixi] =\n1\nm\nm \u2211\ni=1\nyixi = 1\nm Xy. (25)\nAccording to the linearity of expectation, the independence between xi and xj, and with (25), we have\nE[\u2207fL(w,xi,xj)] = w + 2\u03bb1E[xix \u22a4 i ]w \u2212 2\u03bb1E[yixi]E[yjxj] \u22a4w\n\u2212 \u03bb2E[yixi]\u2212 CE[yixiI(i \u2208 I1)]\n= w + 2\u03bb1 m XX\u22a4w \u2212 2\u03bb1 m2 Xyy\u22a4X\u22a4w\n\u2212 \u03bb2 m Xy \u2212 C m\nm \u2211\ni=1\nyixiI(i \u2208 I1)\n= \u2207fL(w)\nIt is shown that \u2207fL(w,xi,xj) is a noisy unbiased gradient of fL(w). Again the gradient of fO(w) is\n\u2207fO(w) = w + 2C1 m\nm \u2211\ni=1\n(yiw \u22a4xi +D \u2212 1)yixiI(i \u2208 I2)\n+ 2C2 m\nm \u2211\ni=1\n(yiw \u22a4xi \u2212D \u2212 1)yixiI(i \u2208 I3).\nAccording to the linearity of expectation, and with (25), we have\nE[\u2207fO(w,xi)] = w + 2C1E[(yiw \u22a4xi +D \u2212 1)yixiI(i \u2208 I2)]\n+ 2C2E[(yiw \u22a4xi \u2212D \u2212 1)yixiI(i \u2208 I3)]\n= w + 2C1 m\nm \u2211\ni=1\n(yiw \u22a4xi +D \u2212 1)yixiI(i \u2208 I2)\n+ 2C2 m\nm \u2211\ni=1\n(yiw \u22a4xi \u2212D \u2212 1)yixiI(i \u2208 I3)\n= \u2207fO(w)\nIt is shown that \u2207fO(w,xi) is a noisy unbiased gradient of fO(w).\nWith Theorem 1, the stochastic gradient update can be formed as\nwt+1 = wt \u2212 \u03b7tgt, (26)\nwhere gt = \u2207fL(wt,xi,xj) for ODM L and gt = \u2207fO(wt,xi) for ODM, \u03b7t is a suitably chosen step-size parameter in the t-th iteration.\nSince the objective function of ODM is differentiable, in practice we use the stochastic gradient descent with variance reduction (SVRG) which is more robust than SGD (Johnson and Zhang, 2013). Besides performing the normal stochastic gradient update (26) at each iteration, it also occasionally compute full gradient, which can be used to reduce the variance of the stochastic gradient estimation.\nAlgorithm 2 summarizes the pseudo-code of ODMsvrg.\nAlgorithm 2 ODMsvrg Input: Data set X. Output: w\u0304\nInitialize w\u03040 = 0. for s = 1, 2, . . . do w\u0304 = w\u0304s\u22121. Compute full gradient \u00b5\u0304 w0 = w\u0304 for t = 1, 2, . . . , m do Randomly sample training example (xi, yi). Compute gt as in (24). wt = wt\u22121 \u2212 \u03b7(\u2207fO(wt\u22121,xi)\u2212\u2207fO(w\u0304,xi) + \u00b5\u0304).\nend for\nSet w\u0304s = wt for randomly chosen t \u2208 {1, 2, . . . , m}. end for"}, {"heading": "5. Analysis", "text": "In this section, we study the statistical property of ODML and ODM. Here we only consider the linear case for simplicity, however, the results are also applicable to any other feature mapping \u03c6. As indicated in Section 4.1, the dual problem both take the following form,\nmin \u03b1\nf(\u03b1) = 1\n2 \u03b1\u22a4H\u03b1+ q\u22a4\u03b1,\ns.t. 0 \u2264 \u03b1i \u2264 ui, i = 1, . . . , m. (27)\nLemma 2. Let \u03b1 denote the optimal solution of (27), and suppose\n\u03b1\u2217 = argmin 0\u2264\u03b1\u2264u f(\u03b1), \u03b1i = argmin 0\u2264\u03b1\u2264u,\u03b1i=0 f(\u03b1), i = 1, . . . , m, (28)\nthen we have\n[H\u03b1i + q]2i 2hii \u2264 \u03b1\u2217i 2 2 hii \u2212 \u03b1 \u2217 i [H\u03b1 \u2217 + q]i,\nwhere [\u00b7]i denotes the i-th component of a vector and hii is the (i, i)-th entry of the matrix H.\nProof. According to the definition in (28), we have\nf(\u03b1i)\u2212min t f(\u03b1i + tei) \u2264 f(\u03b1 i)\u2212 f(\u03b1\u2217) \u2264 f(\u03b1\u2217 \u2212 \u03b1\u2217iei)\u2212 f(\u03b1 \u2217), (29)\nwhere ei denotes a vector with 1 in the i-th coordinate and 0\u2019s elsewhere. Note that\nf(\u03b1i)\u2212min t f(\u03b1i + tei) = f(\u03b1 i)\u2212min t\n{\nf(\u03b1i) + t2\n2 hii + t\u03b1\ni\u22a4Hei + tq \u22a4ei\n}\n= \u2212min t\n{\nt2 2 hii + t(H\u03b1 i + q)\u22a4ei\n}\n= [H\u03b1i + q]2i\n2hii\nand\nf(\u03b1\u2217 \u2212 \u03b1\u2217i ei)\u2212 f(\u03b1 \u2217) =\n\u03b1\u2217i 2\n2 hii \u2212 \u03b1\n\u2217 i (H\u03b1 \u2217 + q)\u22a4ei = \u03b1\u2217i\n2\n2 hii \u2212 \u03b1\n\u2217 i [H\u03b1 \u2217 + q]i\ncombine with (29), it is shown that\n[H\u03b1i + q]2i 2hii \u2264 \u03b1\u2217i 2 2 hii \u2212 \u03b1 \u2217 i [H\u03b1 \u2217 + q]i.\nBased Lemma 2, we derive the following two bounds for ODML and ODM on the expectation of error according to the leave-one-out cross-validation estimate, which is an unbiased estimate of the probability of test error. As shown in (Luntz and Brailovsky, 1969),\nE[R(\u03b1)] = E[L((x1, y1), . . . , (xm, ym))]\nm , (30)\nwhere L((x1, y1), . . . , (xm, ym)) is the number of errors in the leave-one-out procedure.\nTheorem 2. Let \u03b1 denote the optimal solution of the dual problem of ODML, then we have\nE[R(\u03b1)] \u2264 E[ \u2211 i\u2208I1 \u03b1\u2217ihii + |I2|]\nm , (31)\nwhere I1 \u2261 {i | 0 < \u03b1 \u2217 i < C/m}, I2 \u2261 {i | \u03b1 \u2217 i = C/m}.\nProof. According to the derivation in Section 4.1.1, for ODML we have\n[H\u03b1 + q]i =\n[\nH\u03b1+ \u03bb2 m He\u2212 e\n]\ni\n=\n[\nY X\u22a4X(I +AG)\u22121Y\n(\n\u03b1+ \u03bb2 m e\n)\n\u2212 e\n]\ni\n,\nfurther note that\nw = X(I +AG)\u22121Y\n(\n\u03b1+ \u03bb2 m e\n)\n,\nso it is shown that\n[H\u03b1+ q]i = [Y X \u22a4w \u2212 e]i = yix \u22a4 i w \u2212 1.\nSuppose the corresponding solution of \u03b1\u2217 and \u03b1i for the primal problem of ODML are w\u2217 and wi, respectively. According to Lemma 2 we have\n(yix \u22a4 i w i \u2212 1)2\n2hii \u2264\n\u03b1\u2217i 2\n2 hii \u2212 \u03b1\n\u2217 i (yix \u22a4 i w \u2217 \u2212 1).\n1) \u03b1\u2217i = 0. The right-hand side equals 0, which indicates that the lefthand side must equal 0, i.e., all these examples will be correctly classified by wi.\n2) 0 < \u03b1\u2217i < C/m. According to the complementary slackness conditions, in this case we have yix \u22a4 i w\n\u2217 = 1. For any misclassified example (xi, yi), i.e., yix \u22a4 i w\ni < 0, we have 1 \u2264 \u03b1\u2217ihii. 3) \u03b1\u2217i = C/m. All these examples may be misclassified in the leave-one-\nout procedure. So we have\nL((x1, y1), . . . , (xm, ym)) \u2264 \u2211\ni\u2208I1\n\u03b1\u2217ihii + |I2|,\nwhere I1 \u2261 {i | 0 < \u03b1 \u2217 i < C/m}, I2 \u2261 {i | \u03b1 \u2217 i = C/m}. Take expectation on both side and with (30), we get that (31) holds.\nTheorem 3. Let \u03b1 denote the optimal solution of the dual problem of ODM, then we have\nE[R(\u03b1)] \u2264 E[ \u2211 i\u2208I1 \u03b1\u2217i ( \u2016xi\u2016 2 + m 2C1 ) + \u2211 i\u2208I2 \u03b1\u2217i ( \u2016xi\u2016 2 + m 2C2 ) +D(|I1| \u2212 |I2|)]\nm ,\n(32)\nwhere I1 \u2261 {i | \u03b1 \u2217 i > 0 and 1 \u2264 i \u2264 m}, I2 \u2261 {i | \u03b1 \u2217 i > 0 and m + 1 \u2264 i \u2264 2m}.\nProof. Denote \u03b1\u22a4 = [\u03b6\u22a4,\u03b2\u22a4], according to the derivation in Section 4.1.2 we have\n[H\u03b1+ q]i =\n[\nQ(\u03b6 \u2212 \u03b2) + 2C1 m\n\u03b6 + (D \u2212 1)e \u2212Q(\u03b6 \u2212 \u03b2) + 2C2\nm \u03b2 + (D + 1)e\n]\ni\n=\n[\nY X\u22a4XY (\u03b6 \u2212 \u03b2) + 2C1 m\n\u03b6 + (D \u2212 1)e \u2212Y X\u22a4XY (\u03b6 \u2212 \u03b2) + 2C2\nm \u03b2 + (D + 1)e\n]\ni\n,\nfurther note that w = XY (\u03b6 \u2212 \u03b2) and the definition in (28), so it is shown that\n[H\u03b1i + q]i =\n{\nyix \u22a4 i w i \u2212 (1\u2212D), 1 \u2264 i \u2264 m. \u2212yix \u22a4 i w i + (1 +D), m+ 1 \u2264 i \u2264 2m.\nand\n[H\u03b1\u2217 + q]i =\n{\nyix \u22a4 i w \u2217 + \u03bei \u2212 (1\u2212D), 1 \u2264 i \u2264 m. \u2212yix \u22a4 i w \u2217 + \u01ebi + (1 +D), m+ 1 \u2264 i \u2264 2m.\naccording to Lemma 2 we have\n((1\u2212D)\u2212 yix \u22a4 i w i)2\n2(\u2016xi\u20162 + m 2C1\n) \u2264\n\u03b1\u2217i 2\n2\n(\n\u2016xi\u2016 2 +\nm\n2C1\n)\n\u2212 \u03b1\u2217i (yix \u22a4 i w \u2217 + \u03bei \u2212 (1\u2212D)), 1 \u2264 i \u2264 m.\n((1 +D)\u2212 yix \u22a4 i w i)2\n2(\u2016xi\u20162 + m 2C2\n) \u2264\n\u03b1\u2217i 2\n2\n(\n\u2016xi\u2016 2 +\nm\n2C2\n)\n\u2212 \u03b1\u2217i (\u2212yix \u22a4 i w \u2217 + \u01ebi + (1 +D)), m+ 1 \u2264 i \u2264 2m.\n1) \u03b1\u2217i = 0. The right-hand side equals 0, which indicates that the lefthand side must equal 0, i.e., all these examples will be correctly classified by wi.\n2) \u03b1\u2217i > 0. According to the complementary slackness conditions, in this case the second term of the right-hand side must equal 0. For any misclassified example (xi, yi), i.e., yix \u22a4 i w i < 0, we have\n1 \u2264 \u03b1\u2217i\n(\n\u2016xi\u2016 2 +\nm\n2C1\n)\n+D, 1 \u2264 i \u2264 m.\n1 \u2264 \u03b1\u2217i\n(\n\u2016xi\u2016 2 +\nm\n2C2\n)\n\u2212D, m+ 1 \u2264 i \u2264 2m.\nSo we have\nL((x1, y1), . . . , (xm, ym)) \u2264 \u2211\ni\u2208I1\n\u03b1\u2217i\n(\n\u2016xi\u2016 2 +\nm\n2C1\n)\n+ \u2211\ni\u2208I2\n\u03b1\u2217i\n(\n\u2016xi\u2016 2 +\nm\n2C2\n)\n+D(|I1| \u2212 |I2|),\nwhere I1 \u2261 {i | \u03b1 \u2217 i > 0 and 1 \u2264 i \u2264 m}, I2 \u2261 {i | \u03b1 \u2217 i > 0 and m+1 \u2264 i \u2264 2m}. Take expectation on both side and with (30), we get that (32) holds."}, {"heading": "6. Empirical Study", "text": "In this section, we empirically evaluate the effectiveness of our methods on a broad range of data sets. We first introduce the experimental settings in Section 6.1, and then compare ODML and ODM with SVM and Linear Discriminant Analysis (LDA) in Section 6.2 and Section 6.3. In addition, we also study the cumulative margin distribution produced by ODML, ODM and SVM in Section 6.4. The computational cost is presented in Section 6.5."}, {"heading": "6.1. Experimental Setup", "text": "We evaluate the effectiveness of our proposed methods on thirty two regular scale data sets and ten large scale data sets, including both UCI data sets and real-world data sets like KDD20101. Table 1 summarizes the statistics of these data sets. The data set size is ranged from 62 to more than 8,000,000, and the dimensionality is ranged from 2 to more than 20,000,000, covering a broad range of properties. All features are normalized into the interval [0, 1]. For each data set, half of examples are randomly selected as the training data, and the remaining examples are used as the testing data. For regular scale data sets, both linear and RBF kernels are evaluated. Experiments are repeated for 30 times with random data partitions, and the average accuracies as well as the standard deviations are recorded. For large scale data sets, linear kernel is evaluated. Experiments are repeated for 10 times with random data partitions, and the average accuracies (with standard deviations) are recorded.\n1https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp\nODML and ODM are compared with standard SVM which ignores the margin distribution, and Linear Discriminant Analysis (LDA) (Fisher, 1936).\nFor SVM and ODML, the regularization parameter C is selected by 5-fold cross validation from [10, 50, 100]. In addition, the regularization parameters\n\u03bb1, \u03bb2 are selected from the set of [2 \u22128, . . . , 2\u22122]. For ODM, the regularization parameter C1 and C2 are selected from the set of [2 0, . . . , 210], while the parameter D is selected from the set of [0, 0.1, . . . , 0.5]. The parameters \u03b7 used for ODMsvrg are set with the same setup in (Johnson and Zhang, 2013). The width of the RBF kernel for SVM, LDA, ODML and ODM are selected by 5-fold cross validation from the set of [2\u22122\u03b4, . . . , 22\u03b4], where \u03b4 is the average distance between instances. All selections are performed on training sets."}, {"heading": "6.2. Results on Regular Scale Data Sets", "text": "Tables 2 and 3 summarize the results on thirty two regular scale data sets. As can be seen, the overall performance of our methods are superior or highly competitive to SVM. Specifically, for linear kernel, ODML/ODM performs significantly better than SVM on 17/24 over 32 data sets, respectively, and achieves the best accuracy on 31 data sets; for RBF kernel, ODML/ODM performs significantly better than SVM on 15/25 over 32 data sets, respectively, and achieves the best accuracy on 31 data sets. In addition, as can be seen, in comparing with standard SVM which does not consider margin distribution, the win/tie/loss counts show that ODML and ODM are always better or comparable, never worse than SVM."}, {"heading": "6.3. Results on Large Scale Data Sets", "text": "Table 4 summarizes the results on ten large scale data sets. LDA did not return results on some data sets due to the high computational cost. As can be seen, the overall performance of our methods are superior or highly competitive to SVM. Specifically, ODML/ODM performs significantly better than SVM on 6/7 over 10 data sets, respectively, and achieves the best accuracy on almost all data sets.\n6.4. Margin Distributions\nFigure 1 plots the cumulative margin distribution of SVM, ODML and ODM on some representative regular scale data sets. The curves for other data sets are more or less similar. The point where a curve and the x-axis\ncrosses is the corresponding minimum margin. As can be seen, our methods usually have a little bit smaller minimum margin than SVM, whereas the curve of ODML and ODM generally lies on the right side, showing that the margin distribution of ODML and ODM are generally better than that of SVM. In other words, for most examples, our methods generally produce a larger margin than SVM."}, {"heading": "6.5. Time Cost", "text": "We compare the time cost of our methods and SVM on the ten large scale data sets. All the experiments are performed with MATLAB 2012b on a machine with 8\u00d72.60 GHz CPUs and 32GB main memory. The average CPU time (in seconds) on each data set is shown in Figure 2. We denote SVM implemented by the LIBLINEAR (Fan et al., 2008) package as SVMl and SVM implemented by SGD2 as SVMs, respectively. It can be seen that, both SVMs and our methods are faster than SVMl, owing to the use of SGD. ODML and ODM are just slightly slower than SVMs on three data sets (adult-a, w8a and mini-boo-ne) but highly competitive with SVMs on the rest data sets. Note that both SVMl and SVMs are very fast implementations of SVMs; this shows that ODML and ODM are also computationally efficient.\n2http://leon.bottou.org/projects/sgd"}, {"heading": "7. Related Work", "text": "There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008). Garg et al. (Garg and Roth, 2003) proposed the Margin Distribution Optimization (MDO) algorithm which minimizes the sum of the cost of each instance, where the cost is a function which assigns larger values to instances with smaller margins. MDO can be viewed as a method of optimizing weighted margin combination, where the weights are related to the margins. The objective function optimized by MDO, however, is non-convex, and thus, it may get stuck in local minima. In addition, MDO can only be used for linear kernel.\nPelckmans et al. (Pelckmans et al., 2008) proposed the Maximal Average Margin for Classifiers (MAMC) and it can be viewed as a special case of ODML assuming that the margin variance is zero. MAMC has a closed-form solution, however, it will degenerate to a trivial solution when the classes are not with equal sizes.\nAiolli et al. (Aiolli et al., 2008) proposed a Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) from a game theoretical perspective. Similar to MDO, this method also directly optimizes a weighted combination of margins over the training data, ignoring the influence of margin variances. Besides, this method considers hard-margin only, which may be another reason why it can\u2019t work well. It is noteworthy that the computational cost prohibits KM-OMD to be applied to large scale data.\nThe superiority of ODML to the above methods have been presented in (Zhang and Zhou, 2014), so we don\u2019t choose them as compared methods in Section 6."}, {"heading": "8. Conclusions", "text": "Support vector machines work by maximizing the minimum margin. Recent theoretical results suggested that the margin distribution, rather than a single-point margin such as the minimum margin, is more crucial to the generalization performance. In this paper, we propose a new method, named Optimal margin Distribution Machine (ODM), which try to optimize the margin distribution by considering the margin mean and the margin variance simultaneously. Our method is a general learning approach which can be used in any place where SVM can be applied. Comprehensive experiments\non thirty two regular scale data sets and ten large scale data sets validate the superiority of our method to SVM. In the future it will be interesting to generalize the idea of optimal margin distribution to other learning settings."}], "references": [{"title": "A kernel method for the optimization of the margin distribution", "author": ["F. Aiolli", "G. San Martino", "A. Sperduti"], "venue": "Proceedings of the 18th International Conference on Artificial Neural Networks. Prague,", "citeRegEx": "Aiolli et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Aiolli et al\\.", "year": 2008}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A. Bordes", "L. Bottou", "P. Gallinari"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Bordes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2009}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of the 19th International Conference on Computational Statistics. Paris,", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "Prediction games and arcing classifiers", "author": ["L. Breiman"], "venue": "Neural Computation", "citeRegEx": "Breiman,? \\Q1999\\E", "shortCiteRegEx": "Breiman", "year": 1999}, {"title": "Learning optimally sparse support vector machines", "author": ["A. Cotter", "S. Shalev-shwartz", "N. Srebro"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Cotter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2013}, {"title": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2000\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2000}, {"title": "Convex formulations of radius-margin based support vector machines", "author": ["H. Do", "K. Alexandre"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Do and Alexandre,? \\Q2013\\E", "shortCiteRegEx": "Do and Alexandre", "year": 2013}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research 9,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "The use of multiple measurements in taxonomic", "author": ["R.A. Fisher"], "venue": null, "citeRegEx": "Fisher,? \\Q1936\\E", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "On the doubt about margin explanation", "author": ["W. pp. 23\u201337. Gao", "Zhou", "Z.-H"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Margin distribution and learning", "author": ["A. Garg", "D. Roth"], "venue": "boosting. Artificial Intelligence", "citeRegEx": "Garg and Roth,? \\Q2003\\E", "shortCiteRegEx": "Garg and Roth", "year": 2003}, {"title": "Accelerating stochastic gradient descent using", "author": ["R. Johnson", "T. Zhang"], "venue": "Helsinki, Finland,", "citeRegEx": "Johnson and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2013}, {"title": "Stochastic approximation and recursive", "author": ["H.J. Kushner", "G.G. Yin"], "venue": "International Conference on Machine Learning. Atlanta,", "citeRegEx": "Kushner and Yin,? \\Q2003\\E", "shortCiteRegEx": "Kushner and Yin", "year": 2003}, {"title": "On estimation of characters", "author": ["A. pp. 53\u201361. Luntz", "V. Brailovsky"], "venue": null, "citeRegEx": "Luntz and Brailovsky,? \\Q1969\\E", "shortCiteRegEx": "Luntz and Brailovsky", "year": 1969}, {"title": "A risk minimization principle for a class of parzen estimators", "author": ["K. Pelckmans", "J. Suykens", "B.D. Moor"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pelckmans et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pelckmans et al\\.", "year": 2008}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization", "citeRegEx": "Polyak and Juditsky,? \\Q1992\\E", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["S.J. Reddi", "A. Hefny", "S. Sra", "B. Poczos", "A. Smola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Reddi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reddi et al\\.", "year": 2015}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["L. Reyzin", "R.E. Schapire"], "venue": "Proceedings of 23rd International Conference on Machine Learning", "citeRegEx": "Reyzin and Schapire,? \\Q2006\\E", "shortCiteRegEx": "Reyzin and Schapire", "year": 2006}, {"title": "Boosting the margin: a new explanation for the effectives of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P.L. Bartlett", "W.S. Lee"], "venue": "Annuals of Statistics", "citeRegEx": "Schapire et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1998}, {"title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2001}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "Proceedings of the 24th International Conference on Machine Learning. Helsinki,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes", "author": ["O. Shamir", "T. Zhang"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Shamir and Zhang,? \\Q2013\\E", "shortCiteRegEx": "Shamir and Zhang", "year": 2013}, {"title": "Mini-batch primal and dual methods for svms", "author": ["M. Takac", "A. Bijral", "P. Richtarik", "N. Srebro"], "venue": "Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Takac et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Takac et al\\.", "year": 2013}, {"title": "The Nature of Statistical Learning Theory. Springer-Verlag, New York", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "A refined margin analysis for boosting algorithms via equilibrium margin", "author": ["L.W. Wang", "M. Sugiyama", "C. Yang", "Zhou", "Z.-H", "J. Feng"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent. CoRR, abs/1107.2490", "author": ["W. Xu"], "venue": null, "citeRegEx": "Xu,? \\Q2011\\E", "shortCiteRegEx": "Xu", "year": 2011}, {"title": "Recent advances of large-scale linear classification", "author": ["G.X. Yuan", "C.H. Ho", "C.J. Lin"], "venue": "Proceedings of the IEEE", "citeRegEx": "Yuan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the 21st International Conference on Machine learning. Banff,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Large margin distribution machine", "author": ["T. Zhang", "Zhou", "Z.-H"], "venue": "Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Stochastic optimization with importance sampling for regularized loss minimization", "author": ["Zhao", "P.-L", "T. Zhang"], "venue": "Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Zhou", "Z.-H"], "venue": null, "citeRegEx": "Zhou and Z..H.,? \\Q2012\\E", "shortCiteRegEx": "Zhou and Z..H.", "year": 2012}, {"title": "Large margin distribution learning", "author": ["Zhou", "Z.-H"], "venue": "Proceedings of the 6th IAPR International Workshop on Artificial Neural Networks in Pattern Recognition (ANNPR\u201914). Montreal, Canada,", "citeRegEx": "Zhou and Z..H.,? \\Q2014\\E", "shortCiteRegEx": "Zhou and Z..H.", "year": 2014}], "referenceMentions": [{"referenceID": 23, "context": "Introduction Support Vector Machine (SVM) (Cortes and Vapnik, 1995; Vapnik, 1995) has always been one of the most successful learning algorithms.", "startOffset": 42, "endOffset": 81}, {"referenceID": 23, "context": ", the smallest distance from the examples to the classification boundary, and the margin theory (Vapnik, 1995) provided a good support to the generalization performance of SVMs.", "startOffset": 96, "endOffset": 110}, {"referenceID": 18, "context": "(Schapire et al., 1998) first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting; soon after, Breiman (Breiman, 1999) indicated that the minimum margin is crucial and developed a boosting-style algorithm, named Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": ", 1998) first suggested margin theory to explain the phenomenon that AdaBoost seems resistant to overfitting; soon after, Breiman (Breiman, 1999) indicated that the minimum margin is crucial and developed a boosting-style algorithm, named Arc-gv, which is able to maximize the minimum margin but with a poor generalization performance.", "startOffset": 130, "endOffset": 145}, {"referenceID": 17, "context": "(Reyzin and Schapire, 2006) found that although Arc-gv tends to produce larger minimum margin, it suffers from a poor margin distribution; they conjectured that the margin distribution, rather than the minimum margin, is more crucial to the generalization performance.", "startOffset": 0, "endOffset": 27}, {"referenceID": 24, "context": "Such a conjecture has been theoretically studied (Wang et al., 2011; Gao and Zhou, 2013), and it was recently proven by Gao and Zhou (Gao and Zhou, 2013).", "startOffset": 49, "endOffset": 88}, {"referenceID": 23, "context": "According to (Cortes and Vapnik, 1995; Vapnik, 1995), the margin of instance (xi, yi) is formulated as \u03b3i = yiw \u03c6(xi), \u2200i = 1, .", "startOffset": 13, "endOffset": 52}, {"referenceID": 5, "context": "From (Cristianini and Shawe-Taylor, 2000), it is shown that SVM with hardmargin (or Hard-margin SVM) is regarded as the maximization of the minimum distance, max w \u03b3\u0302", "startOffset": 5, "endOffset": 41}, {"referenceID": 19, "context": "According to the representer theorem (Sch\u00f6lkopf and Smola, 2001), the optimal solution will be spanned by the support vectors.", "startOffset": 37, "endOffset": 64}, {"referenceID": 26, "context": "As suggested by (Yuan et al., 2012), it can be efficiently solved by the dual coordinate descent method.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "To make them more useful, in the following, we present a fast linear kernel ODM and ODM for large scale problems by adopting the stochastic gradient descent with variance reduction (SVRG) (Polyak and Juditsky, 1992; Johnson and Zhang, 2013).", "startOffset": 188, "endOffset": 240}, {"referenceID": 11, "context": "To make them more useful, in the following, we present a fast linear kernel ODM and ODM for large scale problems by adopting the stochastic gradient descent with variance reduction (SVRG) (Polyak and Juditsky, 1992; Johnson and Zhang, 2013).", "startOffset": 188, "endOffset": 240}, {"referenceID": 12, "context": "Theoretically, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution (Kushner and Yin, 2003; Bottou, 2010).", "startOffset": 127, "endOffset": 164}, {"referenceID": 2, "context": "Theoretically, when the objective is convex, it can be shown that in expectation, SGD converges to the global optimal solution (Kushner and Yin, 2003; Bottou, 2010).", "startOffset": 127, "endOffset": 164}, {"referenceID": 27, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 20, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 1, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 21, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 11, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 16, "context": "machine learning problems and achieved promising performances (Zhang, 2004; Shalev-Shwartz et al., 2007; Bordes et al., 2009; Shamir and Zhang, 2013; Johnson and Zhang, 2013; Reddi et al., 2015; Zhao and Zhang, 2015).", "startOffset": 62, "endOffset": 216}, {"referenceID": 11, "context": "Since the objective function of ODM is differentiable, in practice we use the stochastic gradient descent with variance reduction (SVRG) which is more robust than SGD (Johnson and Zhang, 2013).", "startOffset": 167, "endOffset": 192}, {"referenceID": 13, "context": "As shown in (Luntz and Brailovsky, 1969),", "startOffset": 12, "endOffset": 40}, {"referenceID": 8, "context": "ODM and ODM are compared with standard SVM which ignores the margin distribution, and Linear Discriminant Analysis (LDA) (Fisher, 1936).", "startOffset": 121, "endOffset": 135}, {"referenceID": 11, "context": "The parameters \u03b7 used for ODMsvrg are set with the same setup in (Johnson and Zhang, 2013).", "startOffset": 65, "endOffset": 90}, {"referenceID": 7, "context": "We denote SVM implemented by the LIBLINEAR (Fan et al., 2008) package as SVMl and SVM implemented by SGD as SVMs, respectively.", "startOffset": 43, "endOffset": 61}, {"referenceID": 10, "context": "Related Work There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).", "startOffset": 91, "endOffset": 157}, {"referenceID": 14, "context": "Related Work There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).", "startOffset": 91, "endOffset": 157}, {"referenceID": 0, "context": "Related Work There are a few studies considered margin distribution in SVM-like algorithms (Garg and Roth, 2003; Pelckmans et al., 2008; Aiolli et al., 2008).", "startOffset": 91, "endOffset": 157}, {"referenceID": 10, "context": "(Garg and Roth, 2003) proposed the Margin Distribution Optimization (MDO) algorithm which minimizes the sum of the cost of each instance, where the cost is a function which assigns larger values to instances with smaller margins.", "startOffset": 0, "endOffset": 21}, {"referenceID": 14, "context": "(Pelckmans et al., 2008) proposed the Maximal Average Margin for Classifiers (MAMC) and it can be viewed as a special case of ODM assuming that the margin variance is zero.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "(Aiolli et al., 2008) proposed a Kernel Method for the direct Optimization of the Margin Distribution (KM-OMD) from a game theoretical perspective.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. Based on this idea, we propose a new method, named Optimal margin Distribution Machine (ODM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the firstand second-order statistics, i.e., the margin mean and variance. The proposed method is a general learning approach which can be used in any place where SVM can be applied, and their superiority is verified both theoretically and empirically in this paper.", "creator": "LaTeX with hyperref package"}}}