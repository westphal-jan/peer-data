{"id": "1604.03640", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex", "abstract": "we informally discuss relations between residual immune networks ( resnet ), recurrent neural networks ( rnns ) and the primate visual cortex. we begin with the observation regarding that a shallow rnn is measured exactly equivalent to a very deep resnet with weight sharing among the layers. a direct implementation of such a rnn, although while having orders of magnitude fewer parameters, leads to a performance similar to the corresponding resnet. we propose 1 ) a precise generalization of both rnn and resnet architectures and 2 ) the conjecture that choosing a class of moderately deep rnns is a biologically - plausible derived model of the ventral stream in visual cortex. we must demonstrate the effectiveness of the architectures by testing them on the ieee cifar - 10 dataset.", "histories": [["v1", "Wed, 13 Apr 2016 02:59:34 GMT  (1036kb,D)", "http://arxiv.org/abs/1604.03640v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["qianli liao", "tomaso poggio"], "accepted": false, "id": "1604.03640"}, "pdf": {"name": "1604.03640.pdf", "metadata": {"source": "CRF", "title": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex", "authors": ["Qianli Liao"], "emails": [], "sections": [{"heading": null, "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF - 1231216.\nar X\niv :1\n60 4."}, {"heading": "1 Introduction", "text": "Residual learning [8], a novel deep learning scheme characterized by ultra-deep architectures has recently achieved state-of-the-art performance on several popular vision benchmarks. The most recent incarnation of this idea [10] with hundreds of layers demonstrate consistent performance improvement over shallower networks. The 3.57% top-5 error achieved by residual networks on the ImageNet test set arguably rivals human performance.\nBecause of recent claims [33] that networks of the AlexNet[16] type successfully predict properties of neurons in visual cortex, one natural question arises: how similar is an ultra-deep residual network to the primate cortex? A notable difference is the depth. While a residual network has as many as 1202 layers[8], biological systems seem to have two orders of magnitude less, if we make the customary assumption that a layer in the NN architecture corresponds to a cortical area. In fact, there are about half a dozen areas in the ventral stream of visual cortex from the retina to the Inferior Temporal cortex. Notice that it takes in the order of 10ms for neural activity to propagate from one area to another one (remember that spiking activity of cortical neurons is usually well below 100 Hz). The evolutionary advantage of having fewer layers is apparent: it supports rapid (100msec from image onset to meaningful information in IT neural population) visual recognition, which is a key ability of human and non-human primates [31, 28].\nIt is intriguingly possible to account for this discrepancy by taking into account recurrent connections within each visual area. Areas in visual cortex comprise six different layers with lateral and feedback connections [17], which are believed to mediate some attentional effects [2, 17, 14, 25, 12] and even learning (such as backpropagation [20]). \u201cUnrolling\u201d in time the recurrent computations carried out by the visual cortex provides an equivalent \u201cultra-deep\u201d feedforward network, which might represent a more appropriate comparison with the state-of-the-art computer vision models.\nIn addition, we conjecture that the effectiveness of recent \u201cultra-deep\u201d neural networks primarily come from the fact they can efficiently model the recurrent computations that are required by the recognition task. We show compelling evidences for this conjecture by demonstrating that 1. a deep residual network is formally equivalent to a shallow RNN; 2. such a RNN with weight sharing, thus with orders of magnitude less parameters (depending on the unrolling depth), can retain most of the performance of the corresponding deep residual network.\nFurthermore, we generalize such a RNN into a class of models that are more biologically-plausible models of cortex and show their effectiveness on CIFAR-10."}, {"heading": "2 Equivalence of ResNet and RNN", "text": ""}, {"heading": "2.1 Intuition", "text": "We discuss here a very simple observation: a Residual Network (ResNet) approximates a specific, standard Recurrent Neural Network (RNN) implementing the discrete dynamical system described by\nht = K \u25e6 (ht\u22121) + ht\u22121 (1)\nwhere ht is the activity of the neural layer at time t and K is a nonlinear operator. Such a dynamical systems corresponds to the feedback system of Figure 1 (B). Figure 1 (A) shows that unrolling in (discrete) time the feedback system gives a deep residual network with the same (that is, shared) weights among the layers. The number of layers in the unrolled network corresponds to the discrete time iterations of the dynamical system. The identity shortcut mapping that characterizes residual learning appears in the figure.\nThus, ResNets with shared weights can be reformulated into the form of a recurrent system. In section 5.2) we show experimentally that a ResNet with shared weights retains most of its performance (on CIFAR-10).\nA comparison of a plain RNN and ResNet with shared weights is in the Appendix Figure 11."}, {"heading": "2.2 Formulation in terms of Dynamical Systems (and Feedback)", "text": "We frame recurrent and residual neural networks in the language of dynamical systems. We consider here dynamical systems in discrete time, though most of the definitions carry over to continuous time. A neural network (that we assume for simplicity to have a single layer with n neurons) can be a dynamical system with a dynamics defined as\nht+1 = f(ht;wt) + xt (2)\nwhere ht \u2208 Rn is the activity of the n neurons in the layer at time t and f : Rn \u2192 Rn is a continuous, bounded function parametrized by the vector of weights wt. In a typical neural network, f is synthesized by the following relation between the activity yt of a single neuron and its inputs xt\u22121:\nyt = \u03c3(\u3008w, xt\u22121\u3009+ b), (3)\nwhere \u03c3 is a nonlinear function such as the linear rectifier \u03c3(\u00b7) = | \u00b7 |+.\nA standard classification of dynamical systems defines the system as\n1. homogeneous if xt = 0, \u2200t > 0 (alternatively the equation reads as ht+1 = f(ht;wt) with the inital condition h0 = x0)\n2. time invariant if wt = w.\nResidual networks with weight sharing thus correspond to homogeneous, time-invariant systems which in turn correspond to a feedback system (see Figure 1) with an input which is non-zero only at time t = 0 (xt=0 = x0, xt = 0 \u2200t > 0) and with f(z) = (K + I) \u25e6 z:\nhn = f(ht;wt) = (K + I) n \u25e6 x0 (4)\n\u201cNormal\u201d residual networks correspond to homogeneous, time-variant systems. An analysis of the corresponding inhomogeneous, time-invariant system is provided in the Appendix."}, {"heading": "3 A Generalized RNN for Multi-stage Fully Recurrent Processing", "text": "As shown in the previous section, the recurrent form of a ResNet is actually shallow (if we ignore the possible depth of the operator K). In this section, we generalize it into a moderately deep RNN that reflects the multi-stage processing in the primate visual cortex."}, {"heading": "3.1 Multi-state Graph", "text": "We propose a general formulation that can capture the computations performed by a multi-stage processing hierarchy with full recurrent connections. Such a hierarchy can be characterized by a directed (cyclic) graph G with vertices V and edges E.\nG = {V,E} (5)\nwhere vertices V is a set contains all the processing stages (i.e., we also call them states). Take the ventral stream of visual cortex for example, V = {LGN, V 1, V 2, V 4, IT}. Note that retina is not listed since there is no known feedback from primate cortex to the retina. The edges E are a set that contains all the connections (i.e., transition functions) between all vertices/states, e.g., V1-V2, V1-V4, V2-IT, etc. One example of such a graph is in Figure 2 (A)."}, {"heading": "3.2 Pre-net and Post-net", "text": "The multi-state fully recurrent system does not have to receive raw inputs. Rather, a (deep) neural network can serve as a preprocesser. We call the preprocesser a \u201cpre-net\u201d as shown in Figure 2 (B). On the other hand, one also needs a \u201cpost-net\u201d as a postprocessor and provide supervisory signals to the recurrent system and the pre-net. The pre-net, recurrent system and post-net are trained in an end-to-end fashion with backpropagation.\nFor most models in this paper, unless stated otherwise, the pre-net is a simple 3x3 convolutional layer and the post-net is a pipeline of a batch normalization, a ReLU, a global average pooling and a fully connected layer (or a 1x1 convolution, we use these terms interchangeably).\nTake primate visual system for instance, the retina is a part of the \u201cpre-net\u201d. It does not receive any feedback from the cortex and thus can be separated from the recurrent system for simplicity. In Section 5.3.3, we also tried 3 layers of 3x3 convolutions as an pre-net, which might be more similar to a retina, and observed slightly better performance."}, {"heading": "3.3 Transition Matrix", "text": "The set of edges E can be represented as a 2-D matrix where each element (i,j) represents the transition function from state i to state j.\nOne can also extend the representation of E to a 3-D matrix, where the third dimension is time and each element (i,j,t) represents the transition function from state i to state j at time t. In this formulation, the transition functions\ncan vary over time (e.g., being blocked from time t1 to time t2, etc.). The increased expressive power of this formulation allows us to design a system where multiple locally recurrent systems are connected sequentially: a downstream recurrent system only receives inputs when its upstream recurrent system finishes, similar to recurrent convolutional neural networks (e.g., [19]). This system with non-shared weights can also represent exactly the state-of-the-art ResNet (see Figure 3).\nNevertheless, time-dependent dynamical systems, that is recurrent networks of real neurons and synapses, offer interesting questions about the flexibility in controlling their time dependent parameters.\nExample transition matrices used in this paper are shown in Figure 4.\nWhen there are multiple transition functions to a state, their outputs are summed together."}, {"heading": "3.4 Shared vs. Non-shared Weights", "text": "Weight sharing is described at the level of an unrolled network. Thus, it is possible to have unshared weights with a 2D transition matrix \u2014 even if the transitions are stable over time, their weights could be time-variant.\nGiven an unrolled network, a weight sharing configuration can be described as a set S, whose element is a set of tied\nweights s = {Wi1,j1,t1 , ...,Wim,jm,tm}, where Wim,jm,tm denotes the weight of the transition functions from state im to jm at time tm. This requires: 1. all weights Wim,jm,tm \u2208 s have the same initial values. 2. the actual gradients used for updating each element of s is the sum of the gradients of all elements in s:\n\u2200W \u2208 s, ( \u2202E \u2202W )used = \u2211 W \u2032\u2208s ( \u2202E \u2202W \u2032 )original (6)\nwhere E is the training objective.\nFor RNNs, weights are usually shared across time, but one could unshare the weights, share across states or perform more complicated sharing using this framework."}, {"heading": "3.5 Notations: Unrolling Depth vs. Readout Time", "text": "The meaning of \u201cunrolling depth\u201d may vary in different RNN models since \u201cunrolling\u201d a cyclic graph is not well defined. In this paper, we adopt a biologically-plausible definition: we simulate the time after the onset of the visual stimuli assuming each transition function takes constant time 1. We use the term \u201creadout time\u201d to refer to the time the post-net reads the data from the last state.\nThis definition in principle allows one to have quantitive comparisons with biological systems. e.g., for a model with readout time t in this paper, the wall clock time can be estimated to be 20t to 50t ms, considering the latency of a single layer of biological neurons.\nRegarding the initial values, at t = 0 all states are empty except that the first state has some data received from the\npre-net. We only start simulate a transition function when its input state is populated."}, {"heading": "3.6 Sequential vs. Static Inputs/Outputs", "text": "As an RNN, our model supports sequential data processing and in principle all other tasks supported by traditional RNNs. See Figure 5 for illustrations. However, if there is a batch normalization in the model, we have to use \u201ctime-specific normalization\u201d described in Section 3.7, which might not be feasible for some tasks."}, {"heading": "3.7 Batch Normalizations for RNNs", "text": "As an additional observation, we found that it generally hurts performance when the normalization statistics (e.g., average, standard deviation, learnable scaling and shifting parameters) in batch normalization are shared across time. This may be consistent with the observations from [18].\nHowever, good performance is restored if we apply a procedure we call a \u201ctime-specific normalization\u201d: mean and standard deviation are calculated independently for every t (using training set). The learnable scaling and shifting parameters should be time-specific. But in most models we do not use the learnable parameters of BN since they tend not to affect the performance much.\nWe expect this procedure to benefit other RNNs trained with batch normalization. However, to use this procedure, one needs to have a initial t = 0 and enumerate all possible ts. This is feasible for visual processing but needs modifications for other tasks."}, {"heading": "4 Related Work", "text": "Deep Recurrent Neural Networks: Our final model is deep and similar to a stacked RNN [27, 5, 7] with several main differences: 1. our model has feedback transitions between hidden layers and self-transition from each hidden layer to itself. 2. our model has identity shortcut mappings inspired by residual learning. 3. our transition functions are deep and convolutional.\nAs suggested by [23], the term depth in RNN could also refer to input-to-hidden, hidden-to-hidden or hidden-to-output connections. Our model is deep in all of these senses. See Section 3.2.\nRecursive Neural Networks and Convolutional Recurrent Neural Networks: When unfolding RNN into a feedforward network, the weights of many layers are tied. This is reminiscent of Recursive Neural Networks (Recursive NN), first proposed by [29]. Recursive NN are characterized by applying same operations recursively on a structure. The convolutional version was first studied by [4]. Subsequent related work includes [24] and [19]. One characteristic distinguishes our model and residual learning from Recursive NN and convolutional recurrent NN is whether there are identity shortcut mappings. This discrepancy seems to account for the superior performance of residual learning and of our model over the latters.\nA recent report [3] we became aware of after we finished this work discusses the idea of imitating cortical feedback by introducing loops into neural networks.\nA Highway Network [30] is a feedforward network inspired by Long Short Term Memory [11] featuring more general shortcut mappings (instead of hardwired identity mappings used by ResNet)."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Dataset and training details", "text": "We test all models on the standard CIFAR-10 [15] dataset. All images are 32x32 pixels with color. Data augmentation is performed in the same way as [8].\nMomentum was used with hyperparameter 0.9. Experiments were run for 60 epochs with batchsize 64 unless stated otherwise. The learning rates are 0.01 for the first 40 epochs, 0.001 for epoch 41 to 50 and 0.0001 for the last 10 epochs. All experiments used the cross-entropy loss function and softmax for classification. Batch Normalization (BN) [13] is used for all experiments. But the learnable scaling and shifting parameters are not used (except for the last BN layer in the post-net). Network weights were initialized with the method described in [9]. Although we do not expect the initialization to matter as long as batch normalization is used. The implementations are based on MatConvNet[32]."}, {"heading": "5.2 Experiment A: ResNet with shared weights", "text": ""}, {"heading": "5.2.1 Sharing Weights Across Time", "text": "We conjecture that the effectiveness of ResNet mainly comes from the fact that it efficiently models the recurrent computations required by the recognition task. If this is the case, one should be able to reinterpret ResNet as a RNN with weight sharing and achieve comparable performance to the original version. We demonstrate various incarnations of this idea and show it is indeed the case.\nWe tested the 1-state and 3-state ResNets described in Figure 3 and 4. The results are shown in Figure 6."}, {"heading": "5.2.2 Sharing Weights Across All Convolutional Layers (Less Biologically-plausible)", "text": "Out of pure engineering interests, one could further push the limit of weight sharing by not only sharing across time but also across states. Here we show two 3-state ResNets that use a single set of convolutional weights across all convolutional layers and achieve reasonable performance with very few parameters (Figure 7).\nCIFAR\u221210 with Very Few Parameters"}, {"heading": "5.3 Experiment B: Multi-state Fully/Densely Recurrent Neural Networks", "text": ""}, {"heading": "5.3.1 Shared vs. Non-shared Weights", "text": "Although an RNN is usually implemented with shared weights across time, it is however possible to unshare the weights and use an independent set of weights at every time t. For practical applications, whenever one can have a initial t = 0 and enumerate all possible ts, an RNN with non-shared weights should be feasible, similar to the time-specific batch normalization described in 3.7. The results of 2-state fully recurrent neural networks with shared and non-shared weights are shown in Figure 6."}, {"heading": "5.3.2 The Effect of Readout Time", "text": "In visual cortex, useful information increases as time proceeds from the onset of the visual stimuli. This suggests that recurrent system might have better representational power as more time is allowed. We tried training and testing the 2-state fully recurrent network with various readout time (i.e., unrolling depth, see Section 3.5) and observe similar effects. See Figure 8."}, {"heading": "5.3.3 Larger Models With More States", "text": "We have shown the effectiveness of 2-state fully recurrent network above by comparing it with 1-State ResNet. Now we discuss several observations regarding 3-state and 4-state networks.\nFirst, 3-state models seem to generally outperform 2-state ones. This is expected since more parameters are introduced. With a 3-state models with minimum engineering, we were able to get 7.47% validation error on CIFAR-10.\n2\u2212State Fully Recurrent NN With Different Readout Time t\nNext, for computational efficiency, we tried only allowing each state to have transitions to adjacent states and to itself by disabling bypass connections (e.g., V1-V3, V2-IT, etc.). In this case, the number of transitions scales linearly as the number of states increases, instead of quadratically. This setting performs well with 3-state networks and slightly less well with 4-state networks (perhaps as a result of small feature/parameter sizes). With only adjacent connections, the models are no longer fully recurrent.\nFinally, for 4-state fully recurrent networks, the models tend to become overly computationally heavy if we train it with large t or large number of feature maps. With small t and feature maps, we have not achieved better performance than 3-state networks. Reducing the computational cost of training multi-state densely recurrent networks would be an important future work.\nFor experiments in this subsection, we choose a moderately deep pre-net of three 3x3 convolutional layers to model the layers between retina and V1: Conv-BN-ReLU-Conv-BN-ReLU-Conv. This is not essential but outperforms shallow pre-net slightly (within 1% validation error).\nThe results are shown in Figure 9."}, {"heading": "5.3.4 Generalization Across Readout Time", "text": "As an RNN, our model supports training and testing with different readout time. Based on our theoretical analyses in Section 2.2, the representation is usually not guaranteed to converge when running a model with time t\u2192\u221e. Nevertheless, the model exhibits good generalization over time. Results are shown in Figure 10. As a minor detail, the model in this experiment has only adjacent connections and does not have any self-transition, but we do not expect this to affect the conclusion."}, {"heading": "6 Discussion", "text": "The dark secret of Deep Networks: trying to imitate Recurrent Shallow Networks?\nA radical conjecture would be: the effectiveness of most of the deep feedforward neural networks, including but not limited to ResNet, can be attributed to their ability to approximate recurrent computations that are prevalent in most tasks with larger t than shallow feedforward networks. This may offer a new perspective on the theoretical pursuit of the long-standing question \u201cwhy is deep better than shallow\u201d [22, 21].\nEquivalence between Recurrent Networks and Turing Machines\nDynamical systems (in particular discrete time systems, that is difference equations) are Turing universal (the game \u201cLife\" is a cellular automata that has been demonstrated to be Turing universal). Thus dynamical systems such as the feedback systems we discussed can be equivalent to Turing machine. This offers the possibility of representing a computation more complex than a single (for instance boolean) function with the same number of learnable parameters. Consider for instance the powercase of learning a mapping F between an input vector x and an output vector y = F (x) that belong to the same n-dimensional space. The output can be thought as the asymptotic states of the discrete dynamical system obtained iterating some map f . We expect that in many cases the dynamical system that asymptotically performs the mapping may have a much simpler structure than the direct mapping F . In other words, we expect that the mapping f such that f (n)(x) = F (x) for appropriate, possibly very large n can be much simpler than the mapping F (here f (n) means the n-th iterate of the map f).\nEmpirical Finding: Recurrent Network or Residual Networks with weight sharing work well\nOur key finding is that multi-state time-invariant recurrent networks seem to perform as well as very deep residual networks (each state corresponds to a cortical area) without shared weights. On one hand this is surprising because the number of parameters is much reduced. On the other hand a recurrent network with fixed parameters can be equivalent to a Turing machine and maximally powerful.\nConjecture about Cortex and Recurrent Computations in Cortical Areas\nMost of the models of cortex that led to the Deep Convolutional architectures and followed them \u2013 such as the Neocognitron [6], HMAX [26] and more recent models [1] \u2013 have neglected the layering in each cortical area and the feedforward and recurrent connections within each area and between them. They also neglected the time evolution of selectivity and invariance in each of the areas. The conjecture we propose in this paper changes this picture quite drastically and makes several interesting predictions. Each area corresponds to a recurrent network and thus to a\nsystem with a temporal dynamics even for flashed inputs; with increasing time one expects asymptotically better performance; masking with a mask an input image flashed briefly should disrupt recurrent computations in each area; performance should increase with time even without a mask for briefly flashed images. Finally, we remark that our proposal, unlike relatively shallow feedforward models, implies that cortex, and in fact its component areas are computationally as powerful a universal Turing machines."}, {"heading": "Acknowledgments", "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF \u2013 1231216."}, {"heading": "A An Illustrative Comparison of a Plain RNN and a ResNet", "text": "B Inhomogeneous, Time-invariant ResNet\nThe inhomogeneous, time-invariant version of ResNet is shown in Figure 12.\nLet K \u2032 = K + I, asymptotically we have: h = Ix+K \u2032h (7)\n(I \u2212K \u2032)h = x (8) h = (I \u2212K \u2032)\u22121x (9)\nThe power series expansion of above equation is:\nh = (I \u2212K \u2032)\u22121x = (I +K \u2032 +K \u2032 \u25e6K \u2032 +K \u2032 \u25e6K \u2032 \u25e6K \u2032 + ...)x (10)\nInhomogeneous, time-invariant version of ResNet corresponds to the standard ResNet with shared weights and shortcut connections from input to every layer. If the model has only one state, it is experimentally observed that these shortcuts undesirably add raw inputs to the final representations and degrade the performance. However, if\nh\nh\n...\n8\nthe model has multiple states (like the visual cortex), it might be biologically-plausible for the first state (V1) to receive constant inputs from the pre-net (retina and LGN). Figure 13 shows the performance of an inhomogeneous 3-state recurrent network in comparison with homogeneous ones.\nInput\nt=4\nt=3\nt=2\nt=1\nInput Homogeneous Inhomogeneous\n40 45 50 55 60 0\n0.002\n0.004\n0.006\n0.008\n0.01\n0.012\n0.014\n0.016\n0.018\n0.02\nEpoch\nTr ai ni ng\nE rr or\non C IF A R \u22121\n0\nFull #Param 4210186 Adjacent #Param 3287946 Adjacent, inhomogeneous #Param 3287946\n3\u2212State Fully/Densely Recurrent Neural Networks\n40 45 50 55 60 0.07\n0.072\n0.074\n0.076\n0.078\n0.08\n0.082\n0.084\n0.086\n0.088\nEpoch\nV al id at io n E rr or\non C IF A R \u22121\n0\nFull #Param 4210186 Adjacent #Param 3287946 Adjacent, inhomogeneous #Param 3287946\nV1 V2 V4 IT\nV1 V2 V4 IT\nV1 V2 V4 IT\nV1 V2 V4 IT\nV1 V2 V4 IT\nV1 V2 V4 IT\nV1 V2 V4 IT\nV1 V2 V4 IT\nFigure 13: Inhomogeneous 3-state models. All settings are the same as Figure 9. All models are time-invariant."}], "references": [{"title": "Modulation of connectivity in visual pathways by attention: cortical interactions evaluated with structural equation modelling and fmri", "author": ["Christian B\u00fcchel", "KJ Friston"], "venue": "Cerebral cortex,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Loopy neural nets: Imitating feedback loops in the human brain. CS231n Report, Stanford, http://cs231n.stanford.edu/reports2016/110_Report.pdf", "author": ["Isaac Caswell", "Chuanqi Shen", "Lisa Wang"], "venue": "Google Scholar time stamp: March", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Understanding deep architectures using a recursive convolutional network", "author": ["David Eigen", "Jason Rolfe", "Rob Fergus", "Yann LeCun"], "venue": "arXiv preprint arXiv:1312.1847,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Cortical feedback improves discrimination between figure and background by v1", "author": ["JM Hupe", "AC James", "BR Payne", "SG Lomber", "P Girard", "J Bullier"], "venue": "v2 and v3 neurons. Nature,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Attention modulates contextual influences in the primary visual cortex of alert monkeys", "author": ["Minami Ito", "Charles D Gilbert"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Feedforward, horizontal, and feedback processing in the visual cortex", "author": ["Victor AF Lamme", "Hans Super", "Henk Spekreijse"], "venue": "Current opinion in neurobiology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Batch normalized recurrent neural networks", "author": ["C\u00e9sar Laurent", "Gabriel Pereyra", "Phil\u00e9mon Brakel", "Ying Zhang", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1510.01378,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Recurrent convolutional neural network for object recognition", "author": ["Ming Liang", "Xiaolin Hu"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "How important is weight symmetry in backpropagation", "author": ["Qianli Liao", "Joel Z Leibo", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1510.05067,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1312.6026,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Recurrent convolutional neural networks for scene parsing", "author": ["Pedro HO Pinheiro", "Ronan Collobert"], "venue": "arXiv preprint arXiv:1306.2795,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects", "author": ["Rajesh PN Rao", "Dana H Ballard"], "venue": "Nature neuroscience,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M Riesenhuber", "T Poggio"], "venue": "Nature Neuroscience,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1992}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["Andrea Vedaldi", "Karel Lenc"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Dicarlo. Using goal-driven deep learning models to understand sensory cortex, 2016", "author": ["J.D.D.L.K. Yamins"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Deconvolutional networks", "author": ["Matthew D Zeiler", "Dilip Krishnan", "Graham W Taylor", "Rob Fergus"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the effectiveness of the architectures by testing them on the CIFAR-10 dataset. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. 1 ar X iv :1 60 4. 03 64 0v 1 [ cs .L G ] 1 3 A pr 2 01 6", "creator": "LaTeX with hyperref package"}}}