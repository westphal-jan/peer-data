{"id": "1604.04144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2016", "title": "Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle", "abstract": "visual representation is crucial for a visual tracking method's performances. conventionally, visual representations adopted early in visual tracking rely on hand - crafted computer vision invariant descriptors. these descriptors were developed generically without considering tracking - specific information. in this paper, we propose to learn complex - valued invariant graphical representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. the deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our underlying proposed tracker. the proposed observational model quickly retains old training samples to alleviate drift, and collect negative samples which assume are coherent with target's motion pattern for better discriminative tracking. with the learned representation and online training samples, increasingly a logistic depth regression classifier is adopted to distinguish target from background, and graders retrained online to adapt to appearance changes. subsequently, the observational model is integrated into a particle filter correction framework to peform visual tracking. experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state - of - the - art trackers.", "histories": [["v1", "Thu, 14 Apr 2016 13:12:07 GMT  (2968kb,D)", "http://arxiv.org/abs/1604.04144v1", "Pattern Recognition (Elsevier), 2015"]], "COMMENTS": "Pattern Recognition (Elsevier), 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["jason kuen", "kian ming lim", "chin poo lee"], "accepted": false, "id": "1604.04144"}, "pdf": {"name": "1604.04144.pdf", "metadata": {"source": "CRF", "title": "Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle", "authors": ["Jason Kuen", "Kian Ming Lima", "Chin Poo Lee"], "emails": ["jason7fd@gmail.com", "kmlim@mmu.edu.my", "cplee@mmu.edu.my"], "sections": [{"heading": null, "text": "Visual representation is crucial for a visual tracking method\u2019s performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target\u2019s motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.\nKeywords: Visual tracking, temporal slowness, deep learning, self-taught learning, invariant representation"}, {"heading": "1. Introduction", "text": "Visual tracking is one of the most important research topics in computer vision because it is in the core of many real-world applications. Applications of such include human-computer interactions, video surveillance, and robotics. Due to the need for generality, recent years have seen the rise of online model-free visual tracking methods which attempt to learn the appearance of the target object over time, without prior knowledge about the object. Despite much research efforts have been made, visual tracking is still regarded as a challenging task due to various appearance changes of the target object and background distractions. Illumination variations, occlusion, fast motion, and background clutters are some challenges in visual tracking.\nA typical visual tracking method is dependent on its two major components [1], namely dynamic model (motion estimation) and observational model. A dynamic model is used to model the states and state transition of the target object, whereas an observational model describes the target object and observations based on certain visual representations. To deal with the abovementioned visual tracking challenges, most recent tracking methods tend to put focus on adopting or developing more effective representations. However, variants of image representations (e.g.,\n\u2217Corresponding author. Tel.: +606 2523066; fax.: +606 2318840. Email addresses: jason7fd@gmail.com (Jason Kuen),\nkmlim@mmu.edu.my (Kian Ming Lim), cplee@mmu.edu.my (Chin Poo Lee)\nHistogram of Oriented Gradients (HOG), Scale Invariant Feature Transform (SIFT), Local Binary Patterns (LBP)) developed in the computer vision domain are not universally effective on wide-range of vision tasks, and they lack of customizability. One recent and highly effective approach to have better task-specific representations, is to learn representations from raw data itself. Representation learning techniques seek to bypass the conventional way of labor-intensive feature engineering, by disentangling the underlying explanatory factors for the observed input. Thus, representation learning will be the main focus of our approach.\nObjects in a video are likely to be subject to small transformations across frames but the content remains largely unchanged. Our work presented in this paper aims to exploit temporal slowness principle to learn an image representation which change slowly over time, thus making it robust against these local transformations. Making use of a big amount of unlabeled tracked sequential data, generic local features invariant to transformations commonly found in tracking tasks can be learned offline. To that end, a complex-cell-like autoencoder model with temporal slowness constraint is proposed for learning separate representations of invariances and their transformations in the image sequences. To learn more complex invariances, a deep learning model is formed by training a second autoencoder with the convolved activations of the first autoencoders on larger image patches.\nThe overview of our proposed method with its three ma-\nar X\niv :1\n60 4.\n04 14\n4v 1\n[ cs\n.C V\n] 1\n4 A\npr 2\n01 6\njor components is illustrated in Fig. 1. Firstly, in Fig. 1(a), tracked image patches are used to train the deep stacked autoencoders via temporal slowness constraint (refer to Fig. 2 for visual details). The trained stacked autoencoders are then transferred to an adaptive observational model for visual tracking (Fig. 1(b)(c)). Based on certain conditions during tracking, the observational model is updated online to account for appearance changes. Fig. 1(b) describes the steps for observational model update, whereby logistic regression classifier is trained on an accumulative training set, with the features obtained from the transferred stacked autoencoders. In Fig. 1(c), tracking is performed by sampling tracking candidates via particle filtering. With the learned representation and trained logistic regression, the candidate with the highest predicted probability is chosen to be the target object.\nThe main contributions of this paper are: 1. We present an autoencoder algorithm to learn generic\ninvariant features offline for visual tracking. To train the model, we perform tracking on unlabeled sequential data and obtain tracked image patches as training data. Transformation-invariant features are learned by enforcing strong temporal slowness between tracked image patches. With subspace pooling, we construct a complex-valued representation which separates invariances from their transformations. We further add another autoencoder layer to construct a stacked convolutional autoencoders model for learning higher-level invariances. The stacked autoencoders are then transferred for use in visual tracking, based on self-taught learning paradigm [2]. 2. With the learned representations, we propose an adaptive observational model for tracking. Both first and second layer of the stacked autoencoders are transferred to form a final tracking representation. For better discriminative tracking, the proposed observational model is equipped with a novel negative sampling method which collects more relevant negative training samples. Besides, to alleviate visual drift, we propose a simple technique for the observational model to retain early and recent training samples. 3. We integrate the proposed adaptive observational model into a particle filter framework and evaluate our proposed tracker on a number of challenging benchmark sequences, comparing with several state-of-theart trackers. Results demonstrate that the proposed tracker performs favourably against the competing trackers."}, {"heading": "2. Related work", "text": "Observational model, also known as appearance model is undoubtedly the most crucial component in visual tracking. In this section, literature review is done for existing trackers in terms of the two common categories of observational model, namely generative and discriminative\napproaches. Subsequently, several existing representation learning-based trackers are reviewed.\nGenerative approaches represent target object with low reconstruction error and identifies the best matched candidate among many observations. To adapt to appearance changes of target object, most of the recent generative tracking methods learn the appearance of the object online. Subspace learning methods learn expressive representations in low dimensional space. To develop a subspace learning-based tracker, Ross et al. [3] used Principal Component Analysis (PCA) to construct and incrementally update a subspace model of the target object. Liwicki et al. [4] formulated an incremental kernel PCA in Krein space to learn a nonlinear subspace representation for tracking. To account for partial occlusion during tracking, Kim [5] proposed a Canonical Correlation Analysis (CCA)-based tracker which considers the correlations among sub-patches of tracking observations. Mixture models can also be used for tracking, Jepson et al. [6] learned a mixture model to model appearance changes of target object via an online Expectation-Maximisation (EM) algorithm. Wang et al. [7] develop an adaptive observational model in the joint spatial-color space using Gaussian mixture model.\nAlthough generative trackers can work well in certain circumstances, they are inferior to discriminative trackers in dealing with complicated environments. Unlike generative trackers, discriminative trackers take background information into account and distinguish between target object and background. Collins et al. [8] selected color features online which best discriminates target object from current background. To deal with appearance changes, Grabner et al. [9] proposed an online boosting classifier that adaptively selects discriminative features for tracking. Klein and Cremers [10] introduced a novel scaleinvariant gradient feature and used boosting to track target object efficiently. To alleviate visual drift, Zhang and Song [11] proposed a tracking method based on online multiple instance boosting that handles ambiguously labeled samples and weights each positive sample differently for update. Besides online feature selection and ensemble methods, Support Vector Machine (SVM)-based tracking methods have received much attention lately. Tang et al. [12] trained multiple SVMs, each on an independent feature and locates the target object by combining confidence scores from all SVM classifiers. To reduce the error of selecting inaccurate samples in updating observational model online, Hare et al. [13] presented a structured output SVM to directly predict the trajectory of the target object between frames. Discriminative tracking methods are advantageous because they generally allow the incorporation of various kinds of feature representations. However, if these representations are not well formulated, they can be devastating for the tracking performances.\nRepresentation learning [14] is an emerging field aims to learn good representations from raw input or low-level representations. Many representation learning techniques\n(e.g., Netzer et al. [15]; Yu et al. [16]) have proven to be superior to conventional hand-engineered representations. Two popular techniques in representation learning are dictionary learning and deep learning. Dictionary learning aims to learn a dictionary or codebook in which only a few atoms can be linearly combined to well approximate a given signal. Considering the absence of object prior information in existing model-free tracking methods, Wang et al. [17] performed sparse coding on SIFT features extracted from labeled object recognition datasets. Liu et al. [18] learned a sparse coded dictionary online from raw unlabeled patches and discriminates target object from background regions using SVM. Deep learning is relatively new in visual tracking research. In deep learning, deep layered architectures are employed to learn complex high-level representations in terms of less simpler low-level representations. Using k -means clustering, Jonghoon et al. [19] trained a convolutional neural network offline in an unsupervised manner for tracking. Their method however only maintains a static observational model over time. To account for object appearance changes, Wang and Yeung [20] pre-trained a stacked denoising autoencoders and finetuned the deep neural network online during tracking."}, {"heading": "3. Learning deep invariant representations using temporal slowness", "text": "In this paper, we aim to exploit temporal slowness to learn generic invariant representations in an unsupervised way and transfer them for visual tracking. Temporal slowness is one of the major priors for representation learning [14] and it has been successfully used in object recognition tasks (e.g., Mobahi et al. [21]; Zou et al. [22]). The main motivation of learning such representation is that it is difficult to develop or hand-craft an exact feature extraction algorithm that is robust against object transformations found in videos. To the best of knowledge, this paper is the first attempt to employ temporal slowness principle for visual tracking. Zou et al. [22] showed that temporal slowness constraint can be simply added to conventional autoencoder cost function to learn invariant features, we also choose autoencoder to be our representation learning model. Although our proposed tracker is discriminative in nature, the representations are learned via generative constraints.\nThis section is organized as follows: Firstly, the conventional autoencoder is introduced and modifications are\nprogressively added to the autoencoder to achieve the temporally slow autoencoder algorithm. Secondly, the details on how a stacked convolution autoencoder can be constructed are described. Thirdly, the complex-valued representation formed by the subspace pooled autoencoder is described and the visualized optimal stimuli for the learned features are shown. Lastly, the sequential dataset used and the preprocessing method to obtain tracked training image patches are explained."}, {"heading": "3.1. Autoencoder with temporal slowness constraint", "text": "An autoencoder is an unsupervised artificial neural network which learns a mapping to reconstruct input data in its final layer. Given an input vector of N number of d1-dimensional data samples [x\n(1), ...,x(N)] \u2208 Rd1\u00d7N , an autoencoder has a squared error cost function :\nN\u2211 i=1 \u2016x(i) \u2212 fd(Wdfe(Wex(i) + \u03b2e) + \u03b2d)\u201622 (1)\nwhere W is the autoencoder weights, f is the activation function, \u03b2 is the network biases, and subscripts e and d indicate the associations of the components with the encoder and decoder respectively. Generally, the latent representation fe(Wex\n(i) + \u03b2e) learned in the hidden layer of an autoencoder is regarded to be meaningful for classification purposes. The conventional autoencoder with mere reconstruction cost can hardly learn any useful latent representation of the data. To allow autoencoder to discover better representations, many autoencoder regularization schemes such as denoising autoencoders [23] and contractive autoencoders [24] have been proposed. However, these autoencoder variants use only static and temporally uncorrelated object images to learn features for object recognition tasks. Although such approach can be borrowed directly for visual tracking purpose (e.g., Wang and Yeung [20]), we contend that image representation for visual tracking should be learned in a way more specific to the task.\nTo this end, we propose to adapt autoencoder to learn invariant features from tracked image sequences for visual tracking. Our autoencoder model draws inspiration from Independent Subspace Analysis (ISA) [25] which was proposed for learning motion invariance. Unlike conventional sparse autoencoders which enforce sparsity in the hidden layer, the proposed autoencoder performs subspace pooling on the hidden layer activations and enforces sparsity in the pooling layer, in a way identical to ISA. When learning features, invariance is achieved by enforcing a strong temporal slowness constraint to minimize the distance between the subspace pooling representations of any two temporally correlated tracked image patches. To preserve the architectural properties of ISA that it has no biases and nonlinear activation function, biases are omitted and linear activation function is chosen in the reconstruction cost in Eq. (1). Furthermore, the encoder and decoder weights are tied (Wd = W T e ) to reduce the number of free parameters\nto train. In the proposed autoencoder, the reconstruction cost replaces hard orthonormality constraint in ISA to prevent feature degeneracy [26] and the sparsity cost helps to discover interesting features [27] (e.g., edges, corners).\nTo train the autoencoder, we use a dataset consists of N number of tracked patches, formed by NT number of track sessions and NF number of frames per track (refer to Section 3.4). The modified autoencoder with p number of hidden units is then trained by :\nN\u2211 i=1 \u2016x(i)\u2212WTWx(i)\u201622 +\u03b1 NT\u2211 t=1 NF\u22121\u2211 f=1 \u2016h(t,f)\u2212h(t,f+1)\u20161\n+ \u03b3 N\u2211 i=1 \u2016h(i)\u20161 (2)\nwhere W \u2208 Rp\u00d7d1 is the autoencoder weights, \u03b1 \u2208 R is the weight of the temporal slowness cost, and h(t,f) \u2208 R( p 2 )\u00d71 is the subspace pooling representation of image patch at f -th frame of t-th track. L1-norm minimization is a common way to achieve sparsity in representation learning. In\nthe second cost term (temporal slowness constraint), as in [21] and [22], we minimize the temporal representation differences in L1-norm to allow invariance to be sparsely represented, that is a kind of motion invariance is represented by only a small number of features and thus they become specialized for different invariances. L1-norm regularization is too applied to the third cost function term to enforce sparsity in the subspace pooling layer h, where \u03b3 \u2208 R parameterizes the weight of the sparsity regularization.\nThe i-th subspace pooling unit h(i) is obtained by performing L2-norm subspace pooling on its hidden layer counterpart :\nh(i) = \u221a P (Wx(i))\u00b72 (3)\nwhere ()\u00b72 indicates element-wise square operation and P \u2208 R( p 2 )\u00d7p is a subspace pooling matrix which sums up every two adjacent features in a non-overlapping way. The encoder diagram with subspace pooling is shown in Fig. 2(b). Subspace pooling has been successfully used in temporal slowness feature learning techniques (e.g., Bengio and Bergstra [28]; Zou et al. [22]) to group similar features in each of the pooled units, therefore achieving invariance. It pairs every 2 adjacent hidden units to form a complexvalued representation and each pair can be decomposed into amplitude (degree of presence of the features) and phase (transformations of the features over time) variables [29]. The pooling units resemble complex-cells in the visual cortex, in which the amplitudes are insensitive to phase changes. The proposed autoencoder with temporal slowness constraint on any two paired tracked patches is illustrated in Fig. 2(a).\nUnlike ISA which relies on inconvenient constrained optimization methods for training, the cost function in Eq. (2) can be optimized efficiently using any of the unconstrained optimization methods [30]."}, {"heading": "3.2. Stacked convolutional autoencoders", "text": "Autoencoders have been commonly stacked to form deep layered architectures to learn higher-level representations from its low-level counterparts. In this paper, we stack and train a second autoencoder (known as the second layer) on the convolved features of the first autoencoder (known as the first layer) in a greedy layer-wise training fashion [31]. The convolutional learning architecture allows the reuse of first layer features to learn higher-level features from bigger image patches.\nBefore training the second layer, the first layer subspace pooling features are densely extracted from larger d2-dimensional tracked image patches (where d2 > d1) with a predefined spatial stride or step-size k1, where k1 \u2265 1. Experimentally, we choose k1 in such a way that the overlaps between the dense patches is small. A small k1 is computationally demanding, while a big k1 with no overlap impedes the feature performances severely. Unlike in object recognition tasks whereby a small k1 is always\nrecommended to achieve good performances [32], there is a great need to balance the feature and run-time performances in visual tracking.\nSubsequently, the convolved first layer representation from the densely extracted patches are flattened into a feature vector per large tracked image, and they are used as training dataset to train the second layer. The convolutional learning architecture of this method is illustrated in Fig. 2(b) and 3. Due to a bigger k1 chosen, we get a reasonable number of feature dimensions in the convolved representations, and thus a dimensionality reduction technique is not required unlike the strategy of Zou et al. [22]. The second layer shares the same autoencoder cost function as the first layer but they differ in parameter settings for the cost term weights."}, {"heading": "3.3. Feature extraction and visualization", "text": "As pointed out in Section 3.1, there are two kinds of information can be obtained from the subspace pooled units of the proposed autoencoder. Let A \u2208 R and B \u2208 R denote any paired adjacent activations in the hidden layer of the autoencoder, a complex representation A+ iB \u2208 C is formed. The amplitude or magnitude of the complex representation A + iB indicates the degree of presence of the feature while being invariant to its phase changes and transformations. It is computed as the Euclidean norm of the complex number : \u221a\nA2 +B2 (4)\nwhich is exactly what Eq. (3) does. The amplitudes are a good invariant representation for supervised classifications. The second information obtained from the subspace pooled units is the phase. Phase of the complex representation A+ iB is defined as :\ntan\u22121 ( B\nA\n) . (5)\nWith temporal slowness constraint, the angular-valued phase models the feature transformations of each pooled unit in smooth transitions, which means that the features\nchange slowly with gradual change in phase. For visual tracking, both amplitude and phase features are considered due to their invariant and discriminative properties respectively.\nAn interesting property of learning with subspace pooling and temporal slowness is that it allows the phase of the pooled units to be shifted (Olshausen et al. [29]; Zou et al. [22]), to visualize what transformations the features are invariant to. We follow the phase-shifting procedure of Zou et al. [33] and use linear combination of filters [34] to visualize the optimal stimuli for the features and invariances learned in the stacked autoencoders. Fig. 4 shows some representative optimal stimuli for the two layers of our stacked autoencoders, with a 36 \u25e6 increment in the phase shift. It is shown that the first layer learns Gabor-like edge detectors which are invariant against small location translations, and the second layer learns features which are invariant against complicated transformations such as out-of-plane rotation."}, {"heading": "3.4. Sequential training data collection", "text": "We employ self-taught learning principle to learn a representation for visual tracking. In self-taught learning [2],\nfeatures are learned from unlabeled data and transferred for use in supervised learning tasks, whereby the generating distribution of the unlabeled data is different from the labeled data. In the context of this paper, features are learned from image sequences unrelated to tracking benchmark sequences used to gauge the proposed tracker\u2019s performances. This setting is analogous to the tracking algorithm [17] which exploits patch-level similarity and transfers visual prior from unlabeled dataset to tracking tasks.\nHarnessing a diverse unlabeled dataset for self-taught feature learning is essential because the learned features are expected to generalize well to unseen examples. The deep learning-based tracking algorithm [20] which transfer features learned from unlabeled datasets, employ a very large dataset with great amount of visual diversity. In this paper, we use a large aggregation of tracking benchmark sequences compiled by Wu et al. [35]1 and Klein et al. [36]2, in which the objects and scenes are diverse in terms of their appearances. More importantly, they exhibit possibly all variants of tracking challenges such as occlusion, out-ofplane rotation, deformation, and scale variation. The experimental benchmark sequences used in Section 6.2 are excluded to uphold the self-taught learning principle.\nBy employing a tracker developed by Jia et al. [37], tracked image patches are collected randomly from the sequences. The chosen tracker is not an arbitrary choice, it is the best performing tracker among the trackers we evaluated on a set of tracking benchmark sequences in Section 6.2, excluding our own proposed tracker. To learn feature invariances via temporal slowness, it is important that the tracked image patches are accurately obtained. As a way to affirm this hypothesis, we randomly shuffled the first half of the our collected tracked dataset to disrupt the temporal ordering, and subsequently adopted it for learning first-layer features. This can be thought as a rough simulation of using a weak performing tracker to collect the tracked patches. The optimal stimuli for a few of the learned first-layer features are shown in Fig. 5. It can be seen that the optimal stimuli are made up of noisy patterns and they do not resemble the sharp edge detectors as in Fig. 4.\nDue to the presence of much uninteresting regions (e.g., flat appearance, appearance that remains constant over time) in the sequences, we find \u2018interesting\u2019 regions before performing tracking. For interest point detection, two approaches are first considered. The first approach is about identifying motional pixels via binary-thresholded accumulative difference pictures [38], and a size filter is used to remove trivially small connected components among the identified pixels. Subsequently, initial tracking regions are chosen from random frame numbers and random spatial locations, with the constraint that the initial regions must have at least a small overlaps with the motional pixels. In\n1http://visual-tracking.net 2http://www.iai.uni-bonn.de/ kleind/tracking\ncontrast, the second approach involves a space-time Harris interest point detection algorithm [39] (referred to as STIP) that identifies regions which are \u2018interesting\u2019 spatially and temporally. We then qualitatively compare the results of these two approaches on an arbitrary short video segment, as shown in Fig. 6. The video segment contains two running persons with a relatively unchanged background. Noticeably, STIP outperforms the first approach because the latter takes into account only temporal differences while ignoring much of regions which are rich in spatial information (e.g., corners, textures). Experimentally, we choose STIP over accumulative difference pictures because spatial and temporal interestingnesses help to learn good features and good invariances respectively.\nCompared to the work of Zou et al. [22], we use a larger number of track sessions to encourage diversity in the training dataset, and use a smaller number of frames per track session to minimize the occurences of tracking drift in the dataset. Although it may seem intuitive that using\na higher number of frames per track is more advantageous than a lower number of frames, we qualitatively evaluate the features learned on the same unlabeled dataset using 5 frames/track (default setting in this paper) and 20 frames/track settings respectively, and show in Fig. 7 that features learned in 20 frames/track setting are incredibly similar to those of 5 frames/track setting in Fig. 4. In a low frames per track setting, interest points are detected using low number of frames. If an object remains \u2018interesting\u2019 for a long period of time, then it is very likely that multiple interest points on the same object can be detected at different timesteps, which implicitly forms a long sequence of tracked patches. Therefore, using higher number of frames per track is not significantly advantageous in this circumstance. An example of tracked patches from a track session is shown in Fig. 1(a)."}, {"heading": "4. Adaptive observational model", "text": "The observational model we use in this paper is both discriminative and adaptive. It is discriminative in the sense that it utilizes a supervised binary classifier to classify tracking observations into positive (target) class and negative (background) class. We also retrain the classifier periodically with new training samples to keep the observational model adapted to appearance changes of the target object and background over time. Observations are represented using representations learned from the stacked autoencoders introduced previously."}, {"heading": "4.1. Online training samples collection", "text": "In the first frame of tracking, there are no preceding positive samples which can be concatenated with the current tracking target to form a positive training set. Therefore, in this circumstance, a set of positive samples are collected from regions that are located few pixels away from the target object. Let (xt, yt) denote the target object\u2019s coordinate on horizontal and vertical axes respectively in t-th frame. In the first frame where t = 1, a positive sample is collected at the location (x1+, y 1 +) :\nj 1+ = j 1 +DU(\u2212v, v) , \u2200j \u2208 {x, y} (6)\nwhere v \u2208 Z determines the maximum pixel translation on any of the axes, DU(\u2212v, v) \u2208 Z is a random integer sampled from a discrete uniform distribution in the range of [\u2212v, v]. In the subsequent frames where t > 1, every predicted target object observation is added to the positive sample set, by replacing the oldest positive sample in the set. Considering the fact that early target observations are more likely to be reliable predictions of the tracker, positive samples from early Fes \u2208 Z number of frames are kept permanently in the training set, to alleviate tracking drift. We too retain positive samples of recent Fr+ \u2208 Z frames in the training set, in case of occasional false positive training samples the tracker obtains.\nUnlike positive samples, we use only a single procedure to collect negative samples in all the frames. In this paper, a novel method to collect negative samples is proposed, in such a way that the negative samples have a little overlapping regions with the target object and they are somehow coherent with the particle filter\u2019s dynamic model in our proposed tracker. The rationale behind this coherence is that tracker which tracks fast-moving objects require some negative samples farther from the target, whereas tracker which tracks slow-moving objects does not need far negative samples. In the context of visual tracking, a particle filter\u2019s dynamic model generally requires normallydistributed translational affine parameters (refer to Section 5.1) to model the spatial translations of the target object. Let \u03c3x \u2208 R and \u03c3y \u2208 R denote the normal distribution\u2019s standard deviations for translational affine parameters on horizontal and vertical axes respectively. At t-th frame, a negative sample is collected at the location (xt\u2212, y t \u2212) :\nj t\u2212 = j t + (Stj \u00d7 \u03d5\u00d7 sgn(rj )) + rj , \u2200j \u2208 {x, y} (7)\nwhere Stj \u2208 R refers to either width Stx or height Sty of target object, sgn() denotes the sign function, \u03d5 \u2208 R is a constant determining the maximum overlaps between negative samples and target object on each of the axes. rj \u2208 R is a random value computed as :\nrj = N (0, \u03b7 \u00d7 \u03c3j) , \u2200j \u2208 {x, y} (8)\nwhere N (0, \u03b7\u00d7\u03c3j) \u2208 R is a random number sampled from a normal distribution, with zero mean and \u03b7 \u00d7 \u03c3j \u2208 R as its standard deviation. \u03b7 \u2208 R is a constant multiplier. The second term of Eq. (7) specifies the maximum overlaps between negative samples and target object, whereas the third and last term of Eq. (7) determines the gap between negative samples and target object. At every frame, N\u2212s \u2208 Z (where N\u2212s 1) number of negative samples are collected and they replace the negative samples of the oldest frame in the training set. Similar to positive samples, negative samples of recent Fr\u2212 \u2208 Z frames are retained in the training set to alleviate visual drift. At any time of tracking, more negative samples than positive samples will be present in the training set, which causes class imbalance problem. We address this problem in Section 4.3."}, {"heading": "4.2. Feature extraction for visual tracking", "text": "After training the stacked autoencoders in Section 3 offline, it is transferred for use in visual tracking. Both first and second layers of the stacked autoencoders are used to extract dense local features (amplitude and phase features) from tracking observations and training samples. Before performing feature extraction, all tracking observations are normalized to a standard tracking template size of 32\u00d732. Image size of 32\u00d732 is a good balance between computational efficiency and image details.\nAs in Section 3.2, local d1-dimensional patches are densely extracted from tracking observations with k1 stride, and passed into the first layer to obtain first-layer features. Subsequently, second-layer features are densely extracted from the first-layer feature map with a stride of k2. Finally, the convolved representations from both the layers are concatenated to form a final representation for visual tracking. Due to the use of relatively large convolution strides, spatial pyramid pooling [40] is not performed on the convolved representations. Doing so would greatly encourage translational invariance, which is not favourable in visual tracking [41]."}, {"heading": "4.3. Supervised binary classification", "text": "After performing feature extraction on positive and negative samples, a training dataset with approximate labels is obtained. A linear binary classifier is adopted to distinguish between target object and background during tracking. Linear classifiers are less prone to overfitting, and high-level representations such as our learned deep representations are likely to be linearly separable. The classifier used is logistic regression due to its capability of providing predictions in probability estimates. Probability estimates or soft labels are more useful than hard labels in the case where we want to identify the most likely target object candidate among many other candidates. Taking into account the class imbalance problem in the training set, a class-weighted logistic regression is proposed for our observational model. Let z i \u2208 Rr\u00d71 denote the final tracking representation (refer to Section 4.2) for i-th training sample, Z+ = [z1+ , z1+ ..., zD+ ] \u2208 Rr\u00d7D +\nrepresents the positive training set with their respective labels as Y + = [y1+ , y2+ , ..., yD+ ]\nT \u2208 {\u22121,+1}D+\u00d71, D+ \u2208 Z is the number of training samples, r indicates the number of features in the final tracking representation z. The negative sample counterparts of Z+, Y +, and D+ are denoted as Z\u2212 \u2208 Rr\u00d7D\u2212 , Y \u2212 \u2208 {\u22121,+1}D\u2212\u00d71, and D\u2212 \u2208 Z respectively. The logistic regression classifier is trained by optimizing :\nmin w C+ D+\u2211 i+=1 log(1+eyi+w Tzi+ )+C\u2212 D\u2212\u2211 i\u2212=1 log(1+eyi\u2212w Tzi\u2212 )\n(9)\nwhere C+ \u2208 R is the weight parameter for positive-class logistic cost and C\u2212 \u2208 R is the weight parameter for negative-class logistic cost. To balance the learning contributions from both classes, C+ and C\u2212 are set in such a way that they are inversely proportional to D+ and D\u2212 respectively. Additionally, weight decay or L2 weight regularization, \u2016w\u201622 is added to the cost function in Eq. (9), to penalize large weights and therefore reducing overfitting. In the prediction stage, the trained logistic regression classifier computes the probability or confidence score as follows:\nf(zc) = 1\n1 + e\u2212wTzc (10)\nwhere zc \u2208 Rr\u00d71 is the representation of a target object candidate. The candidate with the highest probability is chosen to be the target object.\nBesides initializing the classifier in the first frame of tracking, the maximum probability estimate among all tracking observations is checked every Ff \u2208 Z frames, and if the maximum probability obtained is below a threshold \u03a5, then the classifier is retrained with the current training set. Ff is set to a small value to allow fast update of the classifier, in case of abrupt appearance changes to the target object. Without the probability threshold \u03a5, the update would be too often that small and trivial errors are more likely to be accumulated, eventually causing tracking drift. To account for the poorly diversified positive samples in the early frames, the maximum probability check is done every frame for early Fet \u2208 Z frames. Finally, there are also circumstances whereby the target object does not change by much (maximum probability remains very high) but the background has changed. To allow new background information to be learned by the classifier in a slow manner, the classifier is retrained if it has not been updated for a number of Fs \u2208 Z frames, where Fs Ff ."}, {"heading": "5. Proposed tracker", "text": "Before visual tracking can be carried out, the adaptive observational model in Section 4 should be integrated into an object state estimation method. For this, particle filter [42][43] is chosen over other methods (e.g., Kalman filter) due to its nonlinearity, non-Gaussian assumption, and capability of maintaining multiple hypotheses. In the last part of this section, we contrast our proposed tracking method with the reviewed state-of-the-art representation learning trackers."}, {"heading": "5.1. Particle filter", "text": "Particle filter is an implementation of Bayesian recursive filter. The purpose of the filter is to estimate the target state s\u0302t of a dynamical system, based on a sequence of observations x1:t = {x1, x2, ..., xt} up to time t :\ns\u0302t = argmax p(st|x1:t) (11)\nThe posterior distribution p(st|x1:t) is inferred via Bayes theorem in a recursive manner :\np(st|x1:t) = p(xt|st)p(st|x1:t\u22121)\np(xt|x1:t\u22121) (12) where the prior distribution p(st|x1:t\u22121) = \u222b p(st|st\u22121) p(st\u22121|x1:t\u22121)dst\u22121. The distribution p(st|st\u22121) expresses the state transition or dynamic model, and p(xt|st) denotes the observation likelihood function tied with the observational model.\nIn particle filtering, the posterior distribution p(stx1:t) is constructed recursively using a finite set of random samples (called particles) {sit, i = 1, 2, ..., Ns} with importance weights {w it , i = 1, 2, ..., Ns}, where Ns is the number of particles. Each particle corresponds to a hypothesis of the state. Given a candidate particle sit drawn from an importance distribution q(st|s1:t\u22121, x1:t), the weight of the i-th particle is computed as:\nw it = w i t\u22121 p(xt|sit)p(sit|sit\u22121) q(sit|si1:t\u22121, x1:t)\n(13)\nIn the context of visual tracking, the importance distribution q(st|s1:t\u22121, x1:t) is generally chosen to be p(st|st\u22121), the dynamic model. Therefore, only the observational mode p(xt|st) has influence on the weight update of the particles.\nThe dynamic model p(st|st\u22121) propagates the particles from time t \u2212 1 to t, describing the temporal transition of target states between time steps. For model-free visual tracking using rectangular bounding box, we employ affine transformation parameters (e.g., translation, scale, aspect ratio of the bounding box) as state elements to approximate the motion of target object between frames. The dynamic model is formulated such that each state element in st is modeled independently by a normal distribution centered at its previous state st\u22121 :\np(st|st\u22121) = N (st\u22121, Q) (14)\nwhere Q is a diagonal covariance matrix whose elements are the variances of the affine transformation parameters.\nAlthough dynamic model plays an important role in particle filter-based trackers, the observational model p(xt|st) is the key factor to a trackers performance when dealing with various tracking challenges. In this paper, the observation likelihood is exponentially proportional to the confidence score f(zt) given by the periodically updated linear classifier at time t :\np(xt|st) \u221d exp(f(zt)) (15)\nThe exponentiation is to penalize low-weighted particles so that they are less likely to be chosen in particle resampling.\nExcluding the offline training of the stacked autoencoders with temporal slowness, the high-level summary of our proposed tracker is given in Algorithm 1.. We refer to our proposed tracker as Deep Slow Tracker (DST)."}, {"heading": "5.2. Comparison with other representation learning trackers", "text": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways. In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences. To allow the learned filters to be specific to the\nAlgorithm 1. Deep Slow Tracker\nInput: tracking frames F1, ..., FT Output: target object states s\u03021, ..., s\u0302T\n1: Transfer stacked autoencoders for feature extraction 2: if t == 1 then 3: Initialize classifier with positive samples (small translations) and negative samples 4: Store training samples 5: else 6: Estimate s\u0302t using particle filter 7: Collect negative training samples 8: Store positive sample (target object at t-th frame) and negative samples 9: if t > Fr+ + Fes then 10: Remove the oldest frame\u2019s positive sample after Fes 11: end if 12: if t > Fr\u2212 + Fes then 13: Remove the oldest frame\u2019s negative samples after Fes 14: end if 15: if max probability < \u03a5 and frames passed without update == Ff then 16: Retrain linear classifier 17: else if max probability < \u03a5 and t <= Fet then 18: Retrain linear classifier 19: else if frames passed without update == Fs then 20: Retrain linear classifier 21: end if 22: end if\ntracking environment, [18] performs online training solely on image patches sampled from the tracking sequences itself. The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes. However, for the sake of generality, we cannot assume objects in real-world applications share similar appearances with the limited object classes. Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20]. All of the trackers use raw image patches to learn representations from, except [17] which extracts SIFT features from the patches as bases to build a sparse coded dictionary.\nIn terms of observational models adaptivity, [19] is the only tracker that uses a non-adaptive offline classifier to distinguish between target object and object. [17] and [18] including DST employ linear classifiers which are independent of representation learning models, to build adaptive observational models. Wang and Yeung [20] uses a more sophisticated way for classification during tracking, by fine-tuning the deep neural network (unrolled from pre-trained stacked autoencoders) using classification error\ncost function. This results in a nonlinear classifier. Supervised fine-tuning too can be applied to our proposed autoencoder but the advantages from the temporal slowness constraint might be diminished as a result of minimizing only classification errors."}, {"heading": "6. Experiments", "text": "In this section, we describe the implementation details and parameter settings of DST, along with the experimental setups for the tracking experiments. DST is tested on several challenging sequences, against 7 state-of-the-art trackers. We then present the results from the experiments in quantitative and qualitative means."}, {"heading": "6.1. Implementation details", "text": "In this subsection, we provide the parameter settings for the parameters described in previous section. All parameter settings of the proposed method are obtained empirically.\nOne of the most important aspects of DST is the offline training of the stacked autoencoders (Section 3). In terms of sequential training datasets, 8 \u00d7 8 tracked patches are employed for training the first layer and the second layer trains on 14 \u00d7 14 tracked patches. The number of track sessions NT is 15000 and the number of frames per track NF is 5. Next, the weight parameters \u03b1 and \u03b3 in the temporally slow autoencoder cost function (Eq. (2) are set as [100, 20] and [300, 20] for first and second layer respectively. To get the convolved representation for training the second layer, first layer features are densely extracted from 14\u00d7 14 tracked patches with spatial stride k1 = 6. During visual tracking, first layer features are densely extracted from 32 \u00d7 32 tracking observations with the same spatial stride of k1 = 6, and second layer features are densely extracted from the first layer feature map with spatial stride k2 = 2. For unconstrained optimization of the autoencoders, we employ off-the-shelf Limited-memory BroydenFletcher-Goldfarb-Shanno (BFGS) algorithm that is relatively memory-efficient and fast-converging [30]. The optimization process stops once it reaches a fixed number of iterations, which is 200 in this paper.\nIn the adaptive observational model of DST, training samples of binary classes are collected online (Section 4.1) to train a linear classifier. The gap parameter v to collect first-frame positive samples is set as 1 and the constant multiplier \u03b7 to collect negative samples is set as 2. To alleviate tracking drift, positive samples from early Fes = 15 frames and recent Fr+ = 55 are retained in the current training set. Likewise, negative samples of recent Fr\u2212 = 15 are retained, and N\u2212s = 25 number of negative samples are collected per frame. The classifier update frequency parameters (Section 4.3) Ff , Fet, and Fs are set as 5, 10, and 25 respectively. As a parameter to allow quick observational model update, the probability threshold \u03a5 is configured as 0.99. The above parameter settings are fixed\nfor all benchmark sequences and we fix the number of particles in particle filter (Section 5.1) to 1000. Our proposed tracker, DST is implemented in MATLAB without code optimization and it runs at about 1.8 frames per second."}, {"heading": "6.2. Experimental setups", "text": "We evaluate DST on 24 challenging benchmark sequences. They are part of benchmark sequences compiled by Wu et al. [35], Wang et al. [44], and Babenko et al. [45]. For a more comprehensive evaluation, these sequences include the various challenges in visual tracking such as fast motion, illumination variation, cluttered background, occlusion, pose variation and object deformation.\nWe compare DST\u2019s performances with 8 state-of-the-art trackers. The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50]. We run the experiments based on the codes provided by the authors. ASLA builds an efficient incremental sparse appearance model that takes into account the structural information of target object. Using random measurement matrix, CT generates compressed representation from Haar-like features and performs tracking discriminatively. DLT is especially relevant because of its use of deep denoising autoencoders to learn a compact representation online for tracking. IVT uses a novel incremental PCA approach to generatively learn an updatable subspace representation online. ODFS performs online feature selection using weak classifiers to maximize the confidence of positive samples. On the other hand, PLS makes use of binary training samples to learn a low-dimensional\ndiscriminative subspace representation. SPT introduces sparsity and trivial templates into generative PCA subspace learning, to explicitly handle occlusion and motion blur. TLD meticulously combines detection, tracking, and learning components into a framework for long-term tracking. In contrast to other visual tracking literatures, we test DST against very recent state-of-the-art trackers, instead of earlier ones. The tracking results transcribed on the sequences can be viewed at http://www.youtube.com/user /DeepSlowTracker/videos.\nFor particle filter-based trackers (ASLA, DLT, IVT, PLS, SPT, and DST), the affine parameter settings in particle filter\u2019s dynamic model are heuristically chosen according to the target object\u2019s nature in each benchmark sequence. No grid-search or deliberate optimization is done to obtain the settings. For fair comparisons, they are configured to share the same affine parameter settings and number of particles. For trackers that do not employ particle filter (CT and ODFS), we find the best setting for their object search window parameter from some possible values (in the range suggested by the authors), for each sequence. Since TLD carries out object detection densely on the image window, there is no search parameter to be tuned. Other parameters such as feature-related and training sample collection parameters are left in their default settings and fixed for all sequences, just like the way it is done for DST. Finally, all trackers are initialized with the same target object locations."}, {"heading": "6.3. Quantitative evaluation", "text": "In this subsection, the trackers are evaluated quantitatively in terms of success rates (SR) and center-oflocation (COL) errors. Given a ground truth bounding\nbox RG and a tracking result RT , SR is computed as area(RT\u2229RG) area(RT\u222aRG) > 0.5. COL error is obtained by computing the Euclidean distance in pixels between the center of RG and RT . Since all the evaluated trackers carry out random sampling, we run the trackers 5 times for each sequence and get the median results. The median results are obtained by adding max-min normalized SR and inverse COL errors from the five trials. The average SR\nand COL errors are shown in Table 1 and 2, respectively. DST achieves the best or second best performance in most sequences, in terms of both SR and COL errors. Numerically, DST fares a lot better than its deep learning-based competitor, DLT in many of the sequences. To understand the performances of the trackers over time, the COL error plots for all tested sequences are presented in Fig. 8."}, {"heading": "6.4. Qualitative evaluation", "text": "For qualitative evaluation, we choose a number of sequences from the 18 benchmark sequences presented in Section 6.3. The sequences chosen span across the typical tracking challenges, and each of them is a representative sequence for a kind of challenge.\nOcclusion: In the coke sequence (Fig. 9(a)), the target object undergoes partial occlusion (#041), full occlusion (#262), out-of-plane pose change, and illumination change. ASLA, CT, ODFS, and SPT lost track of the target coming out from heavy partial occlusion (#041). It is notable that DST performs better than DLT and as well as PLS and TLD (#232). IVT and SPT perform poorly because their holistic representations are not robust against partial occlusion. Our highly invariant representation learned via temporal slowness can deal well with pose change. Furthermore, the training set accumulation technique helps to alleviate post-occlusion drift. However, after the long full occlusion (#282), only DLT can recover fully because it can retain initial target appearance model well if its particle confidence scores remain high much of the time. The target object in the woman sequence (Fig. 9(b)) is a deformable woman figure heavily occluded by cars. PLS drifts away when the background is cluttered (#098). The first occlusion (#122) from the car is the deciding point, the trackers which make through this point can track the target well till the end. Overall, only ASLA,\nDLT, and DST can perform well in this sequence. Unlike other generative trackers (IVT and SPT) which are holistic, ASLA is able to handle moderate partial occlusion because of its part-based sparse representation. Even though TLD loses tracker of the object during heavy partial occlusion (#122, #225), its detector component can help it to recover when the object later appears unoccluded (#340, #524).\nIllumination variation: Fig. 10 shows the tracking results on sequences with illumination variation. For the car4 (Fig. 10(a)) sequence, the target undergoes illumination and scale changes. Although DLT employs particle filter for tracking, it does not perform well when the scale of the car changes (#068) due to its unstable incremental training of deep neural networks. Generative representations (ASLA, IVT, SPT) perform well which can be attributed to the fact that they are robust against illumination changes (#198, #244, #304). DST is on par with the generative trackers mainly due to the invariant features extracted out of background training samples, allowing relatively unchanged background candidates to be rejected even though the target\u2019s appearance has changed. In the trellis (Fig. 10(b)) sequence, there are pose changes as well as frequent illumination changes on the target. All trackers except ASLA and DST drift away from the target experience illumination change from the sunlight (#228). Besides illumination change, DST is the only tracker which can effectively deal with out-of-plane pose change (#392). Our novel online negative sampling method collects di-\nverse and relevant background samples, to alleviate the problem of tracker drifting to background regions similar to the target.\nPose variation and deformation: Fig. 11(a) shows the tracking results of the bird2 sequence, in which the target is a bird walks back and forth while undergoing pose variation (#050) and partial occlusion (#011, #093). Other than that, the target is a deformable object which requires rectangular tracking bounding boxes to include much of background region. Generative trackers (ASLA, IVT, SPT) fail to track after heavy partial occlusion (#018) by objects with similar appearances. Only CT and DST remain accurate after the object undergoes a large out-of-plane rotation (#050). ASLA recovers when certain parts of the target become more recognizable with regards to its early appearance. PLS uses a non-adaptive appearance model in its second particle filtering step, which causes drift when significant appearance change such as pose change occurs. The discriminative nature of our observational model is more robust against partial occlusion by similar objects, whereas the invariant amplitude features learned by the stacked autoencoders help the tracker to deal with pose changes. In the bolt sequence, the target object is an athlete who sprints on a track, undergoing shape deformation and gradual pose variation. bolt (Fig. 11(b)) sequence is the most challenging among all tested sequences because all except DST drift away from the target at the beginning (#025). DST performs the best in this sequence and tracks the target well until the end. The success of DST in this sequence is attributed to the highly descriptive representation, formed by the convolutionally trained second-layer features and the edge detector-like first-layer features.\nCluttered background: In the cardark sequence (Fig. 12(a)), there is a low contrast between background and foreground (#272) as well as illumination changes. CT and ODFS fail because they make use of illuminationsensitive Haar-like features as base features and perform update every frame which accumulates tracking errors easily. TLD\u2019s optical flow-based tracking causes gradual drift in the highly cluttered environment while not being able to revert to object detection mode. In situations where appearances of target and background do not change much, DLT and DST perform well because they are both updated in a slower manner depending on the maximum confidence score of all tracking observations. Fig. 12(b) shows some representative tracking results of the football1 sequence, in which the target object is the helmeted head of a football athelete. The sequence is tough because the background is cluttered by atheletes with similar appearances, and the athete undergoes motion blur (#042) and significant pose change (#070). Overall, only ASLA, SPT, and DST perform well. ASLA\u2019s novel alignment pooling makes it less prone to drifting problem when other similar objects are around. By handling motion blur explicity, SPT avoids bad updates and performs better than IVT. DST is able to achieve similar result, using accumulation of training samples to weaken the contribution of bad training samples.\nFast motion and motion blur: Fig. 13 shows the\ntracking results on sequences with fast motion. Objects with fast motion tend to generate motion blur in images captured by the common cameras. The target object in the deer sequence is the head of a quickly moving deer with abrupt location change and motion blur. Overall, all trackers are able to track the target due to heuristically chosen affine settings and fine-tuned search window settings. Furthermore, there is always a big contrast between the target and background (water). PLS experiences slight drift (#041) which is attributed by the fact that its static appearance model does not consider appearance variations in the target and background over time. TLD drifts to a confusingly similar background object (#041) when it cannot track realiably and has to switch to detection mode. For the jumping sequence, the target object undergoes large translation between frames and exhibits significant motion blur. Nearly all discriminative trackers (CT, DLT, ODFS, PLS) except DST perform poorly because they rely on negative sampling methods that do not consider target\u2019s motion pattern. TLD works reasonably well by learning object appearance cautiously through the estimation of false negatives and false positives. Without considering negative templates, generative trackers (ASLA, IVT, SPT) track the target well. DST succeeds in this sequence by virtue of a novel negative sampling method that adapts to the particle filter\u2019s dynamic model."}, {"heading": "6.5. Discussions", "text": "Temporally slow representation versus handcrafted representation: Traditionally, visual tracking\nresearch relies on hand-crafted representations such as SIFT, HOG, and LBP. The main contribution in this paper is to take an alternative approach of learning features for visual tracking. We learn slow invariant features offline via stacked autoencoders and transfer them for online visual tacking. To this end, we isolate the merits of our proprosed tracking representation by substituting it with dense SIFT, HOG, and LBP local descriptors. All other components of our proprosed tracker remain the same. To obtain the hand-crafted descriptors for tracking, we use a reputable computer vision library package known as VLFeat [51]. For SIFT, we use the same spatial stride as our first layer autoencoder and find the best width (in pixels) of SIFT spatial bins, from some possible values close to the first layer\u2019s input size. Then, for HOG and LBP, we find the best setting in terms of cell sizes. Only the settings that yield the best overall results are selected for evaluation. All other parameters in the hand-crafted feature extractions are left in their default settings. Experimentally, we evaluate the handcrafted representation trackers and our proprosed tracker DST on all the 18 benchmark sequences (Section 6.2). The average SR and COL errors for each sequence are shown in Table 3. From the table, it is shown that DST is comparable with trackers substituted with hand-crafted representations. DST is most the well-rounded tracker, achieving the best results in terms of average SR and COL error. The advantage of the proposed representation over hand-crafted representations is especially evident on bird2, bolt, football1, shaking, and surfer sequences, where large pose changes are prevalent.\nEffect of temporal slowness strength: The deep invariant representation proposed in this paper is learned\nby enforcing strong temporal slowness constraint Eq. (2) in the first and second layer autoencoders. To understand the importance of temporal slowness for visual tracking, we train the stacked autoencoders with varying temporal slowness weight \u03b1 and evaluate the learned representations on visual tracking tasks. The two autoencoder layers are assessed independently by fixing the temporal slowness weight of one layer and varying the slowness weight of another. The default settings for the autoencoder parameters follow the same settings in DST (Section 6.1). For the varying temporal slowness strengths, they are obtained by choosing some relevant values lower than the default parameter settings. Due to the fixing of sparsity and reconstruction weights, the reduction of temporal slowness forces the autoencoders to rely more on sparsity and reconstruction to learn features. Experimentally, the tracking representation of DST is substituted with the representations with varying temporal slowness strengths and they are evaluated on all benchmark sequences. The plots of SR and COL errors (averaged from all sequences) against the varying temporal slowness strengths for first and second layer are presented in Fig. 14. The plots demonstrate that tracking performance improves with increased temporal slowness strength in either layer of the proposed stacked autoencoders. To understand the effects of varying temporal slowness strengths on the learned features visually, we present some phase-shifted optimal stimuli with varying \u03b1 in Fig. 15. It is worth noting that a lower temporal slowness strength produces features which are invariant to very limited transformations, and the transitions between the transformations are sudden and unsmooth. Conversely, it is the other way round for features learned with higher temporal slowness strengths, such as Fig. 4, and as well as when first layer\u2019s \u03b1 is 50 and second layer\u2019s \u03b1 is 100.\nOnline negative sampling: Online negative sampling is one of the most important components in discriminative tracking. We propose a novel negative sampling method (Section 4.1) which takes into account the maximum overlap between target object and negative samples. Moreover, the negative sampling method is adapted to the motion-\npattern of the target objects, according to the affine parameter settings of particle filter. In this part, we compare our proposed negative sampling method with existing methods employed by PLS and DLT. PLS draws negative samples in an annular region [48] defined by inner and outer radius, where the inner radius is the radius of circle minimally enclosing the target and outer radius is a fixed parameter. In DLT, negative samples are collected at locations sampled from zero-mean normal distribution, in which the standard deviation is a fixed parameter multiplied by the height or width (depending on the axes) of target object. Generally, PLS collects further negative samples without overlap and DLT collects overlapping negative samples which are very near. To have a fair assessment, we substitute our negative sampling method with the methods from PLS and DLT, and use the same parameter settings. We name our tracker with PLS negative sampling method as N1 and the one with DLT negative sampling method as N2. Experimentally, we evaluate N1, N2, our proposed tracker (DST) on coke, faceocc2, jump-\ning, and shaking challenging benchmark sequences. The COL error plots are presented in Fig. 16. For comparisons, we include the tracking results from PLS and DLT trackers. Noticeably, our proposed negative sampling method is superior to N2 and comparable to N1 in overall. DST performs better than both N1 and N2 in faceocc2 in which there are abrupt lighting and appearance changes. For sequence with abrupt appearance changes and fast motion (jumping), DST does better than N2 and it is comparable to N1. Even without the proposed negative sampling method, N1 performs better than its original counterparts (PLS) in most of the sequences (faceocc2, jumping, and shaking), whereas N2 performs significantly better than DST in sequences with abrupt appearance changes (jumping and shaking), due to the effective deep slow representation."}, {"heading": "7. Conclusion", "text": "This work exploits the temporal slowness principle to learn invariant representations for visual tracking. Temporal slowness constraint is incorporated to a autoencoder algorithm to facilitate representation learning. To allow the learned representations to be specific to visual tracking tasks, large number of tracked image patches are collected via an existing tracker to be the training set. A deep learning model is formed by stacking the autoencoders to learn higher-level invariances with temporal slowness. We then transfer the offline learned representations to an observational model for online visual tracking. The adaptive observational is formulated such that it collects more relevant negative samples and it utilizes accumulative training set to alleviate tracking drift. Tracking is carried out in the particle filter framework to estimate the target state sequentially. Compared to several state-of-the-art trackers, the proposed tracker demonstrates favourable performances in challenging benchmark sequences with various tracking challenges. In future work, we will explore the possibility of online learning of representations using temporal slowness for visual tracking, without relying on selftaught learning paradigm [2]."}], "references": [{"title": "A Survey of Appearance Models in Visual Object Tracking", "author": ["X. Li", "W. Hu", "C. Shen", "Z. Zhang", "A. Dick", "A.V.D. Hengel"], "venue": "ACM Transactions on Intelligent Systems and Technology 4 (4) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": "in: Proceedings of International Conference on Machine Learning, 759\u2013766", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Incremental learning for robust visual tracking", "author": ["D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang"], "venue": "International Journal of Computer Vision 77 (1-3) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficient Online Subspace Learning With an Indefinite Kernel for Visual Tracking and Recognition", "author": ["S. Liwicki", "S. Zafeiriou", "G. Tzimiropoulos", "M. Pantic"], "venue": "IEEE Transactions on Neural Networks and Learning Systems 23 (10) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Correlation-based incremental visual tracking", "author": ["M. Kim"], "venue": "Pattern Recognition 45 (3) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust online appearance models for visual tracking", "author": ["A.D. Jepson", "D.J. Fleet", "T.F. El-Maraghi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 25 (10) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive object tracking based on an effective appearance filter", "author": ["H. Wang", "D. Suter", "K. Schindler", "C. Shen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 29 (9) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Online selection of discriminative tracking features", "author": ["R.T. Collins", "Y. Liu", "M. Leordeanu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (10) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-Time Tracking via On-line Boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "in: Proceedings of the British Machine Vision Conference, vol. 1, BMVA Press, 47\u201356", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Boosting scalable gradient features for adaptive real-time tracking", "author": ["D.A. Klein", "A.B. Cremers"], "venue": "in: Proceedings of IEEE International Conference on Robotics and Automation, IEEE, 4411\u20134416", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Real-time visual tracking via online weighted multiple instance learning", "author": ["K. Zhang", "H. Song"], "venue": "Pattern Recognition 46 (1) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Co-tracking using semisupervised support vector machines", "author": ["F. Tang", "S. Brennan", "Q. Zhao", "H. Tao"], "venue": "in: Proceedings of IEEE International Conference on Computer Vision, IEEE, 1\u20138", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P.H. Torr"], "venue": "in: Proceedings of IEEE International Conference on Computer Vision, IEEE, 263\u2013270", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (8) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "in: NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning image representations from the pixel level via hierarchical sparse coding", "author": ["K. Yu", "Y. Lin", "J. Lafferty"], "venue": "in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 1713\u20131720", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Transferring visual prior for online object tracking", "author": ["Q. Wang", "F. Chen", "J. Yang", "W. Xu", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 21 (7) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Tracking with deep neural networks", "author": ["J. Jonghoon", "A. Dundar", "J. Bates", "C. Farabet", "E. Culurciello"], "venue": "in: Proceedings of Conference on Information Sciences and Systems, 1\u20135", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a Deep Compact Image Representation for Visual Tracking", "author": ["N. Wang", "D.-Y. Yeung"], "venue": "in: Proceedings of Conference on Neural Information Processing Systems, 809\u2013817", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "in: Proceedings of International Conference on Machine Learning, 737\u2013744", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning of invariant features via simulated fixations in video", "author": ["W. Zou", "A. Ng", "S. Zhu", "K. Yu"], "venue": "in: Proceedings of Conference on Neural Information Processing Systems, 3212\u20133220", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "in: Proceedings of Iinternational Conference on Machine Learning, 1096\u20131103", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "in: Proceedings of International Conference on Machine Learning, 833\u2013840", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis", "author": ["Q.V. Le", "W.Y. Zou", "S.Y. Yeung", "A.Y. Ng"], "venue": "in: Proceeedings of IEEE Conference on Computer Vision and Pattern Recognition, 17  IEEE, 3361\u20133368", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "in: Proceedings of Conference on Neural Information Processing Systems, 1017\u20131025", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in: Proceedings of International Conference on Artificial Intelligence and Statistics, vol. 15, 315\u2013323", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Slow", "author": ["Y. Bengio", "J.S. Bergstra"], "venue": "Decorrelated Features for Pretraining Complex Cell-like Networks, in: Proceedings of Conference on Neural Information Processing Systems, 99\u2013107", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Bilinear models of natural images", "author": ["B.A. Olshausen", "C. Cadieu", "J. Culpepper", "D.K. Warland"], "venue": "in: Proceedings of SPIE, vol. 6492, 649206\u2013649206\u201310", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "in: Proceedings of International Conference on Machine Learning, 265\u2013272", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploring strategies for training deep neural networks", "author": ["H. Larochelle", "Y. Bengio", "J. Louradour", "P. Lamblin"], "venue": "The Journal of Machine Learning Research 10 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "in: Proceedings of International Conference on Artificial Intelligence and Statistics, 215\u2013223", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of visual invariance with temporal coherence", "author": ["W.Y. Zou", "A.Y. Ng", "K. Yu"], "venue": "in: NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing higher-layer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Tech. Rep. 1341, University of Montreal", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Online object tracking: A benchmark", "author": ["Y. Wu", "J. Lim", "M.-H. Yang"], "venue": "in: Proceeedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2411\u20132418", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive real-time video-tracking for arbitrary objects", "author": ["D.A. Klein", "D. Schulz", "S. Frintrop", "A.B. Cremers"], "venue": "in: Proceedings of International Conference on Intelligent Robots and Systems, IEEE, 772\u2013777", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual tracking via adaptive structural local sparse appearance model", "author": ["X. Jia", "H. Lu", "M.-H. Yang"], "venue": "in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 1822\u20131829", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "On the analysis of accumulative difference pictures from image sequences of real world scenes", "author": ["R. Jain", "H.-H. Nagel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 1 (2) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1979}, {"title": "On Space-Time Interest Points", "author": ["I. Laptev"], "venue": "International Journal of Computer Vision 64 (2-3) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "in: Proceeedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2169\u20132178", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Human tracking using convolutional neural networks", "author": ["J. Fan", "W. Xu", "Y. Wu", "Y. Gong"], "venue": "IEEE Transactions on Neural Networks 21 (10) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "author": ["N. Gordon", "D. Salmond", "A.F.M. Smith"], "venue": "IEE Proceedings F Radar and Signal Processing 140 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1993}, {"title": "A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking", "author": ["M. Arulampalam", "S. Maskell", "N. Gordon", "T. Clapp"], "venue": "IEEE Transactions on Signal Processing 50 (2) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2002}, {"title": "Superpixel tracking", "author": ["S. Wang", "H. Lu", "F. Yang", "M.-H. Yang"], "venue": "in: Proceedings of IEEE International Conference on Computer Vision, ISSN 1550-5499, 1323\u20131330", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "in: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 983\u2013990", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time compressive tracking", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "in: Proceedings of European Conference on Computer Vi-  sion, Springer, 864\u2013877", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-Time Object Tracking Via Online Discriminative Feature Selection", "author": ["K. Zhang", "L. Zhang", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 22 (12) ", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Object tracking via partial least squares analysis", "author": ["Q. Wang", "F. Chen", "W. Xu", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 21 (10) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Online object tracking with sparse prototypes", "author": ["D. Wang", "H. Lu", "M.-H. Yang"], "venue": "IEEE Transactions on Image Processing 22 (1) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Tracking-Learning- Detection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (7) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "VLFeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "in: Proceedings of the International Conference on Multimedia, ACM, 1469\u20131472", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "A typical visual tracking method is dependent on its two major components [1], namely dynamic model (motion estimation) and observational model.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "The stacked autoencoders are then transferred for use in visual tracking, based on self-taught learning paradigm [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "[3] used Principal Component Analysis (PCA) to construct and incrementally update a subspace model of the target object.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] formulated an incremental kernel PCA in Krein space to learn a nonlinear subspace representation for tracking.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To account for partial occlusion during tracking, Kim [5] proposed a Canonical Correlation Analysis (CCA)-based tracker which considers the correlations among sub-patches of tracking observations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "[6] learned a mixture model to model appearance changes of target object via an online Expectation-Maximisation (EM) algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] develop an adaptive observational model in the joint spatial-color space using Gaussian mixture model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] selected color features online which best discriminates target object from current background.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed an online boosting classifier that adaptively selects discriminative features for tracking.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Klein and Cremers [10] introduced a novel scaleinvariant gradient feature and used boosting to track target object efficiently.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "To alleviate visual drift, Zhang and Song [11] proposed a tracking method based on online multiple instance boosting that handles ambiguously labeled samples and weights each positive sample differently for update.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "[12] trained multiple SVMs, each on an independent feature and locates the target object by combining confidence scores from all SVM classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] presented a structured output SVM to directly predict the trajectory of the target object between frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Representation learning [14] is an emerging field aims to learn good representations from raw input or low-level representations.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "[15]; Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16]) have proven to be superior to conventional hand-engineered representations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] performed sparse coding on SIFT features extracted from labeled object recognition datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] trained a convolutional neural network offline in an unsupervised manner for tracking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "To account for object appearance changes, Wang and Yeung [20] pre-trained a stacked denoising autoencoders and finetuned the deep neural network online during tracking.", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Temporal slowness is one of the major priors for representation learning [14] and it has been successfully used in object recognition tasks (e.", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "[21]; Zou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] showed that temporal slowness constraint can be simply added to conventional autoencoder cost function to learn invariant features, we also choose autoencoder to be our representation learning model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "To allow autoencoder to discover better representations, many autoencoder regularization schemes such as denoising autoencoders [23] and contractive autoencoders [24] have been proposed.", "startOffset": 128, "endOffset": 132}, {"referenceID": 22, "context": "To allow autoencoder to discover better representations, many autoencoder regularization schemes such as denoising autoencoders [23] and contractive autoencoders [24] have been proposed.", "startOffset": 162, "endOffset": 166}, {"referenceID": 18, "context": ", Wang and Yeung [20]), we contend that image representation for visual tracking should be learned in a way more specific to the task.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "Our autoencoder model draws inspiration from Independent Subspace Analysis (ISA) [25] which was proposed for learning motion invariance.", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "In the proposed autoencoder, the reconstruction cost replaces hard orthonormality constraint in ISA to prevent feature degeneracy [26] and the sparsity cost helps to discover interesting features [27] (e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 25, "context": "In the proposed autoencoder, the reconstruction cost replaces hard orthonormality constraint in ISA to prevent feature degeneracy [26] and the sparsity cost helps to discover interesting features [27] (e.", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "the second cost term (temporal slowness constraint), as in [21] and [22], we minimize the temporal representation differences in L-norm to allow invariance to be sparsely represented, that is a kind of motion invariance is represented by only a small number of features and thus they become specialized for different invariances.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "the second cost term (temporal slowness constraint), as in [21] and [22], we minimize the temporal representation differences in L-norm to allow invariance to be sparsely represented, that is a kind of motion invariance is represented by only a small number of features and thus they become specialized for different invariances.", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": ", Bengio and Bergstra [28]; Zou et al.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "[22]) to group similar features in each of the pooled units, therefore achieving invariance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "It pairs every 2 adjacent hidden units to form a complexvalued representation and each pair can be decomposed into amplitude (degree of presence of the features) and phase (transformations of the features over time) variables [29].", "startOffset": 226, "endOffset": 230}, {"referenceID": 28, "context": "(2) can be optimized efficiently using any of the unconstrained optimization methods [30].", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "In this paper, we stack and train a second autoencoder (known as the second layer) on the convolved features of the first autoencoder (known as the first layer) in a greedy layer-wise training fashion [31].", "startOffset": 201, "endOffset": 205}, {"referenceID": 30, "context": "Unlike in object recognition tasks whereby a small k1 is always recommended to achieve good performances [32], there is a great need to balance the feature and run-time performances in visual tracking.", "startOffset": 105, "endOffset": 109}, {"referenceID": 20, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29]; Zou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22]), to visualize what transformations the features are invariant to.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] and use linear combination of filters [34] to visualize the optimal stimuli for the features and invariances learned in the stacked autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] and use linear combination of filters [34] to visualize the optimal stimuli for the features and invariances learned in the stacked autoencoders.", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "In self-taught learning [2], features are learned from unlabeled data and transferred for use in supervised learning tasks, whereby the generating distribution of the unlabeled data is different from the labeled data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 16, "context": "This setting is analogous to the tracking algorithm [17] which exploits patch-level similarity and transfers visual prior from unlabeled dataset to tracking tasks.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "The deep learning-based tracking algorithm [20] which transfer features learned from unlabeled datasets, employ a very large dataset with great amount of visual diversity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "[35] and Klein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36], in which the objects and scenes are diverse in terms of their appearances.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37], tracked image patches are collected randomly from the sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "The first approach is about identifying motional pixels via binary-thresholded accumulative difference pictures [38], and a size filter is used to remove trivially small connected components among the identified pixels.", "startOffset": 112, "endOffset": 116}, {"referenceID": 37, "context": "contrast, the second approach involves a space-time Harris interest point detection algorithm [39] (referred to as STIP) that identifies regions which are \u2018interesting\u2019 spatially and temporally.", "startOffset": 94, "endOffset": 98}, {"referenceID": 20, "context": "[22], we use a larger number of track sessions to encourage diversity in the training dataset, and use a smaller number of frames per track session to minimize the occurences of tracking drift in the dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Due to the use of relatively large convolution strides, spatial pyramid pooling [40] is not performed on the convolved representations.", "startOffset": 80, "endOffset": 84}, {"referenceID": 39, "context": "Doing so would greatly encourage translational invariance, which is not favourable in visual tracking [41].", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "For this, particle filter [42][43] is chosen over other methods (e.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "For this, particle filter [42][43] is chosen over other methods (e.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways.", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways.", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Our proposed tracking method, DST is both similar to and different from the reviewed representation learning trackers [17], [18], [19], and [20] in some ways.", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "In terms of datasets used for training the representation learning models, [17], [19], [20], and DST trains on datasets unrelated (self-taught learning [2]) to the tracking video sequences.", "startOffset": 152, "endOffset": 155}, {"referenceID": 18, "context": "The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "The datasets used in [20] and DST are generic and unlabeled, thus they are different from [17] and [19] which use datasets with some specified object classes.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20].", "startOffset": 207, "endOffset": 211}, {"referenceID": 17, "context": "Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "Besides, the dataset used in DST is different from others, in the sense that we train the stacked autoencoders on tracked image patches instead of temporally uncorrelated object recognition datasets used in [17], [18], [19], and [20].", "startOffset": 229, "endOffset": 233}, {"referenceID": 16, "context": "All of the trackers use raw image patches to learn representations from, except [17] which extracts SIFT features from the patches as bases to build a sparse coded dictionary.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "In terms of observational models adaptivity, [19] is the only tracker that uses a non-adaptive offline classifier to distinguish between target object and object.", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "[17] and [18] including DST employ linear classifiers which are independent of representation learning models, to build adaptive observational models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Wang and Yeung [20] uses a more sophisticated way for classification during tracking, by fine-tuning the deep neural network (unrolled from pre-trained stacked autoencoders) using classification error cost function.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "(2) are set as [100, 20] and [300, 20] for first and second layer respectively.", "startOffset": 15, "endOffset": 24}, {"referenceID": 18, "context": "(2) are set as [100, 20] and [300, 20] for first and second layer respectively.", "startOffset": 29, "endOffset": 38}, {"referenceID": 28, "context": "For unconstrained optimization of the autoencoders, we employ off-the-shelf Limited-memory BroydenFletcher-Goldfarb-Shanno (BFGS) algorithm that is relatively memory-efficient and fast-converging [30].", "startOffset": 196, "endOffset": 200}, {"referenceID": 33, "context": "[35], Wang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44], and Babenko et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 151, "endOffset": 155}, {"referenceID": 2, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 190, "endOffset": 193}, {"referenceID": 45, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 250, "endOffset": 254}, {"referenceID": 46, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 292, "endOffset": 296}, {"referenceID": 47, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 330, "endOffset": 334}, {"referenceID": 48, "context": "The competing trackers are Adaptive Structural Local-sparse Appearance (ASLA) tracker [37], Compressive Tracker (CT) [46], Deep Learning Tracker (DLT) [20], Incremental Visual Tracker (IVT) [3], Online Discriminative Feature Selection (ODFS) tracker [47], Partial Least Squares (PLS) tracker [48], Sparse Prototypes Tracker (SPT) [49], and Tracking-Learning-Detection (TLD) [50].", "startOffset": 374, "endOffset": 378}, {"referenceID": 49, "context": "To obtain the hand-crafted descriptors for tracking, we use a reputable computer vision library package known as VLFeat [51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 46, "context": "PLS draws negative samples in an annular region [48] defined by inner and outer radius, where the inner radius is the radius of circle minimally enclosing the target and outer radius is a fixed parameter.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "In future work, we will explore the possibility of online learning of representations using temporal slowness for visual tracking, without relying on selftaught learning paradigm [2].", "startOffset": 179, "endOffset": 182}], "year": 2016, "abstractText": "Visual representation is crucial for a visual tracking method\u2019s performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target\u2019s motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.", "creator": "LaTeX with hyperref package"}}}