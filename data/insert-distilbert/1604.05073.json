{"id": "1604.05073", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2016", "title": "Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization", "abstract": "we address the problem of narrowly automatically finding specifically the parameters of a statistical machine translation system that maximize bleu scores while ensuring whether that decoding speed exceeds a minimum value. we propose the proposed use of bayesian optimization to efficiently tune the speed - related decoding parameters by easily incorporating speed as from a noisy constraint function. the obtained corresponding parameter values are guaranteed locally to satisfy the speed constraint with an associated confidence margin. across three language domain pairs described and two speed constraint values, we report overall optimization time reduction compared to grid and random interface search. we also show that bayesian optimization can decouple speed and bleu measurements, resulting in a further linear reduction of overall optimization time as speed is gradually measured over a small subset of adjacent sentences.", "histories": [["v1", "Mon, 18 Apr 2016 10:27:49 GMT  (273kb,D)", "http://arxiv.org/abs/1604.05073v1", "To appear at NAACL 2016"]], "COMMENTS": "To appear at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel beck", "adri\u00e0 de gispert", "gonzalo iglesias", "aurelien waite", "bill byrne"], "accepted": true, "id": "1604.05073"}, "pdf": {"name": "1604.05073.pdf", "metadata": {"source": "CRF", "title": "Speed-Constrained Tuning for Statistical Machine Translation Using Bayesian Optimization", "authors": ["Daniel Beck", "Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Aurelien Waite", "Bill Byrne"], "emails": ["debeck1@sheffield.ac.uk", "agispert@sdl.com", "giglesias@sdl.com", "rwaite@sdl.com", "bbyrne@sdl.com"], "sections": [{"heading": "1 Introduction", "text": "Research in Statistical Machine Translation (SMT) aims to improve translation quality, typically measured by BLEU scores (Papineni et al., 2001), over a baseline system. Given a task defined by a language pair and its corpora, the quality of a system is assessed by contrasting choices made in rule/phrase extraction criteria, feature functions, decoding algorithms and parameter optimization techniques.\nSome of these choices result in systems with significant differences in performance. For example, in phrase-based translation (PBMT) (Koehn et al.,\n\u2217 This work was done during an internship of the first author at SDL Research, Cambridge.\n2003), decoder parameters such as pruning thresholds and reordering constraints can have a dramatic impact on both BLEU and decoding speed. However, unlike feature weights, which can be optimized by MERT (Och and Ney, 2004), it is difficult to optimize decoder parameters either for speed or for BLEU.\nWe are interested in the problem of automatically finding the decoder parameters and feature weights that yield the best BLEU at a specified minimum decoding speed. This is potentially very expensive because each change in a decoder parameter requires re-decoding to assess both BLEU and translation speed. This is under-studied in the literature, despite its importance for real-life commercial SMT engines whose speed and latency can be as significant for user satisfaction as overall translation quality.\nWe propose to use Bayesian Optimization (Brochu et al., 2010b; Shahriari et al., 2015) for this constrained optimization task. By using prior knowledge of the function to be optimized and by exploring the most uncertain and the most promising regions of the parameter space, Bayesian Optimization (BO) is able to quickly find optimal parameter values. It is particularly well-suited to optimize expensive and non-differentiable functions such as the BLEU score of a decoder on a tuning set. The BO framework can also incorporate noisy constraints, such as decoder speed measurements, yielding parameters that satisfy these constraints with quantifiable confidence values.\nFor a set of fixed feature weights, we use BO to optimize phrase-based decoder parameters for speed and BLEU. We show across 3 different language\nar X\niv :1\n60 4.\n05 07\n3v 1\n[ cs\n.C L\n] 1\n8 A\npr 2\npairs that BO can find fast configurations with high BLEU scores much more efficiently than other tuning techniques such as grid or random search. We also show that BLEU and decoding speed can be treated as decoupled measurements by BO. This results in a further reduction of overall optimization time, since speed can be measured over a smaller set of sentences than is needed for BLEU.\nFinally, we discuss the effects of feature weights reoptimization after speed tuning, where we show that further improvements in BLEU can be obtained. Although our analysis is done on a phrase-based system with standard decoder parameters (decoding stack size, distortion limit, and maximum number of translations per source phrase), BO could be applied to other decoding paradigms and parameters.\nThe paper is organized as follows. Section 2 gives a brief overview of Bayesian Optimization and describes how it can be applied to our problem, Section 3 reports our speed-constrained tuning experiments, Section 4 reviews related work, and Section 5 concludes."}, {"heading": "2 Bayesian Optimization", "text": "We are interested in finding a global maximizer of an objective function f :\n\u03b8? = arg max \u03b8\u2208\u0398 f(\u03b8) (1)\nwhere \u03b8 is a parameter vector from a search space \u0398. It is assumed that f has no simple closed form but can be evaluated at an arbitrary \u03b8 point. In this paper, we take f as the BLEU score produced by an SMT system on a tuning set, and \u03b8 will be the PBMT decoder parameters.\nBayesian Optimization is a powerful framework to efficiently address this problem. It works by defining a prior model over f and evaluating it sequentially. Evaluation points are chosen to maximize the utility of the measurement, as estimated by an acquisition function that trades off exploration of uncertain regions in \u0398 versus exploitation of regions that are promising, based on function evaluations over all x points gathered so far. BO is particularly well-suited when f is non-convex, nondifferentiable and costly to evaluate (Shahriari et al., 2015)."}, {"heading": "2.1 Prior Model", "text": "The first step in performing BO is to define the prior model over the function of interest. While a number of different approaches exist in the literature, in this work we follow the concepts presented in Snoek et al. (2012) and implemented in the Spearmint1 toolkit, which we detail in this Section. The prior over f is defined as a Gaussian Process (GP) (Rasmussen and Williams, 2006):\nf \u223c GP(m(\u03b8), k(\u03b8, \u03b8\u2032)) (2)\nwhere m and k are the mean and kernel (or covariance) functions. The mean function is fixed to the zero constant function, as usual in GP models. This is not a large restriction because the posterior over f will have non-zero mean in general. We use the Mate\u0300rn52 kernel, which makes little assumptions about the function smoothness.\nThe observations, BLEU scores in our work, are assumed to have additive Gaussian noise over f evaluations. In theory we do not expect variations in BLEU for a fixed set of decoding parameters but in practice assuming some degree of noise helps to make the posterior calculation more stable."}, {"heading": "2.2 Adding Constraints", "text": "The optimization problem of Equation 1 can be extended to incorporate an added constraint on some measurement c(\u03b8):\n\u03b8? = arg max \u03b8\u2208\u0398 f(\u03b8) s.t. c(\u03b8) > t (3)\nIn our setup, c(\u03b8) is the decoding speed of a configuration \u03b8, and t is the minimum speed we wish the decoder to run at. This formulation assumes c is deterministic given a set of parameters \u03b8. However, as we show in Section 3.2, speed measurements are inherently noisy, returning different values when using the same decoder parameters.\nSo, we follow Gelbart et al. (2014) and redefine Equation 3 by assuming a probabilistic model p over c(\u03b8):\n\u03b8? = arg max \u03b8\u2208\u0398\nf(\u03b8) s.t. p(c(\u03b8) > t) \u2265 1\u2212 \u03b4 (4)\nwhere \u03b4 is a user-defined tolerance value. For our problem, the formulation above states that we wish\n1https://github.com/HIPS/Spearmint\nto optimize the BLEU score for decoders that run at speeds faster than t with probability 1\u2212 \u03b4. Like f , c is also assumed to have a GP prior with zero mean, Mate\u0300rn52 kernel and additive Gaussian noise."}, {"heading": "2.3 Acquisition Function", "text": "The prior model combined with observations gives rise to a posterior distribution over f . The posterior mean gives information about potential optima in \u0398, in other words, regions we would like to exploit. The posterior variance encodes the uncertainty in unknown regions of \u0398, i.e., regions we would like to explore. This exploration/exploitation trade-off is a fundamental aspect not only in BO but many other global optimization methods.\nAcquisition functions are heuristics that use information from the posterior to suggest new evaluation points. They naturally encode the exploration/exploitation trade-off by taking into account the full posterior information. A suggestion is obtained by maximizing this function, which can be done using standard optimization techniques since they are much cheaper to evaluate compared to the original objective function.\nMost acquisition functions used in the literature are based on improving the best evaluation obtained so far. However, it has been shown that this approach has some pathologies in the presence of constrained functions (Gelbart et al., 2014). Here we employ Predictive Entropy Search with Constraints (PESC) (Herna\u0301ndez-Lobato et al., 2015), which aims to maximize the information about the global optimum \u03b8?. This acquisition function has been empirically shown to obtain better results when dealing with constraints and it can easily take advantage of a scenario known as decoupled constraints (Gelbart et al., 2014), where the objective (BLEU) and the constraint (speed) values can come from different sets of measurements. This is explained in the next Section.\nAlgorithm 1 summarizes the BO procedure under constraints. It starts with a set D0 of data points (selected at random, for instance), where each data point is a (\u03b8, f, c) triple made of parameter values, one function evaluation (BLEU) and one constraint evaluation (decoding speed). Initial posteriors over the objective and the constraint are calculated2. At\n2Note that the objective posterior p(f |D) does not depend\nevery iteration, the algorithm selects a new evaluation point by maximizing the acquisition function \u03b1, measures the objective and constraint values on this point and updates the respective posterior distributions. It repeats this process until it reaches a maximum number of iterations N , and returns the best set of parameters obtained so far that is valid according to the constraint.\nAlgorithm 1 Constrained Bayesian Optimization Input max. number of iterations N , acquisition\nfunction \u03b1, initial evaluations D0, min. constraint value t, tolerance \u03b4\n1: \u0398 = \u2205 2: for i = 1, . . . , N do 3: select new \u03b8i by maximizing \u03b1:\n\u03b8i = arg max \u03b8\n\u03b1(\u03b8, p(f |Di\u22121), p(c|Di\u22121))\n4: \u0398 = \u0398\u222a \u03b8i 5: query objective f(\u03b8i) 6: query constraint c(\u03b8i) 7: augment data Di = Di \u222a (\u03b8, f, c)i 8: update objective posterior p(f |Di) 9: update constraint posterior p(c|Di)\n10: end for 11: return \u03b8? as per Equation 4"}, {"heading": "2.4 Decoupling Constraints", "text": "Translation speed can be measured on a much smaller tuning set than is required for reliable BLEU scores. In speed-constrained BLEU tuning, we can decouple the constraint by measuring speed on a small set of sentences, while still measuring BLEU on the full tuning set. In this scenario, BO could spend more time querying values for the speed constraint (as they are cheaper to obtain) and less time querying the BLEU objective.\nWe use PESC as the acquisition function because it can easily handle decoupled constraints (Gelbart, 2015, Sec. 4.3). Effectively, we modify Algorithm 1 to update either the objective or the constraint posterior at each iteration, according to what is obtained by maximizing PESC at line 3. This kind of decoupling is not allowed by standard acquisition functions used in BO.\non the constraint measurements, and the constraint posterior p(c|D) does not depend on the objective measurements.\nThe decoupled scenario makes good use of heterogeneous computing resources. For example, we are interested in measuring decoding speed on a specific machine that will be deployed. But translating the tuning set to measure BLEU can be parallelized over whatever computing is available."}, {"heading": "3 Speed Tuning Experiments", "text": "We report translation results in three language pairs, chosen for the different challenges they pose for SMT systems: Spanish-to-English, English-toGerman and Chinese-to-English. For each language pair, we use generic parallel data extracted from the web. The data sizes are 1.7, 1.1 and 0.3 billion words, respectively.\nFor Spanish-to-English and English-to-German we use mixed-domain tuning/test sets, which have about 1K sentences each and were created to evenly represent different domains, including world news, health, sport, science and others. For Chinese-toEnglish we use in-domain sets (2K sentences) created by randomly extracting unique parallel sentences from in-house parallel text collections; this in-domain data leads to higher BLEU scores than in the other tasks, as will be reported later. In all cases we have one reference translation.\nWe use an in-house implementation of a phrase-based decoder with lexicalized reordering model (Galley and Manning, 2008). The system uses 21 features, whose weights are optimized for BLEU via MERT (Och and Ney, 2004) at very slow decoder parameter settings in order to minimize search errors in tuning. The feature weights remain fixed during the speed tuning process."}, {"heading": "3.1 Decoder Parameters", "text": "We tune three standard decoder parameters \u03b8 = (d, s, n) that directly affect the translation speed. We describe them next.\nd: distortion limit. The maximum number of source words that may be skipped by the decoder as it generates phrases left-to-right on the target side.\ns: stack size. The maximum number of hypotheses allowed to survive histogram pruning in each decoding stack.\nn: number of translations. The maximum number of alternative translations per source phrase considered in decoding."}, {"heading": "3.2 Measuring Decoding Speed", "text": "To get a better understanding of the speed measurements we decode the English-German tuning set 100 times with a slow decoder parameter setting, i.e. \u03b8 = (5, 100, 100), and repeat for a fast setting with \u03b8 = (0, 1, 1). We collect speed measurements in number of translated words per minute (wpm)3.\nThe plots in Figure 1 show histograms containing the measurements obtained for both slow and fast settings. While both fit in a Gaussian distribution, the speed ranges approximately from 750 to 950 wpm in the slow setting and from 90K to 120K wpm in the fast setting. This means that speed measurements exhibit heteroscedasticity: they follow Gaussian distributions with different variances that depend on the decoder parameter values. This is a problem for our BO setting because the GP we use to model the constraint assumes homoscedasticity, or constant noise over the support set \u0398.\nA simple way to reduce the effect of heteroscedasticity is to take the logarithm of the speed measurements, which is also a standard practice when modeling non-negative measures in a GP (Gelbart et al., 2014). Table 1 shows the values for mean and standard deviation before and after the log transformation. Using the logarithm keeps the GP inference\n3Measured on an Intel Xeon E5-2450 at 2.10GHz.\nformulas tractable so we use this solution in our experiments."}, {"heading": "3.3 BO Details and Baselines", "text": "All BO experiments use Spearmint (Snoek et al., 2012) with default values unless explicitly stated otherwise. We set the minimum and maximum values for d, s and n as [0, 10], [1, 500] and [1, 100], respectively. We model d in linear scale but s and n in logarithmic scale for both BO and the baselines. This scaling is based on the intuition that optimal values for s and n will be in the lower interval values, which was confirmed in preliminary experiments on all three datasets.\nWe run two sets of experiments, using 2000wpm and 5000wpm as minimum speed constraints. In addition, we use the following BO settings:\nStandard (BO-S): in this setting each BO iteration performs a full decoding of the tuning set in order to obtain both the BLEU score and the decoding speed jointly. We use \u03b4 = 0.01 as the constraint tolerance described in Section 2.2.\nDecoupled (BO-D): here we decouple the objective and the constraint as explained in Section 2.4. We still decode the full tuning set to get BLEU scores, but speed measurements are taken from a smaller subset of 50 sentences. Since speed measurements are faster in this case, we enforce BO to query for speed more often by modeling the task duration as described by Snoek et al. (2012). We use a higher constraint tolerance (\u03b4 = 0.05), as we found that BO otherwise focused on the speed constraints at the expense of optimizing BLEU.\nWe compare these settings against two baselines: grid search and random search (Bergstra and Bengio, 2012). Grid search and random search seek parameter values in a similar way: a set of parameter values is provided; the decoder runs over the tuning set for all these values; the parameter value that\nyields the highest BLEU at a speed above the constraint is returned. For grid search, parameter values are chosen to cover the allowed value range in even splits given a budget of a permitted maximum number of decodings. For random search, parameters are chosen from a uniform distribution over the ranges specified above. BO-S, grid search and random search use a maximum budget of 125 decodings. BO-D is allowed a larger budget of 250 iterations, as the speed measurements can be done quickly. This is not a bias in favour of BO-D, as the overall objective is to find the best, fast decoder in as little CPU time as possible."}, {"heading": "3.4 Results", "text": "Our results using the 2000wpm speed constraint are shown in Figure 2. The solid lines in the figure show the tuning set BLEU score obtained from the current best parameters \u03b8, as suggested by BO-S, as a function of CPU time (in logarithmic scale). Given that \u03b4 = 0.01, we have a 99% confidence under the GP model that the speed constraint is met.\nFigure 2 also shows the best BLEU scores of fast systems found by grid and random search at increasing budgets of 8, 27, and 125 decodings of the tuning set4. These results are represented by squares/circles of different sizes in the plot: the larger the square/circle, the larger the budget. For grid and random search we report only the single highest BLEU score found amongst the sufficiently fast systems; the CPU times reported are the total time spent decoding the batch. For BO, the CPU times include both decoding time and the time spent evaluating the acquisition function for the next decoder parameters to evaluate (see Section 3.5).\nIn terms of CPU time, BO-S finds optimal parameters in less time than either grid search or random search. For example, in Spanish-to-English, BO-S takes \u223c70 min (9 iterations) to achieve 36.6 BLEU score. Comparing to the baselines using a budget of 27 decodings, random search and grid search need \u223c160 min and \u223c6 hours, respectively, to achieve 36.5 BLEU. Note that, for a given budget, grid search proves always slower than random search because it always considers parameters val-\n4For grid search, these correspond to 2, 3 and 5 possible values per parameter.\nues at the high end of the ranges (which are the slowest decoding settings).\nIn terms of translation quality, we find that BO-S reaches the best BLEU scores across all language pairs, although all approaches eventually achieve similar scores, except in Chinese-to-English where random search is unable to match the BO-S BLEU score even after 125 decodings.\nThe dotted lines show the results obtained by the decoupled BO-D approach. BO-D does manage to find good BLEU scores, but it proceeds somewhat erratically. As the figure shows, BO-D spends a good deal of time testing systems at parameter values that are too slow. There are also negative excursions in the BLEU score, which we observed were due to updates of the posterior constraint model. For\neach new iteration, the confidence on the best parameter values may decrease, and if the confidence drops below 1\u2212 \u03b4, then BO suggests parameter values which are more likely to satisfy the speed constraint; this potentially hurts BLEU by decoding too fast. Interestingly, this instability is not seen on the Chinese-to-English pair. We speculate this is due to the larger tuning set for this language pair. Because the task time difference between BLEU and speed measurements is higher compared to the other language pairs, BO-D tends to query speed more in this case, resulting in a better posterior for the constraint.\nOur results using the stricter 5000 wpm speed constraint are shown in Figure 3. As in the 2000wpm case, BO-S tends to find better parameter values faster than any of the baselines. One exception is found in Spanish-to-English after \u223c40 min, when random search finds a better BLEU after 8 iterations when compared to BO-S. However, later BOS catches up and finds parameters that yield the same score. In Chinese-to-English BO is able to find pa-\nrameters that yield significantly better BLEU scores than any of the baselines. It appears that the harsher the speed constraint, the more difficult the optimization task, and the more chances BO will beat the baselines.\nInterestingly, the decoupled BO-D approach is more stable than in the less strict 2000wpm case. After some initial oscillations in BLEU for English-toGerman, BO-D curves climb to optimal parameters in much less CPU time than BO-S. This is clearly seen in Spanish-to-English and Chinese-to-English. We conclude that the harsher the speed constraint, the more benefit in allowing BO to query for speed separately from BLEU.\nTables 2 and 3 report the final parameters \u03b8 found by each method after spending the maximum allowed budget, and the BLEU and speed measured (average of 3 runs) when translating the tuning and test using \u03b8. These show how different each language pair behaves when optimizing for speed and BLEU. For Spanish-to-English and English-toGerman it is possible to find fast decoding configurations (well above 5K wpm) that nearly match the BLEU score of the slow system used for MERT tuning, i.e. \u03b8MERT = (10, 1000, 500). In contrast, significant degradation in BLEU is observed at 5000wpm for Chinese-to-English, a language pair with complicated reordering requirements \u2013 notice that all methods consistently keep a very high distortion limit for this language pair. However, both BO-S and BO-D strategies yield better performance on test (at least +0.5BLEU improvement) than the grid and random search baselines.\nOnly BO is able to find optimal parameters across all tasks faster. The optimum parameters yield similar performance on the tuning and test sets, allowing for the speed variations discussed in Section 3.2. All the optimization procedures guarantee that the constraint is always satisfied over the tuning set. However, this strict guarantee does not necessarily extend to other data in the same way that there might be variations in BLEU score. This can be seen in the Chinese-English experiments. Future work could focus on improving the generalization of the confidence over constraints."}, {"heading": "3.5 BO Time Analysis", "text": "The complexity of GP-based BO is O(n3), n being the number of GP observations, or function evaluations (Rasmussen and Williams, 2006). As the objective function f is expected to be expensive, this\nshould not be an issue for low budgets. However, as the number of iterations grows there might reach a point at the time spent on the GP calculations surpasses the time spent evaluating the function.\nThis is investigated in Figure 4, where the time spent in decoding versus BO (in logarithmic scale) for Chinese-to-English using the 2K wpm constraint is reported, as a function of the optimization iteration. For BO-S (top), decoding time is generally constant but can peak upwards or downwards depending on the chosen parameters. For BO-D (bottom), most of the decoding runs are faster (when BO is querying for speed), and shoot up significantly only when the full tuning set is decoded (when BO is querying for BLEU). For both cases, BO time increases with the number of iterations, becoming nearly as expensive as decoding when a high maximum budget is considered. As shown in the previous section, this was no problem for our speed-tuning experiments because optimal parameters could be found with few iterations, but more complex settings (for example, with more decoder parameters) might require more iterations to find good solutions. For these cases the time spent in BO could be significant."}, {"heading": "3.6 Reoptimizing Feature Weights", "text": "We have used BO to optimize decoder parameters for feature weights that had been tuned for BLEU using MERT. However, there is no reason to believe\nthat the best feature weights for a slow setting are also the best weights at the fast settings we desire.\nTo assess this we now fix the decoder parameters \u03b8 and re-run MERT on Chinese-to-English with 2000 wpm using the fast settings found by BO-S: \u03b8BO = (10, 25, 100) in Table 2. We run MERT starting from flat weights (MERT-flat) and from the optimal weights (MERT-opt), previously tuned for the MERT baseline with \u03b8MERT . Table 4 reports the results.\nWe find that MERT-opt is able to recover from the BLEU drops observed during speed-constrained tuning and close the gap with the slow baseline (from 41.9 to 42.4 BLEU at 1.8 Kwpm, versus 42.5 for MERT at only 51wpm). Note that this performance is not achieved using MERT-flat, so rather than tune from flat parameters in a fixed fast setting, we conclude that it is better to: (1) use MERT to find feature weights in slow settings; (2) optimize decoder parameters for speed; (3) run MERT again with the fast decoder parameters from the feature weights found at the slow settings. As noted earlier, this may reduce the impact of search errors encountered in MERT when decoding at fast settings. However, this final application MERT is unconstrained and there is no guarantee that it will yield a decoder configuration that satisfies the constraints. This must be verified through subsequent testing.\nIdeally, one should jointly optimize decoder parameters, feature weights and all decisions involved in building an SMT system, but this can be very challenging to do using only BO. We note anecdotally that we have attempted to replicate the feature weight tuning procedure of Miao et al. (2014) but obtained mixed results on our test sets. Effective ways to combine BO with well-established feature tuning algorithms such as MERT could be a promising research direction."}, {"heading": "4 Related Work", "text": "Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT, Miao et al. (2014) use BO for feature weight tuning and report better results in some language pairs when compared to traditional tuning algorithms.\nOur approach is heavily based on the work of Gelbart et al. (2014) and Herna\u0301ndez-Lobato et al. (2015) which uses BO in the presence of unknown constraints. They set speed and memory constraints on neural network trainings and report better results compared to those of naive models which explicitly put high costs on regions that violate constraints. A different approach based on augmented Lagrangians is proposed by Gramacy et al. (2014). The authors apply BO in a water decontamination setting where the goal is to find the optimal pump positioning subject to restrictions on water and contaminant flows. All these previous work in constrained BO use GPs as the prior model.\nOptimizing decoding parameters for speed is an understudied problem in the MT literature. Chung and Galley (2012) propose direct search methods to optimize feature weights and decoder parameters jointly but aiming at the traditional goal of maximizing translation quality. To enable search parameter optimization they enforce a deterministic time penalty on BLEU scores, which is not ideal due to the stochastic nature of time measurements shown\non Section 3.2 (this issue is also cited by the authors in their manuscript). It would be interesting to incorporate their approach into BO for optimizing translation quality under speed constraints."}, {"heading": "5 Conclusion", "text": "We have shown that Bayesian Optimisation performs well for translation speed tuning experiments and is particularly suited for low budgets and for tight constraints. There is much room for improvement. For better modeling of the speed constraint and possibly better generalization in speed measurements across tuning and test sets, one possibility would be to use randomized sets of sentences. Warped GPs (Snelson et al., 2003) could be a more accurate model as they can learn transformations for heteroscedastic data without relying on a fixed transformation, as we do with log speed measurements.\nModelling of the objective function could also be improved. In our experiments we used a GP with a Mate\u0300rn52 kernel, but this assumes f is doubly-differentiable and exhibits Lipschitzcontinuity (Brochu et al., 2010b). Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al., 2010) could yield better results. Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al., 2014).\nIn our application we treat Bayesian Optimisation as a sequential model. Parallel approaches do exist (Snoek et al., 2012; Gonza\u0301lez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements. However for more complex optimisation scenarios or for problems that require lengthy searches, parallelization might be needed to keep the computations required for optimisation in line with what is needed to measure translation speed and quality."}], "references": [{"title": "Random Search for Hyper-Parameter", "author": ["Bergstra", "Bengio2012] James Bergstra", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bergstra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2012}, {"title": "Algorithms for Hyper-Parameter Optimization", "author": ["R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In Proceedings of NIPS", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "A Bayesian Interactive Optimization Approach to Procedural Animation Design", "author": ["Brochu et al.2010a] Eric Brochu", "Tyson Brochu", "Nando de Freitas"], "venue": "In Proceedings of ACM SIGGRAPH/Eurographics Symposium on Computer Animation", "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. arXiv:1012.2599v1 [cs.LG", "author": ["Brochu et al.2010b] Eric Brochu", "Vlad M. Cora", "Nando de Freitas"], "venue": null, "citeRegEx": "Brochu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2010}, {"title": "Direct Error Rate Minimization for Statistical Machine Translation", "author": ["Chung", "Galley2012] Tagyoung Chung", "Michel Galley"], "venue": "In Proceedings of WMT,", "citeRegEx": "Chung et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2012}, {"title": "A Simple and Effective Hierarchical Phrase Reordering Model", "author": ["Galley", "Manning2008] Michel Galley", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Galley et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2008}, {"title": "Bayesian Optimization with Unknown Constraints", "author": ["Jasper Snoek", "Ryan P. Adams"], "venue": "In Proceedings of UAI", "citeRegEx": "Gelbart et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gelbart et al\\.", "year": 2014}, {"title": "Constrained Bayesian Optimization and Applications", "author": ["Michael A. Gelbart"], "venue": "Ph.D. thesis,", "citeRegEx": "Gelbart.,? \\Q2015\\E", "shortCiteRegEx": "Gelbart.", "year": 2015}, {"title": "Batch Bayesian Optimization via Local Penalization", "author": ["Zhenwen Dai", "Philipp Hennig", "Neil D. Lawrence"], "venue": null, "citeRegEx": "Gonz\u00e1lez et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gonz\u00e1lez et al\\.", "year": 2015}, {"title": "Modeling an Augmented Lagrangian for Blackbox Constrained Optimization", "author": ["Genetha A. Gray", "Sebastien Le Digabel", "Herbert K.H. Lee", "Pritam Ranjan", "Garth Wells", "Stefan M. Wild"], "venue": null, "citeRegEx": "Gramacy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gramacy et al\\.", "year": 2014}, {"title": "Predictive Entropy Search for Bayesian Optimization with Unknown Constraints", "author": ["Michael A. Gelbart", "Matthew W. Hoffman", "Ryan P. Adams", "Zoubin Ghahramani"], "venue": "Proceedings of ICML", "citeRegEx": "Hern\u00e1ndezLobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndezLobato et al\\.", "year": 2015}, {"title": "Sequential Model-based Optimization for General Algorithm Configuration", "author": ["Hutter et al.2011] Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In Proceedings of LION", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Bayesian Optimisation for Machine", "author": ["Miao et al.2014] Yishu Miao", "Ziyu Wang", "Phil Blunsom"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2014}, {"title": "The Alignment Template Approach to Statistical Machine Translation", "author": ["Och", "Ney2004] Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Och et al\\.", "year": 2004}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2001}, {"title": "Gaussian processes for machine learning, volume 1", "author": ["Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Bbn system description for wmt10 system combination task", "author": ["Bing Zhang", "Spyros Matsoukas", "Richard Schwartz"], "venue": "In Proceedings of WMT and MetricsMATR,", "citeRegEx": "Rosti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rosti et al\\.", "year": 2010}, {"title": "Taking the Human Out of the Loop : A Review of Bayesian Optimization", "author": ["Kevin Swersky", "Ziyu Wang", "Ryan P. Adams", "Nando de Freitas"], "venue": "Technical Report 1,", "citeRegEx": "Shahriari et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2015}, {"title": "Practical Bayesian optimization of Machine Learning Algorithms", "author": ["Snoek et al.2012] Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": "In Proceedings of NIPS", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Multi-task Bayesian Optimization", "author": ["Jasper Snoek", "Ryan P. Adams"], "venue": "In Proceedings of NIPS", "citeRegEx": "Swersky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2013}, {"title": "Freeze-Thaw Bayesian Optimization. arXiv:1406.3896v1 [stat.ML", "author": ["Jasper Snoek", "Ryan P. Adams"], "venue": null, "citeRegEx": "Swersky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Swersky et al\\.", "year": 2014}, {"title": "Lattice Minimum Bayes-Risk decoding for statistical machine translation", "author": ["Tromble et al.2008] Roy Tromble", "Shankar Kumar", "Franz Och", "Wolfgang Macherey"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Tromble et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tromble et al\\.", "year": 2008}, {"title": "Bayesian Multi-Scale Optimistic Optimization", "author": ["Wang et al.2014] Ziyu Wang", "Babak Shakibi", "Lin Jin", "Nando de Freitas"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Efficient Hyper-parameter Optimization for NLP Applications", "author": ["Wang et al.2015] Lidan Wang", "Minwei Feng", "Bowen Zhou", "Bing Xiang", "Sridhar Mahadevan"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Research in Statistical Machine Translation (SMT) aims to improve translation quality, typically measured by BLEU scores (Papineni et al., 2001), over a baseline system.", "startOffset": 121, "endOffset": 144}, {"referenceID": 18, "context": "We propose to use Bayesian Optimization (Brochu et al., 2010b; Shahriari et al., 2015) for this constrained optimization task.", "startOffset": 40, "endOffset": 86}, {"referenceID": 18, "context": "BO is particularly well-suited when f is non-convex, nondifferentiable and costly to evaluate (Shahriari et al., 2015).", "startOffset": 94, "endOffset": 118}, {"referenceID": 19, "context": "While a number of different approaches exist in the literature, in this work we follow the concepts presented in Snoek et al. (2012) and implemented in the Spearmint1 toolkit, which we detail in this Section.", "startOffset": 113, "endOffset": 133}, {"referenceID": 6, "context": "So, we follow Gelbart et al. (2014) and redefine Equation 3 by assuming a probabilistic model p over c(\u03b8):", "startOffset": 14, "endOffset": 36}, {"referenceID": 6, "context": "However, it has been shown that this approach has some pathologies in the presence of constrained functions (Gelbart et al., 2014).", "startOffset": 108, "endOffset": 130}, {"referenceID": 6, "context": "This acquisition function has been empirically shown to obtain better results when dealing with constraints and it can easily take advantage of a scenario known as decoupled constraints (Gelbart et al., 2014), where the objective (BLEU) and the constraint (speed) values can come from different sets of measurements.", "startOffset": 186, "endOffset": 208}, {"referenceID": 6, "context": "A simple way to reduce the effect of heteroscedasticity is to take the logarithm of the speed measurements, which is also a standard practice when modeling non-negative measures in a GP (Gelbart et al., 2014).", "startOffset": 186, "endOffset": 208}, {"referenceID": 19, "context": "All BO experiments use Spearmint (Snoek et al., 2012) with default values unless explicitly stated otherwise.", "startOffset": 33, "endOffset": 53}, {"referenceID": 19, "context": "Since speed measurements are faster in this case, we enforce BO to query for speed more often by modeling the task duration as described by Snoek et al. (2012). We use a higher constraint tolerance (\u03b4 = 0.", "startOffset": 140, "endOffset": 160}, {"referenceID": 13, "context": "We note anecdotally that we have attempted to replicate the feature weight tuning procedure of Miao et al. (2014) but obtained mixed results on our test sets.", "startOffset": 95, "endOffset": 114}, {"referenceID": 19, "context": "Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al.", "startOffset": 107, "endOffset": 150}, {"referenceID": 1, "context": "Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al.", "startOffset": 107, "endOffset": 150}, {"referenceID": 11, "context": ", 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems.", "startOffset": 8, "endOffset": 273}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks.", "startOffset": 8, "endOffset": 549}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system.", "startOffset": 8, "endOffset": 772}, {"referenceID": 0, "context": ", 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT, Miao et al. (2014) use BO for feature weight tuning and report better results in some language pairs when compared to traditional tuning algorithms.", "startOffset": 8, "endOffset": 892}, {"referenceID": 6, "context": "Our approach is heavily based on the work of Gelbart et al. (2014) and Hern\u00e1ndez-Lobato et al.", "startOffset": 45, "endOffset": 67}, {"referenceID": 6, "context": "Our approach is heavily based on the work of Gelbart et al. (2014) and Hern\u00e1ndez-Lobato et al. (2015) which uses BO in the presence of unknown constraints.", "startOffset": 45, "endOffset": 102}, {"referenceID": 6, "context": "Our approach is heavily based on the work of Gelbart et al. (2014) and Hern\u00e1ndez-Lobato et al. (2015) which uses BO in the presence of unknown constraints. They set speed and memory constraints on neural network trainings and report better results compared to those of naive models which explicitly put high costs on regions that violate constraints. A different approach based on augmented Lagrangians is proposed by Gramacy et al. (2014). The authors apply BO in a water decontamination setting where the goal is to find the optimal pump positioning subject to restrictions on water and contaminant flows.", "startOffset": 45, "endOffset": 440}, {"referenceID": 22, "context": "Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 17, "context": ", 2008) or expected BLEU (Rosti et al., 2010) could yield better results.", "startOffset": 25, "endOffset": 45}, {"referenceID": 20, "context": "Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 21, "context": ", 2013) or freeze-thaw optimization (Swersky et al., 2014).", "startOffset": 36, "endOffset": 58}, {"referenceID": 19, "context": "Parallel approaches do exist (Snoek et al., 2012; Gonz\u00e1lez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements.", "startOffset": 29, "endOffset": 72}, {"referenceID": 8, "context": "Parallel approaches do exist (Snoek et al., 2012; Gonz\u00e1lez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements.", "startOffset": 29, "endOffset": 72}], "year": 2016, "abstractText": "We address the problem of automatically finding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efficiently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated confidence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences.", "creator": "LaTeX with hyperref package"}}}