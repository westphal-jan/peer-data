{"id": "1604.05557", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2016", "title": "AGI and Reflexivity", "abstract": "we define each a property of intelligent systems, which we call reflexivity. in human beings, it is one aspect of consciousness, and an element of deliberation. we propose a conjecture, that this property is conditioned by a topological intuitive property of the processes which implement this reflexivity. these processes may be symbolic, or non symbolic e. g. connexionnist. an architecture which implements reflexivity may ultimately be based on the interaction ability of one or several modules of deep human learning, which may be specialized or not, accessible and interconnected in a relevant way. a necessary condition of reflexivity is the existence of recurrence in its processes, we will examine in which cases this condition may inevitably be sufficient. we will then examine how this topology and this property make possible the expression of a second property, satisfying the deliberation. in a final paragraph, we propose an evaluation of intelligent processing systems, based on the fulfillment of all characteristics or some of these properties.", "histories": [["v1", "Fri, 15 Apr 2016 19:39:54 GMT  (634kb)", "http://arxiv.org/abs/1604.05557v1", "submitted to ECAI-2016"], ["v2", "Wed, 20 Apr 2016 18:48:02 GMT  (343kb)", "http://arxiv.org/abs/1604.05557v2", "submitted to ECAI-2016"], ["v3", "Thu, 28 Apr 2016 18:49:12 GMT  (277kb)", "http://arxiv.org/abs/1604.05557v3", "submitted to ECAI-2016"]], "COMMENTS": "submitted to ECAI-2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pascal faudemay"], "accepted": false, "id": "1604.05557"}, "pdf": {"name": "1604.05557.pdf", "metadata": {"source": "CRF", "title": "AGI and reflexivity", "authors": ["Pascal Faudemay"], "emails": [], "sections": [{"heading": null, "text": "call Reflexivity. In human beings it is one aspect of consciousness, and an element of deliberation.\nWe propose a conjecture, that this property is conditioned by a topological property of the processes which implement this reflexivity. These processes may be symbolic, or non symbolic e.g. connexionnist. An architecture which implements reflexivity may be based on the interaction of one or several modules of deep learning, which may be specialized or not, and interconnected in a relevant way. A necessary condition of reflexivity is the existence of recurrence in its processes, we will examine in which cases this condition may be sufficient.\nWe will then examine how this topology and this property make possible the expression of a second property, the deliberation. In a final paragraph, we propose an evaluation of intelligent systems, based on the fulfillment of all or some of these properties.\n1. INTRODUCTION\n1.1. Focusing on high-level functions\nThe goal of Artificial General Intelligence (AGI) is to model human intelligence (or at first time intelligence of superior animals), and to make available in a computer system, the capabilities of a human being. AGI differ in this way from Specialized Artificial Intelligence, which target more limited application domains.\nAn AGI may also designate a prototype of an Artificial General Intelligence, more or less close to the goals of this field [4, 11, 33]. It may also be a seed AGI (or Baby AGI), with a limited initial knowledge or even intelligence, but a good learning capability [38].\nAmong the functions of human mind, we consider \u201csuperior\u201d functions, including goals management, attention, emotions, inference and analogy, conceptual blending, deliberation. Some of them are mainly unconscious, such as attention and part of inference. Others are part of consciousness, like deliberation.\n1.2. An axiomatic approach\nWe propose an axiomatic approach, and therefore it should be as simple as possible. However, let\u2019s notice that we do not consider\n1 Previously at LIP6, Paris, France. Presently retired.\nIf the previous states which are considered begin at the origin of life of the system, we shall say that this reflexivity is unlimited, and if these states begin at the origin of the considered data system, we shall say that this reflexivity is infinite.\nThe reference to emotions is explained at paragraph 4.1.\nDefinition 2. Deliberation is the production of a flow of sentences in a natural or artificial language (or of tuples which represent sentences, or of lists of concepts), by a reflexive system in which these sentences are part of the future cognitive processes.\nWe first present in a more systematic way reflexivity and deliberation, then we show how the implementation of reflexivity implies recurrence. We then discuss which modalities of recurrence guarantee reflexivity, and at which conditions. The recurrence phenomenon may be observed with a partially or entirely non symbolic architecture, but also with an architecture based on the management of symbolic data (sentences or tuples). We also propose some aspects of the recurrent management of deliberation. A last paragraph concludes on the limits of reflexivity as a function of the complexity of the underlying architecture, and proposes an evaluation scheme for reflexive and deliberative AGIs.\n2. REFLEXIVITY.\nAn intelligent system implemented by a neural network [18] is reflexive if it has the following properties :\nSt = H(Wt-1, P, E, A, St-1) (1) Kt = F( Wo, P, E, A, S). (2) Wt = G(Wt-1, P, E, A, S). (3)\nAnd the following one:\nSj=0, t+1 = T(Sj=jmax, t, P, E, A). (4)\nW, P, E, A, S, K, are matrices :\nP (perceptions) W (synaptic weights) A (bias or activation thresholds) S (states of neurons)\nContact: pascal.faudemay @ gmail.com\n- 2 -\nK (actions of the system) E (emotions)\nThese matrices have the following dimensions:\n Time t=0 to i\n Layers j=0 \u00e0 jmax\n Neurons k=0 to kmax(j) in layer j\n For weights w, the index of targets neurons k\u2019=0 to\nk\u2019max(j+1)\nWe shall note Wo as a weights matrix at time t=0, j the index of a neurons layer, jmax the index of the upmost neurons layer (which is projected by recurrence on layer j=0), k the index of a neuron in layer j.\nWhen j=0 then P=Sj, and when j=p then K= Sj Therefore (2) is directly derived from (1). (3) does not seem to be a direct condition of reflexivity.\nEquation (1) and (4) define a topology of the neural network, or of a symbolic system with comparable properties.\nHere by topology we mean a set of properties which can be expressed as properties of a related graph.\nThe properties displayed by equations (1) to (4) define the network as a recurrent neural network (RNN) [16, 18]. However, these conditions (or equivalent ones) are probably not sufficient.\nOur project is to reach the definition of sufficient conditions for the implementation of reflexivity in a neuron network. This does not forbid its implementation with a symbolic architecture, possibly with an equivalent topology.\n<page>\n<title>Antichrist</title> <id>865</id> <revision>\n<id>15900676</id> <timestamp>2002-08-\n03T18:14:12Z</timestamp>\n<contributor>\n<username>Paris</username> <id>23</id>\n</contributor> <minor /> <comment>Automated\nconversion</comment>\n<text\nxml:space=\"preserve\">#REDIRECT [[Christianity]]</text>\n</revision>\n</page>\nFig. 1. XML produced by a RNN. (Andrej Karpathy,2015, [16]).\nWith suitably chosen parameters, and a relevant network topology and size, it is possible to teach a neuron network how\nto invent XML structures, random but realistic Wikipedia pages, etc. We give here an example proposed by Karpathy, of an XML file produced by such a system.\nThis example shows the capability of the deep RNN (i.e. a RNN with many layers, typically 20 or more) to learn both the grammar of an XML structure, and the semantic or the vocabulary of a specific field. Let's remember that data are input character by character, and that the system has no prior knowledge of XML or the utilized language."}, {"heading": "1. DELIBERATION", "text": "Deliberation is the fact for the system to infer sentences based on the ones which it has already produced by reflexive processes, to produce such sentences and to decide which sentences among them, it can output towards the external world.\nThus it is a level of meta-inference, which can be implemented by a deep learning system, or by a symbolic process, e.g. derived from one of the existing pre-AGI projects.\nWe will assume that the deliberation is done by a neuronal subsystem, referenced as B, which receives as input the productions of the previously defined reflexive system, and produces as an output a flow of sentences (or more generally files) or actions, by which it communicates with its environment.\nIn this schema we have : IN the input unit, of one character (one byte, or possibly two bytes if we want to allow complex Unicode characters)\n- 3 -\nA0..An: a 1D systolic vector A and B: two recurrent neuron networks, where the output layer is projected on the input layer. K1 to Kn output units, with actions L1 to Lj, and the sentence Lj+1 to Ln. This device can possibly be a 1D systolic vector. OUTPUT output character of network B, and/ or a Boolean which can control the output character from device A.\nWe do not represent the register stack which may manage a short term memory for unit A input or output, and possibly also two other register stacks for unit B, under the control of dedicated actions from these two units. The access of a neural network to external memories is subject to active research, which we briefly mention in paragraph 4.14. These external memories do not necessarily need to be limited to a register stack [15, 17].\nIn order for process B to be effective, it must have the same knowledge of the outside world as the reflexive system A, and be itself a reflexive system. It must also have one or more supplementary neuron layers, connected to a single decision neuron (or to a group of neurons with majority decision), which will produce the result <OUTPUT> or <DON\u2019T OUTPUT>. It may also directly produce one or several sentences of its internal dialogue.\nFor this purpose, one solution is that system B would have an initial configuration equivalent to that of system A, and would learn from the outside world in an identical way as A. We shall leave to further research to know if B must, or must not, be identical to A."}, {"heading": "2. MODALITIES", "text": "2.1. Emotions.\nIn human beings as well as in animals, emotions are conscious states which are accompanied by a sensation, notably of pleasure or displeasure, and which have an effect on memorization. Emotions have a degree or an intensity. Perceptions and states of consciousness which are accompanied by a more intense emotion, are more memorized than those accompanied by a less intense emotion [8, 20, 22].\nThese consciousness states are the result or the cause of the presence in brain of neurotransmitters or neuromediators. L\u00f6vheim [19] proposes a representation of emotions by a cube, as a function of the presence or absence of three neuromediators, dopamin, noradrenalin and serotonin, which have a specially important role. However, there would be a thousand chemical substances at the synaptic junction, and at least one hundred of them would be neurotransmitters or neuromediators [34].\nAs an example, we can add to those which are cited by L\u00f6vheim, endorphins, cannabino\u00efds and ocytocin. Endorphins modulate pain and well-being, cannabino\u00efds act on serenity and the sensation of parano\u00efa (belief in others hostility), ocytocin regulates attachment and indifference.\nWe speak about human or animal emotions. Indeed, the neuromediators which are present in humans are also present in mammals, and fear or even anxiety can also be observed, even in drosophila [9].\nIt is important to simulate emotions in an intelligent system, first as a modulator of memorization. A human being without emotions is also at risk of becoming a psychopath. We cannot exclude that a similar phenomenon might appear in artificial intelligence. As human or animals emotions are a powerful modulator of decisions, there absence may lead to inappropriate ones. Last, the presence of emotions in an intelligent system may facilitate its communication with human beings.\nThis does not necessarily imply that all human or animal emotions should be found in a robot or in an intelligent system. Deciding which are the relevant or minimal emotions for such a system, is outside the scope of this paper.\nIn an intelligent system an emotion ete with index e (which characterizes the type of emotion) at time t, can be seen as an environment variable defined as the sum of the outputs of neurons of index e, of layers 1 to j, or in a simpler way as the state of neuron of index e in layer jmax. It will be used as the bias of a collection of neurons from various layers.\nEmotion Ee activates a collection of Ee-dependent neurons, which have a non-null input weight for their connection with neuron of index e. A neuron may be dependent on several emotions, which can either lower or increase its bias, and therefore have an action of inhibition or activation of this neuron.\nThe system of emotions can also include some coupling between different emotions, some of them may reinforce or inhibit other ones. This is implemented by the production of emotions by a subset of the neuronal network. Any neuron can contribute to the production of some emotion (or of its mediators), and can also produce several emotions. Last, it is possible to introduce some inertia in the emotions, so that it would not change too fast.\nIt is also possible to associate each emotion with a sense of \u201cpleasure\u201d, which will activate a process of continuation of this emotion, or a sense of \u201cdispleasure\u201d, which will activate a process to end this emotion. These processes may be a simple positive or negative reinforcement on appropriate layers, or a predefined procedural resource, possibly modified by an evolutionary program.\nThese emotions may also be input in the system by an outside agent (the designer or an user), or be deduced from the activation of some concepts in the neural network.\n2.2. Cognitive areas, action on bias.\nSeveral subsets of neurons are characterized by bias, i.e. by an activation threshold which is a function of the level of one or\n- 4 -\nseveral emotions [22]. In each of these subsets, which we call a cognitive area, the bias is equal for all the neurons of the subset, or at least, it depends on a same function of the emotions.\nE.g. in a cognitive area associated with the attachment towards a person or a human group, all neurons will have a bias which will be a function of this attachment. The other properties of cognitive areas, will be discussed shortly in the paragraph about the system initialization.\n2.3. Research and approximate inference,\nblending\nThis property is implicit in a system based on a neuronal architecture, which can easily implement associative retrieval and approximate inference. However, our proposal may also apply to a symbolic architecture.\nFor examples of approximate inference and of blending in symbolic or partially symbolic architectures, the reader may refer to the works of projects like NARS or OpenCog [11, 33].\nThe interest of a neuronal approach for AGI is underlined by the works of Schmidhuber [29].\n2.4. Goals management\nWe introduce here the notion of an autonomous system.\nDefinition 3. An autonomous reflexive system is a reflexive system with a set of actions, which depend on a system of goals.\nWe shall notice that the property of deliberation may not be mandatory in a non reflexive autonomous system, e.g. In the case of an autonomous car. However, we only consider here reflexive and deliberative systems.\nThe definition of a goals system for an AGI is a complex task. According to Yudkowsky and others, it is very difficult to define them in a non ambig\u00fcous way. There would often be a risk that the system would escape the control of its designer, as people do not necessarily understand the reasoning which can be made by the system, when it is implemented with a neuronal architecture, or even with a symbolic one [5, 6, 13, 32, 37].\nIn a very interesting approach, Joscha Bach [3, 4] proposes to derive at any time the goals of an AGI from a set of needs, which include the needs of existence (access to calculation means, energy, cooling,\u2026), social needs (communication with third parties, either human or not,..) and epistemically needs (acquisition of new knowledge). The insufficient satisfaction of a need creates a goal, which can be more or less prioritary.\nThe MicroPsi system created by Bach is based on the Psi theory proposed by Dietrich D\u00f6rner [3]. This theory may enable AGI designers to escape to the various problems of a direct goals definition [36].\n2.5. Attention\nAttention is a property which enables the system to focus its perceptions on some part of the perception field, or to focus inference on part of the sentences or concepts produced by the system. E.g., if the system has a vision capability, with some part in high definition, and some part in low definition, the informations of the low definition part will enable the system to focus the high definition part within the vision field [23].\nAn example of this property is given by Andrej Karpathy [16], for the access to hand-written characters which are spread on a page, by using a convolutive neural network with a size which is independent of the size of the page.\nAttention may also exist for a system which operates on texts, e.g. belonging to a texts database. It may then make actions such as right or left move in the text, text read, move from one document to the another one, etc. The text database can also be more generally considered as a tree, with operators to move within this tree.\nThis property will then be represented by a set of actions, which will be learned by the neural network in the same way as other actions. A small number of attention properties may also,be implemented as \u201cinnate properties\u201d, e.g, in a procedural way, before starting the system\n2.6. Perceptions estimation\nWe define : Pt,1..p the p elementary perceptions at time t, they may be pixels (r, g, b) in a matrix representing an image, or characters in a text, or phonemes in an audio flux, etc.\nLet then be Pt+2jmax, p+1..2p the estimations of these perceptions at time t+2jmax, where jmax is the number of layers both in encoder and in decoder part of the network.\nThere are two blocks of layers, one of them acts as an encoder which outputs a series of possible actions K, the next ones acts as a decoder which estimate the next state of perceptions. The difference between estimated and real perceptions can produce an increase in attention, or various emotions (surprise, interest, etc..). These effects are now well known for human beings.\nIn a simplified version of the system layers p+jmax+1 to p+2jmax do not exist, Pt+2jmax is replaced by a subset of Sjmax.\nWe assume that the system learns by gradient retro-propagation or any other learning method, which is done from max to 1 for the encoder part, and from 1 to jmax for the decoder part, if it exists. For this part, the Wt weights are modified as a function of the difference between the predicted and observed perceptions.\n- 5 -\n2.7. Models of others and of itself\nWe assume that the system encodes an internal discourse, composed of trees of sentences, as trees of tuples, where a sentence can be represented by several tuples. In these tuples, either by construction or by learning, there is a field or attribute which designates the author of the sentence (or the assumption which it represents) and attributes of place and time or duration.\nAn author designation may be done explicitly by the user or by a teacher of the system, through dedicated inputs, or be learned by the system, through the input discourse. It should be the same for place and time imformations. These author or hypotheses informations will also be used to build models of the various partners of the system. These models will propose an assumed behavior of these partners, in the context of one or several concepts. A concept is a class of situations or a set of sentences or a class of words.\nA specific author represents the system. It's model enables it to make inferences on its own behavior or concepts. An assumption may of course have an author (or a class of authors), including the system itself..\nWe will notice the importance of images of the self and of others, and of emotions, in the learning of the outside world by an intelligent system. These images (or models) can make possible an imprinting mechanism, by which the system will adopt the values and concepts of its designer, or of a teacher who plays a privileged role,in its development. The adoption of the values of a predefined group, may also play a role in this sense.\nWe may consider that if there had been an imprinting mechanism or an image of third parties in Tay, the intelligent system experimented by Microsoft on Twitter, this system would have identified the discourse of hostile third parties or of people with values opposed to those of its group or designers, and would have limited it to these third parties [30].\n2.8. Spatial representation\nIdeally, the system should be able to spot the presence of an agent or any object on one or several maps, at various scales, and recognize which objects are situated in various neighborhoods of itself. Map management operators should then be available. These operators could thus manage interfaces with external memory, which would hold these maps.\nHowever in a first step it seems possible to manage this issue through the representation of inclusion relationships of an object in another one, as the system will have a known capability to process trees.\n2.9. Time representation\nMany different representations of time may be present in texts, such as relative ones \u201cbefore this event\u201d, \u201cthe day before\u201d, or\nimaginary times \u201conce upon a time\u201d, or different time scales \u201cone millisecond after the Big Bang\u201d.\nThe time may be represented by a scale of time, and by a number, which represents the distance to an origin of time, and by a duration. It is also necessary to take into account two distinct times : the event time, and the time of its recording or description. E.g., an event may relate to the origin of the Universe, more than 13 billion years ago, and be described in an article published in \u201cDecember 2015\u201d.\nHowever, one issue is the representation of this notion, in a RNN architecture. For the moment, we don\u2019t have fully resolved this point. A provisional solution is similar to that which we have proposed for the space representation : it is to represent a tree of time intervals, with operators of inclusion, precedence and succession.\n2.10. Degree of confidence in a data or a\nproperty.\nA very important information in an AGI is the degree of confidence that the system may have towards a sentence, or a model, an assumption, a logical clause, an author, etc. This confidence level may itself be decomposed into several elements, such as the evaluation of the sentence, the level of confidence of this evaluation, the cardinality of the validation set (the number of examples), etc.\nAs an example, some opponent I of the system will be characterized with certainty C, to produce declarations with value V, with an experimental basis X (number of examples). C describes the degree of knowledge that the designer or observer O has about I, the level of X indicates the plausibility of further revisions of these data. V describes the community or opposition of values between I and O.\nIn the same way, an assumption H can be characterized by the triple (C, X, V). If these variables are defined, without loss of generality, between 0 and 1, the author of a new hypothesis will possibly characterize it by the triple {0.9, 0.1, 0.7}. Her confidence in the assumption will be C= 0.9, the experimental basis will possibly be X= 0.1, and the agreement between the author and O may be V= 0.7. It will be possible to make some calculations on these coefficients, the calculations will be more or less approximate.\nIn the neuronal system which we describe here, the confidence coefficient may be reduced to a scalar, which is the arc value between the input character (INPUT) and the entry layer of the RNN. The possibility to use a multi-valued confidence coefficient should be the subject of further research.\n- 6 -\n2.11. Non-monotony, Open World\nAssumption (OWA)\nAn important property of a reflexive system is non-monotony, which enables the calculation of clauses with negation. This property enables exceptions to the knowledge acquired by the system, or also systems of concepts (ontologies) with only partial coherence.\nThis property also makes possible a progress in the system knowledge, by enabling detection of a theories limit.\nIn a neuron network, the non-monotony can be implemented by the use of inhibitor arcs between two neurons, i.e. the use of negative connection weights (synaptic weights). This implies an adaptation of the retro-propagation algorithm, or more generally the learning algorithm.\nThe non-monotony also implies the Open World Assumption (OWA), i.e. the assumption that the absence of a clause in the knowledge base (or of a connection in the neural network) does not imply its negation.\n2.12. Multimedia perception\nDefinition 4. We shall say that a system is fully reflexive if it includes multimedia perceptions, with an input of documents (text, images, page layout), visual data (still image and video), audio data (sounds and speech), and is able to process them.\nIn the case of an embodied system (robot or autonomous vehicle) it will possibly include other parameters, such as the gps position and localization on a map.\n2.13. Actions and physical support\nDefinition 5. We shall say that an autonomous system is complete if it is a reflexive and embodied system [11], able of mobility and with a set of actions implemented by effectors (limbs, mobility systems), under the control of the reflexive system.\nHowever the system will be more or less complete, depending on the properties of its mobility system and of its effectors.\n2.14. External interfaces\nThe access of a neuron network, possibly recurrent, to external memory (and more generally to external interfaces) is the subject of relevant work. These interfaces may enable an access to sets of registers, stacks, or random access memory [15, 17, 38]. The access can also be implemented in an associative way, by one or several keywords produced by the system.\nWe assume that the external documents accessed by the system can be memorized in an external memory, and that the system can access it by one of these methods to answer a user query."}, {"heading": "3. ARCHITECTURE", "text": "The architecture of the intelligent system which we consider here, will be characterized by a first level of n memorization cells, making a 1D systolic network, in which the user will input a character at a time. This network will possibly be connected to a stack of N registers of n characters each. The operations on this stack will be a subset of the actions of the system.\nThe n cells of the first layer will be connected to jmax layers of h(j) neurons each, forming a recurrent network. The output of layer jmax will include first, the k actions of the system, and secondly, an output of c characters. Both elements will form the retroaction loop of the system, and will be connected to its entry layer, beside the sentence contained in the first n memorization cells.\nThe relevance of the coupling of this unit with another equivalent recurrent unit, according to the schema of paragraph 3, is intended to enable a degree of deliberation. It will be the subject of further experimentation."}, {"heading": "4. INITIALIZATIONS", "text": "The initialization will be made by the choice of a global architecture among the few variants presented before, and of the meta-parameters such as the number of neurons per layer, the number of neurons per layer and cognitive area, the projections between cognitive areas, the initial synaptic weights. The functions which will return the bias depending on the level of the various emotions, will also be part of initializations.\nWe shall assume that when the list of connections will be defined, the initial weights will be drawn according to a simple law, which remains to be defined.\nIt is possible to evolve these meta-parameters, through the use of an evolutionary program. However, this leads to two issues:\nFirst the choice of a success criterium of a set of parameters, such a criterium is needed to guide the evolution program. It can be the number of iterations needed to reach some quality of results, as measured by subjective indices. These results may also be used to measure the quality of the reflexivity, as we propose it in paragraph 8.\nSecondly, Eliezer Yudkowsky stresses the risk, if an AGI is defined by an evolutionary program, that it would escape to the control of its designers [35, 36]. This risk should be carefully contained.\nA solution may be in a first time, to reserve these evolutionary programs to system versions containing relatively few neurons (e.g. less than 100 k, or possibly even less than 10 k), and\n- 7 -\ntherefore for which the risk of escape would appear neglectible. This remains to be worked out.\nAs an example, a system with 30 to 50 k neurons has a comparable size to the brain (or equivalent neural system) of an insect with a limited complexity, such as the fly.. Even if the functions of neurons in an AGI of comparable cardinality are very different, the considered risk would probably remain very low. It might not be the case for cardinalities about or above one million neurons, comparable to very evolved insects like the honey bee, which can recognize faces and have complex social behavior [12]."}, {"heading": "5. EXPERIMENTATION", "text": "An implementation of the system in Python is scheduled for the next months, it will use Theano libraries. It should be first implemented on an OS X environment, with relatively low processing power, then in a CUDA or OpenCL environment on a Mac Pro, with a D700 graphic board (more than 4000 GPU cores).\nHowever the possibility of implementing part of the initial development and experimentation on Ubuntu, is also considered as an alternative solution.\nThe goal of experimentation will be to work out a system which should be able to learn from Wikipedia and text bases available on the Internet, and to participate to a process of Queries & Answers. The system should learn the concepts and values associated with several authors, and will be able to take them into account to evaluate queries and answers. We believe that this accounting of the model of discussion partners is essential for a mastered learning of the outside world."}, {"heading": "6. REFLEXIVITY EVALUATION.", "text": "We believe that the proposed architecture is a minimal one to obtain a perceivable degree of reflexivity. Let us remember that our goal is not to realize a \u201cconscious\u201d system, but to simulate some properties of the cognitive systems of human beings and of some superior animals (primates, mammals, some birds\u2026), or maybe of some simpler animals (drosophila, honeybee) [9, 12].\nWe propose the following conjectures:\nConjecture 1."}, {"heading": "A necessary condition for a neuron network to implement a reflexive system is that it should possess the properties described in paragraph 2.", "text": "Conjecture 2."}, {"heading": "A sufficient condition for an AGI to be a complete reflexive autonomous system is that it would possess all the properties", "text": "described in paragraphs 2, 3, and 4.1 to 4.14.\nLets notice that an AGI can be a complete reflexive system without being an autonomous one. We should also stress the fact that, in our opinion, the condition of conjecture 2 is sufficient, but not necessary. We will have to refine the analysis and the experimentation in order to reach necessary and sufficient conditions.\nWe will provide a reflexivity index, which will evaluate the satisfaction of conditions from paragraphs 2 to 4.14, in order to determine to which degree an intelligent system is reflexive. It will be possible to compare this index with subjective evaluations of the system intelligence, such as those resulting from Turing tests or other measures of the effectiveness of an AGI. The composition of the reflexivity index will be the subject of further research.\nConjecture 3. The properties of a complete reflexive AGI, or a complete autonomous one, can be obtained either with a neuronal system, or with a partially or fully symbolic system.\nLet us notice that it seems easier to implement a system according to the conditions of paragraphs 2 to 4.14 with a neuronal system (e.g. to implement non-monotony, fuzzy inference, etc.), but that the interest of a symbolic system is that it could be understandable by users, which is not necessarily the case with neuronal systems."}, {"heading": "7. CONCLUSION", "text": "In this paper, we have proposed several conjectures on the properties that should be those of a neuron network, or possibly of a symbolic architecture, to implement a reflexive and deliberative system, and possibly a reflexive and autonomous system..\nIn the next months we intend to experiment a neuronal architecture which will display these properties. The neuronal network which will be implemented will not exceed the cardinality of the neural system of a drosophila, or possibly of a honeybee, i.e. cardinalities between 30 k and one million of neurons, and a number of weights between 30 million and 10 billion, or more probably not greater than one billion weights.\nHowever, with a specific topology, this system should be applicable to problems which interest people, such as the summarization of Wikipedia pages or the participation to a query and answers system."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The author thanks Jean-Fran\u00e7ois Perrot and Francis Wolinski for useful comments.\n- 8 -"}], "references": [{"title": "B", "author": ["S.S. Adams", "I. Arel", "J. Bach", "R. Coop", "R. Furlan"], "venue": "Goertzel, et al., Mapping the Landscape of Human-Level Artificial General Intelligence, AI Magazine, Spring", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "A cognitive theory of consciousness", "author": ["B. Baar"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Modeling Motivation in MicroPsi2, in Artificial General Intelligence", "author": ["J. Bach"], "venue": "Proceedings 8th International Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Our Final Invention: Artificial Intelligence and the End of the Human Era, Thomas Dunne Books", "author": ["J. Barrat"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Facing Up the Problem of Consciousness", "author": ["D.J. Chalmers"], "venue": "Journal of Consciousness Studies,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Consciousness and the Brain: Deciphering How the Brain Codes our Thoughts", "author": ["S. Dehaene"], "venue": "Penguin Books,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Behavioral Responses to a Repetitive Visual Threat Stimulus Express a Persistent State of Defensive Arousal in Drosophila", "author": ["W.T. Gibson"], "venue": "Current Biology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Artificial General Intelligence: Concept, State of the Art, and Future Prospects", "author": ["B. Goertzel"], "venue": "Journal of Artificial General Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Reconnaissance des visages : l'ordinateur devrait imiter... les abeilles (in French) 2-2-2009, in www.futura-sciences.com", "author": ["J.-L. Goudet"], "venue": "consulte\u0301 le 20-5-2015", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Transcendence looks at the implications of artificial intelligence. But are we taking AI seriously enough ", "author": ["S. Hawking"], "venue": "The Independent,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets, Facebook AI Research, arXiv:1503.01007v4 [cs.NE] 1 Jun 2015", "author": ["A. Joulin", "T. Mikolov"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "The Unreasonable Effectiveness of Recurrent Neural Networks, in Andrej", "author": ["A. Karpathy"], "venue": "Karpathy blog,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Neural RandomAccess Machines, Google, arXiv1511.06392v2 [cs.LG", "author": ["K. Kurach", "M. Andrychowicz", "I. Sutskever"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "L\u2019apprentissage profond, cours au Coll\u00e8ge de France, chaire", "author": ["Y. Le Cun"], "venue": "Informatique et Sciences Nume\u0301riques,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "A new three-dimensional model for emotions and monoamine neuro-transmitters, Med", "author": ["H. L\u00f6vheim"], "venue": "Hypotheses,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Making Robots Conscious of their Mental States, Stanford", "author": ["J. McCarthy"], "venue": "http://wwwformal.stanford.edu/jmc/ published in Machine Intelligence", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "Awareness and Understanding in Computer Programs, a Review of \u201cShadows of the Mind", "author": ["J. McCarthy"], "venue": "by Roger Penrose, Psyche,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "The Emotion Machine (Commonsense thinking, artificial intelligence and the future of the human mind)", "author": ["M. Minsky"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Recurrent Models of Visual Attention, Google DeepMind, arXiv:1406.6247v1 [cs.LG", "author": ["V. Mnih", "N. Hees", "A. Graves", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Les abeilles, bonnes physionomistes", "author": ["J.-Y. Nau"], "venue": "Slate,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "The Emperor\u2019s New Mind, Oxford Univ", "author": ["R. Penrose"], "venue": "Press, Oxford, UK,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1989}, {"title": "J", "author": ["A. Potapov"], "venue": "Bieger, editors, Artificial General Intelligence, 8 Int\u2019l Conference, AGI", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Creating Human Level AI by Educating a Child Machine, in The Convergence of Machine and Biological Intelligence", "author": ["R. Reddy"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Using Stories to Teach Human Values to Artificial Agents, Georgia", "author": ["M.O. Riedl", "B. Harrison"], "venue": "Institute of Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Deep Learning In Neural Networks, an Overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "A peine lanc\u00e9e, une intelligence artificielle de Microsoft d\u00e9rape sur Twitter, Le Monde.fr", "author": ["M. Tual"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Grammar as a Foreign Language, Google, arXiv:1412.7449v3 [cs.CL", "author": ["Vinyls Oriol", "Kaiser Lukasz", "Koo Terry", "Petrov Slav", "Sutskever Ilya", "Hinton Geoffrey"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Motivation Management in AGI Systems, Temple University, Philadelphia, USA, Proceedings of AGI-12, Oxford, UK, December 2012", "author": ["P. Wang"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Analogy in a General Purpose Reasoning System", "author": ["P. Wang"], "venue": "Cognitive Systems Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Artificial Intelligence as a Positive and Negative Factor in Global Risk, in Global Catastrophic Risks, edited by Nick Bostr\u00f6m and Milan M \u0106ircovi\u0107", "author": ["E. Yudkowzky"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Creating Friendly AI 1.0 : The Analysis and Design of Benevolent Goal Architectures, The Singularity Institute, San Francisco", "author": ["E. Yudkowsky"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Complex Value Systems In Friendly AI, Artificial General Intelligence 4 International Conference, AGI 2011, USA", "author": ["E. Yudkowsky"], "venue": "Proceedings, in LNCS", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "General Intelligence and Seed AI-Creating Complete Minds Capable of Open-Ended Self-Improvement", "author": ["E. Yudkowsky"], "venue": "Research report, MIRI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2001}, {"title": "I", "author": ["W. Zaremba"], "venue": "Sutskever, Reinforcement Learning Neural Turing Machines (revised), Facebook AI Research & Google Brain, submitted to ICLR", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "An AGI may also designate a prototype of an Artificial General Intelligence, more or less close to the goals of this field [4, 11, 33].", "startOffset": 123, "endOffset": 134}, {"referenceID": 28, "context": "An AGI may also designate a prototype of an Artificial General Intelligence, more or less close to the goals of this field [4, 11, 33].", "startOffset": 123, "endOffset": 134}, {"referenceID": 32, "context": "It may also be a seed AGI (or Baby AGI), with a limited initial knowledge or even intelligence, but a good learning capability [38].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "An intelligent system implemented by a neural network [18] is reflexive if it has the following properties :", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "The properties displayed by equations (1) to (4) define the network as a recurrent neural network (RNN) [16, 18].", "startOffset": 104, "endOffset": 112}, {"referenceID": 13, "context": "The properties displayed by equations (1) to (4) define the network as a recurrent neural network (RNN) [16, 18].", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "(Andrej Karpathy,2015, [16]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "These external memories do not necessarily need to be limited to a register stack [15, 17].", "startOffset": 82, "endOffset": 90}, {"referenceID": 12, "context": "These external memories do not necessarily need to be limited to a register stack [15, 17].", "startOffset": 82, "endOffset": 90}, {"referenceID": 5, "context": "Perceptions and states of consciousness which are accompanied by a more intense emotion, are more memorized than those accompanied by a less intense emotion [8, 20, 22].", "startOffset": 157, "endOffset": 168}, {"referenceID": 15, "context": "Perceptions and states of consciousness which are accompanied by a more intense emotion, are more memorized than those accompanied by a less intense emotion [8, 20, 22].", "startOffset": 157, "endOffset": 168}, {"referenceID": 17, "context": "Perceptions and states of consciousness which are accompanied by a more intense emotion, are more memorized than those accompanied by a less intense emotion [8, 20, 22].", "startOffset": 157, "endOffset": 168}, {"referenceID": 14, "context": "L\u00f6vheim [19] proposes a representation of emotions by a cube, as a function of the presence or absence of three neuromediators, dopamin, noradrenalin and serotonin, which have a specially important role.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "Indeed, the neuromediators which are present in humans are also present in mammals, and fear or even anxiety can also be observed, even in drosophila [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 17, "context": "- 4 several emotions [22].", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "For examples of approximate inference and of blending in symbolic or partially symbolic architectures, the reader may refer to the works of projects like NARS or OpenCog [11, 33].", "startOffset": 170, "endOffset": 178}, {"referenceID": 24, "context": "The interest of a neuronal approach for AGI is underlined by the works of Schmidhuber [29].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "There would often be a risk that the system would escape the control of its designer, as people do not necessarily understand the reasoning which can be made by the system, when it is implemented with a neuronal architecture, or even with a symbolic one [5, 6, 13, 32, 37].", "startOffset": 254, "endOffset": 272}, {"referenceID": 9, "context": "There would often be a risk that the system would escape the control of its designer, as people do not necessarily understand the reasoning which can be made by the system, when it is implemented with a neuronal architecture, or even with a symbolic one [5, 6, 13, 32, 37].", "startOffset": 254, "endOffset": 272}, {"referenceID": 27, "context": "There would often be a risk that the system would escape the control of its designer, as people do not necessarily understand the reasoning which can be made by the system, when it is implemented with a neuronal architecture, or even with a symbolic one [5, 6, 13, 32, 37].", "startOffset": 254, "endOffset": 272}, {"referenceID": 31, "context": "There would often be a risk that the system would escape the control of its designer, as people do not necessarily understand the reasoning which can be made by the system, when it is implemented with a neuronal architecture, or even with a symbolic one [5, 6, 13, 32, 37].", "startOffset": 254, "endOffset": 272}, {"referenceID": 2, "context": "In a very interesting approach, Joscha Bach [3, 4] proposes to derive at any time the goals of an AGI from a set of needs, which include the needs of existence (access to calculation means, energy, cooling,.", "startOffset": 44, "endOffset": 50}, {"referenceID": 30, "context": "This theory may enable AGI designers to escape to the various problems of a direct goals definition [36].", "startOffset": 100, "endOffset": 104}, {"referenceID": 18, "context": ", if the system has a vision capability, with some part in high definition, and some part in low definition, the informations of the low definition part will enable the system to focus the high definition part within the vision field [23].", "startOffset": 234, "endOffset": 238}, {"referenceID": 11, "context": "An example of this property is given by Andrej Karpathy [16], for the access to hand-written characters which are spread on a page, by using a convolutive neural network with a size which is independent of the size of the page.", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "We may consider that if there had been an imprinting mechanism or an image of third parties in Tay, the intelligent system experimented by Microsoft on Twitter, this system would have identified the discourse of hostile third parties or of people with values opposed to those of its group or designers, and would have limited it to these third parties [30].", "startOffset": 352, "endOffset": 356}, {"referenceID": 10, "context": "These interfaces may enable an access to sets of registers, stacks, or random access memory [15, 17, 38].", "startOffset": 92, "endOffset": 104}, {"referenceID": 12, "context": "These interfaces may enable an access to sets of registers, stacks, or random access memory [15, 17, 38].", "startOffset": 92, "endOffset": 104}, {"referenceID": 32, "context": "These interfaces may enable an access to sets of registers, stacks, or random access memory [15, 17, 38].", "startOffset": 92, "endOffset": 104}, {"referenceID": 29, "context": "Secondly, Eliezer Yudkowsky stresses the risk, if an AGI is defined by an evolutionary program, that it would escape to the control of its designers [35, 36].", "startOffset": 149, "endOffset": 157}, {"referenceID": 30, "context": "Secondly, Eliezer Yudkowsky stresses the risk, if an AGI is defined by an evolutionary program, that it would escape to the control of its designers [35, 36].", "startOffset": 149, "endOffset": 157}, {"referenceID": 8, "context": "It might not be the case for cardinalities about or above one million neurons, comparable to very evolved insects like the honey bee, which can recognize faces and have complex social behavior [12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 6, "context": "), or maybe of some simpler animals (drosophila, honeybee) [9, 12].", "startOffset": 59, "endOffset": 66}, {"referenceID": 8, "context": "), or maybe of some simpler animals (drosophila, honeybee) [9, 12].", "startOffset": 59, "endOffset": 66}], "year": 2016, "abstractText": "We define a property of intelligent systems, which we call Reflexivity. In human beings it is one aspect of consciousness, and an element of deliberation. We propose a conjecture, that this property is conditioned by a topological property of the processes which implement this reflexivity. These processes may be symbolic, or non symbolic e.g. connexionnist. An architecture which implements reflexivity may be based on the interaction of one or several modules of deep learning, which may be specialized or not, and interconnected in a relevant way. A necessary condition of reflexivity is the existence of recurrence in its processes, we will examine in which cases this condition may be sufficient. We will then examine how this topology and this property make possible the expression of a second property, the deliberation. In a final paragraph, we propose an evaluation of intelligent systems, based on the fulfillment of all or some of these properties.", "creator": "Microsoft\u00ae Word 2016"}}}