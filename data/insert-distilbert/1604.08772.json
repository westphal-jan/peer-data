{"id": "1604.08772", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Apr-2016", "title": "Towards Conceptual Compression", "abstract": "we introduce a simple / recurrent variational template auto - encoder architecture principle that significantly improves image mapping modeling. the system represents the advanced state - of - the - art in latent variable dimension models for both the imagenet and omniglot datasets. we show that it naturally separates global content conceptual information processes from lower level details, thus continually addressing one of the fundamentally desired properties of unsupervised learning. furthermore, the possibility of restricting ourselves to just storing only global information about an image allows us to achieve high quality'conceptual compression '.", "histories": [["v1", "Fri, 29 Apr 2016 11:02:52 GMT  (8449kb,D)", "http://arxiv.org/abs/1604.08772v1", "14 pages, 13 figures"]], "COMMENTS": "14 pages, 13 figures", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["karol gregor", "frederic besse", "danilo jimenez rezende", "ivo danihelka", "daan wierstra"], "accepted": true, "id": "1604.08772"}, "pdf": {"name": "1604.08772.pdf", "metadata": {"source": "META", "title": "Towards Conceptual Compression", "authors": ["Karol Gregor", "Frederic Besse", "Danilo Jimenez Rezende", "Ivo Danihelka", "Daan Wierstra"], "emails": ["KAROLG@GOOGLE.COM", "FBESSE@GOOGLE.COM", "DANILOR@GOOGLE.COM", "DANIHELKA@GOOGLE.COM", "WIERSTRA@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "Images contain a large amount of information that is a priori stored independently in the pixels. In the semisupervised learning regime where a large number of images is available but only a small number of labels, one would like to leverage this information to create representations that allow for better (and especially faster) generalization. Intuitively one expects such representations to explicitly extract global conceptual aspects of an image.\nIn this paper we propose a method that is able to transform an image into a progression of increasingly detailed representations, ranging from global conceptual aspects to low level details (see Figures 1 & 2). At the same time, our model greatly improves latent variable image modeling compared to earlier implementations of deep variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2014). Furthermore, it has the advantage of being a simple homogeneous architecture not requiring complex design choices, which is similar to the recurrent structure of DRAW (Gregor et al., 2015). Last, it provides an important insight into building good variational auto-encoder models of images: the use of multiple layers of stochastic variables that are all \u2018close\u2019 to the pix-\nels significantly improves performance.\nThe system\u2019s ability to stratify information enables it to perform high quality lossy compression, by storing only a subset of latent variables, starting with the high level ones, and generating the remainder during decompression (see Figure 4).\nCurrently the ultimate arbiter of lossy compression remains human evaluation. Other simple measures such as the L2 distance between compressed and original images are inappropriate \u2013 for example if a particular generated grass texture is sharp, but different from the one in the original image, it will yield a large L2 distance yet should, at the\nar X\niv :1\n60 4.\n08 77\n2v 1\n[ st\nat .M\nL ]\n2 9\nA pr\n2 01\nsame time, be considered conceptually close to the original.\nAchieving good lossy compression while storing only high level latent variables would imply that representations learned at a high level contain information similar to that used by humans to judge images. As humans outperform the best machines at learning abstract representations, human evaluation of lossy compression obtained by these generative models constitutes a reasonable test of the quality of representations learned by these models.\nIn the following we discuss variational auto-encoders and compression in more detail, present the algorithm and demonstrate the results on generation quality and compression."}, {"heading": "1.1. Variational Auto-Encoders", "text": "Numerous techniques exist for unsupervised learning in deep networks, e.g. sparse auto-encoders and sparse coding (Kavukcuoglu et al., 2010; Le, 2013), denoising autoencoders (Vincent et al., 2010), deconvolutional networks (Zeiler et al., 2010), restricted Boltzmann machines (Hinton & Salakhutdinov, 2006), deep Boltzmann machines (Salakhutdinov & Hinton, 2009), generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2014).\nIn this paper we focus on the class of models in the\nvariational auto-encoding framework. Since we are also interested in compression, we present them from an information-theoretic perspective. Variational autoencoders typically consist of two neural networks: one that generates samples from latent variables (\u2018imagination\u2019), and one that infers latent variables from observations (\u2018recognition\u2019). The two networks share the latent variables. Intuitively speaking one might think of these variables as specifying, for a given image, at different levels of abstraction, whether a particular object such as a cat or a dog is present in the input, or perhaps what the exact position and intensity of an edge at a given location might be.\nDuring the recognition phase the network acquires information about the input and stores it in the latent variables, reducing their uncertainty. For example, at first not knowing whether a cat or a dog is present in the image, the network observes the input and becomes nearly certain that it is a cat. The reduction in uncertainty is quantitatively equal to the amount of information the network acquired about the input. During generation the network starts with uncertain latent variables and selects their values from a prior distribution that specifies this uncertainty (e.g. it chooses a dog). Different choices will produce different samples.\nVariational auto-encoders provide a natural framework for unsupervised learning \u2013 we can build networks with layers of stochastic variables and expect that, after learning, the representations become increasingly more abstract for higher levels of the hierarchy. The questions then are: can such a framework indeed discover such representations both in principle and in practice, are such networks powerful enough for modeling real data, and what techniques one needs to make it work well."}, {"heading": "1.2. Conceptual Compression", "text": "Variational auto-encoders can not only be used for representation learning but also for compression. The training objective of variational auto-encoders is to compress the total amount of information needed to encode the input. They achieve this by using information-carrying latent variables that express what, before compression, was encoded using a larger amount of information in the input. The information in the layers and the remaining information in the input can be encoded in practice as explained later in this paper.\nThe amount of lossless compression one is able to achieve is bounded by the underlying entropy of the image distribution. Additionally, most image information as measured in bits is contained in the fine details of the image. Thus we might reasonably expect that lossless compression will never improve by more than a factor of two in comparison to current performance.\nLossy compression, on the other hand, holds much more potential for improvement. In this case we want to compress an image by a certain amount, allowing some information loss, while maximizing both quality and similarity to the original image. As an example, at a low level of compression (close to lossless compression), we could start by reducing pixel precision, e.g. from 8 bits to 7 bits. Then, as in JPEG, we could express a local 8x8 neighborhood in a discrete cosine transform basis and store only the most significant components. This way, instead of introducing quantization artifacts in the image that would appear if we kept decreasing pixel precision, we preserve higher level structures but to a lower level of precision. However, what can we do beyond that as we keep pushing the compression? We would like to preserve the most important aspects of the image. What determines what is important?\nLet us imagine that we are compressing images of cats and dogs and would like to compress an image down to one bit. What would that bit be? One would imagine that it should represent whether the image contains either a cat or a dog. How would we then get an image out of this single bit? If we have a good generative model, we can simply generate the entire image from this one latent variable, an image of a cat if the bit corresponds to \u2018cat\u2019, and an image of a dog otherwise. Now let us imagine that instead of compressing to one bit we wanted to compress down to ten bits. Now we can store the most important properties of the animal as well \u2013 e.g. its type, color, and basic pose. The rest would be \u2018filled in\u2019 by the generative model that is conditioned on this information. If we increase the number of bits further we can preserve more and more about the image, while generating the fine details such as hair, or the exact pattern of the floor, etc. Most bits are in fact about such low level details. We call this kind of compression \u2013 compressing by giving priority to higher levels of representation and generating the remainder \u2013 \u2018conceptual compression\u2019. We suggest that this should be the ultimate objective of lossy compression.\nImportantly, if we solve deep representation learning with latent variable generative models that generate high quality samples, we achieve the objective of lossy compression mentioned above. We can see this as follows. Assume that the network has learned a hierarchy of progressively more abstract representations. Then, to get different levels of compression, we can store only the corresponding number of topmost layers and generate the rest. By solving unsupervised deep learning, the network would order information according to its importance and store it with that priority.\nWhile the ultimate goal of unsupervised learning remains elusive, we make a step in this direction, and show that our network learns to order information from a rather global level to precise details in images, without being hand-engineered to do this explicitly, as illustrated in Figures 1 and 2. This information separation already allows us to achieve better compression quality than JPEG and JPEG2000 as shown in Figure 4. While we are not bound by the same constraints as these algorithms, such as speed and memory, these results demonstrate the potential of this method, which will get better as latent variable generative models improve."}, {"heading": "1.3. The Importance of Recurrent Feedback", "text": "What are the challenges involved in turning latent variable models into state-of-the-art generative models of images? Many successful vision architectures (e.g. Simonyan & Zisserman, 2014) have highly over-complete representations that contain many more neurons in hidden layers than pixels. These representations need to be combined to get a very sharp distribution at the pixel level if the pixels are modeled independently. This distribution corresponds to salt and pepper noise which is not present in natural images to a perceptible level. This poses a major challenge.\nAfter experimenting with deep variational auto-encoders we concluded that it was exceedingly difficult to obtain satisfactory results with a single computational pass through the network. Instead we propose that the network needs the ability to correct itself over a number of time steps. Thus, sharp reconstructions should not be a property of high-precision values in the network, but should rather be the result of an iterative feedback mechanism that is robust to network parameter change.\nSuch a mechanism is provided by the DRAW algorithm (Gregor et al., 2015), which is a recurrent type of variational auto-encoder. At each time step, DRAW maintains a provisionary reconstruction, takes in information about a given image, stores it in latent variables and updates the reconstruction. Keeping track of the reconstruction aids the iterative feedback mechanism which is learned by backpropagation. Computation is both deep \u2013 in iterations \u2013\nand close to the pixels.\nWe introduce convolutional DRAW. It features convolutions, latent prior modeling, a Gaussian input distribution (for natural images) and, in some experiments, a multilayer architecture. However, it does not use an explicit attentional mechanism. We note that even the single-layer version is already a deep generative model which can decide to process higher level information first before focusing on details, as we demonstrate to some degree. We also experiment with making convolutional DRAW hierarchical in a similar way that we would build conventional deep variational auto-encoders (Gregor et al., 2014) \u2013 stacking more layers of latent and deterministic variables. We believe that the recurrence is important not just for accurate pixel reconstructions, but also at higher levels. For example, when the network decides to generate edges at different locations, it needs to make sure that they are aligned. It is hard to imagine this happening in a single computational pass through the network. Similarly at higher levels, when it decides to generate objects, they need to be generated with the right relationship to one another. And finally at the scene level, one probably does not generate entire scenes at once, but rather one step at a time."}, {"heading": "1.4. Comparison to Non-variational Models", "text": "Let us relate this discussion to two other families of generative models, specifically generative adversarial networks (GANs; Goodfellow et al. (2014)) and auto-regressive pixel models. GANs have been demonstrated to be able to generate realistic looking images (Denton et al., 2015; Radford et al., 2015), with properly aligned edges, using a simple feedforward generative network (Radford et al., 2015). GANs also contain two networks \u2013 a generative network that is the same as in variational auto-encoders, and a classification network. The classification network is presented with both real and model-generated images and tries to classify them according to their true nature \u2013 real or modelgenerated. The generative network gets gradients from the classification network, changing its weights in an attempt to make the generated images be judged as real ones by the classification network. This makes the generation network produce realistic images that \u2018fool\u2019 the classification network. It needs to produce a wide diversity of images, not just one realistic image, because if it produced only one (or a small number of them), the classification network would classify that image as generated and others as realistic, and be almost always correct. This actually happens in practice, and one has to apply a variety of techniques, e.g. as in (Radford et al., 2015), to obtain sufficient image diversity. However the extent of GANs\u2019 sampling diversity is unknown and currently there is no satisfactory measure for it. So while a given network doesn\u2019t produce just one image, it is possible that it produces only a tiny subset of pos-\nsible realistic images, as it simply competes with the power of the classifier. For example if it generates a sharp image, it is unclear whether the system is also capable of generating its translated version, or simply a slightly distorted version.\nFinally there is another way to get low uncertainty at the pixel level: instead of predicting pixels independently given the latents, we can decide not to use latents and iterate sequentially over the pixels, predicting a given pixel from the previous ones (from top left to bottom right) in an autoregressive fashion (Bengio & Bengio, 1999; Graves & Schmidhuber, 2009; Larochelle & Murray, 2011; Gregor & LeCun, 2011; van den Oord et al., 2016). This is as \u2018close\u2019 to the pixels as one can possibly be, and furthermore the procedure is purely deterministic. The disadvantage is conceptual \u2013 the information and decisions are not done at a conceptual level but at the pixel level. For example when generating cats vs dogs the decisions at the first set of pixels (top left of the image) will contain no information about a hypothetical cat or dog. But as we get to the region where these objects are, we start choosing pixels that will slowly tip the probability of generating a cat vs a dog one way or the other. As we start generating an ear, it will more likely be a cat\u2019s or a dog\u2019s and so on. However this pixel level approach and our approach are orthogonal and can be easily combined, for example by feeding the output of convolutional DRAW into the conditional computation of a pixel level model. In this paper we study the latent variable approach and make the pixels independent given the latents."}, {"heading": "2. Convolutional DRAW", "text": "In this section we describe the details of a single-layer version of the algorithm. Convolutional DRAW contains the following variables: input x, reconstruction r, reconstruction error , the state of the encoder recurrent net he, the state of the decoder recurrent net hd and latent variable z. The variables he, hd and r are recurrent (passed between different time steps) and are initialized with learned biases. Then at each time step t \u2208 {1, T}, convolutional DRAW performs the following updates:\n= x\u2212 r (1) he = Rnn(x, , he, hd) (2) z \u223c q = q(z|he) (3) p = p(z|hd) (4) hd = Rnn(z, hd, r) (5) r = r +Whd (6)\nLzt = KL(q|p) (7)\nwhere W denotes a convolution and Rnn denotes a recurrent network. We use LSTM (Hochreiter & Schmidhuber,\n1997) with convolutional operators instead of the usual linear ones. The final value of rfinal = rT contains the parameters of the input distribution. For binary images we use the Bernoulli distribution. For natural images we use the Gaussian distribution with mean and log variance given by splitting the vector rT to obtain the input cost Lx and the total cost L:\nqx = U(x\u2212 s/2, x+ s/2) (8) px = N (r\u00b5T , exp(r \u03b1 T )) (9) Lx = log(qx/px) (10)\nL = \u03b2Lx + T\u2211 t=1 Lzt (11)\nwhere the handling of real valued-ness of the inputs (8, 10) is explained below, and \u03b2 = 1 being the standard setting. The algorithm is schematically illustrated in the first layer of Figure 3. The network is trained by calculating the gradient of this loss and using a stochastic gradient descent algorithm. Stochastic back-propagation through a sampling function is done as in variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014). Both the approximate posterior q and the prior p are Gaussian, with mean and log variance being linear functions of he or hd, respectively.\nLet us discuss how we handle the input distribution for natural images. Each pixel (per color) is one of 256 values. We could use a soft-max distribution to model it. This would result in a rather large output vector at every time step and also does not take advantage of the underlying real valuedness of intensities and therefore we opted for a Gaussian distribution. However this still needs to be converted to a discrete distribution over 256 values to calculate the negative likelihood loss Lx = \u2212 log p(x|z). Instead of this, we add uniform noise to the input with width corresponding to the spacing between discrete values and calculate Lx = \u2212 log p(x|z)/q0(x) where q0(x) = U(x \u2212 s/2, x + s/2) with s = 1/256 if inputs are scaled to the interval [0, 1]."}, {"heading": "2.1. Multi-layer Architectures", "text": "Next we explain how we can stack convolutional DRAW with a two layer example. The first layer is the same as the one just introduced. The second layer has the same structure: recurrent encoder, recurrent decoder and a stochastic layer. The input to the second layer is the mean \u00b5 of the approximate posterior of the first layer. The output of the second layer biases the prior of the latent variable of the first layer and is also passed as input into the first layer decoder recurrent net. This is illustrated in Figure 3. We don\u2019t use any reconstruction or error in the second layer.\nHere we describe a given computational step in detail. Indices 1 and 2 denote the variables of layers 1 and 2, respectively. In addition, let \u00b51(q1) be the mean of q1. Then, the\nupdate at a given time step is given by\n= x\u2212 r (12) he1 = Rnn(x, , h e 1, h d 1) (13) z1 \u223c q1 = q1(z1|he1) (14) he2 = Rnn(\u00b51(q1), h e 2, h d 2) (15) z2 \u223c q2 = q2(z2|he2) (16) p2 = p(z2|hd2) (17) hd2 = Rnn(z2, h d 2) (18) p1 = p(z1|hd1, hd2) (19) hd1 = Rnn(z1, h d 1, h d 2, r) (20)\nr = r +Whd1 (21) Lzt = KL(q1|p1) +KL(q2|p2) (22)\nSystems with more layers can be built analogously."}, {"heading": "3. Compression", "text": "Here we show how one can turn variational auto-encoders including convolutional DRAW into compression algorithms. We have not built the actual compressor, however, as we explain, we have strong reasons to believe it would perform as well as calculated here. Two basic approaches exist. The first one is less convenient because it needs to add extra data to the bitstream when compressing an image but has essentially a guaranteed compression rate. The other one may require some experimentation but is expected to yield a similar compression rate and can be used on a given image without needing extra data.\nThe underlying compression mechanism for all cases is arithmetic coding (Witten et al., 1987). Arithmetic coding takes as input a sequence of discrete variables x1, . . . , xt and a set of probabilities p(xt|x1, . . . , xt\u22121) that predict the variable at time t from previous variables. It then compresses this sequence to L = \u2212 \u2211 t log2(p(xt|x1, . . . , xt\u22121)) bits plus a constant of order one.\nCompressing inputs using variational auto-encoders proceeds as follows: discretize each latent variable in each layer using the width of q (eq. 3), treat the resulting variables as a sequence with predictions p (eq. 4) and compress using arithmetic coding.\nFor this to work as explained, several things are needed. First, the discretization should be independent of the input. This can be achieved by training the network with the variance of q being a learned constant that does not depend on the input. We found that this does not have much effect on the likelihood. Second, one should evaluate the log likelihood using this discretization. One has to decide on the exact manner in which p should be computed for each discrete value. Significant tuning might be required here, for\nthe obtained likelihoods to be as good as the ones obtained\nwith sampling. However this is likely to be fruitful since there exists a second, less convenient way to compress that is guaranteed to achieve this rate.\nThis second approach uses bits-back coding (Hinton & Van Camp, 1993). We explain only the basic idea here. We discretize the latents down to a very high level of precision and use p to transmit the information. Because the discretization precision is high, the probabilities for discrete values are easily assigned. That will preserve the information but it will cost many bits, namely\u2212 log2(pd) where pd is a probability under that discretization. Now, instead of sampling from the approximate posterior q when encoding an input, we encode \u2212 log2 q(z) bits of other information into the choice of z, that we also want to transmit. When z is recovered at the receiving end, both the information about the current input and the other information is recovered and thus the information needed to encode the current input is \u2212 log2(p) + log2(q) = \u2212 log2(p/q). The expectation of this quantity is the KL-divergence in (7), which therefore measures the amount of information stored in a given latent layer. The disadvantage of this approach is that we cannot encode a given input without also having some other information we want to transmit. However, this coding scheme works even if the variance of the approximate posterior is dependent on the input."}, {"heading": "4. Results", "text": "For natural images, all models except otherwise specified were single-layer, with nt = 32, a kernel size of 5 \u00d7 5, and stride 2 convolutions between input layers and hidden layers with 12 latent feature maps. We trained the models on Cifar-10 and ImageNet with 320 and 160 LSTM feature maps respectively. We use the version of ImageNet presented in (van den Oord et al., 2016) that will soon be released as a standard dataset. We train the network with the Adam algorithm (Kingma & Ba, 2014) with learning rate 5 \u00d7 10\u22124. Occasionally, we find that the cost suddenly increases dramatically. This is probably due to the Gaussian nature of the distribution, when a given variable is produced too far from the mean relative to sigma. We observed this happening approximately once per run. To be able to keep training we store older parameters, detect such jumps and revert to the old parameters when they occur. The network then just keeps training as if nothing had happened."}, {"heading": "4.1. Modeling Quality", "text": ""}, {"heading": "4.1.1. OMNIGLOT", "text": "The recently introduced Omniglot dataset (Lake et al., 2015) is comprised of 1628 character classes drawn from multiple alphabets with just 20 samples per class. Referred\nto by many as the \u2018inverse of MNIST\u2019, it was designed to study conceptual representations and generative models in a low-data regime. Table 1 shows likelihoods of different models compared to ours. For our model, we only calculate the upper bound (variational bound) and therefore the true likelihood is actually better. Samples generated by the model are shown in Figure 5."}, {"heading": "4.1.2. CIFAR-10", "text": "Table 2 shows likelihoods of different models on Cifar-10. We see that our method outperforms previous methods except for the just released Pixel RNN model of (van den Oord et al., 2016). As mentioned, the advantage of our model compared to such auto-regressive models is that it is a latent variable model that can be used for representation learning and lossy compression. At the same time, the two approaches are orthogonal and can be combined, for example by feeding the output of convolutional DRAW into the recurrent network of Pixel RNN.\nWe also report the likelihood for a (non-recurrent) variational auto-encoder and standard DRAW. For the variational auto-encoder we tested architectures with multiple layers, both deterministic and stochastic but with standard functional forms, and this was the best result that we obtained. Convolutional DRAW performs significantly better."}, {"heading": "4.1.3. IMAGENET", "text": "Additionally, we trained on the version of ImageNet prepared in (van den Oord et al., 2016) which was created with the aim of making a standardized dataset to test generative models. The results are in Table 3. Note that being a new dataset, no other methods have been reported on it.\nIn Figure 6 and Figure 7 we show generations from the model. We trained networks with varying input cost scales as explained in the next section. The generations are sharp and contain many details, unlike previous versions of variational auto-encoder that tend to generate blurry images."}, {"heading": "4.2. Input Cost Scaling", "text": "Each pixel (and color channel) of the data consists of 256 values, and as such, likelihood and lossless compression are well defined. When compressing the image there is much to be gained in capturing precise correlations between nearby pixels. There are a lot more bits in these low level details than in the higher level structure that we are actually interested in when learning higher level represen-\ntations. The network might focus on these details, ignoring higher level structure.\nOne way to make it focus less on the details is to scale down the cost of the input relative to the latents, that is, setting \u03b2 < 1 in (11). Generations for different cost scalings are shown in Figure 6, with the original objective being scale \u03b2 = 1. We see that lower scales indeed have a \u2018cleaner\u2019 high level structure. Scale 1 contains a lot of information at the precise pixel values and the network tries to capture that, while not being good enough to properly align details and produce realistic patterns. This might be simply a matter of scaling, making layers larger, networks deeper, using more iterations, or using better functional forms."}, {"heading": "4.3. The Dependence on Computational Depth", "text": "Convolutional DRAW uses many iterations and might be considered expensive. However we found that networks with a larger number of time steps train faster per data example as shown in Figure 8 (left). To study how they train with respect to real time, we multiply the time scale of each input by the number of iterations as seen in Figure 8 (right). We see that despite having to do several iterations, up to about nt < 16, convolutional DRAW does not take more wall clock time to train than the same architecture with smaller nt. For larger nt, the training slows down, but it does eventually reach better performance than at lower nt."}, {"heading": "32\u00d7 32 Method NLL Validation (Train)", "text": ""}, {"heading": "4.4. Information Distribution", "text": "We look at how much information different levels and time steps contain. This information is simply the KL divergence in (7) and (22). For a two layer system with one convolutional and one fully connected layer, this is shown in Figure 9.\nWe see that the higher level contains information mainly at the beginning of the computation, whereas the lower layer starts with low information which then gradually increases. This is desirable from a conceptual point of view. It suggests that the network first finds out about an overall structure of the image and then explains the details contained within that structure. Understanding the overall structure rapidly is also convenient if the algorithm needs to respond to observations in a timely manner."}, {"heading": "4.5. Lossy Compression", "text": "We can compress an image with loss of information by storing only a subset of the latent variables, typically the high levels of the hierarchy. We can do this in multilayer convolutional DRAW, storing only higher levels. However we can also store only a subset of time steps, specifically a given number of time steps at the beginning, and let the network generate the rest.\nThe units not stored should be generated from the prior\ndistribution (Equation 4). However we can also generate a more likely image by lowering the variance of the prior Gaussian. We show generations with full variance in row 3 of each block of Figure 4 and with variance zero in row 4 of that figure. We see that using the original variance, the network generates sharp details. Because the generative model is not perfect, the resulting images are less realistic looking as we lower the number of stored time steps. For zero variance we see that the network starts with rough details making a smooth image and then refines it with more time steps. All these generations are produced with a singlelayer convolutional DRAW, and thus, despite being singlelayer, it achieves some level of \u2018conceptual compression\u2019 by first capturing the global structure of the image and then focusing on details.\nThere is another dimension we can vary for lossy compression \u2013 the input scale introduced in subsection 4.2. Even if we store all the latent variables (but not the input bits), the reconstructed images will get less detailed as we scale down the input cost.\nTo build a really good compressor, at each compression rate, we need to find which of the networks, input scales and number of time steps would produce visually good images. For several compression levels, we have looked at images produced by different methods and selected qualitatively which network gave the best looking images. We have not done this per image, just per compression level. We then display compressed images that we have not seen with this selection.\nWe compare our results to JPEG and JPEG2000 compres-\nsion which we obtained using ImageMagick. We found however that these compressors are unable to produce reasonable results for small images (3\u00d732\u00d732) at high compression rates. Instead, we concatenated 100 images into one 3 \u00d7 320 \u00d7 320 image, compressed that and extracted back the compressed small images. The number of bits per image reported is then the number of bits of this image divided by 100. This is actually unfair to our algorithm since any correlations between nearby images can be exploited. Nevertheless we show the comparison in Figure 4. Our algorithm shows better quality than JPEG and JPEG 2000 at all levels where a corruption is easily detectable. Note that even when our algorithm is trained on one specific image size, it can be used on arbitrarily sized images for those networks that contain only convolutional operators."}, {"heading": "5. Conclusion", "text": "In this paper, we introduced convolutional DRAW, a stateof-the-art generative model which demonstrates the potential of sequential computation and recurrent neural networks in scaling up latent variable models. During inference, the algorithm sequentially arrives at a natural stratification of information, ranging from global aspects to lowlevel details. An interesting feature of the method is that, when we restrict ourselves to storing just the high level latent variables, we arrive at a \u2018conceptual compression\u2019 algorithm that rivals the quality of JPEG2000. As a generative model, it outperforms earlier latent variable models on both the Omniglot and ImageNet datasets."}, {"heading": "Acknowledgements", "text": "We thank Aaron van den Oord, Diederik Kingma and Koray Kavukcuoglu for fruitful discussions."}], "references": [{"title": "Modeling highdimensional discrete data with multi-layer neural networks", "author": ["Bengio", "Yoshua", "Samy"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1999}, {"title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "author": ["Denton", "Emily L", "Chintala", "Soumith", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Nice: Non-linear independent components estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1410.8516,", "citeRegEx": "Dinh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Learning representations by maximizing compression", "author": ["Gregor", "Karol", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1108.1169,", "citeRegEx": "Gregor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2011}, {"title": "Deep autoregressive networks", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Mnih", "Andriy", "Blundell", "Charles", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan R"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Keeping the neural networks simple by minimizing the description length of the weights", "author": ["Hinton", "Geoffrey E", "Van Camp", "Drew"], "venue": "In Proceedings of the sixth annual conference on Computational learning theory,", "citeRegEx": "Hinton et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1993}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["Kavukcuoglu", "Koray", "Sermanet", "Pierre", "Boureau", "Y-Lan", "Gregor", "Karol", "Mathieu", "Micha\u00ebl", "LeCun", "Yann"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Le and V.,? \\Q2013\\E", "shortCiteRegEx": "Le and V.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["Sohl-Dickstein", "Jascha", "Weiss", "Eric A", "Maheswaranathan", "Niru", "Ganguli", "Surya"], "venue": "arXiv preprint arXiv:1503.03585,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2015}, {"title": "Factoring variations in natural images with deep gaussian mixture models", "author": ["van den Oord", "Aaron", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Oord et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2014}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "Aaron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Arithmetic coding for data compression", "author": ["Witten", "Ian H", "Neal", "Radford M", "Cleary", "John G"], "venue": "Communications of the ACM,", "citeRegEx": "Witten et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1987}, {"title": "Deconvolutional networks", "author": ["Zeiler", "Matthew D", "Krishnan", "Dilip", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "At the same time, our model greatly improves latent variable image modeling compared to earlier implementations of deep variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2014).", "startOffset": 146, "endOffset": 213}, {"referenceID": 6, "context": "At the same time, our model greatly improves latent variable image modeling compared to earlier implementations of deep variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2014).", "startOffset": 146, "endOffset": 213}, {"referenceID": 7, "context": "Furthermore, it has the advantage of being a simple homogeneous architecture not requiring complex design choices, which is similar to the recurrent structure of DRAW (Gregor et al., 2015).", "startOffset": 167, "endOffset": 188}, {"referenceID": 11, "context": "sparse auto-encoders and sparse coding (Kavukcuoglu et al., 2010; Le, 2013), denoising autoencoders (Vincent et al.", "startOffset": 39, "endOffset": 75}, {"referenceID": 25, "context": ", 2010), deconvolutional networks (Zeiler et al., 2010), restricted Boltzmann machines (Hinton & Salakhutdinov, 2006), deep Boltzmann machines (Salakhutdinov & Hinton, 2009), generative adversarial networks (Goodfellow et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": ", 2010), restricted Boltzmann machines (Hinton & Salakhutdinov, 2006), deep Boltzmann machines (Salakhutdinov & Hinton, 2009), generative adversarial networks (Goodfellow et al., 2014) and variational autoencoders (Kingma & Welling, 2014; Rezende et al.", "startOffset": 159, "endOffset": 184}, {"referenceID": 18, "context": ", 2014) and variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2014).", "startOffset": 37, "endOffset": 104}, {"referenceID": 6, "context": ", 2014) and variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014; Gregor et al., 2014).", "startOffset": 37, "endOffset": 104}, {"referenceID": 7, "context": "Such a mechanism is provided by the DRAW algorithm (Gregor et al., 2015), which is a recurrent type of variational auto-encoder.", "startOffset": 51, "endOffset": 72}, {"referenceID": 6, "context": "We also experiment with making convolutional DRAW hierarchical in a similar way that we would build conventional deep variational auto-encoders (Gregor et al., 2014) \u2013 stacking more layers of latent and deterministic variables.", "startOffset": 144, "endOffset": 165}, {"referenceID": 1, "context": "GANs have been demonstrated to be able to generate realistic looking images (Denton et al., 2015; Radford et al., 2015), with properly aligned edges, using a simple feedforward generative network (Radford et al.", "startOffset": 76, "endOffset": 119}, {"referenceID": 17, "context": "GANs have been demonstrated to be able to generate realistic looking images (Denton et al., 2015; Radford et al., 2015), with properly aligned edges, using a simple feedforward generative network (Radford et al.", "startOffset": 76, "endOffset": 119}, {"referenceID": 17, "context": ", 2015), with properly aligned edges, using a simple feedforward generative network (Radford et al., 2015).", "startOffset": 84, "endOffset": 106}, {"referenceID": 17, "context": "as in (Radford et al., 2015), to obtain sufficient image diversity.", "startOffset": 6, "endOffset": 28}, {"referenceID": 2, "context": "Let us relate this discussion to two other families of generative models, specifically generative adversarial networks (GANs; Goodfellow et al. (2014)) and auto-regressive pixel models.", "startOffset": 126, "endOffset": 151}, {"referenceID": 18, "context": "Stochastic back-propagation through a sampling function is done as in variational auto-encoders (Kingma & Welling, 2014; Rezende et al., 2014).", "startOffset": 96, "endOffset": 142}, {"referenceID": 24, "context": "The underlying compression mechanism for all cases is arithmetic coding (Witten et al., 1987).", "startOffset": 72, "endOffset": 93}, {"referenceID": 14, "context": "The recently introduced Omniglot dataset (Lake et al., 2015) is comprised of 1628 character classes drawn from multiple alphabets with just 20 samples per class.", "startOffset": 41, "endOffset": 60}, {"referenceID": 2, "context": "[1] (Dinh et al., 2014), [2] (Sohl-Dickstein et al.", "startOffset": 4, "endOffset": 23}, {"referenceID": 21, "context": ", 2014), [2] (Sohl-Dickstein et al., 2015), [3] (van den Oord & Schrauwen, 2014), [4] (van den Oord et al.", "startOffset": 13, "endOffset": 42}], "year": 2016, "abstractText": "We introduce a simple recurrent variational autoencoder architecture that significantly improves image modeling. The system represents the stateof-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality \u2018conceptual compression\u2019.", "creator": "LaTeX with hyperref package"}}}