{"id": "1605.01133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Deep Motif: Visualizing Genomic Sequence Classifications", "abstract": "this paper similarly applies a deep groove convolutional / highway mlp framework to classify genomic sequences on the transcription factor / binding site task. to make the model understandable, we propose an optimization driven strategy to extract \" motifs \", or symbolic patterns which visualize the positive class learned mediated by the network. we show that our system, deep motif ( demo ), extracts motifs that are similar to, and in some cases outperform the current well known motifs. in possible addition, we find insight that a continuously deeper model consisting of multiple convolutional and highway layers can outperform a unique single convolutional and fully connected layer in the five previous collaborative state - of - the - art.", "histories": [["v1", "Wed, 4 May 2016 03:33:48 GMT  (625kb,D)", "https://arxiv.org/abs/1605.01133v1", "5 pages; 3 figures ; deep learning ; genomic sequence classification; understanding deep models"], ["v2", "Thu, 2 Jun 2016 14:17:51 GMT  (626kb,D)", "http://arxiv.org/abs/1605.01133v2", "5 pages; 3 figures ; deep learning ; genomic sequence classification; understanding deep models"]], "COMMENTS": "5 pages; 3 figures ; deep learning ; genomic sequence classification; understanding deep models", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jack lanchantin", "ritambhara singh", "zeming lin", "yanjun qi"], "accepted": false, "id": "1605.01133"}, "pdf": {"name": "1605.01133.pdf", "metadata": {"source": "CRF", "title": "DEEP MOTIF: VISUALIZING GENOMIC SEQUENCE CLASSIFICATIONS", "authors": ["Jack Lanchantin", "Ritambhara Singh", "Zeming Lin"], "emails": ["jjl5sw@virginia.edu", "rs3zz@virginia.edu", "zl4ry@virginia.edu", "y2qh@virginia.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Understanding genetic sequences is one of the fundamental tasks of health advancements due to the high correlation of genes with diseases and drugs. An important problem within genetic sequence understanding is related to transcription factors (TFs), which are regulatory proteins that bind to DNA. Each different TF binds to specific transcription factor binding sites (TFBSs) on the DNA sequence to regulate cell machinery. In this work, we focus on accurately classifying and understanding the DNA subsequences that TFs bind to, which will allow us to better understand the underlying biological processes and potentially influence biomedical studies of human health. This task classifies whether or not there is a binding site for a particular TF of interest when given an input DNA sequence.\nChromatin immunoprecipitation (ChIP-seq) technologies and databases such as ENCODE (Consortium et al., 2012) have made binding site sequences available for hundreds of different TFs. Despite these advancements, there are two major drawbacks: (1) ChIP-seq experiments are slow and expensive, (2) although ChIP-seq experiments can find the binding site locations, they cannot find patterns that are common across all of the positive binding sites which can give insight as to why TFs bind to those locations. Thus, there is a need for large scale computational methods that can not only make accurate binding site classifications, but also produce clear patterns that represent the positive binding sites.\nIn order to computationally predict the binding sites, researchers initially used subset frequency counts (Stormo, 2000). Such generative frequency based searching techniques may, however, fail to generalize to unseen examples (Setty & Leslie, 2015). Discriminative techniques such as SVMs have shown to outperform the generative methods by using k-mer features (Ghandi et al., 2014; Setty & Leslie, 2015), but the string kernel based algorithms are limited by the computational complexity of the number of training and testing sequences. Gomes et al. (2014) used a blind-deconvolution approach with motif finding to improve the binding site resolution as well as find multiple sites in an enriched region. Our task focuses on classifying a wide subsequence as a binding site or not, and is not concerned with the resolution or possibility of multiple sites.\nMost recently, DeepBind (Alipanahi et al., 2015) has shown state-of-the-art results on the TFBS classification task by using a neural network based approach. A neural network model is particularly well suitable for the TFBS task considering that it is scalable to a large number of genomic sequences. Although DeepBind achieves better accuracy than previous methods, they use a shallow model with only one convolutional\n1An earlier version of this work was presented at the ICLR 2016 Workshops (Lanchantin et al., 2016). This paper shows the same methods, with a slight improvement on TF prediction and motif generation by tuning a different model for each TF.\nar X\niv :1\n60 5.\n01 13\n3v 2\n[ cs\n.L G\n] 2\nJ un\n2 01\nand one fully connected layer. It has been widely shown that the deeper, or multiple layer models outperform shallow models (Szegedy et al., 2015; Srivastava et al., 2015). For the TFBS task, there is a need to model long range dependencies. Therefore, we introduce a deeper model which is able to detect higher level features from the raw nucleotide sequences and make more accurate binding site classifications.\nAs with many biomedical tasks, obtaining high accuracy results is not of sole importance. There is a need to find interpretable visualizations which help understand the biological process of interest. For TFBS classifications, this is typically done by finding \u201cmotifs\u201d, or consensus sequences which define the positive binding sites for a particular TF. Motifs are represented by position weight matrices (PWMs) corresponding to the probability of each character occurring at a specific position (see Fig. 2) (Stormo, 2013). DeepBind finds motifs by mapping the strongest activations of each feature map in the convolutional layer back to the input space for each positive example in their test set, and then counts the nucleotide frequencies of all subsequences to compute a PWM. However, this method is dependent on the specific testing sequences used, and does not represent positive TFBS patterns in general. We present a method (section 2), which finds motifs that depict the notion of a positive TFBS class learned by our model, and is not specific to any particular sequence. We argue that this method is more applicable for the biomedical task where it is important to get a general understanding of what a positive TFBS site looks like rather than the strong subsequences of specific positive samples.\nThe two major contributions of our Deep Motif (DeMo) model are: (1) we are able to achieve state-of-the-art TFBS classification accuracies by using a deep convolutional/highway MLP network, (2) we show that we can extract visual representations of positive binding sites from our model."}, {"heading": "2 NETWORK DETAILS AND MOTIF EXTRACTION", "text": "Since ChIP-seq experiments output the binding sites in the format of sequences of nucleotide base pairs (i.e. strings with characters A,T,C,G), we can use similar sequence learning models to those used in NLP classification tasks such as sentiment analysis. We introduce the DeMo model (fig. 1a) which uses multiple convolutional layers and a highway multi-layer perceptron (MLP) to make binary classifications. Our experimental results prove that DeMo outperforms the previous state-of-the-art model on the TFBS classification task."}, {"heading": "2.1 DEEPER MODEL FOR TFBS CLASSIFICATION", "text": "We use the raw nucleotide characters as inputs to our network, which are encoded into a one-hot encoding. The encoded input then gets fed through several convolutional layers containing convolutions of 128 feature maps and rectified linear units (ReLUs). Certain convolutional layers contain a length 2 max-pooling. All of our filter sizes are length 5, which is much shorter than the 24 length filters of the one convolutional layer in DeepBind (Alipanahi et al., 2015). However, we note that since we use a length 2 max-pooling in each of the convolutional layers, the final convolution actually \u201csees\u201d a large subsequence of characters from the input sequence, so it is simply a deeper representation of their one layer of filters. The output of the convolutional layers are then max-pooled across the temporal domain resulting in a 128-dimensional vector. We use dropout (Srivastava et al., 2014) for regularization in the convolutional layers.\nTraditionally, following the convolutional layers are fully connected MLP layers. Recently, a new technique called highway networks (Srivastava et al., 2015) have proven effective for deeper representations. Highway networks use gating units which learn to regulate the flow of information through a network. Kim et al. (2015) showed that a highway MLP was more effective than a standard MLP when used after a series of convolutions, hypothesizing that highway networks are especially well-suited to work with convolutional layers due to their ability to adaptively combine local features. We use a fully connected highway network after the max-pooled output of the convolutional layers. The output of the highway MLP is fed to a 2-way softmax function.\nIn our experiments, we train a different model for each TF dataset, and we vary the hyperparameters for each model. Table 1 shows the hyperparameters which were are tuned and selected on the training set for each TF dataset. We found that the TF datasets with fewer training samples had better AUC scores for the smaller (fewer layer) models."}, {"heading": "2.2 CLASS VISUALIZATION FOR MOTIF GENERATION", "text": "Upon training completion, we propose a strategy to extract class specific visualizations, providing an easy interpretation of what the model has learned (fig. 1b). Similar to the methods used in Simonyan et al. (2013) and Yosinski et al. (2015), we seek to optimize the following equation where P+(S) is the probability of the input sequence S (matrix of input length\u00d74, where 4 is our alphabet size) being a positive TFBS computed by the softmax output of our trained model for a specific TF:\nargmax S\nP+(S) + \u03bb\u2016S\u201622 (1)\nwhere \u03bb is the regularization parameter. We find a locally optimal S through backpropagation, where the optimization is with respect to the input sequence and the model weights remain unchanged. Each element of the input matrix S is uniformly initialized to 0.25, and then S is optimized using (1). We clip the optimized values to the interval [0, 1] and convert S into a PWM, using Laplace smoothing. Although we are generating a dense matrix S when the model was trained on a one-hot encoded input matrix, the experiments show promising results of motif generation."}, {"heading": "2.3 CONNECTING TO PREVIOUS STUDIES", "text": "Simonyan et al. (2013) and Yosinski et al. (2015) showed that visualisations of a certain class can be obtained from a ConvNet by optimizing the input, where the samples are images rather than sequences. Vidovic et al. (2015) showed that it is possible to extract the underlying \u201cmotif\u201d from a discriminative model, but do it on kernel machines. Lastly, Zhang et al. (2015) show that deep character-level ConvNets can outperform other models for sequence classification. However, they do not do any type of visual analysis to understand why it works well. DeMo connects all three of these works into a single model which can make high accuracy predictions on biomedical sequences, and also produce a motif which represents a positive binding site class."}, {"heading": "3 EXPERIMENTS AND RESULTS", "text": "In order to prove the effectiveness of a deeper model on the TFBS task, we ran DeMo on the same 108 leukemia cell TF datasets used in Alipanahi et al. (2015). Each TF dataset has an average of 30,819 training sequences, and each sequence consists of 101 DNA-base characters (A,C,G,T). Due to the separate train/test data for each TF, we train a separate model for each individual TF dataset.\nFor the TFBS classification task, our model outperforms DeepBind\u2019s by achieving a higher AUC for 92 out of the 108 TF datasets. A comparison is shown in figure 2. In addition, our model achieves a median AUC of 0.951 whereas DeepBind\u2019s is 0.931.\nTo evaluate our motifs, we performed two comparison strategies against JASPAR motifs (Mathelier et al., 2015), which are widely known within the biological community to be the \u201cgold standard\u201d representations of positive binding sites for hundreds of TFs. We were limited to a comparison of 57 out of our 108 TF datasets by the TFs which JASPAR has motifs for.\nFor our first strategy, in order to compare the similarity of our motifs, we use a tool called Tomtom (Gupta et al., 2007; Bailey et al., 2009), which compares a specific motif against JASPAR motifs and returns significant matches using their defined statistical measure of motif-motif similarity. Out of the 57 tested, we find that 36 of our motifs (using the windowing approach) significantly match JASPAR motifs (q-value < 0.5). A comparison of motifs can be seen in figure 2.\nFor our second strategy, we compare how well our motifs score on the positive TFBS test sequences against JASPAR motifs using the Average Motif Affinity (AMA) tool (Buske et al., 2010; Bailey et al., 2009), which scores a set of sequences given a motif, treating each position in the sequence as a possible binding site. Although our method can generate motifs up to length 101 (size of our input sequences), JASPAR motifs are much shorter. In order to handle this issue, we split our motif into all possible windows which are the same size as the JASPAR motif. We then rank each window by average information content, and select the most informative motif to compare against JASPAR. We run the AMA tool on all positive test sequences for each TF, and compare the scoring of our motif vs the JASPAR motif. We find that our motifs are able to outscore (> 50% of test sequences) JASPAR motifs on 29 out of the 57 motifs. It is important to note that although the JASPAR motifs have been carefully generated using an ensemble approach with much larger TFBS datasets compared to ours, they are not guaranteed to be accurate representations of the positive binding sites."}, {"heading": "4 CONCLUSION", "text": "We present Deep Motif (DeMo), a convolutional/highway MLP network which outperforms the state-ofthe-art baseline for 92 different TFBS datasets, as well as generate motifs, or interpretable patterns that represent the important transcription factor binding patterns. Although our experiments are on genomic sequence classifcation, DeMo is a generic model for visualizing sequence classifcation tasks. We believe our model is applicable to other sequence classification tasks which demand a visual interpretation of the classes."}], "references": [{"title": "Predicting the sequence specificities of dna-and rna-binding proteins by deep learning", "author": ["Babak Alipanahi", "Andrew Delong", "Matthew T Weirauch", "Brendan J Frey"], "venue": "Nature biotechnology,", "citeRegEx": "Alipanahi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alipanahi et al\\.", "year": 2015}, {"title": "Meme suite: tools for motif discovery and searching", "author": ["Timothy L Bailey", "Mikael Boden", "Fabian A Buske", "Martin Frith", "Charles E Grant", "Luca Clementi", "Jingyuan Ren", "Wilfred W Li", "William S Noble"], "venue": "Nucleic acids research, pp. gkp335,", "citeRegEx": "Bailey et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bailey et al\\.", "year": 2009}, {"title": "Assigning roles to dna regulatory motifs using comparative genomics", "author": ["Fabian A Buske", "Mikael Bod\u00e9n", "Denis C Bauer", "Timothy L Bailey"], "venue": null, "citeRegEx": "Buske et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buske et al\\.", "year": 2010}, {"title": "An integrated encyclopedia of dna elements in the human", "author": ["ENCODE Project Consortium"], "venue": "genome. Nature,", "citeRegEx": "Consortium,? \\Q2012\\E", "shortCiteRegEx": "Consortium", "year": 2012}, {"title": "Enhanced regulatory sequence prediction using gapped k-mer features", "author": ["Mahmoud Ghandi", "Dongwon Lee", "Morteza Mohammad-Noori", "Michael A Beer"], "venue": null, "citeRegEx": "Ghandi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ghandi et al\\.", "year": 2014}, {"title": "Decoding chip-seq with a double-binding signal refines binding peaks to single-nucleotides and predicts cooperative interaction", "author": ["Antonio LC Gomes", "Thomas Abeel", "Matthew Peterson", "Elham Azizi", "Anna Lyubetskaya", "Lu\u0131\u0301s Carvalho", "James Galagan"], "venue": "Genome research,", "citeRegEx": "Gomes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2014}, {"title": "Quantifying similarity between motifs", "author": ["Shobhit Gupta", "John A Stamatoyannopoulos", "Timothy L Bailey", "William S Noble"], "venue": "Genome biology,", "citeRegEx": "Gupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2007}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Deep motif: Visualizing genomic sequence classifications", "author": ["Jack Lanchantin", "Ritambhara Singh", "Zeming Lin", "Yanjun Qi"], "venue": "ICLR Workshops,", "citeRegEx": "Lanchantin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lanchantin et al\\.", "year": 2016}, {"title": "Jaspar 2016: a major expansion and update of the open-access database of transcription factor binding profiles", "author": ["Anthony Mathelier", "Oriol Fornes", "David J Arenillas", "Chih-yu Chen", "Gr\u00e9goire Denay", "Jessica Lee", "Wenqiang Shi", "Casper Shyr", "Ge Tan", "Rebecca Worsley-Hunt"], "venue": "Nucleic acids research, pp. gkv1176,", "citeRegEx": "Mathelier et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mathelier et al\\.", "year": 2015}, {"title": "Seqgl identifies context-dependent binding signals in genome-wide regulatory element maps", "author": ["Manu Setty", "Christina S Leslie"], "venue": null, "citeRegEx": "Setty and Leslie.,? \\Q2015\\E", "shortCiteRegEx": "Setty and Leslie.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Dna binding sites: representation and discovery", "author": ["Gary D Stormo"], "venue": null, "citeRegEx": "Stormo.,? \\Q2000\\E", "shortCiteRegEx": "Stormo.", "year": 2000}, {"title": "Modeling the specificity of protein-dna interactions", "author": ["Gary D Stormo"], "venue": "Quantitative biology,", "citeRegEx": "Stormo.,? \\Q2013\\E", "shortCiteRegEx": "Stormo.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Opening the black box: Revealing interpretable sequence motifs in kernel-based learning algorithms", "author": ["Marina M-C Vidovic", "Nico G\u00f6rnitz", "Klaus-Robert M\u00fcller", "Gunnar R\u00e4tsch", "Marius Kloft"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Vidovic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vidovic et al\\.", "year": 2015}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "In order to computationally predict the binding sites, researchers initially used subset frequency counts (Stormo, 2000).", "startOffset": 106, "endOffset": 120}, {"referenceID": 4, "context": "Discriminative techniques such as SVMs have shown to outperform the generative methods by using k-mer features (Ghandi et al., 2014; Setty & Leslie, 2015), but the string kernel based algorithms are limited by the computational complexity of the number of training and testing sequences.", "startOffset": 111, "endOffset": 154}, {"referenceID": 0, "context": "Most recently, DeepBind (Alipanahi et al., 2015) has shown state-of-the-art results on the TFBS classification task by using a neural network based approach.", "startOffset": 24, "endOffset": 48}, {"referenceID": 8, "context": "Although DeepBind achieves better accuracy than previous methods, they use a shallow model with only one convolutional An earlier version of this work was presented at the ICLR 2016 Workshops (Lanchantin et al., 2016).", "startOffset": 192, "endOffset": 217}, {"referenceID": 2, "context": "Chromatin immunoprecipitation (ChIP-seq) technologies and databases such as ENCODE (Consortium et al., 2012) have made binding site sequences available for hundreds of different TFs. Despite these advancements, there are two major drawbacks: (1) ChIP-seq experiments are slow and expensive, (2) although ChIP-seq experiments can find the binding site locations, they cannot find patterns that are common across all of the positive binding sites which can give insight as to why TFs bind to those locations. Thus, there is a need for large scale computational methods that can not only make accurate binding site classifications, but also produce clear patterns that represent the positive binding sites. In order to computationally predict the binding sites, researchers initially used subset frequency counts (Stormo, 2000). Such generative frequency based searching techniques may, however, fail to generalize to unseen examples (Setty & Leslie, 2015). Discriminative techniques such as SVMs have shown to outperform the generative methods by using k-mer features (Ghandi et al., 2014; Setty & Leslie, 2015), but the string kernel based algorithms are limited by the computational complexity of the number of training and testing sequences. Gomes et al. (2014) used a blind-deconvolution approach with motif finding to improve the binding site resolution as well as find multiple sites in an enriched region.", "startOffset": 84, "endOffset": 1263}, {"referenceID": 16, "context": "It has been widely shown that the deeper, or multiple layer models outperform shallow models (Szegedy et al., 2015; Srivastava et al., 2015).", "startOffset": 93, "endOffset": 140}, {"referenceID": 13, "context": "It has been widely shown that the deeper, or multiple layer models outperform shallow models (Szegedy et al., 2015; Srivastava et al., 2015).", "startOffset": 93, "endOffset": 140}, {"referenceID": 15, "context": "2) (Stormo, 2013).", "startOffset": 3, "endOffset": 17}, {"referenceID": 0, "context": "All of our filter sizes are length 5, which is much shorter than the 24 length filters of the one convolutional layer in DeepBind (Alipanahi et al., 2015).", "startOffset": 130, "endOffset": 154}, {"referenceID": 13, "context": "Recently, a new technique called highway networks (Srivastava et al., 2015) have proven effective for deeper representations.", "startOffset": 50, "endOffset": 75}, {"referenceID": 0, "context": "All of our filter sizes are length 5, which is much shorter than the 24 length filters of the one convolutional layer in DeepBind (Alipanahi et al., 2015). However, we note that since we use a length 2 max-pooling in each of the convolutional layers, the final convolution actually \u201csees\u201d a large subsequence of characters from the input sequence, so it is simply a deeper representation of their one layer of filters. The output of the convolutional layers are then max-pooled across the temporal domain resulting in a 128-dimensional vector. We use dropout (Srivastava et al., 2014) for regularization in the convolutional layers. Traditionally, following the convolutional layers are fully connected MLP layers. Recently, a new technique called highway networks (Srivastava et al., 2015) have proven effective for deeper representations. Highway networks use gating units which learn to regulate the flow of information through a network. Kim et al. (2015) showed that a highway MLP was more effective than a standard MLP when used after a series of convolutions, hypothesizing that highway networks are especially well-suited to work with convolutional layers due to their ability to adaptively combine local features.", "startOffset": 131, "endOffset": 960}, {"referenceID": 11, "context": "Similar to the methods used in Simonyan et al. (2013) and Yosinski et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 11, "context": "Similar to the methods used in Simonyan et al. (2013) and Yosinski et al. (2015), we seek to optimize the following equation where P+(S) is the probability of the input sequence S (matrix of input length\u00d74, where 4 is our alphabet size) being a positive TFBS computed by the softmax output of our trained model for a specific TF: argmax S P+(S) + \u03bb\u2016S\u20162 (1) where \u03bb is the regularization parameter.", "startOffset": 31, "endOffset": 81}, {"referenceID": 11, "context": "3 CONNECTING TO PREVIOUS STUDIES Simonyan et al. (2013) and Yosinski et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "3 CONNECTING TO PREVIOUS STUDIES Simonyan et al. (2013) and Yosinski et al. (2015) showed that visualisations of a certain class can be obtained from a ConvNet by optimizing the input, where the samples are images rather than sequences.", "startOffset": 33, "endOffset": 83}, {"referenceID": 11, "context": "3 CONNECTING TO PREVIOUS STUDIES Simonyan et al. (2013) and Yosinski et al. (2015) showed that visualisations of a certain class can be obtained from a ConvNet by optimizing the input, where the samples are images rather than sequences. Vidovic et al. (2015) showed that it is possible to extract the underlying \u201cmotif\u201d from a discriminative model, but do it on kernel machines.", "startOffset": 33, "endOffset": 259}, {"referenceID": 11, "context": "3 CONNECTING TO PREVIOUS STUDIES Simonyan et al. (2013) and Yosinski et al. (2015) showed that visualisations of a certain class can be obtained from a ConvNet by optimizing the input, where the samples are images rather than sequences. Vidovic et al. (2015) showed that it is possible to extract the underlying \u201cmotif\u201d from a discriminative model, but do it on kernel machines. Lastly, Zhang et al. (2015) show that deep character-level ConvNets can outperform other models for sequence classification.", "startOffset": 33, "endOffset": 407}, {"referenceID": 0, "context": "3 EXPERIMENTS AND RESULTS In order to prove the effectiveness of a deeper model on the TFBS task, we ran DeMo on the same 108 leukemia cell TF datasets used in Alipanahi et al. (2015). Each TF dataset has an average of 30,819 training sequences, and each sequence consists of 101 DNA-base characters (A,C,G,T).", "startOffset": 160, "endOffset": 184}, {"referenceID": 9, "context": "To evaluate our motifs, we performed two comparison strategies against JASPAR motifs (Mathelier et al., 2015), which are widely known within the biological community to be the \u201cgold standard\u201d representations of positive binding sites for hundreds of TFs.", "startOffset": 85, "endOffset": 109}, {"referenceID": 6, "context": "For our first strategy, in order to compare the similarity of our motifs, we use a tool called Tomtom (Gupta et al., 2007; Bailey et al., 2009), which compares a specific motif against JASPAR motifs and returns significant matches using their defined statistical measure of motif-motif similarity.", "startOffset": 102, "endOffset": 143}, {"referenceID": 1, "context": "For our first strategy, in order to compare the similarity of our motifs, we use a tool called Tomtom (Gupta et al., 2007; Bailey et al., 2009), which compares a specific motif against JASPAR motifs and returns significant matches using their defined statistical measure of motif-motif similarity.", "startOffset": 102, "endOffset": 143}, {"referenceID": 2, "context": "For our second strategy, we compare how well our motifs score on the positive TFBS test sequences against JASPAR motifs using the Average Motif Affinity (AMA) tool (Buske et al., 2010; Bailey et al., 2009), which scores a set of sequences given a motif, treating each position in the sequence as a possible binding site.", "startOffset": 164, "endOffset": 205}, {"referenceID": 1, "context": "For our second strategy, we compare how well our motifs score on the positive TFBS test sequences against JASPAR motifs using the Average Motif Affinity (AMA) tool (Buske et al., 2010; Bailey et al., 2009), which scores a set of sequences given a motif, treating each position in the sequence as a possible binding site.", "startOffset": 164, "endOffset": 205}], "year": 2016, "abstractText": "We apply a deep convolutional/highway MLP framework to classify genomic sequences on the transcription factor binding site task. To make the model understandable, we propose an optimization driven strategy to extract \u201cmotifs\u201d, or symbolic patterns which visualize the positive class learned by the network. We show that our system, Deep Motif (DeMo), extracts motifs that are similar to, and in some cases outperform the current well known motifs. In addition, we find that a deeper model consisting of multiple convolutional and highway layers can outperform a single convolutional and fully connected layer in the previous state-of-the-art.1", "creator": "LaTeX with hyperref package"}}}