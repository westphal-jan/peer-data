{"id": "1605.01329", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2016", "title": "Single Channel Speech Enhancement Using Outlier Detection", "abstract": "distortion of the entire underlying speech is a common problem for single - channel speech enhancement algorithms, and hinders such experimental methods from them being used more specifically extensively. a dictionary based speech enhancement method that emphasizes preserving the underlying speech is proposed. spectral patches of clean speech sampled are sampled and clustered to train a dictionary. given a noisy speech spectral patch, the best matching dictionary acoustic entry is selected and used to estimate the noise power applied at each time - frequency detection bin. removing the noise artifact estimation step is formulated as an outlier detection problem, where the noise at each bin is assumed present only if it is an orthogonal outlier to enter the corresponding bin of the best spaced matching dictionary entry. this framework assigns higher priority jobs in removing spectral elements that strongly deviate from a typical spoken unit stored in the trained dictionary. even without the aid effort of a separate noise model, this method can achieve significant noise reduction techniques for various non - stationary noises, while effectively preserving merely the underlying speech in more intensely challenging noisy environments.", "histories": [["v1", "Wed, 4 May 2016 16:16:12 GMT  (611kb,D)", "http://arxiv.org/abs/1605.01329v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["eunjoon cho", "bowon lee", "ronald schafer", "bernard widrow"], "accepted": false, "id": "1605.01329"}, "pdf": {"name": "1605.01329.pdf", "metadata": {"source": "CRF", "title": "SINGLE CHANNEL SPEECH ENHANCEMENT USING OUTLIER DETECTION", "authors": ["Eunjoon Cho", "Bowon Lee", "Ronald Schafer", "Bernard Widrow"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Speech enhancement, noise estimation, outlier detection, speech distortion\n1. INTRODUCTION\nSingle-channel speech enhancement is an underdetermined problem since only the noisy speech is available. A widely used assumption is that the speech is temporally sparse, and the noise properties vary slowly compared to speech. With this assumption, the noise power estimate at a certain frequency is updated when the spectral bin is judged to be dominated by noise. This noise estimate is used to enhance speech until another chance to update the noise estimate occurs. Noise estimation methods [1, 2, 3] apply different criteria on when to update the noise, and are typically used to compute the spectral gain for speech enhancement algorithms [4, 5].\nGiven the assumption that these methods rely on, such algorithms are more susceptible to non-stationary noise. Methods that use prior knowledge of the speech and/or noise have shown to be effective under more realistic non-stationary\nThis was work was done while Eunjoon Cho was at Stanford University. He is currently affiliated with Google Inc.\nnoisy environments [6, 7, 8, 9, 10]. These methods learn a representation the sources in advance, and use this to enhance the noisy speech input. Parametric models capture the sources in a compact representation by training the coefficients of a model [6, 8]. Non-parametric models learn representations of the sources directly from the spectrum, and are trained using vector quantization (VQ) [7, 11], non-negative matrix factorization (NMF) and its probabilistic counterparts [12, 9], and variants of singular value decomposition (SVD) [10].\nHowever, these methods often rely on a separate noise model [6, 10, 9, 11], which limits the algorithms to work in environments that have been previously trained on. When used in new environments, the algorithm can distort the underlying speech. For offline enhancement, semi-supervised NMF [12, 13] provides a solution to enhace without a separate noise model. However, for real-time speech enhancement this can be computationally expensive, since a non-stationary environment would require frequent updates of the noise bases.\nSpeech distortion is one of the reasons why enhancement methods are not being used more extensively in real life applications. We thus provide a framework where the speech distortion can be limited for a wide range of noisy environments, without using explicit noise models. Noise is detected when a region of the noisy speech spectrum is considered an outlier to the corresponding region of a trained speech dictionary entry. In other words, a region of the noisy speech spectrogram is detected as noise if it doesn\u2019t fit our understanding of what a typical speech spectrogram should look like. Unfortunately, without the use of a separate noise model, noise reduction for speech-shaped noise, such as babble noise, is limited. However, the benefit of the outlier framework is that it removes noise that is strongly inconsistent with the speech dictionary with higher priority. For noise that is more overlapped with speech, it focuses on preserving the underlying speech as much as possible, and thus limits the amount of speech distortion while removing less noise.\n2. PROPOSED METHOD\nThe overall enhancement method is a two stage process. The training stage learns a dictionary of clean speech units. The enhancement stage uses the dictionary in an outlier detection framework to reduce the noise in noisy speech inputs.\nar X\niv :1\n60 5.\n01 32\n9v 1\n[ cs\n.S D\n] 4\nM ay\n2 01\n6"}, {"heading": "2.1. Dictionary training", "text": "Given a clean speech sentence, x(n), the magnitude-squared Short-time Fourier Transform (STFT) is computed to estimate the power. The magnitude-squared STFT can be denoted as |Xm(k)|2, where k is the Discrete Fourier Transform (DFT) frequency bin and m is the frame index. |Xm(k)|2 is normalized, such that the average power of a time-frequency bin over a training sentence1 is 1. The normalized spectrum is denoted as |X\u0303m(k)|2 , A \u00b7 |Xm(k)|2, where A is the normalization constant. The purpose of this stage is to compensate the amplitude difference of recordings by different speakers. This normalization, however, only corrects for a scalar multiplication of the speakers\u2019 inputs, and doesn\u2019t necessarily correct differences in perceived loudness or filtering effects due to the different recording environments.\nPatches of the normalized magnitude-squared spectra are sampled throughout the sentence. A patch can span multiple frames. For example, if a patch has a length of L in the time dimension, the patch sampled at frame m, would consist of a sequence of magnitude-squared spectra from frame m to m+L\u2212 1, i.e., {|X\u0303m|2, |X\u0303m+1|2, ..., |X\u0303m+L\u22121|2}. In order to guarantee a good mix of patches in our training data, we sample patches such that there are no overlapped patches. We sample a patch beginning at every other M \u2019th frame where M > L.\nIn order to keep the input feature at a computationally reasonable size, we map the frequency bins through a sequence of triangular filter bands. The Mel or Bark scale can also be used to center the filter bands such that the lower frequencies are emphasized [14]. For simplicity, here we use uniformly separated filter bands. These filters are also normalized such that the power remains the same after the transformation. The output patch has a reduced dimension of L \u00d7 N \u2032, where N \u2032 is the number of triangular filter bands. We concatenate the end of each column in this patch, and express the patch as a column vector, FX , of size L \u00b7N \u2032.\nThe sampled patches are then clustered using the k-means algorithm. To match the dynamic range in human auditory perception, logarithmic distance is used to cluster FX . The cluster centroids are stored as entries in the dictionary, where each entry can be viewed as a commonly spoken speech unit. The constant K determines the number of dictionary entries."}, {"heading": "2.2. Speech enhancement using the outlier framework", "text": "The magnitude-squared STFT is computed from the noisy speech input, y(n), and patches beginning at each frame are created. A patch is passed through the same filter bands used for training, resulting in a feature vector, FY , of dimension L \u00b7N \u2032.\nFor each noisy input patch, we search for the best matching dictionary entry. Logarithmic distance is used to find the\n1The TIMIT database is used for training the dictionary, and a sentence refers to a sample utterance from the TIMIT database.\nclosest entry, since it was also used for clustering the training data. The cluster (or dictionary entry), j, that minimizes the logarithmic distance is selected as the best match.\n(a\u2217, j\u2217) = arg min a,j\n\u2016logFY \u2212 log ( a \u00b7 S\u0304j ) \u20162\nS\u0304j is the j\u2019th dictionary entry, and a is a factor that corrects the amplitude difference between the speech used in training and the input noisy speech. Simply normalizing the noisy speech spectrum in advance, as we did for training, will not work since the noise will affect the normalization. The optimal a for an entry j can be computed as\n\u2202\n\u2202a L\u00b7N \u2032\u2211 i=1 ( logFY (i)\u2212 log a\u2212 log S\u0304j(i) )2 = 0\n\u21d4 a = exp  1 L \u00b7N \u2032 L\u00b7N \u2032\u2211 i=1 log FY (i) S\u0304j(i)  The optimal j is searched by iterating over the dictionary entries. Given the best match, an estimate of the clean speech patch can be computed as F\u0302X , a\u2217S\u0304j\u2217 .\nIf F\u0302X is an accurate representation of the underlying speech, it could be used directly to replace the noisy speech. However, as shown in [7], a VQ representation of the speech spectrum is, by itself, insufficient to capture all the subtle nuances of the underlying speech. Also, without a separate noise model, distortion of the speech is likely to occur if we simply replace the noisy patch with the best dictionary entry.\nTherefore, instead of using the dictionary entries to directly quantize the noisy speech patch, we use it as a reference to estimate the noise. If the power at a noisy patch bin, FY (i), is much greater than the corresponding bin of the best dictionary entry, F\u0302X(i), it is likely that the bin is dominated by noise. The greater the deviation, the more likely it is a noise component. Instead of trying to remove all the noise, we prioritize in first removing noise components that are strong outliers to the selected dictionary entry.\nTo detect whether a spectral bin, FY (i), is an outlier, an underlying distribution for F\u0302X(i) is necessary. One distribution commonly used for speech enhancement [4] is to model Fourier Transform coefficients using a complex Gaussian distribution. Under this assumption, Xm(k), is complex Gaussian distributed, and |Xm(k)|2 and |X\u0303m(k)|2 are exponentially distributed. We rely on computing a wideband spectrum to capture the formant envelope of the speech spectrum. With sufficient number of triangular filter bands, the power within each filter band will be approximately the same. We thus assume FX(i) is also exponentially distributed. A dictionary entry, S\u0304j , is the cluster centroid and each training patch, FX(i), in cluster j can be viewed as independent exponential random variables with mean S\u0304j(i). Since F\u0302X(i) is just a scaled cluster centroid, we can compare whether FY (i) is a\noriginal spectrogram. Frame: 196best estimat using odel basis. Basis: 166 Min distance = 47348.5036\nNoise to signal ratio, MSE: 47348.5036\n2 4 6 8 10\n10\n20\n30\n40\n50\n60\noriginal spectrogram. Frame: 196best estimat using odel basis. Basis: 166 Min distance = 47348.5036\nNoise to signal ratio, MSE: 47348.5036\n2 4 6 8 10\n10\n20\n30\n40\n50\n60\nNoisy speech patch\nBest entry patch Outlier\nNormal\nie iF (i)\nje jF (j)\ni\nj\nF\u0302X(i)\nF\u0302X(j)\nFY (j)\nFY (i)\nFig. 1: Only two dimensions (i, j) of the patches are projected here for illustration purposes. F (i), F (j) are exponential random variables with mean \u03bbi , 1F\u0302X(i) and \u03bbj , 1\nF\u0302X(j)\n. Here,\nFY (i) is more likely to be an outlier than FY (j).\ngood fit to this cluster by comparing it against an exponential distribution that has a mean of F\u0302X(i).\nAssume that F (i) is a random exponential variable with mean F\u0302X(i). In other words, F (i) is a potential patch inside a cluster, where the cluster\u2019s centroid is F\u0302X(i). The p-value of FY (i), i.e., P (F (i) \u2265 FY (i)), is used to determine whether FY (i) is an outlier or not. If this p-value is less than a threshold, c, it is considered an outlier. Fig. 1 illustrates the process of evaluating whether a frequency bin of a patch is an outlier.\nIf an element, FY (i), is an outlier, we assume the noise is present and use spectral subtraction to estimate the noise. If it is not an outlier, we assume there is no noise. Specifically,\nF\u0302D(i)\n=\n{ max [ FY (i)\u2212 F\u0302X(i), 0 ] , if P (F (i) \u2265 FY (i)) < c\n0 , otherwise\nwhere F\u0302D(i) is the estimated noise patch. The decision to ignore the case when an element is not an outlier, is to preserve the underlying speech as much as possible when in doubt. Nonetheless, the user can control the level of noise reduction by changing the threshold, c.\nWith the estimated noise, a Wiener-like mask is computed.\nFH(i) = FY (i)\u2212 F\u0302D(i)\nFY (i) (1)\nA mask is computed for every frame, so if each patch is of length L > 1, these patches will overlap. For each frame, we average the L different mask gains to compute the gain at that specific frame. To enhance the noisy spectrum, we need a mask in the original frequency domain. We thus transform the mask, by interpolating it with the same triangular filter bands\nTime\nF re\nq u e n c y\n\u221260\n\u221240\n\u221220\n0\n(a) Noisy speech\nTime\nF re\nq u e n c y\n0\n0.5\n1\nused for analysis, so that the final mask has a frequency dimension equal to the original DFT size. This mask is then applied to the input noisy speech spectrum to get our enhanced spectrum. An example of this mask is shown in Fig. 2.\n3. EVALUATION\nTwo methods, both of which assume no knowledge of the noise, are used for comparison. The first algorithm is based on Ellis\u2019s method [7] where a VQ representation of the speech is used to quantize the noisy speech. To independently evaluate the effect of the outlier framework, only Eq. (1) is replaced with FH(i) = F\u0302X(i)/FY (i). In other words, the mask is computed based on the best selected dictionary entry. The second algorithm is the MMSE noise estimation algorithm by Gerkmann used with an a-priori SNR estimated Wiener filter [3]. Unlike our method, the MMSE algorithm relies on no prior knowledge of speech in general. However, given that the MMSE algorithm has recently shown to be one of the more effective methods for single channel speech enhancement [15], we compare it here to highlight some of the more challenging situations a speech enhancement system can encounter. A smoothing factor of \u03b1 = 0.98 was used for the a-priori SNR estimation. Our outlier method was used with a threshold of c = 0.0001.\nTo train the speech dictionary, 10,000 patches were sampled from randomly selected sentences in the training section of the TIMIT speech database [16]. These sentences consisted of both male and female speakers with various accents. 60 filter bands (N \u2032) were used to reduce the frequency dimen-\nsion of the patches, and various patch lengths (L = 1, 2, 4, 8) were evaluated. A similar search was done for other parameters such as the dictionary size (K = 100, 200, 400, 800, 1600) and the analysis window length (5, 10, 15, 20, 25 ms) used to compute the STFT. The parameters that maximized our results (L = 2, K = 800, window length = 10 ms) are shown here. The effect of these parameters are analyzed in [14].\nIn the literature, a longer analysis window is often used [9, 10] such that the harmonics are better defined. This makes the entries more incoherent with the noise and can lead to better separation when mixed with wideband noise. However, by using a shorter window the formant structure is emphasized, and this allows a smaller number of dictionary entries to capture the multi-speaker training data.\nTo generate the noisy speech, 5 different noise sources (bird, siren, train, wind and crowd babble) available online [17], were mixed with sentences from the TIMIT database. 10 sentences from 10 different speakers (5 male, 5 female), not included in the training data, were used to generate these noisy sentences. The average scores on the test set are provided for each of the experiments discussed below.\nFig. 3 shows the Perceptual Evaluation of Speech Quality (PESQ) for the enhanced output. The benefit of using a model based on speech is clear for bird and siren noises. When the spectral shape of the noise is incoherent with speech, a great amount of noise reduction is achievable. However, when the noise is strongly mixed with speech, enhancement is much more challenging. This is shown by the PESQ gains for train, wind and crowd babble noise. The gain is minimal or even a loss in perceptual quality is often observed.\nThe benefit of the outlier method is highlighted in Fig. 4 where the distortion of the underlying speech is measured. The mask in Eq. (1) is initially computed from a noisy speech input. Then, only the underlying clean speech is fed through this computed mask to measure how the speech is affected by the algorithm. The frequency-weighted segmental SNR (fwSegSNR) [18] is another measure known to highly correlate with subjective mean opinion scores (MOS) [19]. This is used to measure the quality of the clean speech output. Fig. 4 shows that the outlier method is consistent in preserving the underlying speech regardless of the environment. The framework only reduces the noise when it is possible to do so without distorting the speech. Being able to limit the amount of speech distortion regardless of the noise encountered is a key benefit of the outlier framework.\n4. CONCLUSION\nA speech enhancement method that is generally applicable to various noisy environments without extensive modification for each environment can be useful. We achieve this by first removing noise components that strongly differ from the trained speech model, and passing the noisy speech when in doubt. Using this outlier framework we are able to greatly reduce noises that are incoherent with speech even if they are non-stationary. Moreover, in environments where separation of the mixed sources is difficult, speech distortion is minimized. This allows one to use this system as a pre-processing step for various speech processing/recognition applications without much worry of distorting the underlying speech.\n5. REFERENCES\n[1] Rainer Martin, \u201cNoise power spectral density estimation based on optimal smoothing and minimum statistics,\u201d Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 5, pp. 504\u2013512, 2001.\n[2] Israel Cohen, \u201cNoise spectrum estimation in adverse environments: Improved minima controlled recursive averaging,\u201d Speech and Audio Processing, IEEE Transactions on, vol. 11, no. 5, pp. 466\u2013475, 2003.\n[3] Timo Gerkmann and Richard C Hendriks, \u201cUnbiased mmse-based noise power estimation with low complexity and low tracking delay,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 4, pp. 1383\u20131393, 2012.\n[4] Yariv Ephraim and David Malah, \u201cSpeech enhancement using a minimum-mean square error short-time spectral amplitude estimator,\u201d Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 32, no. 6, pp. 1109\u2013 1121, 1984.\n[5] Pascal Scalart et al., \u201cSpeech enhancement based on a priori signal to noise estimation,\u201d in Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 2, pp. 629\u2013632.\n[6] Sriram Srinivasan, Jonas Samuelsson, and W Bastiaan Kleijn, \u201cCodebook driven short-term predictor parameter estimation for speech enhancement,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, no. 1, pp. 163\u2013176, 2006.\n[7] Daniel PW Ellis and Ron J Weiss, \u201cModel-based monaural source separation using a vector-quantized phase-vocoder representation,\u201d in Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on. IEEE, 2006, vol. 5, pp. V\u2013V.\n[8] Ji Ming, Ramji Srinivasan, and Danny Crookes, \u201cA corpus-based approach to speech enhancement from nonstationary noise,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 4, pp. 822\u2013836, 2011.\n[9] Zhiyao Duan, Gautham J Mysore, and Paris Smaragdis, \u201cSpeech enhancement by online non-negative spectrogram decomposition in non-stationary noise environments.,\u201d in INTERSPEECH, 2012.\n[10] Christian D Sigg, Tomas Dikk, and Joachim M Buhmann, \u201cSpeech enhancement using generative dictionary learning,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 6, pp. 1698\u2013 1712, 2012.\n[11] Mads Gr\u00e6sb\u00f8ll Christensen and Pejman Mowlaee, \u201cA new metric for vq-based speech enhancement and separation,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4764\u20134767.\n[12] Paris Smaragdis, \u201cConvolutive speech bases and their application to supervised speech separation,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 15, no. 1, pp. 1\u201312, 2007.\n[13] Gautham J Mysore and Paris Smaragdis, \u201cA nonnegative approach to semi-supervised separation of speech from noise with the use of temporal dynamics,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 17\u201320.\n[14] Eunjoon Cho, Exploiting speech structure for noise estimation in single channel speech enhancement, Ph.D. thesis, Stanford University, 2013.\n[15] Jalal Taghia, N Mohammadiha, Jinqiu Sang, V Bouse, and R Martin, \u201cAn evaluation of noise power spectral density estimation algorithms in adverse acoustic environments,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4640\u20134643.\n[16] John S Garofolo, Lori F Lamel, William M Fisher, Jonathon G Fiscus, and David S Pallett, \u201cDarpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1,\u201d NASA STI/Recon Technical Report N, vol. 93, pp. 27403, 1993.\n[17] \u201cGrsites,\u201d http://www.grsites.com/archive/sounds/.\n[18] JM Tribolet, P Noll, B McDermott, and R Crochiere, \u201cA study of complexity and quality of speech waveform coders,\u201d in Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201978. IEEE, 1978, vol. 3, pp. 586\u2013590.\n[19] Philipos C Loizou, Speech enhancement: theory and practice, CRC press, 2007."}], "references": [{"title": "Noise power spectral density estimation based on optimal smoothing and minimum statistics", "author": ["Rainer Martin"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 9, no. 5, pp. 504\u2013512, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Noise spectrum estimation in adverse environments: Improved minima controlled recursive averaging", "author": ["Israel Cohen"], "venue": "Speech and Audio Processing, IEEE Transactions on, vol. 11, no. 5, pp. 466\u2013475, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Unbiased mmse-based noise power estimation with low complexity and low tracking delay", "author": ["Timo Gerkmann", "Richard C Hendriks"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 4, pp. 1383\u20131393, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator", "author": ["Yariv Ephraim", "David Malah"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 32, no. 6, pp. 1109\u2013 1121, 1984.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1984}, {"title": "Speech enhancement based on a priori signal to noise estimation", "author": ["Pascal Scalart"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 2, pp. 629\u2013632.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Codebook driven short-term predictor parameter estimation for speech enhancement", "author": ["Sriram Srinivasan", "Jonas Samuelsson", "W Bastiaan Kleijn"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, no. 1, pp. 163\u2013176, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Model-based monaural source separation using a vector-quantized phase-vocoder representation", "author": ["Daniel PW Ellis", "Ron J Weiss"], "venue": "Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference on. IEEE, 2006, vol. 5, pp. V\u2013V.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "A corpus-based approach to speech enhancement from nonstationary noise", "author": ["Ji Ming", "Ramji Srinivasan", "Danny Crookes"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 19, no. 4, pp. 822\u2013836, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech enhancement by online non-negative spectrogram decomposition in non-stationary noise environments", "author": ["Zhiyao Duan", "Gautham J Mysore", "Paris Smaragdis"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement using generative dictionary learning", "author": ["Christian D Sigg", "Tomas Dikk", "Joachim M Buhmann"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 6, pp. 1698\u2013 1712, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "A new metric for vq-based speech enhancement and separation", "author": ["Mads Gr\u00e6sb\u00f8ll Christensen", "Pejman Mowlaee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4764\u20134767.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutive speech bases and their application to supervised speech separation", "author": ["Paris Smaragdis"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 15, no. 1, pp. 1\u201312, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "A nonnegative approach to semi-supervised separation of speech from noise with the use of temporal dynamics", "author": ["Gautham J Mysore", "Paris Smaragdis"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 17\u201320.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting speech structure for noise estimation in single channel speech enhancement", "author": ["Eunjoon Cho"], "venue": "Ph.D. thesis, Stanford University,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "An evaluation of noise power spectral density estimation algorithms in adverse acoustic environments", "author": ["Jalal Taghia", "N Mohammadiha", "Jinqiu Sang", "V Bouse", "R Martin"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4640\u20134643.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon Technical Report N, vol. 93, pp. 27403, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "A study of complexity and quality of speech waveform coders", "author": ["JM Tribolet", "P Noll", "B McDermott", "R Crochiere"], "venue": "Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP\u201978. IEEE, 1978, vol. 3, pp. 586\u2013590.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1978}, {"title": "Speech enhancement: theory and practice", "author": ["Philipos C Loizou"], "venue": "CRC press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Noise estimation methods [1, 2, 3] apply different criteria on when to update the noise, and are typically used to compute the spectral gain for speech enhancement algorithms [4, 5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 1, "context": "Noise estimation methods [1, 2, 3] apply different criteria on when to update the noise, and are typically used to compute the spectral gain for speech enhancement algorithms [4, 5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 2, "context": "Noise estimation methods [1, 2, 3] apply different criteria on when to update the noise, and are typically used to compute the spectral gain for speech enhancement algorithms [4, 5].", "startOffset": 25, "endOffset": 34}, {"referenceID": 3, "context": "Noise estimation methods [1, 2, 3] apply different criteria on when to update the noise, and are typically used to compute the spectral gain for speech enhancement algorithms [4, 5].", "startOffset": 175, "endOffset": 181}, {"referenceID": 4, "context": "Noise estimation methods [1, 2, 3] apply different criteria on when to update the noise, and are typically used to compute the spectral gain for speech enhancement algorithms [4, 5].", "startOffset": 175, "endOffset": 181}, {"referenceID": 5, "context": "noisy environments [6, 7, 8, 9, 10].", "startOffset": 19, "endOffset": 35}, {"referenceID": 6, "context": "noisy environments [6, 7, 8, 9, 10].", "startOffset": 19, "endOffset": 35}, {"referenceID": 7, "context": "noisy environments [6, 7, 8, 9, 10].", "startOffset": 19, "endOffset": 35}, {"referenceID": 8, "context": "noisy environments [6, 7, 8, 9, 10].", "startOffset": 19, "endOffset": 35}, {"referenceID": 9, "context": "noisy environments [6, 7, 8, 9, 10].", "startOffset": 19, "endOffset": 35}, {"referenceID": 5, "context": "Parametric models capture the sources in a compact representation by training the coefficients of a model [6, 8].", "startOffset": 106, "endOffset": 112}, {"referenceID": 7, "context": "Parametric models capture the sources in a compact representation by training the coefficients of a model [6, 8].", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "Non-parametric models learn representations of the sources directly from the spectrum, and are trained using vector quantization (VQ) [7, 11], non-negative matrix factorization (NMF) and its probabilistic counterparts [12, 9], and variants of singular value decomposition (SVD) [10].", "startOffset": 134, "endOffset": 141}, {"referenceID": 10, "context": "Non-parametric models learn representations of the sources directly from the spectrum, and are trained using vector quantization (VQ) [7, 11], non-negative matrix factorization (NMF) and its probabilistic counterparts [12, 9], and variants of singular value decomposition (SVD) [10].", "startOffset": 134, "endOffset": 141}, {"referenceID": 11, "context": "Non-parametric models learn representations of the sources directly from the spectrum, and are trained using vector quantization (VQ) [7, 11], non-negative matrix factorization (NMF) and its probabilistic counterparts [12, 9], and variants of singular value decomposition (SVD) [10].", "startOffset": 218, "endOffset": 225}, {"referenceID": 8, "context": "Non-parametric models learn representations of the sources directly from the spectrum, and are trained using vector quantization (VQ) [7, 11], non-negative matrix factorization (NMF) and its probabilistic counterparts [12, 9], and variants of singular value decomposition (SVD) [10].", "startOffset": 218, "endOffset": 225}, {"referenceID": 9, "context": "Non-parametric models learn representations of the sources directly from the spectrum, and are trained using vector quantization (VQ) [7, 11], non-negative matrix factorization (NMF) and its probabilistic counterparts [12, 9], and variants of singular value decomposition (SVD) [10].", "startOffset": 278, "endOffset": 282}, {"referenceID": 5, "context": "However, these methods often rely on a separate noise model [6, 10, 9, 11], which limits the algorithms to work in environments that have been previously trained on.", "startOffset": 60, "endOffset": 74}, {"referenceID": 9, "context": "However, these methods often rely on a separate noise model [6, 10, 9, 11], which limits the algorithms to work in environments that have been previously trained on.", "startOffset": 60, "endOffset": 74}, {"referenceID": 8, "context": "However, these methods often rely on a separate noise model [6, 10, 9, 11], which limits the algorithms to work in environments that have been previously trained on.", "startOffset": 60, "endOffset": 74}, {"referenceID": 10, "context": "However, these methods often rely on a separate noise model [6, 10, 9, 11], which limits the algorithms to work in environments that have been previously trained on.", "startOffset": 60, "endOffset": 74}, {"referenceID": 11, "context": "For offline enhancement, semi-supervised NMF [12, 13] provides a solution to enhace without a separate noise model.", "startOffset": 45, "endOffset": 53}, {"referenceID": 12, "context": "For offline enhancement, semi-supervised NMF [12, 13] provides a solution to enhace without a separate noise model.", "startOffset": 45, "endOffset": 53}, {"referenceID": 13, "context": "The Mel or Bark scale can also be used to center the filter bands such that the lower frequencies are emphasized [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "However, as shown in [7], a VQ representation of the speech spectrum is, by itself, insufficient to capture all the subtle nuances of the underlying speech.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "One distribution commonly used for speech enhancement [4] is to model Fourier Transform coefficients using a complex Gaussian distribution.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "The first algorithm is based on Ellis\u2019s method [7] where a VQ representation of the speech is used to quantize the noisy speech.", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "The second algorithm is the MMSE noise estimation algorithm by Gerkmann used with an a-priori SNR estimated Wiener filter [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 14, "context": "However, given that the MMSE algorithm has recently shown to be one of the more effective methods for single channel speech enhancement [15], we compare it here to highlight some of the more challenging situations a speech enhancement system can encounter.", "startOffset": 136, "endOffset": 140}, {"referenceID": 15, "context": "To train the speech dictionary, 10,000 patches were sampled from randomly selected sentences in the training section of the TIMIT speech database [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "The effect of these parameters are analyzed in [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "In the literature, a longer analysis window is often used [9, 10] such that the harmonics are better defined.", "startOffset": 58, "endOffset": 65}, {"referenceID": 9, "context": "In the literature, a longer analysis window is often used [9, 10] such that the harmonics are better defined.", "startOffset": 58, "endOffset": 65}, {"referenceID": 16, "context": "The frequency-weighted segmental SNR (fwSegSNR) [18] is another measure known to highly correlate with subjective mean opinion scores (MOS) [19].", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "The frequency-weighted segmental SNR (fwSegSNR) [18] is another measure known to highly correlate with subjective mean opinion scores (MOS) [19].", "startOffset": 140, "endOffset": 144}], "year": 2016, "abstractText": "Distortion of the underlying speech is a common problem for single-channel speech enhancement algorithms, and hinders such methods from being used more extensively. A dictionary based speech enhancement method that emphasizes preserving the underlying speech is proposed. Spectral patches of clean speech are sampled and clustered to train a dictionary. Given a noisy speech spectral patch, the best matching dictionary entry is selected and used to estimate the noise power at each time-frequency bin. The noise estimation step is formulated as an outlier detection problem, where the noise at each bin is assumed present only if it is an outlier to the corresponding bin of the best matching dictionary entry. This framework assigns higher priority in removing spectral elements that strongly deviate from a typical spoken unit stored in the trained dictionary. Even without the aid of a separate noise model, this method can achieve significant noise reduction for various non-stationary noises, while effectively preserving the underlying speech in more challenging noisy environments.", "creator": "LaTeX with hyperref package"}}}