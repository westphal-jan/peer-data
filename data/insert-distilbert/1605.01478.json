{"id": "1605.01478", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Modeling Rich Contexts for Sentiment Classification with LSTM", "abstract": "analytic sentiment taxonomy analysis on social media data such include as tweets and weibo ads has become a very important continuous and challenging task. due to the intrinsic properties of such data, tweets are short, noisy, and of divergent topics, and sentiment classification concentrating on these data requires to modeling various contexts such as the retweet / reply history of a tweet, and the social context about authors and relationships. while few cited prior study has approached the issue of language modeling contexts in tweet, this paper proposes to repeatedly use a hierarchical inverse lstm to model rich contexts in tweet, particularly long - range range context. experimental meta results show that contexts can help us to perform sentiment classification programs remarkably better.", "histories": [["v1", "Thu, 5 May 2016 03:06:47 GMT  (620kb,D)", "http://arxiv.org/abs/1605.01478v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["minlie huang", "yujie cao", "chao dong"], "accepted": false, "id": "1605.01478"}, "pdf": {"name": "1605.01478.pdf", "metadata": {"source": "CRF", "title": "Modeling Rich Contexts for Sentiment Classification with LSTM", "authors": ["Minlie Huang", "Yujie Cao", "Chao Dong"], "emails": ["aihuang@tsinghua.edu.cn,", "caoyujieboy@163.com,", "neutronest@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Social media have been a major source for people to express their opinions online. Sentiment analysis on social media like Twitter and Weibo has drawn more and more attentions recently. However, due to the intrinsic properties of social media data, tweet is short, noisy, and of divergent topics. Sentiment analysis on such data requires to model contexts for a current tweet, for instance, to take into account social context (such as the relations between follower and followee), discourse relations (such as the connectives and conditionals between sentences), topic-based and conversationbased context, and many more. It is even more challenging to consider long-range contexts such as the entire retweet/reply history for a tweet, which has been largely ignored by prior studies.\nThe long-range context indeed influences sentiment classification of a tweet. By analysing a dataset consisting of more than 14,000 tweets in\nabout 1,600 threads where each thread consists of a sequence of retweeting/replying tweets discussing the same event or topic, we find out that 46.9% of tweets have the same sentiment polarities with their root tweets in the re-tweeting process and 64.7% with their parent tweets (threeclass polarity: positive, neutral, and negative). That means, the polarity of an origin tweet has drifted to a large degree during the retweeting/replying process while neighboring tweets have higher possibility to maintain the same polarity. See the example in Figure 1 which shows an origin tweet and a reply tweet. The sentiment polarity of the reply tweet is easy to be misclassified if we only predict it based on the its own content, since it contains several positive words such as \u201c\u798f \u5229 (welfare)\u201d and \u201c\u4eab\u53d7 (enjoyment)\u201d. If we trace back to its \u201cancestor\u201d tweet, we can judge the polarity correctly.\nAs a matter of fact, many works have utilized\nar X\niv :1\n60 5.\n01 47\n8v 1\n[ cs\n.C L\n] 5\nM ay\n2 01\n6\nsome kinds of contextual information of tweets. Deng et al. (2013) and Hu et al. (2013b) studied to use social relationships of authors of tweets, and they pointed out that authors with closer social relationships tend to show more similar sentiment polarities towards a target topic. Mukherjee and Bhattacharyya (2012) took advantage of discourse relationships between sentences as feature, such as connectives, conditionals and semantic operators etc. Vanzo et al. (2014) model a sequence of tweets in a conversation or containing same topics with SVMHMM . Recently, Ren et al. (2016) proposed a context-sensitive neural network in Twitter sentiment analysis. They use three kinds of contextual information, such as tweets in a conversation, those with same author and sharing same hashtags. Ghosh et al. (2016) proposed contextual LSTM by incorporating contextual features like topics of preceeding and current sentences into the model. However, all these studies haven\u2019t fully utilized the long-range contextual information.\nIn this work we propose a hierarchical LSTM model with two levels of LSTM networks to model the retweeting/replying process and capture the long-range dependency. The first LSTM is a word-level LSTM, which can generate a representation of a single tweet. And the second is a tweet-level LSTM which can model the longrange context of the current tweet. Moreover, we incorporate some additional context features into our model, which will be shown in detail later.\nTo evaluate how the proposed model performs on sentiment classification, particularly with longrange contexts, we collect a new dataset from Weibo.com. For each topic, we collect tens of threads where each thread consists of a sequence of retweeting/replying tweets to the origin tweet. Different from previous studies that usually define contextual tweets as those containing the same hashtags, we believe that retweet/reply is more focused on topic and thus context can play a more important role in sentiment classification.\nTo sum up, the main contribution of this paper is as follows:\n\u2022 We collect a new dataset from Weibo.com, where there are about 50 topics, 1,600 threads, and 14,000 tweets. Each tread consists of a sequence of retweeting/replying tweets.\n\u2022 We propose a hierarchical LSTM model containing two level LSTM networks. This\nmodel can capture long-range dependencies between a tweet and its contextual tweets.\n\u2022 We adopt additional contexts including social context and text-based context to help the model capture more features, and these contexts are derived from the tweets automatically.\nThe rest of the paper is organized as follows. In Section 2, we will overview some related works. In Section 3, we will describe our proposed methods in detail, including our hierarchical LSTM model, some context features and the way how we add them. And we present the experiments in Section 4. We conclude this work in Section 5."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Twitter Sentiment Analysis", "text": "Twitter sentiment analysis has been a very popular topic in the field of NLP these years. Many existing works were based on bag-of-words representation. Following Pang et al. (2002), many works attempted to design more effective features such as emotional signals, lexicons, n-grams and so on (Hu et al., 2013a; Basile and Novielli, 2015; Taboada et al., 2011; Feldman, 2013). Mohammad et al. (2013) built their system in SemEval2013 with a number of features like POS tags, hashtags, characters in upper case, punctuations and so on.\nRecently, many methods leveraged contextual information. Based on the characteristics of social network, there are social relationships that can be utilized to enhance the performance of predicting the polarities of tweets (Tan et al., 2011; Deng et al., 2013; Hu et al., 2013b). Speriosu et al. (2011) proposed a label propagation algorithm by combining textual features as well as social context. Another important kind of context for sentiment analysis is the discourse relations between sentences (Asher et al., 2008). Somasundaran et al. (2008) proposed \u201copinion frame\u201d as a representation to capture the discourse features. Based on this frame, Somasundaran et al. (2009) modelled the discourse relations as graph, and conducted sentiment classification on the graph by means of collective classification. Yang and Cardie (2014) modelled the discourse relations as constraints with the posterior regularization learning framework. For Twitter sentiment analysis, Mukherjee and Bhattacharyya (2012)\nutilized the discourse relations like conjunctions and conditionals between sentences as context. There are also other kinds of contextual information like topic-based and conversation-based contexts. Vanzo et al. (2014) proposed a contextbased SVMHMM model which models a sequence of tweets in a conversation or in a same topic.\nA more recent work is the context-sensitive neural network (Ren et al., 2016) where embeddings of keywords extracted from contexts are pooled to the output layer together with the CNN output layer. An extension of LSTM named contextual LSTM was presented by Ghosh et al. (2016), incorporating contextual features like topics of the preceeding, current sentences or paragraphs.\nIn comparison, our model can not only make use of some ordinary context features, but also model the long-range context features of tweets during the retweet/reply process by taking advantage of the hierarchical LSTM."}, {"heading": "2.2 Deep Learning Approaches for Sentiment Analysis", "text": "Deep learning approaches have also benefited a variety of sentiment analysis tasks. One class of the deep learning approaches is convolutional neural network. Many works have built CNN architectures to model semantic and sentiment information of sentences (Kalchbrenner et al., 2014; Kim, 2014). In Semeval-2015, Severyn et al. (2015) obtained top performance on predicting polarities at the message and phrase level by a deep CNN.\nAnother class of the deep learning approaches for sentiment analysis is recursive neural network introduced by Socher et al. (2011; 2013) which composes a phrase recursively from its child phrases. Dong et al. (2014) proposed an adaptive RNN which combines a set of composition functions whose weights are learned adaptively. A deep RNN which stacks multiple recursive layers performed well in fine-grained sentiment analysis (Irsoy and Cardie, 2014a).\nSome sequence models such as recurrent neural network, are suitable for sentiment classification tasks, since a sentence is composed of a sequence of words and similarly a document is composed of a sequence of sentences. Some recurrent neural networks were used to make prediction for words (Zhang and Lapata, 2014) or sequence labelling (Irsoy and Cardie, 2014b). Tai et al. (2015)\nproposed a tree-structured LSTM beating other sequential models in sentiment classification. Wang et al. (2015) cast a tweet as a sequence of words using LSTM to generate a sentence representation for predicting the polarities of tweets. Further Tang et al. (2015) utilized LSTM and gated RNN on the document level sentiment classification task."}, {"heading": "3 Methodology", "text": ""}, {"heading": "3.1 Long Short-Term Memory (LSTM)", "text": "Some sequence models such as recurrent neural network are very suitable for sentiment analysis, since they can model the long-range dependency and thus utilize the contextual information. However, recurrent neural network has a vital problem of vanishing and exploding gradients (Bengio et al., 1994). Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) can tackle this issue to model long-range dependency. Like other recurrent neural networks, LSTM also has a recurrent layer consisting of memory blocks. In each memory block, there is a memory cell unit which can store memory state information and several gates which can control the change of the memory state. More formally, we have\nft = \u03c3(Wfzzt\u22121 +Wfxxt + bf ) (1)\nit = \u03c3(Wizzt\u22121 +Wixxt + bi) (2)\nC\u0303t = \u03c6(Wczzt\u22121 +Wcxxt + bc) (3)\nCt = it C\u0303t + ft Ct\u22121 (4) ot = \u03c3(Wozzt\u22121 +Woxxt + bo) (5)\nzt = ot \u03c6(Ct) (6)\nHere, it, ft and ot denote the input gate, forget gate and output gate, respectively. xt is an input vector and zt is the hidden representation. W and b are the weight matrix and the bias term respectively. \u03c3 is sigmoid and \u03c6 is tanh. is the element-wise multiplication.\nAs can be seen, the input, output and the cell state is controlled by the gates, and in this way the LSTM can decide to remember or forget the information in the recurrent layer. This gives this model the capacity of learning the long-term dependencies, which is very helpful for our task. For a single tweet, LSTM may focus more on the important words and generate a more meaningful representation for the tweet. Similarly, for a tweets thread, LSTM may focus more on the important\ntweets in the propagation process. Based on this idea, we propose our model, a two-level Hierarchical LSTM."}, {"heading": "3.2 Hierarchical LSTM (HLSTM) for Twitter Sentiment Analysis", "text": "Since many tweets are short and informal in grammar, there is very limited information available in a single tweet. However, the tweets which have the relations of reply or retweet with the targeted tweet can be utilized as long-range context. This inspires us to design another layer of LSTM above the word-level LSTM.\nFrom Figure 2 we can see that in the word-level LSTM, the input is individual words. The hidden state of the last word is taken as the representation of the tweet. Each tweet in a thread will go through the word-level LSTM, and the t-th tweet in the thread will generate a tweet representation of zt which will be an input of the tweet-level LSTM.\nThe inputs for the tweet-level LSTM is the representations of tweets in a propagation thread. The first one is the origin tweet, which is the base of the whole thread, while the others are the retweets or replies of the origin tweet. And the last one (here we suppose it\u2019s the m-th) is the targeted tweet that we are determined to predict the polarity. The output of the tweet-level LSTM at m is ym which represents the m-th tweet with all the preceeding tweets. In this way the tweet-level LSTM incorporates the long-range context. We use the function of softmax on ym to make sentiment classification. This model combines both the local features of a tweet and the long-range context to perform a context-aware analysis."}, {"heading": "3.3 HLSTM with Additional Contexts", "text": "In addition to the long-range context, tweets have many additional contexts, such as social context, conversation-based context and topic-based context. These contextual information is very easy to obtain and they can be utilized properly to enhance our model.\nThese additional contextual information is extracted from the parent tweet or the root tweet of a targeted tweet in the thread, which will be shown in detail in the next subsection. For each type of context, we take it as a binary-value feature and encode it into a 0 or 1. These context features can be incorporated into the tweet-level LSTM, as follows:\nft = \u03c3(Wfhht\u22121 +Wfzzt +Wfddt + bf ) (7)\nit = \u03c3(Wihht\u22121 +Wizzt +Widdt + bi) (8)\nC\u0303t = \u03c6(Wchht\u22121 +Wczzt +Wcddt + bc) (9)\nCt = it C\u0303t + ft Ct\u22121 (10) ot = \u03c3(Wohht\u22121 +Wozzt +Woddt + bo) (11)\nht = ot \u03c6(Ct) (12)\nwhere dt is the feature vector and zt is the input tweet representation from the first level LSTM. From the above equations we can see that the additional context feature vector is fed into every gate. Actually, they can be regarded as an extension of the input vector x, which enrichs the representation of a targeted tweet. The way we add context features can be shown in Figure 3."}, {"heading": "3.4 Context Features", "text": "We take three types of contexts into account, the social context, the conversation-based context and the topic-based contexts. These contexts will be formed as binary features. We use tweet j to denote the current tweet, and tweet i to denote its\nparent tweet or its root tweet. The first one is a social context. SameAuthor A same person usually holds consistent attitude towards a specific topic, especially in a tweet thread. If two tweets in the thread is posted by the same person, the two tweets tend to have same sentiment polarity. This context can be formulated as:\nSameAuthor(i, j) =\n{ 1, ai = aj\n0, else (13)\nai and aj are the authors of tweet i and tweet j respectively. We should notice that tweet i is the parent tweet or the root tweet, so that we get two features from the SameAuthor context for the current tweet. Similar for the following contexts.\nThe second one is a conversation-based context. Conversation In a tweet thread, some tweets will mention other users\u2019 names by the function of @username. This indicates that the author of this tweet wants to have a conversation with or to show his opinions towards who has been mentioned. If a user who has been mentioned posts a tweet as response, then the two tweets constitute a conversation. The relation can be encoded as below:\nConversation(i, j) = { 1, a(j) \u2208M(i) 0, else (14)\nHere, aj is the author of tweet j andM(i) is the set of users\u2019 names mentioned in tweet i.\nThe third type of contexts is topic-based contexts. We suppose that tweets which contain overlapping hashtags have similar topics; and tweets which contains overlapping emoticons have similar sentiment polarity towards their topics.\nSameHashtag In tweets, hashtags are used to highlight a topic. In a tweet thread, tweets on the\nsame topic tend to show similar sentiment polarities.\nSameHashtag(i, j) = { 1, H(i) \u2229H(j) 6= \u2205 0, else (15)\nH(i) and H(j) are the sets of hashtags of tweet i and tweet j.\nSameEmoji Emoticons are used to express emotions. In a tweet thread, tweets with overlapping emoticons might have similar sentiment polarities towards their topics.\nSameEmoji(i, j) = { 1, E(i) \u2229 E(j) 6= \u2205 0, else (16)\nE(i) andE(j) are the sets of emoticons of tweet i and tweet j."}, {"heading": "3.5 Training", "text": "The training objective of our model is the crossentropy over the training examples. Here we use P g(xi) to denote the golden-standard sentiment distribution of tweet i, and use P (xi) to denote the sentiment distribution that our model predicts. The loss function is shown below.\nloss = \u2212 1 N N\u2211 i=1 C\u2211 j=1 P gj (xi)log(Pj(xi)) (17)\nP (xi) = Softmax(hi) (18)\nWe useN to denote the total number of training examples and C the number of sentiment classes. Here hi is the hidden state of the Tweet level LSTM of the tweet i.\nWe use AdaDelta (Zeiler, 2012) to optimize the parameters."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "We collected tweets from a Chinese Microblog website\uff0c Weibo.com. In Weibo.com, the retweets and replies always maintain their preceding tweets during the propagation process, and therefore long-range contexts are visible to a current tweet. We collected about 15k tweets consisting of over 1.6k tweet threads of 51 topics. We should notice that a tweet thread forms a tree structure started with an origin tweet, and the other nodes in the tree are retweets or replies. The average number of tweets in all the threads is 8.93, and the average depth is 3.75.\nThe sentiment polarity of each tweet is labelled by two independent annotators into three classes, positive, neutral and negative. The annotation consistency of the dataset is 63.4%, and those tweets with inconsistent labels are then checked by a third judger. Finally, there are 36%, 39.6%, and 24.6% of positive, negative, and neutral tweets respectively. By analysing the dataset, we find that 46.9% of tweets have the same sentiment polarities with their root tweets and 64.7% with their parent twees , indicating that long-range context indeed influences the classification task. The statistics of the dataset is summarized in Table 1."}, {"heading": "4.2 Baseline Methods", "text": "We compare our approach with the following baselines.\n\u2022 SVMmulticlass: a classical method for multiclass classification, which doesn\u2019t take any contextual information into account. In SVMmulticlass, we use emoticons and bagof-words as features, and train the classifier with SVMlight.\n\u2022 SVMHMM (Vanzo et al., 2014): a SVM variant that combines HMM. This method follows the Markov first order hypothesis. In SVMHMM , we use the same features as those in SVMmulticlass, and we also use several other features between the two adjacent tweets in the Markov chain, such as whether they have a same author, whether they constitute a conversation, whether they are in a same thread, whether they contain overlapping emoticons and hashtags, and the similarity of their bag-of-words vectors.\n\u2022 Convolutional Neural Network (CNN): a typical neural network for sentiment analysis, which demonstrates competitive performance on sentiment classification (Kalchbrenner et al., 2014).\n\u2022 Long Short-Term Memory (LSTM): the LSTM approach has been proposed as a stateof-the-art method in the sentiment classification task (Wang et al., 2015). It has the ability of capturing complex linguistic phenomena.\n\u2022 LSTM-RNN: We replace the second layer of our proposed HLSTM with RNN to assess the influence of the vanishing gradient problem in our task."}, {"heading": "4.3 Experiment Settings", "text": "Implementation We implement our model by Theano. AdaDelta (Zeiler, 2012) is adopted to optimize the parameters. We add a dropout layer before the softmax layer to prevent overfitting. The dropout rate is 0.5. We use mini-batch to speed up the convergence. And the batch size is 5, which means we update our parameters after every 5 tweet threads. Parameter Settings For CNN, we set the size of convolution filters as 2, 3 and 4 while the number of feature maps is set to 100. The hidden states of the LSTM baseline, the LSTM layer of LSTMRNN, the first LSTM layer of HLSTM all have a dimension of 128, while the hidden states of the second LSTM layer of HLSTM have 64 dimensions. Evaluation Methods We split our dataset randomly into the training, validation, and test set with a partition of 3:1:1. The validation set is used to tune the hyper-parameters. The classification results are measured by Macro \u2212 F1 and Accuracy. Word Embeddings The word embeddings are pre-trained on a large Chinese tweet corpus with 100k tweets, and the embeddings will be finetuned during the training process. We set the dimension of the word vector to 128."}, {"heading": "4.4 Results and Analysis", "text": "The experimental results are shown in Table 2. We can find that the conventional SVM methods perform relatively poorly. The reason may be due to the fact that SVM features suffer from sparseness and are less effective than word embeddings. SVMmulticlass performs worse than SVMHMM since SVMmulticlass doesn\u2019t take into account any contextual information. And this justifies the effectiveness of utilizing the contextual information.\nCompared with LSTM, our hierarchical LSTM (HLSTM) model achieves much better perfor-\nmance. This result confirms our assumption that utilizing the long-range contextual information is helpful for sentiment classification on such short and noisy texts.\nIn the LSTM-RNN model, we replace the second LSTM layer in HLSTM with a Recurrent Neural Network layer. The result shows that the LSTM-RNN model performs slightly worse than our HLSTM model. The reason is that RNN suffers from the vanishing gradient problem.\nIn HLSTM-f, we adopt additional contexts to help our model to encode more contextual information. We use the context features described in Section 3.4. We can see that these additional contexts contribute superior performance, indicating that modeling rich contexts in our model is helpful. However we also notice that HLSTM-f model doesn\u2019t have a large promotion over HLSTM in terms of Accuracy, and we conjecture that this is because the context features are sparse and most of tweet pairs do not observe such features."}, {"heading": "4.5 Case Study", "text": "In this section, we will show some cases to demonstrate the advantages of HLSTM over LSTM. One important case is that the polarities of some sentiment words are changed in some particular context, as exemplified in Table 3. Actually this example is a conversation between two authors. We can find out that in this tweet thread, the HLSTM model predicts correctly all the tweets, while the LSTM model is correct on the root tweet only. By observing the second tweet from author B, we may find that it contains some positive sentiment words such as \u201c\u798f\u5229 (welfare)\u201d and \u201c\u4eab\u53d7 (enjoyment)\u201d. Then LSTM predicts these words as positive without considering the long-range context. However we can see clearly that the second tweet is negative because it is an obvious sarcasm of the haze in\nBeijing. In the third tweet, there is an ambiguous sentiment word in Chinese, \u201c\u5389\u5bb3(severe, powverful)\u201d. Without any context LSTM may not be able to decide the correct polarity.\nAnother common case is that some tweets express agreement or disagreement with the \u201cancestor\u201d tweets. Without context, the LSTM model cannot decide the exact sentiment polarity of the agreeing or disagreeing statement such as \u201c\u6211\u5b8c\u5168 \u540c\u610f! (I totally agree!)\u201d. However with the longrange context, we can trace back to its \u201cancestor\u201d tweets and judge the polarity more reasonably."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a hierarchical LSTM model with two levels of LSTM networks to model long-range dependency. Moreover, we adopt rich additional contexts including social context and text-based context to help our model to encode more contextual information. The experimental results has shown that our method is superior to the baselines. Case study further shows that our proposed model can capture the polarity drift with contexts."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Sentiment analysis on social media data<lb>such as tweets and weibo has become<lb>a very important and challenging task.<lb>Due to the intrinsic properties of such data, tweets are short, noisy, and of diver-<lb>gent topics, and sentiment classification<lb>on these data requires to modeling various<lb>contexts such as the retweet/reply history<lb>of a tweet, and the social context about authors and relationships. While few prior<lb>study has approached the issue of model-<lb>ing contexts in tweet, this paper proposes<lb>to use a hierarchical LSTM to model rich<lb>contexts in tweet, particularly long-range context. Experimental results show that<lb>contexts can help us to perform sentiment<lb>classification remarkably better.", "creator": "TeX"}}}