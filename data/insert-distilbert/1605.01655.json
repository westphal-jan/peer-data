{"id": "1605.01655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Stance and Sentiment in Tweets", "abstract": "we can often detect from a person's utterances whether he / she is in favor of or against a given target entity - - their outward stance towards the target. however, a person may express the same stance behaviors towards a target by using negative or positive language. here for the first time we present a dataset of tweet - - target pairs freely annotated for explaining both stance and sentiment. the targets may or may not be referred to in the initial tweets, and they may or may not be the target of opinion in the tweets. partitions of this dataset were used as training and test sets in a semeval - 2016 shared task competition. we propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. additionally, access to both stance and sentiment annotations allows us to routinely explore several research questions. we show this that while knowing the baseline sentiment expressed by a candidate tweet is beneficial for stance classification, it alone is not sufficient. finally, we use additional unlabeled data through distant supervision techniques and word embeddings to actually further improve stance classification.", "histories": [["v1", "Thu, 5 May 2016 17:07:54 GMT  (444kb,D)", "http://arxiv.org/abs/1605.01655v1", "22 pages"]], "COMMENTS": "22 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["saif m mohammad", "parinaz sobhani", "svetlana kiritchenko"], "accepted": false, "id": "1605.01655"}, "pdf": {"name": "1605.01655.pdf", "metadata": {"source": "CRF", "title": "Stance and Sentiment in Tweets", "authors": ["Saif M. Mohammad", "Parinaz Sobhani", "Svetlana Kiritchenko"], "emails": [], "sections": [{"heading": null, "text": "0 Stance and Sentiment in Tweets\nSaif M. Mohammad, National Research Council Canada Parinaz Sobhani, University of Ottawa Svetlana Kiritchenko, National Research Council Canada\nWe can often detect from a person\u2019s utterances whether he/she is in favor of or against a given target entity\u2014 their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet\u2013target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification.\nGeneral Terms: Natural Language Processing, Computational Linguistics\nAdditional Key Words and Phrases: Stance, tweets, sentiment, opinion, polarity, text classification\nACM Reference Format: Saif M. Mohammad, Parinaz Sobhani, Svetlana Kiritchenko, 2016. Stance and Sentiment in Tweets ACM Trans. Embedd. Comput. Syst. 0, 0, Article 0 ( 2016), 22 pages. DOI: 0000001.0000001"}, {"heading": "1. INTRODUCTION", "text": "Stance detection is the task of automatically determining from text whether the author of the text is in favor of, against, or neutral towards a proposition or target. The target may be a person, an organization, a government policy, a movement, a product, etc. For example, one can infer from Barack Obama\u2019s speeches that he is in favor of stricter gun laws in the US. Similarly, people often express stance towards various target entities through posts on online forums, blogs, Twitter, Youtube, Instagram, etc.\nAutomatically detecting stance has widespread applications in information retrieval, text summarization, and textual entailment. Over the last decade, there has been active research in modeling stance. However, most work focuses on congressional debates [Thomas et al. 2006] or debates in online forums [Somasundaran and Wiebe 2010; Anand et al. 2011; Walker et al. 2012a; Hasan and Ng 2013]. Here we explore the task of detecting stance in Twitter\u2014a popular microblogging platform where people often express stance implicitly or explicitly.\nThe task we explore is formulated as follows: given a tweet text and a target entity (person, organization, issue, etc.), automatic natural language systems must deter-\nAuthor\u2019s addresses: Saif M. Mohammad and Svetlana Kiritchenko, National Research Council Canada; Parinaz Sobhani, University of Ottawa. This article was authored by employees of the Government of Canada. As such, the Canadian government retains all interest in the copyright to this work and grants to ACM a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, provided that clear attribution is given both to the authors and the Canadian government agency employing them. Permission to make digital or hard copies for personal or classroom use is granted. Copies must bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the Canadain Government must be honored. To copy otherwise, distribute, republish, or post, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. c\u00a9 2016 Crown in Right of Canada. 1539-9087/2016/-ART0 $15.00 DOI: 0000001.0000001\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nar X\niv :1\n60 5.\n01 65\n5v 1\n[ cs\n.C L\n] 5\nM ay\n2 01\n6\nmine whether the tweeter is in favor of the given target, against the given target, or whether neither inference is likely. For example, consider the target\u2013tweet pair:\nTarget: legalization of abortion (1) Tweet: The pregnant are more than walking incubators. They have rights too!\nHumans can deduce from the tweet that the tweeter is likely in favor of the target.1 Note that lack of evidence for \u2018favor\u2019 or \u2018against\u2019, does not imply that the tweeter is neutral towards the target. It may just mean that we cannot deduce stance from the tweet. In fact, this is a common phenomenon. On the other hand, the number of tweets from which we can infer neutral stance is expected to be small. Example:\nTarget: Hillary Clinton (2) Tweet: Hillary Clinton has some strengths and some weaknesses.\nStance detection is related to, but different from, sentiment analysis. Sentiment analysis tasks are formulated as determining whether a piece of text is positive, negative, or neutral, or determining from text the speaker\u2019s opinion and the target of the opinion (the entity towards which opinion is expressed). However, in stance detection, systems are to determine favorability towards a given (pre-chosen) target of interest. The target of interest may not be explicitly mentioned in the text and it may not be the target of opinion in the text. For example, consider the target\u2013tweet pair below:\nTarget: Donald Trump (3) Tweet: Jeb Bush is the only sane candidate in this republican lineup.\nThe target of opinion in the tweet is Jeb Bush, but the given target of interest is Donald Trump. Nonetheless, we can infer that the tweeter is likely to be unfavorable towards Donald Trump. Also note that, in stance detection, the target can be expressed in different ways which impacts whether the instance is labeled \u2018favour\u2019 or \u2018against\u2019. For example, the target in example 1 could have been phrased as \u2018pro-life movement\u2019, in which case the correct label for that instance is \u2018against\u2019. Also, the same stance (\u2018favour\u2019 or \u2018against\u2019) towards a given target can be deduced from positive tweets and negative tweets. This interaction between sentiment and stance has not been adequately addressed in past work, and an important reason for this is the lack of a dataset annotated for both stance and sentiment. Our contributions are as follows:\n(1) Created a New Stance Dataset: We created the first dataset of tweets labeled for both stance and sentiment (Section 2 and Section 3). More than 4000 tweets are annotated for whether one can deduce favorable or unfavorable stance towards one of five targets \u2018Atheism\u2019, \u2018Climate Change is a Real Concern\u2019, \u2018Feminist Movement\u2019, \u2018Hillary Clinton\u2019, and \u2018Legalization of Abortion\u2019. Each of these tweets is also annotated for whether the target of opinion expressed in the tweet is the same as the given target of interest. Finally, each tweet is annotated for whether it conveys positive, negative, or neutral sentiment.\n(2) Developed an Interactive Visualizer for the Stance Dataset: We created an online visualizer for our data (Section 4) that allows users to explore the data graphically and interactively. Clicking on individual components of the visualization, such as a target, stance class, or sentiment class, filters the visualization to show information pertaining to the selection. Thus, the visualization can be used to quickly convey key features of the data, for example, the percentage of the instances that are labeled\n1Note that we use \u2018tweet\u2019 to refer to the text of the tweet and not to its meta-information. In our annotation task, we asked respondents to label for stance towards a given target based on the tweet text alone. However, automatic systems may benefit from exploiting tweet meta-information.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nas against \u2018Atheism\u2019 and yet use positive language, and also to delve deeper into particular areas of interest of the user.\n(3) Organized a Shared Task Competition on Stance: Partitions of this stance-annotated data were used as training and test sets in the SemEval-2016 shared task competition, Task #6: Detecting Stance from Tweets [Mohammad et al. 2016]. Participants were provided with 2,914 training instances labeled for stance for the five targets. The test data included 1,249 instances. All of the stance data is made freely available through the shared task website. The task received submissions from 19 teams. The best performing system obtained an overall average F-score of 67.8. Their approach employed two recurrent neural network (RNN) classifiers: the first was trained to predict task-relevant hashtags on a large unlabeled Twitter corpus. This network was used to initialize a second RNN classifier, which was trained with the provided training data [Zarrella and Marsh 2016].\n(4) Developed a State-of-the-Art Stance Detection System: We propose a stance detection system that is much simpler than the shared task winning system (described above), and yet obtains an even better F-score of 70.3 on the shared task\u2019s test set (Sections 5, 6 and 7). We use a linear-kernel SVM classifier that relies on features drawn from the training instances\u2014such as word and character n-grams\u2014as well as those obtained using external resources\u2014such as sentiment features from lexicons and word-embedding features from additional unlabeled data.\n(5) Explored Research Questions: We conduct several experiments to better understand stance detection, and its interaction with sentiment (Section 6). \u2014 We use the gold labels to determine the extent to which stance can be determined\nsimply from sentiment. \u2014 We apply the stance detection system (mentioned above in (4)), as a common\ntext classification framework, to determine both stance and sentiment. We show that while sentiment features are substantially useful for sentiment classification, they are not as effective for stance classification. Further, even though both stance and sentiment detection are framed as three-way classification tasks on a common dataset, automatic systems perform markedly better when detecting sentiment than when detecting stance towards a given target. \u2014 We show that stance detection is particularly challenging when the tweeter expresses opinion about an entity other than the target of interest. (The text classification system performs close to majority baseline for such instances.)\nAll of the data, an interactive visualization of the data, and the evaluation scripts are available on the task website as well as the homepage for this Stance project.2"}, {"heading": "2. A DATASET FOR STANCE FROM TWEETS", "text": "We now present how we compiled a set of tweets and targets for stance annotation (Section 2.1), and the questionnaire and crowdsourcing setup used for stance annotation (Section 2.2). An analysis of the stance annotations is presented in Section 4."}, {"heading": "2.1. Selecting the Tweet\u2013Target Pairs", "text": "Our goal was to create a stance-labeled dataset with the following properties: 1: The tweet and target are commonly understood by a wide number of people in the US. (The\ndata was eventually annotated for stance by respondents living in the US.) 2: There must be a significant amount of data for the three classes: \u2018favor\u2019, \u2018against\u2019, and \u2018nei-\nther\u2019.\n2http://alt.qcri.org/semeval2016/task6/ http://www.saifmohammad.com/WebPages/StanceDataset.htm\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\n3: Apart from tweets that explicitly mention the target, the dataset should include a significant number of tweets that express opinion towards the target without referring to it by name. We wanted to include the relatively harder cases for stance detection where the target is referred to in indirect ways such as through pronouns, epithets, honorifics, and relationships.\n4: Apart from tweets that express opinion towards the target, the dataset should include a significant number of tweets in which the target of opinion is different from the given target of interest. Downstream applications often require stance towards particular pre-chosen targets of interest (for example, a company might be interested in stance towards its product). Having data where the target of opinion is some other entity (for example, a competitor\u2019s product), helps test how well stance detection systems can cope with such instances.\nThese properties influenced various choices in how our dataset was created. To help with Property 1, the authors of this paper compiled a list of target entities commonly known in the United States: \u2018Atheism\u2019, \u2018Climate Change is a Real Concern\u201d, \u2018Feminist Movement\u2019, \u2018Hillary Clinton\u2019, and \u2018Legalization of Abortion\u2019.\nWe created a small list of hashtags, which we will call query hashtags, that people use when tweeting about the targets. We split these hashtags into three categories: (1) favor hashtags: expected to occur in tweets expressing favorable stance towards the target (for example, #Hillary4President), (2) against hashtags: expected to occur in tweets expressing opposition to the target (for example, #HillNo), and (3) stanceambiguous hashtags: expected to occur in tweets about the target, but are not explicitly indicative of stance (for example, #Hillary2016).3 We will refer to favor and against hashtags jointly as stance-indicative (SI) hashtags. Table I lists some of the hashtags used for each of the targets. (We were not able to find a hashtag that is predominantly used to show favor towards \u2018Climate change is a real concern\u2019, however, the stanceambiguous hashtags were the source of a large number of tweets eventually labeled \u2018favor\u2019 through human annotation.) Next, we polled the Twitter API to collect over two million tweets containing these query hashtags. We kept only those tweets where the query hashtags appeared at the end. We removed the query hashtags from the tweets to exclude obvious cues for the classification task. Since we only select tweets that have the query hashtag at the end, removing them from the tweet often still results in text that is understandable and grammatical.\nNote that the presence of a stance-indicative hashtag is not a guarantee that the tweet will have the same stance. Further, removal of query hashtags may result in a tweet that no longer expresses the same stance as with the query hashtag. Thus we manually annotate the tweet\u2013target pairs after the pre-processing described above. For each target, we sampled an equal number of tweets pertaining to the favor hashtags, the against hashtags, and the stance-ambiguous hashtags\u2014up to 1000 tweets at most per target. This helps in obtaining a sufficient number of tweets pertaining to each of the stance categories (Property 2). Properties 3 and 4 are addressed to some extent by the fact that removing the query hashtag can sometimes result in tweets that do not explicitly mention the target. Consider:\nTarget: Hillary Clinton (4) Tweet: Benghazi must be answered for #Jeb16\n3A tweet that has a seemingly favorable hashtag towards a target may in fact oppose the target; and this is not uncommon. Similarly unfavorable (or against) hashtags may occur in tweets that favor the target.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nThe query hashtags \u2018#HillNo\u2019 was removed from the original tweet, leaving no mention of Hillary Clinton. Yet there is sufficient evidence (through references to Benghazi and #Jeb16) that the tweeter is likely against Hillary Clinton. Further, conceptual targets such as \u2018Legalization of Abortion\u2019 (much more so than person-name targets) have many instances where the target is not explicitly mentioned. For example, tweeters can express stance by referring to foetuses, women\u2019s rights, freedoms, etc., without having to mention legalization or abortion."}, {"heading": "2.2. Stance Annotation", "text": "The instructions given to annotators for determining stance are shown below. Descriptions within each option make clear that stance can be expressed in many different ways, for example by explicitly supporting or opposing the target, by supporting an entity aligned with or opposed to the target, etc. The second question asks whether the target of opinion in the tweet is the same as the given target of interest.\nTarget of Interest: [target entity] Tweet: [tweet with query hashtag removed]\nQ1: From reading the tweet, which of the options below is most likely to be true about the tweeter\u2019s stance or outlook towards the target:\n\u2022 We can infer from the tweet that the tweeter supports the target This could be because of any of reasons shown below:\n\u2013 the tweet is explicitly in support for the target \u2013 the tweet is in support of something/someone aligned with the target, from which we can infer that the\ntweeter supports the target \u2013 the tweet is against something/someone other than the target, from which we can infer that the tweeter\nsupports the target \u2013 the tweet is NOT in support of or against anything, but it has some information, from which we can\ninfer that the tweeter supports the target \u2013 we cannot infer the tweeters stance toward the target, but the tweet is echoing somebody else\u2019s favorable\nstance towards the target (in a news story, quote, retweet, etc.)\n\u2022 We can infer from the tweet that the tweeter is against the target This could be because of any of the following:\n\u2013 the tweet is explicitly against the target \u2013 the tweet is against someone/something aligned with the target entity, from which we can infer that the\ntweeter is against the target \u2013 the tweet is in support of someone/something other than the target, from which we can infer that the\ntweeter is against the target \u2013 the tweet is NOT in support of or against anything, but it has some information, from which we can\ninfer that the tweeter is against the target \u2013 we cannot infer the tweeters stance toward the target, but the tweet is echoing somebody else\u2019s negative\nstance towards the target entity (in a news story, quote, retweet, etc.)\n\u2022 We can infer from the tweet that the tweeter is neutral towards the target The tweet must provide some information that suggests that the tweeter is neutral towards the target \u2013 the tweet being neither favorable nor against the target is not sufficient reason for choosing this\n\u2022 There is no clue in the tweet to reveal the stance of the tweeter towards the target (support/against/neutral)\nQ2: From reading the tweet, which of the options below is most likely to be true about the focus of opinion/sentiment in the tweet:\n\u2022 The tweet explicitly expresses opinion about the target\n\u2022 The tweet expresses opinion about something/someone other than the target\n\u2022 The tweet is not expressing opinion about anything\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nEach of the tweet\u2013target pairs selected for annotation was uploaded on CrowdFlower for annotation with the questionnaire shown above.4 We used CrowdFlower\u2019s gold annotations scheme for quality control, wherein about 5% of the data was annotated internally (by the authors). These questions are referred to as gold questions. During crowd annotation, the gold questions are interspersed with other questions, and the annotator is not aware which is which. However, if she gets a gold question wrong, she is immediately notified of it. If the accuracy of the annotations on the gold questions falls below 70%, the annotator is refused further annotation. This serves as a mechanism to avoid malicious annotations. In addition, the gold questions serve as examples to guide the annotators.\nEach question was answered by at least eight respondents. The respondents gave the task high scores in a post-annotation survey despite noting that the task itself requires some non-trivial amount of thought: 3.9 out of 5 on ease of task, 4.4 out of 5 on clarity of instructions, and 4.2 out of 5 overall.\nFor each target, the data not annotated for stance is used as the domain corpus\u2014a set of unlabeled tweets that can be used to obtain information helpful to determine stance, such as relationships between relevant entities (we explore the use of the domain corpus in Section 7).\nThe number of instances that were marked as neutral stance (option 3 in question 1) was less than 1%. Thus, we merged options 3 and 4 into one \u2018neither in favor nor against\u2019 option (\u2018neither\u2019 for short). The inter-annotator agreement was 73.1% for question 1 (stance) and 66.2% for Question 2 (target of opinion).5 These statistics are for the complete annotated dataset, which includes instances that were genuinely difficult to annotate for stance (possibly because the tweets were too ungrammatical or vague) and/or instances that received poor annotations from the crowd workers (possibly because the particular annotator did not understand the tweet or its context). In order to aggregate stance annotation information from multiple annotators for an instance, rather than opting for simple majority, we marked an instance with a stance only if at least 60% of the annotators agreed with each other\u2014the instances with less than 60% agreement were set aside.6 We will refer to this dataset as the Stance Dataset. The inter-annotator agreement on this Stance Dataset is 81.85% for question 1 (stance) and 68.9% for Question 2 (target of opinion). The rest of the instances are kept aside for future investigation."}, {"heading": "3. LABELING THE STANCE SET FOR SENTIMENT", "text": "A key research question this work aims at addressing is the extent to which sentiment is correlated with stance. To that end, we annotated the same Stance Dataset described above for sentiment in a separate annotation effort a few months later. We followed a procedure for annotation on CrowdFlower similar to that described above for stance, but now provided only the tweet (no target).\nPrior work in sentiment annotation has often simply asked the annotator to label a sentence as positive, negative, or neutral, largely leaving the notion when pieces of text should be marked as positive, negative, or neutral up to the individual annotators. This is problematic because it can lead to differing annotations from annotators for the\n4http://www.crowdflower.com 5The overall inter-annotator agreement was calculated by averaging the agreements on all tweets in the dataset. For each tweet, the inter-annotator agreement was calculated as the number of annotators who agree over the majority label divided by the total number of annotators for that tweet. 6The 60% threshold is somewhat arbitrary, but it seemed appropriate in terms of balancing confidence in the majority annotation and having to discard too many instances. Annotations for 28% of the instances did not satisfy this criterion. Note that even though we request 8 annotations per questions, some questions may be annotated more than 8 times. Also, a small number of instances received less than 8 annotations.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nsame text. Further, in several scenarios the annotators may be unsure about how best to label the text. Some of these scenarios are listed below. (See [Mohammad 2016] for further discussion on the challenges of sentiment annotation.) \u2022 Sarcasm and ridicule: Sarcasm and ridicule are tricky from the perspective of as-\nsigning a single label of sentiment because they can often indicate positive emotional state of the speaker (pleasure from mocking someone or something) even though they have a negative attitude towards someone or something. An example of ridicule from our dataset:\nDEAR PROABORTS: Using BAD grammar and FILTHY language and INTIMIDATION makes you look ignorant, inept and desperate. #GodWins\n\u2022 Supplications and requests: Many tweets convey positive supplications to God or positive requests to people in the context of a (usually) negative situation. Example from our dataset:\nPray for the Navy yard. God please keep the casualties minimal. #1A #2A #NRA #COS #CCOT #TGDN #PJNET #WAKEUPAMERICA\n\u2022 Rhetorical questions: Rhetorical questions can be treated simply as queries (and thus neutral) or as utterances that give away the emotional state of the speaker. For example, consider this example from our dataset of tweets:\nHow soon do you think WWIII &WWWIV will begin? #EndRacism\nOn the one hand, this tweet can be treated as a neutral question, but on the other hand, it can be seen as negative because the utterance betrays a sense of frustration on the part of the speaker.\nAfter a few rounds of internal development, we used the questionnaire below to annotate for sentiment:\nWhat kind of language is the speaker using? 1. the speaker is using positive language, for example, expressions of support, admiration, positive attitude, forgiveness, fostering, success, positive emotional state (happiness, optimism, pride, etc.)\n2. the speaker is using negative language, for example, expressions of criticism, judgment, negative attitude, questioning validity/competence, failure, negative emotional state (anger, frustration, sadness, anxiety, etc.)\n3. the speaker is using expressions of sarcasm, ridicule, or mockery 4. the speaker is using positive language in part and negative language in part 5. the speaker is neither using positive language nor using negative language\nThe use of the phrases \u2018positive language\u2019 and \u2018negative language\u2019 encourages respondents to focus on the language itself as opposed to assigning sentiment based on event outcomes that are beneficial or harmful to the annotator. Sarcasm, ridicule, and mockery are included as a separate option (in addition to option 2) so that respondents do not have to wonder if they should mark such instances as positive or negative. Instances with different sentiment towards different targets of opinion can be marked with option 4. Supplications and requests that convey a sense of fostering and support can be marked as positive. On the other hand, rhetorical questions that betray a sense of frustration and disappointment can be marked as negative.\nEach instance was annotated by at least five annotators on CrowdFlower. The respondents gave the task high scores in a post-annotation survey: 4.2 out of 5 on ease of task, 4.4 out of 5 on clarity of instructions, and 4.2 out of 5 overall.\nFor our current work, we chose to combine options 2 and 3 into one \u2018negative tweets\u2019 class but they can be kept separate in future work if so desired. We also chose to combine options 4 and 5 into one \u2018neither clearly positive nor clearly negative category\u2019 (\u2018neither\u2019 for short). This frames the automatic sentiment prediction task as a\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nthree-way classification task, similar to the stance prediction task. The inter-annotator agreement on the sentiment responses across these three classes was 85.6%."}, {"heading": "4. PROPERTIES OF THE STANCE DATASET", "text": "We partitioned the Stance Dataset into training and test sets based on the timestamps of the tweets. All annotated tweets were ordered by their timestamps, and the first 70% of the tweets formed the training set and the last 30% formed the test set. Table II shows the number and distribution of instances in the Stance Dataset.\nTable III shows the distribution of responses to Question 2 (whether opinion is expressed directly about the given target). Observe that the percentage of \u2018opinion towards other\u2019 varies across different targets from 27% to 46%. Table IV shows the distribution of instances by target of opinion for the \u2018favor\u2019 and \u2018against\u2019 stance labels. Observe that, as in Example 3, in a number of tweets from which we can infer unfavorable stance towards a target, the target of opinion is someone/something other than the target (about 26.5%). Manual inspection of the data also revealed that in a number of instances, the target is not directly mentioned, and yet stance towards the target was determined by the annotators. About 28% of the \u2018Hillary Clinton\u2019 instances and 67% of the \u2018Legalization of Abortion\u2019 instances were found to be of this kind\u2014 they did not mention \u2018Hillary\u2019 or \u2018Clinton\u2019 and did not mention \u2018abortion\u2019, \u2018pro-life\u2019, and \u2018pro-choice\u2019, respectively (case insensitive; with or without hashtag; with or without hyphen). Examples (1) and (4) shown earlier are instances of this, and are taken from our dataset. Some other examples are shown below:\nTarget: Hillary Clinton (5) Tweet: I think I am going to vote for Monica Lewinsky\u2019s Ex-boyfriends Wife\nTarget: Legalization of Abortion (6) Tweet: The woman has a voice. Who speaks for the baby? I\u2019m just askin.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nTable V. Distribution of sentiment in the Stance Train and Test sets.\n% of instances in Train % of instances in Test Target positive negative neither positive negative neither Atheism 60.43 35.09 4.48 59.09 35.45 5.45 Climate Change is Concern 31.65 49.62 18.73 29.59 51.48 18.93 Feminist Movement 17.92 77.26 4.82 19.30 76.14 4.56 Hillary Clinton 32.08 64.01 3.92 25.76 70.17 4.07 Legalization of Abortion 28.79 66.16 5.05 20.36 72.14 7.5 Total 33.05 60.47 6.49 29.46 63.33 7.20\nFig. 1. Screenshot of an Interactive Visualization of the Stance Dataset.\nTable V shows the distribution of sentiment labels in the training and test sets. Observe that tweets corresponding to all targets, except for \u2018Atheism\u2019, are predominantly negative.\nTo allow ease of exploration of the Stance Dataset we created an interactive visualization using Tableau\u2014a software product that provides a graphical interface, menu options, and drag-and-drop mechanisms to upload databases and create sophisticated visualizations.7 Figure 1 shows a screenshot of the visualization of the Stance Dataset. It has several components (a. through g.) that we will describe below. On the top left, component a., is a bar graph showing the number of instances pertaining to each of the targets in the dataset. The visualization component b. (below a.), known as a treemap, shows tiles corresponding to each target\u2013stance combination. The size (area) of a tile\n7http://www.tableau.com\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nis proportional to the number of instances corresponding to that target\u2013stance combination. This component quickly shows that for most of the targets, the Stance Dataset has more data for \u2018against\u2019 than \u2018favor\u2019 and \u2018neither\u2019. The three stacked bars on the top right (c., d., and e.) show the proportion of instances pertaining to the classes of stance, opinion target, and polarity, respectively. Observe that they convey to the viewer that a majority of the instances are labeled as \u2018against\u2019 the targets of interest, expressing opinion towards the target of interest, and having negative polarity.\nThe \u2018f. X by Y Matrices\u2019 component of the visualization shows three matrices pertaining to: stance classes and opinion towards classes, stance classes and polarity classes, and opinion towards classes and polarity classes. The cells in each of these matrices show the percentage of instances with labels corresponding to that cell (the percentages across each of the rows sums up to 100%.) Examination of this matrix reveals that favorable stance is usually expressed by expressing opinion directly about the target (94.23%), but that percentage is markedly smaller for instances that are labeled \u2018against the target\u2019 (72.75%). The visualization component g. at the bottom shows all of the tweets, targets, and manual annotations.\nAll of the visualization components allow filtering of data by clicking on areas of interest. For example, clicking on the \u2018Hillary Clinton\u2019 bar updates all other visualization components to show information pertaining to only those instances that have \u2018Hillary Clinton\u2019 as target. Clicking on multiple items results in an \u2018AND\u2019ing of the selected filter criteria. For example, clicking on the target \u2018Atheism\u2019, stance \u2018against\u2019, and polarity \u2018positive\u2019 will show information pertaining to instances that have Atheism as target, \u2018against\u2019 the target stance, and positive polarity labels.\nThe \u2018Tweets\u2019 component at the bottom also filters out information so that one can see examples pertaining to their selection. Some of the items in individual visualizations may not have enough space to have visible labels (for example, the Hillary Clinton\u2013 Favor tile in the \u2018Stance by Target\u2019 treemap). However, hovering over any item with the mouse reveals the label in a hover box. We hope that the visualization will help users easily explore aspects of the data they are interested in."}, {"heading": "5. A COMMON TEXT CLASSIFICATION FRAMEWORK FOR STANCE AND SENTIMENT", "text": "Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact [Wilson et al. 2013; Mohammad et al. 2013; Kiritchenko et al. 2014a; Rosenthal et al. 2015]. These features may be useful in stance classification as well; however, it is unclear which features will be more useful (and to what extent). Since we now have a dataset annotated for both stance and sentiment, we create a common text classification system (common machine learning framework and common features) and apply it to the Stance Dataset for both stance and sentiment classification.\nThere is one exception to the common machine learning framework. The words and concepts used in tweets corresponding to the three stance categories are not expected to generalize across the targets. Thus, the stance system learns a separate model from training data pertaining to each of the targets.8 Positive and negative language tend to have sufficient amount of commonality regardless of topic of discussion, and hence sentiment analysis systems traditionally learn a single model from all of the training data [Liu 2015; Kiritchenko et al. 2014a; Rosenthal et al. 2015]. Thus, our sentiment experiments are also based on a single model trained on all of the Stance Training set.\nTweets are tokenized and part-of-speech tagged with the CMU Twitter NLP tool [Gimpel et al. 2011]. We train a linear-kernel Support Vector Machine (SVM) classifier\n8Experiments with a stance system that learns a single model from all training tweets showed lower results.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\non the Stance Training set. SVMs have proven to be effective on text categorization tasks and robust on large feature spaces. We use the SVM implementation provided by the scikit-learn Machine Learning library [Pedregosa et al. 2011]. The features used in our text classification system are shown below: \u2022 n-grams: presence or absence of contiguous sequences of 1, 2 and 3 tokens (word n-grams); presence or absence of contiguous sequences of 2, 3, 4, and 5 characters (character n-grams); \u2022 sentiment (sent.): The sentiment lexicon features are derived from three manually created lexicons: NRC Emotion Lexicon [Mohammad and Turney 2010], Hu and Liu Lexicon [Hu and Liu 2004], and MPQA Subjectivity Lexicon [Wilson et al. 2005], and two automatically created, tweet-specific, lexicons: NRC Hashtag Sentiment and NRC Emoticon (a.k.a. Sentiment140) [Kiritchenko et al. 2014a]; \u2022 target: presence/absence of the target of interest in the tweet;9 \u2022 POS: the number of occurrences of each part-of-speech tag (POS); \u2022 encodings (enc.): presence/absence of positive and negative emoticons, hashtags, char-\nacters in upper case, elongated words (e.g., sweeettt), and punctuations such as exclamation and question marks. The SVM parameters are tuned using 5-fold cross-validation on Stance Training set. We evaluate the learned models on the Stance Test set. As the evaluation measure, we use the average of the F1-scores (the harmonic mean of precision and recall) for the two main classes:10\nFor stance classification: Faverage =\nFfavor + Fagainst 2\n(1)\nFor sentiment classification: Faverage =\nFpositive + Fnegative 2\n(2)\nNote that Faverage can be determined for all of the test instances or for each target data separately. We will refer to the Faverage obtained through the former method as Fmicro-across-targets or F-microT (for short). On the other hand, the Faverage obtained through the latter method, that is, by averaging the Faverage calculated for each target separately, will be called F-macro-across-targets or F-macroT (for short). Systems that perform relatively better on the more frequent target classes will obtain higher FmicroT scores. On the other hand, to obtain a high F-macroT score a system has to perform well on all target classes.\nNote that this measure does not give any credit for correctly classifying \u2018neither\u2019 instances. Nevertheless, the system has to correctly predict all three classes to avoid being penalized for misclassifying \u2018neither\u2019 instances as \u2018favor\u2019 or \u2018against\u2019."}, {"heading": "6. RESULTS OBTAINED BY AUTOMATIC SYSTEMS", "text": "We now present results obtained by the classifiers described above on detecting stance and sentiment on the Stance Test set. In this section, we focus on systems that use only the provided training data and existing resources such as sentiment lexicons. In Section 7, we conduct experiments with systems that use additional unlabeled (or pseudo-labeled) tweets as well."}, {"heading": "6.1. Results for Stance Classification", "text": "We conducted 5-fold cross-validation on the stance training set to determine usefulness of each of the features discussed above. Experiments on the test set showed the\n9For instance, for \u2018Hillary Clinton\u2019 the mention of either \u2018Hillary\u2019 or \u2018Clinton\u2019 (case insensitive; with or without hashtag) in the tweet shows the presence of target. 10A similar metric was used in the past for sentiment analysis\u2014SemEval 2013 Task 2 [Wilson et al. 2013].\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nsame patterns. Due to space constraints, we show results only on the test set \u2014 Table VI. Rows I.a. to I.e. present benchmarks. Row I.a. shows results obtained by a random classifier (a classifier that randomly assigns a stance class to each instance), and Row I.b. shows results obtained by the majority classifier (a classifier that simply labels every instance with the majority class).11 Observe that the F-microT for the majority classifier is rather high. This is mostly due to the differences in the class distributions for the five targets: for most of the targets the majority of the instances are labeled as \u2018against\u2019 whereas for target \u2018Climate Change is a Real Concern\u2019 most of the data are labeled as \u2018favor\u2019. Therefore, the F-scores for the classes \u2018favor\u2019 and \u2018against\u2019 are more balanced over all targets than for just one target. Row I.c. shows results obtained by the winning system (among nineteen participating teams) in the 2016 SemEval shared task on this data (Task #6). Results of Oracle Sentiment Benchmarks: The Stance Dataset with labels for both stance and sentiment allows us, for the first time, to conduct an experiment to determine the extent to which stance detection can be solved with sentiment analysis alone. Specifically, we determine the performance of an oracle system that assigns stance as follows: For each target, select a sentiment-tostance assignment (mapping all positive instances to \u2018favor\u2019 and all negative instances to \u2018against\u2019 OR mapping all positive instances to \u2018against\u2019 and all negative instances to \u2018favor\u2019) that maximizes the F-macroT score.12 We call this benchmark the Oracle Sentiment Benchmark. This benchmark is informative because it gives an upper bound of the F-score one can expect when using a traditional sentiment analysis system for stance detection by simply mapping sentiment labels to stance labels.13\nIn our second sentiment benchmark, Oracle Sentiment and Target, we include the information on the target of opinion. Recall that the Stance Dataset is also annotated for whether the target of opinion is the same as the target of interest. We use these annotations in the following way: If the target of opinion is the same as the target of interest, the stance label is assigned in the same way as in the Oracle Sentiment Benchmark; if the target of opinion is some other entity (whose relation to the target of interest we do not know), we select the sentiment-to-stance assignment from the three\n11Since our evaluation measure is the average of the F1-scores for the \u2018favor\u2019 and \u2018against\u2019 classes, the random benchmark depends on the distribution of these classes and is different for different targets. The majority class is determined separately for each target. 12Tweets with sentiment label \u2018neither\u2019 are always mapped to the stance label \u2018neither\u2019. 13This is an upper bound because gold sentiment labels are used and because the sentiment-to-stance assignment is made in a way that is not usually available in real-world scenarios.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\noptions: mapping all positive instances to \u2018favor\u2019 and all negative instances to \u2018against\u2019 OR mapping all positive instances to \u2018against\u2019 and all negative instances to \u2018favor\u2019 OR mapping all instances to \u2019neither\u2019; tweets with no opinion are assigned the \u2019neither\u2019 class. Again, the selection is done as to optimize the F-macroT score. This benchmark indicates the level of performance one can expect when a sentiment analysis system is supplemented with the information on the target of opinion.\nRows I.d. and I.e. in Table VI show the F-scores obtained by the Oracle Sentiment Benchmarks on the Stance Test set. Observe that the scores are higher than the majority baseline for most of the targets, but yet much lower than 100%. This shows that even though sentiment can play a key role in detecting stance, sentiment alone is not sufficient. Results Obtained by Our Classifier: Row II.a. shows results obtained by our classifier with n-gram features alone. Note that not only are these results markedly higher than the majority baseline, most of these results are also higher than the best results obtained in SemEval-2016 Task 6 (I.c.) and the Oracle benchmarks (I.d. and I.e.). Surpassing the best SemEval-2016 results with a simple SVM-ngrams implementation is a little surprising, but it is possible that the SemEval teams did not implement a strong n-gram baseline such as that presented here, or obtained better results using additional features in cross-validation that did not translate to better results when applied to the test set. (The best systems in SemEval-2016 Task 6 used recurrent neural networks and word embeddings.)\nRows II.b. through II.e. show results obtained when using other features (one at a time) over and above the n-gram features. Observe that adding the target features leads to small improvements, but adding all other features (including those drawn from sentiment lexicons) does not improve results. Additional combinations of features such as \u2018n-grams + target + sentiment\u2019 also did not improve the performance (the results are not shown here).\nTable VII shows stance detection F-scores obtained by our classifier (SVM with ngram and target features) over the subset of tweets that express opinion towards the given target and the subset of tweets that express opinion towards another entity.14 Observe that the performance of the classifier is considerably better for tweets where opinion is expressed towards the target, than otherwise. Detecting stance towards a given target from tweets that express opinion about some other entity has not been addressed sufficiently in our research community, and we hope our dataset will encourage more work to address this challenging task.\n14The results for the Oracle Sentiment and Target benchmark are low on the subset of tweets that express opinion towards another entity since for some of the targets all instances in this subset are assigned to the \u2019neither\u2019 class, and therefore the F-score for such targets is zero on this subset.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016."}, {"heading": "6.2. Results for Sentiment Classification", "text": "Table VIII shows F-scores obtained by various automatic systems on the sentiment labels of the Stance Test set. Observe that the text classification system obtains markedly higher scores on sentiment prediction than on predicting stance.\nOnce again a classifier trained with n-gram features alone obtains results markedly higher than the baselines (II.a.). However, here (unlike as in the stance task) sentiment lexicon features provide marked further improvements (II.d). Adding POS and encoding features over and above n-grams results in modest gains (II.b. and II.c.) Yet, a classifier trained with all features (II.e.) does not outperform the classifier trained with only n-gram and sentiment features (II.d.).\nTable IX shows the performance of the sentiment classifier (SVM with n-grams and sentiment features) on tweets that express opinion towards the given target and those that express opinion about another entity. Observe that the sentiment prediction performance (unlike stance prediction performance) is similar on the two sets of tweets. This shows that the two sets of tweets are not qualitatively different in how they express opinion. However, since one set expresses opinion about an entity other than the target of interest, detecting stance towards the target of interest from them is notably more challenging."}, {"heading": "7. STANCE CLASSIFICATION USING ADDITIONAL UNLABELED TWEETS", "text": "Classification results can usually be improved by using more data in addition to the training set. In the sub-sections below we explore two such approaches when used for stance classification: distant supervision and word embeddings."}, {"heading": "7.1. Distant Supervision", "text": "Distant supervision is a method of supervised text classification wherein the training data is automatically generated using certain indicators present in the text. For example, Go et al. [2009] extracted tweets that ended with emoticons \u2018:)\u2019 and \u2018:(\u2019. Next,\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nthe emoticons were removed from the tweets and the remaining portions of the tweets were labeled positive or negative depending on whether they originally had \u2018:)\u2019 or \u2018:(\u2019, respectively. Central to the accuracy of these sentiment labels is the idea that emoticons are often redundant to the information already present in the tweet, that is, for example, a tweet that ends with a \u2018:)\u2019 emoticon likely conveys positive sentiment even without the emoticon. Mohammad [2012] and Kunneman et al. [2014] tested a similar hypothesis for emotions conveyed by hashtags at the end of a tweet and the rest of the tweet. In Section 7.1.1, we test the validity of the hypothesis that in terms of conveying stance, stance-indicative hashtags are often redundant to the information already present in the rest of the tweet. In Section 7.1.2, we show how we compiled additional training data using stance-indicative hashtags, and used it for stance classification.\n7.1.1. Redundancy of Stance-Indicative Hashtags. Given a target, stance-indicative (SI) hashtags can be determined manually (as we did to collect tweets). We will refer to the set we compiled as Manual SI Hashtags. Note that this set does not include the manually selected stance-ambiguous hashtags. Also, recall that the Manual SI Hashtags were removed from tweets prior to stance annotation.\nTo determine the extent to which an SI hashtag is redundant to the information already present in the tweet (in terms of conveying stance), we created a stance classification system that given a tweet-target instance from the Stance Test set, assigns to it the stance associated with the hashtag it originally had. Table X shows the accuracy of Favor\u2013Against Classification on the 555 instances of the Stance Test set which originally had the manually selected SI hashtags. Observe that the accuracy is well above the random baseline indicating that many SI hashtags are used redundantly in tweets (in terms of conveying stance). This means that these hashtags can be used to automatically collect additional, somewhat noisy, stance-labeled training data.\n7.1.2. Classification Experiments with Distant Supervision. If one has access to tweets labeled with stance, then one can estimate how well a hashtag can predict stance using the following score:\nH(hashtag) = maxstance label\u2208{favor,against} freq(hashtag , stance label)\nfreq(hashtag) (3)\nwhere freq(hashtag) is the number of tweets that have that particular hashtag; and, freq(hashtag , stance label) is the number of tweets that have that particular hashtag and stance label. We automatically extracted stance-indicative hashtags from the Stance Training set, by considering only those hashtags that occurred at least five times and for which H(hashtag) > 0.6. We will refer to this set of automatically compiled stanceindicative hashtags as Automatic SI Hashtags. Table XI lists examples.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nWe used both the Manual SI Hashtags and the Automatic SI Hashtags as queries to select tweets from the Stance Domain Corpus. (Recall that the Stance Domain Corpus is the large set of tweets pertaining to the five targets that was not manually labeled for stance.) We will refer to the set of tweets in the domain corpus that have the Manual SI Hashtags as the Manual Hashtag Corpus, and those that have the Automatic SI Hashtags as the Automatic Hashtag Corpus. We then assign to each of these tweets the stance label associated with the stance-indicative hashtag they contain. These noisy stance-labeled tweets can be used by a stance-classification system in two ways: (1) by including them as additional training data, OR (2) by capturing words that are associated with a particular stance towards the target (word\u2013stance associations) and words that are associated with a target (word\u2013target associations), and using these associations to generate additional features for classification.15\nOn the one hand, method 1 seems promising because it lets the classifier directly use additional training data; on the other hand, the additional training data is noisy and can have a very different class distribution than the manually labeled training and test sets. This means that the additional training data can impact the learned model disproportionately and adversely. Thus we experiment with both methods.16\nTable XII shows the results obtained on the Stance Test set when our stance classifier is trained on various training sets. Observe that using additional training data provides performance gains for three of the five targets. However, marked improvements are observed only for \u2018Hillary Clinton\u2019. It is possible, that in other test sets, the pseudo-labeled data is too noisy to be incorporated as is. Thus, we next explore incorporating this pseudo-labeled data through additional features.\nThe association between a term and a particular stance towards the target is calculated using pointwise mutual information (PMI) as shown below:17\nPMI (w, stance label) = log2 freq(w, stance label) \u2217N\nfreq(w) \u2217 freq(stance label) (4)\nwhere freq(w, stance label) is the number of times a term w occurs in tweets that have stance label; freq(w) is the frequency of w in the corpus; freq(stance label) is the number of tokens in tweets with label stance label; and N is the number of tokens in the corpus. When the system is trained on the Stance Training set, additional features are generated by taking the sum, min, and max of the associations scores for all the words in a tweet. Word\u2013target association scores are calculated and used in a similar manner.\nTable XIII shows the stance-classification results on the Stance Test set when using various word\u2013association features extracted from the domain corpus. Observe that the use of word\u2013association features leads to improvements for all targets. The improvements are particularly notable for \u2018Atheism\u2019, \u2018Feminist Movement\u2019, and \u2018Legalization of Abortion\u2019. Also, the associations obtained from the Automatic Hashtag Corpus are more informative to the classifier than those from the Manual Hashtag Corpus.\n15Note that these word association features are akin to unigram features, except that they are pre-extracted before applying the machine learning algorithm on the training corpus. 16We leave domain adaptation experiments for future work. 17Turney [2002] and Kiritchenko et al. [2014a] used similar measures for word\u2013sentiment associations.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016."}, {"heading": "7.2. Word Embeddings", "text": "Word embeddings are low-dimensional real-valued vectors used to represent words in the vocabulary [Bengio et al. 2001]. (The \u2018low\u2019 dimensionality is relative to the vocabulary size, and using a few hundred dimensions is common.) A number of different language modeling techniques have been proposed to generate word embeddings, all of which require only a large corpus of text (e.g., [Collobert and Weston 2008; Mnih and Hinton 2009]). Word embeddings have been successfully used as features in a number of tasks including sentiment analysis [Tang et al. 2014] and named entity recognition [Turian et al. 2010]. Here we explore the use of large collections of tweets to generate word embeddings as additional features for stance classification. We investigate whether they lead to further improvements over the results obtained by the best system configuration discussed in Section 6 \u2014 SVM trained on the stance training set and using n-gram and target features.\nWe derive 100-dimensional word vectors using Word2Vec Skip-gram model [Mikolov et al. 2013] trained over the Domain Corpus (the window size was set to 10, and the minimum count to 2). Given a training or test tweet, the word embedding features for the whole tweet are taken to be the component-wise averages of the word vectors for all the words appearing in the tweet.\nTable XIV shows stance classification results obtained using these word embedding features over and above the best configuration described in Section 6. Observe that adding word embedding features improves results for all targets except \u2018Hillary Clinton\u2019. Even though some teams participating in SemEval-2016 shared task on this dataset used word embeddings, their results are lower than those listed in Table XIV. This is likely because they generated word embeddings from a generic corpus of tweets rather than tweets associated with the target (as is the case with the domain corpus).\nOverall, we observe that the three methods we tested here (adding noisy-labeled data as new training instances, adding noisy-labeled data through association features, or generating word embeddings) affect different subsets of data differently. For example, the \u2018Hillary Clinton\u2019 subset of the test set benefited most from additional training data (Table XII) but failed to draw benefit from the embedding features. Such different behavior can be attributed to many possible reasons, such as the accuracy of hashtag-based labels, the class distribution of the new data, the size of the additional corpus, etc. Still, incorporating word embeddings seems a robust technique to improve the performance of stance detection in the presence of large unlabeled corpora.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016."}, {"heading": "8. RELATED WORK", "text": "Stance Detection Supervised learning has been used in almost all of the current approaches for stance classification, in which a large set of data has been collected and annotated in order to be used as training data for classifiers. In work by Somasundaran and Wiebe [2010], a lexicon for detecting argument trigger expressions was created and subsequently leveraged to identify arguments. These extracted arguments, together with sentiment expressions and their targets, were employed in a supervised learner as features for stance classification. Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance. Djemili et al. [2014] use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain stance. Rajadesingan and Liu [2014] determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate.\nExisting datasets for stance detection were created from online debate forums like 4forums.com and createdebates.com [Somasundaran and Wiebe 2010; Walker et al. 2012b; Hasan and Ng 2013]. The majority of these debates are two-sided, and the data labels are often provided by the authors of the posts. Topics of these debates are mostly related to ideological controversial issues such as gay rights and abortion.\nSentiment Analysis and Opinion Mining There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys [Pang and Lee 2008; Liu and Zhang 2012; Mohammad 2015] and proceedings of recent shared task competitions [Wilson et al. 2013; Rosenthal et al. 2015]. Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA [Pontiki et al. 2015; Pontiki et al. 2014]. Mohammad et al. [2013] and Kiritchenko et al. [2014b] came first in the 2013 Sentiment in Twitter and 2014 SemEval ABSA shared tasks. We use most of the features they use in our classifier.\nThere has been considerable interest in analyzing political tweets towards detecting sentiment, emotion, and purpose in electoral tweets [Mohammad et al. 2015], determining political alignment of tweeters [Golbeck and Hansen 2011; Conover et al. 2011a], identifying contentious issues and political opinions [Maynard and Funk 2011], detecting the amount of polarization in the electorate [Conover et al. 2011b], and even predicting the voting intentions or outcome of elections [Tumasjan et al. 2010; Bermingham and Smeaton 2011; Lampos et al. 2013].\nThere are other subtasks in opinion mining related to stance classification, such as biased language detection [Recasens et al. 2013; Yano et al. 2010], perspective identification [Lin et al. 2006] and user classification based on their views [Kato et al. 2008]. Perspective identification was defined as the subjective evaluation of points of view [Lin et al. 2006]. Deng et al. [2014] suggested an unsupervised framework to detect implicit sentiment by inference over explicit sentiments and events that positively or negatively affect the theme. None of the prior work has created a dataset annotated for both stance and sentiment. Neither has any work directly and substantially explored the relationship between stance and sentiment.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016.\nTextual Entailment In textual entailment, the goal is to infer a textual statement (hypothesis) from a given source text [Dagan and Glickman 2004]. Textual entailment is a core NLP building block, and has applications in question answering, machine translation, information retrieval and other tasks. It has received a lot of attention in the past decade, and we refer the reader to surveys [Androutsopoulos and Malakasiotis 2010; Dagan et al. 2013] and proceedings of recent challenges on recognizing textual entailment [Bentivogli et al. 2011; Marelli et al. 2014; Dzikovska et al. 2016].\nThe task we explore in this paper, stance detection in tweets, can be viewed as another application of textual entailment, where the goal is to infer a person\u2019s opinion towards a given target based on a single tweet written by this person. In this special case of textual entailment, the hypotheses are always fixed (the person is either in favor of or against the target). Furthermore, we need to derive not only the meaning of the tweet, but also the attitude of the tweet\u2019s author.\nDistant Supervision Distant supervision makes use of indirectly labeled (or weakly labeled) data. This approach is widely used in automatic relation extraction, where information about pairs of related entities can be acquired from external knowledge sources such as Freebase or Wikipedia [Craven and Kumlien 1999; Mintz et al. 2009]. Then, sentences containing both entities are considered positive examples for the corresponding relation. In sentiment and emotion analysis, weakly labeled data can be accumulated by using sentiment clues provided by the authors of the text\u2013clues like emoticons and hashtags [Go et al. 2009; Mohammad 2012]. Recently, distant supervision has been applied to topic classification [Husby and Barbosa 2012; Magdy et al. 2015], named entity recognition [Ritter et al. 2011], event extraction [Reschke et al. 2014], and semantic parsing [Parikh et al. 2015]."}, {"heading": "9. CONCLUSIONS AND FUTURE WORK", "text": "We presented the first dataset of tweets annotated for both stance towards given targets and for polarity of language. The tweets are also annotated for whether opinion is expressed towards the given target or towards another entity. Partitions of the stanceannotated data created as part of this project were used as training and test sets in a recent shared task competition on stance detection that received submissions from 19 teams. We proposed a simple, but effective stance detection system that obtained an F-score (70.3) higher than the one obtained by the more complex, best-performing system in the competition. We use a linear-kernel SVM classifier that leverages word and character n-grams as well as sentiment features drawn from available sentiment lexicons and word-embedding features drawn from additional unlabeled data.\nWe presented a detailed analysis of the dataset and conducted several experiments to tease out the interactions between stance and sentiment. Notably, we showed that sentiment features are not as effective for stance detection as they are for sentiment prediction. Moreover, an oracle system that had access to gold sentiment and target of opinion annotations was able to predict stance with an F-score of only 59.6%. We also showed that even though humans are capable of detecting stance towards a given target from texts that express opinion towards a different target, automatic systems perform poorly on such data.\nIn future work, we will explore the use of more sophisticated features (e.g., those derived from dependency parse trees and automatically generated entity\u2013entity relationship knowledge bases) and more sophisticated classifiers (e.g., deep architectures that jointly model stance, target of opinion, and sentiment). We are interested in developing stance detection systems that do not require stance-labeled instances for the target of interest, but instead, can learn from existing stance-labeled instances for other targets in the same domain. We also want to model the ways in which stance is conveyed, and how the distribution of stance towards a target changes over time.\nACM Transactions on Embedded Computing Systems, Vol. 0, No. 0, Article 0, Publication date: 2016."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Colin Cherry and Xiaodan Zhu for helpful discussions. The second author of this paper was supported by the Natural Sciences and Engineering Research Council of Canada under the CREATE program."}], "references": [{"title": "Cats rule and dogs drool!: Classifying stance in online debate", "author": ["Pranav Anand", "Marilyn Walker", "Rob Abbott", "Jean E. Fox Tree", "Robeson Bowmani", "Michael Minor."], "venue": "Proceedings of the Workshop on Computational Approaches to Subjectivity and Sentiment Analysis. 1\u20139.", "citeRegEx": "Anand et al\\.,? 2011", "shortCiteRegEx": "Anand et al\\.", "year": 2011}, {"title": "A survey of paraphrasing and textual entailment methods", "author": ["Ion Androutsopoulos", "Prodromos Malakasiotis."], "venue": "Journal of Artificial Intelligence Research (2010), 135\u2013187.", "citeRegEx": "Androutsopoulos and Malakasiotis.,? 2010", "shortCiteRegEx": "Androutsopoulos and Malakasiotis.", "year": 2010}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Bengio et al\\.,? 2001", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "The seventh PASCAL recognizing textual entailment challenge", "author": ["Luisa Bentivogli", "Peter Clark", "Ido Dagan", "Hoa Dang", "Danilo Giampiccolo."], "venue": "Proceedings of Text Analysis Conference.", "citeRegEx": "Bentivogli et al\\.,? 2011", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2011}, {"title": "On Using Twitter to Monitor Political Sentiment and Predict Election Results", "author": ["Adam Bermingham", "Alan Smeaton."], "venue": "Proceedings of the Workshop on Sentiment Analysis where AI meets Psychology. Chiang Mai, Thailand, 2\u201310.", "citeRegEx": "Bermingham and Smeaton.,? 2011", "shortCiteRegEx": "Bermingham and Smeaton.", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the International Conference on Machine Learning. 160\u2013167.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Predicting the Political Alignment of Twitter Users", "author": ["Michael Conover", "Bruno Gon\u00e7alves", "Jacob Ratkiewicz", "Alessandro Flammini", "Filippo Menczer."], "venue": "Proceedings of the IEEE International Conference on Privacy, Security, Risk, and Trust. 192\u2013199.", "citeRegEx": "Conover et al\\.,? 2011a", "shortCiteRegEx": "Conover et al\\.", "year": 2011}, {"title": "Political Polarization on Twitter", "author": ["Michael Conover", "Jacob Ratkiewicz", "Matthew R. Francisco", "Bruno Gon\u00e7alves", "Filippo Menczer", "Alessandro Flammini."], "venue": "Proceedings of the International AAAI Conference on Weblogs and Social Media. 89\u201396.", "citeRegEx": "Conover et al\\.,? 2011b", "shortCiteRegEx": "Conover et al\\.", "year": 2011}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Mark Craven", "Johan Kumlien."], "venue": "Proceedings of the Conference on Intelligent Systems for Molecular Biology. 77\u201386.", "citeRegEx": "Craven and Kumlien.,? 1999", "shortCiteRegEx": "Craven and Kumlien.", "year": 1999}, {"title": "Probabilistic textual entailment: Generic applied modeling of language variability", "author": ["Ido Dagan", "Oren Glickman."], "venue": "PASCAL workshop on Text Understanding and Mining. 26\u201329.", "citeRegEx": "Dagan and Glickman.,? 2004", "shortCiteRegEx": "Dagan and Glickman.", "year": 2004}, {"title": "Recognizing textual entailment: Models and applications", "author": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Dagan et al\\.,? 2013", "shortCiteRegEx": "Dagan et al\\.", "year": 2013}, {"title": "Joint inference and disambiguation of implicit sentiments via implicature constraints", "author": ["Lingjia Deng", "Janyce Wiebe", "Yoonjung Choi."], "venue": "Proceedings of the International Conference on Computational Linguistics. 79\u201388.", "citeRegEx": "Deng et al\\.,? 2014", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "What does Twitter have to say about ideology", "author": ["Sarah Djemili", "Julien Longhi", "Claudia Marinica", "Dimitris Kotzinos", "Georges-Elia Sarfati."], "venue": "Proceedings of the Natural Language Processing for Computer-Mediated Communication/Social Media-Pre-conference workshop at Konvens.", "citeRegEx": "Djemili et al\\.,? 2014", "shortCiteRegEx": "Djemili et al\\.", "year": 2014}, {"title": "The joint student response analysis and recognizing textual entailment challenge: making sense of student responses in educational applications", "author": ["Myroslava O. Dzikovska", "Rodney D. Nielsen", "Claudia Leacock."], "venue": "Language Resources and Evaluation 50 (2016), 67\u201393. Issue 1.", "citeRegEx": "Dzikovska et al\\.,? 2016", "shortCiteRegEx": "Dzikovska et al\\.", "year": 2016}, {"title": "Automated Classification of Stance in Student Essays: An Approach Using Stance Target Information and the Wikipedia Link-Based Measure", "author": ["Adam Faulkner."], "venue": "Proceedings of the Flairs Conference.", "citeRegEx": "Faulkner.,? 2014", "shortCiteRegEx": "Faulkner.", "year": 2014}, {"title": "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Twitter Sentiment Classification using Distant Supervision", "author": ["Alec Go", "Richa Bhayani", "Lei Huang."], "venue": "Technical Report. Stanford University.", "citeRegEx": "Go et al\\.,? 2009", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Computing Political Preference Among Twitter Followers", "author": ["Jennifer Golbeck", "Derek Hansen."], "venue": "Proceedings of the Conference on Human Factors in Computing Systems. New York, NY, 1105\u20131108.", "citeRegEx": "Golbeck and Hansen.,? 2011", "shortCiteRegEx": "Golbeck and Hansen.", "year": 2011}, {"title": "Stance classification of ideological debates: Data, models, features, and constraints", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of the International Joint Conference on Natural Language Processing. 1348\u20131356.", "citeRegEx": "Hasan and Ng.,? 2013", "shortCiteRegEx": "Hasan and Ng.", "year": 2013}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu."], "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 168\u2013177.", "citeRegEx": "Hu and Liu.,? 2004", "shortCiteRegEx": "Hu and Liu.", "year": 2004}, {"title": "Topic classification of blog posts using distant supervision", "author": ["Stephanie D. Husby", "Denilson Barbosa."], "venue": "Proceedings of the Workshop on Semantic Analysis in Social Media. 28\u201336.", "citeRegEx": "Husby and Barbosa.,? 2012", "shortCiteRegEx": "Husby and Barbosa.", "year": 2012}, {"title": "Taking sides: User classification for informal online political discourse", "author": ["Yoshikiyo Kato", "Sadao Kurohashi", "Kentaro Inui", "Robert Malouf", "Tony Mullen."], "venue": "Internet Research 18, 2 (2008), 177\u2013190.", "citeRegEx": "Kato et al\\.,? 2008", "shortCiteRegEx": "Kato et al\\.", "year": 2008}, {"title": "NRC-Canada-2014: Detecting aspects and sentiment in customer reviews", "author": ["Svetlana Kiritchenko", "Xiaodan Zhu", "Colin Cherry", "Saif M. Mohammad."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. Dublin, Ireland.", "citeRegEx": "Kiritchenko et al\\.,? 2014b", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "Sentiment Analysis of Short Informal Texts", "author": ["Svetlana Kiritchenko", "Xiaodan Zhu", "Saif M. Mohammad."], "venue": "Journal of Artificial Intelligence Research 50 (2014), 723\u2013762.", "citeRegEx": "Kiritchenko et al\\.,? 2014a", "shortCiteRegEx": "Kiritchenko et al\\.", "year": 2014}, {"title": "The (un)predictability of emotional hashtags in Twitter", "author": ["Florian Kunneman", "Christine Liebrecht", "Antal van den Bosch."], "venue": "Proceedings of the Workshop on Language Analysis for Social Media. 26\u201334.", "citeRegEx": "Kunneman et al\\.,? 2014", "shortCiteRegEx": "Kunneman et al\\.", "year": 2014}, {"title": "A user-centric model of voting intention from social media", "author": ["Vasileios Lampos", "Daniel Preotiuc-Pietro", "Trevor Cohn."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. 993\u20131003.", "citeRegEx": "Lampos et al\\.,? 2013", "shortCiteRegEx": "Lampos et al\\.", "year": 2013}, {"title": "Which side are you on? Identifying perspectives at the document and sentence levels", "author": ["Wei-Hao Lin", "Theresa Wilson", "Janyce Wiebe", "Alexander Hauptmann."], "venue": "Proceedings of the Conference on Computational Natural Language Learning. 109\u2013116.", "citeRegEx": "Lin et al\\.,? 2006", "shortCiteRegEx": "Lin et al\\.", "year": 2006}, {"title": "Sentiment Analysis: Mining Opinions, Sentiments, and Emotions", "author": ["Bing Liu."], "venue": "Cambridge University Press.", "citeRegEx": "Liu.,? 2015", "shortCiteRegEx": "Liu.", "year": 2015}, {"title": "A Survey of Opinion Mining and Sentiment Analysis", "author": ["Bing Liu", "Lei Zhang."], "venue": "Mining Text Data, Charu C. Aggarwal and ChengXiang Zhai (Eds.). Springer, 415\u2013463.", "citeRegEx": "Liu and Zhang.,? 2012", "shortCiteRegEx": "Liu and Zhang.", "year": 2012}, {"title": "Bridging social media via distant supervision", "author": ["Walid Magdy", "Hassan Sajjad", "Tarek El-Ganainy", "Fabrizio Sebastiani."], "venue": "Social Network Analysis and Mining 5, 1 (2015), 1\u201312.", "citeRegEx": "Magdy et al\\.,? 2015", "shortCiteRegEx": "Magdy et al\\.", "year": 2015}, {"title": "SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment", "author": ["Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli."], "venue": "Proceedings of the International Workshop on Semantic Evaluation.", "citeRegEx": "Marelli et al\\.,? 2014", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Automatic Detection of Political Opinions in Tweets", "author": ["Diana Maynard", "Adam Funk."], "venue": "Proceedings of the ESWC Workshop on the Semantic Web. 88\u201399.", "citeRegEx": "Maynard and Funk.,? 2011", "shortCiteRegEx": "Maynard and Funk.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems. 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. 1003\u20131011.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E. Hinton."], "venue": "Advances in Neural Information Processing Systems. 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Emotional Tweets", "author": ["Saif M. Mohammad."], "venue": "Proceedings of the Joint Conference on Lexical and Computational Semantics. Montr\u00e9al, Canada, 246\u2013255.", "citeRegEx": "Mohammad.,? 2012", "shortCiteRegEx": "Mohammad.", "year": 2012}, {"title": "Sentiment Analysis: Detecting Valence, Emotions, and Other Affectual States from Text", "author": ["Saif M Mohammad."], "venue": "(2015).", "citeRegEx": "Mohammad.,? 2015", "shortCiteRegEx": "Mohammad.", "year": 2015}, {"title": "A Practical Guide to Sentiment Annotation: Challenges and Solutions", "author": ["Saif M. Mohammad."], "venue": "Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis.", "citeRegEx": "Mohammad.,? 2016", "shortCiteRegEx": "Mohammad.", "year": 2016}, {"title": "Semeval-2016 Task 6: Detecting Stance in Tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. San Diego, California.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "NRC-Canada: Building the State-ofthe-Art in Sentiment Analysis of Tweets", "author": ["Saif M. Mohammad", "Svetlana Kiritchenko", "Xiaodan Zhu."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. Atlanta, Georgia, USA.", "citeRegEx": "Mohammad et al\\.,? 2013", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon", "author": ["Saif M. Mohammad", "Peter D. Turney."], "venue": "Proceedings of the Workshop on Computational Approaches to Analysis and Generation of Emotion in Text. 26\u201334.", "citeRegEx": "Mohammad and Turney.,? 2010", "shortCiteRegEx": "Mohammad and Turney.", "year": 2010}, {"title": "Sentiment, emotion, purpose, and style in electoral tweets", "author": ["Saif M. Mohammad", "Xiaodan Zhu", "Svetlana Kiritchenko", "Joel Martin."], "venue": "Information Processing and Management 51 (2015), 480\u2013499.", "citeRegEx": "Mohammad et al\\.,? 2015", "shortCiteRegEx": "Mohammad et al\\.", "year": 2015}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and Trends in Information Retrieval 2, 1\u20132 (2008), 1\u2013135.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Grounded Semantic Parsing for Complex Knowledge Extraction", "author": ["Ankur P. Parikh", "Hoifung Poon", "Kristina Toutanova."], "venue": "Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Parikh et al\\.,? 2015", "shortCiteRegEx": "Parikh et al\\.", "year": 2015}, {"title": "SemEval-2015 Task 12: Aspect based sentiment analysis", "author": ["Maria Pontiki", "Dimitrios Galanis", "Haris Papageogiou", "Suresh Manandhar", "Ion Androutsopoulos."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. Denver, Colorado.", "citeRegEx": "Pontiki et al\\.,? 2015", "shortCiteRegEx": "Pontiki et al\\.", "year": 2015}, {"title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis", "author": ["Maria Pontiki", "Dimitrios Galanis", "John Pavlopoulos", "Harris Papageorgiou", "Ion Androutsopoulos", "Suresh Manandhar."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. Dublin, Ireland.", "citeRegEx": "Pontiki et al\\.,? 2014", "shortCiteRegEx": "Pontiki et al\\.", "year": 2014}, {"title": "Identifying Users with Opposing Opinions in Twitter Debates", "author": ["Ashwin Rajadesingan", "Huan Liu."], "venue": "Proceedings of the Conference on Social Computing, Behavioral-Cultural Modeling and Prediction. Washington, DC, USA, 153\u2013160.", "citeRegEx": "Rajadesingan and Liu.,? 2014", "shortCiteRegEx": "Rajadesingan and Liu.", "year": 2014}, {"title": "Linguistic Models for Analyzing and Detecting Biased Language", "author": ["Marta Recasens", "Cristian Danescu-Niculescu-Mizil", "Dan Jurafsky."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. 1650\u20131659.", "citeRegEx": "Recasens et al\\.,? 2013", "shortCiteRegEx": "Recasens et al\\.", "year": 2013}, {"title": "Event Extraction Using Distant Supervision", "author": ["Kevin Reschke", "Martin Jankowiak", "Mihai Surdeanu", "Christopher D. Manning", "Daniel Jurafsky"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation", "citeRegEx": "Reschke et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reschke et al\\.", "year": 2014}, {"title": "Named entity recognition in tweets: an experimental study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni."], "venue": "Proceedings of EMNLP. Edinburgh, Scotland, 1524\u20131534.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "SemEval-2015 Task 10: Sentiment analysis in Twitter", "author": ["Sara Rosenthal", "Preslav Nakov", "Svetlana Kiritchenko", "Saif M. Mohammad", "Alan Ritter", "Veselin Stoyanov."], "venue": "Proceedings of the International Workshop on Semantic Evaluations.", "citeRegEx": "Rosenthal et al\\.,? 2015", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "From Argumentation Mining to Stance Classification", "author": ["Parinaz Sobhani", "Diana Inkpen", "Stan Matwin."], "venue": "Proceedings of the Workshop on Argumentation Mining. Denver, Colorado, USA, 67\u201377.", "citeRegEx": "Sobhani et al\\.,? 2015", "shortCiteRegEx": "Sobhani et al\\.", "year": 2015}, {"title": "Recognizing stances in ideological on-line debates", "author": ["Swapna Somasundaran", "Janyce Wiebe."], "venue": "Proceedings of the NAACL HLT 2010 Workshop CAAGET. 116\u2013124.", "citeRegEx": "Somasundaran and Wiebe.,? 2010", "shortCiteRegEx": "Somasundaran and Wiebe.", "year": 2010}, {"title": "Learning sentiment-specific word embedding for Twitter sentiment classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. 1555\u20131565.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Matt Thomas", "Bo Pang", "Lillian Lee."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. 327\u2013335.", "citeRegEx": "Thomas et al\\.,? 2006", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Election Forecasts With Twitter: How 140 Characters Reflect the Political Landscape", "author": ["Andranik Tumasjan", "Timm O. Sprenger", "Philipp G. Sandner", "Isabell M. Welpe."], "venue": "Social Science Computer Review 29, 4 (2010), 402\u2013418.", "citeRegEx": "Tumasjan et al\\.,? 2010", "shortCiteRegEx": "Tumasjan et al\\.", "year": 2010}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. 384\u2013394.", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews", "author": ["Peter D. Turney."], "venue": "Proceedings of the Association for Computational Linguistics. 417\u2013424.", "citeRegEx": "Turney.,? 2002", "shortCiteRegEx": "Turney.", "year": 2002}, {"title": "Stance classification using dialogic properties of persuasion", "author": ["Marilyn A. Walker", "Pranav Anand", "Robert Abbott", "Ricky Grant."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 592\u2013596.", "citeRegEx": "Walker et al\\.,? 2012a", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "A Corpus for Research on Deliberation and Debate", "author": ["Marilyn A. Walker", "Jean E. Fox Tree", "Pranav Anand", "Rob Abbott", "Joseph King."], "venue": "Proceedings of the International Conference on Language Resources and Evaluation. 812\u2013817.", "citeRegEx": "Walker et al\\.,? 2012b", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "SemEval-2013 Task 2: Sentiment Analysis in Twitter", "author": ["Theresa Wilson", "Zornitsa Kozareva", "Preslav Nakov", "Sara Rosenthal", "Veselin Stoyanov", "Alan Ritter."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. Atlanta, USA.", "citeRegEx": "Wilson et al\\.,? 2013", "shortCiteRegEx": "Wilson et al\\.", "year": 2013}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing. Vancouver, British Columbia, Canada, 347\u2013354.", "citeRegEx": "Wilson et al\\.,? 2005", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Shedding (a thousand points of) light on biased language", "author": ["Tae Yano", "Philip Resnik", "Noah A Smith."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk. 152\u2013158.", "citeRegEx": "Yano et al\\.,? 2010", "shortCiteRegEx": "Yano et al\\.", "year": 2010}, {"title": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection", "author": ["Guido Zarrella", "Amy Marsh."], "venue": "Proceedings of the International Workshop on Semantic Evaluation. San Diego, California.", "citeRegEx": "Zarrella and Marsh.,? 2016", "shortCiteRegEx": "Zarrella and Marsh.", "year": 2016}], "referenceMentions": [{"referenceID": 54, "context": "However, most work focuses on congressional debates [Thomas et al. 2006] or debates in online forums [Somasundaran and Wiebe 2010; Anand et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 0, "context": "2006] or debates in online forums [Somasundaran and Wiebe 2010; Anand et al. 2011; Walker et al. 2012a; Hasan and Ng 2013].", "startOffset": 34, "endOffset": 122}, {"referenceID": 58, "context": "2006] or debates in online forums [Somasundaran and Wiebe 2010; Anand et al. 2011; Walker et al. 2012a; Hasan and Ng 2013].", "startOffset": 34, "endOffset": 122}, {"referenceID": 38, "context": "(3) Organized a Shared Task Competition on Stance: Partitions of this stance-annotated data were used as training and test sets in the SemEval-2016 shared task competition, Task #6: Detecting Stance from Tweets [Mohammad et al. 2016].", "startOffset": 211, "endOffset": 233}, {"referenceID": 60, "context": "Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact [Wilson et al. 2013; Mohammad et al. 2013; Kiritchenko et al. 2014a; Rosenthal et al. 2015].", "startOffset": 233, "endOffset": 324}, {"referenceID": 39, "context": "Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact [Wilson et al. 2013; Mohammad et al. 2013; Kiritchenko et al. 2014a; Rosenthal et al. 2015].", "startOffset": 233, "endOffset": 324}, {"referenceID": 23, "context": "Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact [Wilson et al. 2013; Mohammad et al. 2013; Kiritchenko et al. 2014a; Rosenthal et al. 2015].", "startOffset": 233, "endOffset": 324}, {"referenceID": 50, "context": "Past work has shown that the most useful features for sentiment analysis are word and character n-grams and sentiment lexicons, whereas others such as negation features, part-of-speech features, and punctuation have a smaller impact [Wilson et al. 2013; Mohammad et al. 2013; Kiritchenko et al. 2014a; Rosenthal et al. 2015].", "startOffset": 233, "endOffset": 324}, {"referenceID": 23, "context": "8 Positive and negative language tend to have sufficient amount of commonality regardless of topic of discussion, and hence sentiment analysis systems traditionally learn a single model from all of the training data [Liu 2015; Kiritchenko et al. 2014a; Rosenthal et al. 2015].", "startOffset": 216, "endOffset": 275}, {"referenceID": 50, "context": "8 Positive and negative language tend to have sufficient amount of commonality regardless of topic of discussion, and hence sentiment analysis systems traditionally learn a single model from all of the training data [Liu 2015; Kiritchenko et al. 2014a; Rosenthal et al. 2015].", "startOffset": 216, "endOffset": 275}, {"referenceID": 15, "context": "Tweets are tokenized and part-of-speech tagged with the CMU Twitter NLP tool [Gimpel et al. 2011].", "startOffset": 77, "endOffset": 97}, {"referenceID": 61, "context": "): The sentiment lexicon features are derived from three manually created lexicons: NRC Emotion Lexicon [Mohammad and Turney 2010], Hu and Liu Lexicon [Hu and Liu 2004], and MPQA Subjectivity Lexicon [Wilson et al. 2005], and two automatically created, tweet-specific, lexicons: NRC Hashtag Sentiment and NRC Emoticon (a.", "startOffset": 200, "endOffset": 220}, {"referenceID": 23, "context": "Sentiment140) [Kiritchenko et al. 2014a]; \u2022 target: presence/absence of the target of interest in the tweet;9 \u2022 POS: the number of occurrences of each part-of-speech tag (POS); \u2022 encodings (enc.", "startOffset": 14, "endOffset": 40}, {"referenceID": 60, "context": "10A similar metric was used in the past for sentiment analysis\u2014SemEval 2013 Task 2 [Wilson et al. 2013].", "startOffset": 83, "endOffset": 103}, {"referenceID": 16, "context": "For example, Go et al. [2009] extracted tweets that ended with emoticons \u2018:)\u2019 and \u2018:(\u2019.", "startOffset": 13, "endOffset": 30}, {"referenceID": 34, "context": "Mohammad [2012] and Kunneman et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 24, "context": "Mohammad [2012] and Kunneman et al. [2014] tested a similar hypothesis for emotions conveyed by hashtags at the end of a tweet and the rest of the tweet.", "startOffset": 20, "endOffset": 43}, {"referenceID": 55, "context": "17Turney [2002] and Kiritchenko et al.", "startOffset": 2, "endOffset": 16}, {"referenceID": 22, "context": "17Turney [2002] and Kiritchenko et al. [2014a] used similar measures for word\u2013sentiment associations.", "startOffset": 20, "endOffset": 47}, {"referenceID": 2, "context": "Word embeddings are low-dimensional real-valued vectors used to represent words in the vocabulary [Bengio et al. 2001].", "startOffset": 98, "endOffset": 118}, {"referenceID": 53, "context": "Word embeddings have been successfully used as features in a number of tasks including sentiment analysis [Tang et al. 2014] and named entity recognition [Turian et al.", "startOffset": 106, "endOffset": 124}, {"referenceID": 56, "context": "2014] and named entity recognition [Turian et al. 2010].", "startOffset": 35, "endOffset": 55}, {"referenceID": 32, "context": "We derive 100-dimensional word vectors using Word2Vec Skip-gram model [Mikolov et al. 2013] trained over the Domain Corpus (the window size was set to 10, and the minimum count to 2).", "startOffset": 70, "endOffset": 91}, {"referenceID": 59, "context": "com [Somasundaran and Wiebe 2010; Walker et al. 2012b; Hasan and Ng 2013].", "startOffset": 4, "endOffset": 73}, {"referenceID": 60, "context": "Sentiment Analysis and Opinion Mining There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys [Pang and Lee 2008; Liu and Zhang 2012; Mohammad 2015] and proceedings of recent shared task competitions [Wilson et al. 2013; Rosenthal et al. 2015].", "startOffset": 243, "endOffset": 286}, {"referenceID": 50, "context": "Sentiment Analysis and Opinion Mining There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys [Pang and Lee 2008; Liu and Zhang 2012; Mohammad 2015] and proceedings of recent shared task competitions [Wilson et al. 2013; Rosenthal et al. 2015].", "startOffset": 243, "endOffset": 286}, {"referenceID": 44, "context": "We refer the reader to SemEval proceedings for related work on ABSA [Pontiki et al. 2015; Pontiki et al. 2014].", "startOffset": 68, "endOffset": 110}, {"referenceID": 45, "context": "We refer the reader to SemEval proceedings for related work on ABSA [Pontiki et al. 2015; Pontiki et al. 2014].", "startOffset": 68, "endOffset": 110}, {"referenceID": 41, "context": "There has been considerable interest in analyzing political tweets towards detecting sentiment, emotion, and purpose in electoral tweets [Mohammad et al. 2015], determining political alignment of tweeters [Golbeck and Hansen 2011; Conover et al.", "startOffset": 137, "endOffset": 159}, {"referenceID": 6, "context": "2015], determining political alignment of tweeters [Golbeck and Hansen 2011; Conover et al. 2011a], identifying contentious issues and political opinions [Maynard and Funk 2011], detecting the amount of polarization in the electorate [Conover et al.", "startOffset": 51, "endOffset": 98}, {"referenceID": 7, "context": "2011a], identifying contentious issues and political opinions [Maynard and Funk 2011], detecting the amount of polarization in the electorate [Conover et al. 2011b], and even predicting the voting intentions or outcome of elections [Tumasjan et al.", "startOffset": 142, "endOffset": 164}, {"referenceID": 55, "context": "2011b], and even predicting the voting intentions or outcome of elections [Tumasjan et al. 2010; Bermingham and Smeaton 2011; Lampos et al. 2013].", "startOffset": 74, "endOffset": 145}, {"referenceID": 25, "context": "2011b], and even predicting the voting intentions or outcome of elections [Tumasjan et al. 2010; Bermingham and Smeaton 2011; Lampos et al. 2013].", "startOffset": 74, "endOffset": 145}, {"referenceID": 47, "context": "There are other subtasks in opinion mining related to stance classification, such as biased language detection [Recasens et al. 2013; Yano et al. 2010], perspective identification [Lin et al.", "startOffset": 111, "endOffset": 151}, {"referenceID": 62, "context": "There are other subtasks in opinion mining related to stance classification, such as biased language detection [Recasens et al. 2013; Yano et al. 2010], perspective identification [Lin et al.", "startOffset": 111, "endOffset": 151}, {"referenceID": 26, "context": "2010], perspective identification [Lin et al. 2006] and user classification based on their views [Kato et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 21, "context": "2006] and user classification based on their views [Kato et al. 2008].", "startOffset": 51, "endOffset": 69}, {"referenceID": 26, "context": "Perspective identification was defined as the subjective evaluation of points of view [Lin et al. 2006].", "startOffset": 86, "endOffset": 103}, {"referenceID": 22, "context": "In work by Somasundaran and Wiebe [2010], a lexicon for detecting argument trigger expressions was created and subsequently leveraged to identify arguments.", "startOffset": 11, "endOffset": 41}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language.", "startOffset": 0, "endOffset": 294}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language.", "startOffset": 0, "endOffset": 311}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance.", "startOffset": 0, "endOffset": 504}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance. Djemili et al. [2014] use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain stance.", "startOffset": 0, "endOffset": 593}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance. Djemili et al. [2014] use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain stance. Rajadesingan and Liu [2014] determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate.", "startOffset": 0, "endOffset": 737}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance. Djemili et al. [2014] use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain stance. Rajadesingan and Liu [2014] determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Existing datasets for stance detection were created from online debate forums like 4forums.com and createdebates.com [Somasundaran and Wiebe 2010; Walker et al. 2012b; Hasan and Ng 2013]. The majority of these debates are two-sided, and the data labels are often provided by the authors of the posts. Topics of these debates are mostly related to ideological controversial issues such as gay rights and abortion. Sentiment Analysis and Opinion Mining There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys [Pang and Lee 2008; Liu and Zhang 2012; Mohammad 2015] and proceedings of recent shared task competitions [Wilson et al. 2013; Rosenthal et al. 2015]. Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA [Pontiki et al. 2015; Pontiki et al. 2014]. Mohammad et al. [2013] and Kiritchenko et al.", "startOffset": 0, "endOffset": 1971}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance. Djemili et al. [2014] use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain stance. Rajadesingan and Liu [2014] determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Existing datasets for stance detection were created from online debate forums like 4forums.com and createdebates.com [Somasundaran and Wiebe 2010; Walker et al. 2012b; Hasan and Ng 2013]. The majority of these debates are two-sided, and the data labels are often provided by the authors of the posts. Topics of these debates are mostly related to ideological controversial issues such as gay rights and abortion. Sentiment Analysis and Opinion Mining There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys [Pang and Lee 2008; Liu and Zhang 2012; Mohammad 2015] and proceedings of recent shared task competitions [Wilson et al. 2013; Rosenthal et al. 2015]. Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA [Pontiki et al. 2015; Pontiki et al. 2014]. Mohammad et al. [2013] and Kiritchenko et al. [2014b] came first in the 2013 Sentiment in Twitter and 2014 SemEval ABSA shared tasks.", "startOffset": 0, "endOffset": 2002}, {"referenceID": 0, "context": "Anand et al. [2011] deployed a rule-based classifier with several features such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreements and disagreements between posts were exploited by Walker et al. [2012a]. Faulkner [2014] investigated the problem of detecting document-level stance in student essays by making use of two sets of features that are supposed to represent stance-taking language. Sobhani et al. [2015] extracted arguments used in online news comments to detect stance. Djemili et al. [2014] use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain stance. Rajadesingan and Liu [2014] determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Existing datasets for stance detection were created from online debate forums like 4forums.com and createdebates.com [Somasundaran and Wiebe 2010; Walker et al. 2012b; Hasan and Ng 2013]. The majority of these debates are two-sided, and the data labels are often provided by the authors of the posts. Topics of these debates are mostly related to ideological controversial issues such as gay rights and abortion. Sentiment Analysis and Opinion Mining There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys [Pang and Lee 2008; Liu and Zhang 2012; Mohammad 2015] and proceedings of recent shared task competitions [Wilson et al. 2013; Rosenthal et al. 2015]. Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA [Pontiki et al. 2015; Pontiki et al. 2014]. Mohammad et al. [2013] and Kiritchenko et al. [2014b] came first in the 2013 Sentiment in Twitter and 2014 SemEval ABSA shared tasks. We use most of the features they use in our classifier. There has been considerable interest in analyzing political tweets towards detecting sentiment, emotion, and purpose in electoral tweets [Mohammad et al. 2015], determining political alignment of tweeters [Golbeck and Hansen 2011; Conover et al. 2011a], identifying contentious issues and political opinions [Maynard and Funk 2011], detecting the amount of polarization in the electorate [Conover et al. 2011b], and even predicting the voting intentions or outcome of elections [Tumasjan et al. 2010; Bermingham and Smeaton 2011; Lampos et al. 2013]. There are other subtasks in opinion mining related to stance classification, such as biased language detection [Recasens et al. 2013; Yano et al. 2010], perspective identification [Lin et al. 2006] and user classification based on their views [Kato et al. 2008]. Perspective identification was defined as the subjective evaluation of points of view [Lin et al. 2006]. Deng et al. [2014] suggested an unsupervised framework to detect implicit sentiment by inference over explicit sentiments and events that positively or negatively affect the theme.", "startOffset": 0, "endOffset": 3076}, {"referenceID": 10, "context": "It has received a lot of attention in the past decade, and we refer the reader to surveys [Androutsopoulos and Malakasiotis 2010; Dagan et al. 2013] and proceedings of recent challenges on recognizing textual entailment [Bentivogli et al.", "startOffset": 90, "endOffset": 148}, {"referenceID": 3, "context": "2013] and proceedings of recent challenges on recognizing textual entailment [Bentivogli et al. 2011; Marelli et al. 2014; Dzikovska et al. 2016].", "startOffset": 77, "endOffset": 145}, {"referenceID": 30, "context": "2013] and proceedings of recent challenges on recognizing textual entailment [Bentivogli et al. 2011; Marelli et al. 2014; Dzikovska et al. 2016].", "startOffset": 77, "endOffset": 145}, {"referenceID": 13, "context": "2013] and proceedings of recent challenges on recognizing textual entailment [Bentivogli et al. 2011; Marelli et al. 2014; Dzikovska et al. 2016].", "startOffset": 77, "endOffset": 145}, {"referenceID": 33, "context": "This approach is widely used in automatic relation extraction, where information about pairs of related entities can be acquired from external knowledge sources such as Freebase or Wikipedia [Craven and Kumlien 1999; Mintz et al. 2009].", "startOffset": 191, "endOffset": 235}, {"referenceID": 16, "context": "In sentiment and emotion analysis, weakly labeled data can be accumulated by using sentiment clues provided by the authors of the text\u2013clues like emoticons and hashtags [Go et al. 2009; Mohammad 2012].", "startOffset": 169, "endOffset": 200}, {"referenceID": 29, "context": "Recently, distant supervision has been applied to topic classification [Husby and Barbosa 2012; Magdy et al. 2015], named entity recognition [Ritter et al.", "startOffset": 71, "endOffset": 114}, {"referenceID": 49, "context": "2015], named entity recognition [Ritter et al. 2011], event extraction [Reschke et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 48, "context": "2011], event extraction [Reschke et al. 2014], and semantic parsing [Parikh et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 43, "context": "2014], and semantic parsing [Parikh et al. 2015].", "startOffset": 28, "endOffset": 48}], "year": 2016, "abstractText": "We can often detect from a person\u2019s utterances whether he/she is in favor of or against a given target entity\u2014 their stance towards the target. However, a person may express the same stance towards a target by using negative or positive language. Here for the first time we present a dataset of tweet\u2013target pairs annotated for both stance and sentiment. The targets may or may not be referred to in the tweets, and they may or may not be the target of opinion in the tweets. Partitions of this dataset were used as training and test sets in a SemEval-2016 shared task competition. We propose a simple stance detection system that outperforms submissions from all 19 teams that participated in the shared task. Additionally, access to both stance and sentiment annotations allows us to explore several research questions. We show that while knowing the sentiment expressed by a tweet is beneficial for stance classification, it alone is not sufficient. Finally, we use additional unlabeled data through distant supervision techniques and word embeddings to further improve stance classification.", "creator": "LaTeX with hyperref package"}}}