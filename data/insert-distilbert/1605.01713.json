{"id": "1605.01713", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Not Just a Black Box: Learning Important Features Through Propagating Activation Differences", "abstract": "the purported \" fuzzy black box \" nature modification of neural networks is a barrier to adoption in applications where interpretability is essential. here we present deeplift ( learning important features ), an efficient and effective method for computing importance scores in a neural network. deeplift compares the differential activation of each neuron to gain its'reference activation'and actively assigns contribution scores according to setting the difference. we broadly apply deeplift regression to models trained on processed natural images and genomic data, and show significant advantages over gradient - based methods.", "histories": [["v1", "Thu, 5 May 2016 19:52:32 GMT  (1441kb,D)", "http://arxiv.org/abs/1605.01713v1", "6 pages, 3 figures, a version of this is under review for the ICML Workshop on Human Interpretability in Machine Learning"], ["v2", "Sun, 8 May 2016 21:34:42 GMT  (1441kb,D)", "http://arxiv.org/abs/1605.01713v2", "6 pages, 3 figures, a version of this is under review for the ICML Workshop on Human Interpretability in Machine Learning"], ["v3", "Tue, 11 Apr 2017 15:58:48 GMT  (1624kb,D)", "http://arxiv.org/abs/1605.01713v3", "6 pages, 3 figures, this is an older version; seethis https URLfor the newer version"]], "COMMENTS": "6 pages, 3 figures, a version of this is under review for the ICML Workshop on Human Interpretability in Machine Learning", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["avanti shrikumar", "peyton greenside", "anna shcherbina", "anshul kundaje"], "accepted": false, "id": "1605.01713"}, "pdf": {"name": "1605.01713.pdf", "metadata": {"source": "META", "title": "Not Just A Black Box:  Interpretable Deep Learning by Propagating Activation Differences", "authors": ["Avanti Shrikumar", "Anna Y. Shcherbina", "Anshul Kundaje"], "emails": ["(avanti@stanford.edu),", "(pgreens@stanford.edu)", "(annashch@stanford.edu),", "(akundaje@stanford.edu)"], "sections": [{"heading": "1. Introduction", "text": "As neural networks become increasingly popular, their \u201cblack box\u201d reputation is a barrier to adoption when interpretability is paramount. Understanding the features that lead to a particular output builds trust with users and can lead to novel scientific discoveries. Simonyan et al. (2013) proposed using gradients to generate saliency maps and showed that this is a generalization of the deconvolutional nets of Zeiler et al. (2013). Guided backpropagation (Springenberg et al. 2014) is another variant which only considers gradients that have positive error signal. As shown in Figure 2, saliency maps can be substantially improved by simply multiplying the gradient with the input signal, which corresponds to a first-order Taylor approximation of how the output would change if the input were set to zero; the layer-wise relevance propagation rules described in Samek et al. (2015) reduce to this approach, assuming bias terms are included in the denominators.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nGradient-based approaches are problematic because activation functions such as Rectified Linear Units (ReLUs) have a gradient of zero when they are not firing, and yet a ReLU that does not fire can still carry information (Figure 1). Similarly, sigmoid or tanh activations are popular choices for the activation functions of gates in memory units of recurrent neural networks such as GRUs and LSTMs (Chung et al. 2014; Hochreiter and Schmidhuber 1997), but these activations have a near-zero gradient at high or low inputs even though such inputs can be very significant.\nWe present DeepLIFT, a method for assigning feature importance that compares a neuron\u2019s activation to its \u2018reference\u2019, where the reference is the activation that the neuron has when the network is provided a \u2018reference input\u2019 (the reference input is defined according to what is appropriate for the task at hand). This addresses the limitation of gradient-based approaches because the difference from the reference may be non-zero even when the gradient is zero."}, {"heading": "2. DeepLIFT Method", "text": "We denote the contribution of x to y as Cxy . Let the activation of a neuron n be denoted as An. Further, let the reference activation of neuron n be denoted A0n, and let the An \u2212 A0n be denoted as \u03b4n. We define our contributions Cxy to satisfy the following properties.\nar X\niv :1\n60 5.\n01 71\n3v 1\n[ cs\n.L G\n] 5\nM ay"}, {"heading": "2.1. Summation to \u03b4", "text": "For any set of neurons S whose activations are minimally sufficient to compute the activation of y (that is, if we know the activations of S, we can compute the activation of y, and there is no set S\u2032 \u2282 S such that S\u2032 is sufficient to compute the activation of y - in layman\u2019s terms, S is a full set of non-redundant inputs to y), the following property holds: \u2211\ns\u2208S Csy = \u03b4y (1)\nThat is, the sum over all the contributions of neurons in S to y equals the difference-from-reference of y."}, {"heading": "2.2. Linear composition", "text": "Let Ox represent the output neurons of x. The following property holds:\nCxy = \u2211 o\u2208Ox Cxo \u03b4o Coy (2)\nIn layman\u2019s terms, each neuron \u2018inherits\u2019 a contribution through its outputs in proportion to how much that neuron contributes to the difference-from-reference of the output."}, {"heading": "2.3. Backpropagation Rules", "text": "We show that the contributions as defined above can be computed using the following rules (which can be implemented to run on a GPU). The computation is reminiscent of the chain rule used during gradient backpropagation, as equation 2 makes it possible to start with contribution scores of later layers and use them to find the contribution scores of preceding layers. To avoid issues of numerical stability when \u03b4n for a particular neuron is small, rather than computing the contribution scores explicitly, we instead compute multipliers mxy that, when multiplied with the difference-from-reference, give the contribution:\nmxy\u03b4x = Cxy (3)\nLet t represent the target neuron that we intend to compute contributions to, and let Ox represent the set of outputs of x. We show that:\nmxt = \u2211 y\u2208Ox mxymyt (4)\nThe equation above follows from the linear composition property and the definition of the multipliers, as proved be-\nlow:\nCxt = \u2211 y\u2208Ox Cxy \u03b4y Cyt\nmxt\u03b4x = \u2211 y\u2208Ox Cxy \u03b4y (myt\u03b4y) = \u2211 y\u2208Ox Cxymyt\nmxt = \u2211 y\u2208Ox Cxy \u03b4x myt = \u2211 y\u2208Ox mxymyt\n(5)\nIn the equations below, Iy denotes the set of inputs of y."}, {"heading": "2.3.1. AFFINE FUNCTIONS", "text": "Let\nAy = \u2211 x\u2208Iy wxyAx + b (6) Then mxy = wxy\nProof. We show that \u03b4y = \u2211\nx\u2208Iy mxy\u03b4x.\nUsing the fact that An = A0n + \u03b4n, we have:\n(A0y + \u03b4y) = \u2211 x\u2208Iy wxy(A 0 x + \u03b4x) + b =\n\u2211 x\u2208Iy wxyA 0 x + b+ \u2211 x\u2208Iy wxy\u03b4x (7)\nWe also note that the reference activation A0y can be found as follows:\nA0y = \u2211 x\u2208Iy wxyA 0 x + b (8) Thus, canceling out A0y yields:\n\u03b4y = \u2211 x\u2208Iy wxy\u03b4x = \u2211 x\u2208Iy mxy\u03b4x (9)"}, {"heading": "2.3.2. MAX OPERATION", "text": "We consider the case of max operation such as a maxpool:\nAy = max x\u2208Iy Ax (10)\nThen we have:\nmxy = 1{Ax = Ay} \u03b4y \u03b4x\n(11)\nWhere 1{} is the indicator function. If a symbolic computation package is used, then the gradient of y with respect to x can be used in place of 1{Ax = Ay}.\nProof. \u2211 x\u2208y mxy\u03b4x = (\u2211 x\u2208y 1{Ax = Ay} \u03b4y \u03b4x ) \u03b4x\n= \u2211 x\u2208y 1{Ax = Ay}\u03b4y = \u03b4y (12)"}, {"heading": "2.3.3. MAXOUT UNITS", "text": "A maxout function has the form\nAy = n\nmax i=1 (\u2211 x wixyAx ) + bi (13)\ni.e. it is the max over n affine functions of the input vector ~x. For a given vector of activations A~x of the inputs, we split A~x \u2212 A0~x into segments such that over each segment s, a unique affine function dominates the maxout and the coefficient of an individual input x over that segment is w(s)xy . Let l(s) denote the fraction ofA~x\u2212A0~x in segment s. We have:\nmxy = \u2211 s l(s)w(s)xy (14)\nIntuitively speaking, we simply split the piecewise-linear maxout function into regions where it is linear, and do a weighted sum of the coefficients of x in each region according to how much of A~x \u2212A~x falls in that region."}, {"heading": "2.3.4. OTHER ACTIVATIONS", "text": "The following choice for mxy , which is the same for all inputs to y, satisfies summation-to-delta:\nmxy = \u03b4y\u2211\nx\u2032\u2208Iy \u03b4x\u2032 (15)\nThis rule may be used for nonlinearities like ReLUs, PReLUs, sigmoid and tanh (where y has only one input). Situations where the denominator is near zero can be handled by applying L\u2019hopital\u2019s rule, because by definition:\n\u03b4y \u2192 0 as \u2211 x\u2208Iy \u03b4x \u2192 0 (16)"}, {"heading": "2.3.5. ELEMENT-WISE PRODUCTS", "text": "Consider the function:\nAy = A 0 y + \u03b4y = (A 0 x1 + \u03b4x1)(A 0 x2 + \u03b4x2) (17)\nWe have:\n\u03b4y = (A 0 x1 + \u03b4x1)(A 0 x2 + \u03b4x2)\u2212 (A 0 x1A 0 x2)\n= A0x1\u03b4x2 +A 0 x2\u03b4x1 + \u03b4x1\u03b4x2\n= \u03b4x1 ( A0x2 + \u03b4x2 2 ) + \u03b4x2 ( A0x1 + \u03b4x1 2 ) (18) Thus, viable choices for the multipliers are mx1y = A 0 x2 + 0.5\u03b4x2 and mx2y = A 0 x1 + 0.5\u03b4x1"}, {"heading": "2.4. A note on final activation layers", "text": "Activation functions such as a softmax or a sigmoid have a maximum \u03b4 of 1.0. Due to the summation to \u03b4 property, the contribution scores for individual features are lower when there are several redundant features present. As an example, consider At = \u03c3(Ay) (where sigma is the sigmoid transformation) and Ay = Ax1 +Ax2 . Let the default activations of the inputs be A0x1 = A 0 x2 = 0. When x1 = 100 and x2 = 0, we have Cx1t = 0.5. However, when both x1 = 100 and x2 = 100, we have Cx1t = Cx2t = 0.25. To avoid this attenuation of contribution in the presence of redundant inputs, we can use the contributions to y rather than t; in both cases, Cx1y = 100."}, {"heading": "2.5. A note on Softmax activation", "text": "Let t1, t2...tn represent the output of a softmax transformation on the nodes y1, y2...yn, such that:\nAti = eAyi\u2211n\ni\u2032=1 e Ay\u2032 i\n(19)\nHere, Ay1 ...Ayn are affine functions of their inputs. Let x represent a neuron that is an input to Ay1 ...Ayn , and let wxyi represent the coefficient of Ax in Ayi . Because Ay1 ...Ayn are followed by a softmax transformation, if wxyi is the same for all yi (that is, x contributes equally to all yi), then x effectively has zero contribution to Ati . This can be observed by substituting Ayi = wxyiAx +ryi in the expression for Ati and canceling out e\nwxyiAx (here, ryi is the sum of all the remaining terms in the affine expression for Ayi )\nAti = eAyi\u2211n\ni\u2032=1 e Ay\u2032 i\n= ewxyiAx+ryi\u2211n\ni\u2032=1 e wxy i\u2032 Ax+ry i\u2032\n= ewxyiAx+ryi\u2211n\ni\u2032=1 e wxyiAx+ryi\u2032\n= eryi\u2211n\ni\u2032=1 e ry i\u2032\n(20)\nAs mentioned in the previous subsection, in order to avoid attenuation of signal for highly confident predictions, we should compute Cxyi rather than Cxti . One way to ensure that Cxyi is zero if wxyi is the same for all yi is to meannormalized the weights as follows:\nw\u0304xyi = wxyi \u2212 1\nn n\u2211 i\u2032=1 wxyi\u2032 (21)\nThis transformation will not affect the output of the softmax, but will ensure that the DeepLIFT scores are zero when a particular node contributes equally to all softmax classes."}, {"heading": "2.6. Weight normalization for constrained inputs", "text": "Let y be a neuron with some subset of inputs Sy that are constrained such that \u2211 x\u2208Sy Ax = c (for example, one-hot\nencoded input satisfies the constraint \u2211\nx\u2208Sy Ax = 1, and a convolutional neuron operating on one-hot encoded rows has one constraint per column that it sees). Let the weights from x to y be denoted wxy and let by be the bias of y. It is advisable to use normalized weights w\u0304xy = wxy \u2212 \u00b5 and bias b\u0304y = by + c\u00b5, where \u00b5 is the mean over all wxy . We note that this maintains the output of the neural net because, for any constant \u00b5:\nAy = (\u2211 Ax(w\u0304xy \u2212 \u00b5) ) + (by + c\u00b5)\n= (\u2211\nAxwxy\n) \u2212 (\u2211 Ax\u00b5 ) + (by + c\u00b5)\n= (\u2211\nAxwxy ) \u2212 c\u00b5+ (by + c\u00b5)\n= (\u2211\nAxwxy ) + by\n(22)\nThe normalization is desirable because, for affine functions, the multipliers mxy are equal to the weights wxy and are thus sensitive to \u00b5. To take the example of a convolutional neuron operating on one-hot encoded rows: by mean-normalizing wxy for each column in the filter, one can ensure that the contributions Cxy from some columns are not systematically overestimated or underestimated relative to the contributions from other columns."}, {"heading": "3. Results", "text": ""}, {"heading": "3.1. Tiny ImageNet", "text": "A model with the VGG16 (Long et al., 2015) architecture was trained using the Keras framework (Chollet, 2015) on a scaled-down version of the Imagenet dataset, dubbed \u2018Tiny Imagenet\u2019. The images were 64 \u00d7 64 in dimension and belonged to one of 200 output classes. Results shown in Figure 2; the reference input was an input of all zeros after preprocessing."}, {"heading": "3.2. Genomics", "text": "We apply DeepLIFT to models trained on genomic sequence. The positive class requires that the DNA patterns \u2019GATA\u2019 and \u2019CAGATG\u2019 appear in the length-200 sequence together. The negative class has only one of the two patterns appearing once or twice. Outside the core patterns (which were sampled from a generative model) we randomly sample the four bases A, C, G and T. A CNN was trained using the Keras framework (Chollet, 2015) on one-hot encoded sequences with 20 convolutional filters of length 15 and stride 1 and a max pool layer of width and stride 50, followed by two fully connected layers of size 200. PReLU nonlinearities were used for the hidden layers. This model performs well with auROC of 0.907. The misclassified examples primarily occur when one of the patterns erroneously arises in the randomly sampled background.\nWe then run DeepLIFT to assign an importance score to each base in the correctly predicted sequences (the reference input for DeepLIFT was an input of all zeros post weight-normalization (see 2.6) of the first conv layer) and compared the results to the gradient*input (Figure 3)."}, {"heading": "3.3. Discussion", "text": "Prevailing feature importance methods such as the saliency maps of Simonyan et al. (which generalize the deconvolutional nets of Zeiler et al.) and the guided backpropagation of Springenberg et al. are variants of computing gradients. As shown in Figure 1, this can give misleading results when the local gradient is zero. DeepLIFT instead considers the deviation from a neuron\u2019s reference activity. This makes it capable of handling RNN memory units gated by activations that have vanishing gradients (eg: sigmoid, tanh).\nLayer-wise Relevance Propagation (LRP), described in Samek et al. and first proposed by Bach et al. (2015), does not obviously rely on gradients; however, it can be shown that if bias terms are included in the relevance propaga-\ntion and all activations are piecewise linear, LRP reduces to gradient*input (a first-order Taylor approximation of the change in output if the input is set to zero). If all reference activations are zero (as happens when all bias terms are zero and all reference input values are zero), DeepLIFT and LRP give similar results (except that by computing contributions using multipliers, DeepLIFT circumvents the numerical stability problems that LRP faces). In practice, biases are often non-zero, which is why DeepLIFT produces superior results (Figures 2 & 3)."}, {"heading": "3.4. Equivalence of grad*input to Layer-wise Relevance Propagation", "text": "We show when all activations are piecewise linear and bias terms are included in the calculation, the Layer-wise Relevance Propagation (LRP) of Bach et al., reduces to gradient*input. We refer to Samek et al. (2015) for the concise description of LRP:\nUnpooling: \u201cThe backwards signal is redirected proportionally onto the location for which the activation was recorded in the forward pass\u201d: This is trivially the same as gradient*input, because the gradient*input will be zero for all locations which do not activation the pooling layer, and equal to the output for the location that does.\nFiltering: We consider the first rule described in Samek et al., where zij = a (l) i w (l,l+1) ij is the weighted activation of neuron i onto neuron j in the next layer, and l is the index of the layer:\nR (l) i = \u2211 j zij\u2211\u2032 i zi\u2032j + sign( \u2211 i\u2032 zi\u2032j) R (l+1) j (23)\nThe term involving is included to avoid issues of numerical instability when \u2211\u2032 i zi\u2032j is near zero. The second rule described Samek et al. is another variant designed to address the problem of numerical instability. We show that gradient*input gives the exact result as \u2192 0 (i.e. it solves the issue of numerical instability altogether).\nDropping the term for and substituting zij = a (l) i w (l,l+1) ij , we have:\nR ( il) = \u2211 j a (l) i w (l,l+1) ij\u2211\u2032 i a (l) i\u2032 w (l,l+1) i\u2032j R (l+1) j (24)\nAssuming the bias term is included (which would be necessary for the conservation property described in Bach et al. to hold), the denominator is simply the activation of neuron j, i.e.:\nR ( il) = \u2211 j a (l) i w (l,l+1) ij a (l+1) j R (l+1) j (25)\nLet us now consider what happens when there are two filtering operations applied sequentially. Let Rik denote the\nrelevance inherited by neuron i in layer l from neuron k in layer l + 2, passing through the neurons in layer l + 1. We have:\nR (l) ik = \u2211 j a (l) i w (l,l+1) ij a (l+1) j a (l+1) j w (l+1,l+2) jk a (l+1) k R (l+2) k\n= \u2211 j a (l) i w (l,l+1) ij w (l+1,l+2) jk a (l+1) k R (l+2) k\n(26)\nThus, we see that denominator a(l+1)j for the intermediate layer cancelled out, leaving us with a(l)i w (l,l+1) ij w (l+1,l+2) jk , where w(l,l+1)ij w (l+1,l+2) jk is the gradient of a (l+1) k with respect to a(l)i . The only term left in the denominator is the activation of the last layer, a(l+1)k ; if we set the relevance of neurons in the final layer to be equal to their own activation, then R(l+2)k (assuming k is the last layer) would cancel out a (l+1) k in the denominator, leaving us with:\nR (l) ik = \u2211 j a (l) i w (l,l+1) ij w (l+1,l+2) jk (27)\nWhich is simply equal to the activation a(l)i multiplied by the gradient of a(l)i with respect to ak. In situations where the relevance of the last layer is not the same as its activation (which may happen if there is a nonlinear transformation such as a sigmoid), one can simply compute gradient*input with respect to the linear term before the final nonlinearity (which is what we did; for softmax layers, we apply the normalization described in 2.5).\nNonlinearity: \u201cThe backward signal is simply propagated onto the lower layer, ignoring the rectification operation\u201d: While this is not obviously the same as gradient*input, it should be noted that when a rectified linear unit is inactive, it has an activation of zero and the rule for filtering (described above) would assign it zero importance. Furthermore, when the rectified linear unit is active, its gradient is 1. Thus, when the unit is inactive, gradient*input is 0 and LRP assigns 0 signal; when a unit is active, gradient*input is equal to the output and LRP assigns all signal. The two approaches converge."}, {"heading": "4. Author contributions", "text": "AS & PG conceived of DeepLIFT. AS implemented DeepLIFT in software. PG led application to genomics. AYS led application to Tiny Imagenet. AK provided guidance and feedback. AS, PG, AYS & AK prepared the manuscript."}], "references": [{"title": "On Pixel-Wise explanations for Non-Linear classifier decisions by Layer-Wise relevance propagation", "author": ["Bach", "Sebastian", "Binder", "Alexander", "Montavon", "Gr\u00e9goire", "Klauschen", "Frederick", "M\u00fcller", "Klaus-Robert", "Samek", "Wojciech"], "venue": "PLoS One, 10(7):e0130140,", "citeRegEx": "Bach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["S Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., 9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Long", "Jonathan", "Shelhamer", "Evan", "Darrell", "Trevor"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Long et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Long et al\\.", "year": 2015}, {"title": "Evaluating the visualization of what a deep neural network has learned", "author": ["Samek", "Wojciech", "Binder", "Alexander", "Montavon", "Gr\u00e9goire", "Bach", "Sebastian", "M\u00fcller", "Klaus-Robert"], "venue": null, "citeRegEx": "Samek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Samek et al\\.", "year": 2015}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Simonyan", "Karen", "Vedaldi", "Andrea", "Zisserman", "Andrew"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Springenberg", "Jost Tobias", "Dosovitskiy", "Alexey", "Brox", "Thomas", "Riedmiller", "Martin"], "venue": null, "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "In Computer vision\u2013", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}, {"title": "Deconvolutional networks", "author": ["Zeiler", "Matthew D", "Krishnan", "Dilip", "Taylor", "Graham W", "Fergus", "Rob"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zeiler et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Guided backpropagation (Springenberg et al. 2014) is another variant which only considers gradients that have positive error signal.", "startOffset": 23, "endOffset": 49}, {"referenceID": 4, "context": "Simonyan et al. (2013) proposed using gradients to generate saliency maps and showed that this is a generalization of the deconvolutional nets of Zeiler et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Simonyan et al. (2013) proposed using gradients to generate saliency maps and showed that this is a generalization of the deconvolutional nets of Zeiler et al. (2013). Guided backpropagation (Springenberg et al.", "startOffset": 0, "endOffset": 167}, {"referenceID": 4, "context": "As shown in Figure 2, saliency maps can be substantially improved by simply multiplying the gradient with the input signal, which corresponds to a first-order Taylor approximation of how the output would change if the input were set to zero; the layer-wise relevance propagation rules described in Samek et al. (2015) reduce to this approach, assuming bias terms are included in the denominators.", "startOffset": 298, "endOffset": 318}, {"referenceID": 1, "context": "Similarly, sigmoid or tanh activations are popular choices for the activation functions of gates in memory units of recurrent neural networks such as GRUs and LSTMs (Chung et al. 2014; Hochreiter and Schmidhuber 1997), but these activations have a near-zero gradient at high or low inputs even though such inputs can be very significant.", "startOffset": 165, "endOffset": 217}, {"referenceID": 2, "context": "Similarly, sigmoid or tanh activations are popular choices for the activation functions of gates in memory units of recurrent neural networks such as GRUs and LSTMs (Chung et al. 2014; Hochreiter and Schmidhuber 1997), but these activations have a near-zero gradient at high or low inputs even though such inputs can be very significant.", "startOffset": 165, "endOffset": 217}, {"referenceID": 3, "context": "Tiny ImageNet A model with the VGG16 (Long et al., 2015) architecture was trained using the Keras framework (Chollet, 2015) on a scaled-down version of the Imagenet dataset, dubbed \u2018Tiny Imagenet\u2019.", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "and first proposed by Bach et al. (2015), does not obviously rely on gradients; however, it can be shown that if bias terms are included in the relevance propaga-", "startOffset": 22, "endOffset": 41}, {"referenceID": 0, "context": "Equivalence of grad*input to Layer-wise Relevance Propagation We show when all activations are piecewise linear and bias terms are included in the calculation, the Layer-wise Relevance Propagation (LRP) of Bach et al., reduces to gradient*input. We refer to Samek et al. (2015) for the concise description of LRP: Unpooling: \u201cThe backwards signal is redirected proportionally onto the location for which the activation was recorded in the forward pass\u201d: This is trivially the same as gradient*input, because the gradient*input will be zero for all locations which do not activation the pooling layer, and equal to the output for the location that does.", "startOffset": 206, "endOffset": 278}], "year": 2016, "abstractText": "The purported \u201cblack box\u201d nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its \u2018reference activation\u2019 and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.", "creator": "LaTeX with hyperref package"}}}