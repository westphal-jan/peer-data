{"id": "1605.02019", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2016", "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "abstract": "distributed dense word vectors have been shown to be effective at capturing token - level semantic and syntactic regularities in language, while specific topic models can form rich interpretable protein representations over documents. in this work, we describe lda2vec, a model that learns dense tensor word vectors jointly with dirichlet - free distributed latent document - level mixtures of topic vectors. ; in contrast to continuous matrix dense document representations, this formulation produces sparse, interpretable document mixtures through a non - negative definite simplex constraint. our method is simple to incorporate into existing automatic differentiation tree frameworks and allows for unsupervised document representations somewhat geared explicitly for use physically by scientists while simultaneously learning word vectors readily and the linear relationships between them.", "histories": [["v1", "Fri, 6 May 2016 18:13:18 GMT  (175kb,D)", "http://arxiv.org/abs/1605.02019v1", "Submitted to CoNLL 2016"]], "COMMENTS": "Submitted to CoNLL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christopher e moody"], "accepted": false, "id": "1605.02019"}, "pdf": {"name": "1605.02019.pdf", "metadata": {"source": "CRF", "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec", "authors": ["Christopher Moody"], "emails": ["chrisemoody@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Topic models are popular for their ability to organize document collections into a smaller set of prominent themes. In contrast to dense distributed representations, these document and topic representations are generally accessible to humans and more easily lend themselves to being interpreted. This interpretability provides additional options to highlight the patterns and structures within our systems of documents. For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al., 2015). See Blei et al. (2010) for an overview of topic modelling in domains as diverse as computer vision, genetic markers, survey data, and social network data.\nar X\niv :1\n60 5.\n02 01\n9v 1\n[ cs\n.C L\n] 6\nM ay\n2 01\n6\nDense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al. (2016) construct contextual LSTMs that predict proceeding sentence features. Probabilistic topic models tend to form documents as a sparse mixed-membership of topics while neural network models tend to model documents as dense vectors. By virtue of both their sparsity and low-dimensionality, representations from the former are simpler to inspect and more immediately yield high level intuitions about the underlying system (although not without hazards, see Chang et al. (2009)). This paper explores hybrid approaches mixing sparse document representations with dense word and topic vectors.\nUnfortunately, crafting a new probabilistic topic model requires deriving a new approximation, a procedure which takes substantial expertise and must be customized to every model. As a result, prototypes are time-consuming to develop and changes to model architectures must be carefully considered. However, with modern automatic differentiation frameworks the practitioner can focus development time on the model design rather than the model approximations. This expedites the process of evaluating which model features are relevant. This work takes advantage of the Chainer (Tokui et al., 2015) framework to quickly develop models while also enabling us to utilize GPUs to dramatically improve computational speed.\nFinally, traditional topic models over text do not take advantage of recent advances in distributed word representations which can capture semantically meaningful regularities between tokens. The examination of word co-occurrences has proven to be a fruitful research paradigm. For example, Mikolov et al. (2013) utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus. These vector representations ultimately encode remarkable linearities such as king \u2212 man + woman = queen. In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones. Closely related to the PMI matrix, Pen-\nnington et al. (2014) factorize a large global word count co-occurrence matrix to yield more efficient and slightly more performant computed embeddings than SGNS. Once created, these representations are then useful for information retrieval (Manning et al., 2009) and parsing tasks (Levy and Goldberg, 2014a). In this work, we will take advantage of word-level representations to build document-level abstractions.\nThis paper extends distributed word representations by including interpretable document representations and demonstrate that model inference can be performed and extended within the framework of automatic differentiation."}, {"heading": "2 Model", "text": "This section describes the model for lda2vec. We are interested in modifying the Skipgram Negative-Sampling (SGNS) objective in (Mikolov et al., 2013) to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors. The network architecture is shown in Figure 1.\nThe total loss term L in (1) is the sum of the Skipgram Negative Sampling Loss (SGNS) Lnegij with the addition of a Dirichlet-likelihood term over document weights, Ld that will be discussed later. The loss is conducted using a context vector, ~cj , pivot word vector ~wj , target word vector ~wi, and negatively-sampled word vector ~wl.\nL = Ld + \u03a3ijLnegij (1) Lnegij = log \u03c3(~cj \u00b7 ~wi) + \u03a3 n l=0 log \u03c3(\u2212~cj \u00b7 ~wl)\n(2)"}, {"heading": "2.1 Word Representation", "text": "As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus. In our experiments, the window contains five tokens before and after the pivot token. For every pivot-target pair of words the pivot word is used to predict the nearby target word. Each word is represented with a fixedlength dense distributed-representation vector, but unlike Mikolov et al. (2013) the same word vectors are used in both the pivot and target representations. The SGNS loss shown in (2) attempts to discriminate context-word pairs that appear in the corpus from those randomly sampled from a \u2018negative\u2019 pool of words. This loss is minimized when\nthe observed words are completely separated from the marginal distribution. The distribution from which tokens are drawn is u\u03b2 , where u denotes the overall word frequency normalized by the total corpus size. Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in Mikolov et al. (2013). Note that a distribution of u0.0 would draw negative tokens from the vocabulary with no notion of popularity while a distribution proportional with u1.0 draws from the empirical unigram distribution. Compared to the unigram distribution, the choice of u3/4 slightly emphasizes choosing infrequent words for negative samples. In contrast to optimizing the softmax cross entropy, which requires modelling the overall popularity of each token, negative sampling focuses on learning word vectors conditional on a context by drawing negative samples from each token\u2019s marginal popularity in the corpus."}, {"heading": "2.2 Document Representations", "text": "lda2vec embeds both words and document vectors into the same space and trains both representations simultaneously. By adding the pivot and document vectors together, both spaces are effectively joined. Mikolov et al. (2013) provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both words. For example, the vector representation for Germany + airline is similar to the vector for Lufthansa. We would like to exploit the additive property of word vectors to construct a meaningful sum of word and document vectors. For example, if as lda2vec is scanning a document the jth word is Germany, then neighboring words are predicted to be similar such as France, Spain, and Austria. But if the document is specifically about airlines, then we would like to construct a document vector similar to the word vector for airline. Then instead of predicting tokens similar to Germany alone, predictions similar to both the document and the pivot word can be made such as: Lufthansa, Condor F lugdienst, and Aero Lloyd. Motivated by the meaningful sums of words vectors, in lda2vec the context vector is explicitly designed to be the sum of a document vector and a word vector as in (3):\n~cj = ~wj + ~dj (3)\nThis models document-wide relationships by preserving ~dj for all word-context pairs in a document, while still leveraging local inter-word relationships stemming from the interaction between the pivot word vector ~wj and target word ~wi. The document and word vectors are summed together to form a context vector that intuitively captures long- and short-term themes, respectively. In order to prevent co-adaptation, we also perform dropout on both the unnormalized document vector ~dj and the pivot word vector ~wj (Hinton et al., 2012)."}, {"heading": "2.2.1 Document Mixtures", "text": "If we only included structure up to this point, the model would produce a dense vector for every document. However, lda2vec strives to form interpretable representations and to do so an additional constraint is imposed such that the document representations are similar to those in traditional LDA models. We aim to generate a document vector from a mixture of topic vectors and to do so, we begin by constraining the document vector ~dj to project onto a set of latent topic vectors ~t0, ~t1, ..., ~tk:\n~dj = pj0\u00b7~t0+pj1\u00b7~t1+...+pjk\u00b7~tk+...+pjn\u00b7~tn (4)\nEach weight 0 \u2264 pjk \u2264 1 is a fraction that denotes the membership of document j in the topic k. For example, the Twenty Newsgroups model described later has 11313 documents and n = 20 topics so j = 0...11312, k = 0...19. When the word vector dimension is set to 300, it is assumed that the document vectors ~dj , word vectors ~wi and topic vectors ~tk all have dimensionality 300. Note that the topics ~tk are shared and are a common component to all documents but whose strengths are modulated by document weights pjk that are unique to each document. To aid interpretability, the document memberships are designed to be non-negative, and to sum to unity. To achieve this constraint, a softmax transform maps latent vectors initialized in R300 onto the simplex defined by pjk. The softmax transform naturally enforces the constraint that \u03a3kpjk = 1 and allows us interpret memberships as percentages rather than unbounded weights.\nFormulating the mixture in (4) as a sum ensures that topic vectors ~tk, document vectors ~dj and word vectors ~wi, operate in the same space. As a result, what words ~wi are most similar to\nany given topic vector ~tk can be directly calculated. While each topic is not literally a token present in the corpus, it\u2019s similarity to other tokens is meaningful and can be measured. Furthermore, by examining the list of most similar words one can attempt to interpret what the topic represents. For example, by calculating the most similar token to any topic vector (e.g. argmaxi(~t0 \u00b7 ~wi)) one may discover that the first topic vector ~t0 is similar to the tokens pitching, catcher, and Braves while the second topic vector ~t1 may be similar to Jesus, God, and faith. This provides us the option to interpret the first topic as baseball topic, and as a result the first component in every document proportion pj0 indicates how much document j is in the baseball topic. Similarly, the second topic may be interpreted as Christianity and the second component of any document proportion pj1 indicates the membership of that document in the Christianity topic."}, {"heading": "2.2.2 Sparse Memberships", "text": "Finally, the document weights pij are sparsified by optimizing the document weights with respect to a Dirichlet likelihood with a low concentration parameter \u03b1:\nLd = \u03bb\u03a3jk (\u03b1\u2212 1) log pjk (5)\nThe overall objective in (5) measures the likelihood of document j in topic k summed over all available documents. The strength of this term is modulated by the tuning parameter \u03bb. This simple likelihood encourages the document proportions coupling in each topic to be sparse when \u03b1 < 1 and homogeneous when \u03b1 > 1. To drive interpretability, we are interested in finding sparse memberships and so set \u03b1 = n\u22121 where n is the number of topics. We also find that setting the overall strength of the Dirichlet optimization to \u03bb = 200 works well. Document proportions are initialized to be relatively homogeneous, but as time progresses, the Ld encourages document proportions vectors to become more concentrated (e.g. sparser) over time. In experiments without this sparsity-inducing term (or equivalently when \u03b1 = 1) the document weights pij tend to have probability mass spread out among all elements. Without any sparsity inducing terms the existence of so many non-zero weights makes interpreting the document vectors difficult. Furthermore, we\nfind that the topic basis are also strongly affected, and the topics become incoherent."}, {"heading": "2.3 Preprocessing and Training", "text": "The objective in (1) is trained in individual minibatches at a time while using the Adam optimizer (Kingma and Ba, 2014) for two hundred epochs across the dataset. The Dirichlet likelihood term Ld is typically computed over all documents, so in modifying the objective to minibatches we adjust the loss of the term to be proportional to the minibatch size divided by the size of the total corpus. Our software is open source, available online, documented and unit tested1. Finally, the top ten most likely words in a given topic are submitted to the online Palmetto2 topic quality measuring tool and the coherence measure Cv is recorded. After evaluating multiple alternatives, Cv is the recommended coherence metric in Ro\u0308der et al. (2015). This measure averages the Normalized Pointwise Mutual Information (NPMI) for every pair of words within a sliding window of size 110 on an external corpus and returns mean of the NPMI for the submitted set of words. Token-toword similarity is evaluated using the 3COSMUL measure (Levy and Goldberg, 2014b).\n1The code for lda2vec is available online at https:// github.com/cemoody/lda2vec\n2The online evaluation tool can be accessed at http:// palmetto.aksw.org/palmetto-webapp/"}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Twenty Newsgroups", "text": "This section details experiments in discovering the salient topics in the Twenty Newsgroups dataset, a popular corpus for machine learning on text. Each document in the corpus was posted to one of twenty possible newsgroups. While the text of each post is available to lda2vec, each of the newsgroup partitions is not revealed to the algorithm but is nevertheless useful for post-hoc qualitative evaluations of the discovered topics. The corpus is preprocessed using the data loader available in Scikit-learn (Pedregosa et al., 2012) and tokens are identified using the SpaCy parser (Honnibal and Johnson, 2015). Words are lemmatized to group multiple inflections into single tokens. Tokens that occur fewer than ten times in the corpus are removed, as are tokens that appear to be URLs, numbers or contain special symbols within their orthographic forms. After preprocessing, the dataset contains 1.8 million observations of 8,946 unique tokens in 11,313 documents. Word vectors are initialized to the pretrained values found in Mikolov et al. (2013) but otherwise updates are allowed to these vectors at training time.\nA range of lda2vec parameters are evaluated by varying the number of topics n \u2208 20, 30, 40, 50 and the negative sampling exponent \u03b2 \u2208 0.75, 1.0. The best topic coherences were achieved with n = 20 topics and with negative sampling power \u03b2 = 0.75 as summarized in Figure 2. We briefly experimented with variations on dropout ratios but we did not observe any substantial differences.\nFigure 3 lists four example topics discovered in the Twenty Newsgroups dataset. Each topic is associated with a topic vector that lives in the same space as the trained word vectors and listed are\nthe most similar words to each topic vector. The first topic shown has high similarity to the tokens astronomical, Astronomy, satellite, planetary, and telescope and is thus likely a \u2018Space\u2019-related topic similar to the \u2018sci.space\u2019 newsgroup. The second example topic is similar to words semantically related to \u2018Encryption\u2019, such as Clipper and encrypt, and is likely related to the \u2018sci.crypt\u2019 newsgroup. The third and four example topics are \u2018X Windows\u2019 and \u2018Middle East\u2019 which likely belong to the \u2018comp.windows.x\u2019 and \u2018talk.politics.mideast\u2019 newsgroups."}, {"heading": "3.2 Hacker News Comments corpus", "text": "This section evaluates lda2vec on a very large corpus of Hacker News 3 comments. Hacker News is social content-voting website and community whose focus is largely on technology and entrepreneurship. In this corpus, a single document is composed of all of the words in all comments posted to a single article. Only stories with more than 10 comments are included, and only comments from users with more than 10 comments are included. We ignore other metadata such as votes, timestamps, and author identities. The raw dataset 4 is available for download online. The corpus is nearly fifty times the size of the Twenty Newsgroups corpus which is sufficient for learning a specialized vocabulary. To take advantage of this rich corpus, we use the SpaCy to tokenize whole noun phrases and entities at once (Honnibal and Johnson, 2015). The specific tokenization procedure5 is also available online, as are the pre-\n3See https://news.ycombinator.com/ 4The raw dataset is freely available at https:// zenodo.org/record/45901 5The tokenization procedure is available online at https://github.com/cemoody/lda2vec/blob/ master/lda2vec/preprocess.py\nprocessed datasets 6 results. This allows us to capture phrases such as community policing measure and prominent figures such as Steve Jobs as single tokens. However, this tokenization procedure generates a vocabulary substantially different from the one available in the Palmetto topic coherence tool and so we do not report topic coherences on this corpus. After preprocessing, the corpus contains 75 million tokens in 66 thousand documents with 110 thousand unique tokens. Unlike the Twenty Newsgroups analysis, word vectors are initialized randomly instead of using a library of pretrained vectors.\nWe train an lda2vec model using 40 topics and 256 hidden units and report the learned topics that demonstrate the themes present in the corpus. Furthermore, we demonstrate that word vectors and semantic relationships specific to this corpus are learned.\nIn Figure 4 five example topics discovered by lda2vec in the Hacker News corpus are listed. These topics demonstrate that the major themes of the corpus are reproduced and represented in learned topic vectors in a similar fashion as in LDA (Blei et al., 2003). The first, which we\n6A tokenized dataset is freely available at https:// zenodo.org/record/49899\nhand-label Housing Issues has prominent tokens relating to housing policy issues such as housing supply (e.g. more housing), and costs (e.g. basic income and house prices). Another topic lists major internet portals, such as the privacyconscious search engine \u2018Duck Duck Go\u2019 (in the corpus abbreviated as DDG), as well as other major search engines (e.g. Bing), and home pages (e.g. Google+, and iGoogle). A third topic is that of the popular online curency and payment system Bitcoin, the abbreviated form of the currency btc, and the now-defunct Bitcoin trading platform Mt. Gox. A fourth topic considers salaries and compensation with tokens such as current salary, more equity and vesting, the process by which employees secure stock from their employers. A fifth example topic is that of technological hardware like HDMI and glossy screens and includes devices such as the Surface Pro and Mac Pro.\nFigure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS (Mikolov et al., 2013) but specialized to the Hacker News corpus. Tokens similar to the token Artificial sweeteners include other sugar-related tokens like fructose and food-related tokens such as paleo diet. Tokens similar to Black holes include physics-related concepts such as galaxies and dark\nmatter. The Hacker News corpus devotes a substantial quantity of text to fonts and design, and the words most similar to Comic Sans are other popular fonts (e.g. Times New Roman and Helvetica) as well as font-related concepts such as typeface and serif font. Tokens similar to Functional Programming demonstrate similarity to other computer science-related tokens while tokens similar to San Francisco include other large American cities as well smaller cities located in the San Francisco Bay Area.\nFigure 6 demonstrates that in addition to learning topics over documents and similarities to word tokens, linear regularities between tokens are also learned. The \u2018Query\u2019 column lists a selection of tokens that when combined yield a token vector closest to the token shown in the \u2018Result\u2019 column. The subtractions and additions of vectors are evaluated literally, but instead take advantage of the 3COSMUL objective (Levy and Goldberg, 2014b). The results show that relationships between tokens important to the Hacker News community exists between the token vectors. For example, the vector for Silicon Valley is similar to both California and technology, Bitcoin is indeed a digital currency, Node.js is a technology that enables running Javascript on servers instead of on clientside browsers, Jeff Bezos and Mark Zuckerberg are CEOs of Amazon and Facebook respectively, NLP and computer vision are fields of machine learning research primarily dealing with text and images respectively, Edward Snowden and Julian As-\nsange are both whistleblowers who were primarily located in the United States and Sweden and finally the Kindle and the Surface Pro are both tablets manufactured by Amazon and Microsoft respectively. In the above examples semantic relationships between tokens encode for attributes and features including: location, currencies, server v.s. client, leadership figures, machine learning fields, political figures, nationalities, companies and hardware."}, {"heading": "3.3 Conclusion", "text": "This work demonstrates a simple model, lda2vec, that extends SGNS (Mikolov et al., 2013) to build unsupervised document representations that yield coherent topics. Word, topic, and document vectors are jointly trained and embedded in a common representation space that preserves semantic regularities between the learned word vectors while still yielding sparse and interpretable documentto-topic proportions in the style of LDA (Blei et al., 2003). Topics formed in the Twenty Newsgroups corpus yield high mean topic coherences which have been shown to correlate with human evaluations of topics (Ro\u0308der et al., 2015). When applied to a Hacker News comments corpus, lda2vec discovers the salient topics within this community and learns linear relationships between words that allow it solve word analogies in the specialized vocabulary of this corpus. Finally, we note that our method is simple to implement in automatic differentiation frameworks and can lead to more readily interpretable unsupervised representations."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "The Journal of Machine Learning Research, 3:993\u20131022, March.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Probabilistic Topic Models", "author": ["David Blei", "Lawrence Carin", "David Dunson."], "venue": "IEEE Signal Processing Magazine, 27(6):55\u201365.", "citeRegEx": "Blei et al\\.,? 2010", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["J Chang", "S Gerrish", "C Wang."], "venue": "Advances in . . . .", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "Dynamic Poisson Factorization", "author": ["Laurent Charlin", "Rajesh Ranganath", "James McInerney", "David M Blei."], "venue": "ACM, September.", "citeRegEx": "Charlin et al\\.,? 2015", "shortCiteRegEx": "Charlin et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing coadaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv.org, July.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "An Improved Non-monotonic Transition System for Dependency Parsing", "author": ["Matthew Honnibal", "Mark Johnson."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1373\u20131378, Stroudsburg,", "citeRegEx": "Honnibal and Johnson.,? 2015", "shortCiteRegEx": "Honnibal and Johnson.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv.org, December.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Skip-Thought Vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in Neural . . . , pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "ICML, pages 1188\u20131196.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "DependencyBased Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308, Stroudsburg, PA, USA. Association for", "citeRegEx": "Levy and Goldberg.,? 2014a", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "CoNLL, pages 171\u2013180.", "citeRegEx": "Levy and Goldberg.,? 2014b", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Advances in Neural Information Processing . . . , pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg.,? 2014c", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Schutze."], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Manning et al\\.,? 2009", "shortCiteRegEx": "Manning et al\\.", "year": 2009}, {"title": "Inferring Networks of Substitutable and Complementary Products", "author": ["Julian McAuley", "Rahul Pandey", "Jure Leskovec."], "venue": "ACM, New York, New York, USA, August.", "citeRegEx": "McAuley et al\\.,? 2015", "shortCiteRegEx": "McAuley et al\\.", "year": 2015}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Scikit-learn: Machine Learning in Python", "author": ["Brucher", "Matthieu Perrot", "\u00c9douard Duchesnay."], "venue": "arXiv.org, January.", "citeRegEx": "Brucher et al\\.,? 2012", "shortCiteRegEx": "Brucher et al\\.", "year": 2012}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Strouds-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Exploring the Space of Topic Coherence Measures", "author": ["Michael R\u00f6der", "Andreas Both", "Alexander Hinneburg."], "venue": "ACM, February.", "citeRegEx": "R\u00f6der et al\\.,? 2015", "shortCiteRegEx": "R\u00f6der et al\\.", "year": 2015}, {"title": "Chainer: a Next-Generation Open Source Framework for Deep Learning", "author": ["S Tokui", "K Oono", "S Hido", "CA San Mateo", "J Clayton."], "venue": "learningsys.org.", "citeRegEx": "Tokui et al\\.,? 2015", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al.", "startOffset": 111, "endOffset": 130}, {"referenceID": 3, "context": ", 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al.", "startOffset": 35, "endOffset": 57}, {"referenceID": 13, "context": ", 2015), and infer networks of complementary products (McAuley et al., 2015).", "startOffset": 54, "endOffset": 76}, {"referenceID": 0, "context": "For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al., 2015). See Blei et al. (2010) for an overview of topic modelling in domains as diverse as computer vision, genetic markers, survey data, and social network data.", "startOffset": 112, "endOffset": 274}, {"referenceID": 6, "context": "Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 6, "context": "Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al.", "startOffset": 177, "endOffset": 197}, {"referenceID": 6, "context": "Dense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al. (2016) construct contextual LSTMs that predict proceeding sentence features.", "startOffset": 177, "endOffset": 310}, {"referenceID": 2, "context": "By virtue of both their sparsity and low-dimensionality, representations from the former are simpler to inspect and more immediately yield high level intuitions about the underlying system (although not without hazards, see Chang et al. (2009)).", "startOffset": 224, "endOffset": 244}, {"referenceID": 18, "context": "(Tokui et al., 2015) framework to quickly develop models while also enabling us to utilize GPUs to dramatically improve computational speed.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Once created, these representations are then useful for information retrieval (Manning et al., 2009) and parsing tasks (Levy and Goldberg, 2014a).", "startOffset": 78, "endOffset": 100}, {"referenceID": 9, "context": ", 2009) and parsing tasks (Levy and Goldberg, 2014a).", "startOffset": 26, "endOffset": 52}, {"referenceID": 10, "context": "For example, Mikolov et al. (2013) utilize Skipgram NegativeSampling (SGNS) to train word embeddings using word-context pairs formed from windows moving across a text corpus.", "startOffset": 13, "endOffset": 35}, {"referenceID": 9, "context": "In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones.", "startOffset": 9, "endOffset": 35}, {"referenceID": 9, "context": "In fact, Levy and Goldberg (2014c) demonstrate that this is implicitly factorizing a variant of the Pointwise Mutual Information (PMI) matrix that emphasizes predicting frequent co-occurrences over rare ones. Closely related to the PMI matrix, Pennington et al. (2014) factorize a large global word count co-occurrence matrix to yield more efficient and slightly more performant computed embeddings than SGNS.", "startOffset": 9, "endOffset": 269}, {"referenceID": 14, "context": "As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus.", "startOffset": 6, "endOffset": 28}, {"referenceID": 14, "context": "As in Mikolov et al. (2013), pairs of pivot and target words (j, i) are extracted when they cooccur in a moving window scanning across the corpus. In our experiments, the window contains five tokens before and after the pivot token. For every pivot-target pair of words the pivot word is used to predict the nearby target word. Each word is represented with a fixedlength dense distributed-representation vector, but unlike Mikolov et al. (2013) the same word vectors are used in both the pivot and target representations.", "startOffset": 6, "endOffset": 446}, {"referenceID": 14, "context": "Unless stated otherwise, the negative sampling power beta is set to 3/4 and the number of negative samples is fixed to n = 15 as in Mikolov et al. (2013). Note that a distribution of u0.", "startOffset": 132, "endOffset": 154}, {"referenceID": 14, "context": "Mikolov et al. (2013) provide the intuition that word vectors can be summed together to form a semantically meaningful combination of both", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "In order to prevent co-adaptation, we also perform dropout on both the unnormalized document vector ~ dj and the pivot word vector ~ wj (Hinton et al., 2012).", "startOffset": 136, "endOffset": 157}, {"referenceID": 17, "context": "The topic coherence has been demonstrated to correlate with human evaluations of topic models (R\u00f6der et al., 2015).", "startOffset": 94, "endOffset": 114}, {"referenceID": 6, "context": "The objective in (1) is trained in individual minibatches at a time while using the Adam optimizer (Kingma and Ba, 2014) for two hundred epochs", "startOffset": 99, "endOffset": 120}, {"referenceID": 10, "context": "Token-toword similarity is evaluated using the 3COSMUL measure (Levy and Goldberg, 2014b).", "startOffset": 63, "endOffset": 89}, {"referenceID": 14, "context": "After evaluating multiple alternatives, Cv is the recommended coherence metric in R\u00f6der et al. (2015). This measure averages the Normalized Pointwise Mutual Information (NPMI) for every pair of words within a sliding window of size 110 on an external corpus and returns mean of the NPMI for the submitted set of words.", "startOffset": 82, "endOffset": 102}, {"referenceID": 5, "context": ", 2012) and tokens are identified using the SpaCy parser (Honnibal and Johnson, 2015).", "startOffset": 57, "endOffset": 85}, {"referenceID": 14, "context": "Word vectors are initialized to the pretrained values found in Mikolov et al. (2013) but otherwise updates are allowed to these vectors at training time.", "startOffset": 63, "endOffset": 85}, {"referenceID": 5, "context": "To take advantage of this rich corpus, we use the SpaCy to tokenize whole noun phrases and entities at once (Honnibal and Johnson, 2015).", "startOffset": 108, "endOffset": 136}, {"referenceID": 0, "context": "These topics demonstrate that the major themes of the corpus are reproduced and represented in learned topic vectors in a similar fashion as in LDA (Blei et al., 2003).", "startOffset": 148, "endOffset": 167}, {"referenceID": 14, "context": "Figure 5 demonstrates that token similarities are learned in a similar fashion as in SGNS (Mikolov et al., 2013) but specialized to the Hacker News corpus.", "startOffset": 90, "endOffset": 112}, {"referenceID": 10, "context": "The subtractions and additions of vectors are evaluated literally, but instead take advantage of the 3COSMUL objective (Levy and Goldberg, 2014b).", "startOffset": 119, "endOffset": 145}, {"referenceID": 14, "context": "This work demonstrates a simple model, lda2vec, that extends SGNS (Mikolov et al., 2013) to build unsupervised document representations that yield coherent topics.", "startOffset": 66, "endOffset": 88}, {"referenceID": 0, "context": "representation space that preserves semantic regularities between the learned word vectors while still yielding sparse and interpretable documentto-topic proportions in the style of LDA (Blei et al., 2003).", "startOffset": 186, "endOffset": 205}, {"referenceID": 17, "context": "groups corpus yield high mean topic coherences which have been shown to correlate with human evaluations of topics (R\u00f6der et al., 2015).", "startOffset": 115, "endOffset": 135}], "year": 2016, "abstractText": "Distributed dense word vectors have been shown to be effective at capturing tokenlevel semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.", "creator": "TeX"}}}