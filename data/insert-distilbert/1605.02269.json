{"id": "1605.02269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2016", "title": "Predicting Performance on MOOC Assessments using Multi-Regression Models", "abstract": "the past few years has seen the rapid growth of data min - ing approaches for the analysis of data measurements obtained from mas - sive open online courses ( gs moocs ). the objectives of this study are to develop approaches targeted to predict the scores a fellow stu - dent may achieve on a given survey grade - related assessment based on factual information, considered as prior performance assessment or prior ac - notch tivity in the course. we develop a personalized linear mul - tiple regression ( plmr ) model to predict the grade for a student, prior to potentially attempting the assessment activity. the developed model is real - time responsive and constantly tracks the perceived participation of a student within a mooc ( via click - stream server logs ) and predicts the performance of both a student on the next as - averaged sessment within the course offering. we perform quite a com - prehensive set of experiments on data obtained from utilizing three openedx moocs constructed via a stanford visiting university initiative. our experimental results show arguably the promise of the proposed ap - proach in comparison to baseline approaches existing and also helps in identification selection of seven key features that are associated with the study habits and learning behaviors of students.", "histories": [["v1", "Sun, 8 May 2016 04:00:31 GMT  (664kb,D)", "http://arxiv.org/abs/1605.02269v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.CY cs.LG", "authors": ["zhiyun ren", "huzefa rangwala", "aditya johri"], "accepted": false, "id": "1605.02269"}, "pdf": {"name": "1605.02269.pdf", "metadata": {"source": "CRF", "title": "Predicting Performance on MOOC Assessments using Multi-Regression Models", "authors": ["Zhiyun Ren", "George Mason", "Huzefa Rangwala", "Aditya Johri"], "emails": ["zen4@masonlive.gmu.edu", "rangwala@cs.gmu.edu", "johri@gmu.edu"], "sections": [{"heading": null, "text": "Keywords Personalized Linear Multi-Regression Models, MOOC, Performance prediction"}, {"heading": "1. INTRODUCTION", "text": "Since their inception, Massive Open Online Courses (MOOCs) have aimed at delivering online learning on a wide variety of topics to a large number of participants across the world. Due to the low cost (most times zero) and lack of entry barriers (e.g., prerequisites or skill requirements) for the participants, large number of students enroll in MOOCs but only a small fraction of them keep themselves engaged in the learning materials and participate in the various activities associated with the course offering such as viewing the video lectures, studying the material, completing the various quizzes and homework-based assessments.\nGiven, this high attrition rate and potential of MOOCs to deliver low-cost but high quality education, several researchers have analyzed the server logs associated with these MOOCs to determine the factors associated with students dropping out. Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20]. Using self reported surveys, studies have determined the different motivations for students enrolling and participating in a MOOC. Participants enroll in a MOOC sometimes to learn a subset of topics within the curriculum, sometimes to earn degree certificates for future career promotion or college credit, social experience or/and exploration of free online education [10]. Students with similar motivation have different learning outcomes from a MOOC based on the number of invested hours, prior education background, knowledge and skills [6].\nIn this paper, we present models to predict a student\u2019s future performance for a certain assessment activity witin a MOOC. Specifically, we develop an approach based on personalized linear multi-regression (PLMR) to predict the performance of a student as they attempt various graded activities (assessments) within the MOOC. This approach was previously studied within the context of predicting a student\u2019s performance based on graded activities within a traditional university course with data extracted from a learning management system (Moodle) [5]. The developed model is real-time and tracks the participation of a student within a MOOC (via click-stream server logs) and predicts the performance of a student on the next assessment within the course offering. Our approach also allows us to capture the varying studying patterns associated with different students, and responsible for their performance. We evaluate our predictive model on three MOOCs offered using the OpenEdX platform and made available for learning analytics research via the Center for Advanced Research through Online Learning at Stanford University 1.\nWe extract features that seek to identify the learning behavior and study habits for different students. These features capture the various interactions that show engagement, effort, learning and behavior for a given student participating in studying; by viewing the various video and text-based materials available within the MOOC offering coupled with student attempts on graded and non-graded activities like quizzes and homeworks. Our experimental evaluation shows accurate grade prediction for different types of homework as-\n1datastage.stanford.edu\nar X\niv :1\n60 5.\n02 26\n9v 1\n[ cs\n.C Y\n] 8\nM ay\n2 01\n6\nsessments in comparison to baseline models. Our approach also identifies the features found to be useful for predicting an accurate homework grade.\nBaker et. al [1] have presented systems that can adapt based on predictions of future student performance, and they were able to incorporate interventions, which were effective in improving student experiences within Intelligent Tutoring Systems (ITS). Inspired by this prior work, tracking student performance within a MOOC, allows personalized feedback for high performing and low performing students; motivating students to stay on track and achieve their educational goals. It also provides feedback to the MOOC instructor about the usage of different course materials and helps in improving the MOOC offering."}, {"heading": "2. RELATED WORK", "text": "Several researchers have focused on the analysis of education data (including MOOCs), in an effort to understand the characteristics of student learning behaviors and motivation within this education model [13]. Boyer et. al. [2] focus on the stopout prediction problem within MOOCs; by designing a set of processes using information from previous courses and the previous weeks of the current course. Brinton et. al. [3] developed an approach to predict if a student answers a question correct on the first attempt via clickstream information and social learning networks. Kennedy et. al. [9] analyzed the relationship between a student\u2019s prior knowledge on end-of-MOOC performance. Sunar et. al. [14] developed an approach to predict the possible interactions between peers participating in a MOOC.\nMost similar to our proposed work, Bayesian Knowledge Tracing (BKT) [12] has been adapted to predict whether a student can get a MOOC assessment correct or not. BKT was first developed [4] for modeling the evolving knowledge states of students monitored within Intelligent Tutoring Systems (ITS). Pardos et. al. proposed a model\u201cItem Difficulty Effect Model\u201d (IDEM) that incorporates the difficulty levels of different questions and modifies the original BKT by adding an \u201cItem\u201d node to every question node. By identifying the challenges associated with modeling MOOC data, the IDEM approach and extensions that involve splitting questions into several sub-parts and incorporating resource (knowledge) information [11] are considered state-of-the-art MOOC assessment prediction approaches and referred as KT-IDEM. However, this approach can only predict a binary value grade. In contrast, the model proposed in this paper is able to predict both, a continuous and a binary grade.\nWithin learning analytics literature, outside of MOOC analysis, predicting student performance is a popular and extensive topic. Wang et. al. [16] performed a study to predict student\u2019s performance by capturing data relevant to study habits and learning behaviors from their smartphones. Specific examples of data captured include location, time, ambient noise and social activity. Coupled with self-reported information, this work captured the influence of a student\u2019s daily activity on the academic performance. Elbadrawy et. al. [5] proposed the use of personalized linear multi-regression models to predict student performance in a traditional university by extracting data from course man-\nagement systems (Moodle). With a particular membership vector for each student, the model was able to capture personal learning behaviors and outperformed several baseline approaches. Our study focuses on MOOCs, which presents different assumptions, challenges and features in comparison to a traditional university environment."}, {"heading": "3. METHODS", "text": ""}, {"heading": "3.1 Personal Linear Multi-Regression Models", "text": "We train a personalized linear multi-regression (PLMR) model [5] to predict student performance within a MOOC. Specifically, the grade g\u0302s,a for a student s in an assessment activity a is predicted as follows:\ng\u0302s,a = bs + p t sWfsa\n= bs + l\u2211 d=1 (ps,d nF\u2211 k=1 fsa,kwd,k), (1)\nwhere bs is bias term for student s, fsa is the feature vector of an interaction between student s and activity a. The features extracted from the MOOC server logs are described in the next Section. nF is the length of fsa, indicating the dimension of our feature space. l is the number of linear regression models, W is the coefficient matrix of dimensions l \u00d7 nF that holds the coefficients of the l linear regression models, and ps is a vector of length l that holds the memberships of student s within the l different regression models [5]. Using lasso [15], we solve the following optimization problem:\nminimize (W,P,B) L(W,P,B) + \u03b3(\u2016P\u2016F + \u2016W\u2016F ), (2)\nwhere W , P and B denote the feature weights, student memberships and bias terms, respectively. The loss function L(\u00b7) is the least square loss for regression problems. \u03b3(\u2016P\u2016F + \u2016W\u2016F ) is a regularizer that controls the model complexity by controling the values of feature weights and student memberships. Tuning the scalar \u03b3 prevents model from over-fitting."}, {"heading": "3.2 Feature Description", "text": "We extract features from MOOC server logs and formulate the PLMR model to predict real-time assessment grade for a given student. Figure 1 shows the various activities, generally available within a MOOC. Fig 1 (a) shows that each homework has corresponding quizzes, each of which has its corresponding video as resources for learning. Fig 1 (b) shows that while watching a video, a student can have a series of actions. Fig 1 (c) shows that while studying using a MOOC, a student can have several login sessions, each of which may involve watching videos, attempting quizzes and homework related activities. In order to capture the latent information behind the click-stream for each student, we extract six types of features: (i) session features, (ii) quiz related features, (iii) video related features, (iv) homework related features, (v) time related features and (vi) intervalbased features. These features constitute the feature vector fsa for a student and a homework assessment. The description of these features are as follows:\n(i) Session features:. A single study session is defined by a student login combined with the various available study interactions that a student may partake in. Since, students do not always log out of a session, we assume that a \u201cno activity\u201d period of more than one hour constitutes a student logging out of a session. We show a \u201cno activity\u201d period for a student between tarwo consecutive sessions in Fig 1 (c).\n\u2022 NumSession are the the average number of daily study sessions a student engages in, before a homework attempt.\n\u2022 AvgSessionLen is the average length of each session in minutes. We calculate the average study time of a study session by\nAvgSessionLen = Total study time\nNumSession . (3)\n\u2022 AvgNumLogin is the percentage of days before a homework attempt that a student logs in to MOOC (or has a session). Students are free to choose when to login and study in a MOOC environment. We consider a day as a \u201cwork day\u201d if a student logs into the system and has some study related activities; and a day as \u201crest day\u201d if a student does not login and has no study-related activities. The rate of \u201cwork\u201d and \u201crest\u201d can capture a student\u2019s learning habits and engagement characteristics.\nAvgNumLogin =\n# of\u201cwork day\u201d\n# of \u201cwork day\u201d + # of \u201crest day\u201d .\n(4)\n(ii) Quiz Related features:\n\u2022 NumQuiz are the number of quizzes a student takes before a homework attempt. In the analyzed MOOCs, every homework has its corresponding quizzes, and each quiz has its own corresponding video(s) as shown in Fig 1 (a). Students are expected to watch the videos and attempt the quizzes before they attempt each homework. The number of quizzes a student attempts reflects the student\u2019s dedication towards the course material and a factor towards performance in a homework.\n\u2022 AvgQuiz is the average number of attempts for each quiz. The MOOCs studied in this paper allow unlimited attempts on a quiz.\n(iii) Video Related features:\n\u2022 VideoNum denotes the number of distinct video sessions for a student before a homework attempt.\n\u2022 VideoNumPause is the average number of pause actions per video. There are several actions associated with viewing videos, including \u201cpause video\u201d, \u201cplay video\u201d, \u201cseek video\u201d and \u201cload video\u201d. Tracking these student actions allows for capturing a student\u2019s focus level and learning habits. If a student pauses a video several times, we assume that the student is thinking about the content and stops to research other materials. However, we can also assume that the student may pause several times due to a lack of focus. On the contrary, if a student does not pause a video during the watching time, it could suggest that either the student understands everything or is distracted and loses focus.\n\u2022 VideoViewTime is the total video viewing time. Different videos have different lengths. Students can also stop watching the video in the middle. We calculate the whole video watching time instead of average watching time for each video.\n\u2022 VideoPctWatch In a large amount of cases, students do not watch the complete video session. As such, we calculate the average fraction of watched part out of the total video length.\n(iv) Homework Related features:\n\u2022 HWProblemSave is the average number of saves (event coding is \u201cproblem save\u201d) for each homework assessment. Students only have one chance to do the homework and the action \u201cproblem save\u201d is for the situation that the students have already done some part of a homework or all of it, but are not ready to submit it for assessment and grading. Students may save the homework and submit it after a few days during which time they may check the homework several times. As such, the \u201cproblem save\u201d event reflects studying patterns for students.\n(v) Time Related features:\n\u2022 TimeHwQuiz is the time difference between a homework attempt and the last quiz a student attempts before that homework. Quizzes help student understand the material. The corresponding quizzes of a homework might have similar questions as with the homework. Attempting a quiz helps students recall the knowledge and may lead to improved performance in the upcoming homework assessment.\n\u2022 TimeHwVideo is the time difference between a homework attempt and the last video a student watches before that homework.\n\u2022 TimePlayVideo is the average fraction of study sessions that have \u201dplay video\u201d over all the study sessions. We calculate TimePlayVideo by:\n# of study sessions that have \u201cplay video\u201d\n# of all study sessions . (5)\n\u2022 HwSessions is the number of sessions that have homework related activities (save and submit). Although students have only one chance to submit a homework, they have sufficient time to review saved homework\u2019s answers. As such, saving and submitting the same homework could occur in different sessions and possibly different days.\n(vi) Interval-Based features:. Several of the features described above are cumulative in nature and aggregated from the time (session) the student signs on to participate in a MOOC. However, we also want to capture the features aggregated between consecutive homeworks. It is expected that there will be some changes in student learning related activities once, they know the former homework\u2019s grade. For example, some students will study harder if they do not perform well on a previous homework. So we extract a group of features that represents activities between two consecutive homeworks.\n\u2022 IntervalNumQuiz: denotes the number of quizzes the student takes between two homeworks.\n\u2022 IntervalQuizAttempt: is the average number of quiz attempts between two homeworks.\n\u2022 IntervalVideo: is the number of videos a student watches between two homeworks.\n\u2022 IntervalDailySession: is the average number of sessions per day between two homeworks.\n\u2022 IntervalLogin: is the percentage of login days between two homeworks.\nWe also use the cumulative grade (so-far) on quizzes and homeworks for a student as a feature and denote it by Meanscore. For our baseline approach we only consider the averages computed on the previous homeworks."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1 Datasets", "text": "We evaluated our methods on three MOOCs: \u201cStatistics in Medicine\u201d (represented as StMed in this paper) taught in Summer 2014,\u201cStatistical Learning\u201d(represented as StLearn in this paper) taught in Winter 2015 and \u201cIntroduction to Computer Networking\u201d (represented as IntroCN in this paper) taught in Spring 2015.\nStMed: This dataset includes server logs tracking information about a student viewing video lectures, checking text/web articles, attempting quizzes and homeworks (which\nare graded). Specifically, this MOOC contains 9 learning units with 111 assessments, including 79 quizzes, 6 homeworks and 26 single questions. The course had 13,130 students enrolled, among which 4337 students submitted at least one assignment (quiz or homework) and had corresponding scores, 1262 students have completed part of the six homeworks and 1099 students have attempted all the homeworks. 193 students attempted all the 79 quizzes and six homeworks. This course had 131 videos and 6481 students had video related activity.\nStLearn: This course had ten units. Except the first one, all units have quizzes and end of unit homeworks, which add up to 103 assessments in total. 52,821 students enrolled in this course, and 4987 students had assessment activities, 3509 students attempted a subsets of the available homeworks while 346 students attempted all the 9 homeworks, and 118 students attempted all the 103 assessments. The key difference between the homeworks in the StLearn in comparison to the StMed is that homeworks have only one question which a student can either get correct or incorrect. As such, scoring in this MOOC is binary instead of continuous. To predict whether a student answers a question correctly, we reformulate the regression problem as a classification problem using a logistic loss function.\nIntroCN: This class had 8 units, a midterm exam and a final exam, including 244 assessment activities. 16,395 students enrolled in this course out of which 3263 students had assessment activities, among whom 84 students finished both the midterm and final exams. For this dataset, we predict the a student\u2019s performance for the final exam with the model trained by the information prior to their midterm exam.\nFigure 2 shows the distribution of students attempting the different assessments available across the three MOOCs studied here.\n4.2 Experimental Protocol\nIn order to gain a deep insight of students\u2019 performance in a MOOC, we perform three types of experiments. Given n, homework assessments represented as {H1, . . . , Hn} our objective is to predict the score a student achieves in each of the n homeworks. Depicting the most realistic setting, for the i-th homework, Hi we define the training set as all homework and student pairs who attempt and have a score for all homeworks up to the Hi\u22121. For predicting the score for Hi for a given student, we use all the features extracted just before attempting the target homework Hi. We refer to this as PreviousHW-based Prediction. Secondly, for the predicting i-th homework Hi\u2019s score, we use training data of student-homework pairs restricted from only the previous one homework i.e., Hi\u22121. This experiment is referred by PreviousOneHW-based Prediction. Note, in these cases we cannot make any prediction for the first homework (H1) since, we do not have any training information for a given student. We also formulate an experiment that ignores the sequence of homeworks and makes a prediction for the target Hi using training data of student-homework pairs from all the homeworks except Hi i.e., {H1 . . . Hn} \u2212 Hi. This allows for assessment of the models using the most training data available from the MOOC, and does not assume that students should follow the sequence of homeworks as suggested by the instructor. We refer to this experiment by MixData-based Prediction."}, {"heading": "4.3 Data Partition", "text": "We partition the students for StLearn and StMed into two groups: the group of students who attempt all the requested homeworks, and the group of students who finish few of the homeworks. This allows us to consider the different motivations and expectations of students enrolling in a MOOC.\nFor example, the students who aim to learn in a MOOC may watch videos for a long time and not attempt the homeworks. While, the students who want to achieve a degree certificate may not pay so much effort in watching the videos but focus on the homework scores. We refer to the first group by \u201cPartial homeworks accomplished group\u201d, and the second group by\u201cAll homeworks accomplished group\u201d. We evaluate our models on the two groups for the AllStMed and AllStLearn datasets. Specifically, we name the four group of students as AllStMed, AllStLearn, PartialStMed and PartialStLearn based on their group and MOOC class.\nFor the IntroCN course, both midterm exams and final exams have a certain amount of quizzes available for practice beforehand. For this dataset our goal is to predict the final exam prediction score. As such, we include all students who attempt this final exam in our analysis."}, {"heading": "4.4 Evaluation Metrics", "text": "StMed and IntroCN courses have continuous scores for a homework or an exam, which are scaled between 0 and\n1. However, the homework score is binary in the StLearn course, indicating whether the student answers a question correctly or incorrectly. For StLearn, we use a logistic loss and formulate a classification problem instead of the regression problem as done for the StMed and IntroCN courses. To evaluate the performance of our approach, we use the root mean squared error (RMSE) as the metric of choice for regression problem. For classification problem, we use accuracy and the F1-score (harmonic mean of precision and recall), known to be a suitable metric for imbalanced datasets."}, {"heading": "4.5 Comparative Approaches.", "text": "In this work, we compare the performance of our proposed methods with two different competitive baseline approaches.\n1. Average grade of the previous homeworks. We calculate the mean score of a given student\u2019s previous homeworks to predict their future performance and is denoted as Meanscore. We use this method to compare our prediction results on StMed.\n2. KT-IDEM [12]. KT-IDEM is a modified version of original BKT model. By adding an \u201citem\u201d node to every question node, the model assigns different probability of \u201cslip\u201d and \u201cguess\u201d to different questions, due to uneven difficulty each question has. Since this model can only predict a binary value grade, we use this model to compare our prediction results on StLearn."}, {"heading": "5. RESULTS AND DISCUSSION", "text": ""}, {"heading": "5.1 Assessment Prediction Results", "text": "Figures 3 and 4 show the prediction results with varying number of regression models for the AllStMed and AllStLearn MOOCs, respectively. Analyzing Figure 3 we observe that as the number of regression models increase the RMSE metric goes lower and use of five models seems to be good choice for all the different homeworks. Comparing the PreviousHW-\nand PreviousOneHW-based results, we notice that predictions for all the homeworks (HW3, HW4, HW5, and HW6) benefits from using all the available training data prior to those homeworks i.e., to predict grade for Hi it is better to use training information extracted from H1 . . . Hi\u22121 rather than just Hi\u22121. Comparing the MixData-based prediction results we notice the improved performance for all the homeworks in comparison to the PreviousHW-based prediction results. Similar observations can be made while analyzing the prediction results for the AllStLearn cohort which includes nine homework correct/incorrect binary assessments. Figure 4 shows the accuracy scores (higher is better) for the three experiments. For the PreviousOneHW- and PreviousHWbased experiments HW5 shows the best prediction results. This suggests that in the middle of a MOOC, students tend to have stable study activities and the performance is more predictable than other phases. Other interesting observations include, that for the MixData-based experiment HW1 shows the best accuracy results. Also, some homeworks thrive well with just using training data from the previous homework (PreviousOneHW-based, e.g. HW3).\nFigure 5 shows the comparison of prediction results (RMSE) for IntroCN of our method and baseline with increasing number of regression models. For a single PLMR model, the Meanscore baseline has better performance. But as the number of personalized models increases, the PLMR outperforms the baseline approach.\n5.1.1 Comparative Performance Table 1 shows the comparison between baseline approach (Meanscore) and the predictive model for the PreviousHWbased experiments for the AllStMed group. We cannot report results for the KT-IDEM model since, it solves the binary classification problem only. Table 2 shows the comparison of the accuracy and F1 scores of the AllStLearn groups with baseline approaches. We notice that for predicting the second homework, which only uses the information from HW1, the predictive model is not as good as the mean baseline, which reflects that under the situation of lack of necessary amount of information, linear regression models cannot always outperform the baseline. But as the dataset gets larger, our approach outperforms the baseline due to the availability of more training data. From Table 2, we also notice for some homework, KT-IDEM has better performance than PLMR (HW7 and HW4). This could be due to unstable academic activities during these two study periods, which can effect the performance of PLMR. However, for most of the situation, our model can gain better prediction performance.\nTable 3 shows the comparison of PreviousOneHW-based prediction results of AllStLearn group. With limited information, i.e. using only the previous one homework\u2019s information, our PLMR approach outperforms the KT-IDEM baseline.\n5.1.2 Feature Importance We test the effect of each feature set in predicting the assessment scores by training the models under the absence of each feature group. For the StLearn course, since there is no limit on homework attempts, we do not add Interval-Based feature groups to the predictive model. Figure 6 shows the\ncomparison of each prediction result for AllStMed, PartialStMed, AllStLearn and PartialStLearn cohorts.\nAnalyzing these results we observe that for the StLearn MOOC, the meanscore is a significant feature and removing it leads to a substantial decrease in the accuracy results for the All and Partial- cohorts. For the AllStMed MOOC the removal of session features leads to the most decrease in performance (i.e., increased RMSE). This suggests that features related to the sessions which capture student engagement are crucial for predicting the final homework scores. For the PartialStMed MOOC, the use of all feature types or a subset does not show a clear winner. This could be due the varying characteristics of students within these group.\nAnother way to analyze feature importance is to exclude the influence of meanscore which is a dominant feature in predicting a student\u2019s future performance. The evaluation formula of the importance of the ith feature (excluding meanscore feature) is as follows:\nIi = 1\nN N\u2211 n=1 \u2211l d=1 |pnS ,dfnS ,iwd,i|\u2211l d=1 |pnS ,d \u2211nF k=1 fnS ,kwd,k| , (6)\nwhere N is number of test samples, nS is the student number corresponding to the nth test sample. fnS ,i is the feature value of an interaction between student nS and activity i. nF is the number of features. l is the number of linear regression models. wd,i is the coefficient of dth linear regression model with ith feature, and pnS ,d is the membership of student nS with the dth regression model. We calculate each feature\u2019s importance by calculating the percentage contribution of each feature to the overall grade prediction.\nFigure 7 shows the feature importance on AllStMed and PartialStMed group, excluding Meanscore feature. We can see these two groups have completely different feature importance. NumQuiz and VideoPctWatch are the most important for AllStMed group besides Meanscore feature while all the Session features are important for PartialStMed group.\n6. CONCLUSION AND FUTURE WORK\nIn this work we formulated a personalized multiple linear regression model to predict the homework/exam grades for a student enrolled and participating within a MOOC. Our contributions include engineering features that capture a student\u2019s studying behavior and learning habits, derived solely from the server logs of MOOCs.\nWe evaluated our framework on three OpenEdX MOOC courses provided by an initiative at Stanford University. Our experimental evaluation shows improved performance in terms of prediction of real time homework scores when compare to baseline methods. We also studied on different groups of student participants according to their motivation and representation, some who complete all the assessments and some who only finish a subset of the provided assignments. Features associated with engagement (logging multiple times), studying materials (viewing videos and attempting quizzes) were found to be important along with prior homework scores for this prediction problem.\nGiven, the large number of users it is extremely hard to monitor the progress of users and provide them with individualized feedback. If MOOCs are to move beyond being a content repository, the ability to guide users through the course successfully is essential. For this we need to know when to intervene and how to be productive in our intervention. In the future, we seek to use this formulation within a real-time early warning or intervention system that will seek to improve student retention and improve their overall performance."}, {"heading": "7. REFERENCES", "text": "[1] Ryan SJd Baker, Albert T Corbett, and Vincent\nAleven. Improving contextual models of guessing and slipping with a truncated training set. Human-Computer Interaction Institute, page 17, 2008.\n[2] Sebastien Boyer and Kalyan Veeramachaneni. Transfer learning for predictive models in massive open online courses. In Artificial Intelligence in Education, pages 54\u201363. Springer, 2015.\n[3] Christopher G Brinton and Mung Chiang. Mooc performance prediction via clickstream data and social learning networks. To appear, 34th IEEE INFOCOM. IEEE, 2015.\n[4] Albert T Corbett and John R Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction, 4(4):253\u2013278, 1994.\n[5] Asmaa Elbadrawy, Scott Studham, and George Karypis. Personalized multi-regression models for predicting students performance in course activities. UMN CS 14-011, 2014.\n[6] Jeffrey A Greene, Christopher A Oswald, and Jeffrey Pomerantz. Predictors of retention and achievement in a massive open online course. American Educational Research Journal, page 0002831215584621, 2015.\n[7] Glyn Hughes and Chelsea Dobbins. The utilization of data analysis techniques in predicting student performance in massive open online courses (moocs). Research and Practice in Technology Enhanced Learning, 10(1):1\u201318, 2015.\n[8] Suhang Jiang, Adrienne Williams, Katerina Schenke,\nMark Warschauer, and Diane O\u2019dowd. Predicting mooc performance with week 1 behavior. In Educational Data Mining 2014, 2014.\n[9] Gregor Kennedy, Carleton Coffrin, Paula de Barba, and Linda Corrin. Predicting success: how learners\u2019 prior knowledge, skills and activities predict mooc performance. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 136\u2013140. ACM, 2015.\n[10] Daniel FO Onah and Jane Sinclair. Learners expectations and motivations using content analysis in a mooc. In EdMedia 2015-World Conference on Educational Media and Technology, volume 2015, pages 185\u2013194. Association for the Advancement of Computing in Education (AACE), 2015.\n[11] Zachary Pardos, Yoav Bergner, Daniel Seaton, and David Pritchard. Adapting bayesian knowledge tracing to a massive open online course in edx. In Educational Data Mining 2013, 2013.\n[12] Zachary A Pardos and Neil T Heffernan. Kt-idem: Introducing item difficulty to the knowledge tracing model. In User Modeling, Adaption and Personalization, pages 243\u2013254. Springer, 2011.\n[13] Alejandro Pen\u0303a-Ayala. Educational data mining: A survey and a data mining-based analysis of recent works. Expert systems with applications, 41(4):1432\u20131462, 2014.\n[14] Ayse Saliha Sunar, Nor Aniza Abdullah, Susan White, and Hugh C Davis. Analysing and predicting recurrent interactions among learners during online discussions in a mooc. Proceedings of the 11th International Conference on Knowledge Management, 2015.\n[15] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267\u2013288, 1996.\n[16] Rui Wang, Gabriella Harari, Peilin Hao, Xia Zhou, and Andrew T Campbell. Smartgpa: how smartphones can assess and predict academic performance of college students. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pages 295\u2013306. ACM, 2015.\n[17] Jacob Whitehill, Joseph Jay Williams, Glenn Lopez, Cody Austun Coleman, and Justin Reich. Beyond prediction: First steps toward automatic intervention in mooc student stopout. Available at SSRN 2611750, 2015.\n[18] Diyi Yang, Tanmay Sinha, David Adamson, and Carolyn Penstein Rose. Turn on, tune in, drop out: Anticipating student dropouts in massive open online courses. In Proceedings of the 2013 NIPS Data-Driven Education Workshop, volume 11, page 14, 2013.\n[19] Cheng Ye and Gautam Biswas. Early prediction of student dropout and performance in moocs using higher granularity temporal information. Journal of Learning Analytics, 1(3):169\u2013172, 2014.\n[20] Cheng Ye, John S Kinnebrew, Gautam Biswas, Brent J Evans, Douglas H Fisher, Gayathri Narasimham, and Katherine A Brady. Behavior prediction in moocs using higher granularity temporal information. In Proceedings of the Second (2015) ACM Conference on Learning@ Scale, pages 335\u2013338. ACM, 2015."}], "references": [{"title": "Improving contextual models of guessing and slipping with a truncated training", "author": ["Ryan SJd Baker", "Albert T Corbett", "Vincent Aleven"], "venue": "set. Human-Computer Interaction Institute,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Transfer learning for predictive models in massive open online courses", "author": ["Sebastien Boyer", "Kalyan Veeramachaneni"], "venue": "In Artificial Intelligence in Education,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Mooc performance prediction via clickstream data and social learning networks", "author": ["Christopher G Brinton", "Mung Chiang"], "venue": "To appear,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Knowledge tracing: Modeling the acquisition of procedural knowledge", "author": ["Albert T Corbett", "John R Anderson"], "venue": "User modeling and user-adapted interaction,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Personalized multi-regression models for predicting students performance in course activities", "author": ["Asmaa Elbadrawy", "Scott Studham", "George Karypis"], "venue": "UMN CS 14-011,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Predictors of retention and achievement in a massive open online course", "author": ["Jeffrey A Greene", "Christopher A Oswald", "Jeffrey Pomerantz"], "venue": "American Educational Research Journal,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "The utilization of data analysis techniques in predicting student performance in massive open online courses (moocs)", "author": ["Glyn Hughes", "Chelsea Dobbins"], "venue": "Research and Practice in Technology Enhanced Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Predicting mooc performance with week 1 behavior", "author": ["Suhang Jiang", "Adrienne Williams", "Katerina Schenke", "Mark Warschauer", "Diane O\u2019dowd"], "venue": "In Educational Data", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Predicting success: how learners\u2019 prior knowledge, skills and activities predict mooc performance", "author": ["Gregor Kennedy", "Carleton Coffrin", "Paula de Barba", "Linda Corrin"], "venue": "In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learners expectations and motivations using content analysis in a mooc", "author": ["Daniel FO Onah", "Jane Sinclair"], "venue": "In EdMedia 2015-World Conference on Educational Media and Technology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Adapting bayesian knowledge tracing to a massive open online course in edx", "author": ["Zachary Pardos", "Yoav Bergner", "Daniel Seaton", "David Pritchard"], "venue": "In Educational Data", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Kt-idem: Introducing item difficulty to the knowledge tracing model", "author": ["Zachary A Pardos", "Neil T Heffernan"], "venue": "In User Modeling, Adaption and Personalization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Educational data mining: A survey and a data mining-based analysis of recent works", "author": ["Alejandro Pe\u00f1a-Ayala"], "venue": "Expert systems with applications,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Analysing and predicting recurrent interactions among learners during online discussions in a mooc", "author": ["Ayse Saliha Sunar", "Nor Aniza Abdullah", "Susan White", "Hugh C Davis"], "venue": "Proceedings of the 11th International Conference on Knowledge Management,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Smartgpa: how smartphones can assess and predict academic performance of college students", "author": ["Rui Wang", "Gabriella Harari", "Peilin Hao", "Xia Zhou", "Andrew T Campbell"], "venue": "In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Beyond prediction: First steps toward automatic intervention in mooc student stopout", "author": ["Jacob Whitehill", "Joseph Jay Williams", "Glenn Lopez", "Cody Austun Coleman", "Justin Reich"], "venue": "Available at SSRN 2611750,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Turn on, tune in, drop out: Anticipating student dropouts in massive open online courses", "author": ["Diyi Yang", "Tanmay Sinha", "David Adamson", "Carolyn Penstein Rose"], "venue": "In Proceedings of the 2013 NIPS Data-Driven Education Workshop,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Early prediction of student dropout and performance in moocs using higher granularity temporal information", "author": ["Cheng Ye", "Gautam Biswas"], "venue": "Journal of Learning Analytics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Behavior prediction in moocs using higher granularity temporal information", "author": ["Cheng Ye", "John S Kinnebrew", "Gautam Biswas", "Brent J Evans", "Douglas H Fisher", "Gayathri Narasimham", "Katherine A Brady"], "venue": "In Proceedings of the Second", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 6, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 7, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 16, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 17, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 18, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 19, "context": "Several predictive methods have been developed to predict when a participant will drop out from a MOOC [6, 7, 8, 17, 18, 19, 20].", "startOffset": 103, "endOffset": 128}, {"referenceID": 9, "context": "Participants enroll in a MOOC sometimes to learn a subset of topics within the curriculum, sometimes to earn degree certificates for future career promotion or college credit, social experience or/and exploration of free online education [10].", "startOffset": 238, "endOffset": 242}, {"referenceID": 5, "context": "Students with similar motivation have different learning outcomes from a MOOC based on the number of invested hours, prior education background, knowledge and skills [6].", "startOffset": 166, "endOffset": 169}, {"referenceID": 4, "context": "This approach was previously studied within the context of predicting a student\u2019s performance based on graded activities within a traditional university course with data extracted from a learning management system (Moodle) [5].", "startOffset": 223, "endOffset": 226}, {"referenceID": 0, "context": "al [1] have presented systems that can adapt based on predictions of future student performance, and they were able to incorporate interventions, which were effective in improving student experiences within Intelligent Tutoring Systems (ITS).", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Several researchers have focused on the analysis of education data (including MOOCs), in an effort to understand the characteristics of student learning behaviors and motivation within this education model [13].", "startOffset": 206, "endOffset": 210}, {"referenceID": 1, "context": "[2] focus on the stopout prediction problem within MOOCs; by designing a set of processes using information from previous courses and the previous weeks of the current course.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] developed an approach to predict if a student answers a question correct on the first attempt via clickstream information and social learning networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] analyzed the relationship between a student\u2019s prior knowledge on end-of-MOOC performance.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] developed an approach to predict the possible interactions between peers participating in a MOOC.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Most similar to our proposed work, Bayesian Knowledge Tracing (BKT) [12] has been adapted to predict whether a student can get a MOOC assessment correct or not.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "BKT was first developed [4] for modeling the evolving knowledge states of students monitored within Intelligent Tutoring Systems (ITS).", "startOffset": 24, "endOffset": 27}, {"referenceID": 10, "context": "By identifying the challenges associated with modeling MOOC data, the IDEM approach and extensions that involve splitting questions into several sub-parts and incorporating resource (knowledge) information [11] are considered state-of-the-art MOOC assessment prediction approaches and referred as KT-IDEM.", "startOffset": 206, "endOffset": 210}, {"referenceID": 15, "context": "[16] performed a study to predict student\u2019s performance by capturing data relevant to study habits and learning behaviors from their smartphones.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] proposed the use of personalized linear multi-regression models to predict student performance in a traditional university by extracting data from course management systems (Moodle).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "We train a personalized linear multi-regression (PLMR) model [5] to predict student performance within a MOOC.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "l is the number of linear regression models, W is the coefficient matrix of dimensions l \u00d7 nF that holds the coefficients of the l linear regression models, and ps is a vector of length l that holds the memberships of student s within the l different regression models [5].", "startOffset": 269, "endOffset": 272}, {"referenceID": 14, "context": "Using lasso [15], we solve the following optimization problem:", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "KT-IDEM [12].", "startOffset": 8, "endOffset": 12}], "year": 2016, "abstractText": "The past few years has seen the rapid growth of data mining approaches for the analysis of data obtained from Massive Open Online Courses (MOOCs). The objectives of this study are to develop approaches to predict the scores a student may achieve on a given grade-related assessment based on information, considered as prior performance or prior activity in the course. We develop a personalized linear multiple regression (PLMR) model to predict the grade for a student, prior to attempting the assessment activity. The developed model is real-time and tracks the participation of a student within a MOOC (via click-stream server logs) and predicts the performance of a student on the next assessment within the course offering. We perform a comprehensive set of experiments on data obtained from three openEdX MOOCs via a Stanford University initiative. Our experimental results show the promise of the proposed approach in comparison to baseline approaches and also helps in identification of key features that are associated with the study habits and learning behaviors of students.", "creator": "LaTeX with hyperref package"}}}