{"id": "1605.02677", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark", "abstract": "psychological research results have independently confirmed researchers that observing people can have different emotional reactions to different visual stimuli. here several papers have been published on the problem of visual emotion analysis. in particular, attempts even have been made attempts to analyze and predict people's emotional reaction towards images. to this excellent end, different kinds of hand - tuned speech features are proposed. the results reported on several carefully selected and labeled remarkably small image data sets have confirmed the full promise of such features. while the recent successes consequences of solving many computer vision related tasks are due to the adoption of convolutional modeled neural decision networks ( cnns ), visual emotion content analysis has not achieved the same overwhelming level of success. this may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. in this work, we introduce clearly a wide new data set, which started from 3 + million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available primary visual emotion data set. we very hope that this data set encourages further research on visual emotion analysis. we also perform extensive benchmarking analyses on this large single data memory set using the state of the art methods including cnns.", "histories": [["v1", "Mon, 9 May 2016 18:14:52 GMT  (4063kb,D)", "http://arxiv.org/abs/1605.02677v1", "7 pages, 7 figures, AAAI 2016"]], "COMMENTS": "7 pages, 7 figures, AAAI 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["quanzeng you", "jiebo luo", "hailin jin", "jianchao yang"], "accepted": true, "id": "1605.02677"}, "pdf": {"name": "1605.02677.pdf", "metadata": {"source": "META", "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine Print and The Benchmark", "authors": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "emails": ["jluo}@cs.rochester.edu", "hljin@adobe.com", "jianchao.yang@snapchat.com"], "sections": [{"heading": "Introduction", "text": "Psychological studies have provided evidence that human emotions can be aroused by visual content, e.g. images (Lang 1979; Lang, Bradley, and Cuthbert 1998; Joshi et al. 2011). Based on these findings, recently computer scientists also started to delve into this research topic. However, differently from psychological studies, which mainly focus on studying the changes between physiological and psychological activities of human beings on visual stimuli, most of the works in computer science are trying to predict the aroused human emotion given a particular piece of visual content. Indeed, affective computing, which aims to recognize, interpret and process human affects (http://en.wikipedia.org/wiki/Affective computing), has achieved significant progress in recent years. However, the problem of visual emotion prediction is more difficult in that we are trying to predict the emotional reactions given\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."}, {"heading": "Amusement Awe Contentment Excitement", "text": ""}, {"heading": "Anger Disgust Fear Sadness", "text": "a general visual stimulus, instead of using the collected signals from human\u2019s physiological reactions of visual stimuli as studied in affective computing.\nThe increasing popularity of social networks attracts more and more people to publish multimedia content in online social network platforms. Online users can easily add textual data, e.g., title, descriptions, tags, to their uploaded images and videos. However, these textual information can only help the retrieval of multimedia content in the cognitive level (Machajdik and Hanbury 2010), i.e., semantic level. The meta text data has limited help in bridging the affective semantic gap between images pixels and human feelings. In (Hanjalic 2006), the authors call visual emotion prediction affective content analysis.\nInspired by the psychology and art theory, different groups of manually crafted features are designed to study the emotional reactions towards visual content. For example, based on art theory (Itten and Van Haagen 1973; Valdez and Mehrabian 1994), Machajdik and Hanbury (Machajdik and Hanbury 2010) defined eight different kinds of pixel level features (e.g. color, texture and composition), which have been empirically proved to be related to emotional reactions. In another recent work (Zhao et al. 2014), principles-or-art based features are extracted to classify emotions. Following their works, we study the same eight emotions, Amusement, Awe, Contentment, Excitement, Anger, Disgust, Fear\nar X\niv :1\n60 5.\n02 67\n7v 1\n[ cs\n.A I]\n9 M\nay 2\n01 6\nand Sadness. Figure 1 shows the example images for these studied emotions. All these images are selected from the newly constructed data set in this work, where each image is labeled by five Amazon Mechanical Turk workers.\nRecently deep learning has enabled robust and accurate feature learning, which in turn produces the state-of-theart performance on many computer vision related tasks, e.g. digit recognition (LeCun et al. 1989; Hinton, Osindero, and Teh 2006), image classification (Cires\u0327an et al. 2011; Krizhevsky, Sutskever, and Hinton 2012), aesthetics estimation (Lu et al. 2014) and scene recognition (Zhou et al. 2014). One of the main factors that prompt the success of deep learning on these problems is the availability of a large scale data set. From ImageNet (Deng et al. 2009) to AVA dataset (Murray, Marchesotti, and Perronnin 2012) and the very recent Places Database (Zhou et al. 2014), the availability of these data sets have significantly promoted the development of new algorithms on these research areas. In visual emotion analysis, there is no such a large data set with strong labels. More recently, You et al. (You et al. 2015) employed CNNs to address visual sentiment analysis, which tries to bridge the high-level, abstract sentiments concept and the image pixels. They employed the weakly label images to train their CNN model. However, they are trying to solve a binary classification problem instead of a multi-class (8 emotions) problem as studied in this work.\nIn this work, we are interested in investigating the possibility of solving the challenging visual emotion analysis problem. First, we build a large scale emotion data set. On top of the data set, we intend to find out whether or not applying CNNs to visual emotion analysis provides advantages over using a predefined collection of art and psychology theory inspired visual features or visual attributes, which have been done in prior works. To that end, we make the following contributions in this work.\n\u2022 We collect a large number of weakly labeled emotion related images. Next, we employ Amazon Mechanical Turk to manually label these images to obtain a relatively strongly labeled image data set, which makes the usage of CNN for visual emotion analysis possible. All the data set will be released to the research community upon publishing this work.\n\u2022 We evaluate the performance of Convolutional Neural Networks on visual emotion analysis and establish it as the baseline for future research. Compared with the stateof-the-art manually crafted visual features, our results suggest that using CNN can achieve significant performance improvement on visual emotion analysis."}, {"heading": "Related Work", "text": "Our work is mostly related to both visual emotion analysis and Convolutional Neural Networks (CNNs). Recently, deep learning has achieved massive success on a wide range of artificial intelligence tasks. In particular, deep Convolutional Neural Networks have been widely employed to solve traditional computer vision related problems. Deep convolutional neural networks typically consist of several convolutional layers and several fully connected layers. Between\nthe convolutional layers, there may also be pooling layers and normalization layers. In early studies, CNNs (LeCun et al. 1998) have been very successful in document recognition, where the inputs are relatively small images. Thanks to the increasing computational power of GPU, it is now possible to train a deep convolutional neural network on large collections of images (e.g. (Krizhevsky, Sutskever, and Hinton 2012)), to solve other computer vision problems, such as scene parsing (Grangier, Bottou, and Collobert 2009), feature learning (LeCun, Kavukcuoglu, and Farabet 2010), visual recognition (Kavukcuoglu et al. 2010) and image classification (Krizhevsky, Sutskever, and Hinton 2012).\nHowever, to the best of our knowledge, there are no related works on using CNNs for visual emotion analysis. Currently, most of the works on visual emotion analysis can be classified into either dimensional approach (Nicolaou, Gunes, and Pantic 2011; Lu et al. 2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images. With additional social relationships, they build a factor graph model for the prediction of emotions. Inspired by art and psychology theory, Machajdik and Hanbury (2010) proposed richer hand-tuned features, including color, texture, composition and content features. Furthermore, by exploring the principles of art, Zhao et al. (2014) defined more robust and invariant visual features, such as balance, variety, and gradation. Their features achieved the best reported performance on several publicly accessible emotion data sets.\nThose hand-tuned visual features have been validated on several publicly available small data sets (see following sections for details). However, we want to verify whether or not deep learning could be applied to this challenging problem and more importantly, on a much larger scale image set. The main issue is that there are no available well labeled data sets for training deep neural networks. Our work intends to provide such a data set for the research community and verify the performance of the widely used deep convolutional neural architecture on this emotion data set."}, {"heading": "Visual Emotion Data Sets", "text": "Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999). This data set is categorized into eight emotional categories as shown in Figure 1 in a study conducted in (Mikels et al. 2005). (2) ArtPhoto: Machajdik and Hanbury (2010) built this data set, which contains photos by professional artists. They obtain the ground truth by the labels provided by the owner of each image. (3) Abstract Paintings: These are images consisting of both color and texture from (Machajdik and Hanbury 2010). They obtain the ground truth of each image by asking people to vote for the emotions of each image in the given eight emotion\ncategories. Table 1 shows the statistics of the existing three data sets. The numbers show that each data set only consists of a very small number of images. Meanwhile, images in all the three different data sets are highly imbalanced. All three data sets are relatively small with images coming from a few specific domains. In particular, for some categories, such as Anger, there are less than 10 images. Therefore, if we employ the same methodology (5-fold Cross Validation within each data set) (Machajdik and Hanbury 2010; Zhao et al. 2014), we may have only several images in the training data. This may lead to the possibility that their trained models may have been either over or under fitted.\nThe above results suggest that the previous efforts on visual emotion analysis deal with small emotion-centric data sets compared with other vision data sets, such as ImageNet (Deng et al. 2009) and Places (Zhou et al. 2014). In this work, we present here a new emotion data set, which is by far the largest available emotion-centric database."}, {"heading": "Building An Image Emotion Dataset from the Wild", "text": "There are many different emotion definition systems1 from psychological and cognitive science. In this work, we use the same eight emotions defined in Table 1, which is derived from a psychological study in (Mikels et al. 2005). Using the similar approach in (Jia et al. 2012), we query the image search engines (Flickr and Instagram) using the eight emotions as keywords. In this way, we are able to collect a total of over 3 million weakly labeled images, i.e., labeled by the queries. Next, we delete images which have tags of any two different emotions. We also remove duplicate images using fdupes2. Figure 2 shows the statistics of the remaining images. As we can see, the number of images in different categories is imbalanced. In particular, there are only small numbers of contentment and disgust images in both social media platforms. Meanwhile, the number of per category images from Instagram varies significantly. There are much more images from both Fear and Sadness. This agrees with the finding (http://goo.gl/vhBBF6) that people are likely to share sadness from their Instagram accounts.\nNext, we employ Amazon Mechanical Turk (AMT) to further label these weakly labeled images. In particular, we design a qualification test to filter all workers who want to work on our tasks. The qualification test is designed as an image annotation problem. We randomly select images from the publicly available ArtPhoto data set and use the groundtruth labels as the answers. For each given image, we ask the workers to choose the emotion they feel from the eight\n1http://en.wikipedia.org/wiki/Emotion 2https://code.google.com/p/fdupes/\nemotion categories. At first, we conduct experiments within members in our research group. Indeed, the results suggest that this qualification is challenging difficult, in particular when you have to choose only one emotion for each image. Therefore, we design our AMT tasks (HITs) as a much easier verification task instead of the annotation task. Since we have crawled all the images with emotion queries, we have a weakly labeled data set. In each HIT, we assign five AMT workers to verify the emotion of each image. For each given image and its weak label, we ask them to answer a question like Do you feel anger seeing this image? The workers are asked to choose a YES or NO for each image. All workers have to meet the rigorous requirement of correctly answering at least half of the questions (a total of 20) in our qualification test. By the time of finishing work, we have over 1000 workers on our qualification task. Among them, 225 workers meet our qualification criteria.\nTo start the verification task, we randomly select 11, 000 images for each emotion category. After collecting the batch results from AMT, we keep those images which receive at least three Yeses from their assigned fiver AMT workers. In this way, we are able to build a relatively strongly labeled data set for visual emotion analysis. Table 2 summarizes the number of images for our current data set. The numbers show that different categories may have different acceptance rates for workers to reject or accept the positive samples of each verification task. In particular, we add another 2, 000 images to make sure that the number of images in Fear category is also larger than 1, 000 images. In total, we collect about 23, 000 images, which is about 30 times as large as the\ncurrent largest emotion data set, i.e., ArtPhoto."}, {"heading": "Fine-tuning Convolutional Neural Network for Visual Emotion Analysis", "text": "Convolutional Neural Networks (CNN) have been proven to be effective in image classification tasks, e.g., achieving the state-of-the-art performance in ImageNet Challenge (Krizhevsky, Sutskever, and Hinton 2012). Meanwhile, there are also successful applications by fine-tuning the pre-trained ImageNet model, including recognizing image style (Karayev et al. 2013) and semantic segmentation (Long, Shelhamer, and Darrell 2014). In this work, we employ the same strategy to fine-tune the pre-trained ImageNet reference network (Jia et al. 2014). The same neural network architecture is employed. We only change the last layer of the neural network from 1000 to 8. The remain layers keep the same as the ImageNet reference network. We randomly split the collected 23, 000 samples into training (80%), testing (15%) and validating sets (5%).\nMeanwhile, we also employ the weak labels (Jia et al. 2012) to fine-tune another model as described in (You et al. 2015). We exclude those images that have been chosen to be submitted to AMT for labeling. Next, since contentment contains only about 16, 000 images, we randomly select 20, 000 images for other emotion categories. In this way, we have a total of 156, 000 images. We call this model NoisyFine-tuned CNN. We fine-tune both models using Caffe with a Linux server with 2 NVIDIA TITAN GPUs on top of the pre-trained ImageNet CNN model."}, {"heading": "Performance of Convolutional Neural Networks on Visual Emotion Analysis", "text": "After the fine-tuning of the pre-trained CNN model, we obtain two new CNN models. To compare with the ImageNetCNN, we also show the results of using the SVM trained on features extracted from the second to the last layer of the pre-trained ImageNet-CNN model. In particular, we employ PCA to reduce the dimensionality of the features. We also try several different numbers of principal components. The results are almost the same. To overcome the imbalance problem in the data, we adjust the weights of SVM for different classes (in our implementation, we use LIBSVM3, which provides such a mechanism). Table 3 summarizes the performance of the three groups of features on the 15% randomly chosen testing data. The overall accuracy of the Finetuned-CNN is almost 60%. As a baseline, the visual features extracted from ImageNet-CNN only lead to an overall accuracy of about 30%, which is half of Fine-tuned-CNN. The Noisy-Fine-tuned-CNN model has an overall accuracy of\n3http://www.csie.ntu.edu.tw/\u223ccjlin/libsvm/\nabout 46%, which suggests that this model can learn some knowledge from the noisily labeled images. However, even though it has much more training samples compared with Fine-tuned CNN, it fails to outperform Fine-tuned CNN, which is trained on strongly labeled samples.\nWe also calculate the confusion matrix of the three algorithms from their prediction results on the testing data to further analyze their performance. Figure 3(c) shows the confusion matrix of the Fine-tuned CNN model. Compared with the other two models, the true negative rates from Finetuned-CNN are the best in most emotion categories. Meanwhile, the confusion matrix of Noisy-Fine-tuned CNN seems to be more balanced, except for the contentment emotion (seeFigure 3(b)). Indeed, these findings are consistent with the number of available labeled samples (see Table 2). The more the labeled images, the higher probability that the corresponding emotion will receive a higher true positive rate. Figure 3(a) shows the confusion matrix using the more general ImageNet-CNN features. It is interesting to see that overall the performance is worse than the Fine-tuned CNN features. However, the true positive rate of fear is higher than that of using the Fine-tuned features.\nThe embedding of the testing images using deep visual features (we do not show the embedding for Noisy-Finetuned-CNN due to space arrangement) is shown in Figure 4. The features are also processed using t-SNE (Van der Maaten and Hinton 2008). The embedding using ImageNetCNN shows that images from the same scene or of similar objects are embedded into neighboring areas. However, the embedding using Figure 4(b) seems to make the images more diverse in terms of objects or scenes. This is indeed comply with the fact that even the same object could lead to different visual emotion at its different state, e.g., angry dog and cute dog."}, {"heading": "Performance of Convolutional Neural Networks on Public Existing Visual Emotion Data Set", "text": "We have described several existing data sets in Section . Table 1 summarizes the statistics of the three data sets. To the best of our knowledge, no related studies have been conducted on evaluating the performance of Convolutional Neural Networks on visual emotion analysis. In this section, we\nevaluate all the three deep neural network models on all the three data sets and compare the results with several other state-of-the-art methods on these data sets.\nIn particular, we extract deep visual features for all the images in the three data sets using the trained deep neural network models from the second to the last layer. In this way, we obtain a 4096 dimensional feature representation for each image from each deep model. Next, we follow the same evaluation routine described in (Machajdik and Hanbury 2010) and (Zhou et al. 2014). At first, PCA is employed to reduce the dimensions of the features respectively. For all the three data sets, we reduce the number of feature dimensions from 4096 to 20, which is capable of keeping at least 90% variance. Next, a linear SVM is trained on the reduced feature space. Following the same experimental approach, the one v.s. all strategy is employed to train the classifier. In\nparticular, we randomly split the data into 5 batches such that 5-fold Cross Validation is used to obtain the results. Also, we assign larger penalties to true negative samples in the SVM training stage in order to optimize the per class true positive rate as suggested by both (Machajdik and Hanbury 2010) and (Zhou et al. 2014).\nWe compare the performance of deep features on visual emotion analysis with several other baseline features, including Wang et al. (Wei-ning, Ying-lin, and Sheng-ming 2006), Yanulevskaya et al. (Yanulevskaya et al. 2008), Machajdik and Hanbury (Machajdik and Hanbury 2010) and Zhao et al. (Zhou et al. 2014). Figures 5, 6 and 7 show the performance of these features on the three data sets respectively. Note that since emotion anger only contains 8 and 3 images in IAPS-Subset and Abstract Paintings data sets, which are not enough to perform the 5-fold Cross Valida-\ntion. We do not report the true positive rates for emotion anger on these two data sets.\nIt is interesting to find out that deep visual features significantly outperform the state-of-the-art manually crafted visual features in some emotion categories. However, the performance of using deep visual features are not consistent across the emotion categories at present. In particular, the performance of directly employing deep visual features from ImageNet-CNN and Noisy-Fine-tuned-CNN differ significantly among categories as well as across data sets. The performance of deep visual features from Fine-tuned-CNN is relatively more consistent. However, it has poor performance on emotions Contentment and Fear in the ArtPhoto data. These results suggest that it is still challenging to solve visual emotion analysis even with the state-of-the-art deep visual features. Meanwhile, the performance of deep visual features also suggests the promise of using CNNs in visual emotion analysis. Overall, this may encourage the development of more advanced deep architectures for visual emotion analysis, as well as development of other approaches."}, {"heading": "Conclusions", "text": "In this work, we introduce the challenging problem of visual emotion analysis. Due to the unavailability of a large scale well labeled data set, little research work has been published on studying the impact of Convolutional Neural Networks on visual emotion analysis. In this work, we are introducing such a data set and intend to release the data set to the research community to promote the research on visual emotion analysis with the deep learning and other learning frameworks. Meanwhile, we also evaluate the deep visual features extracted from differently trained neural network models. Our experimental results suggest that deep convolutional neural network features outperform the state-of-theart hand-tuned features for visual emotion analysis. In addition, fine-tuned neural network on emotion related data sets can further improve the performance of deep neural network. Nevertheless, the results obtained in this work are only\na start for the research on employing deep learning or other learning frameworks for visual emotion analysis. We will continue the collection of labeled data from AMT with a plan to submit additional 1 million images for labeling. We hope our visual emotion analysis results can encourage further research on online user generated multimedia content in the wild. Better understanding the relationship between emotion arousals and visual stimuli and further extending the understanding to valence are the primary future directions for visual emotion analysis."}, {"heading": "Acknowledgement", "text": "This work was generously supported in part by Adobe Research, and New York State CoE IDS. We thank the authors of (Zhao et al. 2014) for providing their algorithms for comparison."}], "references": [{"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["D. Borth", "R. Ji", "T. Chen", "T. Breuel", "S.-F. Chang"], "venue": "ACM MM, 223\u2013232. ACM.", "citeRegEx": "Borth et al\\.,? 2013", "shortCiteRegEx": "Borth et al\\.", "year": 2013}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Cire\u015fan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "IJCAI, 1237\u20131242.", "citeRegEx": "Cire\u015fan et al\\.,? 2011", "shortCiteRegEx": "Cire\u015fan et al\\.", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. FeiFei"], "venue": "CVPR, 248\u2013255. IEEE.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deep convolutional networks for scene parsing", "author": ["D. Grangier", "L. Bottou", "R. Collobert"], "venue": "ICML 2009 Deep Learning Workshop, volume 3.", "citeRegEx": "Grangier et al\\.,? 2009", "shortCiteRegEx": "Grangier et al\\.", "year": 2009}, {"title": "Extracting moods from pictures and sounds: Towards truly personalized tv", "author": ["A. Hanjalic"], "venue": "Signal processing magazine, IEEE 23(2):90\u2013100.", "citeRegEx": "Hanjalic,? 2006", "shortCiteRegEx": "Hanjalic", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "The art of color: the subjective experience and objective rationale of color", "author": ["J. Itten", "E. Van Haagen"], "venue": "Van Nostrand Reinhold New York, NY, USA.", "citeRegEx": "Itten and Haagen,? 1973", "shortCiteRegEx": "Itten and Haagen", "year": 1973}, {"title": "Can we understand van gogh\u2019s mood?: learning to infer affects from images in social networks", "author": ["J. Jia", "S. Wu", "X. Wang", "P. Hu", "L. Cai", "J. Tang"], "venue": "ACM MM, 857\u2013 860. ACM.", "citeRegEx": "Jia et al\\.,? 2012", "shortCiteRegEx": "Jia et al\\.", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093.", "citeRegEx": "Jia et al\\.,? 2014", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Aesthetics and emotions in images", "author": ["D. Joshi", "R. Datta", "E. Fedorovskaya", "Q.-T. Luong", "J.Z. Wang", "J. Li", "J. Luo"], "venue": "Signal Processing Magazine, IEEE 28(5):94\u2013115.", "citeRegEx": "Joshi et al\\.,? 2011", "shortCiteRegEx": "Joshi et al\\.", "year": 2011}, {"title": "Recognizing image style", "author": ["S. Karayev", "M. Trentacoste", "H. Han", "A. Agarwala", "T. Darrell", "A. Hertzmann", "H. Winnemoeller"], "venue": "arXiv preprint arXiv:1311.3715.", "citeRegEx": "Karayev et al\\.,? 2013", "shortCiteRegEx": "Karayev et al\\.", "year": 2013}, {"title": "Learning convolutional feature hierarchies for visual recognition", "author": ["K. Kavukcuoglu", "P. Sermanet", "Y.-L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun"], "venue": "NIPS, 5.", "citeRegEx": "Kavukcuoglu et al\\.,? 2010", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Emotion, motivation, and anxiety: brain mechanisms and psychophysiology", "author": ["P.J. Lang", "M.M. Bradley", "B.N. Cuthbert"], "venue": "Biological psychiatry 44(12):1248\u20131263.", "citeRegEx": "Lang et al\\.,? 1998", "shortCiteRegEx": "Lang et al\\.", "year": 1998}, {"title": "International affective picture system (iaps): Technical manual and affective ratings", "author": ["P.J. Lang", "M.M. Bradley", "B.N. Cuthbert"], "venue": null, "citeRegEx": "Lang et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lang et al\\.", "year": 1999}, {"title": "A bio-informational theory of emotional imagery", "author": ["P.J. Lang"], "venue": "Psychophysiology 16(6):495\u2013512.", "citeRegEx": "Lang,? 1979", "shortCiteRegEx": "Lang", "year": 1979}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation 1(4):541\u2013551.", "citeRegEx": "LeCun et al\\.,? 1989", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Convolutional networks and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "ISCAS, 253\u2013256. IEEE.", "citeRegEx": "LeCun et al\\.,? 2010", "shortCiteRegEx": "LeCun et al\\.", "year": 2010}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4038.", "citeRegEx": "Long et al\\.,? 2014", "shortCiteRegEx": "Long et al\\.", "year": 2014}, {"title": "On shape and the computability of emotions", "author": ["X. Lu", "P. Suryanarayan", "Adams, Jr., R.B.", "J. Li", "M.G. Newman", "J.Z. Wang"], "venue": "ACM MM, 229\u2013238. ACM.", "citeRegEx": "Lu et al\\.,? 2012", "shortCiteRegEx": "Lu et al\\.", "year": 2012}, {"title": "Rapid: Rating pictorial aesthetics using deep learning", "author": ["X. Lu", "Z. Lin", "H. Jin", "J. Yang", "J.Z. Wang"], "venue": "ACM MM, 457\u2013466. ACM.", "citeRegEx": "Lu et al\\.,? 2014", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Affective image classification using features inspired by psychology and art theory", "author": ["J. Machajdik", "A. Hanbury"], "venue": "ACM MM, 83\u201392. ACM.", "citeRegEx": "Machajdik and Hanbury,? 2010", "shortCiteRegEx": "Machajdik and Hanbury", "year": 2010}, {"title": "Emotional category data on images from the international affective picture system", "author": ["J.A. Mikels", "B.L. Fredrickson", "G.R. Larkin", "C.M. Lindberg", "S.J. Maglio", "P.A. Reuter-Lorenz"], "venue": "Behavior research methods 37(4):626\u2013 630.", "citeRegEx": "Mikels et al\\.,? 2005", "shortCiteRegEx": "Mikels et al\\.", "year": 2005}, {"title": "Ava: A large-scale database for aesthetic visual analysis", "author": ["N. Murray", "L. Marchesotti", "F. Perronnin"], "venue": "CVPR, 2408\u20132415. IEEE.", "citeRegEx": "Murray et al\\.,? 2012", "shortCiteRegEx": "Murray et al\\.", "year": 2012}, {"title": "A multilayer hybrid framework for dimensional emotion classification", "author": ["M.A. Nicolaou", "H. Gunes", "M. Pantic"], "venue": "ACM MM, 933\u2013936. ACM.", "citeRegEx": "Nicolaou et al\\.,? 2011", "shortCiteRegEx": "Nicolaou et al\\.", "year": 2011}, {"title": "Effects of color on emotions", "author": ["P. Valdez", "A. Mehrabian"], "venue": "Journal of Experimental Psychology 123(4):394.", "citeRegEx": "Valdez and Mehrabian,? 1994", "shortCiteRegEx": "Valdez and Mehrabian", "year": 1994}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research 9(25792605):85.", "citeRegEx": "Maaten and Hinton,? 2008", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Image retrieval by emotional semantics: A study of emotional space and feature extraction", "author": ["W. Wei-ning", "Y. Ying-lin", "J. Sheng-ming"], "venue": "Systems, Man and Cybernetics, IEEE International Conference on, volume 4, 3534\u2013 3539. IEEE.", "citeRegEx": "Wei.ning et al\\.,? 2006", "shortCiteRegEx": "Wei.ning et al\\.", "year": 2006}, {"title": "Emotional valence categorization using holistic image features", "author": ["V. Yanulevskaya", "J. Van Gemert", "K. Roth", "A.-K. Herbold", "N. Sebe", "J.-M. Geusebroek"], "venue": "ICIP, 101\u2013 104. IEEE.", "citeRegEx": "Yanulevskaya et al\\.,? 2008", "shortCiteRegEx": "Yanulevskaya et al\\.", "year": 2008}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks", "author": ["Q. You", "J. Luo", "H. Jin", "J. Yang"], "venue": "AAAI.", "citeRegEx": "You et al\\.,? 2015", "shortCiteRegEx": "You et al\\.", "year": 2015}, {"title": "Exploring principles-of-art features for image emotion recognition", "author": ["S. Zhao", "Y. Gao", "X. Jiang", "H. Yao", "T.-S. Chua", "X. Sun"], "venue": "Proceedings of the ACM International Conference on Multimedia, MM \u201914, 47\u201356. New York, NY, USA: ACM.", "citeRegEx": "Zhao et al\\.,? 2014", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in Neural Information Processing Systems, 487\u2013495.", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "images (Lang 1979; Lang, Bradley, and Cuthbert 1998; Joshi et al. 2011).", "startOffset": 7, "endOffset": 71}, {"referenceID": 9, "context": "images (Lang 1979; Lang, Bradley, and Cuthbert 1998; Joshi et al. 2011).", "startOffset": 7, "endOffset": 71}, {"referenceID": 22, "context": "However, these textual information can only help the retrieval of multimedia content in the cognitive level (Machajdik and Hanbury 2010), i.", "startOffset": 108, "endOffset": 136}, {"referenceID": 4, "context": "In (Hanjalic 2006), the authors call visual emotion prediction affective content analysis.", "startOffset": 3, "endOffset": 18}, {"referenceID": 26, "context": "For example, based on art theory (Itten and Van Haagen 1973; Valdez and Mehrabian 1994), Machajdik and Hanbury (Machajdik and Hanbury 2010) defined eight different kinds of pixel level features (e.", "startOffset": 33, "endOffset": 87}, {"referenceID": 22, "context": "For example, based on art theory (Itten and Van Haagen 1973; Valdez and Mehrabian 1994), Machajdik and Hanbury (Machajdik and Hanbury 2010) defined eight different kinds of pixel level features (e.", "startOffset": 111, "endOffset": 139}, {"referenceID": 31, "context": "In another recent work (Zhao et al. 2014), principles-or-art based features are extracted to classify emotions.", "startOffset": 23, "endOffset": 41}, {"referenceID": 16, "context": "digit recognition (LeCun et al. 1989; Hinton, Osindero, and Teh 2006), image classification (Cire\u015fan et al.", "startOffset": 18, "endOffset": 69}, {"referenceID": 1, "context": "1989; Hinton, Osindero, and Teh 2006), image classification (Cire\u015fan et al. 2011; Krizhevsky, Sutskever, and Hinton 2012), aesthetics estimation (Lu et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 21, "context": "2011; Krizhevsky, Sutskever, and Hinton 2012), aesthetics estimation (Lu et al. 2014) and scene recognition (Zhou et al.", "startOffset": 69, "endOffset": 85}, {"referenceID": 32, "context": "2014) and scene recognition (Zhou et al. 2014).", "startOffset": 28, "endOffset": 46}, {"referenceID": 2, "context": "From ImageNet (Deng et al. 2009) to AVA dataset (Murray, Marchesotti, and Perronnin 2012) and the very recent Places Database (Zhou et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 32, "context": "2009) to AVA dataset (Murray, Marchesotti, and Perronnin 2012) and the very recent Places Database (Zhou et al. 2014), the availability of these data sets have significantly promoted the development of new algorithms on these research areas.", "startOffset": 99, "endOffset": 117}, {"referenceID": 30, "context": "(You et al. 2015) employed CNNs to address visual sentiment analysis, which tries to bridge the high-level, abstract sentiments concept and the image pixels.", "startOffset": 0, "endOffset": 17}, {"referenceID": 17, "context": "In early studies, CNNs (LeCun et al. 1998) have been very successful in document recognition, where the inputs are relatively small images.", "startOffset": 23, "endOffset": 42}, {"referenceID": 11, "context": "(Krizhevsky, Sutskever, and Hinton 2012)), to solve other computer vision problems, such as scene parsing (Grangier, Bottou, and Collobert 2009), feature learning (LeCun, Kavukcuoglu, and Farabet 2010), visual recognition (Kavukcuoglu et al. 2010) and image classification (Krizhevsky, Sutskever, and Hinton 2012).", "startOffset": 222, "endOffset": 247}, {"referenceID": 20, "context": "Currently, most of the works on visual emotion analysis can be classified into either dimensional approach (Nicolaou, Gunes, and Pantic 2011; Lu et al. 2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al.", "startOffset": 107, "endOffset": 157}, {"referenceID": 22, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class.", "startOffset": 30, "endOffset": 95}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class.", "startOffset": 30, "endOffset": 95}, {"referenceID": 31, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class.", "startOffset": 30, "endOffset": 95}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images.", "startOffset": 59, "endOffset": 334}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images. With additional social relationships, they build a factor graph model for the prediction of emotions. Inspired by art and psychology theory, Machajdik and Hanbury (2010) proposed richer hand-tuned features, including color, texture, composition and content features.", "startOffset": 59, "endOffset": 544}, {"referenceID": 0, "context": "2012) or categorical approach (Machajdik and Hanbury 2010; Borth et al. 2013; Zhao et al. 2014), where the former represents emotion in a continuous two dimensional space and in the later model each emotion is a distinct class. We focus on the categorical approach, which has been studied in several previous works. Jia et al. (2012) extract color features from the images. With additional social relationships, they build a factor graph model for the prediction of emotions. Inspired by art and psychology theory, Machajdik and Hanbury (2010) proposed richer hand-tuned features, including color, texture, composition and content features. Furthermore, by exploring the principles of art, Zhao et al. (2014) defined more robust and invariant visual features, such as balance, variety, and gradation.", "startOffset": 59, "endOffset": 709}, {"referenceID": 29, "context": "Visual Emotion Data Sets Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999).", "startOffset": 102, "endOffset": 174}, {"referenceID": 22, "context": "Visual Emotion Data Sets Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999).", "startOffset": 102, "endOffset": 174}, {"referenceID": 31, "context": "Visual Emotion Data Sets Several small data sets have been have been used for visual emotion analysis (Yanulevskaya et al. 2008; Machajdik and Hanbury 2010; Zhao et al. 2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999).", "startOffset": 102, "endOffset": 174}, {"referenceID": 23, "context": "This data set is categorized into eight emotional categories as shown in Figure 1 in a study conducted in (Mikels et al. 2005).", "startOffset": 106, "endOffset": 126}, {"referenceID": 22, "context": "(3) Abstract Paintings: These are images consisting of both color and texture from (Machajdik and Hanbury 2010).", "startOffset": 83, "endOffset": 111}, {"referenceID": 15, "context": "2014), including (1) IAPSSubset: This data set is a subset of the International Affective Picture System (IAPS) (Lang, Bradley, and Cuthbert 1999). This data set is categorized into eight emotional categories as shown in Figure 1 in a study conducted in (Mikels et al. 2005). (2) ArtPhoto: Machajdik and Hanbury (2010) built this data set, which contains photos by professional artists.", "startOffset": 113, "endOffset": 319}, {"referenceID": 22, "context": "Therefore, if we employ the same methodology (5-fold Cross Validation within each data set) (Machajdik and Hanbury 2010; Zhao et al. 2014), we may have only several images in the training data.", "startOffset": 92, "endOffset": 138}, {"referenceID": 31, "context": "Therefore, if we employ the same methodology (5-fold Cross Validation within each data set) (Machajdik and Hanbury 2010; Zhao et al. 2014), we may have only several images in the training data.", "startOffset": 92, "endOffset": 138}, {"referenceID": 2, "context": "The above results suggest that the previous efforts on visual emotion analysis deal with small emotion-centric data sets compared with other vision data sets, such as ImageNet (Deng et al. 2009) and Places (Zhou et al.", "startOffset": 176, "endOffset": 194}, {"referenceID": 32, "context": "2009) and Places (Zhou et al. 2014).", "startOffset": 17, "endOffset": 35}, {"referenceID": 23, "context": "In this work, we use the same eight emotions defined in Table 1, which is derived from a psychological study in (Mikels et al. 2005).", "startOffset": 112, "endOffset": 132}, {"referenceID": 7, "context": "Using the similar approach in (Jia et al. 2012), we query the image search engines (Flickr and Instagram) using the eight emotions as keywords.", "startOffset": 30, "endOffset": 47}, {"referenceID": 10, "context": "Meanwhile, there are also successful applications by fine-tuning the pre-trained ImageNet model, including recognizing image style (Karayev et al. 2013) and semantic segmentation (Long, Shelhamer, and Darrell 2014).", "startOffset": 131, "endOffset": 152}, {"referenceID": 8, "context": "In this work, we employ the same strategy to fine-tune the pre-trained ImageNet reference network (Jia et al. 2014).", "startOffset": 98, "endOffset": 115}, {"referenceID": 7, "context": "Meanwhile, we also employ the weak labels (Jia et al. 2012) to fine-tune another model as described in (You et al.", "startOffset": 42, "endOffset": 59}, {"referenceID": 30, "context": "2012) to fine-tune another model as described in (You et al. 2015).", "startOffset": 49, "endOffset": 66}, {"referenceID": 22, "context": "Next, we follow the same evaluation routine described in (Machajdik and Hanbury 2010) and (Zhou et al.", "startOffset": 57, "endOffset": 85}, {"referenceID": 32, "context": "Next, we follow the same evaluation routine described in (Machajdik and Hanbury 2010) and (Zhou et al. 2014).", "startOffset": 90, "endOffset": 108}, {"referenceID": 22, "context": "Also, we assign larger penalties to true negative samples in the SVM training stage in order to optimize the per class true positive rate as suggested by both (Machajdik and Hanbury 2010) and (Zhou et al.", "startOffset": 159, "endOffset": 187}, {"referenceID": 32, "context": "Also, we assign larger penalties to true negative samples in the SVM training stage in order to optimize the per class true positive rate as suggested by both (Machajdik and Hanbury 2010) and (Zhou et al. 2014).", "startOffset": 192, "endOffset": 210}, {"referenceID": 29, "context": "(Yanulevskaya et al. 2008), Machajdik and Hanbury (Machajdik and Hanbury 2010) and Zhao et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 22, "context": "2008), Machajdik and Hanbury (Machajdik and Hanbury 2010) and Zhao et al.", "startOffset": 29, "endOffset": 57}, {"referenceID": 32, "context": "(Zhou et al. 2014).", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "Figure 5: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 29, "context": "Figure 5: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al. 2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 32, "context": "2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al. 2014), ImageNet-CNN, Noisy-Fine-tunedCNN and Fine-tuned-CNN on IAPS-Subset data set.", "startOffset": 60, "endOffset": 78}, {"referenceID": 22, "context": "Figure 6: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 29, "context": "Figure 6: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al. 2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 32, "context": "2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al. 2014), ImageNet-CNN, Noisy-Fine-tunedCNN and Fine-tuned-CNN on Abstract Paintings data set.", "startOffset": 60, "endOffset": 78}, {"referenceID": 22, "context": "Figure 7: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al.", "startOffset": 53, "endOffset": 81}, {"referenceID": 29, "context": "Figure 7: Per-class true positive rates of Machajdik (Machajdik and Hanbury 2010), Yanulevskaya (Yanulevskaya et al. 2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 32, "context": "2008), Wang (Wei-ning, Ying-lin, and Sheng-ming 2006), Zhao (Zhou et al. 2014), ImageNet-CNN, Noisy-Fine-tunedCNN and Fine-tuned-CNN on ArtPhoto data set.", "startOffset": 60, "endOffset": 78}, {"referenceID": 31, "context": "We thank the authors of (Zhao et al. 2014) for providing their algorithms for comparison.", "startOffset": 24, "endOffset": 42}], "year": 2016, "abstractText": "Psychological research results have confirmed that people can have different emotional reactions to different visual stimuli. Several papers have been published on the problem of visual emotion analysis. In particular, attempts have been made to analyze and predict people\u2019s emotional reaction towards images. To this end, different kinds of hand-tuned features are proposed. The results reported on several carefully selected and labeled small image data sets have confirmed the promise of such features. While the recent successes of many computer vision related tasks are due to the adoption of Convolutional Neural Networks (CNNs), visual emotion analysis has not achieved the same level of success. This may be primarily due to the unavailability of confidently labeled and relatively large image data sets for visual emotion analysis. In this work, we introduce a new data set, which started from 3+ million weakly labeled images of different emotions and ended up 30 times as large as the current largest publicly available visual emotion data set. We hope that this data set encourages further research on visual emotion analysis. We also perform extensive benchmarking analyses on this large data set using the state of the art methods including CNNs.", "creator": "TeX"}}}