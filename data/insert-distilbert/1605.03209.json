{"id": "1605.03209", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "Vocabulary Manipulation for Neural Machine Translation", "abstract": "in order to capture rich analytic language phenomena, neural machine translation models have attempts to use a large vocabulary size, which effectively requires high computing time and large memory usage. in this paper, we must alleviate this issue by erroneously introducing a sentence - level or higher batch - level vocabulary, which is only a very small sub - set of the full output vocabulary. for each sentence or batch, we only naturally predict the target words in its sentence - level or batch - level vocabulary. thus, we reduce both the computing time and the memory usage. our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on unlike a word - to - word translation model approach or a bilingual phrase library learned from a traditional machine translation model. all experimental results on the large - scale english - to - french task show that our method achieves better translation performance by 1 bleu point over the large vocabulary neural machine translation system of jean et al. ( 2015 ).", "histories": [["v1", "Tue, 10 May 2016 20:50:56 GMT  (421kb,D)", "http://arxiv.org/abs/1605.03209v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao mi", "zhiguo wang", "abe ittycheriah"], "accepted": true, "id": "1605.03209"}, "pdf": {"name": "1605.03209.pdf", "metadata": {"source": "CRF", "title": "Vocabulary Manipulation for Neural Machine Translation\u2217", "authors": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah"], "emails": ["abei}@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) (Bahdanau et al., 2014) has gained popularity in recent two years. But it can only handle a small vocabulary size due to the computational complexity. In order to capture rich language phenomena and have a better word coverage, neural machine translation models have to use a large vocabulary.\nJean et al. (2015) alleviated the large vocabulary issue by proposing an approach that partitions the training corpus and defines a subset of the full target vocabulary for each partition. Thus, they only use a subset vocabulary for each partition in\n\u2217Accepted as a short paper in ACL 2016.\nthe training procedure without increasing computational complexity. However, there are still some drawbacks of Jean et al. (2015)\u2019s method. First, the importance sampling is simply based on the sequence of training sentences, which is not linguistically motivated, thus, translation ambiguity may not be captured in the training. Second, the target vocabulary for each training batch is fixed in the whole training procedure. Third, the target vocabulary size for each batch during training still needs to be as large as 30k, so the computing time is still high.\nIn this paper, we alleviate the above issues by introducing a sentence-level vocabulary, which is very small compared with the full target vocabulary. In order to capture the translation ambiguity, we generate those sentence-level vocabularies by utilizing word-to-word and phrase-tophrase translation models which are learned from a traditional phrase-based machine translation system (SMT). Another motivation of this work is to combine the merits of both traditional SMT and NMT, since training an NMT system usually takes several weeks, while the word alignment and rule extraction for SMT are much faster (can be done in one day). Thus, for each training sentence, we build a separate target vocabulary which is the union of following three parts: \u2022 target vocabularies of word and phrase trans-\nlations that can be applied to the current sentence. (to capture the translation ambiguity) \u2022 top 2k most frequent target words. (to cover\nthe unaligned target words) \u2022 target words in the reference of the current\nsentence. (to make the reference reachable) As we use mini-batch in the training procedure, we merge the target vocabularies of all the sentences in each batch, and update only those related parameters for each batch. In addition, we also shuffle the training sentences at the beginning of each epoch, so the target vocabulary for a specific sentence varies in each epoch. In the beam search for the development or test set, we\nar X\niv :1\n60 5.\n03 20\n9v 1\n[ cs\n.C L\n] 1\n0 M\nay 2\n01 6\napply the similar procedure for each source sentence, except the third bullet (as we do not have the reference) and mini-batch parts. Experimental results on large-scale English-to-French task (Section 5) show that our method achieves significant improvements over the large vocabulary neural machine translation system."}, {"heading": "2 Neural Machine Translation", "text": "As shown in Figure 1, neural machine translation (Bahdanau et al., 2014) is an encoder-decoder network. The encoder employs a bi-directional recurrent neural network to encode the source sentence x = (x1, ..., xl), where l is the sentence length, into a sequence of hidden states h = (h1, ..., hl), each hi is a concatenation of a left-to-right \u2212\u2192 hi and a right-to-left \u2190\u2212 hi ,\nhi = [\u2190\u2212 h i\u2212\u2192 h i ] = [\u2190\u2212 f (xi, \u2190\u2212 h i+1)\u2212\u2192 f (xi, \u2212\u2192 h i\u22121) ] ,\nwhere \u2190\u2212 f and \u2212\u2192 f are two gated recurrent units (GRU). Given h, the decoder predicts the target translation by maximizing the conditional log-probability of the correct translation y\u2217 = (y\u22171, ...y \u2217 m), where m is the length of target sentence. At each time t,\nthe probability of each word yt from a target vocabulary Vy is:\np(yt|h, y\u2217t\u22121..y\u22171) \u221d exp(g(st, y\u2217t\u22121, Ht)), (1)\nwhere g is a multi layer feed-forward neural network, which takes the embedding of the previous word y\u2217t\u22121, the hidden state st, and the context state Ht as input. The output layer of g is a target vocabulary Vo, yt \u2208 Vo in the training procedure. Vo is originally defined as the full target vocabulary Vy (Cho et al., 2014). We apply the softmax function over the output layer, and get the probability of p(yt|h, y\u2217t\u22121..y\u22171). In Section 3, we differentiate Vo from Vy by adding a separate and sentence-dependent Vo for each source sentence. In this way, we enable to maintain a large Vy, and use a small Vo for each sentence.\nThe st is computed as:\nst = q(st\u22121, y \u2217 t\u22121, ct) (2)\nct =\n[\u2211l i=1 (\u03b1ti \u00b7 \u2190\u2212 h i)\u2211l\ni=1 (\u03b1ti \u00b7 \u2212\u2192 h i)\n] , (3)\nwhere q is a GRU, ct is a weighted sum of h, the weights, \u03b1, are computed with a feed-forward neural network r:\n\u03b1ti = exp{r(st\u22121, hi, y\u2217t\u22121)}\u2211l\nk=1 exp{r(st\u22121, hk, y\u2217t\u22121)} (4)"}, {"heading": "3 Target Vocabulary", "text": "The output of function g is the probability distribution over the target vocabulary Vo. As Vo is defined as Vy in Cho et al. (2014), the softmax function over Vo requires to compute all the scores for all words in Vo, and results in a high computing complexity. Thus, Bahdanau et al. (2014) only uses top 30k most frequent words for both Vo and Vy, and replaces all other words as unknown words (UNK)."}, {"heading": "3.1 Target Vocabulary Manipulation", "text": "In this section, we aim to use a large vocabulary of Vy (e.g. 500k, to have a better word coverage), and, at the same, to reduce the size of Vo as small as possible (in order to reduce the computing time). Our basic idea is to maintain a separate and small vocabulary Vo for each sentence so that we only need to compute the probability distribution of g over a small vocabulary for each\nsentence. Thus, we introduce a sentence-level vocabulary Vx to be our Vo, which depends on the sentence x. In the following part, we show how we generate the sentence-dependent Vx.\nThe first objective of our method aims to capture the real translation ambiguity for each word, and the target vocabulary of a sentence Vo = Vx is supposed to cover as many as those possible translation candidates. Take the English to Chinese translation for example, the target vocabulary for the English word bank should contain y\u0131\u0301nha\u0301ng (a financial institution) and he\u0301a\u0300n (sloping land) in Chinese.\nSo we first use a word-to-word translation dictionary to generate some target vocaularies for x. Given a dictionary D(x) = [y1, y2, ...], where x is a source word, [y1, y2, ...] is a sorted list of candidate translations, we generate a target vocabulary V Dx for a sentence x = (x1, ..., xl) by merging all the candidates of all words x in x.\nV Dx = l\u22c3\ni=1\nD(xi)\nAs the word-to-word translation dictionary only focuses on the source words, it can not cover the target unaligned functional or content words, where the traditional phrases are designed for this purpose. Thus, in addition to the word dictionary, given a word aligned training corpus, we also extract phrases P (x1...xi) = [y1, ..., yj ], where x1...xi is a consecutive source words, and [y1, ..., yj ] is a list of target words1. For each sentence x, we collect all the phrases that can be applied to sentence x, e.g. x1...xi is a sub-sequence of sentence x.\nV Px = \u22c3\n\u2200xi...xj\u2208subseq(x)\nP (xi...xj),\nwhere subseq(x) is all the possible sub-sequence of x with a length limit.\nIn order to cover target un-aligned functional words, we need top n most common target words.\nV Tx = T (n).\nTraining: in our training procedure, our optimization objective is to maximize the loglikelihood over the whole training set. In order\n1Here we change the definition of a phrase in traditional SMT, where the [y1, ...yj ] should also be a consecutive target words. But our task in this paper is to get the target vocabulary, so we only care about the target word set, not the order.\nto make the reference reachable, besides V Dx , V P x and V Tx , we also need to include the target words in the reference y,\nV Rx = \u22c3\n\u2200yi\u2208y yi,\nwhere x and y are a translation pair. So for each sentence x, we have a target vocabulary Vx:\nVx = V D x \u222a V Px \u222a V Tx \u222a V Rx\nThen, we start our mini-batch training by randomly shuffling the training sentences before each epoch. For simplicity, we use the union of all Vx in a batch,\nVo = Vb = Vx1 \u222a Vx2 \u222a ...Vxb , where b is the batch size. This merge gives an advantage that Vb changes dynamically in each epoch, which leads to a better coverage of parameters.\nDecoding: different from the training, the target vocabulary for a sentence x is\nVo = Vx = V D x \u222a V Px \u222a V Tx ,\nand we do not use mini-batch in decoding."}, {"heading": "4 Related Work", "text": "To address the large vocabulary issue in NMT, Jean et al. (2015) propose a method to use different but small sub vocabularies for different partitions of the training corpus. They first partition the training set. Then, for each partition, they create a sub vocabulary Vp, and only predict and apply softmax over the vocabularies in Vp in training procedure. When the training moves to the next partition, they change the sub vocabulary set accordingly.\nNoise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability. But, as suggested by Jean et al. (2015), those methods are only designed to reduce the time complexity in training, not for decoding."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Data Preparation", "text": "We run our experiments on English to French (EnFr) task. The training corpus consists of approximately 12 million sentences, which is identical\nto the set of Jean et al. (2015) and Sutskever et al. (2014). Our development set is the concatenation of news-test-2012 and news-test-2013, which has 6003 sentences in total. Our test set has 3003 sentences from WMT news-test 2014. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002) with the multi-bleu.perl script.\nSame as Jean et al. (2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80. Given the training set, we first run the \u2018fast align\u2019 (Dyer et al., 2013) in one direction, and use the translation table as our word-to-word dictionary. Then we run the reverse direction and apply \u2018grow-diag-final-and\u2019 heuristics to get the alignment. The phrase table is extracted with a standard algorithm in Moses (Koehn et al., 2007).\nIn the decoding procedure, our method is very similar to the \u2018candidate list\u2019 of Jean et al. (2015), except that we also use bilingual phrases and we only include top 2k most frequent target words. Following Jean et al. (2015), we dump the alignments for each sentence, and replace UNKs with the word-to-word dictionary or the source word."}, {"heading": "5.2 Results", "text": ""}, {"heading": "5.2.1 Reference Reachability", "text": "The reference coverage or reachability ratio is very important when we limit the target vocabulary for each source sentence, since we do not have the reference in the decoding time, and we do not want to narrow the search space into a bad space. Table 1 shows the average reference coverage ratios (in word-level) on the training and development\nsets. For each source sentence x, V \u2217x here is a set of target word indexes (the vocabulary size is 500k, others are mapped to UNK). The average reference vocabulary size V Rx for each sentence is 23.7 on the training set (22.6 on the dev. set). The word-to-word dictionary V Dx has a better coverage than phrases V Px , and when we combine the three sets we can get better coverage ratios. Those statistics suggest that we can not use each of them alone due to the low reference coverage ratios. The last three columns show three combinations, all of which have higher than 90% coverage ratios. As there are many combinations, training an NMT system is time consuming, and we also want to keep the output vocabulary size small (the setting in the last column in Table 1 results in an average 11k vocabulary size for mini-batch 80), thus, in the following part, we only run one combination (top 10 candidates for both V Px and V D x , and top 2k for V Tx ), where the full sentence coverage ratio is 20.7% on the development set."}, {"heading": "5.2.2 Average Size of Vo", "text": "With the setting shown in bold column in Table 1, we list average vocabulary size of Jean et al. (2015) and ours in Table 2. Jean et al. (2015) fix the vocabulary size to 30k for each sentence and mini-batch, while our approach reduces the vocabulary size to 2080 for each sentence, and 6153 for each mini-batch. Especially in the decoding time, our vocabulary size for each sentence is about 14.5 times smaller than 30k."}, {"heading": "5.2.3 Translation Results", "text": "The red solid line in Figure 2 shows the learning curve of our method on the development set, which picks at epoch 7 with a BLEU score of 30.72. We also fix word embeddings at epoch 5, and continue several more epochs. The corresponding blue dashed line suggests that there is no significant difference between them.\nWe also run two more experiments: V Dx \u222a V Tx and V Px \u222aV Tx separately (always have V Rx in training). The final results on the test set are 34.20\nand 34.23 separately. Those results suggest that we should use both the translation dictionary and phrases in order to get better translation quality.\nTable 4 shows the single system results on EnFr task. The standard Moses in Cho et al. (2014) on the test set is 33.3. Our target vocabulary manipulation achieves a BLEU score of 34.45 on the test set, and 35.11 after the UNK replacement. Our approach improves the translation quality by 1.0 BLEU point on the test set over the method of Jean et al. (2015). But our single system is still about 2 points behind of the best phrase-based system (Durrani et al., 2014)."}, {"heading": "5.2.4 Decoding with Different Top n Most Common Target Words", "text": "Another interesting question is what is the performance if we vary the size top n most common target words in V Tx . As the training for NMT is time consuming, we vary the size n only in the de-\ncoding time. Table 3 shows the BLEU scores on the development set. When we reduce the n from 2000 to 50, we only loss 0.1 points, and the average size of sentence level Vo is reduced to 202, which is significant smaller than 2067 (shown in Table 2). But we should notice that we train our NMT model in the condition of the bold column in Table 2, and only test different n in our decoding procedure only. Thus there is a mismatch between the training and testing when n is not 2000."}, {"heading": "5.2.5 Speed", "text": "In terms of speed, as we have different code bases2 between Jean et al. (2015) and us, it is hard to conduct an apple to apple comparison. So, for simplicity, we run another experiment with our code base, and increase Vb size to 30k for each batch (the same size in Jean et al. (2015)). Results show that increasing the Vb to 30k slows down the training speed by 1.5 times."}, {"heading": "6 Conclusion", "text": "In this paper, we address the large vocabulary issue in neural machine translation by proposing to use a sentence-level target vocabulary Vo, which is much smaller than the full target vocabulary Vy. The small size of Vo reduces the computing time of the softmax function in each predict step, while the large vocabulary of Vy enable us to model rich language phenomena. The sentence-level vocabulary Vo is generated with the traditional word-to-word and phrase-to-phrase translation libraries. In this way, we decrease the size of output vocabulary Vo under 3k for each sentence, and we speedup and improve the large-vocabulary NMT system.\n2Two code bases share the same architecture, initial states, and hyper-parameters. We simulate Jean et al. (2015)\u2019s work with our code base in the both training and test procedures, the final results of our simulation are 29.99 and 34.16 on dev. and test sets respectively. Those scores are very close to Jean et al. (2015)."}, {"heading": "Acknowledgment", "text": "We thank the anonymous reviewers for their comments."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints, September.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Edinburghs phrase-based machine translation systems for wmt-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of WMT, pages 97\u2013104, Baltimore, Maryland, USA, June.", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Chris Dyer", "Victor Chahuneau", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "Dyer et al\\.,? 2013", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyvarinen."], "venue": "Proceedings of AISTATS.", "citeRegEx": "Gutmann and Hyvarinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2010}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of ACL, pages 1\u201310, Beijing, China, July.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst."], "venue": "Proceedings", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "International Conference on Learning Representations: Workshops Track.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems, volume 21, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton.,? 2009", "shortCiteRegEx": "Mnih and Hinton.", "year": 2009}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu."], "venue": "Proceedings of NIPS, pages 2265\u2013 2273.", "citeRegEx": "Mnih and Kavukcuoglu.,? 2013", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proceedings of the 29th International Conference on Machine Learning, pages 1751\u20131758.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of ACL, pages 311\u2013318, Philadephia, USA, July.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of NIPS, pages 3104\u20133112, Quebec, Canada, December.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).", "startOffset": 202, "endOffset": 221}, {"referenceID": 0, "context": "Neural machine translation (NMT) (Bahdanau et al., 2014) has gained popularity in recent two years.", "startOffset": 33, "endOffset": 56}, {"referenceID": 0, "context": "Neural machine translation (NMT) (Bahdanau et al., 2014) has gained popularity in recent two years. But it can only handle a small vocabulary size due to the computational complexity. In order to capture rich language phenomena and have a better word coverage, neural machine translation models have to use a large vocabulary. Jean et al. (2015) alleviated the large vocabulary issue by proposing an approach that partitions the training corpus and defines a subset of the full target vocabulary for each partition.", "startOffset": 34, "endOffset": 346}, {"referenceID": 5, "context": "However, there are still some drawbacks of Jean et al. (2015)\u2019s method.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "As shown in Figure 1, neural machine translation (Bahdanau et al., 2014) is an encoder-decoder network.", "startOffset": 49, "endOffset": 72}, {"referenceID": 1, "context": "Vo is originally defined as the full target vocabulary Vy (Cho et al., 2014).", "startOffset": 58, "endOffset": 76}, {"referenceID": 0, "context": "As Vo is defined as Vy in Cho et al. (2014), the softmax function over Vo requires to compute all the scores for all words in Vo, and results in a high computing complexity.", "startOffset": 26, "endOffset": 44}, {"referenceID": 0, "context": "Thus, Bahdanau et al. (2014) only uses top 30k most frequent words for both Vo and Vy, and replaces all other words as unknown words (UNK).", "startOffset": 6, "endOffset": 29}, {"referenceID": 4, "context": "Noise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability.", "startOffset": 29, "endOffset": 128}, {"referenceID": 10, "context": "Noise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability.", "startOffset": 29, "endOffset": 128}, {"referenceID": 7, "context": "Noise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability.", "startOffset": 29, "endOffset": 128}, {"referenceID": 9, "context": "Noise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability.", "startOffset": 29, "endOffset": 128}, {"referenceID": 8, "context": ", 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability.", "startOffset": 61, "endOffset": 84}, {"referenceID": 4, "context": "To address the large vocabulary issue in NMT, Jean et al. (2015) propose a method to use different but small sub vocabularies for different partitions of the training corpus.", "startOffset": 46, "endOffset": 65}, {"referenceID": 4, "context": "Noise-contrastive estimation (Gutmann and Hyvarinen, 2010; Mnih and Teh, 2012; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013) and hierarchical classes (Mnih and Hinton, 2009) are introduced to stochastically approximate the target word probability. But, as suggested by Jean et al. (2015), those methods are only designed to reduce the time complexity in training, not for decoding.", "startOffset": 30, "endOffset": 292}, {"referenceID": 11, "context": "We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002) with the multi-bleu.", "startOffset": 75, "endOffset": 98}, {"referenceID": 5, "context": "to the set of Jean et al. (2015) and Sutskever et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 5, "context": "to the set of Jean et al. (2015) and Sutskever et al. (2014). Our development set is the concatenation of news-test-2012 and news-test-2013, which has 6003 sentences in total.", "startOffset": 14, "endOffset": 61}, {"referenceID": 13, "context": "(2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80.", "startOffset": 58, "endOffset": 72}, {"referenceID": 3, "context": "Given the training set, we first run the \u2018fast align\u2019 (Dyer et al., 2013) in one direction, and use the translation table as our word-to-word dictionary.", "startOffset": 54, "endOffset": 73}, {"referenceID": 6, "context": "The phrase table is extracted with a standard algorithm in Moses (Koehn et al., 2007).", "startOffset": 65, "endOffset": 85}, {"referenceID": 4, "context": "Same as Jean et al. (2015), our full vocabulary size is 500k, we use AdaDelta (Zeiler, 2012), and mini-batch size is 80.", "startOffset": 8, "endOffset": 27}, {"referenceID": 5, "context": "In the decoding procedure, our method is very similar to the \u2018candidate list\u2019 of Jean et al. (2015), except that we also use bilingual phrases and we only include top 2k most frequent target words.", "startOffset": 81, "endOffset": 100}, {"referenceID": 5, "context": "In the decoding procedure, our method is very similar to the \u2018candidate list\u2019 of Jean et al. (2015), except that we also use bilingual phrases and we only include top 2k most frequent target words. Following Jean et al. (2015), we dump the alignments for each sentence, and replace UNKs with the word-to-word dictionary or the source word.", "startOffset": 81, "endOffset": 227}, {"referenceID": 5, "context": "With the setting shown in bold column in Table 1, we list average vocabulary size of Jean et al. (2015) and ours in Table 2.", "startOffset": 85, "endOffset": 104}, {"referenceID": 5, "context": "With the setting shown in bold column in Table 1, we list average vocabulary size of Jean et al. (2015) and ours in Table 2. Jean et al. (2015) fix the vocabulary size to 30k for each sentence and mini-batch, while our approach reduces the vocabulary size to 2080 for each sentence, and 6153 for each mini-batch.", "startOffset": 85, "endOffset": 144}, {"referenceID": 1, "context": "Moses from Cho et al. (2014) N/A 33.", "startOffset": 11, "endOffset": 29}, {"referenceID": 2, "context": "best from Durrani et al. (2014) N/A 37.", "startOffset": 10, "endOffset": 32}, {"referenceID": 2, "context": "But our single system is still about 2 points behind of the best phrase-based system (Durrani et al., 2014).", "startOffset": 85, "endOffset": 107}, {"referenceID": 1, "context": "The standard Moses in Cho et al. (2014) on the test set is 33.", "startOffset": 22, "endOffset": 40}, {"referenceID": 1, "context": "The standard Moses in Cho et al. (2014) on the test set is 33.3. Our target vocabulary manipulation achieves a BLEU score of 34.45 on the test set, and 35.11 after the UNK replacement. Our approach improves the translation quality by 1.0 BLEU point on the test set over the method of Jean et al. (2015). But our single system is still about 2 points behind of the best phrase-based system (Durrani et al.", "startOffset": 22, "endOffset": 303}, {"referenceID": 5, "context": "In terms of speed, as we have different code bases2 between Jean et al. (2015) and us, it is hard to conduct an apple to apple comparison.", "startOffset": 60, "endOffset": 79}, {"referenceID": 5, "context": "In terms of speed, as we have different code bases2 between Jean et al. (2015) and us, it is hard to conduct an apple to apple comparison. So, for simplicity, we run another experiment with our code base, and increase Vb size to 30k for each batch (the same size in Jean et al. (2015)).", "startOffset": 60, "endOffset": 285}, {"referenceID": 5, "context": "We simulate Jean et al. (2015)\u2019s work with our code base in the both training and test procedures, the final results of our simulation are 29.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "We simulate Jean et al. (2015)\u2019s work with our code base in the both training and test procedures, the final results of our simulation are 29.99 and 34.16 on dev. and test sets respectively. Those scores are very close to Jean et al. (2015).", "startOffset": 12, "endOffset": 241}], "year": 2016, "abstractText": "In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).", "creator": "TeX"}}}