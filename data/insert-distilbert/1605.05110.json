{"id": "1605.05110", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Incorporating Loose-Structured Knowledge into Conversation Modeling via Recall-Gate LSTM", "abstract": "modeling human conversations is \" the essence for processes building satisfying chat - bots with multi - turn dialog ability. conversation modeling will notably benefit from domain knowledge since the relationships between sentences types can be clarified due to semantic hints introduced by knowledge. in this paper, a deep neural network is proposed to incorporate enhanced background knowledge for conversation modeling. through a specially designed recall gate, domain knowledge can be transformed indirectly into the extra global memory base of long and short - term memory ( lstm ), so as to enhance lstm by cooperating with its local link memory to capture the greater implicit semantic relevance between sentences within both conversations. \u201c in addition, this paper introduces the traditional loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the recall memory gate. historically our model field is often evaluated on the context - object oriented response selecting task, and experimental results on both this two datasets have shown that our approach is possibly promising for modeling human conversations and building key components of automatic chatting systems.", "histories": [["v1", "Tue, 17 May 2016 11:03:25 GMT  (548kb)", "http://arxiv.org/abs/1605.05110v1", "10 pages, 5 figures"], ["v2", "Mon, 6 Feb 2017 03:43:17 GMT  (399kb)", "http://arxiv.org/abs/1605.05110v2", "under review of IJCNN 2017; 10 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhen xu", "bingquan liu", "baoxun wang", "chengjie sun", "xiaolong wang"], "accepted": false, "id": "1605.05110"}, "pdf": {"name": "1605.05110.pdf", "metadata": {"source": "CRF", "title": "Incorporating Loose-Structured Knowledge into LSTM with Recall Gate for Conversation Modeling", "authors": ["Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang"], "emails": ["wangxl}@insun.hit.edu.cn", "baoxwang@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n05 11\n0v 1\n[ cs\n.C L\n] 1\n7 M\nay 2\n01 6"}, {"heading": "1 Introduction", "text": "In recent years, the demand on Chat-bots has changed from answering simple questions as a toy to performing smooth open-domain conversations like real humans. Some good explorations have been conducted by the open-domain chatbots like Clever-bot1, etc., and their final goal is to\n1http://www.cleverbot.com/\nreply almost all kinds of queries instead of completing tasks given by users as the normal dialog system(Mu and Yin, 2010) does. The ability of communicating like a real human is critical for keeping users\u2019 activity, thus various additional functions are possible to be introduced during the dialog and even the commercial services can be adopted (See Duer2).\nApparently, the conversation between humans takes contextual relevance as the essence with no doubt(Ribeiro et al., 2015), that is, the response should be semantic relevant with both the \u201cdirect\u201d question and the history contents. Generally, it is a challenging task for Chat-bots to detect semantic clues of the conversation and provide the contextaware replies. For this purpose, the conversation should be well modeled, so that the semantic continuation and switching of the context can be sensed and the appropriate candidate replies is possible to be further selected.\nIntuitively, the sequence modeling approaches have great potential to capture the context semantic clues. Indeed, the Recurrent Neural Network (RNN) and Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based methods have already utilized for this\n2http://duer.baidu.com/\ntask (Sutskever et al., 2014; Cho et al., 2014). The end-to-end learning ability of such models has already brought promising results to some nontrivial problems like sequence-to-sequence mapping (Shang et al., 2015).\nObviously, conversation modeling can benefit from human knowledge, only if the proper strategy is taken to adopt knowledge into machine learning models reasonably. As shown by the example in Figure 1, the semantic clue can be easily captured if the prior knowledge about celebrity name is involved. In human conversations, such background knowledge generally takes the role of global memory, which can be naturally recalled by humans at the right moment. Heuristically, it is of great value to simulate the knowledge-supported conversation behavior of human beings, by capturing the acting mechanisms of human knowledge in conversation flows, so as to recognize semantic relevances in conversations more precisely and further find better responses for creating human-like chat-agents.\nThis paper proposes a deep neural network to address conversation modeling in the given domain. By introducing a new trainable gate to recall the global domain memory, our deep learning model incorporates background knowledge to enhance the sequence semantic modeling ability of LSTM. Methodologically, this paper concentrates on learning the implicit working mechanism of global memory in capturing semantic clues in conversations, including the functions of recalling and incorporating global memory with shortterm memory. In addition, our model obtains global memory from the loose structured knowledge base. The knowledge base is built by organizing entity-attribute pairs as items which can be absorbed into our model by Recall gate directly. The preparation of such knowledge base requires little amount of manual work, by contrast, building complex-structured knowledge (e.g., graph) is indeed not a trivial task."}, {"heading": "2 Related Work", "text": "Previous studies on conversation modeling are mainly task-completion oriented, such as act classification, state tracking in spoken field (Jurc\u0131cek et al., 2011; Williams et al., 2013; Henderson et al., 2013).With the rapid accumulation of available conversation data from SNS\n(e.g., Twitter3, Weibo4), data-driven tasks like response ranking or generation have attracted more attention in this field (Serban et al., 2015b).\nEarly studies on conversation modeling focus on one turn conversation including one query and a response, thus heuristically the Information Retrieval (IR) or Statistical Machine Translation (SMT) based methods are taken to get responses. IR systems are built on movie scripts (Banchs and Li, 2012) or subtitles (Ameixa et al., 2014) to select responses related to the given questions. Ritter et al. (2011) introduce SMT to generate responses by taking query-response pairs as parallel corpus and get better performance than the IR approaches. To improve the readability of responses generated by SMT, Ji et al. (2014) propose an IR framework by introducing learning to rank strategy with the outputs of SMT as matching features.\nThe multi-turn conversation modeling task becomes even more challenging after taking history contents into consideration, thus complex approaches with better modeling ability are actually needed. Since it is natural to consider the conversation as a sequence of short texts, some studies introduce Neural Network based Sequence-toSequence (S2S) framework to address the conversation modeling issue. Vinyals and Le (2015) predict the next sentence based on the given one or several previous sentences with S2S directly. Serban et al. (2015a) and Shang et al. (2015) take response generation as the decoding process with the given distributed representation generated from previous sentences by the encoding process. The difference between (Serban et al., 2015a) and (Shang et al., 2015) lies in that Shang et al. (2015) add a feedback attention mechanism in the encoder-decoder framework.\nComparing with the generating strategies, response selecting approach is an option that avoids low readability naturally. Sordoni et al. (2015) propose an RNN model to evaluate the relevance between contexts, questions and their candidate response, and take the relevance as the feature to generate response. Lowe et al. (2015) and Kadlec et al. (2015) also present a ranking strategy on the Ubuntu Dialogue Corpus. In their work, an affinity model is used to measure the relevance between the context and a candidate reply,\n3https://twitter.com/ 4http://weibo.com/\nand the relevance score is take for response selection.\nAs pointed by Vinyals and Le (2015), the lack of general background knowledge is an obvious limitation of the current conversation modeling approaches. Noticing that previous studies rarely take background knowledge smoothly in the conversation modeling architectures, this paper tries to explore the effect of the background knowledge to the Neural Network based models."}, {"heading": "3 LSTM with Recall Gate for Conversation Modeling", "text": "As mentioned in Section 1, our motivation of conversation modeling is to provide reliable evidence to detect users\u2019 context-aware queries and further select best answers based on the conversation history, for building automatic dialog systems. Apparently, the utterance5 in a conversation is very likely to be semantically relevant with both the history context and the background knowledge. The context is made up of the sequence of utterances appearing in conversation prior to the query and response. The semantic relevance between utterances implicitly performs as the clue for conversation modeling, meanwhile, background knowledge offers essential hints to enhance the effect of semantic clues. In this section, we propose a deep learning framework taking the specially designed Recall gate to smoothly integrate knowledge hints into LSTM for capturing such semantic clues.\n5This paper takes utterance as the alias of the sentence in conversations. The usage of this word follows (Hurford et al., 2007)"}, {"heading": "3.1 Architecture", "text": "Figure 2 illustrates our conversation modeling framework. The proposed architecture is composed of sentence modeling, knowledge triggering, and conversation modeling components, the details of which will be given as follows:\nConversation Modeling: Since a conversation can be considered as a sequence of utterances that could be mapped into dense semantic vectors by the sentence modeling component, basically we take LSTM based methodology to address the conversation modeling task. As Figure 2 shows, there are two kinds of inputs: background knowledge and utterance vectors (including context, query and candidate response). Obviously, for the traditional LSTM, it is difficult to absorb the knowledge vectors properly.\nIn order to make background knowledge well involved into the deep neural network, this paper proposes a special Recall gate to enhance LSTM for conversation modeling. Inspired by our observation of human memory, the Recall-Gate is designed to convert the loose-structured domain knowledge to the global memory, which cooperates with the local memory in the cell of LSTM to provide evidence to judge whether an utterance is related to the dialog history or not. The details of LSTM with Recall gate(r-LSTM) will be given in Subsection 3.2.\nIn our work, the target of conversation modeling is to select better responses oriented to the given conversations (composed of the context and a query), so as to promote user\u2019s satisfaction in human-machine conversation. For this goal, a binary classifier is set to get the classifying confi-\ndence and decide whether a candidate response is related to the current conversation or not.\nSentence modeling: As the basis of the entire task, sentence modeling aims to provide reasonable semantic representations as inputs of the upper layers. The RNN based methods, especially LSTM, have got good performance on sentence modeling recently, thus our framework adopts LSTM as the component mapping utterances into the real-valued semantic space (Sutskever et al., 2014; Cho et al., 2014). The process of sentence modeling introduced in this paper is shown in Figure 3.\nThe last hidden state ht is always taken as a summary of the sentence, thus we adopt ht as the representation of each utterance in conversation. In our implementation, the LSTM is first pretrained as a language model for initialization, after that, it is tuned according to the end-to-end training of the network in Figure 2\nKnowledge Triggering: As shown in the Figure 2, contexts will trigger the related background knowledge from the loose-structured knowledge base (KB), and the related knowledge will be taken as an input signal of our model and transformed into global memory. We take a simple knowledge trigger method, assuming that each entity can be correctly located in the KB.\nFor the given context, the process of generating related knowledge vector is as follows: First, the context is mapped to a bag of entities by matching words to a pre-defined vocabulary described in section 3.3. After that, attributes in pairs got from the previous step are ranked by their frequencies. We select top N(= 10, 20, ...) attributes as background knowledge of the given context. Finally the pairs are mapped to dense vectors (Sukhbaatar et al., 2015) by Equation 1:\nkb = i=N\u2211\ni=1\nEmbedding(attributei) (1)\nwhere kb stands for the knowledge vector to be absorbed by our model. The embeddings of attributes are pre-trained on a open-domain corpus."}, {"heading": "3.2 LSTM Cell with Recall Gate", "text": "Apparently, prior knowledge is of great value in the process of sequence understanding (Ovchinnikova, 2012). Nevertheless, integrating knowledge into deep learning architectures is still a challenging work. Since knowledge is indeed a global signal, and it is unwise to simply take knowledge as an additional utterance in conversation understanding, which is supported by the experimental results in Section 4. To make domain knowledge perform as a global memory for greater effect, we design a special model component for LSTM.\nFigure 4 shows the detail of the LSTM cell with Recall gate (r-LSTM cell). Basically, our model absorbs the knowledge item embedding converted from the triggering results (as described by Subsection 3.1), and drives it cooperating with the previous sequence state and the present network input to summarize the current status for the next timestep, by adding a new recalling mechanism. Given previous hidden state ht\u22121, previous memory ct\u22121 , current input xt and background knowledge kb, we define the output of Recall gate as follows:\nrt = \u03c3(Wri[ht\u22121, xt] +Wrcct\u22121 +Wrkkb+ br)\n(2) where Wri, Wrc and Wrk indicate the connection weights from current input(including previous state and xt), memory cell and background knowledge respectively, and br is bias of the recall gate.\nGenerated by Recall gate, rt indicates the proportion of global knowledge kb taking effect in the determination of current memory cell ct. This pro-\ncedure is described by Equation 3:\nct = ft \u2217 ct\u22121 + it \u2217 cinput + rt \u2217 kb (3)\nThe current hidden state ht can be obtained by:\nht = ot \u2217 tanh(ct) (4)\nwhere ot indicates the output gate of LSTM. The computations of forget gate, input gate, output gate and input cell are the same as normal LSTM. From the equations above it can be seen that in our methodology, external domain knowledge plays a significant role in generating the memory cell of LSTM and influencing the final hidden state consequently.\nIn the conversation model illustrated by Figure 2, the r-LSTM reads one utterance per timestep and generate a hidden state to represent the current conversation under the condition of global knowledge. With the multiplication to rt shown in Equation 3, global knowledge kb including semantic clues takes effect on memory cell. In this way, the semantic relevance between utterances in conversation process could be sensed well by rLSTM. As the higher-level global memory rather than the shallow input, external knowledge will be utilized more effectively in r-LSTM based conversation model.\nComparing with the normal LSTM, the r-LSTM could be more powerful in capturing semantic clues with better effective support of external knowledge because of the recall mechanism of Recall gate. In other words, the Recall-Gate makes rLSTM to perform better on incorporating external knowledge. In contrast to r-LSTM, it is difficult for the general LSTM to involve external knowledge effectively."}, {"heading": "3.3 Loose-Structured Knowledge Base", "text": "It is known that building a complex-structured knowledge (e.g., WordNet6, Yago7) requires large amount of human work. Obviously, an easily-built knowledge base is quite valuable for applications. This paper introduces loose-structured knowledge base composed of items with a flexible format \u201centity-attribute\u201d, in which the attributes can be either other entities or the related keywords.\nThe extraction process of entity-attribute pairs is described as following: (1) For a given domain,\n6http://wordnet.princeton.edu/ 7http://www.mpi-inf.mpg.de/departments/databases-and-\ninformation-systems/research/yago-naga/yago/\nwe extract entity or attributes from the domainspecific corpus by using statistic metrics such as tf-idf, entropy etc. After that, KL-divergence between domain-specific and general corpora to filter plain words and get entities and attributes for special domain; (2) According to the vocabulary composed of entities and attributes, counting the frequency of \u201centity-attribute\u201d pair with a slide window. (3) The final knowledge base is obtained according to the frequency oriented statistics."}, {"heading": "4 Experiment", "text": "In this part, our conversation modeling approach will be evaluated on context-oriented best response selection task, which is considered as a binary classification problem as illustrated by Figure 2, that is, the goal of our model is to give higher confidence to context related responses."}, {"heading": "4.1 Experimental Setup", "text": "DataSet: We execute experiments on two datasets in special domains: Baidu TieBa Corpus and Ubuntu Corpus8. The Tieba dataset is composed of multi-turn conversations in the celebrity domain, extracted from crawled Tieba threads after several matching and filtering work. The Ubuntu corpus includes conversations collected from Ubuntu chat rooms focusing on technical supports.\nFor training the classifier, we adapt sampling method and dataset constructing strategy in Lowe et al. (2015) to generate training and testing set. For each positive sample in the training set, one negative sample is prepared. By contrast, for every positive sample in testing and validation set, there are 9 negative ones generated correspondingly. Finally, we construct a training set including 1 million conversations for both Tie and Ubuntu, and sample 50,000 conversations for validation and testing respectively.\nFigure 5 shows the distributions of turn numbers of conversations in Tieba and Ubuntu respectively. Our model needs contexts to trigger background knowledge, so we select conversations with turnnumber ranging from 3 to 7 for experiments.\nEvaluation Metric: Since our model is applied in a classification problem, and the output confidence scores can be also used for ranking the can-\n8We have uploaded the Ubuntu dataset to https://www.dropbox.com/s/2fdn26rj6h9bpvl/ubuntu data.zip?dl=0 and the Tieba corpus is confidential in the authors\u2019 working organization.\ndidates, we take Accuracy to measure the performance of classifiers and introduce Recall@k to evaluate the ranking ability of the approaches. The metric Recall@k is applied in the work of Lowe et al. (2015) for the response selection task.\nBaseline: To illustrate the performance of our model, we introduce five methods as the baselines, The descriptions of which are given as follows:\nMLP and LSTM: Multi-Layer Perceptron(MLP) is the first baseline in our experiment. For our task, this model is designed to take the concatenated sentence vectors of both contexts and candidate responses as input, and outputs the binary classification results. LSTM is an intuitive method for our task, because this model can be directly used to model conversations by taking each utterance embedding as the input of each time-step.\nIt should be noted that neither of the baselines above considers external knowledge for prediction, and the architecture of LSTM is similar with Figure 2 but r-LSTM is not involved.\nAffinity Model: As assumed by Lowe et al. (2015), context vectors and response vectors generate by RNN or LSTM can be aligned according to a relevance matrix, which can be formulated as:\np(r|c) = \u03c3(cTMr + b)\nwhere c and r are the embedding results of context and response given by RNN or LSTM, and M indicates the relevance matrix. This methodology achieves the state-of-art results on the Ubuntu corpus without domain knowledge, thus the reason for introducing this baseline is to show the impact of reasonable knowledge integration.\nMLP+kb and LSTM+kb: To examine the effectiveness of domain knowledge in conversation modeling, we incorporate knowledge into the baselines MLP and LSTM above in a trivial way, named as MLP+kb and LSTM+kb. MLP+kb\nadopts knowledge vector by padding it to conversation vector(concatenated vectors of utterances), and LSTM+kb takes knowledge embeddings as the input of the first time-step.\nModel Arch and Training Details: Pre-training of word embeddings: For Ubuntu corpus, we take word vectors from (Lowe et al., 2015) as the initialization of embeddings. For Tieba corpus, the pre-trained character vectors on threads are utilized. All of the approaches (including baselines) in our experiments update word embeddings during the training procedure. The detailed dimension settings are given in Table 1.\nFor all the models, the training process runs until the loss of validation set in current iteration is larger than the previous one. All the models are trained on a K40m GPU with 12G memory."}, {"heading": "4.2 Results and Analysis", "text": "Table 2 and Table 3 list the experimental results of all the approaches for the context-oriented response selection task, on Chinese Tieba and English Ubuntu corpus respectively.\nFrom Table 2 and Table 3, it can be observed that our model (r-LSTM) notably outperforms the baseline methods on both datasets by all the metrics as expected. Basically, we ascribe the promotions to the fact that our model effectively captures semantic clues with the sequence modeling architecture of r-LSTM, meanwhile, benefited from smoothly incorporating the loose structured knowledge of the Recall Gate (see Figure 4), rLSTM is able to detect implicit semantic hints in both conversation histories and candidate responses. In addition, the results also shows that vectors generated by LSTM based sentence modeling can represent sentences well as described in subsection 3.1.\nThe results of Affinity Model shown in Table 3 is lower than those reported in Lowe et al. (2015). There are to possible explanations for this observation: First, our experimental dataset extracted\nfrom the raw corpus are different from that used in (Lowe et al., 2015), even though the extraction method is the same as Lowe et al. (2015). Thus the change of data distribution may influence the performance of this model; Second, the average length of contexts (by word) in the dataset used in this paper is 57.81, by contrast, in (Lowe et al., 2015) the average context length is 108.93. As mentioned in section 1, semantic clues in context are essential for response selection, so more context could offer more semantic information. We will open the dataset to other researchers for further comparison.\nIgnoring RNN Affinity model, the baseline methods shown in the tables can be classified into two categories: MLP based methods and LSTM based ones. It\u2019s clear that LSTM based methods perform better than the MLP based approaches, which indicates the ability of capturing semantic clues plays significant role in our task, and obviously sequence models have congenital advantage. Heuristically, explicit or implicit semantic relationships between utterances exist in humanto-human conversations to keep the continuity of conversations. Due to the \u201cmemory cell\u201d, LSTM can model the long-dependent semantic relationships effectively, so approaches based on LSTM make better understanding of conversations.\nAs shown in Table 2 and 3, methods taking account of background knowledge get 3%-7% improvements comparing with the ones without any external knowledge on the best response selection task. This phenomenon indicates that knowledge is one of the primary factors for conversation un-\nderstanding. In detail, there are two aspects influencing the improvement of approaches incorporating domain knowledge: (1) The quality and quantity of knowledge. Benefited from the characteristics of loose structured knowledge, we can build a knowledge base with large amount of domain specified information and update it with little manual work. Even the quality of such loose structured knowledge is lower than the knowledge base built with much human efforts (like Knowledge Graph), our model can still effectively utilize it. (2) The fusion strategy between knowledge and utterances in conversation modeling. Our model r-LSTM obtains up to 2-4% improvement by all the metrics than \u201cLSTM+kb\u201d on both datasets. This demonstrates the importance of strategies for incorporating external knowledge in conversation modeling. Moreover, there are semantic gaps between loose structured knowledge and sentences, it\u2019s troublesome for methods like \u201cLSTM+kb\u201d, to overcome these gaps for methods taking knowledge as an additional utterance directly.\nThe Recall@3 of our model on Tieba and Ubuntu dataset is 75.27% and 85.78% respectively, that is, most best responses can be recalled in the top three candidate responses (ranked by confidence). This can be attributed to the recall mechanism of r-LSTM involving background knowledge as global memory. As the global input, external domain knowledge can be recalled by r-LSTM and influence the local memory at right moments in the whole conversation process. The introduction of background knowledge provide essential semantic hints thus enhances the ability to\ndetect the semantic relevances between sentences. Due to the limitation of quality and average length of contexts in Tieba, the models\u2019 overall performances are a little lower than those on the Ubuntu, as shown in Table 2 and 3. According the distribution of turn numbers shown in Figure 5, there are few conversations with more than 5 turns in the Tieba corpus. Consequently, there are less history contents involving semantic clues for selecting response as mentioned in section 1."}, {"heading": "4.3 Case Study", "text": "In order to further show the working mechanism in our model intuitively, we give two cases comparing our r-LSTM and LSTM+kb in Table 4. For better understanding, we translate both the contexts and the candidate responses into English from Chinese. As shown by Table 4, each case contains the context composed of three utterances and three candidate responses with labels and predicted scores. The label 1 indicates the true response to the given context. Scores represent the confidence of responses as the best one.\nFrom the table it can be seen that r-LSTM gives highest score to best response, while \u201cLSTM+kb\u201d offers unsatisfied results. Both methods shown in 4 involved background knowledge, so this result is caused by the effectiveness of utilizing knowledge. In our r-LSTM architecture, background knowledge is considered as global memory and can be recalled in the conversation modeling process, and our Recall Gate could also build semantic relationships of utterances validly. Taking knowledge as input directly, LSTM+kb makes less use of knowledge in the modeling process because of the semantic gap between sentences and knowledge items. Furthermore, based on the ob-\nservation of human conversations, it is reasonable to consider background knowledge as global signal of the neural network, instead of an additional input as LSTM+kb does. The large range of confidence scores given by r-LSTM also shows that our model does well on both selecting best responses and recognizing inappropriate responses.\nIn both the two samples, the second response also gets high confidence score, and LSTM+kb even takes it as the best one. Actually, such responses are general responses without any key points. Even though they can be taken to reply the queries, such general answers are not promising because less dialog turns are expected after them. Thus distinguishing general and best responses is of great value for chat-agents, and our model have potential in this scenario."}, {"heading": "5 Conclusions and Future Work", "text": "In this paper, we proposed a deep learning architecture to incorporate loose structured knowledge for end-to-end conversation modeling. Our approach shows good potential on the contextoriented response selecting task.\nThe contributions of this paper can be summarized as follows: (1) By investigating the influence of domain knowledge on conversation understanding, we present a LSTM framework with a designed Recall gate to utilize knowledge smoothly and effectively. Transforming knowledge into global memory, the Recall gate enables LSTM to integrate global memory into sequential local memory to conduct enhanced conversation modeling. (2) To guarantee the flexibility of domain knowledge base for practical usage, this paper introduces the loose-structured knowledge base organized as \u201centity-attribute\u201d pairs, which can be\nextracted easily from raw corpora. The knowledge can be directly absorbed by the Recall gate after being triggered according to dialog contexts.\nOur future study will be conducted along the following directions: first, we will continue exploring the working mechanism of global memory to refine our Recall gate; Second, the utilization of our model for open-domain conversation modeling will be investigated."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Modeling human conversations is the essence for building satisfying chat-bots with multi-turn dialog ability. Conversation modeling will notably benefit from domain knowledge since the relationships between sentences can be clarified due to semantic hints introduced by knowledge. In this paper, a deep neural network is proposed to incorporate background knowledge for conversation modeling. Through a specially designed Recall gate, domain knowledge can be transformed into the extra global memory of Long Short-Term Memory (LSTM), so as to enhance LSTM by cooperating with its local memory to capture the implicit semantic relevance between sentences within conversations. In addition, this paper introduces the loose structured domain knowledge base, which can be built with slight amount of manual work and easily adopted by the Recall gate. Our model is evaluated on the context-oriented response selecting task, and experimental results on both two datasets have shown that our approach is promising for modeling human conversations and building key components of automatic chatting systems.", "creator": "LaTeX with hyperref package"}}}