{"id": "1605.05223", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "On the boosting ability of top-down decision tree learning algorithm for multiclass classification", "abstract": "we analyze the performance of the top - down multiclass classification algorithm for decision tree learning tasks called lomtree, recently proposed in the literature choromanska and langford ( 2014 ) for solving efficiently classification database problems with very large number of classes. the algorithm online optimizes the objective function which simultaneously controls the depth of the tree and its statistical accuracy. we prove important practical properties of this objective and explore its connection to three well - traditionally known entropy - based decision tree objectives, i. e. shannon entropy, gini - entropy and now its own modified version, for which instead online optimization schemes were not yet developed. we show, via boosting - type guarantees, that maximizing the considered objective leads also to the reduction of all of these entropy - based objectives. the bounds we obtain critically estimated depend completely on the strong - concavity properties characteristic of the entropy - based criteria, where the mildest consistency dependence on the number of classes ( only logarithmic ) corresponds to the shannon entropy.", "histories": [["v1", "Tue, 17 May 2016 16:11:36 GMT  (136kb,D)", "http://arxiv.org/abs/1605.05223v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["anna choromanska", "krzysztof choromanski", "mariusz bojarski"], "accepted": false, "id": "1605.05223"}, "pdf": {"name": "1605.05223.pdf", "metadata": {"source": "CRF", "title": "On the boosting ability of top-down decision tree learning algorithm for multiclass classification", "authors": ["Anna Choromanska", "Krzysztof Choromanski", "Mariusz Bojarski"], "emails": ["achoroma@cims.nyu.edu", "kchoro@google.com", "mbojarski@nvidia.com"], "sections": [{"heading": null, "text": "Keywords: multiclass classification, decision trees, boosting, online learning"}, {"heading": "1. Introduction", "text": "This paper focuses on the multiclass classification problem with very large number of classes which becomes of increasing importance with the recent widespread development of dataacquisition web services and devices. Straightforward extensions of the binary approaches to the multiclass setting, such as one-against-all approach Rifkin and Klautau (2004), do not often work in the presence of strict computational constraints. In this case, hierarchical approaches seem particularly favorable since, due to their structure, they potentially can significantly reduce the computational costs. This paper is motivated by very recent advances in the area of multiclass classification, and considers a hierarchical approach for learning a multiclass decision tree structure in a top-down fashion, where splitting the data in every node of the tree is based on the value of a very particular objective function. This objective function controls the balancedness of the splits (thus the depth of the tree) and the statistical error they induce (thus the statistical error of the tree), and was initially in-\n\u2217. Equal contribution.\nar X\niv :1\n60 5.\n05 22\n3v 1\n[ cs\n.L G\n] 1\ntroduced in Choromanska and Langford (2014) along with the algorithm optimizing it in an online fashion called LOMtree. The algorithm was empirically shown to obtain high-quality trees in logarithmic (in the label complexity) train and test running times, simultaneously outperforming state-of-the-art comparators, yet the objective underlying it is still not-well understood. The main contribution of this work is an extensive theoretical analysis of the properties of this objective, and the algorithm1 optimizing it. And in particular, the analysis includes exploring, via the boosting framework, a relation of this objective to some more standard entropy-based decision tree objectives, i.e. Shannon entropy2, Gini-entropy and its modified version, for which online optimization schemes in the context of multiclass classification were not yet developed.\nThe multiclass classification problem was relatively recently explored in the literature, and there exist only few works addressing the problem. In this work we focus on decision tree-based approaches. Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given. It is provably consistent and achieves regret bound which depends logarithmically on the number of classes. The conditional probability tree Beygelzimer et al. (2009a) instead learns the tree structure and uses the node splitting criterion which compromises between obtaining balanced split in a tree node and violating the recommendation for the split from the node regressor. The authors also provide regret bounds which scales with the tree depth. Other works, which come with no guarantees, consider splitting the data in every tree node by optimizing efficiency with accuracy constraints allowing fine-grained control of the efficiency-accuracy tradeoff Deng et al. (2011), or by performing clustering Bengio et al. (2010); Madzarov et al. (2009). The splitting criterion (objective function) analyzed in this paper differs from the criteria considered in previous works and comes with much stronger theoretical justification given in Section 2.\nThe main theoretical analysis of this paper is kept in the boosting framework Schapire and Freund (2012) and relies on the assumption of the existence of weak learners in the tree nodes, where the top-down algorithm we study will amplify this weak advantage to build a tree achieving any desired level of accuracy wrt. entropy-based criteria. We add new theoretical results to the theory of boosting for the multiclass classification problem (the multiclass boosting is largely ununderstood, we refer the reader to Mukherjee and Schapire (2013) for comprehensive review), and we show that LOMtree is a boosting algorithm reducing standard entropy-based criteria, where the obtained bounds depend on the strong concativity properties of these criteria. Our work extends two previous works: it significantly adds to the theoretical analysis of Choromanska and Langford (2014), where only Shannon entropy is considered, in which case we also slightly improve their bound, and it extends beyond the boosting analysis of the binary case of Kearns and Mansour (1999). The main theoretical results are presented in Section 3. Numerical experiments (Section 4) and brief discussion (Section 5) conclude the paper.\n1. We do not discuss the algorithm here, we refer the reader to the original paper. 2. Throughout the paper we refer to Shannon entropy as simply entropy."}, {"heading": "2. Objective function and its theoretical properties", "text": "In this section we describe the objective function that is of central interest to this paper, and we provide its theoretical properties."}, {"heading": "2.1 Objective function", "text": "We receive examples x \u2208 X \u2286 Rd, with labels y \u2208 {1, 2, . . . , k}. We assume access to a hypothesis class H where each h \u2208 H is a binary classifier, h : X 7\u2192 {\u22121, 1}, and each node in the tree consists of a classifier from H. The classifiers are trained in such a way that hn(x) = 1 (hn denotes the classifier in node n of the tree; for fixed node n we will refer to hn simply as h) means that the example x is sent to the right subtree of node n, while hn(x) = \u22121 sends x to the left subtree. When we reach a leaf node, we predict according to the label with the highest frequency amongst the examples reaching that leaf.\nNotice that from the perspective of reducing the computational complexity, we want to encourage the number of examples going to the left and right to be balanced. Furthermore, for maintaining good statistical accuracy, we want to send examples of class i almost exclusively to either the left or the right subtree. A measure of whether the examples of each class reaching the node are then mostly sent to its one child node (pure split) or otherwise to both children (impure split) is referred to as the purity of a tree node. These two criteria, purity and balancedness, were discussed in Choromanska and Langford (2014). This work also proposes an objective (convex) expressing both criteria, and thus measuring the quality of a hypothesis h \u2208 H in creating partitions at a fixed node n in the tree. The objective is given as follows\nJ(h) = 2 k\u2211 i=1 \u03c0i |P (h(x) > 0)\u2212 P (h(x) > 0|i)| , (1)\nwhere \u03c0i denotes the proportion of label i amongst the examples reaching this node, P (h(x) > 0) and P (h(x) > 0|i) denote the fraction of examples reaching n for which h(x) > 0, marginally and conditional on class i respectively. It was shown that this objective can be effectively maximized over hypotheses h \u2208 H, giving high-quality partitions, in an online fashion (recall that it remains unclear how to online optimize some of the more standard decision tree objectives such as entropy-based objectives). Despite that, this objective and its properties (including the relation to the more standard entropy-based objectives) remain not fully understood. Its exhaustive analysis is instead provided in this paper."}, {"heading": "2.2 Theoretical properties of the objective function", "text": "We first define the concept of balancedness and purity of the split which are crucial for providing the theoretical properties of the objective function under consideration in this paper.\nDefinition 1 (Purity and balancedness, Choromanska and Langford (2014)) The hypothesis h \u2208 H induces a pure split if\n\u03b1 := k\u2211 i=1 \u03c0i min(P (h(x) > 0|i), P (h(x) < 0|i)) \u2264 \u03b4,\nwhere \u03b4 \u2208 [0, 0.5), and \u03b1 is called the purity factor. The hypothesis h \u2208 H induces a balanced split if\nc \u2264 P (h(x) > 0)\ufe38 \ufe37\ufe37 \ufe38 =\u03b2 \u2264 1\u2212 c,\nwhere c \u2208 (0, 0.5], and \u03b2 is called the balancing factor.\nA partition is called maximally pure if \u03b1 = 0 (each class is sent exclusively to the left or to the right). A partition is called maximally balanced if \u03b2 = 0.5 (equal number of examples are sent to the left and to the right).\nNext we show the first theoretical property of the objective function J(h). Lemma 2 contains a stronger statement than the one in the original paper Choromanska and Langford (2014) (Lemma 2).\nLemma 2 For any hypothesis h : X 7\u2192 {\u22121, 1}, the objective J(h) satisfies J(h) \u2208 [0, 1]. Furthermore, h induces a maximally pure and balanced partition iff J(h) = 1.\nLemma 2 characterizes the behavior of the objective J(h) at the optimum where J(h) = 1. In practice however we do not expect to have hypotheses producing maximally pure and balanced splits, thus it is of importance to be able to show that larger values of the objective correspond simultaneously to more pure and more balanced splits. This statement would fully justify why it is desired to maximize J(h). We next focus on showing this property. We start by showing that increasing the value of the objective leads to more balanced splits.\nLemma 3 For any hypothesis h, and any distribution over examples (x, y) the balancing factor \u03b2 satisfies \u03b2 \u2208 [ 0.5(1\u2212 \u221a 1\u2212 J(h)), 0.5(1 + \u221a 1\u2212 J(h)) ] .\nThus the larger (closer to 1) the value of J(h) is, the narrower the interval from Lemma 3 is, leading to more balanced splits (\u03b2 closer to 0.5).\nThe next lemma, which we borrow from the literature, relates the balancing and purity factor, and it will be used to show that increasing the value of the objective function corresponds not only to more balanced splits, but also to more pure splits.\nLemma 4 (Choromanska and Langford (2014)) For any hypothesis h, and any distribution over examples (x, y), the purity factor \u03b1 and the balancing factor \u03b2 satisfy \u03b1 \u2264 min {(2\u2212 J(h))/4\u03b2 \u2212 \u03b2, 0.5}.\nRecall that Lemma 3 shows that increasing the value of J(h) leads to a more balanced split (\u03b2 closer to 0.5). From this fact and Lemma 4 it follows that increasing the value of\nJ(h) leads to the upper-bound on \u03b1 being closer to 0 which also corresponds to a more pure split. Thus maximizing the objective recovers more balanced and more pure splits. Proof [Lemma 2] The proof that J(h) \u2208 [0, 1] and that if h induces a maximally pure and balanced partition then J(h) = 1 was done in Choromanska and Langford (2014) (Lemma 2). We therefore prove here the remaining statement in Lemma 2 that if J(h) = 1 then h induces a maximally pure and balanced partition.\nWithout loss of generality assume each \u03c0i \u2208 (0, 1). Recall that \u03b2 = P (h(x) > 0), and let Pi = P (h(x) > 0|i). Also recall that \u03b2 = \u2211k i=1 \u03c0iPi. Thus J(h) = 2 \u2211k i=1 \u03c0i \u2223\u2223\u2223\u2211kj=1 \u03c0jPj \u2212 Pi\u2223\u2223\u2223. The objective is certainly maximized in the extremes of the interval [0, 1], where each Pi is either 0 or 1 (also note that at maximum, where J(h) = 1, it cannot be that all Pi\u2019s are 0 or all Pi\u2019s are 1). The function J(h) is differentiable in these extremes (J(h) is nondifferentiable only when \u2211k j=1 \u03c0jPj = Pi, but at considered extremes the left-hand side of this equality is in (0, 1) whereas the right-hand side is either 0 or 1). We then write\nJ(h) = 2 \u2211 i\u2208P \u03c0i  k\u2211 j=1 \u03c0jPj \u2212 Pi + 2\u2211 i\u2208N \u03c0i Pi \u2212 k\u2211 j=1 \u03c0jPj  , where P = {i : \u2211k j=1 \u03c0jPj \u2265 Pi} and N = {i :\n\u2211k j=1 \u03c0jPj < Pi}. Also let P+ = {i :\u2211k\nj=1 \u03c0jPj > Pi} (clearly \u2211 i\u2208P+ \u03c0i 6= 1 and \u2211\ni\u2208N \u03c0i 6= 1 in the extremes of the interval [0, 1] where J(h) is maximized). We then can compute the derivatives of J(h) with respect\nto Pr, where r = {1, 2, . . . , k}, everywhere where the function is differentiable as follows\n\u2202J\n\u2202Pr =\n{ 2\u03c0r( \u2211 i\u2208P+ \u03c0i \u2212 1) if r \u2208 P+\n2\u03c0r(1\u2212 \u2211 i\u2208N \u03c0i) if r \u2208 N ,\nand note that in the extremes of the interval [0, 1] where J(h) is maximized \u2202J\u2202Pr 6= 0, since\u2211 i\u2208P+ \u03c0i 6= 1, \u2211 i\u2208N \u03c0i 6= 1, and each \u03c0i \u2208 (0, 1). Since J(h) is convex, and by the fact that in particular the derivative of J(h) with respect to any Pr cannot be 0 in the extremes of the interval [0, 1] where J(h) is maximized, it follows that the J(h) can only be maximized (J(h) = 1) at the extremes of the [0, 1] interval. Thus we already proved that if J(h) = 1 then h induces a maximally pure partition. We are left with showing that if J(h) = 1 then h induces also a maximally balanced partition. We prove it by contradiction. Assume \u03b2 6= 0.5. Denote as before I0 = {i : P (h(x) > 0|i) = 0} and I1 = {i : P (h(x) > 0|i) = 1}. Recall \u03b2 = \u2211k i=1 \u03c0iPi = \u2211 i\u2208I0 \u03c0i \u00b7 0 + \u2211 i\u2208I1 \u03c0i \u00b7 1 = \u2211 i\u2208I1 \u03c0i. Thus\nJ(h) = 1 = 2 \u2211 i\u2208I0 \u03c0i |\u03b2|+ 2 \u2211 i\u2208I1 \u03c0i |\u03b2 \u2212 1| = 2\u03b2 \u2211 i\u2208I0 \u03c0i + 2(1\u2212 \u03b2) \u2211 i\u2208I1 \u03c0i\n= 2\u03b2(1\u2212 \u2211 i\u2208I1 \u03c0i) + 2(1\u2212\u03b2) \u2211 i\u2208I1 \u03c0i = 2\u03b2(1\u2212\u03b2) + 2(1\u2212\u03b2)\u03b2 = \u22124\u03b22 + 4\u03b2 < 1,\nwhere the last inequality comes from the fact that the quadratic form \u22124\u03b22 +4\u03b2 is equal to 1 only when \u03b2 = 0.5, and otherwise it is smaller than 1. Thus we obtain the contradiction which ends the proof.\nProof [Lemma 3] As before we use the following notation: \u03b2 = P (h(x) > 0), and Pi = P (h(x) > 0|i). Also let P = {i : \u03b2 \u2265 Pi} and N = {i : \u03b2 < Pi}. Recall that \u03b2 =\u2211 i\u2208{P\u222aN} \u03c0iPi, and \u2211 i\u2208{P\u222aN} \u03c0i = 1. We split the proof into two cases.\n\u2022 Let \u2211\ni\u2208P \u03c0i \u2264 1\u2212 \u03b2. Then\nJ(h) = 2 k\u2211 i=1 \u03c0i |\u03b2 \u2212 Pi| = 2 \u2211 i\u2208P \u03c0i(\u03b2 \u2212 Pi) + 2 \u2211 i\u2208N \u03c0i(Pi \u2212 \u03b2)\n= 2 \u2211 i\u2208P \u03c0i\u03b2 \u2212 2 \u2211 i\u2208P \u03c0iPi + 2(\u03b2 \u2212 \u2211 i\u2208P \u03c0iPi)\u2212 2\u03b2(1\u2212 \u2211 i\u2208P \u03c0i)\n= 4\u03b2 \u2211 i\u2208P \u03c0i \u2212 4 \u2211 i\u2208P \u03c0iPi \u2264 4\u03b2 \u2211 i\u2208P \u03c0i \u2264 4\u03b2(1\u2212 \u03b2)\nThus \u22124\u03b22 + 4\u03b2 \u2212 J(h) \u2265 0 which, when solved, yields the lemma. \u2022 Let \u2211 i\u2208P \u03c0i \u2265 1\u2212 \u03b2 (thus \u2211 i\u2208N \u03c0i \u2264 \u03b2). Note that J(h) can be written as\nJ(h) = 2 k\u2211 i=1 \u03c0i |P (h(x) \u2264 0)\u2212 P (h(x) \u2264 0|i)| ,\nsince P (h(x) \u2264 0) = 1 \u2212 P (h(x) > 0) and P (h(x) \u2264 0|i) = 1 \u2212 P (h(x) > 0|i). Let \u03b2\n\u2032 = P (h(x) \u2264 0) = 1 \u2212 \u03b2, and P \u2032i = P (h(x) \u2264 0|i) = 1 \u2212 Pi. Note that\nP = {i : \u03b2 \u2265 Pi} = {i : \u03b2 \u2032 < P \u2032 i } and N = {i : \u03b2 < Pi} = {i : \u03b2 \u2032 \u2265 P \u2032i }. Also note that \u03b2 \u2032 = \u2211\ni\u2208{P\u222aN} \u03c0iP \u2032 i . Thus\nJ(h) = 2 k\u2211 i=1 \u03c0i \u2223\u2223\u2223\u03b2\u2032 \u2212 P \u2032i \u2223\u2223\u2223 = 2\u2211 i\u2208P \u03c0i(P \u2032 i \u2212 \u03b2 \u2032 ) + 2 \u2211 i\u2208N \u03c0i(\u03b2 \u2032 \u2212 P \u2032i )\n= 2(\u03b2 \u2032 \u2212 \u2211 i\u2208N \u03c0iP \u2032 i )\u2212 2\u03b2 \u2032 (1\u2212 \u2211 i\u2208N \u03c0i) + 2 \u2211 i\u2208N \u03c0i\u03b2 \u2032 \u2212 2 \u2211 i\u2208N \u03c0iP \u2032 i\n= 4\u03b2 \u2032\u2211 i\u2208N \u03c0i \u2212 4 \u2211 i\u2208N \u03c0iP \u2032 i \u2264 4\u03b2 \u2032\u2211 i\u2208N \u03c0i = 4(1\u2212 \u03b2) \u2211 i\u2208N \u03c0i \u2264 4\u03b2(1\u2212 \u03b2)\nThus as before we obtain \u22124\u03b22+4\u03b2\u2212J(h) \u2265 0 which, when solved, yields the lemma.\nWe next consider the quality of the entire tree as we add more nodes. We aim to maximize the objective function in each node we split. In the next section we show that optimizing the objective J(h) leads to the reduction of the more standard decision tree entropy-based objectives. We consider three different objectives in this paper. We focus on the boosting framework, where the analysis depends on the weak learning assumption. Three different entropy-based criteria lead to three different theoretical statements, where we bound the number of splits required to reduce the value of the criterion below given level. The bounds we obtain, and their dependence on k, critically depend on the strong concativity properties of the considered entropy-based criteria. In our analysis we use elements of the proof techniques from Kearns and Mansour (1999) (the proof of Theorem 10) and Choromanska and Langford (2014) (the proof of Theorem 1). We show all the steps for completeness as we make modifications compared to these works."}, {"heading": "3. Main theoretical results", "text": "We begin from explaining the notation. Let T denote the tree under consideration. \u03c0l,i\u2019s denote the probabilities that a randomly chosen data point x drawn from P, where P is a fixed target distribution over X , has label i given that x reaches node l (note that\u2211k\ni=1 \u03c0l,i = 1), L denotes the set of all tree leaves, t denotes the number of internal tree nodes, and wl is the weight of leaf l defined as the probability a randomly chosen x drawn from P reaches leaf l (note that \u2211 l\u2208Lwl = 1). We study a tree construction algorithm where we recursively find the leaf node with the highest weight, and choose to split it into two children. Consider the tree constructed over t steps where in each step we take one leaf node and split it into two (t = 1 corresponds to splitting the root, thus the tree consists of one node (root) and its two children (leaves) in this step). Let n be the heaviest node at time t and its weight wn be denoted by w for brevity. We measure the quality of the tree at any given time t using three different entropy-based criteria:\n\u2022 The entropy function Get : Get = \u2211 l\u2208Lwl \u2211k i=1 \u03c0l,i ln ( 1 \u03c0l,i ) \u2022 The Gini-entropy function Ggt : G g t = \u2211 l\u2208Lwl \u2211k i=1 \u03c0l,i(1\u2212 \u03c0l,i)\n\u2022 The modified Gini-entropy Gmt : Gmt = \u2211 l\u2208Lwl \u2211k i=1 \u221a \u03c0l,i(C \u2212 \u03c0l,i),\nwhere C is a constant such that C > 2.\nThese criteria are natural extensions of the criteria used in Kearns and Mansour (1999) in the context of binary classification, to the multiclass classification setting3. We will next present the main results of this paper which will be followed by their proofs. We begin with introducing the weak hypothesis assumption.\nOur theoretical analysis is held in the boosting framework and critically depends on the weak hypothesis assumption, which ensures that the hypothesis class H is rich enough to guarantee \u2019weakly\u2019 pure and \u2019weakly\u2019 balanced split in any given node.\nDefinition 5 (Weak Hypothesis Assumption, Choromanska and Langford (2014)) Let m denotes any node of the tree T , and let \u03b2m = P (hm(x) > 0) and Pm,i = P (hm(x) > 0|i). Furthermore, let \u03b3 \u2208 R+ be such that for all m, \u03b3 \u2208 (0,min(\u03b2m, 1 \u2212 \u03b2m)]. We say that the weak hypothesis assumption is satisfied when for any distribution P over X at each node m of the tree T there exists a hypothesis hm \u2208 H such that J(hm)/2 =\u2211k\ni=1 \u03c0m,i|Pm,i \u2212 \u03b2m| \u2265 \u03b3.\nWe next state the three main theoretical results of this paper captured in Theorem 6, 7, and 8. They guarantee that the top-down decision tree algorithm which optimizes J(h), such as the one in Choromanska and Langford (2014), will amplify the weak advantage, captured in the weak learning assumption, to build a tree achieving any desired level of accuracy wrt. entropy-based criteria.\nTheorem 6 Under the Weak Hypothesis Assumption, for any \u03b1 \u2208 [0, 2 ln k], to obtain Get \u2264 \u03b1 it suffices to make\nt \u2265 ( 2 ln k\n\u03b1\n) 4(1\u2212\u03b3)2 \u03b32 log2 e ln k\nsplits.4\nTheorem 7 Under the Weak Hypothesis Assumption, for any \u03b1 \u2208 [0, 2 ( 1\u2212 1k ) ], to obtain Ggt \u2264 \u03b1 it suffices to make\nt \u2265\n( 2 ( 1\u2212 1k ) \u03b1 ) 2(1\u2212\u03b3)2 \u03b32 log2 e (k\u22121)\nsplits.\n3. Note that there is more than one way of extending the entropy-based criteria from Kearns and Mansour (1999) to the multiclass classification setting, e.g. the modified Gini-entropy could as well be defined as Gmt = \u2211 l\u2208L wl \u2211k i=1 \u221a \u03c0l,i(C \u2212 \u03c0l,i) where C \u2208 [1, 2]. This and other extensions will be investigated in\nfuture works. 4. This guarantee is slightly tighter compared to Theorem 1 in Choromanska and Langford (2014).\nTheorem 8 Under the Weak Hypothesis Assumption, for any \u03b1 \u2208 [ \u221a C \u2212 1, 2 \u221a kC \u2212 1], to obtain Gmt \u2264 \u03b1 it suffices to make\nt \u2265 (\n2 \u221a kC \u2212 1 \u03b1\n) 2(1\u2212\u03b3)2C3 \u03b32(C\u22122)2 log2 e k \u221a kC\u22121\nsplits.\nClearly, different criteria lead to bounds with different dependence on the number of classes k, where the most advantageous dependence (only logarithmic in k) is obtained for the entropy criterion. This is a consequence of the strong concativity properties of the entropy-based criteria considered in this paper. We next discuss in details the mathematical properties of the entropy-based criteria, which are important to prove the above theorems."}, {"heading": "3.1 Properties of the entropy-based criteria", "text": "Each of the presented entropy-based criteria has a number of useful properties that we give next with their proofs.\nBounds on the entropy-based criteria We first give bounds on the values of the entropy-based functions.\nLemma 9 The entropy function Get at time t is bounded as\n0 \u2264 Get \u2264 (t+ 1)w ln k.\nProof The lower-bound follows from the fact that the entropy of each leaf \u2211k i=1 \u03c0l,i ln ( 1 \u03c0l,i ) is non-negative. We next prove the upper-bound. Note that\nGet = \u2211 l\u2208L wl k\u2211 i=1 \u03c0l,i ln ( 1 \u03c0l,i ) \u2264 \u2211 l\u2208L wl ln k \u2264 w ln k \u2211 l\u2208L 1 = (t+ 1)w ln k,\nwhere the first inequality comes from the fact that uniform distribution maximizes the entropy, and the last equality comes from the fact that a tree with t internal nodes has t+1 leaves (also recall that w is the weight of the heaviest node in the tree at time t which is what we will also use in the next lemmas).\nBefore proceeding to the Gini-entropy criterion we first introduce the helpful result captured in Lemma 10 and Corollary 11.\nLemma 10 (The inequality between Euclidean and arithmetic mean) Let x1, x2, . . . , xk be a set of non-negative numbers. Then Euclidean mean upper-bounds the arithmetic mean\nas follows\n\u221a\u2211k i=1 x 2 i\nk \u2265 \u2211k i=1 xi k .\nCorollary 11 Let {x1, x2, . . . , xk} be a set of non-negative numbers. Then \u2211k i=1 x 2 i \u2265 1 k (\u2211k i=1 xi )2 .\nProof By Lemma 10 we have\n\u221a\u2211k i=1 x 2 i\nk \u2265 \u2211k i=1 xi k \u21d4 \u2211k i=1 x 2 i \u2265 1k (\u2211k i=1 xi )2 .\nWe next proceed to the Gini-entropy.\nLemma 12 The Gini-entropy function Ggt at time t is bounded as\n0 \u2264 Ggt \u2264 (t+ 1)w (\n1\u2212 1 k\n) .\nProof The lower-bound is straightforward since all \u03c0l,i\u2019s are non-negative. The upperbound can be shown as follows (the last inequality results from Corollary 11):\nGgt = \u2211 l\u2208L wl k\u2211 i=1 \u03c0l,i(1\u2212 \u03c0l,i) \u2264 w \u2211 l\u2208L k\u2211 i=1 (\u03c0l,i \u2212 \u03c02l,i) = w \u2211 l\u2208L\n( 1\u2212\nk\u2211 i=1 \u03c02l,i\n)\n\u2264 w \u2211 l\u2208L 1\u2212 1 k ( k\u2211 i=1 \u03c0l,i )2 = w\u2211 l\u2208L ( 1\u2212 1 k ) = (t+ 1)w ( 1\u2212 1 k ) ,\nLemma 13 The modified Gini-entropy function Gmt at time t is bounded as\n\u221a C \u2212 1 \u2264 Gmt \u2264 (t+ 1)w \u221a kC \u2212 1.\nProof The lower-bound can be shown as follows. Recall that the function\u2211k i=1 \u221a \u03c0l,i(C \u2212 \u03c0l,i) is concave and therefore it is certainly minimized on the extremes of the [0, 1] interval, meaning where each \u03c0l,i is either 0 or 1. Let I0 = {i : \u03c0l,i = 0} and let I1 = {i : \u03c0l,i = 1}. Thus \u2211k i=1 \u221a \u03c0l,i(C \u2212 \u03c0l,i) = \u2211 i\u2208I1 \u221a C \u2212 1 \u2265 \u221a C \u2212 1. Combining\nthis result with the fact that \u2211\nl\u2208Lwl = 1 gives the lower-bound. We next prove the upper-bound. Recall that from Lemma 10 it follows that\n\u2211k i=1 \u221a \u03c0l,i(C \u2212 \u03c0l,i) k \u2264 \u221a\u2211k i=1 \u03c0l,i(C \u2212 \u03c0l,i) k , thus\nGmt = \u2211 l\u2208L wl k\u2211 i=1 \u221a \u03c0l,i(C \u2212 \u03c0l,i) \u2264 \u2211 l\u2208L wl \u221a\u221a\u221a\u221ak k\u2211 i=1 \u03c0l,i(C \u2212 \u03c0l,i)\n= \u2211 l\u2208L wl \u221a\u221a\u221a\u221ak(C \u2212 k\u2211 i=1 \u03c02l,i ) = \u2211 l\u2208L wl \u221a\u221a\u221a\u221akC \u2212 k2 k\u2211 i=1 1 k \u03c02l,i\nBy Jensen\u2019s inequlity \u2211k\ni=1 1 k\u03c0 2 l,i \u2265 ( \u2211k i=1 1 k\u03c0l,i) 2 = 1 k2 . Thus\nGmt \u2264 \u2211 l\u2208L wl \u221a kC \u2212 1 \u2264 (t+ 1)w \u221a kC \u2212 1\nSo far we have been focusing on the time step t, where n was the heaviest node and it had weight w. Consider splitting this leaf to two children n0 and n1. For the ease of notation let w0 = wn0 and w1 = wn1 , \u03b2 = P (hn(x) > 0) and Pi = P (hn(x) > 0|i), and furthermore let \u03c0i and h be the shorthands for \u03c0n,i and hn respectively. Recall that\n\u03b2 = \u2211k i=1 \u03c0iPi and \u2211k\ni=1 \u03c0i = 1. Notice that w0 = w(1 \u2212 \u03b2) and w1 = w\u03b2. Let \u03c0 be the k-element vector with ith entry equal to \u03c0i. Finally, let G\u0303 e(\u03c0) = \u2211k i=1 \u03c0i ln ( 1 \u03c0i ) ,\nG\u0303g(\u03c0) = \u2211k i=1 \u03c0i(1\u2212\u03c0i), and G\u0303m(\u03c0) = \u2211k i=1 \u221a \u03c0i(1\u2212 \u03c0i). Before the split the contribution of node n to resp. Get , G g t , and G m t was resp. wG\u0303 e(\u03c0), wG\u0303g(\u03c0), and wG\u0303m(\u03c0). Note that \u03c0n0,i = \u03c0i(1\u2212Pi) 1\u2212\u03b2 and \u03c0n1,i = \u03c0iPi \u03b2 are the probabilities that a randomly chosen x drawn from P has label i given that x reaches nodes n0 and n1 respectively. For brevity, let \u03c0n0,i and \u03c0n1,i be denoted respectively as \u03c00,i and \u03c01,i. Let \u03c00 be the k-element vector with i th entry equal to \u03c00,i and let \u03c01 be the k-element vector with i th entry equal to \u03c01,i. Notice that \u03c0 = (1 \u2212 \u03b2)\u03c00 + \u03b2\u03c01. After the split the contribution of the same, now internal, node n changes to resp. w((1 \u2212 \u03b2)G\u0303e(\u03c00) + \u03b2G\u0303e(\u03c01)), w((1 \u2212 \u03b2)G\u0303g(\u03c00) + \u03b2G\u0303g(\u03c01)), and w((1\u2212 \u03b2)G\u0303m(\u03c00) + \u03b2G\u0303m(\u03c01)). We denote the difference between the contribution of node n to the value of the entropy-based objectives in times t and t+ 1 as\n\u2206et := G e t \u2212Get+1 = w [ G\u0303e(\u03c0)\u2212 (1\u2212 \u03b2)G\u0303e(\u03c00)\u2212 \u03b2G\u0303e(\u03c01) ] . (2)\n\u2206gt := G g t \u2212G g t+1 = w [ G\u0303g(\u03c0)\u2212 (1\u2212 \u03b2)G\u0303g(\u03c00)\u2212 \u03b2G\u0303g(\u03c01) ] . (3)\n\u2206mt := G m t \u2212Gmt+1 = w [ G\u0303m(\u03c0)\u2212 (1\u2212 \u03b2)G\u0303m(\u03c00)\u2212 \u03b2G\u0303m(\u03c01) ] . (4)\nStrong concativity properties of the entropy-based criteria The next three lemmas, Lemma 14, 16, and 18, describe the strong concativity properties of the entropy, Gini-entropy and modified Gini-entropy which can be used to lower-bound \u2206et , \u2206 g t , and \u2206mt (Equations 2, 3, and 4 corresponds to a gap in the Jensen\u2019s inequality applied to the strongly concave function).\nLemma 14 The entropy function G\u0303e is strongly concave with respect to l1-norm with modulus 1, and thus the following holds\nG\u0303e(\u03c0)\u2212 (1\u2212 \u03b2)G\u0303e(\u03c00)\u2212 \u03b2G\u0303e(\u03c01) \u2265 1\n2 \u03b2(1\u2212 \u03b2)\u2016\u03c00 \u2212 \u03c01\u201621.\nProof Lemma 14 is proven in Shalev-Shwartz (2012) (Example 2.5).\nWe introduce one more lemma and then proceed with Gini-entropy.\nLemma 15 (Shalev-Shwartz (2007)(Lemma 14)) If the function \u03a6(\u03c0) is twice differentiable, then the sufficient condition for strong concativity of \u03a6 is that for all \u03c0, x,\u2329 \u22072\u03a6(\u03c0)x,x \u232a \u2264 \u2212\u03c3\u2016x\u20162, where \u22072\u03a6(\u03c0) is the Hessian matrix of \u03a6 at \u03c0, and \u03c3 > 0 is the strong concativity modulus.\nLemma 16 The Gini-entropy function G\u0303g is strongly concave with respect to l2-norm with modulus 2, and thus the following holds\nG\u0303g(\u03c0)\u2212 (1\u2212 \u03b2)G\u0303g(\u03c00)\u2212 \u03b2G\u0303g(\u03c01) \u2265 \u03b2(1\u2212 \u03b2)\u2016\u03c00 \u2212 \u03c01\u201622.\nProof Note that \u2329 \u22072G\u0303g(\u03c0)x,x \u232a \u2264 \u22122\u2016x\u201622, and apply Lemma 15.\nBefore showing the strong concativity guarantee for the modified Gini-entropy, we first show the statement that will be useful to prove the lemma.\nLemma 17 (Zhukovskiy (2003), Remark 2.2.4.) The sum of strongly concave functions on Rn with modulus \u03c3 is strongly concave with the same modulus.\nLemma 18 The modified Gini-entropy function G\u0303m is strongly concave with respect to l2norm with modulus 2(C\u22122) 2\nC3 , and thus the following holds\nG\u0303m(\u03c0)\u2212 (1\u2212 \u03b2)G\u0303m(\u03c00)\u2212 \u03b2G\u0303m(\u03c01) \u2265 (C \u2212 2)2\nC3 \u03b2(1\u2212 \u03b2)\u2016\u03c00 \u2212 \u03c01\u201622.\nProof Consider functions g(\u03c0i) = \u221a f(\u03c0i), where f(\u03c0i) = \u03c0i(C\u2212\u03c0i), C \u2265 2, and \u03c0i \u2208 [0, 1]. Also let h(x) = \u221a x, where x \u2208 [0, C24 ]. It is easy to see, using Lemma 15, that function f is strongly concave with respect to l2-norm with modulus 2, thus\nf(\u03b8\u03c0 \u2032 i + (1\u2212 \u03b8)\u03c0 \u2032\u2032 i ) \u2265 \u03b8f(\u03c0 \u2032 i) + (1\u2212 \u03b8)f(\u03c0 \u2032\u2032 i ) + \u03b8(1\u2212 \u03b8)\u2016\u03c0 \u2032 i \u2212 \u03c0 \u2032\u2032 i \u201622, (5)\nwhere \u03c0 \u2032 i, \u03c0 \u2032\u2032 i \u2208 [0, 1] and \u03b8 \u2208 [0, 1]. Also note that h is strongly concave with modulus 2C3 in its domain [0, C 2\n4 ] (the second derivative of h is h \u2032\u2032 (x) = \u2212 1 4 \u221a x3 \u2264 \u2212 2C3 ). The strong\nconcativity of h implies that\u221a \u03b8x1 + (1\u2212 \u03b8)x2 \u2265 \u03b8 \u221a x1 + (1\u2212 \u03b8) \u221a x2 + 1\nC3 \u03b8(1\u2212 \u03b8)\u2016x1 \u2212 x2\u201622,\nwhere x1, x2 \u2208 [0, C 2 4 ]. Let x1 = f(\u03c0 \u2032 i) and x2 = f(\u03c0 \u2032\u2032 i ). Then we obtain\u221a\n\u03b8f(\u03c0 \u2032 i) + (1\u2212 \u03b8)f(\u03c0 \u2032\u2032 i ) \u2265 \u03b8\n\u221a f(\u03c0\n\u2032 i) + (1\u2212 \u03b8)\n\u221a f(\u03c0\n\u2032\u2032 i ) +\n1\nC3 \u03b8(1\u2212 \u03b8)\u2016f(\u03c0\u2032i)\u2212 f(\u03c0 \u2032\u2032 i )\u201622. (6)\nNote that\u221a f(\u03b8\u03c0\n\u2032 i + (1\u2212 \u03b8)\u03c0 \u2032\u2032 i ) \u2265\n\u221a f(\u03b8\u03c0\n\u2032 i + (1\u2212 \u03b8)\u03c0 \u2032\u2032 i )\u2212 \u03b8(1\u2212 \u03b8)\u2016\u03c0 \u2032 i \u2212 \u03c0 \u2032\u2032 i \u201622 \u2265 \u221a \u03b8f(\u03c0\n\u2032 i) + (1\u2212 \u03b8)f(\u03c0 \u2032\u2032 i ) \u2265 \u03b8 \u221a f(\u03c0\n\u2032 i) + (1\u2212 \u03b8)\n\u221a f(\u03c0\n\u2032\u2032 i ) +\n1\nC3 \u03b8(1\u2212 \u03b8)\u2016f(\u03c0\u2032i)\u2212 f(\u03c0 \u2032\u2032 i )\u201622,\n2 \u2217 C \u2212 1\u2212\u221a\nC \u2212 1) = (\n\u221a\n\u03c01(C \u2212 \u03c01) +\n\u221a\n(1\u2212 \u03c01)(C \u2212 1 + \u03c01) \u2212\n\u221a\nC \u2212 1)/(\n\u221a\n2 \u2217 C \u2212 1 \u2212\u221a\nC \u2212 1) (functions G\u0303e(\u03c01), G\u0303g(\u03c01), and G\u0303m(\u03c01) were re-scaled to have values in [0, 1]) as a function of \u03c01 (pi1). Figure is recommended to be read in color.\nwhere the second inequality results from Equation 5 and the last (third) inequality results from Equation 6. Finally note that the first derivative of f is f \u2032 (\u03c0i) = C \u2212 2\u03c0i \u2208 [C \u2212 2, C] thus |f(\u03c0\u2032i)\u2212 f(\u03c0 \u2032\u2032 i )|\n|\u03c0\u2032i \u2212 \u03c0 \u2032\u2032 i |\n\u2265 C \u2212 2\u21d4 \u2016f(\u03c0\u2032i)\u2212 f(\u03c0 \u2032\u2032 i )\u20162 \u2265 (C \u2212 2)2\u2016\u03c0 \u2032 i \u2212 \u03c0 \u2032\u2032 i \u20162,\nand combining this result with previous statement yields\u221a f(\u03b8\u03c0\n\u2032 i + (1\u2212 \u03b8)\u03c0 \u2032\u2032 i ) \u2265 \u03b8\n\u221a f(\u03c0\n\u2032 i) + (1\u2212 \u03b8)\n\u221a f(\u03c0\n\u2032\u2032 i ) +\n(C \u2212 2)2\nC3 \u03b8(1\u2212 \u03b8)\u2016\u03c0\u2032i \u2212 \u03c0 \u2032\u2032 i \u20162,\nthus g(\u03c0i) is strongly concave with modulus 2(C\u22122)2 C3 . By Lemma 17, G\u0303\nm(\u03c0) is also strongly concave with the same modulus."}, {"heading": "3.2 Proof of the main theorems", "text": "We finally proceed to proving all three theorems. We first introduce some mathematical tools that will be used in the following proofs. The next two lemma are fundamental. The first one relates l1-norm and l2-norm and the second one is a simple property of the exponential function. Lemma 19 Let x \u2208 Rk then \u2016x\u20161 \u2264 \u221a k\u2016x\u20162.\nLemma 20 For x \u2265 1 the following holds ( 1\u2212 1x )x \u2264 1e .\nWe next proceed to proving Theorem 6, 7, and 8. Proof For the entropy it follows from Equation 2 and Lemma 14 that\n\u2206et \u2265 1\n2 w\u03b2(1\u2212 \u03b2)\u2016\u03c00 \u2212 \u03c01\u201621 =\n1\n2\nw\n\u03b2(1\u2212 \u03b2) ( k\u2211 i=1 |\u03c0i(Pi \u2212 \u03b2)| )2 = wJ(h)2 8\u03b2(1\u2212 \u03b2)\n\u2265 J(h) 2Get 8\u03b2(1\u2212 \u03b2)(t+ 1) ln k \u2265 \u03b3 2Get 2(1\u2212 \u03b3)2(t+ 1) ln k , (7)\nwhere the last inequality comes from the fact that 1\u2212 \u03b3 \u2265 \u03b2 \u2265 \u03b3 (see the definition of \u03b3 in the weak hypothesis assumption) and J(h) \u2265 2\u03b3 (see weak hypothesis assumption). For the Gini-entropy criterion notice that from Equation 3, Lemma 16 and Lemma 19, it follows that\n\u2206gt \u2265 w\u03b2(1\u2212\u03b2)\u2016\u03c00\u2212\u03c01\u201622 \u2265 1\nk w\u03b2(1\u2212\u03b2)\u2016\u03c00\u2212\u03c01\u201621 \u2265 \u03b32Ggt (1\u2212\u03b3)2(t+1)(k\u22121) , (8)\nwhere the last inequality is obtained similarly as the last inequality in Equation 7. And finally for the modified Gini-entropy it follows from Equation 4, Lemma 18 and Lemma 19 that\n\u2206mt \u2265 w (C \u2212 2)2\nC3 \u03b2(1\u2212 \u03b2)\u2016\u03c00 \u2212 \u03c01\u201622 \u2265\n1 k w\n(C \u2212 2)2\nC3 \u03b2(1\u2212 \u03b2)\u2016\u03c00 \u2212 \u03c01\u201621\n\u2265 \u03b3 2Gmt\nC3 (C\u22122)2 (1\u2212 \u03b3)2(t+ 1)k\n\u221a kC \u2212 1 , (9)\nwhere the last inequality is obtained as before. Clearly the larger the objective J(h) is at time t, the larger the entropy reduction ends up being, which confirms the plausibility of the approach in Choromanska and Langford (2014) where the goal is to maximize J(h). Let\n\u03b7e = 2 \u221a 2\u03b3\n(1\u2212 \u03b3) \u221a ln k , \u03b7g =\n4\u03b3\n(1\u2212 \u03b3) \u221a k \u2212 1\n, \u03b7m = 4\u03b3 (1\u2212 \u03b3) \u221a\nC3 (C\u22122)2k\n\u221a kC \u2212 1 . (10)\nFor simplicity of notation assume \u2206t corresponds to either \u2206 e t , or \u2206 g t , or \u2206 m t , and Gt stands for Get , or G g t , or G m t . Thus \u2206t > \u03b72Gt 16(t+1) , and we obtain the recurrence inequality\nGt+1 \u2264 Gt \u2212\u2206t < Gt \u2212 \u03b72Gt\n16(t+ 1) = Gt\n( 1\u2212 \u03b7 2\n16(t+ 1) ) One can now compute the minimum number of splits required to reduce Gt below \u03b1, where \u03b1 \u2208 [0, 1]. Assume log2(t+ 1) \u2208 Z+.\nGt+1 \u2264 Gt ( 1\u2212 \u03b7 2\n16(t+ 1)\n) = G1 ( 1\u2212 \u03b7 2\n16 \u00b7 2\n)( 1\u2212 \u03b7 2\n16 \u00b7 3\n) . . . ( 1\u2212 \u03b7 2\n16 \u00b7 (t+ 1)\n)\n= G1\n( 1\u2212 \u03b7 2\n16 \u00b7 2 ) 4\u220f t\u2032=3 ( 1\u2212 \u03b7 2 16 \u00b7 t\u2032 ) . . . 2r\u220f t\u2032=(2r/2)+1 ( 1\u2212 \u03b7 2 16 \u00b7 t\u2032 ) . . . 2log2(t+1)\u220f t\u2032=(2log2(t+1)/2)+1 ( 1\u2212 \u03b7 2 16 \u00b7 t\u2032 ) ,\nwhere r = {2, 3, . . . , log2(t+ 1)}. Recall that\n2r\u220f t\u2032=(2r/2)+1 ( 1\u2212 \u03b7 2 16 \u00b7 t\u2032 ) \u2264 2r\u220f t\u2032=(2r/2)+1 ( 1\u2212 \u03b7 2 16 \u00b7 2r ) = ( 1\u2212 \u03b7 2 16 \u00b7 2r )2r/2 \u2264 e\u2212\u03b72/32,\nwhere the last step follows from Lemma 20. Also note that by the same lemma ( 1\u2212 \u03b7 2\n16\u00b72\n) \u2264\ne\u2212\u03b7 2/32. Thus\nGt+1 \u2264 G1e\u2212\u03b7 2 log2(t+1)/32. (11)\nTherefore to reduce Gt+1 \u2264 \u03b1 (where \u03b1\u2019s are defined in Theorems 6, 7, and 8) it suffices to make t+1 splits such that log2(t+1) \u2265 ln ( G1 \u03b1 ) 32 \u03b72 splits. Since log2(t+1) = ln(t+1)\u00b7log2(e), where e = exp(1). Thus\nln(t+ 1) \u2265 ln ( G1 \u03b1 ) 32 \u03b72 log2(e) \u21d4 t+ 1 \u2265 ( G1 \u03b1 ) 32 \u03b72 log2(e) . (12)\nRecall that by resp. Lemma 9, 12, and 13 we have resp. Ge1 \u2264 2 ln k, G g 1 \u2264 2(1 \u2212 1k ),\nGg1 \u2264 2 \u221a kC \u2212 1. We consider the worst case setting (giving the largest possible number of split) thus we assume Ge1 = 2 ln k, G g 1 = 2(1 \u2212 1k ), and G g 1 \u2264 2 \u221a kC \u2212 1. Combining that with Equation 10 and Equation 12 yields statements of the main theorems."}, {"heading": "4. Numerical experiments", "text": "We run LOMtree algorithm, which is implemented in the open source learning system Vowpal Wabbit Langford et al. (2007), on four benchmark multiclass datasets: Mnist (10 classes, downloaded from http://yann.lecun.com/exdb/mnist/), Isolet (26 classes, downloaded from http://www.cs.huji.ac.il/~shais/datasets/ClassificationDatasets.html), Sector (105 classes, downloaded from http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/ datasets/multiclass.html), and Aloi (1000 classes, downloaded from http://www.csie. ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html). The datasets were divided into training (90%) and testing (10%), where 10% of the training dataset was used as a validation set. The regressors in the tree nodes are linear and were trained by SGD Bottou (1998) with 20 epochs and the learning rate chosen from the set {0.25, 0.5, 0.75, 1, 2, 4, 8}. We investigated different swap resistances5 chosen from the set {4, 8, 16, 32, 64, 128, 256}. We selected the learning rate and the swap resistance as the one minimizing the validation error, where the number of splits in all experiments were set to 10k.\nFigure 3 shows the entropy, Gini-entropy, modified Gini-entropy, and the error, all normalized to the interval [0, 1], as the function of the number of splits. The behavior of the entropy and Gini-entropy match the theoretical findings. However, the modified Ginientropy instead drops the fastest with the number of splits, which in particular suggests that in this case perhaps tighter bounds could possibly be proved (for the binary case tighter analysis was shown in Kearns and Mansour (1999), but it is highly non-trivial to generalize\n5. see Choromanska and Langford (2014) for details\nthis analysis to the multiclass classification setting). Furthermore, it can be observed that the behavior of the error closely mimics the behavior of the Gini-entropy."}, {"heading": "5. Conclusions", "text": "This paper focuses on the properties of the recently proposed LOMtree algorithm. We provide an exhaustive theoretical analysis of the objective function underlying the algorithm. We show a unified framework for analyzing the boosting ability of the algorithm by exploring the connection of its objective to entropy-based criteria, such as entropy, Gini-entropy and its modified version. We show that the strong concativity properties of these criteria have critical impact on the character of the obtained bounds. The experiments suggest that perhaps tighter bound is possible in particular for the modified version of the Gini-entropy."}], "references": [{"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2010}, {"title": "Conditional probability tree estimation analysis and algorithms", "author": ["A. Beygelzimer", "J. Langford", "Y. Lifshits", "G.B. Sorkin", "A.L. Strehl"], "venue": "In UAI,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Error-correcting tournaments", "author": ["A. Beygelzimer", "J. Langford", "P.D. Ravikumar"], "venue": "In ALT,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Online algorithms and stochastic approximations. In Online Learning and Neural Networks", "author": ["L. Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Logarithmic time online multiclass prediction", "author": ["Anna Choromanska", "John Langford"], "venue": "CoRR, abs/1406.1822,", "citeRegEx": "Choromanska and Langford.,? \\Q2014\\E", "shortCiteRegEx": "Choromanska and Langford.", "year": 2014}, {"title": "Fast and balanced: Efficient label tree learning for large scale object recognition", "author": ["J. Deng", "S. Satheesh", "A.C. Berg", "L. Fei-Fei"], "venue": "In NIPS,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["M. Kearns", "Y. Mansour"], "venue": "Journal of Computer and Systems Sciences,", "citeRegEx": "Kearns and Mansour.,? \\Q1996\\E", "shortCiteRegEx": "Kearns and Mansour.", "year": 1996}, {"title": "A multi-class svm classifier utilizing binary decision", "author": ["G. Madzarov", "D. Gjorgjevikj", "I. Chorbev"], "venue": "tree. Informatica,", "citeRegEx": "Madzarov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Madzarov et al\\.", "year": 2009}, {"title": "A theory of multiclass boosting", "author": ["I. Mukherjee", "R.E. Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mukherjee and Schapire.,? \\Q2013\\E", "shortCiteRegEx": "Mukherjee and Schapire.", "year": 2013}, {"title": "In defense of one-vs-all classification", "author": ["R. Rifkin", "A. Klautau"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Rifkin and Klautau.,? \\Q2004\\E", "shortCiteRegEx": "Rifkin and Klautau.", "year": 2004}, {"title": "Boosting: Foundations and Algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "Schapire and Freund.,? \\Q2012\\E", "shortCiteRegEx": "Schapire and Freund.", "year": 2012}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis, The Hebrew University of Jerusalem,", "citeRegEx": "Shalev.Shwartz.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2007}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "Lyapunov Functions in Differential Games. Stability and Control: Theory, Methods and Applications", "author": ["V.I. Zhukovskiy"], "venue": "Taylor & Francis,", "citeRegEx": "Zhukovskiy.,? \\Q2003\\E", "shortCiteRegEx": "Zhukovskiy.", "year": 2003}], "referenceMentions": [{"referenceID": 4, "context": "Abstract We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes.", "startOffset": 167, "endOffset": 199}, {"referenceID": 9, "context": "Straightforward extensions of the binary approaches to the multiclass setting, such as one-against-all approach Rifkin and Klautau (2004), do not often work in the presence of strict computational constraints.", "startOffset": 112, "endOffset": 138}, {"referenceID": 4, "context": "troduced in Choromanska and Langford (2014) along with the algorithm optimizing it in an online fashion called LOMtree.", "startOffset": 12, "endOffset": 44}, {"referenceID": 0, "context": "Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given.", "startOffset": 12, "endOffset": 39}, {"referenceID": 0, "context": "Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given. It is provably consistent and achieves regret bound which depends logarithmically on the number of classes. The conditional probability tree Beygelzimer et al. (2009a) instead learns the tree structure and uses the node splitting criterion which compromises between obtaining balanced split in a tree node and violating the recommendation for the split from the node regressor.", "startOffset": 12, "endOffset": 311}, {"referenceID": 0, "context": "Filter tree Beygelzimer et al. (2009b) considers simplified instance of the problem where the tree structure over the labels is assumed given. It is provably consistent and achieves regret bound which depends logarithmically on the number of classes. The conditional probability tree Beygelzimer et al. (2009a) instead learns the tree structure and uses the node splitting criterion which compromises between obtaining balanced split in a tree node and violating the recommendation for the split from the node regressor. The authors also provide regret bounds which scales with the tree depth. Other works, which come with no guarantees, consider splitting the data in every tree node by optimizing efficiency with accuracy constraints allowing fine-grained control of the efficiency-accuracy tradeoff Deng et al. (2011), or by performing clustering Bengio et al.", "startOffset": 12, "endOffset": 821}, {"referenceID": 0, "context": "(2011), or by performing clustering Bengio et al. (2010); Madzarov et al.", "startOffset": 36, "endOffset": 57}, {"referenceID": 0, "context": "(2011), or by performing clustering Bengio et al. (2010); Madzarov et al. (2009). The splitting criterion (objective function) analyzed in this paper differs from the criteria considered in previous works and comes with much stronger theoretical justification given in Section 2.", "startOffset": 36, "endOffset": 81}, {"referenceID": 7, "context": "The main theoretical analysis of this paper is kept in the boosting framework Schapire and Freund (2012) and relies on the assumption of the existence of weak learners in the tree nodes, where the top-down algorithm we study will amplify this weak advantage to build a tree achieving any desired level of accuracy wrt.", "startOffset": 78, "endOffset": 105}, {"referenceID": 6, "context": "We add new theoretical results to the theory of boosting for the multiclass classification problem (the multiclass boosting is largely ununderstood, we refer the reader to Mukherjee and Schapire (2013) for comprehensive review), and we show that LOMtree is a boosting algorithm reducing standard entropy-based criteria, where the obtained bounds depend on the strong concativity properties of these criteria.", "startOffset": 172, "endOffset": 202}, {"referenceID": 4, "context": "Our work extends two previous works: it significantly adds to the theoretical analysis of Choromanska and Langford (2014), where only Shannon entropy is considered, in which case we also slightly improve their bound, and it extends beyond the boosting analysis of the binary case of Kearns and Mansour (1999).", "startOffset": 90, "endOffset": 122}, {"referenceID": 4, "context": "Our work extends two previous works: it significantly adds to the theoretical analysis of Choromanska and Langford (2014), where only Shannon entropy is considered, in which case we also slightly improve their bound, and it extends beyond the boosting analysis of the binary case of Kearns and Mansour (1999). The main theoretical results are presented in Section 3.", "startOffset": 90, "endOffset": 309}, {"referenceID": 4, "context": "These two criteria, purity and balancedness, were discussed in Choromanska and Langford (2014). This work also proposes an objective (convex) expressing both criteria, and thus measuring the quality of a hypothesis h \u2208 H in creating partitions at a fixed node n in the tree.", "startOffset": 63, "endOffset": 95}, {"referenceID": 4, "context": "Definition 1 (Purity and balancedness, Choromanska and Langford (2014)) The hypothesis h \u2208 H induces a pure split if", "startOffset": 39, "endOffset": 71}, {"referenceID": 4, "context": "Lemma 2 contains a stronger statement than the one in the original paper Choromanska and Langford (2014) (Lemma 2).", "startOffset": 73, "endOffset": 105}, {"referenceID": 4, "context": "Lemma 4 (Choromanska and Langford (2014)) For any hypothesis h, and any distribution over examples (x, y), the purity factor \u03b1 and the balancing factor \u03b2 satisfy \u03b1 \u2264 min {(2\u2212 J(h))/4\u03b2 \u2212 \u03b2, 0.", "startOffset": 9, "endOffset": 41}, {"referenceID": 4, "context": "Proof [Lemma 2] The proof that J(h) \u2208 [0, 1] and that if h induces a maximally pure and balanced partition then J(h) = 1 was done in Choromanska and Langford (2014) (Lemma 2).", "startOffset": 133, "endOffset": 165}, {"referenceID": 5, "context": "In our analysis we use elements of the proof techniques from Kearns and Mansour (1999) (the proof of Theorem 10) and Choromanska and Langford (2014) (the proof of Theorem 1).", "startOffset": 61, "endOffset": 87}, {"referenceID": 4, "context": "In our analysis we use elements of the proof techniques from Kearns and Mansour (1999) (the proof of Theorem 10) and Choromanska and Langford (2014) (the proof of Theorem 1).", "startOffset": 117, "endOffset": 149}, {"referenceID": 6, "context": "These criteria are natural extensions of the criteria used in Kearns and Mansour (1999) in the context of binary classification, to the multiclass classification setting3.", "startOffset": 62, "endOffset": 88}, {"referenceID": 4, "context": "Definition 5 (Weak Hypothesis Assumption, Choromanska and Langford (2014)) Let m denotes any node of the tree T , and let \u03b2m = P (hm(x) > 0) and Pm,i = P (hm(x) > 0|i).", "startOffset": 42, "endOffset": 74}, {"referenceID": 4, "context": "They guarantee that the top-down decision tree algorithm which optimizes J(h), such as the one in Choromanska and Langford (2014), will amplify the weak advantage, captured in the weak learning assumption, to build a tree achieving any desired level of accuracy wrt.", "startOffset": 98, "endOffset": 130}, {"referenceID": 5, "context": "Note that there is more than one way of extending the entropy-based criteria from Kearns and Mansour (1999) to the multiclass classification setting, e.", "startOffset": 82, "endOffset": 108}, {"referenceID": 4, "context": "This guarantee is slightly tighter compared to Theorem 1 in Choromanska and Langford (2014).", "startOffset": 60, "endOffset": 92}, {"referenceID": 11, "context": "Proof Lemma 14 is proven in Shalev-Shwartz (2012) (Example 2.", "startOffset": 28, "endOffset": 50}, {"referenceID": 11, "context": "Lemma 15 (Shalev-Shwartz (2007)(Lemma 14)) If the function \u03a6(\u03c0) is twice differentiable, then the sufficient condition for strong concativity of \u03a6 is that for all \u03c0, x, \u3008 \u22072\u03a6(\u03c0)x,x \u3009 \u2264 \u2212\u03c3\u2016x\u20162, where \u22072\u03a6(\u03c0) is the Hessian matrix of \u03a6 at \u03c0, and \u03c3 > 0 is the strong concativity modulus.", "startOffset": 10, "endOffset": 32}, {"referenceID": 13, "context": "Lemma 17 (Zhukovskiy (2003), Remark 2.", "startOffset": 10, "endOffset": 28}, {"referenceID": 4, "context": "Clearly the larger the objective J(h) is at time t, the larger the entropy reduction ends up being, which confirms the plausibility of the approach in Choromanska and Langford (2014) where the goal is to maximize J(h).", "startOffset": 151, "endOffset": 183}, {"referenceID": 3, "context": "The regressors in the tree nodes are linear and were trained by SGD Bottou (1998) with 20 epochs and the learning rate chosen from the set {0.", "startOffset": 68, "endOffset": 82}, {"referenceID": 3, "context": "The regressors in the tree nodes are linear and were trained by SGD Bottou (1998) with 20 epochs and the learning rate chosen from the set {0.25, 0.5, 0.75, 1, 2, 4, 8}. We investigated different swap resistances5 chosen from the set {4, 8, 16, 32, 64, 128, 256}. We selected the learning rate and the swap resistance as the one minimizing the validation error, where the number of splits in all experiments were set to 10k. Figure 3 shows the entropy, Gini-entropy, modified Gini-entropy, and the error, all normalized to the interval [0, 1], as the function of the number of splits. The behavior of the entropy and Gini-entropy match the theoretical findings. However, the modified Ginientropy instead drops the fastest with the number of splits, which in particular suggests that in this case perhaps tighter bounds could possibly be proved (for the binary case tighter analysis was shown in Kearns and Mansour (1999), but it is highly non-trivial to generalize", "startOffset": 68, "endOffset": 921}, {"referenceID": 4, "context": "see Choromanska and Langford (2014) for details", "startOffset": 4, "endOffset": 36}], "year": 2016, "abstractText": "We analyze the performance of the top-down multiclass classification algorithm for decision tree learning called LOMtree, recently proposed in the literature Choromanska and Langford (2014) for solving efficiently classification problems with very large number of classes. The algorithm online optimizes the objective function which simultaneously controls the depth of the tree and its statistical accuracy. We prove important properties of this objective and explore its connection to three well-known entropy-based decision tree objectives, i.e. Shannon entropy, Gini-entropy and its modified version, for which instead online optimization schemes were not yet developed. We show, via boosting-type guarantees, that maximizing the considered objective leads also to the reduction of all of these entropy-based objectives. The bounds we obtain critically depend on the strong-concavity properties of the entropy-based criteria, where the mildest dependence on the number of classes (only logarithmic) corresponds to the Shannon entropy.", "creator": "LaTeX with hyperref package"}}}