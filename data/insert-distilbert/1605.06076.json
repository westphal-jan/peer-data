{"id": "1605.06076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "On a convergent off -policy temporal difference learning algorithm in on-line learning environment", "abstract": "in this paper project we provide a rigorous convergence analysis of a \" off \" - policy temporal difference learning algorithm with linear gamma function approximation and per time - averaged step linear computational complexity in \" performance online \" learning environment. the algorithm considered here is optimal tdc with importance weighting introduced by maei et al.. we support our various theoretical results by providing suitable empirical results for standard off - policy counterexamples.", "histories": [["v1", "Thu, 19 May 2016 18:32:50 GMT  (242kb,D)", "http://arxiv.org/abs/1605.06076v1", "14 pages. arXiv admin note: text overlap witharXiv:1503.09105"]], "COMMENTS": "14 pages. arXiv admin note: text overlap witharXiv:1503.09105", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["prasenjit karmakar", "rajkumar maity", "shalabh bhatnagar"], "accepted": false, "id": "1605.06076"}, "pdf": {"name": "1605.06076.pdf", "metadata": {"source": "CRF", "title": "On a convergent off -policy temporal difference learning algorithm in on-line learning environment", "authors": ["Prasenjit Karmakar", "Rajkumar Maity", "Shalabh Bhatnagar"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "We consider the problem of estimating the value function corresponding to a target policy given the realization of a finite state Markov decision process under a behaviour policy which is different from the target policy. This is well known in literature as the off-policy evaluation problem. A solution for this problem might allow one to learn about the optimal policy while behaving according to an exploratory policy. See [7] for additional uses.\nIt is well-known that for this problem the standard temporal difference learning with linear function approximation may diverge ([6], [8, Section 3]). Further, the usual single time-scale stochastic approximation kind of argument may not be useful as the associated ordinary differential equation (o.d.e) may not have the TD(0) solution as its globally asymptotically stable equilibrium. In [9, 10, 2] the gradient temporal difference learning (GTD) algorithms were proposed to solve this problem. The per time-step computational complexity for these algorithms scales only linearly in the size d of the function approximator. However, the authors make the assumption that either\n1. one uses \u201csub-sampling\u201d (see [2, Section 4.1],[9] for details) to filter the data relevant to target policy given the trajectory corresponding to behaviour policy, or\n2. the data itself is available in the off-policy setting i.e. one has direct access to quadruples of the form (state, action, reward, next state) where the first component of the quadruples are sampled independently from the stationary distribution of\nar X\niv :1\n60 5.\n06 07\n6v 1\n[ cs\n.L G\n] 1\nthe underlying Markov chain corresponding to the behaviour policy and the quadruples are formed according to the target policy.\nAmongst all algorithms with the above assumptions, the TDC (temporal difference learning with gradient correction) algorithm was empirically found to be most efficient in terms of the rate of convergence. It was shown in [10] that such an algorithm can be proved to be convergent using the classical convergence proof for two timescale stochastic approximation with martingale difference noise [11]. The reason for using two time-scale framework for the TDC algorithm is to make sure that the O.D.E\u2019s have globally asymptotically stable equilibrium. However, one can prove the convergence using single time-scale convergence analysis as in [2, Theorem 3]; however the extra condition on the step-size ratio \u03b7 mentioned there is hard to verify as stationary distribution will be unknown.\nNote that such works incorporate the off-policy issue into the data as they don\u2019t take the full behvaiour trajectory as input to the algorithm. The assumptions used in the aforementioned reference on off-policy algorithms are highly restrictive as\n1. although in the first case Markov chain sampled at increasing stopping times is time-homogeneous Markov, its transition probability will be different from the same of the Markov chain corresponding to behaviour policy. Further, we are interested in an \u201conline\u201d learning scheme. Also,\n2. the second situation is not realistic too as the aforementioned stationary distribution will be unknown; one has access to only the trajectory corresponding to behaviour policy from which the goal is to evaluate the target policy.\nKeeping this in mind, another algorithm introduced in [2], namely, TDC with importance weighting solves the above off-policy evaluation problem in a more realistic scenario. The idea is to handle the off-policy issue in the algorithm rather than in the data by weighting the updates by the likelihood of action taken by the target policy (as opposed to the behavior policy). The advantage is that, unlike sub-sampling, here all the data from the given trajectory corresponding to the behaviour policy is used which is necessary in an online learning scenario. Another advantage of this method is that we can allow both the behaviour and target policies to be be randomized unlike the sub-sampling scenario where one can use only deterministic policy as a target policy. However, to the best of our knowledge, both its theoretical and empirical convergence properties have not yet been analyzed. Note that one cannot represent the algorithm in the usual two time-scale stochastic approximation framework to prove its convergence and needs to extend such a framework to non-additive Markov noise and additive martingale difference noise. The Markov noise appears in the algorithm as the full trajectory of the realization of the underlying Markov decision process corresponding to the behaviour policy is taken as input to the algorithm.\nIn this work we give a rigorous almost sure convergence proof of TDC algorithm with importance weighting by formulating it into the two time-scale stochastic approximation framework with non-additive Markov noise and additive martingale difference noise. To the best of our knowledge this is the first time an almost sure convergence proof of off-policy temporal difference learning algorithm with linear function approximation is\npresented for step-sizes satisfying Robbins-Monro conditions. We also support these theoretical results by providing empirical results. Our results show that due to the above-mentioned importance weighting factor, online TDC with importance weighting performs much better than the sub-sampling version of TDC for standard off-policy counterexamples when the behaviour policy is much different from the target policy.\nRecently, emphatic temporal difference learning has been introduced in [8] to solve the off-policy evaluation problem. However, such algorithms are proven to be almost surely convergent for special step-size sequences and weakly convergent for a large range of step-sizes [5].\nAnother related work is the much complex off-policy learning algorithms that obtain the benefits of weighted importance sampling (to reduce variance) with O(d) computational complexity [1]. However, nothing is known about the convergence of such algorithms. In this context, we empirically show that in the case of TDC with importance weighting the variance of the difference between true value function and the estimated one for standard off-policy counterexamples such as [6] becomes very small eventually.\nThe organization of the paper is as follows: Section 2 describes the TDC algorithm with importance weighting. Section 3 gives the rigorous convergence proof of the algorithm. Section 4 shows empirical results supporting our theoretical results. Finally we conclude by providing some interesting future directions."}, {"heading": "2 Background and description of TDC with importance weighting", "text": "We need to estimate the value function for a target policy \u03c0 given the continuing evolution of the underlying MDP (with finite state and action spaces S and A respectively, specified by expected reward r(\u00b7, \u00b7, \u00b7) and transition probability kernel p(\u00b7|\u00b7, \u00b7)) for a behaviour policy \u03c0b with \u03c0 6= \u03c0b. Suppose, the above-mentioned on-policy trajectory is (Xn, An, Rn, Xn+1), n \u2265 0 where {Xn} is a time-homogeneous irreducible Markov chain with unique stationary distribution \u03bd and generated from the behavior policy \u03c0b. Here the quadruplet (s, a, r, s\u2032) represents (current state, action, reward, next state). Also, assume that \u03c0b(a|s) > 0 \u2200s \u2208 S, a \u2208 A. We need to find the solution \u03b8\u2217 for the following:\n0 = \u2211 s,a,s\u2032 \u03bd(s)\u03c0(a|s)p(s\u2032|s, a)\u03b4(\u03b8; s, a, s\u2032)\u03c6(s) = E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)]\n= b\u2212A\u03b8, (1)\nwhere\n(i) \u03b8 \u2208 Rd is the parameter for value function,\n(ii) \u03c6 : S \u2192 Rd is a vector of state features,\n(iii) X \u223c \u03bd,\n(iv) 0 < \u03b3 < 1 is the discount factor, (v) E[R|X = s, Y = s\u2032] = \u2211 a\u2208A \u03c0b(a|s)r(s, a, s\u2032),\n(vi) P (Y = s\u2032|X = s) = \u2211 a\u2208A \u03c0b(a|s)p(s\u2032|s, a),\n(vii) \u03b4(\u03b8; s, a, s\u2032) = r(s, a, s\u2032) + \u03b3\u03b8T\u03c6(s\u2032)\u2212 \u03b8T\u03c6(s) is the temporal difference term with expected single-stage reward,\n(viii) \u03c1X,A = \u03c0(A|X) \u03c0b(A|X) ,\n(ix) \u03b4X,R,Y = R+ \u03b3\u03b8T\u03c6(Y )\u2212 \u03b8T\u03c6(X),\n(x) A = E[\u03c1X,A\u03c6(X)(\u03c6(X)\u2212 \u03b3\u03c6(Y ))T ], b = E[\u03c1X,AR\u03c6(X)].\nThe desired approximate value function under the target policy \u03c0 is V \u2217\u03c0 = \u03b8 \u2217T\u03c6. Let V\u03b8 = \u03b8 T\u03c6. It is well-known ([2]) that \u03b8\u2217 (solution to (1)) satisfies the projected fixed point equation namely V\u03b8 = \u03a0G,\u03bdT \u03c0V\u03b8,\nwhere \u03a0G,\u03bd V\u0302 = arg min\nf\u2208G (\u2016V\u0302 \u2212 f\u2016\u03bd),\nwith G = {V\u03b8|\u03b8 \u2208 Rd} and the Bellman operator\nT\u03c0V\u03b8(s) = \u2211 s\u2032\u2208S \u2211 a\u2208A \u03c0(a|s)p(s\u2032|s, a) [\u03b3V\u03b8(s\u2032) + r(s, a, s\u2032)] .\nHere \u2016 \u00b7 \u2016\u03bd is the weighted Euclidean norm defined by \u2016f\u20162\u03bd = \u2211 s\u2208S f(s)\n2\u03bd(s), Therefore to find \u03b8\u2217, the idea is to minimize the mean square projected Bellman error (MSPBE) J(\u03b8) = \u2016V\u03b8 \u2212 \u03a0G,\u03bdT\u03c0V\u03b8\u20162\u03bd using stochastic gradient descent. It can be shown that the expression of gradient contains product of multiple expectations. Such framework can be modelled by two time-scale stochastic approximation where one iterate stores the quasi-stationary estimates of some of the expectations and the other iterate is used for sampling.\nWe consider the TDC (Temporal Difference with Correction) algorithm with importanceweighting from Sections 4.2 and 5.2 of [2]. The gradient in this case can be shown to satisfy\n\u22121 2 \u2207J(\u03b8) = E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)]\u2212 \u03b3E[\u03c1X,A\u03c6(Y )\u03c6(X)T ]w(\u03b8),\nw(\u03b8) = E[\u03c6(X)\u03c6(X)T ]\u22121E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)].\nDefine \u03c6n = \u03c6(Xn), \u03c6\u2032n = \u03c6(Xn+1), \u03b4n(\u03b8) = \u03b4Xn,Rn,Xn+1(\u03b8) and \u03c1n = \u03c1Xn,An . Therefore the associated iterations in this algorithm are:\n\u03b8n+1 = \u03b8n + a(n)\u03c1n [ \u03b4n(\u03b8n)\u03c6n \u2212 \u03b3\u03c6\u2032n\u03c6Tnwn ] , (2)\nwn+1 = wn + b(n) [ (\u03c1n\u03b4n(\u03b8n)\u2212 \u03c6Tnwn)\u03c6n ] , (3)\nwith {a(n)}, {b(n)} satisfying conditions which will be specified later. Note that the second term inside bracket in (2) is essentially an adjustment or correction of the TD update so that it follows the gradient of the MSPBE objective function thus helping in the desired convergence.\nNote that the sub-sampling version of TDC algorithm (therefore the offline version of TDC algorithm) can be written in the following way:\n\u03b8n+1 = \u03b8n + a(n)I{An=\u03c0(Xn)} [ \u03b4n(\u03b8n)\u03c6n \u2212 \u03b3\u03c6\u2032n\u03c6Tnwn ] ,\nwn+1 = wn + b(n)I{An=\u03c0(Xn)} [ (\u03b4n(\u03b8n)\u2212 \u03c6Tnwn)\u03c6n ] ,\nwhere I{An=\u03c0(Xn)} = 1 if An = \u03c0(Xn) and 0 otherwise. In the rest of the paper both the above algorithms will be denoted by ONTDC and OFFTDC respectively except the figures in Section 4 where we mention the full name."}, {"heading": "3 Almost sure convergence proof of ONTDC", "text": "As mentioned earlier, to analyze the convergence of the iterations (2) and (3) one has to first extend the classic two time-scale stochastic approximation framework of Borkar [11] to a setting with Markov noise. The full extension is shown in the Appendix. We only state here a special case of this theory which will be sufficient for us. Hence we start with this extension and then later show how the TDC iterations can be cast into this framework and proven to be convergent."}, {"heading": "3.1 Two timescale stochastic approximation with Markov noise", "text": "Our goal is to perform an asymptotic analysis of the following coupled recursions:\n\u03b8n+1 = \u03b8n + a(n) [ h(\u03b8n, wn, Z (1) n ) +M (1) n+1 ] , (4)\nwn+1 = wn + b(n) [ g(\u03b8n, wn, Z (2) n ) +M (2) n+1 ] , (5)\nwhere \u03b8n \u2208 Rd, wn \u2208 Rk, n \u2265 0 and {Z(i)n }, {M (i)n }, i = 1, 2 are random processes that we describe below.\nWe make the following assumptions:\n(A1) {Z(i)n } takes values in a compact metric space S(i), i = 1, 2. Additionally, the processes {Z(i)n }, i = 1, 2 are Markov processes with their individual dynamics specified by\nP (Z (i) n+1 \u2208 B(i)|Z(i)m ,m \u2264 n) = \u222b B(i) p(i)(dy|Z(i)n ), n \u2265 0,\nfor B(i) Borel in S(i), i = 1, 2, respectively.\n(A2) h : Rd+k \u00d7 S(1) \u2192 Rd is jointly continuous as well as Lipschitz in its first two arguments uniformly w.r.t the third. The latter condition means that\n\u2200z(1) \u2208 S(1), \u2016h(\u03b8, w, z(1))\u2212 h(\u03b8\u2032, w\u2032, z(1))\u2016 \u2264 L(1)(\u2016\u03b8 \u2212 \u03b8\u2032\u2016+ \u2016w \u2212 w\u2032\u2016).\nSame thing is also true for g where the Lipschitz constant is L(2). Note that the Lipschitz constant L(i) does not depend on z(i) for i = 1, 2.\n(A3) {M (i)n }, i = 1, 2 are martingale difference sequences w.r.t the increasing \u03c3-fields\nFn = \u03c3(\u03b8m, wm,M (i)m , Z(i)m ,m \u2264 n, i = 1, 2), n \u2265 0,\nsatisfying\nE[\u2016M (i)n+1\u20162|Fn] \u2264 K(1 + \u2016\u03b8n\u20162 + \u2016wn\u20162), i = 1, 2,\nfor n \u2265 0 and a given constant K > 0.\n(A4) The stepsizes {a(n)}, {b(n)} are positive scalars satisfying\u2211 n a(n) = \u2211 n b(n) =\u221e, \u2211 n (a(n)2 + b(n)2) <\u221e, a(n) b(n) \u2192 0.\nMoreover, a(n), b(n), n \u2265 0 are non-increasing.\n(A5) The map S(i) 3 z(i) \u2192 p(i)(dy|z(i)) \u2208 P(S(i)) is continuous. (A6) The function g\u0302(\u03b8, w) = \u222b g(\u03b8, w, z)\u0393(2)(dz) is Lipschitz continuous where \u0393(2)\nis the unique stationary distribution of Z(2)n . Further, for all \u03b8 \u2208 Rd, the o.d.e\nw\u0307(t) = g\u0302(\u03b8, w(t)) (6)\nhas globally asymptotically stable equilibrium \u03bb(\u03b8) where \u03bb : Rd \u2192 Rk is a Lipschitz map with constant K. Moreover, the function V \u2032 : Rd+k \u2192 [0,\u221e) defined by V \u2032(\u03b8, w) = V\u03b8(w) is continuously differentiable where V\u03b8(.) is the Lyapunov function for \u03bb(\u03b8). This extra condition is needed so that the set graph(\u03bb):={(\u03b8, \u03bb(\u03b8)) : \u03b8 \u2208 Rd} becomes a globally asymptotically stable set of the coupled o.d.e\nw\u0307(t) = g\u0302(\u03b8(t), w(t)), \u03b8\u0307(t) = 0. (A7) Let h\u0302(\u03b8) = \u222b h(\u03b8, \u03bb(\u03b8), z)\u0393(1)(dz) where \u0393(1) is the unique stationary distribu-\ntion of the Markov process Z(1). Then the o.d.e\n\u03b8\u0307(t) = h\u0302(\u03b8(t))), (7)\nhas a globally asymptotically stable equilibrium \u03b8\u2217.\n(A8) Stability of the iterates: supn(\u2016\u03b8n\u2016+ \u2016wn\u2016) <\u221e a.s. The following theorem is our main result:\nTheorem 3.1 (Slower timescale result). Under assumptions (A1)-(A8),\n(\u03b8n, wn)\u2192 (\u03b8\u2217, \u03bb(\u03b8\u2217))a.s. as n\u2192\u221e.\nWe call (6) and (7) as the faster and slower o.d.e to correspond with faster and slower recursions, respectively."}, {"heading": "3.2 Convergence Proof of ONTDC", "text": "Theorem 3.2. Consider the iterations (2) and (3) of the TDC. Assume the following:\n(i) {a(n)}, {b(n)} satisfy (A4).\n(ii) {(Xn, Rn, Xn+1), n \u2265 0} is such that {Xn} is a time-homogeneous finite state irreducible Markov chain generated from the behavior policy \u03c0b with unique stationary distribution \u03bd. E[Rn|Xn = s,Xn+1 = s\u2032] = \u2211 a\u2208A \u03c0b(a|s)r(s, a, s\u2032)\nand P (Xn+1 = s\u2032|Xn = s) = \u2211 a\u2208A \u03c0b(a|s)p(s\u2032|s, a) where \u03c0b is the behaviour policy, \u03c0 6= \u03c0b. Also, E[R2n|Xn, Xn+1] < \u221e for all n almost surely, and\n(iii) C = E[\u03c6(X)\u03c6(X)T ] and A = E[\u03c1X,A\u03c6(X)(\u03c6(X) \u2212 \u03b3\u03c6(Y ))T ] are nonsingular where X \u223c \u03bd.\n(iv) \u03c0b(a|s) > 0 for all s \u2208 S, a \u2208 A.\n(v) supn(\u2016\u03b8n\u2016+ \u2016wn\u2016) <\u221e w.p. 1.\nThen the parameter vector \u03b8n converges with probability one as n\u2192\u221e to the TD(0) solution (1).\nProof 1. The iterations (2) and (3) can be cast into the framework of Section 3.1 with\n(i) Z(i)n = Xn\u22121,\n(ii) h(\u03b8, w, z) = E[(\u03c1n(\u03b4n(\u03b8n)\u03c6n \u2212 \u03b3\u03c6\u2032n\u03c6Tnwn))|Xn\u22121 = z, \u03b8n = \u03b8, wn = w],\n(iii) g(\u03b8, w, z) = E[((\u03c1n\u03b4n(\u03b8n)\u2212 \u03c6Tnwn)\u03c6n)|Xn\u22121 = z, \u03b8n = \u03b8, wn = w],\n(iv) M (1)n+1 = \u03c1n(\u03b4n(\u03b8n)\u03c6n\u2212\u03b3\u03c6\u2032n\u03c6Tnwn)\u2212E[\u03c1n(\u03b4n(\u03b8n)\u03c6n\u2212\u03b3\u03c6\u2032n\u03c6Tnwn)|Xn\u22121, \u03b8n, wn],\n(v) M (2)n+1 = (\u03c1n\u03b4n(\u03b8n)\u2212 \u03c6Tnwn)\u03c6n \u2212 E[(\u03c1n\u03b4n(\u03b8n)\u2212 \u03c6n Twn)\u03c6n|Xn\u22121, \u03b8n, wn],\n(vi) Fn = \u03c3(\u03b8m, wm, Rm\u22121, Xm\u22121, Am\u22121,m \u2264 n, i = 1, 2), n \u2265 0.\nNote that in (ii) and (iii) we can define h and g independent of n due to time-homogeneity of {Xn}.\nNow, we verify the assumptions (A1)-(A8) (mentioned in Section 3.1) for our application:\n(i) (A1): Z(i)n ,\u2200n, i = 1, 2 takes values in compact metric space as {Xn} is a finite state Markov chain.\n(ii) (A5): Continuity of transition kernel follows trivially from the fact that we have a finite state MDP.\n(iii) (A2)\n\u2016h(\u03b8, w, z)\u2212 h(\u03b8\u2032, w\u2032, z)\u2016 = \u2016E[\u03c1n(\u03b8 \u2212 \u03b8\u2032)T (\u03b3\u03c6(Xn+1)\u2212 \u03c6(Xn))\u03c6(Xn) \u2212 \u03b3\u03c1n\u03c6(Xn+1)\u03c6(Xn)T (w \u2212 w\u2032)|Xn\u22121 = z]\u2016 \u2264 L(2\u2016\u03b8 \u2212 \u03b8\u2032\u2016M2 + \u2016w \u2212 w\u2032\u2016M2),\nwhere M = maxs\u2208S \u2016\u03c6(s)\u2016 with S being the state space of the MDP and L = max(s,a)\u2208(S\u00d7A) \u03c0(a|s) \u03c0b(a|s) . Hence h is Lipschitz continuous in the first two arguments uniformly w.r.t the third. In the last inequality above, we use the Cauchy-Schwarz inequality. As with the case of h, g can be shown to be Lipschitz continuous in the first two arguments uniformly w.r.t the third. Joint continuity of h and g follows from the above as well as the finiteness of S.\n(iv) (A3): Clearly, {M (i)n+1}, i = 1, 2 are martingale difference sequences w.r.t. increasing \u03c3-fields Fn. Note that E[\u2016M (i)n+1\u20162|Fn] \u2264 K(1 + \u2016\u03b8n\u20162 + \u2016wn\u20162) a.s., n \u2265 0 since E[R2n|Xn, Xn+1] <\u221e for all n almost surely and S is finite.\n(v) (A4): This follows from the conditions (i) in the statement of Theorem 3.2.\nNow, one can see that the faster o.d.e. becomes\nw\u0307(t) = E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)]\u2212 E[\u03c6(X)\u03c6(X)T ]w(t).\nClearly, C\u22121E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)] is the globally asymptotically stable equilibrium of the o.d.e. The corresponding Lyapunov function V (\u03b8, w) = 12\u2016Cw\u2212E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)]\u2016 2 is continuously differentiable. Additionally, \u03bb(\u03b8) = C\u22121E[\u03c1X,A\u03b4X,R,Y (\u03b8)\u03c6(X)] and it is Lipschitz continuous in \u03b8, verifying (A6). , Further, A\u22121E[\u03c1X,AR\u03c6(X)] is the globally asymptotically stable equilibrium of the slower o.d.e., verifying (A7). Also, (A8) is (v) in the statement of Theorem 3.2. Therefore the assumptions (A1)\u2212 (A8) are verified. The proof then follows from Theorem 3.1.\nRemark 1. Because of the fact that the gradient is a product of two expectations the scheme is a \u201cpseudo\u201d-gradient descent which helps to find the global minimum here.\nRemark 2. Here we assume the stability of the iterates (2) and (3). Certain sufficient conditions have been sketched for showing stability of single timescale stochastic recursions with controlled Markov noise [12, p. 75, Theorem 9]. This subsequently needs to be extended to the case of two time-scale recursions. In this context we mention that the way single timescale Borkar-Meyn theorem was used in [10] to prove stability of two time-scale recursions is not a proper way to prove the same.\nRemark 3. Convergence analysis for ONTDC along with eligibility traces cf. [2, p. 74] where it is called GTD(\u03bb) can be done similarly using our results. The main advantage is that it works for \u03bb < 1L\u03b3 (\u03bb \u2208 [0, 1] being the eligibility function) whereas the analysis in [4] is shown only for \u03bb very close to 1."}, {"heading": "4 Empirical results", "text": "For the assessment of the algorithm experimentally we have compared the result on a variation of the classic Baird\u2019s off-policy counter-example [2, Fig. 2.4] and \u03b8 \u2192 2\u03b8 problem [8, Section 3]. In both cases, we compare the TD(0), OFFTDC and ONTDC. Unlike [10] where updating was done synchronously in dynamic-programming-like sweeps through the state space, we consider the usual stochastic approximation scenario where only simulated sample trajectories are taken as input to the algorithms i.e. the algorithms do not use any knowledge of the probabilities for the underlying Markov decision process. For Baird\u2019s problem our performance metric is Root Mean Squared Error (RMSE) defined to be the square root of the average of the square of the deviation between true value function and the estimated value function. For \u03b8 \u2192 2\u03b8 problem the y-axis is \u03b8 itself. The average is taken over 1000 simulation run and the metric is plotted against the number of times \u03b8n is updated. While the analysis has been shown for the diminishing step-size case, we implement here the algorithm with constant step-sizes as in [2, 10].\nThe \u03b8 \u2192 2\u03b8 problem consists of only 2 states where \u03b8 and 2\u03b8 are the estimated value of the states. According to its behavior policy with probability p = 12 it stays on the same state and chooses the other state. The target policy is to choose the action that accesses the second state with probability 1 (See Fig. 1 in [8, Section 3] for details). The constant step-sizes are chosen as a(n) = .075; b(n) = .05 for the two time-scale algorithms and \u03b1 = .075 for single timescale algorithms. The simulations are run for 1000 different sample paths. Rewards in all transitions are zero. The initial values are \u03b8 = 1 and w = 0. The results are summarized in Figure 2.\nNext we consider the \u20197-star\u2019 version of Baird\u2019s counter example from [2, p .17] All the rewards in transitions are zero and true value function for each state is zero. The value functions are approximated as V (s) = 2\u03b8(s) + \u03b80 \u2200s \u2208 {1, 2 . . . 6} and V (7) = \u03b8(7) + 2\u03b80. The behaviour policy is to choose the state 7 with probability\nq = 17 and choose uniformly states 1 \u2212 6 with probability (1 \u2212 q) = 6 7 . The target policy is to choose the state 7 with probability 1. The step size chosen for this setting is a = .005, b = .05. The initial parameters are \u03b8 = (1, 1, 1, 1, 1, 1, 10, 1) and w = 0. The results in this case are summarized in Figure 1.\nIn both cases (Fig. 2 and 1) ONTDC performs better than the OFFTDC. The difference becomes more apparent when behaviour policy differs significantly from the target policy (Fig 3 and 4). The intuition is that in case of OFFTDC the TD update is weighted by only step-size whereas in case of ONTDC it is additionally weighted by \u03c1n. Therefore by changing the behaviour policy one can improve the rate of convergence of the algorithm. In the case of on-policy learning for the \u03b8 \u2192 2\u03b8 problem, Figure 5 shows that with eligibility traces the performance of ONTDC is much closer to TD(\u03bb) compared to the case with \u03bb = 0.\nAlthough ONTDC uses importance weighting in its update, this is not importance sampling used in Monte-Carlo algorithms which is the source of high variance. Further, ONTDC does not have any follow-on trace like emphatic TD which has a high variance. We show in Fig. 6 that the variances of the performance metric for the ONTDC is negligible eventually for the two standard counterexamples.\nFor both the aforementioned examples the results for the extension to eligibility\ntraces (the algorithm is called GTD(\u03bb) or TDC(\u03bb)) can be seen in Fig. 7 with \u03bb = 0.1. Fig. 8 shows the results of experiments where the step-size sequences obey the requirements in (A5). We observe good convergence behaviour in this case that is also better when compared with the case of constant step-sizes as considered in the main paper."}, {"heading": "5 Conclusion", "text": "We presented almost sure convergence proof for an off-policy temporal difference learning algorithm that is also extendible to eligibility traces (for a sufficiently large range of \u03bb) with linear function approximation under the assumption that the \u201con-policy\u201d trajectory for a behaviour policy is only available. This has previously not been done to\nour knowledge. A future direction would be to similarly extend algorithms for off-policy control ([3]) to the more realistic settings as we consider in this paper."}], "references": [{"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R.Mahmood", "R.S.Sutton"], "venue": "Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R.Maei", "C.Szepesv\u00e1ri", "S.Bhatnagar", "R.S.Sutton"], "venue": "International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S.Sutton", "A.G.Barto"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "An emphatic approach to the problem of offpolicy temporal-difference learning", "author": ["R.S.Sutton", "A.R.Mahmood", "M.White"], "venue": "Technical report, University of Alberta,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Fast gradientdescent methods for temporal-difference learning with linear function approximation", "author": ["R.S.Sutton", "H.R.Maei", "D.Precup", "S.Bhatnagar", "D.Silver", "E.Wiewiora"], "venue": "International Conference on Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "See [7] for additional uses.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "In [9, 10, 2] the gradient temporal difference learning (GTD) algorithms were proposed to solve this problem.", "startOffset": 3, "endOffset": 13}, {"referenceID": 4, "context": "It was shown in [10] that such an algorithm can be proved to be convergent using the classical convergence proof for two timescale stochastic approximation with martingale difference noise [11].", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Recently, emphatic temporal difference learning has been introduced in [8] to solve the off-policy evaluation problem.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Another related work is the much complex off-policy learning algorithms that obtain the benefits of weighted importance sampling (to reduce variance) with O(d) computational complexity [1].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "In this context we mention that the way single timescale Borkar-Meyn theorem was used in [10] to prove stability of two time-scale recursions is not a proper way to prove the same.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "The main advantage is that it works for \u03bb < 1 L\u03b3 (\u03bb \u2208 [0, 1] being the eligibility function) whereas the analysis in [4] is shown only for \u03bb very close to 1.", "startOffset": 54, "endOffset": 60}, {"referenceID": 4, "context": "Unlike [10] where updating was done synchronously in dynamic-programming-like sweeps through the state space, we consider the usual stochastic approximation scenario where only simulated sample trajectories are taken as input to the algorithms i.", "startOffset": 7, "endOffset": 11}, {"referenceID": 4, "context": "While the analysis has been shown for the diminishing step-size case, we implement here the algorithm with constant step-sizes as in [2, 10].", "startOffset": 133, "endOffset": 140}, {"referenceID": 1, "context": "A future direction would be to similarly extend algorithms for off-policy control ([3]) to the more realistic settings as we consider in this paper.", "startOffset": 83, "endOffset": 86}], "year": 2016, "abstractText": "In this paper we provide a rigorous convergence analysis of a \u201coff\u201d-policy temporal difference learning algorithm with linear function approximation and per time-step linear computational complexity in \u201conline\u201d learning environment. The algorithm considered here is TDC with importance weighting introduced by Maei et al. We support our theoretical results by providing suitable empirical results for standard off-policy counterexamples.", "creator": "LaTeX with hyperref package"}}}