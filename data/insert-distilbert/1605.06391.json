{"id": "1605.06391", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "most contemporary multi - task learning methods assume linear models. this setting is considered shallow in the era notion of explicit deep learning. in this paper, we present open a new deep multi - task representation learning framework that learns cross - task sharing vertex structure at every layer in sustaining a deep network. our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional mtl algorithms to tensor random factorisation, to realise automatic multiple learning of end - to - end knowledge involves sharing in deep networks. this is in contrast to existing deep learning approaches that need a user - defined multi - task sharing strategy. our approach applies to both homogeneous and heterogeneous mtl. hybrid experiments demonstrate the efficacy of generating our deep multi - task representation learning in terms of both higher accuracy and fewer design choices.", "histories": [["v1", "Fri, 20 May 2016 15:05:58 GMT  (273kb,D)", "http://arxiv.org/abs/1605.06391v1", "Technical report, 14 pages, 6 figures"], ["v2", "Thu, 16 Feb 2017 20:40:41 GMT  (252kb,D)", "http://arxiv.org/abs/1605.06391v2", "9 pages, Accepted to ICLR 2017 Conference Track. This is a conference version of the paper. For the multi-domain learning part (not in this version), please refer tothis https URL"]], "COMMENTS": "Technical report, 14 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yongxin yang", "timothy hospedales"], "accepted": true, "id": "1605.06391"}, "pdf": {"name": "1605.06391.pdf", "metadata": {"source": "CRF", "title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "authors": ["Yongxin Yang", "Timothy M. Hospedales"], "emails": ["t.hospedales}@qmul.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The paradigm of multi-task learning is to learn multiple related tasks simultaneously so that knowledge obtained from each task can be re-used by the others. Early work in this area focused on neural network models [Caruana, 1997], while more recent methods have shifted focus to kernel methods, sparsity and low-dimensional task representations of linear models [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012]. Nevertheless given the impressive practical efficacy of contemporary deep neural networks (DNN)s in many important applications, we are motivated to revisit MTL from a deep learning perspective. While the machine learning community has focused on MTL for shallow linear models recently, applications have continued to exploit neural network MTL [Zhang et al., 2014, Liu et al., 2015]. The typical design pattern dates back at least 20 years [Caruana, 1997]: define a DNN with shared lower representation layers, which then forks into separate layers and losses for each task. The sharing structure is defined manually: full-sharing up to the fork, and full separation after the fork. However this complicates DNN architecture design because the user must specify the sharing structure: How many task specific layers? How many task independent layers? How to structure sharing if there are many tasks of varying relatedness? In this paper we present a method for end-to-end multi-task learning in DNNs. This contribution can be seen as generalising shallow MTL methods [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012] to learning how to share at every layer of a deep network; or as learning the sharing structure for deep MTL [Caruana, 1997, Zhang et al., 2014, Spieckermann et al., 2014, Liu et al., 2015] which currently must be defined manually on a problem-by-problem basis. Before proceeding it is worth explicitly distinguishing some different problem settings, which have all been loosely referred to as MTL in the literature. Homogeneous MTL: Each task corresponds to a single output. For example, MNIST digit recognition is commonly used to\nar X\niv :1\n60 5.\n06 39\n1v 1\n[ cs\n.L G\n] 2\n0 M\nay 2\n01 6\nevaluate MTL algorithms by casting it as 10 binary classification tasks [Kumar and Daume\u0301 III, 2012]. Heterogeneous MTL: Each task corresponds to a unique set of output(s) [Zhang et al., 2014]. For example, one may want simultaneously predict a person\u2019s age (task one: multi-class classification or regression) as well as identify their gender (task two: binary classification) from a face image. Multi-Domain Learning: Each \u201ctask\u201d corresponds to a dataset [Yang and Hospedales, 2015]. For example, one jointly can train a multi-class object recognition model for images captured by an HD camera (task/domain one) and for those captured by a webcam (task/domain two). The key difference between MTL and MDL is that MTL deals with a change in the target label-space, while MDL deals with differences due input data statistics. Within the machine learning community, key MTL algorithms [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012] have been designed for linear models, and thus have been applied to homogeneous MTL and a special case of MDL when each task just has a single output. Heterogeneous MTL has not been studied systemically, but is popular in applications, which typically use multi-objective (i.e., multiple loss function) neural networks [Zhang et al., 2014, Liu et al., 2015]. In this paper, we propose a multi-task learning method that works on all these settings. The key idea is to use tensor factorisation to divide each set of model parameters (i.e., both FC weight matrices, and convolutional kernel tensors) into shared and task-specific parts. It is a natural generalisation of shallow MTL methods that explicitly or implicitly are based on matrix factorisation [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012, Daume\u0301 III, 2007]. As linear methods, these typically require preengineered features. In contrast, as a deep network, our generalisation can learn directly from raw image data, determining sharing structure in a layer-wise fashion. For the simplest NN architecture \u2013 no hidden layer, single output \u2013 our method reduces to matrix-based ones, therefore matrix-based methods including [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012, Daume\u0301 III, 2007] are special cases of ours."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Multi-Task Learning", "text": "Most contemporary MTL algorithms assume that the input and model are both D dimensional vectors. The models of T tasks can then be stacked into a D \u00d7 T sized matrix W . Despite different motivations and implementations, many matrix-based MTL methods work by placing constrains on W . For example, posing an `2,1 norm on W to encourage low-rank W [Argyriou et al., 2008]. Similarly, [Kumar and Daume\u0301 III, 2012] factorises W as W = LS, i.e., it assigns a lower rank as a hyper-parameter. An earlier work [Evgeniou and Pontil, 2004] proposes that the linear model for each task t can be written as wt = w\u0302t + w\u03020. This is the factorisation L = [w\u03020, w\u03021, . . . , w\u0302T ] and S = [11\u00d7T ; IT ]. In fact, such matrix factorisation encompasses many MTL methods. E.g., [Xue et al., 2007] assumes S\u00b7,i (the ith column of S) is a unit vector generated by a Dirichlet Process and [Passos et al., 2012] models W using linear factor analysis with Indian Buffet Process [Griffiths and Ghahramani, 2011] prior on S."}, {"heading": "2.2 Tensor Factorisation", "text": "In deep learning, tensor factorisation has been used to exploit factorised tensors\u2019 fewer parameters than the original (e.g., 4-way convolutional kernel) tensor, and thus compress and/or speed up the model, e.g., [Lebedev et al., 2015, Novikov et al., 2015]. For shallow linear MTL, tensor factorisation has been used to address problems where tasks are described by multiple independent factors rather than merely indexed by a single factor [Yang and Hospedales, 2015]. Here the D-dimensional linear models for all unique tasks stack into a tensor W, of e.g. D \u00d7 T1 \u00d7 T2 in the case of two task factors. Knowledge sharing is then achieved by imposing tensor norms onW [Romera-paredes et al., 2013, Wimalawarne et al.,\n2014]. Our framework factors tensors for the different reason that for DNN models, parameters include convolutional kernels (N -way tensors) or D1 \u00d7 D2 FC layer weight matrices (2-way tensors). Stacking up these parameters for many tasks results in D1\u00d7 \u00b7 \u00b7 \u00b7 \u00d7DN \u00d7 T tensors within which we share knowledge through factorisation."}, {"heading": "2.3 Heterogeneous MTL and DNNs", "text": "Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs [Caruana, 1997]. This differs from the previously discussed studies [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Bonilla et al., 2007, Jacob et al., 2009, Kumar and Daume\u0301 III, 2012, Romera-paredes et al., 2013, Wimalawarne et al., 2014] which implicitly assume that each task has a single output. Heterogeneous MTL typically uses neural networks with multiple sets of outputs and losses. E.g., [Zhang et al., 2014] uses a DNN to find facial landmarks (regression) as well as recognise facial attributes (classification); while [Liu et al., 2015] proposes a DNN for query classification and information retrieval (ranking for web search). A key commonality of these studies is that they all require a user-defined parameter sharing strategy. A typical design pattern is to use shared layers (same parameters) for lower layers of the DNN and then split (independent parameters) for the top layers. However, there is no systematic way to make such design choices, so researchers usually rely on trialand-error, further complicating the already somewhat dark art of DNN design. In contrast, our method learns where and how much to share representation parameters across the tasks, hence significantly reducing the space of DNN design choices."}, {"heading": "2.4 Multi-Domain Learning", "text": "Multi-domain learning [Dredze et al., 2010, Joshi et al., 2012] is a relatively independent line of research to multi-task learning. There is close relation to domain adaptation (DA), especially supervised DA where all domains have some labelled data, e.g., [Saenko et al., 2010, Duan et al., 2012]. DA and MDL differ in their goals: with DA focusing on improving performance in a specific target domain given a model trained in a different source, and MDL focusing on simultaneously improving performance in all domains analogously to MTL. Though some MTL methods have been applied to MDL scenarios [Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012], they are restricted to single-output problems. For example, School dataset [Argyriou et al., 2007] is MDL as the target is to predict students\u2019 exam score (one single-output task) where students are grouped by school (multiple domains). However, some MDL problems are multi-class classification. E.g., Office object recognition dataset [Saenko et al., 2010] has three domains due to three different capture devices. Our proposed framework applies to this multiclass MDL setting where existing single-output MTL methods [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012, Romera-paredes et al., 2013, Wimalawarne et al., 2014] do not."}, {"heading": "3 Methodology", "text": ""}, {"heading": "3.1 Preliminaries", "text": "We first recap some tensor factorisation basics before explaining how to factorise DNN weight tensors for multi-task representation learning. An N -way tensor W with shape D1\u00d7D2\u00d7\u00b7 \u00b7 \u00b7DN is an N -dimensional array containing \u220fN n=1Dn elements. Scalars, vectors, and matrices can be seen as 0, 1, and 2-way tensors respectively, although the term tensor is usually used for 3-way or higher. A mode-n fibre of W is a Dn-dimensional vector obtained by fixing all but the nth index. The mode-n flattening W(n) of W is the matrix of size Dn \u00d7 \u220f i\u00acnDi constructed by concatenating all of the \u220f i\u00acnDi mode-n fibres along columns.\nThe dot product of two tensors is a natural extension of matrix dot product, e.g., if we have a tensor A of size M1\u00d7M2\u00d7\u00b7 \u00b7 \u00b7P and a tensor B of size P\u00d7N1\u00d7N2 . . . , the tensor dot product A \u2022 B will be a tensor of size M1 \u00d7M2 \u00d7 \u00b7 \u00b7 \u00b7N1 \u00d7N2 \u00b7 \u00b7 \u00b7 by matrix dot product AT(\u22121)B(1) and reshaping1. More generally, tensor dot product can be performed along specified axes, A \u2022 B(i,j) = AT(i)B(j) and reshaping. Here the subscripts indicate the axes of A and B at which dot product is performed. E.g., when A is of size M1\u00d7P\u00d7M3\u00d7\u00b7 \u00b7 \u00b7MI and B is of size N1\u00d7N2\u00d7P \u00d7\u00b7 \u00b7 \u00b7NJ , then A\u2022 B(2,3) is a tensor of size M1\u00d7M3\u00d7\u00b7 \u00b7 \u00b7MI\u00d7N1\u00d7N2\u00d7\u00b7 \u00b7 \u00b7NJ . Matrix-based Knowledge Sharing Assume we have T linear models (tasks) parametrised by D-dimensional weight vectors, so the collection of all models forms a size D \u00d7 T matrix W . One commonly used MTL approach [Kumar and Daume\u0301 III, 2012] is to place a structure constraint on W , e.g., W = LS, where L is a D \u00d7 K matrix and S is a K \u00d7 T matrix. This factorisation recovers a shared factor L and a task-specific factor S. One can see the columns of L as latent basis tasks, and the model w(i) for the ith task is the linear combination of those latent basis tasks with task-specific information S\u00b7,i.\nw(i) := W\u00b7,i = LS\u00b7,i = K\u2211 k=1 L\u00b7,kSk,i (1)\nFrom Single to Multiple Outputs Consider extending this matrix factorisation approach to the case of multiple outputs. The model for each task is then a D1 \u00d7D2 matrix, for D1 input and D2 output dimensions. The collection of all those matrices constructs a D1 \u00d7D2 \u00d7 T tensor. A straightforward extension of Eq. 1 to this case is\nW (i) :=W\u00b7,\u00b7,i = K\u2211\nk=1\nL\u00b7,\u00b7,kSk,i (2)\nThis is equivalent to imposing the same structural constraint on WT(3) (transposed mode-3 flattening of W). It is important to note that this allows knowledge sharing across the tasks only. I.e., knowledge sharing is only across-tasks not across dimensions within a task. However it may be that the knowledge learned in the mapping to one output dimension may be useful to the others within one task. E.g., consider recognising photos of handwritten and print digits \u2013 it may be useful to share across handwritten-print; as well as across different digits within each. In order to support general knowledge sharing across both tasks and outputs within tasks, we propose to use more general tensor factorisation techniques. Unlike matrix factorisation, there are multiple definitions of tensor factorisation, and we use Tucker [Tucker, 1966] and Tensor Train (TT) [Oseledets, 2011] decompositions."}, {"heading": "3.2 Tensor Factorisation for Knowledge Sharing", "text": "Tucker Decomposition Given an N -way tensor of size D1 \u00d7 D2 \u00b7 \u00b7 \u00b7 \u00d7 DN , Tucker decomposition outputs a core tensor S of size K1 \u00d7K2 \u00b7 \u00b7 \u00b7 \u00d7KN , and N matrices U (n) of size Dn \u00d7Kn, such that,\nWd1,d2,...,dN = K1\u2211\nk1=1 K2\u2211 k2=1 \u00b7 \u00b7 \u00b7 KN\u2211 kN=1 Sk1,k2,...,kNU (1) d1,k1 U (2) d2,k2 \u00b7 \u00b7 \u00b7U (N)dN ,kN (3)\nW = S \u2022 U(1)(1,2) \u2022 U (2) (1,2) \u00b7 \u00b7 \u00b7 \u2022 U (N) (1,2) (4)\nTucker decomposition is usually implemented by an alternating least squares (ALS) method [Kolda and Bader, 2009]. However [Lathauwer et al., 2000] treat it as a higher-order singular value decomposition (HOSVD), which is more efficient to solve: U (n) is exactly the U matrix from the SVD of mode-n flattening W(n) of W, and the core tensor S is obtained by,\n1We slightly abuse \u2018-1\u2019 referring to the last axis of the tensor.\nS =W \u2022 U(1)(1,1) \u2022 U (2) (1,1) \u00b7 \u00b7 \u00b7 \u2022 U (N) (1,1) (5)\nTensor Train Decomposition Tensor Train (TT) Decomposition outputs 2 matrices U (1) and U (N) of size D1 \u00d7 K1 and KN\u22121 \u00d7 DN respectively, and (N \u2212 2) 3-way tensors U (n) of size Kn\u22121 \u00d7Dn \u00d7Kn. The elements of W can be computed by,\nWd1,d2,...,dN = K1\u2211\nk1=1 K2\u2211 k2=1 \u00b7 \u00b7 \u00b7 KN\u22121\u2211 kN\u22121=1 U (1) d1,k1 U (2)k1,d2,k2U (3) k2,d3,k3 \u00b7 \u00b7 \u00b7U (N)kN\u22121,dN (6)\n= U (1) d1,\u00b7U (2) \u00b7,d2,\u00b7U (3) \u00b7,d3,\u00b7 \u00b7 \u00b7 \u00b7U (d) \u00b7,dN (7)\nW = U (1) \u2022 U (2) \u00b7 \u00b7 \u00b7 \u2022 U (N) (8)\nwhere U (n)\u00b7,dn,\u00b7 is a matrix of size Kn\u22121\u00d7Kn sliced from U (n) with the second axis fixed at dn. The TT decomposition is typically realised with a recursive SVD-based solution [Oseledets, 2011]. Knowledge Sharing If the final axis of the input tensor above indexes tasks, i.e. if DN = T then the last factor U\n(N) in both decompositions encodes a matrix of task specific knowledge, and the other factors encode shared knowledge."}, {"heading": "3.3 Deep Multi-Task Representation Learning", "text": "To realise deep multi-task representation learning (DMTRL), we learn one DNN per-task each with the same architecture2. However each corresponding layer\u2019s weights are generated with one of the knowledge sharing structures in Eq. 2, Eq. 4 or Eq. 8. Note that we apply these \u2018right-to-left\u2019 in order to generate weight tensors with the specified sharing structure, rather than actually applying Tucker or TT to decompose an input tensor. Thus in the forward pass, we synthesise weight tensors W and perform inference as usual. Our weight generation (construct tensors from smaller pieces) does not introduce nondifferentiable terms, so our deep multi-task representation learner is trainable via standard backpropagation. Specifically, in the backward pass over FC layers, rather than directly learning the 3-way tensor W, our methods learn either {S, U1, U2, U3} (Tucker, Eq. 4), {U1,U2, U3} (TT, Eq. 8), or in the simplest case {L, S} (SVD, Eq. 2). Besides FC layers, contemporary DNN designs often exploit convolutional layers. Those layers usually contain kernel filter parameters that are 3-way tensors of size H \u00d7W \u00d7 C, (where H is height, W is width, and C is the number of input channels) or 4-way tensors of size H \u00d7W \u00d7C \u00d7M , where M is the number of filters in this layer (i.e., the number of output channels). The proposed methods naturally extend to convolution layers as convolution just adds more axes on the left-hand side. E.g., the collection of parameters from a given convolutional layer of T neural networks forms a tensor of shape H \u00d7W \u00d7 C \u00d7M \u00d7 T . These knowledge sharing strategies provide a way to softly share parameters across the corresponding layers of each task\u2019s DNN: where, what, and how much to share are learned from data. This is in contrast to the conventional Deep-MTL approach of manually selecting a set of layers to undergo hard parameter sharing: by tying weights so each task uses exactly the same weight matrix/tensor for the corresponding layer [Zhang et al., 2014, Liu et al., 2015]; and a set of layers to be completely separate: by using independent weight matrices/tensors. In contrast our approach benefits from: (i) automatically learning this sharing structure from data rather than requiring user trial and error, and (ii) smoothly interpolating between fully shared and fully segregated layers, rather than a hard switching between these states. An illustration of the proposed framework for different problem settings can be found in Fig. 1.\n2Except heterogeneous MTL, where the output layer is necessarily unshared due to different dimensionality.\n\u00a0\u00a0\u00a0"}, {"heading": "4 Experiments", "text": "Implementation Details Our method is implemented with TensorFlow [Abadi et al., 2015] . The code is released on GitHub3. For DMTRL-Tucker, DMTRL-TT, and DMTRLSVD, we need to assign the rank of each weight tensor. The DNN architecture itself may be complicated and so can benefit from different ranks at different layers, but grid-search is impractical. However, since all three tensor decomposition methods have the SVD-based solutions, we can initialise the model and set the ranks as follows: First train the DNNs independently in single task learning mode. Then pack the layer-wise parameters as the input for tensor decomposition. When SVD is applied, set a threshold for relative error so\n3https://github.com/wOOL/DMTRL\nSVD will pick the appropriate rank. Thus our method needs only a single hyper parameter of max reconstruction error (we set to = 10% throughout) that indirectly specifies the ranks of every layer. Note that training from random initialisation also works, but the STLbased initialisation makes rank selection easy and transparent. Nevertheless, like [Kumar and Daume\u0301 III, 2012] the framework is not sensitive to ranks choice so long as they are big enough. Our sharing is applied to weight parameters only, bias terms are not shared."}, {"heading": "4.1 Homogeneous MTL", "text": "Dataset, Settings and Baselines We use MNIST handwritten digits. The task is to recognise digit images zero to nine. When this dataset is used for the evaluation of MTL methods, ten 1-vs-all binary classification problems usually define ten tasks [Kumar and Daume\u0301 III, 2012]. The dataset has a given train (60,000 images) and test (10,000 images) split. Each instance is a monochrome image of size 28\u00d7 28\u00d7 1. We use a modified LeNet [LeCun et al., 1998] as the CNN architecture. The first convolutional layer has 32 filters of size 5 \u00d7 5, followed by 2 \u00d7 2 max pooling. The second convolutional layer has 64 filters of size 4 \u00d7 4, and again a 2 \u00d7 2 max pooling. After these two convolutional layers, two fully connected layers with 512 and 1 output(s) are placed sequentially. The convolutional layers and first FC layer use RELU f(x) = max(x, 0) as the activation function. The loss is hinge loss, `(y) = max(0, 1\u2212 y\u0302 \u00b7 y), where y \u2208 \u00b11 is the true label and y\u0302 is the output of each task\u2019s neural network. Conventional matrix-based MTL methods [Evgeniou and Pontil, 2004, Argyriou et al., 2008, Kumar and Daume\u0301 III, 2012, Romera-paredes et al., 2013, Wimalawarne et al., 2014] are linear models taking vector input only, so they need a preprocessing that flattens the image into a vector, and typically reduce dimension by PCA. As per our motivation for studying Deep MTL, our methods will decisively outperform such shallow linear baselines. Thus to find a stronger MTL competitor, we instead search user defined architectures for Deep-MTL parameter sharing (cf [Zhang et al., 2014, Liu et al., 2015, Caruana, 1997]). In all of the four parametrised layers (pooling has no parameters), we set the first N (1 \u2264 N \u2264 3) to be hard shared4. We then use cross-validation to select among the three user-defined MTL architectures and the best option is N = 3, i.e., the first three layers are fully shared (we denote this model UD-MTL). For our methods, all four parametrised layers are softly shared with the different factorisation approaches. To evaluate different MTL methods and a baseline of single task learning (STL), we take ten different fractions of the given 60K training split, train the model, and test on the 10K testing split. For each fraction, we repeat the experiment 5 times with randomly sampled training data. We report two performance metrics: (1) the mean error rate of the ten binary classification problems and (2) the error rate of recognising a digit by ranking each task\u2019s 1-vs-all output (multi-class classification error). Results As we can see in Fig. 2, all MTL approaches outperform STL, and the advantage is more significant when the training data is small. The proposed methods, DMTRL-TT and DMTRL-Tucker outperform the best user-defined MTL when the training data is very small, and their performance is comparable when the training data is larger. For an unfair comparison, shallow matrix-based MTL methods [Kang et al., 2011, Kumar and Daume\u0301 III, 2012] report around 14% error rate (binary classification, 64 dimensional PCA feature) with 1000 training data, and with the same amount of data, our methods have error rate below 6%.\n4This is not strictly all possible user-defined sharing options. For example, another possibility is the first convolutional layer and the first FC layer could be fully shared, with the second convolutional layer being independent (task specific). However, this is against the intuition that lower/earlier layers are more task agnostic, and later layers more task specific. Note that sharing the last layer is technically possible but not intuitive, and in any case not meaningful unless at least one early layer is unshared, as the tasks are different."}, {"heading": "4.2 Heterogeneous MTL: Face Analysis", "text": "Dataset, Settings and Baselines The AdienceFaces [Eidinger et al., 2014] is a largescale face images dataset with the labels of each person\u2019s gender and age group. We use this dataset for the evaluation of heterogeneous MTL with two tasks: (i) gender classification (two classes) and (ii) age group classification (eight classes). Two independent CNN models for this benchmark are introduced in [Levi and Hassncer, 2015]. The two CNNs have the same architecture except for the last fully-connected layer, since the heterogeneous tasks have different number of outputs (two / eight). We take these CNNs from [Levi and Hassncer, 2015] as the STL baseline. We again search for the best possible userdefined MTL architecture as a strong competitor: the proposed CNN has six layers \u2013 three convolutional and three fully-connected layers. The last fully-connected layer has nonshareable parameters because they are of different size. To search the MTL design-space, we try setting the first N (1 \u2264 N \u2264 5) layers to be hard shared between the tasks. Running 5-fold cross-validation on the train set to evaluate the architectures, we find the best choice is N = 5 (i.e., all layers fully shared before the final heterogeneous outputs). For our proposed methods, all the layers before the last heterogeneous dimensionality FC layers are softly shared. We select increasing fractions of the AdienceFaces train split randomly, train the model, and evaluate on the same test set. For reference, there are 12245 images with gender labelled for training, 4007 ones for testing, and 11823 images with age group labelled for training, and 4316 ones for testing.5.\n5https://github.com/GilLevi/AgeGenderDeepLearning/tree/master/Folds/train_val_txt_files_\nper_fold/test_fold_is_0\nResults Fig. 3 shows the error rate for each task. For the gender recognition task, we find that: (i) User-defined MTL is not consistently better than STL, but (ii) our methods, esp., DMTRL-Tucker, consistently outperform both STL and the best user-defined MTL. For the harder age group classification task, our methods generally improve on STL. However UD-MTL does not consistently improve on STL, and even reduces performance when the training set is bigger. This is the negative transfer phenomenon [Rosenstein et al., 2005], where using a transfer learning algorithm is worse than not using it. This difference in outcomes is attributed to sufficient data eventually providing some effective task-specific representation. Our methods can discover and exploit this, but UD-MTL\u2019s hard switch between sharing and not sharing can not represent or exploit such increasing task-specificity of representation."}, {"heading": "4.3 Heterogeneous MTL: Multi-Alphabet Recognition", "text": "Dataset, Settings and Baselines We next consider the task of learning to recognise handwritten letters in multiple languages using the Omniglot [Lake et al., 2015] dataset. Omniglot contains handwritten characters in 50 different alphabets (e.g., Cyrillic, Korean, Tengwar), each with its own number of unique characters (14 \u223c 55). In total, there are 1623 unique characters, and each has exactly 20 instances. Here each task corresponds to an alphabet, and the goal is to recognise its characters. MTL has a clear motivation here, as cross-alphabet knowledge sharing is likely to be useful as one is unlikely to have extensive training data for a wide variety of less common alphabets. The images are monochrome of size 105 \u00d7 105. We design a CNN with 3 convolutional and 2 FC layers. The first conv layer has 8 filters of size 5 \u00d7 5; the second conv layer has 12 filters of size 3 \u00d7 3, and the third convolutional layer has 16 filters of size 3 \u00d7 3. Each convolutional layer is followed by a 2\u00d72 max-pooling. The first FC layer has 64 neurons, and the second FC layer has size corresponding to the number of unique classes in the alphabet. The activation function is tanh. We use a similar strategy to find the best user-defined MTL model: the CNN has 5 parametrised layers, of which 4 layers are potentially shareable. So we tried hard-sharing the first N (1 \u2264 N \u2264 4) layers. Evaluating these options by 5-fold cross-validation, the best option turned out to be N = 3, i.e., the first three layers are hard shared. For our methods, all four shareable layers are softly shared. Since there is no standard train/test split for this dataset, we use the following setting: We repeatedly pick at random 5, . . . 90% of images per class for training. Note that 5% is the minimum, corresponding to one-shot learning. The remaining data are used for evaluation. Results Fig. 4 reports the average error rate across all 50 tasks (alphabets). Our proposed MTL methods surpass the STL baseline in all cases. User-defined MTL does not work well when the training data is very small, but does help when training fraction is larger than 50%. Measuring the Learned Sharing Compared to the conventional user-defined sharing architectures, our method learns how to share from data. We next try to quantify the amount of sharing estimated by our model on the Omniglot data. Returning to the key factorisation W = LS, we can find that S-like matrix appears in all variants of proposed method. It is S in DMTRL-SVD, the transposed U (N) in DMTRL-Tucker, and U (N) in DMTRL-TT (N is the last axis of W). S is a K \u00d7 T size matrix, where T is the number of tasks, and K is the number of latent tasks [Kumar and Daume\u0301 III, 2012] or the dimension of task coding [Yang and Hospedales, 2015]. Each column of S is a set of coefficients that produce the final weight matrix/tensor by linear combination. If we put STL and userdefined MTL (for a certain shared layer) in this framework, we see that STL is to assign (rather than learn) S to be an identity matrix IT . Similarly, user-defined MTL (for a certain shared layer) is to assign S to be a matrix with all zeros but one particular row is all ones, e.g., S = [11\u00d7T ;0]. Between these two extremes, our method learns the sharing structure in S. We propose the following equation to measure the learned sharing strength:\n\u03c1 = 1( T 2 ) \u2211 i<j \u2126(S\u00b7,i, S\u00b7,j) = 2 T (T \u2212 1) \u2211 i<j \u2126(S\u00b7,i, S\u00b7,j) (9)\nHere \u2126(a, b) is a similarity measure for two vectors a and b and we use cosine similarity. \u03c1 is the average on all combinations of column-wise similarity. So \u03c1 measures how much sharing is encoded by S between \u03c1 = 0 for STL (nothing to share) and \u03c1 = 1 for user-defined MTL (completely shared). Since S is a real-valued matrix in our scenario, we normalise it before applying Eq. 9: First we take absolute values, because large either positive or negative value suggests a significant coefficient. Second we normalise each column of S by applying a softmax function, so the sum of every column is 1. The motivation behind the second step is to make a matched range of our S with S = IT or S = [11\u00d7T ;0], as for those two cases, the sum of each column is 1 and the range is [0, 1]. For the Omniglot experiment, we plot the measured sharing amount for training fraction 10%. Fig. 4 reveals that three proposed methods tend to share more for bottom layers (\u2018Conv1\u2019, \u2018Conv2\u2019, and \u2018Conv3\u2019) and share less for top layer (\u2018FC1\u2019). This is qualitatively similar to the best user-defined MTL, where the first three layers are fully shared (\u03c1 = 1) and the 4th layer is completely not shared (\u03c1 = 0). However, our methods: (i) learn this structure in a purely data-driven way and (ii) benefits from the ability to smoothly interpolate between high and low degrees of sharing as depth increases. As an illustration, Fig. 4 also shows example text from the most and least similar language pairs as estimated at our multilingual character recogniser\u2019s FC1 layer (the result can vary across layers)."}, {"heading": "4.4 Multi-Domain Learning", "text": "Dataset, Settings and Baselines For MDL, we consider a digit recognition problem with three different datasets \u2013 MNIST, USPS, and SVHN. Here a domain now corresponds to dataset, and the tasks are multi-class (digits 0-9) recognition. USPS is a smaller handwritten digit dataset with 7291 training and 2007 testing images. The Street View House Numbers (SVHN) [Netzer et al., 2011] is an image dataset that contains photos of door numbers. SVHN \u2018Format 2\u2019 has cropped digits, which makes it a MNIST-like problem. However it is harder than MNIST as more than one digit may appear in an image, and the model should\ncorrectly label the one in the centre. Directly applying a model trained on one of these tasks to another would not yield good performance due to domain shift [Saenko et al., 2010]. The multi-domain learning problem setting considered in this section is related to supervised domain adaptation (where all domains have labels) [Saenko et al., 2010], however the objective is to perform well in all domains rather than only on one special target domain. Few existing MTL methods can deal with this multi-class recognition problem, as they usually assume each task has a single output. Applying the strategy in the last section (heterogeneous MTL) is technically possible, but not intuitive because in this case each task is the same \u2013 classifying ten digits, and task difference is essentially from dataset bias. Therefore we compare our methods with two single task learning modes. STL is to train three independent models for MNIST, USPS, and SVHN respectively. ALL is to train a single model that ignores domain information, aggregating all data sources. We use the pre-given train/test splits for all three datasets in this experiment. We increase the size of MNIST/USPS images from 28\u00d7 28/16\u00d7 16 to 32\u00d7 32, and copy the data to each of three colour channels, so that all data have the same 32 \u00d7 32 \u00d7 3 dimensions. For the network architecture, we use the same design as [Goodfellow et al., 2013]. Results Fig. 5 shows that USPS gets the most\nbenefit from multi-task learning with the other datasets. USPS dataset has a train-test bias, with the images in testing set being harder than the training set. MNIST images serve as a good auxiliary data for transfer in this case. MNIST benefits slightly from MTL in the small data condition, and SVHN not at all. Conceptually, the presence of USPS could be beneficial for MNIST but, as USPS is a much smaller dataset compared to MNIST, its contribution is not significant. It is perhaps unsurprising that MNIST/USPS does not help SVHN due to the extreme difference between written/photo digit appearances. Their significant difference can be seen in that a well trained MNIST model performs just slightly better than random on SVHN. On the other hand, it is noteworthy that the ALL baseline experiences significant negative transfer on both SVHN and MNIST in particular when the amount of data is small. In contrast, despite the huge dataset shift, by learning how much to share, our methods do not suffer from negative transfer."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a novel framework for end-to-end multi-task representation learning in contemporary deep neural networks. The key idea is to generalise matrix factorisationbased multi-task ideas to tensor factorisation, in order to flexibly share knowledge in fully connected and convolutional DNN layers. Our method provides consistently better perfor-\nmance than single task learning and comparable or better performance than the best results from exhaustive search of user-defined MTL architectures. It reduces the design choices and architectural search space that must be explored in the workflow of Deep MTL architecture design [Caruana, 1997, Zhang et al., 2014, Liu et al., 2015], relieving researchers of the need to decide how to structure layer sharing/segregation. Instead sharing structure is determined in a data-driven way on a layer-by-layer basis that moreover allows a smooth interpolation between sharing and not sharing in progressively deeper layers."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from tensorflow.org", "author": ["Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "Yu and Zheng.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Zheng.", "year": 2015}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Multi-task gaussian process prediction", "author": ["E.V. Bonilla", "K.M. Chai", "C. Williams"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Bonilla et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bonilla et al\\.", "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Frustratingly easy domain adaptation", "author": ["H. Daum\u00e9 III"], "venue": "In ACL,", "citeRegEx": "III.,? \\Q2007\\E", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Multi-domain learning by confidence-weighted parameter combination", "author": ["M. Dredze", "A. Kulesza", "K. Crammer"], "venue": "Machine Learning,", "citeRegEx": "Dredze et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2010}, {"title": "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach", "author": ["L. Duan", "D. Xu", "S.-F. Chang"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Duan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2012}, {"title": "Age and gender estimation of unfiltered faces", "author": ["E. Eidinger", "R. Enbar", "T. Hassner"], "venue": "IEEE Transactions on Information Forensics and Security,", "citeRegEx": "Eidinger et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eidinger et al\\.", "year": 2014}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "In Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "Evgeniou and Pontil.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou and Pontil.", "year": 2004}, {"title": "The indian buffet process: An introduction and review", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Griffiths and Ghahramani.,? \\Q2011\\E", "shortCiteRegEx": "Griffiths and Ghahramani.", "year": 2011}, {"title": "Clustered multi-task learning: A convex formulation", "author": ["L. Jacob", "J.-p. Vert", "F.R. Bach"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Multi-domain learning: When do domains matter", "author": ["M. Joshi", "M. Dredze", "W.W. Cohen", "C.P. Ros\u00e9"], "venue": "In Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "Joshi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2012}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review,", "citeRegEx": "Kolda and Bader.,? \\Q2009\\E", "shortCiteRegEx": "Kolda and Bader.", "year": 2009}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daum\u00e9 III"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Kumar and III.,? \\Q2012\\E", "shortCiteRegEx": "Kumar and III.", "year": 2012}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "A multilinear singular value decomposition", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "Speedingup convolutional neural networks using fine-tuned cp-decomposition", "author": ["V. Lebedev", "Y. Ganin", "M. Rakhuba", "I.V. Oseledets", "V.S. Lempitsky"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Lebedev et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev et al\\.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Age and gender classification using convolutional neural networks", "author": ["G. Levi", "T. Hassncer"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Levi and Hassncer.,? \\Q2015\\E", "shortCiteRegEx": "Levi and Hassncer.", "year": 2015}, {"title": "Representation learning using multi-task deep neural networks for semantic classification and information", "author": ["X. Liu", "J. Gao", "X. He", "L. Deng", "K. Duh", "Y.-Y. Wang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D. Vetrov"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Flexible modeling of latent task structures in multitask learning", "author": ["A. Passos", "P. Rai", "J. Wainer", "H. Daum\u00e9 III"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Passos et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2012}, {"title": "Multilinear multitask learning", "author": ["B. Romera-paredes", "H. Aung", "N. Bianchi-berthouze", "M. Pontil"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Romera.paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.paredes et al\\.", "year": 2013}, {"title": "To transfer or not to transfer", "author": ["M.T. Rosenstein", "Z. Marx", "L.P. Kaelbling", "T.G. Dietterich"], "venue": "NIPS Workshop, Inductive Transfer: 10 Years Later,", "citeRegEx": "Rosenstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rosenstein et al\\.", "year": 2005}, {"title": "Adapting visual category models to new domains", "author": ["K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "Saenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Saenko et al\\.", "year": 2010}, {"title": "Data-effiicient temporal regression with multitask recurrent neural networks", "author": ["S. Spieckermann", "S. Udluft", "T. Runkler"], "venue": "In NIPS Workshop on Transfer and Multi-Task Learning,", "citeRegEx": "Spieckermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Spieckermann et al\\.", "year": 2014}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L.R. Tucker"], "venue": null, "citeRegEx": "Tucker.,? \\Q1966\\E", "shortCiteRegEx": "Tucker.", "year": 1966}, {"title": "Multitask learning meets tensor factorization: task imputation via convex optimization", "author": ["K. Wimalawarne", "M. Sugiyama", "R. Tomioka"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "Multi-task learning for classification with dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Xue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Early work in this area focused on neural network models [Caruana, 1997], while more recent methods have shifted focus to kernel methods, sparsity and low-dimensional task representations of linear models [Evgeniou and Pontil, 2004, Argyriou et al.", "startOffset": 57, "endOffset": 72}, {"referenceID": 4, "context": "The typical design pattern dates back at least 20 years [Caruana, 1997]: define a DNN with shared lower representation layers, which then forks into separate layers and losses for each task.", "startOffset": 56, "endOffset": 71}, {"referenceID": 2, "context": "For example, posing an `2,1 norm on W to encourage low-rank W [Argyriou et al., 2008].", "startOffset": 62, "endOffset": 85}, {"referenceID": 9, "context": "An earlier work [Evgeniou and Pontil, 2004] proposes that the linear model for each task t can be written as wt = \u0175t + \u01750.", "startOffset": 16, "endOffset": 43}, {"referenceID": 32, "context": ", [Xue et al., 2007] assumes S\u00b7,i (the ith column of S) is a unit vector generated by a Dirichlet Process and [Passos et al.", "startOffset": 2, "endOffset": 20}, {"referenceID": 25, "context": ", 2007] assumes S\u00b7,i (the ith column of S) is a unit vector generated by a Dirichlet Process and [Passos et al., 2012] models W using linear factor analysis with Indian Buffet Process [Griffiths and Ghahramani, 2011] prior on S.", "startOffset": 97, "endOffset": 118}, {"referenceID": 10, "context": ", 2012] models W using linear factor analysis with Indian Buffet Process [Griffiths and Ghahramani, 2011] prior on S.", "startOffset": 73, "endOffset": 105}, {"referenceID": 4, "context": "3 Heterogeneous MTL and DNNs Some studies consider heterogeneous MTL, where tasks may have different numbers of outputs [Caruana, 1997].", "startOffset": 120, "endOffset": 135}, {"referenceID": 21, "context": ", 2014] uses a DNN to find facial landmarks (regression) as well as recognise facial attributes (classification); while [Liu et al., 2015] proposes a DNN for query classification and information retrieval (ranking for web search).", "startOffset": 120, "endOffset": 138}, {"referenceID": 1, "context": "For example, School dataset [Argyriou et al., 2007] is MDL as the target is to predict students\u2019 exam score (one single-output task) where students are grouped by school (multiple domains).", "startOffset": 28, "endOffset": 51}, {"referenceID": 28, "context": ", Office object recognition dataset [Saenko et al., 2010] has three domains due to three different capture devices.", "startOffset": 36, "endOffset": 57}, {"referenceID": 30, "context": "Unlike matrix factorisation, there are multiple definitions of tensor factorisation, and we use Tucker [Tucker, 1966] and Tensor Train (TT) [Oseledets, 2011] decompositions.", "startOffset": 103, "endOffset": 117}, {"referenceID": 24, "context": "Unlike matrix factorisation, there are multiple definitions of tensor factorisation, and we use Tucker [Tucker, 1966] and Tensor Train (TT) [Oseledets, 2011] decompositions.", "startOffset": 140, "endOffset": 157}, {"referenceID": 14, "context": "Tucker decomposition is usually implemented by an alternating least squares (ALS) method [Kolda and Bader, 2009].", "startOffset": 89, "endOffset": 112}, {"referenceID": 17, "context": "However [Lathauwer et al., 2000] treat it as a higher-order singular value decomposition (HOSVD), which is more efficient to solve: U (n) is exactly the U matrix from the SVD of mode-n flattening W(n) of W, and the core tensor S is obtained by, 1We slightly abuse \u2018-1\u2019 referring to the last axis of the tensor.", "startOffset": 8, "endOffset": 32}, {"referenceID": 24, "context": "The TT decomposition is typically realised with a recursive SVD-based solution [Oseledets, 2011].", "startOffset": 79, "endOffset": 96}, {"referenceID": 19, "context": "We use a modified LeNet [LeCun et al., 1998] as the CNN architecture.", "startOffset": 24, "endOffset": 44}, {"referenceID": 8, "context": "2 Heterogeneous MTL: Face Analysis Dataset, Settings and Baselines The AdienceFaces [Eidinger et al., 2014] is a largescale face images dataset with the labels of each person\u2019s gender and age group.", "startOffset": 84, "endOffset": 107}, {"referenceID": 20, "context": "Two independent CNN models for this benchmark are introduced in [Levi and Hassncer, 2015].", "startOffset": 64, "endOffset": 89}, {"referenceID": 20, "context": "We take these CNNs from [Levi and Hassncer, 2015] as the STL baseline.", "startOffset": 24, "endOffset": 49}, {"referenceID": 27, "context": "This is the negative transfer phenomenon [Rosenstein et al., 2005], where using a transfer learning algorithm is worse than not using it.", "startOffset": 41, "endOffset": 66}, {"referenceID": 16, "context": "3 Heterogeneous MTL: Multi-Alphabet Recognition Dataset, Settings and Baselines We next consider the task of learning to recognise handwritten letters in multiple languages using the Omniglot [Lake et al., 2015] dataset.", "startOffset": 192, "endOffset": 211}, {"referenceID": 22, "context": "The Street View House Numbers (SVHN) [Netzer et al., 2011] is an image dataset that contains photos of door numbers.", "startOffset": 37, "endOffset": 58}, {"referenceID": 28, "context": "Directly applying a model trained on one of these tasks to another would not yield good performance due to domain shift [Saenko et al., 2010].", "startOffset": 120, "endOffset": 141}, {"referenceID": 28, "context": "The multi-domain learning problem setting considered in this section is related to supervised domain adaptation (where all domains have labels) [Saenko et al., 2010], however the objective is to perform well in all domains rather than only on one special target domain.", "startOffset": 144, "endOffset": 165}], "year": 2016, "abstractText": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL, as well as multi-domain learning (MDL). Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "creator": "LaTeX with hyperref package"}}}