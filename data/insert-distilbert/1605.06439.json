{"id": "1605.06439", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2016", "title": "Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning", "abstract": "we consider online learning algorithms that guarantee worst - case regret rates in some adversarial environments ( so they can be deployed safely appropriately and will perform robustly ), yet adapt properly optimally to favorable stochastic environments ( so they will perform well initially in a variety of settings sometimes of practical importance ). we quantify the friendliness of stochastic environments by means of the well - known descriptive bernstein ( a. k. a. generalized tsybakov margin ) condition. for two recent algorithms ( squint for the hedge setting and metagrad for online convex optimization ) we show that the particular form of their data - dependent individual - sequence regret guarantees implies that they adapt automatically to the bernstein parameters of fleeing the stochastic environment. we prove that. these algorithms attain fast graduation rates in their respective settings both in expectation and with associated high probability.", "histories": [["v1", "Fri, 20 May 2016 17:05:55 GMT  (24kb)", "http://arxiv.org/abs/1605.06439v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wouter m koolen", "peter gr\u00fcnwald", "tim van erven"], "accepted": true, "id": "1605.06439"}, "pdf": {"name": "1605.06439.pdf", "metadata": {"source": "CRF", "title": "Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning", "authors": ["Wouter M. Koolen"], "emails": ["wmkoolen@cwi.nl", "pdg@cwi.nl", "tim@timvanerven.nl"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n06 43\n9v 1"}, {"heading": "1 Introduction", "text": "We consider online sequential decision problems. We focus on full information settings, encompassing such interaction protocols as online prediction, classification and regression, prediction with expert advice or the Hedge setting, and online convex optimization (see Cesa-Bianchi and Lugosi 2006). The goal of the learner is to choose a sequence of actions with small regret, i.e. such that his cumulative loss is not much larger than the loss of the best fixed action in hindsight. This has to hold even in the worst case, where the environment is controlled by an adversary. After three decades of research there exist many algorithms and analysis techniques for a variety of such settings. For many\nsettings, adversarial regret lower bounds of order \u221a T are known, along with matching individual sequence algorithms [Shalev-Shwartz, 2011]. A more recent line of development is to design adaptive algorithms with regret guarantees that scale with some more refined measure of the complexity of the problem. For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015]. Interestingly, the price for such adaptivity (i.e. the worsening of the worst-case regret bound) is typically extremely small (i.e. a constant factor in the regret bound). For online convex optimization (OCO), many different types of adaptivity have been explored, including by [Crammer et al., 2009, Duchi et al., 2011, McMahan and Streeter, 2010, Hazan and Kale, 2010, Chiang et al., 2012, Steinhardt and Liang, 2014, Koolen and Van Erven, 2016].\nHere we are interested in the question of whether such adaptive results are strong enough to lead to improved rates in the stochastic case when the data follow a \u201cfriendly\u201d distribution. In specific cases it has been shown that fancy guarantees do imply significantly reduced regret. For example Gaillard et al. [2014] present a generic argument showing that a certain kind of second-order regret guarantees implies constant expected regret (the fastest possible rate) for i.i.d. losses drawn from a distribution with a gap (between expected loss of the best and all other actions). In this paper we significantly extend this result. We show that a variety of individual-sequence second-order regret guarantees imply fast regret rates for distributions under much milder stochastic assumptions. In particular, we will look at the Bernstein condition (see Bartlett and Mendelson 2006), which is the key to fast rates in the batch setting. This condition provides a parametrised interpolation (expressed in terms of the Bernstein exponent \u03ba \u2208 [0,1]) between the friendly gap case (\u03ba = 1) and the stochastic worst case(\u03ba = 0). We show that appropriate second-order guarantees automatically lead to adaptation to these parameters, for both the Hedge setting and for OCO. In the Hedge setting, we build on the guarantees available for the Squint algorithm [Koolen and Van Erven, 2015] and for OCO we rely on guarantees achieved by MetaGrad [Koolen and Van Erven, 2016] to obtain regret rates of order T 1\u2212\u03ba 2\u2212\u03ba (Theorem 3). We show this, not just in expectation, but also with high probability. Our proofs use that, for bounded losses, the Bernstein condition is equivalent to the so-called Central condition [Van Erven et al., 2015], which provides control over a martingale-type quantity that captures the second-order part of the bounds (Lemma 8). The rates we obtain include the slow worst-case \u221a T regime for \u03ba = 0 and the fastest (doubly) logarithmic regime for \u03ba = 1. The next section introduces the two settings we consider and the individual sequence guarantees we will use in each. It also reviews the stochastic criteria for fast rates and presents our main result. In Section 3 we consider a variety of examples illustrating the breadth of cases that we cover. In Section 4 we prove that second-order guarantees imply adaptation to Bernstein conditions."}, {"heading": "2 Setup", "text": ""}, {"heading": "2.1 Hedge Setting", "text": "We start with arguably the simplest setting of online prediction, the Hedge setting popularized by Freund and Schapire [1997]. To be able to illustrate the full reach of our stochastic assumption we will use a minor extension to countably infinitely many actions k \u2208 N = {1,2, . . .}, customarily called experts. The protocol is as follows. Each round t the learner plays a probability mass function wt = (w1t ,w2t , . . .) on experts. Then the environment reveals the losses \u2113t = (\u21131t , \u21132t , . . .) of the experts, where each \u2113kt \u2208 [0,1]. The learner incurs loss\u27e8wt, \u2113t\u27e9 =\u2211k wkt \u2113kt . The regret after T rounds compared to expert k is given by\nRkT \u2236= T\u2211 t=1 (\u27e8wt, \u2113t\u27e9 \u2212 \u2113kt ) . The goal of the learner is to keep the regret small compared to any expert k. We will make use of Squint by Koolen and Van Erven [2015], a self-tuning algorithm for playing wt. Koolen and Van Erven [2015, Theorem 4] show that Squint with prior probability mass function \u03c0 = (\u03c01, \u03c02, . . .) guarantees RkT \u2264 \u221a V kT K k T +KkT where KkT = O(\u2212 ln\u03c0k + ln lnT ) for any expert k.\n(1)\nHere V kT \u2236= \u2211Tt=1 (\u27e8wt, \u2113t\u27e9 \u2212 \u2113kt )2 is a second-order term that depends on the algorithm\u2019s own predictions wt. It is well-known that with K experts the worstcase lower bound is \u0398(\u221aT lnK) [Cesa-Bianchi and Lugosi, 2006, Theorem 3.7]. Taking a fat-tailed prior, for example \u03c0k = 1\nk(k+1) , and using V k T \u2264 T , the above bound implies RkT \u2264 O (\u221aT (lnk + ln lnT )), matching the lower bound in some sense for all k simultaneously.\nThe question we study in this paper is what becomes of the regret when the sequence of losses \u21131, \u21132, . . . is drawn from some distribution P, not necessarily i.i.d. But before we expand on such stochastic cases, let us first introduce another setting."}, {"heading": "2.2 Online Convex Optimization (OCO)", "text": "We now turn to our second setting called online convex optimization [Shalev-Shwartz, 2011]. Here the set of actions is a compact convex set U \u2286 Rd. Each round t the learner plays a point wt \u2208 U . Then the environment reveals a convex loss function \u2113t \u2236 U \u2192 R. The loss of the learner is \u2113t(wt). The regret after T rounds compared to u \u2208 U is given by\nRuT \u2236= T\u2211 t=1 (\u2113t(wt) \u2212 \u2113t(u)) . The goal is small regret compared to any point u \u2208 U . A common tool in the analysis of algorithms is the linear upper bound on the regret obtained from\nconvexity of \u2113t (at non-differentiable points we may take any sub-gradient)\nRuT \u2264 R\u0303 u T \u2236= T\u2211 t=1 \u27e8wt \u2212 u,\u2207\u2113t(wt)\u27e9.\nWewill make use of (the full matrix version of)MetaGrad by Koolen and Van Erven [2016]. In their Theorem 8, they show that, simultaneously, R\u0303uT \u2264 O (DG\u221aT) and\nR\u0303uT \u2264 \u221a V uT KT +DGKT where KT = O(d lnT ) for any u \u2208 U , (2) where D bounds the two-norm diameter of U , G bounds \u2225\u2207\u2113t(wt)\u22252 the twonorm of the gradients and V uT \u2236= \u2211Tt=1\u27e8wt\u2212u,\u2207\u2113t(wt)\u27e92. The first bound matches the worst-case lower bound. The second bound (2) may be a factor \u221a KT worse, as V uT \u2264 G 2D2T by Cauchy-Schwarz. Yet in this paper we will show fast rates in certain stochastic settings arising from (2). To simplify notation we will assume from now on that DG = 1 (this can always be achieved by scaling the loss).\nTo talk about stochastic settings we will assume that the sequence \u2113t of loss functions (and hence the gradients \u2207\u2113t(wt)) are drawn from a distribution P, not necessarily i.i.d. This includes the common case of linear regression and classification where \u2113t(u) = loss(\u27e8u,xt\u27e9, yt) with (xt, yt) sampled i.i.d. from some distribution and loss a fixed one-dimensional convex loss function (e.g. square loss, absolute loss, log loss, hinge loss, . . . )."}, {"heading": "2.3 Parameterized Family of Stochastic Assumptions", "text": "We now recall the Bernstein [Bartlett and Mendelson, 2006] and Central [Van Erven et al., 2015] stochastic conditions. In both cases the idea behind the assumption is to control the variance of the excess loss of the actions in the neighborhood of the best action.\nWe do not require that the losses are i.i.d., nor that the Bayes act is in the model. For the Hedge setting it suffices if there is a fixed expert k\u2217 that is always best, i.e. E [\u2113k\u2217t \u2223Gt\u22121] = infk E [\u2113kt \u2223Gt\u22121] almost surely for all t. (Here we denote by Gt\u22121 the sigma algebra generated by \u21131, . . . , \u2113t\u22121, and the almost surely quantification refers to the distribution of \u21131, . . . , \u2113t\u22121.) Similarly, for OCO we assume there is a fixed point u\u2217 \u2208 U attaining minu\u2208U E [\u2113t(u)\u2223Gt\u22121] at every round t. In either case there may be multiple candidate k\u2217 or u\u2217. In the succeeding we assume that one is selected. Note that for i.i.d. losses the existence of a minimiser is not such a strong assumption (it is even automatic in the OCO case due to compactness of U), while it is very strong beyond i.i.d. Yet it is not impossible (and actually interesting) as we will show by example in Section 3.\nBased on this loss minimiser, we define the excess losses, a family of random variables indexed by time t \u2208 N and expert/point k \u2208 N/u \u2208 U as follows\nxkt \u2236= \u2113kt \u2212 \u2113k\u2217t (Hedge) and xut \u2236= \u27e8u \u2212 u\u2217,\u2207\u2113t(u)\u27e9 (OCO). (3)\nNote that for the Hedge setting we work with the loss directly. For OCO instead we talk about the linear upper bound on the loss, for this is the quantity that needs to be controlled to make use of the MetaGrad bound (2). With these variables in place, from this point on the story is the same for Hedge and for OCO. So let us write F for either the set N of experts or the set U of points, and f\u2217 for k\u2217 resp. u\u2217, and let us consider the family {xft \u2223 f \u2208 F , t \u2208 N}. We call f \u2208 F predictors. The point of these stochastic conditions is that they imply that the variance in the excess loss gets smaller the closer a predictor gets to the optimum in terms of expected excess loss. This is most directly seen in the Bernstein condition: Condition 1. Fix B \u2265 0 and \u03ba \u2208 [0,1]. The family (3) satisfies the (B,\u03ba)Bernstein condition if E [(xft )2\u2223Gt\u22121] \u2264 B E [xft \u2223Gt\u22121]\u03ba almost surely for all f \u2208 F and rounds t \u2208 N. Some authors refer to the \u03ba = 1 case as the Massart condition. As shown by Van Erven et al. [2015, Theorem 5.4], for bounded excess losses (which we assume throughout), the Bernstein condition is equivalent to the following condition: Condition 2. Fix a function \u01eb \u2236 R+ \u2192 R+. The family (3) satisfies the \u01eb-central condition if\n1 \u03b7 lnE [e\u2212\u03b7xft \u2223Gt\u22121] \u2264 \u01eb(\u03b7) almost surely for all f \u2208 F , \u03b7 \u2265 0 and t \u2208 N.\nVan Erven et al. explicitly convert back and forth between the parameters of the Bernstein and Central Condition. In the remainder we will use that Bernstein implies Central (with \u01eb(\u03b7) = O((B\u03b7) 11\u2212\u03ba )). For completeness we include a proof in Appendix B."}, {"heading": "2.4 Main Result", "text": "In the stochastic case we evaluate the performance of algorithms by Rf \u2217\nT , i.e. the regret compared to the predictor f\u2217 with minimal expected loss. The expectation E[Rf\u2217T ] is sometimes called the pseudo-regret. The following result shows that second-order methods automatically adapt to the Bernstein condition. (Proof sketch in Section 4.) Theorem 3. In any stochastic setting satisfying the (B,\u03ba)-Bernstein Condition 1, the guarantees (1) for Squint and (2) for MetaGrad imply fast rates for the respective algorithms both in expectation and with high probability. That is,\nE[Rf\u2217T ] = O (K 12\u2212\u03baT T 1\u2212\u03ba2\u2212\u03ba ) , and for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4,\nR f\u2217 T = O ((KT \u2212 ln \u03b4) 12\u2212\u03baT 1\u2212\u03ba2\u2212\u03ba ) ,\nwhere for Squint KT \u2236=Kf\u2217T from (1) and for MetaGrad KT is as in (2). We see that Squint and MetaGrad (and any other second-order methods achieving the same bounds, as our results only use these bounds and do not depend on the details of the algorithms) adapt automatically to the Bernstein parameters of the distribution, without any tuning. Appendix F provides an extension of Theorem 3 that allows using Squint with uncountable F .\nCrucially, the bound provided by Theorem 3 is natural, and, in general, the best one can expect. This can be seen from considering the statistical learning setting, which is a special case of our setup. Here (xt, yt) are i.i.d. \u223c P and F is a set of functions from X to a set of predictions A, with \u2113ft \u2236= \u2113(yt, f(xt)) for some loss function \u2113 \u2236 Y \u00d7 A \u2192 [0,1] such as squared, 0/1, or absolute loss. In this setting one usually considers excess risk, which is the expected loss difference between the learned f\u0302 and the optimal f\u2217. The minimax expected (over training sample (xt, yt)) risk relative to f\u2217 is of order T \u22121/2 (see e.g. Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions.\nIf F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 12\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf\u2217T ] of order O ((logT ) \u22c5 T 1\u2212\u03ba2\u2212\u03ba ). Theorem 3 shows that this is, indeed, also the rate that Squint attains in such cases if F is countable and the optimal f\u2217 has positive prior mass \u03c0f \u2217\n(more on this condition below)\u2014 we thus see that Squint obtains exactly the rates one would expect from a statistical learning/classification perspective, and the minimax excess risk results in that setting suggests that these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert [2004] that, when equipped with an oracle to tune the learning rate \u03b7 as a function of t, the rates O ((logT ) \u22c5 T 1\u2212\u03ba2\u2212\u03ba ) can also be achieved by Hedge, but the exact tuning depends on the unknown \u03ba. Gr\u00fcnwald [2012] provides a means to tune \u03b7 automatically in terms of the data, but his method \u2014 like ERM and all algorithms in the references above \u2014 may achieve linear regret in worst-case settings, whereas Squint keeps the O(\u221aT ) guarantee for such cases.\nTheorem 3 only gives the desired rate for Squint if F is countable and \u03c0f\u2217 > 0. The combination of these two assumptions is strong or at least unnatural, and OCO cannot be readily used in all such cases either, so in Appendix F we therefore show how to extend Theorem 3 to the case of infinite F , which can be continuous and thus have \u03c0f \u2217\n= 0, as long as F admits sufficiently small entropy numbers. Incidentally, this also allows us to show that Squint achieves regret rate O ((logT ) \u22c5 T 1\u2212\u03ba2\u2212\u03ba ) when F = \u22c3i=1,2,...Fi is a countably infinite union of Fi with appropriate entropy numbers; in such cases there can be, at every sample size, a classifier f\u0302 \u2208 F with 0 empirical error, so that ERM/FTL will always overfit and cannot be used even if the Bernstein condition holds; Squint allows for aggregation of such models. In the remainder of the main text, we concentrate on applications for which Theorem 3 can be used directly, without extensions."}, {"heading": "3 Examples", "text": "We give examples motivating and illustrating the Bernstein/Central condition for the Hedge and OCO settings. Our examples in the Hedge setting will illustrate Bernstein with \u03ba < 1 and non i.i.d. distributions. Our OCO examples were chosen to be natural and illustrate fast rates without curvature."}, {"heading": "3.1 Hedge Setting: Gap implies Bernstein with \u03ba = 1", "text": "In the Hedge setting, we say that a distribution P (not necessarily i.i.d.) of expert losses {\u2113kt \u2223 t, k \u2208 N} has gap \u03b1 > 0 if there is an expert k\u2217 such that\nE [\u2113k\u2217t \u2223Gt\u22121] + \u03b1 \u2264 inf k\u2260k\u2217 E [\u2113kt \u2223Gt\u22121] almost surely for each round \u2208 N. It is clear that the condition can only hold for k\u2217 the minimiser of the expected loss. Lemma 4. A distribution with gap \u03b1 is ( 1 \u03b1 ,1)-Bernstein. Proof. For all k, t we have E [(xkt )2\u2223Gt\u22121] \u2264 1 = 1\u03b1\u03b1 \u2264 1\u03b1 E [xkt \u2223Gt\u22121] . By Theorem 3 we get the Rk \u2217\nT = O(KT ) = O(ln lnT ) rate. Gaillard et al. [2014] show constant regret for finitely many experts and i.i.d. losses with a gap. Our alternative proof above shows that neither finiteness nor i.i.d. are essential for fast rates in this case.\n3.2 Hedge Setting: Any (1, \u03ba)-Bernstein\nThe next example illustrates that we can sometimes get the fast rates without a gap. And it also shows that we can get any intermediate rate: we construct an example satisfying the Bernstein condition for any \u03ba \u2208 [0,1] of our choosing\n(such examples occur naturally in classiciation settings such as those consider in the example in Appendix F).\nFix \u03ba \u2208 [0,1]. Each expert k is parametrised by a real number \u03b4k \u2208 [0,1/2]. The only assumption we make is that \u03b4k = 0 for some k, and infk{\u03b4k \u2223 \u03b4k > 0} = 0. For a concrete example let us choose \u03b41 = 0 and \u03b4k = 1/k. Expert \u03b4 has loss 1/2\u00b1\u03b4 with probability 1\u00b1\u03b4 2/\u03ba\u22121\n2 independently between experts and rounds. Expert \u03b4\nhas mean loss 1 2 + \u03b42/\u03ba, and so \u03b4 = 0 is best, with loss deterministically equal\nto 1/2. The squared excess loss of \u03b4 is \u03b42. So we have the Bernstein condition with exponent \u03ba (but no \u03ba\u2032 > \u03ba) and constant 1, and the associated regret rate by Theorem 3.\nNote that for \u03ba = 0 (the hard case) all experts have mean loss equal to 1 2 . So\nno matter which k\u2217 we designate as the best expert our expected regret is zero. Yet the experts do not agree, as their losses deviate from 1\n2 independently at\nrandom. Hence, by the central limit theorem, with high probability our regret is of order \u221a T . On the other side of the spectrum, for \u03ba = 1 (the best case), we do not find a gap. We still have experts arbitrary close to the best expert in mean, but their expected excess loss squared equals their expected excess loss.\nERM/FTL may fail miserably on this type of examples. The clearest case is when {k \u2223 \u03b4k > \u01eb} is infinite for some \u01eb > 0. Then at any t there will be experts that, by chance, incurred their lower loss every round. Picking any of them will result in expected instantaneous regret at least \u01eb2/\u03ba, leading to linear regret overall.\nThe requirement \u03b4k = 0 for some k is essential. If instead \u03b4k > 0 for all k then there is no best expert in the class. Theorem 13 in Appendix F shows how to deal with this case."}, {"heading": "3.3 Hedge Setting: Markov Chains", "text": "Suppose we model a binary sequence z1, z2, . . . , zT with m-th order Markov chains. As experts we consider all possible functions f \u2236{0,1}m \u2192 {0,1} that map a history of length m to a prediction for the next outcome, and the loss of expert f is the 0/1-loss: \u2113ft = \u2223f(zt\u2212m, . . . , zt\u22121) \u2212 zt\u2223. (We initialize z1\u2212m = . . . = z0 = 0.) A uniform prior on this finite set of 22 m\nexperts results in worst-case regret of order \u221a T 2m. Then, if the data are actually generated by an m-th order Markov chain with transition probabilities P(zt = 1 \u2223 (zt\u2212m, . . . , zt\u22121) = a) = pa, we have f\u2217(a) = 1{pa \u2265 12} and\nE [(xft )2\u2223(zt\u2212m, . . . , zt\u22121) = a] = 1, E [xft \u2223(zt\u2212m, . . . , zt\u22121) = a] = 2\u2223pa \u2212 12 \u2223 for any f such that f(a) \u2260 f\u2217(a). So the Bernstein condition holds with \u03ba = 1 and B = 1\n2mina \u2223pa\u22121 2 \u2223 ."}, {"heading": "3.4 OCO: Hinge Loss on the Unit Ball", "text": "Let (x1, y1), (x2, y2), . . . be classification data, with yt \u2208 {\u22121,+1}, and consider the hinge loss \u2113t(u) = max{0,1 \u2212 yt\u27e8xt, u\u27e9}. Now suppose, for simplicity, that\nboth xt and u come from the d-dimensional unit Euclidean ball, such that\u27e8xt, u\u27e9 \u2208 [\u22121,+1] and the hinge is never active, i.e. \u2113t(u) = 1 \u2212 yt\u27e8xt, u\u27e9. Then, if the data turn out to be i.i.d. observations from a fixed distribution P, the Bernstein condition holds with \u03ba = 1 (The proof can be found in Appendix D):\nLemma 5 (Unregularized Hinge Loss Example). Consider the hinge loss setting above, where \u2223\u27e8xt, u\u27e9\u2223 \u2264 1. If the data are i.i.d., then the (B,\u03ba)-Bernstein condition is satisfied with \u03ba = 1 and B = 2\u03bbmax\u2225\u00b5\u2225 , where \u03bbmax is the maximum\neigenvalue of E [XX\u22ba] and \u00b5 = E[YX], provided that \u2225\u00b5\u2225 > 0. In particular, if Xt is uniformly distributed on the sphere and Yt = sign(\u27e8u\u0304,Xt\u27e9) is the noiseless classification of Xt according to the hyperplane with normal vector u\u0304, then B \u2264 c\u221a\nd for some absolute constant c > 0.\nThe excluded case \u2225\u00b5\u2225 = 0 only happens in the degenerate case that there is nothing to learn, because \u00b5 = 0 implies that the expected hinge loss is 1, its maximal value, for all u."}, {"heading": "3.5 OCO: Absolute Loss", "text": "Let U = [0,1] be the unit interval. Consider \u2113t(u) = \u2223u\u2212 xt\u2223 where xt \u2208 [0,1] are drawn i.i.d. from P. Let u\u2217 \u2208 argminuE\u2223u \u2212 x\u2223 minimize the expected loss. In this case we may simplify \u27e8w \u2212u\u2217,\u2207\u2113(w)\u27e9 = (w \u2212u\u2217) sign(w \u2212x). To satisfy the Bernstein condition, we therefore want B such that, for all w \u2208 [0,1],\nE [((w \u2212 u\u2217) sign(w \u2212 x))2] \u2264 BE [(w \u2212 u\u2217) sign(w \u2212 x)]\u03ba . That is, \u2223w \u2212 u\u2217\u22232\u2212\u03ba \u2264 B2\u03ba\u2223P(x \u2264 w) \u2212 1 2 \u2223\u03ba. For instance, if the distribution of x has a strictly positive density p(x) \u2265m > 0, then u\u2217 is the median and \u2223P(x \u2264 w) \u2212 1 2 \u2223 = \u2223P(x \u2264 w) \u2212 P(x \u2264 u\u2217)\u2223 \u2265 m\u2223w \u2212 u\u2217\u2223, so the condition holds with \u03ba = 1 and B = 1 2m\n. Alternatively, for a discrete distribution on two points a and b with probabilities p and 1 \u2212 p, the condition holds with \u03ba = 1 and B = 1\u22232p\u22121\u2223 , provided that p \u2260 1\n2 , as can be seen by bounding\u2223w \u2212 u\u2217\u2223 \u2264 1 and \u2223P(x \u2264 w) \u2212 1 2 \u2223 \u2265 \u2223p \u2212 1 2 \u2223."}, {"heading": "4 Proof of Main Result", "text": "This section builds up to prove our main result Theorem 3. We first introduce a handy abbreviation that allows us to reason simultaneously in expectation and with high probability. We then identify the minimal \u01eb for which the Central Condition 2 holds. We then show how we can introduce a second-order adjustment, and characterize the cost. Combination with either worst-case regret bound then yields the desired result."}, {"heading": "4.1 Notation: Exponential Stochastic Negativity and Inequality", "text": "We introduce a convenient shorthand notation that we will use throughout this paper.\nDefinition 6. A random variable X is exponentially stochastically negative, denoted X \u22b4 0, if E[eX] \u2264 1. For any \u03b7 \u2265 0, we write X \u22b4\u03b7 0 if \u03b7X \u22b4 0. For any pair of random variables X and Y , we say that X is exponentially stochastically less than Y , denoted X \u22b4 Y , if X \u2212 Y \u22b4 0.\nLemma 7. Exponential stochastic negativity has the following useful properties:\n1. (Negativity). Let X \u22b4 0. As the notation suggests X is negative in expectation and with high probability. That is E [X] \u2264 0 and P{X \u2265 \u2212 ln \u03b4} \u2264 \u03b4 for all \u03b4 > 0. 2. (Convex combination). Let {Xf} f\u2208F be a family of random variables and\nlet w be a distribution on F . If Xf \u22b4 0 for all f then Ef\u223cw[Xf ] \u22b4 0. 3. (Chain rule). Let X1,X2, . . . be adapted to filtration G1 \u2286 G2 . . . (i.e. Xt\nis Gt-measurable for each t). If Xt\u2223Gt\u22121 \u22b4 0 almost surely for all t, then\u2211Tt=1Xt \u22b4 0 for all T \u2265 0. Proof. Negativity: By Jensen\u2019s inequality E [X] \u2264 lnE [eX] \u2264 0, whereas by Markov\u2019s inequality P{X \u2265 \u2212 ln \u03b4} = P{eX \u2265 1 \u03b4 } \u2264 \u03b4E [eX] \u2264 \u03b4. Convex combination: By Jensen\u2019s inequality E [eEf\u223cw[Xf ]] \u2264 Ef\u223cw E [eXf ] \u2264 1. Chain rule: By induction. The base case T = 0 holds trivially. For T > 0 we have E [e\u2211Tt=1 Xt] = E [e\u2211T\u22121t=1 Xt E [eXT \u2223GT\u22121]] \u2264 E [e\u2211T\u22121t=1 Xt] \u2264 1."}, {"heading": "4.2 Normalized Cumulant Generating Function", "text": "To prove our main result we will make use of the Central Condition 2. For any distribution P this condition will hold for some \u01eb (which may be trivial). In this section we construct the smallest \u01eb for which it holds and derive a few useful properties of that \u01eb.\nConsider the family (3) of excess loss variables xft . We assume that x f t \u2208 [\u22121,1]\nis bounded in a range of width 2 and has positive mean E[xft \u2223Gt\u22121] \u2265 0 by definition of f\u2217. As we will see, the complexity of our learning problem will be governed by the distribution of xft . In particular, we will look at the normalized cumulant generating function for \u03b7 \u2265 0:\n\u01eb f t (\u03b7) \u2236= 1\u03b7 lnE [e\u2212\u03b7xft \u2223Gt\u22121]\nBy construction \u2212xft \u22b4\u03b7 \u01eb f t (\u03b7). Boundedness of xft \u2208 [\u22121,1] immediately results\nin \u01ebft (\u03b7) \u2208 [\u22121,1]. Moreover, Hoeffding\u2019s inequality (see e.g. Cesa-Bianchi and Lugosi\n[2006, Lemma 2.2]) tells us that \u01ebft (\u03b7) \u2264 \u03b7/2 while Jensen\u2019s inequality gives \u01eb f t (\u03b7) \u2265 \u2212E [xft \u2223Gt\u22121]. The dual representation \u01ebft (\u03b7) = supQ \u2212EQ [x]\u2212 1\u03b7 KL(Q(x)\u2225P(xft \u2223Gt\u22121)) reveals that \u01ebft (\u03b7) is increasing in \u03b7. The value at \u03b7 = 0 is obtained by continuity from \u01ebft (0) \u2236= lim\u03b7\u21920 \u01ebft (\u03b7) = \u2212E[xft \u2223Gt\u22121] \u2264 0.\nTo get a uniform control over the class F , we will make use of the maximum \u01ebt(\u03b7) \u2236= sup\nf\u2208F \u01eb f t (\u03b7) and \u01eb(\u03b7) \u2236= sup t \u01ebt(\u03b7). (4)\nThe functions \u01ebt and \u01eb inherit most properties of each \u01eb f t , but in addition since f\u2217 \u2208 F and \u01ebf\u2217t (\u03b7) = 0, we see that \u01ebt(\u03b7) \u2265 0 and also that \u01ebt(0) = 0. Moreover, since \u01ebt(\u03b7) \u2264 \u03b7/2 we have lim\u03b7\u21920 \u01ebt(\u03b7) = 0. In this paper we will judge the complexity of the interplay of the generating distribution P with the class F by how \u01ebt(\u03b7) \u2192 0 as \u03b7 \u2192 0. By construction the Central Condition 2 holds with \u01eb(\u03b7)."}, {"heading": "4.3 A Second-order Adjustment to Exponential Stochastic Inequality", "text": "We now show a technical lemma showing that, roughly, the square of a bounded Central random variable is exponentially stochastically less than that variable itself. Consider any random variable x \u2208 [\u22121,1], and let us denote its normalized cumulant generating function by \u01eb(\u03b7) = 1 \u03b7 lnE [e\u2212\u03b7x]. (In particular, see\nDefinition 6, \u2212x \u22b4\u03b7 \u01eb(\u03b7) for all \u03b7 \u2265 0.) Intuitively, small \u01eb(\u03b7) \u226a \u03b7/2 is special, indicating that x cannot be often very negative. The following lemma shows that if x is special to degree \u01eb(\u03b7), then the smaller quantity \u2248 x \u2212 \u03b7\n2 x2 is also\nspecial at only mildly weaker degree \u2248 \u01eb(2\u03b7). Lemma 8. For any random variable x \u2208 [\u22121,1] and any \u03b7 \u2265 0\n1 \u03b7 lnE [ec\u03b72x2\u2212\u03b7x] \u2264 \u01eb(2\u03b7) + c\u03b7\u01eb(2\u03b7)2 where c = 1 1 + \u221a 1 + 4\u03b72 .\nIn the notation of Section 4.1, the lemma reads\n\u2212x \u22b4\u03b7 \u01eb(\u03b7) for all \u03b7 \u2265 0 implies c\u03b7x2\u2212x \u22b4\u03b7 \u01eb(2\u03b7)+c\u03b7\u01eb(2\u03b7)2 for all \u03b7 \u2265 0. Proof. By Theorem 12 in Appendix A with \u03b3 = 2\u03b7 and the largest admissible c for (5).\nNote that for \u03b7 = 0 the lemma trivializes, telling us \u2212E [x] \u2264 \u01eb(0) where we have in fact equality. Note also that the right-hand side is an increasing function in \u01eb(2\u03b7) (the quadratic in \u01eb(2\u03b7) has positive derivative for all \u01eb(2\u03b7) \u2265 \u2212 1+ \u221a\n4\u03b72+1 2\u03b7 < \u22121.)"}, {"heading": "4.4 From Second-order Bound to Bound in Terms of Parameter of Distribution", "text": "The next step toward fast rates is to obtain from a second-order bound, which involves the algorithm, another bound strictly in terms of the parameters of the distribution, which do not refer to the algorithm. The proof is in Appendix C.\nTheorem 9. Consider either Squint in the Hedge setting or MetaGrad for OCO. Let {xft \u2223f \u2208 F} be the associated the excess loss family from (3), and let \u01eb(\u03b7) be, as in (4), the corresponding maximal normalized cumulant generating function. For the Hedge setting let KT \u2236= K f\u2217\nT as in (1), for OCO let KT be as in (2). Then for each \u03b3 \u2265 0 with c as in Lemma 8,\nR f\u2217\nT \u22b4\u03b3\nKT c\u03b3 + T \u01eb(2\u03b3)(1+ c\u03b32) + 2KT .\nTo prove our main Theorem 3 we invoke the Bernstein Condition 1 to bound \u01eb(2\u01eb) as a polynomial in \u03b3, and then tune \u03b3 to optimize the bound. The details of the proof can be found in Appendix E."}, {"heading": "5 Conclusion", "text": "We show that it is possible for online learning methods to provide both the safety and robustness of a worst-case regret bound and be adaptive to favorable stochastic environments. We focus on Squint and MetaGrad, methods for online learning with individual sequence regret guarantees of a particular second order form. We show that such guarantees imply automatic adaptivity to the Bernstein parameters of stochastic environments, and result in the corresponding fast regret rates."}, {"heading": "A Second-order Adjustment of Exponential Stochas-", "text": "tic Inequality\nIn this section we prove a stronger form of Lemma 8. We would like to remark that our solution to this problem was inspired by the general moments problem studied by Mehta and Williamson [2014, Section 3], especially because this connection became invisible during the simplification of our proofs.\nWe will be thinking about two learning rates, 0 \u2264 \u03b7 \u2264 \u03b3. The larger one, \u03b3, will be where we evaluate \u01eb(\u03b3). So \u03b3 controls the strength of the assumption. The smaller one, \u03b7, will be the learning rate at which we obtain the conclusion. The point is to get a large amount of quadratic x2 in the conclusion, as governed by the constant c. Obviously, the more greedy we are in \u03b7 and \u03b3, the smaller the c for which we can get any traction. This trade-off is captured by the following relationship between \u03b3, \u03b7 and c that we will make use of throughout this section.\n0 \u2264 c \u2264\n\u221a 2\u22232\u03b7 \u2212 \u03b3\u2223 + \u03b32 + 1 \u2212 \u22232\u03b7 \u2212 \u03b3\u2223 \u2212 1\n4\u03b72 (5)\nPositivity of c is not that important, as the desired inequality is trivial for c \u2264 0. The following inequality is useful later.\nLemma 10. Let 0 \u2264 \u03b7 \u2264 \u03b3 and c satisfy (5). Then\n1 \u2265 2c\u03b7\nProof. We need to show\n2\u03b7 \u2265 \u221a 2\u22232\u03b7 \u2212 \u03b3\u2223 + \u03b32 + 1 \u2212 \u22232\u03b7 \u2212 \u03b3\u2223 \u2212 1\nthat is (2\u03b7 + \u22232\u03b7 \u2212 \u03b3\u2223 + 1)2 \u2265 2\u22232\u03b7 \u2212 \u03b3\u2223 + \u03b32 + 1 Expanding the left-hand side square results in 4\u03b72+4\u03b7\u22232\u03b7\u2212\u03b3\u2223+4\u03b7+\u22232\u03b7\u2212\u03b3\u22232+2\u22232\u03b7\u2212\u03b3\u2223+1 = 4\u03b7(2\u03b7\u2212\u03b3)+4\u03b7\u22232\u03b7\u2212\u03b3\u2223+4\u03b7+\u03b32+2\u22232\u03b7\u2212\u03b3\u2223+1 which definitely exceeds the right-hand side above.\nWe now put our assumption to use. In the following Lemma we show that it implies a not-in-expectation-but-with-a-correction-term version of the result we are after. Lemma 11. Fix 0 \u2264 \u03b7 \u2264 \u03b3 and let c satisfy (5). Then for each x \u2208 [\u22121,1] and \u01eb \u2208 [\u22121,1] we have\nec\u03b7 2 x 2\u2212\u03b7x \u2212 e\u2212\u03b3(x+\u01eb) \u2212 1 \u03b3 \u03b7(1 + 2c\u03b7\u01eb)ec\u03b72\u01eb2+\u03b7\u01eb \u2264 ec\u03b72\u01eb2+\u03b7\u01eb.\nProof. We will show that the left-hand side is maximized over x \u2208 [\u22121,1] at x = \u2212\u01eb. First, its derivative equals\ne\u2212\u03b3x\u03b7(h(\u2212\u01eb)\u2212 h(x)) where h(x) = (1 \u2212 2c\u03b7x)ec\u03b72x2+(\u03b3\u2212\u03b7)x. This indeed equals zero at x = \u2212\u01eb. To show that x = \u2212\u01eb is indeed a maximum and that there are no other maxima it suffices to show that h(x) is increasing on x \u2208 [\u22121,1]. We have\nh\u2032(x) = (\u22124c2\u03b73x2 + 2c\u03b7x(2\u03b7 \u2212 \u03b3) \u2212 2c\u03b7 + \u03b3 \u2212 \u03b7) ec\u03b72x2+(\u03b3\u2212\u03b7)x As the term in parentheses is concave in x, it suffices to show that h\u2032(x) \u2265 0 for x \u2208 {\u22121,1}, i.e. \u22124c2\u03b73 \u2212 2c\u03b7\u22232\u03b7 \u2212 \u03b3\u2223 \u2212 2c\u03b7 + \u03b3 \u2212 \u03b7 \u2265 0 Solving the quadratic in c, we see that this holds if (5), as required.\nFinally, we are ready for the general version of the claim.\nTheorem 12. Pick 0 \u2264 \u03b7 \u2264 \u03b3 and c satisfying (5). Let \u01eb \u2208 [\u22121,1]. Then for any x \u2208 [\u22121,1] with E e\u2212\u03b3x \u2264 e\u03b3\u01eb we have\nE ec\u03b7 2x2\u2212\u03b7x \u2264 ec\u03b7 2\u01eb2+\u03b7\u01eb.\nProof. Taking expectation over Lemma 11, we find\nE ec\u03b7 2x2\u2212\u03b7x \u2264 ec\u03b7 2\u01eb2+\u03b7\u01eb + E e\u2212\u03b3(x+\u01eb) \u2212 1 \u03b3 \u03b7(1 + 2c\u03b7\u01eb)ec\u03b72\u01eb2+\u03b7\u01eb,\nand the claim follows by bounding the right-most term by 0. (Note that the factor 1 + 2c\u03b7\u01eb is positive by Lemma 10.)"}, {"heading": "B Bernstein to Central", "text": "An indexed family of random variables {xf \u2223 f \u2208 F} satisfies the (B,\u03ba)-Bernstein condition if E [(xf)2] \u2264 BE [xf ]\u03ba for all f \u2208 F and it satisfies the \u01eb-Central condition if \u2212xf \u22b4\u03b7 \u01eb(\u03b7) for all f \u2208 F and \u03b7 \u2265 0 We now show that Bernstein implies Central. This is a special case of [Van Erven et al., 2015, Theorem 5.4, Part 1]. Assume Bernstein. Then by the Bernstein Sandwich [Koolen et al., 2014, Lemma C.2], simplifying e\u03b7 \u2212 \u03b7 \u2212 1 \u2264 \u03b72, which holds for small enough \u03b7 \u2264 1.79328, and by the Bernstein assumption\n\u01ebf(\u03b7) = 1 \u03b7 lnE [e\u2212\u03b7xf ] \u2264 \u03b7E[(xf)2] \u2212E[xf ] \u2264 \u03b7B E[xf ]\u03ba \u2212E[xf ]\nThen (the maximizer is found at x = (B\u03b7\u03ba) 11\u2212\u03ba (which is \u2264 1 when B\u03ba\u03b7 \u2264 1, so for small enough \u03b7 this is a reasonable point))\n\u01eb(\u03b7) = sup f \u01ebf(\u03b7) \u2264 sup x \u03b7Bx\u03ba \u2212 x = 1 \u2212 \u03ba \u03ba (B\u03b7\u03ba) 11\u2212\u03ba ."}, {"heading": "C Proof of Theorem 9", "text": "Proof. For the Hedge setting, let us write xt \u2236= Ek\u223cwt[xkt ] for the excess loss of the learner in round t. Then xt \u2208 [\u22121,1] and \u2212xt \u22b4\u03b7 \u01eb(\u03b7) by Lemma 7. Now by definition of xkt in (3) and R k T and V k T (see Section 2.1)\nT\u2211 t=1 xt = T\u2211 t=1 (\u27e8wt, \u2113t\u27e9 \u2212 \u2113k\u2217t ) = Rk\u2217T and T\u2211 t=1 (xt)2 = T\u2211 t=1\n(\u27e8wt, \u2113t\u27e9 \u2212 \u2113k\u2217t )2 = V k\u2217T . For OCO, let us write xt \u2236= x wt t for the excess loss of the learner in round t.\nAgain xt \u2208 [\u22121,1] and we have \u2212xt \u22b4\u03b7 \u01eb(\u03b7) by construction of \u01eb(\u03b7). Moreover, from the definition of R\u0303uT and V u T from Section 2.2,\nT\u2211 t=1 xt = T\u2211 t=1 \u27e8wt \u2212 u\u2217,\u2207\u2113t(wt)\u27e9 = R\u0303u\u2217T and T\u2211 t=1 (xt)2 = T\u2211 t=1\n\u27e8wt \u2212 u\u2217,\u2207\u2113t(wt)\u27e92 = V u\u2217T . With this notation the second-order regret bounds (1) and (2) both state\nT\u2211 t=1 xt \u2264 \u00bf\u00c1\u00c1\u00c0( T\u2211 t=1\nx2t)KT +KT . (6) Now fix \u03b3 \u2265 0. For any t, as \u2212xt \u22b4\u03b7 \u01eb(\u03b7), Lemma 8 gives c\u03b3x2t \u2212 xt \u22b4\u03b3 \u01eb(2\u03b3)(1+ c\u03b32) By telescoping over rounds (using the chain rule Lemma 7), we obtain\nc\u03b3 T\u2211 t=1 x2t \u2212 T\u2211 t=1 xt \u22b4\u03b3 T \u01eb(2\u03b3)(1+ c\u03b32). (7) The individual sequence regret bound (6) gives us (since 2 \u221a ab = inf\u03b7 \u03b7a + b/\u03b7) for every \u03b7 \u2265 0 T\u2211 t=1 xt \u2264 \u03b7 2 T\u2211 t=1 x2t + KT 2\u03b7 +KT .\nPlugging in \u03b7 = c\u03b3 (this implies \u03b7 \u2208 [0,1/2] and \u03b3 = 2\u03b7 1\u22124\u03b72 ) and combination with (7) results in T\u2211 t=1 xt \u22b4\u03b3 KT c\u03b3 + T \u01eb(2\u03b3)(1+ c\u03b32) + 2KT .\nFor the Hedge setting this proves the theorem. For OCO we finish with Ru \u2217\nT \u2264\nR\u0303u \u2217\nT ."}, {"heading": "D Proof of Lemma 5", "text": "Proof. Since, by assumption, u and X have length at most 1, the hinge loss simplifies to \u2113(u) = 1\u2212Y \u27e8u,X\u27e9 with gradient \u2207\u2113(u) = \u2212YX. This implies that\nu\u2217 \u2236= argmin u E [\u2113(u)] = \u00b5\u2225\u00b5\u2225 , (8)\nand\n(w \u2212u\u2217)\u22baE [\u2207\u2113(w)\u2207\u2113(w)\u22ba] (w \u2212u\u2217) = (w \u2212u\u2217)\u22baE [XX\u22ba] (w \u2212u\u2217) \u2264 \u03bbmax(w \u2212u\u2217)\u22ba(w \u2212u\u2217) \u2264 2\u03bbmax(1 \u2212 \u27e8w,u\u2217\u27e9) = 2\u03bbmax\u2225\u00b5\u2225 (w \u2212u\u2217)\u22ba(\u2212\u00b5) = 2\u03bbmax\u2225\u00b5\u2225 (w \u2212u\u2217)\u22ba E [\u2207\u2113(w)] ,\nwhich proves the first part of the lemma For the second part, we first observe that \u03bbmax = 1/d. Then, to compute\u2225\u00b5\u2225, assume without loss of generality that \u2225u\u0304\u2225 = 1, in which case u\u0304 = u\u2217. Now symmetry of the distribution of X conditional on \u27e8X,u\u2217\u27e9 gives E [YX \u2223 \u27e8X,u\u2217\u27e9] = sign(\u27e8X,u\u2217\u27e9)E [X \u2223 \u27e8X,u\u2217\u27e9] = sign(\u27e8X,u\u2217\u27e9)\u27e8X,u\u2217\u27e9u\u2217 = \u2223\u27e8X,u\u2217\u27e9\u2223u\u2217.\nBy rotational symmetry, we may further assume without loss of generality that u\u2217 = e1 is the first unit vector in the standard basis, and therefore \u2225\u00b5\u2225 = \u2225E [\u2223\u27e8X,u\u2217\u27e9\u2223]u\u2217\u2225 = E [\u2223X1\u2223] . If Z = (Z1, . . . , Zd) is multivariate Gaussian N(0, I). Then X = Z/\u2225Z\u2225 is uniformly distributed on the sphere, so\nE[\u2223X1\u2223] = E [ \u2223Z1\u2223\u2225Z\u2225] \u2265 14\u221ad P(\u2223Z1\u2223 \u2265 12 \u2227 \u2225Z\u2225 \u2264 2 \u221a d) .\nSince P (\u2223Z1\u2223 < 12) \u2264 0.4 and P (\u2225Z\u2225 \u2265 2\u221ad) \u2264 14d E [\u2225Z\u22252] = 14 , we have P(\u2223Z1\u2223 \u2265 12 \u2227 \u2225Z\u2225 \u2264 2\u221ad) \u2265 1 \u2212 0.4 \u2212 14 = 0.35, from which the conclusion of the second part follows with c = 8/0.35."}, {"heading": "E Proof of Theorem 3", "text": "Proof. As pointed out below Condition 2, the (B,\u03ba)-Bernstein condition implies the central condition with \u01eb(\u03b7) \u2264 (\u03b7B) 11\u2212\u03ba . By Theorem 9, using 1/c \u2264 2(1+ \u03b32) and c \u2264 1\n2 , we find that for all \u03b3 \u2265 0\nR f\u2217 T \u22b4\u03b3 (1 + \u03b32)2KT\u03b3 + (1 + 12\u03b32)\u01eb(2\u03b3)T + 2KT . (9) By Lemma 7 this implies for all \u03b3 \u2265 0\nE[Rf\u2217T ] \u2264 (1 + \u03b32)2KT\u03b3 + (1 + 12\u03b32)\u01eb(2\u03b3)T + 2KT . It remains to tune \u03b3 to exploit the stochastic condition expressed by \u01eb. Reducing the above right-hand-side expression to its main terms and setting the derivative to zero suggests picking\n\u03b3\u0302 = (2KT (1 \u2212 \u03ba)(2B)\u2212 11\u2212\u03ba T ) 1\u2212\u03ba 2\u2212\u03ba\n= O ((KT /T ) 1\u2212\u03ba2\u2212\u03ba ) . Plugging in this tuning, we find\nE[Rf\u2217T ] \u2264 (2 \u2212 \u03ba) (4KTB) 12\u2212\u03ba (T /(1\u2212 \u03ba)) 1\u2212\u03ba2\u2212\u03ba + (5 \u2212 \u03ba)KT Finally, using that (2\u2212\u03ba) (4B) 12\u2212\u03ba (1/(1 \u2212 \u03ba)) 1\u2212\u03ba2\u2212\u03ba is maximized in \u03ba at \u03ba = 1\u2212 1\n4B\nwhere it takes value 1 + 4B, we may simplify this to\nE[Rf\u2217T ] \u2264 (1 + 4B) (KT /4) 12\u2212\u03baT 1\u2212\u03ba2\u2212\u03ba + (5 \u2212 \u03ba)KT = O (K 12\u2212\u03baT T 1\u2212\u03ba2\u2212\u03ba ) , which gives the first claim of the Theorem. Lemma 7 applied to (9) also implies that for all \u03b4 \u2265 0 with probability at least 1 \u2212 \u03b4\nR f\u2217 T \u2264 (1 + \u03b32)2KT\u03b3 + (1 + 12\u03b32)\u01eb(2\u03b3)T + 2KT + \u2212 ln \u03b4\u03b3 . Tuning \u03b3 as before with KT replaced by KT + \u2212 ln \u03b42 yields the second claim."}, {"heading": "F Continuous Models", "text": "We now consider Squint with models of predictors F that have uncountably many elements so that in general \u03c0f \u2217\n= 0, and each f \u2208 F is a function from X to A, with \u2113ft \u2236= \u2113(yt, f(xt)) for some fixed loss function \u2113 \u2236 Y \u00d7A \u2192 [0,1]. This setting includes standard parametric models in classification and regression but also countable unions thereof as well as nonparametric models. We first present an extension of Theorem 3 to this case; we then give an illustration of this result with sup-norm metric entropy numbers.\nSquint can be straightforwardly applied to uncountable models, but now the weight vector wt output by Squint at time t takes the form of a distribution on the set F . For general distributions u on F , the loss u incurs at time t is now defined as \u2113ut \u2236= Ef\u223cu[\u2113ft ], so that the loss of Squint at time t is given by \u2113wtt , which generalizes the expression \u27e8wt, \u2113t\u27e9 for the countable case. The regret of Squint relative to an arbitrary u is thus given by RuT = Ef\u223cu\u2211Tt=1(\u2113wtt \u2212 \u2113ft ), and the variance term in (1) generalizes to V uT = Ef\u223cu\u2211Tt=1 vft with vft = (\u2113wtt \u2212 \u2113ft )2.\nFor such models we will use that, as shown by Koolen [2015], Squint satisfies the following quantile or \u2018KL\u2019 bound:\nRuT \u2264 2 \u221a V uT K u T +K u T (10)\nwhich holds for every distribution u on F and prior \u03c0, where nowKuT = O(KL(u\u2225\u03c0)+ ln lnT ) and KL(u\u2225\u03c0) is the KL divergence between prior \u03c0 and the distribution u.\nNote that (10) generalizes the countable bound (1), which is retrieved if u is taken to be a point mass on k.\nTheorem 13 (Extension of Theorem 3). In any stochastic setting satisfying the (B,\u03ba)-Bernstein Condition 1, the guarantee (10) for Squint implies fast rates for Squint in expectation (if there is sufficient prior mass on f that behave similarly to f\u2217 in expectation) and with high probability (if there is sufficient prior mass on f taht are guaranteed to behave similarly to f\u2217 on all x). That is, for all T , for any sequence u1, u2, . . . of distributions on F and sequence of numbers C1,C2, . . . that satisfy\nE[ T\u2211 t=1 \u2113uTt ] \u2264 E [ T\u2211 t=1 \u2113 f\u2217 t ] +CT , (11) we have E[Rf\u2217T ] = O ((KT +CT ) 12\u2212\u03ba T 1\u2212\u03ba2\u2212\u03ba ) , and if (11) holds for every sequence (xT , yT ), then we also have for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4,\nR f\u2217 T = O ((KT +CT \u2212 ln \u03b4) 12\u2212\u03baT 1\u2212\u03ba2\u2212\u03ba ) where KT \u2236=K uT T from (10).\nWhile this theorem does allow us to use priors u with uncountable support, it is easiest to illustrate with priors with support on a discretized version (countable subset) of F which may assign probability 0 to f\u2217: Example 1. Consider the classification setting where J is either finite or N, and F = \u22c3j\u2208J Fj is a finite or countable union of sub-models such that for \u03b4 > 0, F\u0308j,\u03b4 \u2282 Fj is a minimal \u03b4-cover of F\u0308j in the \u2113\u221e-norm (that is, we require\nsupf\u2208Fj minf\u0307\u2208F\u0308j,\u03b4 supx\u2208X ,y\u2208Y \u2225\u2113(y, f(x)\u2212\u2113(y, f\u0307(x)\u2225 \u2264 \u03b4). Define \u0393 \u2236= {20,2\u22121, . . .}. Assume that for all j, N (Fj , \u03b4) \u2236= \u2223F\u0308j,\u03b4 \u2223 < \u221e and note that logN (Fj , \u03b4) is the metric entropy of Fj in the sup norm at scale \u03b4. Let \u03c0J be a probability mass function on J and let \u03c0N be a probability distribution on N with \u2212 log\u03c0J (j)\u03c0N(k) = O(log(jk)) and let \u03c0 be the prior on \u22c3j\u2208N,\u03b4\u2208\u0393 F\u0308j,\u03b4 with mass function \u03c0 given by, for f \u2208 F\u0308j,2\u2212k , \u03c0(f) = \u03c0J (j)\u03c0N(j)/N (Fj,2\u2212k)). Then Theorem 13 gives the following bound in expectation (and mutatis m utandis in probability):\nR f\u2217 T = O ((T 2\u2212k +min j,k log(jk) + logN (Fj ,2\u2212k)) 12\u2212\u03ba T 1\u2212\u03ba2\u2212\u03ba) . Bounds in terms of models with bounded \u2113\u221e-entropy numbers were considered before by, e.g. Gaillard and Gerchinovitz [2015] with bounded squared error loss. We note that, if F has logarithmic entropy numbers (e.g. F = F1 and logN (F1, \u01eb) = O(\u2212 log \u01eb), then, by plugging in k = \u2308log2 T \u2309, we find that this cumulative regret bound is of the form O((logT ) \u22c5T 1\u2212\u03ba2\u2212\u03ba ), the standard rate referred to in the discussion underneath Theorem 3. In the case of larger (polynomial) entropy numbers, our bounds are presumably suboptimal compared to the bounds that can be obtained by ERM, since Squint is essentially a form of an exponentially weighted forecaster that cannot exploit the chaining technique, viz. the discussion by Gaillard and Gerchinovitz [2015], Audibert [2009] and Rakhlin and Sridharan [2014]. Nevertheless, unlike ERM, Squint is robust and will continue to achieve nontrivial regret under nonstochastic, adversarially generated data, even with polynomial entropy numbers.\nIn practice, one may often work with Fi which have small (e.g. logarithmic) entropy numbers relative to the pseudo-distance d(f1, f2) = P(f1(X) \u2260 f2(X)) considered by e.g. Tsybakov [2004], Audibert [2004], which may be much smaller than the \u2113\u221e-numbers. In such cases, Theorem 13 can still be used to give good bounds in expectation.\nProof of Theorem 13 Consider a (for now) arbitrary sequence u1, u2, . . ., define KT \u2236=K uT T and K \u2032 T =KT /4, and\nC\u2032T = \u2212(RuTT \u2212Rf\u2217T ) or equivalently T\u2211 t=1 \u2113uTt = T\u2211 t=1 \u2113 f\u2217 t +C \u2032 T .\nOne easily shows that for general a, b, c \u2208 R, one has (a\u2212b)2/2 \u2264 (a\u2212c)2+(b\u2212c)2. Applying the statement with a = \u2113wtt , b = \u2113 f t and c = \u2113\nf\u2217t gives vft \u2264 2(vf\u2217t +(xft )2). Summing over t = 1..T and taking expectation over f \u223c uT now gives V uT T \u2264 2V f \u2217 T + 2ET where ET = Ef\u223cuT [\u2211Tt=1(xft )2].\nApplying this to the bound (10) above at uT , we get\nR f\u2217 T \u2264 C \u2032 T +2 \u221a(V f\u2217T +ET )2K \u2032T +K \u2032T = inf\u03b7 {C\u2032T + \u03b7(V f\u2217T +ET ) + 2K \u2032 T \u03b7 +K \u2032T} .\n(12)\nWe first prove an analogue to Theorem 9 for the uncountable setting, based on (12). As in that theorem, let, for given F , {xft \u2223f \u2208 F} be the associated the excess loss family from (3), and let \u01eb(\u03b7) be, as in (4), the corresponding maximal normalized cumulant generating function. Let K \u2032T be as above. Fix \u03b3 \u2265 0 and let c be as in Lemma 8. Now as in the proof of Theorem 9 we have for all f \u2208 F , x f t \u2208 [\u22121,1] and \u2212xft \u22b4\u03b7 \u01eb(\u03b7) by construction of \u01eb(\u03b7). Hence \u2212xft \u22b4\u03b3 \u01eb(\u03b3) for all\nf \u2208 F , which implies \u2212Ef\u223cwt xft \u22b4\u03b3 \u01eb(\u03b3) and also \u2212Ef\u223cuT xft \u22b4\u03b3 \u01eb(\u03b3) , and hence by Lemma 8 (see remark below the lemma),\nc\u03b3 E f\u223cwt [xft ]2 \u2212 E f\u223cwt [xft ] \u22b4\u03b3 \u01eb(2\u03b3)(1 + c\u03b32) and (13) c\u03b3 E\nf\u223cuT [xft ]2 \u2212 E f\u223cuT [xft ] \u22b4\u03b3 \u01eb(2\u03b3)(1 + c\u03b32). (14)\nUsing rf \u2217 t = Ef\u223cwt [xft ], again analogously to the proof of Theorem 9, we may telescope (13) over rounds to\nc\u03b3V f\u2217 T \u2212R f\u2217 T \u22b4\u03b3 T \u01eb(2\u03b3)(1+ c\u03b32) (15) Now we use (12) with \u03b7 = c\u03b3\n2 , which implies 2Rf\n\u2217 T \u2264 2C \u2032 T + c\u03b3(V f\u2217T + ET ) +\n4K \u2032T /(c\u03b3)+ 2K \u2032T . Combining this with (15), we find: U \u22b4 0 with U = \u03b3Rf \u2217 T \u2212 (2\u03b3C\u2032T + c\u03b32ET + 4K \u2032T c + \u03b32K \u2032T + \u03b3T \u01eb(2\u03b3)(1+ c\u03b32)) . (16) Similarly to deriving (15), using the definition of ET , we may telescope (14) over rounds to get\nU \u2032 \u22b4 0 with U \u2032 = c\u03b32ET \u2212 \u03b3C \u2032 T \u2212 \u03b3T \u01eb(2\u03b3)(1+ c\u03b32). (17)\nWe may now combine (16) and (17) using Lemma 7 with w a distribution that puts mass 1/2 on random variable U and 1/2 on U \u2032, to get (U +U \u2032)/2 \u22b4 0, which can be rewritten to:\n\u03b3\n2 \u239b\u239dRf \u2217\nT \u2212 (2C\u2032T + c\u03b3ET + 4K \u2032Tc\u03b3 + 2K \u2032T + T \u01eb(2\u03b3)(1+ c\u03b32)) + c\u03b3ET \u2212 \u03b3C \u2032 T \u2212 \u03b3\u01eb(2\u03b3)(1+ c\u03b32)\u239e\u23a0 \u22b4 0,\nand further to\n1 2 R f \u2217 T \u22b4\u03b3 3 2 C\u2032T + KT c\u03b3 + T \u01eb(2\u03b3)(1+ c\u03b32) + 2K \u2032T , (18)\nwhich is the required analogue of the statement of Theorem 9. Note that this statement holds for every sequence u1, . . . , uT , and C\u2032T is a random variable that depends on data (xT , yT ).\nThe remainder of the proof of Theorem 13 now follows in a fashion entirely analogous to the proof of Theorem 3, as in Appendix E, where we use that we can bound C\u2032T by CT , either in expectation or on all sequences; we omit further details where one uses (18) instead of the corresponding statement of Theorem 9; we omit further details."}], "references": [{"title": "PAC-Bayesian statistical learning theory", "author": ["Jean-Yves Audibert"], "venue": "PhD thesis, Universite\u0301 Paris VI,", "citeRegEx": "Audibert.,? \\Q2004\\E", "shortCiteRegEx": "Audibert.", "year": 2004}, {"title": "Fast learning rates in statistical inference through aggregation", "author": ["Jean-Yves Audibert"], "venue": "The Annals of Statistics,", "citeRegEx": "Audibert.,? \\Q2009\\E", "shortCiteRegEx": "Audibert.", "year": 2009}, {"title": "Empirical minimization", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Probability Theory and Related Fields,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2006}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["Nicol\u00f2 Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Adaptive regularization of weight vectors", "author": ["Koby Crammer", "Alex Kulesza", "Mark Dredze"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Crammer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Fast rates in statistical and online learning", "author": ["Tim van Erven", "Peter D. Gr\u00fcnwald", "Nishant A. Mehta", "Mark D. Reid", "Robert C. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Erven et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2015}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "A chaining algorithm for online nonparametric regression", "author": ["Pierre Gaillard", "S\u00e9bastien Gerchinovitz"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Gaillard and Gerchinovitz.,? \\Q2015\\E", "shortCiteRegEx": "Gaillard and Gerchinovitz.", "year": 2015}, {"title": "A second-order bound with excess losses", "author": ["Pierre Gaillard", "Gilles Stoltz", "Tim van Erven"], "venue": "In JMLR Workshop and Conference Proceedings,", "citeRegEx": "Gaillard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gaillard et al\\.", "year": 2014}, {"title": "The safe Bayesian: learning the learning rate via the mixability gap", "author": ["Peter Gr\u00fcnwald"], "venue": "In Proceedings 23rd International Conference on Algorithmic Learning Theory (ALT", "citeRegEx": "Gr\u00fcnwald.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2012}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Machine learning,", "citeRegEx": "Hazan and Kale.,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2010}, {"title": "Local Rademacher complexities and oracle inequalities in risk minimization", "author": ["Vladimir Koltchinskii"], "venue": "The Annals of Statistics,", "citeRegEx": "Koltchinskii.,? \\Q2006\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2006}, {"title": "The relative entropy bound for Squint", "author": ["Wouter M. Koolen"], "venue": "Blog entry on http://blog.wouterkoolen.info/,", "citeRegEx": "Koolen.,? \\Q2015\\E", "shortCiteRegEx": "Koolen.", "year": 2015}, {"title": "Second-order quantile methods for experts and combinatorial games", "author": ["Wouter M. Koolen", "Tim van Erven"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Koolen and Erven.,? \\Q2015\\E", "shortCiteRegEx": "Koolen and Erven.", "year": 2015}, {"title": "Metagrad: Faster convergence without curvature in online convex optimization", "author": ["Wouter M. Koolen", "Tim van Erven"], "venue": null, "citeRegEx": "Koolen and Erven.,? \\Q2016\\E", "shortCiteRegEx": "Koolen and Erven.", "year": 2016}, {"title": "Learning the learning rate for prediction with expert advice", "author": ["Wouter M. Koolen", "Tim van Erven", "Peter D. Gr\u00fcnwald"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koolen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2014}, {"title": "Achieving all with no parameters: Adaptive normalhedge", "author": ["Haipeng Luo", "Robert E. Schapire"], "venue": "In Proceedings of the 28th Conference on Learning Theory (COLT),", "citeRegEx": "Luo and Schapire.,? \\Q2015\\E", "shortCiteRegEx": "Luo and Schapire.", "year": 2015}, {"title": "Risk bounds for statistical learning", "author": ["Pascal Massart", "\u00c9lodie N\u00e9d\u00e9lec"], "venue": "The Annals of Statistics,", "citeRegEx": "Massart and N\u00e9d\u00e9lec.,? \\Q2006\\E", "shortCiteRegEx": "Massart and N\u00e9d\u00e9lec.", "year": 2006}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew J. Streeter"], "venue": "In Proceedings of the 23rd Conference on Learning Theory (COLT),", "citeRegEx": "McMahan and Streeter.,? \\Q2010\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2010}, {"title": "From stochastic mixability to fast rates", "author": ["Nishant A Mehta", "Robert C Williamson"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mehta and Williamson.,? \\Q2014\\E", "shortCiteRegEx": "Mehta and Williamson.", "year": 2014}, {"title": "Online nonparametric regression", "author": ["Alexander Rakhlin", "Karthik Sridharan"], "venue": "In Proceedings of the 27th Conference on Learning Theory (COLT),", "citeRegEx": "Rakhlin and Sridharan.,? \\Q2014\\E", "shortCiteRegEx": "Rakhlin and Sridharan.", "year": 2014}, {"title": "Follow the leader if you can, Hedge if you must", "author": ["Steven de Rooij", "Tim van Erven", "Peter D. Gr\u00fcnwald", "Wouter M. Koolen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rooij et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rooij et al\\.", "year": 2014}, {"title": "Exploiting easy data in online optimization", "author": ["Amir Sani", "Gergely Neu", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Adaptivity and optimism: An improved exponentiated gradient algorithm", "author": ["Jacob Steinhardt", "Percy Liang"], "venue": "In Proceedings of the 31th Annual International Conference on Machine Learning (ICML),", "citeRegEx": "Steinhardt and Liang.,? \\Q2014\\E", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2014}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["Alexandre B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Tsybakov.,? \\Q2004\\E", "shortCiteRegEx": "Tsybakov.", "year": 2004}, {"title": "Optimal learning with Bernstein Online Aggregation", "author": ["Olivier Wintenberger"], "venue": null, "citeRegEx": "Wintenberger.,? \\Q2015\\E", "shortCiteRegEx": "Wintenberger.", "year": 2015}, {"title": "Nevertheless, unlike ERM, Squint is robust and will continue to achieve nontrivial regret under nonstochastic, adversarially generated data, even with polynomial entropy numbers", "author": ["nique", "viz"], "venue": null, "citeRegEx": "nique and viz.,? \\Q2014\\E", "shortCiteRegEx": "nique and viz.", "year": 2014}], "referenceMentions": [{"referenceID": 26, "context": "settings, adversarial regret lower bounds of order \u221a T are known, along with matching individual sequence algorithms [Shalev-Shwartz, 2011].", "startOffset": 117, "endOffset": 139}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al.", "startOffset": 83, "endOffset": 110}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al.", "startOffset": 83, "endOffset": 134}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al.", "startOffset": 83, "endOffset": 158}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al.", "startOffset": 83, "endOffset": 178}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015].", "startOffset": 83, "endOffset": 200}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015].", "startOffset": 83, "endOffset": 229}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015].", "startOffset": 83, "endOffset": 254}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015]. Interestingly, the price for such adaptivity (i.", "startOffset": 83, "endOffset": 275}, {"referenceID": 4, "context": "For the Hedge setting, results of this type have been obtained, amongst others, by Cesa-Bianchi et al. [2007], De Rooij et al. [2014], Gaillard et al. [2014], Sani et al. [2014], Koolen et al. [2014], Koolen and Van Erven [2015], Luo and Schapire [2015], Wintenberger [2015]. Interestingly, the price for such adaptivity (i.e. the worsening of the worst-case regret bound) is typically extremely small (i.e. a constant factor in the regret bound). For online convex optimization (OCO), many different types of adaptivity have been explored, including by [Crammer et al., 2009, Duchi et al., 2011, McMahan and Streeter, 2010, Hazan and Kale, 2010, Chiang et al., 2012, Steinhardt and Liang, 2014, Koolen and Van Erven, 2016]. Here we are interested in the question of whether such adaptive results are strong enough to lead to improved rates in the stochastic case when the data follow a \u201cfriendly\u201d distribution. In specific cases it has been shown that fancy guarantees do imply significantly reduced regret. For example Gaillard et al. [2014] present a generic argument showing that a certain kind of second-order regret guarantees implies constant expected regret (the fastest possible rate) for i.", "startOffset": 83, "endOffset": 1044}, {"referenceID": 9, "context": "1 Hedge Setting We start with arguably the simplest setting of online prediction, the Hedge setting popularized by Freund and Schapire [1997]. To be able to illustrate the full reach of our stochastic assumption we will use a minor extension to countably infinitely many actions k \u2208 N = {1,2, .", "startOffset": 115, "endOffset": 142}, {"referenceID": 14, "context": "We will make use of Squint by Koolen and Van Erven [2015], a self-tuning algorithm for playing wt.", "startOffset": 30, "endOffset": 58}, {"referenceID": 26, "context": "2 Online Convex Optimization (OCO) We now turn to our second setting called online convex optimization [Shalev-Shwartz, 2011].", "startOffset": 103, "endOffset": 125}, {"referenceID": 15, "context": "Wewill make use of (the full matrix version of)MetaGrad by Koolen and Van Erven [2016]. In their Theorem 8, they show that, simultaneously, R\u0303 T \u2264 O (DG\u221aT) and", "startOffset": 59, "endOffset": 87}, {"referenceID": 2, "context": "3 Parameterized Family of Stochastic Assumptions We now recall the Bernstein [Bartlett and Mendelson, 2006] and Central [Van Erven et al.", "startOffset": 77, "endOffset": 107}, {"referenceID": 13, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]).", "startOffset": 0, "endOffset": 27}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]).", "startOffset": 28, "endOffset": 44}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.", "startOffset": 28, "endOffset": 232}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.", "startOffset": 28, "endOffset": 263}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.", "startOffset": 28, "endOffset": 283}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al.", "startOffset": 28, "endOffset": 302}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions.", "startOffset": 28, "endOffset": 331}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.", "startOffset": 28, "endOffset": 803}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.", "startOffset": 28, "endOffset": 831}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.", "startOffset": 28, "endOffset": 848}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al.", "startOffset": 28, "endOffset": 1024}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf T ] of order O ((logT ) \u22c5 T 1\u2212\u03ba 2\u2212\u03ba ).", "startOffset": 28, "endOffset": 1071}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf T ] of order O ((logT ) \u22c5 T 1\u2212\u03ba 2\u2212\u03ba ). Theorem 3 shows that this is, indeed, also the rate that Squint attains in such cases if F is countable and the optimal f\u2217 has positive prior mass \u03c0 \u2217 (more on this condition below)\u2014 we thus see that Squint obtains exactly the rates one would expect from a statistical learning/classification perspective, and the minimax excess risk results in that setting suggests that these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert [2004] that, when equipped with an oracle to tune the learning rate \u03b7 as a function of t, the rates O ((logT ) \u22c5 T 1\u2212\u03ba 2\u2212\u03ba ) can also be achieved by Hedge, but the exact tuning depends on the unknown \u03ba.", "startOffset": 28, "endOffset": 1780}, {"referenceID": 0, "context": "Massart and N\u00e9d\u00e9lec [2006], Audibert [2009]). To get better risk rates, one has to impose further assumptions on P. A standard assumption made in such cases is a Bernstein condition with exponent \u03ba > 0; see e.g. Koltchinskii [2006], Bartlett and Mendelson [2006], or Audibert [2004] or Audibert [2009]; see Van Erven et al. [2015] for how it generalizes the Tsybakov margin and other conditions. If F is sufficiently \u2018simple\u2019, e.g. a class with logarithmic entropy numbers (see Appendix F), or, in classification, a VC class, then, if a \u03ba-Bernstein condition holds, ERM (empirical risk minimization) achieves, in expectation, a better excess risk bound of order O ((logT ) \u22c5 T \u2212 1 2\u2212\u03ba ). The bound interpolates between T \u22121/2 for \u03ba = 0 and T \u22121 for \u03ba = 1 (Massart condition). Results of Tsybakov [2004], Massart and N\u00e9d\u00e9lec [2006], Audibert [2009] suggest that this rate can, in general, not be improved upon, and exactly this rate is achieved by ERM and various other algorithms in various settings by e.g. Tsybakov [2004], Audibert [2004, 2009], Bartlett et al. [2006]. By summing from t = 1 to T and using ERM at each t to classify the next data point (so that ERM becomes FTL, follow-theleader), this suggests that we can achieve a cumulative expected regret E[Rf T ] of order O ((logT ) \u22c5 T 1\u2212\u03ba 2\u2212\u03ba ). Theorem 3 shows that this is, indeed, also the rate that Squint attains in such cases if F is countable and the optimal f\u2217 has positive prior mass \u03c0 \u2217 (more on this condition below)\u2014 we thus see that Squint obtains exactly the rates one would expect from a statistical learning/classification perspective, and the minimax excess risk results in that setting suggests that these cumulative regret rates cannot be improved in general. It was shown earlier by Audibert [2004] that, when equipped with an oracle to tune the learning rate \u03b7 as a function of t, the rates O ((logT ) \u22c5 T 1\u2212\u03ba 2\u2212\u03ba ) can also be achieved by Hedge, but the exact tuning depends on the unknown \u03ba. Gr\u00fcnwald [2012] provides a means to tune \u03b7 automatically in terms of the data, but his method \u2014 like ERM and all algorithms in the references above \u2014 may achieve linear regret in worst-case settings, whereas Squint keeps the O(\u221aT ) guarantee for such cases.", "startOffset": 28, "endOffset": 1992}, {"referenceID": 11, "context": "Gaillard et al. [2014] show constant regret for finitely many experts and i.", "startOffset": 0, "endOffset": 23}], "year": 2016, "abstractText": "We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.", "creator": "LaTeX with hyperref package"}}}