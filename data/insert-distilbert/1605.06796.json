{"id": "1605.06796", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2016", "title": "Interpretable Distribution Features with Maximum Testing Power", "abstract": "two semimetrics programs on probability marginal distributions estimation are proposed, given as the sum level of differences of expectations of analytic functions evaluated at spatial or frequency locations ( i. e, surface features ). the features are chosen so as to maximize the distinguishability of the appropriate distributions, by optimizing eliminating a lower bound on test power for a statistical test using these features. the result is creating a parsimonious and continuously interpretable indication of showing how and where two distributions differ locally. an empirical estimate of the test power criterion values converges with increasing sample size, ensuring the quality guarantee of the periodically returned features. in real - world benchmarks on high - dimensional text and image data, linear - time tests using the proposed semimetrics achieve comparable performance to the state - of - the - art quadratic - time maximum mean discrepancy test, while ultimately returning human - level interpretable features yields that explain half the test results.", "histories": [["v1", "Sun, 22 May 2016 14:10:13 GMT  (579kb,D)", "https://arxiv.org/abs/1605.06796v1", null], ["v2", "Fri, 28 Oct 2016 10:48:05 GMT  (4359kb,D)", "http://arxiv.org/abs/1605.06796v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["wittawat jitkrittum", "zolt\u00e1n szab\u00f3 0001", "kacper p chwialkowski", "arthur gretton"], "accepted": true, "id": "1605.06796"}, "pdf": {"name": "1605.06796.pdf", "metadata": {"source": "CRF", "title": "Interpretable Distribution Features with Maximum Testing Power", "authors": ["Wittawat Jitkrittum", "Zolt\u00e1n Szab\u00f3", "Kacper Chwialkowski", "Arthur Gretton"], "emails": ["wittawatj@gmail.com", "zoltan.szabo.m@gmail.com", "kacper.chwialkowski@gmail.com", "arthur.gretton@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "We address the problem of discovering features of distinct probability distributions, with which they can most easily be distinguished. The distributions may be in high dimensions, can differ in non-trivial ways (i.e., not simply in their means), and are observed only through i.i.d. samples. One application for such divergence measures is to model criticism, where samples from a trained model are compared with a validation sample: in the univariate case, through the KL divergence (Cinzia Carota and Polson, 1996), or in the multivariate case, by use of the maximum mean discrepancy (MMD) (Lloyd and Ghahramani, 2015). An alternative, interpretable analysis of a multivariate difference in distributions may be obtained by projecting onto a discriminative direction, such that the Wasserstein distance on this projection is maximized (Mueller and Jaakkola, 2015). Note that both recent works require low dimensionality, either explicitly (in the case of Lloyd and Gharamani, the function becomes difficult to plot in more than two dimensions), or implicitly in the case of Mueller and Jaakkola, in that a large difference in distributions must occur in projection along a particular one-dimensional axis. Distances between distributions in high dimensions may be more subtle, however, and it is of interest to find interpretable, distinguishing features of these distributions.\nIn the present paper, we take a hypothesis testing approach to discovering features which best distinguish two multivariate probability measures P and Q, as observed by samples X := {xi}ni=1 drawn independently and identically (i.i.d.) from P , and Y := {yi}ni=1 \u2282 Rd from Q. Nonparametric two-sample tests based on RKHS distances (Eric et al., 2008; Fromont et al., 2012; Gretton et al., 2012a) or energy distances (Sz\u00e9kely and Rizzo, 2004; Baringhaus and Franz, 2004) have as their test statistic an integral probability metric, the Maximum Mean Discrepancy (Gretton et al., 2012a; Sejdinovic et al., 2013). For this metric, a smooth witness function is computed, such that the amplitude is largest where the probability mass differs most (e.g. Gretton et al., 2012a,\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n06 79\n6v 2\n[ st\nat .M\nL ]\n2 8\nO ct\nFigure 1). Lloyd and Ghahramani (2015) used this witness function to compare the model output of the Automated Statistician (Lloyd et al., 2014) with a reference sample, yielding a visual indication of where the model fails. In high dimensions, however, the witness function cannot be plotted, and is less helpful. Furthermore, the witness function does not give an easily interpretable result for distributions with local differences in their characteristic functions. A more subtle shortcoming is that it does not provide a direct indication of the distribution features which, when compared, would maximize test power - rather, it is the witness function norm, and (broadly speaking) its variance under the null, that determine test power.\nOur approach builds on the analytic representations of probability distributions of Chwialkowski et al. (2015), where differences in expectations of analytic functions at particular spatial or frequency locations are used to construct a two-sample test statistic, which can be computed in linear time. Despite the differences in these analytic functions being evaluated at random locations, the analytic tests have greater power than linear time tests based on subsampled estimates of the MMD (Gretton et al., 2012b; Zaremba et al., 2013). Our first theoretical contribution, in Sec. 3, is to derive a lower bound on the test power, which can be maximized over the choice of test locations. We propose two novel tests, both of which significantly outperform the random feature choice of Chwialkowski et al.. The (ME) test evaluates the difference of mean embeddings at locations chosen to maximize the test power lower bound (i.e., spatial features); unlike the maxima of the MMD witness function, these features are directly chosen to maximize the distinguishability of the distributions, and take variance into account. The Smooth Characteristic Function (SCF) test uses as its statistic the difference of the two smoothed empirical characteristic functions, evaluated at points in the frequency domain so as to maximize the same criterion (i.e., frequency features). Optimization of the mean embedding kernels/frequency smoothing functions themselves is achieved on a held-out data set with the same consistent objective.\nAs our second theoretical contribution in Sec. 3, we prove that the empirical estimate of the test power criterion asymptotically converges to its population quantity uniformly over the class of Gaussian kernels. Two important consequences follow: first, in testing, we obtain a more powerful test with fewer features. Second, we obtain a parsimonious and interpretable set of features that best distinguish the probability distributions. In Sec. 4, we provide experiments demonstrating that the proposed linear-time tests greatly outperform all previous linear time tests, and achieve performance that compares to or exceeds the more expensive quadratic-time MMD test (Gretton et al., 2012a). Moreover, the new tests discover features of text data (NIPS proceedings) and image data (distinct facial expressions) which have a clear human interpretation, thus validating our feature elicitation procedure in these challenging high-dimensional testing scenarios."}, {"heading": "2 ME and SCF tests", "text": "In this section, we review the ME and SCF tests (Chwialkowski et al., 2015) for two-sample testing. In Sec. 3, we will extend these approaches to learn features that optimize the power of these tests. Given two samples X := {xi}ni=1,Y := {yi}ni=1 \u2282 Rd independently and identically distributed (i.i.d.) according to P and Q, respectively, the goal of a two-sample test is to decide whether P is different from Q on the basis of the samples. The task is formulated as a statistical hypothesis test proposing a null hypothesis H0 : P = Q (samples are drawn from the same distribution) against an alternative hypothesis H1 : P 6= Q (the sample generating distributions are different). A test calculates a test statistic \u03bb\u0302n from X and Y, and rejectsH0 if \u03bb\u0302n exceeds a predetermined test threshold (critical value). The threshold is given by the (1\u2212 \u03b1)-quantile of the (asymptotic) distribution of \u03bb\u0302n under H0 i.e., the null distribution, and \u03b1 is the significance level of the test.\nME test The ME test uses as its test statistic \u03bb\u0302n, a form of Hotelling\u2019s T-squared statistic, defined as \u03bb\u0302n := nz>nS \u22121 n zn, where zn := 1 n \u2211n i=1 zi, Sn := 1 n\u22121 \u2211n i=1(zi \u2212 zn)(zi \u2212 zn)>, and zi := (k(xi,vj)\u2212 k(yi,vj))Jj=1 \u2208 RJ . The statistic depends on a positive definite kernel k : X \u00d7X \u2192 R (with X \u2286 Rd), and a set of J test locations V = {vj}Jj=1 \u2282 Rd. Under H0, asymptotically \u03bb\u0302n follows \u03c72(J), a chi-squared distribution with J degrees of freedom. The ME test rejects H0 if \u03bb\u0302n > T\u03b1, where the test threshold T\u03b1 is given by the (1 \u2212 \u03b1)-quantile of the asymptotic null distribution \u03c72(J). Although the distribution of \u03bb\u0302n under H1 was not derived, Chwialkowski et al. (2015) showed that if k is analytic, integrable and characteristic (in the sense of Sriperumbudur et al. (2011)), under H1, \u03bb\u0302n can be arbitrarily large as n\u2192\u221e, allowing the test to correctly reject H0.\nOne can intuitively think of the ME test statistic as a squared normalized (by the inverse covariance S\u22121n ) L\n2(X , VJ) distance of the mean embeddings (Smola et al., 2007) of the empirical measures Pn := 1 n \u2211n i=1 \u03b4xi , and Qn := 1 n \u2211n i=1 \u03b4yi where VJ := 1 J \u2211J i=1 \u03b4vi , and \u03b4x is the Dirac measure concentrated at x. The unnormalized counterpart (i.e., without S\u22121n ) was shown by Chwialkowski et al. (2015) to be a metric on the space of probability measures for any V . Both variants behave similarly for two-sample testing, with the normalized version being a semimetric having a more computationally tractable null distribution, i.e., \u03c72(J).\nSCF test The SCF uses the test statistic which has the same form as the ME test statistic with a modified zi := [l\u0302(xi) sin(x>i vj) \u2212 l\u0302(yi) sin(y>i vj), l\u0302(xi) cos(x>i vj) \u2212 l\u0302(yi) cos(y>i vj)]Jj=1 \u2208 R2J , where l\u0302(x) = \u222b Rd exp(\u2212iu\n>x)l(u) du is the Fourier transform of l(x), and l : Rd \u2192 R is an analytic translation-invariant kernel i.e., l(x \u2212 y) defines a positive definite kernel for x and y. In contrast to the ME test defining the statistic in terms of spatial locations, the locations V = {vj}Jj=1 \u2282 Rd in the SCF test are in the frequency domain. As a brief description, let \u03d5P (w) := Ex\u223cP exp(iw>x) be the characteristic function of P . Define a smooth characteristic function as \u03c6P (v) = \u222b Rd \u03d5P (w)l(v \u2212 w) dw (Chwialkowski et al., 2015, Definition 2). Then, similar to the ME test, the statistic defined by the SCF test can be seen as a normalized (by S\u22121n ) version of L2(X , VJ) distance of empirical \u03c6P (v) and \u03c6Q(v). The SCF test statistic has asymptotic distribution \u03c72(2J) under H0. We will use J \u2032 to refer to the degrees of freedom of the chi-squared distribution i.e., J \u2032 = J for the ME test, and J \u2032 = 2J for the SCF test.\nIn this work, we modify the statistic with a regularization parameter \u03b3n > 0, giving \u03bb\u0302n := nz>n (Sn + \u03b3nI) \u22121 zn, for stability of the matrix inverse. Using multivariate Slutsky\u2019s theorem, under H0, \u03bb\u0302n still asymptotically follows \u03c72(J \u2032) provided that \u03b3n \u2192 0 as n\u2192\u221e."}, {"heading": "3 Lower bound on test power, consistency of empirical power statistic", "text": "This section contains our main results. We propose to optimize the test locations V and kernel parameters (jointly referred to as \u03b8) by maximizing a lower bound on the test power in Proposition 1. This criterion offers a simple objective function for fast parameter tuning. The bound may be of independent interest in other Hotelling\u2019s T-squared statistics, since apart from the Gaussian case (e.g. Bilodeau and Brenner, 2008, Ch. 8), the characterization of such statistics under the alternative distribution is challenging. The optimization procedure is given in Sec. 4. We use Exy as a shorthand for Ex\u223cPEy\u223cQ and let \u2016 \u00b7 \u2016F be the Frobenius norm. Proposition 1 (Lower bound on ME test power). Let K be a uniformly bounded (i.e., \u2203B < \u221e such that supk\u2208K sup(x,y)\u2208X 2 |k(x,y)| \u2264 B) family of k : X \u00d7 X \u2192 R measurable kernels. Let V be a collection in which each element is a set of J test locations. Assume that c\u0303 := supV\u2208V,k\u2208K \u2016\u03a3\u22121\u2016F < \u221e. For large n, the test power P ( \u03bb\u0302n \u2265 T\u03b1 ) of the ME test sat-\nisfies P ( \u03bb\u0302n \u2265 T\u03b1 ) \u2265 L(\u03bbn) where\nL(\u03bbn) := 1\u2212 2e\u2212\u03be1(\u03bbn\u2212T\u03b1) 2/n \u2212 2e\u2212\n[\u03b3n(\u03bbn\u2212T\u03b1)(n\u22121)\u2212\u03be2n] 2\n\u03be3n(2n\u22121)2 \u2212 2e\u2212[(\u03bbn\u2212T\u03b1)/3\u2212c3n\u03b3n] 2\u03b32n/\u03be4 ,\nand c3, \u03be1, . . . \u03be4 are positive constants depending on only B, J and c\u0303. The parameter \u03bbn := n\u00b5>\u03a3\u22121\u00b5 is the population counterpart of \u03bb\u0302n := nz>n (Sn + \u03b3nI) \u22121 zn where \u00b5 = Exy[z1] and \u03a3 = Exy[(z1 \u2212 \u00b5)(z1 \u2212 \u00b5)>]. For large n, L(\u03bbn) is increasing in \u03bbn.\nProof (sketch). The idea is to construct a bound for |\u03bb\u0302n \u2212 \u03bbn| which involves bounding \u2016zn \u2212 \u00b5\u20162 and \u2016Sn\u2212\u03a3\u2016F separately using Hoeffding\u2019s inequality. The result follows after a reparameterization of the bound on P(|\u03bb\u0302n \u2212 \u03bbn| \u2265 t) to have P ( \u03bb\u0302n \u2265 T\u03b1 ) . See Sec. F for details.\nProposition 1 suggests that for large n it is sufficient to maximize \u03bbn to maximize a lower bound on the ME test power. The same conclusion holds for the SCF test (result omitted due to space constraints). Assume that k is characteristic (Sriperumbudur et al., 2011). It can be shown that \u03bbn = 0 if and only if P = Q i.e., \u03bbn is a semimetric for P and Q. In this sense, one can see \u03bbn as encoding the ease of rejecting H0. The higher \u03bbn, the easier for the test to correctly reject H0 when H1 holds. This observation justifies the use of \u03bbn as a maximization objective for parameter tuning.\nContributions The statistic \u03bb\u0302n for both ME and SCF tests depends on a set of test locations V and a kernel parameter \u03c3. We propose to set \u03b8 := {V, \u03c3} = arg max\u03b8 \u03bbn = arg max\u03b8 \u00b5>\u03a3\u22121\u00b5. The optimization of \u03b8 brings two benefits: first, it significantly increases the probability of rejecting H0 when H1 holds; second, the learned test locations act as discriminative features allowing an interpretation of how the two distributions differ. We note that optimizing parameters by maximizing a test power proxy (Gretton et al., 2012b) is valid under both H0 and H1 as long as the data used for parameter tuning and for testing are disjoint. If H0 holds, then \u03b8 = arg max 0 is arbitrary. Since the test statistic asymptotically follows \u03c72(J \u2032) for any \u03b8, the optimization does not change the null distribution. Also, the rejection threshold T\u03b1 depends on only J \u2032 and is independent of \u03b8.\nTo avoid creating a dependency between \u03b8 and the data used for testing (which would affect the null distribution), we split the data into two disjoint sets. Let D := (X,Y) and Dtr,Dte \u2282 D such that Dtr \u2229Dte = \u2205 and Dtr \u222aDte = D. In practice, since \u00b5 and \u03a3 are unknown, we use \u03bb\u0302trn/2 in place of \u03bbn, where \u03bb\u0302trn/2 is the test statistic computed on the training set D\ntr. For simplicity, we assume that each of Dtr and Dte has half of the samples in D. We perform an optimization of \u03b8 with gradient ascent algorithm on \u03bb\u0302trn/2(\u03b8). The actual two-sample test is performed using the test statistic \u03bb\u0302 te n/2(\u03b8) computed on Dte. The full procedure from tuning the parameters to the actual two-sample test is summarized in Sec. A.\nSince we use an empirical estimate \u03bb\u0302trn/2 in place of \u03bbn for parameter optimization, we give a finitesample bound in Theorem 2 guaranteeing the convergence of z>n (Sn + \u03b3nI)\n\u22121zn to \u00b5>\u03a3\u22121\u00b5 as n increases, uniformly over all kernels k \u2208 K (a family of uniformly bounded kernels) and all test locations in an appropriate class. Kernel classes satisfying conditions of Theorem 2 include the widely used isotropic Gaussian kernel classKg = { k\u03c3 : (x,y) 7\u2192 exp ( \u2212(2\u03c32)\u22121\u2016x\u2212 y\u20162 ) | \u03c3 > 0 } , and\nthe more general full Gaussian kernel class Kfull = {k : (x,y) 7\u2192 exp ( \u2212(x\u2212 y)>A(x\u2212 y) ) | A is positive definite} (see Lemma 5 and Lemma 6). Theorem 2 (Consistency of \u03bb\u0302n in the ME test). Let X \u2286 Rd be a measurable set, and V be a collection in which each element is a set of J test locations. All suprema over V and k are to be understood as supV\u2208V and supk\u2208K respectively. For a class of kernels K on X \u2286 Rd, define\nF1 := {x 7\u2192 k(x,v) | k \u2208 K,v \u2208 X}, F2 := {x 7\u2192 k(x,v)k(x,v\u2032) | k \u2208 K,v,v\u2032 \u2208 X}, (1) F3 := {(x,y) 7\u2192 k(x,v)k(y,v\u2032) | k \u2208 K,v,v\u2032 \u2208 X}. (2)\nAssume that (1) K is a uniformly bounded (by B) family of k : X \u00d7 X \u2192 R measurable kernels, (2) c\u0303 := supV,k \u2016\u03a3\u22121\u2016F < \u221e, and (3) Fi = {f\u03b8i | \u03b8i \u2208 \u0398i} is VC-subgraph with VC-index V C(Fi), and \u03b8 7\u2192 f\u03b8i(m) is continuous (\u2200m, i = 1, 2, 3). Let c1 := 4B2J \u221a Jc\u0303, c2 := 4B \u221a Jc\u0303, and c3 := 4B2Jc\u03032. Let Ci-s (i = 1, 2, 3) be the universal constants associated to Fi-s according to Theorem 2.6.7 in van der Vaart and Wellner (2000). Then for any \u03b4 \u2208 (0, 1) with probability at least 1\u2212 \u03b4,\nsup V,k \u2223\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\u2223 \u2264 2TF1 ( 2\n\u03b3n c1BJ 2n\u2212 1 n\u2212 1 + c2 \u221a J\n) + 2\n\u03b3n c1J(TF2 + TF3) +\n8\n\u03b3n\nc1B 2J\nn\u2212 1 + c3\u03b3n,where\nTFj = 16 \u221a 2B\u03b6j\u221a n\n( 2 \u221a log [ Cj \u00d7 V C(Fj)(16e)V C(Fj) ] + \u221a 2\u03c0[V C(Fj)\u2212 1]\n2\n) +B\u03b6j \u221a 2 log(5/\u03b4)\nn ,\nfor j = 1, 2, 3 and \u03b61 = 1, \u03b62 = \u03b63 = 2.\nProof (sketch). The idea is to lower bound the difference with an expression involving supV,k \u2016zn \u2212 \u00b5\u20162 and supV,k \u2016Sn \u2212\u03a3\u2016F . These two quantities can be seen as suprema of empirical processes, and can be bounded by Rademacher complexities of their respective function classes (i.e., F1,F2, and F3). Finally, the Rademacher complexities can be upper bounded using Dudley entropy bound and VC subgraph properties of the function classes. Proof details are given in Sec. D.\nTheorem 2 implies that if we set \u03b3n = O(n\u22121/4), then we have supV,k \u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223 = Op(n\u22121/4) as the rate of convergence. Both\nProposition 1 and Theorem 2 require c\u0303 := supV\u2208V,k\u2208K \u2016\u03a3\u22121\u2016F < \u221e as a precondition. To guarantee that c\u0303 < \u221e, a concrete construction of K is the isotropic Gaussian kernel class Kg, where \u03c3 is constrained to be in a compact set. Also, consider V := {V | any two locations are at least distance apart, and all test locations have their norms bounded by \u03b6} for some , \u03b6 > 0. Then, for any non-degenerate P,Q, we have c\u0303 < \u221e since (\u03c3,V) 7\u2192 \u03bbn is continuous, and thus attains its supremum over compact sets K and V.\n4 Experiments\nIn this section, we demonstrate the effectiveness of the proposed methods on both toy and real problems. We consider the isotropic Gaussian kernel class Kg in all kernel-based tests. We study seven two-sample test algorithms. For the SCF test, we set l\u0302(x) = k(x,0). Denote by MEfull and SCF-full the ME and SCF tests whose test locations and the Gaussian width \u03c3 are fully optimized using gradient ascent on a separate training sample (Dtr) of the same size as the test set (Dte). ME-grid and SCF-grid are as in Chwialkowski et al. (2015) where only the Gaussian width is optimized by a grid search,1and the test locations are randomly drawn from a multivariate normal distribution. MMD-quad (quadratic-time) and MMD-lin (linear-time) re-\nfer to the nonparametric tests based on maximum mean discrepancy of Gretton et al. (2012a), where to ensure a fair comparison, the Gaussian kernel width is also chosen so as to maximize a criterion for the test power on training data, following the same principle as (Gretton et al., 2012b). For MMDquad, since its null distribution is given by an infinite sum of weighted chi-squared variables (no closed-form quantiles), in each trial we randomly permute the two samples 400 times to approximate the null distribution. Finally, T 2 is the standard two-sample Hotelling\u2019s T-squared test, which serves as a baseline with Gaussian assumptions on P and Q.\nIn all the following experiments, each problem is repeated for 500 trials. For toy problems, new samples are generated from the specified P,Q distributions in each trial. For real problems, samples are partitioned randomly into training and test sets in each trial. In all of the simulations, we report an empirical estimate of P(\u03bb\u0302ten/2 \u2265 T\u03b1) which is the proportion of the number of times the statistic \u03bb\u0302 te n/2 is above T\u03b1. This quantity is an estimate of type-I error under H0, and corresponds to test power when H1 is true. We set \u03b1 = 0.01 in all the experiments. All the code and preprocessed data are available at https://github.com/wittawatj/interpretable-test.\nOptimization The parameter tuning objective \u03bb\u0302trn/2(\u03b8) is a function of \u03b8 consisting of one real-valued \u03c3 and J test locations each of d dimensions. The parameters \u03b8 can thus be regarded as a Jd + 1 Euclidean vector. We take the derivative of \u03bb\u0302trn/2(\u03b8) with respect to \u03b8, and use gradient ascent to maximize it. J is pre-specified and fixed. For the ME test, we initialize the test locations with realizations from two multivariate normal distributions fitted to samples from P and Q; this ensures that the initial locations are well supported by the data. For the SCF test, initialization using the standard normal distribution is found to be sufficient. The parameter \u03b3n is not optimized; we set the regularization parameter \u03b3n to be as small as possible while being large enough to ensure that (Sn + \u03b3nI)\n\u22121 can be stably computed. We emphasize that both the optimization and testing are linear in n. The testing cost O(J3 + J2n + dJn) and the optimization costs O(J3 + dJ2n) per gradient ascent iteration. Runtimes of all methods are reported in Sec. C in the appendix.\n1. Informative features: simple demonstration We begin with a demonstration that the proxy \u03bb\u0302trn/2(\u03b8) for the test power is informative for revealing the difference of the two samples in the ME\n1Chwialkowski et al. (2015) chooses the Gaussian width that minimizes the median of the p-values, a heuristic that does not directly address test power. Here, we perform a grid search to choose the best Gaussian width by maximizing \u03bb\u0302trn/2 as done in ME-full and SCF-full.\ntest. We consider the Gaussian Mean Difference (GMD) problem (see Table 1), where both P and Q are two-dimensional normal distributions with the difference in means. We use J = 2 test locations v1 and v2, where v1 is fixed to the location indicated by the black triangle in Fig. 1. The contour plot shows v2 7\u2192 \u03bb\u0302trn/2(v1,v2).\nFig. 1 (top) suggests that \u03bb\u0302trn/2 is maximized when v2 is placed in either of the two regions that captures the difference of the two samples i.e., the region in which the probability masses of P and Q have less overlap. Fig. 1 (bottom), we consider placing v1 in one of the two key regions. In this case, the contour plot shows that v2 should be placed in the other region to maximize \u03bb\u0302trn/2, implying that placing multiple test locations in the same neighborhood will not increase the discriminability. The two modes on the left and right suggest two ways to place the test location in a region that reveals the difference. The non-convexity of the \u03bb\u0302trn/2 is an indication of many informative ways to detect differences of P and Q, rather than a drawback. A convex objective would not capture this multimodality.\n2. Test power vs. sample size n We now demonstrate the rate of increase of test power with sample size. When the null hypothesis holds, the type-I error stays at the specified level \u03b1. We consider the following four toy problems: Same Gaussian (SG), Gaussian mean difference (GMD), Gaussian variance difference (GVD), and Blobs. The specifications of P and Q are summarized in Table. 1. In the Blobs problem, P and Q are defined as a mixture of Gaussian distributions arranged on a 4\u00d7 4 grid in R2. This problem is challenging as the difference of P and Q is encoded at a much smaller length scale compared to the global structure (Gretton et al., 2012b). Specifically, the eigenvalue ratio for the covariance of each Gaussian distribution is 2.0 in P , and 1.0 in Q. We set J = 5 in this experiment.\nThe results are shown in Fig. 2 where type-I error (for SG problem), and test power (for GMD, GVD and Blobs problems) are plotted against test sample size. A number of observations are worth noting. In the SG problem, we see that the type-I error roughly stays at the specified level: the rate of rejection of H0 when it is true is roughly at the specified level \u03b1 = 0.01.\nGMD with 100 dimensions turns out to be an easy problem for all the tests except MMD-lin. In the GVD and Blobs cases, ME-full and SCFfull achieve substantially higher test power than ME-grid and SCF-grid,\nrespectively, suggesting a clear advantage from optimizing the test locations. Remarkably, ME-full consistently outperforms the quadratic-time MMD across all test sample sizes in the GVD case. When the difference of P and Q is subtle as in the Blobs problem, ME-grid, which uses randomly drawn test locations, can perform poorly (see Fig. 2d) since it is unlikely that randomly drawn locations will be placed in the key regions that reveal the difference. In this case, optimization of the test locations can considerably boost the test power (see ME-full in Fig. 2d). Note also that SCF variants perform significantly better than ME variants on the Blobs problem, as the difference in P and Q is localized in the frequency domain; ME-full and ME-grid would require many more test locations in the spatial domain to match the test powers of the SCF variants. For the same reason, SCF-full does much better than the quadratic-time MMD across most sample sizes, as the latter represents a weighted distance between characteristic functions integrated across the entire frequency domain (Sriperumbudur et al., 2010, Corollary 4).\n3. Test power vs. dimension d We next investigate how the dimension (d) of the problem can affect type-I errors and test powers of ME and SCF tests. We consider the same artificial problems: SG, GMD and GVD. This time, we fix the test sample size to 10000, set J = 5, and vary the dimension. The results are shown in Fig. 3. Due to the large dimensions and sample size, it is computationally infeasible to run MMD-quad.\nWe observe that all the tests except the T-test can maintain type-I error at roughly the specified significance level \u03b1 = 0.01 as dimension increases. The type-I performance of the T-test is incorrect at large d because of the difficulty in accurately estimating the covariance matrix in high dimensions. It is interesting to note the high performance of ME-full in the GMD problem in Fig. 3b. ME-full achieves the maximum test power of 1.0 throughout and matches the power T-test, in spite of being nonparametric and making no assumption on P andQ (the T-test is further advantaged by its excessive Type-I error). However, this is true only with optimization of the test locations. This is reflected in the test power of ME-grid in Fig. 3b which drops monotonically as dimension increases, highlighting the importance of test location optimization. The performance of MMD-lin degrades quickly with increasing dimension, as expected from Ramdas et al. (2015).\n4. Distinguishing articles from two categories We now turn to performance on real data. We first consider the problem of distinguishing two categories of publications at the conference on Neural Information Processing Systems (NIPS). Out of 5903 papers published in NIPS from 1988 to 2015, we manually select disjoint subsets related to Bayesian inference (Bayes), neuroscience (Neuro), deep learning (Deep), and statistical learning theory (Learn) (see Sec. B). Each paper is represented as a bag of words using TF-IDF (Manning et al., 2008) as features. We perform stemming, remove all stop words, and retain only nouns. A further filtering of document-frequency (DF) of words that satisfies 5 \u2264 DF \u2264 2000 yields approximately 5000 words from which 2000 words (i.e., d = 2000 dimensions) are randomly selected. See Sec. B for more details on the preprocessing. For ME and SCF tests, we use only one test location i.e., set J = 1. We perform 1000 permutations to approximate the null distribution of MMD-quad in this and the following experiments.\nType-I errors and test powers are summarized in Table. 2. The first column indicates the categories of the papers in the two samples. In Bayes-Bayes problem, papers on Bayesian inference are randomly partitioned into two samples in each trial. This task represents a case in which H0 holds. Among all the linear-time tests, we observe that ME-full has the highest test power in all the tasks, attaining a maximum test power of 1.0 in the Bayes-Neuro problem. This high performance assures that although different test locations V may be selected in different trials, these locations are each informative. It is interesting to observe that ME-full has performance close to or better than MMD-quad, which requires O(n2) runtime complexity. Besides clear advantages of interpretability and linear runtime of the proposed tests, these results suggest that evaluating the differences in expectations of analytic functions at particular locations can yield an equally powerful test at a much lower cost, as opposed to\ncomputing the RKHS norm of the witness function as done in MMD. Unlike Blobs, however, Fourier features are less powerful in this setting.\nWe further investigate the interpretability of the ME test by the following procedure. For the learned test location vt \u2208 Rd (d = 2000) in trial t, we construct v\u0303t = (v\u0303t1, . . . , v\u0303td) such that v\u0303tj = |vtj |. Let \u03b7tj \u2208 {0, 1} be an indicator variable taking value 1 if v\u0303tj is among the top five largest for all j \u2208 {1, . . . , d}, and 0 otherwise. Define \u03b7j := \u2211 t \u03b7 t j as a proxy indicating the significance of word j i.e., \u03b7j is high if word j is frequently among the top five largest as measured by v\u0303tj . The top seven words as sorted in descending order by \u03b7j in the Bayes-Neuro problem are spike, markov, cortex, dropout, recurr, iii, gibb, showing that the learned test locations are highly interpretable. Indeed, \u201cmarkov\u201d and \u201cgibb\u201d (i.e., stemmed from Gibbs) are discriminative terms in Bayesian inference category, and \u201cspike\u201d and \u201ccortex\u201d are key terms in neuroscience. We give full lists of discriminative terms learned in all the problems in Sec. B.1. To show that not all the randomly selected 2000 terms are informative, if the definition of \u03b7tj is modified to consider the least important words (i.e., \u03b7j is high if word j is frequently among the top five smallest as measured by v\u0303tj), we instead obtain circumfer, bra, dominiqu, rhino, mitra, kid, impostor, which are not discriminative.\n(a) HA (b) NE (c) SU\n(d) AF (e) AN (f) DI (g) v1\nFigure 4: (a)-(f): Six facial expressions of actor AM05 in the KDEF data. (g): Average across trials of the learned test locations v1.\n5. Distinguishing positive and negative emotions In the final experiment, we study how well ME and SCF tests can distinguish two samples of photos of people showing positive and negative facial expressions. Our emphasis is on the discriminative features of the faces identified by ME test showing how the two groups differ. For this purpose, we use Karolinska Directed Emotional Faces (KDEF) dataset (Lundqvist et al., 1998) containing 5040 aligned face images of 70 amateur actors, 35 females and 35 males. We use only photos showing front views of the faces. In the dataset, each actor displays seven expressions: happy (HA), neutral (NE), surprised (SU), sad (SA), afraid (AF), angry (AN), and disgusted (DI). We assign HA, NE, and SU faces into the positive emotion group (i.e., samples from P ), and AF, AN and DI faces into the negative emotion group (samples from Q). We denote this problem as \u201c+ vs. \u2212\u201d. Examples of six facial expressions from one actor are shown in Fig. 4. Photos of the SA group are unused to keep the sizes of the two samples\nthe same. Each image of size 562 \u00d7 762 pixels is cropped to exclude the background, resized to 48\u00d7 34 = 1632 pixels (d), and converted to grayscale. We run the tests 500 times with the same setting used previously i.e., Gaussian kernels, and J = 1. The type-I errors and test powers are shown in Table 3. In the table, \u201c\u00b1 vs. \u00b1\u201d is a problem in which all faces expressing the six emotions are randomly split into two samples of equal sizes i.e., H0 is true. Both ME-full and SCF-full achieve high test powers while maintaining the correct type-I errors.\nAs a way to interpret how positive and negative emotions differ, we take an average across trials of the learned test locations of ME-full in the \u201c+ vs. \u2212\u201d problem. This average is shown in Fig. 4g. We see that the test locations faithfully capture the difference of positive and negative emotions by giving more weights to the regions of nose, upper lip, and nasolabial folds (smile lines), confirming the interpretability of the test in a high-dimensional setting."}, {"heading": "Acknowledgement", "text": "We thank the Gatsby Charitable Foundation for the financial support."}, {"heading": "A Algorithm", "text": "The full algorithm for the proposed tests from parameter tuning to the actual two-sample testing is given in Algorithm 1.\nAlgorithm 1 Optimizing parameters and testing Input: Two samples X, Y, significance level \u03b1, and number of test locations J\n1: Split D := (X,Y) into disjoint training and test sets, Dtr and Dte, of the same size nte. 2: Optimize parameters \u03b8 = arg max\u03b8 \u03bb\u0302trn/2(\u03b8) where \u03bb\u0302 tr n/2(\u03b8) is computed with the training set D\ntr. 3: Set T\u03b1 to the (1\u2212 \u03b1)-quantile of \u03c72(J \u2032). 4: Compute the test statistic \u03bb\u0302ten/2(\u03b8) using D te. 5: Reject H0 if \u03bb\u0302ten/2(\u03b8) > T\u03b1."}, {"heading": "B Experiments on NIPS text collection", "text": "The full procedure for processing the NIPS text collection is summarized as following.\n1. Download all 5903 papers from 1988 to 2015 from https://papers.nips.cc/ as PDF files.\n2. Convert each PDF file to text with pdftotext2.\n3. Remove all stop words. We use the list of stop words from http://www.ranks.nl/stopwords.\n4. Keep only nouns. We use the list of nouns as available in WordNet-3.03.\n5. Keep only words which contain only English alphabets i.e., does not contain punctuations or numbers. Also, word length must be between 3 and 20 characters (inclusive).\n6. Keep only words which occur in at least 5 documents, and in no more than 2000 documents.\n7. Convert all characters to small case. Stem all words with SnowballStemmer in NLTK (Bird et al., 2009). For example, \u201crecognize\u201d and \u201crecognizer\u201d become \u201crecogn\u201d after stemming.\n8. Categorize papers into disjoint collections. A paper is treated as belonging to a group if its title has at least one word from the list of keywords for the category. Papers that match the criteria of both categories are not considered. The lists of keywords are as follows.\n(a) Bayesian inference (Bayes): graphical model, bayesian, inference, mcmc, monte carlo, posterior, prior, variational, markov, latent, probabilistic, exponential family.\n(b) Deep learning (Deep): deep, drop out, auto-encod, convolutional, neural net, belief net, boltzmann. (c) Learning theory (Learn): learning theory, consistency, theoretical guarantee, complexity, pac-\nbayes, pac-learning, generalization, uniform converg, bound, deviation, inequality, risk min, minimax, structural risk, VC, rademacher, asymptotic.\n(d) Neuroscience (Neuro): motor control, neural, neuron, spiking, spike, cortex, plasticity, neural decod, neural encod, brain imag, biolog, perception, cognitive, emotion, synap, neural population, cortical, firing rate, firing-rate, sensor.\n9. Randomly select 2000 words from the remaining words.\n10. Treat each paper as a bag of words and construct a feature vector with TF-IDF (Manning et al., 2008).\n2pdftotext is available at http://poppler.freedesktop.org. 3WordNet is available online at https://wordnet.princeton.edu/wordnet/citing-wordnet/.\nB.1 Discriminative terms identified by ME test\nIn this section, we provide full lists of discriminative terms following the procedure described in Sec. 4. The top ten words in each problem are as follows.\n\u2022 Bayes-Bayes: collabor, traffic, bay, permut, net, central, occlus, mask, draw, joint. \u2022 Bayes-Deep: infer, bay, mont, adaptor, motif, haplotyp, ecg, covari, boltzmann, classifi. \u2022 Bayes-Learn: infer, markov, graphic, segment, bandit, boundari, favor, carlo, prioriti, prop. \u2022 Bayes-Neuro: spike, markov, cortex, dropout, recurr, iii, gibb, basin, circuit, subsystem. \u2022 Learn-Deep: deep, forward, delay, subgroup, bandit, recept, invari, overlap, inequ, pia. \u2022 Learn-Neuro: polici, interconnect, hardwar, decay, histolog, edg, period, basin, inject, human."}, {"heading": "C Runtimes", "text": ""}, {"heading": "D Proof of theorem 2", "text": "Recall Theorem 2: Theorem 2 (Consistency of \u03bb\u0302n in the ME test). Let X \u2286 Rd be a measurable set, and V be a collection in which each element is a set of J test locations. All suprema over V and k are to be understood as supV\u2208V and supk\u2208K respectively. For a class of kernels K on X \u2286 Rd, define\nF1 := {x 7\u2192 k(x,v) | k \u2208 K,v \u2208 X}, F2 := {x 7\u2192 k(x,v)k(x,v\u2032) | k \u2208 K,v,v\u2032 \u2208 X}, (1) F3 := {(x,y) 7\u2192 k(x,v)k(y,v\u2032) | k \u2208 K,v,v\u2032 \u2208 X}. (2)\nAssume that (1) K is a uniformly bounded (by B) family of k : X \u00d7 X \u2192 R measurable kernels, (2) c\u0303 := supV,k \u2016\u03a3\u22121\u2016F <\u221e, and (3) Fi = {f\u03b8i | \u03b8i \u2208 \u0398i} is VC-subgraph with VC-index V C(Fi), and \u03b8 7\u2192 f\u03b8i(m) is continuous (\u2200m, i = 1, 2, 3). Let c1 := 4B2J \u221a Jc\u0303, c2 := 4B \u221a Jc\u0303, and c3 := 4B2Jc\u03032. Let Ci-s (i = 1, 2, 3) be the universal constants associated to Fi-s according to Theorem 2.6.7 in van der Vaart and Wellner (2000). Then for any \u03b4 \u2208 (0, 1) with probability at least 1\u2212 \u03b4,\nsup V,k \u2223\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\u2223 \u2264 2TF1 ( 2\n\u03b3n c1BJ 2n\u2212 1 n\u2212 1 + c2 \u221a J\n) + 2\n\u03b3n c1J(TF2 + TF3) +\n8\n\u03b3n\nc1B 2J\nn\u2212 1 + c3\u03b3n,where\nTFj = 16 \u221a 2B\u03b6j\u221a n\n( 2 \u221a log [ Cj \u00d7 V C(Fj)(16e)V C(Fj) ] + \u221a 2\u03c0[V C(Fj)\u2212 1]\n2\n) +B\u03b6j \u221a 2 log(5/\u03b4)\nn ,\nfor j = 1, 2, 3 and \u03b61 = 1, \u03b62 = \u03b63 = 2.\nA proof is given as follows."}, {"heading": "D.1 Notations", "text": "Let \u3008A,B\u3009F := tr ( A>B ) be the Frobenius inner product, and \u2016A\u2016F := \u221a \u3008A,A\u3009F . A 0 means that A \u2208 Rd\u00d7d is symmetric, positive semidefinite. For a \u2208 Rd, \u2016a\u20162 = \u3008a,a\u30092 = a>a. [a1; . . . ; aN ] \u2208 Rd1+...+dN is the concatenation of the an \u2208 Rdn vectors. R+ is the set of positive reals. f \u25e6 g is the composition of function f and g. LetM denote a general metric space below. In measurability requirements metric spaces are meant to be endowed with their Borel \u03c3-algebras.\nLet C be a collection of subsets ofM (C \u2286 2M). C is said to shatter an {p1, p2, . . . , pi} \u2286 M set, if for any S \u2286 {p1, p2, . . . , pi} there exist C \u2208 C such that S = C \u2229 {p1, p2, . . . , pi}; in other words, arbitrary subset of {p1, p2, . . . , pi} can be cut out by an element of C. The VC index of C is the smallest i for which no set of size i is shattered:\nV C (C) = inf { i : max\np1,...,pi |{C \u2229 {p1, . . . , pi} : C \u2208 C}| < 2i\n} .\nA collection C of measurable sets is called VC-class if its index V C (C) is finite. The subgraph of a real-valued function f :M\u2192 R is sub(f) = {(m,u) : u < f(m)} \u2286 M\u00d7 R. A collection of F measurable functions is called VC-subgraph class, or shortly VC if the collection of all subgraphs of F , {sub(f)}f\u2208F is a VC-class of sets; its index is defined as V C (F) := V C ( {sub(f)}f\u2208F ) .\nLet L0(M) be the set ofM\u2192 R measurable functions. Given an i.i.d. (independent identically distributed) sample from P (wi i.i.d.\u223c P), let w1:n = (w1, . . . , wn) and let Pn = 1n \u2211n i=1 \u03b4wi denote the empirical measure.\nLq (M,Pn) = { f \u2208 L0 (M) : \u2016f\u2016Lq(M,Pn) := [\u222b M |f(w)| qdPn(w) ] 1 q = [ 1 n \u2211n i=1 |f(wi)|q ] 1 q <\u221e } (1 \u2264\nq <\u221e), \u2016f\u2016L\u221e(M) := supm\u2208M |f(m)|. Define Pf := \u222b M f(w)dP(w), where P is a probability distribution onM. Let \u2016Pn \u2212 P\u2016F := supf\u2208F |Pnf \u2212 Pf |.\nThe diameter of a class F \u2286 L2 (M,Pn) is diam(F , L2(M,Pn)) := supf,f \u2032\u2208F \u2016f \u2212 f \u2032\u2016L2(M,Pn), its rcovering number (r > 0) is the size of the smallest r-net\nN ( r,F , L2 (M,Pn) ) = inf { t \u2265 1 : \u2203f1, . . . , ft \u2208 F such that F \u2286 \u222ati=1B(r, fi) } ,\nwhere B(r, f) = { g \u2208 L2 (M,Pn) | \u2016f \u2212 g\u2016L2(M,Pn) \u2264 r } is the ball with center f and radius r. \u00d7Ni=1Qi is the N -fold product measure. For sets Qi, \u00d7ni=1Qi is their Cartesian product. For a function class F \u2286 L0 (M) and w1:n \u2208Mn, R(F , w1:n) := Er [ supf\u2208F \u2223\u2223 1 n \u2211n i=1 rif(wi)\n\u2223\u2223] is the empirical Rademacher average, where r := r1:n and ri-s are i.i.d. samples from a Rademacher random variable [P(ri = 1) = P(ri = \u22121) = 12 ]. Let (\u0398, \u03c1) be a metric space; a collection of F = {f\u03b8 | \u03b8 \u2208 \u0398} \u2286 L0(M) functions is called a separable Carath\u00e9odory family if \u0398 is separable and \u03b8 7\u2192 f\u03b8(m) is continuous for all m \u2208M. span(\u00b7) denotes the linear hull of its arguments. \u0393(t) = \u222b\u221e 0 ut\u22121e\u2212udu denotes the Gamma function."}, {"heading": "D.2 Bound in terms of Sn and zn", "text": "For brevity, we will interchangeably use Sn for Sn(V) and zn for zn(V). Sn(V) and zn(V) will be used mainly when the dependency of V needs to be emphasized. We start with supV,k \u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223and upper bound the argument of supV,k as\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\n= \u2223\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5> (\u03a3 + \u03b3nI)\u22121 \u00b5 + \u00b5> (\u03a3 + \u03b3nI)\u22121 \u00b5\u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5> (\u03a3 + \u03b3nI)\u22121 \u00b5\u2223\u2223\u2223+ \u2223\u2223\u2223\u00b5> (\u03a3 + \u03b3nI)\u22121 \u00b5\u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\u2223\n:= ( 1) + ( 2).\nFor ( 1), we have\u2223\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5> (\u03a3 + \u03b3nI)\u22121 \u00b5\u2223\u2223\u2223 = \u2223\u2223\u2223\u2329znz>n , (Sn + \u03b3nI)\u22121\u232aF \u2212 \u2329\u00b5\u00b5>, (\u03a3 + \u03b3nI)\u22121\u232aF \u2223\u2223\u2223\n= \u2223\u2223\u2223\u2329znz>n , (Sn + \u03b3nI)\u22121\u232aF \u2212 \u2329znz>n , (\u03a3 + \u03b3nI)\u22121\u232aF + \u2329znz>n , (\u03a3 + \u03b3nI)\u22121\u232aF \u2212 \u2329\u00b5\u00b5>, (\u03a3 + \u03b3nI)\u22121\u232aF \u2223\u2223\u2223\n\u2264 \u2223\u2223\u2223\u2329znz>n , (Sn + \u03b3nI)\u22121 \u2212 (\u03a3 + \u03b3nI)\u22121\u232a\nF \u2223\u2223\u2223+ \u2223\u2223\u2223\u2329znz>n \u2212 \u00b5\u00b5>, (\u03a3 + \u03b3nI)\u22121\u232a F \u2223\u2223\u2223 = \u2016znz>n \u2016F \u2016(Sn + \u03b3nI)\u22121 \u2212 (\u03a3 + \u03b3nI) \u22121 \u2016F + \u2016znz>n \u2212 \u00b5\u00b5>\u2016F \u2016 (\u03a3 + \u03b3nI) \u22121 \u2016F\n(a) \u2264 \u2016znz>n \u2016F \u2016(Sn + \u03b3nI)\u22121[(\u03a3 + \u03b3nI)\u2212 (Sn + \u03b3nI)] (\u03a3 + \u03b3nI) \u22121 \u2016F + \u2016znz>n \u2212 zn\u00b5> + zn\u00b5> \u2212 \u00b5\u00b5>\u2016F \u2016\u03a3\u22121\u2016F\n(a) \u2264 \u2016znz>n \u2016F \u2016(Sn + \u03b3nI)\u22121\u2016F \u2016\u03a3\u2212 Sn\u2016F \u2016\u03a3\u22121\u2016F + \u2016zn(zn \u2212 \u00b5)>\u2016F \u2016\u03a3\u22121\u2016F + \u2016(zn \u2212 \u00b5)\u00b5>\u2016F \u2016\u03a3\u22121\u2016F (b)\n\u2264 \u221a J\n\u03b3n \u2016zn\u201622\u2016\u03a3\u2212 Sn\u2016F \u2016\u03a3\u22121\u2016F + \u2016zn\u20162\u2016zn \u2212 \u00b5\u20162\u2016\u03a3\u22121\u2016F + \u2016\u00b5\u20162\u2016zn \u2212 \u00b5\u20162\u2016\u03a3\u22121\u2016F ,\nwhere at (a) we use \u2016 (\u03a3 + \u03b3nI)\u22121 \u2016F \u2264 \u2016\u03a3\u22121\u2016F and at (b) we use \u2016(Sn + \u03b3nI)\u22121\u2016F \u2264 \u221a J\u2016(Sn + \u03b3nI) \u22121\u20162 \u2264 \u221a J/\u03b3n. For ( 2), we have\u2223\u2223\u2223\u00b5> (\u03a3 + \u03b3nI)\u22121 \u00b5\u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\u2223 = \u2223\u2223\u2223\u2329\u00b5\u00b5>, (\u03a3 + \u03b3nI)\u22121 \u2212\u03a3\u22121\u232a F\n\u2223\u2223\u2223 \u2264 \u2016\u00b5\u00b5>\u2016F \u2016 (\u03a3 + \u03b3nI)\u22121 \u2212\u03a3\u22121\u2016F = \u2016\u00b5\u201622\u2016 (\u03a3 + \u03b3nI) \u22121 [\u03a3\u2212 (\u03a3 + \u03b3nI)] \u03a3\u22121\u2016F = \u03b3n\u2016\u00b5\u201622\u2016\u2016 (\u03a3 + \u03b3nI) \u22121\n\u03a3\u22121\u2016F \u2264 \u03b3n\u2016\u00b5\u201622\u2016\u2016 (\u03a3 + \u03b3nI)\n\u22121 \u2016F \u2016\u03a3\u22121\u2016F (a)\n\u2264 \u03b3n\u2016\u00b5\u201622\u2016\u2016\u03a3\u22121\u20162F . Combining the upper bounds for ( 1) and ( 2), we arrive at\u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223\n\u2264 \u221a J\n\u03b3n \u2016zn\u201622\u2016\u03a3\u2212 Sn\u2016F \u2016\u03a3\u22121\u2016F + (\u2016zn\u20162 + \u2016\u00b5\u20162)\u2016zn \u2212 \u00b5\u20162\u2016\u03a3\u22121\u2016F + \u03b3n\u2016\u00b5\u201622\u2016\u2016\u03a3\u22121\u20162F\n\u2264 4B2Jc\u0303 \u221a J\n\u03b3n \u2016\u03a3\u2212 Sn\u2016F + 4B\n\u221a Jc\u0303\u2016zn \u2212 \u00b5\u20162 + 4B2Jc\u03032\u03b3n\n= c1 \u03b3n \u2016\u03a3\u2212 Sn\u2016F + c2\u2016zn \u2212 \u00b5\u20162 + c3\u03b3n (3)\nwith c1 := 4B2J \u221a Jc\u0303, c2 := 4B \u221a Jc\u0303, c3 := 4B\n2Jc\u03032, and c\u0303 := supV,k \u2016\u03a3\u22121\u2016F < \u221e, where we applied the triangle inequality, the CBS (Cauchy-Bunyakovskii-Schwarz) inequality, and \u2016ab>\u2016F = \u2016a\u20162\u2016b\u20162. The boundedness of kernel k with the Jensen inequality implies that\n\u2016z\u0304n\u201622 = \u2016 1\nn n\u2211 i=1 zi\u201622 \u2264 1 n n\u2211 i=1 \u2016zi\u201622 = 1 n n\u2211 i=1 \u2016(k(xi,vj)\u2212 k(yi,vj))Jj=1\u201622 (4)\n= 1\nn n\u2211 i=1 J\u2211 j=1 [k(xi,vj)\u2212 k(yi,vj)]2\n\u2264 2 n n\u2211 i=1 J\u2211 j=1 k2(xi,vj) + k 2(yi,vj) \u2264 4B2J, (5)\n\u2016\u00b5(V)\u201622 = J\u2211 j=1 (Exy [k(x,vj)\u2212 k(y,vj)])2 \u2264 J\u2211 j=1 Exy [k(x,vj)\u2212 k(y,vj)]2 \u2264 4B2J. (6)\nTaking sup in (3), we get\nsup V,k \u2223\u2223z>n (Sn + \u03b3nI)\u22121zn \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223 \u2264 c1\u03b3n supV,k \u2016\u03a3\u2212 Sn\u2016F + c2 supV,k \u2016zn \u2212 \u00b5\u20162 + c3\u03b3n."}, {"heading": "D.3 Empirical process bound on z\u0304n", "text": "Recall that z\u0304n (V) = 1n \u2211n i=1 zi (V) \u2208 RJ , zi (V) = (k(xi,vj) \u2212 k(yi,vj))Jj=1 \u2208 RJ , \u00b5(V) = (Exy [k(x,vj)\u2212 k(y,vj)])Jj=1; thus\nsup V sup k\u2208K \u2016z\u0304n(V)\u2212 \u00b5(V)\u20162 = sup V sup k\u2208K sup b\u2208B(1,0) \u3008b, z\u0304n(V)\u2212 \u00b5(V)\u30092\nusing that \u2016a\u20162 = supb\u2208B(1,0) \u3008a,b\u30092. Let us bound the argument of the supremum: \u3008b, z\u0304n(V)\u2212 \u00b5(V)\u30092 \u2264 J\u2211 j=1 |bj | \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 [k(xi,vj)\u2212 k(yi,vj)]\u2212 Exy [k(x,vj)\u2212 k(y,vj)] \u2223\u2223\u2223\u2223\u2223 \u2264\nJ\u2211 j=1 |bj | (\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 k(xi,vj)\u2212 Exk(x,vj) \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 k(yi,vj)\u2212 Eyk(y,vj) \u2223\u2223\u2223\u2223\u2223 )\n\u2264 \u221a J sup\nv\u2208X sup k\u2208K \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 k(xi,v)\u2212 Exk(x,v) \u2223\u2223\u2223\u2223\u2223+\u221aJ supv\u2208X supk\u2208K \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 k(yi,v)\u2212 Eyk(y,v) \u2223\u2223\u2223\u2223\u2223 = \u221a J \u2016Pn \u2212 P\u2016F1 + \u221a J \u2016Qn \u2212Q\u2016F1 (7)\nby the triangle inequality and exploiting that \u2016b\u20161 \u2264 \u221a J \u2016b\u20162 \u2264 \u221a J with b \u2208 B(1,0). Thus, we have\nsup V sup k\u2208K \u2016z\u0304n(V)\u2212 \u00b5(V)\u20162 \u2264\n\u221a J \u2016Pn \u2212 P\u2016F1 + \u221a J \u2016Qn \u2212Q\u2016F1 ."}, {"heading": "D.4 Empirical process bound on Sn", "text": "Noting that\n\u03a3(V) = Exy [ z(V)z>(V) ] \u2212 \u00b5(V)\u00b5>(V), Sn(V) = 1\nn n\u2211 a=1 za(V)z>a (V)\u2212 1 n(n\u2212 1) n\u2211 a=1 \u2211 b6=a zaz T b ,\nExy [ z(V)z>(V) ] = Exy\n[ 1\nn n\u2211 a=1 za(V)z>a (V)\n] , \u00b5(V)\u00b5>(V) = Exy  1 n(n\u2212 1) n\u2211 a=1 \u2211 b6=a za(V)zTb (V)  , we bound our target quantity as\n\u2016Sn(V)\u2212\u03a3(V)\u2016F \u2264 \u2225\u2225\u2225\u2225\u2225 1n n\u2211 a=1 za(V)z>a (V)\u2212 Exy [ z(V)z>(V) ]\u2225\u2225\u2225\u2225\u2225 F + \u2225\u2225\u2225\u2225\u2225\u2225 1n(n\u2212 1) n\u2211 a=1 \u2211 b 6=a za(V)zTb (V)\u2212 \u00b5(V)\u00b5>(V) \u2225\u2225\u2225\u2225\u2225\u2225 F\n=: (\u22171) + (\u22172). (8)\n(\u22172) = \u2225\u2225\u2225\u2225\u2225\u2225 1n n\u2211 a=1 za(V)  1 n\u2212 1 \u2211 b6=a z>b (V) \u2212 \u00b5(V)\u00b5>(V) \u2225\u2225\u2225\u2225\u2225\u2225 F\n\u2264 \u2225\u2225\u2225\u2225\u2225\u2225 1n n\u2211 a=1 za(V)  1 n\u2212 1 \u2211 b 6=a z>b (V)\u2212 \u00b5>(V) \u2225\u2225\u2225\u2225\u2225\u2225 F + \u2225\u2225\u2225\u2225\u2225 ( 1 n n\u2211 a=1 za(V)\u2212 \u00b5(V) ) \u00b5>(V) \u2225\u2225\u2225\u2225\u2225 F\n\u2264 \u2225\u2225\u2225\u2225\u2225\u2225 ( 1 n n\u2211 a=1 za(V) )( 1 n\u2212 1 n\u2211 b=1 zb(V)\u2212 \u00b5(V) )>\u2225\u2225\u2225\u2225\u2225\u2225 F + \u2225\u2225\u2225\u2225\u2225 ( 1 n n\u2211 a=1 za(V) ) z>a (V) n\u2212 1 \u2225\u2225\u2225\u2225\u2225 F\n+ \u2225\u2225\u2225\u2225\u2225 ( 1 n n\u2211 a=1 za(V)\u2212 \u00b5(V) ) \u00b5>(V) \u2225\u2225\u2225\u2225\u2225 F\n= \u2016z\u0304n(V)\u20162 \u2225\u2225\u2225\u2225\u2225 1n\u2212 1 n\u2211 b=1 zb(V)\u2212 \u00b5(V) \u2225\u2225\u2225\u2225\u2225 2 + 1 n\u2212 1 \u2016z\u0304n(V)\u20162 \u2016za(V)\u20162 + \u2016z\u0304n(V)\u2212 \u00b5(V)\u20162 \u2016\u00b5(V)\u20162\n\u2264 2B \u221a J\n( n\nn\u2212 1 \u2016z\u0304n \u2212 \u00b5(V)\u20162 +\n2B \u221a J\nn\u2212 1\n) + 1\nn\u2212 1 4B2J + 2B\n\u221a J \u2016z\u0304n(V)\u2212 \u00b5(V)\u20162\n= 8B2J\nn\u2212 1 + 2B\n\u221a J 2n\u2212 1 n\u2212 1 \u2016z\u0304n \u2212 \u00b5(V)\u20162 (9)\nusing the triangle inequality, the sub-additivity of sup, \u2225\u2225abT\u2225\u2225\nF = \u2016a\u20162 \u2016b\u20162, \u2016z\u0304n(V)\u20162 \u2264 2B \u221a J , \u2016za(V)\u20162 \u2264\n2B \u221a J [see Eq. (5)] and\u2225\u2225\u2225\u2225\u2225 1n\u2212 1 n\u2211 b=1 zb(V)\u2212 \u00b5(V) \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225 nn\u2212 1 z\u0304n \u2212 nn\u2212 1\u00b5(V) + 1n\u2212 1\u00b5(V) \u2225\u2225\u2225\u2225 2 \u2264 n n\u2212 1 \u2016z\u0304n \u2212 \u00b5(V)\u20162 + 1 n\u2212 1 \u2016\u00b5(V)\u20162 with Eq. (6). Considering the first term in Eq. (8)\u2225\u2225\u2225\u2225\u2225 1n n\u2211 a=1 za(V)z>a (V)\u2212 Exy [ z(V)z>(V) ]\u2225\u2225\u2225\u2225\u2225 F = sup B\u2208B(1,0) \u2329 B, 1 n n\u2211 a=1 za(V)z>a (V)\u2212 Exy [ z(V)z>(V) ]\u232a F\n\u2264 sup B\u2208B(1,0) J\u2211 i,j=1 |Bij | \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 [k(xa,vi)\u2212 k(ya,vi)][k(xa,vj)\u2212 k(ya,vj)]\u2212 Exy ([k(x,vi)\u2212 k(y,vi)] [k(x,vj)\u2212 k(y,vj)]) \u2223\u2223\u2223\u2223\u2223 \u2264 sup\nB\u2208B(1,0) J\u2211 i,j=1 |Bij | (\u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(xa,vi)k(xa,vj)\u2212 Ex [k(x,vi)k(x,vj)] \u2223\u2223\u2223\u2223\u2223 +\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(xa,vi)k(ya,vj)\u2212 Exy [k(x,vi)k(y,vj)] \u2223\u2223\u2223\u2223\u2223 +\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(ya,vi)k(xa,vj)\u2212 Exy [k(y,vi)k(x,vj)] \u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(ya,vi)k(ya,vj)\u2212 Ey [k(y,vi)k(y,vj)] \u2223\u2223\u2223\u2223\u2223 )\n\u2264J sup v,v\u2032\u2208X sup k\u2208K \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(xa,v)k(xa,v \u2032)\u2212 Ex [k(x,v)k(x,v\u2032)] \u2223\u2223\u2223\u2223\u2223\n+ 2J sup v,v\u2032\u2208X sup k\u2208K \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(xa,v)k(ya,v \u2032)\u2212 Exy [k(x,v)k(y,v\u2032)] \u2223\u2223\u2223\u2223\u2223 + J sup\nv,v\u2032\u2208X sup k\u2208K \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 k(ya,v)k(ya,v \u2032)\u2212 Ey [k(y,v)k(y,v\u2032)] \u2223\u2223\u2223\u2223\u2223 by exploiting that \u2016A\u2016F = supB\u2208B(1,0) \u3008B,A\u3009F , and \u2211J i,j=1 |Bij | \u2264 J \u2016B\u2016F \u2264 J with B \u2208 B(1,0). Using the bounds obtained for the two terms of Eq. (8), we get\nsup V sup k\u2208K \u2016Sn(V)\u2212\u03a3(V)\u2016F \u2264\n\u2264 8B 2J\nn\u2212 1 + 2B\n\u221a J 2n\u2212 1 n\u2212 1 sup V sup k\u2208K \u2016z\u0304n \u2212 \u00b5(V)\u20162 + J ( \u2016Pn \u2212 P\u2016F2 + 2 \u2016(P \u00d7Q)n \u2212 (P \u00d7Q)\u2016F3 + \u2016Qn \u2212Q\u2016F2 ) .\n(10)"}, {"heading": "D.5 Bounding by concentration and the VC property", "text": "By combining Eqs. (3), (7) and (10)\nsup V sup k \u2223\u2223z\u0304>n (Sn + \u03b3nI)\u22121z\u0304n \u2212 \u00b5>\u03a3\u22121\u00b5\u2223\u2223 \u2264 \u2264 c\u03041 \u03b3n [ 8B2J n\u2212 1 + 2B \u221a J 2n\u2212 1 n\u2212 1 \u221a J ( \u2016Pn \u2212 P\u2016F1 + \u2016Qn \u2212Q\u2016F1\n) +J ( \u2016Pn \u2212 P\u2016F2 + 2 \u2016(P \u00d7Q)n \u2212 (P \u00d7Q)\u2016F3 + \u2016Qn \u2212Q\u2016F2\n) ] +c\u03042 \u221a J ( \u2016Pn \u2212 P\u2016F1 + \u2016Qn \u2212Q\u2016F1 ) + c\u03043\u03b3n\n= ( \u2016Pn \u2212 P\u2016F1 + \u2016Qn \u2212Q\u2016F1 )( 2 \u03b3n c\u03041BJ 2n\u2212 1 n\u2212 1 + c\u03042 \u221a J ) + c\u03043\u03b3n\n+ c\u03041 \u03b3n J [ \u2016Pn \u2212 P\u2016F2 + \u2016Qn \u2212Q\u2016F2 + 2 \u2016(P \u00d7Q)n \u2212 (P \u00d7Q)\u2016F3 ] + 8 \u03b3n c\u03041B 2J n\u2212 1 . (11)\nApplying Lemma 3 with \u03b45 , we get the statement with a union bound.\nLemma 3 (Concentration of the empirical process for uniformly bounded separable Carath\u00e9odory VC classes). Let F be\n1. VC-subgraph class ofM\u2192 R functions with VC index VC(F),"}, {"heading": "2. a uniformly bounded (\u2016f\u2016L\u221e(M) \u2264 K <\u221e,\u2200f \u2208 F) separable Carath\u00e9odory family.", "text": "Let Q be a probability measure, and let Qn = 1n \u2211n i=1 \u03b4xi be the corresponding empirical measure. Then for any \u03b4 \u2208 (0, 1) with probability at least 1\u2212 \u03b4\n\u2016Qn \u2212Q\u2016F \u2264 16 \u221a\n2K\u221a n\n[ 2 \u221a log [ C \u00d7 V C(F)(16e)V C(F) ] + \u221a 2\u03c0[V C(F)\u2212 1]\n2\n] +K \u221a 2 log ( 1 \u03b4 ) n\nwhere the universal constant C is associated according to Lemma 7(iv).\nProof. Notice that g(x1, . . . , xn) = \u2016Qn \u2212Q\u2016F satisfies the bounded difference property with b = 2K n [see Eq. (18)]:\n|g(x1, . . . ,xn)\u2212 g ( x1, . . . ,xj ,x \u2032 j ,xj+1, . . . ,xn ) |\n\u2264 \u2223\u2223\u2223\u2223\u2223supf\u2208F \u2223\u2223\u2223Qf \u2212 1 n n\u2211 i=1 f(xi) \u2223\u2223\u2223\u2212 sup f\u2208F \u2223\u2223\u2223Qf \u2212 1 n n\u2211 i=1 f(xi) + 1 n [ f(xj)\u2212 f(x\u2032j) ] \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2264 1 n sup f\u2208F |f(xj)\u2212 f(x\u2032j)| \u2264 1 n ( sup f\u2208F |f(xj)|+ sup f\u2208F |f(x\u2032j)| ) \u2264 2K n .\nHence, applying Lemma 8, and using symmetrization Steinwart and Christmann (2008) (Prop. 7.10) for the uniformly bounded separable Carath\u00e9odory F class, for arbitrary \u03b4 \u2208 (0, 1) with probability at least 1\u2212 \u03b4\n\u2016Qn \u2212Q\u2016F \u2264 Ex1:n \u2016Qn \u2212Q\u2016F +K\n\u221a 2 log ( 1 \u03b4 ) n\n\u2264 2Ex1:nR(F , x1:n) +K\n\u221a 2 log ( 1 \u03b4 ) n .\nBy the Dudley entropy bound Bousquet (2003) [see Eq. (4.4); diam(F , L2(M,Qn)) \u2264 2 supf\u2208F \u2016f\u2016L2(M,Qn) \u2264 2 supf\u2208F \u2016f\u2016L\u221e(M) \u2264 2K < \u221e], Lemma 7(iv) [with F \u2261 K, q = 2 M = Qn] and the monotone decreasing property of the covering number, one arrives at R(F , x1:n) \u2264 8 \u221a\n2\u221a n \u222b 2K 0 \u221a logN(r,F , L2(M,Qn))dr\n\u2264 8 \u221a\n2\u221a n [\u222b K 0 \u221a logN(r,F , L2(M,Qn))dr +K \u221a logN(K,F , L2(M,Qn)) ]\n\u2264 8 \u221a\n2K\u221a n [\u222b 1 0 \u221a logN(rK,F , L2(M,Qn))dr + \u221a logN(K,F , L2(M,Qn)) ]\n\u2264 8 \u221a\n2K\u221a n [\u222b 1 0 \u221a log [ a1 ( 1 r )a2] dr + \u221a log(a1) ] = 8 \u221a 2K\u221a n [ 2 \u221a log(a1) + \u222b 1 0 \u221a a2 log ( 1 r ) dr ]\n= 8 \u221a\n2K\u221a n\n[ 2 \u221a\nlog(a1) + \u221a \u03c0a2 2\n] ,\nwhere a1 := C \u00d7 V C(F)(16e)V C(F), a2 := 2[V C(F) \u2212 1] and \u222b 1 0 \u221a log ( 1 r ) dr = \u222b\u221e 0 t 1 2 e\u2212tdt = \u0393 ( 3 2 ) = \u221a \u03c0 2 .\nLemma 4 (Properties of Fi from K).\n1. Uniform boundedness of Fi-s [see Eqs. (1)-(2)]: If K is uniformly bounded, i.e., \u2203B < \u221e such that supk\u2208K sup(x,y)\u2208X 2 |k(x,y)| \u2264 B; then F1, F2 and F3 [Eqs. (1)-(2)] are also uniformly bounded with B, B2, B2 constants, respectively. That is, supk\u2208K,v\u2208X |k(x,v)| \u2264 B, supk\u2208K,(v,v\u2032)\u2208X 2 |k(x,v)k(x,v\u2032)| \u2264 B2, supk\u2208K,(v,v\u2032)\u2208X 2 |k(x,v)k(y,v\u2032)| \u2264 B2.\n2. Separability ofFi: sinceF1, F2 andF3 is parameterized by \u0398 = K\u00d7X ,K\u00d7X 2,K\u00d7X 2, separability of K implies that of \u0398.\n3. Measurability of Fi: \u2200k \u2208 K is measurable, then the elements of Fi (i = 1, 2, 3) are also measurable."}, {"heading": "E Example kernel families", "text": "Below we give examples for K kernel classes for which the associated Fi-s are VC-subgraph and uniformly bounded separable Carath\u00e9odory families. The VC property will be a direct consequence of the VC indices of finite-dimensional function classes and preservation theorems (see Lemma 7); for a nice example application see Srebro and Ben-David (2006) (Section 5) who study the pseudo-dimension of (x,y) 7\u2192 k(x,y) kernel classes, for different Gaussian families. We take these Gaussian classes (isotropic, full) and use the preservation trick to bound the VC indices of the associated Fi-s. Lemma 5 (Fi-s are VC-subgraph and uniformly bounded separable Carath\u00e9odory families for isotropic Gaussian\nkernel). Let K = { k\u03c3 : (x,y) \u2208 X \u00d7 X \u2286 Rd \u00d7 Rd 7\u2192 e\u2212 \u2016x\u2212y\u201622 2\u03c32 : \u03c3 > 0 } . Then the F1, F2, F3 classes [see\nEqs. (1)-(2)] associated to K are\n\u2022 VC-subgraphs with indices V C(F1) \u2264 d+ 4, V C(F2) \u2264 d+ 4, V C(F3) \u2264 2d+ 4, and\n\u2022 uniformly bounded separable Carath\u00e9odory families, with \u2016f\u2016L\u221e(M) \u2264 1 for all f \u2208 {F1,F2,F3}.4\nProof. VC subgraph property:\n\u2022 F1: Consider the function class G = { x 7\u2192 \u2016x\u2212v\u2016 2 2 2\u03c32 = 1 2\u03c32 ( \u2016x\u201622 \u2212 2 \u3008x,v\u30092 + \u2016v\u2016 2 2 ) : \u03c3 > 0,v \u2208 X } \u2286\nL0(Rd). G \u2286 G\u0303 := span ( x 7\u2192 \u2016x\u201622 , {x 7\u2192 xi}di=1 ,x 7\u2192 1 ) vector space, dim(G) \u2264 d + 2. Thus by Lemma 7(i)-(ii), G is VC with V C(G) \u2264 d+4; applying Lemma 7(iii) with \u03c6(z) = e\u2212z , F1 = \u03c6\u25e6G is also VC with index V C(F1) \u2264 d+ 4.\n\u2022 F2: Since F2 = { x 7\u2192 k(x,v)k(x,v\u2032) = e\u2212 \u2016x\u2212v\u201622+\u2016x\u2212v\u2032\u201622 2\u03c32 : \u03c3 > 0,v \u2208 X ,v\u2032 \u2208 X } ,\nand { x 7\u2192 \u2016x\u2212v\u201622+\u2016x\u2212v\u2032\u201622\n2\u03c32 : \u03c3 > 0,v \u2208 X ,v \u2032 \u2208 X\n} \u2286 S =\nspan ( x 7\u2192 \u2016x\u201622 , {x 7\u2192 xi}di=1 ,x 7\u2192 1 ) , V C(F2) \u2264 d+ 4.\n\u2022 F3: Since F3 = { (x,y) 7\u2192 k(x,v)k(y,v\u2032) = e\u2212 \u2016x\u2212v\u20162+\u2016y\u2212v\u2032\u201622 2\u03c32 = e\u2212 \u2016[x;y]\u2212[v;v\u2032]\u201622 2\u03c32 : \u03c3 > 0,v \u2208 Rd,v\u2032 \u2208 Rd } ,\nfrom the result on F1 we get that V C(F3) \u2264 2d+ 4.\nUniformly bounded, separable Carath\u00e9odory family: The result follows from Lemma 4 by noting that |k(x,y)| \u2264 1 =: B, (x,y) 7\u2192 e\u2212 \u2016x\u2212y\u201622 2\u03c32 is continuous (\u2200\u03c3 > 0), R+ is separable, and the (\u03c3,v) 7\u2192 e\u2212 \u2016x\u2212v\u201622 2\u03c32 , (\u03c3,v,v\u2032) 7\u2192 e\u2212 \u2016x\u2212v\u201622 2\u03c32 e\u2212 \u2016x\u2212v\u2032\u201622 2\u03c32 , (\u03c3,v,v\u2032) 7\u2192 e\u2212 \u2016x\u2212v\u201622 2\u03c32 e\u2212 \u2016y\u2212v\u2032\u201622 2\u03c32 mappings are continuous (\u2200x,y \u2208 X ).\nLemma 6 (Fi-s are VC-subgraph and uniformly bounded separable Carath\u00e9odory families for full Gaussian kernel). Let K = {kA : (x,y) \u2208 X \u00d7 X \u2286 Rd \u00d7 Rd 7\u2192 e\u2212(x\u2212y)\n>A(x\u2212y) : A 0}. Then the F1, F2, F3 classes [see Eqs. (1)-(2)] associated to K are\n\u2022 VC-subgraphs with indices V C(F1) \u2264 d(d+1)2 + d + 2, V C(F2) \u2264 d(d+1)+2 2 + d + 2, V C(F3) \u2264 d(d+ 1) + 2d+ 3,\n\u2022 uniformly bounded separable Carath\u00e9odory families, with \u2016f\u2016L\u221e(M) \u2264 1 for all f \u2208 {F1,F2,F3}.4\nProof. We prove the VC index values; the rest is essentially identical to the proof of Lemma 5.\n\u2022 F1: Using that G = { x 7\u2192 (x\u2212 v)>A(x\u2212 v) : A 0,v \u2208 X } \u2286 S :=\nspan ({x 7\u2192 xixj}1\u2264i\u2264j\u2264d, {x 7\u2192 xi}1\u2264i\u2264d,x 7\u2192 1), we have V C(F1) \u2264 V C(G) \u2264 dim(S) + 2 \u2264 d(d+1)\n2 + d+ 3. \u2022 F2: SinceF2 = { x 7\u2192 k(x,v)k(x,v\u2032) = e\u2212 [ (x\u2212v)>A(x\u2212v)+(x\u2212v\u2032) > A(x\u2212v\u2032) ] : A 0,v \u2208 X ,v\u2032 \u2208 X } ,\nand { (x,y) 7\u2192 (x\u2212 v)>A(x\u2212 v) + (x\u2212 v\u2032)>A (x\u2212 v\u2032) } \u2286 S\n:= span ({x 7\u2192 xixj}1\u2264i\u2264j\u2264d, {x 7\u2192 xi}1\u2264i\u2264d,x 7\u2192 1) ,\nwe have V C(F2) \u2264 V C(S) = dim(S) + 2 \u2264 d(d+1)2 + d+ 3.\n4M = X for F1 and F2, andM = X 2 in case of F3.\n\u2022 F3: Exploiting that F3 = { (x,y) 7\u2192 k(x,v)k(y,v\u2032) = e\u2212 [ (x\u2212v)>A(x\u2212v)+(y\u2212v\u2032) > B(y\u2212v\u2032) ] : A 0,B 0,v \u2208 X ,v\u2032 \u2208 X } ,\nand { (x,y) 7\u2192 (x\u2212 v)>A(x\u2212 v) + (y \u2212 v\u2032)>B(y \u2212 v\u2032) }\n\u2286 S := span ({(x,y) 7\u2192 xixj}1\u2264i\u2264j\u2264d, {(x,y) 7\u2192 xi}1\u2264i\u2264d, (x,y) 7\u2192 1, {(x,y) 7\u2192 yiyj}1\u2264i\u2264j\u2264d, {(x,y) 7\u2192 yi}1\u2264i\u2264d), we have V C(F3) \u2264 V C(S) = dim(S) + 2 \u2264 d(d+ 1) + 2d+ 3."}, {"heading": "F Proof of proposition 1", "text": "Recall Proposition 1: Proposition 1 (Lower bound on ME test power). Let K be a uniformly bounded (i.e., \u2203B < \u221e such that supk\u2208K sup(x,y)\u2208X 2 |k(x,y)| \u2264 B) family of k : X \u00d7 X \u2192 R measurable kernels. Let V be a collection in which each element is a set of J test locations. Assume that c\u0303 := supV\u2208V,k\u2208K \u2016\u03a3\u22121\u2016F <\u221e. For large n, the test power P ( \u03bb\u0302n \u2265 T\u03b1 ) of the ME test satisfies P ( \u03bb\u0302n \u2265 T\u03b1 ) \u2265 L(\u03bbn) where\nL(\u03bbn) := 1\u2212 2e\u2212\u03be1(\u03bbn\u2212T\u03b1) 2/n \u2212 2e\u2212\n[\u03b3n(\u03bbn\u2212T\u03b1)(n\u22121)\u2212\u03be2n] 2\n\u03be3n(2n\u22121)2 \u2212 2e\u2212[(\u03bbn\u2212T\u03b1)/3\u2212c3n\u03b3n] 2\u03b32n/\u03be4 ,\nand c3, \u03be1, . . . \u03be4 are positive constants depending on only B, J and c\u0303. The parameter \u03bbn := n\u00b5>\u03a3\u22121\u00b5 is the population counterpart of \u03bb\u0302n := nz>n (Sn + \u03b3nI) \u22121 zn where \u00b5 = Exy[z1] and \u03a3 = Exy[(z1\u2212\u00b5)(z1\u2212\u00b5)>]."}, {"heading": "For large n, L(\u03bbn) is increasing in \u03bbn.", "text": ""}, {"heading": "F.1 Proof", "text": "By (3), we have\n|\u03bb\u0302n \u2212 \u03bbn| \u2264 c1n\n\u03b3n \u2016\u03a3\u2212 Sn\u2016F + c2n\u2016zn \u2212 \u00b5\u20162 + c3n\u03b3n. (12)\nWe will bound each of the three terms in (12)."}, {"heading": "Bounding \u2016zn \u2212 \u00b5\u20162 (second term in (12))", "text": "Let g(x,y,v) := k(x,v)\u2212k(y,v). Define v\u2217 := arg maxv\u2208{v1,...,vJ} \u2223\u2223 1 n \u2211n i=1 g(xi,yi,v)\u2212 Exy [g(x,y,v)] \u2223\u2223. Define Gi := g(xi,yi,v\u2217).\n\u2016zn \u2212 \u00b5\u20162 = sup b\u2208B(1,0) \u3008b, zn \u2212 \u00b5\u30092\n\u2264 sup b\u2208B(1,0) J\u2211 j=1 |bj | \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 [k(xi,vj)\u2212 k(yi,vj)]\u2212 Exy [k(x,vj)\u2212 k(y,vj)] \u2223\u2223\u2223\u2223\u2223 = sup\nb\u2208B(1,0) J\u2211 j=1 |bj | \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 g(xi,yi,vj)\u2212 Exy [g(x,y,vj)] \u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 Gi \u2212 Exy [G1] \u2223\u2223\u2223\u2223\u2223 supb\u2208B(1,0) J\u2211 j=1 |bj |\n\u2264 \u221a J \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 Gi \u2212 Exy [G1] \u2223\u2223\u2223\u2223\u2223 supb\u2208B(1,0) \u2016b\u20162 = \u221a J\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 Gi \u2212 Exy [G1] \u2223\u2223\u2223\u2223\u2223 ,\nwhere we used the fact that \u2016b\u20161 \u2264 \u221a J\u2016b\u20162. It can be seen that \u22122B \u2264 Gi \u2264 2B because\nGi = k(xi,v \u2217)\u2212 k(yi,v\u2217) \u2264 |k(xi,v\u2217)|+ |k(yi,v\u2217)| \u2264 2B. Using Hoeffding\u2019s inequality (Lemma 9) to bound \u2223\u2223 1 n \u2211n i=1Gi \u2212 Exy[G1] \u2223\u2223, we have P (nc2\u2016zn \u2212 \u00b5\u20162 \u2264 \u03b1) \u2265 1\u2212 2 exp ( \u2212 \u03b1 2\n8B2c22Jn\n) . (13)\nBounding first (\u2016\u03a3\u2212 Sn\u2016F ) and third terms in (12) Let \u03b7(vi,vj) := \u2223\u2223 1 n \u2211n a=1 g(xa,ya,vi)g(xa,ya,vj)\u2212 Exy [g(x,y,vi)g(x,y,vj)] \u2223\u2223. Define (v\u22171,v\u22172) = arg max(v(1),v(2))\u2208{(vi,vj)}Ji,j=1 \u03b7(v (1),v(2)). Define Hi := g(xi,yi,v\u22171)g(xi,yi,v \u2217 2). By (8), we have\n\u2016Sn \u2212\u03a3\u2016F \u2264 (\u22171) + (\u22172), (\u22171) = \u2225\u2225\u2225\u2225 1n n\u2211 a=1 zaz > a \u2212 Exy[z1z>1 ] \u2225\u2225\u2225\u2225 F ,\n(\u22172) = 8B2J\nn\u2212 1 + 2Bk\n\u221a J 2n\u2212 1 n\u2212 1 \u2016zn \u2212 \u00b5\u20162.\nWe can upper bound (\u22172) by applying Hoeffding\u2019s inequality to bound \u2016zn \u2212 \u00b5\u20162 giving P ( c1n\n\u03b3n (\u22172) \u2264 \u03b1\n) \u2265 1\u2212 2 exp ( \u2212 (\u03b1\u03b3n \u2212 \u03b1\u03b3nn+ 8B 2c1Jn) 2\n32B4c21J 2n(2n\u2212 1)2\n) . (14)\nWe can upper bound (\u22171) with\n(\u22171) = sup B\u2208B(1,0)\n\u2329 B, 1\nn n\u2211 a=1 zaz > a \u2212 Exy[z1z>1 ] \u232a F\n\u2264 sup B\u2208B(1,0) J\u2211 i=1 J\u2211 j=1 |Bij | \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 g(xa,ya,vi)g(xa,ya,vj)\u2212 Exy [g(x,y,vi)g(x,y,vj)] \u2223\u2223\u2223\u2223\u2223 \u2264\n\u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 Ha \u2212 Exy [H1] \u2223\u2223\u2223\u2223\u2223 supB\u2208B(1,0) J\u2211 i=1 J\u2211 j=1 |Bij |\n\u2264 J \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 Ha \u2212 Exy [H1] \u2223\u2223\u2223\u2223\u2223 supB\u2208B(1,0) \u2016B\u2016F = J \u2223\u2223\u2223\u2223\u2223 1n n\u2211 a=1 Ha \u2212 Exy [H1] \u2223\u2223\u2223\u2223\u2223 , where we used the fact that \u2211J i=1 \u2211J j=1 |Bij | \u2264 J\u2016B\u2016F . It can be seen that \u22124B2 \u2264 Ha \u2264 4B2. Using\nHoeffding\u2019s inequality (Lemma 9) to bound \u2223\u2223 1 n \u2211n a=1Ha \u2212 Exy [H1] \u2223\u2223, we have P ( c1n\n\u03b3n (\u22171) \u2264 \u03b1\n) \u2265 1\u2212 2 exp ( \u2212 \u03b1\n2\u03b32n 32B4J2c21n\n) , (15)\nimplying that\nP ( c1n\n\u03b3n (\u22171) + c3n\u03b3n \u2264 \u03b1\n) \u2265 1\u2212 2 exp ( \u2212 (\u03b1\u2212 c3n\u03b3n) 2 \u03b32n\n32B4J2c21n\n) . (16)\nApplying a union bound on (13), (14), and (16) with t = \u03b1/3, we can conclude that P (\u2223\u2223\u2223\u03bb\u0302n \u2212 \u03bbn\u2223\u2223\u2223 \u2264 t) \u2265 P(c1n\n\u03b3n \u2016\u03a3\u2212 Sn\u2016F + c2n\u2016zn \u2212 \u00b5\u20162 + c3n\u03b3n \u2264 t ) \u2265 1\u2212 2 exp ( \u2212 t 2\n32 \u00b7 8B2c22Jn\n) \u2212 2 exp ( \u2212 (t\u03b3nn\u2212 t\u03b3n \u2212 24B 2c1Jn) 2\n32 \u00b7 32B4c21J2n(2n\u2212 1)2\n) \u2212 2 exp ( \u2212 (t/3\u2212 c3n\u03b3n) 2 \u03b32n\n32B4J2c21n\n) .\nA rearrangement yields P ( \u03bb\u0302n \u2265 T\u03b1 )\n\u2265 1\u2212 2 exp ( \u2212 (\u03bbn \u2212 T\u03b1) 2\n32 \u00b7 8B2c22Jn\n) \u2212 2 exp ( \u2212 (\u03b3n(\u03bbn \u2212 T\u03b1)(n\u2212 1)\u2212 24B 2c1Jn) 2\n32 \u00b7 32B4c21J2n(2n\u2212 1)2\n) \u2212 2 exp ( \u2212 ((\u03bbn \u2212 T\u03b1)/3\u2212 c3n\u03b3n)\n2 \u03b32n 32B4J2c21n\n) .\nDefine \u03be1 := 132\u00b78B2c22J , \u03be2 := 24B 2c1J, \u03be3 := 3 2 \u00b7 32B4c21J2, \u03be4 := 32B4J2c21. We have\nP ( \u03bb\u0302n \u2265 T\u03b1 ) \u2265 1\u2212 2 exp ( \u2212\u03be1(\u03bbn \u2212 T\u03b1) 2\nn\n) \u2212 2 exp ( \u2212 (\u03b3n(\u03bbn \u2212 T\u03b1)(n\u2212 1)\u2212 \u03be2n) 2\n\u03be3n(2n\u2212 1)2\n) \u2212 2 exp ( \u2212 ((\u03bbn \u2212 T\u03b1)/3\u2212 c3n\u03b3n)\n2 \u03b32n \u03be4\n) ."}, {"heading": "G External lemmas", "text": "In this section we detail some external lemmas used in our proof. Lemma 7 (properties of VC classes, see page 141, 146-147 in van der Vaart and Wellner (2000) and page 160-161 in Kosorok (2008)).\n(i) Monotonicity: G \u2286 G\u0303 \u2286 L0(M)\u21d2 V C(G) \u2264 V C(G\u0303).\n(ii) Finite-dimensional vector space: if G is a finite-dimensional vector space of measurable functions, then V C(G) \u2264 dim(G) + 2.\n(iii) Composition with monotone function: If G is VC and \u03c6 : R\u2192 R is monotone, then for \u03c6 \u25e6 G := {\u03c6 \u25e6 g : g \u2208 G}, V C(\u03c6 \u25e6 G) \u2264 V C(G).\n(iv) The r-covering number of a VC class grows only polynomially in 1r : Let F be VC on the domainM with measurable envelope F (|f(m)| \u2264 F (m), \u2200m \u2208 M, f \u2208 F). Then for any q \u2265 1 and M probability measure for which \u2016F\u2016Lq(M,M) > 0\nN ( r \u2016F\u2016Lq(M,M) ,F , L q(M,M) ) \u2264 C \u00d7 V C(F)(16e)V C(F) ( 1\nr\n)q[V C(F)\u22121] (17)\nfor any r \u2208 (0, 1) with a universal constant C. Lemma 8 (McDiarmid\u2019s inequality). LetX1, . . . , Xn \u2208M be independent random variables and let g :Mn \u2192 R be a function such that the\nsup x1,...,xn,x\u2032j\u2208M \u2223\u2223g(x1, . . . ,xn)\u2212 g (x1, . . . ,xj ,x\u2032j ,xj+1, . . . ,xn)\u2223\u2223 \u2264 b (18) bounded difference property holds. Then for arbitrary \u03b4 \u2208 (0, 1)\nP g(X1, . . . , Xn) \u2264 E[g(X1, . . . , Xn)] + b \u221a log ( 1 \u03b4 ) n\n2  \u2265 1\u2212 \u03b4. Lemma 9 (Hoeffding\u2019s inequality). Let X1, . . . , Xn be i.i.d. random variables with P(a \u2264 Xi \u2264 b) = 1. Let X := 1n \u2211n i=1Xi. Then,\nP (\u2223\u2223X \u2212 E[X]\u2223\u2223 \u2264 t) \u2265 1\u2212 2 exp(\u2212 2nt2\n(b\u2212 a)2\n) .\nEquivalently, for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, it holds that\u2223\u2223X \u2212 E[X]\u2223\u2223 \u2264 b\u2212 a\u221a 2n \u221a log(2/\u03b4)."}], "references": [{"title": "On a new multivariate two-sample test", "author": ["L. Baringhaus", "C. Franz"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Baringhaus and Franz.,? \\Q2004\\E", "shortCiteRegEx": "Baringhaus and Franz.", "year": 2004}, {"title": "Theory of multivariate statistics", "author": ["M. Bilodeau", "D. Brenner"], "venue": "Springer Science & Business Media,", "citeRegEx": "Bilodeau and Brenner.,? \\Q2008\\E", "shortCiteRegEx": "Bilodeau and Brenner.", "year": 2008}, {"title": "New approaches to statistical learning theory", "author": ["O. Bousquet"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Bousquet.,? \\Q2003\\E", "shortCiteRegEx": "Bousquet.", "year": 2003}, {"title": "Fast two-sample testing with analytic representations of probability measures", "author": ["K. Chwialkowski", "A. Ramdas", "D. Sejdinovic", "A. Gretton"], "venue": null, "citeRegEx": "Chwialkowski et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 1972}, {"title": "Diagnostic measures for model criticism", "author": ["G.P. Cinzia Carota", "N.G. Polson"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Carota and Polson.,? \\Q1996\\E", "shortCiteRegEx": "Carota and Polson.", "year": 1996}, {"title": "Testing for homogeneity with kernel Fisher discriminant analysis", "author": ["M. Eric", "F.R. Bach", "Z. Harchaoui"], "venue": "In NIPS,", "citeRegEx": "Eric et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Eric et al\\.", "year": 2008}, {"title": "Kernels based tests with non-asymptotic bootstrap approaches for two-sample problems", "author": ["M. Fromont", "B. Laurent", "M. Lerasle", "P. Reynaud-Bouret"], "venue": "In COLT,", "citeRegEx": "Fromont et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fromont et al\\.", "year": 2012}, {"title": "A kernel two-sample test", "author": ["A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Sch\u00f6lkopf", "A. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Optimal kernel choice for large-scale two-sample tests", "author": ["A. Gretton", "D. Sejdinovic", "H. Strathmann", "S. Balakrishnan", "M. Pontil", "K. Fukumizu", "B.K. Sriperumbudur"], "venue": "In NIPS,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Introduction to Empirical Processes and Semiparametric Inference", "author": ["M.R. Kosorok"], "venue": null, "citeRegEx": "Kosorok.,? \\Q2008\\E", "shortCiteRegEx": "Kosorok.", "year": 2008}, {"title": "Statistical model criticism using kernel two sample tests", "author": ["J.R. Lloyd", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Lloyd and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Lloyd and Ghahramani.", "year": 2015}, {"title": "Automatic construction and Natural-Language description of nonparametric regression models", "author": ["J.R. Lloyd", "D. Duvenaud", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani"], "venue": "In AAAI,", "citeRegEx": "Lloyd et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lloyd et al\\.", "year": 2014}, {"title": "The Karolinska directed emotional faces-KDEF", "author": ["D. Lundqvist", "A. Flykt", "A. \u00d6hman"], "venue": "Technical report, ISBN 91-630-7164-9,", "citeRegEx": "Lundqvist et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lundqvist et al\\.", "year": 1998}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Principal differences analysis: Interpretable characterization of differences between distributions", "author": ["J. Mueller", "T. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "Mueller and Jaakkola.,? \\Q2015\\E", "shortCiteRegEx": "Mueller and Jaakkola.", "year": 2015}, {"title": "On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions", "author": ["A. Ramdas", "S. Jakkam Reddi", "B. P\u00f3czos", "A. Singh", "L. Wasserman"], "venue": "In AAAI,", "citeRegEx": "Ramdas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2015}, {"title": "Equivalence of distance-based and RKHS-based statistics in hypothesis testing", "author": ["D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu"], "venue": "Annals of Statistics,", "citeRegEx": "Sejdinovic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sejdinovic et al\\.", "year": 2013}, {"title": "A Hilbert space embedding for distributions", "author": ["A. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In ALT,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["N. Srebro", "S. Ben-David"], "venue": "In COLT, pages 169\u2013183,", "citeRegEx": "Srebro and Ben.David.,? \\Q2006\\E", "shortCiteRegEx": "Srebro and Ben.David.", "year": 2006}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "B. Schoelkopf", "G. Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2010}, {"title": "Universality, characteristic kernels and RKHS embedding of measures", "author": ["B.K. Sriperumbudur", "K. Fukumizu", "G.R. Lanckriet"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2011}, {"title": "Testing for equal distributions in high dimension. InterStat", "author": ["G. Sz\u00e9kely", "M. Rizzo"], "venue": null, "citeRegEx": "Sz\u00e9kely and Rizzo.,? \\Q2004\\E", "shortCiteRegEx": "Sz\u00e9kely and Rizzo.", "year": 2004}, {"title": "Weak Convergence and Empirical Processes: With Applications to Statistics (Springer Series in Statistics)", "author": ["A. van der Vaart", "J. Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner.,? \\Q2000\\E", "shortCiteRegEx": "Vaart and Wellner.", "year": 2000}, {"title": "B-test: A non-parametric, low variance kernel two-sample test", "author": ["W. Zaremba", "A. Gretton", "M. Blaschko"], "venue": "In NIPS,", "citeRegEx": "Zaremba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 10, "context": "One application for such divergence measures is to model criticism, where samples from a trained model are compared with a validation sample: in the univariate case, through the KL divergence (Cinzia Carota and Polson, 1996), or in the multivariate case, by use of the maximum mean discrepancy (MMD) (Lloyd and Ghahramani, 2015).", "startOffset": 300, "endOffset": 328}, {"referenceID": 14, "context": "An alternative, interpretable analysis of a multivariate difference in distributions may be obtained by projecting onto a discriminative direction, such that the Wasserstein distance on this projection is maximized (Mueller and Jaakkola, 2015).", "startOffset": 215, "endOffset": 243}, {"referenceID": 5, "context": "Nonparametric two-sample tests based on RKHS distances (Eric et al., 2008; Fromont et al., 2012; Gretton et al., 2012a) or energy distances (Sz\u00e9kely and Rizzo, 2004; Baringhaus and Franz, 2004) have as their test statistic an integral probability metric, the Maximum Mean Discrepancy (Gretton et al.", "startOffset": 55, "endOffset": 119}, {"referenceID": 6, "context": "Nonparametric two-sample tests based on RKHS distances (Eric et al., 2008; Fromont et al., 2012; Gretton et al., 2012a) or energy distances (Sz\u00e9kely and Rizzo, 2004; Baringhaus and Franz, 2004) have as their test statistic an integral probability metric, the Maximum Mean Discrepancy (Gretton et al.", "startOffset": 55, "endOffset": 119}, {"referenceID": 21, "context": ", 2012a) or energy distances (Sz\u00e9kely and Rizzo, 2004; Baringhaus and Franz, 2004) have as their test statistic an integral probability metric, the Maximum Mean Discrepancy (Gretton et al.", "startOffset": 29, "endOffset": 82}, {"referenceID": 0, "context": ", 2012a) or energy distances (Sz\u00e9kely and Rizzo, 2004; Baringhaus and Franz, 2004) have as their test statistic an integral probability metric, the Maximum Mean Discrepancy (Gretton et al.", "startOffset": 29, "endOffset": 82}, {"referenceID": 16, "context": ", 2012a) or energy distances (Sz\u00e9kely and Rizzo, 2004; Baringhaus and Franz, 2004) have as their test statistic an integral probability metric, the Maximum Mean Discrepancy (Gretton et al., 2012a; Sejdinovic et al., 2013).", "startOffset": 173, "endOffset": 221}, {"referenceID": 11, "context": "Lloyd and Ghahramani (2015) used this witness function to compare the model output of the Automated Statistician (Lloyd et al., 2014) with a reference sample, yielding a visual indication of where the model fails.", "startOffset": 113, "endOffset": 133}, {"referenceID": 23, "context": "Despite the differences in these analytic functions being evaluated at random locations, the analytic tests have greater power than linear time tests based on subsampled estimates of the MMD (Gretton et al., 2012b; Zaremba et al., 2013).", "startOffset": 191, "endOffset": 236}, {"referenceID": 7, "context": "Lloyd and Ghahramani (2015) used this witness function to compare the model output of the Automated Statistician (Lloyd et al.", "startOffset": 0, "endOffset": 28}, {"referenceID": 3, "context": "Our approach builds on the analytic representations of probability distributions of Chwialkowski et al. (2015), where differences in expectations of analytic functions at particular spatial or frequency locations are used to construct a two-sample test statistic, which can be computed in linear time.", "startOffset": 84, "endOffset": 111}, {"referenceID": 3, "context": "2 ME and SCF tests In this section, we review the ME and SCF tests (Chwialkowski et al., 2015) for two-sample testing. In Sec. 3, we will extend these approaches to learn features that optimize the power of these tests. Given two samples X := {xi}i=1,Y := {yi}i=1 \u2282 R independently and identically distributed (i.i.d.) according to P and Q, respectively, the goal of a two-sample test is to decide whether P is different from Q on the basis of the samples. The task is formulated as a statistical hypothesis test proposing a null hypothesis H0 : P = Q (samples are drawn from the same distribution) against an alternative hypothesis H1 : P 6= Q (the sample generating distributions are different). A test calculates a test statistic \u03bb\u0302n from X and Y, and rejectsH0 if \u03bb\u0302n exceeds a predetermined test threshold (critical value). The threshold is given by the (1\u2212 \u03b1)-quantile of the (asymptotic) distribution of \u03bb\u0302n under H0 i.e., the null distribution, and \u03b1 is the significance level of the test. ME test The ME test uses as its test statistic \u03bb\u0302n, a form of Hotelling\u2019s T-squared statistic, defined as \u03bb\u0302n := nznS \u22121 n zn, where zn := 1 n \u2211n i=1 zi, Sn := 1 n\u22121 \u2211n i=1(zi \u2212 zn)(zi \u2212 zn), and zi := (k(xi,vj)\u2212 k(yi,vj))j=1 \u2208 R . The statistic depends on a positive definite kernel k : X \u00d7X \u2192 R (with X \u2286 R), and a set of J test locations V = {vj}j=1 \u2282 R. Under H0, asymptotically \u03bb\u0302n follows \u03c7(J), a chi-squared distribution with J degrees of freedom. The ME test rejects H0 if \u03bb\u0302n > T\u03b1, where the test threshold T\u03b1 is given by the (1 \u2212 \u03b1)-quantile of the asymptotic null distribution \u03c7(J). Although the distribution of \u03bb\u0302n under H1 was not derived, Chwialkowski et al. (2015) showed that if k is analytic, integrable and characteristic (in the sense of Sriperumbudur et al.", "startOffset": 68, "endOffset": 1678}, {"referenceID": 3, "context": "2 ME and SCF tests In this section, we review the ME and SCF tests (Chwialkowski et al., 2015) for two-sample testing. In Sec. 3, we will extend these approaches to learn features that optimize the power of these tests. Given two samples X := {xi}i=1,Y := {yi}i=1 \u2282 R independently and identically distributed (i.i.d.) according to P and Q, respectively, the goal of a two-sample test is to decide whether P is different from Q on the basis of the samples. The task is formulated as a statistical hypothesis test proposing a null hypothesis H0 : P = Q (samples are drawn from the same distribution) against an alternative hypothesis H1 : P 6= Q (the sample generating distributions are different). A test calculates a test statistic \u03bb\u0302n from X and Y, and rejectsH0 if \u03bb\u0302n exceeds a predetermined test threshold (critical value). The threshold is given by the (1\u2212 \u03b1)-quantile of the (asymptotic) distribution of \u03bb\u0302n under H0 i.e., the null distribution, and \u03b1 is the significance level of the test. ME test The ME test uses as its test statistic \u03bb\u0302n, a form of Hotelling\u2019s T-squared statistic, defined as \u03bb\u0302n := nznS \u22121 n zn, where zn := 1 n \u2211n i=1 zi, Sn := 1 n\u22121 \u2211n i=1(zi \u2212 zn)(zi \u2212 zn), and zi := (k(xi,vj)\u2212 k(yi,vj))j=1 \u2208 R . The statistic depends on a positive definite kernel k : X \u00d7X \u2192 R (with X \u2286 R), and a set of J test locations V = {vj}j=1 \u2282 R. Under H0, asymptotically \u03bb\u0302n follows \u03c7(J), a chi-squared distribution with J degrees of freedom. The ME test rejects H0 if \u03bb\u0302n > T\u03b1, where the test threshold T\u03b1 is given by the (1 \u2212 \u03b1)-quantile of the asymptotic null distribution \u03c7(J). Although the distribution of \u03bb\u0302n under H1 was not derived, Chwialkowski et al. (2015) showed that if k is analytic, integrable and characteristic (in the sense of Sriperumbudur et al. (2011)), under H1, \u03bb\u0302n can be arbitrarily large as n\u2192\u221e, allowing the test to correctly reject H0.", "startOffset": 68, "endOffset": 1783}, {"referenceID": 17, "context": "One can intuitively think of the ME test statistic as a squared normalized (by the inverse covariance S\u22121 n ) L (X , VJ) distance of the mean embeddings (Smola et al., 2007) of the empirical measures Pn := 1 n \u2211n i=1 \u03b4xi , and Qn := 1 n \u2211n i=1 \u03b4yi where VJ := 1 J \u2211J i=1 \u03b4vi , and \u03b4x is the Dirac measure concentrated at x.", "startOffset": 153, "endOffset": 173}, {"referenceID": 2, "context": ", without S\u22121 n ) was shown by Chwialkowski et al. (2015) to be a metric on the space of probability measures for any V .", "startOffset": 31, "endOffset": 58}, {"referenceID": 20, "context": "Assume that k is characteristic (Sriperumbudur et al., 2011).", "startOffset": 32, "endOffset": 60}, {"referenceID": 22, "context": "7 in van der Vaart and Wellner (2000). Then for any \u03b4 \u2208 (0, 1) with probability at least 1\u2212 \u03b4,", "startOffset": 13, "endOffset": 38}, {"referenceID": 3, "context": ", 1)) Blobs Gaussian mixtures in R as studied in Chwialkowski et al. (2015); Gretton et al.", "startOffset": 49, "endOffset": 76}, {"referenceID": 3, "context": ", 1)) Blobs Gaussian mixtures in R as studied in Chwialkowski et al. (2015); Gretton et al. (2012b).", "startOffset": 49, "endOffset": 100}, {"referenceID": 3, "context": "ME-grid and SCF-grid are as in Chwialkowski et al. (2015) where only the Gaussian width is optimized by a grid search,1and the test locations are randomly drawn from a multivariate normal distribution.", "startOffset": 31, "endOffset": 58}, {"referenceID": 3, "context": "ME-grid and SCF-grid are as in Chwialkowski et al. (2015) where only the Gaussian width is optimized by a grid search,1and the test locations are randomly drawn from a multivariate normal distribution. MMD-quad (quadratic-time) and MMD-lin (linear-time) refer to the nonparametric tests based on maximum mean discrepancy of Gretton et al. (2012a), where to ensure a fair comparison, the Gaussian kernel width is also chosen so as to maximize a criterion for the test power on training data, following the same principle as (Gretton et al.", "startOffset": 31, "endOffset": 347}, {"referenceID": 3, "context": "ME-grid and SCF-grid are as in Chwialkowski et al. (2015) where only the Gaussian width is optimized by a grid search,1and the test locations are randomly drawn from a multivariate normal distribution. MMD-quad (quadratic-time) and MMD-lin (linear-time) refer to the nonparametric tests based on maximum mean discrepancy of Gretton et al. (2012a), where to ensure a fair comparison, the Gaussian kernel width is also chosen so as to maximize a criterion for the test power on training data, following the same principle as (Gretton et al., 2012b). For MMDquad, since its null distribution is given by an infinite sum of weighted chi-squared variables (no closed-form quantiles), in each trial we randomly permute the two samples 400 times to approximate the null distribution. Finally, T 2 is the standard two-sample Hotelling\u2019s T-squared test, which serves as a baseline with Gaussian assumptions on P and Q. In all the following experiments, each problem is repeated for 500 trials. For toy problems, new samples are generated from the specified P,Q distributions in each trial. For real problems, samples are partitioned randomly into training and test sets in each trial. In all of the simulations, we report an empirical estimate of P(\u03bb\u0302 n/2 \u2265 T\u03b1) which is the proportion of the number of times the statistic \u03bb\u0302 te n/2 is above T\u03b1. This quantity is an estimate of type-I error under H0, and corresponds to test power when H1 is true. We set \u03b1 = 0.01 in all the experiments. All the code and preprocessed data are available at https://github.com/wittawatj/interpretable-test. Optimization The parameter tuning objective \u03bb\u0302 n/2(\u03b8) is a function of \u03b8 consisting of one real-valued \u03c3 and J test locations each of d dimensions. The parameters \u03b8 can thus be regarded as a Jd + 1 Euclidean vector. We take the derivative of \u03bb\u0302 n/2(\u03b8) with respect to \u03b8, and use gradient ascent to maximize it. J is pre-specified and fixed. For the ME test, we initialize the test locations with realizations from two multivariate normal distributions fitted to samples from P and Q; this ensures that the initial locations are well supported by the data. For the SCF test, initialization using the standard normal distribution is found to be sufficient. The parameter \u03b3n is not optimized; we set the regularization parameter \u03b3n to be as small as possible while being large enough to ensure that (Sn + \u03b3nI) \u22121 can be stably computed. We emphasize that both the optimization and testing are linear in n. The testing cost O(J + Jn + dJn) and the optimization costs O(J + dJn) per gradient ascent iteration. Runtimes of all methods are reported in Sec. C in the appendix. 1. Informative features: simple demonstration We begin with a demonstration that the proxy \u03bb\u0302 n/2(\u03b8) for the test power is informative for revealing the difference of the two samples in the ME Chwialkowski et al. (2015) chooses the Gaussian width that minimizes the median of the p-values, a heuristic that does not directly address test power.", "startOffset": 31, "endOffset": 2869}, {"referenceID": 15, "context": "The performance of MMD-lin degrades quickly with increasing dimension, as expected from Ramdas et al. (2015).", "startOffset": 88, "endOffset": 109}, {"referenceID": 13, "context": "Each paper is represented as a bag of words using TF-IDF (Manning et al., 2008) as features.", "startOffset": 57, "endOffset": 79}, {"referenceID": 12, "context": "For this purpose, we use Karolinska Directed Emotional Faces (KDEF) dataset (Lundqvist et al., 1998) containing 5040 aligned face images of 70 amateur actors, 35 females and 35 males.", "startOffset": 76, "endOffset": 100}, {"referenceID": 13, "context": "Treat each paper as a bag of words and construct a feature vector with TF-IDF (Manning et al., 2008).", "startOffset": 78, "endOffset": 100}, {"referenceID": 22, "context": "7 in van der Vaart and Wellner (2000). Then for any \u03b4 \u2208 (0, 1) with probability at least 1\u2212 \u03b4,", "startOffset": 13, "endOffset": 38}, {"referenceID": 2, "context": "By the Dudley entropy bound Bousquet (2003) [see Eq.", "startOffset": 28, "endOffset": 44}, {"referenceID": 18, "context": "The VC property will be a direct consequence of the VC indices of finite-dimensional function classes and preservation theorems (see Lemma 7); for a nice example application see Srebro and Ben-David (2006) (Section 5) who study the pseudo-dimension of (x,y) 7\u2192 k(x,y) kernel classes, for different Gaussian families.", "startOffset": 178, "endOffset": 206}, {"referenceID": 21, "context": "Lemma 7 (properties of VC classes, see page 141, 146-147 in van der Vaart and Wellner (2000) and page 160-161 in Kosorok (2008)).", "startOffset": 68, "endOffset": 93}, {"referenceID": 9, "context": "Lemma 7 (properties of VC classes, see page 141, 146-147 in van der Vaart and Wellner (2000) and page 160-161 in Kosorok (2008)).", "startOffset": 113, "endOffset": 128}], "year": 2016, "abstractText": "Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. We show that the empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on highdimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.", "creator": "LaTeX with hyperref package"}}}