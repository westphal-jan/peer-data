{"id": "1605.07766", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction", "abstract": "we propose a novel vector representation that integrates lexical contrast into 2d distributional vectors vertically and strengthens the most salient features for determining apparent degrees of word similarity. the improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0. 49 66 - 0. 49 76 across word classes ( adjectives, nouns, verbs ). moreover, we integrate the lexical contrast vectors into the objective function principle of modifying a skip - gram model. the novel platform embedding procedure outperforms state - of - the - art models on predicting word similarities in simlex - 999,, and on distinguishing antonyms from synonyms.", "histories": [["v1", "Wed, 25 May 2016 08:05:37 GMT  (63kb,D)", "http://arxiv.org/abs/1605.07766v1", "6 pages, 4 figures, InProc ACL 2016"]], "COMMENTS": "6 pages, 4 figures, InProc ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kim anh nguyen", "sabine schulte im walde", "ngoc thang vu"], "accepted": true, "id": "1605.07766"}, "pdf": {"name": "1605.07766.pdf", "metadata": {"source": "CRF", "title": "Integrating Distributional Lexical Contrast into Word Embeddings for Antonym\u2013Synonym Distinction", "authors": ["Kim Anh Nguyen"], "emails": ["nguyenkh@ims.uni-stuttgart.de", "schulte@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "Antonymy and synonymy represent lexical semantic relations that are central to the organization of the mental lexicon (Miller and Fellbaum, 1991). While antonymy is defined as the oppositeness between words, synonymy refers to words that are similar in meaning (Deese, 1965; Lyons, 1977). From a computational point of view, distinguishing between antonymy and synonymy is important for NLP applications such as Machine Translation and Textual Entailment, which go beyond a general notion of semantic relatedness and require to identify specific semantic relations. However, due to interchangeable substitution, antonyms and synonyms often occur in similar contexts, which makes it challenging to automatically distinguish between them.\nDistributional semantic models (DSMs) offer a means to represent meaning vectors of words and to determine their semantic \u201crelatedness\u201d (Budanitsky and Hirst, 2006; Turney and Pantel, 2010).\nThey rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations.\nIn recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations).\nLately, antonym\u2013synonym distinction has also been a focus of word embedding models. For example, Adel and Schu\u0308tze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) pro-\nar X\niv :1\n60 5.\n07 76\n6v 1\n[ cs\n.C L\n] 2\n5 M\nay 2\n01 6\nposed thesaurus-based word embeddings to capture antonyms. They proposed two models: the WE-T model that trains word embeddings on thesaurus information; and the WE-TD model that incorporated distributional information into the WET model. Pham et al. (2015) introduced the multitask lexical contrast model (mLCM) by incorporating WordNet into a skip-gram model to optimize semantic vectors to predict contexts. Their model outperformed standard skip-gram models with negative sampling on both general semantic tasks and distinguishing antonyms from synonyms.\nIn this paper, we propose two approaches that make use of lexical contrast information in distributional semantic space and word embeddings for antonym\u2013synonym distinction. Firstly, we incorporate lexical contrast into distributional vectors and strengthen those word features that are most salient for determining word similarities, assuming that feature overlap in synonyms is stronger than feature overlap in antonyms. Secondly, we propose a novel extension of a skip-gram model with negative sampling (Mikolov et al., 2013b) that integrates the lexical contrast information into the objective function. The proposed model optimizes the semantic vectors to predict degrees of word similarity and also to distinguish antonyms from synonyms. The improved word embeddings outperform state-of-the-art models on antonym\u2013 synonym distinction and a word similarity task."}, {"heading": "2 Our Approach", "text": "In this section, we present the two contributions of this paper: a new vector representation that improves the quality of weighted features to distinguish between antonyms and synonyms (Section 2.1), and a novel extension of skip-gram models that integrates the improved vector representations into the objective function, in order to predict similarities between words and to identify antonyms (Section 2.2)."}, {"heading": "2.1 Improving the weights of feature vectors", "text": "We aim to improve the quality of weighted feature vectors by strengthening those features that are most salient in the vectors and by putting less emphasis on those that are of minor importance, when distinguishing degrees of similarity between words. We start out with standard corpus co-occurrence frequencies and apply local mutual\ninformation (LMI) (Evert, 2005) to determine the original strengths of the word features. Our score weightSA(w, f) subsequently defines the weights of a target word w and a feature f :\nweightSA(w, f) = 1#(w,u) \u2211 u\u2208W (f)\u2229S(w) sim(w, u)\n\u2212 1#(w\u2032,v) \u2211\nw\u2032\u2208A(w)\n\u2211 v\u2208W (f)\u2229S(w\u2032) sim(w \u2032, v)\n(1)\nThe new weightSA scores of a target word w and a feature f exploit the differences between the average similarities of synonyms to the target word (sim(w, u), with u \u2208 S(w)), and the average similarities between antonyms of the target word (sim(w\u2032, v), with w\u2032 \u2208 A(w) and v \u2208 S(w\u2032)). Only those words u and v are included in the calculation that have a positive original LMI score for the feature f : W (f). To calculate the similarity sim between two word vectors, we rely on cosine distances. If a word w is not associated with any synonyms or antonyms in our resources (cf. Section 3.1), or if a feature does not co-occur with a word w, we define weightSA(w, f) = 0.\nThe intuition behind the lexical contrast information in our new weightSA is as follows. The strongest features of a word also tend to represent strong features of its synonyms, but weaker features of its antonyms. For example, the feature conception only occurs with synonyms of the adjective formal but not with the antonym informal, or with synonyms of the antonym informal. weightSA(formal, conception), which is calculated as the average similarity between formal and its synonyms minus the average similarity between informal and its synonyms, should thus return a high positive value. In contrast, a feature such as issue that occurs with many different adjectives, would enforce a feature score near zero for weightSA(formal, issue), because the similarity scores between formal and its synonyms and informal and its synonyms should not differ strongly. Last but not least, a feature such as rumor that only occurs with informal and its synonyms, but not with the original target adjective formal and its synonyms, should invoke a very low value for weightSA(formal, rumor). Figure 1 provides a schematic visualization for computing the new weightSA scores for the target formal.\nSince the number of antonyms is usually much smaller than the number of synonyms, we enrich the number of antonyms: Instead of using the\ndirect antonym links, we consider all synonyms of an antonym w\u2032 \u2208 A(w) as antonyms of w. For example, the target word good has only two antonyms in WordNet (bad and evil), in comparison to 31 synonyms. Thus, we also exploit the synonyms of bad and evil as antonyms for good."}, {"heading": "2.2 Integrating the distributional lexical contrast into a skip-gram model", "text": "Our model relies on Levy and Goldberg (2014) who showed that the objective function for a skip-gram model with negative sampling (SGNS) can be defined as follows:\n\u2211 w\u2208V \u2211 c\u2208V {#(w, c) log \u03c3(sim(w, c))\n+k#(w)P0(c) log \u03c3(\u2212sim(w, c))} (2)\nThe first term in Equation (2) represents the cooccurrence between a target word w and a context c within a context window. The number of appearances of the target word and that context is defined as #(w, c). The second term refers to the negative sampling where k is the number of negatively sampled words, and #(w) is the number of\nappearances of w as a target word in the unigram distribution P0 of its negative context c.\nTo incorporate our lexical contrast information into the SGNS model, we propose the objective function in Equation (3) to add distributional contrast followed by all contexts of the target word. V is the vocabulary; \u03c3(x) = 11+e\u2212x is the sigmoid function; and sim(w1, w2) is the cosine similarity between the two embedded vectors of the corresponding two words w1 and w2. We refer to our distributional lexical-contrast embedding model as dLCE.\u2211\nw\u2208V \u2211 c\u2208V {(#(w, c) log \u03c3(sim(w, c)) +k#(w)P0(c) log \u03c3(\u2212sim(w, c))) +( 1#(w,u) \u2211 u\u2208W (c)\u2229S(w) sim(w, u)\n\u2212 1#(w,v) \u2211 v\u2208W (c)\u2229A(w) sim(w, v))}\n(3)\nEquation (3) integrates the lexical contrast information in a slightly different way compared to Equation (1): For each of the target words w, we only rely on its antonyms A(w) instead of using the synonyms of its antonyms S(w\u2032). This makes the word embeddings training more efficient in running time, especially since we are using a large amount of training data.\nThe dLCE model is similar to the WETD model (Ono et al., 2015) and the mLCM model (Pham et al., 2015); however, while the WE-TD and mLCM models only apply the lexical contrast information from WordNet to each of the target words, dLCE applies lexical contrast to every single context of a target word in order to better capture and classify semantic contrast."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental Settings", "text": "The corpus resource for our vector representations is one of the currently largest web corpora: ENCOW14A (Scha\u0308fer and Bildhauer, 2012; Scha\u0308fer, 2015), containing approximately 14.5 billion tokens and 561K distinct word types. As distributional information, we used a window size of 5 tokens for both the original vector representation and the word embeddings models. For word embeddings models, we trained word vectors with 500 dimensions; k negative sampling was set to 15; the threshold for sub-sampling was set to 10\u22125; and we ignored all words that occurred < 100 times in the corpus. The parameters of the models were estimated by backpropagation of error via stochastic gradient descent. The learning rate strategy was similar to Mikolov et al. (2013a) in which the initial learning rate was set to 0.025. For the lexical contrast information, we used WordNet (Miller, 1995) and Wordnik1 to collect antonyms and synonyms, obtaining a total of 363,309 synonym and 38,423 antonym pairs."}, {"heading": "3.2 Distinguishing antonyms from synonyms", "text": "The first experiment evaluates our lexical contrast vectors by applying the vector representations with the improved weightSA scores to the task of distinguishing antonyms from synonyms. As gold standard resource, we used the English dataset described in (Roth and Schulte im Walde, 2014), containing 600 adjective pairs (300 antonymous pairs and 300 synonymous pairs), 700 noun pairs (350 antonymous pairs and 350 synonymous pairs) and 800 verb pairs (400 antonymous pairs and 400 synonymous pairs). For evaluation, we applied Average Precision (AP) (Voorhees and Harman, 1999), a common metric in information retrieval previously used by Kotlerman et al.\n1http://www.wordnik.com\n(2010) and Santus et al. (2014a), among others. Table 1 presents the results of the first experiment, comparing our improved vector representations with the original LMI representations across word classes, without/with applying singular-value decomposition (SVD), respectively. In order to evaluate the distribution of word pairs with AP, we sorted the synonymous and antonymous pairs by their cosine scores. A synonymous pair was considered correct if it belonged to the first half; and an antonymous pairs was considered correct if it was in the second half. The optimal results would thus achieve an AP score of 1 for SY N and 0 for ANT . The results in the tables demonstrate that weightSA significantly2 outperforms the original vector representations across word classes.\nIn addition, Figure 2 compares the medians of cosine similarities between antonymous pairs (red) vs. synonymous pairs (green) across word classes, and for the four conditions (1) LMI, (2) weightSA, (3) SVD on LMI, and (4) SVD on weightSA. The plots show that the cosine similarities of the two relations differ more strongly with our improved vector representations in comparison to the original LMI representations, and even more so after applying SVD."}, {"heading": "3.3 Effects of distributional lexical contrast on word embeddings", "text": "The second experiment evaluates the performance of our dLCE model on both antonym\u2013synonym distinction and a word similarity task. The similarity task requires to predict the degree of similarity for word pairs, and the ranked list of predictions is evaluated against a gold standard of human ratings, relying on the Spearman rank-order correlation coefficient \u03c1 (Siegel and Castellan, 1988).\nIn this paper, we use the SimLex-999 dataset (Hill et al., 2015) to evaluate word embedding models on predicting similarities. The resource contains 999 word pairs (666 noun, 222 verb and 111 adjective pairs) and was explicitly built to test models on capturing similarity rather than relatedness or association. Table 2 shows that our dLCE model outperforms both SGNS and mLCM, proving that the lexical contrast information has a positive effect on predicting similarity.\n2\u03c72,\u2217\u2217\u2217 p < .001,\u2217\u2217 p < .005, \u2217p < .05\nTherefore, the improved distinction between synonyms (strongly similar words) and antonyms (often strongly related but highly dissimilar words) in the dLCE model also supports the distinction between degrees of similarity.\nFor distinguishing between antonyms and synonyms, we computed the cosine similarities between word pairs on the dataset described in Section 3.2, and then used the area under the ROC curve (AUC) to evaluate the performance of dLCE compared to SGNS and mLCM. The results in Table 3 report that dLCE outperforms SGNS and mLCM also on this task."}, {"heading": "4 Conclusion", "text": "This paper proposed a novel vector representation which enhanced the prediction of word similarity, both for a traditional distributional semantics model and word embeddings. Firstly, we significantly improved the quality of weighted features to distinguish antonyms from synonyms by using lexical contrast information. Secondly, we incorporated the lexical contrast information into a skip-gram model to successfully predict degrees of similarity and also to identify antonyms."}], "references": [{"title": "Using mined coreference chains as a resource for a semantic task", "author": ["Adel", "Sch\u00fctze2014] Heike Adel", "Hinrich Sch\u00fctze"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Adel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Adel et al\\.", "year": 2014}, {"title": "Evaluating WordNet-based measures of lexical semantic relatedness", "author": ["Budanitsky", "Hirst2006] Alexander Budanitsky", "Graeme Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Budanitsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky et al\\.", "year": 2006}, {"title": "The Statistics of Word Cooccurrences", "author": ["Stefan Evert"], "venue": "Ph.D. thesis,", "citeRegEx": "Evert.,? \\Q2005\\E", "shortCiteRegEx": "Evert.", "year": 2005}, {"title": "Papers in Linguistics 1934-51", "author": ["John R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1957\\E", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Directional distributional similarity for lexical inference", "author": ["Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet"], "venue": "Natural Language Processing,", "citeRegEx": "Kotlerman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kotlerman et al\\.", "year": 2010}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Lin et al.2003] Dekang Lin", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Computing Research Repository,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A lexical database for English", "author": ["George A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Computing lexical contrast", "author": ["Bonnie J. Dorr", "Graeme Hirst", "Peter D. Turney"], "venue": null, "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Word embedding-based antonym detection using thesauri and distributional information", "author": ["Ono et al.2015] Masataka Ono", "Makoto Miwa", "Yutaka Sasaki"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association", "citeRegEx": "Ono et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ono et al\\.", "year": 2015}, {"title": "A multitask objective to inject lexical contrast into distributional semantics", "author": ["Pham et al.2015] Nghia The Pham", "Angeliki Lazaridou", "Marco Baroni"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Combining word patterns and discourse markers for paradigmatic relation classification", "author": ["Roth", "Sabine Schulte im Walde"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "Chasing hypernyms in vector spaces with entropy", "author": ["Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Lin-", "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Taking antonymy mask off in vector space", "author": ["Qin Lu", "Alessandro Lenci", "Chu-Ren Huang"], "venue": "In Proceedings of the 28th Pacific Asia Conference on Language, Information and Computation,", "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Building large corpora from the web using a new efficient tool chain", "author": ["Sch\u00e4fer", "Bildhauer2012] Roland Sch\u00e4fer", "Felix Bildhauer"], "venue": "In Proceedings of the 8th International Conference on Language Resources and Evaluation,", "citeRegEx": "Sch\u00e4fer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sch\u00e4fer et al\\.", "year": 2012}, {"title": "Processing and querying large web corpora with the COW14 architecture", "author": ["Roland Sch\u00e4fer"], "venue": "In Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora,", "citeRegEx": "Sch\u00e4fer.,? \\Q2015\\E", "shortCiteRegEx": "Sch\u00e4fer.", "year": 2015}, {"title": "Uncovering distributional differences between synonyms and antonyms in a word space model", "author": ["Sabine Schulte im Walde", "Sylvia Springorum"], "venue": "In Proceedings of the 6th International Joint Conference on Natu-", "citeRegEx": "Scheible et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Scheible et al\\.", "year": 2013}, {"title": "Nonparametric Statistics for the Behavioral Sciences", "author": ["Siegel", "Castellan1988] Sidney Siegel", "N. John Castellan"], "venue": null, "citeRegEx": "Siegel et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Siegel et al\\.", "year": 1988}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "The 7th Text REtrieval Conference (trec-7)", "author": ["Voorhees", "Harman1999] Ellen M. Voorhees", "Donna K. Harman"], "venue": "National Institute of Standards and Technology,", "citeRegEx": "Voorhees et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Voorhees et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning.", "startOffset": 43, "endOffset": 70}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often.", "startOffset": 58, "endOffset": 718}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites.", "startOffset": 58, "endOffset": 948}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features.", "startOffset": 58, "endOffset": 1184}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al.", "startOffset": 58, "endOffset": 1357}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations).", "startOffset": 58, "endOffset": 1383}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym\u2013synonym distinction has also been a focus of word embedding models. For example, Adel and Sch\u00fctze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms.", "startOffset": 58, "endOffset": 1767}, {"referenceID": 3, "context": "They rely on the distributional hypothesis (Harris, 1954; Firth, 1957), in which words with similar distributions have related meaning. For computation, each word is represented by a weighted feature vector, where features typically correspond to words that co-occur in a particular context. However, DSMs tend to retrieve both synonyms (such as formal\u2013conventional) and antonyms (such as formal\u2013informal) as related words and cannot sufficiently distinguish between the two relations. In recent years, a number of distributional approaches have accepted the challenge to distinguish antonyms from synonyms, often in combination with lexical resources such as thesauruses or taxonomies. For example, Lin et al. (2003) used dependency triples to extract distributionally similar words, and then in a post-processing step filtered out words that appeared with the patterns \u2018from X to Y\u2019 or \u2018either X or Y\u2019 significantly often. Mohammad et al. (2013) assumed that word pairs that occur in the same thesaurus category are close in meaning and marked as synonyms, while word pairs occurring in contrasting thesaurus categories or paragraphs are marked as opposites. Scheible et al. (2013) showed that the distributional difference between antonyms and synonyms can be identified via a simple word space model by using appropriate features. Santus et al. (2014a) and Santus et al. (2014b) aimed to identify the most salient dimensions of meaning in vector representations and reported a new average-precisionbased distributional measure and an entropy-based measure to discriminate antonyms from synonyms (and further paradigmatic semantic relations). Lately, antonym\u2013synonym distinction has also been a focus of word embedding models. For example, Adel and Sch\u00fctze (2014) integrated coreference chains extracted from large corpora into a skip-gram model to create word embeddings that identified antonyms. Ono et al. (2015) proar X iv :1 60 5.", "startOffset": 58, "endOffset": 1919}, {"referenceID": 2, "context": "We start out with standard corpus co-occurrence frequencies and apply local mutual information (LMI) (Evert, 2005) to determine the original strengths of the word features.", "startOffset": 101, "endOffset": 114}, {"referenceID": 10, "context": "Pham et al. (2015) introduced the multitask lexical contrast model (mLCM) by incorporating WordNet into a skip-gram model to optimize semantic vectors to predict contexts.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "The dLCE model is similar to the WETD model (Ono et al., 2015) and the mLCM model (Pham et al.", "startOffset": 44, "endOffset": 62}, {"referenceID": 13, "context": ", 2015) and the mLCM model (Pham et al., 2015); however, while the WE-TD and mLCM models only apply the lexical contrast information from WordNet to each of the target words, dLCE applies lexical contrast to every single context of a target word in order to better capture and classify semantic contrast.", "startOffset": 27, "endOffset": 46}, {"referenceID": 18, "context": "1 Experimental Settings The corpus resource for our vector representations is one of the currently largest web corpora: ENCOW14A (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015), containing approximately 14.", "startOffset": 129, "endOffset": 173}, {"referenceID": 10, "context": "For the lexical contrast information, we used WordNet (Miller, 1995) and Wordnik1 to collect antonyms and synonyms, obtaining a total of 363,309 synonym and 38,423 antonym pairs.", "startOffset": 54, "endOffset": 68}, {"referenceID": 8, "context": "The learning rate strategy was similar to Mikolov et al. (2013a) in which the initial learning rate was set to 0.", "startOffset": 42, "endOffset": 65}, {"referenceID": 5, "context": "For evaluation, we applied Average Precision (AP) (Voorhees and Harman, 1999), a common metric in information retrieval previously used by Kotlerman et al. http://www.wordnik.com (2010) and Santus et al.", "startOffset": 139, "endOffset": 186}, {"referenceID": 5, "context": "For evaluation, we applied Average Precision (AP) (Voorhees and Harman, 1999), a common metric in information retrieval previously used by Kotlerman et al. http://www.wordnik.com (2010) and Santus et al. (2014a), among others.", "startOffset": 139, "endOffset": 212}, {"referenceID": 4, "context": "In this paper, we use the SimLex-999 dataset (Hill et al., 2015) to evaluate word embedding models on predicting similarities.", "startOffset": 45, "endOffset": 64}], "year": 2016, "abstractText": "We propose a novel vector representation that integrates lexical contrast into distributional vectors and strengthens the most salient features for determining degrees of word similarity. The improved vectors significantly outperform standard models and distinguish antonyms from synonyms with an average precision of 0.66\u20130.76 across word classes (adjectives, nouns, verbs). Moreover, we integrate the lexical contrast vectors into the objective function of a skip-gram model. The novel embedding outperforms state-of-the-art models on predicting word similarities in SimLex999, and on distinguishing antonyms from synonyms.", "creator": "LaTeX with hyperref package"}}}