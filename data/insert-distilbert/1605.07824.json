{"id": "1605.07824", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Action Classification via Concepts and Attributes", "abstract": "classes discovered in natural images likewise tend to follow long tail distributions. this is problematic when there are insufficient classical training examples for rare classes. this effect is emphasized in compound classes, strongly involving the conjunction of several concepts, such as those appearing in action - frequency recognition datasets. in accompanying this paper, we propose to address this issue by learning how to utilize common visual data concepts which are readily available. we detect the presence of prominent concepts in images and use them to infer the target labels instead only of using visual features directly, combining tools from vision and natural - compound language processing. we validate fundamentally our method on the recently introduced quantum hico dataset reaching a map of 31. 54 % and conditional on the stanford - harvard 40 actions dataset, where the proposed binary method outperforms current state - of - the art and, combined with direct visual features, obtains an acquired accuracy 83. 5 12 %. almost moreover, the method provides for each class class a semantically meaningful list of keywords and relevant image regions relating it to match its constituent concepts.", "histories": [["v1", "Wed, 25 May 2016 11:06:58 GMT  (831kb,D)", "http://arxiv.org/abs/1605.07824v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "shimon ullman"], "accepted": false, "id": "1605.07824"}, "pdf": {"name": "1605.07824.pdf", "metadata": {"source": "CRF", "title": "Action Classification via Concepts and Attributes", "authors": ["Amir Rosenfeld", "Shimon Ullman"], "emails": ["amir.rosenfeld@weizmann.ac.il", "shimon.ullman@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In many tasks in pattern recognition, and specifically in computer vision, target classes follow a long-tail distribution. In the domain of action recognition this particularly true since the product of actions and objects is much bigger than each alone, and some examples may not be observed at all. This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21]. However, the distribution of these datasets will inevitably be long-tailed as well. One way to tackle this problem is to borrow information from external data sources. For instance, it has become popular to combine language and vision using joint embedded spaces [13, 17], which allow recognizing unseen classes more reliably than using a purely visual approach.\nIn this work, we propose to use an annotated concept dataset to learn a mapping from images to concepts with a visual meaning (e.g., objects and object attributes). This mapping is then used as a feature representation for classifying a target dataset, instead of describing the images with visual features extracted directly. This allows to describe an image or scene directly with high-level concepts instead of using visual features directly for the task. We show that training image classifiers this way, specifically in action-recognition, is as effective as training on the visual features. Moreover, we show that the concepts learned to be relevant to each category carry semantic meaning, which enables us to gain further insights into their success and failure modes. Our concept dataset is the recently introduced Visual-Genome dataset [10], in which we leverage the rich region annotations."}, {"heading": "2 Previous Work", "text": "We list several lines of work related to ours. An early related work is ObjectBank [12], where the outputs of detectors for 200 common objects are aggregated via a spatial-pyramid to serve as feature\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 5.\n07 82\n4v 1\n[ cs\n.C V\n] 2\n5 M\nay 2\nrepresentations. In the same spirit, ActionBank [22] learns detectors for various action types in videos and uses them to represent others as weighted combinations of actions. The work of [11] learns object attributes to describe objects in a zero-shot learning setting, so that new classes (animals) can be correctly classified by matching them to human generated lists of attributes. Recently, [20] learned various types of relations between actions (e.g., part of / type of / mutually exclusive) via visual and linguistic cues and leveraged those to be able to retrieve images of actions from a very large variety (27k) of action descriptions.\nOther works leverage information from natural language: in [17] an image is mapped to a semantic embedding space by a convex combination of word embeddings according to a pre-trained classifier on ImageNet [21], allowing to describe unseen classes as combinations of known ones. [13] makes this more robust by considering the output of the classifier along with the WordNet [16] hierarchy, generating image tags more reliably. The work of [8] mines a large image-sentence corpora for actor-verb-object triplets and clusters them into groups of semantically related actions. Recently, [15] used detected or provided person-bounding boxes in a multiple-instance learning framework, fusing global image context and person appearance. [7] uses spatial-pyramid pooling on activations from a convolutional layer in a network and encodes them using Fisher Vectors [7], with impressive results.\nIn our work we do not aim for a concept dataset with only very common objects or one that is tailored to our specific target task (such as action-recognition or animal classification) and automatically learn how to associate the learned concepts to target classes, either directly or via a language-based model."}, {"heading": "3 Approach", "text": "We begin by describing our approach at a high level, and elaborate on the detail in subsequent sub-sections.\nOur goal is to learn a classifier for a given set of images and target labels. Assume we are given access to two datasets: (1) a target dataset F and (2) a concept datasetD. The target dataset contains training pairs (Ii, yi) of images Ii and target labels yi. The concept dataset D is an additional annotated dataset containing many images labeled with a broad range of common concepts. The general idea is to learn high-level concepts from the dataset D and use those concepts to describe the images in F . More formally: let C = (c1, c2 . . . cN ) be a set of N concepts appearing in D. We learn a set of concept classifiers Fc, one for each c \u2208 C. Once we have the concept classifiers, we use them to describe each image I \u2208 F : we apply each classifier Fc to the image I, obtaining a set of concept scores:\nS(I) := [F1(I), F2(I), . . . FN (I)] \u2208 RN (1)\nFor brevity, we\u2019ll use the notation SI = S(I). SI defines a mapping from the samples I to a concept-space. We use SI as a new feature-map, allowing us to learn a classifier in terms of concepts, instead of features extracted directly from each image I. We note that the dataset D from which we learn concepts should be rich enough to enable learning of a broad range of concepts, to allow to describe each image in F well enough to facilitate the classification into the target labels. Next, we describe the source of our concept space, and how we learn various concepts.\nLearning Concepts\nTo learn a broad enough range of concepts, we use the recently introduced Visual Genome (VG) dataset [10]. It contains 108,249 images, all richly annotated with bounding boxes of various regions within each image. Each region spans an entire objects or an object part, and is annotated with a noun, an attribute, and a natural language description, all collected using crowd-sourcing (see [10] for full details). The average number of objects per image is 21.26, for a total of 2.1 million object instances. In addition, it contains object pair relationships for a subset of the object pairs of each image. Its richness and diversity makes it an ideal candidate from which to learn various concepts.\n3.1 Concepts as Classifiers\nWe explore the use of three sources of information from the VG dataset, namely (1) object annotations (2) attribute annotations (group all objects with a given attribute to a single entity) and (3) objectattributes (a specific object with a specific attribute). We assign each image a binary concept-vector P \u2208 RN where Pi indicates if the i\u2019th concept is present or not in the respective image. This is done separately each of the above sources of information. Despite the objects being annotated via bounding boxes, rather than training detectors, we train image-level predictors for each. This is both simpler and more robust, as it can be validated from various benchmarks ([5, 21] and many others) that detecting the presence of objects in images currently works more accurately than correctly localizing them. Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels. Given labellings for N different concepts (where N may vary depending on the type of concept), we train a one-versus-all SVM for each one separately, using features extracted from a CNN (see experiments for details). Denote these classifiers as (Fi)Ni=1.\nThis process results in a scoring of each image I \u2208 F (our target dataset) with a concept-feature SI as in Eqn. 1. For each concept score SI,j = Fj(I), we have\nSI,j = \u03c5 T j fI (2)\nSI = V fI (3)\nwhere fI are features extracted from image I and \u03c5j the weight vector of the j\u2019th classifier (we drop the bias term for brevity). V is a matrix whose rows are the vj .\nWe then train a classifier to map from SI to its target label, by using an additional SVM for each target class; denote by (Gj)Lj=1 each classifier, where L is the number target labels of images in target dataset F . The final score assigned to an image I for a target class j is denoted by\nHj(I) = \u03c9 T j SI (4)\n= \u03c9Tj V fI (5)\nWhere \u03c9j is the learned weight vector for the classifier Gj . Before we train Gi , we apply PCA to the collection of training SI vectors and project them to first n = 900 dimensions according to the strongest singular values (n was chosen by validation in early experiments). We found this to reduce the runtime and improve the classification results.\nWe also experimented with training a neural net to predict the class scores; this brought no performance gain over the SVM , despite trying various architectures and learning rates.\nWe next describe how we deal with training a large number of concept classifiers using the information from the VG dataset."}, {"heading": "3.2 Refinement via Language", "text": "The object nouns and attributes themselves have very long-tail distributions in the VG dataset, and contain many redundancies. Training classifiers for all of the concepts would be unlikely for several reasons: first, the objects themselves follow a long-tail distribution, which would cause most of the classifiers to perform poorly; second, there are many overlapping concepts, which cannot be regarded as mutually exclusive classes; third, the number of parameters required to train such a number of classifiers would become prohibitively large.\nTo reduce the number of concepts to be learned, we remove redundancies by standardizing the words describing the various concepts and retain only concepts with at least 10 positive examples (more details in Section 4.1).\nMany of the concepts overlap, such as \u201cperson\u201d and \u201cman\u201d, or are closely related, such as A being a sub-type of B (\u201ccat\u201d vs \u201canimal\u201d). Hence it would harm the classifiers to treat them as mutually exclusive. To overcome this, we represent each concept by its representation produced by the GloVe method of [19]. This 300-D representation has been shown to correlate well with semantic meaning, embedding semantically related words closely together in vector space, as well as other interesting properties.\nWe perform K-means [1] clustering on the embedded word-vectors for a total of 100 clusters. As expected, these produce semantically meaningful clusters, forming groups such as : (sign,streetsign,traffic-sign,...),(collar,strap,belt,...),(flower,leaf,rose,...),(beer,coffee,juice), etc.\nWe assign to each concept c a cluster hc. Denote by CI the set of concepts in an image I according to the ground-truth in the VG [10]dataset. We define the positive and negative training sets as follows:\nRpos(c) = {I : c \u2208 CI} (6) Rneg(c) = {I : hc \u2229 CI = \u2205} (7)\nIn words, Rpos is the set of all images containing the target concept c and Rneg is the set of all images which do not contain any concept in the same cluster as c. We sample a random subset from Rneg to avoid highly-imbalanced training sets for concepts which have few positive examples. In addition, sampling lowers the chance to encounter images in which c or members of hc were not labeled. In practice, we limit the number of positive samples of each concept to 1000 , as using more samples increased the run-times significantly with no apparent benefits in performance.\nThe classifiers trained on the set concepts in this way serve as the Fi in Section 3.1. We next proceed to describe experiments validating our approach and comparing it to standard approaches."}, {"heading": "4 Experiments", "text": "To validate our approach, we have tested it on the Standford-40 Action dataset [25]. It contains 9532 images with a diverse set of of 40 different action classes, 4000 images for training and the rest for testing. In addition we test our method on the recently introduced HICO [3] dataset, with 47774 images and 600 (not all mutually exclusive) action classes. Following are some technical details, followed by experiments checking various aspects of the method.\nAs a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9]. The feature dimensions are 1024, 4096 and 2048, respectively. In all cases, we train a linear SVM [6] in a one-versus-all manner on `2 normalized features, or the `2 normalized concatenation of `2 features in case of using several feature types. To assign GloVe [19] vectors to object names or attributes, we use the pre-trained model on the Common-Crawl (42B) corpus, which contains a vocabulary of 1.9M words. We break up phrases into their words and assign to them their mean GloVe vector. We discard a few words which are not found in the corpus at all (e.g., \u201cossicones\u201d)."}, {"heading": "4.1 Visual vs Concept features", "text": "Training and testing by using the visual features is straightforward. For the concept features, we train concept detectors on both the objects and object attributes of the VG dataset. Directly using these in their raw form is infeasible as explained in Sec. 3.2. To reduce the number of classes, we normalize each object name beforehand. The object name can be either a single word, such as \u201cdog\u201d, or a phrase, such as \u201ca baseball bat\u201d. To normalize the names, we remove stop-words, such as \u201cthe\u201d,\u201dher\u201d,\u201dis\u201d,\u201da\u201d, as well as punctuation marks. We turn plural words into their singular form. We avoid lemmatizing words since we found that this slightly hinders performance, for example, the word \u201cbuilding\u201d usually refers to the structure and has a different meaning if turned to \u201cbuild\u201d by lemmatization. Following this process we are left with 66,390 unique object names. They are distributed unevenly, the most common being \u201cman\u201d,\u201dsky\u201d,\u201dground\u201d,\u201dtree\u201d, etc. We remove all objects with less than 10 images containing them in the dataset, leaving us with 6,063 total object names. We do the same for object attributes: we treat attributes regardless of the object type (e.g, \u201csmall dog\u201d and \u201csmall tree\u201d are both mapped to \u201csmall\u201d), as the number of common object-attribute pairs is much smaller than the number of attributes. Note that the stop-word list for attributes is slightly different, as \u201cwhite\u201d, a common word in the dataset, is not a noun, but is a proper attribute. We are left with 1740 attributes appearing at least 10 times in the dataset. See Fig. 1 (a) for a visualization of the distribution of the object and attribute frequency. Specifically, one can see that for object-attribute pairs the long-tail distribution is much more accentuated, making them a bad candidate for concepts to learn as features for our target task.\nWe train classifiers using the detected objects, attributes and object-attributes pairs, as described in sections 3.1 and 3.2. Please refer to Table 2 for a comparison of the direct-visual results to our method. Except for the attribute-object concepts, we see that the concept based classification does nearly as well as the direct visual-based method, where the addition of ResNet-151 [9] clearly improves results. Combining the predictions from the direct and concept-based (object) predictions and using the ResNet features along with the other representations achieves a new state-of-the art of 83.12% on Stanford-40 [25]. On the recent HICO [3] dataset we obtain a mean average precision of 31.54%. [15] obtain higher results (36.1%) by fusing detected person bounding-boxes with global image context and using a weighted loss."}, {"heading": "4.2 Describing Actions using Concepts", "text": "For each target class, the learned classifier assigns a weight for each of the concepts. Examining this weight vector reveals the concepts deemed most relevant by the classifier. Recall that the weight vector of each learned classifier for class j is \u03c9j \u2208 RN (N is the number of target concepts). We checked if the highest-weighted concepts carry semantic meaning with respect to the target classes as follows: for each target class j \u2208 L (L being the set of target classes in F) we sort the values of the learned weight-vector \u03c9j in descending order, and list the concepts corresponding to the obtained ranking. Table 1 shows ten arbitrarily chosen classes from the Stanford-40 Actions [25] dataset, with the top 5 ranked object-concepts according to the respective weight-vector. In most cases, the classes are semantically meaningful. However, in some classes we see unexpected concepts, such as holding_an_umbrella\u2192handbag. This points to a likely bias in the Stanford-40 dataset, such as that many of the subjects holding umbrellas in the training images also carry handbags, which was indeed found to be the case by examining the training images for this class.\nAn interesting comparison is the concepts differentiating between related classes. For example, the top 5 ranked keywords for the class \u201cfeeding a horse\u201d are (\u201cmane\u201d, \u201cleft_ear\u201d, \u201chay\u201d, \u201cnostril\u201d, \u201chorse\u201d) whereas for \u201criding a horse\u201d they are (\u201csaddle\u201d, \u201chorse\u201d, \u201crider\u201d, \u201choof\u201d, \u201cjockey\u201d). While \u201chorse\u201d is predictably common to both, other words are indeed strongly related to one of the classes but not the other, for example, \u201chay\u201d for feeding vs \u201cjockey\u201d, \u201csaddle\u201d for riding."}, {"heading": "4.3 Concept Visualization", "text": "To test what features contribute most to the classification result, we use the Class-Activation-Map (CAM) approach of [26]. This method allows to visualize what image regions contributed the most to the score of each class. We can do this for the version of our method which uses only the VGG-GAP features, as the method relies on the structure specific to this network to re-project the classification scores to the image (see [26] for details). We visualize the average CAMs of the top-5 ranked keywords for different classes (as in the above section). We do this for two target classes for each image, one correct class and the other incorrect, to explain what image regions drive the method to decide on the image class. See Fig. 2. When the method is \u201cforced\u201d to explain the riding image as \u201cfeeding a horse\u201d, we see negative weights on the rider and strong positive weights on the lower part of the horse, whereas examining the regions contributing to \u201criding a horse\u201d gives a high weight to the region containing the jockey."}, {"heading": "4.4 Distribution of Weights", "text": "We have also examined the overall statistics of the learned weight vectors. For a single weight vector \u03c9, we define:\nabs(\u03c9) = [|\u03c91| , |\u03c92| , . . . , |\u03c9N |] (8)\n\u03c9\u0304 = 1\nN L\u2211 j=1 abs(\u03c9j) (9)\ni.e, \u03c9\u0304 is the mean of abs(\u03c9) for all classifiers of the target classes. Fig. 1 (b) displays these mean absolute weights assigned to object concepts, ordered by their frequency in VG. Not surprisingly, the first few tens of concepts have low-magnitude weights, as they are too common to be discriminative. The next few hundreds of concepts exhibit higher weights, and finally, weights become lower with diminished frequency. This can be explained due to such concepts having weaker classifiers as they have fewer positive examples, making them less reliable. A similar trend was observed when examining attributes.\n4.5 Feature Selection by Semantic Relatedness\nSection 4.2 provided a qualitative measure of the keywords found by the proposed method. Here, we take on a different approach, which is selecting concepts by a relatedness measure to the target classes, and measuring how well training using these concepts alone compares with choosing the top-k most common concepts. To do so, we measure their mean \u201cimportance\u201d. As described in Section 3.2 we assign to each concept c \u2208 C a GloVe [19] representation Vc. Similarly, we assign a vector Vp to each target class p \u2208 L according to its name; for instance, \u201criding a horse\u201d is assigned the mean of the vectors of the words \u201cride\u201d and \u201chorse\u201d. Then, for each class p we rank the Vc vectors according to their euclidean distance from Vp in increasing order. This induces a per-class order \u03c3p = cp,1 . . . cp,2, which is a permutation of 1... |C|, such that cp,i is the ranking of ci in the ordering induced by p. We use this to define the new mean rank r(c) of each concept:\nr(c) = L\u2211 p=1 exp(\u2212\u03c3p(c)) (10)\nNow, we test the predictive ability of concepts chosen from C according to two orderings. The first is the frequency of c, in ascending order, and the second is the sorted values (descending) of r(c) as defined in Eqn. 10. We select the first k concepts for the first k \u2208 (0, 5, 10, 15, . . . , 100) features (k = 0 for chance performance). For a small amount of features, e.g., k = 15, the concepts chosen according to r(c) outperform those chosen according to frequency by a large margin, i.e, 42.2 vs 34.2 respectively."}, {"heading": "5 Conclusions & Future Work", "text": "We have presented a method which learns to recognize action in images by describing them as a weighted sum of detected concepts (objects and object attributes). The method utilizes the annotations in the VG dataset to learn a broad range of concepts, which are then used to recognize action in still images. Together with the visual features, we are able to obtain state-of-the art classification performance, as well as provide a visual and semantic explanation of the classifier\u2019s decisions. In the future we indeed to broaden our work to capture object relationships, which are very important to action-classification as well ."}], "references": [{"title": "k-means++: The advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Weakly Supervised Deep Detection Networks", "author": ["Hakan Bilen", "Andrea Vedaldi"], "venue": "arXiv preprint arXiv:1511.02853,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "HICO: A Benchmark for Recognizing Human-Object Interactions in Images", "author": ["Yu-Wei Chao", "Zhan Wang", "Yugeng He", "Jiaxuan Wang", "Jia Deng"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning", "author": ["Ramazan Gokberk Cinbis", "Jakob Verbeek", "Cordelia Schmid"], "venue": "arXiv preprint arXiv:1503.00949,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "The pascal visual object classes (voc) challenge", "author": ["Mark Everingham", "Luc Van Gool", "Christopher KI Williams", "John Winn", "Andrew Zisserman"], "venue": "International journal of computer vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Deep spatial pyramid: The devil is once again in the details", "author": ["Bin-Bin Gao", "Xiu-Shen Wei", "Jianxin Wu", "Weiyao Lin"], "venue": "arXiv preprint arXiv:1504.05277,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "ACD: Action Concept Discovery from Image-Sentence Corpora", "author": ["Jiyang Gao", "Chen Sun", "Ram Nevatia"], "venue": "arXiv preprint arXiv:1604.04784,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Visual Genome: Connecting Language and Vision Using Crowdsourced", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A. Shamma", "Michael S. Bernstein", "Fei-Fei Li"], "venue": "Dense Image Annotations. CoRR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Object bank: A high-level image representation for scene classification & semantic feature sparsification", "author": ["Li-Jia Li", "Hao Su", "Li Fei-Fei", "Eric P Xing"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Zero-shot image tagging by hierarchical semantic embedding", "author": ["Xirong Li", "Shuai Liao", "Weiyu Lan", "Xiaoyong Du", "Gang Yang"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Learning Models for Actions and Person-Object Interactions with Transfer to Question Answering", "author": ["Arun Mallya", "Svetlana Lazebnik"], "venue": "arXiv preprint arXiv:1604.04808,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "WordNet: a lexical database for English", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Constrained convolutional neural networks for weakly supervised segmentation", "author": ["Deepak Pathak", "Philipp Krahenbuhl", "Trevor Darrell"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning semantic relationships for better action retrieval in images", "author": ["Vignesh Ramanathan", "Congcong Li", "Jia Deng", "Wei Han", "Zhen Li", "Kunlong Gu", "Yang Song", "Samy Bengio", "Chuck Rossenberg", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Action bank: A high-level representation of activity in video", "author": ["Sreemanananth Sadanand", "Jason J Corso"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning to share visual appearance for multiclass object detection", "author": ["Ruslan Salakhutdinov", "Antonio Torralba", "Josh Tenenbaum"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["Bangpeng Yao", "Xiaoye Jiang", "Aditya Khosla", "Andy Lai Lin", "Leonidas Guibas", "Li Fei-Fei"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Learning Deep Features for Discriminative Localization", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1512.04150,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Capturing long-tail distributions of object subcategories", "author": ["Xiangxin Zhu", "Dragomir Anguelov", "Deva Ramanan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 42, "endOffset": 54}, {"referenceID": 22, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 42, "endOffset": 54}, {"referenceID": 26, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 42, "endOffset": 54}, {"referenceID": 13, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 153, "endOffset": 161}, {"referenceID": 20, "context": "This has been observed in several studies [20, 23, 27] and it is becoming increasingly popular to overcome this problem by building ever larger datasets [14, 21].", "startOffset": 153, "endOffset": 161}, {"referenceID": 12, "context": "For instance, it has become popular to combine language and vision using joint embedded spaces [13, 17], which allow recognizing unseen classes more reliably than using a purely visual approach.", "startOffset": 95, "endOffset": 103}, {"referenceID": 16, "context": "For instance, it has become popular to combine language and vision using joint embedded spaces [13, 17], which allow recognizing unseen classes more reliably than using a purely visual approach.", "startOffset": 95, "endOffset": 103}, {"referenceID": 9, "context": "Our concept dataset is the recently introduced Visual-Genome dataset [10], in which we leverage the rich region annotations.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "An early related work is ObjectBank [12], where the outputs of detectors for 200 common objects are aggregated via a spatial-pyramid to serve as feature", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "In the same spirit, ActionBank [22] learns detectors for various action types in videos and uses them to represent others as weighted combinations of actions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "The work of [11] learns object attributes to describe objects in a zero-shot learning setting, so that new classes (animals) can be correctly classified by matching them to human generated lists of attributes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "Recently, [20] learned various types of relations between actions (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "Other works leverage information from natural language: in [17] an image is mapped to a semantic embedding space by a convex combination of word embeddings according to a pre-trained classifier on ImageNet [21], allowing to describe unseen classes as combinations of known ones.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "Other works leverage information from natural language: in [17] an image is mapped to a semantic embedding space by a convex combination of word embeddings according to a pre-trained classifier on ImageNet [21], allowing to describe unseen classes as combinations of known ones.", "startOffset": 206, "endOffset": 210}, {"referenceID": 12, "context": "[13] makes this more robust by considering the output of the classifier along with the WordNet [16] hierarchy, generating image tags more reliably.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[13] makes this more robust by considering the output of the classifier along with the WordNet [16] hierarchy, generating image tags more reliably.", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "The work of [8] mines a large image-sentence corpora for actor-verb-object triplets and clusters them into groups of semantically related actions.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "Recently, [15] used detected or provided person-bounding boxes in a multiple-instance learning framework, fusing global image context and person appearance.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "[7] uses spatial-pyramid pooling on activations from a convolutional layer in a network and encodes them using Fisher Vectors [7], with impressive results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] uses spatial-pyramid pooling on activations from a convolutional layer in a network and encodes them using Fisher Vectors [7], with impressive results.", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "To learn a broad enough range of concepts, we use the recently introduced Visual Genome (VG) dataset [10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "Each region spans an entire objects or an object part, and is annotated with a noun, an attribute, and a natural language description, all collected using crowd-sourcing (see [10] for full details).", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "Table 1: Highest ranking concepts linked to each action class according to the proposed method, for 10 arbitrarily selected actions from the Stanford-40 Actions dataset[25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 4, "context": "This is both simpler and more robust, as it can be validated from various benchmarks ([5, 21] and many others) that detecting the presence of objects in images currently works more accurately than correctly localizing them.", "startOffset": 86, "endOffset": 93}, {"referenceID": 20, "context": "This is both simpler and more robust, as it can be validated from various benchmarks ([5, 21] and many others) that detecting the presence of objects in images currently works more accurately than correctly localizing them.", "startOffset": 86, "endOffset": 93}, {"referenceID": 1, "context": "Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels.", "startOffset": 85, "endOffset": 95}, {"referenceID": 3, "context": "Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels.", "startOffset": 85, "endOffset": 95}, {"referenceID": 17, "context": "Moreover, weakly-supervised localization methods are becoming increasingly effective [2, 4, 18], further justified the use of image-level labels.", "startOffset": 85, "endOffset": 95}, {"referenceID": 18, "context": "To overcome this, we represent each concept by its representation produced by the GloVe method of [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "We perform K-means [1] clustering on the embedded word-vectors for a total of 100 clusters.", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "Denote by CI the set of concepts in an image I according to the ground-truth in the VG [10]dataset.", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "To validate our approach, we have tested it on the Standford-40 Action dataset [25].", "startOffset": 79, "endOffset": 83}, {"referenceID": 2, "context": "In addition we test our method on the recently introduced HICO [3] dataset, with 47774 images and 600 (not all mutually exclusive) action classes.", "startOffset": 63, "endOffset": 66}, {"referenceID": 25, "context": "As a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9].", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "As a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9].", "startOffset": 285, "endOffset": 289}, {"referenceID": 8, "context": "As a baseline for purely-visual categorization, we train and test several baseline approaches using feature combinations from various sources: (1) The global average pooling of the VGG-GAP network [26] (2) the output of the 2-before last fully connected layer (termed fc6) from VGG-16 [24] and (3) The pool-5 features from the penultimate layer of ResNet-151 [9].", "startOffset": 359, "endOffset": 362}, {"referenceID": 5, "context": "In all cases, we train a linear SVM [6] in a one-versus-all manner on `2 normalized features, or the `2 normalized concatenation of `2 features in case of using several feature types.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "To assign GloVe [19] vectors to object names or attributes, we use the pre-trained model on the Common-Crawl (42B) corpus, which contains a vocabulary of 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "Figure 1: (a) Object (red) and attributes (green) in the VG dataset [10] follow a long tail distribution.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "Except for the attribute-object concepts, we see that the concept based classification does nearly as well as the direct visual-based method, where the addition of ResNet-151 [9] clearly improves results.", "startOffset": 175, "endOffset": 178}, {"referenceID": 24, "context": "12% on Stanford-40 [25].", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "On the recent HICO [3] dataset we obtain a mean average precision of 31.", "startOffset": 19, "endOffset": 22}, {"referenceID": 14, "context": "[15] obtain higher results (36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "G: GlobalAverage-Pooling layer from [26].", "startOffset": 36, "endOffset": 40}, {"referenceID": 23, "context": "V: fc6 from VGG-16 [24].", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "R: pool5 (penultimate layer) from ResNet-151[9].", "startOffset": 44, "endOffset": 47}, {"referenceID": 24, "context": "Stanford-40[25](accuracy) Method \\ Features G G+V G+V+R", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "81 [7] HICO[3](mAP) G G+V G+V+R", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "81 [7] HICO[3](mAP) G G+V G+V+R", "startOffset": 11, "endOffset": 14}, {"referenceID": 14, "context": "1\u2020[15]", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "Table 1 shows ten arbitrarily chosen classes from the Stanford-40 Actions [25] dataset, with the top 5 ranked object-concepts according to the respective weight-vector.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "To test what features contribute most to the classification result, we use the Class-Activation-Map (CAM) approach of [26].", "startOffset": 118, "endOffset": 122}, {"referenceID": 25, "context": "We can do this for the version of our method which uses only the VGG-GAP features, as the method relies on the structure specific to this network to re-project the classification scores to the image (see [26] for details).", "startOffset": 204, "endOffset": 208}, {"referenceID": 25, "context": "We visualize (using [26]) highlighted regions contributing to the strongest concepts related to the correct vs the incorrect class .", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "2 we assign to each concept c \u2208 C a GloVe [19] representation Vc.", "startOffset": 42, "endOffset": 46}], "year": 2016, "abstractText": "Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54% and on the Stanford40 Actions dataset, where the proposed method outperforms current state-of-the art and, combined with direct visual features, obtains an accuracy 83.12%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts.", "creator": "LaTeX with hyperref package"}}}