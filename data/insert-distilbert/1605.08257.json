{"id": "1605.08257", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2016", "title": "Low-rank tensor completion: a Riemannian manifold preconditioning approach", "abstract": "we propose proposes a novel riemannian manifold design preconditioning approach for constructing the tensor completion problem with rank constraint. a novel riemannian metric or inner product is proposed that exploits the least - squares lattice structure independently of the cost function and takes into account the structured symmetry that exists in corresponding tucker decomposition. the specific metric allows to use the versatile framework of riemannian optimization on quotient manifolds to develop preconditioned nonlinear spectral conjugate gradient and stochastic gradient descent algorithms for batch and structured online setups, respectively. concrete matrix representations of various optimization - related convergence ingredients are listed. numerical comparisons suggest that our proposed algorithms robustly outperform state - of - the - ball art algorithms across different synthetic and real - world datasets.", "histories": [["v1", "Thu, 26 May 2016 12:55:02 GMT  (4088kb,D)", "http://arxiv.org/abs/1605.08257v1", "The 33rd International Conference on Machine Learning (ICML 2016). arXiv admin note: substantial text overlap witharXiv:1506.02159"]], "COMMENTS": "The 33rd International Conference on Machine Learning (ICML 2016). arXiv admin note: substantial text overlap witharXiv:1506.02159", "reviews": [], "SUBJECTS": "cs.LG cs.NA math.OC stat.ML", "authors": ["hiroyuki kasai", "bamdev mishra"], "accepted": true, "id": "1605.08257"}, "pdf": {"name": "1605.08257.pdf", "metadata": {"source": "CRF", "title": "Low-rank tensor completion: a Riemannian manifold preconditioning approach", "authors": ["Hiroyuki Kasai", "Bamdev Mishra"], "emails": ["kasai@is.uec.ac.jp", "bamdevm@amazon.com"], "sections": [{"heading": "1 Introduction", "text": "This paper addresses the problem of low-rank tensor completion when the rank is a priori known or estimated. We focus on 3-order tensors in the paper, but the developments can be generalized to higher order tensors in a straightforward way. Given a tensor X n1\u00d7n2\u00d7n3 , whose entries X ?i1,i2,i3 are only known for some indices (i1, i2, i3) \u2208 \u2126, where \u2126 is a subset of the complete set of indices {(i1, i2, i3) : id \u2208 {1, . . . , nd}, d \u2208 {1, 2, 3}}, the fixed-rank tensor completion problem is formulated as\nmin X\u2208Rn1\u00d7n2\u00d7n3\n1\n|\u2126|\u2016P\u2126(X )\u2212P\u2126(X ?)\u20162F\nsubject to rank(X ) = r, (1)\nwhere the operator P\u2126(X )i1,i2,i3 = Xi1,i2,i3 if (i1, i2, i3) \u2208 \u2126 and P\u2126(X )i1,i2,i3 = 0 otherwise and (with a slight abuse of notation) \u2016 \u00b7 \u2016F is the Frobenius norm. |\u2126| is the number of known entries. rank(X ) (= r = (r1, r2, r3)), called the multilinear rank of X , is the set of the ranks of for each of mode-d unfolding matrices. rd nd enforces a low-rank structure. The mode is a matrix obtained by concatenating the mode-d fibers along columns, and mode-d unfolding of a D-order tensor X is Xd \u2208 Rnd\u00d7nd+1\u00b7\u00b7\u00b7nDn1\u00b7\u00b7\u00b7nd\u22121 for d = {1, . . . , D}.\nProblem (1) has many variants, and one of those is extending the nuclear norm regularization approach from the matrix case [6] to the tensor case. This results in a summation of nuclear norm regularization terms, each one corresponds to each of the unfolding matrices of X . While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations. A different approach exploits Tucker decomposition [12, Section 4] of a low-rank tensor X to develop large-scale algorithms for (1), e.g., in [8, 13].\n1This paper extends the earlier work [11] to include a stochastic gradient descent algorithm for low-rank tensor completion.\nar X\niv :1\n60 5.\n08 25\n7v 1\n[ cs\n.L G\n] 2\n6 M\nay 2\n01 6\nThe present paper exploits both the symmetry present in Tucker decomposition and the least-squares structure of the cost function of (1) to develop competitive algorithms. The multilinear rank constraint forms a smooth manifold [13]. To this end, we use the concept of manifold preconditioning. While preconditioning in unconstrained optimization is well studied [20, Chapter 5], preconditioning on constraints with symmetries, owing to non-uniqueness of Tucker decomposition [12], is not straightforward. We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18]. The differences with respect to the work of Kressner et al. [13], which also exploits the manifold structure, are twofold. (i) Kressner et al. [13] exploit the search space as an embedded submanifold of the Euclidean space, whereas we view it as a product of simpler search spaces with symmetries. Consequently, certain computations have straightforward interpretation. (ii) Kressner et al. [13] work with the standard Euclidean metric, whereas we use a metric that is tuned to the least-squares cost function, thereby inducing a preconditioning effect. This novel idea of using a tuned metric leads to a superior performance of our algorithms. They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].\nThe paper is organized as follows. Section 2 discusses the two fundamental structures of symmetry and least-squares associated with (1) and proposes a novel metric that captures the relevant second order information of the problem. The optimization-related ingredients on the Tucker manifold are developed in Section 3. The cost function specific ingredients are developed in Section 4. The final formulas are listed in Table 1, which allow to develop preconditioned conjugate gradient descent algorithm in the batch setup and stochastic gradient descent algorithm in the online setup. In Section 5, numerical comparisons with state-of-the-art algorithms on various synthetic and real-world benchmarks suggest a superior performance of our proposed algorithms. Our proposed algorithms are implemented in the Matlab toolbox Manopt [5]. The concrete proofs of propositions, development of optimization-related ingredients, and additional numerical experiments are shown in Sections A and B, respectively, of the supplementary material file. The Matlab codes for first and second order implementations, e.g., gradient descent and trust-region methods, are available at https://bamdevmishra.com/codes/ tensorcompletion/."}, {"heading": "2 Exploiting the problem structure", "text": "Construction of efficient algorithms depends on properly exploiting the problem structure. To this end, we focus on two fundamental structures in (1): symmetry in the constraints and the least-squares structure of the cost function. Finally, a novel metric is proposed.\nThe symmetry structure in Tucker decomposition. The Tucker decomposition of a tensor X \u2208 Rn1\u00d7n2\u00d7n3 of rank r (=(r1, r2, r3)) is\nX = G\u00d71U1\u00d72U2\u00d73U3, (2) where Ud \u2208 St(rd, nd) for d \u2208 {1, 2, 3} belongs to the Stiefel manifold of matrices of size nd \u00d7 rd with orthogonal columns and G \u2208 Rr1\u00d7r2\u00d7r3 [12]. Here, W\u00d7dV \u2208 Rn1\u00d7\u00b7\u00b7\u00b7nd\u22121\u00d7m\u00d7nd+1\u00d7\u00b7\u00b7\u00b7nD computes the d-mode product of a tensor W \u2208 Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nD and a matrix V \u2208 Rm\u00d7nd . Tucker decomposition (2) is not unique as X remains unchanged under the transformation\n(U1,U2,U3,G) 7\u2192 (U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ) (3) for all Od \u2208 O(rd), which is the set of orthogonal matrices of size of rd \u00d7 rd. The classical remedy to remove this indeterminacy is to have additional structures on G like sparsity or restricted orthogonal rotations [12, Section 4.3]. In contrast, we encode the transformation (3) in an abstract search space of equivalence classes, defined as,\n[(U1,U2,U3,G)] := {(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ) : Od \u2208 O(rd)}. (4) The set of equivalence classes is the quotient manifold [14]\nM/\u223c := M/(O(r1)\u00d7O(r2)\u00d7O(r3)), (5)\nwhereM is called the total space (computational space) that is the product space\nM := St(r1, n1)\u00d7 St(r2, n2)\u00d7 St(r3, n3)\u00d7 Rr1\u00d7r2\u00d7r3 . (6)\nDue to the invariance (3), the local minima of (1) inM are not isolated, but they become isolated onM/\u223c. Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7]. A requirement is to endow endowM/\u223cwith a Riemannian structure, which conceptually translates (1) into an unconstrained optimization problem over the search spaceM/\u223c. We callM/\u223c, defined in (5), the Tucker manifold as it results from Tucker decomposition.\nThe least-squares structure of the cost function. In unconstrained optimization, the Newton method is interpreted as a scaled steepest descent method, where the search space is endowed with a metric (inner product) induced by the Hessian of the cost function [20]. This induced metric (or its approximation) resolves convergence issues of first order optimization algorithms. Analogously, finding a good inner product for (1) is of profound consequence. Specifically for the case of quadratic optimization with rank constraint (matrix case), Mishra and Sepulchre [18] propose a family of Riemannian metrics from the Hessian of the cost function. Applying this approach directly for the particular cost function of (1) is computationally costly. To circumvent the issue, we consider a simplified cost function by assuming that \u2126 contains the full set of indices, i.e., we focus on \u2016X \u2212 X ?\u20162F to propose a metric candidate. Applying the metric tuning approach of [18] to the simplified cost function leads to a family of Riemannian metrics. A good trade-off between computational cost and simplicity is by considering only the block diagonal elements of the Hessian of \u2016X \u2212X ?\u20162F . It should be noted that the cost function \u2016X \u2212 X ?\u20162F is convex and quadratic in X . Consequently, it is also convex and quadratic in the arguments (U1,U2,U3,G) individually. Equivalently, the block diagonal approximation of the Hessian of \u2016X \u2212X ?\u20162F in (U1,U2,U3,G) is\n((G1GT1 )\u2297 In1 , (G2GT2 )\u2297 In2 , (G3GT3 )\u2297 In3 , Ir1r2r3), (7)\nwhere Gd is the mode-d unfolding of G and is assumed to be full rank. \u2297 is the Kronecker product. The terms GdGTd for d \u2208 {1, 2, 3} are positive definite when r1 \u2264 r2r3, r2 \u2264 r1r3, and r3 \u2264 r1r2, which is a reasonable assumption.\nA novel Riemannian metric. An element x in the total space M has the matrix representation (U1,U2,U3,G). Consequently, the tangent space TxM is the Cartesian product of the tangent spaces of the individual manifolds of (6), i.e., TxM has the matrix characterization [7]\nTxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) \u2208 Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 : UTd ZUd + Z T Ud Ud = 0, for d \u2208 {1, 2, 3}}. (8)\nFrom the earlier discussion on symmetry and least-squares structure, we propose the novel metric or inner product gx : TxM\u00d7 TxM\u2192 R\ngx(\u03bex, \u03b7x) = \u3008\u03beU1 , \u03b7U1(G1GT1 )\u3009+ \u3008\u03beU2 , \u03b7U2(G2GT2 )\u3009+ \u3008\u03beU3 , \u03b7U3(G3GT3 )\u3009+ \u3008\u03beG , \u03b7G\u3009, (9)\nwhere \u03bex, \u03b7x \u2208 TxM are tangent vectors with matrix characterizations, shown in (8), (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) and (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G), respectively and \u3008\u00b7, \u00b7\u3009 is the Euclidean inner product. It should be emphasized that the proposed metric (9) is induced from (7).\nProposition 1. Let (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) and (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G) be tangent vectors to the quotient manifold (5) at (U1,U2,U3,G), and (\u03beU1O1 , \u03beU2O2 , \u03beU3O3 , \u03beG\u00d71OT1 \u00d72OT2 \u00d73OT3 ) and (\u03b7U1O1 , \u03b7U2O2 , \u03b7U3O3 , \u03b7G\u00d71OT1 \u00d72OT2 \u00d73OT3 ) be tangent vectors to the quotient manifold (5) at (U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ). The metric (9) is invariant along the equivalence class (4), i.e.,\ng(U1,U2,U3,G)((\u03beU1 , \u03beU2 , \u03beU3 , \u03beG), (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G))\n= g(U1O1,U2O2,U3O3,G\u00d71OT1 \u00d72OT2 \u00d73OT3 ) ((\u03beU1O1 , \u03beU2O2 , \u03beU3O3 , \u03beG\u00d71OT1 \u00d72OT2 \u00d73OT3 ),\n(\u03b7U1O1 , \u03b7U2O2 , \u03b7U3O3 , \u03b7G\u00d71OT1 \u00d72OT2 \u00d73OT3 ))."}, {"heading": "3 Notions of manifold optimization", "text": "Each point on a quotient manifold represents an entire equivalence class of matrices in the total space. Abstract geometric objects on the quotient manifold M/ \u223c call for matrix representatives in the total space M. Similarly, algorithms are run in the total space M, but under appropriate compatibility between the Riemannian structure ofM and the Riemannian structure of the quotient manifoldM/\u223c, they define algorithms on the quotient manifold. The key is endowingM/\u223c with a Riemannian structure. Once this is the case, a constraint optimization problem, for example (1), is conceptually transformed into an unconstrained optimization over the Riemannian quotient manifold (5). Below we briefly show the development of various geometric objects that are required to optimize a smooth cost function on the quotient manifold (5) with first order methods, e.g., conjugate gradients.\nQuotient manifold representation and horizontal lifts. Figure 1 illustrates a schematic view of optimization with equivalence classes, where the points x and y inM belong to the same equivalence class (shown in solid blue color) and they represent a single point [x] := {y \u2208 M : y \u223c x} on the quotient manifoldM/\u223c. The abstract tangent space T[x](M/\u223c) at [x] \u2208M/\u223c has the matrix representation in TxM, but restricted to the directions that do not induce a displacement along the equivalence class [x]. This is realized by decomposing TxM into two complementary subspaces, the vertical and horizontal subspaces. The vertical space Vx is the tangent space of the equivalence class [x]. On the other hand, the horizontal space Hx is the orthogonal subspace to Vx in the sense of the metric (9). Equivalently, TxM = Vx \u2295 Hx. The horizontal subspace Hx provides a valid matrix representation to the abstract tangent space T[x](M/\u223c). An abstract tangent vector \u03be[x] \u2208 T[x](M/\u223c) at [x] has a unique element \u03bex \u2208 Hx that is called its horizontal lift.\nA Riemannian metric gx : TxM \u00d7 TxM \u2192 R at x \u2208 M defines a Riemannian metric g[x] : T[x](M/\u223c) \u00d7 T[x](M/\u223c) \u2192 R, i.e., g[x](\u03be[x], \u03b7[x]) := gx(\u03bex, \u03b7x) on the quotient manifoldM/\u223c, if gx(\u03bex, \u03b7x) does not depend on a specific representation along the equivalence class [x]. Here, \u03be[x] and \u03b7[x] are tangent vectors in T[x](M/\u223c), and \u03bex and \u03b7x are their horizontal lifts in Hx at x, respectively. Equivalently, the definition of the Riemannian metric is well posed when gx(\u03bex, \u03b6x) = gx(\u03bey, \u03b6y) for all x, y \u2208 [x], where \u03bex, \u03b6x \u2208 Hx and \u03bey, \u03b6y \u2208 Hy are the horizontal lifts of \u03be[x], \u03b6[x] \u2208 T[x](M/\u223c) along the same equivalence class [x]. This holds true for the proposed metric (9) as shown in Proposition 1. From [1], endowed with the Riemannian metric (9), the quotient manifoldM/\u223c is a Riemannian submersion ofM. The submersion principle allows to work out concrete matrix representations of abstract object onM/\u223c, e.g., the gradient of a smooth cost function [1].\nStarting from an arbitrary matrix (with appropriate dimensions), two linear projections are needed: the first projection \u03a8x is onto the tangent space TxM, while the second projection \u03a0x is onto the horizontal subspaceHx. The computation cost of these is O(n1r21 + n2r22 + n3r23).\nThe tangent space TxM projection is obtained by extracting the component normal to TxM in the ambient space. The normal spaceNxM has the matrix characterization {(U1SU1(G1GT1 )\u22121,U2SU2(G2GT2 )\u22121,\nU3SU3(G3G T 3 ) \u22121, 0) : SUd \u2208 Rrd\u00d7rd ,STUd = SUd , for d \u2208 {1, 2, 3}}. Symmetric matrices SUd for all d \u2208 {1, 2, 3} parameterize the normal space. Finally, the operator \u03a8x : Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 \u2192 TxM : (YU1 ,YU2 ,YU3 ,YG) 7\u2192 \u03a8x(YU1 ,YU2 ,YU3 ,YG) is given as follows. Proposition 2. The quotient manifold (5) endowed with the metric (9) admits the tangent space projector defined as\n\u03a8x(YU1 ,YU2 ,YU3 ,YG) = (YU1 \u2212 U1SU1(G1GT1 )\u22121,YU2 \u2212 U2SU2(G2GT2 )\u22121, YU3 \u2212 U3SU3(G3GT3 )\u22121,YG),\n(10)\nwhere SUd is the solution to the Lyapunov equation SUdGdG T d +GdG T d SUd = GdG T d (Y T UdUd+U T d YUd)GdG T d for d \u2208 {1, 2, 3}. The Lyapunov equations in Proposition 2 are solved efficiently with the Matlab\u2019s lyap routine. The horizontal space projection of a tangent vector is obtained by removing the component along the vertical space. The vertical space Vx has the matrix characterization {(U1\u21261,U2\u21262,U3\u21263,\u2212(G\u00d71\u21261+ G\u00d72\u21262+G\u00d73\u21263)) : \u2126d \u2208 Rrd\u00d7rd ,\u2126Td = \u2212\u2126d for d \u2208 {1, 2, 3}}. Skew symmetric matrices \u2126d for all d \u2208 {1, 2, 3} parameterize the vertical space. Finally, the horizontal projection operator \u03a0x : TxM :\u2192 Hx : \u03b7x 7\u2192 \u03a0x(\u03b7x) is given as follows. Proposition 3. The quotient manifold (5) endowed with the metric (9) admits the horizontal projector defined as\n\u03a0x(\u03b7x) = (\u03b7U1 \u2212 U1\u21261, \u03b7U2 \u2212 U2\u21262, \u03b7U3 \u2212 U3\u21263, \u03b7G \u2212 (\u2212(G\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263))), where \u03b7x = (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G) \u2208 TxM and \u2126d is a skew-symmetric matrix of size rd \u00d7 rd that is the solution to the coupled Lyapunov equations G1GT1 \u21261 + \u21261G1GT1 \u2212G1(Ir3 \u2297\u21262)GT1 \u2212G1(\u21263 \u2297 Ir2)GT1 = Skew(UT1 \u03b7U1G1G T 1 ) + Skew(G1\u03b7TG1), G2GT2 \u21262 + \u21262G2GT2 \u2212G2(Ir3 \u2297\u21261)GT2 \u2212G2(\u21263 \u2297 Ir1)GT2 = Skew(UT2 \u03b7U2G2G T 2 ) + Skew(G2\u03b7TG2),\nG3GT3 \u21263 + \u21263G3GT3 \u2212G3(Ir2 \u2297\u21261)GT3 \u2212G3(\u21262 \u2297 Ir1)GT3 = Skew(UT3 \u03b7U3G3G T 3 ) + Skew(G3\u03b7TG3),\n(11)\nwhere Skew(\u00b7) extracts the skew-symmetric part of a square matrix, i.e., Skew(D) = (D\u2212 DT )/2. The coupled Lyapunov equations (11) are solved efficiently with the Matlab\u2019s pcg routine that is combined with a specific symmetric preconditioner resulting from the Gauss-Seidel approximation of (11). For the variable \u21261, the preconditioner is of the form G1GT1 \u21261 + \u21261G1GT1 . Similarly, for the variables \u21262 and \u21263.\nRetraction. A retraction is a mapping that maps vectors in the horizontal space to points on the search spaceM and satisfies the local rigidity condition [1]. It provides a natural way to move on the manifold along a search direction. Because the total spaceM has the product nature, we can choose a retraction by combining retractions on the individual manifolds, i.e., Rx(\u03bex) = (uf(U1 + \u03beU1), uf(U2 + \u03beU2),uf(U3 + \u03beU3),G + \u03beG), where \u03bex \u2208 Hx and uf(\u00b7) extracts the orthogonal factor of a full column rank matrix, i.e., uf(A) = A(ATA)\u22121/2. The retractionRx defines a retractionR[x](\u03be[x]) := [Rx(\u03bex)] on the quotient manifoldM/ \u223c, as the equivalence class [Rx(\u03bex)] does not depend on specific matrix representations of [x] and \u03be[x], where \u03bex is the horizontal lift of the abstract tangent vector \u03be[x] \u2208 T[x](M/ \u223c).\nVector transport. A vector transport on a manifoldM is a smooth mapping that transports a tangent vector \u03bex \u2208 TxM at x \u2208M to a vector in the tangent space at a pointRx(\u03b7x). It is defined by the symbol T\u03b7x\u03bex. It generalizes the classical concept of translation of vectors in the Euclidean space to manifolds [1, Section 8.1.4]. The horizontal lift of the abstract vector transport T\u03b7[x]\u03be[x] onM/\u223c has the matrix characterization \u03a0Rx(\u03b7x)(T\u03b7x\u03bex) = \u03a0Rx(\u03b7x)(\u03a8Rx(\u03b7x)(\u03bex)), where \u03bex and \u03b7x are the horizontal lifts in Hx of \u03be[x] and \u03b7[x] that belong to T[x](M/\u223c). \u03a8x(\u00b7) and \u03a0x(\u00b7) are projectors defined in Propositions 2 and 3. The computational cost of transporting a vector solely depends on the projection and retraction operations."}, {"heading": "4 Riemannian algorithms for (1)", "text": "We propose two Riemannian preconditioned algorithms for the tensor completion problem (1) that are based on the developments in Section 3. The preconditioning effect follows from the specific choice of the metric (9). In the batch setting, we use the off-the-shelf conjugate gradient implementation of Manopt for any smooth cost function [5]. A complete description of the Riemannian nonlinear conjugate gradient method is in [1, Chapter 8]. In the online setting, we use the stochastic gradient descent implementation [2]. For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2]. However, as simulations show, convergence to global minima is observed in many challenging instances.\nIn addition to the manifold-related ingredients in Section 3, the ingredients needed are the cost function specific ones. To this end, we show the computation of the Riemannian gradient as well as a way to compute an initial guess for the step-size, which is used in the conjugate gradient method. The concrete formulas are shown in Table 1.\nRiemannian gradient computation. Let f(X ) = \u2016P\u2126(X )\u2212P\u2126(X ?)\u20162F /|\u2126| be the mean square error function of (1), and S = 2(P\u2126(G\u00d71U1\u00d72U2\u00d73U3)\u2212P\u2126(X ?))/|\u2126| be an auxiliary sparse tensor variable that is interpreted as the Euclidean gradient of f in Rn1\u00d7n2\u00d7n3 . The partial derivatives of f with respect to (U1,U2,U3,G) are computed in terms of the unfolding matrices Sd. Due to the specific scaled metric (9), the partial derivatives are further scaled by ((G1GT1 )\u22121, (G2GT2 )\u22121, (G3GT3 )\u22121,I), denoted as egradxf (after scaling). Finally, from the Riemannian submersion theory [1, Section 3.6.2], the horizontal lift of grad[x]f is equal to gradxf = \u03a8(egradxf). The total numerical cost of computing the Riemannian gradient depends on computing the partial derivatives, which is O(|\u2126|r1r2r3).\nProposition 4. The cost function (1) at (U1,U2,U3,G) under the quotient manifold (5) endowed with\nthe Riemannian metric (9) admits the horizontal lift of the Riemannian gradient\n(S1(U3 \u2297 U2)GT1 (G1GT1 )\u22121 \u2212 U1BU1(G1GT1 )\u22121, S2(U3 \u2297 U1)GT2 (G2GT2 )\u22121 \u2212 U2BU2(G2GT2 )\u22121, S3(U2 \u2297 U1)GT3 (G3GT3 )\u22121 \u2212 U3BU3(G3GT3 )\u22121,S \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 ),\n(12)\nwhere BUd for d \u2208 {1, 2, 3} are the solutions to the Lyapunov equations BU1G1G T 1 + G1GT1 BU1 = 2Sym(G1G T 1 UT1 (S1(U3 \u2297 U2)GT2 ), BU2G2G T 2 + G2GT2 BU2 = 2Sym(G2G T 2 UT2 (S2(U3 \u2297 U1)GT2 ),\nBU3G3G T 3 + G3GT3 BU3 = 2Sym(G3G T 3 UT3 (S3(U2 \u2297 U1)GT3 ),\nwhere Sym(\u00b7) extracts the symmetric part of a matrix.\nInitial guess for the step-size. Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold. Given a search direction \u03bex \u2208 Hx, the step-size guess is arg mins\u2208R+ \u2016P\u2126(G\u00d71U1\u00d72U2\u00d73U3 + sG\u00d71\u03beU1\u00d72U2\u00d73U3 + sG\u00d71U1\u00d72\u03beU2\u00d73U3+sG\u00d71U1\u00d72U2\u00d73\u03beU3+s\u03beG\u00d71U1\u00d72U2\u00d73U3)\u2212P\u2126(X ?)\u20162F , which has a closedform expression and the numerical cost of computing it is O(|\u2126|r1r2r3).\nStochastic gradient descent in online setting. In the online setting, we update (U1,U2,U3,G) every time a frontal slice, i.e., a matrix \u2208 Rn1\u00d7n2 , is randomly sampled from X ?i1,i2,i3 . Equivalently, we assume that the tensor grows along the third dimension. More concretely, we calculate the rank-one Riemannian gradient (12) for the input slice. (U1,U2,U3,G) are updated by taking a step along the negative Riemannian gradient direction. Subsequently, we retract using Rx. A popular formula for the step-size \u03b3k at k-th update is \u03b3k = \u03b30/(1 + \u03b30\u03bbk), where \u03b30 is the initial step-size and \u03bb is a fixed reduction factor. Following [3], we select \u03b30 in the pre-training phase using a small sample size of a training set. \u03bb is fixed to 10\u22127.\nComputational cost. The total computational cost per iteration of our proposed conjugate gradient implementation is O(|\u2126|r1r2r3), where |\u2126| is the number of known entries. It should be stressed that the computational cost of our conjugate gradient implementation is equal to that of [13]. In the online setting, each stochastic gradient descent update costs O(|\u2126slice|r1r2 + n1r21 + n2r22 + Tr23 + r1r2r3), where |\u2126slice| is the number of known entries of the current frontal slice of the incomplete tensor X ?i1,i2,i3 , and T is the number of slices that we have seen along n3 direction."}, {"heading": "5 Numerical comparisons", "text": "In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms. In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10]. All simulations are performed in Matlab on a 2.6 GHz Intel Core i7 machine with 16 GB RAM. For specific operations with unfoldings of S, we use the mex interfaces for Matlab that are provided by the authors of geomCG. For large-scale instances, our algorithm is only compared with geomCG as others cannot handle them. Cases S and R are for batch instances, whereas Case O is for online instances. Since the dimension of the space of a tensor \u2208 Rn1\u00d7n2\u00d7n3 of rank r = (r1, r2, r3) is dim(M/\u223c) =\u22113 d=1(ndrd \u2212 r2d) + r1r2r3, we randomly and uniformly select known entries based on a multiple of the dimension, called the over-sampling (OS) ratio, to create the train set \u2126. Algorithms are initialized randomly, as suggested in [13], and are stopped when either the mean square error (MSE) on the train set \u2126 is below 10\u221212 or the number of iterations exceeds 250. We also evaluate the mean square error on a test set \u0393, which is different from \u2126. Five runs are performed in each scenario and the plots show all of them. The time plots are shown with standard deviations. It should be noted that we show most numerical\ncomparisons on the test set \u0393 as it allows to compare with nuclear norm minimization algorithms, which optimize a different (training) cost function. Additional plots are provided as supplementary material.\nCase S1: comparison with the Euclidean metric. We first show the benefit of the proposed metric (9) over the conventional choice of the Euclidean metric that exploits the product structure of M and symmetry (3). This is defined by combining the individual natural metrics for St(rd, nd) and Rr1\u00d7r2\u00d7r3 . For simulations, we randomly generate a tensor of size 200 \u00d7 200 \u00d7 200 and rank r = (10, 10, 10). OS is 10. For simplicity, we compare gradient descent algorithms with Armijo backtracking linesearch for both the metric choices. Figure 2(a) shows that the algorithm with the metric (9) gives a superior performance in train error than that of the conventional metric choice.\nCase S2: small-scale instances. Small-scale tensors of size 100 \u00d7 100 \u00d7 100, 150 \u00d7 150 \u00d7 150, and 200 \u00d7 200 \u00d7 200 and rank r = (10, 10, 10) are considered. OS is {10, 20, 30}. Figure 2(b) shows that our proposed algorithm has faster convergence than others. In Figure 2(c), the lowest test errors are obtained by our proposed algorithm and geomCG.\nCase S3: large-scale instances. We consider large-scale tensors of size 3000 \u00d7 3000 \u00d7 3000, 5000 \u00d7 5000 \u00d7 5000, and 10000 \u00d7 10000 \u00d7 10000 and ranks r = (5, 5, 5) and (10, 10, 10). OS is 10. Our proposed algorithm outperforms geomCG in Figure 2(d).\nCase S4: influence of low sampling. We look into problem instances from scarcely sampled data, e.g., OS is 4. The test requires completing a tensor of size 10000\u00d710000\u00d710000 and rank r = (5, 5, 5).\nFigure 2(e) shows the superior performance of the proposed algorithm against geomCG. Whereas the test error increases for geomCG, it decreases for the proposed algorithm.\nCase S5: influence of ill-conditioning and low sampling. We consider the problem instance of Case S4 with OS = 5. Additionally, for generating the instance, we impose a diagonal core G with exponentially decaying positive values of condition numbers (CN) 5, 50, and 100. Figure 2(f) shows that the proposed algorithm outperforms geomCG for all the considered CN values.\nCase S6: influence of noise. We evaluate the convergence properties of algorithms under the presence of noise by adding scaled Gaussian noise P\u2126(E) to P\u2126(X ?) as in [13]. The different noise levels are = {10\u22124, 10\u22126, 10\u22128, 10\u221210, 10\u221212}. In order to evaluate for = 10\u221212, the stopping threshold on the MSE of the train set is lowered to 10\u221224. The tensor size and rank are same as in Case S4 and OS is 10. Figure 2(g) shows that the test error for each is almost identical to the 2\u2016P\u2126(X ?)\u20162F [13], but our proposed algorithm converges faster than geomCG.\nCase S7: rectangular instances. We consider instances where the dimensions and ranks along certain modes are different than others. Two cases are considered. Case (7.a) considers tensors size 20000\u00d7 7000\u00d7 7000, 30000\u00d7 6000\u00d7 6000, and 40000\u00d7 5000\u00d7 5000 with rank r = (5, 5, 5). Case (7.b) considers a tensor of size 10000\u00d7 10000\u00d7 10000 with ranks (7, 6, 6), (10, 5, 5), and (15, 4, 4). In all the cases, the proposed algorithm converges faster than geomCG as shown in Figure 2(h).\nCase R1: hyperspectral image. We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13]. The tensor size is 1017 \u00d7 1340 \u00d7 33, where each slice corresponds to a particular image measured at a different wavelength. As suggested in [23, 13], we resize it to 203 \u00d7 268 \u00d7 33. We perform five random samplings of the pixels based on the OS values 11 and 22, corresponding to the rank r=(15, 15, 6) adopted in [13]. This set is further randomly split into 80/10/10\u2013train/validation/test partitions. The algorithms are stopped when the MSE on the validation set starts to increase. While OS = 22 corresponds to the observation ratio of 10% studied in [13], OS = 11 considers a challenging scenario with the observation ratio of 5%. Figures 2(i) shows the good performance of our algorithm. Table 2 compiles the results.\nCase R2: MovieLens-10M2. This dataset contains 10000054 ratings corresponding to 71567 users and 10681 movies. We split the time into 7-days wide bins results, and finally, get a tensor of size 71567 \u00d7 10681 \u00d7 731. The fraction of known entries is less than 0.002%. The completion task on this dataset reveals periodicity of the latent genres. We perform five random 80/10/10\u2013train/validation/test partitions. The maximum iteration threshold is set to 500. In Table 2, our proposed algorithm consistently gives lower test errors than geomCG across different ranks.\nCase O: online instances. We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10]. As the implementations of TeCPSGD and OLSTEC are computationally more intensive than ours, our plots\n2http://grouplens.org/datasets/movielens/.\nonly show test MSE against the number of outer iterations, i.e., the number of the passes through the data.\nFigure 3(a) shows comparisons on a synthetic instance of tensor size 100 \u00d7 100 \u00d7 10000 with rank r = (5, 5, 5). \u03b30 is selected from the step-size list {8, 9, 10, 11, 12} in the pre-training phase. 10% entries are randomly observed. The pre-training uses 10% frontal slices of all the slices. The maximum number of outer loops is set to 100. Figure 3(a) shows five different runs, where the online algorithm has the same asymptotic convergence behavior as the batch counterpart on a test dataset \u0393. Figure 3(b) shows comparisons on the Airport Hall surveillance video sequence dataset3 of size 176\u00d7144 with 1000 frames. \u03b30 is selected from {30, 40, 50, 60, 70} and 10% frontal slices are selected for pre-training. 2% of the entries are observed. In Figure 3(b), both the proposed online and batch algorithms achieve lower test errors than TeCPSGD and OLSTEC."}, {"heading": "6 Conclusion", "text": "We have proposed preconditioned batch (conjugate gradient) and online (stochastic gradient descent) algorithms for the tensor completion problem. The algorithms stem from the Riemannian preconditioning approach that exploits the fundamental structures of symmetry (due to non-uniqueness of Tucker decomposition) and least-squares of the cost function. A novel Riemannian metric (inner product) is proposed that enables to use the versatile Riemannian optimization framework. Numerical comparisons suggest that our proposed algorithms have a superior performance on different benchmarks."}, {"heading": "Acknowledgments", "text": "We thank Rodolphe Sepulchre, Paul Van Dooren, and Nicolas Boumal for useful discussions on the paper. This paper presents research results of the Belgian Network DYSCO (Dynamical Systems, Control, and Optimization), funded by the Interuniversity Attraction Poles Programme, initiated by the Belgian State, Science Policy Office. The scientific responsibility rests with its authors. Hiroyuki Kasai is (partly) supported by the Ministry of Internal Affairs and Communications, Japan, as the SCOPE Project (150201002). This work was initiated while Bamdev Mishra was with the Department of Electrical Engineering and Computer Science, University of Lie\u0300ge, 4000 Lie\u0300ge, Belgium and was visiting the Department of Engineering (Control Group), University of Cambridge, Cambridge, UK. He was supported as a research fellow (aspirant) of the Belgian National Fund for Scientific Research (FNRS).\n3http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html"}, {"heading": "A Proof and derivation of manifold-related ingredients", "text": "The concrete computations of the optimization-related ingredients presented in the paper are discussed below.\nThe total space isM := St(r1, n1)\u00d7St(r2, n2)\u00d7St(r3, n3)\u00d7Rr1\u00d7r2\u00d7r3 . Each element x \u2208M has the matrix representation (U1,U2,U3,G). Invariance of Tucker decomposition under the transformation (U1,U2,U3,G) 7\u2192 (U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ) for all Od \u2208 O(rd), the set of orthogonal matrices of size of rd \u00d7 rd results in equivalence classes of the form [x] = [(U1,U2,U3,G)] := {(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ) : Od \u2208 O(rd)}.\nA.1 Tangent space characterization and the Riemannian metric\nThe tangent space, TxM, at x given by (U1,U2,U3,G) in the total spaceM is the product space of the tangent spaces of the individual manifolds. From [1], the tangent space has the matrix characterization\nTxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) \u2208 Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 : UTd ZUd + Z T Ud Ud = 0, for d \u2208 {1, 2, 3}}. (A.1)\nThe proposed metric gx : TxM\u00d7 TxM\u2192 R is\ngx(\u03bex, \u03b7x) = \u3008\u03beU1 , \u03b7U1(G1GT1 )\u3009+ \u3008\u03beU2 , \u03b7U2(G2GT2 )\u3009+ \u3008\u03beU3 , \u03b7U3(G3GT3 )\u3009+ \u3008\u03beG , \u03b7G\u3009, (A.2)\nwhere \u03bex, \u03b7x \u2208 TxM are tangent vectors with matrix characterizations (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) and (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G), respectively and \u3008\u00b7, \u00b7\u3009 is the Euclidean inner product.\nA.2 Characterization of the normal space\nGiven a vector in Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 , its projection onto the tangent space TxM is obtained by extracting the component normal, in the metric sense, to the tangent space. This section describes the characterization of the normal space, NxM.\nLet \u03b6x = (\u03b6U1 , \u03b6U2 , \u03b6U3 , \u03b6G) \u2208 NxM, and \u03b7x = (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G) \u2208 TxM. Since \u03b6x is orthogonal to \u03b7x, i.e., gx(\u03b6x, \u03b7x) = 0, the conditions\nTrace(GdGTd \u03b6 T Ud\u03b7Ud) = 0, for d \u2208 {1, 2, 3} (A.3)\nmust hold for all \u03b7x in the tangent space. Additionally from [1], \u03b7Ud has the characterization\n\u03b7Ud = Ud\u2126 + Ud\u22a5K, (A.4)\nwhere \u2126 is any skew-symmetric matrix, K is a any matrix of size (nd \u2212 rd)\u00d7 rd, and Ud\u22a5 is any nd \u00d7 (nd \u2212 rd) that is orthogonal complement of Ud. Let \u03b6\u0303Ud = \u03b6UdGdGTd and let \u03b6\u0303Ud is defined as\n\u03b6\u0303Ud = UdA + Ud\u22a5B (A.5)\nwithout loss of generality, where A \u2208 Rrd\u00d7rd and B \u2208 R(nd\u2212rd)\u00d7rd are to be characterized from (A.3) and (A.4). A few standard computations show that A has to be symmetric and B = 0. Consequently, \u03b6\u0303Ud = UdSUd , where SUd = S T Ud . Equivalently, \u03b6Ud = UdSUd(GdG T d ) \u22121 for a symmetric matrix SUd . Finally, the normal space NxM has the characterization\nNxM = {(U1SU1(G1GT1 )\u22121,U2SU2(G2GT2 )\u22121,U3SU3(G3GT3 )\u22121, 0) : SUd \u2208 Rrd\u00d7rd ,STUd = SUd , for d \u2208 {1, 2, 3}}.\n(A.6)\nA.3 Characterization of the vertical space\nThe horizontal space projector of a tangent vector is obtained by removing the component along the vertical direction. This section shows the matrix characterization of the vertical space Vx. Vx is the defined as the linearization of the equivalence class [(U1,U2,U3,G)] at x = [(U1,U2,U3,G)]. Equivalently, Vx is the linearization of (U1O1,U2O2,U3O3,G\u00d71OT1 \u00d72OT2 \u00d73OT3 ) along Od \u2208 O(rd) at the identity element for d \u2208 {1, 2, 3}. From the characterization of linearization of an orthogonal matrix [1], we have the characterization for the vertical space as\nVx = {(U1\u21261,U2\u21262,U3\u21263,\u2212(G\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263)) : \u2126d \u2208 Rrd\u00d7rd ,\u2126Td = \u2212\u2126d for d \u2208 {1, 2, 3}}.\n(A.7)\nA.4 Characterization of the horizontal space\nThe characterization of the horizontal space Hx is derived from its orthogonal relationship with the vertical space Vx.\nLet \u03bex = (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) \u2208 Hx, and \u03b6x = (\u03b6U1 , \u03b6U2 , \u03b6U3 , \u03b6G) \u2208 Vx. Since \u03bex must be orthogonal to \u03b6x, which is equivalent to gx(\u03bex, \u03b6x) = 0 in (A.2), the characterization for \u03bex is derived from (A.2) and (A.7).\ngx(\u03bex, \u03b6x) = \u3008\u03beU1 , \u03b6U1(G1GT1 )\u3009+ \u3008\u03beU2 , \u03b6U2(G2GT2 )\u3009+ \u3008\u03beU3 , \u03b6U3(G3GT3 )\u3009+ \u3008\u03beG , \u03b6G\u3009 = \u3008\u03beU, (U1\u21261)(G1GT1 )\u3009+ \u3008\u03beU2 , (U2\u21262)(G2GT2 )\u3009+ \u3008\u03beU3 , (U3\u21263)(G3GT3 )\u3009\n+\u3008\u03beG ,\u2212(G\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263)\u3009 (Switch to unfoldings of G.)\n= Trace((G1GT1 )\u03be T U1(U1\u21261)) + Trace((G2G T 2 )\u03be T U2(U2\u21262)) + Trace((G3G T 3 )\u03be T U3(U3\u21263))\n+Trace(\u03beG1(\u2212\u21261G1)T ) + Trace(\u03beG2(\u2212\u21262G2)T ) + Trace(\u03beG3(\u2212\u21263G3)T ) = Trace [{ (G1GT1 )\u03be T U1U1 + \u03beG1G T 1 } \u21261 ] + Trace [{ (G2GT2 )\u03be T U2U2 + \u03beG2G T 2 } \u21262 ]\n+Trace [{\n(G3GT3 )\u03be T U3U3 + \u03beG3G T 3 } \u21263 ] ,\nwhere \u03beGd is the mode-d unfolding of \u03beG . Since gx(\u03bex, \u03b6x) above should be zero for all skew-matrices \u2126d, \u03bex = (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) \u2208 Hx must satisfy\n(GdGTd )\u03beTUdUd + \u03beGdG T d is symmetric for d \u2208 {1, 2, 3}. (A.8)\nA.5 Proof of Proposition 1\nWe first introduce the following lemma:\nLemma 1. Let (U1,U2,U3,G) \u2208 St(r1, n1) \u00d7 St(r2, n2) \u00d7 St(r3, n3) \u00d7 Rr1\u00d7r2\u00d7r3 and \u03be[(U1,U2,U3,G)] be a tangent vector to the quotient manifold at [(U1,U2,U3,G)]. The horizontal lifts of \u03be[(U1,U2,U3,G)] at (U1,U2,U3,G) and (U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ) are related for Od \u2208 O(rd) as follows,\n(\u03beU1O1 , \u03beU2O2 , \u03beU3O3 , \u03beG\u00d71OT1 \u00d72OT2 \u00d73OT3 ) = (\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 ). (A.9)\nProof. Let f : (St(r1, n1)\u00d7 St(r2, n2)\u00d7 St(r3, n3)\u00d7 Rr1\u00d7r2\u00d7r3/(O(r1)\u00d7O(r2)\u00d7O(r3)))\u2192 R be an arbitrary smooth function, and define\nf\u0304 := f \u25e6 \u03c0 : (St(r1, n1)\u00d7 St(r2, n2)\u00d7 St(r3, n3)\u00d7 Rr1\u00d7r2\u00d7r3/(O(r1)\u00d7O(r2)\u00d7O(r3)))\u2192 R,\nwhere \u03c0 is the mapping \u03c0 :M\u2192M/ \u223c defined by x 7\u2192 [x]. Consider the mapping\nh : (U1,U2,U3,G) 7\u2192 (U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ),\nwhere Od \u2208 O(rd). Since \u03c0(h(U1,U2,U3,G)) = \u03c0(U1,U2,U3,G) for all (U1,U2,U3,G), we have\nf\u0304(h(U1,U2,U3,G)) = f\u0304(U1,U2,U3,G).\nBy taking the differential of both sides,\nDf\u0304(h(U1,U2,U3,G))[Dh(U1,U2,U3,G)[(\u03beU1 , \u03beU2 , \u03beU3 , \u03beG)]] = Df\u0304(U1,U2,U3,G)[(\u03beU1 , \u03beU2 , \u03beU3 , \u03beG)]. (A.10)\nBy noting the definition of (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG), i.e., D\u03c0(U1,U2,U3,G)[\u03beU1 , \u03beU2 , \u03beU3 , \u03beG ] = \u03be[(\u03beU1 ,\u03beU2 ,\u03beU3 ,\u03beG)], the right side of (A.10) is\nDf\u0304(U1,U2,U3,G)[(\u03beU1 , \u03beU2 , \u03beU3 , \u03beG)] = Df(\u03c0(U1,U2,U3,G))[D\u03c0(U1,U2,U3,G)[(\u03beU1 , \u03beU2 , \u03beU3 , \u03beG)] = Df(\u03c0(U1,U2,U3,G))[\u03be[(U1,U2,U3,G)]],\nwhere the chain rule is applied to the first equality. Moreover, from the directional derivatives of the mapping h, the bracket of the left side of (A.10) is obtained as\nDh(U1,U2,U3,G)[(\u03beU1 , \u03beU2 , \u03beU3 , \u03beG)] = (\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 ).\nTherefore, (A.10) yields\nDf\u0304(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 )[(\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 )] = Df(\u03c0(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ))[\u03be[(U1,U2,U3,G)]],\n(A.11) where we address the equivalence class \u03c0(U1,U2,U3,G) = \u03c0(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ). The left side of (A.11) is further transformed by the chain rule as\nDf\u0304(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 )[(\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 )] = Df(\u03c0(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ))[D\u03c0(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 )]\n[(\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 )]. (A.12)\nBy comparing the right sides of (A.11) and (A.12), since this equality holds for any smooth function f , it implies that\nD\u03c0(U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 )[(\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 )] = \u03be[(U1,U2,U3,G)].\n(A.13) Finally, we check whether (\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 ) is an element of H(U1O1,U2O2,U3O3,G\u00d71OT1 \u00d72OT2 \u00d73OT3 ). Addressing that the mode-1 unfolding of G\u00d71O T 1\u00d72OT2\u00d73OT3 is OT1 G1(OT3 \u2297 OT2 )T , plugging (\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 ) into (GdGTd )\u03beTUdUd + \u03beGdG T d in (A.8) yields\n(OT1 G1(OT3 \u2297OT2 )T ) ( OT1 G1(OT3 \u2297OT2 )T )T (\u03beU1O1)T (U1O1)\n+(OT1 )T \u03beG1(O T 3 \u2297OT2 )(OT1 G1(OT3 \u2297OT2 )T )T\n= OT1 G1GT1 O1 + OT1 \u03beG1G T 1 O1 = OT1 ((G1GT1 )\u03beTU1U1 + \u03beG1G T 1 )O1.\n(A.14)\nSince (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) is a symmetric matrix, the obtained result is also symmetric. Therefore, (\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 ) is a horizontal vector at (U1O1,U2O2,U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ). This implies that (\u03beU1O1, \u03beU2O2, \u03beU3O3, \u03beG\u00d71OT1\u00d72OT2\u00d73OT3 ) is the horizontal lift of \u03be at (U1O1,U2O2, U3O3,G\u00d71OT1\u00d72OT2\u00d73OT3 ), and the proof is completed.\nNow, the proof of Proposition 1 is given below using the result (A.9) in Lemma 1.\nProof. Plugging \u03be \u2032 U1 = \u03beU1O1 , \u03b7 \u2032 U1 = \u03b7U1O1 , and G \u2032 1 = OT1 G1(OT3 \u2297 OT2 ) into the first term of (A.2) yields\n\u3008\u03beU1O1 , \u03b7U1O1(G \u2032 1G \u2032T 1 ))\u3009 = Trace(\u03beTU1O1\u03b7U1O1(G \u2032 1G \u2032T 1 ))\n(A.9) = Trace((\u03beU1O1)T \u03b7U1O1(G \u2032 1G \u2032T 1 )) = Trace [ (\u03beU1O1)T (\u03b7U1O1)(O T 1 G1(OT3 \u2297OT2 )T ) ( OT1 G1(OT3 \u2297OT2 )T )T ] = Trace [ (\u03beU1O1)T (\u03b7U1O1)O T 1 G1(OT3 \u2297OT2 )T (OT3 \u2297OT2 )GT1 O1\n] = Trace [ OT1 \u03beTU1\u03b7U1O1O T 1 G1GT1 O1\n] = Trace [ \u03beTU1\u03b7U1G1G T 1\n] = \u3008\u03beU1 , \u03b7U1(G1GT1 )\u3009.\nSince the same equalities against the each term in the metric (A.2) corresponding to U2, U3 and G hold, we finally obtain the invariant property that the proposition claims;\ng(U1,U2,U3,G)((\u03beU1 , \u03beU2 , \u03beU3 , \u03beG), (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G))\n= g(U1O1,U2O2,U3O3,G\u00d71OT1 \u00d72OT2 \u00d73OT3 ) ((\u03beU1O1 , \u03beU2O2 , \u03beU3O3 , \u03beG\u00d71OT1 \u00d72OT2 \u00d73OT3 ),\n(\u03b7U1O1 , \u03b7U2O2 , \u03b7U3O3 , \u03b7G\u00d71OT1 \u00d72OT2 \u00d73OT3 )).\nA.6 Proof of Proposition 2 (derivation of the tangent space projector)\nProof. The tangent space TxM projector is obtained by extracting the component normal to TxM in the ambient space. The normal space NxM has the matrix characterization shown in (A.6). The operator \u03a8x : Rn1\u00d7r1\u00d7Rn2\u00d7r2\u00d7Rn3\u00d7r3\u00d7Rr1\u00d7r2\u00d7r3 \u2192 TxM : (YU1 ,YU2 ,YU3 ,YG) 7\u2192 \u03a8x(YU1 ,YU2 ,YU3 ,YG) has the expression\n\u03a8x(YU1 ,YU2 ,YU3 ,YG) = (YU1 \u2212 U1SU1(G1GT1 )\u22121,YU2 \u2212 U2SU2(G2GT2 )\u22121, YU3 \u2212 U3SU3(G3GT3 )\u22121,YG).\n(A.15)\nFrom the definition of the tangent space in (A.1), Ud should satisfy\n\u03b7TUdUd + U T d \u03b7Ud = (YUd \u2212 UdSUd(GdGTd )\u22121)TUd + UTd (YUd \u2212 UdSUd(GdGTd )\u22121)\n= YTUdUd \u2212 (GdG T d ) \u22121STUdU T d Ud + U T d YUd \u2212 UTd UdSUd(GdGTd )\u22121 = YTUdUd \u2212 (GdG T d ) \u22121SUd + U T d YUd \u2212 SUd(GdGTd )\u22121 = 0.\nMultiplying (GdGTd ) from the right and left sides results in\n(GdGTd ) \u22121SUd + SUd(GdG T d ) \u22121 = YTUdUd + U T d YUd\nSUdGdG T d + GdG T d SUd = GdG T d (Y T UdUd + U T d YUd)GdG T d .\nFinally, we obtain the Lyapunov equation as\nSUdGdG T d + GdG T d SUd = GdG T d (Y T UdUd + U T d YUd)GdG T d for d \u2208 {1, 2, 3}, (A.16)\nthat are solved efficiently with the Matlab\u2019s lyap routine.\nA.7 Proof of Proposition 3 (derivation of the horizontal space projector)\nProof. We consider the projection of a tangent vector \u03b7x = (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G) \u2208 TxM into a vector \u03bex = (\u03beU1 , \u03beU2 , \u03beU3 , \u03beG) \u2208 Hx. This is achieved by subtracting the component in the vertical space Vx in (A.7) as \u03b7U1 = \u03b7U1 \u2212 U1\u21261\ufe38 \ufe37\ufe37 \ufe38 =\u03beU1\u2208Hx + U1\u21261\ufe38 \ufe37\ufe37 \ufe38 \u2208Vx ,\n\u03b7U2 = \u03b7U2 \u2212 U2\u21262 + U2\u21262, \u03b7U3 = \u03b7U3 \u2212 U3\u21263 + U3\u21263, \u03b7G = \u03b7G \u2212 (\u2212(G\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263)) + (\u2212(G\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263)).\nAs a result, the horizontal operator \u03a0x : TxM\u2192Hx : \u03b7x 7\u2192 \u03a0x(\u03b7x) has the expression \u03a0x(\u03b7x) = (\u03b7U1 \u2212 U1\u21261, \u03b7U2 \u2212 U2\u21262, \u03b7U3 \u2212 U3\u21263, \u03b7G \u2212 (\u2212(G\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263))),\n(A.17) where \u03b7x = (\u03b7U1 , \u03b7U2 , \u03b7U3 , \u03b7G) \u2208 TxM and \u2126d is a skew-symmetric matrix of size rd \u00d7 rd. The skew-matrices \u2126d for d = {1, 2, 3} that are identified based on the conditions (A.8).\nIt should be noted that the tensor G\u00d71\u21261 +G\u00d72\u21262 +G\u00d73\u21263 in (A.7) has the following equivalent unfoldings.\nG\u00d71\u21261 + G\u00d72\u21262 + G\u00d73\u21263 mode \u22121\u21d0===\u21d2 \u21261G1 + G1(Ir3 \u2297\u21262)T + G1(\u21263 \u2297 Ir2)T mode \u22122\u21d0===\u21d2 G2(Ir3 \u2297\u21261)T + \u21262G2 + G2(\u21263 \u2297 Ir1)T mode \u22123\u21d0===\u21d2 G3(Ir2 \u2297\u21261)T + G3(\u21262 \u2297 Ir1)T + \u21263G3.\nPlugging \u03beU1 = \u03b7U1 \u2212U1\u21261 and \u03beG1 = \u03b7G1 + \u21261G1 + G1(Ir3 \u2297\u21262)T + G1(\u21263\u2297 Ir2)T into (A.8) and using the relation (A\u2297 B)T = AT \u2297 BT results in\n(G1GT1 )\u03beTU1U+\u03beG1G T 1 = (G1GT1 )(\u03b7U1 \u2212 U1\u21261)TU1 + { \u03b7G1 + (\u21261G1 + G1(Ir3 \u2297\u21262)T + G1(\u21263 \u2297 Ir2)T ) } GT1\n= (G1GT1 )\u03b7TU1U1 \u2212 (G1G T 1 )(U1\u21261)TU1 + \u03b7G1G T 1 + \u21261G1GT1\n+G1(Ir3 \u2297\u21262)TGT1 + G1(\u21263 \u2297 Ir2)TGT1 = (G1GT1 )\u03b7TU1U1 + (G1G T 1 )\u21261 + \u03b7G1G T 1 + \u21261G1GT1\n\u2212G1(Ir3 \u2297\u21262)GT1 \u2212G1(\u21263 \u2297 Ir2)GT1 , which should be a symmetric matrix due to (A.8), i.e., (G1GT1 )\u03beTU1U+\u03beG1G T 1 = ((G1GT1 )\u03beTU1U+\u03beG1G T 1 ) T .\nSubsequently,\n(G1GT1 )\u03b7TU1U1 + (G1G T 1 )\u21261 + \u03b7G1G T 1 + \u21261G1GT1 \u2212G1(Ir3 \u2297\u21262)GT1 \u2212G1(\u21263 \u2297 Ir2)GT1\n= UT1 \u03b7U1(G1G T 1 )\u2212\u21261G1GT1 + G1\u03b7TG1 \u2212G1G T 1 \u21261 + G1(Ir3 \u2297\u21262)GT1 + G1(\u21263 \u2297 Ir2)GT1 ,\nwhich is equivalent to\nG1GT1 \u21261 + \u21261G1GT1 \u2212G1(Ir3 \u2297\u21262)GT1 \u2212G1(\u21263 \u2297 Ir2)GT1 = Skew(UT1 \u03b7U1G1GT1 ) + Skew(G1\u03b7TG1).\nHere Skew(\u00b7) extracts the skew-symmetric part of a square matrix, i.e., Skew(D) = (D\u2212 DT )/2. Finally, we obtain the coupled Lyapunov equations G1GT1 \u21261 + \u21261G1GT1 \u2212G1(Ir3 \u2297\u21262)GT1 \u2212G1(\u21263 \u2297 Ir2)GT1 = Skew(UT1 \u03b7U1G1G T 1 ) + Skew(G1\u03b7TG1), G2GT2 \u21262 + \u21262G2GT2 \u2212G2(Ir3 \u2297\u21261)GT2 \u2212G2(\u21263 \u2297 Ir1)GT2 = Skew(UT2 \u03b7U2G2G T 2 ) + Skew(G2\u03b7TG2),\nG3GT3 \u21263 + \u21263G3GT3 \u2212G3(Ir2 \u2297\u21261)GT3 \u2212G3(\u21262 \u2297 Ir1)GT3 = Skew(UT3 \u03b7U3G3G T 3 ) + Skew(G3\u03b7TG3),\n(A.18)\nthat are solved efficiently with the Matlab\u2019s pcg routine that is combined with a specific preconditioner resulting from the Gauss-Seidel approximation of (A.18).\nA.8 Proof of Proposition 4 (derivation of the Riemannian gradient formula)\nProof. Let f(X ) = \u2016P\u2126(X )\u2212P\u2126(X ?)\u20162F /|\u2126| and S = 2(P\u2126(G\u00d71U1\u00d72U2\u00d73U3)\u2212P\u2126(X ?))/|\u2126| be an auxiliary sparse tensor variable that is interpreted as the Euclidean gradient of f in Rn1\u00d7n2\u00d7n3 .\nThe partial derivatives of f(U1,U2,U3,G) are \u2202f1(U1,U2,U3,G1) \u2202U1 = 2 |\u2126|(P\u2126(U1G1(U3 \u2297 U2) T )\u2212P\u2126(X?1))(U3 \u2297 U2)GT1 = S1(U3 \u2297 U2)GT1 , \u2202f2(U1,U2,U3,G2) \u2202U2 = 2 |\u2126|(P\u2126(U2G2(U3 \u2297 U1) T )\u2212P\u2126(X?2))(U3 \u2297 U1)GT2 = S2(U2 \u2297 U1)GT2 , \u2202f3(U1,U2,U3,G3) \u2202U3 = 2 |\u2126|(P\u2126(U3G3(U2 \u2297 U1) T )\u2212P\u2126(X?3))(U2 \u2297 U1)GT3 = S3(U2 \u2297 U1)GT3 , \u2202f(U1,U2,U3,G) \u2202G = 2 |\u2126|(P\u2126(G\u00d71U1\u00d72U2\u00d73U3)\u2212P\u2126(X ?))\u00d71 UT1 \u00d72 UT2 \u00d73 UT3\n= S \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 , where X?d is mode-d unfolding of X ? and S1 = 2 |\u2126|(P\u2126(U1G1(U3 \u2297 U2) T )\u2212P\u2126(X?1)) S2 = 2 |\u2126|(P\u2126(U2G2(U3 \u2297 U1) T )\u2212P\u2126(X?2)) S3 = 2 |\u2126|(P\u2126(U3G3(U2 \u2297 U1) T )\u2212P\u2126(X?3))\nS = 2|\u2126|(P\u2126(G\u00d71U1\u00d72U2\u00d73U3)\u2212P\u2126(X ?)).\nDue to the specific scaled metric (A.2), the partial derivatives of f are further scaled by ((G1GT1 )\u22121, (G2GT2 )\u22121, (G3GT3 )\u22121,I), denoted as egradxf (after scaling), i.e.,\negradxf = (S1(U3 \u2297 U2)GT1 (G1GT1 )\u22121,S2(U3 \u2297 U1)GT2 (G2GT2 )\u22121,S3(U2 \u2297 U1)GT3 (G3GT3 )\u22121, S \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 ).\nConsequently, from the relationship that horizontal lift of grad[x]f is equal to gradxf = \u03a8(egradxf), we obtain that, using (A.15),\nthe horizontal lift of grad[x]f = (S1(U3 \u2297 U2)GT1 (G1GT1 )\u22121 \u2212 U1BU1(G1GT1 )\u22121, S2(U3 \u2297 U1)GT2 (G2GT2 )\u22121 \u2212 U2BU2(G2GT2 )\u22121, S3(U2 \u2297 U1)GT3 (G3GT3 )\u22121 \u2212 U3BU3(G3GT3 )\u22121, S \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 ),\nFrom the requirements in (A.16) for a vector to be in the tangent space, we have the following relationship for mode-1.\nBU1G1G T 1 + G1G T 1 BU1 = G1G T 1 (Y T U1U1 + U T 1 YU1)G1G T 1 ,\nwhere YU1 = (S1(U3 \u2297 U2)GT1 (G1GT1 )\u22121. Subsequently,\nG1GT1 (YTU1U1 + U T 1 YU1)G1G T 1 = G1GT1 { ((S1(U3 \u2297 U2)GT1 (G1GT1 )\u22121)TU1\n+ UT1 (S1(U3 \u2297 U2)GT1 (G1GT1 )\u22121 }\nG1GT1 = ((S1(U3 \u2297 U2)GT1 )TU1G1GT1 + G1GT1 UT1 (S1(U3 \u2297 U2)GT1 = (G1GT1 UT1 (S1(U3 \u2297 U2)GT1 )T + G1GT1 UT1 (S1(U3 \u2297 U2)GT1 = 2Sym(G1GT1 UT1 (S1(U3 \u2297 U2)GT1 ).\nFinally, BUd for d \u2208 {1, 2, 3} are obtained by solving the Lyapunov equations BU1G1G T 1 + G1GT1 BU1 = 2Sym(G1G T 1 UT1 (S1(U3 \u2297 U2)GT2 ), BU2G2G T 2 + G2GT2 BU2 = 2Sym(G2G T 2 UT2 (S2(U3 \u2297 U1)GT2 ),\nBU3G3G T 3 + G3GT3 BU3 = 2Sym(G3G T 3 UT3 (S3(U2 \u2297 U1)GT3 ),\nwhere Sym(\u00b7) extracts the symmetric part of a square matrix, i.e., Sym(D) = (D + DT )/2. The above Lyapunov equations are solved efficiently with the Matlab\u2019s lyap routine."}, {"heading": "B Additional numerical comparisons", "text": "In addition to the representative numerical comparisons in the paper, we show additional numerical experiments spanning synthetic and real-world datasets.\nExperiments on synthetic datasets: Case S1: comparison with the Euclidean metric. We first show the benefit of the proposed metric (A.2) over the conventional choice of the Euclidean metric that exploits the product structure ofM and symmetry. We compare steepest descent algorithms with Armijo backtracking linesearch for both the metric choices. Figure A.1 shows that the algorithm with the metric (A.2) gives a superior performance in test error than that of the conventional metric choice.\nCase S2: small-scale instances. We consider tensors of size 100\u00d7100\u00d7100, 150\u00d7150\u00d7150, and 200\u00d7200\u00d7200 and ranks (5, 5, 5), (10, 10, 10), and (15, 15, 15). OS is {10, 20, 30}. Figures A.2(a)-(c) and Figures A.3(a)-(c) show the convergence behavior of different algorithms on a train set \u2126 and on a test set \u0393, where Figures A.3(b) is identical to the figure in the manuscript paper. Figures A.2(d)-(f) and A.3(d)-(f) show the mean square error on \u2126 and \u0393 on each algorithm. Furthermore, Figure A.2(g)-(i) and Figure A.3(g)-(i) show the mean square error on \u2126 and \u0393 when OS is 10 in all the five runs. From Figures A.2 and Figures A.3, our proposed algorithm is consistently competitive or faster than geomCG, HalRTC, and TOpt. In addition, the mean square errors on a train set \u2126 and a test set \u0393 are consistently competitive or lower than those of geomCG and HalRTC, especially for lower sampling ratios, e.g, for OS 10.\nCase S3: large-scale instances. We consider large-scale tensors of size 3000 \u00d7 3000 \u00d7 3000, 5000\u00d7 5000\u00d7 5000, and 10000\u00d7 10000\u00d7 10000 and ranks r=(5, 5, 5) and (10, 10, 10). OS is 10. We compare our proposed algorithm to geomCG. Figure A.4 and Figure A.5 show the convergence behavior of the algorithms. The proposed algorithm outperforms geomCG in all the cases.\nCase S4: influence of low sampling. We look into problem instances which result from scarcely sampled data. The test requires completing a tensor of size 10000\u00d7 10000\u00d7 10000 and rank r=(5, 5, 5). Figure A.6 and Figure A.7 show the convergence behavior when OS is {8, 6, 5}. The case of OS = 5 is particularly interesting. In this case, while the mean square errors on \u2126 and \u0393 increase for geomCG, the proposed algorithm stably decreases the error in all the five runs.\nCase S5: influence of ill-conditioning and low sampling. We consider the problem instance of Case S4 with OS = 5. Additionally, for generating the instance, we impose a diagonal core G with exponentially decaying positive values of condition numbers (CN) 5, 50, and 100. Figure A.8 shows that the proposed algorithm outperforms geomCG for all the considered CN values on a train set \u2126.\nCase S6: influence of noise. We evaluate the convergence properties of algorithms under the presence of noise The tensor size and rank are same as in Case S4 and OS is 10. Figure A.9 shows that the train error on a train set \u2126 for each is almost identical to the 2\u2016P\u2126(X ?)\u20162F , but our proposed algorithm converges faster than geomCG.\nCase S7: rectangular instances. We consider instances where dimensions and ranks along certain modes are different than others. Two cases are considered. Case (7.a) considers tensors size 20000 \u00d7 7000\u00d77000, 30000\u00d76000\u00d76000, and 40000\u00d75000\u00d75000 and rank r = (5, 5, 5). Case (7.b) considers a tensor of size 10000\u00d710000\u00d710000 with ranks r = (7, 6, 6), (10, 5, 5), and (15, 4, 4). Figures A.10(a)(c) and Figures A.11(a)-(c) show that the convergence behavior of our proposed algorithm is superior to\nthat of geomCG on \u2126 and \u0393, respectively. Our proposed algorithm also outperforms geomCG for the asymmetric rank cases as shown in Figure A.10(d)-(f) and Figure A.11(d)-(f).\nCase S8: medium-scale instances. We additionally consider medium-scale tensors of size 500 \u00d7 500 \u00d7 500, 1000 \u00d7 1000 \u00d7 1000, and 1500 \u00d7 1500 \u00d7 1500 and ranks r = (5, 5, 5), (10, 10, 10), and (15, 15, 15). OS is {10, 20, 30, 40}. Our proposed algorithm and geomCG are only compared as the other algorithms cannot handle these scales efficiently. Figures A.12(a)-(c) and A.13(a)-(c) show the convergence behavior on \u2126 and \u0393, respectively. Figures A.12(d)-(f) and Figures A.13(d)-(f) also show the mean square error on \u2126 and \u0393 of rank r = (15, 15, 15) in all the five runs. The proposed algorithm performs better than geomCG in all the cases.\nExperiments on real-world datasets: Case R1: hyperspectral image. We also show the performance of our algorithm on the hyperspectral image \u201cRibeira\u201d. We show the mean square error on \u2126 and \u0393 when OS is {11, 22} in Figure A.14 and Figure A.15, where Figure A.15(a) is identical to the figure in the manuscript paper. Our proposed algorithm gives lower test errors than those obtained by the other algorithms. We also show the image recovery results. Figures A.16 and A.17 show the reconstructed images when OS is {11, 22}, respectively. From these figures, we find that the proposed algorithm shows a good performance, especially for the lower sampling ratio.\nCase R2: MovieLens-10M. Figure A.18 and Figure A.19 show the convergence plots for all the five runs of ranks r = (4, 4, 4), (6, 6, 6), (8, 8, 8) and (10, 10, 10) on \u2126 and \u0393, respectively. These figures show the superior performance of our proposed algorithm.\nExperiments for online algorithms: Case O: online instances. Figure A.20 and A.21 show the convergence plots for all the five runs on tensors of ranks 100 \u00d7 100 \u00d7 5000, and 100 \u00d7 100 \u00d7 10000 with rank r = (5, 5, 5) on \u2126 and \u0393, respectively. These figures show that the proposed stochastic gradient descent algorithm gives similar or faster convergence than the proposed batch gradient descent algorithm.\nFigure A.22 and A.23 show the convergence speed comparisons in the train error and the test error of the proposed online and batch algorithms with TeCPSGD and OLSTEC with rank r = (5, 5, 5) on the real-world video sequence Airport Hall dataset. These figures show that the proposed stochastic gradient descent algorithm gives similar or faster convergence than the proposed batch algorithm. In addition, Table B shows that the final train and test MSEs show the superior performance of the proposed algorithms.\n0 200 400 600\n10 \u221220\n10 \u221210\n10 0\nTime in seconds\nM e a n\ns q\nu a re\ne rr\no r\no n\n\u2126\nProposed (\u03b5=0) geomCG (\u03b5=0) Proposed (\u03b5=1e\u221212) geomCG (\u03b5=1e\u221212) Proposed (\u03b5=1e\u221210) geomCG (\u03b5=1e\u221210) Proposed (\u03b5=1e\u221208) geomCG (\u03b5=1e\u221208) Proposed (\u03b5=1e\u221206) geomCG (\u03b5=1e\u221206) Proposed (\u03b5=0.0001) geomCG (\u03b5=0.0001)\nFigure A.9: Case S6: noisy data on \u2126 (train error)."}], "references": [{"title": "Optimization Algorithms on Matrix Manifolds", "author": ["Absil", "P.-A", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Trans. Autom. Control,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade (2nd ed.),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Low-rank matrix completion via preconditioned optimization on the Grassmann manifold", "author": ["N. Boumal", "Absil", "P.-A"], "venue": "Linear Algebra Appl.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Manopt: a Matlab toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "Absil", "P.-A", "R. Sepulchre"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Found. Comput. Math.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Tucker factorization with missing data with application to low-n-rank tensor completion", "author": ["M. Filipovi\u0107", "A. Juki\u0107"], "venue": "Multidim. Syst. Sign. P.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Information limits on neural identification of colored surfaces in natural scenes", "author": ["D.H. Foster", "S.M.C. Nascimento", "K. Amano"], "venue": "Visual Neurosci.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Online low-rank tensor subspace tracking from incomplete data by CP decomposition using recursive least squares", "author": ["H. Kasai"], "venue": "In IEEE ICASSP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Riemannian preconditioning for tensor completion", "author": ["Kasai", "Hiroyuki", "Mishra", "Bamdev"], "venue": "Technical report, arXiv preprint arXiv:1506.02159,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Tensor decompositions and applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Rev.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Low-rank tensor completion by Riemannian optimization", "author": ["D. Kressner", "M. Steinlechner", "B. Vandereycken"], "venue": "BIT Numer. Math.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Introduction to smooth manifolds, volume 218 of Graduate Texts in Mathematics", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Subspace learning and imputation for streaming big data matrices and tensors", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "R3MC: A Riemannian three-factor algorithm for low-rank matrix completion", "author": ["B. Mishra", "R. Sepulchre"], "venue": "In IEEE CDC, pp", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Scaled gradients on Grassmann manifolds for matrix completion", "author": ["T. Ngo", "Y. Saad"], "venue": "In NIPS, pp", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Numerical Optimization, volume Second Edition", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Optimization methods on Riemannian manifolds and their application to shape space", "author": ["W. Ring", "B. Wirth"], "venue": "SIAM J. Optim.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "A new, globally convergent Riemannian conjugate gradient method", "author": ["H. Sato", "T. Iwai"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Tensor versus matrix completion: A comparison with application to spectral data", "author": ["M. Signoretto", "Plas", "R.V. d", "B.D. Moor", "J.A.K. Suykens"], "venue": "IEEE Signal Process. Lett.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["M. Signoretto", "Q.T. Dinh", "L.D. Lathauwer", "J.A.K. Suykens"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Estimation of low-rank tensors via convex optimization", "author": ["R. Tomioka", "K. Hayashi", "H. Kashima"], "venue": "Technical report, arXiv preprint arXiv:1010.0789,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Low-rank matrix completion by Riemannian optimization", "author": ["B. Vandereycken"], "venue": "SIAM J. Optim.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Problem (1) has many variants, and one of those is extending the nuclear norm regularization approach from the matrix case [6] to the tensor case.", "startOffset": 123, "endOffset": 126}, {"referenceID": 14, "context": "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.", "startOffset": 48, "endOffset": 60}, {"referenceID": 23, "context": "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.", "startOffset": 48, "endOffset": 60}, {"referenceID": 22, "context": "While this generalization leads to good results [15, 25, 24], its applicability to large-scale instances is not trivial, especially due to the necessity of high-dimensional singular value decomposition computations.", "startOffset": 48, "endOffset": 60}, {"referenceID": 7, "context": ", in [8, 13].", "startOffset": 5, "endOffset": 12}, {"referenceID": 12, "context": ", in [8, 13].", "startOffset": 5, "endOffset": 12}, {"referenceID": 10, "context": "This paper extends the earlier work [11] to include a stochastic gradient descent algorithm for low-rank tensor completion.", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "The multilinear rank constraint forms a smooth manifold [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "While preconditioning in unconstrained optimization is well studied [20, Chapter 5], preconditioning on constraints with symmetries, owing to non-uniqueness of Tucker decomposition [12], is not straightforward.", "startOffset": 181, "endOffset": 185}, {"referenceID": 0, "context": "We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18].", "startOffset": 174, "endOffset": 184}, {"referenceID": 6, "context": "We build upon the recent work [18] that suggests to use preconditioning with a tailored metric (inner product) in the Riemannian optimization framework on quotient manifolds [1, 7, 18].", "startOffset": 174, "endOffset": 184}, {"referenceID": 12, "context": "[13], which also exploits the manifold structure, are twofold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] exploit the search space as an embedded submanifold of the Euclidean space, whereas we view it as a product of simpler search spaces with symmetries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] work with the standard Euclidean metric, whereas we use a metric that is tuned to the least-squares cost function, thereby inducing a preconditioning effect.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].", "startOffset": 61, "endOffset": 76}, {"referenceID": 16, "context": "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].", "startOffset": 61, "endOffset": 76}, {"referenceID": 3, "context": "They also connect to state-of-the-art algorithms proposed in [19, 27, 17, 4].", "startOffset": 61, "endOffset": 76}, {"referenceID": 4, "context": "Our proposed algorithms are implemented in the Matlab toolbox Manopt [5].", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "The Tucker decomposition of a tensor X \u2208 Rn1\u00d7n2\u00d7n3 of rank r (=(r1, r2, r3)) is X = G\u00d71U1\u00d72U2\u00d73U3, (2) where Ud \u2208 St(rd, nd) for d \u2208 {1, 2, 3} belongs to the Stiefel manifold of matrices of size nd \u00d7 rd with orthogonal columns and G \u2208 Rr1\u00d7r2\u00d7r3 [12].", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "(4) The set of equivalence classes is the quotient manifold [14] M/\u223c := M/(O(r1)\u00d7O(r2)\u00d7O(r3)), (5)", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 6, "context": "Consequently, the problem (1) is an optimization problem on a quotient manifold for which systematic procedures are proposed in [1, 7].", "startOffset": 128, "endOffset": 134}, {"referenceID": 18, "context": "In unconstrained optimization, the Newton method is interpreted as a scaled steepest descent method, where the search space is endowed with a metric (inner product) induced by the Hessian of the cost function [20].", "startOffset": 209, "endOffset": 213}, {"referenceID": 6, "context": ", TxM has the matrix characterization [7] TxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) \u2208 Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 : Ud ZUd + Z T Ud Ud = 0, for d \u2208 {1, 2, 3}}.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "From [1], endowed with the Riemannian metric (9), the quotient manifoldM/\u223c is a Riemannian submersion ofM.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": ", the gradient of a smooth cost function [1].", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "A retraction is a mapping that maps vectors in the horizontal space to points on the search spaceM and satisfies the local rigidity condition [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "In the batch setting, we use the off-the-shelf conjugate gradient implementation of Manopt for any smooth cost function [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "In the online setting, we use the stochastic gradient descent implementation [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 20, "context": "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].", "startOffset": 138, "endOffset": 149}, {"referenceID": 19, "context": "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].", "startOffset": 138, "endOffset": 149}, {"referenceID": 1, "context": "For fixed rank, theoretical convergence of the Riemannian algorithms are to a stationary point, and the convergence analysis follows from [22, 21, 2].", "startOffset": 138, "endOffset": 149}, {"referenceID": 16, "context": "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.", "startOffset": 10, "endOffset": 22}, {"referenceID": 24, "context": "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.", "startOffset": 10, "endOffset": 22}, {"referenceID": 12, "context": "Following [17, 26, 13], the least-squares structure of the cost function in (1) is exploited to compute a linearized step-size guess efficiently along a search direction by considering a polynomial approximation of degree 2 over the manifold.", "startOffset": 10, "endOffset": 22}, {"referenceID": 2, "context": "Following [3], we select \u03b30 in the pre-training phase using a small sample size of a training set.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "It should be stressed that the computational cost of our conjugate gradient implementation is equal to that of [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 7, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 184, "endOffset": 187}, {"referenceID": 12, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 199, "endOffset": 203}, {"referenceID": 14, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 276, "endOffset": 280}, {"referenceID": 23, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 289, "endOffset": 293}, {"referenceID": 22, "context": "5 Numerical comparisons In the batch setting, we show a number of numerical comparisons of our proposed conjugate gradient algorithm with state-of-the-art algorithms that include TOpt [8] and geomCG [13], for comparisons with Tucker decomposition based algorithms, and HaLRTC [15], Latent [25], and Hard [24] as nuclear norm minimization algorithms.", "startOffset": 304, "endOffset": 308}, {"referenceID": 15, "context": "In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10].", "startOffset": 122, "endOffset": 126}, {"referenceID": 9, "context": "In the online setting, we compare our proposed stochastic gradient descent algorithm with CANDECOMP/PARAFAC based TeCPSGD [16] and OLSTEC [10].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "Algorithms are initialized randomly, as suggested in [13], and are stopped when either the mean square error (MSE) on the train set \u03a9 is below 10\u221212 or the number of iterations exceeds 250.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "We evaluate the convergence properties of algorithms under the presence of noise by adding scaled Gaussian noise P\u03a9(E) to P\u03a9(X ?) as in [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 12, "context": "Figure 2(g) shows that the test error for each is almost identical to the \u2016P\u03a9(X )\u2016F [13], but our proposed algorithm converges faster than geomCG.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13].", "startOffset": 46, "endOffset": 49}, {"referenceID": 21, "context": "We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13].", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "We consider the hyperspectral image \u201cRibeira\u201d [9] discussed in [23, 13].", "startOffset": 63, "endOffset": 71}, {"referenceID": 21, "context": "As suggested in [23, 13], we resize it to 203 \u00d7 268 \u00d7 33.", "startOffset": 16, "endOffset": 24}, {"referenceID": 12, "context": "As suggested in [23, 13], we resize it to 203 \u00d7 268 \u00d7 33.", "startOffset": 16, "endOffset": 24}, {"referenceID": 12, "context": "We perform five random samplings of the pixels based on the OS values 11 and 22, corresponding to the rank r=(15, 15, 6) adopted in [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "While OS = 22 corresponds to the observation ratio of 10% studied in [13], OS = 11 considers a challenging scenario with the observation ratio of 5%.", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "We compare the proposed stochastic gradient descent algorithm with its batch counterpart gradient descent algorithm and with TeCPSGD [16] and OLSTEC [10].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "References [1] Absil, P.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Bonnabel, S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Bottou, L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Boumal, N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Boumal, N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Cand\u00e8s, E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Edelman, A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Filipovi\u0107, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Foster, D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Kasai, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Kasai, Hiroyuki and Mishra, Bamdev.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Kolda, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Kressner, D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Lee, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Liu, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Mardani, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Mishra, B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Ngo, T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Nocedal, J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Ring, W.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Sato, H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Signoretto, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Signoretto, M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Tomioka, R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] Vandereycken, B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "From [1], the tangent space has the matrix characterization TxM = {(ZU1 ,ZU2 ,ZU3 ,ZG) \u2208 Rn1\u00d7r1 \u00d7 Rn2\u00d7r2 \u00d7 Rn3\u00d7r3 \u00d7 Rr1\u00d7r2\u00d7r3 : Ud ZUd + Z T Ud Ud = 0, for d \u2208 {1, 2, 3}}.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Additionally from [1], \u03b7Ud has the characterization \u03b7Ud = Ud\u03a9 + Ud\u22a5K, (A.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "From the characterization of linearization of an orthogonal matrix [1], we have the characterization for the vertical space as Vx = {(U1\u03a91,U2\u03a92,U3\u03a93,\u2212(G\u00d71\u03a91 + G\u00d72\u03a92 + G\u00d73\u03a93)) : \u03a9d \u2208 Rrd\u00d7rd ,\u03a9d = \u2212\u03a9d for d \u2208 {1, 2, 3}}.", "startOffset": 67, "endOffset": 70}], "year": 2016, "abstractText": "We propose a novel Riemannian manifold preconditioning approach for the tensor completion problem with rank constraint. A novel Riemannian metric or inner product is proposed that exploits the least-squares structure of the cost function and takes into account the structured symmetry that exists in Tucker decomposition. The specific metric allows to use the versatile framework of Riemannian optimization on quotient manifolds to develop preconditioned nonlinear conjugate gradient and stochastic gradient descent algorithms for batch and online setups, respectively. Concrete matrix representations of various optimization-related ingredients are listed. Numerical comparisons suggest that our proposed algorithms robustly outperform state-of-the-art algorithms across different synthetic and real-world datasets1.", "creator": "LaTeX with hyperref package"}}}