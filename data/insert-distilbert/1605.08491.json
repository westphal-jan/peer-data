{"id": "1605.08491", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "Provable Algorithms for Inference in Topic Models", "abstract": "recently, there has been considerable progress on designing algorithms with provable guarantees - - typically using linear algebraic methods - - substitute for parameter hypothesis learning in simpler latent variable models. but designing provable algorithms for inference has proven to be be more challenging. here we take a first step towards provable inference in topic models. we leverage a property of underlying topic matching models that inherently enables us to construct simple linear estimators for considering the unknown topic proportions that have small variance, and consequently can work with short documents. our estimators also correspond to finding an estimate around which the posterior is well - concentrated. we always show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. it yields good solutions on synthetic data and runs in time signatures comparable to a { \\ em single } iteration of gibbs sampling.", "histories": [["v1", "Fri, 27 May 2016 02:18:43 GMT  (426kb,D)", "http://arxiv.org/abs/1605.08491v1", "to appear at ICML'2016"]], "COMMENTS": "to appear at ICML'2016", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["sanjeev arora", "rong ge 0001", "frederic koehler", "tengyu ma", "ankur moitra"], "accepted": true, "id": "1605.08491"}, "pdf": {"name": "1605.08491.pdf", "metadata": {"source": "META", "title": "Provable Algorithms for Inference in Topic Models", "authors": ["Sanjeev Arora", "Rong Ge", "Frederic Koehler", "Tengyu Ma", "Ankur Moitra"], "emails": ["ARORA@CS.PRINCETON.EDU", "RONGGE@CS.DUKE.EDU", "FKOEHLER@PRINCETON.EDU", "TENGYU@CS.PRINCETON.EDU", "MOITRA@MIT.EDU"], "sections": [{"heading": "1. Introduction", "text": "Generative models of data are ubiquitous in unsupervised learning, and lead to two types of computational problems: In parameter learning, the goal is find the parameters of\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nthe model that best fits a given collection of data. In inference, the goal is to learn the values of latent variables for a specific datapoint. A wide range of approaches are empirically effective for both tasks, including Gibbs sampling and variational inference. However, for the most part we lack strong provable guarantees \u2014 on running time, or quality of solution \u2014 for these approaches.\nRecently, there has been considerable progress on designing new algorithms for parameter learning with such provable guarantees. Since the usual maximum likelihood estimator is often NP-hard to compute even in simple models, these new algorithms use alternative estimators based on the method of moments and linear algebra. Their analysis usually involves making a structural assumption about the parameters of the problem, which can often be justified in applications. Some highlights include algorithms for topic modeling (Arora et al., 2013b; Anandkumar et al., 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al., 2015), community detection (Anandkumar et al., 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).\nBut there has been comparatively much less progress on designing algorithms with provable guarantees for inference. The current paper takes a first step in this direction, in context of topic models. Our algorithms leverage a property of topic models (Definition 3.1) that turns out to hold in many datasets \u2014 the existence of a good approximate inverse matrix. We also give empirical results that demonstrate that our algorithm works on realistic topic models. On synthetic data, its error is competitive with state-of-theart approaches (which have no such provable guarantees).\nar X\niv :1\n60 5.\n08 49\n1v 1\n[ cs\n.L G\nIt obtains somewhat weaker results on real data."}, {"heading": "1.1. Setup and Overview", "text": "Here we describe topic modeling, and why inference appears more difficult than parameter learning. In topic modeling, each document is represented as a bag of words where we ignore the order in which words occur. The model assumes there is a fixed set of k topics, each of which is a distribution on words. Thus the ith topic is a vector Ai \u2208 RD (where D is the number of words in the language) whose coordinates are nonnegative and sum to 1. Each document is generated by first picking its topic proportions from some distribution; say xi is the proportion of topic i, so that \u2211 i xi = 1. The model assumes a distribution on x that favors sparse or approximately sparse vectors; a popular choice is the Dirichlet distribution (Blei et al., 2003). Then the document {w1, w2, . . . , wn} is generated by drawing n words independently from the distribution A \u00b7 x where A is the matrix whose columns are the topics. It is important to note that the document size n can be quite small (e.g., n may be 400, and D may be 50, 000) so the empirical distribution of words in a document is in general a very inaccurate approximation toAx. With some abuse of notation we also think of y as a vector in RD, whose jth coordinate is the number of occurences of word j in the document.\nParameter learning involves recovering the bestA for a corpus of documents; this can be seen as a the latent structure in the corpus. Recent (provable) algorithms for this problem (Anandkumar et al., 2012; Arora et al., 2013b) use the method of moments, leveraging the fact that some form of averaging over the corpus yields a linear algebraic problem for recovering A. For example the word-word cooccurence matrix (whose i, j entry is the probability that words i, j co-occur in a document) is given by\nEx[Axx TAT ] = AZAT\nwhere Z is the 2nd moment matrix of the prior distribution on x. It is possible to recoverA from this expression, under natural conditions like separability (Arora et al., 2013b). Alternatively, one can use a co-occurrence tensor and recover A under weaker assumptions (Anandkumar et al., 2012).\nIn the inference problem, we know the topic matrix A and are given a single document y generated using this matrix. The goal is to find the posterior distribution x|y. This can be seen as labeling or categorizing this document, which is important in applications. Inference is reminiscent of classical regression problems where the goal is to find x given y = Ax + noise vector. The key difference here is the nature of noise \u2014for each word coordinate j is 1 with probability (Ax)j , and 0 otherwise\u2014 which means that\nthe noise on a coordinate-by-coordinate basis can be much larger than the signal. In particular the vector y \u2208 RD is very sparse even though Ax is dense. This problem can be seen as an analog of sparse linear regression when the target (regression) vector x has nonnegative coordinate and\u2211 i xi = 1. (This is distinct from usual `1-regression where regression vector is in `2 even though the loss function is `1.) The difficulty here, in addition to the issue of high coordinate-wise error already mentioned, is that the usual sparsity-enforcing `1-regularization buys nothing since the solution needs to exactly satisfy \u2016x\u20161 = 1.\nInference seems more difficult than parameter learning because averaging over many documents is no longer an option. Furthermore, the solution x is not unique in general, and in some cases the posterior distribution on x is not well concentrated around any particular value. (In practice Gibbs Sampling can be used to sample from the posterior (Griffiths & Steyvers, 2004; Yao et al., 2009), but as mentioned, a rigorous analysis has proved difficult. The inference is actually NP-hard.) We will view inference as a problem of recovering some ground truth x\u2217 that was used to generate the document, and we show that with probability close to 1 our estimate x\u0302 is close to x\u2217 in `1 norm.\nBayesian vs Frequentist Views. So far we have not differentiated between Bayesian and frequentist approaches to frame the inference problem, and now we show that the two are closely related here. The above description is frequentist, assuming an unknown \u201cground truth\u201dvector x\u2217 of topic proportions (which is r-sparse for some small r) was used to generate a document y, using a distribution y|x\u2217. Let Ex\u2217 be the event that our algorithm recovers a vector x\u0302 such that \u2016x\u0302 \u2212 x\u2217\u20161 \u2264 . For our algorithm Pry|x\u2217 [Ex\u2217 ] \u2265 1 \u2212 \u03b42 for some \u03b4 > 0. By contrast, in the Bayesian view, one assumes a prior distribution on x\u2217 and seeks to output a sample from the conditional distribution x\u2217|y. Now we show that the success of our frequentist algorithm implies that the posterior x\u2217|y must also be concentrated, and place most probability mass on set of x such that \u2016x \u2212 x\u0302\u20161 \u2264 . By law of total expectation, we have Prx\u2217,y [Ex\u2217 ] = Prx\u2217 [ Pry|x\u2217 [Ex\u2217 |x\u2217] ] \u2265 1\u2212\u03b42. Switching the order of expectation, we obtain\nPry [ Prx\u2217|y [Ex\u2217 | y] ] \u2265 1\u2212 \u03b42 .\nThen it follows by Markov argument that Pry [ Prx\u2217|y [Ex\u2217 | y] \u2265 1\u2212 \u03b4 ] \u2265 1\u2212 \u03b4 .\nNote that the inner probability is over the posterior distribution px\u2217|y . But the event Ex\u2217 only depends on the output x\u0302 of the algorithm given y. Thus the probability is at least 1 \u2212 \u03b4 over choice of y, that 1 \u2212 \u03b4 of the probability mass of x\u2217|y is concentrated in the `1 ball of radius around the algorithm\u2019s answer x\u0302.\nFrom now on the goal of our algorithm is to recover x\u2217 given y, and we identify conditions under which the event has probability close to 1.\nMinimum Variance Estimators (with Bias). Having set up the problem as above, next we consider how to recover an approximation to x\u2217 given a document y generated with topic proportions x\u2217.\nSince A has orders of magnitude more rows than columns, it has many left inverses to choose from. If we find any matrix B where BA is equal to the identity matrix, then By is an unbiased estimate for x\u2217. However this estimate has high variance if B has large entries, necessitating working with only very large documents. Motivated by applications to collaborative filtering, Kleinberg & Sandler (2008) introduce the notion of the `1 condition number (see Definition 2.1) of A, which allows them to construct a left inverse B with a much smaller maximum entry. We introduce a weaker notion of condition number called the `\u221e- to-`1 condition number, which leverages the observation that even if BA is close to the identity matrix it still yields a good linear estimator for x\u2217. We call B an approximate inverse of A. Moreover it has the benefit that the condition number as well as the approximate left inverseB with minimum variance can also be computed in polynomial time using a linear program (Proposition 3.2)!\nIn our experiments, we compute the exact condition number of word-topic matrices that were found using standard topic modeling algorithms on real-life corpora. (By contrast, we do not know the `1 condition number of these matrices.) In all of the examples, we found that the condition number is at most a small constant, which allows us to compute good approximate left inverses to the topic matrix A to enable us to estimate x\u2217 even with relatively short documents.\nMain results. Our main result (Theorem 4.1) shows that when the condition number is small, it is possible to estimate x\u2217 using a combination of thresholding and a left inverse B of minimum variance. Our overall algorithm runs efficiently and requires time O(nk) and O\u0303(r2) samples to achieve o(1) error in `1 norm and o(1/r) error in `\u221e norm, where r is the number of topics represented in the document. Note that we do not need to assume a particular model (e.g. uniform random) for the r topics, the algorithm works even when the topics may be correlated with each other.\nAs an intermediate step, we are able to recover the support of x\u2217 when each of its non-zero coordinates is suitably bounded away from zero. We complement this result by showing that maximizing the log-likelihood function over the recovered support can further reduce the estimation er-\nror (measured in the `1-norm) to O\u0303( \u221a r/n) (see Section 5). The experiments show that it indeed yields estimates for x\u2217 with smaller error (see Section 7).\nFinally we show that in order to recover support of x\u2217, it is necessary to observe \u2126(r2) words, even if A is perfectly conditioned and x\u2217 is promised to have all non-zero coordinates larger than \u2126(1/r) (see Lemma 6.2 for a family of such perfectly conditioned but hard instances of A, and Lemma 6.3 for hard instance of x\u2217).\nThus to sum up, our overall approach involves simple linear algebraic primitives followed by convex programming. For a topic model with k topics, the sample complexity of our algorithms depend on log k instead of k. This is important in practice as k is often at least 100. The accuracy on synthetic data is good for sparse x, though not quite as good as Gibbs sampling. However, if we forgo the convex programming step we can compute a reasonable estimate for x from a single matrix vector multiplication plus thresholding, which is an order of magnitude faster than finding an estimate of the same quality via Gibbs sampling. And of course, our approach comes with a performance guarantee."}, {"heading": "2. Notations and Setup", "text": "In addition to the description of topic model in Section 1.1, we introduce the following notations. We use Sk = {z \u2208 Rk\u22650 : |z|1 = 1} to denote the k-dimensional probability simplex. We assume that the true topic proportion vector x\u2217 \u2208 Sk is r-sparse throughout the paper. Sometimes we also abuse notations and use y as a D dimensional vector instead of a set, in this case yi is the number of times word i appears in the document. We will use a>i to denote the i-th row of A. We will use cat(p) to denoted the categorical distribution defined by probability vector p. Euclidean norm, `1, `\u221e norm of a vector is denoted by \u2016 \u00b7 \u2016, \u2016 \u00b7 \u20161 and \u2016 \u00b7 \u2016\u221e respectively.\nCondition Numbers of Matrices Condition number of a matrix usually represents the ratio of the largest and smallest singular values. However, this concept is tied to `2 norm, and for probability distributions the most natural norms are `1 and `\u221e.\nNext we define various matrix norms that we will utilize. Let |A|\u221e = maxi,j |Aij | denotes the maximum absolute value of the entries of the matrix A, and |A|1 = \u2211 i,j |Aij | denotes the sum of the absolute value of the entries of the matrix A. Let Idk denotes the identity matrix of dimension k. For a matrix, let \u2016 \u00b7 \u2016 denote the spectral norm, and \u2016 \u00b7 \u2016Q denote the norm defined by \u2016x\u2016Q = \u221a x>Qx where Q is a positive semidefinite matrix. We will use this norm particularly with Q being fisher information matrix.\nWe will also work with various notions of condition num-\nber, that we will use in our guarantees.\nDefinition 2.1 (`1-condition number). For a nonnegative matrix A, define its `1-condition number \u03ba(A) to be the minimum \u03ba such that for any x \u2208 Rk,\n\u2016Ax\u20161 \u2265 \u2016x\u20161/\u03ba (1)\nThis condition number was introduced by Kleinberg & Sandler (2008) in analyzing various algorithms for collaborative filtering. We will use a weaker (i.e. smaller) notion of condition number. Empirically, it seems that most of the word-topic matrices that we have encountered have a reasonably small `1-condition number, and have an even smaller `\u221e \u2192 `1-condition number. Definition 2.2 (`\u221e \u2192 `1-condition number). Let \u03bb(A) be the minimum number \u03bb such that for any x \u2208 Rk,\n\u2016Ax\u20161 \u2265 \u2016x\u2016\u221e/\u03bb (2)\nRemark 1. Based on the relationship between `1 and `\u221e norm, we have that \u03bb(A) \u2264 \u03ba(A) \u2264 k\u03bb(A). In Section 6 we give an example where the `1 \u2192 `1 condition number is significantly worse: \u03ba(A) \u2265 \u2126( \u221a k)\u03bb(A)."}, {"heading": "3. \u03b4-Biased Minimum Variance Estimators", "text": "Let y \u2208 RD be the document vector whose i-th entry yi is the number of times word i appears. We try to estimate the true topic vector x\u2217 by left multiplying y with some matrix B. Intuitively, E[By] = BAx\u2217, so we want BA to be close to the identity matrix. On the other hand, when we apply B to the document vector, each word will select a column of B, and its variance on any entry is bounded by the maximum entry in B. Therefore we would like to optimize over two things: first, we want BA to be close to identity; second, we want the matrixB to have small |B|\u221e. This inspires the following linear program:\nDefinition 3.1. For A \u2208 RD\u00d7k and \u03b4 \u2265 0, define \u03bb\u03b4(A) to be the solution of the following convex program:\n\u03bb\u03b4(A) = min |B|\u221e s.t. |BA\u2212 Idk|\u221e \u2264 \u03b4\nB \u2208 Rk\u00d7D (3)\nWe will refer to the minimizer B of the above convex program as the \u03b4-biased minimum variance inverse for A. The solution to the above convex program will help minimize our sample complexity both theoretically and empirically.\nAllowing a nonzero \u03b4 can potentially reduce the variance of the estimator while introducing a small bias. Such biasvariance trade-off has been studied in other settings (Moitra & Saks, 2013; Javanmard & Montanari, 2014).\nWhat is the optimal |B|\u221e? To answer this question we get the dual of the LP 3 (with variable Q \u2208 Rk\u00d7k),\nmaximize tr(Q)\u2212 \u03b4|Q|1 s.t. |AQ|1 \u2264 1 (4)\nWe can further show that (4) is equivalent to the following (non-convex) program with vector variables x \u2208 Rk (see Appendix B for the proof):\nmaximize \u2016x\u2016\u221e \u2212 \u03b4\u2016x\u20161 s.t. \u2016Ax\u20161 \u2264 1\nNote that this is very closely related to the condition number \u03bb in Definition 2.2. In particular, the optimal value is exactly \u03bb(A) when \u03b4 = 0! When \u03b4 > 0 this can be viewed as a relaxation of the `\u221e \u2192 `1 condition number. This is summarized in the following Proposition whose proof is deferred to appendix.\nProposition 3.2. For any \u03b4 \u2265 0, we have that\n\u03bb\u03b4(A) \u2264 \u03bb0(A) = \u03bb(A) \u2264 \u03ba(A) .\n4. Recovery Guarantees in the `1-Norm In this section we show how to estimate the topic proportion vector using a \u03b4-biased minimum variance inverse B of word-topic matrix A (Definition 3.1). For a small \u03b4 (that is 1/r), given a solution B of program (3) with entries of absolute value at most \u03bb\u03b4(A), the following Thresholded Linear Inverse estimator (Algorithm 1) is guaranteed to be close to the true x\u2217 in both `1 and `\u221e norm.\nAlgorithm 1 Thresholded Linear Inverse Algorithm (TLI) Input: Document y with n words, and \u03b4-biased inverse matrix B of matrix A. Output: Topic vector estimator x.\n1. Compute x\u0302 = 1nBy. 2. Let \u03c4 = 2\u03bb\u03b4(A) \u221a\nlog k/n + \u03b4. For all i \u2208 [k], , if x\u0302i < \u03c4 , set xi = 0, otherwise set xi = x\u0302i.\nTheorem 4.1. Suppose document y is generated from rsparse topic vector x\u2217. For any > 4\u03b4r, given n = \u2126(\u03bb\u03b4(A)\n2r2 log k/ 2) samples, with high probability Algorithm 1 returns a vector that has `1-distance at most with x\u2217.\nOur first step is to bound the variance of the partial estimator x\u0302 before thresholding. Our bound will utilize the maximum entry in B, which is why we tried to find B that minimizes this quantity in the first place. In particular we can show:\nLemma 4.2. With probability at least 1 \u2212 1/k2, for every i we have |x\u0302i \u2212 x\u2217i | \u2264 \u03b4 + 2\u03bb\u03b4(A) \u221a (log k)/n. Proof of Lemma 4.2. By definition, x\u0302i = 1n \u2211n j=1(B1wj )i where 1wj is the indicator vector for the jth word in the document. By summing over words in the document as opposed to words in the vocabulary, we have written x\u0302j as a sum of independent random variables, and we will use Bernstein\u2019s inequality to show that it is concentrated around its mean. This is straightforward, but the key is the way we have chosenB ensures that the estimator is at most \u03b4-based. To elaborate, we can compute\nE[x\u0302i] = (BAx\u2217)i = k\u2211 j=1 (BA)i,jx \u2217 j\n= x\u2217j + k\u2211 j=1 ((BA)i,j \u2212 1i=j)x\u2217j ,\nwhere 1i=j = 1 if i = j and 1i=j = 0 otherwise. Now by construction we have that for all i and j, |(BA)i,j\u22121i=j | \u2264 \u03b4. Hence, | \u2211k j=1((BA)i,j \u2212 1i=j)x\u2217j | \u2264 \u03b4 \u2211k j=1 x \u2217 j = \u03b4 . Therefore we conclude that |E[x\u0302i]\u2212 x\u2217i | \u2264 \u03b4 which shows that our estimator has bias at most \u03b4.\nNow we can appeal to standard concentration arguments. Recall that x\u0302i is a sum of independent random variables x\u0302i = 1n \u2211n j=1(B1wj )i, and each summand here is bounded by max(B1wj )i \u2264 \u03bb\u03b4(A). We apply Hoeffding\u2019s inequality and obtain that with probability at least 1\u22121/k2, |x\u0302i \u2212 E[x\u0302i]| \u2264 2\u03bb\u03b4(A) \u221a (log k)/n and this completes the proof of the lemma.\nLemma above shows that the vector x\u0302 is close to the true x\u2217 in infinity norm. As a corollary, we know the algorithm finds the correct support if x\u2217 does not have very small entries\nCorollary 4.3. With high probability, x output by Algorithm 1 satisfies that for every i \u2208 [k], if x\u2217i = 0 then xi = 0, and if x\u2217i \u2265 4\u03bb\u03b4(A) \u221a (log k)/n+ 2\u03b4 then xi > 0. In particular, if all the nonzero entries of x\u2217 are at least /r for some > 4\u03b4r, the algorithm finds the correct support with O(\u03bb\u03b4(A)2r2 log k/ 2) samples.\nUsing the corollary above we can then prove Theorem 4.1. The key intuition is x can only incur error on non-zero coordinates of x\u2217, and a fixed amount of error on non-zero coordinates of x\u2217.\nProof of Theorem 4.1. By Lemma 4.2 and union bound, we have that with probability at least 1 \u2212 1/k, for every i \u2208 [k] |x\u0302i \u2212 x\u2217i | \u2264 \u03b4 + 2\u03bb\u03b4(A) \u221a (log k)/n). Thus in Step 2 of the algorithm we are guaranteed that if x\u2217i = 0 then x\u0302i must be smaller than the threshold and therefore xi = 0.\nOn the other hand, if x\u2217i > 0, we know\n|x\u2217i\u2212xi| \u2264 |x\u2217i\u2212x\u0302i|+|x\u0302i\u2212xi| \u2264 2\u03b4+4\u03bb\u03b4(A) \u221a (log k)/n)\nAgain appealing to the fact that x\u0302 and x\u2217 are entry-wise close, there can be at most r entries where we set x\u2217i > 0 for \u03b4 \u2264 /4r. When n = 64\u03ba2r2 log k/ 2 we also have 4\u03bb\u03b4(A) \u221a (log k)/n) \u2264 /2r. Combining these two facts\nwe conclude |x\u2217\u2212x|1 \u2264 r(2\u03b4+4\u03bb\u03b4(A) \u221a\n(log k)/n)) \u2264 , which completes the proof."}, {"heading": "5. Rate of MLE estimator", "text": "In this section, we show that given the correct support R of x\u2217, we can optimize the log-likelihood function over the variables inR and obtain a finer solution with smaller `1 error. We make the following two assumptions: first, that the non-zero coordinates of x\u2217 are bounded away from zero; second, that the word-topic matrix has small restricted `1 \u2192 `1 condition number. Assumption 5.1. We assume that x\u2217 \u2208 Sk satisfies that R = supp(x\u2217) is of size at most r and x\u2217i \u2265 \u03c4/r for any i \u2208 R. Assumption 5.2 (restricted `1 \u2192 `1 condition number). We assume that word-topic matrix A satisfies that for any r-sparse vector v \u2208 Rd,\n\u2016Av\u20161 \u2265 \u2016v\u20161/\u03ba\u0304 .\nWe note that by definition \u03ba\u0304 \u2264 \u03ba(A). Moreover, the restricted `1 \u2192 `1 condition number can be viewed as `1 analog of the restricted isometry property (Candes & Tao, 2005) or restricted eigenvalue conditions (Bickel et al., 2009; Meinshausen & Yu, 2009) associated to `2 norm. This type of assumption is particularly useful (and somewhat necessary) for the estimation problem.\nWe will restricted our attention to support R throughout this section. Let a\u0302w \u2208 Rr be the restriction of aw to the support R, and A\u0302 be the word-topic matrix restricted to columns indexed by R. Let f(x) be the log-likelihood function restricted to the support R. That is, for x \u2208 Rr,\nf(x) = log Pr[y | x] = \u2211 w\u2208y log(\u3008a\u0302w, x\u3009), . (5)\nThe main theorem in this section below shows that when n = \u2126(r2), the maximum likelihood estimator (MLE) restricted to the supportR has `1 rate O\u0303(\u03ba\u0304 \u221a r/n). Moreover,\nthe error on predicting Ax\u2217 is O\u0303( \u221a r/n) which doesn\u2019t depend on the condition number.\nTheorem 5.3. Under assumption 5.1 and 5.2, suppose n \u2265 c\u03ba\u03042r2 log k/\u03c42 for a sufficiently large constant c. Let xMLE be the maximizer of the log-likelihood function f(x)\nrestricted to support R. Then with high probability xMLE satisfies that\n\u2016AxMLE \u2212Ax\u2217\u20161 \u2264 O\u0303 (\u221a r\nn\n) ,\nand,\n\u2016xMLE \u2212 x\u2217\u20161 \u2264 O\u0303 ( \u03ba\u0304 \u221a r\nn\n) .\nAsymptotically, we know that the error vector xMLE \u2212 x\u2217 converges to standard normal with covariance matrixQ being the Fisher information matrix (see Equation (7)). This means that Q\u22121/2(xMLE \u2212 x\u2217) is bounded in `2 norm. Therefore the keys towards proving Theorem 5.3 consists of a) converting the above to a non-asymptotic bound with careful concentration inequality b) understanding how Q\u22121/2 converts `1 space to `2 space so that an `1 norm of the error can be obtained.\nWe give intuitions for the proofs here, which mostly follows from the classical asymptotic normality of Maximum Likelihood Estimator, and our main contribution here is to give a finite sample bound using concentration inequalities.\nFirst we consider the gradients and Hessians of the likelihood function.\n\u2207f(x) = \u2211 w\u2208y a\u0302w \u3008a\u0302w, x\u3009 , \u22072f(x) = \u2212 \u2211 w\u2208y a\u0302wa\u0302 > w \u3008a\u0302w, x\u30092 . (6)\nLet Q be the Fisher information matrix as defined below, Q = E [ a\u0302wa\u0302 > w\n\u3008a\u0302w, x\u30092 ] = \u2211 i\u2208[D] \u3008a\u0302i, x\u2217\u3009 a\u0302ia\u0302 > i \u3008a\u0302i, x\u2217\u30092 . (7)\nNote that we have E[\u22072f(x\u2217)] = \u2212nQ. When n is sufficiently large and xMLE is sufficiently close to x\u2217, we have,\n\u2212\u2207f(x\u2217) = \u2207f(xMLE)\u2212\u2207f(x\u2217) \u2248 \u22072f(x\u2217)(xMLE\u2212x\u2217) .\nTherefore, it follows that\nx\u2217 \u2212 xMLE \u2248 \u22072f(x\u2217)\u22121\u2207f(x\u2217) .\nIt can be shown that the covariance of the gradient is E[\u2207f(x\u2217)\u2207f(x\u2217)>] = nQ. Therefore when \u22072f(x\u2217)\u22121 is sufficiently close to its expectation nQ, the covariance of the error x\u2217 \u2212 xMLE is approximately equal to 1nQ \u22121.\nTowards establishing a non-asymptotic result, we bound from above \u2207f(x\u2217) and lower from below \u22072f(x) in a proper Euclidean norm \u2013 the norm defined by the Fisher information matrix Q in the following three lemmas. Lemma 5.4 below controls the `2 and `\u221e norm of Q\u22121/2\u2207f(x\u2217) (which is supposed to be spherical Gaussian with covariance matrix \u221a nIdr asymptotically).\nLemma 5.4. Under assumption 5.1 and 5.2, suppose n \u2265 c\u03ba\u03042r2/\u03c42 \u00b7 log k for a sufficiently large constant c. Then, with high probability we have\n\u2016Q\u22121/2(\u2207f(x\u2217)\u2212 n1r)\u2016 \u2264 O\u0303( \u221a nr) , (8)\nand,\n\u2016Q\u22121/2(\u2207f(x\u2217)\u2212 n1r)\u2016\u221e \u2264 O\u0303( \u221a n) . (9)\nLemma 5.5 relates \u2212\u22072f(x) with the Fisher information matrix Q spectrally around a neighborhood of x\u2217 which MLE will be proved to fall in. We essentially show that \u2212\u2207f(x\u2217) concentrates around its expectation nQ, and moreover we can effectively approximate the Hessian \u2212\u22072f(x) around a neighborhood of x\u2217 by nQ as well. Lemma 5.5. Under assumption 5.1 and 5.2, suppose n = c0\u03ba\u0304\n2r2 log k/\u03c42 for sufficiently large constant c0. Then, with high probability over the randomness of y, it holds that for all x such that x \u2264 Cx\u2217,\n\u2212\u22072f(x) n 2C2 \u00b7Q . (10)\nFinally, as alluded before, Lemma 5.6 characterizes the distortion caused by Q\u22121/2 transforming `2 to `1 space. We note that the square-root of Fisher information matrixQ1/2 converts naturally `1 to `2. Therefore we can get the desired `1 error bound in Theorem 5.3.\nLemma 5.6. Under assumption 5.1 and 5.2, Fisher information matrix Q satisfies that\n\u2016A\u0302Q\u22121/2\u20162\u21921 \u2264 1, and \u2016Q\u22121/2\u20162\u21921 \u2264 \u03ba\u0304 . (11)"}, {"heading": "As a corollary,", "text": "Q 1 \u03ba\u03042 \u00b7 Idr . (12)"}, {"heading": "6. Sample Complexity Lower Bounds", "text": "In this section we construct a natural (distribution of) wordtopic matrix A with low \u039b\u03b4(A) value for very small \u03b4 for which given document with o(r2) words, it is impossible to determine the support x\u2217 even if all the nonzero coordinates of x\u2217 are roughly 1/r. This shows that for the task of support recovery, our algorithm in Section 4 achieves optimal sample complexity up to logarithmic factor.\nTheorem 6.1. There exists a (distribution of) word-topic matrix A with \u039b\u03b4(A) = 1 for \u03b4 = O( \u221a log k/D) such that any algorithm A that takes document of o(r2) words as input cannot recover the support of the topic vector x\u2217 that is used to generate y with probability 3/4. This is still true when x\u2217 is promised to have only non-zero entries that are larger than 1/r.\nThe hard instance that we constructed is pretty simple: we consider a word-topic matrix where every topic contains roughly half of words in the vocabulary, and gives probability roughly 2/D to each of the words. The words in the topic are uniformly randomly selected. To make this more precise, let S1, S2, ..., Sk \u2282 [D] be k independent subsets that are uniformly chosen among all subsets of [D]. Let matrix Ai,j = 1/|Sj | if i \u2208 Sj and Ai,j = 0 otherwise.\nWe first show that indeed \u039b\u03b4(A) is very small, and therefore it has a good \u03b4-biased minimum variance estimator and our algorithm works well on this matrix. Lemma 6.2. With high probability over the randomness of A, we have 1\u2212 \u03b4 \u2264 \u039b\u03b4(A) \u2264 1 for any \u03b4 \u2265 c \u221a (log k)/D where c is a sufficiently large constant.\nProof of Lemma 6.2. For any matrix A with columns having `1 norm 1, we have that \u039b\u03b4(A) \u2265 1 \u2212 \u03b4. We show the upper bound by constructing the linear inverse matrix B with small |B|\u221e \u2264 1 explicitly: Bj,i = 1 if Ai,j > 0, and Bj,i = \u22121 ifAi,j = 0. Let bi be the i-th row ofB and ai be the i-th column of A, to verify |BA \u2212 I|\u221e \u2264 \u03b4, it suffices to show that 1) \u3008bi, ai\u3009 = 1 for all i, and 2) |\u3008bi, aj\u3009| \u2264 \u03b4 for all i 6= j.\nThe first equation is easy because bi is 1 on the support of ai, so \u3008bi, ai\u3009 = |ai|1 = 1.\nFor the second equation, consider an arbitrary pair bi, aj (i 6= j). Consider Si, Sj , the supports of ai and aj respectively. By the construction of A,B we know that all entries in Si \u2229 Sj contribute +1/|Sj | to the inner product \u3008bi, aj\u3009, that all entries in Sj\\Si contribute \u22121/|Sj | to the inner product, and that all other entries contribute 0. Therefore, \u3008bi, aj\u3009 = 1|Sj | (|Si \u2229 Sj | \u2212 |Sj\\Si|) .\nStandard concentration bounds show that |Si\u2229Sj |, |Sj\\Si| are within D/4\u00b1 O( \u221a D log k), and |Sj | is within D/2\u00b1 O( \u221a D log k) with probability at least 1\u2212 1/k4. Therefore by union bound with probability at least 1 \u2212 1/k2 for all pairs i, j we have |\u3008bi, aj\u3009| \u2264 O( \u221a (log k)/D).\nLemma 6.2 in particular implies that if nonzero entries in the true topic vector x\u2217 are at least 1/r, our algorithm can detect the support with O(r2 log k) samples. We show below that no algorithm can do much better by constructing the following hard instance: Lemma 6.3. With high probability over the choice of A, there exists vector r-sparse vectors x, x\u2212 with non-zero entries bounded below from 1/r, such that no algorithm that only takes document of o(r2) words can distinguish distributions cat(Ax) and cat(Ax\u2212) with probability better than 1/2 + o(1).\nThe full proof is deferred to Section C. Here we sketch the main idea. We construct x, x\u2212 randomly as follows. Let\nR \u2282 [k] be a random subset of size r, and let R\u2212 \u2282 R be a random subset with one item removed from R (so |R\u2212| = r \u2212 1). Let xi = 1/r if i \u2208 R, and xi = 0 otherwise. Let x\u2212i = 1/(r \u2212 1) if i \u2208 R\u2212 and 0 otherwise. The key here is to show that with high probability over the choice of x, x\u2212 and the choice of A, the KL-divergence between the two distribution cat(Ax) and cat(Ax\u2212) isO(1/r2), and therefore \u2126(r2) is need for distinguishing them.\nLet p = Ax and q = Ax\u2212. We will show that pj/qj is between 1\u00b1O(1/r) for most of the word j, and the rest of words are negligible. To see this, we observe that for most of the words, pj \u2265 \u2126(1/D) , and |pj \u2212 qj | = |(Ax)j \u2212 (Ax\u2212)j | \u2264 O(1/r) maxiAj,i = O(1/(rD)). Finally, we can bound KL-divergence of p, q by O(1/r2), KL(p\u2016q) \u2264 \u03c72(p, q) = \u2211 j(pj \u2212 qj)2/qj \u2264 O(1/r2), as desired.\nIn fact, using the same matrix we can bound the difference between `1 \u2192 `1 condition number \u03ba(A) and `1 \u2192 `\u221e condition number \u03bb(A).\nLemma 6.4. When D k log k, with high probability, the `1 \u2192 `1 condition number \u03ba(A) \u2265 \u2126( \u221a k). When D k2 log k, with high probability \u03bb(A) \u2264 2.\nWe give the proof in Appendix C. Intuitively, for \u03ba(A) we show that the uniform mixture of first half of topics is very similar to the uniform mixture of the last half of topics. For \u03bb(A), we use the construction for the \u03b4-biased linear inverse, and show that \u201cfixing\u201d the bias does not change the condition number by too much."}, {"heading": "7. Experiments", "text": "The corpora we use consist of New York Times articles (295,000 documents, average document length 298), Enron emails (39,861 documents, average document length 136), and NIPS papers (1500 documents, average length 1042). We compute the word-topic matrices using the algorithm in (Arora et al., 2013a), using 100 topics for NIPS, 100 topics for Enron, and 400 topics for NYTimes.\nCondition Numbers of Matrices First we empirically verify the assumption that the word-topic matrix have small `\u221e \u2192 `1 condition number (see Table 1). Solving LP (3) on 16 processors in parallel using the Mosek LP solver takes 1 minute and a half for the NIPS dataset, 4 minutes for the Enron dataset, and roughly 4 hours for the NYTimes dataset (this is partly the result of using more topics for this dataset).\nNote that we only need to compute the inverse matrix once and then it can be used to do inference on all the documents, so the running time for computing the inverse is not a major concern. The procedure can also be easily parallelized because the LP for different rows of B are independent.\nFor comparison, we also list lower bounds on the `1 \u2192 `1 condition number \u03ba(\u00b7), the condition used by (Kleinberg & Sandler, 2008). We see that it\u2019s at least 2 times larger than `\u221e \u2192 `1 condition number. There is no efficient algorithm known for computing `1 \u2192 `1 condition number, and therefore we only compute provable lower bounds for various dataset (see Section D for the approach).\nSynthetic Experiments We first verify the recovery guarantee of our algorithm on synthetic documents. For each document, we sample r = 5 topics uniformly at random, and choose weights for these topics uniformly from the r-dimensional probability simplex. The results1 are listed in Figure 1.\nWe compare our TLI algorithm (Algorithm 1)2 with the collapsed Gibbs sampling algorithm implemented in MALLET (McCallum, 2002) and its anchor word compatible extension3; we use 200 iteration of burn-in and 1000 further iterations of sampling. Note that when applying Gibbs sampling for the topic vectors, we treat the word-topic matrix as a fixed constant. It is not easy to do Gibbs Sampling for the prior we specified, so we compare against a Dirichlet prior with \u03b1 = rk1 which encourages r-sparse vectors. Note that TLI-Unnormalized is the output of the TLI algorithm as described, whereas TLI is the result after normalizing the entries to give a probability distribution. We can also improve the quality of the TLI estimate by using gradient ascent on the likelihood (see Section 5), restricted to the top r entries of our initial estimate; this is denoted TLI+MLE in the figure. Finally, we give the result of gradient ascent on the posterior (treating the prior as Dirichlet, similarly to Gibbs sampling) starting from the TLI estimate, and denote this TLI+MAP.\nWe can also replace the uniform sparse prior with a Dirichlet prior with \u03b1 = rk1 and get similar results; see Figure 2.\nWhen using a uniform sparse prior, we can improve the recovery score of TLI by replacing the thresholding step in algorithm TLI with one that drops all but the top-r values; see Figure 3.\nThe performance of the TLI algorithm is 3 to 5 times worse than Gibbs sampling in terms of `1 or `\u221e error with same number of words. This is mostly because a simple linear estimator cannot capture the correlations between weights of different topics. The post processing using maximum like-\n1Code to reproduce the results is available at: https:// github.com/frytvm/topic-inference\n2For documents of the length found in the corpuses, the thresholding value \u03c4 used in the theoretical section is too conservative (large). As a more practical alternative we replace \u03c4 as given in the theoretical section by \u03c4/4.5. We use unbiased pseudoinverses, taking \u03b4 = 0.\n3https://github.com/mimno/anchor\nlihood improves the performance significantly. The performance of the algorithms seems to be related to the `\u221e \u2192 `1 condition number (with NIPS being best and Enron being worst).\nA virtue of our algorithm is its speed. On the NYTimes dataset, computing the TLI estimate for a single document, which is just a matrix multiplication and a single thresholding step, takes approximately 0.8 milliseconds. In contrast, on a document of length 1600, a single iteration of Gibbs sampling takes approximately 1.0 ms (and to get the result in the plot we used 1000 iterations). On the Enron semisynthetic data with uniform sparse prior and documents of length 1600, we find it takes about 20 total iterations of Gibbs sampling (15 of them as burn in) to return a result of similar accuracy to TLI. The speed of these methods for different length documents is illustrated in Figure 4.\nInference on Real Documents We run both our algorithm TLI and Gibbs sampling on a subsample of real documents and test the similarity of results. See Table 2. Since we don\u2019t have the ground truth in this setting, we focus on trying to recover a small number of high-weight topics: we take the top-3 scoring topics from each estimated topic vector, and then the overlap is the cardinality of the intersection divided by 3. If we treat the Gibbs sampling result as the \u201cground truth\u201d, this gives the fraction of the highprobability topics our algorithm correctly finds. We also observe that by taking 5 instead of 3 topics from TLI, we can improve recall (fraction of Gibbs topics found by TLI) at the expense of precision (fraction of TLI topics that are also from Gibbs)4."}, {"heading": "8. Conclusion", "text": "This work takes a step towards designing algorithms with provable guarantees for inference in topic modeling, building upon earlier work of Kleinberg and Sandler (Kleinberg & Sandler, 2008) in collaborative filtering. We use a notion of the approximate inverse of a topic matrix (as opposed to its exact inverse) and characterize the mathematical con-\n4We only list recall values because in this setting precision = 3/5 \u00b7 recall.\ndition \u2014 namely, the `\u221e \u2192 `1 condition number \u2014 that determines how well it behaves as an estimator. Furthermore, we showed that this algorithm approximately solves inference in a document with as few as O(r2 log k) words where k is the number of topics and r is the sparsity of the topic vector generating the document. We have showed that such guarantees are optimal in the sense that there are word-topic matrices for which it is information theoretically impossible to make meaningful conclusions about the topic vector with fewer than r2 samples. We also show that our linear estimator identifies a reasonable set of topics, which allows us to solve (via convex programming) the maximum likelihood problem restricted to this set of topics and get better estimations in theory and practice. We also find that in practice, the standard pseudoinverse of the topic matrix is a good choice for B, though we do not have theory to support it.\nThe experiments show that topic model matrices associated with real-life corpora have good `\u221e \u2192 `1 condition number, and that the above method works well with synthetic documents generated using these topic matrices. Moreover the running time is comparable to a single iteration of Gibbs sampling. Topic recovery on real-life documents seems to be slightly weaker, and seems to require further modifications to be more robust to modelmisspecification.\nAcknowledgments. Sanjeev Arora would like to thank the support in part by NSF grants CCF-1527371, DMS1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR- N00014-16-1-2329. Tengyu Ma would like to thank the support in part by Sanjeev Arora\u2019s grants, Simons Award in Theoretical Computer Science and IBM PhD Fellowship. Ankur Moitra would like to thank the support in part by NSF CAREER Award CCF1453261, a grant from the MIT NEC Corporation and a Google Faculty Research Award."}, {"heading": "A. Missing proofs in Section 5", "text": "A.1. Proof of Main Theorem\nWe start by claiming that we only need to consider a reasonable neighborhood of x\u2217 which contains xMLE.\nClaim A.1. Under the setting of Theorem 5.3, we have xMLE \u2264 2x\u2217.\nProof of Claim A.1. For the sake of contradiction, assume this is not true. Then since x\u2217i \u2265 \u03c4/r, we have that xMLE 6\u2208 B where B = B(x\u2217, \u03c4/r, \u2016 \u00b7 \u2016\u221e) be the \u03c4/rinfinity norm ball around x\u2217. For simplicity we do not write\nthe projection to the orthogonal subspace of 1, but all the gradients will be in that subspace while `1/`\u221e norms are measured in original space.\nLet u = x\u2217 \u2212 xMLE. By simple integration along the line of u, we have\n\u2207f(x\u2217) = \u2207f(x\u2217)\u2212\u2207f(xMLE)\n= (\u222b 1 0 \u22072f(tu+ xMLE)dt ) u = \u2212Hu\nwhere H = \u2212 (\u222b 1\n0 \u22072f(tu)dt ) . Let Ht = \u2212\u22072f(tu),\nand therefore H = \u222b 1\n0 Htdt.\nLet x = su + xMLE be the point in B with maximum s. Therefore we have that x is on the boundary of B and therefore \u2016x\u2212 x\u2217\u2016\u221e = \u03c4/r. Then for any 1 \u2265 t \u2265 s, we claim that Hs satisfies that v>Htv \u2265 \u2126(n/\u03ba\u03042)\u2016v\u201621 for any vector v. Indeed, let x = tu+ xMLE, we can verify that\nv>Htv \u2265 1\n4 \u00b7 v>H1v \u2265\nn 8 v>Qv \u2265 \u2126(n/\u03ba\u03042)\u2016v\u201621 .\nTherefore we obtain that\nv>Hv = \u222b 1 0 v>Htvdt\n\u2265 \u222b 1 s \u2126(n/\u03ba\u03042)\u2016v\u201621dt = \u2126((1\u2212 s)n/\u03ba\u03042)\u2016v\u201621 .\nTherefore we obtain that \u2016H\u22121/2\u20162\u21921 \u2264 O (\u221a \u03ba\u03042\n(1\u2212s)n\n) .\nSimilarly we have that since Ht \u2126(n) \u00b7 Q for 1 \u2265 t \u2265 s, and Ht 0 for any t \u2208 [0, 1] , we have that H =\u222b t\n0 Htdt (1 \u2212 s)nQ and therefore \u2016H\u22121/2Q1/2\u20162\u21922 \u2264\nO\n( 1\u221a\n(1\u2212s)n\n) . Then we have that\n\u2016H\u22121Q1/2\u20162\u21921 \u2264 \u2016H\u22121/2\u20162\u21921\u2016H\u22121/2Q1/2\u20162\u21922 \u2264 O ( \u03ba\u0304\n(1\u2212 s)n\n) .\nwhich in turn implies (by Lemma A.4) that \u2016H\u22121Q1/2\u2016\u221e\u2192\u221e \u2264 O(\n\u221a \u03ba\u03042\n(1\u2212s)n ). Therefore,\n\u2016x\u2217 \u2212 x\u2016\u221e = \u2016(s\u2212 1)(x\u2217 \u2212 xMLE)\u2016\u221e = (1\u2212 s)\u2016H\u22121Q1/2Q\u22121/2\u2207f(x\u2217)\u2016\u221e\n\u2264 O( \u03ba\u0304 n ) \u00b7 \u2016Q\u22121/2\u2207f(x\u2217)\u2016\u221e\nThen by equation (9) of Lemma 5.4, we have that\n\u2016x\u2217 \u2212 x\u2016\u221e \u2264 O( \u03ba\u0304\nn ) \u00b7 \u2016Q\u22121/2\u2207f(x\u2217)\u2016\u221e \u2264 O( \u03ba\u0304\u221a n ) .\nThis contradicts with the fact that \u2016x\u2217 \u2212 x\u2016\u221e = \u03c4/r for some n \u2265 \u2126(\u03ba\u03042r2/\u03c42). Hence we showed that xMLE indeed satisfies that xMLE \u2264 2x\u2217, which completes the proof of the claim.\nProof of Theorem 5.3. We restrict out attention to the convex region C = {x : x \u2264 2x\u2217} which contains both xMLE and x\u2217. We change the basis of the space according to the Fisher information matrix. Let g(z) = f(Q\u22121/2z), and let z\u2217 = Q1/2x\u2217, and zMLE = Q1/2xMLE. Then zMLE is also the maximizer of g(z) and therefore \u2207g(zMLE) = 0. Moreover, we have that \u2207g(z) = Q\u22121/2\u2207f(z) and \u22072g(z) = Q\u22121/2\u22072f(z)Q\u22121/2. By Theorem 5.5, for any z \u2208 Q1/2C, we have that \u22072g(z) \u2212Q\u22121/2 \u00b7 nQ/8 \u00b7 Q\u22121/2 = \u2212n/8 \u00b7 Idr. Therefore g(z) is \u2126(n)-strongly concave in Q1/2C. Then by strong concavity of g, we obtain that\n\u2016\u2207g(z\u2217)\u2016 = \u2016\u2207g(z\u2217)\u2212\u2207g(zMLE)\u2016 \u2265 n/2 \u00b7\u2016z\u2217\u2212zMLE\u2016 .\nBy Theorem 5.4, we have \u2016\u2207g(z\u2217) \u2212 nQ\u22121/21r\u2016 = \u2016Q\u22121/2(\u2207f(x\u2217)\u2212n1r)\u2016 \u2264 O\u0303( \u221a nr). Here we are projecting out the all 1\u2019s direction 1r in the original space because x is a probability simplex and \u30081r, x\u3009 is fixed by constraint (similarly the constraint will fix \u3008Q\u22121/21r, z\u3009). Let Proj be the projection to the orthogonal subspace of Q\u22121/21r, we know\n\u2016z\u2217 \u2212 zMLE\u2016 \u2264 O(1/n) \u00b7 \u2016Proj\u2207g(z\u2217)\u2016 \u2264 O\u0303 (\u221a r\nn\n) .\nConverting z to x, and observing that by Lemma 5.6 \u2016AQ\u22121/2\u20162\u21921 \u2264 1 and \u2016Q\u22121/2\u20162\u21921 \u2264 \u03ba\u0304, we obtain that\n\u2016Ax\u2217 \u2212AxMLE\u20161 = \u2016AQ\u22121/2z\u2217 \u2212AQ\u22121/2zMLE\u20161 \u2264 \u2016z\u2217 \u2212 zMLE\u2016 \u2264 O\u0303 (\u221a r\nn\n) .\nSimilarly, we have \u2016x\u2217 \u2212 xMLE\u20161 \u2264 \u2016Q\u22121/2(z\u2217 \u2212 zMLE)\u20161 \u2264 \u03ba\u0304\u2016z\u2217 \u2212 zMLE\u2016 \u2264 O\u0303 ( \u03ba\u0304 \u221a r n ) .\nA.2. Proof of Lemma 5.5 and Lemma 5.6\nWe establish equation (10) by first showing that with high probability, \u2212\u22072f(x\u2217) \u2265 nQ/2. Then using the fact that x \u2264 Cx\u2217, we obtain that \u2212\u22072f(x) \u2212\u22072f(x\u2217)/C2 nQ/(2C2). Towards obtaining the lower bound for \u2212\u22072f(x\u2217), we first consider its expectation, which is lowerbounded in Lemma 5.6, whose proof is as follows.\nProof of Lemma 5.6. For any unit vector v \u2208 Rr, we con-\nsider the quadratic form of v over Q,\nv>Qv = \u2211 i\u2208[D] \u3008a\u0302i, v\u30092 \u3008a\u0302i, x\u2217\u3009\n= \u2211 i\u2208[D] \u3008a\u0302i, v\u30092 \u3008a\u0302i, x\u2217\u3009 \u2211 i\u2208[D] \u3008a\u0302i, x\u2217\u3009  \u2265\n\u2211 i\u2208[D] |\u3008a\u0302i, v\u3009| 2 = \u2016A\u0302v\u201621 (13) \u2265 \u2016v\u201621/\u03ba\u03042 (14)\nwhere the second line uses the fact that \u2211 i\u3008a\u0302i, x\u2217\u3009 = 1, the third line uses Cauchy-Schwartz inequality, and the last line uses that A\u0302 has `1-condition number \u03ba\u0304. (Note that v could take negative entries and therefore \u2016A\u0302v\u20161 could be smaller than \u2016v\u20161. )\nReplacing v in equation (14) by Q\u22121/2z, we have \u2016z\u2016 \u2265 \u2016Q\u22121/2z\u20161/\u03ba\u0304 for any z \u2208 Rr. Therefore, \u2016Q\u22121/2\u20162\u21921 \u2264 \u03ba\u0304. It follows that \u2016Q\u22121/2\u20162\u21922 \u2264 \u03ba\u0304, which is equivalent to Q 1\u03ba\u03042 \u00b7 Idr.\nFinally, using the intermediate result equation (13), plugging v = Q\u22121/2z, we obtain that\n\u2016z\u20162 \u2265 \u2016A\u0302Q\u22121/2z\u201621\nwhich implies that \u2016A\u0302Q\u22121/2\u20162\u21921 \u2264 1.\nThe following Lemma gives high probability (lower) bound for the spectrum of E[\u2212\u22072f(x\u2217)], from which Theorem 5.5 follows straightforwardly.\nLemma A.2. Let Q\u0302 = \u2212 1n\u2207 2f(x\u2217) be the empirical fisher information matrix. Suppose n = c0\u03ba\u03042r2/\u03b22 \u00b7 log k for sufficiently large constant c0. Then with probability at least 1\u2212 k\u221210, we have that\nQ\u0302 1 2 \u00b7Q .\nMoreover, it holds that\n\u2016Q\u0302\u22121/2\u20162\u21921 \u2264 \u221a\n2\u03ba\u0304 , and Q\u0302 1 2\u03ba\u03042 \u00b7 Idr\nSince we have shown that Q 1/\u03ba\u03042 \u00b7 Idr, it would suffice to prove (by matrix concentration inequality) that with high probability, Q\u0302 concentrates around its expectation with variance significantly less than n2. However, it turns out that the variance Q\u0302 could have spectral norm as large as O(r4n). Then it will require n r4 to guarantee that the spectral norm of the variance can be smaller n2, which turns out to be a suboptimal bound.\nWe weaken the requirement to n r2 by observing the following inefficiency in the argument above: Though the\nvariance matrix E[Q\u03022] could have a large eigenvalue with some eigenvector v, when this happens the expectation Q = E[Q\u0302] also have eigenvalue much larger than n/\u03ba\u03042 around some direction correlated with v (in the meantime it might also have a small eigenvalue n/\u03ba\u03042 in some other direction so that the lower bound Q n/\u03ba\u03042 \u00b7 Idr cannot be improved). This suggests us to first whiten the expectation matrix Q to identity matrix, so that we can avoid the inefficiency caused by using spectral norm to measure the size of a very skew matrix.\nProof of Lemma A.2. For w \u2208 y, let Zw = a\u0302wa\u0302 > w\n\u3008a\u0302w,x\u2217\u30092 be the single term in the sum of\u22072f(x\u2217) (see equation (6) for the representation of Hessian). That is, we have\nQ\u0302 = \u2212 1 n \u00b7 \u22072f(x\u2217) = 1 n \u2211 w\u2208y Zw\nWe change basis usingQ\u22121/2. Let Z \u2032w = Q \u22121/2ZwQ \u22121/2. Consider the random variable Q\u0302\u2032 = Q\u22121/2Q\u0302Q\u22121/2 \u2208 Rr\u00d7r, which is a sum of independent random matrices,\nQ\u0302\u2032 = 1\nn \u2211 w\u2208y Z \u2032w .\nToward bounding the fluctuation of Q\u0302, we apply Bernstein inequality on Q\u0302\u2032. By Lemma A.3, we know have that Zw is almost surely bounded by \u2016Zw\u2016 \u2264 r2/\u03b22. Then using equation (12),\nZ \u2032w = Q \u22121/2ZwQ \u22121/2 Q\u22121/2\u00b7r2/\u03b22Idr\u00b7Q\u22121/2 = r2/\u03b22\u00b7Q\u22121\nTherefore it follows that Z \u2032w is almost surely bounded by \u2016Z \u2032w\u2016 \u2264 r2/\u03b22\u2016Q\u22121\u2016 \u2264 r2\u03ba\u03042/\u03b22. Next we bound the variance of Q\u0302\u2032:\nE[(Z \u2032w)2] = 1\nn \u2211 i\u2208[D] \u3008a\u0302i, x\u2217\u3009 \u00b7 ( Q\u22121/2 a\u0302ia\u0302 > i \u3008a\u0302i, x\u2217\u30092 Q\u22121/2 )2\n= 1\nn \u2211 i\u2208[D] a\u0302>i Q \u22121a\u0302i \u3008a\u0302i, x\u2217\u30092 \u00b7Q\u22121/2 a\u0302ia\u0302 > i \u3008a\u0302i, x\u2217\u3009 Q\u22121/2\n1 n \u2211 i\u2208[D] \u2016a\u0302i\u201622\u2016Q\u22121\u2016 \u3008a\u0302i, x\u2217\u30092 \u00b7Q\u22121/2 a\u0302ia\u0302 > i \u3008a\u0302i, x\u2217\u3009 Q\u22121/2\n1 n \u00b7 1/\u03ba\u03042 \u00b7 r2/\u03b22 \u00b7 \u2211 i\u2208[D] Q\u22121/2 a\u0302ia\u0302 > i \u3008a\u0302i, x\u2217\u3009 Q\u22121/2\n= \u03ba\u03042 \u00b7 r2/\u03b22 \u00b7Q\u22121/2QQ\u22121/2 = \u03ba\u03042r2/\u03b22 \u00b7 Idr\u00b7\nwhere the second line is just a re-arrangement of the first line, and the third line uses the definition of spectral norm, and the fourth line uses that \u2016a\u0302i\u2016 2 2\n\u3008a\u0302i,x\u2217\u30092 \u2264 r 2/\u03b22 (see\nLemma A.3) and Q 1/\u03ba\u03042 \u00b7 Idr, and finally the fifth line uses the definition of Q.\nNow we are ready to apply Bernstein inequality on Q\u0302\u2032. We conclude that with high probability,\n\u2225\u2225\u2225Q\u0302\u2032 \u2212 E[Q\u0302\u2032]\u2225\u2225\u2225 \u2264 O(\u221a\u03ba\u03042r2/\u03b22 \u00b7 n log k+r2\u03ba\u03042/\u03b22 \u00b7log k) Recall the definition of Q\u0302\u2032 = Q\u22121/2Q\u0302Q\u22121/2 and that E[Q\u0302\u2032] = Q\u22121/2 E[Q]Q\u22121/2 = Idr , we obtain that for n \u2265 c0r2\u03ba\u03042/\u03b22 \u00b7 log k with sufficiently large constant c0,\n1 2 \u00b7 Idr Q\u0302\u2032 3 2 \u00b7 Idr (15)\nTherefore towards upperbounding the operator norm \u2016Q\u0302\u22121/2\u20162\u21921, we consider the quadratic form v>Q\u0302v and have that\nv>Q\u0302v = (Q1/2v)> \u00b7 Q\u0302\u2032 \u00b7 (Q1/2v) \u2265 1 2 \u2016Q1/2v\u20162 \u2265 1 2\u03ba\u03042 \u2016v\u201621\nwhere the first inequality uses (15) and the second uses Lemma 5.6 (or equation (14)). It follows easily that \u2016Q\u0302\u22121/2\u20162\u21921 \u2264 \u221a 2\u03ba\u0304.\nLemma A.3. Suppose x \u2208 Sr such that x \u2265 \u03b2/r \u00b71r, then Then for any vector a \u2208 Rr\u22650, \u2016a\u20162 \u2264 \u03b2/r \u00b7 \u3008a, x\u3009 and\u2225\u2225\u2225 aa>\u3008a,x\u30092 \u2225\u2225\u2225 \u2264 r2/\u03b22. Proof of Lemma A.3. Then we have that \u3008a, x\u3009 \u2265 \u03b2/r \u00b7 \u2016a\u20161 \u2265 \u03b2/r \u00b7 \u2016a\u20162, and therefore\n\u2225\u2225\u2225 aa>\u3008a,x\u30092 \u2225\u2225\u2225 \u2264 \u2016a\u201622\u3008a,x\u30092 \u2264 r2/\u03b22.\nLemma A.4. For any symmetric positive semidefinite matrix G, it holds that \u2016G\u2016\u221e\u2192\u221e \u2264 \u2016G\u20162\u21921.\nProof of Lemma A.4. We have that since `1 norm is larger than `2, we have that \u2016G\u20161\u21921 \u2264 \u2016G\u20162\u21922. For symmetric matrix, we have \u2016G\u2016\u221e\u2192\u221e = \u2016G\u20161\u21921 which completes the proof.\nA.3. Proof of Lemma 5.4\nBefore proving the concentration of \u2207f(x\u2217), we start by calculating its mean and variance.\nLemma A.5. The mean and variance of\u2207f(x\u2217) are equal to\nE [\u2207f(x\u2217)] = n \u00b7 1r ,\nand E [ \u2016\u2207f(x\u2217)\u20162Q\u22121 ] = nr .\nProof. Plugging x = x\u2217 into equation (6), we have that\n\u2207f(x\u2217) = \u2211 w\u2208y a\u0302w \u3008a\u0302w, x\u2217\u3009 .\nIt follows straightforwardly that\nE [\u2207f(x\u2217)] = \u2211 w\u2208y E [ a\u0302w \u3008a\u0302w, x\u2217\u3009 ] = n\n\u2211 i\u2208[D] \u3008a\u0302i, x\u2217\u3009 a\u0302i \u3008a\u0302i, x\u2217\u3009 = n \u2211 i\u2208[D] a\u0302i = n1r .\nTowards computing the variance of \u2207f(x\u2217) (under the Q\u22121 norm), we observe that\nE [ \u2207f(x\u2217)\u2207f(x\u2217)> ] = \u2211 i\u2208[D] \u3008a\u0302i, x\u2217\u3009 a\u0302ia\u0302 > i \u3008a\u0302i, x\u2217\u30092\n= \u2212E [ \u22072f(x\u2217) ] ] = Q .\nTherefore\nE[\u2016\u2207f(x\u2217)\u20162Q\u22121 ] = E[tr(Q \u22121\u2207f(x\u2217)\u2207f(x\u2217)>)] = nr .\nNow we are ready to prove Lemma 5.4 by using Bernstein inequality on\u2207f(x\u2217).\nProof of Lemma 5.4. Note that \u2016\u2207f(x\u2217)\u20162Q\u22121 = \u2016Q\u22121/2\u2207f(x\u2217)\u20162. We apply concentration inequality on\nQ\u22121/2\u2207f(x\u2217) = \u2211 w\u2208y Q\u22121/2a\u0302w \u3008a\u0302w, x\u2217\u3009 ,\nwhich is a sum of independent random variables. Toward using Bernstein inequality, we first verify that\u2225\u2225\u2225\u2225Q\u22121/2a\u0302w\u3008a\u0302w, x\u2217\u3009 \u2225\u2225\u2225\u2225 \u2264 \u03c3\u22121/2min (Q) \u00b7 \u2225\u2225\u2225\u2225 a\u0302w\u3008a\u0302w, x\u2217\u3009 \u2225\u2225\u2225\u2225 \u2264 \u03ba\u0304 \u00b7 r/\u03c4\nwhere the last inequality uses the As bounded in Lemma A.5, the variance of Q\u22121/2\u2207f(x\u2217) is at most nr, and the mean of Q\u22121/2\u2207f(x\u2217) is nQ\u22121/21r. Therefore, by Bernstein inequality, we obtain that with high probability\u2225\u2225\u2225Q\u22121/2(\u2207f(x\u2217)\u2212 n1r)\u2225\u2225\u2225 \u2264 O\u0303(\u03ba\u0304r/\u03c4+\u221anr) = O\u0303(\u221anr) . for n \u2265 \u2126(\u03ba\u03042r/\u03c42).\nFor the infinity norm bound, we fix a coordinate j and have that the j-th coordinate of Q\n\u22121/2a\u0302w \u3008a\u0302w,x\u2217\u3009 is smaller than\nits `2 norm, which has been shown to be smaller than \u03ba\u0304r/\u03c4 . Therefore applying Bernstein\u2019s inequality, and taking union bound over all coordinates, we have that\u2225\u2225\u2225Q\u22121/2(\u2207f(x\u2217)\u2212 n1r)\u2225\u2225\u2225\n\u221e \u2264 O(\u03ba\u0304r/\u03c4 \u00b7 log2 k +\n\u221a n log3 k)\n= O( \u221a n log1.5 k) .\nfor some n \u2265 \u2126((\u03ba\u03042r2 log2 k)/\u03c42)"}, {"heading": "B. Proof of Proposition 3.2", "text": "Proof of Proposition 3.2. Let J be the all 1\u2019s matrix. We rewrite the program (3) as an LP by introducing auxiliary variable t.\n\u03bb\u03b4(A) = min t s.t. B \u2264 tJ\n\u2212B \u2264 \u2212tJ BA\u2212 Id \u2264 \u03b4J \u2212BA+ Id \u2264 \u2212\u03b4J\nLet P1, P2 \u2208 Rk\u00d7D, Q1, Q2 \u2208 Rk\u00d7k be the dual variables for the four (set of) constraints. Let \u3008X,Y \u3009 = tr(XTY ) denote the inner product of the two matrices. Then, the dual of the program above is\nmaximize \u3008Q2 \u2212Q1, Id\u3009 \u2212 \u03b4\u3008Q2 +Q1, J\u3009 s.t. (P1 \u2212 P2) + (Q1 \u2212Q2)A> = 0\n\u3008P1 + P2, J\u3009 = 1 P1, P2, Q1, Q2 \u2265 0 (16)\nLet Q = Q2 \u2212Q1 and P = P1 \u2212 P2. Observe that \u3008P1 + P2, J\u3009 \u2265 |P |1 and \u3008Q1 +Q2, J\u3009 \u2265 |Q|1, it is easy to verify that program (16) is equivalent to the program below\nmaximize tr(Q)\u2212 \u03b4|Q|1 s.t. P +QA> = 0\n|P |1 \u2264 1 (17)\nTowards further simplification, we claim that program (17) is equivalent to the following (non-convex) program with vector variables x \u2208 Rk:\nmaximize \u2016x\u2016\u221e \u2212 \u03b4\u2016x\u20161 s.t. \u2016Ax\u20161 \u2264 1 (18)\nIndeed, suppose program (4) has optimal value \u03bb and program (18) has optimal value \u03bb\u2032 with optimal solution xopt. We first show that for any x,\n\u2016x\u2016\u221e \u2212 \u03b4\u2016x\u20161 \u2264 \u03bb\u2032\u2016Ax\u20161 , (19)\nwhich is due to the homogeneity of the equation. Then, consider any P,Q that satisfies the constraint of (4). Let Pj and Qj be the rows of P and Q. We have\ntr(Q)\u2212 \u03b4|Q|1 \u2264 \u2211 j (\u2016Qj\u2016\u221e \u2212 \u03b4\u2016Qj\u20161)\n\u2264 \u2211 j (\u2016AQj\u20161) = |P |1\nwhere the second inequality is by equation (19). Therefore \u03bb \u2264 \u03bb\u2032.\nOn the other hand, suppose the xopt has coordinate i with the largest absolute value. Then let Q be the matrix which has it\u2019s j-th row as xopt and 0 elsewhere, and P = \u2212QA>. Then it\u2019s straightforward to check P,Q satisfy the constraint of (4) and have objective value \u03bb\u2032. Therefore \u03bb\u2032 \u2264 \u03bb. Hence we obtained that \u03bb = \u03bb\u2032. Finally, from (18) it\u2019s easy to see \u03bb0(A) = \u03bb(A), and \u03bb\u03b4(A) \u2264 \u03bb0(A)."}, {"heading": "C. Missing proofs in Section 6", "text": "Proof of Lemma 6.3. We construct x, x\u2212 randomly as follows. Let R \u2282 [k] be a random subset of size r, and let R\u2212 \u2282 R be a random subset with one item removed from R (so |R\u2212| = r \u2212 1). Let xi = 1/r if i \u2208 R, and xi = 0 otherwise. Let x\u2212i = 1/(r \u2212 1) if i \u2208 R\u2212 and 0 otherwise.\nEach word of the document is from a multinomial distribution whose probabilities are specified by either Ax or Ax\u2212. We shall show the two distributions have small KLdivergence so no algorithm can distinguish between the two.\nFirst we observe that most rows will have between r/4 and 3r/4 entries with value roughly 2/D in the subset R\u2212 of coordinates . We say a row is biased if it has less than r/4 or more than 3r/4 nonzero entries in R\u2212, otherwise it\u2019s balanced.\nClaim C.1. With high probability when r is larger than a fixed constant, and when D r2, there are at most D/r2 biased rows.\nProof. For every row, when we look at the entries in R\u2212, these entries are independent and has probability 1/2 of being nonzero. Therefore the probability that the row is biased is e\u2212\u2126(r\n2) which is much smaller than 1/2r2. Different rows are also independent, so by Chernoff bound we know with high probability there are at most D/r2 biased rows.\nWe know the entries in i-th column of A are all equal to 1/|Si|, and with high probability 1/|Si| is within (2 \u00b1\no(1))/D. Therefore the probability of every word in Ax or Ax\u2212 is at most (2 + o(1))/D, and the probability that we see a biased row is less than o(1) if we have o(r2) samples. Therefore we can condition on the event that the algorithm does not see any biased row.\nSuppose row i that is balanced, let {j} = R\\R\u2212, we know |(Ax)i\u2212(Ax\u2212)i| = | 1rAi,j\u2212 1 r(r\u22121) \u2211 t\u2208R\u2212 Ai,t| \u2264 maxAi,j \u00b7 2r < 5 rD . On the other hand, we know (Ax\u2212)i \u2208 [(1 \u2212 o(1))/2D, (3 + o(1))/2D] because row i is balanced. Let pi be the probability of seeing word i fromAx, conditioned on that we don\u2019t see any biased row(word). Similarly let qi be the probability of seeing word i from Ax\u2212 with the same conditioning. The probabilities pi and qi are within 1 + 1/r2 multiplicative factor with (Ax)i and (Ax\u2212)i by the rule of conditioning, and in particular we know (pi \u2212 qi) \u2264 10qi/r. Therefore the \u03c72-distance between p and q is \u03c72(p, q) = \u2211 i (pi\u2212qi)2 qi \u2264 \u2211 i 100qi/r\n2 = 100/r2 , where the sum is over all balanced row i. This implies that the KL-divergence between p and q is less than 100/r2 and therefore it is impossible to distinguish between p and q with more than 1/2 + o(1) accuracy with o(r2) samples.\nNext we prove Lemma 6.4.\nProof of Lemma 6.4. For the lowerbound of \u03ba(A), we will construct a vector x = (1, 1, 1, ..., 1,\u22121, ...,\u22121). That is, xi = 1 for i \u2264 k/2 and xi = \u22121 if i > k/2 (without loss of generality here we assume k is even).\nNow consider the `1 norm of Ax. To do that, consider a different matrix A\u0302 where A\u0302i,j = 2/D if Ai,j > 0, and A\u0302i,j = 0 if Ai,j = 0. Basically, A\u0302 has the same support as A, except its row may not normalized.\nUsing Chernoff bound and Union bound, we know with high probability, the size of the sets Si\u2019s are bounded by D/2\u00b1O( \u221a D log k). Therefore the maximum entry in A\u0302\u2212\nA is at most 2D \u00b7 \u221a\n(log k)/D. Therefore |(A\u0302 \u2212 A)x|1 \u2264 D \u00b7 2D \u00b7 \u221a (log k)/D \u00b7 |x|1 \u2264 O( \u221a (log k)/D)|x|1.\nOn the other hand, we know for the x that we constructed, E[A\u0302x] = 0, and E[|(A\u0302x)i|] = O( \u221a k/D) because entries in A\u0302 are independent of each other. By Chernoff bounds we know with high probability |A\u0302x|1 = O( \u221a k). However, |x|1 = k, so when D k log k we have\n|Ax|1 \u2264 |A\u0302x|1 + |(A\u0302\u2212A)x|1 \u2264 O( \u221a k) = O(\n1\u221a k )|x|1.\nTherefor \u03ba(A) \u2265 \u2126( \u221a k).\nFor \u03bb(A), first notice that by Lemma 6.2, we know \u039b\u03b4(A) \u2264 1 when D is larger than \u2126((log k)/\u03b42. That means there is a matrix B with maximum entry at most 1 and BA = I \u2212\u2206 where \u2206 has maximum entry \u03b4. Now let \u03b4 < 1/2k, then the rows of \u03b4 have `1 norm at most 1/2. By Gershgorin\u2019s Disk Theorem we know \u2016\u2206\u2016 < 1/2. Therefore we can write\n(I \u2212\u2206)\u22121 = I + \u221e\u2211 i=1 \u2206i =: I + C.\nThe matrix C is defined to be \u2211\u221e i=1 \u2206\ni. The maximum entry |\u2206i|\u221e is bounded by (1/2)i\u22121\u03b4. Therefore the maximum entry |C|\u221e is bounded by 2\u03b4. Now let B\u0302 = (I+C)B, we know B\u0302A = (I + C)BA = (I \u2212 \u2206)\u22121(I \u2212 \u2206) = I . On the other hand,\n|B\u0302|\u221e \u2264 |B|\u221e+|CB|\u221e \u2264 1+|C|\u221e|B|\u221e\u00b7k \u2264 1+2\u03b4k \u2264 2.\nSo A has a pseudoinverse with maximum entry at most 2. By Proposition 3.2 the condition number \u03bb(A) \u2264 2."}, {"heading": "D. Lower bound on `1 \u2192 `1 condition number", "text": "Here we describe how we bound from below the `1 \u2192 `1 condition number in Table 1. Fix \u03b4 > 0 and letB\u03b4 \u2208 Rk\u00d7D be a minimizer of LP (3) with given \u03b4, i.e. |B\u03b4|\u221e = \u03bb\u03b4 and |BA\u2212I|\u221e \u2264 \u03b4. By compactness, there exists a v such that |v|\u221e/|Av|1 = \u03bb(A). Then,\n\u03bb\u03b4(A) \u2265 |B\u03b4Av|\u221e |Av|1\n\u2265 (|v|\u221e \u2212 |(BA\u2212 I)v|\u221e)/|Av|1 \u2265 (|v|\u221e \u2212 |BA\u2212 I|\u221e|v|1)/|Av|1 \u2265 \u03bb(A)\u2212 \u03b4\u03ba(A).\nHence, we can bound from below \u03ba(A) by,\n\u03ba(A) \u2265 \u03bb\u03b4(A)\u2212 \u03bb(A) \u03b4 ."}], "references": [{"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "In NIPS,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel", "Kakade", "Sham M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["Zhu", "Michael"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Zhu and Michael.,? \\Q2013\\E", "shortCiteRegEx": "Zhu and Michael.", "year": 2013}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Moitra", "Ankur"], "venue": null, "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Provable bounds for learning some deep representations", "author": ["Arora", "Sanjeev", "Bhaskara", "Aditya", "Ge", "Rong", "Ma", "Tengyu"], "venue": "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,", "citeRegEx": "Arora et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2014}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Bickel", "Peter J", "Ritov", "Yaacov", "Tsybakov", "Alexandre B"], "venue": "Ann. Statist., 37(4):1705\u20131732,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research, pp. 993\u20131022,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Decoding by linear programming", "author": ["E.J. Candes", "T. Tao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Candes and Tao,? \\Q2005\\E", "shortCiteRegEx": "Candes and Tao", "year": 2005}, {"title": "Learning mixtures of gaussians in high dimensions", "author": ["Ge", "Rong", "Huang", "Qingqing", "Kakade", "Sham M"], "venue": "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,", "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Finding scientific topics", "author": ["Griffiths", "Thomas L", "Steyvers", "Mark"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2004}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Hsu", "Daniel", "Kakade", "Sham M"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2013}, {"title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods", "author": ["M. Janzamin", "H. Sedghi", "A. Anandkumar"], "venue": null, "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Some highlights include algorithms for topic modeling (Arora et al., 2013b; Anandkumar et al., 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al.", "startOffset": 54, "endOffset": 100}, {"referenceID": 8, "context": ", 2012), learning mixture models (Moitra & Valiant, 2010; Hsu & Kakade, 2013; Ge et al., 2015), community detection (Anandkumar et al.", "startOffset": 33, "endOffset": 94}, {"referenceID": 1, "context": ", 2015), community detection (Anandkumar et al., 2014) and (special cases of) deep learning (Arora et al.", "startOffset": 29, "endOffset": 54}, {"referenceID": 4, "context": ", 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).", "startOffset": 45, "endOffset": 88}, {"referenceID": 11, "context": ", 2014) and (special cases of) deep learning (Arora et al., 2014; Janzamin et al., 2015).", "startOffset": 45, "endOffset": 88}, {"referenceID": 6, "context": "The model assumes a distribution on x that favors sparse or approximately sparse vectors; a popular choice is the Dirichlet distribution (Blei et al., 2003).", "startOffset": 137, "endOffset": 156}, {"referenceID": 0, "context": "Recent (provable) algorithms for this problem (Anandkumar et al., 2012; Arora et al., 2013b) use the method of moments, leveraging the fact that some form of averaging over the corpus yields a linear algebraic problem for recovering A.", "startOffset": 46, "endOffset": 92}, {"referenceID": 0, "context": "Alternatively, one can use a co-occurrence tensor and recover A under weaker assumptions (Anandkumar et al., 2012).", "startOffset": 89, "endOffset": 114}, {"referenceID": 5, "context": "Moreover, the restricted `1 \u2192 `1 condition number can be viewed as `1 analog of the restricted isometry property (Candes & Tao, 2005) or restricted eigenvalue conditions (Bickel et al., 2009; Meinshausen & Yu, 2009) associated to `2 norm.", "startOffset": 170, "endOffset": 215}], "year": 2016, "abstractText": "Recently, there has been considerable progress on designing algorithms with provable guarantees \u2014 typically using linear algebraic methods \u2014 for parameter learning in latent variable models. But designing provable algorithms for inference has proven to be more challenging. Here we take a first step towards provable inference in topic models. We leverage a property of topic models that enables us to construct simple linear estimators for the unknown topic proportions that have small variance, and consequently can work with short documents. Our estimators also correspond to finding an estimate around which the posterior is well-concentrated. We show lower bounds that for shorter documents it can be information theoretically impossible to find the hidden topics. Finally, we give empirical results that demonstrate that our algorithm works on realistic topic models. It yields good solutions on synthetic data and runs in time comparable to a single iteration of Gibbs sampling.", "creator": "LaTeX with hyperref package"}}}