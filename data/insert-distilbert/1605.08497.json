{"id": "1605.08497", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "Universum Learning for SVM Regression", "abstract": "this paper extends the idea of universum learning [ 18, 19 ] to regression problems. we do propose useful new universum - svm formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. these additional functional data samples or universum belong physically to the same application domain as the training component samples, but they do follow a different hierarchical distribution. several empirical comparisons are presented accordingly to illustrate the utility of the proposed approach.", "histories": [["v1", "Fri, 27 May 2016 03:11:41 GMT  (260kb,D)", "http://arxiv.org/abs/1605.08497v1", "10 pages,11 figures, Thesis:this http URL"]], "COMMENTS": "10 pages,11 figures, Thesis:this http URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sauptik dhar", "vladimir cherkassky"], "accepted": false, "id": "1605.08497"}, "pdf": {"name": "1605.08497.pdf", "metadata": {"source": "CRF", "title": "Universum Learning for SVM Regression", "authors": ["Sauptik Dhar", "Vladimir Cherkassky"], "emails": ["sauptik.dhar@us.bosch.com", "cherk001@umn.edu"], "sections": [{"heading": "CCS Concepts", "text": "\u2022Theory of computation\u2192 Support vector machines;"}, {"heading": "Keywords", "text": "Support Vector Regression; learning through contradiction; Universum Learning"}, {"heading": "1. INTRODUCTION", "text": "The technique of Universum learning or learning through contradiction [18,19] provides a formal mechanism for incorporating a priori knowledge about the application domain, in the form of additional (unlabeled) Universum samples. Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21]. More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22]. However, most research on Universum learning has been limited to binary classification problems. It is not clear how to extend or modify the Universum learning approach to other types of learning problems.\n\u2217Majority of this work was done during S.Dhar\u2019s PhD, and is available in his thesis: http://conservancy.umn.edu/handle/ 11299/162636\nBesides classification setting, another common supervised learning problem is regression or real-valued function estimation from noisy samples [4, 8]. The output in regression problems is a random variable that takes on real values and can be interpreted as the sum of a deterministic function t(x) and a random error \u03b4 with zero mean:\ny = t(x) + \u03b4 (1)\nwhere, the deterministic part, aka the target function, is the mean of the output conditional probability,\nt(x) = \u222b yp(y | x) dx (2)\nHere, (x, y) follows an underlying distribution described by the joint density function,\np(x, y) = p(y | x)p(x) (3)\nThe goal of the learning problem is to estimate the \u2018best \u2019 function (model) from a set of approximating functions f(x, \u03c9) parameterized by \u03c9 \u2208 \u2126. The quality of the approximation is measured by a loss or discrepancy function L(y, f(x, \u03c9)). Thus, the problem of regression involves estimation of a realvalued function that minimizes the risk functional,\nR(\u03c9) = \u222b L(y, f(x, \u03c9))p(x, y)dxdy (4)\nA typically chosen loss function for most regression problems is the squared loss,\nL(y, f(x, \u03c9)) = (y \u2212 f(x, \u03c9))2 (5)\nThe difficulty of regression estimation is due to the fact that statistical distribution of (x, y) is unknown, and the only information (about it) is available in the form of finite training data set (xi, yi) n i=1. For regression problems, one can also expect to achieve improved generalization performance by incorporating a priori knowledge in the form of additional Universum samples. However, formalizing the notion of \u2018contradiction\u2019 for regression setting is not straightforward. This paper describes the concept of Universum learning for regression problems and provides new optimization formulation that incorporates additional Universum data into SVM regression setting.\nThis paper is organized as follows. Section 2 describes standard SVM regression (SVR) formulation. Section 3 extends the notion of Universum learning for regression problems and introduces the new Universum-SVM regression (USVR) formulation. Next we provide empirical results to illustrate the effectiveness of this new formulation in Section 4. Finally, Section 5 presents conclusions.\nar X\niv :1\n60 5.\n08 49\n7v 1\n[ cs\n.L G\n] 2\n7 M\nay 2\n01 6"}, {"heading": "2. SVM REGRESSION", "text": "This section provides brief description of standard SVM regression (SVR) formulation following Vapnik [18,19]. This SVM regression setting employs special margin-based loss function known as \u03b5 -insensitive loss L\u03b5(f(x,w), y) = max(| y \u2212 f(x,w) | \u2212\u03b5, 0). Note that this loss function is defined in the space of the target values y \u2208 <, as shown in Fig. 1. Then SVM regression optimization formulation (for linear parameterization) can be stated as follows:\nGiven i.i.d training samples (xi, yi) n i=1; with x \u2208 <d, y \u2208 <, the linear SVM model can be found by solving the optimization problem\nmin w,b\n1 2 (w \u00b7w) + C n\u2211 i=1 (\u03bei + \u03be \u2217 i ) (6)\ns.t. yi \u2212 (w \u00b7 xi)\u2212 b \u2264 \u03b5+ \u03bei \u03bei \u2265 0 i = 1 . . . n (w \u00b7 xi) + b\u2212 yi \u2264 \u03b5+ \u03be\u2217i \u03be\u2217i \u2265 0 i = 1 . . . n\nhere n := number of training samples, and d := dimensionality of the input space (or the number of input variables). Note that training samples falling inside the \u03b5 - tube have zero loss, and samples outside the \u03b5 - insensitive zone are linearly penalized using the slack variables \u03bei, \u03be \u2217 i \u2265 0, i = 1 . . . n (as shown in Fig. 1b). These slack variables contribute to the empirical risk for the SVR formulation\nRemp(w) = n\u2211 i=1 (\u03bei + \u03be \u2217 i ). The SVR formulation attempts to strike a balance between the minimization of empirical risk and the penalization term. The user-defined parameter C \u2265 0 controls the trade-off between the empirical risk and the penalization term, and \u03b5 \u2265 0 controls the size of the \u03b5 - insensitive zone in margin-based loss. Both of these parameters jointly control the SVM model complexity and hence its generalization performance. For most SVR solvers, problem (6) is usually solved in its dual form (see [4] for details):\nmin \u03b1,\u03b2\n\u03b5 n\u2211 i=1 (\u03b1i + \u03b2i)\u2212 n\u2211 i=1 yi(\u03b1i \u2212 \u03b2i) (7)\n+ 1\n2 n\u2211 i,j=1 (\u03b1i \u2212 \u03b2i)(\u03b1j \u2212 \u03b2j)(xi \u00b7 xj)\ns.t.\nn\u2211 i=1 \u03b1i = n\u2211 i=1 \u03b2i, 0 \u2264 \u03b1i, \u03b2i \u2264 C, i = 1 . . . n\nSolution of the dual formulation (7) yields optimal values of parameters (\u03b1\u2217i , \u03b2 \u2217 i ) n i=1 that can be used to construct the\noptimal SVR function: f(x) = n\u2211 i=1 (\u03b1\u2217i\u2212\u03b2\u2217i )(xi \u00b7x)+b. In this optimal solution, training samples with non-zero coefficients are the support vectors (SVs), corresponding to data points at the boundary or outside \u03b5-insensitive zone.\nThis dual formulation (7) can be also used to extend linear SVR to a non-linear setting. This is accomplished by replacing the dot product (xi \u00b7 xj) in (7) with a non-linear kernel K(xi,xj). This kernel K(xi,xj) = (\u03d5(xi) \u00b7 \u03d5(xj)) implicitly captures the non-linear mapping of the data x\u2192 \u03d5(x). Commonly used kernel functions include:\n\u2013 Polynomial kernel (of degree q):\nK(xi,xj) = ((xi \u00b7 xj) + 1)q\n\u2013 Radial Basis Function (RBF) (with parameter \u03b3):\nK(xi,xj) = exp(\u2212\u03b3\u2016xi \u2212 xj\u20162)\nFor more details see [4, 11]."}, {"heading": "3. UNIVERSUM-SVM REGRESSION", "text": ""}, {"heading": "3.1 Universum-SVM Regression formulation", "text": "This section describes proposed Universum-SVM regression formulation, using new notion of falsification for regression setting, as explained next. Consider regression setting where available training data (xi, yi) n i=1 is modeled using linear SVR. As described in Section 2, for SVM regression the concept of \u2018margin\u2019 is implemented via \u03b5 - insensitive zone (Fig. 1a). That is, training samples falling inside \u03b5 - insensitive zone are \u2018explained\u2019 by SVM model, and samples falling outside \u2018falsify\u2019 or \u2018contradict\u2019 this model. Next, consider two SVR models which explain training samples equally well, e.g. both SVR models use the same value of \u03b5\nand achieve the same empirical risk Remp(w) = n\u2211 i=1 (\u03bei + \u03be \u2217 i ) for training samples. For example, Fig. 2 shows two SVR models that explain available training data equally well, i.e. have zero empirical risk. Now, consider additional Universum samples (x\u2217j , y \u2217 j ) m j=1. These samples are defined in the same (x, y) space as the training samples, and they reflect a priori knowledge that they should not be explained well by SVM regression model. That is, Universum samples should\nlie outside the \u03b5 - insensitive tube. For the toy example shown in Fig. 2, we should favor the model shown in black, for which most Universum samples cannot be explained by SVR model. Note that for regression setting, the Universum samples are labeled, unlike unlabeled Universum samples for classification. This reasoning motivates new Universum support vector regression (U-SVR) formulation where:\n\u2013 Standard \u03b5 - insensitive loss is used for training samples. This loss forces training samples to lie inside \u03b5 - insensitive tube.\n\u2013 Universum samples are penalized by a different loss function as shown in Fig. 3.\nThis new loss function forces the universum samples to lie \u2018far away\u2019 from the regression model, so that samples outside a\u00b1\u2206 zone have zero loss. Penalization of universum samples inside the \u00b1\u2206 zone is achieved via the slack variables \u03b6j as shown in Fig. 3. Note that the tunable parameter \u2206 can be larger (or smaller) than \u03b5. This leads to the following optimization formulation for U-SVR:\nmin w,b,\u03be,\u03be\u2217,\u03b6\nL(w, b, \u03bei, \u03be \u2217, \u03b6) = (8)\n(Training samples) (Universum samples)\n1 2 (w \u00b7w) + C n\u2211 i=1 (\u03bei + \u03be \u2217 i ) s.t. yi \u2212 (w \u00b7 xi)\u2212 b \u2264 \u03b5+ \u03bei (w \u00b7 xi) + b\u2212 yi \u2264 \u03b5+ \u03be\u2217i \u03bei, \u03be \u2217 i \u2265 0, i = 1 . . . n\n(convex)\n+\nC\u2217 m\u2211 j=1 \u03b6j\n|y\u2217j \u2212 (w \u00b7 x\u2217j )\u2212 b| \u2265 \u2206\u2212 \u03b6j\n\u03b6j \u2265 0, j = 1 . . .m\n(concave U\u2206(yj \u2212 f(x\u2217j )))\nHere, parameters : \u03b5,\u2206 \u2265 0 and C,C\u2217 \u2265 0 control the tradeoff between \u2018explanation\u2019 of training samples and \u2018falsification\u2019 of the universum samples. This new optimization formulation for U-SVR has two additional tuning parameters, C\u2217 and \u2206, relative to standard SV regression setting. Note that setting C\u2217 = 0 or \u2206 = 0 in the U-SVR formulation yields standard SVR formulation."}, {"heading": "3.2 Computational Implementation of U-SVR", "text": "The U-SVR formulation (8) is non-convex due to nonconvexity of the Universum loss U\u2206(y \u2217 j \u2212 f(x\u2217j )) shown in Fig. 3. Hence, it cannot be solved using standard convex solvers commonly used in machine learning. Recently, similar non-convex optimization problems have been addressed in [9,16] using the ConCave Convex Programming (CCCP) strategy. According to CCCP strategy, the cost function J(\u03b8) is decomposed as the sum of a convex part Jvex(\u03b8) and a concave part Jcav(\u03b8), where \u03b8 is the optimization argument. Each iteration of the CCCP procedure approximates the concave part by its tangent and minimizes the resulting convex function (see Algorithm 1).\nAlgorithm 1: ConCave Convex Programming (CCCP)\nInitialize \u03b80 ; repeat\n\u03b8t+1 = argmin \u03b8\n(Jvex(\u03b8) + J \u2032 cav(\u03b8t) \u00b7 \u03b8);\nuntil convergence of \u03b8;\nHence, we propose to apply the CCCP strategy for solving the non-convex optimization formulation (8). Detailed application of the CCCP strategy and the resulting algorithm for solving the U-SVM regression formulation (8) are presented next.\nThe Universum loss function can be represented as a sum of two ramp losses, U\u2206(y \u2217 j \u2212 f(x\u2217j )) = A\u2206(y\u2217j \u2212 f(x\u2217j )) + A\u2206(f(x \u2217 j ) \u2212 y\u2217j )+ a constant (see Fig. 4a); where A\u2206(y\u2217j \u2212 f(x\u2217j )) = max(0,\u2206 \u2212 t) \u2212 max(0,\u2212t) (see Fig. 4b). The constant term does not affect the optimization; and hence (8) can be re-written as,\nmin w,b,\u03be,\u03be\u2217,\u03b6,\u03b6\u2217\n1 2 (w \u00b7w) + C n\u2211 i=1 (\u03bei + \u03be \u2217 i ) + C \u2217 m\u2211 j=1 (\u03b6i + \u03b6 \u2217 i ) \u2212 2m\u2211 j=1 H(y\u2217j , f(x \u2217 j )) (9)\ns.t. yi \u2212 (w \u00b7 xi)\u2212 b \u2264 \u03b5+ \u03bei, \u03bei \u2265 0 (w \u00b7 xi) + b\u2212 yi \u2264 \u03b5+ \u03be\u2217i , \u03be\u2217i \u2265 0 y\u2217j \u2212 (w \u00b7 x\u2217j )\u2212 b \u2264 \u2212\u2206 + \u03b6j , \u03b6j \u2265 0 (w \u00b7 x\u2217j )\u2212 b\u2212 y\u2217j \u2264 \u2212\u2206 + \u03b6\u2217j , \u03b6\u2217j \u2265 0 i = 1 . . . n, j = 1 . . .m\nwhere,\nH(y\u2217j , f(x \u2217 j )) ={\nmax(0,\u2212y\u2217j + (w \u00b7 x\u2217j )\u2212 b); j = 1 . . .m max(0, y\u2217j \u2212 (w \u00b7 x\u2217j )\u2212 b); j = m+ 1 . . . 2m\nand f(x) = (w \u00b7 x) + b. Next, define :\nkj = \u2212C\u2217 \u2202H(y\u2217j , f(x \u2217 j ))\n\u2202f(x\u2217j ) (10)\n=  \u2212C \u2217; if y\u2217j < f(x \u2217 j ); j = 1 . . .m C\u2217; if y\u2217j > f(x \u2217 j ); j = m+ 1 . . . 2m\n0; else\n\u2261  \u2212C \u2217; if y\u2217j < f(x \u2217 j ); j = 1 . . .m C\u2217; if y\u2217j > f(x \u2217 j ); j = 1 . . .m\n0; else\nThe last equivalence follows as the conditions are mutually exclusive. Hence,\n\u2212C\u2217 \u2202H(y \u2217 j ,f(x \u2217 j ))\n\u2202\u03b8 \u00b7 \u03b8 = \u2212C\u2217 \u2202H(y\n\u2217 j ,f(x \u2217 j ))\n\u2202f(x\u2217j ) \u00b7 \u2202f(x\n\u2217 j )\n\u2202\u03b8 \u00b7 \u03b8\n= kj [ x\u2217j 1 ]> [ w b ] = kj(w \u00b7 x\u2217j + b); with \u03b8 = [ w b ] .\nHence, application of the CCCP strategy to solving formulation (8) yields the following Algorithm 2.\nAlgorithm 2: CCCP algorithm for U-SVR\n1. Initialize (w0, b0) using the standard SVR model (see eq. (6)) ; repeat\n2. At t+ 1 iteration update,\nkt+1j =  \u2212C \u2217; if y\u2217j < (w t \u00b7 x\u2217j ) + bt; j = 1 . . .m C\u2217; if y\u2217j > (w\nt \u00b7 x\u2217j ) + bt; j = 1 . . .m 0; else\n3. Solve the following eq. (9) to obtain (wt+1, bt+1)\nmin w,b,\u03be,\u03be\u2217,\u03b6,\u03b6\u2217\n1 2 (w \u00b7w) + C n\u2211 i=1 (\u03bei + \u03be \u2217 i )\n+ C\u2217 m\u2211 j=1 (\u03b6i + \u03b6 \u2217 i \u2212 kt+1j (w \u00b7 x \u2217 j + b))\ns.t. yi \u2212 (w \u00b7 xi)\u2212 b \u2264 \u03b5+ \u03bei, \u03bei \u2265 0 (w \u00b7 xi) + b\u2212 yi \u2264 \u03b5+ \u03be\u2217i , \u03be\u2217i \u2265 0 y\u2217j \u2212 (w \u00b7 x\u2217j )\u2212 b \u2264 \u2212\u2206 + \u03b6j , \u03b6j \u2265 0 (w \u00b7 x\u2217j )\u2212 b\u2212 y\u2217j \u2264 \u2212\u2206 + \u03b6\u2217j , \u03b6\u2217j \u2265 0 i = 1 . . . n, j = 1 . . .m\nuntil convergence i.e. kt+1j = k t j \u2200j = 1 . . .m;\nThis Algorithm 2 can be extended to nonlinear case by transforming the problem in its dual form (as shown next).\nRewrite,\nxi = { xi i = 1 . . . n (training samples) x\u2217j i = n+ 1 . . . n+m (universum samples)\nyi = { yi i = 1 . . . n y\u2217j i = n+ 1 . . . n+m\n\u03c1i = { \u03b5 i = 1 . . . n \u2212\u2206 i = n+ 1 . . . n+m (11)\nCi = { C i = 1 . . . n C\u2217 i = n+ 1 . . . n+m\n\u03b4i = { C\u2217 if yi < f(xi); i = n+ 1 . . . n+m 0 else\n\u03b3i = { C\u2217 if yi > f(xi); i = n+ 1 . . . n+m 0 else\nThen, we obtain the following Algorithm 3 in dual form. The proof follows from standard KKT equations and it is omitted in this paper.\nAlgorithm 3: CCCP algorithm for U-SVR in dual form\n1. Initialize (\u03b10, \u03b20, b0) using the standard SVR model (see eq. (7)) ; repeat\n2. At t+ 1 iteration update,\n\u03b4t+1i =  C\u2217 if yi < n+m\u2211 i=1 (\u03b1ti \u2212 \u03b2ti ) + bt\n0 else; i = n+ 1 . . . n+m\n\u03b3t+1i =  C\u2217 if yi > n+m\u2211 i=1 (\u03b1ti \u2212 \u03b2ti ) + bt\n0 else; i = n+ 1 . . . n+m\n3. Solve the following eq. (12) to obtain (\u03b1t+1, \u03b2t+1)\nmin \u03b1,\u03b2\n1\n2 n+m\u2211 i,j=1 (\u03b1i \u2212 \u03b2i)(\u03b1j \u2212 \u03b2j)K(xi \u00b7 xj) (12)\n+ n+m\u2211 i=1 \u03c1i(\u03b1i + \u03b2i)\u2212 n+m\u2211 i=1 yi(\u03b1i \u2212 \u03b2i)\ns.t.\nn+m\u2211 i=1 \u03b1i = n+m\u2211 i=1 \u03b2i; i = 1 . . . n+m\n\u2212 \u03b3t+1i \u2264 \u03b1i \u2264 Ci \u2212 \u03b3 t+1 i ; \u2212\u03b4 t+1 i \u2264 \u03b2i \u2264 Ci \u2212 \u03b4 t+1 i\nuntil convergence i.e. \u03b4t+1i = \u03b4 t i , \u03b3 t+1 i = \u03b3 t i \u2200i = n+ 1 . . . n+m;\nThis CCCP based non-convex minimization may have local optima, so a good initialization and stopping criteria are critical for this algorithm. In our implementation, standard SVR model is used as the initial condition (as shown in Algorithms 2 and 3). Thus the CCCP strategy searches for local minima near the SVR solution. Further, at each iteration we are solving an SVR-like formulation (see eq. (12)). The only difference is in the constraints as shown in eq. (12). That is, the dual variables in U-SVR formulation (12) have different upper and lower range values, as compared to standard SVR formulation (7). The time complexity for solving the U-SVR formulation using CCCP is similar to solving standard SVR\nformulation with (n + m) samples at each iteration. Our preliminary experiments suggests fast convergence (2-5 iterations) for several data sets. Hence, this strategy should be scalable for most real-life datasets.\nThe kernelized version of U-SVR formulation has five tunable parameters: C,C\u2217, kernel parameter, \u03b5 and \u2206. So model selection (parameter tuning) becomes an issue for real-life applications. In this paper, we propose simple twostep strategy for U-SVR model selection:\n1. Model selection for standard SVM regression. This step performs estimation of standard SVM regression model using only training samples. Following [4, 7, 8], we can select C parameter analytically, e.g. by setting C = ymax \u2212 ymin , and then perform tuning of \u03b5 and kernel parameters via resampling or separate validation data set. This step performs model selection for tuning parameters specific only to the training samples in the U-SVR formulation in (8).\n2. Model selection for tuning two Universum-specific parameters. This step performs selection (or tuning) of parameters C\u2217/C and \u2206 specific to the U-SVR formulation, while keeping C, \u03b5 and kernel parameters fixed (as obtained in Step 1). This can be performed using a separate validation set or via resampling."}, {"heading": "3.3 Generating Universum Data", "text": "Generally, Universum contains data samples from the same application domain as available training data. For example, for handwritten digit recognition application one can use examples of handwritten letters as Universum data samples, along with examples of handwritten digits used as training data. Note that Universum samples follow a different distribution than the training data. For most real-life applications, Universum data is often available. However, selection of good Universum requires application-domain knowledge and good engineering. Another strategy is to generate synthetic Universum directly from available labeled training data - which is often used for classification setting [6, 21]. Hence, in this section we introduce certain strategies for generating synthetic Universum data under regression setting. Generating such synthetic Universum samples requires minimal domain knowledge and should be applicable to most real life problems (as discussed next).\nThe notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21]. For binary classification, the Universum data belongs to the same x - space as the training data, but these samples are known not to belong to either of the two classes (\u2018+1\u2019 or \u2018-1\u2019). The most popular approach used for generating synthetic universum is called \u2018random averaging \u2019 or RA, where the Universum samples are generated by randomly selecting one positive and one negative training sample, and then computing their average x- value. Then the x- values of the generated RA Universum samples would follow a different distribution than either of the two classes.\nAs discussed in Section 3.1, Universum samples for regression are labeled, unlike unlabeled Universum samples under classification setting. Hence, under regression setting, the distribution of Universum samples can be different from the distribution of x - values (of the training data) or their y - values or both. All these scenarios contribute to a distribution of (x, y) - values of Universum that is different from\nthe training data distribution. This observation motivates several strategies for generating synthetic universum, as discussed next. Strategy 1: keep the marginal distributions of x and y values fixed, but change the underlying conditional distribution p(y|x). For example, randomly select any two samples from the training data (x1, y1) and (x2, y2), such that y1 \u2265 \u00b5y and y2 \u2264 \u00b5y; where \u00b5y is the mean of the y - values of training samples. Next, permute the samples to create two new universum samples, i.e. (x1, y2) and (x2, y1). Following this strategy, the marginal distributions of x, y - values remain the same; but the conditional distribution changes. Strategy 2: change the distribution of the y - values of training samples. For example, randomly select a training sample (x, y) and then replace its y- value as y\u2032 \u223c N (\u00b5y, \u03c3y) (normal distribution), where \u00b5y and \u03c3y are the mean and standard deviation of y - values of the training data. Strategy 3: change the distribution of the x - values of the training samples. For example, randomly select a training sample (x, y) and randomly permute the input features x\u2192 x\u2032 to create a new universum sample (x\u2032, y). Strategy 4: change the marginal distributions of both (x, y) - values. For example, randomly select a training sample (x, y) and randomly permute the input features x \u2192 x\u2032 as well as replace the y - value of the sample as, y\u2032 \u223c N (\u00b5y, \u03c3y); where \u00b5y and \u03c3y are the mean and standard deviation of the y - values of the training data.\nNote that each of the above strategies modifies the overall distribution of the (x, y) - values of the training data. So these strategies yield universum data having distribution different from the training data. Hence, such Universum data should be falsified by the U-SVR formulation (8). Next, in Section 4, we show empirical performance comparisons for U-SVR using Universum generated by strategies 1 and 2 only. Empirical results using strategies 3 & 4 have been omitted due to space constraints."}, {"heading": "4. EMPIRICAL RESULTS", "text": "This section provides empirical results to illustrate the effectiveness of the proposed U-SVR formulation relative to standard SVR. Most examples use synthetic data sets and linear SVM parameterization, in order to clarify the effect of Universum on the prediction performance of SVR. All experiments follow the two-step experimental modeling strategy (presented in Section 3.2), where optimal standard SVR model is estimated first (using only labeled training data), and then U-SVR model is estimated (using both the training data and Universum data). This experimental strategy simplifies comparisons between standard SVR and U-SVR modeling, and also enables tractable model selection for USVR. Further, all empirical comparisons use separate validation data set for model selection. That is, the regression model is estimated by fitting the training data set, but tuning parameters (for each method) are selected using separate validation data set, and prediction performance (of the final regression model) is estimated using independent test set. For synthetic data sets, training, validation and test samples are generated according to the same (fixed) distribution, where the input x-values are uniformly distributed in d-dimensional input space, and output (response) values represent a target function corrupted by additive noise: y = t(x) + noise, where t(x) is a target function and noise is Gaussian. The performance index is the normalized root\nmean squared error:\nNRMS =\n\u221a MSE\nstd(y) (13)\nwhere, MSE = 1 n \u2211n i=1(y \u2212 y\u0302)\n2, n = no. of samples, and y\u0302 denotes estimated output values. Our empirical results show the NRMS error separately for training and test data sets. Of course, only the test error is meaningful for prediction performance comparisons, and the training error is shown mainly for additional understanding of modeling results. Further, following [11], our empirical results for synthetic data use NRMS error calculated using the true output values t(x)(not corrupted by noise), whereas results for real-life data sets use NRMS errors calculated using noisy y -values.\nFor all comparisons, prediction performance of U-SVR is compared against two common benchmark methods: standard SVM regression and ridge regression. Hence, these comparisons illustrate possible advantages (or limitations) of introducing Universum into regression modeling."}, {"heading": "4.1 Hypercube dataset", "text": "Our first experiment uses a synthetic 30-dimensional hypercube data set, where each input is uniformly distributed in [0, 1] interval. The output is generated as:\ny = x1 + . . .+ x5 \u2212 x6 \u2212 . . .\u2212 x10 + . . . + x21 + . . .+ x25 \u2212 x26 \u2212 . . .\u2212 x30 + \u03b4\nwhere, the noise is Gaussian: \u03b4 \u223c N (0, \u03c3I). For this data set, we use linear SVR parameterization and consider two different types of universum. Universum 1 : input samples follow the same distribution as training samples, e.g., x \u2208 <30 and uniformly distributed in [0, 1]. The output is generated as:\ny = \u2212x1 \u2212 . . .\u2212 x5 + x6 + . . .+ x10 + . . . \u2212 x21 \u2212 . . .\u2212 x25 + x26 + . . .+ x30\nUniversum 2 : following strategy 2 for generating synthetic universum, we randomly select a training sample (x, y) and re-set its y-value as y\u2032 \u223c N (\u00b5y, \u03c3y), where \u00b5y and \u03c3y are the mean and std. deviation of y - values of the training data. The experimental setting is specified below:\n\u2013 No. of training and validation samples = 30, 150 (characterizing low and high sample-size settings, respectively. The number of validation samples is always set to be the same as the number of training samples).\n\u2013 No. of test samples = 5000.\n\u2013 No. of universum samples = 300.\n\u2013 Two additive noise levels \u03c3=0.5 and 2 are considered, in order to capture the effects of low and high noise levels respectively. For the high sample size setting we provide the results for \u03c3=0.5 only. Experiments involving high noise conditions for high sample size settings did not yield any additional improvement when using the U-SVR formulation.\nPerformance results in Tables 1 and 2 show the average NRMS error for training and test data observed over 25 random experiments. Here, for each experiment we randomly\nselect the training/validation/test set. The standard deviation of the NRMS values (over 25 experiments) is shown in parenthesis. All comparisons assume linear parameterization for standard SVM regression, U-SVR and ridge regression. These empirical results indicate that for low-sample settings introducing Universum can indeed improve prediction performance, especially for low-noise settings (as shown in Table 1). However, for high-sample size settings introducing Universum does not yield any improvement relative to standard SVR or ridge regression (as evident from Table 2).\nFor understanding of the U-SVR modeling results we adopt the technique known as \u2018histogram of projections\u2019 originally introduced for SVM classification setting [5, 6]. Under classification setting, the \u2018projection value\u2019 for a given sample measures its distance from SVM decision boundary. For regression, conceptually similar quantity is the residual y \u2212 f(x) that measures the difference between response y and its estimate f(x). So for regression models we use the univariate histogram of residuals or residual values y\u2212 f(x), where f(x) is the trained regression model with optimally tuned parameters. The typical histogram of residuals of training data for trained SVR model is shown in Fig. 5. In addition, Fig. 5 also shows projections of the residual values (y\u2217\u2212f(x\u2217)) of universum samples (shown in black). Similar to methodology developed for U-SVM classification, visual interpretation of the histograms of residuals for training data and Universum data (such as shown in Fig. 5) can be used for understanding the effectiveness of Universum under regression setting. In particular, Fig. 5 shows the effect of data piling or clustering of residual values for training samples\nnear the boundaries of \u03b5-tube, which is similar to data piling at the margin borders for SVM classification [5,6]. This effect of data piling is typically observed for \u2018small-sample\u2019 regression data sets corresponding to very ill-posed estimation problems [2]. For such ill-posed settings, introducing additional constraints (in the form of Universum data) usually improves the quality of estimated models. For example, assuming the distribution of residuals for Universum samples (relative to standard SVR model) is as shown in Fig. 5, application of U-SVR is expected to modify/improve the original SVR model by pushing the Universum samples further away from a regression model, according to the Universum loss function in Fig. 3.\nNext, we present actual typical histograms for residuals of training samples and Universum samples, for several representative data sets under small-sample size setting, e.g. 30 training samples. Figs. 6 and 7 show the histograms for low noise level, and Figs. 8 and 9 show the histograms for high noise level in the data. For example, Fig. 6a shows the histogram of residuals for optimally trained SVR model (in blue) and the histogram of residuals for Universum (in black). This histogram for training data clearly shows the effect of data piling for training samples, and also shows that the distribution for Universum 1 data is unimodal and centered around the SVR model (marked as the point \u20180\u2019 on x-axis in Fig. 6a). Therefore, we can expect that introducing Universum 1 will change/improve the regression model.\nThis is confirmed by analyzing the histograms of residuals for the trained U-SVR shown in Fig. 6b, which shows the effect of pushing Universum 1 samples away from the estimated regression model (marked as the point \u20180\u2019 on x-axis in Fig. 6b). Specifically, for the SVR model (in Fig. 6a), the fraction of universum samples lying within the \u00b1\u2206 zone is \u223c 92% and that for the U-SVR model (in Fig. 6b) this fraction is \u223c 85%. Hence, the U-SVR model (in Fig. 6b) increases the contradiction for the universum samples and improves the prediction performance (relative to standard SVR), which is consistent with results in Table 1. Similar reasoning applies to Fig. 7 showing the histograms for the same training data but using type 2 Universum. In this case, however, the fraction of Universum samples lying within the \u00b1\u2206 zone for SVR model (in Fig. 7a) is \u223c 90% and that for the U-SVR model (in Fig. 7b) is \u223c 90%. So we can expect little or no improvement in prediction performance (relative to SVR), which is confirmed by results in Table 1.\nSimilarly, we can analyze histograms of residuals for the low sample size, high noise level data shown in Figs. 8 and 9. In this case, the data piling effect for standard SVR model is less strong (as compared with low-noise level data in Figs. 6 and 7). Further, visual comparison of the histograms for Universum data for standard SVR and U-SVR suggests no significant change in the fraction of Universum samples within the \u00b1\u2206 zone. Hence, we can expect only minor or no improvement in the prediction performance for U-SVR\n(relative to standard SVR), which is confirmed by results in Table 1.\nFinally, consider large sample size, low noise training data set (n = 150, \u03c3 = 0.5). Having large number of training samples yields very accurate SVR model estimation. For such data sets, we do not observe the data piling effect at the \u00b1\u03b5 values (see Figs. 10 & 11), so we expect no improvement from introducing Universum data (as evident from results shown in Table 2).\nAs evident from experiments above, U-SVR is particularly effective for very sparse settings (\u223c high-dimensional and low sample size) under low noise conditions. Under such settings, the training data exhibits large data-piling effect near the \u00b1\u03b5 margins for the SVR model, and introducing the Universum usually helps to improve the prediction performance. With increased noise level (in the data), the data-piling effect becomes less prominent and the USVR yields no notable improvement over the SVR solution. Finally, when the number of training samples is large, the estimation problem becomes well-posed and standard SVR model does not exhibit any data-piling effect at the margins. In this case, application of U-SVR does not provide any improvement over standard SVR.\nThe next experiment follows the same experimental set up for 30-dimensional hypercube data set under low-sample size setting as above, except that the additive noise level is set to zero (\u03c3= 0). Based on our previous discussion, this data set\nis expected to show the most significant improvement in prediction performance of U-SVR (relative to standard SVR). Table 3 shows performance comparisons, suggesting that USVR (using Universum 1) indeed provides very significant improvement over standard SVR solution.\nFinally, the last set of experiments demonstrates how the generalization performance of U-SVR is affected by the number of Universum data samples. Let us consider the same synthetic hypercube data set under small sample size, low noise setting, where the size of Universum data set varies as m = 50, 100, 300, 500. Table 4 shows performance comparisons between SVR vs. U-SVR, suggesting that:\n\u2013 for Universum 1 : prediction performance of U-SVR improves with the number universum samples. However, increasing the number of universum samples above certain value (\u223c300) does not provide additional improvement.\n\u2013 for Univerum 2 : increasing the number of universum samples does not provide any improvement.\nThese results indicate that for sufficiently large number of universum samples the effectiveness of U-SVR depends mostly on the type (\u223cstatistical characteristics) of the universa. Similar to classification settings [6, 17], the effectiveness of Universum for regression problems depends on the statistical characteristics of both the training data and the Universum data. Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]). Such practical conditions for the effectiveness of U-SVR are open for future research."}, {"heading": "4.2 Computer Hardware Dataset", "text": "Our next experiment uses the publicly available real-life Computer Hardware dataset [12]. The goal is to predict the published relative CPU performance using several other CPU properties. As preprocessing, the categorical variable vendor name has been transformed to a binary representation. Further the y- values have been scaled as log(1 + y) (see [12]). In this experiment, we use two types of synthetic\nuniversum generated directly from the training data, following Strategy 1 (Universum 1) and Strategy 2 (Universum 2) as described in Section 3.3. The experimental set up is summarized next:\n\u2013 No. of training samples = 50.\n\u2013 No. of validation samples = 50. (This independent validation samples is used for model selection).\n\u2013 No. of test samples =109.\n\u2013 No. of universum samples = 100. (Increasing the number of universum samples does not provide additional improvement)\n\u2013 No. of input variables = 36.\nAs a part of pre-processing, the x- values of the training data have been pre-scaled uniformly to the same range [\u22121, 1]. Such a pre-scaling of inputs (to the same range) is typically required for SVM modeling. Model selection for SVR and U-SVR is performed over the range of parameters, C = ymax \u2212 ymin, \u03b5 = [0, 2\u22121, . . . , 23], C\u2217/C = [2\u22124, . . . , 24] and \u2206 = [2\u22124, . . . , 24] and follows the two-step strategy presented in Section 3. Prediction performance results are calculated for 25 random partitionings of the data into training, validation and test data sets. Table 5 provides the average NRMS and MSE errors for training and test data sets (averaged over 25 random partitioning of the data) and the standard deviation (shown in parenthesis). For most experiments, the typical optimal SVR model parameters are C \u223c 4.5, \u03b5 = 1. Likewise, typical optimal parameters for U-SVR when using Universum 1 are C\u2217/C = 2\u22122\u221220,\u2206 = 0.25; and when using Universum 2 are C\u2217/C = 2\u22126,\u2206 = 0.5. For this dataset, preliminary experiments showed significant datapiling effects for the SVR model. Hence, we can expect improved generalization for the U-SVR model. As evident from Table 5, U-SVR (using Universum 1) provides improved generalization over the standard SVR, whereas Universum 2 yields only minor improvement. For this data set, the ridge regression provided performance similar to standard SVR and hence it has not been reported."}, {"heading": "4.3 Vilmann\u2019s Rat Dataset", "text": "This data set illustrates the effectiveness of U-SVR under non-linear SVR parameterization. The real-life Vilmann\u2019s Rat dataset contains the skull X-ray images of 21 different rats, represented using 8 - landmarks for 2-dimensions [1]. The skull X-ray images are available for each rat at ages 7, 14, 21, 30, 40, 60, 90 and 150 days. The task is to predict the ontogenetic development (age) of a rat using the X-ray images. The dataset contains 4 missing data which has been"}, {"heading": "Test Data", "text": ""}, {"heading": "Training Data", "text": ""}, {"heading": "Test Data", "text": ""}, {"heading": "Training Data", "text": "removed from the analysis. The processed dataset contains 164 samples. In this experiment, we use two types of synthetic universum generated directly from the training data, following Strategy 1(Universum 1) and Strategy 2(Universum 2) as described in Section 3.3. The experimental set up is summarized next:\n\u2013 No. of training samples \u223c skull x-ray images of 5 different rats. (= 40 images)\n\u2013 No. of validation samples \u223c skull x-ray images of 5 different rats (= 40 images. This independent validation data set is used for model selection).\n\u2013 No. of test samples \u223c skull x-ray images of remaining 11 rats. (= 88 images)\n\u2013 No. of universum samples = 200 (increasing the number universum samples does not provide additional improvement).\n\u2013 Dimensionality of each sample = 16. (\u223c8 landmarks per 2D)\nFor this data set, using RBF kernel of the formK(xi,xj) = exp(\u2212\u03b3\u2016xi\u2212xj\u20162) provided optimal results for SVR modeling. The model selection is done over the range of parameters, C = ymax\u2212ymin, \u03b3 = [2\u22126, . . . , 20], \u03b5 = [0, 2\u22124, . . . , 26], C\u2217/C = [0, 2\u22127, . . . , 21] and \u2206 = [2\u22124, . . . , 24] . Typical optimal parameters for SVR selected through model selection are C = 143, \u03b3 = 2\u22123 \u2212 2\u22122, \u03b5 = 0, 4, 8 (observed high variance). Optimal tuning parameters specific to U-SVR when using Universum 1 are: C\u2217/C \u223c 2\u22123 \u2212 2\u22121, \u2206 = 25 \u2212 26;\nand when using Universum 2 are: C\u2217/C \u223c 0.5, \u2206 = 25\u221226. Table 6 provides the average NRMS and MSE for training and test data sets (calculated over 25 random partitionings of the data). The standard deviation is provided in parenthesis. Preliminary experiments showed data-piling effects for the SVR model. Hence, we can expect improved generalization for the U-SVR model. This is confirmed by results presented in Table 6, where U-SVR (using Universum 2) provides improved prediction."}, {"heading": "5. CONCLUSIONS", "text": "This paper extends the idea of universum learning to regression problems and provides new optimization formulation called Universum Support Vector Regression (U-SVR). This U-SVR formulation is non-convex and cannot be solved using standard convex solvers typically adopted for existing SVM software packages. This paper adopts the method of Con-Cave Convex Problem (CCCP) and provides a new algorithm (Algorithm 1 & 2) for solving the proposed U-SVR formulation. Following this strategy, the current U-SVR formulation can be solved by several iterations of standard SVM-like optimization problem.\nMoreover, the proposed U-SVR formulation has 5 tunable parameters: C, C\u2217, \u03b5, \u2206 and kernel parameter. Hence a successful practical application of U-SVR depends on the optimal selection of these model parameters. We propose simple two-step strategy for model selection where optimal model parameters for standard SVR are estimated first, and then model selection for U-SVR involves tuning only two remaining parameters C\u2217/C and \u2206. Such a two-step strategy significantly simplifies model selection for U-SVR.\nFinally, the paper provides empirical results to show the effectiveness of the proposed U-SVR over standard SVR. Additional results showing the effectiveness of U-SVR are available for several real-life datasets, and have been omitted for space constraints. Our results suggest that U-SVR is particularly effective for high-dimension low (training) sample size settings. Under such settings, the SVR model exhibits significant data piling of the training samples near the \u00b1\u03b5 margin. For such ill-posed settings, introducing the Universum can provide improved generalization over the standard SVR solution. However, the effectiveness of U-SVR depends both on the statistical characteristics of both the training data and Universum data. These statistical characteristics can be conveniently captured using the \u2018histogram-of-residuals\u2019 method introduced in this paper."}, {"heading": "6. REFERENCES", "text": "[1] F. L. Bookstein. Morphometric tools for landmark\ndata: geometry and biology. Cambridge University Press, 1997.\n[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004.\n[3] S. Chen and C. Zhang. Selecting informative universum sample for semi-supervised learning. In IJCAI, pages 1016\u20131021, 2009.\n[4] V. Cherkassky. Predictive Learning. VCtextbook, 2013.\n[5] V. Cherkassky and S. Dhar. Simple method for interpretation of high-dimensional nonlinear svm classification models. In R. Stahlbock, S. F. Crone, M. Abou-Nasr, H. R. Arabnia, N. Kourentzes,\nP. Lenca, W.-M. Lippe, and G. M. Weiss, editors, DMIN, pages 267\u2013272. CSREA Press, 2010.\n[6] V. Cherkassky, S. Dhar, and W. Dai. Practical conditions for effectiveness of the universum learning. Neural Networks, IEEE Transactions on, 22(8):1241\u20131255, 2011.\n[7] V. Cherkassky and Y. Ma. Practical selection of svm parameters and noise estimation for svm regression. Neural networks, 17(1):113\u2013126, 2004.\n[8] V. Cherkassky and F. M. Mulier. Learning from Data: Concepts, Theory, and Methods. Wiley-IEEE Press, 2007.\n[9] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Large scale transductive svms. The Journal of Machine Learning Research, 7:1687\u20131712, 2006.\n[10] S. Dhar and V. Cherkassky. Development and evaluation of cost-sensitive universum-svm. Cybernetics, IEEE Transactions on, 45(4):806\u2013818, 2015.\n[11] H. Drucker, Chris, B. L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines. In Advances in Neural Information Processing Systems 9, volume 9, pages 155\u2013161, 1997.\n[12] M. Lichman. Uci machine learning repository. http://archive.ics.uci.edu/ml. Accessed: 2016-02-05.\n[13] S. Lu and L. Tong. Weighted twin support vector machine with universum. Advances in Computer Science: an International Journal, 3(2):17\u201323, 2014.\n[14] Z. Qi, Y. Tian, and Y. Shi. A nonparallel support vector machine for a classification problem with universum learning. Journal of Computational and Applied Mathematics, 263:288\u2013298, 2014. [15] C. Shen, P. Wang, F. Shen, and H. Wang. {cal U} boost: Boosting with the universum. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(4):825\u2013832, 2012.\n[16] X. Shen, G. C. Tseng, X. Zhang, and W. H. Wong. On psi-Learning. Journal of the American Statistical Association, 98:724\u2013734, Jan. 2003.\n[17] F. Sinz, O. Chapelle, A. Agarwal, and B. Scho\u0308lkopf. An analysis of inference with the universum. In Advances in neural information processing systems 20, pages 1369\u20131376, Red Hook, NY, USA, Sept. 2008. Max-Planck-Gesellschaft, Curran.\n[18] V. Vapnik. Estimation of Dependences Based on Empirical Data (Information Science and Statistics). Springer, Mar. 2006.\n[19] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.\n[20] Z. Wang, Y. Zhu, W. Liu, Z. Chen, and D. Gao. Multi-view learning with universum. Knowledge-Based Systems, 70:376\u2013391, 2014.\n[21] J. Weston, R. Collobert, F. Sinz, L. Bottou, and V. Vapnik. Inference with the universum. In Proceedings of the 23rd international conference on Machine learning, pages 1009\u20131016. ACM, 2006.\n[22] D. Zhang, J. Wang, F. W. 0001, and C. Zhang. Semi-supervised classification with universum. In SDM, pages 323\u2013333. SIAM, 2008."}], "references": [{"title": "Morphometric tools for landmark data: geometry and biology", "author": ["F.L. Bookstein"], "venue": "Cambridge University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Selecting informative universum sample for semi-supervised learning", "author": ["S. Chen", "C. Zhang"], "venue": "IJCAI, pages 1016\u20131021", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive Learning", "author": ["V. Cherkassky"], "venue": "VCtextbook", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Simple method for interpretation of high-dimensional nonlinear svm classification models", "author": ["V. Cherkassky", "S. Dhar"], "venue": "R. Stahlbock, S. F. Crone, M. Abou-Nasr, H. R. Arabnia, N. Kourentzes,  P. Lenca, W.-M. Lippe, and G. M. Weiss, editors, DMIN, pages 267\u2013272. CSREA Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Practical conditions for effectiveness of the universum learning", "author": ["V. Cherkassky", "S. Dhar", "W. Dai"], "venue": "Neural Networks, IEEE Transactions on, 22(8):1241\u20131255", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical selection of svm parameters and noise estimation for svm regression", "author": ["V. Cherkassky", "Y. Ma"], "venue": "Neural networks, 17(1):113\u2013126", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning from Data: Concepts", "author": ["V. Cherkassky", "F.M. Mulier"], "venue": "Theory, and Methods. Wiley-IEEE Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Large scale transductive svms", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "The Journal of Machine Learning Research, 7:1687\u20131712", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Development and evaluation of cost-sensitive universum-svm", "author": ["S. Dhar", "V. Cherkassky"], "venue": "Cybernetics, IEEE Transactions on, 45(4):806\u2013818", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Chris", "author": ["H. Drucker"], "venue": "B. L. Kaufman, A. Smola, and V. Vapnik. Support vector regression machines. In Advances in Neural Information Processing Systems 9, volume 9, pages 155\u2013161", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Uci machine learning repository. http://archive.ics.uci.edu/ml. Accessed: 2016-02-05", "author": ["M. Lichman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Weighted twin support vector machine with universum", "author": ["S. Lu", "L. Tong"], "venue": "Advances in Computer Science: an International Journal, 3(2):17\u201323", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparallel support vector machine for a classification problem with universum learning", "author": ["Z. Qi", "Y. Tian", "Y. Shi"], "venue": "Journal of Computational and Applied Mathematics, 263:288\u2013298", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "cal U} boost: Boosting with the universum", "author": ["C. Shen", "P. Wang", "F. Shen", "H. Wang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(4):825\u2013832", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "On psi-Learning", "author": ["X. Shen", "G.C. Tseng", "X. Zhang", "W.H. Wong"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "An analysis of inference with the universum. In Advances in neural information processing systems 20, pages 1369\u20131376", "author": ["F. Sinz", "O. Chapelle", "A. Agarwal", "B. Sch\u00f6lkopf"], "venue": "Red Hook, NY,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Estimation of Dependences Based on Empirical Data (Information Science and Statistics)", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Wiley-Interscience", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Multi-view learning with universum", "author": ["Z. Wang", "Y. Zhu", "W. Liu", "Z. Chen", "D. Gao"], "venue": "Knowledge-Based Systems, 70:376\u2013391", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Inference with the universum", "author": ["J. Weston", "R. Collobert", "F. Sinz", "L. Bottou", "V. Vapnik"], "venue": "Proceedings of the 23rd international conference on Machine learning, pages 1009\u20131016. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "F", "author": ["D. Zhang", "J. Wang"], "venue": "W. 0001, and C. Zhang. Semi-supervised classification with universum. In SDM, pages 323\u2013333. SIAM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "This paper extends the idea of Universum learning [18, 19] to regression problems.", "startOffset": 50, "endOffset": 58}, {"referenceID": 18, "context": "This paper extends the idea of Universum learning [18, 19] to regression problems.", "startOffset": 50, "endOffset": 58}, {"referenceID": 17, "context": "The technique of Universum learning or learning through contradiction [18,19] provides a formal mechanism for incorporating a priori knowledge about the application domain, in the form of additional (unlabeled) Universum samples.", "startOffset": 70, "endOffset": 77}, {"referenceID": 18, "context": "The technique of Universum learning or learning through contradiction [18,19] provides a formal mechanism for incorporating a priori knowledge about the application domain, in the form of additional (unlabeled) Universum samples.", "startOffset": 70, "endOffset": 77}, {"referenceID": 17, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 85, "endOffset": 93}, {"referenceID": 20, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 85, "endOffset": 93}, {"referenceID": 5, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 196, "endOffset": 205}, {"referenceID": 16, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 196, "endOffset": 205}, {"referenceID": 20, "context": "Universum learning has been originally introduced for binary classification problems [18, 21] and it has been shown to be particularly effective for high-dimensional low-sample size data settings [6,17,21].", "startOffset": 196, "endOffset": 205}, {"referenceID": 2, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 9, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 12, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 13, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 14, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 19, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 21, "context": "More recently, Universum learning has been extended to various non-standard classification settings [3, 10, 13\u201315, 20, 22].", "startOffset": 100, "endOffset": 122}, {"referenceID": 3, "context": "edu/handle/ 11299/162636 Besides classification setting, another common supervised learning problem is regression or real-valued function estimation from noisy samples [4, 8].", "startOffset": 168, "endOffset": 174}, {"referenceID": 7, "context": "edu/handle/ 11299/162636 Besides classification setting, another common supervised learning problem is regression or real-valued function estimation from noisy samples [4, 8].", "startOffset": 168, "endOffset": 174}, {"referenceID": 17, "context": "This section provides brief description of standard SVM regression (SVR) formulation following Vapnik [18,19].", "startOffset": 102, "endOffset": 109}, {"referenceID": 18, "context": "This section provides brief description of standard SVM regression (SVR) formulation following Vapnik [18,19].", "startOffset": 102, "endOffset": 109}, {"referenceID": 3, "context": "For most SVR solvers, problem (6) is usually solved in its dual form (see [4] for details):", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "For more details see [4, 11].", "startOffset": 21, "endOffset": 28}, {"referenceID": 10, "context": "For more details see [4, 11].", "startOffset": 21, "endOffset": 28}, {"referenceID": 8, "context": "Recently, similar non-convex optimization problems have been addressed in [9,16] using the ConCave Convex Programming (CCCP) strategy.", "startOffset": 74, "endOffset": 80}, {"referenceID": 15, "context": "Recently, similar non-convex optimization problems have been addressed in [9,16] using the ConCave Convex Programming (CCCP) strategy.", "startOffset": 74, "endOffset": 80}, {"referenceID": 3, "context": "Following [4, 7, 8], we can select C parameter analytically, e.", "startOffset": 10, "endOffset": 19}, {"referenceID": 6, "context": "Following [4, 7, 8], we can select C parameter analytically, e.", "startOffset": 10, "endOffset": 19}, {"referenceID": 7, "context": "Following [4, 7, 8], we can select C parameter analytically, e.", "startOffset": 10, "endOffset": 19}, {"referenceID": 5, "context": "Another strategy is to generate synthetic Universum directly from available labeled training data - which is often used for classification setting [6, 21].", "startOffset": 147, "endOffset": 154}, {"referenceID": 20, "context": "Another strategy is to generate synthetic Universum directly from available labeled training data - which is often used for classification setting [6, 21].", "startOffset": 147, "endOffset": 154}, {"referenceID": 2, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 5, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 16, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 20, "context": "The notion of generating synthetic universum samples has already been used for binary classification problems [3,6,17, 21].", "startOffset": 110, "endOffset": 122}, {"referenceID": 10, "context": "Further, following [11], our empirical results for synthetic data use NRMS error calculated using the true output values t(x)(not corrupted by noise), whereas results for real-life data sets use NRMS errors calculated using noisy y -values.", "startOffset": 19, "endOffset": 23}, {"referenceID": 0, "context": "Our first experiment uses a synthetic 30-dimensional hypercube data set, where each input is uniformly distributed in [0, 1] interval.", "startOffset": 118, "endOffset": 124}, {"referenceID": 0, "context": ", x \u2208 < and uniformly distributed in [0, 1].", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "For understanding of the U-SVR modeling results we adopt the technique known as \u2018histogram of projections\u2019 originally introduced for SVM classification setting [5, 6].", "startOffset": 160, "endOffset": 166}, {"referenceID": 5, "context": "For understanding of the U-SVR modeling results we adopt the technique known as \u2018histogram of projections\u2019 originally introduced for SVM classification setting [5, 6].", "startOffset": 160, "endOffset": 166}, {"referenceID": 4, "context": "near the boundaries of \u03b5-tube, which is similar to data piling at the margin borders for SVM classification [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 5, "context": "near the boundaries of \u03b5-tube, which is similar to data piling at the margin borders for SVM classification [5,6].", "startOffset": 108, "endOffset": 113}, {"referenceID": 1, "context": "This effect of data piling is typically observed for \u2018small-sample\u2019 regression data sets corresponding to very ill-posed estimation problems [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "Similar to classification settings [6, 17], the effectiveness of Universum for regression problems depends on the statistical characteristics of both the training data and the Universum data.", "startOffset": 35, "endOffset": 42}, {"referenceID": 16, "context": "Similar to classification settings [6, 17], the effectiveness of Universum for regression problems depends on the statistical characteristics of both the training data and the Universum data.", "startOffset": 35, "endOffset": 42}, {"referenceID": 2, "context": "Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]).", "startOffset": 157, "endOffset": 165}, {"referenceID": 5, "context": "Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]).", "startOffset": 157, "endOffset": 165}, {"referenceID": 16, "context": "Hence, there is a need for additional research on the characterization of \u2018good \u2019 universum data sets (similar to practical conditions for classification in [3,6,17]).", "startOffset": 157, "endOffset": 165}, {"referenceID": 11, "context": "Our next experiment uses the publicly available real-life Computer Hardware dataset [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Further the y- values have been scaled as log(1 + y) (see [12]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "The real-life Vilmann\u2019s Rat dataset contains the skull X-ray images of 21 different rats, represented using 8 - landmarks for 2-dimensions [1].", "startOffset": 139, "endOffset": 142}], "year": 2016, "abstractText": "This paper extends the idea of Universum learning [18, 19] to regression problems. We propose new Universum-SVM formulation for regression problems that incorporates a priori knowledge in the form of additional data samples. These additional data samples or Universum belong to the same application domain as the training samples, but they follow a different distribution. Several empirical comparisons are presented to illustrate the utility of the proposed approach. CCS Concepts \u2022Theory of computation\u2192 Support vector machines;", "creator": "LaTeX with hyperref package"}}}