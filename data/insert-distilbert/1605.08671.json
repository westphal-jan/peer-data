{"id": "1605.08671", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2016", "title": "An optimal algorithm for the Thresholding Bandit Problem", "abstract": "we study a specific \\ textit { combinatorial pure exploration stochastic bandit problem } where the learner aims at finding the necessary set of arms whose means are feasible above a given threshold, scaled up to a specially given precision, and \\ textit { for a fixed limit time horizon }. tomorrow we hence propose a parameter - free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper bound and lower bounds. to the best of our knowledge, this option is the first non - trivial - pure exploration setting with \\ textit { fixed budget } for which optimal strategies are constructed.", "histories": [["v1", "Fri, 27 May 2016 14:35:29 GMT  (80kb,D)", "http://arxiv.org/abs/1605.08671v1", "ICML 2016"]], "COMMENTS": "ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["andrea locatelli", "maurilio gutzeit", "alexandra carpentier"], "accepted": true, "id": "1605.08671"}, "pdf": {"name": "1605.08671.pdf", "metadata": {"source": "META", "title": "An optimal algorithm for the Thresholding Bandit Problem", "authors": ["Andrea Locatelli", "Maurilio Gutzeit", "Alexandra Carpentier"], "emails": ["ANDREA.LOCATELLI@UNI-POTSDAM.DE", "MGUTZEIT@UNI-POTSDAM.DE", "CARPENTIER@UNI-POTSDAM.DE"], "sections": [{"heading": "1. Introduction", "text": "In this paper we study a specific combinatorial, pure exploration, stochastic bandit setting. More precisely, consider a stochastic bandit setting where each arm has mean \u00b5k. The learner can sample sequentially T > 0 samples from the arms and aims at finding as efficiently as possible the set of arms whose means are larger than a threshold \u03c4 \u2208 R. In this paper, we refer to this setting as the Thresholding Bandit Problem (TBP), which is a specific instance of the combinatorial pure exploration bandit setting introduced in (Chen et al., 2014). A simpler \u201done armed\u201d version of this problem is known as the SIGN-\u03be problem, see (Chen & Li, 2015).\nThis problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al., 2002; Mannor & Tsitsiklis, 2004; Bubeck et al., 2009; Audibert & Bubeck,\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\n2010; Gabillon et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015). To formulate this link with a simple metaphor, the TopM problem is a \u201dcontest\u201d and the TBP problem is an \u201dexam\u201d: in the former, the learner wants to select the M arms with highest mean, in the latter the learner wants to select the arms whose means are higher than a certain threshold. We believe that this distinction is important and that in many applications the TBP problem is more relevant than the TopM, as in many domains one has a natural \u201defficiency\u201d, or \u201dcorrectness\u201d threshold above which one wants to use an option. For instance in industrial applications, one wants to keep a machine if its production\u2019s value is above its functioning costs, in crowd-sourcing one wants to hire a worker as long as its productivity is higher than its wage, etc. In addition to these applications derived from the TopM problem, the TBP problem has applications in dueling bandits and is a natural way to cast the problem of active and discrete level set detection, which is in turn related to the important applications of active classification, and active anomaly detection - we detail this point more in Subsection 3.1.\nAs mentioned previously, the TBP problem is a specific instance of the combinatorial pure exploration bandit framework introduced in (Chen et al., 2014). Without going into the details of the combinatorial pure exploration setting for which the paper (Chen et al., 2014) derives interesting general results, we will summarize what these results imply for the particular TBP and TopM problems, which are specific cases of the combinatorial pure exploration setting. As it is often the case for pure exploration problems, the paper (Chen et al., 2014) distinguishes between two settings:\n\u2022 The fixed budget setting where the learner aims, given a fixed budget T , at returning the set of arms that are above the threshold (in the case of TBP) or the set of M best arms (in the case of TopM), with highest possible probability. In this setting, upper and lower bounds are on the probability of making an error when returning the set of arms.\nar X\niv :1\n60 5.\n08 67\n1v 1\n[ st\nat .M\nL ]\n2 7\nM ay\n2 01\n\u2022 The fixed confidence setting where the learner aims, given a probability \u03b4 of acceptable error, at returning the set of arms that are above the threshold (in the case of TBP) or the set of M best arms (in the case of TopM) with as few pulls of the arms as possible. In this setting, upper and lower bounds are on the number of pulls T that are necessary to return the correct set of arm with probability at least 1\u2212 \u03b4.\nThe similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014). While as explained in (Audibert & Bubeck, 2010; Gabillon et al., 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015). In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting. In this case, without the knowledge of additional information on the problem such as e.g. the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015). This knowledge gap is more acute for the general combinatorial exploration bandit problem defined in the paper (Chen et al., 2014) (see their Theorem 3) - and therefore for the TBP problem (where in fact no fixed budget lower bound exists to the best of our knowledge). We summarize in Table 1 the state of the art results for the TBP problem and for the TopM problem with M = 1.\nThe summary of Table 1 highlights that in the fixed budget setting, both for the TopM and the TBP problem, the correct complexity H\u2217 that should appear in the bound, i.e. what is the problem dependent quantity H\u2217 such that the upper and lower bounds on the probability of error is of order exp(\u2212n/H\u2217), is still an open question. In the TopM problem, Table 1 implies that H \u2264 H\u2217 \u2264 log(2K)H2. In the TBP problem, Table 1 implies 0 \u2264 H\u2217 \u2264 log(2K)H2, since to the best of our knowledge a lower bound for this problem exists only in the case of the fixed confidence setting. Note that although this gap may appear small in particular in the case of the TopM problem as it involves \u201donly\u201d a log(K) multiplicative factor, it is far from being negligible since the log(K) gap factor acts on a term of order exponential minus T exponentially.\nIn this paper we close, up to constants, the gap in the fixed budget setting for the TBP problem - we prove that H\u2217 = H . In addition, we also prove that our strategy minimizes at the same time the cumulative regret, and identifies optimally the best arm, provided that the highest mean of the arms is known to the learner. Our findings are summarized in Table 1. In order to do that, we introduce a new algorithm for the TBP problem which is entirely parameter free, and based on an original heuristic. In Section 2, we describe formally the TBP problem, the algorithm, and the results. In Section 3, we describe how our algorithm can be applied to the active detection of discrete level sets, and therefore to the problem of active classification and active anomaly detection. We also describe what are the implications of our results for the TopM problem. Finally Section 4 presents some simulations for evaluating our algorithm with respect to the state of the art competitors. The proofs of all theorems are in Appendix A, as well as additional simulation results."}, {"heading": "2. The Thresholding Bandit Problem", "text": ""}, {"heading": "2.1. Problem formulation", "text": "Learning setting Let K be the number of arms that the learner can choose from. Each of these arms is characterized by a distribution \u03bdk that we assume to be R-subGaussian. Definition (R-sub-Gaussian distribution). Let R > 0. A distribution \u03bd is R-sub-Gaussian if for all t \u2208 R we have\nEX\u223c\u03bd [exp(tX \u2212 tE[X])] \u2264 exp(R2t2/2). This encompasses various distributions such as bounded distributions or Gaussian distributions of variance R2 for R \u2208 R. Such distributions have a finite mean, let \u00b5k = EX\u223c\u03bdk [X] be the mean of arm k.\nWe consider the following dynamic game setting which is common in the bandit literature. For any time t \u2265 1, the learner chooses an arm It from A = {1, ...,K}. It receives a noisy reward drawn from the distribution \u03bdIt associated to the chosen arm. An adaptive learner bases its decision at time t on the samples observed in the past.\nSet notations Let u \u2208 R and A be the finite set of arms. We define Su as the set of arms whose means are over u, that is Su := {k \u2208 A, \u00b5k \u2265 u}. We also define SCu as the complimentary set of Su in A, i.e. SCu = {k \u2208 A, \u00b5k < u}.\nObjective Let T > 0 (not necessarily known to the learner beforehand) be the horizon of the game, let \u03c4 \u2208 R be the threshold and \u2265 0 be the precision. We define the (\u03c4, ) thresholding problem as such : after T rounds of the game described above, the goal of the learner is to correctly identify the arms whose means are over or under the threshold \u03c4 up to a certain precision , i.e. to correctly discriminate arms that belong to S\u03c4+ from those in SC\u03c4\u2212 . In the rest of the paper, the sentence \u201dthe arm is over the threshold \u03c4\u201d is to be understood as \u201dthe arm\u2019s mean is over the threshold\u201d.\nAfter T rounds of the previously defined game, the learner has to output a set S\u0302\u03c4 := S\u0302\u03c4 (T ) \u2282 A of arms and it suffers the following loss:\nL(T ) = I(S\u03c4+ \u2229 S\u0302C\u03c4 6= \u2205 \u2228 SC\u03c4\u2212 \u2229 S\u0302\u03c4 6= \u2205).\nA good learner minimizes this loss by correctly discriminating arms that are outside of a 2 band around the threshold: arms whose means are smaller than (\u03c4 \u2212 ) should not belong to the output set S\u0302\u03c4 , and symmetrically those whose means are bigger than (\u03c4 + ) should not belong to S\u0302C\u03c4 . If it manages to do so, the algorithm suffers no loss and otherwise it incurs a loss of 1. For arms that lie inside this 2 strip, mistakes on the other hand bear no cost. If we set to 0 we recover the exact TBP thresholding problem described in the introduction, and the algorithm suffers no loss if it discriminates exactly arms that are over the threshold from those under.\nLet E be the expectation according to the samples collected by an algorithm, its expected loss is:\nE[L(T )] = P(S\u03c4+ \u2229 S\u0302C\u03c4 6= \u2205 \u2228 SC\u03c4\u2212 \u2229 S\u0302\u03c4 6= \u2205),\ni.e. it is the probability of making a mistake, that is rejecting an arm over (\u03c4 + ) or accepting an arm under (\u03c4 \u2212 ). The lower this probability of error, the better the algorithm, as an oracle strategy would simply rightly classify each arm and suffer an expected loss of 0.\nOur problem is a pure exploration bandit problem, and is in fact, shifting the means by \u2212\u03c4 , a specific case of the pure exploration bandit problem considered in (Chen et al., 2014) - namely the specific case where the set of sets of arms that they callM and which is their decision class is the set of all possible set of arms. We will comment more on this later in Subsection 2.4.\nProblem complexity We define \u2206\u03c4, i the gap of arm i with respect to \u03c4 and as:\n\u2206i := \u2206 \u03c4, i = |\u00b5i \u2212 \u03c4 |+ . (1)\nWe also define the complexity H of the problem as\nH := H\u03c4, = K\u2211 i=1 (\u2206\u03c4, i ) \u22122. (2)\nWe callH complexity as it is a characterization of the hardness of the problem. A similar quantity was introduce for general combinatorial bandit problems (Chen et al., 2014) and is similar in essence to the complexity introduced for the best arm identification problem, see (Audibert & Bubeck, 2010)."}, {"heading": "2.2. A lower bound", "text": "In this section, we exhibit a lower bound for the thresholding problem. More precisely, for any sequence of gaps (dk)k, we define a finite set of problems where the distributions of the arms of these problems correspond to these gaps and are Gaussian of variance 1. We lower bound the largest probability of error among these problems, for the best possible algorithm. Theorem 1. Let K,T \u2265 0. Let for any i \u2264 K, di \u2265 0. Let \u03c4 \u2208 R, > 0.\nFor 0 \u2264 i \u2264 K, we write Bi for the problem where the distribution of arm j \u2208 {1, . . . ,K} is N (\u03c4 + di + , 1) if i 6= j and N (\u03c4 \u2212 di \u2212 , 1) otherwise. For all these problems, H := H\u03c4, = \u2211 i(di + 2 )\n\u22122 is the same by definition."}, {"heading": "It holds that for any bandit algorithm", "text": "max\ni\u2208{0,...,K} EBi(L(T )) \u2265 exp\n( \u2212 3T/H\u2212\n4 log(12(log(T ) + 1)K) ) ,\nwhere EBi is the expectation according to the samples of problem Bi.\nThis lower bound implies that even if the learner is given the distance of the mean of each arm to the threshold and the shape of the distribution of each arm (here Gaussian of variance 1), any algorithm still makes an error of at least exp(\u22123T/H \u2212 4 log(12(log(T ) + 1)K)) on one of the problems. This is a lower bound in a very strong sense because we really restrict the set of possibilities to a setting where we know all gaps and prove that nevertheless this lower bounds holds. Also it is non-asymptotic and holds for any T , and implies therefore a non-asymptotic minimax lower bound. The closer the means of the distributions to the threshold, the larger the complexity H , and the larger the lower bound. The proof is to be found in Appendix A.\nThis theorem\u2019s lower bound contains two terms in the exponential, a term that is linear in T and a term that is of order log((log(T ) + 1)K) \u2248 log(log(T )) + log(K). For large enough values of T , one has the following simpler corollary. Corollary. Let H\u0304 > 0 and R > 0, \u03c4 \u2208 R and \u2265 0. Consider BH\u0304,R the set of K-armed bandit problems where the distributions of the arms are R-sub-Gaussian and which have all a complexity smaller than H\u0304 .\nAssume that T \u2265 4H\u0304R2 log(12(log(T ) + 1)K). It holds that for any bandit algorithm\nsup B\u2208BH\u0304,R\nEB(L(T )) \u2265 exp ( \u2212 4T/(R2H\u0304) ) ,\nwhere EB is the expectation according to the samples of problem B \u2208 BH\u0304,R."}, {"heading": "2.3. Algorithm APT and associated upper bound", "text": "In this section we introduce APT (Anytime Parameter-free Thresholding algorithm), an anytime parameter-free learning algorithm. Its heuristic is based on a simple observation, namely that a near optimal static strategy that allocates Tk samples to arm k is such that Tk\u22062k is constant across k (and increasing with T ) - see Theorem 1, and in particular the second half of Step 3 of its proof in Appendix A - and that therefore a natural idea is to simply pull at time t the arm that minimizes an estimator of this quantity. Note that in this paper, we consider for the sake of simplicity that each arm is tested against the same threshold, however this can be relaxed to (\u03c4k)k at no additional cost.\nAlgorithm The algorithm receives as input the definition of the problem (\u03c4, ). First, it pulls each arm of the game once. At time t > K, APT updates Ti(t), the number of pulls up to time t of arm i, and the empirical mean \u00b5\u0302i(t) of arm k after Ti(t) pulls. Formally, for each k \u2208 A it computes Ti(t) = \u2211t s=1 I(Is = i) and the updated means\n\u00b5\u0302i(t) = 1\nTi(t) Ti(t)\u2211 s=1 Xi,s, (3)\nAlgorithm 1 APT algorithm Input: \u03c4 , Pull each arm once for t = K + 1 to T do\nPull arm It = arg min k\u2264K Bk(t) from Equation (5)\nObserve reward X \u223c \u03bdIt end for Output: S\u0302\u03c4 = {k : \u00b5\u0302k(T ) \u2265 \u03c4}\nwhere Xi,s denotes the sample received when pulling i for the s-th time. The algorithm then computes:\n\u2206\u0302i(s) := \u2206\u0302 \u03c4, i (s) = |\u00b5\u0302i(t)\u2212 \u03c4 |+ , (4)\nthe current empirical estimate of the gap associated with arm i. The algorithm then computes:\nBi(t+ 1) = \u221a Ti(t)\u2206\u0302i(t). (5)\nand pulls the arm It+1 = arg min i\u2264K Bi(t+1) that minimizes this quantity. At the end of the horizon T , the algorithm outputs the set of arms S\u0302\u03c4 = {k : \u00b5\u0302k(T ) \u2265 \u03c4}.\nThe expected loss of this algorithm can be bounded as follows.\nTheorem 2. Let K \u2265 0, T \u2265 2K, and consider a problem B. Assume that all arms \u03bdk of the problem are R-subGaussian with means \u00b5k. Let \u03c4 \u2208 R, \u2265 0\nAlgorithm APT\u2019s expected loss is upper bounded on this problem as\nE(L(T )) \u2264 exp ( \u2212 1\n64R2 T H + 2 log((log(T ) + 1)K)\n) ,\nwhere we remind thatH = \u2211 i(|\u00b5i\u2212\u03c4 |+ )\u22122 and where E is the expectation according to the samples of the problem.\nThe bound of Theorem 2 holds for any R-sub-Gaussian bandit problem. Note that one does not need to know R in order to implement the algorithm, e.g. if the distributions are bounded, one does not need to know the bound. This is a desirable feature for an algorithm, yet e.g. all algorithms based on upper confidence bounds need a bound on R. This bound is non-asymptotic (one just needs T \u2265 2K so that one can initialize the algorithm) and therefore Theorem 2 provides a minimax upper bound result over the class of problems that have sub-Gaussian constant R and complexity H .\nThe term in the exponential of the lower bound of Theorem 2 matches the lower bound of Theorem 1 up to a multiplicative factor and the log((log(T )+1)K) term. Now as in the case of the lower bound, for large enough values of T , one has the following simpler corollary.\nCorollary. Let H\u0304 > 0 and R > 0, \u03c4 \u2208 R and \u2265 0. Consider BH\u0304,R the set of K-armed bandit problems where the distributions of the arms are R-sub-Gaussian and whose complexity is smaller than H\u0304 .\nAssume that T \u2265 256H\u0304R2 log((log(T ) + 1)K). For Algorithm APT it holds that\nsup B\u2208BH\u0304,R\nEB(L(T )) \u2264 exp ( \u2212 T/(128R2H) ) ,\nwhere EB is the expectation according to the samples of problem B \u2208 BH\u0304,R\nThis corollary and Corollary 2.2 imply that for T large enough - i.e. of larger order than HR2 log((log(T ) + 1)K) - Algorithm APT is order optimal over the class of problems whose complexity is bounded by H\u0304 and whose arms are R-sub-Gaussian."}, {"heading": "2.4. Discussion", "text": "A parameter free algorithm An important point that we want to highlight for our strategy APT is that it does not need any parameter, such as the complexity H , the horizon T or the sub-Gaussian constant R. This contrasts with any upper confidence based approach as in e.g. (Audibert & Bubeck, 2010; Gabillon et al., 2012) (e.g. the UCB-E algorithm in (Audibert & Bubeck, 2010)), which need as parameter an upper bound on R and the exact knowledge of H , while the bound of Theorem 2 will hold for any R and any H , and our algorithm adapts to these quantities. Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.\nExtensions to distributions that are not sub-Gaussian as opposed to adaptation to sub- models It is easy to see in the light of (Bubeck et al., 2013a) that one could extend our algorithm to non sub-Gaussian distributions by using an estimator other than the empirical means, as e.g. the estimators in (Catoni et al., 2012) or in (Alon et al., 1996). These estimators have sub-Gaussian concentration asymptotically under the only assumption that the distributions have a finite (1 + v) moment with v > 0 (and the sub-Gaussian concentration will depend on v). Using our algorithm with a such estimator will therefore provide a result that is similar to the one of Theorem 2 - and that without requiring the knowledge of v, which means that our algorithm APT modified for using these robust estimators instead of the empirical mean will work for any bandit problem where the arm distributions have a finite (1 + v) moment with v > 0. On the other hand, if we consider more specific, e.g. exponential, models, it is possible to obtain a refined lower\nbound in terms of Kullback- Leibler divergences rather than gaps following (Kaufmann et al., 2015). However, an upper bound of the same order clearly comes at the cost of a more complicated strategy and holds in less generality than our bound.\nOptimality of our strategy As explained previously, the upper bound on the expected risk of algorithm APT is comparable to the lower bound on the expected risk up to a log ( (log(T ) + 1)K ) term (see Theorems 2 and Theorems 1) - and this term vanishes when the horizon T is large enough, namely when T \u2265 O(HR2 log ( (log(T )+1)K ) ), which is the case for most problems. So for T large enough, our strategy is order optimal over the class of problems that have complexity smaller thanH and sub-Gaussian constant smaller than R.\nComparison with existing results Our setting is a specific combinatorial pure exploration setting with fixed budget where the objective is to find the set of arms that are above a given threshold. Settings related to ours have been analyzed in the literature and the state of the art result on our problem can be found (to the best of our knowledge) in the paper (Chen et al., 2014). In this paper, the authors consider a general pure exploration combinatorial problem. Given a set M of subsets of {1, . . . ,K}, they aim at finding a subset of arms M\u2217 \u2208 M such that M\u2217 = arg maxM\u2208M \u2211 k\u2208M\u2217 \u00b5k. In the specific case whereM is the set of all subsets of {1, . . . ,K}, their problem in the fixed budget setting is exactly the same as ours when = 0 and the means are shifted by \u2212\u03c4 . Their algorithm CSAR\u2019s upper bound on the loss is (see their Theorem 3):\nE(L(T )) \u2264 K2 exp ( \u2212 T \u2212K\n72R2 log(K)HCSAR,2\n) ,\nwhere HCSAR,2 = maxi i\u2206\u22122(i) . As HCSAR,2 log(K) \u2265 H by definition, there is a gap for their strategy in the fixed budget setting with respect to the lower bound of Theorem 1, which is smaller and of order exp(\u2212T/(HR2)). Our strategy on the contrary does not have this gap, and improves over the CSAR strategy. We believe that this lack of optimality for CSAR is not an artefact of the proof of the paper (Chen et al., 2014), and that CSAR is sub-optimal, as it is a successive reject algorithm with fixed and nonadaptive reject phase length. A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014). It is therefore an interesting fact that there is a parameter free optimal algorithm for our fixed budget problem.\nThe paper (Chen et al., 2014) also provides results in the fixed confidence setting, where the objective is to provide an optimal set using the smallest possible sample size. In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting. This highlights that the fixed budget setting and the fixed confidence setting are fundamentally different (at least in the absence of additional information such as the complexity H), and that providing optimal strategies in the fixed budget setting is a more difficult problem than providing an adaptive strategy in the fixed confidence problem - adaptive algorithms that are nearly optimal in the absence of additional information have only been exhibited in the latter case. To the best of our knowledge, all strategies except ours have such an optimality gap for fixed budget pure exploration combinatorial bandit problems, while there exists fixed confidence strategies for general pure exploration combinatorial bandits that are very close to optimal, see (Chen et al., 2014).\nNow in the case where the learner has additional information on the problem, as e.g. the complexity H , it has been proved in the TopM problem that a UCB-type strategy has probability of error upper bounded as exp(\u2212T/H), see (Audibert & Bubeck, 2010; Gabillon et al., 2012). A similar UCB type of algorithm would also work in the TBP problem, implying the same upper bound results as APT. But we would like to highlight that the exact knowledge of H is needed by these algorithms for reaching this bound - which is unlikely in applications. Our strategy on the other hand reaches, up to constants, the optimal expected loss for the TBP problem, without needing any parameter."}, {"heading": "3. Extensions of our results to related settings", "text": "In this section we detail some implications of the results of the previous section to some specific problems."}, {"heading": "3.1. Active level set detection : Active classification and active anomaly detection", "text": "Here we explain how a simple modification of our setting transforms it into the setting of active level set detection, and therefore why it can be applied to active classification and active anomaly detection. We define the problem of discrete, active level set detection as the problem of deciding as efficiently as possible, in our bandit setting, whether for any k the probabilities that the samples of arms \u03bdk are above or below a given level L are higher or smaller than a threshold \u03c4 up to a precision , i.e. it is the problem of deciding for all k whether \u00b5\u0303k(L) := PX\u223c\u03bdk(X > L) \u2265 \u03c4 , or not up to a precision .\nThis problem can be immediately solved by our approach with a simple change of variable. Namely, for the sample Xt \u223c \u03bdIt collected by the algorithm at time t, consider the transformation X\u0303t = 1Xt>L. Then X\u0303t is a Bernoulli random variable of parameter \u00b5\u0303It(L) (which is a 1/2-subGaussian distribution) - and applying our algorithm to the transformed samples X\u0303t solves the active level set detection problem. This has two interesting applications, namely in active binary classification and in active anomaly detection.\nActive binary classification In active binary classification, the learner aims at deciding, for k points (the arms of the bandit), whether each point belongs to the class 1 or the class 0.\nAt each round t, the learner can request help from a homogeneous mass of experts (which can be a set of previously trained classifiers, where one wants to minimize the computational cost, or crowd-sourcing, when one wants to minimize the costs of the task), and obtain a noisy label for the chosen data point It. We assume that for any point k, the expert\u2019s responses are independent and stochastic random variables in {0, 1} of mean \u00b5k (i.e. the arm distributions are Bernoulli random variables of parameter \u00b5k). We assume that the experts are right on average and that the label lk of k is equal to lk := 1{\u00b5\u0303k > 1/2}. The active classification task therefore amounts to deciding whether \u00b5k > \u03c4 := 1/2 or not, possibly up to a given precision . Our strategy therefore directly applies to this problem by choosing \u03c4 = 1/2.\nActive anomaly detection In the case anomaly detection, a common way to characterize anomalies is to describe them as naturally not concentrated (Steinwart et al., 2005). A natural way to characterize anomalies is thus to define a cutoff level L, and classify the samples e.g. above this level L as anomalous. Such an approach has already received attention for anomaly detection e.g in (Streeter & Smith), albeit in a cumulative regret setting.\nHere we consider an active anomaly detection setting where we face K sources of data (the arms), and we aim at sampling them actively to detect which sources emit anomalous samples with a probability higher than a given threshold \u03c4 - this threshold is chosen e.g. as the maximal tolerable amount of anomalous behavior of a source. This illustrates the fact that as described in (Steinwart et al., 2005), the problem of anomaly detection is indeed a problem of level set detection - and so the problem of active anomaly detection is a problem of active level set detection on which we can use our approach as explained above."}, {"heading": "3.2. Best arm identification and cumulative reward maximization with known highest mean value", "text": "Two classical bandit problems are the best-arm identification problem and the cumulative reward maximization problem. In the former, the goal of the learner is to identify the arm with the highest mean (Bubeck et al., 2009). In the latter, the goal is to maximize the sum of the samples collected by the algorithm up to time T (Auer et al., 1995). Intuitively, both problems should call for different strategies - in the best arm identification problem one wants to explore all arms heavily while in the cumulative reward maximization problem one wants to sample as much as possible the arm with the highest mean. Such intuition is backed up by Theorem 1 of (Bubeck et al., 2009), which states that in the absence of additional information and with a fixed budget, the lower the regret suffered in the cumulative setting, expressed in terms of rewards, the higher the regret suffered in the identification problem, expressed in terms of probability of error. We prove in this section the somewhat non intuitive fact that if one knows the value of best arm\u2019s mean, its possible to perform both tasks simultaneously by running our algorithm where we choose = 0 and \u03c4 = \u00b5\u2217 := maxk \u00b5k. Our algorithm then reduces to the GCL\u2217 algorithm that can be found in (Salomon & Audibert, 2011).\nBest arm identification In the best arm identification problem, the game setting is the same as the one we considered but the goal of the learner is different: it aims at returning an arm JT that with the highest possible mean. The following proposition holds for our strategy APT that runs for T times, and then returns the arm JT that was the most pulled.\nTheorem 3. Let K > 0, R > 0 and T \u2265 2K and consider a problem where the distribution of the arms \u03bdk is R-sub-Gaussian and has mean \u00b5k. Let \u00b5\u2217 := maxk \u00b5k and H\u00b5\u2217 = \u2211 i:\u00b5i 6=\u00b5\u2217(\u00b5 \u2217 \u2212 \u00b5i)\u22122. Then APT run with parameters \u03c4 = \u00b5\u2217 and = 0, recommending the arm JT = arg max\nk\u2208A Tk(T ), is such that\nP(\u00b5JT 6= \u00b5\u2217) \u2264 exp ( \u2212 T\n36R2H\u00b5\u2217 +2 log(log(T )+1)K\n) .\nIf the complexity H is also known to the learner, algorithm UCB-E from (Audibert & Bubeck, 2010) would attain a similar performance.\nRemark This implies that if \u00b5\u2217 is known to the learner, there exists an algorithm such that its probability of error is of order exp(\u2212cT/H). The recent paper (Carpentier & Locatelli, 2016) actually implies that the knowledge of \u00b5\u2217 is actually key here, since without this information, the simple regret is at least of order exp(\u2212cT/(log(K)H)) in a minimax sense.\nCumulative reward maximization In the cumulative reward maximization problem, the game setting is the same as the the one we considered but the aim of the learner is different : if we write Xt for the sample collected at time t by the algorithm, it aims at maximizing \u2211 t\u2264T Xt. The following proposition holds for our strategy APT that runs for T times.\nTheorem 4. Let K > 0, R > 0 and T \u2265 2K and consider a problem where the distribution of the arms \u03bdk is R-subGaussian.\nThen APT run with parameters \u03c4 = \u00b5\u2217 and = 0 is such that\nT\u00b5\u2217 \u2212 E \u2211 t\u2264T Xt \u2264 inf \u03b4\u22651 [ \u2211 k 6=k\u2217 4R2 log(T )\u03b4 \u00b5\u2217 \u2212 \u00b5i\n+ (\u00b5\u2217 \u2212 \u00b5i)(1 + K T 2\u03b4\u22122 ) ] .\nThis bound implies both the problem dependent upper bound of order \u2211 i \u2206 \u22121 i log(T ) and the problem indepen-\ndent upper bound of order \u221a TK log(T ), and this matches the performance of algorithms like UCB for any tuning parameter. A similar result can also be found in (Salomon & Audibert, 2011).\nDiscussion Propositions 3 and 4, whose proofs are provided in Appendix A, imply that our algorithm APT is a good strategy for solving at the same time both problems when \u00b5\u2217 is known. As mentioned previously, this is counter intuitive since one would expect a good strategy for the best arm identification problem to explore significantly more than a good strategy for the cumulative reward maximization problem. To convince oneself, it is sufficient to look at the two-armed case, for which in the fixed budget it is optimal to sample both arms equally, while this strategy has linear regret in the cumulative setting. This intuition is formalized in (Bubeck et al., 2009) where the authors prove that no algorithm can achieve this without additional information. Our results therefore imply that the knowledge of \u00b5\u2217 by the learner is a sufficient information so that Theorem 1 of (Bubeck et al., 2009) does not hold anymore and there exists algorithms that solve both problems at the same time, as APT does.\nTopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015). If the learner has some additional information, such as the mean values of the arms with M th and (M + 1)th highest means, then it is straightforward that one can apply our algorithm APT, setting \u03c4 in the middle between the M th and (M + 1)th highest means. The set S\u0302\u03c4 would then be returned as the es-\ntimated set ofM optimal arms. The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015). If the complexity H were also known to the learner, the strategy in (Gabillon et al., 2012) would attain a similar performance."}, {"heading": "4. Experiments", "text": "We illustrate the performance of algorithm APT in a number of experiments. For comparison, we use the following methods which include the state of the art CSAR algorithm of (Chen et al., 2014) and two minor adaptations of known methods that are also suitable for our problem. Uniform Allocation (UA): For each t \u2208 {1, 2, . . . , T}, we choose It \u223c UA. This method is known to be optimal if all arms are equally difficult to classify, that is in our setting, if the quantities \u2206\u03c4, i , i \u2208 A, are very close. UCB-type algorithm: The algorithm UCBE given and analyzed in (Audibert & Bubeck, 2010) is designed for finding the best arm - and its heuristic is to pull the arm that maximizes a UCB bound - see also (Gabillon et al., 2012) for an adaptation of this algorithm to the general TopM problem. The natural adaptation of the method for our problem corresponds to pulling the arm that minimizes \u2206\u0302k(t) \u2212 \u221a a\nTk(t) . From the theoretical analysis in the\npaper (Audibert & Bubeck, 2010; Gabillon et al., 2012), it is not hard to see that setting a \u2248 (T \u2212K)/H minimizes their upper bound, and that this algorithm attains the same expected loss as ours - but it requires the knowledge of H . In the experiments we choose values ai = 4i T\u2212KH , i \u2208 {\u22121, 0, 4}, and denote the respective results as UCBE(4i). The value a0 can be seen as the optimal choice, while the two other choices give rise to strategies that are sub-optimal because they respectively explore too little or too much. CSAR: As mentioned before, this method is given in (Chen et al., 2014). In our specific setting, via the shift \u00b5\u0303i = \u00b5i \u2212 \u03c4 , the lines 7-17 of the algorithm reduce\nto classifying the arm i that maximizes |\u00b5\u0303i| based on its current mean. The set At corresponds to S\u0302\u03c4 at time t. In fact in our specific setting the CSAR algorithm is a successive reject-type strategy (see (Audibert & Bubeck, 2010) where the arm whose empirical mean is furthest from the threshold is rejected at the end of each phase.\nFigure 1 displays the estimated probability of success on a logarithmic scale with respect to the horizon of the six algorithms based onN = 5000 simulated games with \u03c4 = 12 , = 0.1, K = 10, and T = 500. Experiment 1 (3 groups setting): K Bernoulli arms with means \u00b51:3 \u2261 0.1, \u00b54:7 = (0.35, 0.45, 0.55, 0.65) and \u00b58:10 \u2261 0.9, which amounts to 2 difficult relevant arms (that is, outside the 2 - band), 2 difficult irrelevant arms and six easy relevant arms. Experiment 2 (arithmetic progression): K Bernoulli arms with means \u00b51:4 = 0.2 + (0 : 3) \u00b7 0.05, \u00b55 = 0.45, \u00b56 = 0.55 and \u00b57:10 = 0.65+(0 : 3)\u00b70.05, which amounts to 2 difficult irrelevant arms and eight arms arithmetically progressing away from \u03c4 . Experiment 3 (geometric progression): K Bernoulli arms with means \u00b51:4 = 0.4 \u2212 0.21:4, \u00b55 = 0.45, \u00b56 = 0.55 and \u00b57:10 = 0.6 + d5\u2212(1:4), which amounts to 2 difficult irrelevant arms and eight arms geometrically progressing away from \u03c4 .\nThe experimental results confirm that our algorithm may only be outperformed by methods that have an advantage in the sense that they have access to the underlying problem complexity and, in the case of UCBE(1), an additional optimal parameter choice. In particular, other choices for that parameter lead to significantly less accurate results comparable to the naive strategy UA. These effects are also visible in the further results given in Appendix B.\nConclusion In this paper we proposed a parameter free algorithm based on a new heuristic for the TBP problem in the fixed confidence setting - and we prove that it is optimal which is a kind of result which is highly non trivial for combinatorial pure exploration problems with fixed budget.\nAcknowledgement This work is supported by the DFG\u2019s Emmy Noether grant MuSyAD (CA 1488/1-1)."}, {"heading": "A. Proofs", "text": "A.1. Proof of Theorem 1\nProof. In this proof, we will prove that on at least one instance of the problem, any algorithm makes a mistake of order at least exp(\u2212cT/H).\nStep 0: Setting and notations. Let us consider K real numbers \u2206i \u2265 0, and let us set \u03c4 = 0, = 0. Let us write \u03bdi := N (\u2206i, 1) for the Gaussian distribution of mean \u2206i and variance 1, and \u03bd\u2032i := N (\u2212\u2206i, 1) for the Gaussian distribution of mean \u2212\u2206i and variance 1. Note that this construction is easily generalised to cases where \u03c4 6= 0 or 6= 0 by translation or careful choice of the \u2206i.\nWe define the product distributions Bi where i \u2208 {0, ...,K} as \u03bdi1 \u2297 ... \u2297 \u03bdiK where for k \u2264 K, \u03bdik := \u03bdi1k 6=i + \u03bd\u2032i1k=i is \u03bdi if k 6= i and \u03bd\u2032i otherwise. We also extend this notation to B0, where none of the arms is flipped with respect to the threshold (\u2200k, \u03bd0k := \u03bdi). It is straightforward that the gap \u2206i of arm i with respect to the threshold \u03c4 = 0 does not depend on Bi and is equal to \u2206i. It follows that all these problems have the same complexity H as defined previously (with = 0 and \u03c4 = 0).\nWe write for i \u2264 K, PiB for the probability distribution according to all the samples that a strategy could possibly collect up to horizon T , i.e. according to the samples (Xk,s)k\u2264K,s\u2264T \u223c (Bi)\u2297T . Let (Tk)k\u2264K denote the numbers of samples collected by the algorithm on arm k.\nLet k \u2208 {0, ...,K}. Note that\nKLk := KL(\u03bd\u2032k, \u03bdk) = 2\u2206 2 k,\nwhere KL is the Kullback Leibler divergence. Let T \u2265 t \u2265 0. We define the quantity:\nK\u0302Lk,t = 1\nt t\u2211 s=1 log( d\u03bd\u2032k d\u03bdk (Xk,s)) = \u2212 1 t t\u2211 s=1 2Xk,s\u2206k.\nStep 1: Concentration of the empirical KL. Let us define the event:\n\u03be = { \u2200k \u2264 K,\u2200t \u2264 T, |K\u0302Lk,t \u2212 KLk| \u2264 4\u2206k \u221a log(4(log(T ) + 1)K)\nt\n} .\nSince K\u0302Lk,t = \u2212 1t \u2211t s=1 2Xk,s\u2206k and KLk = 2\u2206 2 k, by Gaussian concentration (a peeling and the maximal martingale inequality), it holds that for any i that PBi(\u03be) \u2265 3/4.\nStep 2: A change of measure. We will now use the change of measure introduced previously for a well chosen event A. Namely, we considerAi = {i \u2208 S\u0302\u03c4}, the event where the algorithm classified arm i as being above the threshold. We have by doing a change of measure between Bi and B0 (since they only differ in arm i and only the Ti first samples of arm i by the algorithm):\nPBi(Ai) = EB0 [ 1Ai exp ( \u2212 TiK\u0302Li,Ti )] \u2265 EB0 [ 1Ai\u2229\u03be exp ( \u2212 TiK\u0302Li,Ti\n)] \u2265 EB0 [ 1Ai\u2229\u03be exp ( \u2212 2\u22062iTi \u2212 4\u2206i \u221a Ti \u221a log((4 log(T ) + 1)K) )] ,\nby definition of \u03be and KLi. Step 3: A union of events. We now consider the event A = K\u22c2 i=1 Ai, i.e. the event where all arms are classified as\nbeing above the threshold \u03c4 = 0. We have:\nmax i\u2208{1,...,K}\nPBi(Ai) \u2265 1\nK K\u2211 i=1 PBi(Ai) (6)\n\u2265 1 K K\u2211 i=1 PBi(Ai \u2229 \u03be)\n\u2265 1 K K\u2211 i=1 EB0 [ 1Ai\u2229\u03be exp ( \u2212 2Ti\u22062i \u2212 4\u2206i \u221a Ti \u221a log(4(log(T ) + 1)K) )]\n\u2265 EB0 [ 1A\u2229\u03be 1\nK K\u2211 i=1 exp ( \u2212 3Ti\u22062i \u2212 4 log(4(log(T ) + 1)K) )] \u2265 exp ( \u2212 4 log(4(log(T ) + 1)K) ) EB0 [ 1A\u2229\u03beS ] , (7)\nwhere the fourth line comes from using 2ab \u2264 a2 + b2 with a = \u2206i \u221a Ti and where:\nS = 1\nK K\u2211 i=1 exp ( \u2212 3Ti\u22062i ) .\nSince \u2211 i Ti = T and all Ti are positive, there exists an arm i such that Ti \u2264\nT H\u22062i . This yields:\nS \u2265 1 K exp ( \u2212 3T H ) = exp ( \u2212 3T H \u2212 log(K) ) .\nThis implies by definition of the risk:\nmax i\u2208{0,...,K}\nEBi(L(T )) \u2265 max (\nmax i\u2208{1,...,K}\nPBi(Ai), 1\u2212 PB0(A) )\n\u2265 1 2 exp ( \u2212 3T H \u2212 4 log(4(log(T ) + 1)K) ) \u2212 log(K)EB0 [ 1A\u2229\u03be ] + 1 2 (1\u2212 PB0(A)) = 1\n2 exp ( \u2212 3T H \u2212 4 log(4(log(T ) + 1)K \u2212 log(K)) ) PB0 [ A \u2229 \u03be ] + 1 2 (1\u2212 PB0(A))\n\u2265 1 8 exp ( \u2212 3T H \u2212 4 log(4(log(T ) + 1)K)\u2212 log(K) ) \u2265 exp\n( \u2212 3T H \u2212 4 log(12(log(T ) + 1)K) ) ,\nThe fourth line comes from P(\u03be) \u2265 3/4, and we consider two cases PB0(A) \u2265 1/2 and PB0(A) \u2264 1/2. The first leads directly to the condition as the intersection is at least of probability 1/4; in the latter case, we have the same bound via\nmax i\u2208{0,...,K}\nEBi(L(T )) \u2265 EB0(L(T )) = PB0(AC) \u2265 1/2.\nThis concludes the proof.\nA.2. Proof of Theorem 2\nProof. In this proof, we will show that on a well chosen event \u03be, we classify correctly the arms which are over \u03c4 + , and reject the arms that are under \u03c4 \u2212 .\nStep 1: A favorable event. Let \u03b4 = (4 \u221a 2)\u22121. Towards this goal, we define the event \u03be as follows:\n\u03be = { \u2200i \u2208 A,\u2200s \u2208 {1, ..., T} : |1\ns s\u2211 t=1 Xi,t \u2212 \u00b5i| \u2264 \u221a T\u03b42 Hs } .\nWe know from Sub-Gaussian martingale inequality that for each i \u2208 A and each u \u2208 {0, ..., blog(T )c}:\nP ( \u2203v \u2208 [2u, 2u+1], {|1\nv v\u2211 t=1 Xi,t \u2212 \u00b5i| \u2265 \u221a T\u03b42 Hv } ) \u2264 exp(\u2212 T\u03b4 2 2R2H ).\n\u03be is the union of these events for all i \u2264 K and s \u2264 blog(T )c. As there are less than (log(T ) + 1)K such combinations, we can lower-bound its probability of occurrence with a union bound by:\nP(\u03be) \u2265 1\u2212 2(log(T ) + 1)K exp(\u2212 T\u03b4 2\n2R2H ).\nStep 2: Characterization of some helpful arm. At time T , we consider an arm k that has been pulled after the initialization phase and such that Tk(T )\u2212 1 \u2265 (T\u2212K)H\u22062k . We know that such an arm exists otherwise we get:\nT \u2212K = K\u2211 i=1 (Ti(T )\u2212 1) < K\u2211 i=1 T \u2212K H\u22062i = T \u2212K,\nwhich is a contradiction. Note that since T \u2265 2K, we have that Tk(T )\u2212 1 \u2265 T2H\u22062k We now consider t \u2264 T the last time that this arm k was pulled. Using Tk(t) \u2265 2 (by the initialisation of the algorithm), we know that:\nTk(t) \u2265 Tk(T )\u2212 1 \u2265 T\n2H\u22062k . (8)\nStep 3: Lower bound on the number of pulls of the other arms. On \u03be, at time t as we defined previously, we have for every arm i:\n|\u00b5\u0302i(t)\u2212 \u00b5i| \u2264\n\u221a T\u03b42\nHTi(t) . (9)\nFrom the reverse triangle inequality and Equation (4), we have:\n|\u00b5\u0302i(t)\u2212 \u00b5i| = |(\u00b5\u0302i(t)\u2212 \u03c4)\u2212 (\u00b5i \u2212 \u03c4)| \u2265 ||\u00b5\u0302i(t)\u2212 \u03c4 | \u2212 |\u00b5i \u2212 \u03c4 || \u2265 |(|\u00b5\u0302i(t)\u2212 \u03c4 |+ )\u2212 (|\u00b5i \u2212 \u03c4 |+ )|\n\u2265 |\u2206\u0302i(t)\u2212\u2206i|.\nCombining this with (9) yields the following:\n\u2206k \u2212\n\u221a T\u03b42\nHTk(t) \u2264 \u2206\u0302k(t) \u2264 \u2206k +\n\u221a T\u03b42\nHTk(t) . (10)\nBy construction, we know that at time t we pulled arm k, which yields for every i \u2208 A:\nBk(t) \u2264 Bi(t). (11)\nWe can lower bound the left-hand side of (11) using (8):( \u2206k \u2212 \u221a T\u03b42\nHTk(t)\n)\u221a Tk(t) \u2264 Bk(t)\n( \u2206k \u2212 \u221a 2\u03b4\u2206k )\u221a T 2H\u22062k \u2264 Bk(t)\n( 1\u221a 2 \u2212 \u03b4 )\u221a T H \u2264 Bk(t), (12)\nand upper bound the right hand side using (10) by: Bi(t) = \u2206\u0302i \u221a Ti(t)\n\u2264 (\n\u2206i +\n\u221a T\u03b42\nHTi(t)\n)\u221a Ti(t)\n\u2264 \u2206i \u221a Ti(t) + \u03b4\n\u221a T\nH . (13)\nAs both \u2206\u0302i and \u2206i are positive by definition, combining (12) and (13) yields the following lower bound on Ti(T ) \u2265 Ti(t):( 1\u2212 2 \u221a 2\u03b4 )2 T\n2H\u22062i \u2264 Ti(T ). (14)\nStep 4: Conclusion. On \u03be, as \u2206i is a positive quantity, combining (9) and (14) yields:\n\u00b5i \u2212\u2206i \u221a 2\u03b4\n1\u2212 2 \u221a 2\u03b4 \u2264 \u00b5\u0302i(T ) \u2264 \u00b5i + \u2206i\n\u221a 2\u03b4 1\u2212 2 \u221a 2\u03b4 , (15)\nwhere \u221a\n2\u03b4 1\u22122 \u221a 2\u03b4 simplifies to 1/2 for \u03b4 = (4\n\u221a 2)\u22121.\nFor arms such that \u00b5i \u2265 \u03c4 + , then \u2206i = \u00b5i \u2212 \u03c4 + and we can rewrite (15):\n\u00b5i \u2212 \u03c4 \u2212 1\n2 \u2206i \u2264 \u00b5\u0302i(T )\u2212 \u03c4\n(\u00b5i \u2212 \u03c4)(1\u2212 1\n2 )\u2212 2 \u2264 \u00b5\u0302i(T )\u2212 \u03c4\n0 \u2264 \u00b5\u0302i(T )\u2212 \u03c4,\nwhere the last line uses \u00b5i \u2265 \u03c4 + . One can easily check through similar derivations that \u00b5\u0302i(T ) \u2212 \u03c4 < 0 holds for \u00b5i < \u03c4 \u2212 . On \u03be, arms over \u03c4 + are all accepted, and arms under \u03c4 \u2212 are all rejected, which means the loss suffered by the algorithm is 0. As 1\u2212 P(\u03be) \u2264 2(log(T ) + 1)K exp(\u2212 164R2 T H ), this concludes the proof.\nA.3. Proof of Theorem 3\nProof. We will prove that on a well defined event \u03be, sub-optimal arms are pulled at most T 2\u22062kH \u2212 1 times, which translates to the best arm being chosen at the end of the horizon as it was pulled more than half of the time.\nStep 1: A favorable event. Let \u03b4 = 1/18. We define the following events \u2200i \u2208 A:\n\u03bei = {\u2200s \u2264 T : |\u00b5\u2217 \u2212 \u00b5\u0302i(s)| \u2264\n\u221a T\u03b4\nHTi(s) },\nWe now define \u03be as the intersection of these events: \u03be = \u22c2 k\u2208A \u03bek.\nUsing the same Sub-Gaussian martingale inequality as in the proof of Theorem 2, we can lower bound its probability of occurrence with a union bound by:\nP (\u03be) \u2265 1\u2212 2(log(T ) + 1)K exp(\u2212 T 36R2H )\nStep 2: The wrong arm at the wrong time. Let us now suppose that a sub-optimal arm k was pulled at least T\u2212K 2\u22062kH times after the initialization which translates to Tk(T ) \u2212 1 \u2265 T\u2212K2\u22062kH . Let us now consider the last time t \u2264 T that this arm was pulled. As it was pulled at time t, the following inequality holds:\nBk(t) \u2264 Bk\u2217(t). (16)\nOn \u03be, we can now lower bound the left hand side by:\n(\u2206k \u2212\n\u221a T\u03b4\nHTk(t) ) \u221a Tk(t) \u2264 Bk(t)\n\u2206k \u221a Tk(t)\u2212\n\u221a T\u03b4\nH \u2264 Bk(t), (17)\nWe also upper bound the right hand side of (16) by: Bk\u2217(t) \u2264 \u221a T\u03b4\nH . (18)\nCombining both bounds (17) and (18) with (16), as well as rearranging the terms yields:\n\u2206k \u221a Tk(t) \u2264 2\n\u221a T\u03b4\nH\nTk(t)\u2206 2 k \u2264\n4T\u03b4\nH . (19)\nUsing Tk(t) \u2265 Tk(T )\u2212 1 \u2265 T\u2212K2\u22062kH as well as T \u2265 2K, we have\nTk(t) \u2265 T\n4\u22062kH . (20)\nPlugging this in (19) brings the following condition:\nT\n4\u22062kH \u22062k \u2264\n4T\u03b4\nH . (21)\nwhich directly reduces to \u03b4 \u2265 1/16, which is a contradiction as we have set \u03b4 = 1/18.\nAs we have proved that for any sub-optimal arm i 6= k\u2217 it satisfies Ti(T ) < T2\u22062iH , summing for all arms yields:\nT \u2212 Tk\u2217(T ) = \u2211 i 6=k\u2217 Ti(T )\n< T\n2H \u2211 i6=k\u2217 1 \u22062i = T 2 . (22)\nWe conclude by observing that Tk\u2217(T ) > T/2, and as such will be chosen by the algorithm at the end as being the best arm.\nA.4. Proof of Theorem 4\nProof. In this proof we will show that with high probability the sub-optimal arms have been pulled at most at a logarithmic rate, and will then bound the expectation of the number of pulls of these arms.\nStep 1: A favorable event. We define the following events \u2200s \u2264 T :\n\u03bek\u2217,s = {\u00b5\u2217 \u2212 \u00b5\u0302k\u2217(s) \u2264 R\n\u221a log(T )\u03b4\nTk\u2217(s) },\nas well as for all arms i 6= k\u2217:\n\u03bei,s = {\u00b5\u0302k(s)\u2212 \u00b5k \u2264 R\n\u221a log(T )\u03b4\nTki(s) }.\nBy Hoeffding\u2019s inequality, the complimentary \u03be\u0304k of each of these events has probability at most T\u22122\u03b4 . We now consider \u03be the intersection of these events for all k \u2208 A. By a union bound, as there are T such events for each arm, we have:\nP(\u03be) \u2265 1\u2212 K T 2\u03b4\u22121 . (23)\nWe also have: P(\u03be\u0304) \u2264 K\nT 2\u03b4\u22121 . (24)\nWe will now prove a bound on the number of pulls on \u03be.\nStep 2: Bound on pulls of sub-optimal arms. We now consider the last time t that arm k 6= k\u2217 was pulled, under the assumption that it was pulled at least once after the initialization. The decision rule of the algorithm yields:\nBk(t) \u2264 Bk\u2217(t). (25)\nOn \u03be, we can now lower-bound the left-side and upper-bound the right hand side, which yields:\n(\u2206k \u2212R\n\u221a log(T )\u03b4\nTk(t) ) \u221a Tk(t) \u2264 R\n\u221a log(T )\u03b4\nTk\u2217(t)\n\u221a Tk\u2217(t), (26)\nwhich can be rearranged as such: \u2206k \u221a Tk(t) \u2264 2R \u221a log(T )\u03b4, (27)\nand the following bound on Tk(T ):\nTk(T ) \u2264 4R2 log(T )\u03b4\n\u22062k + 1. (28)\nNote that we here make the assumption that the arm was pulled at least once by the algorithm after the initialization. If it has only been pulled during the initialization, the bound still trivially holds as we have at least one pull.\nStep 3: Conclusion. We can thus upper-bound the expectation of Tk(t), as when \u03be does not hold we get at most T pulls:\nE[Tk(T )] \u2264 4R2 log(T )\u03b4\n\u22062k + 1 +\nK\nT 2\u03b4\u22122 , (29)\nand we get the following bound on the pseudo-regret when \u03be holds:\nR\u0304T \u2264 \u2211 k 6=k\u2217 4R2 log(T )\u03b4 \u2206k + \u2206k(1 + K T 2\u03b4\u22122 ). (30)\nPlugging \u03b4 = 1 yields:\nR\u0304T \u2264 \u2211 k 6=k\u2217 4R2 log(T ) \u2206k + \u2206k(1 +K), (31)\nand we recover the classical bound of the UCB1 algorithm."}, {"heading": "B. Further Experimental Results", "text": "We now also provide simulation results for our three settings in the case of Gaussian arms with means \u00b5i and variances \u03c32i = 0.25. Again, only the correctly tuned UCBE- algorithm outperforms APT."}], "references": [{"title": "The space complexity of approximating the frequency moments", "author": ["Alon", "Noga", "Matias", "Yossi", "Szegedy", "Mario"], "venue": "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "Alon et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1996}, {"title": "Best arm identification in multi-armed bandits", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien"], "venue": "In Proceedings of the 23rd Conference on Learning Theory,", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Gambling in a Rigged Casino: The Adversarial Multi-Armed Bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Freund", "Yoav", "Schapire", "Robert"], "venue": "In Proceedings of the 36th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "Bandits with heavy tail", "author": ["Bubeck", "Sebastian", "Cesa-Bianchi", "Nicolo", "Lugosi", "G\u00e1bor"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["Bubeck", "S\u00e9bastien", "Munos", "R\u00e9mi", "Stoltz", "Gilles"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Multiple identifications in multi-armed bandits", "author": ["Bubeck", "S\u00e9ebastian", "Wang", "Tengyao", "Viswanathan", "Nitin"], "venue": "In Proceedings of The 30th International Conference on Machine Learning", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "On topk selection in multi-armed bandits and hidden bipartite graphs", "author": ["Cao", "Wei", "Li", "Jian", "Tao", "Yufei", "Zhize"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Tight (lower) bounds for the fixed budget best arm identification bandit problem", "author": ["Carpentier", "Alexandra", "Locatelli", "Andrea"], "venue": "In Proceedings of the 29th Conference on Learning", "citeRegEx": "Carpentier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carpentier et al\\.", "year": 2016}, {"title": "Challenging the empirical mean and empirical variance: a deviation study", "author": ["Catoni", "Olivier"], "venue": "In Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,", "citeRegEx": "Catoni and Olivier,? \\Q2012\\E", "shortCiteRegEx": "Catoni and Olivier", "year": 2012}, {"title": "On the optimal sample complexity for best arm identification", "author": ["Chen", "Lijie", "Li", "Jian"], "venue": "arXiv preprint arXiv:1511.03774,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Combinatorial pure exploration of multiarmed bandits", "author": ["Chen", "Shouyuan", "Lin", "Tian", "King", "Irwin", "Lyu", "Michael R", "Wei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Pac bounds for multi-armed bandit and markov decision processes", "author": ["Even-Dar", "Eyal", "Mannor", "Shie", "Mansour", "Yishay"], "venue": "In Computational Learning Theory,", "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits", "author": ["Jamieson", "Kevin", "Malloy", "Matthew", "Nowak", "Robert", "Bubeck", "S\u00e9bastien"], "venue": "In Proceedings of the 27th Conference on Learning Theory,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Pac subset selection in stochastic multiarmed bandits", "author": ["Kalyanakrishnan", "Shivaram", "Tewari", "Ambuj", "Auer", "Peter", "Stone"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "Almost optimal exploration in multi-armed bandits", "author": ["Karnin", "Zohar", "Koren", "Tomer", "Somekh", "Oren"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "On the complexity of best arm identification in multiarmed bandit models", "author": ["Kaufmann", "Emilie", "Capp\u00e9", "Olivier", "Garivier", "Aur\u00e9lien"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2015}, {"title": "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem", "author": ["S Mannor", "Tsitsiklis", "J N"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Deviations of stochastic bandit regret", "author": ["Salomon", "Antoine", "Audibert", "Jean-Yves"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Salomon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Salomon et al\\.", "year": 2011}, {"title": "A classification framework for anomaly detection", "author": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Steinwart et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2005}, {"title": "Selecting among heuristics by solving thresholded k-armed bandit problems", "author": ["Streeter", "Matthew J", "Smith", "Stephen F"], "venue": "ICAPS", "citeRegEx": "Streeter et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2006}, {"title": "Optimal pac multiple arm identification with applications to crowdsourcing", "author": ["Zhou", "Yuan", "Chen", "Xi", "Li", "Jian"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "In this paper, we refer to this setting as the Thresholding Bandit Problem (TBP), which is a specific instance of the combinatorial pure exploration bandit setting introduced in (Chen et al., 2014).", "startOffset": 178, "endOffset": 197}, {"referenceID": 12, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 16, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 21, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 6, "context": "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.", "startOffset": 189, "endOffset": 294}, {"referenceID": 10, "context": "As mentioned previously, the TBP problem is a specific instance of the combinatorial pure exploration bandit framework introduced in (Chen et al., 2014).", "startOffset": 133, "endOffset": 152}, {"referenceID": 10, "context": "Without going into the details of the combinatorial pure exploration setting for which the paper (Chen et al., 2014) derives interesting general results, we will summarize what these results imply for the particular TBP and TopM problems, which are specific cases of the combinatorial pure exploration setting.", "startOffset": 97, "endOffset": 116}, {"referenceID": 10, "context": "As it is often the case for pure exploration problems, the paper (Chen et al., 2014) distinguishes between two settings:", "startOffset": 65, "endOffset": 84}, {"referenceID": 12, "context": "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).", "startOffset": 165, "endOffset": 228}, {"referenceID": 15, "context": "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).", "startOffset": 165, "endOffset": 228}, {"referenceID": 10, "context": "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).", "startOffset": 165, "endOffset": 228}, {"referenceID": 12, "context": "While as explained in (Audibert & Bubeck, 2010; Gabillon et al., 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al.", "startOffset": 22, "endOffset": 70}, {"referenceID": 4, "context": ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 284, "endOffset": 374}, {"referenceID": 15, "context": ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 284, "endOffset": 374}, {"referenceID": 16, "context": ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 284, "endOffset": 374}, {"referenceID": 14, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 13, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 15, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 16, "context": "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.", "startOffset": 177, "endOffset": 291}, {"referenceID": 12, "context": "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 98, "endOffset": 190}, {"referenceID": 15, "context": "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 98, "endOffset": 190}, {"referenceID": 16, "context": "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).", "startOffset": 98, "endOffset": 190}, {"referenceID": 10, "context": "This knowledge gap is more acute for the general combinatorial exploration bandit problem defined in the paper (Chen et al., 2014) (see their Theorem 3) - and therefore for the TBP problem (where in fact no fixed budget lower bound exists to the best of our knowledge).", "startOffset": 111, "endOffset": 130}, {"referenceID": 10, "context": "The quantitiesH,H2 depend on the means \u03bck of the arm distributions and are defined in (Chen et al., 2014) (and are not the same for TopM and TBP).", "startOffset": 86, "endOffset": 105}, {"referenceID": 10, "context": "Our problem is a pure exploration bandit problem, and is in fact, shifting the means by \u2212\u03c4 , a specific case of the pure exploration bandit problem considered in (Chen et al., 2014) - namely the specific case where the set of sets of arms that they callM and which is their decision class is the set of all possible set of arms.", "startOffset": 162, "endOffset": 181}, {"referenceID": 10, "context": "A similar quantity was introduce for general combinatorial bandit problems (Chen et al., 2014) and is similar in essence to the complexity introduced for the best arm identification problem, see (Audibert & Bubeck, 2010).", "startOffset": 75, "endOffset": 94}, {"referenceID": 12, "context": "(Audibert & Bubeck, 2010; Gabillon et al., 2012) (e.", "startOffset": 0, "endOffset": 48}, {"referenceID": 15, "context": "Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.", "startOffset": 159, "endOffset": 224}, {"referenceID": 10, "context": "Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.", "startOffset": 159, "endOffset": 224}, {"referenceID": 0, "context": ", 2012) or in (Alon et al., 1996).", "startOffset": 14, "endOffset": 33}, {"referenceID": 16, "context": "exponential, models, it is possible to obtain a refined lower bound in terms of Kullback- Leibler divergences rather than gaps following (Kaufmann et al., 2015).", "startOffset": 137, "endOffset": 160}, {"referenceID": 10, "context": "Settings related to ours have been analyzed in the literature and the state of the art result on our problem can be found (to the best of our knowledge) in the paper (Chen et al., 2014).", "startOffset": 166, "endOffset": 185}, {"referenceID": 10, "context": "We believe that this lack of optimality for CSAR is not an artefact of the proof of the paper (Chen et al., 2014), and that CSAR is sub-optimal, as it is a successive reject algorithm with fixed and nonadaptive reject phase length.", "startOffset": 94, "endOffset": 113}, {"referenceID": 15, "context": "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).", "startOffset": 254, "endOffset": 342}, {"referenceID": 16, "context": "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).", "startOffset": 254, "endOffset": 342}, {"referenceID": 10, "context": "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).", "startOffset": 254, "endOffset": 342}, {"referenceID": 10, "context": "The paper (Chen et al., 2014) also provides results in the fixed confidence setting, where the objective is to provide an optimal set using the smallest possible sample size.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 13, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 15, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 16, "context": "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.", "startOffset": 123, "endOffset": 237}, {"referenceID": 10, "context": "To the best of our knowledge, all strategies except ours have such an optimality gap for fixed budget pure exploration combinatorial bandit problems, while there exists fixed confidence strategies for general pure exploration combinatorial bandits that are very close to optimal, see (Chen et al., 2014).", "startOffset": 284, "endOffset": 303}, {"referenceID": 12, "context": "the complexity H , it has been proved in the TopM problem that a UCB-type strategy has probability of error upper bounded as exp(\u2212T/H), see (Audibert & Bubeck, 2010; Gabillon et al., 2012).", "startOffset": 140, "endOffset": 188}, {"referenceID": 19, "context": "Active anomaly detection In the case anomaly detection, a common way to characterize anomalies is to describe them as naturally not concentrated (Steinwart et al., 2005).", "startOffset": 145, "endOffset": 169}, {"referenceID": 19, "context": "This illustrates the fact that as described in (Steinwart et al., 2005), the problem of anomaly detection is indeed a problem of level set detection - and so the problem of active anomaly detection is a problem of active level set detection on which we can use our approach as explained above.", "startOffset": 47, "endOffset": 71}, {"referenceID": 4, "context": "In the former, the goal of the learner is to identify the arm with the highest mean (Bubeck et al., 2009).", "startOffset": 84, "endOffset": 105}, {"referenceID": 2, "context": "In the latter, the goal is to maximize the sum of the samples collected by the algorithm up to time T (Auer et al., 1995).", "startOffset": 102, "endOffset": 121}, {"referenceID": 4, "context": "Such intuition is backed up by Theorem 1 of (Bubeck et al., 2009), which states that in the absence of additional information and with a fixed budget, the lower the regret suffered in the cumulative setting, expressed in terms of rewards, the higher the regret suffered in the identification problem, expressed in terms of probability of error.", "startOffset": 44, "endOffset": 65}, {"referenceID": 4, "context": "This intuition is formalized in (Bubeck et al., 2009) where the authors prove that no algorithm can achieve this without additional information.", "startOffset": 32, "endOffset": 53}, {"referenceID": 4, "context": "Our results therefore imply that the knowledge of \u03bc\u2217 by the learner is a sufficient information so that Theorem 1 of (Bubeck et al., 2009) does not hold anymore and there exists algorithms that solve both problems at the same time, as APT does.", "startOffset": 117, "endOffset": 138}, {"referenceID": 12, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 16, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 21, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 10, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 6, "context": "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).", "startOffset": 196, "endOffset": 320}, {"referenceID": 16, "context": "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).", "startOffset": 188, "endOffset": 270}, {"referenceID": 10, "context": "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).", "startOffset": 188, "endOffset": 270}, {"referenceID": 6, "context": "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).", "startOffset": 188, "endOffset": 270}, {"referenceID": 12, "context": "If the complexity H were also known to the learner, the strategy in (Gabillon et al., 2012) would attain a similar performance.", "startOffset": 68, "endOffset": 91}, {"referenceID": 10, "context": "For comparison, we use the following methods which include the state of the art CSAR algorithm of (Chen et al., 2014) and two minor adaptations of known methods that are also suitable for our problem.", "startOffset": 98, "endOffset": 117}, {"referenceID": 12, "context": "UCB-type algorithm: The algorithm UCBE given and analyzed in (Audibert & Bubeck, 2010) is designed for finding the best arm - and its heuristic is to pull the arm that maximizes a UCB bound - see also (Gabillon et al., 2012) for an adaptation of this algorithm to the general TopM problem.", "startOffset": 201, "endOffset": 224}, {"referenceID": 12, "context": "From the theoretical analysis in the paper (Audibert & Bubeck, 2010; Gabillon et al., 2012), it is not hard to see that setting a \u2248 (T \u2212K)/H minimizes their upper bound, and that this algorithm attains the same expected loss as ours - but it requires the knowledge of H .", "startOffset": 43, "endOffset": 91}, {"referenceID": 10, "context": "CSAR: As mentioned before, this method is given in (Chen et al., 2014).", "startOffset": 51, "endOffset": 70}], "year": 2016, "abstractText": "We study a specific combinatorial pure exploration stochastic bandit problem where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and for a fixed time horizon. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with fixed budget for which optimal strategies are constructed.", "creator": "LaTeX with hyperref package"}}}