{"id": "1605.08872", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2016", "title": "Online Bayesian Collaborative Topic Regression", "abstract": "collaborative topic regression ( ctr ) combines ideas of probabilistic matrix factorization ( pmf ) and topic modeling ( e. g., lda ) for recommender systems, which globally has gained increasing successes in many applications. despite enjoying many advantages, the existing ctr algorithms have some critical limitations. first of all, data they are often designed to work in within a batch learning manner, making them somewhat unsuitable to even deal with streaming data or big data in real - world recommender systems. second, the document - specific topic diagram proportions of lda graphs are fed to the anticipated downstream pmf, but not purely reverse, which is sub - optimal calculation as the rating information is not exploited necessary in discovering the low - dimensional representation of documents and thus can result in a sub - optimal representation usable for prediction. in this paper, we propose a novel scheme of online bayesian collaborative topic regression ( obctr ) algorithms which is efficient and greatly scalable for learning computation from data streams. particularly, we { \\ it jointly } to optimize the combined objective function of both pmf and lda in an online learning fashion, namely in which both pmf and lda tasks can be reinforced each other during the planned online learning activation process. our technologies encouraging experimental computation results conditional on real - world data analysis validate the clinical effectiveness of executing the proposed method.", "histories": [["v1", "Sat, 28 May 2016 10:17:37 GMT  (40kb)", "http://arxiv.org/abs/1605.08872v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["chenghao liu", "tao jin", "steven c h hoi", "peilin zhao", "jianling sun"], "accepted": false, "id": "1605.08872"}, "pdf": {"name": "1605.08872.pdf", "metadata": {"source": "CRF", "title": "Online Bayesian Collaborative Topic Regression", "authors": ["Chenghao Liu", "Tao Jin", "Steven C.H. Hoi", "Peilin Zhao", "Jianling Sun"], "emails": ["twinsken@zju.edu.cn,", "taoj@zju.edu.cn,", "chhoi@smu.edu.sg,", "zhaop@i2r.a-star.edu.sg,", "sunjl@zju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n08 87\n2v 1\n[ cs\n.L G\n] 2\n8 M\nay 2"}, {"heading": "1 Introduction", "text": "Collaborative Topic Regression (CTR) has been actively explored in recent years [Wang and Blei, 2011]. Instead of purely relying on Collaboretive Filgering(CF) approaches, CTR aims to leverages content-based techniques to overcome inaccurate and unreliable predictions with traditional CF methods due to data sparsity and other challenges. More specifically, CTR combines the idea of probabilistic matrix factorization (PMF) [Mnih and Salakhutdinov, 2007] for predicting ratings, and the idea of probabilistic topic modeling, e.g., Latent Dirchelet Allocation (LDA), for analyzing the content of items towards recommendation tasks. CTR has been shown as a promising method that produces more accurate and interpretable results and has been successfully applied in many recommender systems, such\nas tag recommendation [Wang et al., 2013; Lu et al., 2015], and social recommender systems [Purushotham et al., 2012; Kang and Lerman, 2013].\nDespite being studied actively [Wang and Blei, 2011; Wang et al., 2013], the existing CTR techniques suffer from several critical limitations. First of all, they are often designed to work in a batch mode learning fashion, by assuming that all text contents of items as well as the rating training data are given prior to the learning tasks. During the training process, both LDA and PMF models are usually trained separately in a batch training fashion. Such an approach would suffer from a huge scalability drawback when new data (users or items) may arrive sequentially and get updated frequently in a realworld online recommender system. Second, the existing CTR approach only leverages the content information to improve the CF tasks, but not reverse. The document-specific topic proportions of LDA are fed to the downstream PMF. This two-step procedure is rather suboptimal as the the rating information is not used in discovering the low-dimensional representation of documents, which is clearly not an optimal representation for prediction as the two methods are not tightly coupled to fully exploit their potential. Our work is motivated to explore more efficient, scalable, and effective techniques to maximize the potential exploiting extremes in dealing with data streams from real-world online recommender systems.\nTo overcome the limitations of traditional CTR, we propose a novel scheme of Online Bayesian Collaborative Topic Regression (OBCTR), which jointly optimizes a unified objective function by combining both PMF and LDA in an online learning fashion. In contrast to the original CTR model, OBCTR is able to achieve a much tighter coupling of both PMF and LDA, where both LDA and PMF tasks influence each other naturally and gradually via the joint optimization in the online learning process. This interplay yields item representations that are more suitable for making accurate and reliable rating prediction tasks.\nTo the best of our knowledge, the proposed OBCTR algorithm is the first online learning algorithm for solving CTR tasks with fully joint optimization of both LDA and PMF. Our encouraging results from extensive experiments on a large real-world data set show that the proposed online learning algorithms are scalable and effective, and the OBCTR technique not only outperforms the state-of-the-art methods for rating prediction tasks but also yields more suitable latent\ntopic proportions in topic modeling tasks. In the following, we first review some important related work, then present a formal formulation of CTR tasks and the novel Online Bayesian Collaborative Topic Regression algorithms. After that, we conduct extensive empirical studies and compare the proposed algorithms with the existing techniques, and finally set out our conclusions of this work."}, {"heading": "2 Related work", "text": "In this section, we review two groups of studies related to our work, including (1) variants of CTR models and (2) online Bayesian inference.\nVariants of CTR model: Researchers have extended CTR models to different applications of recommender systems. Some researchers extended CTR models by integrating with other side information. In CTRsmf [Purushotham et al., 2012], authors integrated CTR with social matrix factorization models to take social correlation between users into account. In LA-CTR [Kang and Lerman, 2013], they assumed that users divide their limited attention non-uniformly over other people. In HFT [McAuley and Leskovec, 2013], they aligned hidden factors in product ratings with hidden topics in product reviews for product recommendations. Some researchers extended CTR to other recommendation tasks. In CSTR [Ding et al., 2013], authors explored how to recommend celebrities to general users in the context of social network. In CTR-SR [Wang et al., 2013], authors adapted CTR model by combining both item-tag matrix and item content information for tag recommendation tasks. There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013]. However, all of these work follow the same parameter estimation scheme as [Wang and Blei, 2011] in a batch learning mode.\nOnline Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013]. Most of them have adopted stochastic approximation of posterior distribution by sub-sampling a given finite data set, which is unsuitable for many applications where data size is unknown in advance.\nTo relax this assumption, researchers in [Broderick et al., 2013; Ghahramani and Attias, 2000] made streaming updates to the estimated posterior. The intuition behind this idea is that we could treat the posterior after observing T \u2212 1 samples as the new prior for the incoming data points. Specifically, suppose the training data {ot}t\u22650 are generated i.i.d. according to a distribution p(o|x) and the prior p(x) is given. Bayes\u2019 theorem implies the posterior distribution of x given the first T samples (T \u2265 1) satisfies p(x|{o}Tt=0) \u221d p(x|{o} T\u22121 t=0 )p(oT |x). For complex models, we can use approximate inference methods to compute the posterior. For example, [Broderick et al., 2013] explored a mean-field variational Bayes algorithm for LDA inference. In\naddition, [McInerney et al., 2015] introduced the population Variational Bayes (PVB) method which combines traditional Bayesian inference with the frequentist idea of the population distribution for streaming inference. [Shi and Zhu, 2014] proposed the Online Bayesian Passive-Aggressive (BayesPA) method for max-margin Bayesian inference of online streaming data. The high scalability of the above methods motivates us to propose Online Bayesian inference for CTR models."}, {"heading": "3 Collaborative Topic Regression: Revisited", "text": "Suppose there are I users and J items. Each data sample is a 3-tuple (i, j, rij) where i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , I} is the user index, j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , J} is the item index and rij \u2208 R is the rating value assigned to item j by user i. We assume the rating data arrives sequentially in an online recommender system. Let R denote the whole rating samples and the collection of J items is regarded as a document set W = {wj}Jj=1. Let Z = {zj}Jj=1 and \u0398 = {\u03b8j} J j=1 denote all the topic assignments and topic proportions of each item. We represent users and items in a shared latent low-dimensional space of dimension K , which is equal to the number of topics, user i is represented by a latent vector ui \u2208 RK and item j by a latent vector vj \u2208 RK .\nFigure 1(a) shows the graphical model of CTR. Basically, the CTR model assumes that each item is generated by a topic model and additionally includes a latent variable \u01ebj which offsets the topic proportions \u03b8j when modeling the user\u2019s latent vector. This offset variable \u01ebj can capture the item preference of a particular user based on their ratings. Assume there are K topics \u03a6 = {\u03c6k}Kk=1. The generative process of the CTR model is as follows:\n1. For each user i, draw user latent vector ui \u223c N (0,\n1 \u03c32u IK)\n2. For each item j,\n(a) Draw topic proportions \u03b8j \u223c Dirichlet(\u03b1). (b) Draw item latent offset \u01ebj \u223c N (0, 1\u03c32\u01eb IK)) and set the item latent vector as vj = \u01ebj + \u03b8j .\n(c) For each word wjn(1 \u2264 n \u2264 Nj), i. Draw topic assignment zjn \u223c Mult(\u03b8j). ii. Draw word wjn \u223c Mult(\u03c6zjn).\n3. For each user-item pair (i, j), draw the rating rij \u223c N (uTi vj ,\n1 \u03c32r ).\nIn step 2 (c) ii. \u03c6zjn denotes the topic selected by the non-zero entry of zjn. The topics are random samples drawn from a prior, e.g., \u03c6k \u223c Dirichlet(\u03b2). Note that vj = \u01ebj + \u03b8j , where \u01ebj \u223c N (0, 1\u03c32\u01eb IK), is equivalent to vj \u223c N (\u03b8j , 1 \u03c32\u01eb IK). Given the document set W and rating data R, we let U = {ui}Ii=1,V = {vj} J j=1, the goal of CTR is to infer the posterior distribution p(U,V,Z,\u03a6,\u0398|W,R) \u221d p0(U,V,Z,\u03a6,\u0398)\np(W|Z,\u03a6)p(Z|\u0398)p(V|\u0398)p(R|U,V). (1) Because computing the full posterior of U,V,Z,\u03a6,\u0398 directly is intractable, CTR proposed a heuristic two-stage batch learning method for approximate inference . First,\nCTR approximately infers posterior p(Z,\u03a6,\u0398|W) of LDA model via variational inference method [Blei et al., 2003]. Then, it applies ALS algorithm 1 for learning the posterior p(U,V|R,\u0398) of PMF model by feeding the results of \u0398 in the first step. This batch learning approach only leverages the content information to improve the CF tasks, but not reverse and tends to get trapped into local optimum."}, {"heading": "4 Online Bayesian Collaborative Topic Regression", "text": "Before introducing our novel online parameter estimation method of Online Bayesian Collaborative Topic Regression (OBCTR), we first modify the graphical model of CTR as shown in Figure 1 to jointly optimize both LDA and PMF and adapt to our online learning method. It is worth noting that this minor modification 1(b) does not break the main structure of CTR, and our online parameter estimation method could be applied to the various variants of CTR introduced in Section 2.\nCTR depicts the generative process of vj with vj \u223c N (\u03b8j ,\n1 \u03c32\u01eb IK). In their parameter estimation method, topic\nproportions \u03b8j (result of LDA) provide features for vj in PMF, but information flow is one-way, which ignores that vj could provide feedback to guide the extraction of topic proportions \u03b8j (they estimates \u03b8j via traditional LDA algorithm which only based on W not vj ). To address this limitation, we first assume that the item latent vector vj is directly close to z\u0304j , where z\u0304j is a vector with element z\u0304j = 1 N \u2211N n=1 I(z k n = 1) and I is the indicator function that equals to 1 if predicate holds otherwise 0. In this way, vj can directly influence topic assignments zj during the procedure of inferring LDA model (variable zj plays a key role in LDA since other hidden variable \u03a6 and \u0398 depend on zj\n1CTR adopts the ALS algorithm [Hu et al., 2008] to solve an implicit feedback problem. In our context, we use the SGD algorithm [Koren et al., 2009] since ratings data are explicit.\nand we can easily derive the update rule of them based on zj). Second, we replace the generative process of p(vj |\u03b8j) with p(\u01ebj|z\u0304j ,vj), \u01ebj \u223c N (vj \u2212 z\u0304j , 1\u03c32\u01eb IK), as shown in Figure 1. In our setting, vj and z\u0304j are conditionally dependent, which means their probability of occurrence depends on either event\u2019s occurrence and allows two-way interation. In addition, instead of learning two point estimates of coefficients ui,vj , we take a more general Bayesian-style approach and learn the posterior distribution q(ui,vj) in an online method. For rating prediction, we take a weighted average over all the possible latent vectors ui and vj , or more precisely, an expectation of the prediction over q(ui,vj) which is defined as r\u0302ij , E[u \u22a4 i vj ].\nFinally, Algorithm 1 summarizes the detailed framework of the proposed OBCTR algorithm. At each round t, we receive data sample and update both the parameters of LDA and PMF. The following discusses the optimization and each step of the algorithm in detail.\nAlgorithm 1 The Online Bayesian CTR (OBCTR) Initialize U,V,Z randomly. for t = 1 to \u221e do\nReceive data sample (i, j, rij ,wj) Draw samples ztj from Eq. (8) DiscardB burn-in sweeps, use the rest samples to update ui,vj ,\u03a6 following Eq. (5),(6),(7)\nend for Output: U,V and Z\nNow, we propose our novel online parameter estimation method of Online Bayesian Collaborative Topic Regression (OBCTR) which is efficient and scalable for learning from data streams. Let us first review the objective function of CTR defined in (1), from a variational point of view, this posterior is identical to the solution of the following optimization\nproblem: min\nq(U,V,Z,\u03a6,\u0398) KL[q(U,V,Z,\u03a6,\u0398)\u2016p0(U,V,Z,\u03a6,\u0398))]\n\u2212 Eq[log p(W|Z,\u03a6)p(V|\u0398)p(R|U,V)]\ns.t. q(U,V,Z,\u03a6,\u0398) \u2208 P , (2) where KL(q\u2016p) is the Kullback-Leibler divergence, and P is the space of probability distributions. If we add the constant log p(W)p(R) to the objective, it is the minimization of KL(q(U,V,Z,\u03a6,\u0398)\u2016p(U,V,Z,\u03a6,\u0398|W,R)), which is similar with the variational formulation of original LDA [Blei et al., 2003]. Formally, we formulate our OBCTR model as the optimization problem below:\nmin q(U,V,Z,\u03a6,\u0398) KL[q(U,V,Z,\u03a6,\u0398)\u2016p0(U,V,Z,\u03a6,\u0398))]\n\u2212 Eq[log p(W|Z,\u03a6)p(\u01eb|Z,V)p(R|U,V)]\ns.t. q(U,V,Z,\u03a6,\u0398) \u2208 P , (3)\nwhere p(\u01eb|Z,\u0398) = \u220fJ\nj=1 p(\u01ebj |z\u0304j ,vj). Inspired by streaming Bayesian inference [Broderick et al., 2013; Ghahramani and Attias, 2000], on the arrival of new data (i, j, rij ,wj), if we treat the posterior after observing t \u2212 1 samples as the new prior, the post-data posterior distribution qt+1(ui,vj , zj ,\u03a6,\u0398) is equivalent to the solution of the following optimization problem:\nmin q KL[q(ui,vj , zj ,\u03a6,\u0398)\u2016qt(ui,vj , zj ,\u03a6,\u0398))]\n\u2212 Eq[log p(wj|zj ,\u03a6)p(\u01ebj |z\u0304j ,vj)p(rij |u \u22a4 i vj)]\ns.t. q(ui,vj , zj ,\u03a6,\u0398) \u2208 P . (4) This problem is intractable to compute. With the mean field assumption that q(ui,vj , zj) = q(ui)q(vj)q(zj), we can solve this problem via an iterative procedure that alternatively updates each factor distribution as follows in detail.\nFor ui: By fixing the distribution q(vj), we can ignore irrelevant terms and solve\nmin q(ui)\nKL[q(ui)q(vj)\u2016qt(ui)p(rij |u \u22a4 i vj)].\nThe optimal solution has the following closed form solution: qt+1(ui) \u221d qt(ui) exp(Eq(vj)[log p(rij |u \u22a4 i vj)]). If initial prior is normal q0(ui) = N (ui;m0ui,\u03a3 0 ui), by induction we can show that the inferred distribution at each round is also a normal distribution. Let us assume qt(ui) = N (ui;mtui,\u03a3 t ui). Then, we have\nqt+1(ui)\u221d exp(\u2212 1\n2 (ui \u2212m\nt ui) \u22a4(\u03a3tui) \u22121(ui \u2212m t ui)\n+Eq(vj)[\u2212 (ri,j \u2212 u\n\u22a4 i vj) 2\n2\u03c32r ])\n= N (ui;m \u2217 ui,\u03a3 \u2217 ui),\nwhere the posterior parameters are computed as\n\u03a3\u2217ui = ((\u03a3 t ui) \u22121 + mvjm\n\u22a4 vj\n\u03c32rIK )\u22121, (5)\nm \u2217 ui = m t ui +\nri,j \u2212m \u22a4 vjm t ui\n\u03c32r +m \u22a4 vj\u03a3 t uimvj\n\u03a3tuimvj .\nTo make it more efficient, we only update the diagonals of covariance matrix \u03a3\u2217ui.\nFor vj: The update rule of vj is similar to ui except adding a Gaussian distribution p(\u01ebj |z\u0304j ,vj), a constraint about the distance between vj and z\u0304j , that explains the dif-\nference between topic assignments in content and item preference based on ratings. By fixing the distribution of q(ui) and q(zj), we have the update rule qt+1(vj) \u221d qt(vj) exp(Eq(ui,zj)[log p(rij |u \u22a4 i vj)p(\u01ebj |z\u0304j ,vj)])\n\u221d exp(\u2212 1\n2 (vj \u2212m\nt vj) \u22a4(\u03a3tvj) \u22121(vj \u2212m t vj)\n+Eq(ui)q(zj)[\u2212 (ri,j \u2212 u\n\u22a4 i vj) 2\n2\u03c32r \u2212 (z\u0304j \u2212 vj)\n\u22a4(z\u0304j \u2212 vj)\n\u03c32\u01eb IK ])\n= N (vj ;m \u2217 vj ,\u03a3 \u2217 vj),\nwhere the posterior parameters are computed as\n\u03a3mix = (\u03a3 \u22121 vj +\n1\n\u03c32\u01eb )\u22121, (6)\n\u03a3\u2217vj = ((\u03a3 t vj) \u22121 + 1\n\u03c32\u01eb IK +\nmuim \u22a4 ui\n\u03c32rIK )\u22121,\nm \u2217 vj = \u03a3mix\u03a3 \u22121 vj m t vj + \u03a3mix\n1\n\u03c32\u01eb z\u0304j \u2212 \u03a3mix\n1\n\u03c32r mui\n( m\n\u22a4 ui\u03a3mix\u03a3 \u22121 vj m t vj +m \u22a4 ui\u03a3mix 1 \u03c32\u01eb z\u0304j \u2212 rij\n1 +m\u22a4ui\u03a3mix 1 \u03c32r\nmui ).\nBesides, we adopt the same strategy that only updating the diagonals of covariance matrix \u03a3\u2217vj .\nFor \u03a6 and \u0398: By fixing the distribution q(Z), the update rule for Dirichlet distribution \u03a6 and \u0398 is similar to the original LDA, that is,\n\u03b8jk = Ckj + \u03b1\u2211K\nk=1 C k j +K\u03b1\n, \u03a6kw = Cwk + \u03b2\u2211D\nw=1 C w k +D\u03b2\n, (7)\nwhere D is the vocabulary size, Ckj is the number of times that terms being associated with topic k within the j-th item, Cwk is the number of times the term w(1 \u2264 w \u2264 D) being assigned to topic k over the whole corpus.\nFor zj: Given the distribution of other variables, the conditional distribution of zj is: qt+1(zj |vj ,\u03a6,wj)\n\u221d qt(zj) exp(Eq(\u03a6)q(vj )[log p(wj|zj ,\u03a6)p(\u01ebj|z\u0304j ,vj)]) \u221d qt(zt) exp( \u2211\nn\u2208[Nj ]\n\u039bzjn,wjn \u2212 Eq(vj)[ (vj \u2212 z\u0304j)\n\u22a4(vj \u2212 z\u0304j)\n\u03c32\u01eb IK ])\nwhere \u039bzjn,wjn = Eq(\u03a6)[log(\u03a6zjn,wjn)]. We can do Gibbs sampling to infer q(zj) by canceling out common factors. This hybird strategy has shown promising performance for LDA [Mimno et al., 2012; Shi and Zhu, 2014]. Specifically, the conditional distribution of one varibale zjn (the topic assignment of the n-th word in item j ) given others zj\u00acn is q(zjn = k|zj\u00acn,vj ,\u03a6, wjn = w) (8)\n\u221d (\u03b1+ Ckj\u00acn) exp(\u039bk,wjn + 1\n2\u03c32\u01ebNj (2mvjk \u2212 1 + 2Ckj\u00acn Nj )),\nwhere zj\u00acn is the topic assignments in item j (except the nth word) and Ckj\u00acn is the number of words in item j (except the n-th word) that are assigned to topic k."}, {"heading": "5 Experimental Results", "text": "Our experiments were conducted on an extended MovieLens dataset, named as \u201cMovieLens-10M-Plot\u201d2, which was originated from the MovieLens 10M3. Specifically, the original\n2We will release the dataset after the paper is accepted. 3http://grouplens.org/datasets/movielens/\nMovieLens 10M dataset provides a total of 10,000,053 rating records for 10,681 movies (items) by 69,878 users. However, the original dataset has very limited text content information. We enrich the dataset by collecting additional text contents for each of the movie items. Specifically, for each movie item, we first used its identifier number to find the movie listed in the IMDb4 website, and then collected its related text of \u201cplot summary\u201d. We then combine the \u201cplot summary\u201d text together with each movie\u2019s title and category text given in the MovieLens-10M dataset as a text document to represent each movie. For detailed text preprocessing, we follow the same procedure as the one described in [Wang and Blei, 2011] to process text information. Finally, we form a vocabulary with 7,689 distinct words. Note that we did not consider the CiteUlike dataset 5 as used in the previous study [Wang and Blei, 2011], because their dataset only provides \u201clike\u201d and \u201cdislike\u201d preference, which is kind of implicit feedback and thus unsuitable for our regression task. By contrast, the MovieLens-10M dataset has explicit feedback with ratings ranging from 1 to 5."}, {"heading": "5.1 Experimental Setup and Metric", "text": "The dataset has more than 10-million rating records. For each experiment, we randomly shuffle the rating records, and then divide them into two parts: the first 90% of the shuffled rating records are used as the training data, and the rest 10% rating data are used as test set. We also randomly draw 5% out of the training data as the validation set for parameter selection. To make fair comparisons, all the algorithms are conducted over 5 experimental runs of different random permutations. For performance metric, we evaluate the performance of our proposed method for prediction task by measuring Root Mean Square Error (RMSE). In the online learning experiments, we evaluate the RMSE performance on the test set after every 50,000 online iterations. In addition, we also evaluate the performance of topic modeling via the log-likelihood of each word in text collection [Hoffman et al., 2010]."}, {"heading": "5.2 Baselines for Comparison and Experimental Settings", "text": "In our experiments, we evaluate the proposed OBCTR algorithms for rating predictions by comparing with some important baselines as follows:\n\u2022 PA-I: An online learning algorithm for solving online collaborative filtering tasks by applying the popular online Passive-Aggressive (PA) algorithm [Blondel et al., 2014];\n\u2022 CTR: the existing Collaborative Topic Regression [Wang and Blei, 2011] . In our context, we replace the ALS algorithm [Hu et al., 2008] with SGD algorithm [Koren et al., 2009] since ratings data are explicit, and keep the rest same as the original CTR (note that the LDA step is still performed in a batch manner);\n\u2022 OCTR: To evaluate the efficacy of joint optimization. We propose a simplified variant of OBCTR, named\n4http://www.imdb.com 5http://www.citeulike.org/faq/data.adp\n\u2022 OBCTR: The proposed Online Bayesian CTR algorithm in Algorithm 1.\nBesides, to evaluate the topic modeling performance, we also compare our method with the typical Online LDA method: \u2022 Online-LDA: an online Bayesian variational inference\nalgorithom for LDA model[Hoffman et al., 2010]. We take it as a baseline to evaluate how well the model fits the data with the predictive distribution. For parameter settings, we fix \u03b1 = K\u22121, \u03b2 = K\u22121 and find the optimal parameters for different algorithms (PA-I, CTR and OBCTR). Specifically, the parameters including c in PA-I, \u03c3u, \u03c3v and \u03c1 in CTR and OCTR, and \u03c3\u01eb and \u03c3r in OBCTR. All of these parameters are found by performing a grid search as follows: \u03c3\u01eb, \u03c3r \u2208 {0.5, 1, 2, 4, 8, 16, 32}, c \u2208 {0.01, 0.1, 0.2, 0.5, 1}, \u03c1 \u2208 {0.01, 0.05, 0.1, 0.2, 0.5}, \u03c3u, \u03c3v \u2208 {0.01, 0.02, 0.04, 0.08, 0.16, 0.32} and K \u2208 {5, 10, 20}."}, {"heading": "5.3 Evaluation of Online Rating Prediction Tasks", "text": "Figure 2(a),2(b),2(c) compares the online performance of the above methods in K = 5, K = 10 and K = 20. We note that the CTR method took at least 6 hours 6 to precompute the parameters \u0398 and \u03a6 by a batch variational inference algorithm. Figure 2 shows only its performance in the downstream collaborative filtering phase.\nAs we can see from Figure 2(a),2(b),2(c), the CTRbased approaches outperform the online CF algorithm (PAI) for most cases, which is in line the experiments in [Wang and Blei, 2011] and validates the efficacy of leveraging additional text information to improve the performance of PMF for online rating prediction tasks. Second, among different CTR-based approaches, the proposed OBCTR consistently outperforms the other algorithms for most cases. This validates the importance of jointly optimizing both online PMF and online LDA to achieve tight coupling of the two techniques. Moreover, it is interesting to find that the gap between the proposed OCTR variant and OBCTR tends to become more significant when K is smaller. We conjecture that this is because when K is small, the PMF performance is relatively inaccurate and thus including the joint optimization becomes more critical for enhancing the unreliable PMF prediction performance.\n6For the vanilla LDA inference method, a larger K value often needs more time for computation.\nFinally, Table 1 summarizes the final test-set RMSE results after finishing the whole online learning tasks (by a single pass over the training set). Similar observations can be found , in which OBCTR achieves the lowest RMSE result on the test set for rating prediction among all the algorithms. In addition, CTR has better performance than OCTR. This is because CTR directly takes the batch LDA results (precomputed \u0398 and \u03a6) as input for leveraging online PMF task, while online CTR may converge relatively slowly (without the tight coupling). This again shows that it is crucial for the joint optimization in OBCTR."}, {"heading": "5.4 Performance on Online Topic modeling Tasks", "text": "Figure 2(d) shows the results about online average predictive log likelihood for OBCTR and Online LDA. Online learning allows us to conduct a large-scale comparison. We can see that OBCTR exhibits consistently better performance than Online LDA, which ignores ratings information, regardless of how many topics we use. That is due to the utilization of rating information to discover the low-dimensional topic proportions, where OBCTR yields additional benefit on this task."}, {"heading": "5.5 Evaluation of Parameter Sensitivity", "text": "Figure 3(a) shows how RMSE is affected by the choice of two key parameters \u03c3\u01eb and \u03c3r in OBCTR.\nAs observed from Figure 3(a), at the beginning, increasing \u03c3\u01eb leads to decrease the RMSE quickly. After arriving some\noptimal value, increasing \u03c3\u01eb further may increase the RMSE gradually. Second, we found the optimal value of \u03c3\u01eb also largely depends on the setting of the parameter \u03c3r. When \u03c3r is smaller, the optimal value of \u03c3\u01eb is relatively smaller. However, after reaching the optimal value, the further performance changing becomes limited. This indicates that overall, it is relatively easy to choose a good value of \u03c3\u01eb given a fixed \u03c3r setting due to its less sensitivity in the range of optimal values. Our results were consistent to the similar phenomena observed in [Wang and Blei, 2011].\nFigure 3(b) demonstrates the effect of increasing model complexity K . This investigation is done by selecting the best achievable RMSE and log-likelihood during the grid parameter search process. As shown in the diagram, increasing the complexity of models (higherK values) leads to improvement of both RMSE and log-likelihood results. However, the gain of predictive performance is paid by a significant computational overhead for more complex models (as shown in Table 2). In a practical online recommender system, one may want to choose a proper value of K to balance the tradeoff between accuracy and computational efficiency."}, {"heading": "6 Conclusion", "text": "This paper investigated online learning algorithms for making Collaborative Topic Regression (CTR) techniques practical for real-world online recommender systems. Specifically, unlike CTR that loosely combines LDA and PMF, we propose a novel Online Bayesian CTR (OBCTR) algorithm which performs a joint optimization of both LDA and PMF to achieve a tight coupling. Our encouraging results showed that OBCTR converges much faster than the other competing algorithms in the online learning, and thus achieved the best prediction performance among all the compared algorithms. Our future work will analyze model interpretability and theoretical performance of the proposed algorithms."}], "references": [{"title": "the Journal of machine Learning research", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "venue": "3:993\u20131022,", "citeRegEx": "Blei et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics", "author": ["Mathieu Blondel", "Yotaro Kubo", "Naonori Ueda. Online passive-aggressive algorithms for non-negative matrix factorization", "completion"], "venue": "pages 96\u2013104,", "citeRegEx": "Blondel et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Tamara Broderick", "Nicholas Boyd", "Andre Wibisono", "Ashia C Wilson", "Michael I Jordan. Streaming variational bayes"], "venue": "pages 1727\u20131735,", "citeRegEx": "Broderick et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "pages 2612\u20132618", "author": ["Xuetao Ding", "Xiaoming Jin", "Yujia Li", "Lianghao Li. Celebrity recommendation with collaborative social topic regression. In Proceedings of the TwentyThird international joint conference on Artificial Intelligence"], "venue": "AAAI Press,", "citeRegEx": "Ding et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Stochastic collapsed variational bayesian inference for latent dirichlet allocation", "author": ["Foulds et al", "2013] James Foulds", "Levi Boyles", "Christopher DuBois", "Padhraic Smyth", "Max Welling"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Online variational bayesian learning", "author": ["Zoubin Ghahramani", "H Attias"], "venue": "Slides from talk presented at NIPS workshop on Online Learning,", "citeRegEx": "Ghahramani and Attias. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "In advances in neural information processing systems", "author": ["Matthew Hoffman", "Francis R Bach", "David M Blei. Online learning for latent dirichlet allocation"], "venue": "pages 856\u2013864,", "citeRegEx": "Hoffman et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "The Journal of Machine Learning Research", "author": ["Matthew D Hoffman", "David M Blei", "Chong Wang", "John Paisley. Stochastic variational inference"], "venue": "14(1):1303\u20131347,", "citeRegEx": "Hoffman et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "2008", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky. Collaborative filtering for implicit feedback datasets. In Data Mining"], "venue": "ICDM\u201908. Eighth IEEE International Conference on, pages 263\u2013272. IEEE,", "citeRegEx": "Hu et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "La-ctr: A limited attention collaborative topic regression for social media", "author": ["Jeon-Hyung Kang", "Kristina Lerman"], "venue": "arXiv preprint arXiv:1311.1247,", "citeRegEx": "Kang and Lerman. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Computer", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky. Matrix factorization techniques for recommender systems"], "venue": "(8):30\u201337,", "citeRegEx": "Koren et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Content-based collaborative filtering for news topic recommendation", "author": ["Zhongqi Lu", "Zhicheng Dou", "Jianxun Lian", "Xing Xie", "Qiang Yang"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Lu et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["Julian McAuley", "Jure Leskovec"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 165\u2013172. ACM,", "citeRegEx": "McAuley and Leskovec. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "The population posterior and bayesian inference on streams", "author": ["James McInerney", "Rajesh Ranganath", "David M Blei"], "venue": "arXiv preprint arXiv:1507.05253,", "citeRegEx": "McInerney et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse stochastic inference for latent dirichlet allocation", "author": ["David Mimno", "Matt Hoffman", "David Blei"], "venue": "arXiv preprint arXiv:1206.6425,", "citeRegEx": "Mimno et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Advances in neural information processing systems", "author": ["Andriy Mnih", "Ruslan Salakhutdinov. Probabilistic matrix factorization"], "venue": "pages 1257\u20131264,", "citeRegEx": "Mnih and Salakhutdinov. 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Collaborative topic regression with social matrix factorization for recommendation systems", "author": ["Sanjay Purushotham", "Yan Liu", "C-C Jay Kuo"], "venue": "arXiv preprint arXiv:1206.4684,", "citeRegEx": "Purushotham et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of The 31st International Conference on Machine Learning", "author": ["Tianlin Shi", "Jun Zhu. Online bayesian passive-aggressive learning"], "venue": "pages 378\u2013386,", "citeRegEx": "Shi and Zhu. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen. Deep content-based music recommendation"], "venue": "pages 2643\u20132651,", "citeRegEx": "Van den Oord et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "author": ["Chong Wang", "David M Blei. Collaborative topic modeling for recommending scientific articles"], "venue": "pages 448\u2013456. ACM,", "citeRegEx": "Wang and Blei. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "pages 2719\u20132725", "author": ["Hao Wang", "Binyi Chen", "Wu-Jun Li. Collaborative topic regression with social regularization for tag recommendation. In Proceedings of the TwentyThird international joint conference on Artificial Intelligence"], "venue": "AAAI Press,", "citeRegEx": "Wang et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Collaborative deep learning for recommender systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "arXiv preprint arXiv:1409.2944,", "citeRegEx": "Wang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Relational stacked denoising autoencoder for tag recommendation", "author": ["Hao Wang", "Xingjian Shi", "Dit-Yan Yeung"], "venue": "Twenty-Ninth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 20, "context": "Collaborative Topic Regression (CTR) has been actively explored in recent years [Wang and Blei, 2011].", "startOffset": 80, "endOffset": 101}, {"referenceID": 16, "context": "More specifically, CTR combines the idea of probabilistic matrix factorization (PMF) [Mnih and Salakhutdinov, 2007] for predicting ratings, and the idea of probabilistic topic modeling, e.", "startOffset": 85, "endOffset": 115}, {"referenceID": 21, "context": "CTR has been shown as a promising method that produces more accurate and interpretable results and has been successfully applied in many recommender systems, such as tag recommendation [Wang et al., 2013; Lu et al., 2015], and social recommender systems [Purushotham et al.", "startOffset": 185, "endOffset": 221}, {"referenceID": 12, "context": "CTR has been shown as a promising method that produces more accurate and interpretable results and has been successfully applied in many recommender systems, such as tag recommendation [Wang et al., 2013; Lu et al., 2015], and social recommender systems [Purushotham et al.", "startOffset": 185, "endOffset": 221}, {"referenceID": 17, "context": ", 2015], and social recommender systems [Purushotham et al., 2012; Kang and Lerman, 2013].", "startOffset": 40, "endOffset": 89}, {"referenceID": 9, "context": ", 2015], and social recommender systems [Purushotham et al., 2012; Kang and Lerman, 2013].", "startOffset": 40, "endOffset": 89}, {"referenceID": 20, "context": "Despite being studied actively [Wang and Blei, 2011; Wang et al., 2013], the existing CTR techniques suffer from several critical limitations.", "startOffset": 31, "endOffset": 71}, {"referenceID": 21, "context": "Despite being studied actively [Wang and Blei, 2011; Wang et al., 2013], the existing CTR techniques suffer from several critical limitations.", "startOffset": 31, "endOffset": 71}, {"referenceID": 17, "context": "In CTRsmf [Purushotham et al., 2012], authors integrated CTR with social matrix factorization models to take social correlation between users into account.", "startOffset": 10, "endOffset": 36}, {"referenceID": 9, "context": "In LA-CTR [Kang and Lerman, 2013], they assumed that users divide their limited attention non-uniformly over other people.", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "In HFT [McAuley and Leskovec, 2013], they aligned hidden factors in product ratings with hidden topics in product reviews for product recommendations.", "startOffset": 7, "endOffset": 35}, {"referenceID": 3, "context": "In CSTR [Ding et al., 2013], authors explored how to recommend celebrities to general users in the context of social network.", "startOffset": 8, "endOffset": 27}, {"referenceID": 21, "context": "In CTR-SR [Wang et al., 2013], authors adapted CTR model by combining both item-tag matrix and item content information for tag recommendation tasks.", "startOffset": 10, "endOffset": 29}, {"referenceID": 22, "context": "There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013].", "startOffset": 137, "endOffset": 202}, {"referenceID": 23, "context": "There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013].", "startOffset": 137, "endOffset": 202}, {"referenceID": 19, "context": "There were also several works that attempted to extract latent topic proportions of text information in CTR via deep learning techniques [Wang et al., 2014; Wang et al., 2015; Van den Oord et al., 2013].", "startOffset": 137, "endOffset": 202}, {"referenceID": 20, "context": "However, all of these work follow the same parameter estimation scheme as [Wang and Blei, 2011] in a batch learning mode.", "startOffset": 74, "endOffset": 95}, {"referenceID": 6, "context": "Online Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013].", "startOffset": 173, "endOffset": 264}, {"referenceID": 7, "context": "Online Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013].", "startOffset": 173, "endOffset": 264}, {"referenceID": 10, "context": "Online Bayesian Inference: Although the classical regime of online learning is based on decision theory, much progress has been made for developing online variational Bayes [Hoffman et al., 2010; Hoffman et al., 2013; Kingma and Welling, 2013; Foulds et al., 2013].", "startOffset": 173, "endOffset": 264}, {"referenceID": 2, "context": "To relax this assumption, researchers in [Broderick et al., 2013; Ghahramani and Attias, 2000] made streaming updates to the estimated posterior.", "startOffset": 41, "endOffset": 94}, {"referenceID": 5, "context": "To relax this assumption, researchers in [Broderick et al., 2013; Ghahramani and Attias, 2000] made streaming updates to the estimated posterior.", "startOffset": 41, "endOffset": 94}, {"referenceID": 2, "context": "For example, [Broderick et al., 2013] explored a mean-field variational Bayes algorithm for LDA inference.", "startOffset": 13, "endOffset": 37}, {"referenceID": 14, "context": "In addition, [McInerney et al., 2015] introduced the population Variational Bayes (PVB) method which combines traditional Bayesian inference with the frequentist idea of the population distribution for streaming inference.", "startOffset": 13, "endOffset": 37}, {"referenceID": 18, "context": "[Shi and Zhu, 2014] proposed the Online Bayesian Passive-Aggressive (BayesPA) method for max-margin Bayesian inference of online streaming data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "CTR approximately infers posterior p(Z,\u03a6,\u0398|W) of LDA model via variational inference method [Blei et al., 2003].", "startOffset": 92, "endOffset": 111}, {"referenceID": 8, "context": "CTR adopts the ALS algorithm [Hu et al., 2008] to solve an implicit feedback problem.", "startOffset": 29, "endOffset": 46}, {"referenceID": 11, "context": "In our context, we use the SGD algorithm [Koren et al., 2009] since ratings data are explicit.", "startOffset": 41, "endOffset": 61}, {"referenceID": 0, "context": "If we add the constant log p(W)p(R) to the objective, it is the minimization of KL(q(U,V,Z,\u03a6,\u0398)\u2016p(U,V,Z,\u03a6,\u0398|W,R)), which is similar with the variational formulation of original LDA [Blei et al., 2003].", "startOffset": 181, "endOffset": 200}, {"referenceID": 2, "context": "Inspired by streaming Bayesian inference [Broderick et al., 2013; Ghahramani and Attias, 2000], on the arrival of new data (i, j, rij ,wj), if we treat the posterior after observing t \u2212 1 samples as the new prior, the post-data posterior distribution qt+1(ui,vj , zj ,\u03a6,\u0398) is equivalent to the solution of the following optimization problem: min q KL[q(ui,vj , zj ,\u03a6,\u0398)\u2016qt(ui,vj , zj ,\u03a6,\u0398))]", "startOffset": 41, "endOffset": 94}, {"referenceID": 5, "context": "Inspired by streaming Bayesian inference [Broderick et al., 2013; Ghahramani and Attias, 2000], on the arrival of new data (i, j, rij ,wj), if we treat the posterior after observing t \u2212 1 samples as the new prior, the post-data posterior distribution qt+1(ui,vj , zj ,\u03a6,\u0398) is equivalent to the solution of the following optimization problem: min q KL[q(ui,vj , zj ,\u03a6,\u0398)\u2016qt(ui,vj , zj ,\u03a6,\u0398))]", "startOffset": 41, "endOffset": 94}, {"referenceID": 15, "context": "This hybird strategy has shown promising performance for LDA [Mimno et al., 2012; Shi and Zhu, 2014].", "startOffset": 61, "endOffset": 100}, {"referenceID": 18, "context": "This hybird strategy has shown promising performance for LDA [Mimno et al., 2012; Shi and Zhu, 2014].", "startOffset": 61, "endOffset": 100}, {"referenceID": 20, "context": "For detailed text preprocessing, we follow the same procedure as the one described in [Wang and Blei, 2011] to process text information.", "startOffset": 86, "endOffset": 107}, {"referenceID": 20, "context": "Note that we did not consider the CiteUlike dataset 5 as used in the previous study [Wang and Blei, 2011], because their dataset only provides \u201clike\u201d and \u201cdislike\u201d preference, which is kind of implicit feedback and thus unsuitable for our regression task.", "startOffset": 84, "endOffset": 105}, {"referenceID": 6, "context": "In addition, we also evaluate the performance of topic modeling via the log-likelihood of each word in text collection [Hoffman et al., 2010].", "startOffset": 119, "endOffset": 141}, {"referenceID": 1, "context": "\u2022 PA-I: An online learning algorithm for solving online collaborative filtering tasks by applying the popular online Passive-Aggressive (PA) algorithm [Blondel et al., 2014];", "startOffset": 151, "endOffset": 173}, {"referenceID": 20, "context": "\u2022 CTR: the existing Collaborative Topic Regression [Wang and Blei, 2011] .", "startOffset": 51, "endOffset": 72}, {"referenceID": 8, "context": "In our context, we replace the ALS algorithm [Hu et al., 2008] with SGD algorithm [Koren et al.", "startOffset": 45, "endOffset": 62}, {"referenceID": 11, "context": ", 2008] with SGD algorithm [Koren et al., 2009] since ratings data are explicit, and keep the rest same as the original CTR (note that the LDA step is still performed in a batch manner);", "startOffset": 27, "endOffset": 47}, {"referenceID": 6, "context": "Table 1: RMSE results after a single pass over training set OCTR, which runs online LDA [Hoffman et al., 2010] for LDA part and SGD for PMF part (but without joint optimization as OBCTR) sepearately.", "startOffset": 88, "endOffset": 110}, {"referenceID": 6, "context": "Besides, to evaluate the topic modeling performance, we also compare our method with the typical Online LDA method: \u2022 Online-LDA: an online Bayesian variational inference algorithom for LDA model[Hoffman et al., 2010].", "startOffset": 195, "endOffset": 217}, {"referenceID": 20, "context": "As we can see from Figure 2(a),2(b),2(c), the CTRbased approaches outperform the online CF algorithm (PAI) for most cases, which is in line the experiments in [Wang and Blei, 2011] and validates the efficacy of leveraging additional text information to improve the performance of PMF for online rating prediction tasks.", "startOffset": 159, "endOffset": 180}, {"referenceID": 20, "context": "Our results were consistent to the similar phenomena observed in [Wang and Blei, 2011].", "startOffset": 65, "endOffset": 86}], "year": 2016, "abstractText": "Collaborative Topic Regression (CTR) combines ideas of probabilistic matrix factorization (PMF) and topic modeling (e.g., LDA) for recommender systems, which has gained increasing successes in many applications. Despite enjoying many advantages, the existing CTR algorithms have some critical limitations. First of all, they are often designed to work in a batch learning manner, making them unsuitable to deal with streaming data or big data in real-world recommender systems. Second, the document-specific topic proportions of LDA are fed to the downstream PMF, but not reverse, which is sub-optimal as the rating information is not exploited in discovering the lowdimensional representation of documents and thus can result in a sub-optimal representation for prediction. In this paper, we propose a novel scheme of Online Bayesian Collaborative Topic Regression (OBCTR) which is efficient and scalable for learning from data streams. Particularly, we jointly optimize the combined objective function of both PMF and LDA in an online learning fashion, in which both PMF and LDA tasks can be reinforced each other during the online learning process. Our encouraging experimental results on real-world data validate the effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}