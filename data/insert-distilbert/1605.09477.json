{"id": "1605.09477", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "abstract": "this paper proposes using cf - nade, a neural autoregressive architecture for collaborative filtering ( cf ) specific tasks, which is inspired by the restricted boltzmann machine ( if rbm ) based cf model ) and the neural ensemble autoregressive distribution estimator ( nade ). we first describe the basic cf - nade model for cf tasks. then we propose to progressively improve the model by sharing parameters between different ratings. a factored version of cf - augmented nade is also proposed for better scalability. furthermore, we take the ordinal nature of the preferences into consideration outright and propose an ordinal cost to largely optimize cf - nade, which shows dramatically superior performance. finally, cf - nade can indeed be extended to a broader deep learning model, with only an moderately increased computational complexity. experimental results show that cf - enhancing nade with addressing a single virtual hidden layer explicitly beats all previous typical state - of - the - art methods on movielens version 1m, movielens 10m, and netflix datasets, and adding more hidden detection layers can further improve the performance.", "histories": [["v1", "Tue, 31 May 2016 03:07:06 GMT  (282kb,D)", "http://arxiv.org/abs/1605.09477v1", "Accepted by ICML2016"]], "COMMENTS": "Accepted by ICML2016", "reviews": [], "SUBJECTS": "cs.IR cs.LG stat.ML", "authors": ["yin zheng", "bangsheng tang", "wenkui ding", "hanning zhou"], "accepted": true, "id": "1605.09477"}, "pdf": {"name": "1605.09477.pdf", "metadata": {"source": "META", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "authors": ["Yin Zheng", "Bangsheng Tang", "Wenkui Ding", "Hanning Zhou"], "emails": ["YIN.ZHENG@HULU.COM", "BANGSHENG@HULU.COM", "WENKUI.DING@HULU.COM", "ERIC.ZHOU@HULU.COM"], "sections": [{"heading": "1. Introduction", "text": "Collaborative filtering (CF) is a class of methods for predicting a user\u2019s preference or rating of an item, based on his/her previous preferences or ratings and decisions made by similar users. CF lies at the core of most recommender systems and has attracted increasing attention along with the recent boom of e-commerce and social network systems. The premise of CF is that a person\u2019s preference does not change much over time. A good CF algorithm helps a user discover products or services that suit his/her taste efficiently.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nGenerally speaking, there is a dichotomy of CF methods: Memory-based CF and Model-based CF. Memorybased CF usually computes the similarities between users or items directly from the rating data, which are then used for recommendation. The explainatbility of the recommended results as well as the easy-to-implement nature of memory-based CF ensured its popularity in early recommender systems (Resnick et al., 1994). However, memorybased CF has faded out due to its poor performance on reallife large-scale and sparse data.\nDistinct from memory-based CF, model-based CF learns a model from historical data and then uses the model to predict preferences of users. The models are usually developed with machine learning algorithms, such as Bayesian networks, clustering models and latent semantic models. Complex preference patterns can be recognized by these models, allowing model-based CF to perform better for preference prediction tasks. Among all these models, matrix factorization is most popular and successful, c.f. (Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).\nWith the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention. A prominent example is RBMbased CF (RBM-CF) (Salakhutdinov et al., 2007). RBMCF is a two-layer undirected generative graph model which generalizes Restricted Boltzmann Machine (RBM) to modeling the distribution of tabular data, such as user\u2019s ratings of movies. RBM-CF has shown its power in Netflix prize challenge. However, RBM-CF suffers from inaccuracy and impractically long training time, since training RBM-CF is intractable and one has to rely on variational approximation or MCMC sampling.\nRecently, a good alternative to RBM has been proposed by Larochelle & Murray (2011). The so-called Neural Autoregressive Distribution Estimator (NADE) is a tractable distribution estimator for high dimensional binary vectors. NADE computes the conditional probabilities of each el-\nar X\niv :1\n60 5.\n09 47\n7v 1\n[ cs\n.I R\n] 3\n1 M\nay 2\n01 6\nement given the other elements to its left in the binary vector, where all conditionals share the same parameters. The probability of the binary vector can then be obtained by taking the product of these conditionals. Unlike RBM, NADE does not incorporate any latent variable where expensive inference is required, in constrast it can be optimized efficiently by backpropagation. NADE together with its variants achieved competitive results on many machine learning tasks (Larochelle & Lauly, 2012; Uria et al., 2013; Zheng et al., 2014b; Uria et al., 2014; Zheng et al., 2014a; 2015).\nIn this paper, we propose a novel model-based CF approach named CF-NADE, inspired by RBM-CF and NADE models. Specifically, we will show how to adapt NADE to CF tasks and describe how to improve the performance of CFNADE by encouraging the model to share parameters between different ratings. We also propose a factored version of CF-NADE to deal with large-scale dataset efficiently. As Truyen et al. (2009) observed, preference usually has the ordinal nature: if the true rating of an item by a user is 3 stars in a 5-star scale, then predicting 4 stars is preferred to predicting 5 stars. We take this ordinal nature of preferences into consideration and propose an ordinal cost to optimize CF-NADE. Moreover, we propose a deep version of CF-NADE, which can be optimized efficiently. The performance of CF-NADE is tested on 3 real world benchmarks: MovieLens 1M, MovieLens 10M and Netflix dataset. Experimental results show that CF-NADE outperforms all previous state-of-the-art methods."}, {"heading": "2. Related Work", "text": "As mentioned previously, some of the most successful model-based CF methods are based on matrix factorization (MF) techniques, where a prevalent assumption is that the partially observed matrix is of low rank. In general, MF characterizes both users and items by vectors of latent factors, where the number of factors is much smaller than the number of users or items, and the correlation between user and item factor vectors are used for recommendation tasks. Specifically, Billsus & Pazzani (1998) proposed to apply Singular Value Decomposition (SVD) to CF tasks, which is an early work on MF-based CF. Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD. Salakhutdinov & Mnih (2008) proposed a Bayesian treatment of PMF, which can be trained efficiently by MCMC methods. Along this line, Lawrence & Urtasun (2009) proposed a non-linear PMF using Gaussian process latent variable models. There\nare other MF-based CF methods such as (Rennie & Srebro, 2005; Mackey et al., 2011). Recently, Poisson Matrix Factorization (Gopalan et al., 2014b;a; 2013) was proposed, replacing Gaussian assumption of PMF by Poisson distribution. Lee et al. (2013) extended the low-rank assumption by embedding locality into MF models and proposed Local Low-Rank Matrix Approximation (LLORMA) method, which achieved impressive performance on several public benchmarks.\nAnother line of model-based CF is based on neural networks. With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items. Truyen et al. (2009) also extended the standard BM model so as to exploit the ordinal nature of ratings. Recently, Dziugaite & Roy (2015) proposed Neural Network Matrix Factorization (NNMF), where the inner product between the vectors of users and items in MF is replaced by a feed-forward neural network. However, NNMF does not produce convincing results on benchmarks.\nOur proposed method CF-NADE, can be generally categorized as a neural network based CF method. CF-NADE bears some similarities with NADE (Larochelle & Murray, 2011) in that both model vectors with neural autoregressive architectures. The crucial difference between CFNADE and NADE is that CF-NADE is designed to deal with vectors of variable length, while NADE can only deal with binary vectors of fixed length. Though DocNADE (Larochelle & Lauly, 2012) does take inputs with various lengths, it is designed to model unordered sets of words where each element of the input corresponds to a word, while CF-NADE models user rating vectors, where each element corresponds to the rating to a specific item."}, {"heading": "3. NADE for Collaborative Filtering", "text": "This section devotes to CF-NADE, a NADE-based model for CF tasks. Specifically, we describe the basic model of CF-NADE in Section 3.1, and propose to improve CFNADE by sharing parameters between different ratings in\nSection 3.2. At last, a factored version of CF-NADE is described in Section 3.3 to deal with large-scale datasets."}, {"heading": "3.1. The Model", "text": "Suppose that there are M items, N users, and the ratings are integers from 1 to K (K-star scale). One practical and prevalent assumption in CF literature is that a user usually rated D items1, where D M . To tackle sparsity, similar to RBM-CF (Salakhutdinov et al., 2007), we use a different CF-NADE model for each user and all these models share the same parameters. Specifically, all models have the same number of hidden units, but a user-specific model only has D visible units if the user only rated D items. Thus, each CF-NADE has only one single training case, which is a vector of ratings that a user gave to his/her viewed items, but all the weights and biases of these CF-NADE\u2019s are tied.\nIn this paper, we denote the training case for user u as ru = (rumo1 , r u mo2 , . . . , rumoD ), where o is a D-tuple in the set of permutations of (1, 2, . . . , D) which serves as an ordering of the rated items mi \u2208 {1, 2, . . . ,M}, and rumoi\n\u2208 {1, 2, . . . ,K} denotes the rating that the user gave to item moi . For simplicity, we will omit the index u of r\nu, and focus on a single user-specific CF-NADE in the rest of the paper.\nCF-NADE models the probability of the rating vector r by the chain rule as:\np (r) = D\u220f i=1 p ( rmoi |rmo<i ) (1)\nwhere rmo<i = (rmo1 , rmo2 , . . . , rmoi\u22121 ) denotes the first i\u2212 1 elements of r indexed by o.\nSimilar to NADE (Larochelle & Murray, 2011), CF-NADE models the conditionals in Equation 1 with neural networks. To compute the conditionals in Equation 1, CFNADE first computes the hidden representation of dimension H given rmo<i as follows:\nh ( rmo<i ) = g c+\u2211 j<i W rmoj :,moj  (2) where g(\u00b7) is the activation function, such as tanh(x) = exp(x)\u2212exp(\u2212x) exp(x)+exp(\u2212x) , W\nk \u2208 RH\u00d7M is the connection matrix associated with rating k, Wk:,j \u2208 RH is the jth column of Wk and W ki,j is an interaction parameter between the i th hidden unit and item j with rating k, c \u2208 RH is the bias term.\n1D might vary between different users\nThen the conditionals in Equation 1 could be modeled as:\np ( rmoi = k|rmo<i ) =\nexp ( skmoi ( rmo<i )) \u2211K\nk\u2032=1 exp ( sk\u2032moi ( rmo<i )) (3)\nwhere skmoi (rmo<i ) is the score indicating the preference that the user gave rating k for item moi given the previous ratings rmo<i , and s k moi (rmo<i ) is computed as,\nskmoi ( rmo<i ) = bkmoi +Vkmoi ,: h ( rmo<i ) (4)\nwhere Vk \u2208 RM\u00d7H and bk \u2208 RM are the connection matrix and the bias term associated with rating k, respectively.\nCF-NADE is optimized for minimum negative loglikelihood of p (r) (Equation (3)),\n\u2212 log p (r) = \u2212 D\u2211 i=1 log p ( rmoi |rmo<i ) (5)\naveraged over all the training cases. As in NADE, the ordering o in CF-NADE must be predefined and fixed during training for each user. Ideally, the ordering should follow the timestamps when the user gave the ratings. In practice, we find that a ordering that is randomly drawn from the set of permutations of (1, 2, . . . , Du) for each user u yields good results. As Uria et al. (2014) observed, we can think of the models trained with different orderings as different instantiations of CF-NADE for the same user. Section 5 will discuss how to (virtually) train a factorial number of CF-NADE\u2019s with different orderings simultaneously, which is the key to extend CF-NADE to a deep model efficiently.\nOnce the model is trained, given a user\u2019s past behavior r = (rmo1 , rmo2 , . . . , rmoD ), the user\u2019s rating of a new itemm \u2217 can be predicted as\nr\u0302m\u2217 = Ep(rm\u2217=k|r) [k] (6)\nwhere conditional p (rm\u2217 = k|r) are computed by Equation 3 along with the hidden representation h (r) and score skm\u2217 (r) computed as\nskm\u2217 (r) = b k m\u2217 +V k m\u2217,:h (r) (7)\nh (r) = g c+ D\u2211 j=1 W rmoj :,moj  . (8)"}, {"heading": "3.2. Sharing Parameters Between Different Ratings", "text": "In Equations 2 and 4, the connection matrices Wk, Vk and the bias bk are different for different ratings k\u2019s. In other words, CF-NADE uses different parameters for different\nratings. In practice, for a specific item, some ratings can be much more often observed than others. As a result, parameters associated with a rare rating might not be sufficiently optimized. To alleviate this problem, we propose to share parameters between different ratings of the same item.\nParticularly, we propose to compute the hidden representation h(rmo<i ) as follows:\nh ( rmo<i ) = g c+\u2211 j<i rmoj\u2211 k=1 Wk:,moj  (9) Note that, given an item moj rated rmoj by the user, h(rmo<i ) depends on all the weights W\nk, \u2200k \u2264 rmoj . Thus, Equation 9 encourages a solution that Wt is utilized by all the ratings k, \u2200k \u2265 t.\nSimilarly, the score skmoi (rmo<i ) in Equation 3 is adjusted as\nskmoi ( rmo<i ) = \u2211 j\u2264k ( bjmoi +Vjmoi ,: h ( rmo<i )) (10)\nwhere Vj and bj are shared by the rating k, where k \u2265 j.\nSharing parameters between different ratings can again be understood as a kind of regularization, which encourages the model to use as many parameters as possible to explain the data. Experimental results in Section 6.2.1 confirm the advantage of this regularization."}, {"heading": "3.3. Dealing with Large-Scale Datasets", "text": "One disadvantage of CF-NADE we have described so far is that the parameterization of Wk \u2208 RH\u00d7M and Vk \u2208 RM\u00d7H , where k ranges from 1 to K, will result in too many free parameters, especially when dealing with massive datasets. For example, for the Netflix dataset (Bennett & Lanning, 2007), when H = 500, the number of free parameters by Wk and Vk would be around 89 million2. Although severe overfitting can be avoided by proper weightdecay or dropout (Srivastava et al., 2014), learning such a huge network would still be problematic.\nInspired by RBM-CF (Salakhutdinov et al., 2007) and FixationNADE (Zheng et al., 2014a), we propose to address this problem by factorizing Wk and Vk into products of two lower-rank matrices. Particularly,\nW ki,m = J\u2211 j=1 Bi,jA k j,m (11)\nV km,i = J\u2211 j=1 P km,jQj,i (12)\n2Netflix dataset contains 17770 movies (M = 17770) and the ratings are 5-star scale (K = 5). Thus, the number of free parameters from Wk and Vk is 88850000 = 2\u00d717770\u00d75\u00d7500.\nwhere Ak \u2208 RJ\u00d7M , Pk \u2208 RM\u00d7J , B \u2208 RH\u00d7J and Q \u2208 RJ\u00d7H are lower-rank matrices with J H and J M . For example, by setting J = 50, the number of free parameters for W and V decreases from 89 million to about 9 million. In our experiments, this factored version of CF-NADE will be applied on large-scale datasets."}, {"heading": "4. Traing CF-NADE with Ordinal Cost", "text": "CF-NADE can be trained by minimizing the negative loglikelihood based on conditionals defined by Equation 3. To go one step further, following Truyen et al. (2009), we take the ordinal nature of a user\u2019s preference into consideration. That is, if a user rated an item k, the preference of the user to the ratings from 1 to k should increase monotonically and the preference to the ratings from k to K should decrease monotonically. The basic CF-NADE treats different ratings as separate labels, leaving the ordinal information not captured. Here we describe how to equip CF-NADE with an ordinal cost.\nFormally, suppose rmoi = k, the ranking of preferences over all the possible ratings under the ordinal assumption can be expressed as:\nk k \u2212 1 . . . 1 (13) k k + 1 . . . K (14)\nwhere k k \u2212 1 denotes the preference of rating k over k \u2212 1, k \u2208 {1, 2, . . . ,K}3. Two rankings of ratings, ydown and yup, can be induced by Equation 13 and 14:\nydown = (k, k \u2212 1, . . . , 1) (15) yup = (k, k + 1, . . . ,K) (16)\nNote that maximizing the conditional p(rmoi = k|rmo<i ) in Equation 3 only ensures that the probability of rating k is the largest among all possible ratings. To capture the ordinal nature induced by Equations 13 and 14, we propose to compute the conditional p(rmoi = k|rmo<i ) as\np ( rmoi = k|rmo<i ) =\n1\u220f j=k exp(sjmoi )\u2211j t=1 exp(s t moi ) K\u220f j=k exp(sjmoi )\u2211K t=j exp(s t moi ) (17)\nwhere sjmoi is a shorthand for the score s j moi (rmo<i ) introduced in Section 3.1, which indicates the preference to rating k of item moi given the previous context rmo<i .\nBoth two products in Equation 17 can be interpreted as the likelihood loss introduced in (Xia et al., 2008) in the\n3Equation 13 is omitted if the true rating is 1; likewise, Equation 14 is omitted if the true rating is K.\ncontext of Listwise Learning To Rank problem. Actually, from the perspective of learning-to-rank, CF-NADE acts as a ranking function which produces rankings of ratings based on previous ratings, where sjmoi (rmo<i ) corresponds to the score function in (Xia et al., 2008) and the rankings, ydown and yup, corresponds to true rankings that we would like CF-NADE to fit. Thus, the conditional computed by Equation 17 is actually the conditional distribution of the rankings ydown and yup given previous i \u2212 1 ratings. Put differently, the ranking loss in Equation 17 is defined on the ratings, while other learning-to-rank based CF methods, such as (Shi et al., 2010), are on items, which is the crucial difference.\nFor the rest of the paper, we denote the negative loglikelihood based on the conditionals computed by Equation 17 as ordinal cost Cord, and denote the negative loglikelihood based on Equation 3 as regular cost Creg. The final cost to optimize the model is then defined as\nChybrid = (1\u2212 \u03bb)Creg + \u03bbCord (18)\nwhere \u03bb is the hyperparameter to determine the weight of Cord. The impact of the hyperparameter \u03bb on the performance of CF-NADE is discussed in Section 6.2.1."}, {"heading": "5. Extending CF-NADE to a Deep Model", "text": "So far we have described CF-NADE with single hidden layer. As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance. Recently, Uria et al. (2014) proposed an efficient deep extension to original NADE (Larochelle & Murray, 2011) for binary vector observations, which inspires other related deep model (Zheng et al., 2015). Following (Uria et al., 2014), we propose a deep variant of CF-NADE.\nAs mentioned in Section 3.1, a different CF-NADE model is used for each user and the ordering o in r is stochastically sampled from the set of permutations of (1, 2, . . . , D). Training CF-NADE on stochastically sampled orderings corresponds, in expectation, to minimizing the cost in Equation 18 over all possible orderings for each user. As noticed by Uria et al. (2014) and Zheng et al. (2015), training over all possible orderings for CF-NADE implies that for any given context rmo<i , the model performs equally well at predicting all the remaining items in rmo\u2265i , since for each item there is an ordering such that it appears at position i. This is the key observation to extend CF-NADE to a deep model. Specifically, instead of sampling a complete ordering over all the D items, we instead sample a context rmo<i and perform an update of the conditionals using that context.\nThe procedure is done as follows. Given a user who has ratedD items, an ordering o is first sampled randomly from the set of permutations of (1, 2, . . . , D) for each update and a vector r = (rmo1 , rmo2 , . . . , rmoD ) is generated according to the ordering o. Then a split point i is randomly drawn from {1, 2, . . . , D} for each update. The split point i divides r into two parts: rmo<i and rmo\u2265i . According to the analysis above, in the new training procedure, rmo<i is considered as the input of CF-NADE and the training objective is to maximize conditionals p(rmoj |rmo<i ) for each element in rmo\u2265i . The cost function with this procedure is\nC = D D \u2212 i+ 1 \u2211 j\u2265i \u2212 log p ( rmoj |rmo<i ) . (19)\nBy Equation 19, the model predicts the ratings of each items after the splitting position i in the randomly drawn ordering o as if it were actually at position i. The factors in front of the sum come from the fact that the total number of elements in the sum is D and that we are averaging over D\u2212 i+1 possible choices for the item at position i, similar to (Uria et al., 2014) and (Zheng et al., 2015). Derivation of Equation 19 can be found in the supplementary materials.\nIn this procedure, a training update relies only on a single hidden representation h(rmo<i ), more hidden layers can be added into CF-NADE with the computational complexity increased moderately. Particularly, suppose h(1)(rmo<i ) is the hidden representation computed in Equation 2. Then new hidden layers can be added as in a regular deep feedforward neural network:\nh(l) ( rmo<i ) = g ( c(l) +W(l)h(l\u22121) ( rmo<i )) (20)\nfor l = 2, . . . , L, where L is the total number of hidden layers. Then the conditionals p(rmoj = k|rmo<i ), either in Equation 3 or Equation 17, can be computed from h(L)(rmo<i ). To this end, the number of operations a CFNADE takes for one input is O(KD\u0302H+H2L), as in regular multiple layers neural networks, where D\u0302 is the average number of ratings for a user and H is the number of hidden units for each layer. We denote C\u0303reg and C\u0303ord as the cost functions of Equation 19 associated with conditionals computed by Equation 3 and Equation 17, respectively.\nFinally, similar to Equation 18, we can also define a hybrid cost C\u0303hybrid as\nC\u0303hybrid = (1\u2212 \u03bb)C\u0303reg + \u03bbC\u0303ord (21)\nNote that CF-NADE with a single hidden layer can also be trained by Equation 19. In practice, Equation 19 can be implemented efficiently on GPUs, hence we use it throughout our experiments for either one hidden layer or multiplelayers architecture."}, {"heading": "6. Experiments", "text": "In this section, we test the performance of CF-NADE on 3 real-world benchmarks: MovieLens 1M, MovieLens 10M (Harper & Konstan, 2015) and Netflix dataset (Bennett & Lanning, 2007), which contain 106, 107 and 108 ratings, respectively. Following LLORMA (Lee et al., 2013) and AutoRec (Sedhain et al., 2015), 10% of the ratings in each of these datasets are randomly selected as the test set, leaving the remaining 90% of the ratings as the training set. Among the ratings in the training set, 5% are used as validation set. We use a default rating of 3 for items without training observations. Prediction error is measured by Root Mean Squared Error (RMSE),\nRMSE =\n\u221a\u2211S i=1(ri \u2212 r\u0303i)2\nS (22)\nwhere ri is the ith true rating and r\u0303i is the predicted rating by the model, S is the total number of ratings in the testset. We report the average RMSE on test set over 5 different splits and compare CF-NADE with some strong baselines including LLORMA, AutoRec, and other competitive methods. Experimental results show that CF-NADE outperforms the state-of-the-art performance on these benchmarks."}, {"heading": "6.1. Datasets Description", "text": "MovieLens 1M dataset contains around 1 million anonymous ratings of approximately 3900 movies by 6040 users, where each user rated at least 20 items. The ratings in MovieLens 1M dataset are made on a 5-star scale, with 1-star increments. MovieLen 10M dataset contains about 10 million ratings of 10681 movies by 71567 users. The users of MovieLens 10M dataset are randomly chosen and each user rated at least 20 movies. Unlike MovieLens 1M dataset, the ratings in MovieLens 10M are on a 5-star scale with half -star increments. Thus, the number of rating scales of MovieLens 10M is actually 10. In our experiments, we rescale the ratings in MovieLens 10M to 10-star scale with 1-star increments. Netflix dataset comes from the Netflix challenge prize4. It is massive compared to the previous two, which contains more than 100 million ratings of 17770 movies by 480189 users. The ratings of Netflix dataset are on 5-star scale, with 1-star increments."}, {"heading": "6.2. Experiments on MovieLen 1M Dataset", "text": "In this section, we test the performance of CF-NADE on MovieLen 1M dataset. We first evaluate the performance of\n4The test set of Netflix prize challenge dataset is not available now. Following Lee et al. (2013) and Sedhain et al. (2015), we split the available trainset of Netflix dataset into train, valid and test sets.\nthe ordinal cost described in Section 4 with/without sharing parameters between different ratings as described in Section 3.2. Then we compare several variants of CF-NADE with some strong baselines."}, {"heading": "6.2.1. THE PERFORMANCE OF THE ORDINAL COST", "text": "In this section, we evaluate the impact of the ordinal weight \u03bb in Equation 21 on the performance of CF-NADE. As Sedhain et al. (2015) mentioned, item-based CF outperforms user-based CF, therefore we use item-based CF-NADE (ICF-NADE) in this section. Distinct from user-based CFNADE (U-CF-NADE), which builds a different model for each user as we described previously, I-CF-NADE model builds a different CF-NADE model for each item. In other words, the only difference between U-CF-NADE and I-CFNADE is that the roles of users and items are switched. Comparison between U-CF-NADE and I-CF-NADE can be found in Section 6.2.2.\nThe configuration of the experiments is as follows. We use a single hidden layer architecture and the number of hidden units is set to 500, same as AutoRec (Sedhain et al., 2015) and LLORMA (Lee et al., 2013). Adam (Kingma & Ba, 2014) with default parameters (b1 = 0.1,b2 = 0.001 and = 10\u22128) are utilized to optimize the cost function in Equation 19. The learning rate is set to 0.001 , the weight decay is set to 0.015 and we use the tanh activation function.\nLet CF-NADE-S denote the variant of CF-NADE model where parameters are shared between different ratings, as described in Section 3.2. Figure 1 shows the superior performance of CF-NADE and CF-NADE-S w.r.t different values of \u03bb. Effectiveness of parameter sharing and ordinal cost can be justified by observing that: 1) CF-NADE-S always outperforms regular CF-NADE; 2) as the ordinal weight \u03bb increases, test RMSE of both CF-NADE and CFNADE-S decrease monotonically. Based on these observations, we will use CF-NADE-S and fix \u03bb = 1 throughout the rest of the experiments."}, {"heading": "6.2.2. COMPARING WITH STRONG BASELINES ON", "text": "MOVIELENS 1M\nIn this comparison, we compare CF-NADE with other baselines on MoiveLens 1M dataset. During the comparison, the learning rate is chosen on the validation set by cross-validation among {0.001, 0.0005, 0.0002}, and the weight decay is chosen among {0.015, 0.02}. According to Section 6.2.1, the weight \u03bb of ordinal cost is fixed to 1 and CF-NADE-S is adopted. The model is trained with Adam optimizer and tanh as activation function.\nTable 1 shows the performance of CF-NADE-S and baselines. The number of hidden units of CF-NADE is 500,\nsame as AutoRec (Sedhain et al., 2015) for a fair comparison. One can observe that I-CF-NADE-S outperforms UCF-NADE-S by a large margin. I-CF-NADE-S with a single hidden layer achieves RMSE of 0.830, which is comparable with any strong baseline. Moreover, I-CF-NADE-S with 2 hidden layers achieves RMSE of 0.829.\nFigure 2 illustrates the performance of I-CF-NADE-S w.r.t the number of hidden units. Increasing the number of hidden units is beneficial, but the return is diminishing. It can also be observed from Figure 2 that deep CF-NADE models achieve better performance than the shallow ones, as expected."}, {"heading": "6.3. Experiments on MovieLens 10M Dataset", "text": "As mentioned in Section 6.1, the MovieLens 10M is much bigger than the MovieLens 1M, so we opt to use the factored version of CF-NADE described in Section 3.3 and set J = 50. Even in this setting, I-CF-NADE with 71567 users and 10 rating scales will still bring about as many as 70 million free parameters, Hence, we only report the performance of U-CF-NADE in this experiment. Same as in Section 6.2.1, we train the model with Adam optimizer and using tanh as activation function. Other configurations are as follows: The number of hidden units to 500, the weight decay is 0.015 and \u03bb is set to 1 and parameters are shared between ratings following Section 6.2.1. The base learning rate is 0.0005, and we double it for the parameters of the first layer.\nTable 2 shows the comparison between CF-NADE and other baselines on MovieLens 10M dataset. U-CF-NADES with a single hidden layer has already outperformed the baselines, which achieves RMSE of 0.772. The performance of U-CF-NADE-S can be slightly improved by adding another hidden layer. Noticeably, the test RMSE of U-AutoRec is much worse than I-AutoRec, whereas U-CFNADE-S outperforms I-AutoRec."}, {"heading": "6.4. Experiments on Netflix Dataset", "text": "Our final set of experiments are on the massive Netflix dataset, which contains 108 ratings. Similar to Section 6.3, we use the factored version of U-CF-NADE with J = 50. The Netflix dataset is so big that we need not add a strong regularization to avoid overfitting and therefore set the weight decay to 0.001. Other configurations are the same as in Section 6.3.\nTable 3 compares the performance of U-CF-NADE with other baselines. We can see that U-CF-NADE-S with a single hidden layer achieves RMSE of 0.804, outperforming all baselines. Another observation from Table 3 is that using a deep CF-NADE architecture achieves a slight improvement over the shallow one, with a test RMSE of 0.803."}, {"heading": "6.5. The Complexity and Running Time of CF-NADE", "text": "We implement CF-NADE using Theano (Bastien et al., 2012) and Blocks (van Merrie\u0308nboer et al., 2015), and the code is available at https://github.com/Ian09/\nCF-NADE. Table 4 shows the running time of one epoch5 as well as the number of parameters used by CF-NADE. For MovieLens 1M dataset, we used the item-based CFNADE and did not use the factorization method introduced by Sec 3.3, hence the number of parameters for MovieLens 1M is bigger than the other two. Running times in Table 4 include overheads such as transferring data from and to GPU memory for each update. Note that there is still room for faster implementations6."}, {"heading": "7. Conclusions", "text": "In this paper, we propose CF-NADE, a feed-forward, autoregressive architecture for collaborative filtering tasks. CF-NADE is inspired by the seminal work of RBM-CF and the recent advancements of NADE. We propose to share parameters between different ratings to improve the performance. We also describe a factored version of CF-NADE, which reduces the number of parameters by factorizing a large matrix by a product of two lower-rank matrices, for better scalability. Moreover, we take the ordinal nature of preference into consideration and propose an ordinal cost to optimize CF-NADE. Finally, following recent advancements of deep learning, we extend CF-NADE to a deep model with moderate increase of computational complexity. Experimental results on three real-world benchmark datasets show that CF-NADE outperforms the state-of-theart methods on collaborative filtering tasks. all results of this work rely on explicit feedback, namely, ratings explicitly given by users. however, explicit feedback is not always available or as common as implicit feedback (watch, search, browse behaviors) in real-world recommender systems (Hu et al., 2008). Developing a version of CF-NADE tailored for implicit feedback is left for future work."}, {"heading": "Acknowledgements", "text": "We thank Hugo Larochelle and the reviewers for many helpful discussions."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bouchard et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bouchard et al\\.", "year": 2012}, {"title": "The netflix prize", "author": ["Bennett", "James", "Lanning", "Stan"], "venue": "In Proceedings of KDD cup and workshop,", "citeRegEx": "Bennett et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bennett et al\\.", "year": 2007}, {"title": "Learning collaborative information filters", "author": ["Billsus", "Daniel", "Pazzani", "Michael J"], "venue": "In ICML,", "citeRegEx": "Billsus et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Billsus et al\\.", "year": 1998}, {"title": "Neural network matrix factorization", "author": ["Dziugaite", "Gintare Karolina", "Roy", "Daniel M"], "venue": "arXiv preprint arXiv:1511.06443,", "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Scalable recommendation with poisson factorization", "author": ["Gopalan", "Prem", "Hofman", "Jake M", "Blei", "David M"], "venue": "arXiv preprint arXiv:1311.1704,", "citeRegEx": "Gopalan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2013}, {"title": "Bayesian nonparametric poisson factorization for recommendation systems", "author": ["Gopalan", "Prem", "Ruiz", "Francisco JR", "Ranganath", "Rajesh", "Blei", "David M"], "venue": "Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Content-based recommendations with poisson factorization", "author": ["Gopalan", "Prem K", "Charlin", "Laurent", "Blei", "David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "The movielens datasets: History and context", "author": ["Harper", "F Maxwell", "Konstan", "Joseph A"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS),", "citeRegEx": "Harper et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Harper et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Hu", "Yifan", "Koren", "Yehuda", "Volinsky", "Chris"], "venue": "In Data Mining,", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": null, "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["Larochelle", "Hugo", "Murray", "Iain"], "venue": "In International Conference on Artificial Intelligence and Statistics, pp", "citeRegEx": "Larochelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2011}, {"title": "Non-linear matrix factorization with gaussian processes", "author": ["Lawrence", "Neil D", "Urtasun", "Raquel"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Lawrence et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2009}, {"title": "Local low-rank matrix approximation", "author": ["Lee", "Joonseok", "Kim", "Seungyeon", "Lebanon", "Guy", "Singer", "Yoram"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Divide-and-conquer matrix factorization", "author": ["Mackey", "Lester W", "Jordan", "Michael I", "Talwalkar", "Ameet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mackey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "Probabilistic matrix factorization", "author": ["Mnih", "Andriy", "Salakhutdinov", "Ruslan"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["Rennie", "Jasson DM", "Srebro", "Nathan"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Rennie et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Rennie et al\\.", "year": 2005}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Resnick", "Paul", "Iacovou", "Neophytos", "Suchak", "Mitesh", "Bergstrom", "Peter", "Riedl", "John"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "Resnick et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Resnick et al\\.", "year": 1994}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Salakhutdinov", "Ruslan", "Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "List-wise learning to rank with matrix factorization for collaborative filtering", "author": ["Shi", "Yue", "Larson", "Martha", "Hanjalic", "Alan"], "venue": "In Proceedings of the fourth ACM conference on Recommender systems,", "citeRegEx": "Shi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Ordinal boltzmann machines for collaborative filtering", "author": ["Truyen", "Tran The", "Phung", "Dinh Q", "Venkatesh", "Svetha"], "venue": "In Proceedings of the Twenty-fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Truyen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Truyen et al\\.", "year": 2009}, {"title": "Rnade: The real-valued neural autoregressive density-estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "A deep and tractable density estimator", "author": ["Uria", "Benigno", "Murray", "Iain", "Larochelle", "Hugo"], "venue": "JMLR: W&CP,", "citeRegEx": "Uria et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2014}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Listwise approach to learning to rank: theory and algorithm", "author": ["Xia", "Fen", "Liu", "Tie-Yan", "Wang", "Jue", "Zhang", "Wensheng", "Li", "Hang"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Xia et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2008}, {"title": "A deep and autoregressive approach for topic modeling of multimodal data", "author": ["Y. Zheng", "Zhang", "Yu-Jin", "H. Larochelle"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Zheng", "Yin", "Zemel", "Richard S", "Zhang", "Yu-Jin", "Larochelle", "Hugo"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Topic modeling of multimodal data: An autoregressive approach", "author": ["Zheng", "Yin", "Zhang", "Yu-Jin", "H. Larochelle"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "The explainatbility of the recommended results as well as the easy-to-implement nature of memory-based CF ensured its popularity in early recommender systems (Resnick et al., 1994).", "startOffset": 158, "endOffset": 180}, {"referenceID": 11, "context": "(Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).", "startOffset": 0, "endOffset": 91}, {"referenceID": 17, "context": "(Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).", "startOffset": 0, "endOffset": 91}, {"referenceID": 4, "context": "(Koren et al., 2009; Salakhutdinov & Mnih, 2008; Mackey et al., 2011; Gopalan et al., 2013).", "startOffset": 0, "endOffset": 91}, {"referenceID": 12, "context": "With the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention.", "startOffset": 45, "endOffset": 109}, {"referenceID": 25, "context": "With the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention.", "startOffset": 45, "endOffset": 109}, {"referenceID": 8, "context": "With the recent development of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural network based CF, a subclass of model-based CF, has gained enormous attention.", "startOffset": 45, "endOffset": 109}, {"referenceID": 22, "context": "A prominent example is RBMbased CF (RBM-CF) (Salakhutdinov et al., 2007).", "startOffset": 44, "endOffset": 72}, {"referenceID": 27, "context": "NADE together with its variants achieved competitive results on many machine learning tasks (Larochelle & Lauly, 2012; Uria et al., 2013; Zheng et al., 2014b; Uria et al., 2014; Zheng et al., 2014a; 2015).", "startOffset": 92, "endOffset": 204}, {"referenceID": 28, "context": "NADE together with its variants achieved competitive results on many machine learning tasks (Larochelle & Lauly, 2012; Uria et al., 2013; Zheng et al., 2014b; Uria et al., 2014; Zheng et al., 2014a; 2015).", "startOffset": 92, "endOffset": 204}, {"referenceID": 26, "context": "As Truyen et al. (2009) observed, preference usually has the ordinal nature: if the true rating of an item by a user is 3 stars in a 5-star scale, then predicting 4 stars is preferred to predicting 5 stars.", "startOffset": 3, "endOffset": 24}, {"referenceID": 11, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items.", "startOffset": 8, "endOffset": 28}, {"referenceID": 17, "context": "There are other MF-based CF methods such as (Rennie & Srebro, 2005; Mackey et al., 2011).", "startOffset": 44, "endOffset": 88}, {"referenceID": 8, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD.", "startOffset": 9, "endOffset": 169}, {"referenceID": 8, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD. Salakhutdinov & Mnih (2008) proposed a Bayesian treatment of PMF, which can be trained efficiently by MCMC methods.", "startOffset": 9, "endOffset": 365}, {"referenceID": 8, "context": "Bias MF (Koren et al., 2009) is proposed to improve the performance of SVD by introducing systematic biases associated with users and items. Mnih & Salakhutdinov (2007) extended MF to a probabilistic linear model with Gaussian noise referred to as Probabilistic Matrix Factorization (PMF), and showed that PMF performed better than SVD. Salakhutdinov & Mnih (2008) proposed a Bayesian treatment of PMF, which can be trained efficiently by MCMC methods. Along this line, Lawrence & Urtasun (2009) proposed a non-linear PMF using Gaussian process latent variable models.", "startOffset": 9, "endOffset": 496}, {"referenceID": 4, "context": "Recently, Poisson Matrix Factorization (Gopalan et al., 2014b;a; 2013) was proposed, replacing Gaussian assumption of PMF by Poisson distribution. Lee et al. (2013) extended the low-rank assumption by embedding locality into MF models and proposed Local Low-Rank Matrix Approximation (LLORMA) method, which achieved impressive performance on several public benchmarks.", "startOffset": 40, "endOffset": 165}, {"referenceID": 12, "context": "With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks.", "startOffset": 45, "endOffset": 109}, {"referenceID": 25, "context": "With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks.", "startOffset": 45, "endOffset": 109}, {"referenceID": 8, "context": "With the tremendous success of deep learning (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), neural networks have found profound applications in CF tasks.", "startOffset": 45, "endOffset": 109}, {"referenceID": 22, "context": "RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al.", "startOffset": 7, "endOffset": 35}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007).", "startOffset": 8, "endOffset": 116}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks.", "startOffset": 8, "endOffset": 303}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items.", "startOffset": 8, "endOffset": 633}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items. Truyen et al. (2009) also extended the standard BM model so as to exploit the ordinal nature of ratings.", "startOffset": 8, "endOffset": 793}, {"referenceID": 8, "context": ", 2014; He et al., 2015), neural networks have found profound applications in CF tasks. Salakhutdinov et al. (2007) proposed a variant of Restricted Boltzmann Machine (RBM) for CF tasks, which is successfully applied in Netflix prize challenge (Bennett & Lanning, 2007). Recently, Sedhain et al. (2015) proposed AutoRec, an autoencoder-based CF model, which achieved the state-of-the-art performance on some benchmarks. RBM-CF (Salakhutdinov et al., 2007) and AutoRec (Sedhain et al., 2015) are common in that both of them build different models for different users, where all these models share the parameters. Truyen et al. (2009) proposed to apply Boltzmann Machine (BM) on CF tasks, which extends RBM-CF by integrating the correlation between users and between items. Truyen et al. (2009) also extended the standard BM model so as to exploit the ordinal nature of ratings. Recently, Dziugaite & Roy (2015) proposed Neural Network Matrix Factorization (NNMF), where the inner product between the vectors of users and items in MF is replaced by a feed-forward neural network.", "startOffset": 8, "endOffset": 910}, {"referenceID": 22, "context": "To tackle sparsity, similar to RBM-CF (Salakhutdinov et al., 2007), we use a different CF-NADE model for each user and all these models share the same parameters.", "startOffset": 38, "endOffset": 66}, {"referenceID": 27, "context": "As Uria et al. (2014) observed, we can think of the models trained with different orderings as different instantiations of CF-NADE for the same user.", "startOffset": 3, "endOffset": 22}, {"referenceID": 22, "context": "Inspired by RBM-CF (Salakhutdinov et al., 2007) and FixationNADE (Zheng et al.", "startOffset": 19, "endOffset": 47}, {"referenceID": 26, "context": "To go one step further, following Truyen et al. (2009), we take the ordinal nature of a user\u2019s preference into consideration.", "startOffset": 34, "endOffset": 55}, {"referenceID": 30, "context": "Both two products in Equation 17 can be interpreted as the likelihood loss introduced in (Xia et al., 2008) in the", "startOffset": 89, "endOffset": 107}, {"referenceID": 30, "context": "Actually, from the perspective of learning-to-rank, CF-NADE acts as a ranking function which produces rankings of ratings based on previous ratings, where sjmoi (rmo<i ) corresponds to the score function in (Xia et al., 2008) and the rankings, ydown and yup, corresponds to true rankings that we would like CF-NADE to fit.", "startOffset": 207, "endOffset": 225}, {"referenceID": 23, "context": "Put differently, the ranking loss in Equation 17 is defined on the ratings, while other learning-to-rank based CF methods, such as (Shi et al., 2010), are on items, which is the crucial difference.", "startOffset": 131, "endOffset": 149}, {"referenceID": 12, "context": "As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance.", "startOffset": 74, "endOffset": 138}, {"referenceID": 25, "context": "As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance.", "startOffset": 74, "endOffset": 138}, {"referenceID": 8, "context": "As suggested by the recent and impressive success of deep neural networks (Krizhevsky et al., 2012; Szegedy et al., 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance.", "startOffset": 74, "endOffset": 138}, {"referenceID": 31, "context": "(2014) proposed an efficient deep extension to original NADE (Larochelle & Murray, 2011) for binary vector observations, which inspires other related deep model (Zheng et al., 2015).", "startOffset": 161, "endOffset": 181}, {"referenceID": 28, "context": "Following (Uria et al., 2014), we propose a deep variant of CF-NADE.", "startOffset": 10, "endOffset": 29}, {"referenceID": 8, "context": ", 2014; He et al., 2015), extending CF-NADE to a deep, multiple hidden layers architecture could allow us to have better performance. Recently, Uria et al. (2014) proposed an efficient deep extension to original NADE (Larochelle & Murray, 2011) for binary vector observations, which inspires other related deep model (Zheng et al.", "startOffset": 8, "endOffset": 163}, {"referenceID": 27, "context": "As noticed by Uria et al. (2014) and Zheng et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 27, "context": "As noticed by Uria et al. (2014) and Zheng et al. (2015), training over all possible orderings for CF-NADE implies that for any given context rmo<i , the model performs equally well at predicting all the remaining items in rmo\u2265i , since for each item there is an ordering such that it appears at position i.", "startOffset": 14, "endOffset": 57}, {"referenceID": 28, "context": "The factors in front of the sum come from the fact that the total number of elements in the sum is D and that we are averaging over D\u2212 i+1 possible choices for the item at position i, similar to (Uria et al., 2014) and (Zheng et al.", "startOffset": 195, "endOffset": 214}, {"referenceID": 31, "context": ", 2014) and (Zheng et al., 2015).", "startOffset": 12, "endOffset": 32}, {"referenceID": 16, "context": "Following LLORMA (Lee et al., 2013) and AutoRec (Sedhain et al.", "startOffset": 17, "endOffset": 35}, {"referenceID": 16, "context": "Following Lee et al. (2013) and Sedhain et al.", "startOffset": 10, "endOffset": 28}, {"referenceID": 16, "context": "Following Lee et al. (2013) and Sedhain et al. (2015), we split the available trainset of Netflix dataset into train, valid and test sets.", "startOffset": 10, "endOffset": 54}, {"referenceID": 16, "context": ", 2015) and LLORMA (Lee et al., 2013).", "startOffset": 19, "endOffset": 37}, {"referenceID": 9, "context": "however, explicit feedback is not always available or as common as implicit feedback (watch, search, browse behaviors) in real-world recommender systems (Hu et al., 2008).", "startOffset": 153, "endOffset": 170}], "year": 2016, "abstractText": "This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.", "creator": "LaTeX with hyperref package"}}}