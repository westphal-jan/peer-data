{"id": "1605.09507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Deep convolutional neural networks for predominant instrument recognition in polyphonic music", "abstract": "correctly identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field thinking of music information retrieval. it strongly enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. in this paper, we present a theoretical convolutional neural network framework for predominant temporal instrument recognition in real - world polyphonic music. we train our network from fixed - length music excerpts with a single - labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. to obtain the audio - excerpt - wise result, we aggregate multiple outputs from sliding windows over the test audio. in doing so, we investigated two different aggregation methods : one takes the average for each instrument \u2014 and the other takes the instrument - wise sum followed by normalization. in addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for candidate neural networks to find the optimal set of parameters. using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features separation and source separation with support vector imaging machines. experimental results showed that the proposed convolutional network architecture obtained an f1 measure of 0. 602 for micro and 0. 503 for macro, respectively, achieving 19. 6 % and 16. 4 % in performance improvement compared with other state - of - the the - art algorithms.", "histories": [["v1", "Tue, 31 May 2016 07:11:18 GMT  (1695kb,D)", "http://arxiv.org/abs/1605.09507v1", "13 pages, 7 figures, submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING on 17-May-2016"], ["v2", "Fri, 18 Nov 2016 08:54:57 GMT  (1693kb,D)", "http://arxiv.org/abs/1605.09507v2", "13 pages, 7 figures, accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing on 16-Nov-2016"], ["v3", "Mon, 26 Dec 2016 12:29:26 GMT  (1695kb,D)", "http://arxiv.org/abs/1605.09507v3", "13 pages, 7 figures, accepted for publication in IEEE/ACM Transactions on Audio, Speech, and Language Processing on 16-Nov-2016. This is initial submission version. Fully edited version is available atthis http URL"]], "COMMENTS": "13 pages, 7 figures, submitted to IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING on 17-May-2016", "reviews": [], "SUBJECTS": "cs.SD cs.CV cs.LG cs.NE", "authors": ["yoonchang han", "jaehun kim", "kyogu lee"], "accepted": false, "id": "1605.09507"}, "pdf": {"name": "1605.09507.pdf", "metadata": {"source": "CRF", "title": "Deep convolutional neural networks for predominant instrument recognition in polyphonic music", "authors": ["Yoonchang Han", "Jaehun Kim", "Kyogu Lee"], "emails": ["han@snu.ac.kr,", "eldrin@snu.ac.kr,", "kglee@snu.ac.kr)."], "sections": [{"heading": null, "text": "Index Terms\u2014Instrument recognition, convolutional neural networks, deep learning, multi-layer neural network, music information retrieval\nI. INTRODUCTION\nMUSIC can be said to be built by the interplay ofvarious instruments. A human can easily identify what instruments are used in a music, but it is still a difficult task for a computer to automatically recognize them. This is mainly because music in the real world is mostly polyphonic, which makes extraction of information from an audio highly challenging. Furthermore, instrument sounds in the real world vary in many ways such as for timbre, quality, and playing style, which makes identification of the musical instrument even harder.\nIn the music information retrieval (MIR) field, it is highly desirable to know what instruments are used in an audio sample. First of all, instrument information per se is an\nY. Han, J. Kim, and K. Lee are with the Music and Audio Research Group, Graduate School of Convergence Science and Technology, Seoul National University, Seoul 08826, Republic of Korea, e-mail: (yoonchanghan@snu.ac.kr, eldrin@snu.ac.kr, kglee@snu.ac.kr).\nK. Lee is also with the Advanced Institutes of Convergence Technology, Suwon, Republic of Korea\nManuscript received October 8, 2017; revised October 8, 2017.\nimportant and useful information for users, and it can be included in the audio tags. There is a huge demand for music search owing to the increasing number of music files in digital format. Unlike text search, it is difficult to search for music because input queries are usually in text format. If an instrument information is included in the tags, it allows people to search for music with the specific instrument they want. In addition, the obtained instrument information can be used for various audio/music applications. For instance, more instrument-specific and tailored audio equalization can be applied to the music; moreover, a music recommendation system can reflect the preference of users for musical instruments. Furthermore, it can also be used to enhance the performance of other MIR tasks. For example, knowing the number and type of the instrument would significantly improve the performance of source separation and automatic music transcription; it would also be helpful for identifying the genre of the music.\nInstrument recognition can be performed in various forms. Hence, the term \u201cinstrument recognition\u201d or \u201cinstrument identification\u201d might indicate several different research topics. For instance, many of the related works focus on studiorecorded isolated notes. To name a few, Eronen used cepstral coefficients and temporal features to classify 30 orchestral instruments with several articulation styles and achieved a classification accuracy of 95% for instrument family level and about 81% for individual instruments [1]. Diment et al. used a modified group delay feature that incorporates phase information together with mel-frequency cepstral coefficients (MFCCs) and achieved a classification accuracy of about 71% for 22 instruments [2]. Yu et al. applied sparse coding on cepstrum with temporal sum-pooling and achieved an Fmeasure of about 96% for classifying 50 instruments [3]. They also reported their classification result on a multi-source database, which was about 66%.\nSome previous works such as Krishna and Sreenivas [4] experimented with a classification for solo phrases rather than for isolated notes. They proposed line spectral frequencies (LSF) with a Gaussian mixture model (GMM) and achieved an accuracy of about 77% for instrument family and 84% for 14 individual instruments. Moreover, Essid et al. [5] reported that a classification system with MFCCs and GMM along with principal components analysis (PCA) achieved an overall recognition accuracy of about 67% on solo phrases with five instruments.\nMore recent works deal with polyphonic sound, which is closer to real-world music than to monophonic sound. In the case of polyphonic sound, a number of research stud-\nar X\niv :1\n60 5.\n09 50\n7v 1\n[ cs\n.S D\n] 3\n1 M\nay 2\n01 6\nies used synthesized polyphonic audio from studio-recorded single tones. Heittola et al. [6] used a non-negative matrix factorization (NMF)-based source-filter model with MFCCs and GMM for synthesized polyphonic sound and achieved a recognition rate of 59% for six polyphonic notes randomly generated from 19 instruments. Kitahara et al. [7] used various spectral, temporal, and modulation features with PCA and linear discriminant analysis (LDA) for classification. They reported that, using feature weighting and musical context, recognition rates were about 84% for a duo, 78% for a trio, and 72% for a quartet. Duan et al. [8] proposed the uniform discrete cepstrum (UDC) and mel-scale UDC (MUDC) as a spectral representation with a radial basis function (RBF) kernel support vector machine (SVM) to classify 13 types of Western instruments. The classification accuracy of randomly mixed chords of two and six polyphonic notes, generated using isolated note samples from the RWC musical instrument sound database [9], was around 37% for two polyphony notes and 25% for six polyphony notes.\nAs shown above, most of the previous works focused on the identification of the instrument sounds in clean solo tones or phrases. More recent research studies on polyphonic sounds are closer to the real-world situation, but artificially produced polyphonic music is still far from professionally produced music. Real-world music has many other factors that affect the recognition performance. For instance, it might have a highly different timbre, depending on the genre and style of the performance. In addition, an audio file might differ in quality to a great extent, depending on the recording and production environments.\nIn this paper, we investigate a method for predominant instrument recognition in professionally produced Western music recordings. We utilize convolutional neural networks (ConvNets) to learn the spectral characteristics of the music recordings with 11 musical instruments and perform instrument identification on polyphonic music excerpts. The major contributions of the work presented in this paper are as follows.\n1. We present the ConvNet architecture for predominant musical instrument identification where the training data are single labeled and the target data are multi-labeled with an unknown number of classes existing in the data.\n2. We introduce a new method to aggregate the outputs of ConvNets from short-time sliding windows to find the predominant instruments in a music excerpt with variable length, where the conventional method of majority vote often fails.\n3. We conduct an extensive experiment on activation function for the neurons used in ConvNets, which can cause a huge impact on the identification result.\nThe remainder of the paper is organized as follows. In section II, we introduce emerging deep neural network techniques in the MIR field. Next, the system architecture section includes audio preprocessing, the proposed network architecture with\ndetailed training configuration, and an explanation of various activation functions used for the experiment. Section IV, the evaluation section, contains information about the dataset, testing configuration including aggregation strategy, and our evaluation scheme. Then, we illustrate the performance of the proposed ConvNet in section V, the Results section, with an analysis of the effects of activation function, analysis window size, aggregation strategy, and identification threshold, and with an instrument-wise analysis. Moreover, we present a qualitative analysis based on the visualization of the ConvNet\u2019s intermediate outputs to understand how the network captured the pattern from the input data. Finally, we conclude the paper in section VI."}, {"heading": "II. PROLIFERATION OF DEEP NEURAL NETWORKS IN", "text": "MUSIC INFORMATION RETRIEVAL\nThe ability of traditional machine learning approaches was limited in terms of processing input data in their raw form. Hence, usually the input for the learning system, typically a classifier, has to be a hand-crafted feature representation, which requires extensive domain knowledge and a careful engineering process. However, it is getting more common to design the system to automatically discover the higherlevel representation from the raw data by stacking several layers of nonlinear modules, which is called deep learning [10]. Recently, deep learning techniques have been widely used across a number of domains owing to their superior performance. A basic architecture of deep learning is called deep neural network (DNN), which is a feedforward network with multiple hidden layers of artificial neurons. DNN-based approaches have outperformed previous state-of-the-art methods in speech applications such as phone recognition, largevocabulary speech recognition, multi-lingual speech recognition, and noise-robust speech recognition [11].\nThere are many variants and modified architectures of deep learning, depending on the target task. Especially, recurrent neural networks (RNNs) and ConvNets have recently shown remarkable results for various multimedia information retrieval tasks. RNNs are highly powerful approaches for sequential inputs as their recurrent architecture enables their hidden units to implicitly maintain the information about the past elements of the sequence. Since languages natively contain sequential information, it is widely applied to handle text characters or spoken language. It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].\nOn the other hand, ConvNet is useful for data with local groups of values that are highly correlated, forming distinctive local characteristics that might appear at different parts of the array [10]. Hence, it is one of the most popular approaches recently in the image processing area such as handwritten digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset. In addition, it has been reported that it has outperformed state-of-the-art approaches for several computer vision benchmark tasks such as object detection, semantic segmentation, and category-level object recognition [11], and also for speech-recognition tasks [20].\nThe time-frequency representation of a music signal is composed of harmonics from various musical instruments and a human voice. Each musical instrument produces a unique timbre with different playing styles, and this type of spectral characteristics in music signal might appear in a different location in time and frequency as in the image. ConvNets are usually composed of many convolutional layers, and inserting a pooling layer between convolutional layers allows the network to work at different time scales and introduces translation invariance with robustness against local distortions. These hierarchical network structures of ConvNets are highly suitable for representing music audio, because music tends to present a hierarchical structure in time and different features of the music might be more salient at different time scales [21].\nHence, although ConvNets have been a more commonly used technique in image processing, there are an increasing number of attempts to apply ConvNets for music signal. It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].\nAn attempt to apply ConvNets for musical instrument identification can be found in the recent report from Park et al. [27] and Li et al. [28], although it is still an ongoing work and is not a predominant instrument recognition method; hence, there are no other instruments but only target instrument sounds exist. Our research differs from [27] because we deal with polyphonic music, while their work is based on the studio recording of single tones. In addition, our research also differs from [28] because we use single-label data for training and estimate multi-label data, while they used multilabel data from the training phase. Moreover, they focused on an end-to-end approach, which is promising in that using raw audio signals makes the system rely less on domain knowledge and preprocessing, but usually it shows a slightly lower performance than using spectral input such as melspectrogram in recent papers [29], [30]."}, {"heading": "III. SYSTEM ARCHITECTURE", "text": ""}, {"heading": "A. Audio Preprocessing", "text": "The convolutional neural network is one of the representation learning methods that allow a machine to be fed with raw\ndata and to automatically discover the representations needed for classification or detection [10]. However, appropriate preprocessing of input data is still an important issue to improve the performance of the system.\nIn the first preprocessing step, the stereo input audio is converted to mono by taking the mean of the left and right channels, and then it is downsampled to 22,050 Hz from the original 44,100 Hz of sampling frequency. This allows us to use frequencies up to 11,025 Hz, the Nyquist frequency, and it is sufficient to cover most of the harmonics generated by musical instruments while removing noises possibly included in the frequencies above this range. Moreover, all audios are normalized by dividing the time-domain signal with its maximum value. Then, this downsampled time-domain waveform is converted to a time-frequency representation using short-time Fourier transform (STFT) with 1024 samples for the window size (approx. 46 ms) and 512 samples of the hop size (approx. 23 ms).\nNext, the linear frequency scale-obtained spectrogram is converted to a mel-scale. We use 128 for the number of melfrequency bins, following the representation learning papers on music annotation by Nam et al. [31] and Hamel et al. [21], which is a reasonable setting that sufficiently preserves the harmonic characteristics of the music while greatly reducing the dimensionality of the input data. Finally, the magnitude of the obtained mel-frequency spectrogram is compressed with a natural logarithm."}, {"heading": "B. Network Architecture", "text": "ConvNets can be seen as a combination of feature extractor and the classifier. Our ConvNet architecture generally follows a popular AlexNet [18] and VGGNet [32] structure, which contains very deep architecture using repeated several convolution layers followed by max-pooling, as shown in Figure 1. This method of using smaller receptive window size and smaller stride for ConvNet is becoming highly common especially in the computer vision field such as in the study from Zeiler and Fergus [33] and Sermanet et al. [34], which has shown superior performance in ILSVRC-2013.\nAlthough the general architecture style is similar to that of other successful ConvNets in the image processing area, the proposed ConvNet is designed according to our input data. We\nuse filters with a very small 3\u00d7 3 receptive field, with a fixed stride size of 1, and spatial abstraction is done by max-pooling with a size of 3\u00d7 3 and a stride size of 1.\nIn Table I, we illustrate the detailed ConvNet architecture with the input size in each layer with parameter values except the zero-padding process. The input for each convolution layer is zero-padded with 1 \u00d7 1 to preserve the spatial resolution regardless of input window size, and we increase the number of channels for the convolution layer by a factor of 2 after every two convolution layers, starting from 32 up to 256.\nIn the last max-pooling layer after the eight convolutional layers, we perform global max-pooling followed by one fully connected layer. Recently, it has been reported that the use of global average pooling without a fully connected layer before a classifier layer is less prone to overfitting and shows better performance for image processing datasets such as CIFAR-10 and MNIST [35]. However, our empirical experiment found that global average pooling slightly decreases the performance and that global max-pooling followed by a fully connected layer works better for our task.\nFinally, the last classifier layer is the sigmoid layer. It is common to use a softmax layer when there is only one target label, but our system must be able to handle multiple instruments present at the same time, and, thus, a sigmoid output is used."}, {"heading": "C. Training Configuration", "text": "The training was done by optimizing the categorical crossentropy between predictions and targets. We used Adam [36]\nas an optimizer with a learning rate of 0.001, and the minibatch size was set to 128. To accelerate the learning process with parallelization, we used a GTX 970 GPU, which has 4GB of memory.\nThe training was regularized using dropout with a rate of 0.25 after each max-pooling layer. Dropout is a technique that prevents the overfitting of units to the training data by randomly dropping some units from the neural network during the training phase [37]. Furthermore, we added dropout after a fully connected layer as well with a rate of 0.5 since a fully connected layer easily suffers from overfitting.\nIn addition, we conducted an experiment with various time resolutions to find the optimal analysis size. As our training data were a fixed 3-s audio, we performed the training with 3.0, 1.5, 1.0, and 0.5 s by dividing the training audio and used the same label for each divided chunk. The audio was divided without overlap for training as it affects the validation loss used for the early stopping. Fifteen percent of the training data were randomly selected and used as a validation set, and the training was stopped when the validation loss did not decrease for more than two epochs.\nThe initialization of the network weights is another important issue as it can lead to an unstable learning process, especially for a very deep network. We used a uniform distribution with zero biases for both convolutional and fully connected layers following Glorot and Bengio [38]."}, {"heading": "D. Activation Function", "text": "The activation function is followed by each convolutional layer and fully connected layer. In this section, we introduce several activation functions used in the experiment for the comparison.\nThe traditional way to model the activation of a neuron is by using a hyperbolic tangent (tanh) or sigmoid function. However, non-saturating nonlinearities such as the rectified linear unit (ReLU) allow much faster learning than these saturating nonlinearities, particularly for models that are trained on large datasets [18]. Moreover, a number of works have shown that the performance of ReLU is better than that of sigmoid and tanh activation [39]. Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].\nReLU was first introduced by Nair and Hinton in their work on restricted Boltzmann machines [40]. The ReLU activation function is defined as\nyi = max(0, zi) (1)\nwhere zi is the input of the ith channel. ReLU simply suppresses the whole negative part to zero while retaining the positive part. Recently, there have been several modified versions of ReLU introduced to improve the performance further. First, leaky-ReLU (LReLU), introduced by Mass et al. [41], compresses the negative part rather than make it all zero, which might cause some initially inactive units to remain inactive. It is defined as\nyi = { zi zi \u2265 0 \u03b1zi zi < 0\n(2)\nwhere \u03b1 is a parameter between 0 and 1 to give a small gradient in the negative part. Second, parametric ReLU (PReLU), introduced by He et al. [42], is basically similar to LReLU in that it compresses the negative part. However, PReLU automatically learns the parameter for the negative gradient, unlike LReLU. It is defined as\nyi = { zi zi \u2265 0 \u03b1izi zi < 0\n(3)\nwhere \u03b1i is the learned parameters for the ith channel. The choice of activation function considerably influences the identification performance. It is difficult to say which specific activation function always performs the best because it highly depends on the parameter setting and the input data. For instance, an empirical evaluation of the ConvNet activation functions from Xu et al. [43] reported that the performance of LReLU is better than those of ReLU and PReLU, but sometimes it is worse than that of basic ReLU, depending on the dataset and the value for \u03b1. Moreover, most of the works regarding activation function are on the image classification task, not on the audio processing domain.\nHence, we empirically evaluated several activation functions explained above such as tanh, ReLU, LReLU, and PReLU to find the most suitable activation function for our task. For LReLU, very leaky ReLU (\u03b1 = 0.33) and normal leaky ReLU (\u03b1 = 0.01) were used, because it has been reported that the performance of LReLU considerably differs depending on the value and that very leaky ReLU works better [43].\nWe used separate test audio data from the IRMAS dataset, which were not used for the training. First, a sliding window was used to analyze the input test audio, which was of the same size as the analysis window in the training phase. The hop size of the sliding window was set to half of the window size. Then, we aggregated the sigmoid outputs from the sliding windows by summing all outputs class-wise to obtain a total amount of activation for each instrument. These 11 summed sigmoid activations were then normalized to be in a range between 0 and 1 by dividing all with the maximum activation."}, {"heading": "IV. EVALUATION", "text": "A. IRMAS Dataset\nThe IRMAS dataset includes musical audio excerpts with annotations of the predominant instruments present and is intended to be used for the automatic identification of the predominant instruments in the music. This dataset was used in the paper on predominant instrument classification by Bosch et al. [44] and includes music from various decades from the past century, hence differing in audio quality to a great extent. In addition, the dataset covers a wide variability in musical instrument types, articulations, recording and production styles, and performers.\nThe dataset is divided into training and testing data, and all audio files are in 16-bit stereo wave with 44,100 Hz of sampling rate. The training data consisted of 6705 audio files with excerpts of 3 s from more than 2000 distinct recordings. Two subjects were paid to obtain the data for 11 pitched instruments, as shown in Table II from selected music tracks,\nwith the objective of extracting music excerpts that contain a continuous presence of a single predominant instrument.\nOn the other hand, the testing data consisted of 2874 audio files with lengths between 5 s and 20 s, and no tracks from the training data were included. Unlike the training data, the testing data contained one or more predominant target instruments. Hence, the total number of training labels was identical to the number of audio files, but the number of testing labels was more than the number of testing audio files as the latter are multi-label. For both the training and the testing dataset, other musical instruments such as percussion and bass were not included in the annotation even if they exist in the music excerpts."}, {"heading": "B. Testing Configuration", "text": "In the training phase, we used a fixed length window because the input data for ConvNet should be in a specific fixed shape. However, our testing audios had variable lengths between 5 s and 20 s, which were much longer than those of the training audio. Developing a system that can handle variable length of input data is valuable because music in real life varies in its length. We performed short-time analysis using overlapping windows to obtain local instrument information in the audio excerpts. Since an annotation exists per audio clip, we observed multiple sigmoid outputs and aggregated them to make a clip-wise decision. We tried two different strategies for the aggregation, which are the average and the normalized sum, which are referred as S1 and S2 throughout the paper, respectively.\nFor S1, we simply took an average of the sigmoid outputs class-wise (i.e., instrument-wise) over the whole audio clip and thresholded it without normalization. This method is intended to capture the existence of each instrument with its mean probability such that it might return the result without any detected instrument. For S2, we first summed all sigmoid outputs class-wise over the whole audio excerpt and normalized the values by dividing them with the maximum value among classes such that the values were scaled to be placed between zero and one, followed by thresholding. This method is based on the assumption that humans perceive the \u201cpredominant\u201d\ninstrument in a more relatively scaled sense such that the strongest instrument is always detected and the existence of other instruments is judged by their relative strength compared to the most activate instrument.\nMajority vote, one of the most common choices for a number of classification tasks, is not used in our system. Majority vote first predicts the classes for each analysis frame and the one with more vote wins. However, using this method for our task would result in disregarding accompaniment instruments, piano for example, because a music signal is composed of various musical instruments and usually the sounds are overlapped in time domain, and a presence of accompaniments are usually much weaker than that of voice or lead instruments.\nAs our target is to identify an arbitrary number of predominant instruments in testing data, instruments with aggregated value over the threshold were all considered as predominant instruments. Using a higher value for the identification threshold will lead to better precision, but it will obviously decrease the recall. On the other hand, a lower threshold will increase the recall, but will lower the precision. Hence, we tried a range of values for the threshold to find the optimal value for the F1 measure, which is explained in the next Performance Evaluation section.\nFor S1, values between 0.02 and 0.18 were used, and for S2, values between 0.2 and 0.6 were used as a threshold \u03b8. These threshold values were empirically chosen but set to be a wide enough range to find the best performance (i.e., highest F1 measure). The schematic of this aggregation process is illustrated in Figure 2."}, {"heading": "C. Performance Evaluation", "text": "Following the evaluation method widely used in the instrument recognition task, we computed the precision and recall, which are defined as\nP = tp\ntp+ fp (4)\nwhere tp is true positive, fp is false positive, and fn is false negative. In addition, we used the F1 measure to calculate the overall performance of the system, which is the harmonic mean between precision and recall:\nF1 = 2PR\nP +R (6)\nSince the number of annotations for each class (i.e., 11 musical instruments) was not equal, we computed the precision, recall, and F1 measure for both the micro and the macro averages. For the micro averages, we calculated the metrics globally regardless of classes, thus giving more weight to the instrument with a higher number of appearances. On the other hand, we calculated the metrics for each label and found their unweighted average for the macro averages; hence, it is not related to the number of instances, but represents the overall performance over all classes. Finally, we repeated each experiment three times and calculated the mean and standard deviation of the output."}, {"heading": "V. RESULTS", "text": "We used LReLU (\u03b1 = 0.33) for the activation function, 1 s for the analysis window, S2 for the aggregation strategy, and 0.50 for the identification threshold as default settings of the experiment where possible, which showed the best performance. The experiment variables are listed in Table III.\nFirst, we compared the performance of the proposed ConvNet with that of the existing algorithm on the IRMAS dataset. The effect of activation function, analysis window, aggregation strategy, and identification threshold on the recognition performance was analyzed separately in the following subsections."}, {"heading": "A. Comparison to Existing Algorithms", "text": "For the result, our network achieved 0.602 for the micro F1 measure and 0.503 for the macro F1 measure. The existing algorithm from Fuhrmann and Herrera [45] used typical hand-made timbral audio features with their framewise mean and variance statistics to train SVMs, and Bosch et al. [44] improved this algorithm with source separation called FASST (Flexible Audio Source Separation Framework) [46] in a preprocessing step.\nIn terms of precision, Fuhrmann and Herrera\u2019s algorithm showed the best performance for both the micro and the macro measure. However, its recall was very low, around 0.25, which resulted in a low F1 measure. Our proposed ConvNet architecture outperformed existing algorithms on the IRMAS dataset for both the micro and the macro F1 measure, as shown in Figure 3. From this result, it can be observed that the learned feature from the input data that is classified through ConvNet works better than the conventional handcrafted features with SVMs."}, {"heading": "B. Effect of Activation Function", "text": "In the case of using rectified units as an activation function, it was possible to observe a significant performance improvement compared to the tanh baseline as expected, as shown in Table IV. Unlike the result presented in the ImageNet classification work from He et al. [42], PReLU did not show any performance improvement, but just showed a matching performance with ReLU in our task. On the other hand, using LReLU showed better performance than using normal ReLU and PReLU. While using LReLU with a small gradient (\u03b1 = 0.01) showed similar performance to ReLU as expected, LReLU with a very leaky alpha setting (\u03b1 = 0.33) showed the best identification performance, which matched the result of the empirical evaluation work on ConvNet activation function from Xu et al. [43].\nThis result shows that suppressing the negative part of the activation rather than making it all zero certainly improves the performance compared to normal ReLU because making the whole negative part zero might cause some initially inactive units to be never active as mentioned above. Moreover, this result shows that using leaky ReLU, which has been proved to work well in the image classification task, also benefits the musical instrument identification."}, {"heading": "C. Effect of Analysis Window Size", "text": "As mentioned above, we conducted an experiment with diverse analysis window sizes such as 3.0, 1.5, 1.0, and 0.5 s to find the optimal analysis resolution. Figure 4 shows the micro and macro F1 measure with various analysis frame sizes according to identification threshold, and it can be observed that the use of the longest 3.0-s window clearly performed poorer than the use of shorter window sizes regardless of identification threshold. However, shortening the analysis frame down to 0.5 s decreased the overall performance again.\nFrom this result, it can be seen that 1.0 s is the optimal analysis window size for our task. Using a shorter analysis frame helped to increase the temporal resolution, but 0.5 s was found to be too short a window size for identifying the instrument."}, {"heading": "D. Effect of Identification Threshold", "text": "Using a higher value for the identification threshold leads to better precision, but it decreases the recall. On the contrary, a lower threshold results in better recall with lower precision.\nHence, we used the F1 measure, which is the harmonic mean of precision and recall to evaluate the overall performance. In terms of F1 measure, we found that 0.5 is the most appropriate threshold as it showed the best performance for the macro F1 measure, as shown in Figure 4.\nThe current system uses a certain identification threshold for all instruments. However, we think that there might be a room for improvement by using different thresholds for each instrument because there are various types of instruments included in the experiment. For example, the amplitude of the piano sound was relatively small in a number of music excerpts because it is usually used as an accompanying instrument. On the other hand, the flute sound in the music was mostly louder than others because it is usually used as a lead instrument."}, {"heading": "E. Effect of Aggregation Strategy", "text": "We conducted an experiment with two different strategies, S1 and S2, for the aggregation of ConvNet outputs as explained in the Testing Configuration section. The performance of S1 and S2 is demonstrated in Table V with a threshold \u03b8 that returned the highest F1 measure for each strategy. As a result, S2 showed better identification performance than S1 overall. There was only a slight performance gap between S1 and S2 for the micro F1 measure, but the difference was notable for the macro F1 measure. This result shows that performing a class-wise sum followed by normalization is a better aggregation method for predominant instrument identification than taking class-wise mean values. It is likely due to the training and testing audios differing in quality to a great extent, depending on the recording and production time, and the audio-excerpt-wise normalization helped to minimize\nthe effect of quality differences between audio excerpts, which would result in a more generalized output."}, {"heading": "F. Analysis of Instrument-Wise Identification Performance", "text": "The results demonstrated above were focused on the overall identification performance. In this section, we analyze and discuss the result instrument-wise (i.e., class-wise) to observe the system performance in detail. As shown in Figure 5, identification performance varies to a great extent, depending on the instruments. Regardless of parameter setting, it can be observed that the system recognizes the voice in the music very well, showing an F1 measure of about 0.90. On the other hand, cello and clarinet showed relatively poor performance compared to other instruments, showing an F1 measure of around 0.20.\nThese results were highly likely affected by the insufficient number of training audio samples. For deep learning, the number of training examples is critical for the performance compared to the case of using hand-crafted features because it aims to learn a feature from the low-level input data. As illustrated in Table II, the number of training audio samples for voice is 778, which is the largest number of training audio. On the contrary, 338 and 505 audio excerpts were used for cello and clarinet, respectively, which were the least and third least number of training audio. We believe that increasing the number of training data for cello and clarinet would be helpful to increase the identification performance for these instruments.\nIn addition, the number of test audio samples for cello and clarinet was much less than those for other instruments too. The dataset only has 111 and 62 test audio samples for cello and clarinet, respectively, which are the first and second least number of test audio, while it has 1044 audio samples for the human voice. Evaluating the system on a small number of test data would make the result less reliable and less stable than other identification results.\nApart from the issue related to the number of audio, high identification performance of the voice class is highly likely owing to its spectral characteristic that is distinct from other musical instruments. The other instruments used in the experiment usually produce relatively clear harmonic patterns;\nhowever, the human voice produces highly unique spectral characteristics that contain much more inharmonic spectrum with a natural vibrato.\nRegarding aggregation strategy, using S1 instead of S2 decreased the identification performance for organ and piano. This result indicates that S1 showed a slight advantage on instruments that are usually used as an accompaniment instrument, while using S1 for aggregation was better for most of the cases. On the other hand, using a 3-s analysis window instead of the default 1-s window considerably decreased the performance, especially for flute, acoustic guitar, electric guitar, and violin. This result shows that using a longer analysis window is a disadvantage for most of the cases. Finally, using a very low identification threshold, 0.20, caused considerable performance loss especially for flute, saxophone, trumpet, and violin, while it showed a slight improvement for electric guitar, organ, and piano.\nThis result can be understood to mean that using a lower threshold for identification performance helps to detect instruments that are usually in the background, while using a higher threshold is suitable for instruments that are frequently used as a lead instrument or for wind instruments that usually show relatively strong presence in the music. As mentioned in the Results section, this result indicates that there can be a potential performance improvement by using a different identification threshold for each instrument."}, {"heading": "G. Qualitative Analysis with Visualization Methods", "text": "To understand the internal mechanism of the proposed model, we conducted a visual analysis with various visualization methods. First, we tried clustering for each layer\u2019s intermediate hidden states from a given input data sample to verify how the encoding behavior of each layer contributes to the clustering of input samples. We selected the\nt-distributed stochastic neighbor embedding (t-SNE) [47] algorithm, which is a technique for dimensionality reduction of high-dimensional data. Second, we exploited the deconvolution [33], [48] method to identify the functionality of each unit in the proposed ConvNet model by visual analysis. Our system basically repeats two convolutional layers followed by one pooling layer, and we grouped these three components and call it \u201cconvolutional block\u201d throughout this section for simplicity.\nThe t-SNE algorithm is based on the stochastic neighbor embedding (SNE) algorithm, which converts the similarities between given data points to joint probability and then embeds high-dimensional data points to lower-dimensional space by minimizing the Kuller-Leibler divergence between the joint probability of low-dimensional embedding and the high-dimensional data points. This method is highly effective especially in a dataset where its dimension is very high [47]. This advantage of the algorithm accorded well with our condition, where the target observations were necessarily in a high dimension since we reshaped each layer\u2019s filter activations to a single vector respectively.\nWith the visualization exploiting t-SNE, we could observe how each layer contributed to the classification of the dataset. Reflecting a gradually changing inter-distance of data points at each stage of the proposed model, four intermediate activations were extracted at the end of each convolutional block and one from the hidden fully connected layer, and another one from the final output layer. For the compression of dimensionality and computational efficiency, we pooled the maximum values for activation matrices of each unit. By this process, the dimensionality of each layer\u2019s output could be diminished to each layer\u2019s unit size. We visualized on both randomly selected training and validation data samples from the entire dataset to verify both how the model exactly works and how it generalizes its classification capability. In Figure 6, it is clearly\nshown that data samples under the same class of instrument are well grouped and each group is separated farther, with the level of encoding being higher, particularly on the training set. While the clustering was not clearer than the former case, the tendency of clustering on the validation set was also found to be similar to the training set condition.\nAnother visualization method, deconvolution, has recently been introduced as a useful analysis tool to qualitatively evaluate each node of a ConvNet. The main principle of this method is to inverse every stage of operations reaching to the target unit, to generate a visually inspectable image that has been, as a consequence, filtered by the trained subfunctionality of the target unit [33]. With this method, it is possible to reveal intuitively how each internal sub-function works within the entire deep convolutional network, which tends to be thought of as a \u201cblack box\u201d.\nBy this process, the functionality of a sub-part of the proposed model is explored. We generated deconvoluted images like those in Figure 7 from the arbitrary input melspectrogram, for each unit in the entire model. From the visual analysis of the resulting images, we could see several aspects of the sub-functionalities of the proposed model: (1) Most units in the first layer tend to extract vertical, horizontal, and diagonal edges from the input spectrogram, just like the lower layers of ConvNets do in the usual image object recognition task. (2) From the second layer through the fourth layer, each deconvoluted image indicates that each unit of the mid-layers has a functionality that searches for particular combinations of the edges extracted from the first layer. (3) It was found that it is difficult to strongly declare each sub-part of the proposed model that detects a specific musical articulation or expression. However, in an inductive manner, we could see that some units indicate that they can be understood as a sub-function of such musical expression detector.\nWe conducted a visual analysis of the deconvoluted image of two independent music signals, which have the same kind of sound sources, but differently labeled.1 For both cases, the most activated units of the first layer strongly suggested that their primary functionality is to detect a harmonic component on the input mel-spectrogram by finding horizontal edges in it, as shown in the top figures in Figure 7. However, from the second layer to higher layers, the highly activated units\u2019 behavior appeared to be quite different for each respective input signal. For instance, the most activated unit of signal (A)\u2019s second layer showed a functionality similar to onset detection, by detecting a combination of vertical and horizontal edges. Compared to this unit, the most activated units of the third layer showed a different functionality that seems to activate unstable components such as the vibrato articulation or the \u201cslur\u201d of the singing voice part, by detecting a particular combination of diagonal and horizontal edges. On the other hand, the model\u2019s behavior in signal (B) was very different. As is clearly shown in the second and the third layers\u2019 output in Figure 7, the highly activated sub-functions were trying to detect a dense field of stable, horizontal edges which are\n1Both signals were composed of a \u201cvoice\u201d and an \u201cacoustic guitar\u201d instrument, but the predominant instrument of signal (A) was labeled as the \u201cvoice,\u201d while (B) was labeled as the \u201cacoustic guitar\u201d.\noften found in harmonic instruments like guitar. Each field detected from those units corresponded to the region where the strumming acoustic guitar sound is."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we described how to apply ConvNet to identify predominant instrument in the real-world music. We trained the network using fixed-length single-labeled data, and identify an arbitrary number of the predominant instrument in a music clip with a variable length.\nOur results showed that very deep ConvNet is capable of achieving good performance by learning the appropriate feature automatically from the input data. Our proposed ConvNet architecture outperformed previous state-of-the-art approaches in a predominant instrument identification task on the IRMAS dataset. Mel-spectrogram was used as an input to the ConvNet, and we did not use any source separation in the preprocessing unlike in existing works.\nWe conducted several experiments with various activation functions for ConvNet. Tanh and ReLU were used as a baseline, and the recently introduced LReLU and PReLU were also evaluated. Results confirmed that ReLU worked reasonably well, which is a de facto standard in recent ConvNet studies. Furthermore, we obtained the better results with LReLU than with normal ReLU, especially with the very leaky setting (\u03b1 = 0.33). The performance of Tanh was worse than those of other rectifier functions as expected, and PReLU just showed a matching performance with ReLU for our task.\nThis paper also investigated different aggregation methods for ConvNet outputs that can be applied to music excerpts with various lengths. We experimented with two different aggregation methods, which are the class-wise mean probability S2 and the class-wise sum followed by normalization S2. The experimental results showed that S2 is a better aggregation method because it effectively deals with the quality difference between audios through the audio-excerpt-wise normalization process. In addition, we conducted an extensive experiment with various analysis window sizes and identification thresholds. For the analysis window size, using a shorter window improved the performance by increasing the temporal resolution. However, 0.5 s was too short to obtain an accurate identification performance, and 1.0 s was found to be the optimal window size. There was a trade-off between precision and recall, depending on the identification threshold; hence, we used an F1 measure, which is the harmonic mean of precision and recall. For the result, a threshold value of 0.5 showed the best performance.\nVisualization of the intermediate outputs using t-SNE showed that the feature representation became clearer each time the input data were passed through the convolutional blocks. Moreover, visualization using deconvolution showed that the lower layer tended to capture the horizontal and vertical edges, and that the higher layer tended to seek the combination of these edges to describe the spectral characteristics of the instruments.\nOur study shows that many recent advances in a neural network on the image processing area are transferable to the\naudio processing domain. However, audio signal processing, especially music signal processing, has many different aspects compared to the image processing area where ConvNets are most extensively used. For example, spectral characteristics are usually overlapped in both time and frequency unlike the objects in an image, which makes the detection difficult. Moreover, music signals are much more repetitive and continuous compared to natural images and are present in various lengths. We believe that applying more musical knowledge on the aggregation part with adaptive thresholding for each instrument can improve the performance further, which warrants deeper investigation."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported partly by the MSIP (Ministry of Science, ICT and Future Planning), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2016-H8501-16-1016) supervised by the IITP (Institute for Information & communications Technology Promotion), and partly by a National Research Foundation of Korea (NRF) grant funded by the MSIP (NRF-2014R1A2A2A04002619)."}], "references": [{"title": "Musical instrument recognition using cepstral coefficients and temporal features", "author": ["A. Eronen", "A. Klapuri"], "venue": "Acoustics, Speech, and Signal Processing, 2000. ICASSP\u201900. Proceedings. 2000 IEEE International Conference on, vol. 2. IEEE, 2000, pp. II753\u2013II756.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Modified group delay feature for musical instrument recognition", "author": ["A. Diment", "P. Rajan", "T. Heittola", "T. Virtanen"], "venue": "10th International Symposium on Computer Music Multidisciplinary Research (CMMR). Marseille, France, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse cepstral codes and power scale for instrument identification", "author": ["L.-F. Yu", "L. Su", "Y.-H. Yang"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 7460\u20137464.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Music instrument recognition: from isolated notes to solo phrases", "author": ["A. Krishna", "T.V. Sreenivas"], "venue": "Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP\u201904). IEEE International Conference on, vol. 4. IEEE, 2004, pp. iv\u2013265.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Musical instrument recognition on solo performances", "author": ["S. Essid", "G. Richard", "B. David"], "venue": "Signal Processing Conference, 2004 12th European. IEEE, 2004, pp. 1289\u20131292.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Musical instrument recognition in polyphonic audio using source-filter model for sound separation.", "author": ["T. Heittola", "A. Klapuri", "T. Virtanen"], "venue": "in ISMIR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Instrument identification in polyphonic music: Feature weighting to minimize influence of sound overlaps", "author": ["T. Kitahara", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "EURASIP Journal on Applied Signal Processing, vol. 2007, no. 1, pp. 155\u2013155, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A novel cepstral representation for timbre modeling of sound sources in polyphonic mixtures", "author": ["Z. Duan", "B. Pardo", "L. Daudet"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 7495\u20137499.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Rwc music database: Music genre database and musical instrument sound database.", "author": ["M. Goto", "H. Hashiguchi", "T. Nishimura", "R. Oka"], "venue": "in ISMIR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning: Methods and applications", "author": ["L. Deng", "D. Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3\u20134, pp. 197\u2013387, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in INTERSPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Investigation of recurrentneural-network architectures and learning methods for spoken language understanding.", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "INTERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recurrent neural networks for language understanding.", "author": ["K. Yao", "G. Zweig", "M.-Y. Hwang", "Y. Shi", "D. Yu"], "venue": "INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning algorithms for classification: A comparison on handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "C. Cortes", "J.S. Denker", "H. Drucker", "I. Guyon", "U. Muller", "E. Sackinger", "P. Simard"], "venue": "Neural networks: the statistical mechanics perspective, vol. 261, p. 276, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Handwritten digit recognition using convolutional neural networks and gabor filters", "author": ["A. Calder\u00f3n", "S. Roa", "J. Victorino"], "venue": "Proc. Int. Congr. Comput. Intell, 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "A novel hybrid cnn\u2013svm classifier for recognizing handwritten digits", "author": ["X.-X. Niu", "C.Y. Suen"], "venue": "Pattern Recognition, vol. 45, no. 4, pp. 1318\u20131325, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Tiled convolutional neural networks", "author": ["J. Ngiam", "Z. Chen", "D. Chia", "P.W. Koh", "Q.V. Le", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2010, pp. 1279\u20131287.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio.", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "in ISMIR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Improved musical onset detection with convolutional neural networks", "author": ["J. Schluter", "S. Bock"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6979\u20136983.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Rethinking automatic chord recognition with convolutional neural networks", "author": ["E.J. Humphrey", "J.P. Bello"], "venue": "Machine Learning and Applications (ICMLA), 2012 11th International Conference on, vol. 2. IEEE, 2012, pp. 357\u2013362.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Audio chord recognition with recurrent neural networks.", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ISMIR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Boundary detection in music structure analysis using convolutional neural networks.", "author": ["K. Ullrich", "J. Schl\u00fcter", "T. Grill"], "venue": "in ISMIR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Music boundary detection using neural networks on combined features and two-level annotations", "author": ["T. Grill", "J. Schl\u00fcter"], "venue": "Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR 2015), Malaga, Spain, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Musical instrument sound classification with deep convolutional neural network using feature fusion approach", "author": ["T. Park", "T. Lee"], "venue": "arXiv preprint arXiv:1512.07370, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic instrument recognition in polyphonic music using convolutional neural networks", "author": ["P. Li", "J. Qian", "T. Wang"], "venue": "arXiv preprint arXiv:1511.05520, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Y. Hoshen", "R.J. Weiss", "K.W. Wilson"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4624\u20134628.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks", "author": ["D. Palaz", "R. Collobert", "M.M. Doss"], "venue": "arXiv preprint arXiv:1304.1018, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning sparse feature representations for music annotation and retrieval.", "author": ["J. Nam", "J. Herrera", "M. Slaney", "J.O. Smith"], "venue": "in ISMIR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "arXiv preprint arXiv:1312.4400, 2013.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from over-  JOURNAL OF  LATEX CLASS FILES, VOL. 14, NO. 8, MAY 2016  13 fitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1929}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "International conference on artificial intelligence and statistics, 2010, pp. 249\u2013256.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Recent advances in convolutional neural networks", "author": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang"], "venue": "arXiv preprint arXiv:1512.07108, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, 2013, p. 1.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1026\u20131034.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of rectified activations in convolutional network", "author": ["B. Xu", "N. Wang", "T. Chen", "M. Li"], "venue": "arXiv preprint arXiv:1505.00853, 2015.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of sound segregation techniques for predominant instrument recognition in musical audio signals.", "author": ["J.J. Bosch", "J. Janer", "F. Fuhrmann", "P. Herrera"], "venue": "in ISMIR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "Polyphonic instrument recognition for exploring semantic similarities in music", "author": ["F. Fuhrmann", "P. Herrera"], "venue": "Proc. of 13th Int. Conference on Digital Audio Effects DAFx10, 2010, pp. 1\u20138.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Harmonic and percussive sound separation and its application to mir-related tasks", "author": ["N. Ono", "K. Miyamoto", "H. Kameoka", "J. Le Roux", "Y. Uchiyama", "E. Tsunoo", "T. Nishimoto", "S. Sagayama"], "venue": "Advances in music information retrieval. Springer, 2010, pp. 213\u2013236.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "To name a few, Eronen used cepstral coefficients and temporal features to classify 30 orchestral instruments with several articulation styles and achieved a classification accuracy of 95% for instrument family level and about 81% for individual instruments [1].", "startOffset": 257, "endOffset": 260}, {"referenceID": 1, "context": "used a modified group delay feature that incorporates phase information together with mel-frequency cepstral coefficients (MFCCs) and achieved a classification accuracy of about 71% for 22 instruments [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "on cepstrum with temporal sum-pooling and achieved an Fmeasure of about 96% for classifying 50 instruments [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Some previous works such as Krishna and Sreenivas [4] experimented with a classification for solo phrases rather than for isolated notes.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "[5] reported that a classification system with MFCCs and GMM along", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] used a non-negative matrix factorization (NMF)-based source-filter model with MFCCs and GMM for synthesized polyphonic sound and achieved a recognition rate of 59% for six polyphonic notes randomly generated from 19 instruments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used various spectral, temporal, and modulation features with PCA and linear discriminant analysis (LDA) for classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed the uniform discrete cepstrum (UDC) and mel-scale UDC (MUDC) as a spectral representation with a radial basis function (RBF) kernel support vector machine (SVM) to classify 13 types of Western instruments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The classification accuracy of randomly mixed chords of two and six polyphonic notes, generated using isolated note samples from the RWC musical instrument sound database [9], was around 37% for two polyphony notes and 25% for six polyphony notes.", "startOffset": 171, "endOffset": 174}, {"referenceID": 9, "context": "However, it is getting more common to design the system to automatically discover the higherlevel representation from the raw data by stacking several layers of nonlinear modules, which is called deep learning [10].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "DNN-based approaches have outperformed previous state-of-the-art methods in speech applications such as phone recognition, largevocabulary speech recognition, multi-lingual speech recognition, and noise-robust speech recognition [11].", "startOffset": 229, "endOffset": 233}, {"referenceID": 11, "context": "It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "It has been reported that RNNs have shown a successful result on language modeling [12] and spoken language understanding [13], [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 9, "context": "On the other hand, ConvNet is useful for data with local groups of values that are highly correlated, forming distinctive local characteristics that might appear at different parts of the array [10].", "startOffset": 194, "endOffset": 198}, {"referenceID": 14, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "digit recognition [15], [16], [17] for the MNIST dataset and image tagging [18], [19] for the CIFAR-10 dataset.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "In addition, it has been reported that it has outperformed state-of-the-art approaches for several computer vision benchmark tasks such as object detection, semantic segmentation, and category-level object recognition [11], and also for speech-recognition tasks", "startOffset": 218, "endOffset": 222}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "These hierarchical network structures of ConvNets are highly suitable for representing music audio, because music tends to present a hierarchical structure in time and different features of the music might be more salient at different time scales [21].", "startOffset": 247, "endOffset": 251}, {"referenceID": 21, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 23, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 24, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 218, "endOffset": 222}, {"referenceID": 25, "context": "It has been reported that ConvNet has outperformed previous state-of-the-art approaches for various MIR tasks such as onset detection [22], automatic chord recognition [23], [24], and music structure/boundary analysis [25], [26].", "startOffset": 224, "endOffset": 228}, {"referenceID": 26, "context": "[27] and Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28], although it is still an ongoing work and is not a predominant instrument recognition method; hence, there are no other instruments but only target instrument sounds exist.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Our research differs from [27] because we deal with polyphonic music, while their work is based on the studio recording of single tones.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "In addition, our research also differs from [28] because we use single-label data for training and estimate multi-label data, while they used multilabel data from the training phase.", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "on an end-to-end approach, which is promising in that using raw audio signals makes the system rely less on domain knowledge and preprocessing, but usually it shows a slightly lower performance than using spectral input such as melspectrogram in recent papers [29], [30].", "startOffset": 260, "endOffset": 264}, {"referenceID": 29, "context": "on an end-to-end approach, which is promising in that using raw audio signals makes the system rely less on domain knowledge and preprocessing, but usually it shows a slightly lower performance than using spectral input such as melspectrogram in recent papers [29], [30].", "startOffset": 266, "endOffset": 270}, {"referenceID": 9, "context": "The convolutional neural network is one of the representation learning methods that allow a machine to be fed with raw data and to automatically discover the representations needed for classification or detection [10].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "[31] and Hamel et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21], which is a reasonable setting that sufficiently preserves the harmonic characteristics of the music while greatly reducing the dimensionality of the input data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Our ConvNet architecture generally follows a popular AlexNet [18] and VGGNet [32] structure, which contains very deep architecture using repeated several convolution layers followed by max-pooling, as shown in Figure 1.", "startOffset": 61, "endOffset": 65}, {"referenceID": 31, "context": "Our ConvNet architecture generally follows a popular AlexNet [18] and VGGNet [32] structure, which contains very deep architecture using repeated several convolution layers followed by max-pooling, as shown in Figure 1.", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "from Zeiler and Fergus [33] and Sermanet et al.", "startOffset": 23, "endOffset": 27}, {"referenceID": 33, "context": "[34], which has shown superior performance in ILSVRC-2013.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "performance for image processing datasets such as CIFAR-10 and MNIST [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "We used Adam [36] as an optimizer with a learning rate of 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "Dropout is a technique that prevents the overfitting of units to the training data by randomly dropping some units from the neural network during the training phase [37].", "startOffset": 165, "endOffset": 169}, {"referenceID": 37, "context": "We used a uniform distribution with zero biases for both convolutional and fully connected layers following Glorot and Bengio [38].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "However, non-saturating nonlinearities such as the rectified linear unit (ReLU) allow much faster learning than these saturating nonlinearities, particularly for models that are trained on large datasets [18].", "startOffset": 204, "endOffset": 208}, {"referenceID": 38, "context": "the performance of ReLU is better than that of sigmoid and tanh activation [39].", "startOffset": 75, "endOffset": 79}, {"referenceID": 27, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 89, "endOffset": 93}, {"referenceID": 31, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "Thus, most of the modern studies on ConvNets use ReLU to model the output of the neurons [28], [32], [33], [34].", "startOffset": 107, "endOffset": 111}, {"referenceID": 39, "context": "on restricted Boltzmann machines [40].", "startOffset": 33, "endOffset": 37}, {"referenceID": 40, "context": "[41], compresses the negative part rather than make it all zero, which might cause some initially inactive units to remain inactive.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42], is basically similar to LReLU in that it compresses the negative part.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] reported that the performance of LReLU is better than those of ReLU and PReLU, but sometimes it is worse than that of basic ReLU, depending on the dataset and the value for \u03b1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "01) were used, because it has been reported that the performance of LReLU considerably differs depending on the value and that very leaky ReLU works better [43].", "startOffset": 156, "endOffset": 160}, {"referenceID": 43, "context": "[44] and includes music from various decades from the past century, hence differing in audio quality to a great extent.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "The existing algorithm from Fuhrmann and Herrera [45] used typical hand-made timbral audio features with their frame-", "startOffset": 49, "endOffset": 53}, {"referenceID": 43, "context": "[44] improved this algorithm with source separation called FASST (Flexible Audio Source Separation Framework) [46] in a preprocessing step.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[44] improved this algorithm with source separation called FASST (Flexible Audio Source Separation Framework) [46] in a preprocessing step.", "startOffset": 110, "endOffset": 114}, {"referenceID": 44, "context": "Performance comparison of the predominant instrument recognition algorithm from Fuhrmann and Herrra [45], Bosch et al.", "startOffset": 100, "endOffset": 104}, {"referenceID": 43, "context": "[44], and our proposed ConvNet.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42], PReLU did not show any performance improvement, but just showed a matching performance with ReLU in our task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43].", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "We selected the t-distributed stochastic neighbor embedding (t-SNE) [47] algorithm, which is a technique for dimensionality reduction of high-dimensional data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "Second, we exploited the deconvolution [33], [48] method to identify the functionality of each unit in the proposed ConvNet model by visual analysis.", "startOffset": 39, "endOffset": 43}, {"referenceID": 46, "context": "This method is highly effective especially in a dataset where its dimension is very high [47].", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "The main principle of this method is to inverse every stage of operations reaching to the target unit, to generate a visually inspectable image that has been, as a consequence, filtered by the trained subfunctionality of the target unit [33].", "startOffset": 237, "endOffset": 241}], "year": 2017, "abstractText": "Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the field of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identification threshold, and activation functions for neural networks to find the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.", "creator": "LaTeX with hyperref package"}}}