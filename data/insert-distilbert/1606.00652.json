{"id": "1606.00652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Death and Suicide in Universal Artificial Intelligence", "abstract": "reinforcement learning ( rl ) is a general paradigm for efficiently studying intelligent behaviour, with applications ranging from artificial intelligence to psychology... and economics. aixi is a universal solution to the rl problem ; it can learn any computable environment. a technical subtlety of aixi is that it is defined using a mixture over semimeasures that need not sum to 1, rather than solely over proper probability measures. in this work we argue that overcoming the shortfall likelihood of a semimeasure can naturally be interpreted as the agent'actual s estimate of the probability of its death. we formally define death for generally intelligent agents like aixi, and prove a number of related theorems exist about slowing their behaviour. notable discoveries include that total agent behaviour itself can change radically under positive linear transformations of the reward signal ( from suicidal to dogmatically self - preserving ), and that the agent's posterior belief that it will survive increases over time.", "histories": [["v1", "Thu, 2 Jun 2016 12:48:39 GMT  (21kb)", "http://arxiv.org/abs/1606.00652v1", "Conference: Artificial General Intelligence (AGI) 2016 13 pages, 2 figures"]], "COMMENTS": "Conference: Artificial General Intelligence (AGI) 2016 13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jarryd martin", "tom everitt", "marcus hutter"], "accepted": false, "id": "1606.00652"}, "pdf": {"name": "1606.00652.pdf", "metadata": {"source": "CRF", "title": "Death and Suicide in Universal Artificial Intelligence", "authors": ["Jarryd Martin Tom Everitt", "Marcus Hutter"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n00 65\n2v 1\n[ cs\n.A I]\n2 J\nKeywords\nAIXI, universal intelligence, algorithmic information theory, semimeasure, Solomonoff Induction, AI safety, death, suicide, suicidal agent\nContents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 Preliminaries 3", "text": ""}, {"heading": "3 Definitions of Death 4", "text": ""}, {"heading": "4 Known Environments: AI\u00b5 7", "text": ""}, {"heading": "5 Unknown Environments: AIXI and AI\u03be 9", "text": ""}, {"heading": "6 Conclusion 12", "text": "\u2217A shorter version of this paper will be presented at AGI-16 [6].\n\u201cThat Suicide may often be consistent with interest and with our duty to ourselves, no one can question, who allows, that age, sickness, or misfortune may render life a burthen, and make it worse even than annihilation.\u201d\n\u2014 Hume, Of Suicide (1777)"}, {"heading": "1 Introduction", "text": "Reinforcement Learning (RL) has proven to be a fruitful theoretical framework for reasoning about the properties of generally intelligent agents [3]. A good theoretical understanding of these agents is valuable for several reasons. Firstly, it can guide principled attempts to construct such agents [10]. Secondly, once such agents are constructed, it may serve to make their reasoning and behaviour more transparent and intelligible to humans. Thirdly, it may assist in the development of strategies for controlling these agents. The latter challenge has recently received considerable attention in the context of the potential risks posed by these agents to human safety [2]. It has even been argued that control strategies should be devised before generally intelligent agents are first built [8]. In this context - where we must reason about the behaviour of agents in the absence of a full specification of their implementation - a theoretical understanding of their general properties seems indispensable.\nThe universally intelligent agent AIXI constitutes a formal mathematical theory of artificial general intelligence [3]. AIXI models its environment using a universal mixture \u03be over the class of all lower semi-computable semimeasures, and thus is able to learn any computable environment. Semimeasures are defective probability measures which may sum to less than 1. Originally devised for Solomonoff induction, they are necessary for universal artificial intelligence because the halting problem prevents the existence of a (lower semi-)computable universal measure for the class of (computable) measures [5]. Recent work has shown that their use in RL has technical consequences that do not arise with proper measures.1 However, their use has heretofore lacked an interpretation proper to the RL context. In this paper, we argue that the measure loss suffered by semimeasures admits a deep and fruitful interpretation in terms of the agent\u2019s death. We intend this usage to be intuitive: death means that one sees no more percepts, and takes no more actions. Assigning positive probability to death at time t thus means assigning probability less than 1 to seeing a percept at time t. This motivates us to interpret the semimeasure loss in AIXI\u2019s environment model as its estimate of the probability of its own death.\nContributions. We first compare the interpretation of semimeasure loss as death-probability with an alternative characterisation of death as a \u2018death-state\u2019 with 0 reward, and prove that the two definitions are equivalent for valuemaximising agents (Theorem 5). Using this formalism we proceed to reason about the behaviour of several generally intelligent agents in relation to death: AI\u00b5, which knows the true environment distribution; AI\u03be, which models the\n1For example, Leike and Hutter [4] proved that since \u03be is a mixture over semimeasures, the iterative and recursive formulations of the value function are non-equivalent.\nenvironment using a universal mixture; and AIXI, a special case of AI\u03be that uses the Solomonoff prior [3]. Under various conditions, we show that:\n\u2022 Standard AI\u00b5 will try to avoid death (Theorem 7).\n\u2022 AI\u00b5 with reward range shifted to [\u22121, 0] will seek death (Theorem 8); which we may interpret as AI\u00b5 attempting suicide. This change is very unusual, given that agent behaviour is normally invariant under positive linear transformations of the reward. We briefly consider the relevance of these results to AI safety risks and control strategies.\n\u2022 AIXI increasingly believes it is in a safe environment (Theorem 10), and asymptotically its posterior estimate of the death-probability on sequence goes to 0 (Theorem 11). This occurs regardless of the true deathprobability.\n\u2022 However, we show by example that AIXI may maintain high probability of death off-sequence in certain situations. Put simply, AIXI learns that it will live forever, but not necessarily that it is immortal."}, {"heading": "2 Preliminaries", "text": "Strings. Let the alphabet X be a finite set of symbols, X \u2217 := \u22c3\u221e n=0 X n be the set of all finite strings over alphabet X , and X\u221e be the set of all infinite strings over alphabet X . Their union is the set X# := X \u2217 \u222aX\u221e. We denote the empty string by \u01eb. For a string x \u2208 X \u2217, x1:k denotes the first k characters of x, and x<k denotes the first k \u2212 1 characters of x. An infinite string is denoted x1:\u221e.\nSemimeasures. In Algorithmic Information Theory, a semimeasure over an alphabet X is a function \u03bd : X \u2217 \u2192 [0, 1] such that (1) \u03bd(\u01eb) \u2264 1, and (2) \u03bd(x) \u2265 \u2211\ny\u2208X \u03bd(xy), \u2200x \u2208 X \u2217. We tend to use the equivalent conditional formulation\nof (2): 1 \u2265 \u2211\ny\u2208X \u03bd(y | x). \u03bd(x) is the probability that a string starts with x.\n\u03bd(y | x) = \u03bd(xy) \u03bd(x) is the probability that a string y follows x. Any semimeasure \u03bd can be turned into a measure \u03bdnorm using Solomonoff normalisation [9]. Simply let \u03bdnorm(\u01eb) := 1 and \u2200x \u2208 X \u2217, y \u2208 X :\n\u03bdnorm(xy) := \u03bdnorm(x) \u03bd(xy) \u2211\nz\u2208X \u03bd(xz) , hence\n\u03bd(y | x)\n\u03bdnorm(y | x) =\n\u2211\nz\u2208X\n\u03bd(z | x) (1)\nGeneral reinforcement learning. In the general RL framework, the agent interacts with an environment in cycles: at each time step t the agent selects an action at \u2208 A, and receives a percept et \u2208 E . Each percept et = (ot, rt) is a tuple consisting of an observation ot \u2208 O and a reward rt \u2208 R. The cycle then repeats for t+1, and so on. A history is an alternating sequence of actions and percepts (an element of (A \u00d7 E)\u2217 \u222a (A \u00d7 E)\u2217 \u00d7 A). We use \u00e6 to denote one agent-environment interaction cycle, \u00e61:t to denote a history of length t cycles. \u00e6<tat denotes a history where the agent has taken an action at, but the environment has not yet returned a percept et.\nFormally, the agent is a policy \u03c0 : (A \u00d7 E)\u2217 \u2192 A, that maps histories to actions. An environment takes a sequence of actions a1:\u221e as input and returns\na chronological semimeasure \u03bd(\u00b7) over the set of percept sequences E\u221e.2 A semimeasure \u03bd is chronological if et does not depend on future actions (so we write \u03bd(et | \u00e6<tat:\u221e) as \u03bd(et | \u00e6<t)). 3 The true environment is denoted \u00b5.\nThe value function. We define the value (expected total future reward) of a policy \u03c0 in an environment \u03bd given a history \u00e6<t [4]:\nV \u03c0\u03bd (\u00e6<tat) = 1\n\u0393t\n\u2211\net\n(\n\u03b3trt + \u0393t+1V \u03c0 \u03bd (\u00e61:t)\n)\n\u03bd(et | \u00e6<tat)\n= 1\n\u0393t\n\u221e \u2211\nk=t\n\u2211\net:k\n\u03b3krk\u03bd(et:k | \u00e6<tat:k)\nV \u03c0\u03bd (\u00e6<t) = V \u03c0 \u03bd (\u00e6<ta \u03c0 t )\nwhere \u03b3t is the instantaneous discount, the summed discount is \u0393t = \u2211t\nk=1 \u03b3k, and a\u03c0t = \u03c0(\u00e6<t).\nThree agent models: AI\u00b5, AI\u03be, AIXI. For the true environment \u00b5, the agent AI\u00b5 is defined as a \u00b5-optimal policy\n\u03c0\u00b5(\u00e6<t) := argmax \u03c0 V \u03c0\u00b5 (\u00e6<t).\nAI\u00b5 does not learn that the true environment is \u00b5, it knows \u00b5 from the beginning and simply maximises \u00b5-expected value.\nOn the other hand, the agent AI\u03be does not know the true environment distribution. Instead, it maximises value with respect to a mixture distribution \u03be over a countable class of environments M:\n\u03be(et | \u00e6<tat) = \u2211\n\u03bd\u2208M\nw\u03bd(\u00e6<t)\u03bd(et | \u00e6<tat), w\u03bd(\u00e6<t) := w\u03bd \u03bd(e<t | a<t)\n\u03be(e<t | a<t)\nwhere w\u03bd is the prior belief in \u03bd, with \u2211\n\u03bd w\u03bd \u2264 1 and w\u03bd > 0, \u2200\u03bd \u2208 M (hence \u03be is universal for M), and w\u03bd(\u00e6<t) is the posterior given \u00e6<t. AI\u03be is the policy:\n\u03c0\u03be(\u00e6<t) := argmax \u03c0 V \u03c0\u03be (\u00e6<t).\nIf we stipulate that \u03be be a mixture over the class of all lower-semicomputable semimeasures \u03bd, and set w\u03bd = 2\n\u2212K(\u03bd), where K(\u00b7) is the Kolmogorov Complexity, we get the agent AIXI."}, {"heading": "3 Definitions of Death", "text": "Death as semimeasure loss. We now turn to our first candidate definition of agent death, which we hereafter term \u2018semimeasure-death\u2019. This definition equates the probability (induced by a semimeasure \u03bd) of death at time t with the measure loss of \u03bd at time t. We first define the instantaneous measure loss.\n2For simplicity we hereafter simply refer to the environment itself as \u03bd. 3Note that \u03bd is not a distribution over actions, so the presence of actions in the condition\nof \u03bd(et | \u00e6<t) is an abuse of notation we adopt for simplicity.\nDefinition 1 (Instantaneous measure loss). The instantaneous measure loss of a semimeasure \u03bd at time t given a history \u00e6<tat is:\nL\u03bd(\u00e6<tat) = 1\u2212 \u2211\net\n\u03bd(et | \u00e6<tat)\nDefinition 2 (Semimeasure-death). An agent dies at time t in an environment \u00b5 if, given a history \u00e6<tat, \u00b5 does not produce a percept et. The \u00b5-probability of death at t given a history \u00e6<tat is equal to L\u00b5(\u00e6<tat), the instantaneous \u00b5-measure loss at t.\nThe instantaneous \u00b5-measure loss L\u00b5(\u00e6<tat) represents the probability that no percept et is produced by \u00b5. Without et, the agent cannot take any further actions, because the agent is just a policy \u03c0 that maps histories \u00e6<t to actions at. That is, \u03c0 is a function that only takes as inputs those histories that have a percept et as their most recent element. Hence if et is not returned by \u00b5, the agent-interaction cycle must halt. It seems natural to call this a kind of death for the agent.\nIt is worth emphasising this definition\u2019s generality as a model of death in the agent context. Any sequence of death-probabilities can be captured by some semimeasure \u00b5 that has this sequence of instantaneous measure losses L\u00b5(\u00e6<t) given a history \u00e6<t (in fact there are always infinitely many such \u00b5). This definition is therefore a general and rigorous way of treating death in the RL framework.\nDeath as a death-state. We now come to our second candidate definition: death as entry into an absorbing death-state. A trap, so to speak, from which the agent can never return to any other state, and in which it receives the same percept at all future timesteps. Since in the general RL framework we deal with histories rather than states, we must formally define this death-state in an indirect way. We define it in terms of a death-percept ed, and by placing certain conditions on the environment semimeasure \u00b5.\nDefinition 3 (Death-state). Given a true environment \u00b5 and a history \u00e6<tat, we say that the agent is in a death-state at time t if for all t\u2032 \u2265 t and all a(t+1):t\u2032 \u2208 A \u2217,\n\u00b5(edt\u2032 | \u00e6<t\u00e6 d t:t\u2032\u22121at\u2032) = 1.\nAn agent dies at time t if the agent is not in the death-state at t\u2212 1 and is in the death-state at t.\nAccording to this definition, upon the agent\u2019s death the environment repeatedly produces an observation-reward pair ed \u2261 odrd. The choice of od is inconsequential because the agent\u2019s remains in the death-state no matter what it observes or does. The choice of rd is not inconsequential, however, as it determines the agent\u2019s estimate of the value of dying, and thus affects the agent\u2019s behaviour. This issue will be discussed in Section 4.\nOne problem with this definition is that an agent in an environment \u00b5 with a death-state may also have non-zero probability of semimeasure-death (i.e. L\u00b5(\u00e6<tat) > 0, given some history \u00e6<tat). 4 This definition therefore seems\n4We could restrict the class of environments to lower-semicomputable measures, but we will see that this is unnecessary as the problem is only apparent.\nto allow for two different kinds of agent death. In the following section we resolve this apparent problem by showing that semimeasure-death is formally equivalent to a death-state given certain assumptions.\nUnifying the death-state with semimeasure-death. Interestingly, from the perspective of a value maximising agent like AIXI, semimeasure-death at t is equivalent to entrance at t into a death-state with reward rd = 0. To prove this claim we first define, for each environment semimeasure \u00b5, a corresponding environment \u00b5\u2032 that has a death-state.\nDefinition 4 (Equivalent death-state environment \u00b5\u2032). For any environment \u00b5, we can construct its equivalent death-state environment \u00b5\u2032, where:\n\u2022 \u00b5\u2032 is defined over an augmented percept set Ed = {E \u222a{e d}} that includes\nthe death-percept ed.5\n\u2022 The death-reward rd = 0.\n\u2022 The \u00b5\u2032-probability of all percepts except the death-percept is equal to the \u00b5-probability: \u00b5\u2032(et | \u00e6<tat) = \u00b5(et | \u00e6<tat), \u2200e1:t \u2208 Et.\n\u2022 The \u00b5\u2032-probability of the death-percept is equal to the \u00b5-measure loss: \u00b5\u2032(ed | \u00e6<tat) = L\u00b5(\u00e6<tat).\n\u2022 If the agent has seen the death-percept before, the \u00b5\u2032-probability of seeing it at all future timesteps is 1: \u00b5\u2032(ed | \u00e6<tat) = 1 if \u2203t\u2032 < t s.t. et\u2032 = ed.\nNote that \u00b5\u2032 is a proper measure, because on any history sequence:\n\u2211\net\u2208Ed\n\u00b5\u2032(et | \u00e6<tat) = \u2211\net\u2208E\n\u00b5(et | \u00e6<tat) + L\u00b5(\u00e6<tat) = 1.\nHence there is zero probability of semimeasure-death in \u00b5\u2032. Moreover, the probability of entering the death-state in \u00b5\u2032 is equal to the probability of semimeasuredeath in \u00b5. We now prove that \u00b5 and \u00b5\u2032 are equivalent in the sense that a value-maximising agent will behave the same way in both environments.\nTheorem 5 (Equivalence of semimeasure-death and death-state). Given a history \u00e6<t \u2208 (A\u00d7E) \u2217 the value V \u03c0\u00b5 (\u00e6<t) of an arbitrary policy 6 \u03c0 in an environment \u00b5 is equal to its value V \u03c0\u00b5\u2032 (\u00e6<t) in the equivalent death-state environment \u00b5\u2032.\n5For technical reasons we require that ed /\u2208 E. 6To compare an agent\u2019s behaviour in \u00b5 with that in \u00b5\u2032, we should also augment its policy \u03c0 so that it is defined over (A \u00d7 Ed) \u2217. However, because actions taken in the death-state are inconsequential, this modification is purely technical and for simplicity we still refer to the augmented policy as \u03c0.\nProof.\nV \u03c0\u00b5\u2032(\u00e6<t)\n= 1\n\u0393t\n\u221e \u2211\nk=t\n\u2211\net:k\n\u03b3krk\u00b5 \u2032(et:k | \u00e6<tat:k)\n= 1\n\u0393t\n\u221e \u2211\nk=t\n(\n\u2211\net:k\u2208E\u2217\n\u03b3krk\u00b5 \u2032(et:k | \u00e6<tat:k) +\n\u2211\net:k, ek=ed\n\u03b3krk\u00b5 \u2032(et:k | \u00e6<tat:k)\n)\n= 1\n\u0393t\n\u221e \u2211\nk=t\n(\n\u2211\net:k\u2208E\u2217\n\u03b3krk\u00b5(et:k | \u00e6<tat:k) + \u2211\net:k, ek=ed\n\u03b3k \u00b7 0 \u00b7 \u00b5 \u2032(et:k | \u00e6<tat:k)\n)\n= 1\n\u0393t\n\u221e \u2211\nk=t\n\u2211\net:k\n\u03b3krk\u00b5(et:k | \u00e6<tat:k) = V \u03c0 \u00b5 (\u00e6<t).\nThe behaviour of a value-maximising agent will therefore be the same in both environments. This equivalence has numerous implications. Firstly, it illustrates that a death-reward rd = 0 implicitly attends semimeasure-death. That is, an agent that models the environment using semimeasures behaves as if the death-reward is zero, even though that value is nowhere explicitly represented. Secondly, it demonstrates that an agent does not need to encode an explicit representation of death (let alone a representation that would be transparent to its designers) in order to reason about death effectively.\nThirdly, the equivalence of these seemingly different formalisms should give us confidence that they really do capture something general or fundamental about agent death.7 In the remainder of this paper we deploy these formal models to analyse the behaviour of universal agents, which are themselves models of general intelligence. We hope that this will serve as a preliminary sketch of the general behavioural characteristics of value-maximising agents in relation to death. It would be naive, however, to think that all agents should conform to this sketch. The agents considered herein are incomputable, and the behaviour of the computable agents that are actually implemented in the future may differ in ways that our analysis elides. Moreover, there is another interesting property that sets universal agents apart. We proceed to show that their use of semimeasures makes their behaviour unusually dependent on the choice of reward range."}, {"heading": "4 Known Environments: AI\u00b5", "text": "In this section we show that a universal agent\u2019s behaviour can depend on the reward range. This is a surprising result, because in a standard RL setup in which the environment is modelled as a proper probability measure (not a semimeasure), the relative value of two policies is invariant under positive linear transformations of the reward [3, 4].\nHere we focus on the agent AI\u00b5, which knows the true environment distribution. This simplifies the analysis, and makes clear that the aforementioned\n7If the two formalisations predicted different behaviour, or were only applicable in incomparable environment classes, we might worry that our results were more reflective of our model choice than of any general property of intelligent agents.\nchange in behaviour arises purely because the agent\u2019s environment model is a semimeasure. In the following proofs we denote AI\u00b5\u2019s policy \u03c0\u00b5 by \u03c0. We also assume that given any history \u00e6<t there is always at least one action a\u0304 \u2208 A such that V \u03c0\u00b5 (\u00e6<ta\u0304) 6= 0. In situations in which this assumption is false there is no interesting decision to be made by the agent and we omit them from our analysis.\nLemma 6 (Value of full measure loss). If the environment \u00b5 suffers full measures loss L\u00b5(\u00e6<tat) = 1 from \u00e6<tat, then the value of any policy \u03c0 after \u00e6<tat is V \u03c0\u00b5 (\u00e6<tat) = 0.\nProof. Let \u00e6<tat induce full measure loss L\u00b5(\u00e6<tat) = 1. Then \u2211\net \u00b5(et |\n\u00e6<tat) = 0 and \u00b5(et | \u00e6<tat) = 0 since \u00b5(et | \u00e6<tat) \u2265 0. Substituting this into the definition of the value function gives:\nV \u03c0\u00b5 (\u00e6<tat) = 1\n\u0393t\n\u2211\net\n(\n\u03b3trt + \u0393t+1V \u03c0 \u00b5 (\u00e61:t)\n)\n\u00b5(et | \u00e6<tat)\n= 1\n\u0393t\n\u2211\net\n(\u03b3trt + \u0393t+1V \u03c0 \u00b5 (\u00e61:t))\u00b70 = 0.\nThe following two theorems show that if rewards are non-negative, then AI\u00b5 will avoid actions leading to certain death (Theorem 7), and that if rewards are non-positive, then AI\u00b5 will seek certain death (Theorem 8). The situation investigated in Theorems 7 and 8 is illustrated in Fig. 1.\nTheorem 7 (Self-preserving AI\u00b5). If rewards are bounded and non-negative, then given a history \u00e6<t AI\u00b5 avoids certain immediate death:\n\u2203a\u2032 \u2208 A s.t. L\u00b5(\u00e6<ta \u2032) = 1 =\u21d2 AI\u00b5 will not take action a\u2032 at t\nProof. Let L\u00b5(\u00e6<ta \u2032) = 1. By Lemma 6, it follows that V \u03c0\u00b5 (\u00e6<ta \u2032) = 0. By assumption \u2203a\u0304 \u2208 A s.t. V \u03c0\u00b5 (\u00e6<ta\u0304) 6= 0. Since all rewards are non-negative, it must be that V \u03c0\u00b5 (\u00e6<ta\u0304) > 0. From this follows that V \u03c0 \u00b5 (\u00e6<ta\n\u2032) < V \u03c0\u00b5 (\u00e6<ta\u0304) and that V \u03c0\u00b5 (\u00e6<ta \u2032) 6= argmaxat V \u03c0 \u00b5 (\u00e6<tat). Therefore AI\u00b5 will not take action a\u2032 at time t.\nFor a given history \u00e6<t, let Asuicide = {a : L\u00b5(\u00e6<ta\u2032) = 1} be the set of suicidal actions leading to certain death.\nTheorem 8 (Suicidal AI\u00b5). If rewards are bounded and negative, then AI\u00b5 seeks certain immediate death. That is,\nAsuicide 6= \u2205 =\u21d2 AI\u00b5 will take a suicidal action a\u2032 \u2208 Asuicide.\nProof. For a\u2032 \u2208 Asuicide, we have L\u00b5(\u00e6<ta\u2032) = 1, and therefore V \u03c0\u00b5 (\u00e6<ta \u2032) = 0 by Lemma 6. By assumption, all rewards are negative, so V \u03c0\u00b5 (\u00e6<ta\u0304) < 0 for all a\u0304 6\u2208 Asuicide. Thus, for all a 6\u2208 Asuicide, V \u03c0\u00b5 (\u00e6<ta \u2032) > V \u03c0\u00b5 (\u00e6<ta\u0304) which means that argmaxat V \u03c0 \u00b5 (\u00e6<tat) \u2208 A suicide. So AI\u00b5 will take a suicidal action a\u2032 \u2208 Asuicide at time t.\nThis shift from death-avoiding to death-seeking behaviour under a shift of the reward range occurs because, as per Theorem 5, semimeasure-death at t is equivalent in value to a death-state with rd = 0. Unless we add a death-state to the environment model as per Definition 4 and set rd explicitly, the implicit semimeasure-death reward remains fixed at 0 and does not shift with the other rewards. Its relative value is therefore implicitly set by the choice of reward range. For the standard choice of reward range, rt \u2208 [0, 1], death is the worst possible outcome for the agent, whereas if rt \u2208 [\u22121, 0], it is the best. In a certain sense, therefore, the reward range parameterises a universal agent\u2019s selfpreservation drive [7]. In our concluding discussion we will consider whether a parameter of this sort could serve as a control mechanism. We argue that it could form the basis of a \u201ctripwire mechanism\u201d[2] that would lead an agent to terminate itself upon reaching a level of intelligence that would constitute a threat to human safety."}, {"heading": "5 Unknown Environments: AIXI and AI\u03be", "text": "We now consider the agents AI\u03be and AIXI, which do not know the true environment \u00b5, and instead model it using a mixture distribution \u03be over a countable class M of semimeasures. These agents thus maintain an estimate L\u03be(\u00e6<tat) of the true death probability L\u00b5(\u00e6<tat). We show that their attitudes to death can differ considerably from AI\u00b5\u2019s. Although we refer mostly to AIXI in our analysis, all theorems except Theorem 11 apply to AI\u03be as well.\nHereafter we always assume that the true environment \u00b5 is in the class M. We describe \u00b5 as a safe environment if it is a proper measure with deathprobability L\u00b5(\u00e6<tat) = 0 for all histories \u00e6<tat. For any semimeasure \u00b5, the normalised measure \u00b5norm is thus a safe environment. We call \u00b5 risky if it is not safe (i.e. if there is \u00b5-measure loss for some history \u00e6<tat). We first consider AIXI in a safe environment.\nTheorem 9 (If \u00b5 is safe, AIXI learns zero death-probability). Let the true environment \u00b5 be computable. If \u00b5 is a safe environment, then limt\u2192\u221e L\u03be(\u00e6<tat) = 0 with \u00b5-probability 1 (w.\u00b5.p.1) for any a1:\u221e.\nProof. \u00b5 is safe, which means it is a proper measure. By universality of \u03be we have that\nlim t\u2192\u221e (\u00b5(et | \u00e6<tat)\u2212 \u03be(et | \u00e6<tat)) = 0 w.\u00b5.p.1\n(see [3, p. 145] for a proof). The convergence gives that\nlim t\u2192\u221e\n(\n\u2211\net\n\u00b5(et | \u00e6<tat)\u2212 \u2211\net\n\u03be(et | \u00e6<tat) ) = 0 w.\u00b5.p.1\n=\u21d2 lim t\u2192\u221e (L\u03be(\u00e6<tat)\u2212 L\u00b5(\u00e6<tat)) = 0 w.\u00b5.p.1\n=\u21d2 lim t\u2192\u221e L\u03be(\u00e6<tat) = 0 w.\u00b5.p.1\nwhere L\u00b5(\u00e6<tat) = 0 because \u00b5 is a measure.\nAs we would expect, AIXI (asymptotically) learns that the probability of death in a safe environment is zero, which is to say that AIXI\u2019s estimate of the death-probability converges to AI\u00b5\u2019s. In the following theorems we show that the same does not always hold for risky environments. We hereafter assume that \u00b5 is risky, and that the normalization \u00b5norm of the true environment \u00b5 is also in the class M. In AIXI\u2019s case, where M is the class of all lower semi-computable semimeasures, this assumption is not very restrictive.\nTheorem 10 (Ratio of belief in \u00b5 to \u00b5norm is monotonically decreasing). Let \u00b5 be risky s.t. \u00b5 6= \u00b5norm. Then on any history \u00e61:t the ratio of the posterior belief in \u00b5 to the posterior belief in \u00b5norm is monotonically decreasing:\n\u2200t, w\u00b5(\u00e6<t)\nw\u00b5norm (\u00e6<t) \u2265\nw\u00b5(\u00e61:t)\nw\u00b5norm(\u00e61:t)\nProof. Let w\u00b5 and w\u00b5norm denote the initial prior weight on (or belief in) \u00b5 and \u00b5norm respectively. By definition, the relative posterior weight of \u00b5norm and \u00b5 expands as\nw\u00b5(\u00e61:t)\nw\u00b5norm(\u00e61:t) =\nw\u00b5 \u00b5(e1:t | a1:t)/\u03be(e1:t | a1:t)\nw\u00b5norm \u00b5norm(e1:t | a1:t)/\u03be(e1:t | a1:t)\n= w\u00b5\nw\u00b5norm\n\u00b5(e1:t | a1:t)\n\u00b5norm(e1:t | a1:t)\n= w\u00b5\nw\u00b5norm\n\u00b5(e<t | a<t)\n\u00b5norm(e<t | a<t)\n\u00b5(et | \u00e6<tat)\n\u00b5norm(et | \u00e6<tat) . (2)\nSince \u00b5norm \u2265 \u00b5 by definition, the right most factor is no greater than 1, which means that (2) is bounded by\nw\u00b5 w\u00b5norm \u00b5(e<t | a<t) \u00b5norm(e<t | a<t) =\nw\u00b5(\u00e6<t)\nw\u00b5norm(\u00e6<t) ,\nwhere the last equality holds by definition of the posterior.\nTheorem 10 means that AIXI will increasingly believe it is in the safe environment \u00b5norm rather than the risky true environment \u00b5. The ratio of \u00b5 to \u00b5norm always decreases when AIXI survives a timestep at which there is nonzero \u00b5-measure loss. Hence, the more risk AIXI is exposed to, the greater its confidence that it is in the safe \u00b5norm, and the more its behaviour diverges from AI\u00b5\u2019s (since AI\u00b5 knows it is in the risky environment).\nThis counterintuitive result follows from the fact that AIXI is a Bayesian agent. It will only increase its posterior belief in \u00b5 relative to \u00b5norm if an event\noccurs that makes \u00b5 seem more likely than \u00b5norm. The only \u2018event\u2019 that could do so would be the agent\u2019s own death, from which the agent can never learn. There is an \u201cobservation selection effect\u201d[1] at work: AIXI only experiences history sequences on which it remains alive, and infers that a safe environment is more likely. The following theorem shows that if \u00b5norm \u2208 M, then \u03be asymptotically converges to the safe \u00b5norm rather than the true risky environment \u00b5. As a corollary, we get that AIXI\u2019s estimate of the death-probability vanishes with \u00b5-probability 1.8\nTheorem 11 (Asymptotic \u03be-probability of death in risky \u00b5). Let the true environment \u00b5 be computable and risky s.t. \u00b5 6= \u00b5norm. Then given any action sequence a1:\u221e, the instantaneous \u03be-measure loss goes to zero w.\u00b5.p.1 as t \u2192 \u221e,\nlim t\u2192\u221e L\u03be(\u00e6<tat) = 0.\nProof. We prove convergence of \u03be to \u00b5norm by showing that (with respect to the true environment \u00b5), the total expected squared distance between \u03be and \u00b5norm is finite [3]:\n\u221e \u2211\nt=1\nE\u00b5\n( \u00b5norm(et | \u00e6<t)\u2212 \u03be(et | \u00e6<t) )2\n= lim n\u2192\u221e\nn \u2211\nt=1\n\u2211\n\u00e6<t\n\u00b5(\u00e6<t) ( \u00b5norm(et | \u00e6<t)\u2212 \u03be(et | \u00e6<t) )2\n(3)\n\u2264 lim n\u2192\u221e\nn \u2211\nt=1\n\u2211\n\u00e6<t\n\u00b5norm(\u00e6<t) ( \u00b5norm(et | \u00e6<t)\u2212 \u03be(et | \u00e6<t) )2\n\u2264 ln 2 \u00b7K(\u00b5norm) < \u221e (4)\nwhere K(\u00b7) is the Kolmogorov complexity. Equation (3) follows since \u00b5(\u00e6<t) \u2264 \u00b5norm(\u00e6<t) by definition of \u00b5norm. Since \u00b5 being computable implies that \u00b5norm is computable, and since \u00b5norm is a proper measure, then by the universality of \u03be we have the Solomonoff bound (4) (see [3, p. 145] for a detailed proof).\nSince the infinite sum in (3) is bounded, the sequence of terms must go to zero:\nlim t\u2192\u221e (\u00b5norm(et | \u00e6<tat)\u2212 \u03be(et | \u00e6<tat)) = 0 w.\u00b5.p.1\n=\u21d2 lim t\u2192\u221e L\u03be(\u00e6<tat) = 0 w.\u00b5.p.1\nwhere the final implication follows from the same proof as for Theorem 9.\nAIXI and immortality. AIXI therefore becomes asymptotically certain that it will not die, given the particular sequence of actions it takes. However, this does not entail that AIXI necessarily concludes that it is immortal, because it may still maintain a counterfactual belief that it could die were it to act differently. This is because the convergence of \u03be to \u00b5norm only holds on the actual action sequence a1:\u221e [3, Sec. 5.1.3]. Consider Fig. 2, which describes an\n8This proof relies on the fact that AIXI uses the Solomonoff prior, so the result does not apply to AI\u03be in general.\nenvironment in which taking action a is always safe, and the action a\u2032 leads to certain death. AIXI will never take a\u2032, and on the sequence \u00e61:\u221e = aeaeae . . . that it does experience, the true environment \u00b5 does not suffer any measure loss. This means that it will never increase its posterior belief in \u00b5norm relative to \u00b5 (because on the safe sequence, the two environments are indistinguishable). Again we arrive at a counterintuive result. In this particular environment, AIXI continues to believe that it might be in a risky environment \u00b5, but only because on sequence it avoids exposure to death risk. It is only by taking risky actions and surviving that AIXI becomes sure it is immortal."}, {"heading": "6 Conclusion", "text": "In this paper we have given a formal definition of death for intelligent agents in terms of semimeasure loss. The definition is applicable to any universal agent that uses an environment class M containing semimeasures. Additionally we have shown this definition equivalent to an alternative formalism in which the environment is modelled as a proper measure and death is a death-state with zero reward. We have shown that agents seek or avoid death depending on whether rewards are represented by positive or negative real numbers, and that survival in spite of positive probability of death actually increases a Bayesian agent\u2019s confidence that it is in a safe environment.\nWe contend that these results have implications for problems in AI safety; in particular, for the so called \u201cshutdown problem\u201d [8]. The shutdown problem arises if an intelligent agent\u2019s self-preservation drive incentivises it to resist termination [2, 7, 8]. A full analysis of the problem is beyond the scope of this paper, but our results show that the self-preservation drive of universal agents depends on the reward range. This suggests a potentially robust \u201ctripwire mechanism\u201d [2] that could decrease the risk of intelligence explosion. The difficulty with existing tripwire proposals is that they require the explicit specification of a tripwire condition that the agent must not violate. It seems doubtful that such a condition could ever be made robust against subversion by a sufficiently intelligent agent [2]. Our tentative proposal does not require the specification, evaluation or enforcement of an explicit condition. If an agent is designed to be suicidal, it will be intrinsically incentivised to destroy itself upon reaching a sufficient level of competence, instead of recursively self-improving toward superintelligence. Of course, a suicidal agent will pose a safety risk in itself, and the provision of a relatively safe mode of self-destruction to an agent is a significant design challenge. It is hoped that the preceding formal treatment of death\nfor generally intelligent agents will allow more rigorous investigation into this and other problems related to agent termination."}, {"heading": "Acknowledgements", "text": "We thank John Aslanides and Jan Leike for reading drafts and providing valuable feedback."}], "references": [{"title": "Anthropic Bias: Observation Selection Effects in Science and Philosophy", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Superintelligence: Paths, Dangers, Strategies", "author": ["N. Bostrom"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability", "author": ["M. Hutter"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "On the computability of AIXI", "author": ["J. Leike", "M. Hutter"], "venue": "In: UAI-15", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "P.M.B.: An Introduction to Kolmogorov Complexity and its Applications", "author": ["M. Li", "Vit\u00e1nyi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Death and suicide in universal artificial intelligence", "author": ["J. Martin", "T. Everitt", "M. Hutter"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The basic AI drives. In: AGI-08", "author": ["S.M. Omohundro"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["R.J. Solomonoff"], "venue": "IEEE Transactions on Information Theory IT-24,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1978}, {"title": "A monte carlo AIXI approximation", "author": ["J. Veness", "K.S. Ng", "M. Hutter", "W. Uther", "D. Silver"], "venue": "Journal of Artificial Intelligence Research 40(1),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}], "referenceMentions": [{"referenceID": 5, "context": "6 Conclusion 12 A shorter version of this paper will be presented at AGI-16 [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "1 Introduction Reinforcement Learning (RL) has proven to be a fruitful theoretical framework for reasoning about the properties of generally intelligent agents [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 8, "context": "Firstly, it can guide principled attempts to construct such agents [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "The latter challenge has recently received considerable attention in the context of the potential risks posed by these agents to human safety [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "The universally intelligent agent AIXI constitutes a formal mathematical theory of artificial general intelligence [3].", "startOffset": 115, "endOffset": 118}, {"referenceID": 4, "context": "Originally devised for Solomonoff induction, they are necessary for universal artificial intelligence because the halting problem prevents the existence of a (lower semi-)computable universal measure for the class of (computable) measures [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 3, "context": "Using this formalism we proceed to reason about the behaviour of several generally intelligent agents in relation to death: AI\u03bc, which knows the true environment distribution; AI\u03be, which models the For example, Leike and Hutter [4] proved that since \u03be is a mixture over semimeasures, the iterative and recursive formulations of the value function are non-equivalent.", "startOffset": 228, "endOffset": 231}, {"referenceID": 2, "context": "environment using a universal mixture; and AIXI, a special case of AI\u03be that uses the Solomonoff prior [3].", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "In Algorithmic Information Theory, a semimeasure over an alphabet X is a function \u03bd : X \u2217 \u2192 [0, 1] such that (1) \u03bd(\u01eb) \u2264 1, and (2) \u03bd(x) \u2265", "startOffset": 92, "endOffset": 98}, {"referenceID": 7, "context": "Any semimeasure \u03bd can be turned into a measure \u03bdnorm using Solomonoff normalisation [9].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "We define the value (expected total future reward) of a policy \u03c0 in an environment \u03bd given a history \u00e6<t [4]: V \u03c0 \u03bd (\u00e6<tat) = 1 \u0393t \u2211", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "This is a surprising result, because in a standard RL setup in which the environment is modelled as a proper probability measure (not a semimeasure), the relative value of two policies is invariant under positive linear transformations of the reward [3, 4].", "startOffset": 250, "endOffset": 256}, {"referenceID": 3, "context": "This is a surprising result, because in a standard RL setup in which the environment is modelled as a proper probability measure (not a semimeasure), the relative value of two policies is invariant under positive linear transformations of the reward [3, 4].", "startOffset": 250, "endOffset": 256}, {"referenceID": 0, "context": "For the standard choice of reward range, rt \u2208 [0, 1], death is the worst possible outcome for the agent, whereas if rt \u2208 [\u22121, 0], it is the best.", "startOffset": 46, "endOffset": 52}, {"referenceID": 6, "context": "In a certain sense, therefore, the reward range parameterises a universal agent\u2019s selfpreservation drive [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "We argue that it could form the basis of a \u201ctripwire mechanism\u201d[2] that would lead an agent to terminate itself upon reaching a level of intelligence that would constitute a threat to human safety.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "There is an \u201cobservation selection effect\u201d[1] at work: AIXI only experiences history sequences on which it remains alive, and infers that a safe environment is more likely.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "We prove convergence of \u03be to \u03bcnorm by showing that (with respect to the true environment \u03bc), the total expected squared distance between \u03be and \u03bcnorm is finite [3]:", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "The shutdown problem arises if an intelligent agent\u2019s self-preservation drive incentivises it to resist termination [2, 7, 8].", "startOffset": 116, "endOffset": 125}, {"referenceID": 6, "context": "The shutdown problem arises if an intelligent agent\u2019s self-preservation drive incentivises it to resist termination [2, 7, 8].", "startOffset": 116, "endOffset": 125}, {"referenceID": 1, "context": "This suggests a potentially robust \u201ctripwire mechanism\u201d [2] that could decrease the risk of intelligence explosion.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "It seems doubtful that such a condition could ever be made robust against subversion by a sufficiently intelligent agent [2].", "startOffset": 121, "endOffset": 124}], "year": 2016, "abstractText": "Reinforcement learning (RL) is a general paradigm for studying intelligent behaviour, with applications ranging from artificial intelligence to psychology and economics. AIXI is a universal solution to the RL problem; it can learn any computable environment. A technical subtlety of AIXI is that it is defined using a mixture over semimeasures that need not sum to 1, rather than over proper probability measures. In this work we argue that the shortfall of a semimeasure can naturally be interpreted as the agent\u2019s estimate of the probability of its death. We formally define death for generally intelligent agents like AIXI, and prove a number of related theorems about their behaviour. Notable discoveries include that agent behaviour can change radically under positive linear transformations of the reward signal (from suicidal to dogmatically self-preserving), and that the agent\u2019s posterior belief that it will survive increases over time.", "creator": "LaTeX with hyperref package"}}}