{"id": "1606.01292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "An Attentional Neural Conversation Model with Improved Specificity", "abstract": "in this paper we propose towards a neural conversation model for conducting dialogues. we demonstrate the use of this model instead to generate help desk responses, where users are asking questions about different pc applications. our model is distinguished by two characteristics. physical first, it models intention across individual turns with a recurrent network, and incorporates an attention model that automatically is conditioned on the representation of intention. secondly, it avoids generating non - specific responses by incorporating an idf term suffix in the objective function. the model is constantly evaluated both as a target pure generation model in order which a help - desk response term is generated from scratch, acquired and as a retrieval model with performance measured using recall rates of the previous correct response. experimental results indicate implying that the model outperforms previously proposed neural conversation architectures, and demonstrating that using specificity in the objective interaction function significantly improves performances for both generation and retrieval.", "histories": [["v1", "Fri, 3 Jun 2016 22:26:01 GMT  (66kb,D)", "http://arxiv.org/abs/1606.01292v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.HC", "authors": ["kaisheng yao", "baolin peng", "geoffrey zweig", "kam-fai wong"], "accepted": false, "id": "1606.01292"}, "pdf": {"name": "1606.01292.pdf", "metadata": {"source": "CRF", "title": "An Attentional Neural Conversation Model with Improved Specificity", "authors": ["Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Kam-Fai Wong"], "emails": ["kaisheny@microsoft.com", "blpeng@se.cuhk.edu.hk", "gzweig@microsoft.com", "kfwong@se.cuhk.edu.hk"], "sections": [{"heading": "1 Introduction", "text": "In recent years, neural network based conversation models (Serban et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015) have emerged as a promising complement to traditional partially observable Markov decision process (POMDP) models (Young et al., 2013). The neural net based techniques require little in the way of explicit linguistic knowledge, e.g., creating a semantic parser, and therefore have promise of scalability, flexibility, and language independence. Broadly\nspeaking, there are two approaches to building a neural conversation model. The first is to train what is essentially a conversation-conditioned language model, which is used in generative mode to produce a likely response to a given conversation context. The second approach is retrieval-based, and aims at selecting a good response from a list of candidates taken from the training corpus.\nNeural conversation models are usually trained similarly to neural machine translation models (Sutskever et al., 2014; Cho et al., 2014), which treat response generation as a surface-to-surface transformation. While simple, it is possible that these models would benefit from explicit modeling of conversational dynamics, specifically the attention and intention processes hypothesized in discourse theory (Grosz and Sidner, 1986). A recent improvement along these lines is the hierarchical recurrent encoder-decoder in (Serban et al., 2015b; Sordoni et al., 2015b) that incorporates two levels of recurrent networks, one for generating words and one for modeling dependence between conversation turns. In this paper, we extend this approach further, and propose an explicit intention/attention model.\nWe further tackle a second key problem of neural conversation models, their tendency to generate generic, non-specific responses (Vinyals and Le, 2015; Li et al., 2016). To address the problem of specificity, a maximum mutual information (MMI) method for generation was proposed in (Li et al., 2016), which models both sides of the conversation, each conditioned on the other. While we find this method effective, training the additional model doubles the computational cost.\nar X\niv :1\n60 6.\n01 29\n2v 1\n[ cs\n.C L\n] 3\nJ un\n2 01\nThe contributions of this paper are as follows. First, we introduce a novel attention with intention neural conversation model that integrates an attention mechanism into a hierarchical model. Without the attention mechanism, the model resembles the models in (Serban et al., 2015b) but performs better. Visualization of the intention layer in the model shows that it indeed is relevant to intent. Second, we address the specificity problem by incorporating inverse document frequency (IDF) (Salton and Buckley, 1988; Ramos, 2003) into the training process. The proposed training algorithm uses reinforcement learning with the IDF value of the generated sentences as a reward signal. To the best of our knowledge, this is the first method incorporating specificity into the training objective function. Empirically, we find that it performs better than the dual-model method of (Li et al., 2016). Lastly, we demonstrate that the proposed model also performs well for retrieval-based conversation modeling. Using a recently proposed evaluation metric (Lowe et al., 2015; Lowe et al., 2016), we observed that this model was able to incorporate term-frequency inverse document frequency (TF-IDF) (Salton and Buckley, 1988) and significantly outperformed a TFIDF retrieval baseline and the model without using TF-IDF."}, {"heading": "2 The model", "text": "The proposed model is in the encoder-decoder framework (Sutskever et al., 2014) but incorporates a hierarchical structure to model dependence between turns of conversation process. The encoder network processes the user input and represents it as a vector. This vector is the input to a recurrent network that models context or intention to generate response in the decoder network. The decoder generates a response sequence word-by-word. For each word, the decoder uses an attention model on the words in the user input. Following (Grosz and Sidner, 1986), we refer the conversation context as intention. Because an attention model is used in the decoder, we denote this model as attention with intention (AWI) neural conversation model. A detailed illustration for a particular turn is in Figure 1. We elaborate each component of this model in the following sections."}, {"heading": "2.1 Encoder network", "text": "Given a user input sequence with length M at turn k, the encoder network converts it into a sequence of vectors x(k) = [x(k)m : m = 1, \u00b7 \u00b7 \u00b7 ,M ], with vector x (k) m \u2208 Rde denoting a word embedding representation of the word at position m. The model uses a feed-forward network to process this sequence. It has two inputs. The first is a simple word unigram feature, extracted as the average of the word embeddings in x(k). The second input is a representation of the previous response. This representation is also a word unigram feature, but is applied on the past response. The output, u(k) \u2208 Rdx , from the top layer of the feed-forward network is a vector representation of the user input. In addition to this vector representation, the encoder network outputs the vector sequence x(k)."}, {"heading": "2.2 Intention network", "text": "The middle layer represents a conversation context, which we denote it as the intention hypothesized in (Grosz and Sidner, 1986). At turn k, it is a vector z(k) \u2208 Rdz . To model turn-level dynamics, the activity in z(k) is dependent on the activity in the previous turn z(k-1) and the user input at the current turn. Therefore, it is natural to represent z(k) as follows\nz(k) = tanh ( Wiz (k-1) + Uiu (k) ) , (1)\nwhere tanh() is a tanh operation. Wi \u2208 Rdz\u00d7dz and Ui \u2208 Rdz\u00d7dx are matrices to transform inputs to a space with dimension dz . Usually, we apply multiple layers of the above non-linear processing, and the higher layer only applies Wi to the output from the layer below. Notice that Wis are untied across layers. The output from the top one is the representation of the intention network."}, {"heading": "2.3 Decoder network", "text": "Decoder network has output y(k)n \u2208 RV , a vector with dimension of vocabulary size V . Each element of the vector represents a probability of generating a particular word at position n. This probability is conditioned on y(k)n-1, the word generated before, the above described intention vector z(k) and the encoder output x(k). To compute this probability, it uses softmax on a response vector r(k)n \u2208 RV . The probability is defined as follows\np(y (k) n |y(k)n-1, z (k),x(k)) = softmax(r (k) n ). (2)\nThe decoder uses the following modules to generate response vector r(k)n .\nRNN For position n, the hidden state v(k)n of the RNN is recursively computed as\nv (k) n = tanh ( Vrv (k) n-1 + Wry (k) n-1 + Urr (k) n-1 ) , (3)\nwhere v(k)n-1, y (k) n-1 and r (k) n-1 each represent the previous hidden state, the word generated and the response vector for word position n \u2212 1. Vr \u2208 Rdr\u00d7dr , Wr \u2208 Rdr\u00d7dV and Ur \u2208 Rdr\u00d7dV are matrices to transform their right side inputs to a space with dimension dr. During training, y (k) n-1 is a one-hot vector with its non-zero element corresponding to the index of the word at n\u2212 1. During test, it is still a one-hot vector but the index of the non-zero element is from beam search or greedy search at position n\u2212 1. We apply multiple layers of the above process on v(k)n , with the higher layer uses only the lower layer output in the left side of (3). Parameters such as Vr are untied across layers. The top level response in (3) is the RNN output. To incorporate conversation context, the initial state v(k)n at n = 0 is set to z(k) from the intention network.\nAttention layer We use the content-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). It is a single layer neural network that aligns the target side hidden state v(k)n-1 at the previous position n\u2212 1 with the source side representation u(k)m at word position m. The alignment weight wynm is computed as follows\neynm = b T tanh(Vav (k) n-1 + Uax (k) m ), (4)\nwynm = exp eynm\u2211M\nj=1 exp(eynj) , (5)\na (k) n = \u2211 m wynmx (k) m , (6)\nwhere Va \u2208 Rda\u00d7dV and Ua \u2208 Rda\u00d7dx are matrices to transform their inputs to a space with dimension da. b \u2208 Rda is a vector. The softmax operation in (5) is normalized on the user input sequence. a(k)n in (6) is the output of the attention layer.\nResponse generation We use the following feedforward network to generate a response vector r(k)n \u2208 RdV , using the decoder RNN output v(k)n and the attention layer output a(k)n ; i.e.,\nr (k) n = tanh ( Vgv (k) n + Uga (k) n ) , (7)\nwhere Vg \u2208 RdV \u00d7dr and Ug \u2208 RdV \u00d7da are matrices to transform their right side inputs to a space with dimension dV . Similarly as those networks described above, we use untied multiple layers to generate r(k)n , and the input to the higher layer only use the output from the layer below. The top layer output is fed into softmax operation in Eq. (2) to compute the probability of generating words.\nInput similarity feature We construct a linear direct connection between user inputs and the output layer. To achieve this, a large matrix P with dimension V \u00d7 V is used to project each input word to a high dimension vector with dimension size V . The projected words are then averaged to have a vector m(k) with dimension V . This vector is added to the response vector r(k)n , so that Eq. (2) is computed as softmax(r (k) n + m\n(k)). Since m(k) is applied to all word positions at turn k, it provides a global bias to the output distribution."}, {"heading": "3 Training and decoding algorithms", "text": "This section presents training and decoding algorithms for response generation and retrieval. Section 3.1 is the standard cross-entropy training. It is used for training both generation and retrieval models. Section 3.2 introduces training and decoding algorithms to enhance specificity for generation. Algorithms for training and decoding for retrieval are described in Sec. 3.3."}, {"heading": "3.1 Maximum-likelihood training", "text": "The standard training method maximizes the probability of predicting the correct word y(k)n given user input x(k), context z(k), and the past prediction y(k)n-1; i.e., the objective is maximizing the following loglikelihood w.r.t. the model parameter \u03b8,\nL(y(k)) = log N\u220f\nn=1\np(y (k) n |y(k)n-1, z (k),x(k); \u03b8). (8)\nA problem with this training is that the learned models are not optimized for the final metric (Och, 2003; Ranzato et al., 2016). Another problem is that decoding to maximize sentence likelihood typically results in non-specific high-frequency words (Li et al., 2016)."}, {"heading": "3.2 Improving specificity for generation", "text": "We propose using inverse document frequency (IDF) (Salton and Buckley, 1988) to measure specificity of a response. IDF is used in decoding in Sec. 3.2.2. We describe a novel algorithm in Sec. 3.2.3 that incorporates IDF in training objective."}, {"heading": "3.2.1 Specificity", "text": "IDF for a word w is defined as\nidf(w) = log N\n|c \u2208 C : w \u2208 c| , (9)\nwhere N is the number of sentences in a corpus C and c denotes a sentence in that corpus. The denominator represents the number of sentences in which the word w appears. A property of IDF is that words occur very frequently have small IDF values.\nWe further define a sentence-level IDF as the average of IDF values of words in a sentence; i.e.,\nidf(c) = 1 |c| \u2211 w\u2208c idf(w), (10)\nwhere the denominator |c| is the number of occurrence of words in sentence c. A corpus-level IDF value is similarly computed on a corpus with an average operation as idf(C) = 1|C| \u2211 c\u2208C,w\u2208c idf(w) and the denominator in the equation is the number of occurrence of words in the corpus."}, {"heading": "3.2.2 Reranking with IDF", "text": "One way to improve specificity is using IDF to rerank hypotheses from beam search decoding. The length-normalized log-likelihood scores of these hypotheses are interpolated with sentence-level IDF scores. Tuning the interpolation weight is on a development set using minimum error rate training (MERT) (Och, 2003). The interpolation weight that achieves the highest BLEU (Papineni et al., 2002) score on the development set is used for testing."}, {"heading": "3.2.3 Incorporating IDF in training objective", "text": "Alternatively, we cast our problem of optimizing a model directly for specificity in the reinforcement learning framework (Sutton and Barto, 1988). The decoder is an agent with its policy from Eq. (2). Its action is to generate a word using the policy, and therefore it has V actions to take at each time. At the end of generating a whole sequence of words for response, the agent receives a reward, calculated as the sentence level IDF score of the generated response. Training therefore should find a policy that maximizes the expected reward.\nThis problem can be solved using REINFORCE (Williams, 1992), in which the gradient to update model parameter is calculated as follows\n\u2206\u03b8 \u221d ( r(w(k))\u2212 b(x(k)) ) (11)\n\u2202 log \u220f\nn p(y (k) n |y(k)n-1, z(k),x(k); \u03b8)) \u2202\u03b8 ,\nwhere r(w(k)) = idf(w(k)) is the IDF score of the generated response w(k) at turn k. b(x(k)) is called reinforcement baseline. It in practice can be set empirically as an arbitrary number to improve convergence (Zaremba and Sutskever, 2015). One convenient way of estimating the baseline is the mean of the IDF values on the training set, which is used in this paper.\nNotice that the IDF score is computed on the decoded responses w(k), but the log-likelihood is com-\nputed on the correct response. Therefore, the algorithm improves likelihood of the correct response and also encourages generating responses with high IDF scores."}, {"heading": "3.3 Training and decoding for retrieval", "text": "The conversation model can be used for retrieval of the correct responses from candidates. We briefly describe TF-IDF in Sec. 3.3.1. Section 3.3.2 presents the algorithm to train AWI model for retrieval. Section 3.3.3 combines the model with TFIDF. Notice that TF-IDF uses IDF to penalize nonspecific words, combining the AWI model with TFIDF should have improved specificity, which could lead to improved performances for retrieval."}, {"heading": "3.3.1 TF-IDF", "text": "Term-frequency inverse document frequency (TFIDF) (Salton and Buckley, 1988) is an established baseline for conversation model used for retrieval (Lowe et al., 2015). The term-frequency (TF) is a count of the number of times a word appears in a given context, and IDF puts penalty on how often the word appears in the corpus. The TF-IDF is a vector for a context computed as follows for a word w in a context c,\ntfidf(w, c,C) = o(w, c)\u00d7 log N |c \u2208 C : w \u2208 c| ,\nwhere o(w, c) is the number of times the word w occurs in the context c. For retrieval, a vector for a conversation history is firstly created, with element computed as above for each word in the conversation history. Then, a TF-IDF vector is computed for each response. Similarity of these vectors are measured using Cosine similarity. The responses with the top k similarities are selected as the top k outputs."}, {"heading": "3.3.2 Training models with ranking criterion", "text": "In order to train AWI model for retrieval, the model needs two types of responses. The positive response is the correct one, and the negative responses are those randomly sampled from the training set. For a response w(k), its length-normalized log-likelihood is computed as follows,\nllk(w(k)) = L(w(k))\n|w(k)| , (12)\nwhere L(w(k)) is computed using Eq. (8) with y(k) substituted by w(k). |w(k)| is the number of words in w(k).\nThe objective is to have high recall rates such that the correct responses are ranked higher than negative responses. To this end, the algorithm maximizes the difference between the length-normalized log-likelihood of the correct responses to the lengthnormalized log-likelihood of negative responses; i.e.,\nR = max{llk(y(k))\u2212 llk(w(k))}, (13)\nwhere y(k) is the correct response and w(k) is the negative response."}, {"heading": "3.3.3 Ranking with AWI together with TF-IDF", "text": "Naturally, the length-normalized log-likelihood score from the model trained in Sec. 3.3.2 can be interpolated with the similarity score from TF-IDF in Sec. 3.3.1. The optimal interpolation weight is selected on a development set if it achieves the best recall rates."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Data", "text": "We use a real-world commercial data set to evaluate the models. The data contains human-human dialogues from a helpdesk chat service for services of Office and Windows. In this service, costumers seek helps on computer and software related issues from human agents. Training set consists of 141,204 dialogues with 2 million turns. The average number of turns in a dialogue is 12, with the largest number of 140 turns and the minimum number of 1 turn. More than 90% of dialogues have 25 or fewer turns. The number of tokens is 25,410,683 in the source side and is 37,796,000 in the target side. The vocabulary size is 8098 including words from both source and target sides. Development and test sets each have 10,000 turns. The test set has 125,451 tokens in its source side and 187,118 in its target side."}, {"heading": "4.2 Training Details", "text": "Unless otherwise stated, all of the recurrent networks have two layers. The encoder network uses word embedding initialized from 300-dimension GLOVE vector (Pennington et al., 2014) trained\nfrom 840 billion words. Therefore, the embedding dimension de is 300. The hidden layer dimension for encoder, dx, is 1000. Decoder dimension, dr, is 1000. The intention network has a 300 dimension vector; i.e., dz = 300. The alignment dimension da is 100. All parameters, except biases, are initialized using uniform distribution in [\u22121.0, 1.0] but are scaled to be inversely proportional to the number of parameters. All bias vectors are initialized to zero.\nThe maximum number of training epochs is 10. We use RMSProp with Momentum (Tieleman and Hinton, 2012) to update models. We use perplexity to monitor the progress of training; learning rate is halved if the perplexity on the development set is increased. The gradient is re-scaled whenever its norm exceeds 5.0. To speed-up training, dialogues with the same number of turns are processed in one batch. The batch size is 20.\nHyper-parameters such as initial learning rate and dimension sizes are optimized on the development set. These parameters are then used on the test set. Decoding uses beam search with a beam width of 1. Decoding stops when an end of sentence is generated."}, {"heading": "4.3 Evaluation metrics", "text": "As our model is used both for generation and retrieval, we use some established measures in the literature for comparison. The first measure is BLEU (Papineni et al., 2002), which uses the Ngram (Dunning, 1994) to compute similarities between references and responses, together with a penalty for sentence brevity. We use BLEU with 4-gram. While BLEU may unfairly penalize paraphrases with different wording, it has found correlated well with human judgement on responses generation tasks (Galley et al., 2015). The second measure is perplexity (Brown et al., 1992), which measures the likelihood of generating a word given observations. We use it in section 4.4.1 to compare the proposed model with respect to other neural network models that also report perplexity. However, since our training algorithms in Sec. 3.2 is designed to improve specificity, which is not directly correlated with the standard likelihood, we only report perplexity in section 4.4.1. The third metric is corpus level IDF score for specificity, computed in (10).\nSince our model is also used for retrieval, we\nadopt a response selection measure proposed in (Lowe et al., 2015), in which the performance of a conversation model is measured by the recall rate of those correct responses in the top ranks. This metric is called Recall@k (R@k). The model is asked to select the k most likely responses, and it is correct if the true response is among these k responses. The number of candidates for retrieval is 10, following (Lowe et al., 2015). This measure is observed to correlate well with human judgment for retrieval based conversation model (Lowe et al., 2016; Liu et al., 2016)."}, {"heading": "4.4 Performance as a generation model", "text": ""}, {"heading": "4.4.1 Comparison with other methods", "text": "We compared the AWI model with the sequenceto-sequence (Seq2Seq) (Vinyals and Le, 2015) and the hierarchical recurrent encoder-decoder (HRED) (Serban et al., 2015a) models. All of the models had a two layers of encoder and decoder. The hidden dimensions for the encoder and decoder were set to 200 in all of the models. The hidden dimension for the intention network was set to 50. All of the models had their optimal setup on the development set. Both Seq2Seq and HRED used long shortterm memory networks (Hochreiter and Schmidhuber, 1997). The number of parameters was approximately 4.48 \u00d7 106 for Seq2Seq and 4.50 \u00d7 106 for HRED. AWI didn\u2019t have the input similarity feature and it had 5.71\u00d7106 parameters. Greedy search was used in this experiment.\nTable 1 shows that AWI model outperforms other neural network models both in BLEU and perplexity. For comparison, BLEU and perplexity scores from an unconditional N-gram model are also reported, which are much worse than those from neural network based models. The BLEU score for ngram was obtained by sampling from the n-gram\nand comparing the sampled response to a typical response. Its response has BLEU score of 0.08 if using BLEU with 1-gram. Because it cannot be matched in 4 gram with the typical response, its BLEU with 4 gram is 0. On one experiment, not shown in the table due to space limitation, with smaller training set, we observed these models performed worse yet similarly. This suggests that the benefit of incorporating hierarchical structure in both HRED and AWI is more apparent with larger training data."}, {"heading": "4.4.2 Results with specificity improved models", "text": "We report BLEU and IDF scores in table 2. The baseline is AWI trained with standard cross-entropy in Sec. 3.1. For comparison, we used a sampling method (Mikolov et al., 2011) to generate responses, denoted as \u201dAWI + sampling\u201d. Using sampling would lead to an appropriate number of infrequent words and therefore an IDF score that is similar to that of the reference responses. Indeed this is observed in \u201dAWI + sampling\u201d in the table. It has an IDF score of 2.76, close to the IDF score of 2.74 from the training set. However, sampling produces worse BLEU scores, though it has higher IDF score than AWI.\nWe also report result using MMI method for decoding (Li et al., 2016), denoted as \u201dAWI + MMI\u201d. This uses a backward directional model trained for generating source from target, and its decoding uses reranking algorithm in Sec. 3.2.2. The optimal interpolation weight for the backward directional model was 0.05. Both BLEU and IDF scores are improved. On the other hand, \u201dAWI + MMI\u201d requires an additional model as complicated as the baseline AWI model.\nAlternatively, AWI results are reranked with the sentence-level IDF scores using algorithm in Sec. 3.2.2. The optimal interpolation weight to IDF was 0.035. This result, denoted as \u201dAWI + IDF\u201d,\nhas improved BLEU and IDF scores, in comparison to the baseline AWI. Compared against \u201dAWI + MMI\u201d, it has similar BLEU but higher IDF scores. This suggests that IDF score is more directly related to specificity than using MMI.\nThe result from using specificity as reward to train a model in Sec. 3.2.3 is denoted as IDF-rewarded AWI model or \u201dIR-AWI\u201d. The reinforcement baseline b(x(k)) was empirically set to 1.0. We also experimented with a larger value to 1.5 for the baseline and didn\u2019t observe much performance differences. \u201dIR-AWI\u201d consistently outperforms \u201dAWI\u201d and \u201dAWI + MMI\u201d."}, {"heading": "4.4.3 Analysis", "text": "Figure 2 uses t-SNE (van der Maaten and Hinton, 2008) to visualize the intention vectors. It shows clear clusters even though training intention vectors doesn\u2019t use explicit labels. In order to relate these clusters with explicit meaning, we look at the responses generated from these intention vectors, and tag similar responses with the same color. Some responses types are clearly clustered such as \u201dGreeting\u201d and \u201dClose this chat\u201d. Others types are though more distributed and cannot find a clear tag for these responses. We therefore leave them untagged.\nWe also show two examples of responses in Tables 3 and 4 from AWI and IR-AWI. Responses from AWI+MMI and AWI+IDF are the same as from AWI, so we only list responses from AWI and IRAWI. These responses are reasonable. However, the IR-AWI responses in these tables are more specific than the generic responses from AWI."}, {"heading": "4.5 Performance for retrieval", "text": "We report recall rates, R@1 and R@5, in Table 5. Clearly, AWI model trained with ranking criterion in Sec. 3.3.2 outperforms TF-IDF. Importantly, AWI model was able to combine TF-IDF using method described in Sec. 3.3.3, obtaining significant performance improvement."}, {"heading": "5 Related work", "text": "Our work is related both to goal and non-goal oriented dialogue systems as the proposed model can be used as a language generation component in a goal-oriented dialogue (Young et al., 2013) or simply to produce chit-chat style dialogue without a\nspecific goal (Ritter et al., 2010; Banchs and Li, 2012; Ameixa et al., 2014). Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al., 2013), the proposed model may relax such requirement. However, grounding the generated conversation with actions and knowledge is not studied in this paper. It will be a future work.\nThe proposed model is related to the recent works in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015a), which use an encoder-decoder framework to model conversation. The closest work is in (Sordoni et al., 2015a). This model differs from that work in using an attention model, an additional input similarity feature, and its decoder architecture. Importantly, this model is used not only for generation as in those previous work, but also for retrieval.\nPrior work to potentially increase specificity or diversity aims at producing multiple outputs (Carbonell and Goldstein, 1998; Gimpel et al., 2013) and our work is the same as in (Li et al., 2016) to produce a single nontrivial output. Instead of using an objective function in (Li et al., 2016) that has an indirect relation to specificity, our model uses a specificity measure directly for training and decoding."}, {"heading": "6 Conclusions", "text": "We have presented a novel attentional neural conversation model with enhanced specificity using IDF. It has been evaluated for both response generation and retrieval. We have observed significant performance improvements in comparison to alternative methods."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "IRIS: a chat-oriented dialogue system based on the vector space model", "author": ["Banchs", "Li2012] Rafael E. Banchs", "Haizhou Li"], "venue": null, "citeRegEx": "Banchs et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banchs et al\\.", "year": 2012}, {"title": "An estimate of an upper bound for the entropy of English", "author": ["Brown et al.1992] Peter Brown", "Vincent Pietra", "Robert Mercer", "Stephen Pietra", "Jennifer Lai"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "The use of MMR, diversity-based reranking for reordering documents and producing summaries", "author": ["Carbonell", "Goldstein1998] Jaime Carbonell", "Jade Goldstein"], "venue": "Research and development in information retrieval,", "citeRegEx": "Carbonell et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Carbonell et al\\.", "year": 1998}, {"title": "Learning phrase representations using RNN encoderdecoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Statistical identification of language", "author": ["Ted Dunning"], "venue": "Technical Report Technical Report MCCS 94-273,", "citeRegEx": "Dunning.,? \\Q1994\\E", "shortCiteRegEx": "Dunning.", "year": 1994}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Galley et al.2015] Michel Galley", "Chris Quirk", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Online policy optimisation of Bayesian spoken dialogue systems via human interaction", "author": ["Gasic et al.2013] M. Gasic", "C. Breslin", "M. Henderson", "D. Kim", "M. Szummer", "B. Thomson", "P. Tsiakoulis", "S. Young"], "venue": "In ICASSP", "citeRegEx": "Gasic et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gasic et al\\.", "year": 2013}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Gimpel et al.2013] Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Attention, intentions, and the structure of discourse", "author": ["Grosz", "Sidner1986] Barbara J. Grosz", "Candace L. Sidner"], "venue": "Computational Linguistics,", "citeRegEx": "Grosz et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Grosz et al\\.", "year": 1986}, {"title": "Word-based dialog state tracking with recurrent neural networks. In SIGDIAL", "author": ["Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A diversitypromopting objective function for neural conversation model", "author": ["Li et al.2016] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: an empirical study of unsupervised evaluation metics for dialogue response generation", "author": ["Liu et al.2016] Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Lowe et al.2015] Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau"], "venue": "In SIGDIAL", "citeRegEx": "Lowe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "On the evluation of dialogue systems with next utterance classification", "author": ["Lowe et al.2016] Ryan Lowe", "Iulian V. Serban", "Mike Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Lowe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lowe et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["Stefan Kombrink", "Lukas Burget", "Jan Honza Cernock", "Sanjeev Khudanpur"], "venue": "In ICASSP,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Minimum error rate training for statistical machine translation", "author": ["Franz Josef Och"], "venue": null, "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": null, "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Using TF-IDF to determine word relevance in document queires", "author": ["Juan Ramos"], "venue": "In ICML", "citeRegEx": "Ramos.,? \\Q2003\\E", "shortCiteRegEx": "Ramos.", "year": 2003}, {"title": "Sequence level training with recurrent neural networks. In ICLR", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Unsupervised modeling of Twitter conversation", "author": ["Ritter et al.2010] Alan Ritter", "Colin Cherry", "Bill Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["Salton", "Buckley1988] Gerard Salton", "Christopher Buckley"], "venue": "Information Processing and Management,", "citeRegEx": "Salton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1988}, {"title": "Building end-to-end dialogue systems using generative hierachical neural network models", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for shorttext conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": null, "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "2015a. A neural network approach to context-sensitive generation of conversation responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "2015b. A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jacob G. Simonsen", "Jian-Yun Nie"], "venue": "In CIKM", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Barto1988] Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1988}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. In COURSERA: Neural Networks for Machine Learning", "author": ["Tieleman", "Hinton2012] T. Tieleman", "G. Hinton"], "venue": null, "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Visualizing data usin t-SNE", "author": ["van der Maaten", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "A nerual converstion model", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc V. Le"], "venue": "In ICML Deep Learning Workshop", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Simple statistical gradient-flow algorithms for connectionist reinforcement learning", "author": ["Ronald Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Spoken dialogue systems: Challenges, and opportunities for research", "author": ["Jason D. Williams"], "venue": "In ASRU", "citeRegEx": "Williams.,? \\Q2009\\E", "shortCiteRegEx": "Williams.", "year": 2009}, {"title": "POMDPbased statistical spoken dialog systems: A review", "author": ["Young et al.2013] Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}, {"title": "Reinforcement learning neural turing machines", "author": ["Zaremba", "Sutskever2015] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "In recent years, neural network based conversation models (Serban et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015; Shang et al., 2015) have emerged as a promising complement to traditional partially observable Markov decision process (POMDP) models (Young et al.", "startOffset": 58, "endOffset": 145}, {"referenceID": 38, "context": ", 2015) have emerged as a promising complement to traditional partially observable Markov decision process (POMDP) models (Young et al., 2013).", "startOffset": 122, "endOffset": 142}, {"referenceID": 30, "context": "Neural conversation models are usually trained similarly to neural machine translation models (Sutskever et al., 2014; Cho et al., 2014), which treat response generation as a surface-to-surface transformation.", "startOffset": 94, "endOffset": 136}, {"referenceID": 4, "context": "Neural conversation models are usually trained similarly to neural machine translation models (Sutskever et al., 2014; Cho et al., 2014), which treat response generation as a surface-to-surface transformation.", "startOffset": 94, "endOffset": 136}, {"referenceID": 12, "context": "We further tackle a second key problem of neural conversation models, their tendency to generate generic, non-specific responses (Vinyals and Le, 2015; Li et al., 2016).", "startOffset": 129, "endOffset": 168}, {"referenceID": 12, "context": "To address the problem of specificity, a maximum mutual information (MMI) method for generation was proposed in (Li et al., 2016), which models both sides of the conversation, each conditioned on the other.", "startOffset": 112, "endOffset": 129}, {"referenceID": 21, "context": "Second, we address the specificity problem by incorporating inverse document frequency (IDF) (Salton and Buckley, 1988; Ramos, 2003) into the training process.", "startOffset": 93, "endOffset": 132}, {"referenceID": 12, "context": "Empirically, we find that it performs better than the dual-model method of (Li et al., 2016).", "startOffset": 75, "endOffset": 92}, {"referenceID": 14, "context": "Using a recently proposed evaluation metric (Lowe et al., 2015; Lowe et al., 2016), we observed that this model was able to incorporate term-frequency inverse document frequency (TF-IDF) (Salton and Buckley, 1988) and significantly outperformed a TFIDF retrieval baseline and the model without using TF-IDF.", "startOffset": 44, "endOffset": 82}, {"referenceID": 15, "context": "Using a recently proposed evaluation metric (Lowe et al., 2015; Lowe et al., 2016), we observed that this model was able to incorporate term-frequency inverse document frequency (TF-IDF) (Salton and Buckley, 1988) and significantly outperformed a TFIDF retrieval baseline and the model without using TF-IDF.", "startOffset": 44, "endOffset": 82}, {"referenceID": 30, "context": "The proposed model is in the encoder-decoder framework (Sutskever et al., 2014) but incorporates a hierarchical structure to model dependence between turns of conversation process.", "startOffset": 55, "endOffset": 79}, {"referenceID": 0, "context": "Attention layer We use the content-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 61, "endOffset": 104}, {"referenceID": 16, "context": "Attention layer We use the content-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 61, "endOffset": 104}, {"referenceID": 18, "context": "A problem with this training is that the learned models are not optimized for the final metric (Och, 2003; Ranzato et al., 2016).", "startOffset": 95, "endOffset": 128}, {"referenceID": 22, "context": "A problem with this training is that the learned models are not optimized for the final metric (Och, 2003; Ranzato et al., 2016).", "startOffset": 95, "endOffset": 128}, {"referenceID": 12, "context": "Another problem is that decoding to maximize sentence likelihood typically results in non-specific high-frequency words (Li et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 18, "context": "Tuning the interpolation weight is on a development set using minimum error rate training (MERT) (Och, 2003).", "startOffset": 97, "endOffset": 108}, {"referenceID": 19, "context": "The interpolation weight that achieves the highest BLEU (Papineni et al., 2002) score on the development set is used for testing.", "startOffset": 56, "endOffset": 79}, {"referenceID": 36, "context": "This problem can be solved using REINFORCE (Williams, 1992), in which the gradient to update model parameter is calculated as follows", "startOffset": 43, "endOffset": 59}, {"referenceID": 14, "context": "Term-frequency inverse document frequency (TFIDF) (Salton and Buckley, 1988) is an established baseline for conversation model used for retrieval (Lowe et al., 2015).", "startOffset": 146, "endOffset": 165}, {"referenceID": 20, "context": "The encoder network uses word embedding initialized from 300-dimension GLOVE vector (Pennington et al., 2014) trained", "startOffset": 84, "endOffset": 109}, {"referenceID": 19, "context": "The first measure is BLEU (Papineni et al., 2002), which uses the Ngram (Dunning, 1994) to compute similarities between references and responses, together with a penalty for sentence brevity.", "startOffset": 26, "endOffset": 49}, {"referenceID": 5, "context": ", 2002), which uses the Ngram (Dunning, 1994) to compute similarities between references and responses, together with a penalty for sentence brevity.", "startOffset": 30, "endOffset": 45}, {"referenceID": 6, "context": "While BLEU may unfairly penalize paraphrases with different wording, it has found correlated well with human judgement on responses generation tasks (Galley et al., 2015).", "startOffset": 149, "endOffset": 170}, {"referenceID": 2, "context": "The second measure is perplexity (Brown et al., 1992), which measures the likelihood of generating a word given observations.", "startOffset": 33, "endOffset": 53}, {"referenceID": 14, "context": "adopt a response selection measure proposed in (Lowe et al., 2015), in which the performance of a conversation model is measured by the recall rate of those correct responses in the top ranks.", "startOffset": 47, "endOffset": 66}, {"referenceID": 14, "context": "The number of candidates for retrieval is 10, following (Lowe et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 15, "context": "This measure is observed to correlate well with human judgment for retrieval based conversation model (Lowe et al., 2016; Liu et al., 2016).", "startOffset": 102, "endOffset": 139}, {"referenceID": 13, "context": "This measure is observed to correlate well with human judgment for retrieval based conversation model (Lowe et al., 2016; Liu et al., 2016).", "startOffset": 102, "endOffset": 139}, {"referenceID": 12, "context": "AWI + MMI (Li et al., 2016) 11.", "startOffset": 10, "endOffset": 27}, {"referenceID": 17, "context": "For comparison, we used a sampling method (Mikolov et al., 2011) to generate responses, denoted as \u201dAWI + sampling\u201d.", "startOffset": 42, "endOffset": 64}, {"referenceID": 12, "context": "We also report result using MMI method for decoding (Li et al., 2016), denoted as \u201dAWI + MMI\u201d.", "startOffset": 52, "endOffset": 69}, {"referenceID": 38, "context": "Our work is related both to goal and non-goal oriented dialogue systems as the proposed model can be used as a language generation component in a goal-oriented dialogue (Young et al., 2013) or simply to produce chit-chat style dialogue without a Models R@1 R@5", "startOffset": 169, "endOffset": 189}, {"referenceID": 23, "context": "specific goal (Ritter et al., 2010; Banchs and Li, 2012; Ameixa et al., 2014).", "startOffset": 14, "endOffset": 77}, {"referenceID": 10, "context": "Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 54, "endOffset": 116}, {"referenceID": 7, "context": "Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 54, "endOffset": 116}, {"referenceID": 35, "context": "Whereas traditionally a language generation component (Henderson et al., 2014; Gasic et al., 2013; Wen et al., 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 54, "endOffset": 116}, {"referenceID": 37, "context": ", 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al.", "startOffset": 31, "endOffset": 47}, {"referenceID": 38, "context": ", 2015) rely on explicit state (Williams, 2009) in POMDP framework for goal-oriented dialogue system (Young et al., 2013), the proposed model may relax such requirement.", "startOffset": 101, "endOffset": 121}, {"referenceID": 27, "context": "The proposed model is related to the recent works in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015a), which use an encoder-decoder framework to model conversation.", "startOffset": 53, "endOffset": 118}, {"referenceID": 8, "context": "Prior work to potentially increase specificity or diversity aims at producing multiple outputs (Carbonell and Goldstein, 1998; Gimpel et al., 2013) and our work is the same as in (Li et al.", "startOffset": 95, "endOffset": 147}, {"referenceID": 12, "context": ", 2013) and our work is the same as in (Li et al., 2016) to produce a single nontrivial output.", "startOffset": 39, "endOffset": 56}, {"referenceID": 12, "context": "Instead of using an objective function in (Li et al., 2016) that has an indirect relation to specificity, our model uses a specificity measure directly for training and decoding.", "startOffset": 42, "endOffset": 59}], "year": 2016, "abstractText": "In this paper we propose a neural conversation model for conducting dialogues. We demonstrate the use of this model to generate help desk responses, where users are asking questions about PC applications. Our model is distinguished by two characteristics. First, it models intention across turns with a recurrent network, and incorporates an attention model that is conditioned on the representation of intention. Secondly, it avoids generating nonspecific responses by incorporating an IDF term in the objective function. The model is evaluated both as a pure generation model in which a help-desk response is generated from scratch, and as a retrieval model with performance measured using recall rates of the correct response. Experimental results indicate that the model outperforms previously proposed neural conversation architectures, and that using specificity in the objective function significantly improves performances for both generation and retrieval.", "creator": "LaTeX with hyperref package"}}}