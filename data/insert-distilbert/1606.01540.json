{"id": "1606.01540", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2016", "title": "OpenAI Gym", "abstract": "openai gym is a toolkit for reinforcement learning research. org it includes having a growing collection of benchmark problems that expose having a common interface, and a website where people can share their results and compare using the performance metric of algorithms. this whitepaper discusses the components of openai classroom gym and the design decisions that went into the software.", "histories": [["v1", "Sun, 5 Jun 2016 17:54:48 GMT  (546kb,D)", "http://arxiv.org/abs/1606.01540v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["greg brockman", "vicki cheung", "ludwig pettersson", "jonas schneider", "john schulman", "jie tang", "wojciech zaremba"], "accepted": false, "id": "1606.01540"}, "pdf": {"name": "1606.01540.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["OpenAI Gym", "Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) is the branch of machine learning that is concerned with making sequences of decisions. RL has a rich mathematical theory and has found a variety of practical applications [1]. Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].\nTo build on recent progress in reinforcement learning, the research community needs good benchmarks on which to compare algorithms. A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11]. OpenAI Gym aims to combine the best elements of these previous benchmark collections, in a software package that is maximally convenient and accessible. It includes a diverse collection of tasks (called environments) with a common interface, and this collection will grow over time. The environments are versioned in a way that will ensure that results remain meaningful and reproducible as the software is updated.\nAlongside the software library, OpenAI Gym has a website (gym.openai.com) where one can find scoreboards for all of the environments, showcasing results submitted by users. Users are encouraged to provide links to source code and detailed instructions on how to reproduce their results."}, {"heading": "2 Background", "text": "Reinforcement learning assumes that there is an agent that is situated in an environment. Each step, the agent takes an action, and it receives an observation and reward from the environment. An RL algorithm seeks to maximize some measure of the agent\u2019s total reward, as the agent interacts with the environment. In the RL literature, the environment is formalized as a partially observable Markov decision process (POMDP) [12].\nOpenAI Gym focuses on the episodic setting of reinforcement learning, where the agent\u2019s experience is broken down into a series of episodes. In each episode, the agent\u2019s initial state is randomly sampled from a distribution, and the interaction proceeds until the environment reaches a terminal state. The goal in episodic reinforcement learning is to maximize the expectation of total reward per episode, and to achieve a high level of performance in as few episodes as possible.\nThe following code snippet shows a single episode with 100 timesteps. It assumes that there is an object called agent, which takes in the observation at each timestep, and an object called env, which is the\n1gym.openai.com\nar X\niv :1\n60 6.\n01 54\n0v 1\n[ cs\n.L G\n] 5\nJ un\n2 01\n6\nenvironment. OpenAI Gym does not include an agent class or specify what interface the agent should use; we just include an agent here for demonstration purposes.\nob0 = env.reset() # sample environment state, return first observation a0 = agent.act(ob0) # agent chooses first action ob1, rew0, done0, info0 = env.step(a0) # environment returns observation, # reward, and boolean flag indicating if the episode is complete. a1 = agent.act(ob1) ob2, rew1, done1, info1 = env.step(a1) ... a99 = agent.act(o99) ob100, rew99, done99, info2 = env.step(a99) # done99 == True => terminal"}, {"heading": "3 Design Decisions", "text": "The design of OpenAI Gym is based on the authors\u2019 experience developing and comparing reinforcement learning algorithms, and our experience using previous benchmark collections. Below, we will summarize some of our design decisions.\nEnvironments, not agents. Two core concepts are the agent and the environment. We have chosen to only provide an abstraction for the environment, not for the agent. This choice was to maximize convenience for users and allow them to implement different styles of agent interface. First, one could imagine an \u201conline learning\u201d style, where the agent takes (observation, reward, done) as an input at each timestep and performs learning updates incrementally. In an alternative \u201cbatch update\u201d style, a agent is called with observation as input, and the reward information is collected separately by the RL algorithm, and later it is used to compute an update. By only specifying the agent interface, we allow users to write their agents with either of these styles.\nEmphasize sample complexity, not just final performance. The performance of an RL algorithm on an environment can be measured along two axes: first, the final performance; second, the amount of time it takes to learn\u2014the sample complexity. To be more specific, final performance refers to the average reward per episode, after learning is complete. Learning time can be measured in multiple ways, one simple scheme is to count the number of episodes before a threshold level of average performance is exceeded. This threshold is chosen per-environment in an ad-hoc way, for example, as 90% of the maximum performance achievable by a very heavily trained agent. Both final performance and sample complexity are very interesting, however, arbitrary amounts of computation can be used to boost final performance, making it a comparison of computational resources rather than algorithm quality.\nEncourage peer review, not competition. The OpenAI Gym website allows users to compare the performance of their algorithms. One of its inspiration is Kaggle, which hosts a set of machine learning contests with leaderboards. However, the aim of the OpenAI Gym scoreboards is not to create a competition, but rather to stimulate the sharing of code and ideas, and to be a meaningful benchmark for assessing different methods. RL presents new challenges for benchmarking. In the supervised learning setting, performance is measured by prediction accuracy on a test set, where the correct outputs are hidden from contestants. In RL, it\u2019s less straightforward to measure generalization performance, except by running the users\u2019 code on a collection of unseen environments, which would be computationally expensive. Without a hidden test set, one must check that an algorithm did not \u201coverfit\u201d on the problems it was tested on (for example, through parameter tuning). We would like to encourage a peer review process for interpreting results submitted by users. Thus, OpenAI Gym asks users to create a Writeup describing their algorithm, parameters used, and linking to code. Writeups should allow other users to reproduce the results. With the source code available, it is possible to make a nuanced judgement about whether the algorithm \u201coverfit\u201d to the task at hand.\nStrict versioning for environments. If an environment changes, results before and after the change would be incomparable. To avoid this problem, we guarantee than any changes to an environment will be accompanied by an increase in version number. For example, the initial version of the CartPole task is named Cartpole-v0, and if its functionality changes, the name will be updated to Cartpole-v1.\nMonitoring by default. By default, environments are instrumented with a Monitor, which keeps track of every time step (one step of simulation) and reset (sampling a new initial state) are called. The Monitor\u2019s behavior is configurable, and it can record a video periodically. It also is sufficient to produce learning curves. The videos and learning curve data can be easily posted to the OpenAI Gym website."}, {"heading": "4 Environments", "text": "OpenAI Gym contains a collection of Environments (POMDPs), which will grow over time. See Figure 1 for examples. At the time of Gym\u2019s initial beta release, the following environments were included:\n\u2022 Classic control and toy text: small-scale tasks from the RL literature.\n\u2022 Algorithmic: perform computations such as adding multi-digit numbers and reversing sequences. Most of these tasks require memory, and their difficulty can be chosen by varying the sequence length.\n\u2022 Atari: classic Atari games, with screen images or RAM as input, using the Arcade Learning Environment [5].\n\u2022 Board games: currently, we have included the game of Go on 9x9 and 19x19 boards, where the Pachi engine [13] serves as an opponent.\n\u2022 2D and 3D robots: control a robot in simulation. These tasks use the MuJoCo physics engine, which was designed for fast and accurate robot simulation [14]. A few of the tasks are adapted from RLLab [6].\nSince the initial release, more environments have been created, including ones based on the open source physics engine Box2D or the Doom game engine via VizDoom [15]."}, {"heading": "5 Future Directions", "text": "In the future, we hope to extend OpenAI Gym in several ways.\n\u2022 Multi-agent setting. It will be interesting to eventually include tasks in which agents must collaborate or compete with other agents.\n\u2022 Curriculum and transfer learning. Right now, the tasks are meant to be solved from scratch. Later, it will be more interesting to consider sequences of tasks, so that the algorithm is trained on one task after the other. Here, we will create sequences of increasingly difficult tasks, which are meant to be solved in order.\n\u2022 Real-world operation. Eventually, we would like to integrate the Gym API with robotic hardware, validating reinforcement learning algorithms in the real world."}], "references": [{"title": "Dynamic programming and optimal control", "author": ["Dimitri P Bertsekas"], "venue": "Athena Scientific Belmont, MA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Sadik Beattie", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen"], "venue": "C., Antonoglou A., H. I., King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML, pages 1889\u20131897", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "J. Artif. Intell. Res., 47:253\u2013279", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Yan Duan", "Xi Chen", "Rein Houthooft", "John Schulman", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1604.06778,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "RLPy: A value-function-based reinforcement learning framework for education and research", "author": ["A. Geramifard", "C. Dann", "R.H. Klein", "W. Dabney", "J.P. How"], "venue": "J. Mach. Learn. Res., 16:1573\u20131578", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "RL-Glue: Language-independent software for reinforcement-learning experiments", "author": ["B. Tanner", "A. White"], "venue": "J. Mach. Learn. Res., 10:2133\u20132136", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "PyBrain", "author": ["T. Schaul", "J. Bayer", "D. Wierstra", "Y. Sun", "M. Felder", "F. Sehnke", "T. R\u00fcckstie\u00df", "J. Schmidhuber"], "venue": "J. Mach. Learn. Res., 11:743\u2013746", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "RLLib: Lightweight standard and on/off policy reinforcement learning library (C++)", "author": ["S. Abeyruwan"], "venue": "http://web.cs.miami.edu/home/saminda/rilib.html", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "The reinforcement learning competition", "author": ["Christos Dimitrakakis", "Guangliang Li", "Nikoalos Tziortziotis"], "venue": "AI Magazine,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Pachi: State of the art open source go program", "author": ["Petr Baudi\u0161", "Jean-loup Gailly"], "venue": "In Advances in Computer Games,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["Micha\u0142 Kempka", "Marek Wydmuch", "Grzegorz Runc", "Jakub Toczek", "Wojciech Ja\u015bkowski"], "venue": "arXiv preprint arXiv:1605.02097,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "RL has a rich mathematical theory and has found a variety of practical applications [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].", "startOffset": 302, "endOffset": 311}, {"referenceID": 2, "context": "Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].", "startOffset": 302, "endOffset": 311}, {"referenceID": 3, "context": "Recent advances that combine deep learning with reinforcement learning have led to a great deal of excitement in the field, as it has become evident that general algorithms such as policy gradients and Q-learning can achieve good performance on difficult problems, without problem-specific engineering [2, 3, 4].", "startOffset": 302, "endOffset": 311}, {"referenceID": 4, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 234, "endOffset": 237}, {"referenceID": 6, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 7, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 8, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 9, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 10, "context": "A variety of benchmarks have been released, such as the Arcade Learning Environment (ALE) [5], which exposed a collection of Atari 2600 games as reinforcement learning problems, and recently the RLLab benchmark for continuous control [6], to which we refer the reader for a survey on other RL benchmarks, including [7, 8, 9, 10, 11].", "startOffset": 315, "endOffset": 332}, {"referenceID": 11, "context": "In the RL literature, the environment is formalized as a partially observable Markov decision process (POMDP) [12].", "startOffset": 110, "endOffset": 114}, {"referenceID": 4, "context": "\u2022 Atari: classic Atari games, with screen images or RAM as input, using the Arcade Learning Environment [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "\u2022 Board games: currently, we have included the game of Go on 9x9 and 19x19 boards, where the Pachi engine [13] serves as an opponent.", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "These tasks use the MuJoCo physics engine, which was designed for fast and accurate robot simulation [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 5, "context": "A few of the tasks are adapted from RLLab [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "Since the initial release, more environments have been created, including ones based on the open source physics engine Box2D or the Doom game engine via VizDoom [15].", "startOffset": 161, "endOffset": 165}], "year": 2016, "abstractText": "OpenAI Gym1 is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.", "creator": "LaTeX with hyperref package"}}}