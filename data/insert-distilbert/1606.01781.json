{"id": "1606.01781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2016", "title": "Very Deep Convolutional Networks for Text Classification", "abstract": "the historically dominant approach for many nlp tasks are describing recurrent neural networks, in particular lstms, pcs and convolutional neural networks. however, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. we consider present with a new architecture for text processing which operates directly on the character level domains and uses only small convolutions and pooling operations. we are able to show that the performance of this general model increases with the depth : using up to 29 convolutional layers, we report significant improvements over the extensive state - of - the - first art on several public text classification tasks. still to the best of our knowledge, this is the first time that very deep convolutional nets have been applied to nlp.", "histories": [["v1", "Mon, 6 Jun 2016 15:14:50 GMT  (34kb)", "http://arxiv.org/abs/1606.01781v1", null], ["v2", "Fri, 27 Jan 2017 12:49:11 GMT  (41kb)", "http://arxiv.org/abs/1606.01781v2", "10 pages, EACL 2017, camera-ready"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["alexis conneau", "holger schwenk", "lo\\\"ic barrault", "yann lecun"], "accepted": false, "id": "1606.01781"}, "pdf": {"name": "1606.01781.pdf", "metadata": {"source": "CRF", "title": "Very Deep Convolutional Networks for Natural Language Processing", "authors": ["Alexis Conneau", "Holger Schwenk"], "emails": ["aconneau@fb.com", "schwenk@fb.com", "yann@fb.com", "loic.barrault@univ-lemans.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n01 78\n1v 1\n[ cs\n.C L\n] 6"}, {"heading": "1 Introduction", "text": "The goal of natural language processing (NLP) is to process text with computers in order to analyze it, to extract information and eventually to represent the same information differently. We may want to associate categories to parts of the text (e.g. POS tagging or sentiment analysis), structure text differently (e.g. parsing), or convert it to some other form which preserves all or part of the content (e.g. machine translation, summarization). The level of granularity of this processing can range from individual characters or words up to whole sentences or even paragraphs.\nAfter a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks. However, while the use of (deep) neural networks in NLP has shown very good results for many tasks, it seems that they have not yet reached the level to outperform the state-of-the-art by a large margin, as it was observed in computer vision and speech recognition.\nConvolutional neural networks, in short ConvNets, are very successful in computer vision. In early approaches to computer vision, handcrafted features were used, for instance \u201cscale-invariant feature transform (SIFT)\u201d, followed by some classifier. The fundamental idea of ConvNets is to consider feature extraction and classification as one jointly trained task. This idea has been improved over the years, in particular by using many layers of convolutions and pooling to sequentially extract a hierarchical representation of the input. The best networks are using more than 150 layers [7, 8].\nMany NLP approaches consider words as basic units. An important step was the introduction of continuous representations of words. These word embeddings are now the state-of-the-art in NLP. However, it is less clear how we should best represent a sequence of words, e.g. a whole sentence, which has complicated syntactic and semantic relations. In general, in the same sentence, we may be\nfaced with local and long-range dependencies. Currently, the main-stream approach is to consider a sentence as a sequence of tokens (characters or words) and to process them with a recurrent neural network (RNN). Tokens are usually processed in sequential order, from left to right, and the RNN is expected to \u201cmemorize\u201d the whole sequence in its internal states. The most popular and successful RNN variant are certainly LSTMs \u2013 there are many works which have shown the ability of LSTMs to model long-range dependencies in NLP applications, e.g. [17, 18] to name just a few. However, we argue that LSTMs are generic learning machines for sequence processing which are lacking task specific structure.\nWe propose the following analogy. It is well known that a fully connected one hidden layer neural network can in principle learn any real-valued function, but much better results can be obtained with a deep problem-specific architecture which develops hierarchical representations. By these means, the search space is heavily constrained and efficient solutions can be learned with SGD. ConvNets for computer vision are typical examples of this. We argue, that to some extent, RNNs or LSTMs could be considered as an extension of fully connected neural networks for sequence processing. They are unstructured and not adapted to the particular task: the same LSTM cell is used in all applications - only the number of LSTM cells is varied and eventually several LSTM layers are stacked.\nIt is often argued that RNN learn deep representations of sequences by unrolling them over time (or the sequence length). However, we argue that the depth varies considerably for the tokens in the sequence: the first token goes through many recurrent layers while basically a linear operation is applied to the last one. Despite the gating mechanism of LSTMs, this leads to the fact that recent tokens are better \u201cmemorized\u201d than older ones. This has even motivated researchers to process sequences in reverse order, e.g. [18], or in both directions, named bidirectional LSTMs. It is also common to stack several LSTM layers. To the best of our knowledge, no improvements were observed beyond four stacked LSTM layers despite drop-out regularization.\nWe believe that the challenge in NLP is to develop deep architectures which are able to learn hierarchical representations of whole sentences, jointly with the task. In this paper, we propose to use deep architectures of many convolutional layers to approach this goal, using up to 29 layers. The design of our architecture is inspired by recent progress in computer vision, in particular [15, 7].\nThis paper is structured as follows. There have been previous attempts to use ConvNets for text processing. We summarize the previous works in the next section and discuss the relations and differences. Our architecture is described in detail in section 3. We have evaluated our approach on several sentence classification tasks, initially proposed by Zhang et al. [20]. These tasks and our experimental results are detailed in section 4. The proposed deep convolutional network outperforms the state-of-the-art convolutional neural networks on all tasks. The paper concludes with a discussion of future research directions."}, {"heading": "2 Related work", "text": "There is a large body of research on sentiment analysis, or more generally on sentence classification tasks. Initial approaches followed the classical two stage scheme of extraction of (handcrafted) features, followed by a classification stage. Typical features include bag-of-words or n-grams, and their TF-IDF. These techniques have been compared with ConvNets by Zhang et al. [20]. We use the same corpora for our experiments. More recently, words or characters, are projected into a high dimensional space, and these embeddings are combined to obtain a fixed size representation of the input sentence, which then serves as input for the classifier. The simplest combination is the element-wise mean. This usually performs badly since all notion of token order is disregarded.\nAnother class of approaches are recursive neural networks. The main idea is to use an external tool, namely a parser, which specifies the order in which the word embeddings are combined. At each node, the left and right context are combined using weights which are shared for all nodes [16]. The state of the top node is fed to the classifier. A recurrent neural network (RNN) could be considered as a special case of a recursive NN: the combination is performed sequentially, usually from left to right. The last state of the RNN is used as fixed-sized representation of the sentence, or eventually a combination of all the hidden states.\nFirst works using convolutional neural networks for NLP are probably [3, 4]. They have been subsequently applied to sentence classification [12, 11, 20]. We will discuss these techniques in more detail below. If not otherwise stated, all approaches operate on words which are projected into a high-dimensional space.\nA rather shallow neural net was proposed in Kim [12]: one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time. The final classifier uses one fully connected layer with drop-out. Results are reported on six data sets, in particular Stanford Sentiment Treebank (SST). A similar system was proposed in Kalchbrenner et al. [11], but using five convolutional layers. An important difference is also the introduction of multiple temporal k-max pooling layers. This allows to detect the k most important features in a sentence, independent of their specific position, preserving their relative order. The value of k depends on the length of the sentence and the position of this layer in the network. Zhang et al. [20] were the first to perform sentiment analysis entirely at the character level. Their systems use up to six convolutional layers, followed by three fully connected classification layers. Convolutional kernels of size 3 and 7 are used, as well as simple max-pooling layers. Another interesting aspect of this paper is the introduction of several large-scale data sets for text classification. We use the same experimental setting (see section 4.1). The use of character level information was also proposed by dos Santos and Gatti [5]: all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information. The overall architecture is quite shallow with two convolutional layers. Good results are reported on SST and on a very small subset of the Stanford Twitter Sentiment corpus (STS).\nIn the computer vision community, the combination of recurrent and convolutional networks in one architecture has also been investigated, with the goal to \u201cget the best of both worlds\u201d, e.g. [14].\nThe same idea was recently applied to sentence classification [19]. A convolutional network with up to five layers is used to learn high-level features which serve as input for an LSTM. The initial motivation of the authors was to obtain the same performance as Zhang et al. [20] with networks which have significantly fewer parameters. They report results very close to those of Zhang et al. [20] or even outperform ConvNets for some data sets.\nIn summary, we are not aware of any work that uses more than six convolutional layers for sentence classification. Deeper networks were not tried or they were reported to not improve performance. This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks, namely 19 layers [15], or even up to 152 layers [7]. In the remainder of this paper, we describe our very deep convolutional architecture and report results on the same corpora than Zhang et al. [20]. We were able to show that performance improves with increased depth, using up to 29 convolutional layers."}, {"heading": "3 Architecture", "text": "The overall architecture of our network is shown in Figure 1. Our model begins with a look-up table that creates a vectorial representation (embedding) of each character and transforms the input sentence into a two-dimensional tensor of size f0 \u00d7 s, with f0 the dimension of the embedding space and s the number of characters of the input text. In this work, the input is a fixed-sized padded text: f0 can be seen as the equivalent of the \"RGB\" dimension of an image of size 1 \u00d7 s. We first apply one layer of 64\nconvolutions of size 3\u00d7 f0, followed by a stack of temporal \u201cconvolutional blocks\u201d. Inspired by the philosophy of VGG and ResNets we apply these two design rules: (i) for the same output temporal\nresolution, the layers have the same number of feature maps, (ii) when the temporal resolution is halved, the number of feature maps is doubled. Starting with 64 feature maps, we apply three pooling operations (halving the temporal resolution each time by 2), resulting in 3 levels of 128, 256 and 512 feature maps (see Figure 1). Different depths of the overall architecture are obtained by varying the number of convolutional blocks in between the pooling layers (see table 1). The output of these convolutional blocks is a tensor of size 512 \u00d7 sd, where sd = s2p with p = 3 the number of down-sampling operations. At this level of the convolutional network, the resulting tensor can be seen as a high-level representation of the input text. Since we deal with padded input text of fixed size, sd is constant. However, in the case of variable size input, the convolutional encoder provides a representation of the input text that depends on its initial length s. Representations of a text as a set of vectors of variable size can be valuable namely for neural machine translation, in particular when combined with an attention model. In Figure 1, temporal convolutions with kernel size 3 and X feature maps are denoted \"3, Temp Conv, X\", fully connected layers which are linear projections (matrix of size I \u00d7 O) are denoted \"fc(I, O)\" and \"3-max pooling, stride 2\" means temporal max-pooling with kernel size 3 and stride 2.\nMost of the previous applications of ConvNets to NLP use an architecture which is rather shallow (up to 6 convolutional layers) and combines convolutions of different sizes, e.g. spanning 3, 5 and 7 tokens. This was motivated by the fact that convolutions extract n-gram features over tokens and that different n-gram lengths are needed to model short- and long-span relations. In this work, we propose to create instead an architecture which uses many layers of small convolutions (size 3). Stacking 4 layers of such convolutions results in a span of 9 tokens, but the network can learn by itself how to best combine these different \u201c3-gram features\u201d in a deep hierarchical manner. Our architecture can be in fact seen as a temporal adaptation of the VGG network [15]. We have also investigated the same kind of \u201cResNet shortcut\u201d connections as in He et al. [7], namely identity and 1\u00d7 1 convolutions (see Figure 1).\nFor the classification tasks we deal with in this work, the temporal resolution of the output of the convolution blocks is first down-sampled to a fixed dimension using k-max pooling. By these means, the network extracts the k most important features, independently of the position they appear in the sentence. The 512 \u00d7 k resulting features are transformed into a single vector which is the input to a three layer fully connected classifier with ReLU hidden units and softmax outputs. The number of output neurons depends on the classification task, the number of hidden units is set to 2048, and k to 8 in all experiments. It may be relevant to adapt the number of hidden units in the classifier to the size of the training set (see Table 2), but we kept this parameter fixed in the experiments. It was also found to be better to not use drop-out with the fully connected layers, but only temporal batch normalization after convolutional layers.\nConvolutional Block\nEach convolutional block (see Figure 2) is a sequence of two convolutional layers, each one followed by a temporal BatchNorm [9] layer and an ReLU activation. The kernel size of all the temporal convolutions is 3, with padding such that the temporal resolution is preserved (or halved in the case of the convolutional pooling with stride 2, see below). Steadily increasing the depth of the network by adding more convolutional layers is feasible thanks to the limited number of parameters of very small convolutional filters in all layers. Temporal batch normalization applies the same kind of regularization as batch normalization except that the activations in a mini-batch are jointly normalized over temporal (instead of spatial) locations. So, for a mini-batch of size m and feature maps of temporal\nsize s, the sum and the standard deviations related to the BatchNorm algorithm are taken over |B| = m \u00b7 s terms.\nWe explore three different types of down-sampling between blocks Ki and Ki+1 (Figure 1) :\n(i) The first convolutional layer of Ki+1 has stride 2.\n(ii) Ki is followed by a k-max pooling layer [11] where k is such that the resolution is halved.\n(iii) Ki is followed by max-pooling with kernel size 3 and stride 2.\nAll these types of pooling reduce the temporal resolution by a factor 2. At the final convolutional layer, the resolution is thus sd.\nIn this work, we have explored four depths for our networks: 9, 17, 29 and 49, which we define as being the number of convolutional layers. The depth of a network is obtained by summing\nthe number of blocks with 64, 128, 256 and 512 filters, with each block containing two convolutional layers. In Figure 1, the network has 2 blocks of each type, resulting in a depth of 2\u00d7 (2+2+2+2) = 16. Adding the very first convolutional layer, this sums to a depth of 17 convolutional layers. The depth can thus be increased or decreased by adding or removing convolutional blocks with a certain number of filters. The best configurations we observed for depths 9, 17, 29 and 49 are described in Table 1. We also give the number of parameters of all convolutional layers."}, {"heading": "4 Experimental evaluation", "text": ""}, {"heading": "4.1 Tasks and data", "text": "In the computer vision community, the availability of large data sets for object detection and image classification has fueled the development of new architectures. In particular, this made it possible to compare many different architectures and to show the benefit of very deep convolutional networks. We present our results on eight freely available large-scale data sets introduced by Zhang et al. [20] which cover several classification tasks such as sentiment analysis, topic classification or news categorization (see Table 2). The number of training examples varies from 120k up to 3.6M, and the number of classes is comprised between 2 and 14. This is considerably lower than in computer vision (e.g. 1 000 classes for ImageNet). This has the consequence that each example induces less gradient information which may make it harder to train large architectures. It should be also noted that some of the tasks are very ambiguous, in particular sentiment analysis for which it is difficult to clearly associate fine grained labels. There are equal numbers of examples in each class for both training and test sets. The reader is referred to Zhang et al. [20] for more details on the construction of the data sets. Table 3 summarizes the best published results on these corpora we are aware of. It is important to mention that we do not use \u201cThesaurus data augmentation\u201d or any other preprocessing, except lower-casing the texts. Nevertheless, we still outperform the best convolutional neural networks of Zhang et al. [20] for all data sets. The main goal of our work is to show that it is possible and beneficial to train very deep convolutional networks for NLP. Data augmentation may improve our results even further. We will investigate this in future research."}, {"heading": "4.2 Common model settings", "text": ""}, {"heading": "4.3 Experimental results", "text": "In this section, we evaluate several configurations of our model, namely three different depths and three different pooling types (see Section 3). Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with small temporal convolution filters with different types of pooling, which shows that a significant improvement on the state-of-the-art configurations can be achieved on text classification tasks by pushing the depth to 29 convolutional layers.\nOur deep architecture works well on big data sets in particular, even for small depths Table 4 shows the test errors for depths 9, 17 and 29 and for each type of pooling : convolution with stride 2, k-max pooling and temporal max-pooling. For the smallest depth we use (9 convolutional layers), we see that our model already performs better than Zhang\u2019s convolutional baselines (which includes 6 convolutional layers and has a different architecture) on the biggest data sets : Yelp Full, Yahoo Answers and Amazon Full and Polarity. The most important decrease in classification error can be observed on the largest data set Amazon Full which has more than 3 Million training samples. We also observe that for a small depth, temporal max-pooling works best on all data sets.\nDepth improves performance As we increase the network depth to 17 and 29, the test errors decrease on all data sets, for all types\nof pooling (with 2 exceptions for 48 comparisons). Going from depth 9 to 17 and 29 for Amazon Full reduces the error rate by 1% absolute. Since the test is composed of 650K samples, 6.5K more test samples have been classified correctly. These improvements, especially on large data sets, are significant and show that increasing the depth is useful for text processing. Overall, compared to previous state-of-the-art, our best architecture with depth 29 and max-pooling has a test error of 37.0 compared to 40.43%. This represents a gain of 3.43% absolute accuracy. The significant improvements which we obtain on all data sets compared to Zhang\u2019s convolutional models do not include any data augmentation technique.\nMax-pooling performs better than other pooling types In terms of pooling, we can also see that max-pooling performs best overall, very close to convolutions with stride 2, but both are significantly superior to k-max pooling.\nBoth pooling mechanisms perform a max operation which is local and limited to three consecutive tokens, while k-max polling considers the whole sentence at once. According to our experiments, it seems to hurt performance to perform this type of max operation at intermediate layers (with the exception of the smallest data sets).\nOur models outperform the state-of-the-art ConvNets We obtain state-of-the-art results for all data sets, except AG\u2019s news and Sogou news which are the smallest ones. However, with our very deep architecture, we get closer to the state-of-the-art which are ngrams TF-IDF for these data sets and significantly surpass convolutional models presented in [20]. As observed in previous work, differences in accuracy between shallow (TF-IDF) and deep (convolutional) models are more significant on large data sets, but we still perform well on small data sets while getting closer to the non convolutional state-of-the-art results on small data sets. The very deep models even perform as well as ngrams and ngrams-TF-IDF respectively on the sentiment analysis task of Yelp Review Polarity and the ontology classification task of the DBPedia data set.\nGoing even deeper degrades accuracy. Shortcut connections help reduce the degradation As described in He et al. [7], the gain in accuracy due to the the increase of the depth is limited when using standard ConvNets. When the depth increases too much, the accuracy of the model gets saturated and starts degrading rapidly. This degradation problem was attributed to the fact that very deep models are harder to optimize. The gradients which are backpropagated through the very deep networks vanish and SGD with momentum is not able to provide a correct minimum of the loss function. To overcome this degradation of the model, the ResNet model introduced shortcut connections between convolutional blocks that allow the gradients to flow more easily in the network [7].\nWe evaluate the impact of shortcut connections by increasing the number of convolutions to 49 layers. We present an adaptation of the ResNet model to the case of temporal convolutions for text (see Figure 1). Table 5 shows the evolution of the train/test errors on the Yelp Review Full data set with or without shortcut connections.1 When looking at the column \u201cwithout shortcut\u201d, we observe the same degradation problem as in the original ResNet article: when going from 29 to 49 layers, the training goes up from 29.57 to 35.54, and test error rate increases as well (35.28\u219237.40). When using shortcut connections, we observe improved results when the network has 49 layers: both the training and test errors go down and the network is less prone to underfitting than it was without shortcut connections.\nWhile shortcut connections give better results when the network is very deep (49 layers), we were not able to reach state-of-the-art results with them. We plan to further explore adaptations of residual networks to temporal convolutions as we think this a milestone for going deeper in NLP.\nExploring these models on text classification tasks with more classes sounds promising Note that one of the most important difference between the classification tasks discussed in this work and ImageNet is that the latter deals with 1000 classes and thus much more information is backpropagated to the network through the gradients. Exploring the impact of the depth of temporal convolutional models on categorization tasks with hundreds or thousands of classes would be an interesting challenge and is left for future research.\n1Pooling is defined as convolution with stride 2 in this case, as in the ResNet configuration."}, {"heading": "5 Conclusion", "text": "We have presented a new architecture for NLP which follows two design principles: 1) operate at the lowest atomic representation of text, i.e. characters, and 2) use a deep stack of local operations, i.e. convolutions and max-pooling of size 3, to learn a high-level hierarchical representation of a sentence. This architecture has been evaluated on eight freely available large-scale data sets and we were able to show that increasing the depth up to 29 convolutional layers steadily improves performance. Our models are much deeper than previously published convolutional neural networks and they outperform those approaches on all data sets. To the best of our knowledge, this is the first time that the \u201cbenefit of depths\u201d was shown for convolutional neural networks in NLP.\nWhat makes convolutional networks appropriate for image processing is that the visual world is compositional. Pixels combine to form shapes, contours and profiles. With ConvNets, higher level features are derived from lower level features to form a hierarchical representation. The convolution filters of a ConvNet create patterns of increasing scale and complexity: first a contour, then a shape, a face, etc. An image and a small text have similar properties. Texts are also compositional for many languages. Characters combine to form n-grams, stems, words, phrase, sentences etc. These similar properties make the comparison between computer vision and natural language processing very profitable and we believe future research should invest into making text processing models deeper. Our work is a first attempt towards this goal.\nIn this paper, we focus on the use of very deep convolutional neural networks for sentence classification tasks. Applying similar ideas to other sequence processing tasks, in particular neural machine translation, should be interesting. Note that both tasks are conceptually different. On one hand, for sentence classification, the system needs to extract characteristic features while disregarding information which is not relevant for the classification task (e.g. morphology). In neural machine translation, on the other hand, all information in the sentence must be preserved. It needs to be investigated whether the proposed architecture has to be adapted to tackle sequence-to-sequence processing with deep convolutional encoders."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston L\u00e9on Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Deep convolutional neural networks for sentiment analysis of short texts", "author": ["C\u00edcero Nogueira dos Santos", "Ma\u00edra Gatti"], "venue": "In Coling,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In ICCV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In http:// arxiv.org/abs/", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP, pages 1700\u20131709,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A convolutional neural network for modelling sentences. In http:// arxiv.org/ abs", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Convolutional encoders for neural machine translation", "author": ["Andrew Lamb", "Michel Xie"], "venue": "In WEB download,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["Pedro O. Pinhero", "Ronan Collobert"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Richard Socher", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "In Interspeech,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho"], "venue": "In http:// arxiv.org/abs/", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "After a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks.", "startOffset": 33, "endOffset": 42}, {"referenceID": 2, "context": "After a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks.", "startOffset": 33, "endOffset": 42}, {"referenceID": 3, "context": "After a couple of pioneer works ([2, 3, 4] among others), the use of neural networks for NLP applications is attracting huge interest in the research community and they are systematically applied to all NLP tasks.", "startOffset": 33, "endOffset": 42}, {"referenceID": 6, "context": "The best networks are using more than 150 layers [7, 8].", "startOffset": 49, "endOffset": 55}, {"referenceID": 7, "context": "The best networks are using more than 150 layers [7, 8].", "startOffset": 49, "endOffset": 55}, {"referenceID": 16, "context": "[17, 18] to name just a few.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[17, 18] to name just a few.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[18], or in both directions, named bidirectional LSTMs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The design of our architecture is inspired by recent progress in computer vision, in particular [15, 7].", "startOffset": 96, "endOffset": 103}, {"referenceID": 6, "context": "The design of our architecture is inspired by recent progress in computer vision, in particular [15, 7].", "startOffset": 96, "endOffset": 103}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "At each node, the left and right context are combined using weights which are shared for all nodes [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "First works using convolutional neural networks for NLP are probably [3, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 3, "context": "First works using convolutional neural networks for NLP are probably [3, 4].", "startOffset": 69, "endOffset": 75}, {"referenceID": 11, "context": "They have been subsequently applied to sentence classification [12, 11, 20].", "startOffset": 63, "endOffset": 75}, {"referenceID": 10, "context": "They have been subsequently applied to sentence classification [12, 11, 20].", "startOffset": 63, "endOffset": 75}, {"referenceID": 19, "context": "They have been subsequently applied to sentence classification [12, 11, 20].", "startOffset": 63, "endOffset": 75}, {"referenceID": 11, "context": "A rather shallow neural net was proposed in Kim [12]: one convolutional layer (using multiple widths and filters) followed by a max pooling layer over time.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "[11], but using five convolutional layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] were the first to perform sentiment analysis entirely at the character level.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The use of character level information was also proposed by dos Santos and Gatti [5]: all the character embeddings of one word are combined by a max operation and they are then jointly used with the word embedding information.", "startOffset": 81, "endOffset": 84}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The same idea was recently applied to sentence classification [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "[20] with networks which have significantly fewer parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] or even outperform ConvNets for some data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks, namely 19 layers [15], or even up to 152 layers [7].", "startOffset": 161, "endOffset": 165}, {"referenceID": 6, "context": "This is in sharp contrast to the current trend in computer vision where significant improvements have been reported using much deeper networks, namely 19 layers [15], or even up to 152 layers [7].", "startOffset": 192, "endOffset": 195}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Our architecture can be in fact seen as a temporal adaptation of the VGG network [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 6, "context": "[7], namely identity and 1\u00d7 1 convolutions (see Figure 1).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Each convolutional block (see Figure 2) is a sequence of two convolutional layers, each one followed by a temporal BatchNorm [9] layer and an ReLU activation.", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "(ii) Ki is followed by a k-max pooling layer [11] where k is such that the resolution is halved.", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "[20] which cover several classification tasks such as sentiment analysis, topic classification or news categorization (see Table 2).", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] for more details on the construction of the data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] for all data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] for a detailed description.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] best results use a Thesaurus data augmentation technique (marked with an ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Xiao and Cho [19] propose a combination of convolution and recurrent layers.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "[20], all processing is done at the character level which is the atomic representation of a sentence, same as pixels for images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "However, with our very deep architecture, we get closer to the state-of-the-art which are ngrams TF-IDF for these data sets and significantly surpass convolutional models presented in [20].", "startOffset": 184, "endOffset": 188}, {"referenceID": 6, "context": "[7], the gain in accuracy due to the the increase of the depth is limited when using standard ConvNets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "To overcome this degradation of the model, the ResNet model introduced shortcut connections between convolutional blocks that allow the gradients to flow more easily in the network [7].", "startOffset": 181, "endOffset": 184}], "year": 2016, "abstractText": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which are very successful in computer vision. We present a new architecture for text processing which operates directly on the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report significant improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to NLP.", "creator": "LaTeX with hyperref package"}}}