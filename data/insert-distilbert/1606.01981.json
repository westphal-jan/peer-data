{"id": "1606.01981", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "abstract": "recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary number representation. here, we effectively show that this is ostensibly just the tip of the iceberg : these same network networks, during testing, that also generally exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a richer class of non - linear projections where binarization is just a special case. to quantify this robustness, we show easily that one such integrated network achieves 11 % net test correlation error on cifar - 10 subjects even with 0. 68 % effective equivalent bits per the weight. furthermore, we find that using a common training heuristic - - namely, projecting quantized weights during backpropagation - - can be altered ( or even removed ) and networks still achieve a base theoretical level of robustness during testing. specifically, training with weight component projections other than quantization also works, as does simply clipping the weights, both applications of which accuracy have never been reported before. currently we nonetheless confirm our same results for earlier cifar - 10 and imagenet datasets. finally, drawing results from these ideas, we propose a stochastic projection rule that leads to a new commercial state of the art network with 7. 64 % test error on cifar - 10 using no data augmentation.", "histories": [["v1", "Tue, 7 Jun 2016 00:28:42 GMT  (645kb,D)", "http://arxiv.org/abs/1606.01981v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["paul merolla", "rathinakumar appuswamy", "john arthur", "steve k esser", "dharmendra modha"], "accepted": false, "id": "1606.01981"}, "pdf": {"name": "1606.01981.pdf", "metadata": {"source": "CRF", "title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "authors": ["Paul Merolla", "Rathinakumar Appuswamy", "John Arthur", "Steve K. Esser", "Dharmendra Modha"], "emails": ["pameroll@us.ibm.com", "rappusw@us.ibm.com", "arthurjo@us.ibm.com", "sesser@us.ibm.com", "dmodha@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5]. Typically, these networks use a high precision representation for weights (e.g., 32-bit floating point) for both training and for inference. Considering just inference tasks, a long standing goal has been to reduce the precision of weights without sacrificing performance, with the aim of lowering the network\u2019s computational and memory footprint. This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].\nRemarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11]. The basic approach is to apply gradient descent using high precision weights during training, and project the weights via a quantization function (such as the sign function) in forward/backward passes. This way, the high precision weights are able to accumulate small gradient updates\u2014computed with respect to the projected ones\u2014allowing the network to explore discrete configurations in a continuous setting. It has been argued, rather reasonably, that this training procedure is essential to learning low precision representations [10]. In this work, we suggest a more nuanced view. We shift the focus from the idea that projecting quantized weights during training leads to networks robust to that quantization, to a more general phenomenon, where projecting weights via certain functions (not necessarily quantization) lead to networks robust not only to that function (or distortion), but to an entire class of distortions. This is as if a patient given the flu vaccine, finds themselves inoculated against measles, mumps, and malaria as well.\nar X\niv :1\n60 6.\n01 98\n1v 1\n[ cs\n.N E\n] 7\nJ un\n2 01\nHere, we present a number of new findings:\n1. We show that many networks that perform well when their weights are binarized, also perform well for other kinds of distortions. These distortions include additive and multiplicative noise, as well as applying non-linear distortions to the weights.\n2. We report that using weight projections other than quantization during training also lead to robust networks. Furthermore, we show examples where standard backprop with weight clipping can learn a base level of robustness, although performance is slightly reduced.\n3. Based on these observations, we propose a new stochastic projection rule inspired by BinaryConnect [10], except our rule projects weights to random intervals (as opposed to quantized values). This rule results in new state of the art performance for CIFAR-10 with 8.25% in the binary case and 7.64% in the non-binary case.\nThe organization of our paper is as follows: After reviewing related work in Section 2, we describe our training algorithm in Section 3. Next in Section 4, we use this algorithm to train six networks on the CIFAR-10 dataset, exploring different combinations of weight projections and weight clipping parameters. To help frame our results, we first delve into a curious finding in Section 4.1.1. Specifically, we show that a network trained using quantized weights also has low test error for non-quantized weights, suggesting that it is robust to weight distortions. Following this lead in Section 4.1.2, we uncover that this network (and others) are robust to distortions beyond weight quantization. In Section 4.1.3 we try to tease apart the aspects of training that lead to these robust networks, and report for the first time that non-quantized projections (and even no projections at all) can also lead to robustness. A new stochastic projection rule is explored in Section 4.1.4. Then in Sections 4.2 and 4.2.1 we check that our findings hold up for ImageNet. Section 5 puts forth two theoretical ideas on how backprop is able to find these robust solutions. Finally, we conclude and discuss future directions in Section 6."}, {"heading": "2 Related work", "text": "Our current work is related to a flurry of recent research on DNNs with low precision weights. The major advance has been the discovery of a backprop-based learning method that quantizes weights during propagations. This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9]. [13] proposed a similar rule for contrastive divergence. Here we build on this work by exploring a more general class of weight projections that are not restricted to quantization. Another recent approach approximates weights using binary values during training, formulated as a constrained optimization problem, with promising results on ImageNet [7]. We expect their findings are consistent with our results, however, this is left for future work. Other approaches have developed a probabilistic interpretation of discrete weights [14, 11], however, they have not yet been extended to convnets or datasets larger than MNIST."}, {"heading": "3 DNNs with projected weights", "text": "We consider a DNN as two sets of tensors: input activations A, and weights W , where each set is indexed by its layer k. The output of a layer (which is also the input to the next layer) is computed using the convolution operation \u2217, for example Ak+1 = r(Ak \u2217 Proj(Wk)) where r is the typical ReLU activation, and Proj is a projection operator described below. For notational simplicity, we specify fully-connected layers as convolutions, and do not consider neuron biases.\nIn this work, we explore DNN\u2019s under various projections to the weights used for training and testing. These projections are defined in Table 1 as scalar functions and extended to operate on tensors element-wise. We denote the i-th element of tensor Wk by wki. We introduce a per layer factor \u03b1k = max\ni (|wki|) to normalize weights to the interval [\u22121, 1] for certain projections. While\nnormalizing across the entire layer seems crude compared to a filter-wise normalization (as in [7]), we have found that the two cases lead to similar results. It is worth pointing out that the Power projection generalizes Sign and None, since Power(wki, 0) = Sign(wki), and Power(wki, 1) = None(wki).\nThe procedure that we use to train is described in Algorithm 1, which is similar to BinaryConnect [10] except we allow for arbitrary weight projections. The differences between our algorithm and standard\nbackprop, are i) we first project the weights using Proj(Wk, \u03b8) before computing the forward pass, ii) we compute the gradients with respect to the projected weights P but apply updates to W , and iii) we clip the weights after each update via a per layer clip value ck. ck is defined as the standard deviation of the initialized weights (from [15]) scaled by a global factor f , where f = 0.5 unless otherwise noted. This algorithm reduces to standard backprop when the projection is None and ck =\u221e \u2200 k. Testing, just like training, is also performed for a particular projection, however it is important to note that testing and training projections are independently specified. We often refer to projections applied during testing as distortions.\nAlgorithm 1 Training a DNN for a weight projection. Proj(Wk, \u03b8) is a projection from Table 1 with parameter \u03b8, ck are clip boundaries, L is the loss function, and N is the number of layers.\nInput: Minibatch of inputs I and targets T , current weights W t, and learning rate \u03bb. Output: Updated weights W t+1. 1: Project weights: 2: for k = 1 to N do 3: Pk = Proj(W tk, \u03b8) // In standard backprop, Proj is None, so Pk = W tk . 4: Forward propagation: 5: O = Forward(I, P ) // Standard forward pass, computed with respect to projected weights P . 6: Backward propagation: 7: \u2202L\n\u2202P = Backward( \u2202L \u2202O , P ) // Standard backward pass, also computed with respect to P .\n8: Update weights: 9: W\u0302 = Update(W t, \u2202L\n\u2202P , \u03bb) // Updates applied toW t, can use any update rule (e.g., SGD, ADAM, etc).\n10: Clip weights: 11: for k = 1 to N do 12: W t+1k = max(min(W\u0302k, ck),\u2212ck) // Weights remain in range [\u2212ck, ck]. In standard backprop, ck =\u221e ."}, {"heading": "4 Results", "text": "In this section, we explore the performance and robustness of networks trained using different weight projections and clip settings, on both CIFAR-10 (Section 4.1) and ImageNet (Section 4.2)."}, {"heading": "4.1 CIFAR-10 experiments", "text": "CIFAR-10 is an image classification dataset of small (32\u00d7 32) color images with 10 categories [16]. For our experiments, we do not use data augmentation. To train, we use the ADAM learning rule [17] with a learning rate of 0.003, and a batch size of 50; a square hinge loss; and batch normalization [18]. These results were obtained in TensorFlow [19], except for a control network that used Caffe [20].\nOur experimental flow is as follows: Using the training set pre-processed with global contrast normalization and ZCA whitening from Pylearn2 [21], we trained six networks for 500 epochs using a DNN with 6 Conv and 2 FC layers (Figure 1A). These networks are named according to their training parameters. For example Tr-Sign-C specifies a network trained (Tr) using the Sign projection with clipping (C); we append NC for no clipping when ck = \u221e. A seventh network (NiN-ctrl) was downloaded pre-trained from the Caffe model zoo [22] for comparison. After training we evaluate each network\u2019s test error for different distortions to the weights (and no distortions to the biases). Batch norm parameters are re-computed on the training data for each distortion. Tests are specified using a similar naming convention, for example Te-Power refers to a test (Te) for the Power projection. Results for Te-None, Te-Sign, and Te-Round are summarized in Table 2 along with comparisons to prior work. There are a few surprising results in this table, and we analyze them one at a time in the next sections."}, {"heading": "4.1.1 High precision or -1,+1? It doesn\u2019t (much) matter", "text": "One interesting result is that all of our networks have comparable test errors for Te-None and Te-Sign. To see why this is surprising, consider the DNN from Figure 1A trained using the Sign projection (TrSign-C). During training, the loss is computed with respect to the sign of the weights, and not the high precision weights directly. Yet this network performs well when it is evaluated using either binarized weights or the high precision ones (Figure 1B). This result would be expected if the two weight distributions converged to the same values, however, this is not the case: For example, the weights of two corresponding filters are noticeably different (Figure 1B, insets), and these quantization errors are present throughout all the layers (Figure 1C). Yet despite these differences, the activity in the network still converges to similar patterns as it propagates through the layers (Figure 1D), demonstrating a surprising insensitivity to the exact weight values. Based on these observations, we next explore if these networks are also robust to other non-linear distortions beyond weight binarization."}, {"heading": "4.1.2 Robustness to weight distortions beyond -1,+1", "text": "We investigate the premise that networks that perform well with binary weights, also perform well for many types weight distortions. Here, we focus on networks trained using quantization-based weight projections (based on the networks from BinaryConnect), where allowed weight states during training are discrete. Specifically, we consider Tr-Sign-C and Tr-Stoch-C under three distortions: Te-AddNorm, Te-MultUnif, and Te-Power. NiN-ctrl is also shown for comparison.\nIn the case of adding Gaussian noise to each weight (Te-AddNorm), we observe that test error increases with \u03c3 (Figure 2A), however, Tr-Sign-C and Tr-Stoch-C are significantly more resilient than NiN-ctrl. In particular Tr-Stoch-C achieves a test error of 11% even when \u03c3 = 0.55. This corresponds to 0.68 bits per weight using a signal to noise analysis: 12 log2(1 + Qw Qn\n), where Qw and Qn are the second moments of the weight and noise distributions respectively. In the case of\nmultiplicative noise applied to each weight (Te-MultUnif), the trend is similar where Tr-Stoch-C and Tr-Sign-C are more resilient to noise (smaller \u03b3 \u2192 higher noise) compared to the control (Figure 2B). Finally for Te-Power, each weight is normalized to [0, 1], raised to the power \u03b2 \u2208 [0, 2], and multiplied by its sign. Remarkably, Tr-Stoch-C and Tr-Sign-C are also robust to these types of distortions. This is surprising because these networks were never explicitly trained using them (Figure 2C, note the semi-log scale). To visualize how this projection distorts the weights for different values of \u03b2, we show Tr-Sign-C\u2019s weight distribution for Conv2 (Figure 2D); note that lower \u03b2 pushes the weights to a bi-modal distribution that approaches Sign(Wk). In contrast, NiN-ctrl is sensitive to these distortions, and only has low error for low distortion with \u03b2 near 1.\nBringing these results together, it appears that backprop is finding paths to solutions that are generally robust to weight distortions, and not just the specific quantizations used during training."}, {"heading": "4.1.3 Learning robust networks with and without weight projections", "text": "In this section, we attempt to uncover the aspects of training that lead to robust networks. Our first experiment is to see if projections other than weight quantization also work. Accordingly, we trained a network using the Power projection (Tr-Power-C) where a new \u03b2 \u2208 [0, 2] is drawn for each minibatch. The network converges to a solution that is similarly robust to Tr-Sign-C (Figure 2A-C), which is remarkable considering that the projected weights undergo a stochastic non-linear distortion at each training step. This confirms that training with weight projections other than quantization also works, and opens the door to trying more exotic projections (for example, see Section 4.1.4).\nNext, we tried removing weight projections altogether. We trained networks Tr-None-C (no projection with clipping) and Tr-None-NC (no projection without clipping). Putting these networks through the same battery of tests, we observe that Tr-None-C is more robust than Tr-None-NC, although they both exhibit the same basic trends. Notably Tr-None-C still achieves a test error of 11.3% even when its weights are quantized to binary, even though it was never trained with binary weights. Later on in\nSection 5, we hypothesize how weight clipping can be viewed as a type of regularization, which may help explain these results.\nA curious result is that Tr-None-NC also exhibits a base level of robustness while NiN-ctrl does not\u2014even though they are both trained using standard backprop without clipping. This conflicts with previous findings where crude quantization does not lead to good performance [23]. We believe that typically, the process of \u201ctuning\u201d a DNN for the best score often leads to non-robust networks. In our case, however, our networks are first \u201ctuned\u201d using weight projections (e.g., Sign) during training. Therefore, when we remove the weight projections from training, the rest of the parameters are still in a regime where backprop discovers robust solutions. Because this learning regime appears delicate, we take a more practical view in the rest of the paper and focus on backprop with weight clipping and (or) weight projections."}, {"heading": "4.1.4 Stochastic multiplicative projection", "text": "Inspired by the Stoch projection first introduced in [10], we constructed a new stochastic projection rule, StochM. In Stoch, each weight is randomly projected to +1 with probability p = 12 ( wki \u03b1k\n+ 1) and \u22121 with probability 1\u2212 p. StochM derives from a similar idea, but now projects each weight to the interval [\u03b3wki, wki\u03b3 ] with probability p, and [\u2212\u03b3wki,\u2212 wki \u03b3 ] with 1\u2212 p. The rationale is that there is nothing special about projecting to two values, so why not sample the weight space more densely?\nIn terms of performance, Tr-StochM-C achieves 7.64% error for Te-None and 8.25% for Te-Sign (Table 2), which are state of the art for this data set without data augmentation, to the best of our knowledge. The network also exhibits a high degree of robustness (Figure 2A-C). It is interesting to note that using \u03b3 = 0.5 during training, the expected value of the projected weight is no longer the\nsame as wki, which was originally thought to be important for these stochastic projections to work properly [10]."}, {"heading": "4.2 Towards a robust AlexNet", "text": "To see whether our results extend beyond CIFAR-10, we moved to ImageNet (ILSVRC2012), which is a dataset with \u223c 1.2M training images and 1K classes [24]. We use an AlexNet [1] with 5 convolution and 2 fully connected layers, modified with batch norm layers. These experiments were run in MatConvnet [25] using SGD with no momentum and a batch size of 512.\nBefore benchmarking, we tested whether weight projections are needed to obtain robust networks. Accordingly, two AlexNets were trained without projections for 20K iterations (8.5 epochs): one without weight clipping (Tr-None-NC), and one with weight clipping (Tr-None-C). In both cases the top-5 error was reported for Te-None, Te-Round, and Te-Sign. Focusing on network Tr-None-NC (Figure 4A), we find that while Te-None reaches 28.1%1, the test error for Te-Round and Te-Sign are significantly worse. So our previous CIFAR-10 result for Tr-None-NC did not hold up for ImageNet.\nNetwork Tr-None-C tells a different story (Figure 4B): the network has similar performance for Te-None, Te-Round, and Te-Sign, at 35%, 34.7% and 40.5% respectively. This confirms our previous observation that weight clipping during training can influence network robustness. Although in this case, the peak error after 20K iterations is about 7% higher for Te-None compared to network Tr-None-NC; to help to mitigate this effect, we use a clipping scheduling to increase clip values during training in later experiments. Also we observe that, Te-Round performs better than Te-Sign."}, {"heading": "4.2.1 ImageNet benchmarks", "text": "We benchmarked Tr-StochM3-C2 on ImageNet. This network was trained for 150K iterations (64 epochs). The learning rate was dropped from 0.1 by 0.01\u00d7 at iterations 100K and 125K. We also increased clip values by scaling global clip factor f from 4.5, by 1.4\u00d7 at iterations 5K, 10K, and 15K. This had the effect of keeping Te-None and Te-Round scores in sync, while also allowing the network to reach lower error.3 We probed the network\u2019s robustness using Te-Power, and compare test error to two recent models that use binary weights (Figure 4)."}, {"heading": "5 Hypotheses on learning robust networks", "text": "We put forth two ideas on how backprop can find paths to robust solutions.\n1This network reaches 22.5% after 100K iterations. 2StochM3 is the three-value counterpart to StochM. Specifically, the middle 50% of the weights are\ndropped to 0 with probability 0.5. 3Note that when the clip values are scaled, performance temporarily dips as the network adapts.\nFirst, we explore the idea that imposing constraints on weights can act as a regularizer, similar to DropConnect [26]. This idea was first suggested in [10] for the case of weight binarization. Here, we examine how imposing weight clipping (without weight projections) can also act as regularizer in the context of proximal methods; see [27] for an excellent review on proximal methods. Consider minimizing L(w) + G(w) where w is a vector containing all the weights, L(\u00b7) is the loss, and G(\u00b7) is a regularizer. For convex L(\u00b7), the proximal gradient method is\nwt+1 = proxG(w t \u2212 \u03bb \u2202L\n\u2202wt )\nwhere proxG(v) = argmin\nx\n( G(x) + (1/2\u03bb)||x\u2212 v||22 ) .\nIn the trivial case where G(x) = 0, proxG(v) = v and the proximal gradient method reduces to a standard gradient descent. In our case of weight clipping and assuming a per layer clip value of 1, proxG(v) = max(min(v, 1),\u22121), where max and min functions operate element-wise on v. This corresponds to\nG(w) = { 0 \u2016w\u2016\u221e \u2264 1 +\u221e otherwise.\nIn other words, applying weight clipping during an update can be understood (in a convex setting) as imposing a type of regularization that penalizes weights outside the unit ball of the `\u221e norm. Empirically in the non-convex case, this regularization appears to reduces the sensitivity of L(w) to distortions. For future work, one could also try other operations on weights commonly used in proximal methods; see [28] for preliminary work in this vein.\nSecond, consider a stochastic (or deterministic) weight projection. Let \u03c6\u03b8 : R\u2192 R be a stochastic projection function parametrized by a vector \u03b8 that maps a real-valued weight to its projection. We extend \u03c6\u03b8 to operate on vectors element-wise. When w is used without projection during forward and backward steps, gradient descent is traversing the cost surface\nEdata [L(w)] .\nHowever, if the weights are projected with \u03c6\u03b8 before computing forward and backward steps, then backprop only has access to L(\u03c6\u03b8(w)). By sampling from its distribution over several minibatches, gradient descent is traversing the alternative error surface\nEdataE\u03c6\u03b8 [L(\u03c6\u03b8(w))].\nHence the solution obtained by minimizing EdataE\u03c6\u03b8 [L(\u03c6\u03b8(w))] necessarily provides the most robustness against distortions under \u03c6\u03b8 to its weights. Furthermore, the surface EdataE\u03c6\u03b8 [L(\u03c6\u03b8(w))] is a smoothed-over version of the surface Edata [L(w)], where the smoothness is controlled by the distribution of the noise source underlying \u03c6\u03b8\u2019s stochasticity. During training, typically, backprop estimates the gradient of Edata [L(w)] which takes presumed stochasticity of the data into account to produce a smoothed estimate. In our case, backprop estimates the gradient of EdataE\u03c6\u03b8 [L(\u03c6\u03b8(w))] by additionally sampling the weight space in the neighborhood of w. This provides additional gradient smoothing even when \u03c6\u03b8 is deterministic to some extent. Thus, both the error surface and its gradient are smoother which may explain the empirical results observed in this paper. During testing, because\nthe objective function is L(w), using either w directly or with a deterministic projection should yield the best results. For future work, we envision cooling the noise source underlying \u03c6\u03b8\u2019s stochasticity as training progress so that eventually\nEdataE\u03c6\u03b8 [L(\u03c6\u03b8(w))]\u2192 Edata [L(w)] .\nFor example, one may draw \u03c6\u03b8(w) with respect to the normal distribution N (w, \u03c32) and let \u03c3 \u2192 0. Moreover, we are also interested in extending the domain of \u03c6\u03b8 to include vectors, so a weight projection can be a function of more than one weight. For instance, recent work from [7] has already used a projection that depends on the `1 norm of the weights for each filter, although their work is in a slightly different context from ours.\nTaking a step back, it appears that in all these cases (weight clipping, deterministic projections, and stochastic projections) the weight gradients are distorted relative to the gradients from standard backprop. Viewing these distortions as a type of noise may bridge our findings with recent work that suggests adding explicit gradient noise results in better overall performance [29]."}, {"heading": "6 Conclusion and future work", "text": "We expand on previous work demonstrating that networks trained with backprop can become robust to specific weight distortions, such as binary weights. Here we show that imposing certain weight distortions during training leads to a regime where the network becomes robust not only to that distortion, but to an entire family of distortions as well.\nBased on this observation, we proposed a novel rule that stochastically projects each weight to a random interval based on its current value. We hypothesize that this rule, similar to the stochastic projection rule in BinaryConnect, is not optimizing the weight values directly, but instead optimizing the neighborhood of the weight vector. In practice, our rule leads to state of the art performance for CIFAR-10 for both binary and non-binary weighted networks.\nOur finding that a network can achieve 89% on CIFAR-10 with 0.68 bits per weight may also be of practical interest. One potential application is that weights can be implemented with noisy devices, which could have implications for neuromorphic computing.\nMore recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9]. Training these networks is similar to the binary weight case, namely binary activations are imposed during training. We hypothesize that the neuron outputs in these models are also robust to distortions. If confirmed, this would suggest that imposing other activation constraints could improve performance. We cannot help but speculate that the built-in robustness to noise in synapses and neurons is an inherent characteristic of the brain and may prove invaluable in opening new directions in deep learning and neuromorphic computing."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["Steven K Esser", "Paul A Merolla", "John V Arthur", "Andrew S Cassidy", "Rathinakumar Appuswamy", "Alexander Andreopoulos", "David J Berg", "Jeffrey L McKinstry", "Timothy Melano", "Davis R Barch"], "venue": "arXiv preprint arXiv:1603.08270,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["Steve K Esser", "Rathinakumar Appuswamy", "Paul Merolla", "John V Arthur", "Dharmendra S Modha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms", "author": ["Evangelos Stromatias", "Daniel Neil", "Michael Pfeiffer", "Francesco Galluppi", "Steve B Furber", "Shih-Chii Liu"], "venue": "Frontiers in neuroscience,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Expectation backpropagation: parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proceedings of the ACM International Conference on Multimedia,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Neural network adaptations to hardware implementations", "author": ["Perry Moerland", "Emile Fiesler"], "venue": "Technical report, IDIAP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["Andrea Vedaldi", "Karel Lenc"], "venue": "In Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D Collins", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1412.1442,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "arXiv preprint arXiv:1511.06807,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1", "author": ["Matthieu Courbariaux", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 1, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 2, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 3, "context": "Deep neural networks (DNNs) trained using backpropagation have been shown to perform exceptionally well on a wide range of classification tasks [1, 2, 3, 4, 5].", "startOffset": 144, "endOffset": 159}, {"referenceID": 4, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 136, "endOffset": 142}, {"referenceID": 5, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 136, "endOffset": 142}, {"referenceID": 6, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 249, "endOffset": 255}, {"referenceID": 7, "context": "This has practical applications that include network compression, running networks faster and more efficiently on conventional hardware [6, 7], and running networks on specialized hardware designed specifically for reduced precision representations [8, 9].", "startOffset": 249, "endOffset": 255}, {"referenceID": 8, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 7, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 5, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 9, "context": "Remarkably, a slew of recent work has shown that using just two (binary) or three (ternary) values for weights, DNNs can approach state of the art performance on popular benchmarks [10, 9, 7, 11].", "startOffset": 181, "endOffset": 195}, {"referenceID": 8, "context": "It has been argued, rather reasonably, that this training procedure is essential to learning low precision representations [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Based on these observations, we propose a new stochastic projection rule inspired by BinaryConnect [10], except our rule projects weights to random intervals (as opposed to quantized values).", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9].", "startOffset": 39, "endOffset": 47}, {"referenceID": 8, "context": "This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9].", "startOffset": 39, "endOffset": 47}, {"referenceID": 7, "context": "This training method was introduced in [12, 10] with impressive results on CIFAR-10, and developed in the context of neuromorphic hardware in [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 11, "context": "[13] proposed a similar rule for contrastive divergence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Another recent approach approximates weights using binary values during training, formulated as a constrained optimization problem, with promising results on ImageNet [7].", "startOffset": 167, "endOffset": 170}, {"referenceID": 12, "context": "Other approaches have developed a probabilistic interpretation of discrete weights [14, 11], however, they have not yet been extended to convnets or datasets larger than MNIST.", "startOffset": 83, "endOffset": 91}, {"referenceID": 9, "context": "Other approaches have developed a probabilistic interpretation of discrete weights [14, 11], however, they have not yet been extended to convnets or datasets larger than MNIST.", "startOffset": 83, "endOffset": 91}, {"referenceID": 5, "context": "While normalizing across the entire layer seems crude compared to a filter-wise normalization (as in [7]), we have found that the two cases lead to similar results.", "startOffset": 101, "endOffset": 104}, {"referenceID": 8, "context": "The procedure that we use to train is described in Algorithm 1, which is similar to BinaryConnect [10] except we allow for arbitrary weight projections.", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "ck is defined as the standard deviation of the initialized weights (from [15]) scaled by a global factor f , where f = 0.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "CIFAR-10 is an image classification dataset of small (32\u00d7 32) color images with 10 categories [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "To train, we use the ADAM learning rule [17] with a learning rate of 0.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "003, and a batch size of 50; a square hinge loss; and batch normalization [18].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "These results were obtained in TensorFlow [19], except for a control network that used Caffe [20].", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "These results were obtained in TensorFlow [19], except for a control network that used Caffe [20].", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "Our experimental flow is as follows: Using the training set pre-processed with global contrast normalization and ZCA whitening from Pylearn2 [21], we trained six networks for 500 epochs using a DNN with 6 Conv and 2 FC layers (Figure 1A).", "startOffset": 141, "endOffset": 145}, {"referenceID": 1, "context": "2% \u03b2 \u2208 U[0, 2] Tr-StochM-C 7.", "startOffset": 8, "endOffset": 14}, {"referenceID": 8, "context": "4% BC-Sign [10] 9.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "BC-Stoch [10] 8.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Finally for Te-Power, each weight is normalized to [0, 1], raised to the power \u03b2 \u2208 [0, 2], and multiplied by its sign.", "startOffset": 51, "endOffset": 57}, {"referenceID": 1, "context": "Finally for Te-Power, each weight is normalized to [0, 1], raised to the power \u03b2 \u2208 [0, 2], and multiplied by its sign.", "startOffset": 83, "endOffset": 89}, {"referenceID": 1, "context": "Accordingly, we trained a network using the Power projection (Tr-Power-C) where a new \u03b2 \u2208 [0, 2] is drawn for each minibatch.", "startOffset": 90, "endOffset": 96}, {"referenceID": 20, "context": "This conflicts with previous findings where crude quantization does not lead to good performance [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "Inspired by the Stoch projection first introduced in [10], we constructed a new stochastic projection rule, StochM.", "startOffset": 53, "endOffset": 57}, {"referenceID": 8, "context": "same as wki, which was originally thought to be important for these stochastic projections to work properly [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "2M training images and 1K classes [24].", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "We use an AlexNet [1] with 5 convolution and 2 fully connected layers, modified with batch norm layers.", "startOffset": 18, "endOffset": 21}, {"referenceID": 22, "context": "These experiments were run in MatConvnet [25] using SGD with no momentum and a batch size of 512.", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "6% BC-Sign [7] 39%", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "BWN [7] 23%", "startOffset": 4, "endOffset": 7}, {"referenceID": 23, "context": "First, we explore the idea that imposing constraints on weights can act as a regularizer, similar to DropConnect [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "This idea was first suggested in [10] for the case of weight binarization.", "startOffset": 33, "endOffset": 37}, {"referenceID": 24, "context": "For future work, one could also try other operations on weights commonly used in proximal methods; see [28] for preliminary work in this vein.", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "For instance, recent work from [7] has already used a projection that depends on the `1 norm of the weights for each filter, although their work is in a slightly different context from ours.", "startOffset": 31, "endOffset": 34}, {"referenceID": 25, "context": "Viewing these distortions as a type of noise may bridge our findings with recent work that suggests adding explicit gradient noise results in better overall performance [29].", "startOffset": 169, "endOffset": 173}, {"referenceID": 26, "context": "More recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9].", "startOffset": 102, "endOffset": 112}, {"referenceID": 5, "context": "More recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9].", "startOffset": 102, "endOffset": 112}, {"referenceID": 7, "context": "More recently, research on binary weights has been extended to also include binary neuron activations [30, 7, 9].", "startOffset": 102, "endOffset": 112}], "year": 2016, "abstractText": "Recent results show that deep neural networks achieve excellent performance even when, during training, weights are quantized and projected to a binary representation. Here, we show that this is just the tip of the iceberg: these same networks, during testing, also exhibit a remarkable robustness to distortions beyond quantization, including additive and multiplicative noise, and a class of non-linear projections where binarization is just a special case. To quantify this robustness, we show that one such network achieves 11% test error on CIFAR-10 even with 0.68 effective bits per weight. Furthermore, we find that a common training heuristic\u2014 namely, projecting quantized weights during backpropagation\u2014can be altered (or even removed) and networks still achieve a base level of robustness during testing. Specifically, training with weight projections other than quantization also works, as does simply clipping the weights, both of which have never been reported before. We confirm our results for CIFAR-10 and ImageNet datasets. Finally, drawing from these ideas, we propose a stochastic projection rule that leads to a new state of the art network with 7.64% test error on CIFAR-10 using no data augmentation.", "creator": "LaTeX with hyperref package"}}}