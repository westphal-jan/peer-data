{"id": "1606.02206", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "A Minimax Approach to Supervised Learning", "abstract": "given a task of predicting $ y $ from $ x $, a loss function $ l $, and a set composed of estimated probability distributions $ \\ gamma $, what is the optimal mutual decision rule minimizing the worst - case expected loss over $ \\ gamma $? in this paper, we address this question systematically by introducing a generalization of the principle of maximum entropy. collectively applying this principle to sets of distributions with a proposed structure, we must develop a general minimax approach for supervised learning problems, that reduces to the maximum likelihood problem over generalized linear models. through this framework, we develop two classification algorithms called the minimax svm and the minimax brier classifier. the minimax svm, which is a relaxed version factor of the standard svm, minimizes merely the worst - case 0 - 1 loss over the structured set of distribution, and by our numerical experiments can outperform the svm. we also simultaneously explore the emerging application of the developed framework in robust feature selection.", "histories": [["v1", "Tue, 7 Jun 2016 16:39:09 GMT  (205kb,D)", "http://arxiv.org/abs/1606.02206v1", null], ["v2", "Mon, 1 Aug 2016 01:20:30 GMT  (207kb,D)", "http://arxiv.org/abs/1606.02206v2", null], ["v3", "Thu, 4 Aug 2016 23:03:43 GMT  (207kb,D)", "http://arxiv.org/abs/1606.02206v3", null], ["v4", "Sun, 6 Nov 2016 23:19:58 GMT  (284kb,D)", "http://arxiv.org/abs/1606.02206v4", null], ["v5", "Tue, 4 Jul 2017 01:56:04 GMT  (855kb,D)", "http://arxiv.org/abs/1606.02206v5", null]], "reviews": [], "SUBJECTS": "stat.ML cs.IT cs.LG math.IT", "authors": ["farzan farnia", "david tse"], "accepted": true, "id": "1606.02206"}, "pdf": {"name": "1606.02206.pdf", "metadata": {"source": "CRF", "title": "A Minimax Approach to Supervised Learning", "authors": ["Farzan Farnia", "David Tse"], "emails": ["farnia@stanford.edu", "dntse@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Supervised learning, the task of inferring a function that predicts a target Y from a feature vector X = (X1, . . . , Xd) by using n labeled training samples {(x1, y1), . . . , (xn, yn)}, has been a problem of central interest in machine learning. Given the underlying distribution P\u0303X,Y , the optimal prediction rules had long been studied and formulated in the statistics literature. However, the advent of highdimensional problems raised this important question that what would be an optimal prediction rule when we do not have enough samples to estimate the underlying distribution?\nTo understand the difficulty of learning in high-dimensional settings, consider a classification task for a genome-wide association studies (GWAS) problem where we seek to predict a binary label Y from an observation of 3, 000, 000 SNPs, each of which is a categorical variable Xi \u2208 {0, 1, 2}. Hence, to estimate the underlying distribution we need O(33,000,000) samples, that is impossible.\nWith no possibility of estimating the underlying P \u2217 in such problems, several methods have been proposed to deal with high-dimensional settings. A standard approach in statistical learning is the empirical risk minimization (ERM) [1]. ERM learns the prediction rule by minimizing an approximated loss under the empirical distribution of samples P\u0302 . However, to avoid overfitting ERM restricts the set of allowable decision rules to a class of functions with limited complexity, such as hypothesis classes of small VC dimension or spaces of norm-bounded linear functions.\nAs a complementary approach to ERM, one can learn the prediction rule through minimizing a decision rule\u2019s worst-case loss over a larger set of distributions \u0393(P\u0302 ) centered at the empirical distribution P\u0302 . In other words, instead of restricting the class of decision rules, we consider and evaluate all possible decision rules, but based on a more stringent criterion that they will have to perform well over all distributions in \u0393(P\u0302 ). As seen in Figure 1, this minimax approach can be broken into three main steps: First, we compute the empirical distribution P\u0302 from the data; Second, we form a distribution set \u0393(P\u0302 ) based on P\u0302 ; Finally, we learn a prediction rule \u03c8\u2217 that minimizes the worst-case expected loss over \u0393(P\u0302 ). \u2217Department of Electrical Engineering, Stanford University, Stanford, CA 94305.\nar X\niv :1\n60 6.\n02 20\n6v 1\n[ st\nat .M\nL ]\n7 J\nun 2\n01 6\nSome special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme. In this paper, we develop a general minimax approach for supervised learning problems with arbitrary loss function.\nTo formulate Step 3 in Figure 1, given a general loss function L and set of distribution \u0393(P\u0302 ) we generalize the problem formulation discussed at [3] to\nargmin \u03c8\u2208\u03a8 max P\u2208\u0393(P\u0302 )\nE [ L ( Y, \u03c8(X) ) ] . (1)\nHere, \u03a8 is the space of all decision rules. Notice the difference with the ERM setting where \u03a8 was restricted to smaller function classes while \u0393(P\u0302 ) = {P\u0302}. If we have to predict Y with no access to X, (1) will reduce to the formulation studied at [5]. There, the authors propose to use the principle of maximum entropy [6], for a generalized definition of entropy, to find the optimal prediction minimizing the worst-case loss. By the principle of maximum entropy, we should select and predict based on a distribution in \u0393(P\u0302 ) that maximizes the entropy function.\nHow can we use the principle of maximum entropy to solve (1) when we observe X as well? A natural idea is to apply the maximum entropy principle to the conditional PY |X=x instead of the marginal PY . This idea motivates a generalized version of the principle of maximum entropy, which we call the principle of maximum conditional entropy. In fact, this principle breaks Step 3 in Figure 1 into two smaller steps: First, we search for P \u2217 the distribution maximizing the conditional entropy over \u0393(P\u0302 ); Then, we find \u03c8\u2217 the optimal decision rule for P \u2217.\nAlthough the principle of maximum conditional entropy characterizes the solution to (1), computing the maximizing distribution is hard, in general. In [7], the authors propose a conditional version of the principle of maximum entropy, for the specific case of Shannon entropy, and draw the principle\u2019s connection to (1). They call it the principle of minimum mutual information, by which one should predict based on the distribution minimizing mutual information among X and Y . However, they develop their theory targeting a broad class of distribution sets, which results in a convex problem yet with an exponential number of variables in the dimension of the problem.\nIn this paper, we propose to fix the marginal PX across the distributions in \u0393(P\u0302 ) to find the right structure for the distribution set. Note that for a prediction task the goal is to learn the conditional distribution PY |X. Thus, through convex duality we require to learn only the dual variables corresponding to the constraints \u0393(P\u0302 ) enforces on PY |X. Therefore, if the empirical marginal P\u0302X provides sufficient knowledge of the underlying P\u0303X to learn those dual variables, we can learn a predictive model. Moreover, by imposing this specific structure on \u0393(P\u0302 ), (1) reduces to an unconstrained convex problem with a number of variables linear in the number of constraints on PY |X in \u0393(P\u0302 ).\nMore importantly, by applying the described idea for the generalized conditional entropy we provide a generalization of the duality derived in [8] between maximum conditional (Shannon) entropy and maximum likelihood for logistic regression. This generalization justifies all generalized linear models via a unified minimax framework. In particular, we show how under quadratic and logarithmic loss\nfunctions our framework leads to the linear regression and logistic regression models respectively. Through the same framework, we also derive two classification algorithms which we call the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss and by our numerical experiments outperforms the SVM. Note that ERM with the 0-1 loss is known to be NP-hard [9]. The minimax Brier classifier justifies making binary classification using the Huber penalty and extends this binary classification technique to a multi-class version. Finally, we discuss the framework application in robust feature selection."}, {"heading": "2 Principle of Maximum Conditional Entropy", "text": "In this section, we provide a conditional version of the key definitions and results developed in [5]. We propose the principle of maximum conditional entropy to break Step 3 into 3a and 3b in Figure 1. We also define and characterize Bayes decision rules under different loss functions to address Step 3b."}, {"heading": "2.1 Decision Problems, Bayes Decision Rules, Conditional Entropy", "text": "Consider a decision problem. Here the decision maker observes X \u2208 X from which she predicts a random target variable Y \u2208 Y using an action a \u2208 A. Let PX,Y = (PX , PY |X) be the underlying distribution for the random pair (X,Y ). Given a loss function L : Y \u00d7A \u2192 [0,\u221e], L(y, a) indicates the loss suffered by the decision maker by deciding action a when Y = y. The decision maker uses a decision rule \u03c8 : X \u2192 A to select an action a = \u03c8(x) from A based on an observation x \u2208 X . We will in general allow the decision rules to be random, i.e. \u03c8 is random. The main purpose of extending to the space of randomized decision rules is to form a convex set of decision rules. Later in Theorem 2, this convexity is used to prove a saddle-point theorem.\nWe call a (randomized) decision rule \u03c8Bayes a Bayes decision rule if for all decision rules \u03c8 and for all x \u2208 X :\nE[L(Y, \u03c8Bayes(X))|X = x] \u2264 E[L(Y, \u03c8(X))|X = x].\nIt should be noted that \u03c8Bayes depends only on PY |X , i.e. it remains a Bayes decision rule under a different PX . Although we are not generally guaranteed that a Bayes decision rule exists, we can define conditional entropy of Y given X = x as\nH(Y |X = x) := inf \u03c8 E[L(Y, \u03c8(X))|X = x], (2)\nand the conditional entropy of Y given X as H(Y |X) := \u2211 x PX(x)H(Y |X = x). (3)\nWe can also define an (unconditional) entropy [5]\nH(Y ) := inf a\u2208A\nE[L(Y, a)]. (4)\nNote that H(Y |X = x) and H(Y |X) are both concave in PY |X . Applying Jensen\u2019s inequality, this concavity implies that\nH(Y |X) \u2264 H(Y ),\nwhich motivates the following definition for the information that X carries about Y ,\nI(X;Y ) := H(Y )\u2212H(Y |X), (5)\ni.e. the reduction of expected loss in predicting Y by observing X . In [10], the author has defined the same concept to which he calls a coherent dependence measure. It can be seen that I(X;Y ) = EPX [D(PY |X , PY ) ] where D is the divergence measure corresponding to the loss L, defined for any two probability distributions PY , QY with Bayes actions aP , aQ as [5]\nD(PY , QY ) := EP [L(Y, aQ)]\u2212 EP [L(Y, aP )] = EP [L(Y, aQ)]\u2212HP (Y ). (6)"}, {"heading": "2.2 Examples", "text": ""}, {"heading": "2.2.1 Logarithmic Loss", "text": "For any y \u2208 Y and distribution QY , define\nLlog(y,QY ) = \u2212 logQY (y). (7)\nIt can be seen that under the logarithmic loss Hlog(Y ), Hlog(Y |X), Ilog(X;Y ) are the well-known unconditional, conditional Shannon entropy and mutual information [11]. The divergence measure is the well-known KL-divergence. Also, the Bayes decision rule for every distribution PX,Y is given by\n\u03c8Bayes(x) = PY |X(\u00b7|x). (8)"}, {"heading": "2.2.2 0-1 loss function", "text": "The 0-1 loss function is defined for any y, y\u0302 \u2208 Y as L0-1(y, y\u0302) = I(y\u0302 6= y). Then, we can show\nH0-1(Y ) = 1\u2212max y\u2208Y PY (y), H0-1(Y |X) = 1\u2212 \u2211 x\u2208X max y\u2208Y PX,Y (x, y).\nUnder the 0-1 loss function, the Bayes decision rule for a distribution PX,Y is the well-known maximum a posteriori (MAP) rule, i.e.\n\u03c8Bayes(x) = argmax y\u2208Y\nPY |X(y|x). (9)"}, {"heading": "2.2.3 Quadratic loss function", "text": "The quadratic loss function is defined as L2(y, y\u0302) = (y \u2212 y\u0302)2. It can be seen\nH2(Y ) = Var(Y ), H2(Y |X) = E [Var(Y |X)], I2(X;Y ) = Var (E[Y |X]) .\nAlso, the Bayes decision rule for any PX,Y is the well-known minimum mean-square error (MMSE) estimator that is\n\u03c8Bayes(x) = E[Y |X = x]. (10)"}, {"heading": "2.2.4 Brier loss function", "text": "Unlike logarithmic loss and 0-1 loss functions, the quadratic loss function does not make perfect sense for a discrete variable Y. The Brier loss function [12] is an adjusted version of the quadratic loss function targeting a discrete Y , where for any distribution QY on Y and an outcome y \u2208 Y ,\nLBR(y,QY ) = \u2016\u03b4y \u2212 qY \u201622. (11)\nHere \u03b4y denotes a vector of size |Y|, 1 at index y and 0 elsewhere, and qY stands for the vector of probabilities for QY . Then,\nHBR(Y ) = 1\u2212 \u2016pY \u201622, HBR(Y |X) = 1\u2212 E [ PY |X(Y |X) ] ,\nGiven the distribution PX,Y the Bayes decision rule is uniquely\n\u03c8Bayes(x) = PY |X(\u00b7|x). (12)\nConnection to the maximal correlation: Consider the well-known Pearson correlation coefficient \u03c1(X,Y ) = COV(X,Y )\u03c3X\u03c3Y that measures the linear dependence among random variables X and Y . To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315]. The HGR maximal correlation of two random variables X,Y is defined as\n\u03c1m(X;Y ) = sup f,g \u03c1 (f(X), g(Y )) , (13)\nwhere the supremum is taken over all functions f, g with finite non-zero variance. In [15], it has been shown the maximal correlation satisfies several interesting properties. Here, we connect this measure to the information under the Brier loss IBR(X ; Y ).\nTheorem 1. Suppose Y \u2208 Y = {0, . . . , t} takes each value i with probability pi. Consider Y \u2019s one-hot encoding as Yi = I(Y = i), then\nIBR(X ; Y ) = t\u2211 i=0 pi(1\u2212 pi) \u03c12m(X;Yi). (14)\nProof. Refer to the Appendix to see the proof.\nCorollary 1. For a binary Y \u2208 Y = {0, 1},\nIBR(X ; Y ) = 2 p0(1\u2212 p0) \u03c12m(X;Y ). (15)"}, {"heading": "2.3 Principle of Maximum Conditional Entropy & Robust Bayes decision rules", "text": "Given a distribution set \u0393, consider the following minimax problem to find a decision rule minimizing the worst-case expected loss over \u0393\nargmin \u03c8\u2208\u03a8 max P\u2208\u0393\nEP [L(Y, \u03c8(X))], (16)\nwhere \u03a8 is the space of all randomized mappings from X to A and EP denotes the expected value over distribution P . Note that by a randomized mapping \u03c8 we mean a random selection of members of F , the space of deterministic functions from X to A, according to a certain distribution. We call any solution \u03c8\u2217 to the above problem a robust Bayes decision rule against \u0393. When \u0393 is convex, the following theorem guarantees the existence of a saddle point for (16), under some mild conditions. Therefore, Theorem 2 motivates a generalization of the maximum entropy principle to find robust Bayes decision rules. Theorem 2. Suppose that \u0393 is convex and that under any P \u2208 \u0393 there exists a Bayes decision rule. We also assume the continuity of Bayes decision rules for distributions in \u0393 (See the Appendix for the exact condition). Then, if P \u2217 maximizes H(Y |X) over \u0393, a Bayes decision rule for P \u2217 will be a robust Bayes decision rule against \u0393.\nProof. Refer to the Appendix for the proof.\nPrinciple of Maximum Conditional Entropy: Given a set of distributions \u0393, select and predict Y based on a distribution in \u0393 that maximizes the conditional entropy of Y given X , i.e.\nargmax P\u2208\u0393\nH(Y |X) (17)"}, {"heading": "3 Prediction via Maximum Conditional Entropy Principle", "text": "Consider a prediction task with target variable Y and feature vector X = (X1, . . . , Xd). Note that we do not require the variables to be discrete. As discussed earlier, the maximum conditional entropy principle reduces (16) to (17). Notice that (16) and (17) formulate Steps 3 and 3a in Figure 1, respectively. However, a general formulation of (17) in terms of the joint distribution PX,Y leads to an exponential computational complexity in the feature dimension d.\nThe key question is therefore under what structures of \u0393(P\u0302 ) in Step 2 we can solve (17) efficiently. In this section, we propose a specific structure for \u0393(P\u0302 ), under which we provide an efficient solution to Steps 3a and 3b in Figure 1. In fact, we show (17) reduces to the maximum likelihood problem over a generalized linear model, under this specific structure.\nTo describe this structure, consider a set of distributions \u0393(Q) centered around a given distribution QX,Y , where for a given norm \u2016 \u00b7 \u2016, mapping vector \u03b8(Y )t\u00d71,\n\u0393(Q) = { PX,Y : PX = QX , (18) \u2200 1 \u2264 i \u2264 t : \u2016EP [\u03b8i(Y )X]\u2212 EQ [\u03b8i(Y )X] \u2016 \u2264 i }.\nHere \u03b8 encodes Y with t-dimensional \u03b8(Y ), and \u03b8i(Y ) denotes the ith entry of \u03b8(Y ). The first constraint in the definition of \u0393(Q) says that all distributions in \u0393(Q) share the same marginal on X\nas Q; the second imposes constraints on the cross-moments between X and Y , allowing for some uncertainty in estimation. When applied to our supervised learning framework, we will choose Q to be the empirical distribution P\u0302 and select \u03b8 appropriately based on the loss function L. However, for now we will consider the problem of solving (17) over \u0393 = \u0393(Q) for general Q and \u03b8.\nTo that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems. However, we consider a different version of convex conjugate for \u2212H , which is defined with respect to \u03b8. Considering PY as the set of all probability distributions for the variable Y , we define F\u03b8 : Rt \u2192 R as the convex conjugate of \u2212H(Y ) with respect to the mapping \u03b8,\nF\u03b8(z) := max P\u2208PY\nH(Y ) + E[\u03b8(Y )]T z. (19)\nTheorem 3. Define \u0393(Q), F\u03b8 as given by (18), (19). Then the following duality holds\nmax P\u2208\u0393(Q) H(Y |X) = min A\u2208Rt\u00d7d\nEQ [ F\u03b8(AX)\u2212 \u03b8(Y )TAX ] + t\u2211 i=1 i\u2016Ai\u2016\u2217, (20)\nwhere \u2016Ai\u2016\u2217 denotes \u2016 \u00b7 \u2016\u2019s dual norm of the A\u2019s ith row. Furthermore, for the optimal P \u2217 and A\u2217\nEP\u2217 [\u03b8(Y ) |X = x ] = \u2207F\u03b8 (A\u2217x). (21)\nProof. The proof has been relegated to the the Appendix.\nWhen applying Theorem 3 on a supervised learning problem with a specific loss function, \u03b8 will be chosen such that EP\u2217 [\u03b8(Y ) |X = x ] provides sufficient information to compute the Bayes decision rule \u03a8\u2217 for P \u2217. This enables the direct computation of \u03a8\u2217, i.e. step 3 of Figure 1 , without the need to explicitly compute P \u2217 itself. Later in this section, we will discuss three examples to see how this technique applies to different loss functions.\nWe make the key observation that the problem in the RHS of (20), when i = 0 for all i\u2019s, is equivalent to minimizing the negative log-likelihood for fitting a generalized linear model [19] given by\n\u2022 An exponential family distribution p(y|\u03b7) = h(y) exp ( \u03b7T\u03b8(y)\u2212 F\u03b8(\u03b7) ) with the log-partition\nfunction F\u03b8 and the sufficient statistic \u03b8(Y ), \u2022 A linear predictor , \u03b7(X) = AX, \u2022 A link function such that E[\u03b8(Y )|X = x] = \u2207F\u03b8(\u03b7(x)).\nTherefore, Theorem 3 reveals a duality between the maximum conditional entropy problem and the regularized maximum likelihood problem for the specified generalized linear model. This duality further provides a justification for generalized linear models, since given a generalized linear model\nwe can consider the convex conjugate of its log-partition function as the negative entropy in the maximum conditional entropy framework.\nTo interpret this duality geometrically, note that by solving the regularized maximum likelihood problem in the RHS of (20), we in fact minimize a regularized KL-divergence\nargmin PY |X: (QX,PY |X)\u2208SF EQX [DKL(QY |X ||PY |X ) ] + t\u2211 i=1 i\u2016Ai(PY |X)\u2016\u2217, (22)\nwhere SF = { ( QX, PY |X(y|x) = h(y) exp(\u03b8(y)TAx\u2212 F\u03b8(Ax) ) ) |A \u2208 Rt\u00d7s} is the set of all exponential family distributions for the described GLM. This can be viewed as projecting Q onto SF (See Figure 2).\nFurthermore, considering the definition of divergence D given in (6), it can be seen that maximizing H(Y |X) over \u0393(Q) in the LHS of (20) is equivalent to the following divergence minimization problem\nargmin PY |X: (QX,PY |X)\u2208\u0393(Q) EQX [D(PY |X,UY |X) ] (23)\nwhere UY |X denotes the uniform conditional distribution. This can be interpreted as projecting the joint distribution (QX,UY |X) onto \u0393(Q) (See Figure 2). Notice the difference of divergence measures and the ordering of distributions between (23) and (22). Then, the duality shown in Theorem 3 implies the following corollary. Corollary 2. The solution to (22) would also minimize (23), i.e. (22) \u2286 (23).\nTo connect the proposed framework to the ERM setting, suppose Q = P\u0302n is the empirical distribution of n samples drawn i.i.d. from the underlying distribution P\u0303 . Then the problem in the RHS of (20) is equivalent to the ERM problem\nmin \u03c8\u2208\u03a8(SF )\nEQ[Llog(Y, \u03c8(X))] +R(\u03c8), (24)\nwhere \u03a8(SF ) denotes the set of the logarithmic-loss (Llog) Bayes decision rules corresponding to the distributions in SF . Also, R(\u03c8) = \u2211t i=1 i\u2016Ai(\u03c8)\u2016\u2217 is the added regularizer. Then, an important question is how to bound the excess risk, that is the difference between the expected loss of the two decision rules \u03c8\u0302n and \u03c8\u0303 minimizing (24) for the empirical distribution Q = P\u0302n and the underlying distribution Q = P\u0303 , respectively. Replacing the original regularizer \u2211t i=1 i\u2016Ai\u2016\u2217 with\nthe strongly-convex \u03bb \u2211t i=1 \u2016Ai\u20162\u2217, we show the following theorem to bound the excess risk.\nTheorem 4. Let the regularizer R(\u03c8) = \u03bb \u2211t i=1 \u2016Ai(\u03c8)\u20162\u2217. Take \u2016 \u00b7 \u2016/\u2016 \u00b7 \u2016\u2217 to be the `p/`q pair for 1p + 1 q = 1, 1 < q. Assume that \u2016X\u2016p \u2264 B and \u2016\u03b8(Y )\u2016\u221e \u2264 L. Then, for any \u03b4, with probability at least 1\u2212 \u03b4\nEP\u0303 [L(Y, \u03c8\u0302n(X))]\u2212 EP\u0303 [L(Y, \u03c8\u0303(X))] = O ( tL2B2 log( 1\u03b4 )\n(q \u2212 1)\u03bbn\n) . (25)\nProof. Due to the definition given in (19), \u2207F\u03b8(z) = EP [\u03b8(Y )] for some distribution P . Therefore, \u2016\u2207F\u03b8(z)\u2016\u221e \u2264 L and F\u03b8(z) \u2212 \u03b8(Y )T z is 2L-Lipschitz in each entry zi. Then, the theorem is an immediate consequence of Theorem 1 in [20].\nIn the remaining of this section, we apply the described framework to the loss functions discussed at Subsection 2.2."}, {"heading": "3.1 Logarithmic Loss: Logistic Regression", "text": "For classifying Y \u2208 Y = {1, . . . , t + 1}, let \u03b8(Y ) be the one-hot encoding of variable Y , i.e. \u03b8i(Y ) = I(Y = i) for 1 \u2264 i \u2264 t. Here, we exclude i = t+1 as I(Y = t+1) = 1\u2212 \u2211t i=1 I(Y = i). Given this \u03b8, for the logarithmic loss\nF\u03b8(z) = log ( 1+ t\u2211 j=1 exp(zj) ) , \u22001 \u2264 i \u2264 t : ( \u2207F\u03b8(z) ) i = exp (zi) / ( 1+ t\u2211 j=1 exp(zj) ) , (26)\nthat gives the multinomial logistic regression model [21]. Also, the RHS of (20) would be the regularized maximum likelihood problem for this specific GLM. This discussion is well-studied in the literature and straightforward using the duality result shown in [8]."}, {"heading": "3.2 0-1 Loss: Minimax SVM", "text": "Consider the same classification setting and \u03b8 described at the beginning of last subsection. We show in the Appendix that for the 0-1 loss we can calculate the gradient of F\u03b8 using the following procedure. Given z \u2208 Rt, let z\u0303 = (z, 0). Let \u03c3 be the permutation sorting z\u0303 in a descending order, i.e. i \u2264 j : z\u0303\u03c3(i) \u2265 z\u0303\u03c3(j). We find the smallest k where \u2211k i=1[z\u0303\u03c3(i) \u2212 z\u0303\u03c3(k+1) ] > 1. If this does not hold for any k, let k = t+ 1. Then,\n\u2200 1 \u2264 i \u2264 t : ( \u2207F\u03b8(z) ) i = { 1/k if \u03c3(i) \u2264 k, 0 Otherwise.\n(27)\nKnowing that F\u03b8(0) = t/(t+ 1) that is the 0-1 entropy of a uniformly distributed Y , the characterization of F\u03b8 is complete.\nWith\u2207F\u03b8 characterized, for Step 3b in Figure 1 we should apply the MAP rule to the output of\u2207F\u03b8 . We can also learn the linear predictor (Step 3a) through applying the gradient descent to solve the RHS of (20). The classifier minimizes the worst-case 0-1 loss over \u0393(Q). In particular, if Y is binary, i.e. t+ 1 = 2\nF\u03b8(z) = max{ 0 , z + 1\n2 , z }. (28)\nThen, if Y = {\u22121, 1}, the RHS problem of (20) would be\nmin \u03b1\nEQ [ max { 0 ,\n1\u2212 Y\u03b1TX 2\n, \u2212Y\u03b1TX }] + \u2016\u03b1\u2016\u2217. (29)\nSince max{0, 1\u2212z2 ,\u2212z} \u2264 max{0, 1\u2212 z}, if we replace \u2016\u03b1\u2016\u2217 with \u03bb\u2016\u03b1\u2016 2 2 in (29), we get a relaxation of the standard SVM formulated with the hinge loss [22]. We therefore call this classification algorithm the minimax SVM. Note that unlike the standard SVM, the minimax SVM can be naturally extended to a multi-class classification algorithm through (27)."}, {"heading": "3.3 Brier Loss: Minimax Brier Classifier", "text": "Consider the same classification setting and \u03b8 defined in the last two subsections. In the Appendix, we show we can characterize the gradient of F\u03b8 for the Brier loss by repeating the same procedure as in the minimax SVM with two modifications. First, we change the level for finding the smallest k as\u2211k i=1[z\u0303\u03c3(i) \u2212 z\u0303\u03c3(k+1) ] > 2, and second we modify (27) as\n\u2200 1 \u2264 i \u2264 t : ( \u2207F\u03b8(z) ) i =  ( 2\u2212 k\u2211 j=1 z\u0303\u03c3(j) ) /(2k) + z\u0303\u03c3(i)/2 if \u03c3(i) \u2264 k,\n0 Otherwise.\n(30)\nAs discussed in Subsection 2.2.4, the robust Bayes decision rule is the conditional P \u2217Y |X of the distribution maximizing the conditional Brier entropy. Therefore, to make prediction, we can apply the MAP rule to the probability vector\u2207F\u03b8 returns. We call this classification algorithm the minimax Brier Classifier (mmBC). For the binary case when t = 1, it can be seen\nF\u03b8(z)\u2212 z\n2 =  1 8 z2 if |z| \u2264 2, 1\n2 |z| \u2212 1 2 Otherwise,\n(31)\nwhich is the Huber penalty function [23]. This binary classification problem is the same classification problem formulated with the modified Huber loss function at [24]. Through the developed minimax framework, we can naturally extend this binary classification technique to a multi-class classification algorithm.\nBased on Corollary 1, for a binary Y the conditional Brier entropy-maximizing distribution is the distribution minimizing maximal correlation between X and Y in \u0393(P\u0303 ). Assuming some extra conditions, [4] solves the minimax problem of finding the maximal correlation-minimizing distribution, but for a larger class of distributions where only pairwise marginals are fixed. One can see that their proposed solution is based on replacing the Huber function in (31) with the following quadratic function\nF\u03b8(z)\u2212 z\n2 =\n1 8 z2, (32)\nand the condition under which their solution solves the original problem is that for the A\u2217 minimizing the RHS of (20), |A\u2217x| \u2264 1 for every input x. In [4], the authors also show for a binary prediction problem over a convex set of distributions \u0393, there exists a randomized prediction rule based on the maximal correlation-minimizing distribution, achieving a worst-case misclassification rate of at most twice the minimum worst-case misclassification rate. Here, we generalize their result to a multi-class version using the connection between the Brier information and the maximal correlation shown in Theorem 1. Theorem 5. Consider a prediction problem for Y \u2208 Y = {1, . . . , t}. Let p\u2217 denote the conditional PY |X of the distribution maximizing HBR(Y |X) over a convex set of distributions \u0393. Define the randomized decision rule \u03c8BR,\n\u2200i, 1 \u2264 i \u2264 t : \u03c8BR(x) = i, w.p. p\u2217\n2 i|x\u2211t j=1 p \u22172 j|x . (33)\nThen the worst-case misclassification rate of \u03c8BR is bounded by twice the minimum worst-case misclassification rate over \u0393, i.e.\nmax P\u2208\u0393 P (\u03c8BR(X) 6= Y ) \u2264 2 min \u03c8\u2208\u03a8 max P\u2208\u0393 P (\u03c8(X) 6= Y ).\nProof. The proof has been relegated to the Appendix.\nHence, we can predict by applying this randomized decision rule to the probability vector that \u2207F\u03b8 returns. We call this classification algorithm the minimax Randomized Brier Classifier (mmRBC). The above theorem suggests the minimax Brier classification as a natural extension of the results proven in [4] for the binary case."}, {"heading": "3.4 Quadratic Loss: Linear Regression", "text": "For a regression problem on Y \u2208 Y = R, let \u03b8(Y ) = Y be the identity function, so t = 1. To derive F\u03b8 for the quadratic loss, note that if we let PY in (19) include all possible distributions, the maximized entropy (variance for quadratic loss) and thus the F\u03b8 value would be infinity. Therefore, in (19) we restrict PY to {PY : E[Y 2] \u2264 \u03c12} given a parameter \u03c1. We show in the Appendix that a slightly adjusted version of Theorem 3 remains valid after this change, and\nF\u03b8(z)\u2212 \u03c12 = { z2/4 if |z/2| \u2264 \u03c1 \u03c1(|z| \u2212 \u03c1) if |z/2| > \u03c1, (34)\nwhich is the Huber function [23]. To find the Bayes decision rule via (21), note that\ndF\u03b8(z)\ndz =  \u2212\u03c1 if z/2 \u2264 \u2212\u03c1 z/2 if \u2212 \u03c1 < z/2 \u2264 \u03c1 \u03c1 if \u03c1 < z/2.\n(35)\nGiven the samples in a supervised learning problem if we choose the parameter \u03c1 large enough, by solving the RHS of (20) when F\u03b8(z) is replaced with z2/4 and set \u03c1 greater than maxi |A\u2217xi|, we can equivalently take F\u03b8(z) = z2/4 + \u03c12 which by (21) gives the linear regression model. Then, the RHS of (20) would be equivalent to\n\u2013 Simple linear regression when = 0. \u2013 Lasso [25] when \u2016 \u00b7 \u2016/\u2016 \u00b7 \u2016\u2217 is the `\u221e/`1 pair.\n\u2013 Ridge regression [21] when \u2016 \u00b7 \u2016 is the `2-norm. \u2013 Group lasso [26] with the `1,p regularizer when we adjust \u0393(Q)\u2019s definition for disjoint subsets I1, . . . Ik of {1, . . . , d} as\n\u0393GL(Q) = { PX,Y : PX = QX , (36) \u2200 1 \u2264 j \u2264 k : \u2016EP [ YXIj ] \u2212 EQ [ YXIj ] \u2016q \u2264 j }.\nHere, q is chosen such that 1/p + 1/q = 1. Also, XIj denotes the subvector including the Ij entries of X. See the Appendix for the proof of the group lasso case. Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28]."}, {"heading": "4 Robust Feature Selection", "text": "Using a minimax criterion over a set of distributions \u0393, we solve the following problem to select the most informative subset of k features. Here, we evaluate a feature subset based on its minimum worst-case loss over \u0393.\nargmin |S|\u2264k min \u03c8\u2208\u03a8S max P\u2208\u0393\nEP [L(Y, \u03c8( XS )) ], (37)\nwhere XS denotes the feature vector X restricted to the indices in S. Theorem 2 reduces (37) to\nargmin |S|\u2264k max P\u2208\u0393\nH(Y |XS ), (38)\nwhich under the assumption that H(Y ) is fixed across all distributions in \u0393 becomes equivalent to selecting a subset S maximizing the worst-case generalized information I(XS ;Y ) over \u0393, i.e.\nargmax |S|\u2264k min P\u2208\u0393 I(XS ;Y ). (39)\nTo solve (38) when \u0393 = \u0393(Q) (18), we apply the duality shown in Theorem 3 to obtain\nargmin A\u2208Rt\u00d7s: \u2016A\u20160,\u221e\u2264k\nEQ [ F\u03b8(A\u03a6(X))\u2212 \u03b8(Y )TA\u03a6(X) ] + t\u2211 i=1 i\u2016Ai\u2016\u2217. (40)\nHere by constraining \u2016A\u20160,\u221e = \u2016 ( \u2016A(1)\u2016\u221e, . . . , \u2016A(s)\u2016\u221e ) \u20160 where A(i) denotes the ith column of A, we impose the same sparsity pattern across the rows of A. Approximating the `0 with the convex `1 and taking \u2016 \u00b7 \u2016\u2217 to be the `1-norm, we can approximate the solution by\nargmin A\u2208Rt\u00d7s\nEQ [ F\u03b8(A\u03a6(X))\u2212 \u03b8(Y )TA\u03a6(X) ] + \u03bb\u2016A\u20161,\u221e. (41)\nIt is noteworthy that for the quadratic loss and identity \u03b8, (41) is the same as the lasso [25]. Also, for the logarithmic loss and one-hot encoding \u03b8, (41) is equivalent to the `1-regularized logistic regression. Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over \u0393(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30]."}, {"heading": "5 Numerical Experiments", "text": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4]. The results are summarized in Table 1 where the numbers indicate the percentage of error in the classification task.\nWe implemented the three mmSVM, mmBC, and mmRBC by applying gradient descent to solve the RHS of (20) with an added regularizer \u03bb\u2016\u03b1\u201622. We determined the value of \u03bb by cross validation. To determine this coefficient, we used a randomly-selected 70% of the training set for training and\nthe rest 30% of the training set for testing. We tested the values in {2\u221210, . . . , 210}. Using the tuned lambda, we trained the algorithms over all the training set and then evaluated the error rate over the test set. We performed this procedure in 1000 Monte Carlo runs each training on 70% of the data points and testing on the rest 30% and averaged the results.\nAs seen in the table, the minimax Brier Classifier and the minimax SVM result in the best performance for five and three out of the six datasets, respectively. Observe that although our main theoretical guarantee is for the randomized Brier classifier, the non-randomized Brier classifier has outperformed the randomized Brier classifier in all the datasets. Also, except a single dataset the minimax SVM outperforms the SVM.\nTo compare these methods in high-dimensional problems, we ran an experiment over synthetic data with n = 200 samples and d = 10000 features. We generated features by i.i.d. Bernoulli with P (Xi = 1) = 0.75, and considered y = sign(\u03b3Tx + z) where z \u223c N(0, 1). Using the same approach, we evaluated 20.6% error rate for SVM, 20.4% error rate for DRC, 20.0% for the mmSVM and 19.4% for the mmBC, which shows the mmSVM and mmBC can outperform SVM and DRC in high-dimensional settings as well."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Meisam Razaviyayn for helpful discussions on the topic and proofs."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Proof of Theorem 1", "text": "In this proof, we use a known result for \u03c1m(X;Z) for a BernoulliZ \u2208 Z = {0, 1}with probabilities p0, p1 [32]. For simplicity, we use px,z and pz|x to denote PX,Z(x, z) and PZ|X(z|x), respectively. Then,\n\u03c12m(X;Z) = 1\np0p1 \u2211 x [ p0p 2 x,1 + p1p 2 x,0 px ] \u2212 1\n= 1\np0p1 \u2211 x [ px ( p0p 2 1|x + p1p 2 0|x )] \u2212 1\n= 1\np0p1 \u2211 x [ px ( 1 2 (p21|x + p 2 0|x) + p0 \u2212 p1 2 (p21|x \u2212 p20|x) )] \u2212 1\n= 1\n2p0p1\n( 1\u2212 p21 \u2212 p20 \u2212 2 \u2211 x pxp1|xp0|x ) = 1\np0p1\n( p1p0 \u2212 \u2211 x pxp1|xp0|x ) .\nThen, we have t\u2211 i=0 pi(1\u2212 pi) \u03c12m(X;Yi) = t\u2211 i=0 [ pi(1\u2212 pi)\u2212 \u2211 x pxpi|x(1\u2212 pi|x) ]\n= t\u2211 i=0\n[ \u2212p2i +\n\u2211 x pxp 2 i|x ] = E [ PY |X(Y |X)\u2212 PY (Y )\n] = IBR(X;Y )."}, {"heading": "6.2 Proof of Theorem 2", "text": "First, let us recall the assumptions of Theorem 2:\n\u2022 \u0393 is convex. \u2022 For any distribution P \u2208 \u0393, there exists a Bayes decision rule. \u2022 We assume continuity in Bayes decision rules, i.e., if a sequence of distributions (Qn)\u221en=1 with the\ncorresponding Bayes decision rules (\u03c8n)\u221en=1 converges weakly to Q with a Bayes decision rule \u03c8, then under any P \u2208 \u0393, the expected loss of \u03c8n converges to the expected loss of \u03c8.\n\u2022 P \u2217 maximizes the conditional entropy H(Y |X).\nLet \u03c8\u2217 be a Bayes decision rule for P \u2217. We need to show that \u03c8\u2217 is a robust Bayes decision rule against \u0393. To show this, it suffices to show that (P \u2217, \u03c8\u2217) is a saddle point of the mentioned minimax problem, i.e., EP\u2217 [L(Y, \u03c8\u2217(X))] \u2264 EP\u2217 [L(Y, \u03c8(X))], (42) and EP\u2217 [L(Y, \u03c8\u2217(X))] \u2265 EP [L(Y, \u03c8\u2217(X))]. (43) Clearly, inequality (42) holds due to the definition of the Bayes decision rule. To show (43), let us fix an arbitrary distribution P \u2208 \u0393. For any \u03bb \u2208 (0, 1], define P\u03bb = \u03bbP + (1\u2212 \u03bb)P \u2217. Notice that P\u03bb \u2208 \u0393 since \u0393 is convex. Let \u03c8\u03bb be a Bayes decision rule for P\u03bb. Due to the linearity of the expected loss in the probability distribution, we have\nEP [L(Y, \u03c8\u03bb(X))]\u2212 EP\u2217 [L(Y, \u03c8\u03bb(X))] = EP\u03bb [L(Y, \u03c8\u03bb(X))]\u2212 EP\u2217 [L(Y, \u03c8\u03bb(X))]\n\u03bb\n\u2264 HP\u03bb(Y |X)\u2212HP \u2217(Y |X) \u03bb \u2264 0,\nfor any 0 < \u03bb \u2264 1. Here the first inequality is due to the definition of the conditional entropy and the last inequality holds since P \u2217 maximizes the conditional entropy over \u0393. Applying the continuity assumption of the Bayes decision rules, we have\nEP [L(Y, \u03c8\u2217(X))]\u2212 EP\u2217 [L(Y, \u03c8\u2217(X))] = lim \u03bb\u21920 EP [L(Y, \u03c8\u03bb(X))]\u2212 EP\u2217 [L(Y, \u03c8\u03bb(X))] \u2264 0, (44)\nwhich makes the proof complete."}, {"heading": "6.3 Proof of Theorem 3", "text": "Let us recall the definition of the set \u0393(Q):\n\u0393(Q) = { PX,Y : PX = QX , (45) \u2200 1 \u2264 i \u2264 t : \u2016EP [\u03b8i(Y )X]\u2212 EQ [\u03b8i(Y )X] \u2016 \u2264 i }.\nDefining E\u0303i , EQ [\u03b8i(Y )X] and Ci , {u : \u2016u\u2212 E\u0303i\u2016 \u2264 i}, we have\nmax P\u2208\u0393(Q) H(Y |X) = max P,w: \u2200i: wi=EP [\u03b8i(Y )X] EQX [HP (Y |X = x)] + t\u2211 i=1 ICi(wi) (46)\nwhere IC is the indicator function for the set C defined as\nIC(x) = { 0 if x \u2208 C, \u2212\u221e Otherwise.\n(47)\nFirst of all, the law of iterated expectations implies that EP [\u03b8i(Y )X] = EQX [ XE[\u03b8i(Y )|X = x] ] . Further-\nmore, problem (46) is convex and it is not hard to check that the Slater condition is satisfied. Hence strong duality holds and we can write the dual problem as\nmin A sup PY |X,w EQX\n[ HP (Y |X = x) +\nt\u2211 i=1 E[\u03b8i(Y )|X = x]AiX\n] +\nt\u2211 i=1 [ICi(wi)\u2212Aiwi] , (48)\nwhere the rows of matrix A, denoted by Ai, are the Lagrange multipliers for the constraints of wi = EP [\u03b8i(Y )\u03a6(X)]. Notice that the above problem decomposes across PY |X=x\u2019s and wi\u2019s. Hence, the dual problem can be rewritten as\nmin A\n[ EQX [ sup\nPY |X=x HP (Y |X = x) + t\u2211 i=1 E[\u03b8i(Y )|X = x]AiX\n] +\nt\u2211 i=1 sup wi [ICi(wi)\u2212Aiwi]\n] (49)\nFurthermore, according to the definition of F\u03b8, we have\nF\u03b8(Ax) = sup PY |X=x\nH(Y |X = x) + E[\u03b8(Y )|X = x]TAx. (50)\nMoreover, the definition of the dual norm \u2016 \u00b7 \u2016\u2217 implies\nsup wi ICi(wi)\u2212Aiwi = max u\u2208Ci Aiu = AiE\u0303i + i\u2016Ai\u2016\u2217. (51)\nPlugging (50) and (51) in (49), the dual problem can be simplified to\nmin A EQX\n[ F\u03b8(A\u03a6(X))\u2212\nt\u2211 i=1 AiE\u0303i\n] +\nt\u2211 i=1 i\u2016Ai\u2016\u2217\n= min A\nEQ [ F\u03b8(AX)\u2212 \u03b8(Y )TAX ] + t\u2211 i=1 i\u2016Ai\u2016\u2217, (52)\nwhich is equal to the primal problem (46) since the strong duality holds. Furthermore, applying Danskin\u2019s theorem to (50) implies that\nEP\u2217 [\u03b8(Y ) |X = x ] = \u2207F\u03b8 (A\u2217x). (53)"}, {"heading": "6.4 F\u03b8 derivation for the 0-1 Loss, minimax SVM", "text": "Here, we derive \u2207F\u03b8 for the 0-1 loss function, where \u03b8 is the described one-hot encoding that is \u03b8i(Y ) = I(Y = i) for 1 \u2264 i \u2264 t. If P (Y = i) = pi for 1 \u2264 i \u2264 t+ 1, then\nH(Y ) + E[\u03b8(Y )]T z = 1\u2212 max 1\u2264i\u2264t+1 pi + t\u2211 i=1 pizi. (54)\nHence, due to the Danskin\u2019s theorem,\n\u2207F\u03b8(z) = argmax p\u2208Rt+1: p\u22650,\n1Tp=1\nt\u2211 i=1 pizi \u2212 max 1\u2264i\u2264t+1 pi (55)\nTo solve the above problem we define z\u0303 = (z, 0) and rewrite the objective as\nt+1\u2211 i=1 piz\u0303i \u2212 max 1\u2264i\u2264t+1 pi. (56)\nThen, the optimal solution p\u2217 of (55) obeys the same order as the order of z\u0303. Without loss of generality suppose that z\u0303 is sorted in a descending order. Then, (55) is equivalent to\nargmax p\u2208Rt+1\u22650 : 1 Tp=1,\n\u2200i\u2264j: pi\u2265pj\nt+1\u2211 i=1 piz\u0303i \u2212 p1 (57)\nNote that under the constraint 1Tp = 1, for any m \u2264 t t+1\u2211 i=1 piz\u0303i\u2212p1 = (z\u03031\u2212 z\u0303m+1\u22121)p1 + m\u2211 i=2 [ (z\u0303i\u2212 z\u0303m+1)pi ] + z\u0303m+1\u2212 t+1\u2211 j=m+2 [ (z\u0303m+1\u2212 z\u0303j)pj ] . (58)\nFor the coefficients, we know z\u0303m+1 \u2212 z\u0303j is non-negative if j > m and non-positive if j < m. Let k be the smallest index for which \u2211k i=1[z\u0303i \u2212 z\u0303m+1] > 1. If that does not hold for any k, let k = t+ 1. Then, according to (58), for any solution p\u2217 to (57)\n\u2200i > k : p\u2217i = 0. (59)\nThis is because if for some i \u2265 k + 1, p\u2217i > 0 (let i be the largest index this happens), we construct a new feasible point p from p\u2217 by setting pi to be zero and for any j \u2264 k let pj = p\u2217j +p\u2217i /k. Then, for the new feasible point p, we will get a larger objective, that is a contradiction to that p\u2217 maximizes the above objective. Now, we claim that p\u22171 = p \u2217 2 = . . . = p \u2217 k = 1/k. If this is not true, then there is an index i < k where p\u2217i \u2212 p\u2217i+1 > (i + 1) for some > 0. Now if we modify the solution as pj = p \u2217 j \u2212 for any j \u2264 i and pi+1 = p\u2217i+1 + i , we get a feasible point with the same order as p\u2217,\nbut since \u2211i j=1[z\u0303j \u2212 z\u0303i+1] \u2264 1 the objective of (57) grows because of (58), that is a contradiction. Finally, this procedure characterizes the gradient as\n\u2200 1 \u2264 i \u2264 t : ( \u2207F\u03b8(z) ) i = { 1/k if i \u2264 k, 0 Otherwise.\n(60)"}, {"heading": "6.5 F\u03b8 derivation for the Brier Loss, minimax Brier classifer", "text": "Similar to the proof given for the 0-1 loss, we derive \u2207F\u03b8 for the Brier loss function with \u03b8 the described one-hot encoding. If P (Y = i) = pi for 1 \u2264 i \u2264 t+ 1, then\nH(Y ) + E[\u03b8(Y )]T z = 1\u2212 t+1\u2211 i=1 [p2i ] + t\u2211 i=1 [pizi]. (61)\nHence, due to the Danskin\u2019s theorem,\n\u2207F\u03b8(z) = argmax p\u2208Rt+1: p\u22650,\n1Tp=1\nt\u2211 i=1 [pizi \u2212 p2i ]\u2212 p2t+1 (62)\nWe define z\u0303 = (z, 0) and rewrite the objective as\nt+1\u2211 i=1 [piz\u0303i \u2212 p2i ]. (63)\nThen, the optimal solution p\u2217 of (62) obeys the same order as the order of z\u0303. To characterize the solution to (62), we use the KKT conditions. It is not hard to check the Salter condition holds here; thus, the KKT conditions are necessary and sufficient [33]. According to KKT conditions, for any optimal solution p\u2217 there exists a dual solution \u03bb\u2217 \u2265 0, \u03b2\u2217 where\n\u22001 \u2264 i \u2264 t+ 1 : p\u2217i = 1\n2 (z\u0303i + \u03bb\n\u2217 i + \u03b2 \u2217), \u03bb\u2217i p \u2217 i = 0. (64)\nSince p\u2217 has the same ordering as z\u0303, considering \u03c3 as the permutation sorting z\u0303 in a descending order, we find the smallest k such that \u2211k i=1[z\u0303\u03c3(i) \u2212 z\u0303\u03c3(k+1) ] > 2 or let k = t+ 1 if the condition holds for no k \u2264 t. We claim that the following feasible p satisfies (64) and hence provides a solution to (62).\n\u2200 1 \u2264 i \u2264 t : pi =  ( 2\u2212 k\u2211 j=1 z\u0303\u03c3(j) ) /2k + z\u0303\u03c3(i)/2 if \u03c3(i) \u2264 k,\n0 Otherwise.\n(65)\nNote that due to the choice of k, p is a feasible point, i.e. p \u2265 0 and 1Tp = 1. Let \u03b2 =( 2 \u2212 \u2211k j=1 z\u0303\u03c3(j) ) /k and \u03bb\u03c3(i) = 0 for i \u2264 k. Then, for i > k let \u03bb\u03c3(i) = \u2212z\u0303\u03c3(i) \u2212 \u03b2 =(\n\u22122+ \u2211k j=1[z\u0303\u03c3(j)\u2212 z\u0303\u03c3(i)] ) /k \u2265 0, due to the choice of k. Therefore, p satisfies the KKT conditions\nand the procedure returns a solution to (62)."}, {"heading": "6.6 Quadratic Loss: Linear Regression", "text": ""}, {"heading": "6.6.1 F\u03b8 derivation", "text": "Here, we find F\u03b8(z) = maxP\u2208PY H(Y ) + E[\u03b8(Y )]T z for \u03b8(Y ) = Y and PY = {PY : E[Y 2] \u2264 \u03c12}. Since for quadratic loss H(Y ) = Var(Y ) = E[Y 2]\u2212 E[Y ]2, the problem is equivalent to\nF\u03b8(z) = max E[Y 2]\u2264\u03c12\nE[Y 2]\u2212 E[Y ]2 + zE[Y ] (66)\nAs E[Y ]2 \u2264 E[Y 2], it can be seen for the solution EP\u2217 [Y 2] = \u03c12 and therefore we equivalently solve\nF\u03b8(z) = max |E[Y ]|\u2264\u03c1 \u03c12 \u2212 E[Y ]2 + zE[Y ] = { \u03c12 + z2/4 if |z/2| \u2264 \u03c1 \u03c1|z| if |z/2| > \u03c1. (67)"}, {"heading": "6.6.2 Applying Theorem 3 while restricting PY", "text": "For the quadratic loss, we first change PY = {PY : E[Y 2] \u2264 \u03c12} and then apply Theorem 3. Note that by modifying F\u03b8 based on the new PY we also solve a modified version of the maximum conditional entropy problem\nmax P : PX,Y \u2208\u0393(Q) \u2200x: PY |X=x\u2208PY\nH(Y |X) (68)\nIn the case PY = {PY : E[Y 2] \u2264 \u03c12} Theorem 3 remains valid given the above modification in the maximum conditional entropy problem. This is because the inequality constraint E[Y 2|X = x] \u2264 \u03c12 is linear in PY |X=x, and thus the problem remains convex and strong duality still holds. Also, when we move the constraints of wi = EP [\u03b8i(Y )X] to the objective function, we get a similar dual problem\nmin A\nsup PY |X,w:\n\u2200x: PY |X=x\u2208PY\nEQX [ HP (Y |X = x) + t\u2211 i=1 E[\u03b8i(Y )|X = x]AiX ] + t\u2211 i=1 [ICi(wi)\u2212Aiwi]\n(69) Following the next steps of the proof of Theorem 3, the proof remains valid given the modification on F\u03b8 and the maximum conditional entropy problem."}, {"heading": "6.6.3 Derivation of group lasso", "text": "To derive the group lasso, we slightly change the structure of \u0393(Q). Given disjoint subsets I1, . . . , Ik, consider a set of distributions \u0393GL(Q) with the following structure\n\u0393GL(Q) = { PX,Y : PX = QX , (70) \u2200 1 \u2264 j \u2264 k : \u2016EP [ YXIj ] \u2212 EQ [ YXIj ] \u2016 \u2264 j }.\nNow we prove a modified version of Theorem 3,\nmax P\u2208\u0393GL(Q) H(Y |X) = min \u03b1\nEQ [ F\u03b8(\u03b1 TX)\u2212 Y\u03b1TX ] + k\u2211 j=1 j\u2016\u03b1Ij\u2016\u2217. (71)\nTo prove this identity, we can use the same proof provided for Theorem 3. We only need to redefine E\u0303j = EQ [ YXIj ] and Cj = {u : \u2016u\u2212 E\u0303j\u2016 \u2264 j} for 1 \u2264 j \u2264 k. Notice that here t = 1. Using the same technique in that proof, the dual problem is formulated as\nmin \u03b1 sup PY |X,w\nEQX [ HP (Y |X = x) + E[Y |X = x]\u03b1TX ] + k\u2211 j=1 [ ICj (wIj )\u2212\u03b1IjwIj ] . (72)\nSimilar to the proof of Theorem 3, we can decouple and simplify the above problem to show (71). Then, considering the problem for the quadratic loss and taking \u2016 \u00b7 \u2016 as the `q-norm, we get the group lasso problem with the `1,p regularizer."}, {"heading": "6.7 Proof of Theorem 5", "text": "Since entropy measures the infimum expected loss given a distribution, it is sufficient to show that under any distribution P \u2208 \u0393 the misclassification rate of \u03c8BR is bounded by the maximum Brier entropy over \u0393 and the Brier entropy is generally bounded by twice the 0-1 entropy.\nTo show the first part, note that for any sequence (ai)ni=1,\n\u2200j : 2aj \u2264 a2j\u2211 a2i\n+ \u2211\na2i ,\n\u21d2 \u2200j : 1\u2212 a2j\u2211 a2i \u2264 1\u2212 2aj +\n\u2211 a2i ,\n\u21d2 \u2200j : 1\u2212 a2j\u2211 a2i \u2264 (1\u2212 aj)2 + \u2211 i6=j a2i .\nTherefore, since the conditions of Theorem 2 hold, for any distribution P \u2208 \u0393\nP (\u03c8BR(X) 6= Y ) = \u2211 i,x px,i ( 1\u2212\np\u2217 2 i|x\u2211t j=1 p \u22172 j|x\n)\n\u2264 \u2211 i,x px,i ( (1\u2212 p\u2217i|x) 2 + \u2211 j 6=i p\u2217 2 j|x ) = EP [ LBR(Y, P \u2217 Y |X)\n] \u2264 HBR(P \u2217Y |X). (73)\nAlso, note that for any sequence (ai)ni=1,\n\u2200i : 2ai \u2264 1 + a2i \u21d2 2 max i ai \u2264 1 + \u2211 i a2i \u21d2 1\u2212 \u2211 i a2i \u2264 2(1\u2212max i ai).\nTherefore, in general HBR(Y ) \u2264 2H0-1(Y ). (74)\nCombining (73) and (74), we have\nmax P\u2208\u0393 P (\u03c8BR(X) 6= Y ) \u2264 2 max P\u2208\u0393 H0-1(Y |X) = 2 min \u03c8\u2208\u03a8 max P\u2208\u0393 P (\u03c8(X) 6= Y )."}], "references": [{"title": "A robust minimax approach to classification", "author": ["Gert RG Lanckriet", "Laurent El Ghaoui", "Chiranjib Bhattacharyya", "Michael I Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Discrete chebyshev classifiers", "author": ["Elad Eban", "Elad Mezuman", "Amir Globerson"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Discrete r\u00e9nyi classifiers", "author": ["Meisam Razaviyayn", "Farzan Farnia", "David Tse"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Game theory, maximum entropy, minimum discrepancy and robust bayesian decision theory", "author": ["Peter D. Gr\u00fcnwald", "Philip Dawid"], "venue": "The Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Information theory and statistical mechanics", "author": ["Edwin T Jaynes"], "venue": "Physical review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1957}, {"title": "The minimum information principle for discriminative learning", "author": ["Amir Globerson", "Naftali Tishby"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L Berger", "Vincent J Della Pietra", "Stephen A Della Pietra"], "venue": "Computational linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Agnostic learning of monomials by halfspaces is hard", "author": ["Vitaly Feldman", "Venkatesan Guruswami", "Prasad Raghavendra", "Yi Wu"], "venue": "SIAM Journal on Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Coherent measures of discrepancy, uncertainty and dependence, with applications to bayesian predictive experimental design", "author": ["Philip Dawid"], "venue": "Technical Report 139,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W Brier"], "venue": "Monthly weather review,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1950}, {"title": "A connection between correlation and contingency", "author": ["H.O. Hirschfeld"], "venue": "In Mathematical Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1935}, {"title": "Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung", "author": ["H. Gebelein"], "venue": "ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift fu\u0308r Angewandte Mathematik und Mechanik,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1941}, {"title": "On measures of dependence", "author": ["A. R\u00e9nyi"], "venue": "Acta mathematica hungarica,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1959}, {"title": "Unifying divergence minimisation and statistical inference via convex duality", "author": ["Yasemin Altun", "Alexander Smola"], "venue": "In Learning Theory: Conference on Learning Theory COLT 2006,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Maximum entropy density estimation with generalized regularization and an application to species distribution modeling", "author": ["Miroslav Dud\u00edk", "Steven J Phillips", "Robert E Schapire"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Semi-supervised learning via generalized maximum entropy", "author": ["AN Erkan", "Y Altun", "Teh M Titterington"], "venue": "In Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Generalized linear models, volume 37", "author": ["Peter McCullagh", "John A Nelder"], "venue": "CRC press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1989}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "The elements of statistical learning, volume 1", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Pattern recognition and machine learning", "author": ["Christopher M Bishop"], "venue": "springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["Ming Yuan", "Yi Lin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Robust regression and lasso", "author": ["Huan Xu", "Constantine Caramanis", "Shie Mannor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "A unified robust regression model for lasso-like algorithms", "author": ["Wenzhuo Yang", "Huan Xu"], "venue": "In Proceedings of The International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy", "author": ["Hanchuan Peng", "Fuhui Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Normalized mutual information feature selection", "author": ["Pablo Est\u00e9vez", "Michel Tesmer", "Claudio Perez", "Jacek M Zurada"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["CK Chow", "CN Liu"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1968}, {"title": "On maximal correlation, hypercontractivity, and the data processing inequality studied by Erkip and Cover", "author": ["V. Anantharam", "A. Gohari", "S. Kamath", "C. Nair"], "venue": "arXiv preprint arXiv:1304.6133,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.", "startOffset": 163, "endOffset": 166}, {"referenceID": 1, "context": "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.", "startOffset": 281, "endOffset": 284}, {"referenceID": 2, "context": "Some special cases of this minimax approach, which are based on learning a prediction rule from low-order marginal/moments, have been addressed in the literature: [2] solves a robust minimax classification problem for continuous settings with fixed first and second-order moments; [3] develops a classification approach by minimizing the worst-case hinge loss subject to fixed low-order marginals; And [4] fits a model minimizing the maximal correlation under fixed pairwise marginals to design a robust classification scheme.", "startOffset": 402, "endOffset": 405}, {"referenceID": 1, "context": "To formulate Step 3 in Figure 1, given a general loss function L and set of distribution \u0393(P\u0302 ) we generalize the problem formulation discussed at [3] to argmin \u03c8\u2208\u03a8 max P\u2208\u0393(P\u0302 ) E [ L ( Y, \u03c8(X) ) ] .", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "If we have to predict Y with no access to X, (1) will reduce to the formulation studied at [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "There, the authors propose to use the principle of maximum entropy [6], for a generalized definition of entropy, to find the optimal prediction minimizing the worst-case loss.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "In [7], the authors propose a conditional version of the principle of maximum entropy, for the specific case of Shannon entropy, and draw the principle\u2019s connection to (1).", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "More importantly, by applying the described idea for the generalized conditional entropy we provide a generalization of the duality derived in [8] between maximum conditional (Shannon) entropy and maximum likelihood for logistic regression.", "startOffset": 143, "endOffset": 146}, {"referenceID": 7, "context": "Note that ERM with the 0-1 loss is known to be NP-hard [9].", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "In this section, we provide a conditional version of the key definitions and results developed in [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "We can also define an (unconditional) entropy [5]", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "In [10], the author has defined the same concept to which he calls a coherent dependence measure.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "It can be seen that I(X;Y ) = EPX [D(PY |X , PY ) ] where D is the divergence measure corresponding to the loss L, defined for any two probability distributions PY , QY with Bayes actions aP , aQ as [5] D(PY , QY ) := EP [L(Y, aQ)]\u2212 EP [L(Y, aP )] = EP [L(Y, aQ)]\u2212HP (Y ).", "startOffset": 199, "endOffset": 202}, {"referenceID": 9, "context": "(7) It can be seen that under the logarithmic loss Hlog(Y ), Hlog(Y |X), Ilog(X;Y ) are the well-known unconditional, conditional Shannon entropy and mutual information [11].", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "The Brier loss function [12] is an adjusted version of the quadratic loss function targeting a discrete Y , where for any distribution QY on Y and an outcome y \u2208 Y , LBR(y,QY ) = \u2016\u03b4 \u2212 qY \u20162.", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315].", "startOffset": 178, "endOffset": 185}, {"referenceID": 12, "context": "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "To extend this correlation measure to a measure for non-linear dependence, the HGR (HirschfeldGebelein-R\u00e9nyi) maximal correlation has been proposed in the probability literature [13\u201315].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "In [15], it has been shown the maximal correlation satisfies several interesting properties.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "To that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems.", "startOffset": 68, "endOffset": 75}, {"referenceID": 15, "context": "To that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems.", "startOffset": 68, "endOffset": 75}, {"referenceID": 16, "context": "To that end, we apply the Fenchel\u2019s duality technique, also used at [16\u201318] to address f-divergence minimization problems.", "startOffset": 68, "endOffset": 75}, {"referenceID": 17, "context": "We make the key observation that the problem in the RHS of (20), when i = 0 for all i\u2019s, is equivalent to minimizing the negative log-likelihood for fitting a generalized linear model [19] given by \u2022 An exponential family distribution p(y|\u03b7) = h(y) exp ( \u03b7\u03b8(y)\u2212 F\u03b8(\u03b7) ) with the log-partition function F\u03b8 and the sufficient statistic \u03b8(Y ), \u2022 A linear predictor , \u03b7(X) = AX, \u2022 A link function such that E[\u03b8(Y )|X = x] = \u2207F\u03b8(\u03b7(x)).", "startOffset": 184, "endOffset": 188}, {"referenceID": 18, "context": "Then, the theorem is an immediate consequence of Theorem 1 in [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "that gives the multinomial logistic regression model [21].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "This discussion is well-studied in the literature and straightforward using the duality result shown in [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 20, "context": "Since max{0, 1\u2212z 2 ,\u2212z} \u2264 max{0, 1\u2212 z}, if we replace \u2016\u03b1\u2016\u2217 with \u03bb\u2016\u03b1\u2016 2 2 in (29), we get a relaxation of the standard SVM formulated with the hinge loss [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "This binary classification problem is the same classification problem formulated with the modified Huber loss function at [24].", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "Assuming some extra conditions, [4] solves the minimax problem of finding the maximal correlation-minimizing distribution, but for a larger class of distributions where only pairwise marginals are fixed.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "In [4], the authors also show for a binary prediction problem over a convex set of distributions \u0393, there exists a randomized prediction rule based on the maximal correlation-minimizing distribution, achieving a worst-case misclassification rate of at most twice the minimum worst-case misclassification rate.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "The above theorem suggests the minimax Brier classification as a natural extension of the results proven in [4] for the binary case.", "startOffset": 108, "endOffset": 111}, {"referenceID": 22, "context": "\u2013 Lasso [25] when \u2016 \u00b7 \u2016/\u2016 \u00b7 \u2016\u2217 is the `\u221e/`1 pair.", "startOffset": 8, "endOffset": 12}, {"referenceID": 19, "context": "\u2013 Ridge regression [21] when \u2016 \u00b7 \u2016 is the `2-norm.", "startOffset": 19, "endOffset": 23}, {"referenceID": 23, "context": "\u2013 Group lasso [26] with the `1,p regularizer when we adjust \u0393(Q)\u2019s definition for disjoint subsets I1, .", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28].", "startOffset": 146, "endOffset": 154}, {"referenceID": 25, "context": "Another type of minimax, but non-probabilistic, justification of the robustness of lasso and group lasso as regression algorithms can be found in [27, 28].", "startOffset": 146, "endOffset": 154}, {"referenceID": 22, "context": "It is noteworthy that for the quadratic loss and identity \u03b8, (41) is the same as the lasso [25].", "startOffset": 91, "endOffset": 95}, {"referenceID": 26, "context": "Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over \u0393(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30].", "startOffset": 236, "endOffset": 244}, {"referenceID": 27, "context": "Hence, the `1-regularized logistic regression maximizes the worst-case mutual information over \u0393(Q), which seems superior to the heuristic techniques for maximizing an approximation of the mutual information I(XS ;Y ) in the literature [29, 30].", "startOffset": 236, "endOffset": 244}, {"referenceID": 1, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 313, "endOffset": 316}, {"referenceID": 0, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 354, "endOffset": 357}, {"referenceID": 28, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 392, "endOffset": 396}, {"referenceID": 2, "context": "We evaluated the performance of the minimax SVM (mmSVM), the minimax Brier Classifier (mmBC), and the minimax Randomized Brier Classifier (mmRBC), on six binary classification datasets from the UCI repository, compared to these five benchmarks: Support Vector Machines (SVM), Discrete Chebyshev Classifiers (DCC) [3], Minimax Probabilistic Machine (MPM) [2], Tree Augmented Naive Bayes (TAN) [31], and Discrete R\u00e9nyi Classifiers (DRC) [4].", "startOffset": 435, "endOffset": 438}], "year": 2017, "abstractText": "Given a task of predicting Y from X , a loss function L, and a set of probability distributions \u0393, what is the optimal decision rule minimizing the worst-case expected loss over \u0393? In this paper, we address this question by introducing a generalization of the principle of maximum entropy. Applying this principle to sets of distributions with a proposed structure, we develop a general minimax approach for supervised learning problems, that reduces to the maximum likelihood problem over generalized linear models. Through this framework, we develop two classification algorithms called the minimax SVM and the minimax Brier classifier. The minimax SVM, which is a relaxed version of the standard SVM, minimizes the worst-case 0-1 loss over the structured set of distribution, and by our numerical experiments can outperform the SVM. We also explore the application of the developed framework in robust feature selection.", "creator": "LaTeX with hyperref package"}}}