{"id": "1606.02407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Structured Convolution Matrices for Energy-efficient Deep learning", "abstract": "we derive a connectivity relationship between differential network representation in energy - efficient neuromorphic architectures and large block heap toplitz convolutional matrices. inspired by this connection, we to develop deep convolutional networks using a family of structured convolutional computing matrices models and achieve state - of - the - art experimental trade - off between energy efficiency and classification accuracy for well - known image recognition tasks. we also put forward a novel method to train binary convolutional networks by utilising an existing algebraic connection between noisy - rectified linear units functions and binary sequence activations.", "histories": [["v1", "Wed, 8 Jun 2016 05:31:43 GMT  (637kb,D)", "http://arxiv.org/abs/1606.02407v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV cs.LG", "authors": ["rathinakumar appuswamy", "tapan nayak", "john arthur", "steven esser", "paul merolla", "jeffrey mckinstry", "timothy melano", "myron flickner", "dharmendra modha"], "accepted": false, "id": "1606.02407"}, "pdf": {"name": "1606.02407.pdf", "metadata": {"source": "CRF", "title": "Structured Convolution Matrices for Energy-efficient Deep learning", "authors": ["Rathinakumar Appuswamy", "Tapan K. Nayak", "Steven Esser", "Paul A. Merolla"], "emails": ["rappusw@us.ibm.com", "tknayak@us.ibm.com", "arthurjo@us.ibm.com", "sesser@us.ibm.com", "pameroll@us.ibm.com", "jlmckins@us.ibm.com", "tmelano@us.ibm.com", "mdflickner@us.ibm.com", "dmodha@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Deep convolutional networks have, in recent times, achieved near-human performance on an array of visual, auditory, and other cognitive tasks [1, 2]. The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].\nBinary convolutional networks that use binary convolutional kernels, and binary neuron activations are ideally suited to be run on low-power neuromorphic architectures that use spike-based communication [9]. In addition, storage and computational efficiency may be gained by using structured matrices in convolutional layers. Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters. The main idea behind these approaches is to restrict the connectivity matrix of a (fully connected) layer to be from a known family of matrices, which are parametrised by a few variables and adapt the backpropagation to update those variables. On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.\nIn this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21]. By connecting the efficient weight representation schema used in neuromorphic architectures [9, 21] with block Toeplitz matrices that arise in discrete convolution, we identify a family of convolution kernels that are\nar X\niv :1\n60 6.\n02 40\n7v 1\n[ cs\n.N E\nnaturally hardware efficient. The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.\nWe incorporate learning structured convolutional matrices into traditional stochastic gradient descent [31] so that the trained inference networks are hardware-ready. Furthermore, we exploit a known equivalence between stochastic bounded rectified linear units and deterministic threshold neurons and propose a novel approach to training networks with binary neuron activations. This procedure allows us to obtain accurate gradient estimates during backward step, and speed-up convergence of training and enriches the library of tools available to train low-precision deep neural networks. We evaluate our system on Cifar10 data set and compare against best energy vs accuracy numbers reported on currently available hardware [3] \u2013 our approach reduces the number of TrueNorth cores required to achieve 87.5% on Cifar10 from 31872 TrueNoth cores in [3] to 13216 cores.\nWe begin the next section by discussing binary convolutional networks and their suitability for neuromorphic hardware and introduce the weight representation mechanism used in TrueNorth architecture [9]. In Section 3, we discuss the structure of discrete convolution matrices and present our main result connecting that structure with the weight representation in TrueNorth \u2013 thereby identifying the family of structured convolution matrices that efficiently map to that architecture. Section 4 outlines our methodology for training networks to use only structured convolution kernels, and discusses how the connection between noisy ReLUs and binary neurons is exploited during training. We summarize our experimental results in Section 5 followed by concluding remarks in Section 6."}, {"heading": "2 Sparse Binary convolutional networks", "text": "Binary convolutional networks [5, 3] are a particularly elegant instance of low-precision computing targeted toward object recognition tasks. Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3]. Such networks use t\u00b41, 1u-valued (or, t0, 1u) neurons throughout the network and thus best suited to be deployed on neuromorphic hardware using spiking neurons. Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].\nA TrueNorth chip consists of a network of neurosynaptic cores with programmable connectivity, synapses, and neuron parameters [9]. It is a multicore array where each core consists of 256 input lines, 256 neurons, and a 256\u02c6 256 synaptic crossbar array. Each input line in a core can connect to one neuron on any core through a spike router, and that input line is accessible to all of the 256 neurons on that core through the crossbar, thereby resulting in block-wise connectivity. All communication to-, from-, and within- chip is performed using spikes. TrueNorth neurons use a variant of an integrateand-fire model with 23 configurable parameters [33] where a neuron\u2019s state variable updates each tick (typically at 1000 ticks per second, though higher rates are possible). Synapses have individually configurable on/off states and have a strength assigned by look-up table. Specifically, each neuron has a 4-entry table of 8-bit signed-integers, each input line to a core is assigned an input-type of 1, 2, 3 or 4, and each synapse then determines its strength by using the input-type on its source side to index into the table of the neuron on its target side. In summary, for non-negative integers L,N \u010f 256, the crossbar weight matrix is factored into three components: a) a L \u02c6 N binary (t0, 1u-valued) connectivity matrix C, (b) an input-type vector g of length L over the integers t1, 2, 3, 4u, and (c) a set of strength-value functions sj : t1, 2, 3, 4u \u00d1 t\u00b4255, . . . , 255u, j \u201c 1, 2, . . . , N \u2013 one for each of the N neurons on a core. If we extend functions sj to operate on vectors element-wise, then the L\u02c6N weight matrix M of a core can be written as\nMpg, C, tsiuNi\u201c1q \u201c rs1pgq s2pgq \u00a8 \u00a8 \u00a8 sN pgqs \u02dd C (1) where \u02dd denotes the Hadamard product1 between matrices. To account for the fact that a TrueNorth neuron can connect to at most 256 input features, as in [3], network structure in this paper constrained by partitioning features in each layer into one or more\n1Which is simply the element-wise product between matrices\nequally sized groups. Neurons in a group connect to all the input features in that group, and output features from different groups are generated from disjoint set of input features2. The number of groups in a layer is chosen such that the total filter size (rows \u02c6 columns \u02c6 features) as well as the number of output features of each group is less than or equal to 256. To further promote sparsity in connections, we allow the convolutional filters to be t\u00b41, 0, 1u-valued. The number of neurons that spike in response to an input image influence the energy utilization as well as the number of classification per unit time that the underlying hardware can support. With this in mind, we strive to train networks with sparse binary activations. All the networks studied in this work use t0, 1u-valued neurons where average fraction of 1\u2019s is less than 20%. All of the above considerations result in networks that are sparse in connectivity, weights, and activations."}, {"heading": "3 Symmetric kernels", "text": "When discrete convolution is written as a matrix multiplication, the resulting \u2018convolution matrix\u2019 has a block Toeplitz structure3 [34]. By establishing the connection between the weight representation in (1) and Toeplitz matrices, we identify a set of convolution kernels (symmetric kernels 4) that efficiently map to the TrueNorth architecture. We need a few more notations before giving a formal statement of this connection.\nFor an arbitrary matrix H , we denote its i-th column by Hi and its pi, jq-th element by Hi,j . If h is an arbitrary scalar-valued function on real numbers, we extend it to operate on matrices by applying it element-wise. For two functions h1 and h2, h1 \u00a8 h2 denotes function composition. Let X \u02c7 K denote the 2-D convolution5 between an n\u02c6 n data matrix X and an l \u02c6 l convolution kernel matrix K. For any matrix Y , let vecpY q denote the vectorization (i.e., concatenation of columns) of Y . Fact 1 (See for example, [34]): If W denotes the convolution matrix such that\nvecpX \u02c7Kqt \u201c vecpXqtW (2) then W is a block Toeplitz matrix of the form\nW \u201c\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2013\nW1 0 \u00a8 \u00a8 \u00a8 0 0 W2 W1 \u00a8 \u00a8 \u00a8 0 0\n... ... \u00a8 \u00a8 \u00a8\n... ...\nWl Wl\u00b41 \u00a8 \u00a8 \u00a8 0 0 0 Wl \u00a8 \u00a8 \u00a8 0 0 ... ... \u00a8 \u00a8 \u00a8 ...\n... 0 0 \u00a8 \u00a8 \u00a8 W1 0 0 0 \u00a8 \u00a8 \u00a8 W2 W1 ... ... \u00a8 \u00a8 \u00a8 ...\n... 0 0 \u00a8 \u00a8 \u00a8 Wl Wl\u00b41 0 0 \u00a8 \u00a8 \u00a8 0 Wl\nfi\nffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi fl , where submatrix Wi \u201c\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2014 \u2013\nK1,i 0 \u00a8 \u00a8 \u00a8 0 0 K2,i K1,i \u00a8 \u00a8 \u00a8 0 0\n... ... \u00a8 \u00a8 \u00a8\n... ...\nKl,i Kl\u00b41,i \u00a8 \u00a8 \u00a8 0 0 0 Kl,i \u00a8 \u00a8 \u00a8 0 0 ... ... \u00a8 \u00a8 \u00a8 ...\n... 0 0 \u00a8 \u00a8 \u00a8 K1,i 0 0 0 \u00a8 \u00a8 \u00a8 K2,i K1,i ... ... \u00a8 \u00a8 \u00a8 ...\n... 0 0 \u00a8 \u00a8 \u00a8 Kl,i Kl\u00b41,i 0 0 \u00a8 \u00a8 \u00a8 0 Kl,i\nfi\nffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi fl\n(3)\n0 denotes the zero matrices of appropriate dimension, and for i \u201c 1, 2, . . . , l, Wi is an n\u02c6pn\u00b4 l`1q Toeplitz matrix constructed from the i-th column of the convolution kernelK. Such an n2\u02c6npn\u00b4 l` 1q matrix W will be referred to as the block Toeplitz convolution matrix for K, and we write W pKq to make its dependence on K explicit. The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].\nSuppose that the dimension of the data matrix X is such that n2 \u010f 256. Ideally, for every choice of kernel K the corresponding convolution matrix W pKq can be represented in the form of (1) for some choice of type vector g, a set of strength functions, and a binary connectivity matrix C. However, the representation in (1) requires the input-type vector g to be common to all the columns of the weight\n2This is in contrast to typical convolutional network layer whose output features depend on the entire set of input features.\n3We allow the Toeplitz matrices to be non-square. 4The name symmetric kernels is motivated by the fact that these kernels are generated using commutative members of the Symmetric group S4\u2013the group of all permutations on a set of four elements. 5For notational simplicity, we ignore the boundary effects and assume n \u011b 2l and convolution stride of 1 throughout this section. The results presented here can be generalized to other cases.\nmatrix. Hence, it is conceivable that not every block Toeplitz convolution matrix W pKq can be represented in that form. Figure 1 provides a visual summary of the main result of this section: There\nis an one-to-one relationship between the set of convolution kernels and the set of block Toeplitz matrices of the form in (3). Theorem 3.1 asserts that only the set of block Toeplitz matrices associated with symmetric kernels characterized by the theorem can be represented by (1). The characterization also suggests a constructive procedure to generate all symmetric kernels. The proof of the theorem is outlined in the appendix. Example 3, also included in the appendix, explains the problem of relating the weight representation in (1) and convolution matrices in more detail.\nTheorem 3.1. Let n and l be non-negative integers such that L \u201c n2 \u010f 256, N \u201c n \u00b4 l ` 1 \u010f 256 and let K be an l \u02c6 l kernel matrix. There exist g, C, and s1, . . . , sN as in (1) such that Mpg, C, tsiuNi\u201c1q \u201cW pKq if K satisfies\nKi,j \u201c Bi,jfp\u03c3i\u00b411 p\u03c3 j\u00b41 2 p\u03c1qqq (4)\nfor some choice of\n1. commutative permutations \u03c31, and \u03c32 on t1, 2, 3, 4u\n2. seed element \u03c1 P t1, 2, 3, 4u\n3. function f : t1, 2, 3, 4u \u00d1 t\u00b4255, . . . , 255u\n4. l \u02c6 l binary matrix B."}, {"heading": "If K contains at least four distinct entries and no zeros, then the conditions on K are also necessary.", "text": "We refer to the convolution kernels identified in Theorem 3.1 as symmetric kernels and use the notation sympf, \u03c1, \u03c31, \u03c32, Bq to refer to a particular instance. The following two examples use the ordered-pair notation to define the functions \u03c31,\u03c32, and f and show how some of the well-known convolutional operators are in the family of symmetric kernels identified by Theorem 3.1.\nExample 1. If \u03c1 \u201c 1, \u03c31 : tp1 \u00de\u00d1 2q, p2 \u00de\u00d1 1q, p4 \u00de\u00d1 3q, p3 \u00de\u00d1 4qu, \u03c32 : tp1 \u00de\u00d1 2q, p2 \u00de\u00d1 1q, p4 \u00de\u00d1 3q, p3 \u00de\u00d1 4qu, f : tp1 \u00de\u00d1 4q, p2 \u00de\u00d1 \u00b41q, p3 \u00de\u00d1 4q, p4 \u00de\u00d1 4qu, and\nB \u201c \u00ab 0 1 0 1 1 1 0 1 0 ff , then K \u201c\n\u00bb\n\u2013 0 fp\u03c32p\u03c1qq 0 fp\u03c31p\u03c1qq fp\u03c31p\u03c32p\u03c1qqq fp\u03c31p\u03c322p\u03c1qqq\n0 fp\u03c321p\u03c32p\u03c1qqq 0\nfi fl \u201c \u00ab 0 \u00b41 0 \u00b41 4 \u00b41 0 \u00b41 0 ff\nwhich is a widely used approximation to the spatial Laplacian operator.\nExample 2. If \u03c1 \u201c 1, \u03c31 : tp1 \u00de\u00d1 1q, p2 \u00de\u00d1 2q, p3 \u00de\u00d1 3q, p4 \u00de\u00d1 4qu, \u03c32 : tp1 \u00de\u00d1 2q, p2 \u00de\u00d1 3q, p3 \u00de\u00d1 4q, p4 \u00de\u00d1 1qu, f : tp1 \u00de\u00d1 \u00b41q, p2 \u00de\u00d1 \u00b41q, p3 \u00de\u00d1 1q, p4 \u00de\u00d1 1qu, and\nB \u201c \u00ab 1 0 1 1 0 1 1 0 1 ff , then K \u201c \u00ab\u00b41 0 1 \u00b41 0 1 \u00b41 0 1 ff .\nK is the vertical Prewitt operator and the corresponding horizontal operator also may be constructed similarly.\nNotice that the set of symmetric kernels include instances from the set of separable (e.g. Prewitt), as well as nonseparable (e.g. Laplacian) matrices. A simple search reveals that there are 120 unique pairs \u03c31 and \u03c32 of commutative permutations on the set of four elements that can be used in Equation 4. Moreover, since we only consider t\u00b41, 0, 1u-valued convolution kernels in the work, the function f in (4) is restricted to be t\u00b41, 1u-valued, thus there are 16 possible choices for f . Since there are 4 possible choices for \u03c1 and the components ofB are independent, there are a total of 2l\n2\u02c616\u02c6120\u02c64 possible symmetric kernels of dimension l \u02c6 l. We have limited the discussion thus far in this section to 2-D convolutional matrices for ease of presentation. However, recall that deep convolutional networks use 3-D matrices as convolutional kernels and we will now extend our definition of symmetric kernels to cover this important case. An l \u02c6 l \u02c6m symmetric kernel is defined by\n1. a pair of commutative permutations \u03c31, and \u03c32 2. an m-length seed vector \u03c1 P t1, 2, 3, 4um\n3. a function f whose domain is t1, 2, 3, 4u 4. a binary connectivity matrix B\nas\nKi,j,k \u201c Bi,j,kfp\u03c3i\u00b411 \u03c3 j\u00b41 2 \u03c1kq. (5)\nA straight-forward counting argument as before reveals that there are 2ml 2\u02c616\u02c6120\u02c64m symmetric kernels of dimension l\u02c6l\u02c6m. Suppose that l \u201c 3, andm \u201c 8, there are about 1030 t\u00b41, 0, 1u-valued kernels to choose from!"}, {"heading": "4 Training", "text": ""}, {"heading": "4.1 Weights", "text": "In a typical deep learning setting, the goal is to learn a set of kernels for every convolutional layer in order to minimize some loss function using a gradient-based optimization method. In our case, we want the kernels to be from the class of symmetric kernels that efficiently map to TrueNorth architecture. Our strategy is as follows: At first, we allow every kernel K in the network to be learned without any constraints. Next, because our goal is to generate a trained network that maps to TrueNorth, once the performance on the validation data saturates, the unconstrained real-valued kernel K is replaced by a suitable member of the symmetric kernels as described below. If K P Rl\u02c6l\u02c6m denotes the unconstrained convolution kernel, then let\npf\u02da, \u03c1\u02da, \u03c3\u02da1 , \u03c3\u02da2 , B\u02daq \u201c arg min f,\u03c1,\u03c31,\u03c32,BPr0,1sl\u02c6l\u02c6m }K \u00b4 sympf, \u03c1, \u03c31, \u03c32, Bq}2 . (6)\nSubsequently, the kernel K is replaced by sympf\u02da, \u03c1\u02da, \u03c3\u02da1 , \u03c3\u02da2 , B\u02daq in the network. This is done for all the kernels in a layer simultaneously, and the parameters f\u02da, \u03c1\u02da, \u03c3\u02da1 , \u03c3 \u02da 2 are frozen for the remainder of the training. Notice however that the elements of B\u02da are real over r0, 1s at this stage of the training. Backpropagation is used to further train and binarize B\u02da in the ensuing period. We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6). When all the weights of a layer are converted to symmetric kernels , the network is trained for a at least one epoch before the next layer weights are replaced.\nx \u2295\nn\n\u03c6\n\u03c6(x+ n)\nT\nT\n(a)"}, {"heading": "4.2 Neurons", "text": "Rectified linear unites (ReLUs) are the typical form of non-linearity used in deep networks. Our approach seeks to use a binary activations obtained by threshold functions because they map to neuromorphic harware with spike-based communication infrastruture efficiently. Our strategy is to begin the training with ReLU activations and to make them increasingly noisy so that they can be replaced with binary activations in the end. Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44]. An early attempt at using noisy ReLU with the objective of obtaining binary activations was reported in [45] for single-layer networks.\nIt is well-known in information theory [46, 47] that the best way to communicate using an amplitudebounded channel in the presence of additive noise of the form shown in Figure 2 is to use a discrete set of input values for x, and that for an appropriate choice of noise variance, binary signaling is optimal. Moreover, unlike in information theory where the noise distribution and variance are presupposed, we have the freedom to choose and vary them throughout the training period. We seek to utilize this connection to train a binary convolutional network using noisy ReLUs.\nDuring training, we use the bounded noisy ReLU shown in Figure 2 (a) with a zero mean uniform distributed random variable distributed in the range r\u00b4 , s as the noise source. The range of the random variable is slowly increased from 0 to T {2. Towards the end of the training period, noisy ReLUs are replaced by the threshold neuron shown in Figure 2 (b) one layer at a time for fine tuning the network. We use the gradient of the ReLU saturating at T during the backward step throughout the training."}, {"heading": "4.3 Systems and methods", "text": "The networks are trained on GPUs using MatConvNet deep learning libraries [48]. The stochastic gradient descent was used with dropout [49] layers, momentum (0.9), weight decay (10\u00b46), and batch normalization [50]. The parameters learned through training are mapped to hardware using reusable, composable network description functions called corelets [51]. The corelets created for this work automatically compile the learned network parameters, which are independent of any neuromorphic platform, into an platform-specific hardware configuration file that can directly program TrueNorth chips."}, {"heading": "5 Results", "text": "The proposed method was evaluated using the image recognition benchmark dataset CIFAR-10 (Figure 3). The CIFAR-10 dataset [52] consists of color natural images, 32 x 32 pixels in size, in 10 classes, with 50000 training images and 10000 test images."}, {"heading": "5.1 Networks", "text": "We evaluated the representation power of symmetric kernels by designing two convolution networks using one and four TrueNorth chips and trained for CIFAR10 dataset. Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations. So our networks are built by stacking four sets of convolution layers and each set contains four convolution layers. The smaller network is described in Table 1 while the large network we used was obtained simply by increasing the number of features in each layer. The final set of output features are divided uniformly among the classes and the evaluation is performed at 1 classification per hardware tick."}, {"heading": "5.2 Performance", "text": "To characterize performance, the trained 1-Chip convolution networks were deployed on a IBM TrueNorth NS1e board packed with 4096 neuromorphic cores and run to measure the classification accuracy and throughput. The 4-Chip networks were run on simulation [53], and classification accuracies were measured for the dataset. The results are shown in Table 2 in comparison with Energy-efficient deep networks (EEDN) on TrueNorth [3].\nBy using symmetric kernels, we have been able to deploy a network with twice as many features on the same number of cores as was possible using the approach described in [3]. Thus, we have been able to obtain better accuracy for the same amount of hardware even though the set of allowed convolutional kernels are a strict subset of the set of t\u00b41, 0, 1u-valued kernels allowed in [3]. With multi-chip networks, we achieved near state-of-the-art accuracy with significantly less number of TrueNorth cores, improving the energy efficiency by more than two-fold."}, {"heading": "6 Conclusions", "text": "Our study shows that convolutional networks built using only structured kernels are surprisingly rich and maintain their representational power, and that backpropagation can be adapted to learn in the presence of such constraints. Furthermore, our investigation suggests that co-designing algorithms and architecture may offer a successful strategy toward even more energy efficient deployment platform for deep learning."}, {"heading": "7 Appendix", "text": "Remark 1. Suppose that a TrueNorth core receives a vectorized 16\u02c616 matrixX along its input-lines, and computes convolution with the Laplacian operator K in Example 1. Hence, n \u201c 16, l \u201c 3 and it is easily verified that W pKq is a 256 \u02c6 pn \u00b4 l ` 1q2 (i.e., 256 \u02c6 196) matrix. Since K satisfies the conditions of Theorem 3.1, we now use f , \u03c1, \u03c31 and \u03c32 from the example to construct g, C and s1, . . . , s196 such that Mpg, C, tsiu196i\u201c1q \u201c W pKq. Define the 16 \u02c6 16 matrix G by Gi,j \u201c \u03c3i\u00b411 p\u03c3 j\u00b41 2 p\u03c1qq, thus6\nG \u201c\n\u00bb\n\u2014\n\u2014\n\u2013\n\u03c1 \u03c32p\u03c1q \u00a8 \u00a8 \u00a8 \u03c3152 p\u03c1q \u03c31p\u03c1q \u03c31p\u03c32p\u03c1qq \u00a8 \u00a8 \u00a8 \u03c31p\u03c3152 p\u03c1qq\n... ...\n... ...\n\u03c3151 p\u03c1q \u03c3151 p\u03c32p\u03c1qq \u00a8 \u00a8 \u00a8 \u03c3151 p\u03c3152 p\u03c1qq\nfi\nffi\nffi\nfl\n\u201c\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2014\n\u2013 1 2 1 \u00a8 \u00a8 \u00a8 2 2 1 2 \u00a8 \u00a8 \u00a8 1 1 2 1 \u00a8 \u00a8 \u00a8 2 ... ... ...\n... 2 1 2 \u00a8 \u00a8 \u00a8 1\nfi\nffi\nffi\nffi\nffi\nfl\n(7)\nWe use vecpGq as the type-vector g in (1). Consequently, input element Xi,j is assigned the type Gi,j . Next we construct the 256\u02c6 196 binary matrix C one column at a time and define the strength functions. Let k, l P t1, 2, . . . , 14u, let r \u201c pl\u00b4 1q \u00a8 14` k, and define a 16\u02c6 16 intermediate matrix H by\nHi,j \u201c \" Bi\u00b4k`1,j\u00b4l`1 if k \u010f i \u010f pk ` 2q and l \u010f j \u010f pl ` 2q 0 otherwise.\n(8)\nNow define Cr (the r-th column of C) to be vecpHq, and the r-th strength function to be\nsr \u201c f \u00a8 p\u03c3k\u00b411 q\u00b41 \u00a8 p\u03c3l\u00b412 q\u00b41. (9)\nBy iterating over all pairs k and l, the matrix C and all the strength functions are completely defined. Let k, l P t1, 2, . . . , 14u, and r \u201c pl \u00b4 1q \u00a8 14` k. For g and C and s1, . . . , s196 defined above, we have `\nvecpXqt Mpg, C, tsiu196i\u201c1q \u02d8\nr\n\u201c vecpXqt Mpg, C, tsiu196i\u201c1qr paq\u201c vecpXqt psrpgq \u02dd Crq\npbq\u201c r1 1 1s \u02dc\u00ab Xk,l Xk,l`1 Xk,l`2 Xk`1,l Xk`1,l`1 Xk`1,l`2 Xk`2,l Xk`2,l`1 Xk`2,l`2 ff \u02dd \u00ab 0 srpGk,l`1q 0 srpGk`1,lq srpGk`1,l`1q srpGk`1,l`2q 0 srpGk`2,l`1q 0 ff\u00b8\u00ab 1 1 1 ff\n(10)\nwhere paq follows since Mpg, C, tsiu196i\u201c1qr denotes the r-th column of Mpg, C, tsiu196i\u201c1q and its r-th column is simply the element-wise product between srpgq and Cr; pbq follows from the fact that Cr (defined through H in (8)) masks all components of X that do not contribute to the pk, lq-th convolution result and the all-one row and column vectors simply sum all elements of the 3 \u02c6 3 matrix between them.\n6Reader may notice that G defined here is Toeplitz. This is merely a consequence of the fact that \u03c31 \u201c \u03c32 in this example. In general G need not be Toeplitz.\nNow using (9) and Gi,j \u201c \u03c3i\u00b411 p\u03c3 j\u00b41 2 p\u03c1qq, we get\n\u00ab 0 srpGk,l`1q 0 srpGk`1,lq srpGk`1,l`1q srpGk`1,l`2q\n0 srpGk`2,l`1q 0\nff\n\u201c\n\u00bb\n\u2013 0 fp\u03c32p\u03c1qq 0 fp\u03c31p\u03c1qq fp\u03c31p\u03c32p\u03c1qqq fp\u03c31p\u03c322p\u03c1qqq fp\u03c321p\u03c1qq fp\u03c321p\u03c32p\u03c1qqq fp\u03c321p\u03c322p\u03c1qqq\nfi\nfl\npaq\u201c \u00ab 0 fp2q 0 fp2q fp1q fp2q 0 fp2q 0 ff\npbq\u201c \u00ab 0 \u00b41 0 \u00b41 4 \u00b41 0 \u00b41 0 ff\n\u201c K (11)\nwhere paq follows by using \u03c1 \u201c 1 and applying \u03c31 and \u03c32 from the example, pbq is obtained by applying f defined in the example. Substituting (11) in (10), we get\n` vecpXqt Mpg, C, tsiu196i\u201c1q \u02d8 r \u201c r1 1 1s\n\u02dc\u00ab\nXk,l Xk,l`1 Xk,l`2 Xk`1,l Xk`1,l`1 Xk`1,l`2 Xk`2,l Xk`2,l`1 Xk`2,l`2\nff \u02ddK \u00b8\u00ab 1 1 1 ff\npaq\u201c pX \u02c7Kqk,l pbq\u201c ` vecpXqt W pKq \u02d8\nr (12)\nvecpXqt Mpg, C, tsiu196i\u201c1q pcq\u201c vecpXqt W pKq (13)\nMpg, C, tsiu196i\u201c1q pdq\u201c W pKq\nwhere paq follows from the definition of convolution, pbq follows since r \u201c pl\u00b4 1q \u00a8 14` k, and r-th element of the row-vector vecpXqt W pKq corresponds to pk, lq-th convolution result, pcq follows since (12) holds for each r P t1, 2, . . . , 196u, pdq follows since X is arbitrary in (13). The last equality completes the proof of first part of the theorem applied to Example 1.\nThe following example shows the construction of the input-type vector g, the binary connectivity matrix C, and and strength functions in (1) in much more detail for concreteness.\nExample 3. Consider convolving a 4\u02c6 4 matrix X with a 3\u02c6 3 kernels K, and denote the resulting 2\u02c6 2 matrix by Y . The kernel is placed on top of the corresponding elements of matrix X needed to compute Y below:\n0\nB B B B B B B @\nX1;1 X1;2 X1;3 X1;4\nX2;1 X2;2 X2;3 X2;4\nX3;1 X3;2 X3;3 X3;4\nX4;1 X4;2 X4;3 X4;4\n1\nC C C C C C C A\n0\nB B B B B B B @\nX1;1 X1;2 X1;3 X1;4\nX2;1 X2;2 X2;3 X2;4\nX3;1 X3;2 X3;3 X3;4\nX4;1 X4;2 X4;3 X4;4\n1\nC C C C C C C A\n0\nB B B B B B B @\nX1;1 X1;2 X1;3 X1;4\nX2;1 X2;2 X2;3 X2;4\nX3;1 X3;2 X3;3 X3;4\nX4;1 X4;2 X4;3 X4;4\n1\nC C C C C C C A\n0\nB B B B B B B @\nX1;1 X1;2 X1;3 X1;4\nX2;1 X2;2 X2;3 X2;4\nX3;1 X3;2 X3;3 X3;4\nX4;1 X4;2 X4;3 X4;4\n1\nC C C C C C C A\nK1;1 K1;2 K1;3\nK3;1 K3;2 K3;3\nK2;1 K2;2 K2;3\nK1;1 K1;2 K1;3\nK3;1 K3;2 K3;3\nK2;1 K2;2 K2;3\nK1;1 K1;2 K1;3\nK3;1 K3;2 K3;3\nK2;1 K2;2 K2;3\nY1;1 : Y1;2 :\nY2;1 : Y2;2 :\nK1;1 K1;2 K1;3\nK3;1 K3;2 K3;3\nK2;1 K2;2 K2;3\nThis can be expressed as matrix multiplication in the following form:\n(\nY1;1 Y2;1 Y1;2 Y2;2\n)\n=\n0\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\n@\nK1;1 0 0 0\nK2;1 K1;1 0 0\nK3;1 K2;1 0 0\n0 K3;1 0 0\nK1;2 0 K1;1 0\nK2;2 K1;2 K2;1 K1;1\nK3;2 K2;2 K3;1 K2;1\n0 K3;2 0 K3;1\nK1;3 0 K1;2 0\nK2;3 K1;3 K2;2 K1;2\nK3;3 K2;3 K3;2 K2;2\n0 K3;3 0 K3;2\n0 0 K1;3 0\n0 0 K2;3 K1;3\n0 0 K3;3 K2;3\n0 0 0 K3;3\n1\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nA\nK1;1 K1;2 K1;3 0\nK2;1 K2;2 K2;3 0\nK3;1 K3;2 K3;3 0\n0 0 0 0\n0 K1;1 K1;2 K1;3\n0 K2;1 K2;2 K2;3\n0 K3;1 K3;2 K3;3\n0 0 0 0\n0 K1;1 K1;2 K1;3\n0 K2;1 K2;2 K2;3\n0 K3;1 K3;2 K3;3\n0 0 0 0\nK1;1 K1;2 K1;3 0\nK2;1 K2;2 K2;3 0\nK3;1 K3;2 K3;3 0\n0 0 0 0\n0\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\nB\n@\nX1;1\nX2;1\nX3;1\nX4;1\nX1;2\nX2;2\nX3;2\nX4;2\nX1;3\nX2;3\nX3;3\nX4;3\nX1;4\nX2;4\nX3;4\nX4;4\n1\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nC\nA\nt\n\u201c\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd X1,1 X2,1 X3,1 X4,1 X1,2 X2,2 X3,2 X4,2 X1,3 X2,3 X3,3 X4,3 X1,4 X2,4 X3,4 X4,4\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\nt \u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\nK1,1 0 0 0 K2,1 K1,1 0 0 K3,1 K2,1 0 0 0 K3,1 0 0 K1,2 0 K1,1 0 K2,2 K1,2 K2,1 K1,1 K3,2 K2,2 K3,1 K2,1 0 K3,2 0 K3,1 K1,3 0 K1,2 0 K2,3 K1,3 K2,2 K1,2 K3,3 K2,3 K3,2 K2,2 0 K3,3 0 K3,2 0 0 K1,3 0 0 0 K2,3 K1,3 0 0 K3,3 K2,3 0 0 0 K3,3\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n\u02dd\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2013 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1\nfi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nfl\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n(14)\nNow consider performing the above matrix multiplication using a TrueNorth core. Naturally, the input vector vecpXq is routed to 16 of the input lines of a TrueNorth core in the order shown in (14), and the two matrices on either side of the Hadamard product in (14) are implemented using the crossbar by selecting 1q an appropriate binary connectivity matrix C, 2q a set of four strength functions s1, s2, s3, and s4, and 3q a 4\u02c6 4 matrix G such that Gi,j defines the input-type for Xi,j so\nthat by defining g \u201c vecpGq we can get\nrs1pgq s2pgq s3pgq s4pgqs \u02dd C \u201c\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\nK1,1 0 0 0 K2,1 K1,1 0 0 K3,1 K2,1 0 0 0 K3,1 0 0 K1,2 0 K1,1 0 K2,2 K1,2 K2,1 K1,1 K3,2 K2,2 K3,1 K2,1 0 K3,2 0 K3,1 K1,3 0 K1,2 0 K2,3 K1,3 K2,2 K1,2 K3,3 K2,3 K3,2 K2,2 0 K3,3 0 K3,2 0 0 K1,3 0 0 0 K2,3 K1,3 0 0 K3,3 K2,3 0 0 0 K3,3\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n\u02dd\n\u00bb\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2013 1 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1\nfi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nffi\nfl\n(15)\nwhich is sufficient to be able to compute the aforementioned convolution.\nThe obvious choice for C is the binary matrix on the RHS of (15), and the remainder of the discussion focuses on the choice of the type vector and strength functions. Since the strength function is common to all the rows of a particular column, two rows with distinct non-zero entries in a column must be assigned distinct types. To illustrate this further, let us consider the following kernel\nK \u201c \u02dc\u00b41 2 \u00b41 \u00b42 4 \u00b42 \u00b41 2 \u00b41 \u00b8 .\nNow let us rewrite the matrix on the RHS of (15) for this example and color the rows of the resulting matrix such that any two rows of the first column with distinct non-zero elements have different colors:\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\nK1,1 0 0 0 K2,1 K1,1 0 0 K3,1 K2,1 0 0 0 K3,1 0 0 K1,2 0 K1,1 0 K2,2 K1,2 K2,1 K1,1 K3,2 K2,2 K3,1 K2,1 0 K3,2 0 K3,1 K1,3 0 K1,2 0 K2,3 K1,3 K2,2 K1,2 K3,3 K2,3 K3,2 K2,2 0 K3,3 0 K3,2 0 0 K1,3 0 0 0 K2,3 K1,3 0 0 K3,3 K2,3 0 0 0 K3,3\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n\u201c\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd \u00b41 0 0 0 \u00b42 \u00b41 0 0 \u00b41 \u00b42 0 0 0 \u00b41 0 0 2 0 \u00b41 0 4 2 \u00b42 \u00b41 2 4 \u00b41 \u00b42 0 2 0 \u00b41 \u00b41 0 2 0 \u00b42 \u00b41 4 2 \u00b41 \u00b42 2 4 0 \u00b41 0 2 0 0 \u00b41 0 0 0 \u00b42 \u00b41 0 0 \u00b41 \u00b42 0 0 0 \u00b41\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\nThe rows where the first column is 0 are free to choose a color based on subsequent columns so they remain uncolored for now. However, even though the colors so far have been chosen considering only the first column, it is important that no two rows of a subsequent column get the same color but contain distinct non-zero entries. In that case, the coloring scheme is said to be in \u2018conflict\u2019, and a conflictfree coloring scheme may not exist. Notice that the current row coloring does not contain any such conflicts so we can proceed with the remaining columns one at a time (starting position of the rowcolor indicates which column was under consideration while a row was assigned a particular color):\n\u00a8 \u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd \u00b41 0 0 0 \u00b42 \u00b41 0 0 \u00b41 \u00b42 0 0 0 \u00b41 0 0 2 0 \u00b41 0 4 2 \u00b42 \u00b41 2 4 \u00b41 \u00b42 0 2 0 \u00b41 \u00b41 0 2 0 \u00b42 \u00b41 4 2 \u00b41 \u00b42 2 4 0 \u00b41 0 2 0 0 \u00b41 0 0 0 \u00b42 \u00b41 0 0 \u00b41 \u00b42 0 0 0 \u00b41\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n\u00f9\u00f1 g \u201c\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd 1 2 1 2 3 4 3 4 1 2 1 2 3 4 3 4\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a p1, 1q p2, 1q p3, 1q p4, 1q p1, 2q p2, 2q p3, 2q p4, 2q p1, 3q p2, 3q p3, 3q p4, 3q p1, 4q p2, 4q p3, 4q p4, 4q\n,\n/ / / / / / / / / / / / / / / / / / / / / / / .\n/ / / / / / / / / / / / / / / / / / / / / / / -\npi, jq location of the input incident \u00d0\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd\u00dd on the corresponding row as in (14)\n\u00a8\n\u02da\n\u02dd \u00b41 2 \u00b41 0 \u00b42 4 \u00b42 0 \u00b41 2 \u00b41 0 0 0 0 0 \u02db \u2039 \u201a \u00a8 \u02da \u02dd 0 0 0 0 \u00b41 2 \u00b41 0 \u00b42 4 \u00b42 0 \u00b41 2 \u00b41 0 \u02db \u2039 \u201a \u00a8 \u02da \u02dd 0 \u00b41 2 \u00b41 0 \u00b42 4 \u00b42 0 \u00b41 2 \u00b41 0 0 0 0 \u02db \u2039 \u201a \u00a8 \u02da \u02dd 0 0 0 0 0 \u00b41 2 \u00b41 0 \u00b42 4 \u00b42 0 \u00b41 2 \u00b41 \u02db \u2039 \u201a\ns1p1q : \u00b41 s1p2q : \u00b42 s1p3q : 2 s1p4q : 4\ns2p1q : \u00b42 s2p2q : \u00b41 s2p3q : 4 s2p4q : 2\ns3p1q : 2 s3p2q : 4 s3p3q : \u00b41 s3p4q : \u00b42\ns4p1q : 4 s4p2q : 2 s4p3q : \u00b42 s4p4q : \u00b41\nwhere the type-vector g simply indicates the color index of the corresponding row (green \u2013 1, red \u2013 2, blue \u2013 3, yellow \u2013 4). The above coloring scheme represents a valid coloring solution that is conflict-free. For i \u201c 1, 2, 3, 4, the strength function si is defined by letting sipcq to be the unique non-zero value associated with color index c in column i and we have a solution that satisfies (15).\nIn general, suppose that the kernel K satisfies Ki,j \u201c fp\u03c3i\u00b411 p\u03c3 j\u00b41 2 p\u03c1qqq for some function f , \u03c1 P t1, 2, 3, 4u, and two commuting permutations \u03c31 and \u03c32 on t1, 2, 3, 4u. We will now illustrate the idea behind why the above coloring procedure will never run into conflicts for such a K. For simplicity, assume that the function f is invertible. Since such a K has exactly four distinct entries, rows of the matrix whose first column contains a non-zero entry can be colored with four distinct colors without any conflict in that column. Assume that after such a coloring based on just the non-zero entries of the first column, two rows have been already assigned the same color as shown below:\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\nK1,1 0 0 0 K2,1 K1,1 0 0 K3,1 K2,1 0 0 0 K3,1 0 0 K1,2 0 K1,1 0 K2,2 K1,2 K2,1 K1,1 K3,2 K2,2 K3,1 K2,1 0 K3,2 0 K3,1 K1,3 0 K1,2 0 K2,3 K1,3 K2,2 K1,2 K3,3 K2,3 K3,2 K2,2 0 K3,3 0 K3,2 0 0 K1,3 0 0 0 K2,3 K1,3 0 0 K3,3 K2,3 0 0 0 K3,3\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\nThe two rows with the same color imply\nK2,2 \u201c K3,3 \u00f9\u00f1 fp\u03c31p\u03c32p\u03c1qqq \u201c fp\u03c321p\u03c322p\u03c1qqq \u00f9\u00f1 \u03c31p\u03c32p\u03c1qq \u201c \u03c321p\u03c322p\u03c1qq\n\u00f9\u00f1 \u03c1 \u201c \u03c31p\u03c32p\u03c1qq. Now let us use the last equality to show that this coloring choice does not lead to a conflict in any of the three remaining columns:\nK2,3 \u201c \u03c31p\u03c322p\u03c1qq \u201c \u03c32p\u03c31p\u03c32p\u03c1qqq \u201c \u03c32p\u03c1q \u201c K1,2 K3,2 \u201c \u03c321p\u03c32p\u03c1qq \u201c \u03c31p\u03c31p\u03c32p\u03c1qqq \u201c \u03c31p\u03c1q \u201c K2,1 K2,2 \u201c \u03c31p\u03c32p\u03c1qq \u201c \u03c1 \u201c K1,1\nwhich is precisely what we needed. Similar analysis can be used to show that all the rows can be colored without conflicts for such a matrix K. In fact, by defining the 4 \u02c6 4 matrix G by Gi,j \u201c \u03c3i\u00b411 p\u03c3 j\u00b41 2 p\u03c1qq and letting g \u201c vecpGq, we can use g to obtain a conflict-free coloring solution!\nOutline of the proof of Theorem 3.1. Extending the arguments in Remark 1 in a straightforward manner proves the first part of the theorem. We will now outline the proof for the second half the theorem assuming n \u201c 16, l \u201c 3 and the proof for general case is a straightforward extension of arguments here. Assume that K contains at least four distinct non-zero entries as in the statement of the theorem. Suppose that there exist some g, C, and tsiuNi\u201c1 such that Mpg, C, tsiuNi\u201c1q \u201cW pKq, we will now construct f , \u03c1, \u03c31 , and \u03c32 such that\nKi,j \u201c fp\u03c3i\u00b411 p\u03c3 j\u00b41 2 p\u03c1qqq.\nLet k, l P t1, 2, . . . , 14u, and r \u201c pl \u00b4 1q \u00a8 14 ` k. Let X denote the 16 \u02c6 16 input matrix. As in Remark 1, the pk, lq-th convolution result is computed using the r-th column of Mpg, C, tsiuNi\u201c1q by\nvecpXqtMpg, C, tsiuNi\u201c1qr \u201c vecpXqt psrpgq \u02dd Crq . (16) Recall that the range of sr cannot contain zero (since its range must contain all unique entries of K \u2013 of which there are four non-zero elements), consequently srpgq cannot not contain any zero entries. The pk, lq-th convolution only depends on tXi,j : k \u010f i \u010f pk ` 2q, l \u010f j \u010f pl ` 2qu. Furthermore, since the theorem statement also requires that K does not contain zeros, the pk, lq-th convolution depends on all of tXi,j : k \u010f i \u010f pk ` 2q, l \u010f j \u010f pl ` 2qu non-trivially. Hence, if H denotes the 16\u02c6 16 matrix constructed such that Cr \u201c vecpHq), then H must satisfy\nHi,j \u201c \" 1 if k \u010f i \u010f pk ` 2q and l \u010f j \u010f pl ` 2q 0 otherwise\n(17)\nwhich is merely stating that the binary vector Cr masks out all the rows on which the pk, lq-th convolution does not depend on. Now let G denote the 16\u02c6 16 matrix obtained by reshaping g so that the input-type of Xi,j is Gi,j (i.e, g \u201c vecpGq). We have vecpXqt psrpgq \u02dd Crq\npaq\u201c r1 1 1s \u02dc\u00ab Xk,l Xk,l`1 Xk,l`2 Xk`1,l Xk`1,l`1 Xk`1,l`2 Xk`2,l Xk`2,l`1 Xk`2,l`2 ff \u02dd sr \u02dc\u00ab Gk,l Gk,l`1 Gk,l`2 Gk`1,l Gk`1,l`1 Gk`1,l`2 Gk`2,l Gk`2,l`1 Gk`2,l`2 ff\u00b8\u00b8\u00ab 1 1 1 ff\n(18)\npbq\u201c r1 1 1s \u02dc\u00ab Xk,l Xk,l`1 Xk,l`2 Xk`1,l Xk`1,l`1 Xk`1,l`2 Xk`2,l Xk`2,l`1 Xk`2,l`2 ff \u02ddK \u00b8\u00ab 1 1 1 ff\n(19)\nwhere paq follows from (17) and the fact that g \u201c vecpGq, pbq follows from the assumption that vecpXqt psrpgq \u02dd Crq is equal to the pk, lq-th convolution result. Since the input matrixX is arbitrary, by combining (18) and (19) we get\nK \u201c sr\n\u02dc\u00ab\nGk,l Gk,l`1 Gk,l`2 Gk`1,l Gk`1,l`1 Gk`1,l`2 Gk`2,l Gk`2,l`1 Gk`2,l`2\nff\u00b8\n. (20)\nFinally, we have\ns1\n\u02dc\u00ab\nG1,1 G1,2 G1,3 G2,1 G2,2 G2,3 G3,1 G3,2 G3,3\nff\u00b8\npaq\u201c s2\n\u02dc\u00ab\nG2,1 G2,2 G2,3 G3,1 G3,2 G3,3 G4,1 G4,2 G4,3\nff\u00b8\ns\u00b412 \u00a8 s1\n\u02dc\u00ab\nG1,1 G1,2 G1,3 G2,1 G2,2 G2,3 G3,1 G3,2 G3,3\nff\u00b8 pbq\u201c \u00ab G2,1 G2,2 G2,3 G3,1 G3,2 G3,3 G4,1 G4,2 G4,3 ff\n(21)\ns1\n\u02dc\u00ab\nG1,1 G1,2 G1,3 G2,1 G2,2 G2,3 G3,1 G3,2 G3,3\nff\u00b8\npcq\u201c s15\n\u02dc\u00ab\nG1,2 G1,3 G1,4 G2,2 G2,3 G2,4 G3,2 G3,3 G3,4\nff\u00b8\ns\u00b4115 \u00a8 s1\n\u02dc\u00ab\nG1,1 G1,2 G1,3 G2,1 G2,2 G2,3 G3,1 G3,2 G3,3\nff\u00b8 \u201c \u00ab G1,2 G1,3 G1,4 G2,2 G2,3 G2,4 G3,2 G3,3 G3,4 ff\n(22)\nwhere paq follows from the fact that (20) is valid for all r and by equating right-hand-side of (20) for pk, lq \u201c p1, 1q and pk, lq \u201c p2, 1q, pbq follows from observing that if K contains four distinct non-zero elements the domain and range of the strength functions contain exactly four elements, thus they become bijections and hence inverses exist, and pcq is similar to paq but using pk, lq \u201c p1, 1q and pk, lq \u201c p1, 2q.\nBy defining f \u201c s1, \u03c31 \u201c s\u00b412 \u00a8 s1, \u03c32 \u201c s\u00b4115 \u00a8 s1, and \u03c1 \u201c G1,1 we get the desired construction as follows\nK paq\u201c f\n\u02dc\u00ab\nG1,1 G1,2 G1,3 G2,1 G2,2 G2,3 G3,1 G3,2 G3,3\nff\u00b8\npbq\u201c f\n\u00a8\n\u02dd\n\u00bb\n\u2013 G1,1 \u03c32pG1,1q \u03c322pG1,1q \u03c31pG1,1q \u03c31p\u03c32pG1,1qq \u03c31p\u03c322pG1,1qq \u03c321pG1,1q \u03c321p\u03c32pG1,1qq \u03c321p\u03c322pG1,1qq\nfi\nfl\n\u02db\n\u201a\npcq\u201c f\n\u00a8\n\u02dd\n\u00bb\n\u2013 G1,1 \u03c32pG1,1q \u03c322pG1,1q \u03c31pG1,1q \u03c32p\u03c31pG1,1qq \u03c322p\u03c31pG1,1qq \u03c321pG1,1q \u03c32p\u03c321pG1,1qq \u03c322p\u03c321pG1,1qq\nfi\nfl\n\u02db\n\u201a\nwhere paq follows from using (20) with pk, lq \u201c p1, 1q and f \u201c s1, pbq follows from (21) and (22) (for example, (21) implies G2,2 \u201c \u03c31pG1,2q while (22) implies G1,2 \u201c \u03c32pG1,1q, so G2,2 \u201c \u03c31pG1,2q \u201c \u03c31p\u03c32pG1,1qq), and pcq follows by applying (21) and (22) but the order reversed from before\u2013for example, (22) implies G2,2 \u201c \u03c32pG2,1q and (21) implies G2,1 \u201c \u03c31pG1,1q, hence G2,2 \u201c \u03c32pG2,1q \u201c \u03c32p\u03c31pG1,1qq. The last two equalities show \u03c31 and \u03c32 commute on elements of K. Since K contains four distinct elements (which is exactly the number of elements in the dommain of \u03c31 and \u03c32), they must commute for the sake of uniqueness of K."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional networks for fast, energy-efficient neuromorphic computing", "author": ["S.K. Esser", "P.A. Merolla", "J.V. Arthur", "A.S. Cassidy", "R. Appuswamy", "A. Andreopoulos", "D.J. Berg", "J.L. McKinstry", "T. Melano", "D.R. Barch", "C. di Nolfo", "P. Datta", "A. Amir", "B. Taba", "M.D. Flickner", "D.S. Modha"], "venue": "CoRR, vol. abs/1603.08270, 2016. [Online]. Available: http://arxiv.org/abs/1603.08270", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Binarynet: Training deep neural networks with weights and activations constrained to +1 or -1", "author": ["M. Courbariaux", "Y. Bengio"], "venue": "CoRR, vol. abs/1602.02830, 2016. [Online]. Available: http://arxiv.org/abs/1602.02830", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Binarized neural networks", "author": ["I. Hubara", "D. Soudry", "R.E. Yaniv"], "venue": "CoRR, vol. abs/1602.02505, 2016. [Online]. Available: http://arxiv.org/abs/1602.02505 14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["M. Courbariaux", "Y. Bengio", "J.-P. David"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 3105\u2013 3113.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Compression of deep convolutional neural networks for fast and low power mobile applications", "author": ["Y. Kim", "E. Park", "S. Yoo", "T. Choi", "L. Yang", "D. Shin"], "venue": "CoRR, vol. abs/1511.06530, 2015. [Online]. Available: http://arxiv.org/abs/1511.06530", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "CoRR, vol. abs/1603.05279, 2016. [Online]. Available: http://arxiv.org/abs/1603.05279", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["P.A. Merolla", "J.V. Arthur", "R. Alvarez-Icaza", "A.S. Cassidy", "J. Sawada", "F. Akopyan", "B.L. Jackson", "N. Imam", "C. Guo", "Y. Nakamura", "B. Brezzo", "I. Vo", "S.K. Esser", "R. Appuswamy", "B. Taba", "A. Amir", "M.D. Flickner", "W.P. Risk", "R. Manohar", "D.S. Modha"], "venue": "Science, vol. 345, no. 6197, pp. 668\u2013673, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Structured transforms for small-footprint deep learning", "author": ["V. Sindhwani", "T.N. Sainath", "S. Kumar"], "venue": "Neural Information Processing Systems (NIPS), 2015. [Online]. Available: http://arxiv.org/pdf/1510.01722v1.pdf", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Acdc: A structured efficient linear layer", "author": ["M. Moczulski", "M. Denil", "J. Appleyard", "N. de Freitas"], "venue": "International Conference on Learning Representations (ICLR), 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Matrix backpropagation for deep networks with structured layers", "author": ["C. Ionescu", "O. Vantzos", "C. Sminchisescu"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 2965\u20132973.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep neural networks using a rankconstrained topology", "author": ["P. Nakkiran", "R. Alvarez", "R. Prabhavalkar", "C. Parada"], "venue": "Proceedings of Annual Conference of the International Speech Communication Association (Interspeech), 2015, pp. 1473\u20131477.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L.D. Bourdev"], "venue": "CoRR, vol. abs/1412.6115, 2014. [Online]. Available: http://arxiv.org/abs/1412.6115", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "CoRR, vol. abs/1510.00149, 2015. [Online]. Available: http://arxiv.org/abs/1510.00149", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "CoRR, vol. abs/1504.04788, 2015. [Online]. Available: http://arxiv.org/abs/1504.04788", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal brain damage", "author": ["Y.L. Cun", "J.S. Denker", "S.A. Solla"], "venue": "Advances in Neural Information Processing Systems. Morgan Kaufmann, 1990, pp. 598\u2013605.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Auto-encoder bottleneck features using deep belief networks", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran"], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012, 2012, pp. 4153\u20134156. [Online]. Available: http://dx.doi.org/10.1109/ICASSP.2012.6288833", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimizing bottle-neck features for lvcsr", "author": ["F. Gr\u00e9zl", "P. Fousek"], "venue": "Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 4729\u20134732.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Low-rank matrix factorization for deep neural network training with high-dimensional output targets", "author": ["T.N. Sainath", "B. Kingsbury", "V. Sindhwani", "E. Arisoy", "B. Ramabhadran"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6655\u20136659.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "CoRR, vol. abs/1602.01528, 2016. [Online]. Available: http://arxiv.org/abs/1602.01528", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Spinnaker: A 1-w 18-core system-on-chip for massively-parallel neural network simulation", "author": ["E. Painkras", "L.A. Plana", "J. Garside", "S. Temple", "F. Galluppi", "C. Patterson", "D.R. Lester", "A.D. Brown", "S.B. Furber"], "venue": "Solid-State Circuits, IEEE Journal of, vol. 48, no. 8, pp. 1943\u20131953, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1943}, {"title": "Six networks on a universal neuromorphic computing substrate", "author": ["T. Pfeil", "A. Gr\u00fcbl", "S. Jeltsch", "E. M\u00fcller", "P. M\u00fcller", "M.A. Petrovici", "M. Schmuker", "D. Br\u00fcderle", "J. Schemmel", "K. Meier"], "venue": "arXiv preprint arXiv:1210.7083, 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "A neuromorphic network for generic multivariate data classification", "author": ["M. Schmuker", "T. Pfeil", "M.P. Nawrot"], "venue": "Proceedings of the National Academy of Sciences, vol. 111, no. 6, pp. 2081\u20132086, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "An event-based neural network architecture with an asynchronous programmable synaptic memory", "author": ["S. Moradi", "G. Indiveri"], "venue": "Biomedical Circuits and Systems, IEEE Transactions on, vol. 8, no. 1, pp. 98\u2013107, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A 65k-neuron 73-mevents/s 22-pj/event asynchronous micro-pipelined integrate-and-fire array transceiver", "author": ["J. Park", "S. Ha", "T. Yu", "E. Neftci", "G. Cauwenberghs"], "venue": "Biomedical Circuits and Systems Conference (BioCAS), 2014 IEEE. IEEE, 2014, pp. 675\u2013678. 15", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "The design and implementation of fftw3", "author": ["M. Frigo", "S.G. Johnson"], "venue": "Proceedings of the IEEE, vol. 93, no. 2, pp. 216\u2013231, 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "On the computational power of circuits of spiking neurons", "author": ["W. Maass", "H. Markram"], "venue": "Journal of computer and system sciences, vol. 69, no. 4, pp. 593\u2013616, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Guest editors? introduction: The top 10 algorithms", "author": ["J. Dongarra", "F. Sullivan"], "venue": "Computing in Science & Engineering, vol. 2, no. 1, pp. 22\u201323, 2000.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Quicksort", "author": ["C.A. Hoare"], "venue": "The Computer Journal, vol. 5, no. 1, pp. 10\u201316, 1962.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1962}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMP- STAT\u20192010. Springer, 2010, pp. 177\u2013186.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Adjustable bounded rectifiers: Towards deep binary representations", "author": ["Z. Wu", "D. Lin", "X. Tang"], "venue": "http://arxiv.org/abs/1511.06201, vol. abs/1511.06201, 2015. [Online]. Available: http://arxiv.org/abs/1511. 06201", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores", "author": ["A.S. Cassidy", "P. Merolla", "J.V. Arthur", "S.K. Esser", "B. Jackson", "R. Alvarez-Icaza", "P. Datta", "J. Sawada", "T.M. Wong", "V. Feldman"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1\u201310.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Toeplitz and circulant matrices: A review", "author": ["R.M. Gray"], "venue": "now publishers inc,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Optimizing convolution operations on gpus using adaptive tiling", "author": ["B. Van Werkhoven", "J. Maassen", "H.E. Bal", "F.J. Seinstra"], "venue": "Future Generation Computer Systems, vol. 30, pp. 14\u201326, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "CoRR, vol. abs/1410.0759, 2014. [Online]. Available: http://arxiv.org/abs/1410.0759", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolution engine: balancing efficiency & flexibility in specialized computing", "author": ["W. Qadeer", "R. Hameed", "O. Shacham", "P. Venkatesan", "C. Kozyrakis", "M.A. Horowitz"], "venue": "ACM SIGARCH Computer Architecture News, vol. 41, no. 3. ACM, 2013, pp. 24\u201335.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society. Series B (methodological), pp. 1\u201338, 1977.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1977}, {"title": "On the convergence properties of the em algorithm", "author": ["C.J. Wu"], "venue": "The Annals of statistics, pp. 95\u2013103, 1983.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1983}, {"title": "A view of the em algorithm that justifies incremental, sparse, and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in graphical models. Springer, 1998, pp. 355\u2013368.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1998}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 229\u2013256, 1992.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1992}, {"title": "Gradient learning in spiking neural networks by dynamic perturbation of conductances", "author": ["I.R. Fiete", "H.S. Seung"], "venue": "Physical review letters, vol. 97, no. 4, p. 048104, 2006.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2006}, {"title": "Noisy activation functions", "author": ["\u00c7. G\u00fcl\u00e7ehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": "CoRR, vol. abs/1603.00391, 2016. [Online]. Available: http://arxiv.org/abs/1603.00391", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010, pp. 807\u2013814.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Backpropagation learning for systems with discrete-valued functions", "author": ["E. Wilson"], "venue": "Proceedings of the World Congress on Neural Networks, vol. 3, 1994, pp. 332\u2013339.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1994}, {"title": "The information capacity of amplitude and variance constrained scalar gaussian channel", "author": ["J.G. Smith"], "venue": "Information and Control, vol. 18, p. 203?219, 1971.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1971}, {"title": "The capacity-achieving input distribution for some ampli- tude-limited channels with additive noise", "author": ["W. Oettli"], "venue": "IEEE Trans. Inform. Theory, vol. IT?20, p. 372?374, May 1974.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1974}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference. ACM, 2015, pp. 689\u2013692.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of The 32nd International Conference on Machine Learning, 2015, pp. 448\u2013456.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Cognitive computing programming paradigm: a corelet language for composing networks of neurosynaptic cores", "author": ["A. Amir", "P. Datta", "W.P. Risk", "A.S. Cassidy", "J.A. Kusnitz", "S.K. Esser", "A. Andreopoulos", "T.M. Wong", "M. Flickner", "R. Alvarez-Icaza"], "venue": "Neural Networks (IJCNN), The 2013 International Joint Conference on. IEEE, 2013, pp. 1\u201310. 16", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Master\u2019s thesis, Department of Computer Science, University of Toronto, 2009.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2009}, {"title": "Compass: a scalable simulator for an architecture for cognitive computing", "author": ["R. Preissl", "T.M. Wong", "P. Datta", "M. Flickner", "R. Singh", "S.K. Esser", "W.P. Risk", "H.D. Simon", "D.S. Modha"], "venue": "SC Conference on High Performance Computing Networking, Storage and Analysis, 2012, p. 54. 17", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Deep convolutional networks have, in recent times, achieved near-human performance on an array of visual, auditory, and other cognitive tasks [1, 2].", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "1 Introduction Deep convolutional networks have, in recent times, achieved near-human performance on an array of visual, auditory, and other cognitive tasks [1, 2].", "startOffset": 157, "endOffset": 163}, {"referenceID": 2, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 3, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 4, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 5, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 6, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 7, "context": "The intriguing possibility of delivering deep learning applications on mobile devices, as well as providing energy-efficient cognitive solutions on the cloud have inspired an increasing number of researchers to search for low-precision state-of-the-art convolutional networks that can be deployed on extremely energy-efficient platforms [3, 4, 5, 6, 7, 8].", "startOffset": 337, "endOffset": 355}, {"referenceID": 8, "context": "Binary convolutional networks that use binary convolutional kernels, and binary neuron activations are ideally suited to be run on low-power neuromorphic architectures that use spike-based communication [9].", "startOffset": 203, "endOffset": 206}, {"referenceID": 9, "context": "Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters.", "startOffset": 138, "endOffset": 150}, {"referenceID": 10, "context": "Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters.", "startOffset": 138, "endOffset": 150}, {"referenceID": 11, "context": "Using structured matrices in the fully connected layers (also known as linear layers) of deep networks has been studied in the literature [10, 11, 12] with the objective of reducing the number of learned parameters.", "startOffset": 138, "endOffset": 150}, {"referenceID": 12, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 13, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 14, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 6, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 15, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 16, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 17, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 18, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 19, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 20, "context": "On the other hand, reducing the memory and computation requirements of convolutional layers has been addressed by several approaches to model compression [13, 14, 15, 7, 16, 17, 18, 19, 20, 21] \u2013 mostly after training.", "startOffset": 154, "endOffset": 193}, {"referenceID": 8, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 21, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 22, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 23, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 24, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 25, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 20, "context": "In this work, we propose the use of structured matrices in convolutional layers that are inspired by low-power neuromorphic hardware architectures [9, 22, 23, 24, 25, 26, 21].", "startOffset": 147, "endOffset": 174}, {"referenceID": 8, "context": "By connecting the efficient weight representation schema used in neuromorphic architectures [9, 21] with block Toeplitz matrices that arise in discrete convolution, we identify a family of convolution kernels that are ar X iv :1 60 6.", "startOffset": 92, "endOffset": 99}, {"referenceID": 20, "context": "By connecting the efficient weight representation schema used in neuromorphic architectures [9, 21] with block Toeplitz matrices that arise in discrete convolution, we identify a family of convolution kernels that are ar X iv :1 60 6.", "startOffset": 92, "endOffset": 99}, {"referenceID": 26, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 27, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 28, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 29, "context": "The primary motivation behind our investigation is in the tradition of discovering algorithms that are native to a chosen architecture [27, 28, 29, 30] and thereby harvesting the best that a particular architecture offers.", "startOffset": 135, "endOffset": 151}, {"referenceID": 30, "context": "We incorporate learning structured convolutional matrices into traditional stochastic gradient descent [31] so that the trained inference networks are hardware-ready.", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "We evaluate our system on Cifar10 data set and compare against best energy vs accuracy numbers reported on currently available hardware [3] \u2013 our approach reduces the number of TrueNorth cores required to achieve 87.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "5% on Cifar10 from 31872 TrueNoth cores in [3] to 13216 cores.", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "We begin the next section by discussing binary convolutional networks and their suitability for neuromorphic hardware and introduce the weight representation mechanism used in TrueNorth architecture [9].", "startOffset": 199, "endOffset": 202}, {"referenceID": 4, "context": "2 Sparse Binary convolutional networks Binary convolutional networks [5, 3] are a particularly elegant instance of low-precision computing targeted toward object recognition tasks.", "startOffset": 69, "endOffset": 75}, {"referenceID": 2, "context": "2 Sparse Binary convolutional networks Binary convolutional networks [5, 3] are a particularly elegant instance of low-precision computing targeted toward object recognition tasks.", "startOffset": 69, "endOffset": 75}, {"referenceID": 31, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 3, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 4, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 7, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 5, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 2, "context": "Motivated by the possibility of deploying state-of-theart image recognition convolutional networks on low-power mobile devices, there has been an increasing interest around binary networks [32, 4, 5, 8, 6, 3].", "startOffset": 189, "endOffset": 208}, {"referenceID": 8, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 22, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 23, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 24, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 25, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 20, "context": "Although we will restrict our focus to TrueNorth architecture in this study [9], most of our conclusions are also applicable to other platforms offering similar features [22, 23, 24, 25, 26, 21].", "startOffset": 170, "endOffset": 194}, {"referenceID": 8, "context": "A TrueNorth chip consists of a network of neurosynaptic cores with programmable connectivity, synapses, and neuron parameters [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 32, "context": "TrueNorth neurons use a variant of an integrateand-fire model with 23 configurable parameters [33] where a neuron\u2019s state variable updates each tick (typically at 1000 ticks per second, though higher rates are possible).", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "To account for the fact that a TrueNorth neuron can connect to at most 256 input features, as in [3], network structure in this paper constrained by partitioning features in each layer into one or more Which is simply the element-wise product between matrices 2", "startOffset": 97, "endOffset": 100}, {"referenceID": 33, "context": "3 Symmetric kernels When discrete convolution is written as a matrix multiplication, the resulting \u2018convolution matrix\u2019 has a block Toeplitz structure3 [34].", "startOffset": 152, "endOffset": 156}, {"referenceID": 33, "context": "Fact 1 (See for example, [34]): If W denotes the convolution matrix such that vecpX \u02c7Kq \u201c vecpXqW (2) then W is a block Toeplitz matrix of the form W \u201c \u00bb", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 35, "context": "The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 36, "context": "The sparsity and circulant nature of convolution matrices are fully exploited in most approaches to accelerate training and inference [35, 36, 37].", "startOffset": 134, "endOffset": 146}, {"referenceID": 37, "context": "We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6).", "startOffset": 46, "endOffset": 58}, {"referenceID": 38, "context": "We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6).", "startOffset": 46, "endOffset": 58}, {"referenceID": 39, "context": "We used an Expectation Maximization algorithm [38, 39, 40] to speed-up the search in (6).", "startOffset": 46, "endOffset": 58}, {"referenceID": 40, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 41, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 42, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 43, "context": "Using stochastic neuron activation functions has been explored in the literature for various purposes [41, 42, 43, 44].", "startOffset": 102, "endOffset": 118}, {"referenceID": 44, "context": "An early attempt at using noisy ReLU with the objective of obtaining binary activations was reported in [45] for single-layer networks.", "startOffset": 104, "endOffset": 108}, {"referenceID": 45, "context": "It is well-known in information theory [46, 47] that the best way to communicate using an amplitudebounded channel in the presence of additive noise of the form shown in Figure 2 is to use a discrete set of input values for x, and that for an appropriate choice of noise variance, binary signaling is optimal.", "startOffset": 39, "endOffset": 47}, {"referenceID": 46, "context": "It is well-known in information theory [46, 47] that the best way to communicate using an amplitudebounded channel in the presence of additive noise of the form shown in Figure 2 is to use a discrete set of input values for x, and that for an appropriate choice of noise variance, binary signaling is optimal.", "startOffset": 39, "endOffset": 47}, {"referenceID": 47, "context": "3 Systems and methods The networks are trained on GPUs using MatConvNet deep learning libraries [48].", "startOffset": 96, "endOffset": 100}, {"referenceID": 48, "context": "The stochastic gradient descent was used with dropout [49] layers, momentum (0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 49, "context": "9), weight decay (10 \u03016), and batch normalization [50].", "startOffset": 50, "endOffset": 54}, {"referenceID": 50, "context": "The parameters learned through training are mapped to hardware using reusable, composable network description functions called corelets [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 51, "context": "The CIFAR-10 dataset [52] consists of color natural images, 32 x 32 pixels in size, in 10 classes, with 50000 training images and 10000 test images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 4, "context": "Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations.", "startOffset": 15, "endOffset": 24}, {"referenceID": 7, "context": "Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations.", "startOffset": 15, "endOffset": 24}, {"referenceID": 2, "context": "Recent results [5, 8, 3] as well as our own experiments seem to suggest that pooling layers are unsuitable for networks with binary neuron activations.", "startOffset": 15, "endOffset": 24}, {"referenceID": 52, "context": "The 4-Chip networks were run on simulation [53], and classification accuracies were measured for the dataset.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "The results are shown in Table 2 in comparison with Energy-efficient deep networks (EEDN) on TrueNorth [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "State of Multi-Chip Networks 1-Chip Networks the Art EEDN [3] Symmetric Kernel EEDN Symmetric Kernel Accuracy Accuracy #Cores Accuracy #Cores Accuracy #Cores FPS Accuracy #Cores FPS 91.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "By using symmetric kernels, we have been able to deploy a network with twice as many features on the same number of cores as was possible using the approach described in [3].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Thus, we have been able to obtain better accuracy for the same amount of hardware even though the set of allowed convolutional kernels are a strict subset of the set of t \u03011, 0, 1u-valued kernels allowed in [3].", "startOffset": 207, "endOffset": 210}], "year": 2016, "abstractText": "We derive a relationship between network representation in energy-efficient neuromorphic architectures and block Toplitz convolutional matrices. Inspired by this connection, we develop deep convolutional networks using a family of structured convolutional matrices and achieve state-of-the-art trade-off between energy efficiency and classification accuracy for well-known image recognition tasks. We also put forward a novel method to train binary convolutional networks by utilising an existing connection between noisy-rectified linear units and binary activations.", "creator": "LaTeX with hyperref package"}}}