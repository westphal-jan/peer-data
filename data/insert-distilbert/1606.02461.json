{"id": "1606.02461", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Learning Semantically and Additively Compositional Distributional Representations", "abstract": "this paper connects a competing vector - based composition model to proving a parallel formal semantics, the dependency - based compositional semantics ( dcs ). ultimately we show theoretical evidence that the optimal vector compositions in developing our model conform to the logic of dcs. now experimentally, we show that vector - based composition brings a strong concurrent ability to calculate similar phrases as similar vectors, achieving near state - of - the - art on a wide range coupling of phrase similarity tasks and complex relation classification ; meanwhile, dcs notation can guide building vectors for structured queries that can be directly executed. we indirectly evaluate this utility on sentence completion task and report a new state - of - the - art.", "histories": [["v1", "Wed, 8 Jun 2016 09:12:17 GMT  (163kb,D)", "http://arxiv.org/abs/1606.02461v1", "to appear in ACL2016"]], "COMMENTS": "to appear in ACL2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ran tian", "naoaki okazaki", "kentaro inui"], "accepted": true, "id": "1606.02461"}, "pdf": {"name": "1606.02461.pdf", "metadata": {"source": "CRF", "title": "Learning Semantically and Additively Compositional Distributional Representations", "authors": ["Ran Tian"], "emails": ["inui}@ecei.tohoku.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "A major goal of semantic processing is to map natural language utterances to representations that facilitate calculation of meanings, execution of commands, and/or inference of knowledge. Formal semantics supports such representations by defining words as some functional units and combining them via a specific logic. A simple and illustrative example is the Dependency-based Compositional Semantics (DCS) (Liang et al., 2013). DCS composes meanings from denotations of words (i.e. sets of things to which the words apply); say, the denotations of the concept drug and the event ban is shown in Figure 1b, where drug is a list of drug names and ban is a list of the subjectcomplement pairs in any ban event; then, a list of banned drugs can be constructed by first taking the COMP column of all records in ban (projection \u201c\u03c0COMP\u201d), and then intersecting the results with drug (intersection \u201c\u2229\u201d). This procedure defined how words can be combined to form a meaning.\nBetter yet, the procedure can be concisely illustrated by the DCS tree of \u201cbanned drugs\u201d (Figure 1a), which is similar to a dependency tree but possesses precise procedural and logical meaning (Section 2). DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al., 2014).\nOrthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014). However, less effort is devoted to finding a link between vector-based compositions and the composition operations in any formal semantics. We believe that if a link can be found, then symbolic formulas in the formal semantics will be realized by vectors composed from word embeddings, such that similar things are realized by similar vectors; meanwhile, vectors will acquire formal meanings that can directly be used in execution or inference process. Still, to find a link is challenging because any vector compositions that realize such a link must conform to the logic of the formal semantics.\nIn this paper, we establish a link between DCS and certain vector compositions, achieving a vector-based DCS by replacing denotations of words with word vectors, and realizing the composition operations such as intersection and projection as addition and linear mapping, respectively. For example, to construct a vector for \u201cbanned drugs\u201d, one takes the word vector vban and multiply it by a matrix MCOMP, corresponding to the projection \u03c0COMP; then, one adds the result to the word vector vdrug to realize the intersection operation (Figure 1c). We provide a method to train the\nar X\niv :1\n60 6.\n02 46\n1v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\n6\nban COMP\ndrug ARG\nbanned drugs\nARG\nAspirin Thalidomide \u2026\nCOMP\nalcohol Thalidomide \u2026\nSUBJ\ngovernment Canada \u2026\ndrug\nban\n\u2229 = \u03c0COMP projection intersection\ndenotation of \u201cbanned drugs\u201d\n(c) (d)\n(a) (b)\nalcohol Thalidomide \u2026 Thalidomide \u2026\nword vectors and linear mappings (i.e. matrices) jointly from unlabeled corpora.\nThe rationale for our model is as follows. First, recent research has shown that additive composition of word vectors is an approximation to the situation where two words have overlapping context (Tian et al., 2015); therefore, it is suitable to implement an \u201cand\u201d or intersection operation (Section 3). We design our model such that the resulted distributional representations are expected to have additive compositionality. Second, when intersection is realized as addition, it is natural to implement projection as linear mapping, as suggested by the logical interactions between the two operations (Section 3). Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by GloVe (Pennington et al., 2014) or those learned from syntactic dependencies (Section 5.1). Finally, additive composition brings our model a strong ability to calculate similar vectors for similar phrases, whereas syntactic-semantic roles (e.g. SUBJ, COMP) can be distinguished by different projection matrices (e.g. MSUBJ, MCOMP). We achieve near state-of-the-art performance on a wide range of phrase similarity tasks (Section 5.2) and relation classification (Section 5.3).\nFurthermore, we show that a vector as constructed above for \u201cbanned drugs\u201d can be used as a query vector to retrieve a coarse-grained candi-\ndate list of banned drugs, by sorting its dot products with answer vectors that are also learned by our model (Figure 1d). This is due to the ability of our approach to provide a language model that can find likely words to fill in the blanks such as \u201c is a banned drug\u201d or \u201cthe drug is banned by . . . \u201d. A highlight is the calculation being done as if a query is \u201cexecuted\u201d by the DCS tree of \u201cbanned drugs\u201d. We quantitatively evaluate this utility on sentence completion task (Zweig et al., 2012) and report a new state-of-the-art (Section 5.4)."}, {"heading": "2 DCS Trees", "text": "DCS composes meanings from denotations, or sets of things to which words apply. A \u201cthing\u201d (i.e. element of a denotation) is represented by a tuple of features of the form Field=Value, with a fixed inventory of fields. For example, a denotation ban might be a set of tuples ban = {(SUBJ=Canada,COMP=Thalidomide), . . .}, in which each tuple records participants of a banning event (e.g. Canada banning Thalidomide).\nOperations are applied to sets of things to generate new denotations, for modeling semantic composition. An example is the intersection of pet and fish giving the denotation of \u201cpet fish\u201d. Another necessary operation is projection; by \u03c0N we mean a function mapping a tuple to its value of the field N. For example, \u03c0COMP(ban) is the value set of the COMP fields in ban, which consists of banned objects (i.e. {Thalidomide, . . .}). In this paper, we assume a field ARG to be names of things representing themselves, hence for example \u03c0ARG(drug) is the set of names of drugs.\nFor a value set V , we also consider inverse image \u03c0\u22121N (V ) := {x | \u03c0N(x) \u2208 V }. For example,\nD1 := \u03c0 \u22121 SUBJ(\u03c0ARG(man))\nconsists of all tuples of the form (SUBJ=x, . . .), where x is a man\u2019s name (i.e. x \u2208 \u03c0ARG(man)). Thus, sell \u2229 D1 denotes men\u2019s selling events (i.e. {(SUBJ=John,COMP=Aspirin), . . .} as in Figure 2). Similarly, the denotation of \u201cbanned\ndrugs\u201d as in Figure 1b is formally written as"}, {"heading": "D2 := drug \u2229 \u03c0\u22121ARG(\u03c0COMP(ban)),", "text": "Hence the following denotation"}, {"heading": "D3 := sell \u2229 D1 \u2229 \u03c0\u22121COMP(\u03c0ARG(D2))", "text": "consists of selling events such that the SUBJ is a man and the COMP is a banned drug.\nThe calculation above can proceed in a recursive manner controlled by DCS trees. The DCS tree for the sentence \u201ca man sells banned drugs\u201d is shown in Figure 2. Formally, a DCS tree is defined as a rooted tree in which nodes are denotations of content words and edges are labeled by fields at each ends. Assume a node x has children y1, . . . ,yn, and the edges (x,y1), . . . , (x,yn) are labeled by (P1,L1), . . . , (Pn,Ln), respectively. Then, the denotation [[x]] of the subtree rooted at x is recursively calculated as\n[[x]] := x \u2229 n\u22c2\ni=1\n\u03c0\u22121Pi (\u03c0Li([[yi]])). (1)\nAs a result, the denotation of the DCS tree in Figure 2 is the denotation D3 of \u201ca man sells banned drugs\u201d as calculated above. DCS can be further extended to handle phenomena such as quantifiers or superlatives (Liang et al., 2013; Tian et al., 2014). In this paper, we focus on the basic version, but note that it is already expressive enough to at least partially capture the meanings of a large portion of phrases and sentences.\nDCS trees can be learned from question-answer pairs and a given database of denotations (Liang et al., 2013), or they can be extracted from dependency trees if no database is specified, by taking advantage of the observation that DCS trees are similar to dependency trees (Tian et al., 2014). We use the latter approach, obtaining DCS trees by rule-based conversion from universal dependency (UD) trees (McDonald et al., 2013). Therefore, nodes in a DCS tree are content words in a UD tree, which are in the form of lemma-POS pairs\n(Figure 3). The inventory of fields is designed to be ARG, SUBJ, COMP, and all prepositions. Prepositions are unlike content words which denote sets of things, but act as relations which we treat similarly as SUBJ and COMP. For example, a prepositional phrase attached to a verb (e.g. play on the grass) is treated as in Figure 3a. The presence of two field labels on each edge of a DCS tree makes it convenient for modeling semantics in several cases, such as a relative clause (Figure 3b)."}, {"heading": "3 Vector-based DCS", "text": "For any content word w, we use a query vector vw to model its denotation, and an answer vector uw to model a prototypical element in that denotation. Query vector v and answer vector u are learned such that exp(v \u00b7 u) is proportional to the probability of u answering the query v. The learning source is a collection of DCS trees, based on the idea that the DCS tree of a declarative sentence usually has non-empty denotation. For example, \u201ckids play\u201d means there exists some kid who plays. Consequently, some element in the play denotation belongs to \u03c0\u22121SUBJ(\u03c0ARG(kid)), and some element in the kid denotation belongs to \u03c0\u22121ARG(\u03c0SUBJ(play)). This is a signal to increase the dot product of uplay and the query vector of \u03c0\u22121SUBJ(\u03c0ARG(kid)), as well as the dot product of ukid and the query vector of \u03c0\u22121ARG(\u03c0SUBJ(play)). When optimized on a large corpus, the \u201ctypical\u201d elements of play and kid should be learned by uplay and ukid, respectively. In general, one has\nTheorem 1 Assume the denotation of a DCS tree is not empty. Given any path from node x to y, assume edges along the path are labeled by (P, L), . . . , (K,N). Then, an element in the denotation y belongs to \u03c0\u22121N (\u03c0K(. . . (\u03c0 \u22121 L (\u03c0P(x) . . .). Therefore, for any two nodes in a DCS tree, the path from one to another forms a training example, which signals increasing the dot product of the corresponding query and answer vectors.\nIt is noteworthy that the above formalization happens to be closely related to the skip-gram model (Mikolov et al., 2013b). The skip-gram learns a target vector vw and a context vector uw for each word w. It assumes the probability of a word y co-occurring with a word x in a context window is proportional to exp(vx \u00b7 uy). Hence, if x and y co-occur within a context window, then one gets a signal to increase vx \u00b7 uy. If the context window is taken as the same DCS tree, then\nthe learning of skip-gram and vector-based DCS will be almost the same, except that the target vector vx becomes the query vector v, which is no longer assigned to the word x but the path from x to y in the DCS tree (e.g. the query vector for \u03c0\u22121SUBJ(\u03c0ARG(kid)) instead of vkid). Therefore, our model can also be regarded as extending skipgram to take account of the changes of meanings caused by different syntactic-semantic roles.\nAdditive Composition Word vectors trained by skip-gram are known to be semantically additive, such as exhibited in word analogy tasks. An effect of adding up two skip-gram vectors is further analyzed in Tian et al. (2015). Namely, the target vector vw can be regarded as encoding the distribution of context words surrounding w. If another word x is given, vw can be decomposed into two parts, one encodes context words shared with x, and another encodes context words not shared. When vw and vx are added up, the non-shared part of each of them tend to cancel out, because non-shared parts have nearly independent distributions. As a result, the shared part gets reinforced. An error bound is derived to estimate how close 12(vw + vx) gets to the distribution of the shared part. We can see the same mechanism exists in vector-based DCS. In a DCS tree, two paths share a context word if they lead to a same node y; semantically, this means some element in the denotation y belongs to both denotations of the two paths (e.g. given the sentence \u201ckids play balls\u201d, \u03c0\u22121SUBJ(\u03c0ARG(kid)) and \u03c0\u22121COMP(\u03c0ARG(ball)) both contain a playing event whose SUBJ is a kid and COMP is a ball). Therefore, addition of query vectors of two paths approximates their intersection because the shared context y gets reinforced.\nProjection Generally, for any two denotations X1,X2 and any projection \u03c0N, we have\n\u03c0N(X1 \u2229 X2) \u2286 \u03c0N(X1) \u2229 \u03c0N(X2). (2)\nAnd the \u201c\u2286\u201d can often become \u201c=\u201d, for example when \u03c0N is a one-to-one map or X1 = \u03c0 \u22121 N (V ) for some value set V . Therefore, if intersection is realized by addition, it will be natural to realize projection by linear mapping because\n(v1 + v2)MN = v1MN + v2MN (3)\nholds for any vectors v1,v2 and any matrix MN, which is parallel to (2). If \u03c0N is realized by a matrix MN, then \u03c0 \u22121 N should correspond to the inverse matrix M\u22121N , because \u03c0N(\u03c0 \u22121 N (V )) = V for\nany value set V . So we have realized all composition operations in DCS.\nQuery vector of a DCS tree Now, we can define the query vector of a DCS tree as parallel to (1):\nv[[x]] := vx + 1\nn n\u2211 i=1 v[[yi]]MLiM \u22121 Pi . (4)"}, {"heading": "4 Training", "text": "As described in Section 3, vector-based DCS assigns a query vector vw and an answer vector uw to each content word w. And for each field N, it assigns two matrices MN and M \u22121 N . For any path from node x to y sampled from a DCS tree, assume the edges along are labeled by (P, L), . . . , (K,N). Then, the dot product vxMPM \u22121 L . . .MKM \u22121 N \u00b7uy gets a signal to increase. Formally, we adopt the noise-contrastive estimation (Gutmann and Hyva\u0308rinen, 2012) as used in the skip-gram model, and mix the paths sampled from DCS trees with artificially generated noise. Then, \u03c3(vxMPM \u22121 L . . .MKM \u22121 N \u00b7uy) models the probability of a training example coming from DCS trees, where \u03c3(\u03b8) = 1/{1 + exp(\u2212\u03b8)} is the sigmoid function. The vectors and matrices are trained by maximizing the log-likelihood of the mixed data. We use stochastic gradient descent (Bottou, 2012) for training. Some important settings are discussed below.\nNoise For any vxM1M\u221212 . . .M2l\u22121M \u22121 2l \u00b7 uy obtained from a path of a DCS tree, we generate noise by randomly choosing an index i \u2208 [2, 2l], and then replacingMj orM\u22121j (\u2200j \u2265 i) and uy by MN(j) or M \u22121 N(j) and uz, respectively, where N(j) and z are independently drawn from the marginal (i.e. unigram) distributions of fields and words.\nUpdate For each data point, when i is the chosen index above for generating noise, we view indices j < i as the \u201dtarget\u201d part, and j >= i as the \u201dcontext\u201d, which is completely replaced by the noise, as an analogous to the skip-gram model. Then, at each step we only update one vector and one matrix from each of the target, context, and noise part; more specifically, we only update vx, Mi\u22121 or M\u22121i\u22121, Mi or M \u22121 i , MN(i) or M \u22121 N(i), uy and uz, at the step. This is much faster than always updating all matrices.\nInitialization Matrices are initialized as 12(I + G), where I is the identity matrix; and G and all\nvectors are initialized with i.i.d. Gaussians of variance 1/d, where d is the vector dimension. We find that the diagonal component I is necessary to bring information from vx to uy, whereas the randomness of G makes convergence faster. M\u22121N is initialized as the transpose of MN.\nLearning Rate We find that the initial learning rate for vectors can be set to 0.1. But for matrices, it should be less than 0.0005 otherwise the model diverges. For stable training, we rescale gradients when their norms exceed a threshold.\nRegularizer During training, MN and M\u22121N are treated as independent matrices. However, we use the regularizer \u03b3\u2016M\u22121N MN\u2212 1 d tr(M \u22121 N MN)I\u2016\n2 to drive M\u22121N close to the inverse of MN.\n1 We also use \u03ba\u2016M\u22a5N MN\u2212 1 d tr(M \u22a5 N MN)I\u2016\n2 to prevent MN from having too different scales at different directions (i.e., to drive MN close to orthogonal). We set \u03b3 = 0.001 and \u03ba = 0.0001. Despite the rather weak regularizer, we find thatM\u22121N can be learned to be exactly the inverse of MN, and MN can actually be an orthogonal matrix, showing some semantic regularity (Section 5.1)."}, {"heading": "5 Experiments", "text": "For training vector-based DCS, we use Wikipedia Extractor2 to extract texts from the 2015-12-01 dump of English Wikipedia3. Then, we use Stanford Parser4 (Klein and Manning, 2003) to parse all sentences and convert the UD trees into DCS trees by handwritten rules. We assign a weight to each path of the DCS trees as follows.\n1Problem with the naive regularizer \u2016M\u22121M \u2212 I\u20162 is that, when the scale of M goes larger, it will drive M\u22121 smaller, which may lead to degeneration. So we scale I according to the trace of M\u22121M .\n2http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor\n3https://dumps.wikimedia.org/enwiki/ 4http://nlp.stanford.edu/software/\nlex-parser.shtml\nFor any path P passing through k intermediate nodes of degrees n1, . . . , nk, respectively, we set\nWeight(P ) := k\u220f i=1 1 ni \u2212 1 . (5)\nNote that ni \u2265 2 because there is a path P passing through the node; and Weight(P ) = 1 if P consists of a single edge. The equation (5) is intended to degrade long paths which pass through several high-valency nodes. We use a random walk algorithm to sample paths such that the expected times a path is sampled equals its weight. As a result, the sampled path lengths range from 1 to 19, average 2.1, with an exponential tail. We convert all words which are sampled less than 1000 times to *UNKNOWN*/POS, and all prepositions occurring less than 10000 times to an *UNKNOWN* field. As a result, we obtain a vocabulary of 109k words and 211 field names.\nUsing the sampled paths, vectors and matrices are trained as in Section 4 (vecDCS). The vector dimension is set to d = 250. We compare with three baselines: (i) all matrices are fixed to identity (\u201cno matrix\u201d), in order to investigate the effects of meaning changes caused by syntactic-semantic roles and prepositions; (ii) the regularizer enforcingM\u22121N to be actually the inverse matrix ofMN is set to \u03b3 = 0 (\u201cno inverse\u201d), in order to investigate the effects of a semantically motivated constraint; and (iii) applying the same training scheme to UD trees directly, by modeling UD relations as matrices (\u201cvecUD\u201d). In this case, one edge is assigned one UD relation rel, so we implement the transfor-\nmation from child to parent byMrel, and from parent to child by M\u22121rel . The same hyper-parameters are used to train vecUD. By comparing vecDCS with vecUD we investigate if applying the semantics framework of DCS makes any difference. Additionally, we compare with the GloVe (6B, 300d) vector5 (Pennington et al., 2014). Norms of all word vectors are normalized to 1 and Frobenius norms of all matrices are normalized to \u221a d."}, {"heading": "5.1 Qualitative Analysis", "text": "We observe several special properties of the vectors and matrices trained by our model.\nWords are clustered by POS In terms of cosine similarity, word vectors trained by vecDCS and vecUD are clustered by POS tags, probably due to their interactions with matrices during training. This is in contrast to the vectors trained by GloVe or \u201cno matrix\u201d (Table 1).\nMatrices show semantic regularity Matrices learned for ARG, SUBJ and COMP are exactly orthogonal, and some most frequent prepositions6 are remarkably close. For these matrices, the corresponding M\u22121 also exactly converge to their inverse. It suggests regularities in the semantic space, especially because orthogonal matrices preserve cosine similarity \u2013 if MN is orthogonal, two words x, y and their projections \u03c0N(x), \u03c0N(y) will have the same similarity measure, which is semantically reasonable. In contrast, matrices trained by vecUD are only orthogonal for three UD relations, namely conj, dep and appos.\nWords transformed by matrices To illustrate the matrices trained by vecDCS, we start from the query vectors of two words, house and learn,\n5http://nlp.stanford.edu/projects/ glove/\n6of, in, to, for, with, on, as, at, from\napplying different matrices to them, and show the 10 answer vectors of the highest dot products (Tabel 2). These are the lists of likely words which: take house as a subject, take house as a complement, fills into \u201c in house\u201d, serve as a subject of learn, serve as a complement of learn, and fills into \u201clearn about \u201d, respectively. As the table shows, matrices in vecDCS are appropriately learned to map word vectors to their syntacticsemantic roles."}, {"heading": "5.2 Phrase Similarity", "text": "To test if vecDCS has the composition ability to calculate similar things as similar vectors, we conduct evaluation on a wide range of phrase similarity tasks. In these tasks, a system calculates similarity scores for pairs of phrases, and the performance is evaluated as its correlation with human annotators, measured by Spearman\u2019s \u03c1.\nDatasets Mitchell and Lapata (2010) create datasets7 for pairs of three types of two-word phrases: adjective-nouns (AN) (e.g. \u201cblack hair\u201d and \u201cdark eye\u201d), compound nouns (NN) (e.g. \u201ctax charge\u201d and \u201cinterest rate\u201d) and verb-objects (VO) (e.g. \u201cfight war\u201d and \u201cwin battle\u201d). Each dataset consists of 108 pairs and each pair is annotated by 18 humans (i.e., 1,944 scores in total). Similarity scores are integers ranging from 1 to 7. Another dataset8 is created by extending VO to SubjectVerb-Object (SVO), and then assessing similarities by crowd sourcing (Kartsaklis and Sadrzadeh, 2014). The dataset GS11 created by Grefenstette and Sadrzadeh (2011) (100 pairs, 25 annotators) is also of the form SVO, but in each pair only the verbs are different (e.g. \u201cman pro-\n7http://homepages.inf.ed.ac.uk/ s0453356/\n8http://www.cs.ox.ac.uk/activities/ compdistmeaning/\nvide/supply money\u201d). The dataset GS12 described in Grefenstette (2013a) (194 pairs, 50 annotators) is of the form Adjective-Noun-Verb-AdjectiveNoun (e.g. \u201clocal family run/move small hotel\u201d), where only verbs are different in each pair.\nOur method We calculate the cosine similarity of query vectors corresponding to phrases. For example, the query vector for \u201cfight war\u201d is calculated as vwarMARGM \u22121 COMP + vfight. For vecUD we use Mnsubj and Mdobj instead of MSUBJ and MCOMP, respectively. For GloVe we use additive compositions.\nResults As shown in Table 3, vecDCS is competitive on AN, NN, VO, SVO and GS12, consistently outperforming \u201cno inverse\u201d, vecUD and GloVe, showing strong compositionality. The weakness of \u201cno inverse\u201d suggests that relaxing the constraint of inverse matrices may hurt compositionaly, though our preliminary examination on word similarities did not find any difference. The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees. It is tested by Blacoe and Lapata (2012) for compositionality, where vecDCS appears to be better. Neverthe-\nless, we note that \u201cno matrix\u201d performs as good as vecDCS, suggesting that meaning changes caused by syntactic-semantic roles might not be major factors in these datasets, because the syntacticsemantic relations are all fixed in each dataset."}, {"heading": "5.3 Relation Classification", "text": "In a relation classification task, the relation between two words in a sentence needs to be classified; we expect vecDCS to perform better than \u201cno matrix\u201d on this task because vecDCS can distinguish the different syntactic-semantic roles of the two slots the two words fit in. We confirm this conjecture in this section.\nDataset We use the dataset of SemEval-2010 Task 8 (Hendrickx et al., 2009), in which 9 directed relations (e.g. Cause-Effect) and 1 undirected relation Other are annotated, 8,000 instances for training and 2,717 for test. Performance is measured by the 9-class direction-aware Macro-F1 score excluding Other class.\nOur method For any sentence with two words marked as e1 and e2, we construct the DCS tree of the sentence, and take the subtree T rooted at the common ancestor of e1 and e2. We construct four vectors from T , namely: the query vector for the subtree rooted at e1 (resp. e2), and the query vector of the DCS tree obtained from T by rerooting it at e1 (resp. e2) (Figure 4). The four vectors are normalized and concatenated to form the only feature used to train a classifier. For vecUD, we use the corresponding vectors calculated from UD trees. For GloVe, we use the word vector of e1 (resp. e2), and the sum of vectors of all words within the span [e1, e2) (resp. (e1, e2]) as\nthe four vectors. Classifier is SVM9 with RBF kernel, C = 2 and \u0393 = 0.25. The hyper-parameters are selected by 5-fold cross validation.\nResults VecDCS outperforms baselines on relation classification (Table 5). It makes 16 errors in misclassifying the direction of a relation, as compared to 144 such errors made by \u201cno matrix\u201d, 23 by \u201cno inverse\u201d, 30 by vecUD, and 161 by GloVe. This suggests that models with syntactic-semantic transformations (i.e. vecDCS, \u201cno inverse\u201d, and vecUD) are indeed good at distinguishing the different roles played by e1 and e2. VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better. Table 4 shows an example of clustered training instances as assessed by cosine similarities between their features. It suggests that the features used in our method can actually cluster similar relations."}, {"heading": "5.4 Sentence Completion", "text": "If vecDCS can compose query vectors of DCS trees, one should be able to \u201cexecute\u201d the vectors to get a set of answers, as the original DCS trees can do. This is done by taking dot products with answer vectors and then ranking the answers. Examples are shown in Table 6. Since query vectors and answer vectors are trained from unlabeled corpora, we can only obtain a coarsegrained candidate list. However, it is noteworthy that despite a common word \u201cbanned\u201d shared by the phrases, their answer lists are largely different, suggesting that composition actually can be done. Moreover, some words indeed answer the queries\n9https://www.csie.ntu.edu.tw/\u02dccjlin/ libsvm/\n(e.g. Thalidomide for \u201cbanned drugs\u201d and Samizdat for \u201cbanned books\u201d).\nQuantitatively, we evaluate this utility of executing queries on the sentence completion task. In this task, a sentence is presented with a blank that need to be filled in. Five possible words are given as options for each blank, and a system needs to choose the correct one. The task can be viewed as a coarse-grained question answering or an evaluation for language models (Zweig et al., 2012). We use the MSR sentence completion dataset10 which consists of 1,040 test questions and a corpus for training language models. We train vecDCS on this corpus and use it for evaluation.\nResults As shown in Table 7, vecDCS scores better than the N-gram model and demonstrates promising performance. However, to our surprise, \u201cno matrix\u201d shows an even better result which is the new state-of-the-art. Here we might be facing the same problem as in the phrase similarity task (Section 5.2); namely, all choices in a question fill into the same blank and the same syntactic-semantic role, so the transforming matrices in vecDCS might not be able to distinguish different choices; on the other hand, vecDCS would suffer more from parsing and POS-tagging errors. Nonetheless, we believe the result by \u201cno matrix\u201d reveals a new horizon of sentence completion, and suggests that composing semantic vectors according to DCS trees could be a promising direction."}, {"heading": "6 Discussion", "text": "We have demonstrated a way to link a vector composition model to a formal semantics, combining the strength of vector representations to calculate phrase similarities, and the strength of formal semantics to build up structured queries. In this section, we discuss several lines of previous research related to this work.\n10http://research.microsoft.com/en-us/ projects/scc/\nLogic and Distributional Semantics Logic is necessary for implementing the functional aspects of meaning and organizing knowledge in a structured and unambiguous way. In contrast, distributional semantics provides an elegant methodology for assessing semantic similarity and is well suited for learning from data. There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction. In the remarkable work by Beltagy et al. (to appear), word and phrase similarities are explicitly transformed to weighted logical rules that are used in a probabilistic inference framework. However, this approach requires considerable amount of engineering, including the generation of rule candidates (e.g. by aligning sentence fragments), converting distributional similarities to weights, and efficiently handling the rules and inference. What if the distributional representations are equipped with a logical interface, such that the inference can be realized by simple vector calculations? We have shown it possible to realize semantic composition; we believe this may lead to significant simplification of the system design for combining logic and distributional semantics.\nCompositional Distributional Models There has been active exploration on how to combine word vectors such that adequate phrase/sentence similarities can be assessed (Mitchell and Lapata, 2010, inter alia), and there is nothing new in using matrices to model changes of meanings. However, previous model designs mostly rely on linguistic intuitions (Paperno et al., 2014, inter alia), whereas our model has an exact logic interpretation. Furthermore, by using additive composition we enjoy a learning guarantee (Tian et al., 2015).\nVector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations. However, the previous works did not specify how to integrate contextual distributional information, which is necessary for calculating semantic similarity.\nFormal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics\n(Liang et al., 2013). It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).\nLogic for Natural Language Inference The pursue of a logic more suitable for natural language inference is also not new. For example, MacCartney and Manning (2008) has implemented a model of natural logic (Lakoff, 1970). We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic.\nSemantic Parsing DCS-related representations have been actively used in semantic parsing and we see potential in applying our model. For example, Berant and Liang (2014) convert \u03bb-DCS queries to canonical utterances and assess paraphrases at the surface level; an alternative could be using vector-based DCS to bring distributional similarity directly into calculation of denotations. We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data.\nFurther Applications Regarding the usability of distributional representations learned by our model, a strong point is that the representation takes into account syntactic/structural information of context. Unlike several previous models (Pado\u0301 and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntactic-semantic roles. A related application is selectional preference (Baroni and Lenci, 2010; Lenci, 2011; Van de Cruys, 2014), wherein our model might has potential for smoothly handling composition.\nReproducibility Find our code at https:// github.com/tianran/vecdcs\nAcknowledgments This work was supported by CREST, JST. We thank the anonymous reviewers for their valuable comments."}], "references": [{"title": "Broad-coverage ccg semantic parsing with amr", "author": ["Artzi et al.2015] Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Artzi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Artzi et al\\.", "year": 2015}, {"title": "Coupling ccg and hybrid logic dependency semantics", "author": ["Baldridge", "Kruijff2002] Jason Baldridge", "Geert-Jan Kruijff"], "venue": "In Proceedings of ACL", "citeRegEx": "Baldridge et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Baldridge et al\\.", "year": 2002}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Frege in space: A program for compositional distributional semantics", "author": ["Baroni et al.2014] Marco Baroni", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Probabilistic soft logic for semantic textual similarity", "author": ["Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of ACL", "citeRegEx": "Beltagy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beltagy et al\\.", "year": 2014}, {"title": "Semantic parsing via paraphrasing", "author": ["Berant", "Liang2014] Jonathan Berant", "Percy Liang"], "venue": "In Proceedings of ACL", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["Blacoe", "Lapata2012] William Blacoe", "Mirella Lapata"], "venue": "In Proceedings of EMNLP-CoNLL", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Wide-coverage semantic representations from a ccg parser", "author": ["Bos et al.2004] Johan Bos", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "In Proceedings of ICCL", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q2012\\E", "shortCiteRegEx": "Bottou.", "year": 2012}, {"title": "On the decidability of query containment under constraints", "author": ["Giuseppe De Giacomo", "Maurizio Lenzerini"], "venue": "In Proceedings of the 17th ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Sys-", "citeRegEx": "Calvanese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Calvanese et al\\.", "year": 1998}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Coecke et al.2010] Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": null, "citeRegEx": "Coecke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "An algebra for semantic construction in constraint-based grammars", "author": ["Alex Lascarides", "Dan Flickinger"], "venue": "In Proceedings of ACL", "citeRegEx": "Copestake et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2001}, {"title": "Minimal recursion semantics: An introduction", "author": ["Dan Flickinger", "Carl Pollard", "Ivan A. Sag"], "venue": "Research on Language and Computation,", "citeRegEx": "Copestake et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["Bing Xiang", "Bowen Zhou"], "venue": "In Proceedings of ACL-IJCNLP", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Introduction to Montague Semantics", "author": ["Dowty et al.1981] David R. Dowty", "Robert E. Wall", "Stanley Peters"], "venue": null, "citeRegEx": "Dowty et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Dowty et al\\.", "year": 1981}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics", "author": ["Edward Grefenstette"], "venue": "PhD thesis", "citeRegEx": "Grefenstette.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["Edward Grefenstette"], "venue": "In Proceedings of *SEM", "citeRegEx": "Grefenstette.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Dependency language models for sentence completion", "author": ["Gubbins", "Vlachos2013] Joseph Gubbins", "Andreas Vlachos"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Gubbins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gubbins et al\\.", "year": 2013}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gutmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2012}, {"title": "Traversing knowledge graphs in vector space", "author": ["Guu et al.2015] Kelvin Guu", "John Miller", "Percy Liang"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Guu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "Jointly learning word representations and composition functions using predicate-argument structures", "author": ["Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Hashimoto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2014}, {"title": "Semeval-2010 task 8: Multi-way classification", "author": ["Su Nam Kim", "Zornitsa Kozareva", "Preslav Nakov", "Diarmuid \u00d3 S\u00e9aghdha", "Sebastian Pad\u00f3", "Marco Pennacchiotti", "Lorenza Romano", "Stan Szpakowicz"], "venue": null, "citeRegEx": "Hendrickx et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hendrickx et al\\.", "year": 2009}, {"title": "From Discourse to Logic", "author": ["Kamp", "Reyle1993] Hans Kamp", "Uwe Reyle"], "venue": null, "citeRegEx": "Kamp et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Kamp et al\\.", "year": 1993}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Kartsaklis", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL)", "citeRegEx": "Kartsaklis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Fast exact inference with a factored model for natural language parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In Advances in NIPS", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Composing and updating verb argument expectations: A distributional semantic model", "author": ["Alessandro Lenci"], "venue": "In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics", "citeRegEx": "Lenci.,? \\Q2011\\E", "shortCiteRegEx": "Lenci.", "year": 2011}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of ACL", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Combined distributional and logical semantics", "author": ["Lewis", "Steedman2013] Mike Lewis", "Mark Steedman"], "venue": "Transactions of ACL,", "citeRegEx": "Lewis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2013}, {"title": "Bringing machine learning and compositional semantics together", "author": ["Liang", "Potts2015] Percy Liang", "Christopher Potts"], "venue": "Annual Review of Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2015}, {"title": "Learning dependency-based compositional semantics", "author": ["Liang et al.2013] Percy Liang", "Michael I. Jordan", "Dan Klein"], "venue": "Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Modeling semantic containment and exclusion in natural language inference", "author": ["MacCartney", "Manning2008] Bill MacCartney", "Christopher D. Manning"], "venue": null, "citeRegEx": "MacCartney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2008}, {"title": "Efficient estimation of word representations in vector space. arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Advances in NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Higher-order logical inference with compositional semantics", "author": ["Pascual Mart\u0131\u0301nez-G\u00f3mez", "Yusuke Miyao", "Daisuke Bekki"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Mineshima et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mineshima et al\\.", "year": 2015}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Mnih", "Teh2012] Andriy Mnih", "Yee Whye Teh"], "venue": "Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2012}, {"title": "Dependency-based construction of semantic space models", "author": ["Pad\u00f3", "Lapata2007] Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2007}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of ACL", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["Pham et al.2015] Nghia The Pham", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Marco Baroni"], "venue": "Proceedings of ACL", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Unsupervised semantic parsing", "author": ["Poon", "Domingos2009] Hoifung Poon", "Pedro Domingos"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Poon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2009}, {"title": "Low-dimensional embeddings of logic", "author": ["Matko Bosnjak", "Sameer Singh", "Sebastian Riedel"], "venue": "In ACL Workshop on Semantic Parsing (SP\u201914)", "citeRegEx": "Rocktaeschel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rocktaeschel et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H. Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y. Ng"], "venue": "Advances in NIPS", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Taking Scope - The Natural Semantics of Quantifiers", "author": ["Mark Steedman"], "venue": null, "citeRegEx": "Steedman.,? \\Q2012\\E", "shortCiteRegEx": "Steedman.", "year": 2012}, {"title": "Logical inference on dependency-based compositional semantics", "author": ["Tian et al.2014] Ran Tian", "Yusuke Miyao", "Takuya Matsuzaki"], "venue": "In Proceedings of ACL", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "The mechanism of additive composition", "author": ["Tian et al.2015] Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "venue": null, "citeRegEx": "Tian et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2015}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Semantic relation classification via convolutional neural networks with simple negative sampling", "author": ["Xu et al.2015] Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao"], "venue": "Proceedings of EMNLP", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Computational approaches to sentence completion", "author": ["Zweig et al.2012] Geoffrey Zweig", "John C. Platt", "Christopher Meek", "Christopher J.C. Burges", "Ainur Yessenalina", "Qiang Liu"], "venue": "Proceedings of ACL", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 32, "context": "A simple and illustrative example is the Dependency-based Compositional Semantics (DCS) (Liang et al., 2013).", "startOffset": 88, "endOffset": 108}, {"referenceID": 32, "context": "DCS has been shown useful in question answering (Liang et al., 2013) and textual entailment recognition (Tian et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 48, "context": ", 2013) and textual entailment recognition (Tian et al., 2014).", "startOffset": 43, "endOffset": 62}, {"referenceID": 29, "context": "Orthogonal to the formal semantics of DCS, distributional vector representations are useful in capturing lexical semantics of words (Turney and Pantel, 2010; Levy et al., 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al.", "startOffset": 132, "endOffset": 176}, {"referenceID": 46, "context": ", 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014).", "startOffset": 98, "endOffset": 255}, {"referenceID": 40, "context": ", 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014).", "startOffset": 98, "endOffset": 255}, {"referenceID": 22, "context": ", 2015), and progress is made in combining the word vectors to form meanings of phrases/sentences (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Paperno et al., 2014; Hashimoto et al., 2014).", "startOffset": 98, "endOffset": 255}, {"referenceID": 49, "context": "First, recent research has shown that additive composition of word vectors is an approximation to the situation where two words have overlapping context (Tian et al., 2015); therefore, it is suitable to implement an \u201cand\u201d or intersection operation (Section 3).", "startOffset": 153, "endOffset": 172}, {"referenceID": 41, "context": "Experimentally, we show that vectors and matrices learned by our model exhibit favorable characteristics as compared with vectors trained by GloVe (Pennington et al., 2014) or those learned from syntactic dependencies (Section 5.", "startOffset": 147, "endOffset": 172}, {"referenceID": 52, "context": "We quantitatively evaluate this utility on sentence completion task (Zweig et al., 2012) and report a new state-of-the-art (Section 5.", "startOffset": 68, "endOffset": 88}, {"referenceID": 32, "context": "DCS can be further extended to handle phenomena such as quantifiers or superlatives (Liang et al., 2013; Tian et al., 2014).", "startOffset": 84, "endOffset": 123}, {"referenceID": 48, "context": "DCS can be further extended to handle phenomena such as quantifiers or superlatives (Liang et al., 2013; Tian et al., 2014).", "startOffset": 84, "endOffset": 123}, {"referenceID": 32, "context": "DCS trees can be learned from question-answer pairs and a given database of denotations (Liang et al., 2013), or they can be extracted from dependency trees if no database is specified, by taking advantage of the observation that DCS trees are similar to dependency trees (Tian et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 48, "context": ", 2013), or they can be extracted from dependency trees if no database is specified, by taking advantage of the observation that DCS trees are similar to dependency trees (Tian et al., 2014).", "startOffset": 171, "endOffset": 190}, {"referenceID": 48, "context": "An effect of adding up two skip-gram vectors is further analyzed in Tian et al. (2015). Namely, the target vector vw can be regarded as encoding the distribution of context words surrounding w.", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "We use stochastic gradient descent (Bottou, 2012) for training.", "startOffset": 35, "endOffset": 49}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.", "startOffset": 3, "endOffset": 37}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.", "startOffset": 3, "endOffset": 69}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.", "startOffset": 3, "endOffset": 109}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.27 Paperno et al. (2014) - 0.", "startOffset": 3, "endOffset": 138}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.27 Paperno et al. (2014) - 0.36 Hashimoto et al. (2014):Waddnl 0.", "startOffset": 3, "endOffset": 169}, {"referenceID": 17, "context": "17 Grefenstette and Sadrzadeh (2011) - 0.21 Blacoe and Lapata (2012):RAE 0.31 0.30 0.28 Grefenstette (2013a) - 0.27 Paperno et al. (2014) - 0.36 Hashimoto et al. (2014):Waddnl 0.48 0.40 0.39 0.34 Kartsaklis and Sadrzadeh (2014) - 0.", "startOffset": 3, "endOffset": 228}, {"referenceID": 41, "context": "Additionally, we compare with the GloVe (6B, 300d) vector5 (Pennington et al., 2014).", "startOffset": 59, "endOffset": 84}, {"referenceID": 17, "context": "The dataset GS11 created by Grefenstette and Sadrzadeh (2011) (100 pairs, 25 annotators) is also of the form SVO, but in each pair only the verbs are different (e.", "startOffset": 28, "endOffset": 62}, {"referenceID": 17, "context": "The dataset GS12 described in Grefenstette (2013a) (194 pairs, 50 annotators) is of the form Adjective-Noun-Verb-AdjectiveNoun (e.", "startOffset": 30, "endOffset": 51}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014).", "startOffset": 155, "endOffset": 179}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets.", "startOffset": 155, "endOffset": 241}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees.", "startOffset": 155, "endOffset": 382}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees. It is tested by Blacoe and Lapata (2012) for compositionality, where vecDCS appears to be better.", "startOffset": 155, "endOffset": 495}, {"referenceID": 21, "context": "The GS11 dataset appears to favor models that can learn from interactions between the subject and object arguments, such as the non-linear model Waddnl in Hashimoto et al. (2014) and the entanglement model in Kartsaklis and Sadrzadeh (2014). However, these models do not show particular advantages on other datasets. The recursive autoencoder (RAE) proposed in Socher et al. (2011) shares an aspect with vecDCS as to construct meanings from parse trees. It is tested by Blacoe and Lapata (2012) for compositionality, where vecDCS appears to be better. NeverthevecDCS 81.2 -no matrix 69.2 -no inverse 79.7 vecUD 69.2 GloVe 74.1 Socher et al. (2012) 79.", "startOffset": 155, "endOffset": 648}, {"referenceID": 14, "context": "4 dos Santos et al. (2015) 84.", "startOffset": 6, "endOffset": 27}, {"referenceID": 14, "context": "4 dos Santos et al. (2015) 84.1 Xu et al. (2015) 85.", "startOffset": 6, "endOffset": 49}, {"referenceID": 23, "context": "Dataset We use the dataset of SemEval-2010 Task 8 (Hendrickx et al., 2009), in which 9 directed relations (e.", "startOffset": 50, "endOffset": 74}, {"referenceID": 51, "context": "VecDCS scores moderately lower than the state-of-the-art (Xu et al., 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al.", "startOffset": 57, "endOffset": 74}, {"referenceID": 51, "context": ", 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015).", "startOffset": 130, "endOffset": 172}, {"referenceID": 14, "context": ", 2015), however we note that these results are achieved by adding additional features and training task-specific neural networks (dos Santos et al., 2015; Xu et al., 2015). Our method only uses features constructed from unlabeled corpora. From this point of view, it is comparable to the MV-RNN model (without features) in Socher et al. (2012), and vecDCS actually does better.", "startOffset": 135, "endOffset": 345}, {"referenceID": 50, "context": "tw/ \u0303cjlin/ libsvm/ vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 Zweig et al. (2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al.", "startOffset": 91, "endOffset": 111}, {"referenceID": 50, "context": "tw/ \u0303cjlin/ libsvm/ vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 Zweig et al. (2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al.", "startOffset": 91, "endOffset": 134}, {"referenceID": 50, "context": "tw/ \u0303cjlin/ libsvm/ vecDCS 50 -no matrix 60 -no inverse 46 vecUD 31 N-gram (Various) 39-41 Zweig et al. (2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al.", "startOffset": 91, "endOffset": 164}, {"referenceID": 34, "context": "(2012) 52 Mnih and Teh (2012) 55 Gubbins and Vlachos (2013) 50 Mikolov et al. (2013a) 55", "startOffset": 63, "endOffset": 86}, {"referenceID": 52, "context": "The task can be viewed as a coarse-grained question answering or an evaluation for language models (Zweig et al., 2012).", "startOffset": 99, "endOffset": 119}, {"referenceID": 11, "context": "There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al.", "startOffset": 82, "endOffset": 147}, {"referenceID": 4, "context": "There have been repeated calls for combining the strength of these two approaches (Coecke et al., 2010; Baroni et al., 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al.", "startOffset": 82, "endOffset": 147}, {"referenceID": 5, "context": ", 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction.", "startOffset": 52, "endOffset": 119}, {"referenceID": 48, "context": ", 2014; Liang and Potts, 2015), and several systems (Lewis and Steedman, 2013; Beltagy et al., 2014; Tian et al., 2014) have contributed to this direction.", "startOffset": 52, "endOffset": 119}, {"referenceID": 49, "context": "Furthermore, by using additive composition we enjoy a learning guarantee (Tian et al., 2015).", "startOffset": 73, "endOffset": 92}, {"referenceID": 17, "context": "Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 17, "context": "Vector-based Logic Models This work also shares the spirit with Grefenstette (2013b) and Rocktaeschel et al. (2014), in exploring vector calculations that realize logic operations.", "startOffset": 64, "endOffset": 116}, {"referenceID": 32, "context": "Formal Semantics Our model implements a fragment of logic capable of semantic composition, largely due to the simple framework of Dependency-based Compositional Semantics (Liang et al., 2013).", "startOffset": 171, "endOffset": 191}, {"referenceID": 15, "context": "It fits in a long tradition of logic-based semantics (Montague, 1970; Dowty et al., 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al.", "startOffset": 53, "endOffset": 111}, {"referenceID": 12, "context": ", 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al.", "startOffset": 122, "endOffset": 170}, {"referenceID": 13, "context": ", 1981; Kamp and Reyle, 1993), with extensive studies on extracting semantics from syntactic representations such as HPSG (Copestake et al., 2001; Copestake et al., 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al.", "startOffset": 122, "endOffset": 170}, {"referenceID": 8, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 47, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 0, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 36, "context": ", 2005) and CCG (Baldridge and Kruijff, 2002; Bos et al., 2004; Steedman, 2012; Artzi et al., 2015; Mineshima et al., 2015).", "startOffset": 16, "endOffset": 123}, {"referenceID": 10, "context": "We would not reach the current formalization of logic of DCS without reading the work by Calvanese et al. (1998), which is an elegant formalization of database semantics in description logic.", "startOffset": 89, "endOffset": 113}, {"referenceID": 21, "context": "We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data.", "startOffset": 87, "endOffset": 105}, {"referenceID": 21, "context": "We also borrow ideas from previous work, for example our training scheme is similar to Guu et al. (2015) in using paths and composition of matrices, and our method is similar to Poon and Domingos (2009) in building structured knowledge from clustering syntactic parse of unlabeled data.", "startOffset": 87, "endOffset": 203}, {"referenceID": 42, "context": "Unlike several previous models (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014; Pham et al., 2015), our approach learns matrices at the same time that can extract the information according to different syntactic-semantic roles.", "startOffset": 31, "endOffset": 98}, {"referenceID": 27, "context": "A related application is selectional preference (Baroni and Lenci, 2010; Lenci, 2011; Van de Cruys, 2014), wherein our model might has potential for smoothly handling composition.", "startOffset": 48, "endOffset": 105}], "year": 2016, "abstractText": "This paper connects a vector-based composition model to a formal semantics, the Dependency-based Compositional Semantics (DCS). We show theoretical evidence that the vector compositions in our model conform to the logic of DCS. Experimentally, we show that vector-based composition brings a strong ability to calculate similar phrases as similar vectors, achieving near state-of-the-art on a wide range of phrase similarity tasks and relation classification; meanwhile, DCS can guide building vectors for structured queries that can be directly executed. We evaluate this utility on sentence completion task and report a new state-of-the-art.", "creator": "LaTeX with hyperref package"}}}