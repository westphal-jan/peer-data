{"id": "1606.02562", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "DialPort: Connecting the Spoken Dialog Research Community to Real User Data", "abstract": "this paper describes a new spoken dialog portal that extensively connects systems produced by all the spoken dialog academic research community and gives them access to real environment users. we introduce a distributed, multi - modal, multi - agent prototype dialog framework that mainly affords easy integration processes with various remote resources, ranging from end - to - end dialog systems to external knowledge apis. further to date, the dialport portal has several successfully connected clients to the multi - standard domain spoken dialog system at cambridge university, the noaa ( national marine oceanic and atmospheric administration ) weather api and the yelp api.", "histories": [["v1", "Wed, 8 Jun 2016 14:08:21 GMT  (8514kb,D)", "http://arxiv.org/abs/1606.02562v1", "Under Peer Review of SigDial 2016"]], "COMMENTS": "Under Peer Review of SigDial 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["tiancheng zhao", "kyusong lee", "maxine eskenazi"], "accepted": false, "id": "1606.02562"}, "pdf": {"name": "1606.02562.pdf", "metadata": {"source": "CRF", "title": "DialPort: Connecting the Spoken Dialog Research Community to Real User Data", "authors": ["Tiancheng Zhao", "Kyusong Lee", "Maxine Eskenazi"], "emails": ["tianchez@cs.cmu.edu,", "max+@cs.cmu.edu,", "kyusonglee@postech.ac.kr"], "sections": [{"heading": "1 Introduction", "text": "The advent of Siri, Cortana and other agents has generated interest in spoken dialog research. These applications have sparked the imagination of many and led them to believe that speaking to intelligent agents is useful. The research community needs to profit from this interest by creating a service for the general public that can gather real user data that can be used to make dialog systems more robust. It can also be used to carry out experiments on comparative studies. Industry already has access to large data sets and sometimes to pools of real users that are viewed as strategic competitive resources and so not shared with the research community. Yet much fundamental research remains to be done, such as signal processing in noisy conditions, recognition of groups of difficult users (like the elderly and non-natives), management of complex dialogs (such as multi party meetings, negotiations, and multimodal interaction), and the automatic use of meta linguis-\n\u2217 Both authors equally contributed to this work\ntic information such as prosody. The academic community needs to unite through one common portal to be able to gather enough real user data to make significant impact. It is extremely difficult for any one group to devote time to collecting a significant amount of real user data. The users must be found and kept interested and the interface must be created and maintained. One data gathering portal that all dialog systems can be connected to gives potential users a variety of interesting applications, much in the way that virtual assistants do not only provide information about scheduling. The DialPort portal was created for this purpose.\nThe Dialog Research Center at Carnegie Mellon (DialRC) has given the community real user data from the Lets Go System (Raux et al., 2005) as well as access to the system to run studies. But research is carried out in other areas, beyond simple form filling. Just as one research group cannot attract a diverse pool of regular users, one group cannot cover all of the possible applications, such as virtual humans and robots with multimodal communication. Thus the goal of DialPort is to attract and maintain a pool of real users to a group of spoken dialog applications.\nThe first year goal is to create the portal and link it to other systems. Once the working portal can give a variety of useful information, a service such as Prefinery1 will be used to attract the real users. These services solicit potential users, giving bonuses for signup and usage as well as for getting friends to sign up. In this paper, we present the DialPort portal that will link many different research applications and will provide real user data. Section 2 discusses related work; Section 3 describes the overall architecture; Section 4, 5 and 6 discuss the core modules; Section 7 explains the integration protocol; Section 8 reviews the current\n1https://www.prefinery.com/\nar X\niv :1\n60 6.\n02 56\n2v 1\n[ cs\n.A I]\n8 J\nun 2\n01 6\nprogress and Section 9 concludes."}, {"heading": "2 Related Work", "text": "Several SDS-building industrial platforms have recently become available to non-expert developers. Microsoft Research released Language Understanding Intelligent Service (LUIS) (Williams et al., 2015) which helps software developers create cloud-based, machine-learning powered, language understanding models for specific application domains. Facebook is building an AI platform that helps developers create chat bots that can converse in natural language2. The HALEF (Help AssistantLanguage-Enabled and Free) framework from ETS leverages different open-source components to form an SDS framework that is modular and industry-standard-compliant (SuendermannOeft et al., 2015). Although these platforms have helped researchers build robust SDSs more efficiently, the data that they collected has not been shared with the academic research community.\nMost early SDS work focused on single-domain SDSs, such as bus schedules, restaurant, or museum information, etc. A single-domain dialog system has limited semantic coverage. Thus, multi-domain dialog systems have appeared, enabling one system to handle several domains. Past approaches usually followed a two-stage framework (Komatani et al., 2009; Nakano et al., 2011), in which the first stage classifies the domain and the second stage forwards the user\u2019s request to the relevant single-domain dialog manager. This method has shown promising results for scaling up dialog systems to handle multiple domains. DialPort differs from previous frameworks in this area by proposing the concept of a multi-agent dialog system. This system combines both goaldriven and non-goal-driven dialog agents that have been independently developed by different research teams. The task of DialPort is to judiciously assign the user\u2019s utterance to the most relevant dialog agent and to carry out complex nested conversations with real users. The long term goal is to enable any research group to connect their SDS to DialPort using a lightweight integration protocol. DialPort makes it easy for real users to access many state-of-the-art dialog system services all in one place through a universal webbased entry point.\n2https://wit.ai"}, {"heading": "3 Architecture", "text": "Figure 1 presents the system architecture, comprised of three sections: User Interface, DialPort and Remote Agents."}, {"heading": "3.1 User Interface", "text": "The user Interface is the publicly available front end for real users3. It is in charge of both the visual and audio agents representing each dialog system. The visual representation uses WebGL Unity 3D, which will be discussed in Section 4. The audio side uses the Google Chrome Speech ASR API to transform the user\u2019s speech into text and the Google Chrome TTS API to convert DialPort\u2019s text output into English speech."}, {"heading": "3.2 DialPort", "text": "DialPort is scalable and distributed. Its central message broker is ActiveMQ, a well-known open source message broker. ActiveMQ allows us to easily connect multiple components in order to create a larger system. Building on ActiveMQ, DialPort has four main modules: the HTTP API Server, the Natural Language Understanding (NLU), the ReinForest Dialog Manager (DM) and the Natural Language Generation (NLG). With the exception of the ReinForest DM, the modules are RESTful (Representational state transfer) web services: they do not consider any state information when handling requests. All contextual information about a dialog is maintained by the ReinForest DM. The HTTP API Server is the front gate of DialPort. It converts the incoming HTTP messages into proper ActiveMQ messages and sends them to the NLU. The NLU outputs a semantic frame that contains the original utterance along with: entities, an intent and a domain. Assuming the user utterance is: \u201dRecommend a restaurant in Pittsburgh\u201d, an example output from the NLU is given in Table 1. Given the user input annotated by the NLU, ReinForest updates its internal dialog state and generates the next system response. The response is in the format of a list of dialog acts and content value tuples, asys = [(DA0, v0), ...(DAk, vk)]. The NLG is responsible for transforming asys to the natural language surface form. Given the utterance in the previous example, an example ReinForest response is: asys = [(CONFIRM, value=Pittsburgh), (ASK, value=food type)]. And the final system output is\n3https://skylar.speech.cs.cmu.edu\n\u201dI believe you said Pittsburgh. What kind of food do you want?\u201d"}, {"heading": "3.3 Remote Agents", "text": "Easy integration with remote Agents is a major contribution of the proposed architecture. We define a remote agent as any external resource that can be integrated into the DialPort ecosystem. Generally there are three types of remote agents: audio remote agent, text remote agent and knowledge remote agent.\nAudio Remote Agent: this is a self-sustaining spoken dialog system that only has a public audio API. Therefore, an audio remote agent expects audio streaming input and returns an audio clip that contains the system\u2019s spoken response. DialPort does not presently support this type of remote agent due to the difficulty of dealing with real-time audio streaming amongst remote servers. This will be dealt with when connection to a system of this type is proposed.\nText Remote Agent: this type of agent provides text API, which inputs the latest ASR text output and returns the next system response in text form. It should be noted that even end-to-end spoken dialog systems can belong to text remote agents. They only need to provide a text-based API. The Cambridge Multi-domain spoken dialog system in Figure 1 (Gasic et al., 2015) is one example. It has its own VoIP audio server and also provides a text-based API. Therefore, when the Cambridge system connects with DialPort, the latter sends the\ntranscribed speech to Cambridge\u2019s text-based API and bypasses its VoIP server.\nKnowledge Remote Agent: the third type of remote agent is an external knowledge base, ranging from a web API (e.g. Yelp API) to an in-house relational database (e.g Pittsburgh bus schedule). DialPort is in charge of all of the dialog processing and uses the knowledge remote agent as the back-end."}, {"heading": "4 Virtual Agent", "text": "This section introduces Skylar and Jasmin. Skylar is the virtual agent for DialPort. Jasmin is the Cambridge University dialog system agent. Both agents interact with users via a web speech interface and have 3D animated embodiments powered by the Unity 3D Engine4.\nThe way in which non-verbal expression (agent behavior) is handled is important. Users need to be able to easily interpret what the agents\u2019 current and next status are. DialPort uses a variety of character animations such as (a) standing, (b) listening, (c) understanding, (d) thinking, and (e) talking (Figure 2), so that users implicitly know when they should talk or should wait for the ASR results. Moreover, the non-verbal expressions of a virtual agent indicate which agent the system thinks the user is talking to. Generating natural non-verbal expression is an open research problem. At present, non-verbal expressions are manually encoded using the Mixamo engine5 as the following:\n\u2022 When nothing is happening, he/she is standing with his/her arms lowered to his/her sides. (Figure 2-a). \u2022 When a user starts to talk to Skylar, he puts\nhis hands/arms behind his back and inclines 4http://unity3d.com/ 5https://www.mixamo.com\nhis head slightly forward and down. Jasmin puts a hand to her ear, indicating that she is listening (Figure 2-b). \u2022 When he/she has finished listening, he/she\nquickly nods twice to indicate that he/she has understood and is starting to process the turn (Figure 2-c) \u2022 After he/she nods, he/she brings his/her arms\nback down to his/her sides and raises his/her head, looking up at the ceiling - this indicates that the agents is thinking of a response (Figure 2-d). \u2022 when he/she is speaking, he/she lowers\nhis/her head and his/her gaze to look at the user and uses various hand gestures. (Figure 2-e)"}, {"heading": "5 The ReinForest Dialog Manager", "text": "The challenges facing the DialPort dialog manager are that it must 1) support easy extension to a large variety of domains and 2) support mixed initiative and mixed (non)-goal driven dialogs. Specifically to deal with these two challenges, we have developed a new dialog manager based on RavenClaw (Bohus and Rudnicky, 2003). The overview of the ReinForest DM is shown in Figure3. The core of ReinForest has two parts: the knowledge ontology and the dialog engine. The knowledge ontology is a domain-dependent knowledge graph that developers can use to quickly encode domain knowledge and relations between various concepts. The dialog engine is a domain-independent execution mechanism that generates the next system response given the current dialog state. The following sections formally define these two components."}, {"heading": "5.1 Knowledge Ontology", "text": "The Knowledge Ontology can be thought of as fuel for ReinForest. The basic unit of the ontology\nis a concept, which is an abstraction of knowledge. For example, knowledge about the weather is a concept. A concept can have a number of dependent concepts that encode the causal relationship. In the weather domain for example, the weather concepts depend on the location and date time concepts. Therefore, in order to give weather information, the system has to have already acquired the values of date time and location. More importantly, each remote agent is also represented as a concept. For example, the Cambridge MultiDomain Dialog System is represented as a single concept that contains information about all of the domains that it covers.\nConcept Pool: Given the definition of a concept, ReinForest enables developers to construct domain knowledge in the form of a directed acyclic graph (DAG). ReinForest also introduces the idea of a concept pool to create groups of concepts. Figure 4 gives a simple example of a concept pool for a slot-filling dialog manager. Essentially a concept pool is a collection of concepts that share some common properties. There are three concept pools in ReinForest: the agent concept pool, the user concept pool and the remote concept pool. The agent concept pool consists of concepts that are powered by knowledge remote agents, such as the agent\u2019s name and the weather information. The user concept pool contains concepts that only users know, such as the date that the user is asking about, the user\u2019s names etc. Finally the remote concept pool contains all the text remote agents, where each is considered to be a black box that can generate the next system response given the context of certain domains."}, {"heading": "5.2 Dialog Engine", "text": "As illustrated in Figure 3, the dialog engine of ReinForest is a domain-independent execution mechanism that consists of 4 modules: hierarchical pol-\nicy execution, belief update, tree transformation and error handling. Algorithm 1 shows the pseudo code of the main execution loop inside ReinForest. The dialog engine establishes its connection to the knowledge ontology via the dialog state, s, which captures all the information about the ongoing dialogs. Next we formally define these four modules.\nAlgorithm 1 ReinForest Main Loop while dialog.end() 6= True do\nwhile dialog stack.top() \u2208 O do execute(dialog stack.pop()) end while if user has input then belief update() tree transformation() error handle()\nend if end while"}, {"heading": "5.3 Hierarchical Policy", "text": "Hierarchical policy has been studied extensively in the literature of both hierarchical reinforcement learning (HRL) (Dietterich, 2000; Parr and Russell, 1998; Sutton et al., 1999) and plan-based dialog management (Bohus and Rudnicky, 2003;\nBohus and Rudnicky, 2009). The contribution of ReinForest is that it formalizes plan-based dialog management in the language of HRL, which opens up the possibility of applying well-established HRL algorithms to optimize the operations of the plan-based dialog manager. We first introduce the notations of HRL and then define the dialog task tree using that language.\nHierarchical Reinforcement Learning: The mathematical framework of HRL is the Markov Decision Process (MDP). An MDP is a tuple (S,A, P, \u03b3,R), where S is a set of states; A is a set of actions; P defines the transition probability P (s\u2032|s, a); R defines the expected immediate reward R(s, a); and \u03b3 \u2208 [0, 1) is the discounting factor. Furthermore, an MDP M can be decomposed into a set of subtasks, O = {O1, O2, ..Ok}, where O0, by convention, is the root and solving O0 solves M . A subtask is a Semi-Markov Decision Process (SMDP), which is characterized by two additional variables compared to an MDP:\n1. \u03b2i(s) is the termination predicate of subtask Oi that partitions S into a set of active states, Si and a set of terminal states Ti. If Oi enters a state in Ti, Oi and its descendants exit immediately, i.e. \u03b2i(s) = 1 if s \u2208 Ti, otherwise \u03b2i(s) = 0. 2. Ui is a nonempty set of actions that can be performed by Oi. The actions can be either primitive actions from A or other subtasks, Oj , where i 6= j. We will refer to Ui as the children of subtask Oi.\nEventually a hierarchical policy for M is \u03c0 and this is simply a set of policies for each subtask, i.e. \u03c0 = {\u03c00, \u03c01...\u03c0n}. It is evident that a valid hierarchical policy forms a DAG, whose roots come from O0 and whose terminal leaves are primitive actions belonging to A.\nReinForest Dialog Policy: ReinForest Dialog Policy forms a dialog task tree. Each node is a\nsubtask belonging to one of three types: dialog agency, dialog choice agency and dialog agent.\n\u2022 Dialog Agency: a subtask Oi \u2208 O with a fixed policy that will execute its children from left to right. \u2022 Dialog Choice Agency: a subtask Oi \u2208 O\nwith a learned policy that chooses the next executed child based on the context. \u2022 Dialog Agent: a primitive action a \u2208 A that\nactually delivers the action to users."}, {"heading": "5.4 Belief Update", "text": "Belief update takes place when there is new input from the user. This component will first update the generic dialog states, such as incrementing the dialog turn count. Then it loops through all the concepts in the knowledge ontology and checks if the annotations in the new input match with any subscribed domain/intent/entities in each concept. If a match is found, the new values are stored in the concept\u2019s attribute map."}, {"heading": "5.5 Tree Transformation", "text": "Tree transformation is key in ReinForest in order to support mixed-initiative and multi-domain dialogs. The transformation has two steps: candidate tree generation and candidate tree selection.\nCandidate tree generation: scans through the updates made by belief update and generates a list of candidate trees that can be pushed to the dialog stack (the list can be \u2205). Usually a candidate is generated when the user explicitly requests the type of information that is handled by a different domain.\nCandidate selection: the selected candidate trees are appended under a dialog choice agency. The dialog choice agency selects one of the trees and pushes it to the dialog stack."}, {"heading": "5.6 Error Handling", "text": "There are two types of error handling: misunderstand error handling and non-understand error handling (Bohus and Rudnicky, 2003). Misunderstand handling is used to conduct explicit or implicit confirms about concepts in the knowledge ontology. Specifically, the dialog engine will loop through all concepts in the user concept pool and select concepts that are updated but not yet grounded for misunderstanding error handling. A misunderstanding subtask will then be pushed to the stack. The misunderstanding subtask will choose a built-in misunderstanding error handling strategy to confirm each concept. The current implementation supports two types of strategies: implicit and explicit confirm.\nOn the other hand, non-understanding handling is activated when there is user input, but no update or tree-transformation is able to succeed. ReinForest implements a wide range of non-understanding handling strategies, ranging from the simple \u201dcan you repeat that?\u201d to a response from an external chat-bot."}, {"heading": "5.7 Execution Demonstration", "text": "Figure 5 shows an example dialog with ReinForest. The dialog engine first pushes the root on to the stack and asks what the user needs. After recognizing that the user is looking for weather information, it pushes the weather domain tree on\nto the stack. After acquiring all of the dependent concepts, Skylar informs the the user of the weather information. The user then decides to request restaurant information which is covered by the Cambridge Remote Agent so it is pushed to the stack. At this time, ReinForest transfers the control to Cambridge and simply calls next to the remote agent in order to obtain the next response. When the remote agent terminates it\u2019s own session, Skylar takes back control and continues the conversation."}, {"heading": "6 Non-goal-driven Dialog Manager", "text": "When a user\u2019s input cannot be handled by ReinForest, such as out-of-domain utterances (e.g., \u201dyou are smart\u201d), specific questions (e.g., who founded Microsoft?, how much is an iPhone?), the non-understanding error handling policy triggers the non-goal driven DM to generate a system response. Goal-driven dialog systems focus on a set of predefined in-domain requests, nongoal driven dialog systems must handle open domain utterances which have in principle unlimited user intents. Most previous non-goal-driven approaches either used handcrafted pattern matching rules or example-based approaches that used a database manually designed by human experts. Recently, recurrent neural network-based datadriven approaches have been proposed that train on large movie transcription corpora (Table 2). Most of the past approaches focus on building chat bots and do not provide a direct solution for DialPort, because we use the non-goal driven dialog manager together with goal driven dialog processing for both entertainment and error handling. Pattern-based approaches are expensive and time consuming (Table 2-a). Moreover, existing publicly available patterns were designed to maintain a conversation with a chat bot for entertainment\nonly. The Neural Network chat bots need a large amount of data to achieve good performance and their dialogs are biased towards the training data (e.g movie scripts). (Table 2-c). Thus the learned dialog strategy is different from the one we want, which is used to recover non-understanding errors and to encourage users to speak to remote agents in other domains.\nThe initial prototype of DialPort uses an example-based chat bot for the initial prototype, because the precision of the response can be controlled by a similarity threshold (Table 2- b). We use the publicly available large knowledge base, Freebase6 created by Google, to extend coverage. For example, if a user asks about a person, a location or the definition of a word, by using the Freebase ID extracted from the DBpedia spotlight and the Freebase property \u201dcommon.topic.description\u201d, the system can find the requested information. Therefore, the nonunderstanding error handling policy queries the chat bot agent with the out-of-domain user input and the example-based chat bot calculates the similarity scores using sent2vec (Shen et al., 2014) (rather than a traditional vector space model). If the similarity score is over 80%, the system response is selected from the chat bot agent. Otherwise ReinForest follows a deterministic error handling strategy which first asks users to \u201drephrase their request\u201d and then provides more instructions if the error cannot be recovered."}, {"heading": "7 Integration Protocol", "text": "This section describes the integration protocol given to participants who are joining DialPort. This simple integration protocol concerns the text remote agent and the knowledge remote agent.\nText Remote Agent: a text remote agent is a di6http://freebase.com\nalog agency defined in Section 5 and it only needs to implement two APIs:\n\u2022 NewCall(id, s0): The input parameters include the user id and current dialog state s0. The output is the first system response. The initial state s0 enables the remote agent to skip redundant questions that were already asked in the previous conversation. DialPort calls this function to initialize a new session with the remote agent. Also, it is up to the remote research group how they use s0, so the remote agent can operate totally independently. The exact format of s0 can be customized if needed.\n\u2022 Next(utt): The input is the users\u2019 utterance and the output is the system\u2019s response and an end-of-session flag. After NewCall, DialPort continues to call Next to obtain the next system response until the end-of-session flag is true. Thus, the remote agent has complete autonomy during its session. We also note that the terminal flag is equivalent to the termination predicate \u03b2i(s) in the definition of an SMDP.\nThe purpose of DialPort is to collect and share real user data. So when a text remote agent finishes its session, it should be responsible for sending a dialog report along with the response to the lastNext call. The report should contain all the essential information about the conversation, such as the utterance at each turn. The final report format will be found on the DialPort website. Speech data that is collected will be made publicly available by the group who collected the data.\nKnowledge Remote Agent: a knowledge agent is simply a function that outputs a list of matched entities, given a list of input constraints. Therefore, any common database format (e.g. SQL) or service API (e.g. Yelp API) can be a knowledge remote agent."}, {"heading": "8 Current State of DialPort", "text": "The first academic system that was connected to DialPort is from Cambridge University. In the near future, any academic system that can be connected will be welcome to join the portal. Figure 6 shows the two current agents: a butler named Skylar from CMU and a librarian named Jasmin from Cambridge. Their appearance may change at a later time. DialPort will start to attract users to\nthese systems as soon as it passes a series of stability tests and when several other remote agents, such as Yelp are added in order to broaden interest. CMU\u2019s Skylar will give information about the weather and restaurants other than in Cambridge and San Francisco. Its job is also to \u201dsell\u201d the other systems, getting the user to want to try them. When the domain changes, a new avatar appears and handles the conversation. When the dialog on the connected systems topic is over, the user is handed back to Skylar. The transition is seamless from the users point of view. Jasmin from Cambridge gives information on hotels and restaurants in San Francisco. When the user is speaking with her, the logo in the background changes to the Cambridge log to indicate which system the user is speaking to (Figure 6)."}, {"heading": "9 Conclusions", "text": "We propose a novel shared platform, DialPort, which can connect to many research systems enabling them to test new application ideas and gather real user data. In this paper, we have described the architecture of the user interface, DialPort, the virtual agents, and the (non)goal driven dialog managers and we have reported the current progress of the DialPort project. An important purpose of this paper is to encourage our colleagues to link their systems to DialPort so that we can help them to collect real user data."}, {"heading": "10 Acknowledgements", "text": "This work was funded by NSF grant CNS1512973. The opinions expressed in this paper do not necessarily reflect those of NSF. We would also like to thank Dialogue Systems Group at Cambridge for their hard work in making the connection with DialPort."}], "references": [{"title": "Iris: a chat-oriented dialogue system based on the vector space model", "author": ["Banchs", "Li2012] Rafael E Banchs", "Haizhou Li"], "venue": "In Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "Banchs et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Banchs et al\\.", "year": 2012}, {"title": "Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda", "author": ["Bohus", "Rudnicky2003] Dan Bohus", "Alexander I Rudnicky"], "venue": null, "citeRegEx": "Bohus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bohus et al\\.", "year": 2003}, {"title": "The ravenclaw dialog management framework: Architecture and systems", "author": ["Bohus", "Rudnicky2009] Dan Bohus", "Alexander I Rudnicky"], "venue": "Computer Speech & Language,", "citeRegEx": "Bohus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bohus et al\\.", "year": 2009}, {"title": "Hierarchical reinforcement learning with the maxq value function decomposition", "author": ["Thomas G Dietterich"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Dietterich.,? \\Q2000\\E", "shortCiteRegEx": "Dietterich.", "year": 2000}, {"title": "Distributed dialogue policies for multi-domain statistical dialogue management", "author": ["Gasic et al.2015] M Gasic", "Dongho Kim", "Pirros Tsiakoulis", "Steve Young"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Gasic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gasic et al\\.", "year": 2015}, {"title": "Multi-domain spoken dialogue system with extensibility and robustness against speech", "author": ["Naoyuki Kanda", "Mikio Nakano", "Kazuhiro Nakadai", "Hiroshi Tsujino", "Tetsuya Ogata", "Hiroshi G Okuno"], "venue": null, "citeRegEx": "Komatani et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Komatani et al\\.", "year": 2009}, {"title": "A two-stage domain selection framework for extensible multidomain spoken dialogue systems", "author": ["Nakano et al.2011] Mikio Nakano", "Shun Sato", "Kazunori Komatani", "Kyoko Matsuyama", "Kotaro Funakoshi", "Hiroshi G Okuno"], "venue": null, "citeRegEx": "Nakano et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nakano et al\\.", "year": 2011}, {"title": "Reinforcement learning with hierarchies of machines. Advances in neural information processing systems, pages 1043\u20131049", "author": ["Parr", "Russell1998] Ronald Parr", "Stuart Russell"], "venue": null, "citeRegEx": "Parr et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Parr et al\\.", "year": 1998}, {"title": "Lets go public! taking a spoken dialog system to the real world", "author": ["Raux et al.2005] Antoine Raux", "Brian Langner", "Dan Bohus", "Alan W Black", "Maxine Eskenazi"], "venue": "In in Proc. of Interspeech", "citeRegEx": "Raux et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Raux et al\\.", "year": 2005}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Shen et al.2014] Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil"], "venue": "In Proceedings of the 23rd ACM International Conference on Conference", "citeRegEx": "Shen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "Halef: An open-source standard-compliant telephonybased modular spoken dialog system: A review", "author": ["Vikram Ramanarayanan", "Moritz Teckenbrock", "Felix Neutatz", "Dennis Schmidt"], "venue": null, "citeRegEx": "SuendermannOeft et al\\.,? \\Q2015\\E", "shortCiteRegEx": "SuendermannOeft et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Aiml: Artificial intelligence markup language", "author": ["Richard Wallace"], "venue": "DOI= http://www. alicebot", "citeRegEx": "Wallace,? \\Q2005\\E", "shortCiteRegEx": "Wallace", "year": 2005}, {"title": "The anatomy of ALICE", "author": ["Richard S Wallace"], "venue": null, "citeRegEx": "Wallace.,? \\Q2009\\E", "shortCiteRegEx": "Wallace.", "year": 2009}, {"title": "Fast and easy language understanding for dialog systems with microsoft language understanding intelligent service (luis)", "author": ["Eslam Kamal", "Hani Amr Mokhtar Ashour", "Jessica Miller", "Geoff Zweig"], "venue": null, "citeRegEx": "Williams et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "data from the Lets Go System (Raux et al., 2005) as well as access to the system to run studies.", "startOffset": 29, "endOffset": 48}, {"referenceID": 17, "context": "Microsoft Research released Language Understanding Intelligent Service (LUIS) (Williams et al., 2015) which helps software developers create cloud-based, machine-learning powered, language understanding models for specific application domains.", "startOffset": 78, "endOffset": 101}, {"referenceID": 5, "context": "approaches usually followed a two-stage framework (Komatani et al., 2009; Nakano et al., 2011), in which the first stage classifies the domain and the second stage forwards the user\u2019s request to the relevant single-domain dialog manager.", "startOffset": 50, "endOffset": 94}, {"referenceID": 6, "context": "approaches usually followed a two-stage framework (Komatani et al., 2009; Nakano et al., 2011), in which the first stage classifies the domain and the second stage forwards the user\u2019s request to the relevant single-domain dialog manager.", "startOffset": 50, "endOffset": 94}, {"referenceID": 4, "context": "The Cambridge Multi-domain spoken dialog system in Figure 1 (Gasic et al., 2015) is one example.", "startOffset": 60, "endOffset": 80}, {"referenceID": 3, "context": "Hierarchical policy has been studied extensively in the literature of both hierarchical reinforcement learning (HRL) (Dietterich, 2000; Parr and Russell, 1998; Sutton et al., 1999) and plan-based dialog management (Bohus and Rudnicky, 2003; Bohus and Rudnicky, 2009).", "startOffset": 117, "endOffset": 180}, {"referenceID": 12, "context": "Hierarchical policy has been studied extensively in the literature of both hierarchical reinforcement learning (HRL) (Dietterich, 2000; Parr and Russell, 1998; Sutton et al., 1999) and plan-based dialog management (Bohus and Rudnicky, 2003; Bohus and Rudnicky, 2009).", "startOffset": 117, "endOffset": 180}, {"referenceID": 16, "context": "Table 2: Characteristics of systems that handle non-goal driven utterances Characteristics Main Technique Representative Systems (a) Pattern Matching high precision, fast response time, time consuming to make patterns Artificial Intelligence Markup Language (Wallace and others, 2005) Alice (Wallace, 2009)", "startOffset": 291, "endOffset": 306}, {"referenceID": 11, "context": "(c) Neural Chatbot open domain, sometimes inconsistent and ungrammatical, require large corpora sequence-to-sequence learning (Sutskever et al., 2014) CleverBot (Vinyals and Le, 2015)", "startOffset": 126, "endOffset": 150}, {"referenceID": 9, "context": "Therefore, the nonunderstanding error handling policy queries the chat bot agent with the out-of-domain user input and the example-based chat bot calculates the similarity scores using sent2vec (Shen et al., 2014)", "startOffset": 194, "endOffset": 213}], "year": 2016, "abstractText": "This paper describes a new spoken dialog portal that connects systems produced by the spoken dialog academic research community and gives them access to real users. We introduce a distributed, multi-modal, multi-agent prototype dialog framework that affords easy integration with various remote resources, ranging from endto-end dialog systems to external knowledge APIs. To date, the DialPort portal has successfully connected to the multidomain spoken dialog system at Cambridge University, the NOAA (National Oceanic and Atmospheric Administration) weather API and the Yelp API.", "creator": "LaTeX with hyperref package"}}}