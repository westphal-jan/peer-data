{"id": "1606.03002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "MuFuRU: The Multi-Function Recurrent Unit", "abstract": "recurrent neural circuit networks theories such as between the gru and lstm found moderately wide adoption in natural language processing and achieve state - of - the - art results unsuitable for many tasks. internally these models are characterized by defining a memory state that can be written to and read from by applying similarly gated composition operations to the current input and the previous state. however, they only cover a small subset of potentially useful compose compositions. we propose multi - function recurrent units ( mufurus ) that allow for arbitrary differentiable functions as composition operations. whilst furthermore, mufurus allow for an input - and state - dependent choice of these composition operations that is learned. our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, especially including implementing the evaluation elements of propositional verb logic formulae, language modeling tasks and sentiment analysis.", "histories": [["v1", "Thu, 9 Jun 2016 15:41:17 GMT  (38kb,D)", "http://arxiv.org/abs/1606.03002v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL", "authors": ["dirk weissenborn", "tim rockt\\\"aschel"], "accepted": false, "id": "1606.03002"}, "pdf": {"name": "1606.03002.pdf", "metadata": {"source": "CRF", "title": "MuFuRU: The Multi-Function Recurrent Unit", "authors": ["Dirk Weissenborn", "Tim Rockt\u00e4schel"], "emails": ["dirk.weissenborn@dfki.de", "t.rocktaschel@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Recurrent neural networks (RNNs) have been applied successfully to a great variety of sequence modeling tasks. Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few. Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].\nAt the core of every RNN is a recurrent cell - function that specifies how the current input\nand the previous state should be combined to form a new state. Different cell-functions have been proposed in the past, including the traditional tanh-cell (Vanilla), the Long-Short-TermMemory (LSTM [8]) or more recently the Gated Recurrent Unit (GRU [3]). They allow for either replacing (Vanilla, GRU, LSTM), keeping (GRU, LSTM) or additively aggregating (LSTM) features in every hidden dimension. The decision is realized via soft gating mechanisms.\nThough different extensions and variations to GRUs and LSTMs have been investigated recently [6, 9], none of them outperform standard GRUs or LSTMs significantly on a range of different tasks. We believe a promising direction towards a more task-adaptive RNN architecture is to (i) allow for other differentiable composition operations and (ii) learn input-dependent selection of these operations end-to-end from task data.\nTherefore, we propose a novel cell -function, called Multi-Function Recurrent Unit (MuFuRU). It is a generalization of frequently-used recurrent architectures and allows for the introduction of arbitrary differentiable composition operations of the previous RNN state and newly constructed feature vector. Crucially, the selection of the composition operation itself is learned and dependent on the previous state and current input. Thus the function that is applied to obtain the next state can change while processing inputs.\nOur contributions are threefold: (i) we discuss existing commonly-used RNN architectures and introduce the Multiple-Function Recurrent\nar X\niv :1\n60 6.\n03 00\n2v 1\n[ cs\n.N E\n] 9\nJ un\n2 01\nUnit (Sections 2 and 3), (ii) we demonstrate that MuFuRUs can learn to evaluate simple logical expressions with comparably small memory size and without over-fitting when the memory size is increased (Section 4.1), and (iii) we show that MuFuRU outperforms a standard GRU baseline on language modeling and sentiment analysis (Sections 4.2 and 4.3) where it approaches state-of-the-art results without hyper-parameter tuning."}, {"heading": "2 Recurrent Neural Networks", "text": "A recurrent neural network is fully specified via a recurrent function f\u03b8 : RN \u00d7RM \u2192 RM \u00d7RH parameterized by \u03b8, where M is the size of the state vector, H the size of the output vector and N the size of the input vector. Given an input sequence X = (x1, ...,xT ) and a start state s0, the state and output at time step t \u2208 [1, T ] is computed as (st,ht) = f(xt, st\u22121) . Finally, an RNN is defined as the recurrent application of the cell-function f to inputs and previous states from time step 1 to T .\nVanilla RNN The simplest cell-function is the tanh-cell, where the new state at each time step is computed by a non-linear projection of the current input xt and the previous state. Note, that the output of the tanh-cell is its state.\nGated Recurrent Unit The GRU updates its state by the element-wise, weighted sum of a newly constructed feature vector vt and the previous state st\u22121 via the update gate ut. The reset gate rt selects which features of the previous hidden state will be used to create the new features. This is useful in situations where the previous state should be forgotten in favor of the creation of new features. Eq. 1 shows how ht and st are computed at each time step.\n[ rt ut ] = \u03c3 ( Wu [ xt st\u22121 ] + bu ) vt = tanh ( Wv [ xt\nrt st\u22121\n] + bv ) st = ht = ut st\u22121 + (1\u2212 ut) vt\n(1)"}, {"heading": "3 Multi-Function Recurrent Unit", "text": "The Multi-Function Recurrent Unit (MuFuRU) applies a predefined set of composition operations (such as element-wise max, min, absolute difference etc.) to a new feature vector and the previous state, and decides which composition should be used for every feature dimension individually."}, {"heading": "3.1 Architecture", "text": "At every time step t the MuFuRU calculates normalized weights pjt for all composition operations opj : RM \u00d7 RM \u2192 RM , j \u2208 [1, l], via an operation controller kt (Equation 2).\np\u0302jt = W j pkt + b j p[\np1t , ...,p l t\n] = softmax ([ p\u03021t , ..., p\u0302 l t ]) (2)\nIn this work, the operation controller is the concatenation of st\u22121 and the current input xt (Equation 3).1\nkt = [ xt st\u22121 ] (3)\nThe MuFuRU (like the GRU) utilizes a resetgate for the computation of the new feature vector vt and combines it with the previous hidden state by a convex combination of the l different composition operations opj (Equation 4).\nrt = \u03c3 ( Wr [ xt st\u22121 ] + br ) vt = tanh ( Wv [ xt\nrt st\u22121\n] + bv ) st = ht =\nl\u2211 j=1 pjt \u00b7 opj(st\u22121,vt) (4)\nIn summary, a MuFuRU learns to select the composition function from a predefined set of operations based on the current input and previous state.\n1It is also possible to compute a lower dimensional, recurrent controller at every time step to save parameters."}, {"heading": "3.2 Composition Functions", "text": "In Table 1 we list the composition functions that were used in this work. Note that this list can easily be extended by other differentiable functions suitable for a given task. The input to each operation is the previous state s and a feature vector v."}, {"heading": "3.3 Relations to existing Architectures", "text": "The MuFuRU is a generalization of existing architectures. Therefore, with slight adaptations and a correct choice of composition functions, the MuFuRU becomes the same or similar to existing architectures.\nVanilla RNN The MuFuRU becomes a standard tanh-cell when only the replace-operation is allowed and the reset-gate is set to 1.\nGRU A GRU is a MuFuRU where only the keep- and replace-operation are allowed.\nIt follows that the MuFuRU should in principle be able to perform at least as well on sequence modeling tasks as those architectures, since it can learn to operate like these."}, {"heading": "4 Experiments", "text": "We evaluate the MuFuRU with the composition operations shown in Table 1 on different tasks that involve modeling of sequences. In the following experiments we always compare performance to the GRU as baseline. Every model is trained with the same task-specific hyper-parameters to ensure comparability. Although the MuFuRU via the operation controller contains a larger parameter set than a GRU, we believe that this does not affect comparability since the additional parameters are only concerned with selecting a single operation at each time step and our goal is to\ninvestigate whether the introduction of new operations is beneficial or not. Besides, all models easily overfit in most of our experiments giving an advantage to models with better generalization ability.\nWe perform mini-batch stochastic gradient descent using ADAM [11] with \u03b21 = 0.0 (no momentum) and \u03b22 = 0.999 for optimization in all experiments."}, {"heading": "4.1 Propositional Logic", "text": "As a unit test we evaluate the MuFuRU\u2019s ability to learn to evaluate simple propositional logic formulae. We sample sequences of Boolean binary gates and input truth values. For instance [1, 0,\u2227, 0,\u2228, 1,\u21d2] represents the Boolean expression ((1 \u2227 0) \u2228 0) \u21d2 1. We train the GRU and MuFuRU on 1000 Boolean formulae with 5-10 gates and test on 1000 unseen longer formulae with 11-20 gates.\nFigure 1a shows the test accuracy of a GRU and MuFuRU with different hidden dimensions trained for 100 epochs. While the GRU struggles to generalize to longer sequences, the MuFuRU learns to evaluate Boolean formulae with a memory size of only 8. The GRU can emulate the operations needed to evaluate Boolean gates when provided with a much larger hidden dimension, but it quickly starts to overfit with larger hidden dimensions. In contrast, the MuFuRU does not overfit even for large hidden dimensions, which indicates that it learns to apply the correct arithmetic counterpart for every Boolean operation.\nWe plot the average weight of operations used in the MuFuRU for input truth values and Boolean gates in Figure 1b2. Some weights seem counter-intuitive (e.g., multiplication has the highest weight for modeling OR). However, note that the choice of operation is dependent not only on the current input but also the previous state (see Eq. 4). We find that the MuFuRU learns a specific behavior tailored to different inputs and that it makes sparse use of the provided operations.\n2See [4] for a description of the gates."}, {"heading": "4.2 Language Modeling", "text": "Language Modeling is an important task for all applications involving language generation. It requires a system to predict the next word conditioned on the previous text at every time step. For this experiment we use the PTB dataset [12] with a limited vocabulary of 10k words. We trained single-layer models with 200 hidden units. The GRU with a testset perplexity of 123.0 was outperformed by the MuFuRU with a perplexity of 119.7. This result substantiates our claim that the MuFuRU should be at least as good at modeling sequences as the GRU."}, {"heading": "4.3 Sentiment Analysis", "text": "Sentiment classification requires a system to recognize the polarity of a text. In this experiment, we train our models on the Sentiment Treebank [15], which contains 215,154 annotated phrases collected from 11,855 sentences.\nWe trained the models with 100 hidden units using mini-batches of 25 phrases each and feed the final output vector at the end of each phrase as input to a logistic regression classifier. Word embeddings are tuned and initialized with Glove [13] or sampled uniformly between -0.05 and 0.05 for unknown words. We chose the model with the best accuracy on the development set in each run which was evaluated every 200 mini-batches.\nThe results of our experiments along with cur-\nrent state-of-the-art results are presented in Figure 1c. Both, the GRU and MuFuRU, achieve high accuracy. The MuFuRU performs better than the GRU, which indicates that the introduction of new operations helps in this task. The MuFuRU even approaches state-of-the-art results without the need for complex structural biases. However, using more complex structures for RNNs is orthogonal to this work, as the differentiable operations of the MuFuRU can be integrated into other architectures."}, {"heading": "5 Conclusion", "text": "We presented the Multi-Function Recurrent Unit (MuFuRU), a new recurrent neural network architecture that learns to select composition functions for the combination of computed features with an existing state at every time step. It thereby generalizes beyond existing models that are limited to a very small set of such compositions. We demonstrate its theoretical advantages on a toy task that evaluates simple propositional formulae and provide empirical evidence on a language modeling task that additional compositional functionality is useful. Since MuFuRUs are in principle able to learn the same or similar behaviour as GRUs or LSTMs, they can be used in place of these cell -functions in RNNs. Furthermore, other task-specific differentiable composition functions can easily be integrated."}, {"heading": "Acknowledgments", "text": "This research was partially supported by Microsoft Research through its PhD Scholarship Programme and by the German Federal Ministry of Education and Research (BMBF) through the projects ALL SIDES (01IW14002), BBDC (01IS14013E), and Software Campus (01IS12050, sub-project GeNIE)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["P. Blunsom", "E. Grefenstette", "N. Kalchbrenner"], "venue": "In ACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1603.08983,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Kout\u0144\u0131k", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In EMNLP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTER- SPEECH,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u1ef3", "P. Blunsom"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": "In STARSEM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 114, "endOffset": 118}, {"referenceID": 17, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 179, "endOffset": 182}, {"referenceID": 13, "context": "Impressive results were achieved for tasks that involve text, such as language modeling [12], machine translation [17], sentiment analysis [18], document-level question answering [7] or recognizing textual entailment [14], to name just a few.", "startOffset": 217, "endOffset": 221}, {"referenceID": 0, "context": "Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].", "startOffset": 101, "endOffset": 108}, {"referenceID": 15, "context": "Modern architectures extend RNNs with additional functionality like attention [1] or external memory [5, 16].", "startOffset": 101, "endOffset": 108}, {"referenceID": 7, "context": "Different cell-functions have been proposed in the past, including the traditional tanh-cell (Vanilla), the Long-Short-TermMemory (LSTM [8]) or more recently the Gated Recurrent Unit (GRU [3]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Different cell-functions have been proposed in the past, including the traditional tanh-cell (Vanilla), the Long-Short-TermMemory (LSTM [8]) or more recently the Gated Recurrent Unit (GRU [3]).", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "Though different extensions and variations to GRUs and LSTMs have been investigated recently [6, 9], none of them outperform standard GRUs or LSTMs significantly on a range of different tasks.", "startOffset": 93, "endOffset": 99}, {"referenceID": 8, "context": "Though different extensions and variations to GRUs and LSTMs have been investigated recently [6, 9], none of them outperform standard GRUs or LSTMs significantly on a range of different tasks.", "startOffset": 93, "endOffset": 99}, {"referenceID": 10, "context": "We perform mini-batch stochastic gradient descent using ADAM [11] with \u03b21 = 0.", "startOffset": 61, "endOffset": 65}, {"referenceID": 3, "context": "See [4] for a description of the gates.", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "RNTN [15] 85.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "4 DCNN [2] 86.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "8 CNN-MC [10] 88.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "1 CT-LSTM [18] 88.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "0 LSTM [18] 84.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "For this experiment we use the PTB dataset [12] with a limited vocabulary of 10k words.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "we train our models on the Sentiment Treebank [15], which contains 215,154 annotated phrases collected from 11,855 sentences.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Word embeddings are tuned and initialized with Glove [13] or sampled uniformly between -0.", "startOffset": 53, "endOffset": 57}], "year": 2016, "abstractText": "Recurrent neural networks such as the GRU and LSTM found wide adoption in natural language processing and achieve state-ofthe-art results for many tasks. These models are characterized by a memory state that can be written to and read from by applying gated composition operations to the current input and the previous state. However, they only cover a small subset of potentially useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that allow for arbitrary differentiable functions as composition operations. Furthermore, MuFuRUs allow for an inputand state-dependent choice of these composition operations that is learned. Our experiments demonstrate that the additional functionality helps in different sequence modeling tasks, including the evaluation of propositional logic formulae, language modeling and sentiment analysis.", "creator": "LaTeX with hyperref package"}}}