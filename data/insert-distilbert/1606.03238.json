{"id": "1606.03238", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks", "abstract": "here, we present idnet, an original user authentication framework from smartphone - acquired motion signals. its designed goal is to recognize a target unfamiliar user from her / his way of walking, using the accelerometer and dual gyroscope ( inertial ) signals provided by a commercial commercial smartphone worn in with the front pocket possibility of crossing the user's trousers. our design features encompassed several innovations including : a robust and smartphone - orientation - independent walking cycle extraction block, a broadly novel feature extractor based on convolutional neural networks, a one - class support vector machine to classify walking cycles, and the coherent integration of these into a standard multi - stage local authentication system. to the best fit of our knowledge, scaling our system is the first machine exploiting convolutional neural networks as our universal feature extractors accountable for gait recognition, and using classification results from subsequent virtual walking cycles into a multi - stage decision making framework. experimental results show the superiority of embracing our approach against state - like of - the - art techniques, leading to misclassification rates ( either false negatives or positives ) smaller rates than 0. 5 15 % occurs in fewer than five walking cycles. design choices are discussed and motivated throughout, assessing broadly their critical impact on the authentication performance.", "histories": [["v1", "Fri, 10 Jun 2016 09:14:28 GMT  (1410kb)", "https://arxiv.org/abs/1606.03238v1", null], ["v2", "Wed, 15 Jun 2016 13:15:53 GMT  (1624kb)", "http://arxiv.org/abs/1606.03238v2", null], ["v3", "Wed, 19 Oct 2016 12:11:57 GMT  (3094kb)", "http://arxiv.org/abs/1606.03238v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["matteo gadaleta", "michele rossi"], "accepted": false, "id": "1606.03238"}, "pdf": {"name": "1606.03238.pdf", "metadata": {"source": "CRF", "title": "IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks", "authors": ["Matteo Gadaleta", "Michele Rossi"], "emails": ["gadaleta@dei.unipd.it", "rossi@dei.unipd.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n03 23\n8v 3\n[ cs\n.C V\n] 1\n9 O\nHere, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user\u2019s trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance.\nKeywords: Biometric gait analysis, target recognition, classification methods, convolutional neural networks, support vector machines, inertial sensors, feature extraction, signal processing, accelerometer, gyroscope.\n\u2217Corresponding author Email addresses: gadaleta@dei.unipd.it (Matteo Gadaleta), rossi@dei.unipd.it\n(Michele Rossi)\nPreprint submitted to Pattern Recognition October 20, 2016"}, {"heading": "1. Introduction", "text": "Wearable technology is advancing at a very fast pace. Many wearable devices, such as smart watches and wristbands are currently available in the consumer market and they often possess miniaturized inertial motion sensors (accelerometer and gyroscope) as well as other sensing hardware capable of gathering biological signs such as photoplethysmographic signals, skin temperature and so forth. Other wearables, such as commercial physiological monitors, deliver a number of vitals via their wireless interfaces, including electrocardiogram, heart rate, chest motion, etc. The same holds true for recent smartphones, that allow for the collection of user\u2019s feedback and for the realtime assessment of their health condition. They also feature sophisticated sensing technology, among which we consider inertial sensors. With sensing technology growing at a fast pace, two major problems are related to the analysis of wearable data and to the authentication of the mobile users who provide it, so that we can assess with reasonably high accuracy whether the data sources are genuine. Notably, certifying the data sources is a necessary step toward the widespread use of this technology in the medical field and, in this paper, we develop the user authentication technology that is required to make this possible. A great deal of work has been carried out on gait recognition in the last decade [1]. In general, biometric gait recognition can be grouped into three main categories: 1) computer vision based, 2) floor sensor based and 3) wearable sensor based [2]. Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7]. Nevertheless, user authentication from wearables is a sensible approach in those scenarios where the deployment of cameras in not possible.\nIn this paper, we propose IDNet (IDentification Network), a new system for the authentication of mobile users from smartphone-acquired motion data. As shown in [8, 9], modern phones possess highly accurate inertial sensors, which allow for non-obtrusive gait biometrics. IDNet leverages deep Convolutional Neural Networks (CNN) [10] and tools from machine learning, such as Support Vector Machines (SVM) [11], combining them in an innovative fashion. Specifically, we develop algorithms for 1) walking cycle extraction, 2) feature identification and, finally, 3) user authentication. CNNs are used as universal feature extractors to discriminate gait signatures from different subjects. Single- as well as multi-stage classifiers are finally combined with CNNs to authen-\nticate the user through the accumulation of scores from subsequent walking cycles. As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17]. The main contributions of this paper are:\n\u2022 The design and validation of an original preprocessing techniques that includes: a\nrobust algorithm for the extraction of walking cycles and an original transformation to move smartphone acquired motion signals into an orientation invariant reference system. Subsequent processing is carried out within this reference system, as this considerably improves authentication results, see Section 3. As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].\n\u2022 The design of a new CNN-based feature extraction tool, which is trained only\nonce on a representative set of users and then used at runtime as a universal feature extractor, see Section 4. Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].\n\u2022 The combination of CNN-extracted features with a one-class SVM classifier [23],\nwhich is solely trained on the target subject, see Section 5. The resulting SVM scores are then accumulated across multiple walking cycles to get higher accuracies, through a new multi-stage identification framework, see Section 6.\n\u2022 The coherent integration of these techniques into the IDNet authentication frame-\nwork, that uses smartphone-acquired accelerometer and gyroscope motion data. We also show that the integration of gyroscope data provides further performance improvements, see Section 4.2.\n\u2022 The experimental validation of IDNet, proving its superiority against solutions\nfrom the state-of-the-art, see Section 4, and achieving authentication errors below 0.15% using fewer than five walking cycles, see Section 6."}, {"heading": "2. Related Work", "text": "Interest in gait analysis began in the 60\u2019s, when walking patterns from healthy people, termed as normal patterns, were investigated by Murray et al. [24]. These measurements were performed through the analysis of photos acquired using interrupted light photography. Murray compared normal gait parameters against those from pathologic gaits [25] and showed that gait is unique to each individual. Since then, human identification through gait recognition has been enjoying a growing interest. Most recent works are based on computer vision [26, 2]. Currently, multi-view gait recognition problem and condition invariance (e.g., clothing or carried items, walking speed, view angle, etc.) are of special interest [7]. Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6]. However, mobile devices are becoming increasingly sophisticated and can provide high quality inertial measurements. Multiple activities can thus be analyzed using wearable sensors data, and exploited, e.g., for task identification [27]. A thorough review of the latest developments in this area can be found in Sprager\u2019s work [1].\nOur interest in this paper is in human gait identification through smartphone inertial sensors. Ailisto et al. [28] were the first to look at this problem and they did it through accelerometer data. In their paper, they used a triaxial accelerometer worn on a belt with fixed orientation: the x-axis pointed forward, the y-axis to the left and the z-axis was aligned with the direction of gravity. Only data points from the x and z axes were used for identification purposes. Gait cycle extraction was performed through a simple peak detection method, and a template was built for each subject. User identification employed a template matching technique, for which different methods were explored: temporal correlation, frequency domain analysis and data distribution statistics.\nIn [29], Derawi et al. proposed more robust preprocessing, cycle detection and template comparison algorithms. Data were acquired using a mobile phone worn on the hip, and only the vertical z-axis was considered for motion analysis. Dynamic Time Warping (DTW) [30] was used as the distance measure, to ensure robustness against non-linear temporal shifts. This scheme was also tested in [20], where majority voting and cyclic rotation were compared as inference rules. In a further paper [21], Hidden\nMarkov Models (HMM) were explored. Accelerometer data were split into windows of fixed length, which were then utilized to train the HMMs. Good identification results were obtained, but at the cost of long authentication phases (30 seconds).\nClassification algorithms based on machine learning were also investigated. Either gait cycles extraction [31] or fixed windows lengths [13] are possible signal segmentation methods. After that, a feature extraction technique is applied to each segment and statistical measures such as mean, standard deviation, root mean square, zero-crossing rate or histogram bin counts, are commonly used. However, more advanced features are required for better results, like cepstral coefficients, which are widely used for speech recognition [13], or features extracted through frequency analysis, i.e., using Fourier [12] or wavelet transforms [31]. Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].\nAccelerometer based gait analysis has also interest in the medical field. Using timefrequency analysis, Huang et al. showed that signals acquired by a waist-worn device on a patient with cervical disc herniation differed before and after the surgery [19]. In [18], classification algorithms were used to discriminate a group of subjects with non-specific chronic low back pain from healthy subjects. Complex parameters, e.g., dynamic symmetry and cyclic stability of gait, were extracted by Jiang et al. [32]. However, their evaluation requires to place sensors on the legs, and fine gait details are difficult to extract from signals acquired by a single waist-worn sensor.\nWe stress that in most of the related work the acquisition system was placed according to a controlled and well known orientation on the subject body. In real scenarios, this is however unlikely to be the case. It is thus important to implement an algorithm whose performance is invariant to the smartphone orientation, which is somewhat unconstrained (and unknown). This makes the phone reference system with good probability misaligned with respect to the direction of motion and the definition of subject specific and time invariant templates impossible. To deal with this, two different approaches can be used. The first consists in the extraction of features that are rotation invariant (e.g., correlation matrices of Fourier transforms [22] or gait dynamic images [33]). The second relies on the transformation of inertial signals [14], projecting them into a new orientation invariant three-dimensional reference system,\nwhich is extracted directly from the data. Here, we adopt the latter approach. Another distintive feature of our work is that we use an original processing pipeline exploiting automatic feature extraction through convolutional neural networks and a scoring algorithm combining one-class support vector machines and multi-step decision analysis."}, {"heading": "3. Signal Processing Framework", "text": "The aim of IDNet is to correctly recognize a subject from his/her way of walking, through the acquisition of inertial signals from a standard smartphone. The proposed processing workflow is shown in Fig. 1. Walking data is first acquired, then we perform some preprocessing entailing:\n1. pre-filtering to remove motion artifacts (Section 3.1),\n2. the extraction of walking cycles (Section 3.2),\n3. a transformation to move the raw walking data into a new orientation independent\nreference system (Section 3.3),\n4. a normalization to represent each walking cycle (accelerometer and gyroscope\ndata) through fixed length, zero mean and unit variance vectors (Section 3.4).\nAfter this, the walking cycles are ready to be processed to identify the user. The standard approach, called \u201cClassical Machine Learning\u201d entails the computation of a number of pre-established statistical features, the most informative of which are selected and used to train a classifier. Various machine learning techniques are usually exploited to this purpose, and are trained through a supervised approach. Hence, the classification performance is assessed and the whole process is usually iterated for a further feature selection phase. In this way, the features that are used for the classification task are progressively refined until a final feature set is attained. Note\nthat statistical features are often assessed by the designer through educated guesses and a trial and error approach.\nAs opposed to this, with IDNet we advocate the use of convolutional neural networks (see Sections 4 and 5). These have been successfully used by the video processing community [34] but to the best of our knowledge have never been exploited for the analysis of inertial data from wearable devices. One of the main advantages of this approach is that statistical features are automatically assessed by the CNN as a result of a supervised training phase. In Section 4, the CNN is trained to act as a universal feature extractor, whereas in Section 5 a one-class SVM is trained as the final classifier. Once the CNN is trained, our system operates assuming that the smartphone only has access to the walking patterns of the target user (i.e., the legitimate user) and the SVM is solely trained using his/her walking data. Our system is based on the premise that, at runtime, the CNN should be capable of producing discriminant features for unseen users and the one-class SVM, once trained on the target, should reliably detect impostors, although their walks were not used for training.\nThe processing blocks are described in higher details in the following subsections.\nNotation: With x \u2208 Rn we mean a column vector x = (x1, x2, . . . , xn) T with elements xi \u2208 R, where (\u00b7) T is the transpose operator. |x| = n returns the number of elements in vector x. x = ( \u2211n\ni=1 xi)/n, whereas \u2016x\u2016 = ( \u2211n i=1 x 2 i ) 1/2 is the L2-norm operator. If\nx,y \u2208 Rn, we define their inner product as x \u00b7y = xTy and their entrywise product as x\u25e6y = (x1y1, x2y2, . . . , xnyn) T . Vector 1n = (1, 1, . . . , 1) T with |1n| = n. Matrices are denoted by uppercase and bold letters. For example, if x,y, z \u2208 Rn, we define a 3\u00d7 n matrix as M = [x,y, z]T , whose rows contain the three vectors. In addition, element (i, j) of matrixX is denoted by Xi,j \u2208 R. With ~r we mean a 3D vector ~r = (r1, r2, r3) T and r\u0302 is the corresponding 3D versor r\u0302 = ~r/\u2016~r\u2016. For any two 3D vectors ~r and ~s we indicate their cross-product as ~r \u00d7 ~s = (r2s3 \u2212 r3s2, r3s1 \u2212 r1s3, r1s2 \u2212 r2s1) T . The gravity vector is referred to as ~\u03c1. With u(i) we mean a time series, where i = 1, 2, . . . is the discrete time index. For acceleration data, the boldface letter a is reserved for vectors, a(i) for time series and A for matrices. The same notation is adopted for the gyroscope data, using g, g(i) and G, respectively for vectors, time series and matrices."}, {"heading": "3.1. Data Acquisition and Filtering", "text": "A proper dataset is key to the successful design and testing of identity recognition algorithms. Some datasets are publicly available. The largest one was acquired by the Institute of Scientific and Industrial Research (ISIR) at Osaka University (OU) [35]. It contains motion data collected from 744 subjects using four motion sensors: three inertial sensors were placed on a belt, with triaxial accelerometer and gyroscope, and a smartphone was worn in the center back waist, and only measured triaxial accelerometer data. Despite the high number of participants, the main problem with this dataset is that motion data was acquired in a controlled environment, and for each subject only two short data sequences are available, which are not enough for deep network training. Furthermore, smartphone\u2019s gyroscope data is not provided. Other datasets are available, but featuring a much smaller number of participants. Casale et al. collected accelerometer data from a smartphone positioned in the chest pocket from 22 users walking over a predefined path [36]. In [37], a motion capture suit was used to acquire data from 40 subjects walking in a small area at different speeds and with direction changes. However, due to the acquisition environment and conditions, these data are more suitable for human gait modeling rather than for user identification. Frank et al. collected data from a mobile phone in the pocket of 20 individuals at McGill University, performing two separate 15 minute walks on two different days [38]. Also in this case gyroscope data is not provided. All these databases do not meet our requirements. In fact, for proper training we need long data collection phases, preferably from different days and with devices freely worn in the user\u2019s front pockets. Hence, we decided to acquire our own motion traces, which are publicly available at http://signet.dei.unipd.it/human-sensing/.\nSpecifically, we acquired motion data from 50 subjects, during a period of six months using Android smartphones worn in the right front pocket of the users\u2019 trousers. The following devices were used: Asus Zenfone 2, Samsung S3 Neo, Samsung S4, LG G2, LG G4 and a Google Nexus 5. Several acquisition sessions of about five minutes were performed for each subject, in variable conditions, e.g., with different shoes and clothes. We asked each subject to walk as she/he felt comfortable with, to mimic real world scenarios. For the data acquisition, we developed an Android inertial data logger application, which saves accelerometer, gyroscope and magnetometer signals into non-volatile memory and then automatically transfers them to an Internet server for\nfurther processing. The magnetometer signal is not used for identification purposes. In general, IDNet can be used carrying the device in other positions but we remark that each requires a dedicated training.\nIn Fig. 2, we plot the power of accelerometer and gyroscope signals at different frequencies through the Welch\u2019s method [39], considering a full walking trace and setting the Hanning window length to 1 s, with half window overlap. Most of the signal power is located at low frequencies, mostly below 40 Hz (where the power is at least 30 dB smaller than the maximum). The raw inertial signals were acquired using an average sample frequency ranging between 100 and 200 Hz (depending on the smartphone model), which is more than appropriate to capture most of the walking signal\u2019s energy.\nAt the first block of IDNet processing chain, due to the non-uniform sampling performed by the smartphone operating system, we apply a cubic Spline interpolation to represent the input data through evenly spaced points (200 points/second). Hence, a low pass Finite Impulse Response (FIR) filter with a cutoff frequency of fc1 = 40 Hz is used for denoising and to reduce the motion artifacts that may appear at higher frequencies. In fact, given the power profile of Fig. 2, the selected cutoff frequency only removes noise and preserves the relevant (discriminative) information about the user\u2019s motion.\nIn the following, with ax(i) and gx(i) we respectively mean the filtered and interpolated acceleration and gyroscope time series along axis x, where i = 1, 2, . . . is the sample number. The same notation holds for axes y and z."}, {"heading": "3.2. Extraction of Walking Cycles", "text": "For the extraction of walking cycles we use a template-based and iterative method that solely considers the accelerometer\u2019s magnitude signal. This signal is in fact inherently invariant to the rotation of the smartphone and, as such, allows for the precise assessment of walking cycles regardless of how the user carries the device in her/his front pocket. For each sample i = 1, 2, . . . the acceleration magnitude is computed as:\namag(i) = (ax(i) 2 + ay(i) 2 + az(i)) 1/2 . (1)\nTo identify the template, a reference point in amag(i) has to be located. To do so, inspired by [16] we first pass amag(i) through a low-pass filter with cutoff frequency\nfc2 = 3 Hz. Thus, we detect the first minimum of this filtered signal, which corresponds to the heel strike [40], and the corresponding index is called i\u0303. This minimum is then refined by looking at the original signal amag(i) in a neighbourhood of i\u0303 and picking the minimum value of amag(i) in that neighborhood. This identifies a new index i \u22c6 for which amag(i \u22c6) is a local minimum. As an example, in Fig. 3, we show this minimum through a red vertical (dashed-dotted) line. As a second step, we pick a window of one second centered in i\u22c6, which in Fig. 3 is represented through two vertical blue (dashed) lines. Now, the samples of amag(i) falling between the two blue lines define the first gait template, which we call T , with |T | = Ns samples, where Ns corresponds to the number of samples measured in one second.\nThe extracted template is then iteratively refined and, at the same time, used to identify subsequent walking cycles. To this end, we first define the following correlation distance, for any two real vectors u and v of the same size n we have:\ncorr dist(u,v) = 1\u2212 (u\u2212 u1n) \u00b7 (v \u2212 v1n)\n\u2016(u\u2212 u1n)\u2016 \u2016(v \u2212 v1n)\u2016 . (2)\nThe template T is then processed with the acceleration magnitude through the following Eq. (3), leading to a further metric \u03d5(i), where i = 1, 2, . . . is the sample index:\nvrect(amag(i)) = (amag(i), . . . , amag(i+Ns \u2212 1)) T (3)\n\u03d5(i) = corr dist(T , vrect(amag(i)) , i = 1, . . . .\nAs can be seen from Fig. 4, the function \u03d5(i) exhibits some local minima, which are promptly located by comparing \u03d5(i) with a suitable threshold \u03d5th and performing a fast search inside the regions where \u03d5(i) < \u03d5th. The indices corresponding to these minima determine the optimal alignments between the template T and amag(i). In particular, the second of these identifies the beginning of the second gait cycle. From these facts we have that:\n1. the samples between the second and the third minima correspond to the second\ngait cycle. It is thus possible to locate accelerometer and gyroscope vectors for this walking cycle, which are respectively defined as: ax, ay, az and gx, gy, gz, still expressed in the (x, y, z) coordinate system of the smartphone. We remark\nthat the number of samples does not necessarily match the template length and usually differs from cycle to cycle, as it depends on the length and duration of walking steps.\n2. A second template T \u2032 is obtained by reading Ns samples starting from the second\nminimum.\nAt this point, a new template is obtained through a weighted average of the old template T and the new one T \u2032:\nT = \u03b1T + (1\u2212 \u03b1)T \u2032 , (4)\nwhere for the results in this paper we used \u03b1 = 0.9. The new template T is then considered for the extraction of the next walking cycle and the procedure is iterated. Note that this technique makes it possible to obtain an increasingly robust template at each new cycle.\nA template matching approach exploiting a similar rationale was used in [16], where the authors employed the Pearson product-moment correlation coefficient between template and amag(i). The main differences between [16] and our approach are: we obtain the template T in a neighborhood of i\u22c6, using a fixed number of samples Ns, whereas they take the samples between two adjacent minima of \u03d5(i) (which may then differ in size for different cycles). In Eq. (4), a discrete-time filter is utilized to refine the template T at each walking cycle, making it more robust against speed changes. In previous work [16], the template is instead kept unchanged up to a point when minima cannot be longer detected, and a new template is to be obtained.\nFinally, a normalization phase is required to represent all the cycles through the same number of points N , as this is required by the following feature extraction and classification algorithms. Before doing this, however, a transformation of accelerometer and gyroscope signals is performed to express these inertial signals in a rotation invariant reference system, as described next."}, {"heading": "3.3. Orientation Independent Transformation", "text": "To evaluate the new orientation invariant coordinate system, three orthogonal versors \u03be\u0302, \u03b6\u0302, \u03c8\u0302 are to be found, whose orientation is independent of that of the smartphone and aligned with gravity and the direction of motion. Specifically, our aim is to express\naccelerometer and gyroscope signals in a coordinate system that remains fixed during the walk, with versor \u03b6\u0302 pointing up (and parallel to the user\u2019s torso), versor \u03be\u0302 pointing forward (aligned with the direction of motion) and \u03c8\u0302 tracking the lateral movement and being orthogonal to the other two. This entails inferring the orientation of the mobile device carried in the front pocket from the acceleration signal acquired during the walk. To this end, we adopt a technique similar to those of [41, 42].\nGravity is the main low frequency component in the accelerometer data, and will be our starting point for the transform. Moreover, although it is a constant vector, it continuously changes when represented in the (x, y, z) coordinate system of the smartphone, due to the user\u2019s mobility and the subsequent change of orientation of the device. So, even the gravity vector ~\u03c1 is not constant when expressed through the smartphone coordinates. As the first axis of the new reference system, we consider the mean direction of gravity within the current walking cycle. Let nk be the number of samples in the current cycle k, with k = 1, 2, . . . . We recall that, with ax, ay and az we mean the acceleration samples in the current cycle k along the three axes x, y and z, with |ax| = |ay| = |az| = nk, whereas with gx, gy and gz we indicate the gyroscope samples in the same cycle k, with |gx| = |gy| = |gz| = nk. The gravity vector ~\u03c1k within cycle k is estimated as:\n~\u03c1k = (ax, ay, az) T . (5)\nThe first versor of the new system \u03b6\u0302 is obtained as:\n\u03b6\u0302 = ~\u03c1k\n\u2016~\u03c1k\u2016 . (6)\nNow, we define the acceleration matrix A = [ax,ay,az] T of size 3 \u00d7 nk, whose rows corresponds to ax, ay and az. Likewise, the gyroscope matrix is G = [gx, gy, gz ] T , whose rows corresponds to gx, gy and gz . The projected acceleration and gyroscope vectors along axis \u03b6\u0302 are:\na\u03b6 = A \u00b7 \u03b6\u0302 , g\u03b6 = G \u00b7 \u03b6\u0302 , (7)\nwhere the new vectors have the same size nk. By removing this component from the original accelerometer signal, we project the latter on a plane that is orthogonal to \u03b6\u0302. This is the horizontal plane (parallel to the floor). We represent this flattened acceleration data through a new matrix Af = [afx,a f y ,a f z ] T of size 3 \u00d7 nk, where a f x, afy and a f z are vectors of size nk that describe the acceleration on the new plane:\nAf = A\u2212 \u03b6\u0302aT\u03b6 . (8)\nAnalyzing this flattened acceleration, we see that during a walking cycle it is unevenly distributed on the horizontal plane. Also, the acceleration points on this plane are dispersed around a preferential direction, which has the highest excursion (variance). Here, we assume that the direction with the largest variance in our measurement space contain the dynamics of interest, i.e., it is parallel to the direction of motion, as it was also observed and verified in previous research [41]. Given this, we pick this direction as the second axis (versor \u03be\u0302) of the new reference system. This is done by applying the Principal Component Analysis (PCA) [43] on the projected points, which finds the direction along which the variance of the measurements is maximized. The PCA procedure is as follows:\n1. Find the empirical mean along each direction x, y and z (rows 1, 2 and 3 of\nthe flattened acceleration matrix Af ). Store the mean in a new vector u of size 3\u00d7 1., i.e.:\nui = 1\nnk\nnk \u2211\nj=1\nAfi,j , i = 1, 2, 3 . (9)\n2. Subtract the empirical mean vector u from each column of matrix Af , obtaining\nthe new matrix Afnorm:\nAfnorm = A f \u2212 u(1nk) T . (10)\n3. Compute the sample 3\u00d7 3 autocovariance matrix \u03a3:\n\u03a3 = Afnorm(A f norm) T\nnk \u2212 1 . (11)\n4. The eigenvalues and the corresponding eigenvectors of \u03a3 are evaluated. The\neigenvector ~v associated with the maximum eigenvalue identifies the direction of maximum variance in the dataset (i.e., the first principal component of the PCA transform).\nHence, versor \u03be\u0302 is evaluated as:\n\u03be\u0302 = ~v\n\u2016~v\u2016 . (12)\nAccelerometer and gyroscope data are then projected along \u03be\u0302 through the following equations: a\u03be = A \u00b7 \u03be\u0302 and g\u03be = G \u00b7 \u03be\u0302. Being \u03be\u0302 placed on a plane that is orthogonal to \u03b6\u0302, these two versors are also orthogonal. The third axis is then obtained through a cross product:\n\u03c8\u0302 = \u03b6\u0302 \u00d7 \u03be\u0302 , (13)\nand the new accelerometer and gyroscope data along this axis are respectively obtained as: a\u03c8 = A \u00b7 \u03c8\u0302 and g\u03c8 = G \u00b7 \u03c8\u0302. The transformed vectors (a\u03be,a\u03c8,a\u03b6) and (g\u03be, g\u03c8, g\u03b6), along with the magnitude vectors amag and gmag are the output of the Orientation Independent Transformation block of Fig. 1.\nAn example of this transform is shown in Fig. 5, where accelerometer and gyroscope data from two different walks from the same subject are plotted. These signals were acquired carrying the phone in the right front pocket of the subject\u2019s trousers using two different orientations. As highlighted in the figure, our transform makes walking data rotation invariant. In fact, subject-specific gait patterns emerge in the new coordinate system (see the red colored patterns in the right plots)."}, {"heading": "3.4. Normalization", "text": "Each gait cycle has a different duration, which depends on the walking speed and stride length. So, considering the accelerometer and gyroscope data collected during a full walking cycle, we remain with variable-size acceleration and gyroscope vectors, which are now expressed in the new orientation invariant coordinate system discussed in\nSection 3.3. However, since feature extraction and classification algorithms require N - sized vectors for each cycle, where N has to be fixed, a further adjustment is necessary. We cope with this cycle length variability through a further Spline interpolation to represent all walking cycles through vectors of N = 200 samples each. This specific value of N was selected to avoid aliasing. In fact, assuming a maximum cycle duration of \u03c4 = 2 seconds, which corresponds to a very slow walk, and a signal bandwidth of B = 40 Hz, a number of samples N > 2B\u03c4 = 160 samples/cycle is required. Amplitude normalization was also implemented, to obtain vectors with zero mean and unit variance, as this leads to better training and classification performance. This results in a total of eight N -sized vectors for each walking cycle, which are inputted into the feature extraction and classification algorithms of the following sections."}, {"heading": "4. Convolutional Neural Network", "text": "In this section, we present the chosen Convolutional Neural Network (CNN) architecture for IDNet (Section 4.1), its optimization, training and quantitative comparison against gait authentication techniques from the literature (Section 4.2)."}, {"heading": "4.1. CNN Architecture", "text": "CNNs are feed-forward deep neural networks differing from fully connected multilayer networks for the presence of one or more convolutional layers. At each convolutional layer, a number of kernels is defined. Each of them has a number of weights, which are convolved with the input in a way that the same set of weights, i.e., the same kernel, is applied to all the input data, moving the convolution operation across the input span. Note that, as the same weights are reused (shared weights), and each kernel operates on a small portion of the input signal, it follows that the network connectivity structure is sparse. This leads to advantages such as a considerably reduced computational complexity with respect to fully connected feed forward neural networks. For more details the reader is referred to [44]. CNNs have been proven to be excellent feature extractors for images [45] and here we prove their effectiveness for motion data. The CNN architecture that we designed to this purpose is shown in Fig. 6. It is composed of a cascade of two convolutional layers, followed by a pooling and a fully-connected layer. The convolutional layers perform a dimensionality reduction (or feature extraction) task, whereas the fully-connected one acts as a classifier. Accelerometer and gyroscope data from each walking cycle are processed according to the algorithms of Section 3. We refer to the input matrix for a generic walking cycle to as X = (a\u03be,a\u03c8,a\u03b6,amag, g\u03be, g\u03c8, g\u03b6 , gmag) T , where all the vectors are normalized to N samples (see Section 3.4). In detail, we have (CL = Convolutional Layer, FL = Fully-connected Layer):\n\u2022 CL1 The first convolutional layer implements one dimensional kernels (1x10 sam-\nples) performing a first filtering of the input and processing each input vector (rows of X) separately. This means that at this stage we do not capture any correlation among different accelerometer and gyroscope axes. The activation functions are linear and the number of convolutional kernels is referred to as Nk1.\n\u2022 CL2 With the second convolutional layer we seek discriminant and class-invariant\nfeatures. Here, the cross-correlation among input vectors is considered (kernels of size 4x10 samples) and the output activation functions are non-linear hyperbolic tangents. Max pooling is applied to the output of CL2 to further reduce its dimensionality and increase the spatial invariance of features [46]. With Nk2 we mean the number of convolutional kernels used for CL2.\n\u2022 FL1 This is a fully connected layer, i.e., each output neuron of CL2 is connected\nto all input neurons of this layer (weights are not shared). Hyperbolic tangent activation functions are used at the output neurons. FL1 output vector is termed f = (f1, . . . , fF ) T , and contains the F features extracted by the CNN.\n\u2022 FL2 Each output neuron in this layer corresponds to a specific class (one class per\nuser), for a total of K neurons, where K is the number of subjects considered for the training phase. The K dimensional output vector y = (y1, . . . , yK) T is obtained by a softmax activation function, which implies that yj \u2208 (0, 1), j = 1, . . . ,K and \u2211K\nj=1 yj = 1 (stochastic vector). Also, yj can be thought of as the probability that\nthe current data matrix X belongs to class (user) j.\nThe network is trained in a supervised manner for a total of K subjects solving a multi-class classification problem, where each of the input matrices X in the dataset is assigned to one of K mutually exclusive classes. The target output vector t = (t1, . . . , tK) T has binary entries and is encoded using a 1-of-K coding scheme, i.e., they are all zero except for that corresponding to the subject that generated the input data."}, {"heading": "4.2. CNN Optimization and Results", "text": "In this section, we propose some approaches for the optimization of the CNN, quantify its classification performance and compare it against classification techniques from the literature. As said above, the output of layer FL2 is the stochastic vector y, whose j-th entry yj , j = 1, . . . ,K, can be seen as the probability that the input pattern belongs to user j, i.e., yj = yj(w,X) = Prob(tj = 1|w,X), where w is the vector containing all the CNN weights, X is the current input matrix (walking cycle) and tj = 1 if X belongs to class j and tj = 0 otherwise. If X is the set of all training examples, we define the batch set as B \u2282 X . Let X \u2208 B and denote the corresponding output vector by y(w,X) and its j-th entry by yj(w,X). The corresponding target vector is t(X) = (t1(X), . . . , tK(X)) T . The CNN is then trained through a stochastic gradient descend algorithm which minimizes a categorical cross-entropy loss function L(w), defined as [11, Eq. (5.24) of Section 5.2]:\nL(w) = \u2212 \u2211\nX\u2208B\nK \u2211\nj=1\ntj(X) log(yj(w,X)) . (14)\nDuring training, Eq. (14) is iteratively minimized, by rotating the walking cycles (training examples) in the batch set B so as to span the entire input set X . Training continues until a stopping criterion is met (see below).\nWalking patterns from K subjects are used to train the CNN, and the same number of cycles Nc is considered for each of them, for a total of KNc training cycles. Nt randomly chosen walking cycles from each subjects are used to obtain a test set P . The remaining cycles are split into training T and validation V sets, with |P| = KNt, |T | = KNc, X = P \u222a T \u222a V , where all the sets have null pairwise intersection and are built picking input patterns from X evenly at random. Set V is used to terminate the training phase, and termination occurs when the loss function L(w) evaluated on V does not decrease for twenty consecutive training epochs. After that, the network weights which led to the minimum validation loss are used to assess the CNN performance on set P . This is done through an accuracy measure, defined as the number of walking cycles correctly classified by the CNN divided by the total number of cycles in P . In the following graphs, we show the mean accuracy obtained averaging the test set performance over ten different networks, all of them trained through the just explained approach by considering K = 35 subjects from our dataset and Nt = 100 cycles per subject.\nAs a first set of results, we look at the impact of F (neurons in layer FL1) and of the number of convolutional kernels in CL1 and CL2. Since the last layer FL2 acts as a classifier, F can be seen as the number of features extracted by the CNN. In general, a too small F can lead to poor classification results; too many features, instead, would make the state space too big to be effectively dealt with (curse of dimensionality) [47]. Besides F , we also investigate the right number of kernels to use within each convolutional layer. Three networks are considered by picking different (Nk1, Nk2) pairs. For network 1 we use (Nk1 = 10, Nk2 = 20), network 2 has (Nk1 = 20, Nk2 = 40) and network 3 has (Nk1 = 30, Nk2 = 50). In Fig. 7, we show the accuracy performance of these networks as a function of F . From this plot, it can be seen that at least F = 20 neurons have to be used at the output of FL1 and that the accuracy performance stabilizes around F = 40, leading to negligible improvements as N grows beyond this value. As for the number of kernels, we conclude that small networks (network 1) perform worse than bigger ones (networks 2 and 3), but increasing the number of kernels beyond that used for network 2 does not lead to appreciable\nimprovements. Hence, for the results of this paper we used F = 40 with (Nk1 = 20, Nk2 = 40).\nComparison against existing techniques: in Fig. 8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].1 These techniques were used in a large number of papers including [15, 13, 31, 18, 14]. For their training, 112 features were extracted from the signal samples in X, including their variance, mean trend, windowed mean difference, variance trend, windowed variance difference, maxima and minima, spectral entropy, zero crossing rate and bin counts. These features, were then utilized to train the selected classifiers in a supervised manner. Note that, while the CNN automatically extracts its features (vector f), with previous schemes these are manually selected based on experience.\nFrom Fig. 8, we see that the CNN-based algorithm delivers better accuracies across the entire range of Nc. Also, the accuracy increases with an increasing Nc until it saturates and no noticeable improvements are observed. While a higher Nc is always beneficial, a higher number of cycles also entails a longer acquisition time, which we would rather avoid. For this reason, for the following results we have used Nc = 40 as it provided a good tradeoff between accuracy and complexity across all our experiments.\nTo illustrate the superiority of CNN features with respect to manually extracted ones, in the following we conduct an instructive experiment. We consider CNN as a feature extraction block, by removing the output vector y and using the inner feature vector f to train the above classifiers from the literature (CT, NB, k-NN and SVM). The corresponding accuracy results are provided in Fig. 9. All the classifiers perform better when trained using CNN features, with typical improvements in the test accuracy of more than 10%. For instance, for a k-NN classifier trained with Nc = 30 cycles per subject, the accuracy increases from 71% (manually extracted features) to 94% (CNN features). The best performance is provided by the combined use of CNN\n1For SVM, we considered a linear kernel, as it outperformed polynomial and radial basis function ones (results are omitted in the interest of space). A one-versus-all strategy was used solve the considered multiclass problem for the binary classifers.\nfeatures and SVM.\nA last consideration is in order. Most of the previous papers only used accelerometer data, but our results show that using both gyroscope and accelerometer provides further improvements, see Fig. 10."}, {"heading": "5. One-Class Support Vector Machine Training", "text": "In this section, we further extend the IDNet CNN-based authentication chain through the design of an SVM classifier which is trained solely using the motion data of the target subject. This is referred to as One-Class Classification (OCC) and is important for practical applications where motion signals of the target user are available,\nbut those belonging to other subjects are not. More importantly, with this approach the classification framework can be extended to users that were not considered in the CNN training."}, {"heading": "5.1. Revised Classification Architecture", "text": "Due to the generalization properties of convolutional deep networks, once trained, the CNN can be used as a universal feature extractor, providing meaningful features even for subjects that were not included in the training. To take advantage of this, we discard the output neurons of FL2 and utilize the CNN as a dimensionality reduction tool that, given an input matrix X, returns a user dependent feature vector f . The\nCNN is then trained only once considering the optimizations of Section 4.2. All its weights and biases are then precomputed and will not be modified at classification time. Considering the diagram of Fig. 6, at the output of the CNN we obtain the feature vector f . We then apply a feature selection block to reduce the number of features from F to S \u2264 F (dimensionality reduction). PCA is used to accomplish this task and the new feature vector is called s. Hence, we have s = \u03a5(f), where \u03a5(\u00b7) : RF \u2192 RS is the PCA transform.\nA One-class Support Vector Machine (OSVM) is then used as the classification algorithm (Section 5.2). It defines a boundary around the feature (training) vectors belonging to the target subject. At runtime, as a new walking cycle is processed, the OSVM takes the feature vector s and outputs a score, which is a distance measure between the current feature vector and the SVM boundary [11, Chapter 7]. As we discuss shortly, this score relates to the likelihood that the current walking cycle belongs to the target user."}, {"heading": "5.2. One-Class SVM Design", "text": "Next, we design the OSVM block of Fig. 6. It differs from a standard binary SVM classifier as the SVM boundary is built solely using patterns from the positive class (target user). The strategy proposed by Scho\u0308lkopf is to map the data into the feature space of a kernel, and to separate them from the origin with maximum margin [52]. The corresponding minimization problem is similar to that of the original SVM formulation [51]. The trick is to use a hyperplane (in the space transformed by a suitable kernel function) to discriminate the target vectors. OSVM takes as input the reduced feature vector s = (s1, . . . , sS) T and we use the following Radial Basis Function (RBF) kernel, that for any s, s\u2032 \u2208 RS is defined as:\n\u03a8(s, s\u2032) = (\u03a6(s) \u00b7 \u03a6(s\u2032)) = exp ( \u2212\u03b3 \u2016s\u2212 s\u2032\u20162 ) , (15)\nwhere \u03a6(s) is a feature map and \u03b3 is the RBF kernel parameter, which intuitively relates to the radius of influence that each training vector has for the space transformation. With \u2113 we mean the number of training points (feature vectors), \u03c9 and b are the hyperplane parameters in the transformed domain (through Eq. (15)) and \u03b5 = (\u03b51, . . . , \u03b5\u2113) T is the vector of slack variables, which are introduced to deal with outliers. Given this, the following quadratic program is defined to separate the feature\nvectors in the training set, s1, . . . , s\u2113, from the origin:\nmin \u03c9,\u03b5,b\n1 2 \u2016\u03c9\u20162 + 1 \u03bd\u2113 \u2211\u2113 j=1 \u03b5j \u2212 b (16)\nsubject to (\u03c9 \u00b7 \u03a6(sj)) \u2265 b\u2212 \u03b5j , \u03b5j \u2265 0 , j = 1, . . . , \u2113\n\u03bd \u2208 (0, 1) is one of the most important parameters and sets an upper bound on the fraction of outliers and a lower bound on the fraction of Support Vectors (SV) [52]. The decision function for a generic feature vector s is defined as d(s) \u2208 {\u22121,+1}, is obtained solving Eq. (16), and only depends on the training vectors through the following relations:\nd(s) = sgn (h(s)) , h(s) =\n\u2113 \u2211\nj=1\n\u03b1j\u03a8(sj, s)\u2212 b . (17)\nNow, \u03b1j \u2265 0, \u2200 j, and only some of the training vectors have \u03b1j > 0. These are the support vectors associated with the classification problem and are the only ones who count in the definition of the SVM boundary. h(s) is the score associated with vector s. It weighs the distance from the SVM boundary, i.e., is greater than zero if s resides inside the boundary, zero if it lies on it and negative otherwise.\nHence, the SVM is trained using a set of \u2113 feature vectors from the target user, obtaining the SVM boundary (and the related decision function) through Eq. (17). After training, we test the performance of the obtained SVM classifier considering feature vectors from the positive class C1 (target user) and the negative one C0 (any other user). Note that the vectors used for this test were not considered during the SVM training.\nAs it is customary for binary classification approaches, the two most important metrics to assess the goodness of a classifier are the precision and the recall. The precision is the fraction of true positives, i.e., the fraction of patterns identified of the target class that in fact belong to the target user, while the recall corresponds to the fraction of target patterns that are correctly classified out of the entire positive class of samples [53]. Often, these two metrics are combined into their harmonic mean, which is called F-measure and is used as the single quality parameter.\nIn Fig. 11, the F-measure is plotted as a function of the two SVM parameters \u03b3 and \u03bd. As seen from this plot, the area where the classifier\u2019s performance is maximum is quite ample. This is good as it means that even selecting \u03b3 and \u03bd once for all at\ndesign stage, the performance of the SVM classifier is not expected to change much if the signal statistics changes or a new target user is considered. In other words, this relatively weak dependence on the parameters entails an intrinsic robustness for the classifier. For the results that follow we have used \u03b3 = 0.3 and \u03bd = 0.02.\nTwo last considerations are in order. The first relates to the PCA transformation \u03a5(\u00b7) and in particular to how many and which principal components have to be retained for the output feature vectors. In fact, as pointed out in [54], two options are possible to go from the CNN-extracted feature vector f to s. The first is to retain the S \u2264 F entries of the transformed vector (expressed in the PCA basis) that correspond to the principal components with highest variance, whereas a second option is to retain those with the smallest. Fig. 12 shows the F-measure of the OSVM classifier as a function of S for F = 40 (number of CNN-extracted features). From this plot we see that picking S < F in general provides better results and also that considering the principal components with lowest variance provides better results for this class of problems. This is in accordance with [54].\nThe last consideration regards the amount of feature vectors belonging to the target user that should be used for the OSVM training. Note that this number is related to the walking time required for a new subject to train his/her personal authentication system. To perform this analysis, a fixed number of cycles were randomly extracted from the whole target dataset and were used to train the OSVM. The remaining walking cycles were used as the positive test set. In Fig. 13 we show the F-measure as a function\nof this number of cycles. From these results, it follows that increasing the number of cycles beyond 1, 000 leads to little improvement. This number corresponds to about 15 minutes of walking activity, distributed among different acquisition sessions. Multiple sessions are recommended to account for some statistical variation due to wearing different clothes.\nOnce all the model\u2019s parameters are defined, the OSVM score can be analyzed. Let p\u03b8(h(s)) = p(h(s) | s \u2208 C\u03b8) be the estimated probability density function (pdf) of the OSVM score h(s) \u2208 R, provided that the walking cycle belongs to a user of class C\u03b8 with \u03b8 \u2208 {0, 1}. Empirical pdfs p\u03b8(h(s)) from our dataset are provided in Fig. 14."}, {"heading": "6. Multi-Stage Authentication", "text": "The so far discussed processing pipeline returns a score for each walking cycle. However, as seen in Fig. 14, when a score falls near the point where the two pdfs intersect, there is a high uncertainty about the identity of the user who generated it. In IDNet, we resolve this indetermination by jointly considering the scores from successive walking cycles. Let O = (o1, o2, . . . ) be a sequence of subsequent OSVM scores from the same subject, where oi = h(si) \u2208 R and i = 1, 2, . . . is the walking cycle index. From our previous analysis, oi can be thought of as a random process having probability density function p\u03b8(h(si)) = p\u03b8(oi), \u03b8 \u2208 {0, 1}, and our objective is to reliably estimate \u03b8 from the scores in O. Toward this, we assume that subsequent scores belong to the same user and that they are independent and identically distributed (i.i.d), i.e., they are independently drawn from p\u03b8(\u00b7), with \u03b8 unknown.\nFor the estimation of \u03b8 we use Wald\u2019s probability ratio test (SPRT) [55, 56]. We define the two hypotheses {H1 : \u03b8 = 1}, meaning that the sequence O belongs to the target user (class C1), and {H0 : \u03b8 = 0}, meaning that another user generated it (class C0). Hence, we assess which one of these is true through SPRT sequential binary testing. That is, we keep measuring new scores and use them to decrease our uncertainty about \u03b8. Considering n samples (o1, o2, . . . , on), the final decision takes on two values Dn = 0 or Dn = 1, where Dn = j, j \u2208 {0, 1} means that hypothesis Hj is accepted and therefore the alternative hypothesis is rejected. Owing to our assumptions (i.i.d. scores, generated by the same subject), for n scores On = (o1, o2, . . . , on) the joint pdf\nis:\np\u0303\u03b8(On) =\nn \u220f\nj=1\np\u03b8(oj), \u03b8 \u2208 {0, 1} . (18)\nDefining \u03bbj = p1(oj)/p0(oj), the likelihood ratio of the sequence O truncated at index n, On, is\np\u03031(On) p\u03030(On) =\nn \u220f\nj=1\np1(oj) p0(oj) =\nn \u220f\nj=1\n\u03bbj , (19)\nand applying the logarithm, we get:\n\u039bn = log\n(\np\u03031(On) p\u03030(On)\n)\n=\nn \u2211\nj=1\nlog (\u03bbj) . (20)\nIf we wait a further step n + 1 before making a decision, from Eq. (20) the new loglikelihood \u039bn+1 is conveniently obtained as \u039bn+1 = \u039bn + log(\u03bbn+1). The SPRT test starts from time 1, obtaining one-class OSVM scores o1, o2, . . . for each successive walking cycle. After n cycles, the cumulative log-likelihood ratio is \u039bn = \u039bn\u22121 + log(\u03bbn), with \u039b0 = 0. Two suitable thresholds A and B are defined and the test continues to the next cycle n+1 if A < \u039bn < B, H1 is accepted if \u039bn \u2265 B, whereas H0 is accepted if \u039bn \u2264 A. Moreover, defining \u03b1 as the probability of accepting H1 when H0 is true and \u03b2 that of accepting H0 when H1 is true, A and B can be approximated as: A = log(\u03b2/(1 \u2212 \u03b1)) and B = log((1\u2212 \u03b2)/\u03b1), see [55]."}, {"heading": "6.1. Experimental Results", "text": "The motion data fromK = 35 subjects was used to train the CNN feature extractor, with Nc = 40, F = 40 and S = 20. One user out of the remaining 15 was considered as the target user and 14 as the negatives for the final tests. The following results are obtained through a leave-one-out cross-validation approach for the sessions of the target user, i.e., out of twelve sessions, eleven are used for training and one for the final tests. The session that is left out is rotated and the final results are averaged across all trials. The authentication results of the multi-stage framework are shown in Fig. 15. False positive rates (i.e., a user is mistakenly authenticated as the target) and false negative ones (i.e., the target is not recognized) are smaller than 0.15% for an appropriate choice of the SPRT thresholds (\u03b1 and \u03b2). Also, a reliable authentication requires fewer than five walking cycles in 80% of the cases. This means that the framework is very accurate and at the same time fast. We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17]. A comparison with these approaches is very difficult to carry out due to the different datasets (e.g., number of subjects and walking time), acquisition settings (e.g., smartphone or sensors location). The reader can nevertheless refer to Section 4.2 for a fair comparison between our single-step classification framework and classical feature extraction techniques on our dataset.\nAs for our assumptions, in light of the small number of cycles required, it is reasonable to presume that the same subject generates the scores in O. For the i.i.d. assumption, we extended the decision framework to the first-order autoregressive model of [56, Chapter 3, p. 158], which allows tracking the correlation across successive cycles. However, this did not lead to any appreciable performance improvement and only implied a higher complexity. The reason is that scores are lightly correlated in time."}, {"heading": "7. Conclusions", "text": "In this paper we have proposed IDNet, a user authentication framework for inertial signals acquired from smartphones. Various schemes performing manual feature extraction and using the selected features for user classification have appeared in the recent literature. In sharp contrast with these, IDNet exploits convolutional neural networks, as they allow for an automatic feature engineering and have excellent generalization ca-\npabilities. These deep neural networks are then used as universal feature extractors to feed classification techniques, combining them with one-class support vector machines and a novel multi-stage decision algorithm. With our framework, the neural network is trained once for all and subsequently utilized for new users. The one-class classifier is solely trained using motion data from the target subject; it returns a score weighing the dissimilarity of newly acquired data with respect to that of the target. Subsequent scores are then accumulated through a multi-stage decision approach. Experimental results show the superiority of IDNet against prior work, leading to misclassification rates smaller than 0.15% in fewer than five walking cycles. Design choices and the optimization of the various processing blocks were discussed and compared against existing algorithms."}], "references": [{"title": "Inertial sensor-based gait recognition: A review", "author": ["S. Sprager", "M.B. Juric"], "venue": "Sensors 15 (9) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey of biometric gait recognition: Approaches", "author": ["D. Gafurov"], "venue": "security and challenges, in: Annual Norwegian computer science conference, Oslo, Norway", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Silhouette-based gait recognition via deterministic learning", "author": ["W. Zeng", "C. Wang", "F. Yang"], "venue": "Pattern Recognition 47 (11) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust arbitrary view gait recognition based on parametric 3D human body reconstruction and virtual posture synthesis", "author": ["J. Luo", "J. Tang", "T. Tjahjadi", "X. Xiao"], "venue": "Pattern Recognition 60 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Complete canonical correlation analysis with application to multi-view gait recognition", "author": ["X. Xing", "K. Wang", "T. Yan", "Z. Lv"], "venue": "Pattern Recognition 50 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Uncooperative gait recognition: Re-ranking based on sparse coding and multi-view hypergraph learning", "author": ["X. Chen", "J. Xu"], "venue": "Pattern Recognition 53 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust view-invariant multiscale gait recognition", "author": ["S.D. Choudhury", "T. Tjahjadi"], "venue": "Pattern Recognition 48 (3) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Gait Analysis: An Introduction", "author": ["M.W. Whittle"], "venue": "4th ed., Elsevier: Edinburgh, UK", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating and overcoming the challenges in utilizing smart mobile phones and standalone accelerometer for gait analysis", "author": ["H. Chan", "H. Zheng", "H. Wang", "R. Sterritt"], "venue": "in: IET Irish Signals and Systems Conference ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "CNN Features Off-the- Shelf: An Astounding Baseline for Recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition Workshops, Columbus, Ohio, US", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Pattern Recognition and Machine Learning", "author": ["C. Bishop"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Gait identification using accelerometer on mobile phone", "author": ["H.M. Thang", "V.Q. Viet", "N.D. Thuc", "D. Choi"], "venue": "in: International Conference on Control, Automation and Information Sciences (ICCAIS), Saigon, Vietnam", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Authentication of smartphone users based on the way they walk using k-nn algorithm", "author": ["C. Nickel", "T. Wirtl", "C. Busch"], "venue": "in: International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), Piraeus-Athens, Greece", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Influence of Holding Smart Phone for Acceleration-Based Gait Authentication", "author": ["Y. Watanabe"], "venue": "in: International Conference on Emerging Security Technologies (EST), Houston, Texas, US", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Biometric gait recognition based on wireless acceleration sensor using k-nearest neighbor classification", "author": ["S. Choi", "I.H. Youn", "R. LeMay", "S. Burns", "J.H. Youn"], "venue": "in: International Conference on Computing, Networking and Communications (ICNC), Honolulu, Hawaii, US", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Smartphone based user verification leveraging gait recognition for mobile healthcare systems", "author": ["Y. Ren", "Y. Chen", "M.C. Chuah", "J. Yang"], "venue": "in: IEEE Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks (SECON), New Orleans, Louisiana, US", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "An Efficient HOS-Based Gait Authentication of Accelerometer Data", "author": ["S. Sprager", "M.B. Juric"], "venue": "IEEE Transactions on Information Forensics and Security 10 (7) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Smart mobile phone based gait assessment of patients with low back pain", "author": ["H. Chan", "H. Zheng", "H. Wang", "R. Sterritt", "D. Newell"], "venue": "in: Ninth International Conference on Natural Computation (ICNC), San Diego, California, US", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Gait analysis by using tri-axial accelerometer of smart phones", "author": ["G.-S. Huang", "C.C. Wu", "J. Lin"], "venue": "in: International Conference on Computerized Healthcare (ICCH), Hong Kong, China", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Scenario test of accelerometerbased biometric gait recognition", "author": ["C. Nickel", "M.O. Derawi", "P. Bours", "C. Busch"], "venue": "in: International Workshop on Security and Communication Networks (IWSCN), Gj\u00f8vik, Norway", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Using hidden markov models for accelerometer-based biometric gait recognition", "author": ["C. Nickel", "C. Busch", "S. Rangarajan", "M. Mobius"], "venue": "in: IEEE 7th International Colloquium on Signal Processing and its Applications (CSPA), Penang, Malaysia", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Rotation invariant feature extraction from 3-D acceleration signals", "author": ["T. Kobayashi", "K. Hasida", "N. Otsu"], "venue": "in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Prague, Czech Republic", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J.C. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation 13 (7) ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Walking patterns of normal men", "author": ["M.P. Murray", "A.B. Drought", "R.C. Kory"], "venue": "The Journal of Bone & Joint Surgery 46 (2) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1964}, {"title": "Gait as a total pattern of movement: Including a bibliography on gait", "author": ["M.P. Murray"], "venue": "American Journal of Physical Medicine & Rehabilitation", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1967}, {"title": "M", "author": ["T. Nixon"], "venue": "S. ans Tieniu, C. Rama, Human identification based on gait, Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Cell phone-based biometric identification", "author": ["J.R. Kwapisz", "G.M. Weiss", "S.A. Moore"], "venue": "in: Fourth IEEE International Conference on Biometrics: Theory Applications and Systems (BTAS)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Identifying users of portable devices from gait pattern with accelerometers", "author": ["J. Mantyjarvi", "M. Lindholm", "E. Vildjiounaite", "S.M. Makela", "H.A. Ailisto"], "venue": "in: IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Philadelphia, Pennsylvania, US", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Unobtrusive user-authentication on mobile phones using biometric gait recognition", "author": ["M.O. Derawi", "C. Nickel", "P. Bours", "C. Busch"], "venue": "in: 6th International Conference on Intelligent Information Hiding and Multimedia Signal Processing (IIH-MSP), Darmstadt, Germany", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Exact indexing of dynamic time warping", "author": ["E. Keogh", "C. Ratanamahatana"], "venue": "Knowledge and Information Systems 7 (3) ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Gait-id on the move: Pace independent human identification using cell phone accelerometer dynamics", "author": ["F. Juefei-Xu", "C. Bhagavatula", "A. Jaech", "U. Prasad", "M. Savvides"], "venue": "in: Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS), Washington DC, US", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "The possibility of normal gait analysis based on a smart phone for healthcare", "author": ["S. Jiang", "B. Zhang", "G. Zou", "D. Wei"], "venue": "in: IEEE International Conference on Green Computing and Communications (GreenCom), Internet of Things (iThings), and Cyber, Physical and Social Computing (CPSCom), Beijing, China", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Sensor orientation invariant mobile gait biometrics", "author": ["Y. Zhong", "Y. Deng"], "venue": "in: IEEE International Joint Conference on Biometrics (IJCB), Clearwater, FL, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Largescale Video Classification with Convolutional Neural Networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Columbus, Ohio, US", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "The largest inertial sensor-based gait database and performance evaluation of gait-based personal authentication", "author": ["T.T. Ngo", "Y. Makihara", "H. Nagahara", "Y. Mukaigawa", "Y. Yagi"], "venue": "Pattern Recognition 47 (1) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Personalization and user verification in wearable systems using biometric walking patterns", "author": ["P. Casale", "O. Pujol", "P. Radeva"], "venue": "Personal and Ubiquitous Computing 16 (5) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "T", "author": ["J. Tilmanne", "R. Sebbe"], "venue": "Dutoit, A database for stylistic human gait modeling and synthesis ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "D", "author": ["J. Frank", "S. Mannor"], "venue": "Precup, Data sets: Mobile phone gait recognition data ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "The use of fast fourier transform for the estimation of power spectra: A method based on time averaging over short", "author": ["P.D. Welch"], "venue": "modified periodograms, IEEE Transactions on Audio and Electroacoustics 15 (2) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1967}, {"title": "Pem-id: Identifying people by gait-matching using cameras and wearable accelerometers", "author": ["T. Teixeira", "D. Jung", "G. Dublon", "A. Savvides"], "venue": "in: ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC), Como, Italy", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Which Way Am I Facing: Inferring Horizontal Device Orientation from an Accelerometer Signal", "author": ["K. Kunze", "P. Lukowicz", "K. Partridge", "B. Begole"], "venue": "in: IEEE International Symposium on Wearable Computers, Linz, Austria", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Heading Estimation for Indoor Pedestrian Navigation Using a Smartphone in the Pocket", "author": ["Z.-A. Deng", "G. Wang", "Y. Hu", "D. Wu"], "venue": "MDPI Sensors 15 (9) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "The Use and Interpretation of Principal Component Analysis in Applied Research", "author": ["C.R. Rao"], "venue": "Sankhy\u0101: The Indian Journal of Statistics 26 (4) ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1964}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series, in: The Handbook of Brain Theory and Neural Networks, MIT Press", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in Neural Information Processing Systems 25", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of pooling operations in convolutional architectures for object recognition", "author": ["D. Scherer", "A. M\u00fcller", "S. Behnke"], "venue": "in: 20th International Conference on Artificial Neural Networks (ICANN), Thessaloniki, Greece", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Computer Intensive Methods in Control and Signal Processing: The Curse of Dimensionality", "author": ["R. Hanka", "T.P. Harte"], "venue": "Birkh\u00e4user Boston", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1993}, {"title": "Bayesian network classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning 29 (2) ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1997}, {"title": "Nearest neighbor pattern classification", "author": ["T. Cover", "P. Hart"], "venue": "IEEE Transactions on Information Theory 13 (1) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1967}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning 20 (3) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1995}, {"title": "J", "author": ["B. Sch\u00f6lkopf", "R.C. Williamson", "A.J. Smola", "J. Shawe-Taylor"], "venue": "C. Platt, et al., Support vector method for novelty detection, Neural Information Processing Systems (NIPS) 12 ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimizing F-Measure with Support Vector Machines", "author": ["D.R. Musicant", "V. Kumar", "A. Ozgur"], "venue": "in: 16-th International FLAIRS Conference, FLAIRS, St. Augustine, Florida, US", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2003}, {"title": "Artificial Neural Networks and Neural Information Processing", "author": ["D.M.J. Tax", "K.R. M\u00fcller"], "venue": "Springer, Berlin, Heidelberg", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2003}, {"title": "Sequential analysis", "author": ["A. Wald"], "venue": "Dover, New York, NY, US", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1947}, {"title": "Sequential Analysis Hypothesis Testing and Changepoint Detection", "author": ["A. Tartakovsky", "I. Nikiforov", "M. Basseville"], "venue": "CRC Press", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A great deal of work has been carried out on gait recognition in the last decade [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "In general, biometric gait recognition can be grouped into three main categories: 1) computer vision based, 2) floor sensor based and 3) wearable sensor based [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 3, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 4, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 5, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 6, "context": "Most of the recent work belongs to the first category, where image and video analysis are performed to infer the user identity [3, 4, 5, 6, 7].", "startOffset": 127, "endOffset": 142}, {"referenceID": 7, "context": "As shown in [8, 9], modern phones possess highly accurate inertial sensors, which allow for non-obtrusive gait biometrics.", "startOffset": 12, "endOffset": 18}, {"referenceID": 8, "context": "As shown in [8, 9], modern phones possess highly accurate inertial sensors, which allow for non-obtrusive gait biometrics.", "startOffset": 12, "endOffset": 18}, {"referenceID": 9, "context": "IDNet leverages deep Convolutional Neural Networks (CNN) [10] and tools from machine learning, such as Support Vector Machines (SVM) [11], combining them in an innovative fashion.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "IDNet leverages deep Convolutional Neural Networks (CNN) [10] and tools from machine learning, such as Support Vector Machines (SVM) [11], combining them in an innovative fashion.", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 12, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 13, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 14, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 15, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 16, "context": "As shown in Section 4, our solution authenticates the target user with high accuracy and outperforms state-of-the-art techniques such as [12, 13, 14, 15, 16, 17].", "startOffset": 137, "endOffset": 161}, {"referenceID": 14, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 15, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 17, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 11, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 12, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 18, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 19, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 20, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 143, "endOffset": 175}, {"referenceID": 21, "context": "As opposed to making motion data orientation independent, previous papers either use data acquired from a sensor in a known and fixed position [15, 16, 18, 12, 13, 19, 20, 21], or use orientation independent features at the cost of losing information about the direction of the forces [22].", "startOffset": 285, "endOffset": 289}, {"referenceID": 13, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 14, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 17, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 12, "context": "Note that with CNNs, statistical features are automatically extracted as part of the CNN training phase (automatic feature engineering) as opposed to the selection of predefined and often arbitrary features, as commonly done in the literature [14, 15, 18, 13].", "startOffset": 243, "endOffset": 259}, {"referenceID": 22, "context": "\u2022 The combination of CNN-extracted features with a one-class SVM classifier [23], which is solely trained on the target subject, see Section 5.", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Murray compared normal gait parameters against those from pathologic gaits [25] and showed that gait is unique to each individual.", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "Most recent works are based on computer vision [26, 2].", "startOffset": 47, "endOffset": 54}, {"referenceID": 1, "context": "Most recent works are based on computer vision [26, 2].", "startOffset": 47, "endOffset": 54}, {"referenceID": 6, "context": ") are of special interest [7].", "startOffset": 26, "endOffset": 29}, {"referenceID": 3, "context": "Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 5, "context": "Many new approaches have been studied to improve recognition performance, such as 3D body estimation [4], complete canonical correlation analysis [5], sparse coding and hypergraph learning [6].", "startOffset": 189, "endOffset": 192}, {"referenceID": 26, "context": ", for task identification [27].", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "A thorough review of the latest developments in this area can be found in Sprager\u2019s work [1].", "startOffset": 89, "endOffset": 92}, {"referenceID": 27, "context": "[28] were the first to look at this problem and they did it through accelerometer data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In [29], Derawi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Dynamic Time Warping (DTW) [30] was used as the distance measure, to ensure robustness against non-linear temporal shifts.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "This scheme was also tested in [20], where majority voting and cyclic rotation were compared as inference rules.", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "In a further paper [21], Hidden", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "Either gait cycles extraction [31] or fixed windows lengths [13] are possible signal segmentation methods.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "Either gait cycles extraction [31] or fixed windows lengths [13] are possible signal segmentation methods.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "However, more advanced features are required for better results, like cepstral coefficients, which are widely used for speech recognition [13], or features extracted through frequency analysis, i.", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": ", using Fourier [12] or wavelet transforms [31].", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": ", using Fourier [12] or wavelet transforms [31].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 12, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 16, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 99, "endOffset": 115}, {"referenceID": 30, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 147, "endOffset": 159}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 147, "endOffset": 159}, {"referenceID": 13, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 147, "endOffset": 159}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 13, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 191, "endOffset": 199}, {"referenceID": 17, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 230, "endOffset": 238}, {"referenceID": 13, "context": "Supervised algorithms are typically used for classification, including k-Nearest Neighbours (k-NN) [15, 13, 18, 17], Support Vector Machines (SVM) [31, 18, 14], Multi Layer Perceptrons (MLP) [18, 14] and Classification Trees (CT) [18, 14].", "startOffset": 230, "endOffset": 238}, {"referenceID": 18, "context": "showed that signals acquired by a waist-worn device on a patient with cervical disc herniation differed before and after the surgery [19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "In [18], classification algorithms were used to discriminate a group of subjects with non-specific chronic low back pain from healthy subjects.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": ", correlation matrices of Fourier transforms [22] or gait dynamic images [33]).", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": ", correlation matrices of Fourier transforms [22] or gait dynamic images [33]).", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "The second relies on the transformation of inertial signals [14], projecting them into a new orientation invariant three-dimensional reference system,", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "These have been successfully used by the video processing community [34] but to the best of our knowledge have never been exploited for the analysis of inertial data from wearable devices.", "startOffset": 68, "endOffset": 72}, {"referenceID": 34, "context": "The largest one was acquired by the Institute of Scientific and Industrial Research (ISIR) at Osaka University (OU) [35].", "startOffset": 116, "endOffset": 120}, {"referenceID": 35, "context": "collected accelerometer data from a smartphone positioned in the chest pocket from 22 users walking over a predefined path [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 36, "context": "In [37], a motion capture suit was used to acquire data from 40 subjects walking in a small area at different speeds and with direction changes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "collected data from a mobile phone in the pocket of 20 individuals at McGill University, performing two separate 15 minute walks on two different days [38].", "startOffset": 151, "endOffset": 155}, {"referenceID": 38, "context": "2, we plot the power of accelerometer and gyroscope signals at different frequencies through the Welch\u2019s method [39], considering a full walking trace and setting the Hanning window length to 1 s, with half window overlap.", "startOffset": 112, "endOffset": 116}, {"referenceID": 15, "context": "To do so, inspired by [16] we first pass amag(i) through a low-pass filter with cutoff frequency", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "Thus, we detect the first minimum of this filtered signal, which corresponds to the heel strike [40], and the corresponding index is called \u0129.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "A template matching approach exploiting a similar rationale was used in [16], where the authors employed the Pearson product-moment correlation coefficient between template and amag(i).", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "The main differences between [16] and our approach are: we obtain the template T in a neighborhood of i, using a fixed number of samples Ns, whereas they take the samples between two adjacent minima of \u03c6(i) (which may then differ in size for different cycles).", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "In previous work [16], the template is instead kept unchanged up to a point when minima cannot be longer detected, and a new template is to be obtained.", "startOffset": 17, "endOffset": 21}, {"referenceID": 40, "context": "To this end, we adopt a technique similar to those of [41, 42].", "startOffset": 54, "endOffset": 62}, {"referenceID": 41, "context": "To this end, we adopt a technique similar to those of [41, 42].", "startOffset": 54, "endOffset": 62}, {"referenceID": 40, "context": ", it is parallel to the direction of motion, as it was also observed and verified in previous research [41].", "startOffset": 103, "endOffset": 107}, {"referenceID": 42, "context": "This is done by applying the Principal Component Analysis (PCA) [43] on the projected points, which finds the direction along which the variance of the measurements is maximized.", "startOffset": 64, "endOffset": 68}, {"referenceID": 43, "context": "For more details the reader is referred to [44].", "startOffset": 43, "endOffset": 47}, {"referenceID": 44, "context": "CNNs have been proven to be excellent feature extractors for images [45] and here we prove their effectiveness for motion data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "Max pooling is applied to the output of CL2 to further reduce its dimensionality and increase the spatial invariance of features [46].", "startOffset": 129, "endOffset": 133}, {"referenceID": 46, "context": "In general, a too small F can lead to poor classification results; too many features, instead, would make the state space too big to be effectively dealt with (curse of dimensionality) [47].", "startOffset": 185, "endOffset": 189}, {"referenceID": 47, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 187, "endOffset": 191}, {"referenceID": 48, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 210, "endOffset": 214}, {"referenceID": 49, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 243, "endOffset": 247}, {"referenceID": 50, "context": "8, the accuracy is plotted against Nc for our CNN-based approach and four selected authentication algorithms from the literature, featuring classifiers based on Classification Trees (CT) [48], Naive Bayes (NB) [49], k-Nearest Neighbors (k-NN) [50] and Support Vector Machines (SVM) [51].", "startOffset": 282, "endOffset": 286}, {"referenceID": 14, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 12, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 30, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 17, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 13, "context": "These techniques were used in a large number of papers including [15, 13, 31, 18, 14].", "startOffset": 65, "endOffset": 85}, {"referenceID": 51, "context": "The strategy proposed by Sch\u00f6lkopf is to map the data into the feature space of a kernel, and to separate them from the origin with maximum margin [52].", "startOffset": 147, "endOffset": 151}, {"referenceID": 50, "context": "The corresponding minimization problem is similar to that of the original SVM formulation [51].", "startOffset": 90, "endOffset": 94}, {"referenceID": 51, "context": "\u03bd \u2208 (0, 1) is one of the most important parameters and sets an upper bound on the fraction of outliers and a lower bound on the fraction of Support Vectors (SV) [52].", "startOffset": 161, "endOffset": 165}, {"referenceID": 52, "context": ", the fraction of patterns identified of the target class that in fact belong to the target user, while the recall corresponds to the fraction of target patterns that are correctly classified out of the entire positive class of samples [53].", "startOffset": 236, "endOffset": 240}, {"referenceID": 53, "context": "In fact, as pointed out in [54], two options are possible to go from the CNN-extracted feature vector f to s.", "startOffset": 27, "endOffset": 31}, {"referenceID": 53, "context": "This is in accordance with [54].", "startOffset": 27, "endOffset": 31}, {"referenceID": 54, "context": "For the estimation of \u03b8 we use Wald\u2019s probability ratio test (SPRT) [55, 56].", "startOffset": 68, "endOffset": 76}, {"referenceID": 55, "context": "For the estimation of \u03b8 we use Wald\u2019s probability ratio test (SPRT) [55, 56].", "startOffset": 68, "endOffset": 76}, {"referenceID": 54, "context": "Moreover, defining \u03b1 as the probability of accepting H1 when H0 is true and \u03b2 that of accepting H0 when H1 is true, A and B can be approximated as: A = log(\u03b2/(1 \u2212 \u03b1)) and B = log((1\u2212 \u03b2)/\u03b1), see [55].", "startOffset": 194, "endOffset": 198}, {"referenceID": 11, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 12, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 13, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 14, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 15, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}, {"referenceID": 16, "context": "We remark that the best authentication results that were obtained in previous papers lead to error rates ranging from 5 to 15% [12, 13, 14, 15, 16, 17].", "startOffset": 127, "endOffset": 151}], "year": 2016, "abstractText": "Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user\u2019s trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance.", "creator": "LaTeX with hyperref package"}}}