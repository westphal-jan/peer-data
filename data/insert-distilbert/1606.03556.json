{"id": "1606.03556", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "abstract": "we conduct large - scale studies on ` human attention'in mobile visual question answering ( vqa ) to understand where humans choose to look to answer questions about images. we design computers and test multiple passing game - inspired novel attention - annotation interfaces that require the subject to sharpen smooth regions of a blurred image to answer a question. thus, we introduce the vqa - hat ( human attention ) dataset. we evaluate attention maps generated by state - of - the - art vqa models against human attention both qualitatively ( via visualizations ) and quantitatively ( via rank - order correlation ). alternatively we find that depending on the hardware implementation used, machine - path generated attention point maps are either \\ n emph { negatively correlated } with human attention or have positive correlation worse than task - independent saliency. overall, our experiments paint a bleak picture for the larger current generation of attention models in vqa.", "histories": [["v1", "Sat, 11 Jun 2016 05:41:10 GMT  (8091kb,D)", "http://arxiv.org/abs/1606.03556v1", "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016"], ["v2", "Fri, 17 Jun 2016 04:39:01 GMT  (9100kb,D)", "http://arxiv.org/abs/1606.03556v2", "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016"]], "COMMENTS": "9 pages, 6 figures, 3 tables; Under review at EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["abhishek das", "harsh agrawal", "c lawrence zitnick", "devi parikh", "dhruv batra"], "accepted": true, "id": "1606.03556"}, "pdf": {"name": "1606.03556.pdf", "metadata": {"source": "CRF", "title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "authors": ["Abhishek Das", "Harsh Agrawal", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "emails": ["dbatra}@vt.edu", "zitnick@fb.com"], "sections": [{"heading": "1 Introduction", "text": "It helps to pay attention. Humans have the ability to quickly perceive a scene by selectively attending to parts of the image instead of processing the whole scene in its entirety (Rensink, 2000). Inspired by human attention, a recent trend in computer vision and deep learning is to build computational models of attention. Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine translation (Bahdanau et al., 2014; Firat et al., 2016), object recognition (Ba et al., 2015; Mnih et al., 2014;\n\u2217 Denotes equal contribution.\nSermanet et al., 2014), image captioning (Xu et al., 2015; Cho et al., 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016). In this work, we study attention for the task of Visual Question Answering (VQA). Unlike image captioning, where a coarse understanding of an image is often sufficient for producing generic descriptions (Devlin et al., 2015), visual questions selectively target different areas of an image including background details and underlying context. This suggests that a VQA model may benefit from an explicit or implicit attention mechanism to answer a question correctly. In this work, we are interested in the following questions: 1) Which image regions do humans choose to look at in order to answer questions about images? 2) Do deep VQA models with attention mechanisms attend to the same regions as humans? ar X iv :1\n60 6.\n03 55\n6v 1\n[ cs\n.C V\n] 1\n1 Ju\nn 20\nWe design and conduct studies to collect \u201chuman attention maps\u201d. Figure 1 shows human attention maps on the same image for two different questions. When asked \u2018What type is the surface?\u2019, humans choose to look at the floor, while attention for \u2018Which game is being played?\u2019 is concentrated around the player and racket.\nThese human attention maps can be used both for evaluating machine-generated attention maps and for explicitly training attention-based models.\nContributions. First, we design and test multiple game-inspired novel interfaces for collecting human attention maps of where humans choose to look to answer questions from the large-scale VQA dataset (Antol et al., 2015); this VQA-HAT (Human ATtention) dataset will be released publicly. Second, we perform qualitative and quantitative comparison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015; Lu et al., 2016) and a task-independent saliency baseline (Judd et al., 2009) against our human attention maps through visualizations and rank-order correlation. We find that machine-generated attention maps from the most accurate VQA model have a mean rank-correlation of 0.26 with human attention maps, which is worse than task-independent saliency maps that have a mean rank-correlation of 0.49. It is well understood that task-independent saliency maps have a \u2018center bias\u2019 (Tatler, 2007; Judd et al., 2009). After we control for this center bias in our human attention maps, we find that the correlation of task-independent saliency is poor (as expected), while trends for machine-generated VQA-attention maps remain the same (which is promising)."}, {"heading": "2 Related Work", "text": "Our work draws on recent work in attention-based VQA and human studies in saliency prediction. We work with the free-form and open-ended VQA dataset released by (Antol et al., 2015). VQA Models. Attention-based models for VQA typically use convolutional neural networks to highlight relevant regions of image given a question. Stacked Attention Networks (SAN) proposed in (Yang et al., 2015) use LSTM encodings of question words to produce a spatial attention distribution over the convolutional layer features of the image. Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge1 as of the time of this submission. Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (Andreas et al., 2016). Note that all these works are unsupervised attention models, where \u201cattention\u201d is simply an intermediate variable (a spatial distribution) that is produced by the model to optimize downstream loss (VQA cross-entropy). The fact that some (it\u2019s unclear how many) of these spatial distributions end up being interpretable is simply fortuitous. In contrast, we study where humans choose to look to answer visual questions. These human attention maps can be used to evaluate unsupervised maps. Human Studies. There\u2019s a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967). Eye tracking\n1http://visualqa.org/challenge.html\ndata to study natural visual exploration (Jiang et al., 2014; Judd et al., 2009) is useful but difficult and expensive to collect on a large scale. (Jiang et al., 2015) established mouse tracking as an accurate approach to collecting attention maps. They collected large-scale attention annotations for MS COCO (Lin et al., 2014) on Amazon Mechanical Turk (AMT). While (Jiang et al., 2015) studies natural exploration and collects task-independent human annotations by asking subjects to freely move the mouse cursor to anywhere they wanted to look on a blurred image, our approach is task-driven.\nSpecifically, as described in 3, we collect ground truth attention annotations by instructing subjects to sharpen parts of a blurred image that are important for answering the questions accurately. Section 4 covers evaluation of unsupervised attention maps generated by VQA models against our human attention maps."}, {"heading": "3 VQA-HAT (Human ATtention) Dataset", "text": "We design and test multiple game-inspired novel interfaces for conducting large-scale human studies on AMT. Our basic interface design consists of a \u201cdeblurring\u201d exercise for answering visual questions. Specifically, we present subjects with a blurred image and a question about the image, and ask subjects to sharpen regions of the image that will help them answer the question correctly, in a smooth, click-and-drag, \u2018coloring\u2019 motion with the mouse. The sharpening is gradual: successively scrubbing the same region progressively sharpens it. Figure 3 shows intermediate steps in our attention annotation interface, from a completely blurry image to a deblurred attention map."}, {"heading": "3.1 Attention Annotation Interface", "text": "Our interface starts by showing a low-resolution blurry version of the image. This is to convey a partial \u2018holistic\u2019 understanding of the scene to the subjects so they may intelligently choose which regions to sharpen. Gradual sharpening with strokes was aimed to capture initial exploration as they tried to get a better sense of the scene, and eventually focussed sharpening to answer the question. Next we describe the three variants of our attention annotation interface that we experimented with."}, {"heading": "3.1.1 Blurred Image without Answer", "text": "In our first interface, subjects were shown a blurred image and a question without the answer, and were asked to deblur regions and enter the answer. We found that this interface sometimes resulted in \u2018exploratory attention\u2019, where the subject lightly sharpens large regions of an image to find salient regions that eventually lead them to the answer. However, subjects often ended up with \u2018incomplete\u2019 attention maps since they did not see the high-resolution image and the answer, so they did not know when to stop deblurring or exploring. For instance, for an image with 3 players playing a sport, if the question is \u201cHow many players are visible in the image?\u201d, the subject might sharpen a region that seems to have the players, count the 2 players in there and answer 2, and completely miss another region of the image that had 1 more. The resulting attention map in this case is incomplete since there are 3 players in the image. This effect of incomplete human attention maps was seen in counting (\u201cHow many ...\u201d) and binary (\u201cIs there ...\u201d) types of questions, and as a result, the answers to these were often incorrect."}, {"heading": "3.1.2 Blurred Image with Answer", "text": "In our second interface, subjects were shown the correct answer in addition to the question and blurred image. They were asked to sharpen as few regions as possible such that someone can answer the question just by looking at the blurred image with sharpened regions. This interface is shown in Figure 4b. Providing the answer fixed the failure cases from the 1st interface, i.e. for counting and binary questions, since the subjects now knew the answer, they continued to explore till they found the answer region in the image."}, {"heading": "3.1.3 Blurred and Original Image with Answer", "text": "To encourage exploitation instead of exploration, in our third interface, subjects were shown the question-answer pair and full-resolution original image. In principle, seeing the original (fullresolution) image, the question, and answer provides most information to subjects, thus enabling them to provide the most \u2018accurate\u2019 attention maps. However, this task turns out to be fairly counterintuitive \u2013 subjects are shown full-resolution images and the answer, and asked to imagine a scenario where someone else has to answer the question without looking at the original image. Figure 4 shows screen-captures of the 3 attention annotation interfaces."}, {"heading": "3.2 Dataset Evaluation", "text": "We ran pilot studies on AMT to experiment with the above described three interfaces. In order to quantitatively evaluate the interfaces, we conducted a second human study where (a second set of) subjects where shown the attention-sharpened images generated from each of the attention interfaces from the first experiment and asked to answer the question. The intuition behind this experiment is that if the attention map revealed too little information, this second set of subjects would answer the question incorrectly. Table 1 shows VQA accuracies of the answers given by human subjects under these 3 interfaces. We can see that the \u201cBlurred Image with Answer\u201d interface (section 3.1.2) gives the highest accuracy on evaluation by humans. Since the payments structure on AMT encourage completing tasks as quickly as possible, this implicitly incentivizes subjects to deblur as few regions\nas possible, and our human study shows that humans can still answer questions. Thus, overall we achieve a balance between highlighting too little or too much.\nWe collected human attention maps for 58475 train (out of 248349 total) and 1374 val (out of 121512 total) question-image pairs in the VQA dataset. Overall, we conducted approximately 20000 Human Intelligence Tasks (HITs) on AMT, among 800 unique workers. Figure 2 shows examples of collected human attention maps. This VQA-HAT dataset will be released publicly.\nTo visualize the collected dataset, we cluster the human attention maps and visualize the average attention map and example questions falling in each of them for 6 selected clusters in Figure 5."}, {"heading": "4 Human Attention Maps vs Unsupervised Attention Models", "text": "Now that we have collected these human attention maps, we can ask the following question \u2013 do unsupervised attention models learn to predict attention maps that are similar to human attention maps? To\nrephrase, do neural networks look at the same regions as humans to answer a visual question? VQA Attention Models. We evaluate maps generated by the following unsupervised models: \u2022 Stacked Attention Network (SAN) (Yang et al.,\n2015) with a single attention layer (SAN-1)2. \u2022 Hierarchical Co-Attention Network\n(HieCoAtt) (Lu et al., 2016) with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps3.\nComparison Metric: Rank Correlation. We first scale both the machine-generated and human attention maps to 14x14, normalize them spatially and rank the pixels according to their spatial attention, and then compute correlation between these two ranked lists. We choose an order-based metric so as to make the evaluation invariant to absolute spatial probability values which can be made peaky or diffuse by tweaking a \u2018temperature\u2019 parameter.\nTable 2 shows rank-order correlation averaged over all image-question pairs on the validation set. We compare with random attention maps and taskindependent saliency maps generated by a model trained to predict human eye fixation locations\n2Performance of our re-implementation is comparable to that reported in the original paper.\n3Code available at https://github.com/ jiasenlu/HieCoAttenVQA\nwhere subjects are asked to freely view an image for 3 seconds (Judd et al., 2009). Interestingly, SAN-1 is negatively correlated to human attention maps. HieCoAtt attention maps are positively correlated with human attention maps, but not as strongly as task-independent Judd saliency maps. Our findings lead to two take-away messages with significant potential impact on future research in this active field. First, current VQA attention models do not seem to be \u2018looking\u2019 at the same regions as humans to produce an answer. Second, as attentionbased VQA models become more accurate (58.9% SAN \u2192 62.1% HieCoAtt), they seem to be better correlated with humans in terms of where they look. Our dataset will allow for a more thorough validation of this observation as future attention-based VQA models are proposed. Figure 6 shows examples of human attention and machine-generated attention maps with corresponding rank-correlation coefficients. To put these numbers in perspective, we computed inter-human agreement on the validation set by collecting 3 human attention maps per image-question pair and computing mean rank-correlation, which is 0.62289. Lastly, all reported correlation values are averaged over 3 trials by adding random noise (order of 10\u221214) to the human attention maps to account for ranking variations in case of uniformly weighted regions. Center Bias. Judd saliency maps aim to predict human eye fixations during natural visual exploration. These tend to have a strong center bias (Tatler, 2007; Judd et al., 2009). Although our human attention maps dataset is not an eye tracking study, the center bias still exists albeit not as severe. One potential source of this center bias is the fact that the VQA dataset was human-generated by subjects looking at the images. Thus, salient objects in the center of the image are likely be potential subjects of the questions. We compute rank-correlation of a synthetically generated central attention map with Judd saliency and human attention maps. Judd saliency maps have a mean rank-correlation of 0.87659 and human attention maps have a mean rank-correlation of 0.45781 on the validation set. To eliminate the effect of center bias in this evaluation, we removed human attention maps that have a positive rank-correlation with the center attention\nmap. We compute rank-correlation of machinegenerated attention with human attention on this reduced set. See Table 3. Mean correlation goes down significantly for Judd saliency maps since they have a strong center bias. Relative trends among SAN1 & HieCoAtt are similar to those over the whole validation set (reported in Table 2). HieCoAtt-Q now has a higher correlation with human attention maps than Judd saliency. This demonstrates that discounting the center bias, VQA-specific machine attention maps correlate better with VQA-specific human attention maps than task independent machine saliency maps."}, {"heading": "5 Conclusion & Discussion", "text": "We introduce and release the VQA-HAT dataset. This dataset can be used to evaluate attention maps generated in an unsupervised manner by attentionbased VQA models, or to explicitly train models with attention supervision for VQA. We quantify whether current attention-based VQA models are \u2018looking\u2019 at the same regions of the image as humans do to produce an answer. Necessary vs Sufficient Maps. Are human attention maps \u2018necessary\u2019 and/or \u2018sufficient\u2019? If regions highlighted by the human attention maps are sufficient to answer the question accurately, then so is any region that is a superset. For example, if attention mass is concentrated on a \u2018cat\u2019 for \u2018What animal is present in the picture?\u2019, then an attention map that\nassigns weights to any arbitrary-sized region that includes the \u2018cat\u2019 is sufficient as well. On the contrary, a necessary and sufficient attention map would be the smallest visual region sufficient for answering the question accurately. It is an ill-posed problem to define a necessary attention map in the space of pixels; random pixels can be blacked out and chances are that humans would still be able to answer the question given the resulting subset attention map. Our work thus poses an interesting question for future work \u2013 what is the right semantic space in which it is meaningful to talk about necessary and sufficient attention maps for humans?"}, {"heading": "6 Acknowledgements", "text": "We thank Jiasen Lu and Ramakrishna Vedantam for helpful suggestions and discussions. This work was supported in part by the following: National Science Foundation CAREER awards to DB and DP, Army Research Office YIP awards to DB and DP, ICTAS Junior Faculty awards to DB and DP, Army Research Lab grant W911NF-15-2-0080 to DP and DB, Office of Naval Research grant N00014-14-10679 to DB, Paul G. Allen Family Foundation Allen Distinguished Investigator award to DP, Google Faculty Research award to DP and DB, AWS in Education Research grant to DB, and NVIDIA GPU donation to DB."}], "references": [{"title": "Learning to compose neural networks for question answering", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": "CoRR, abs/1601.01705", "citeRegEx": "Andreas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Multiple Object Recognition With Visual Attention", "author": ["Ba et al.2015] Jimmy Lei Ba", "Volodymyr Mnih", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Describing multimedia content using attention-based encoder-decoder networks. CoRR, abs/1507.01053", "author": ["Cho et al.2015] KyungHyun Cho", "Aaron C. Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Exploring Nearest Neighbor Approaches for Image Captioning", "author": ["Devlin et al.2015] Jacob Devlin", "Saurabh Gupta", "Ross Girshick", "Margaret Mitchell", "C. Lawrence Zitnick"], "venue": "arXiv preprint", "citeRegEx": "Devlin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "What do we perceive in a glance of a real-world scene", "author": ["Fei-Fei et al.2007] Li Fei-Fei", "Asha Iyer", "Christof Koch", "Pietro Perona"], "venue": "Journal of Vision,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1601.01073", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Salicon: Saliency in context", "author": ["Jiang et al.2015] Ming Jiang", "Shengsheng Huang", "Juanyong Duan", "Qi Zhao"], "venue": "In CVPR, June", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Learning to predict where humans look", "author": ["Judd et al.2009] Tilke Judd", "Krista Ehinger", "Fr\u00e9do Durand", "Antonio Torralba"], "venue": "In ICCV. 2,", "citeRegEx": "Judd et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Judd et al\\.", "year": 2009}, {"title": "Hierarchical Co-Attention for Visual Question Answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "ArXiv e-prints, May. 1,", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Recurrent Models of Visual Attention", "author": ["Mnih et al.2014] Volodymyr Mnih", "Nicolas Heess", "Alex Graves", "Koray Kavukcuoglu"], "venue": "arXiv preprint", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "The dynamic representation of scenes", "author": ["Ronald A. Rensink"], "venue": "Visual Cognition,", "citeRegEx": "Rensink.,? \\Q2000\\E", "shortCiteRegEx": "Rensink.", "year": 2000}, {"title": "Attention for fine-grained categorization", "author": ["Andrea Frome", "Esteban Real"], "venue": "CoRR, abs/1412.7054", "citeRegEx": "Sermanet et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2014}, {"title": "The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions", "author": ["Benjamin W. Tatler"], "venue": "Journal of Vision,", "citeRegEx": "Tatler.,? \\Q2007\\E", "shortCiteRegEx": "Tatler.", "year": 2007}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "CoRR, abs/1603.01417", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Xu", "Saenko2015] Huijuan Xu", "Kate Saenko"], "venue": "CoRR, abs/1511.05234", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "CoRR, abs/1502.03044", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Yang et al.2015] Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alexander J. Smola"], "venue": "CoRR, abs/1511.02274", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Humans have the ability to quickly perceive a scene by selectively attending to parts of the image instead of processing the whole scene in its entirety (Rensink, 2000).", "startOffset": 153, "endOffset": 168}, {"referenceID": 2, "context": "Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine translation (Bahdanau et al., 2014; Firat et al., 2016), object recognition (Ba et al.", "startOffset": 148, "endOffset": 191}, {"referenceID": 6, "context": "Given an input signal, these models learn to attend to parts of it for further processing and have been successfully applied in machine translation (Bahdanau et al., 2014; Firat et al., 2016), object recognition (Ba et al.", "startOffset": 148, "endOffset": 191}, {"referenceID": 15, "context": ", 2014), image captioning (Xu et al., 2015; Cho et al., 2015) and visual question answering (Yang et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 3, "context": ", 2014), image captioning (Xu et al., 2015; Cho et al., 2015) and visual question answering (Yang et al.", "startOffset": 26, "endOffset": 61}, {"referenceID": 17, "context": ", 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016).", "startOffset": 38, "endOffset": 115}, {"referenceID": 9, "context": ", 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016).", "startOffset": 38, "endOffset": 115}, {"referenceID": 14, "context": ", 2015) and visual question answering (Yang et al., 2015; Lu et al., 2016; Xu and Saenko, 2015; Xiong et al., 2016).", "startOffset": 38, "endOffset": 115}, {"referenceID": 4, "context": "Unlike image captioning, where a coarse understanding of an image is often sufficient for producing generic descriptions (Devlin et al., 2015), visual questions selectively target different areas of an image including background details and underlying context.", "startOffset": 121, "endOffset": 142}, {"referenceID": 17, "context": "parison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015; Lu et al., 2016) and a task-independent saliency baseline (Judd et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 9, "context": "parison of the maps generated by state-of-the-art attention-based VQA models (Yang et al., 2015; Lu et al., 2016) and a task-independent saliency baseline (Judd et al.", "startOffset": 77, "endOffset": 113}, {"referenceID": 8, "context": ", 2016) and a task-independent saliency baseline (Judd et al., 2009) against our human attention maps through visualizations and rank-order correlation.", "startOffset": 49, "endOffset": 68}, {"referenceID": 13, "context": "It is well understood that task-independent saliency maps have a \u2018center bias\u2019 (Tatler, 2007; Judd et al., 2009).", "startOffset": 79, "endOffset": 112}, {"referenceID": 8, "context": "It is well understood that task-independent saliency maps have a \u2018center bias\u2019 (Tatler, 2007; Judd et al., 2009).", "startOffset": 79, "endOffset": 112}, {"referenceID": 17, "context": "Stacked Attention Networks (SAN) proposed in (Yang et al., 2015) use LSTM encodings of ques-", "startOffset": 45, "endOffset": 64}, {"referenceID": 9, "context": "Hierarchical Co-Attention Network (Lu et al., 2016) generates multiple levels of image attention based on words, phrases and complete questions, and is the top entry on the VQA Challenge1 as of the time of this submission.", "startOffset": 34, "endOffset": 51}, {"referenceID": 0, "context": "Another interesting approach uses question parsing to compose the neural network from modules, attention being one of the sub-tasks addressed by these modules (Andreas et al., 2016).", "startOffset": 159, "endOffset": 181}, {"referenceID": 8, "context": "There\u2019s a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967).", "startOffset": 148, "endOffset": 223}, {"referenceID": 5, "context": "There\u2019s a rich history of work in collecting eye tracking data from human subjects to gain an understanding of image saliency and visual perception (Jiang et al., 2014; Judd et al., 2009; Fei-Fei et al., 2007; Yarbus, 1967).", "startOffset": 148, "endOffset": 223}, {"referenceID": 8, "context": "data to study natural visual exploration (Jiang et al., 2014; Judd et al., 2009) is useful but difficult and expensive to collect on a large scale.", "startOffset": 41, "endOffset": 80}, {"referenceID": 7, "context": "(Jiang et al., 2015) established mouse tracking as an accurate ap-", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "While (Jiang et al., 2015) studies natural exploration and collects task-independent human annotations by", "startOffset": 6, "endOffset": 26}, {"referenceID": 17, "context": "We evaluate maps generated by the following unsupervised models: \u2022 Stacked Attention Network (SAN) (Yang et al., 2015) with a single attention layer (SAN-1)2.", "startOffset": 99, "endOffset": 118}, {"referenceID": 9, "context": "\u2022 Hierarchical Co-Attention Network (HieCoAtt) (Lu et al., 2016) with word-level (HieCoAtt-W), phrase-level (HieCoAtt-P) and question-level (HieCoAtt-Q) attention maps; we evaluate all three maps3.", "startOffset": 47, "endOffset": 64}, {"referenceID": 17, "context": "SAN-1 (Yang et al., 2015) -0.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "HieCoAtt-W (Lu et al., 2016) 0.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "004 HieCoAtt-P (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 9, "context": "004 HieCoAtt-Q (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 8, "context": "(Judd et al., 2009) 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 8, "context": "jiasenlu/HieCoAttenVQA where subjects are asked to freely view an image for 3 seconds (Judd et al., 2009).", "startOffset": 86, "endOffset": 105}, {"referenceID": 17, "context": "SAN-1 (Yang et al., 2015) -0.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": "HieCoAtt-W (Lu et al., 2016) 0.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "012 HieCoAtt-P (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 9, "context": "010 HieCoAtt-Q (Lu et al., 2016) 0.", "startOffset": 15, "endOffset": 32}, {"referenceID": 8, "context": "(Judd et al., 2009) -0.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "We conduct large-scale studies on \u2018human attention\u2019 in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by stateof-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). We find that depending on the implementation used, machine-generated attention maps are either negatively correlated with human attention or have positive correlation worse than task-independent saliency. Overall, our experiments paint a bleak picture for the current generation of attention models in VQA.", "creator": "LaTeX with hyperref package"}}}