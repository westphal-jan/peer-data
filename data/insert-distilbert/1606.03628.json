{"id": "1606.03628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2016", "title": "metricDTW: local distance metric learning in Dynamic Time Warping", "abstract": "usually we propose to learn multiple local pattern mahalanobis distance metrics to perform k - nearest neighbor ( knn ) classification of temporal sequences. temporal sequences are essentially first aligned by dynamic time warping ( dtw ) ; given the alignment path, similarity between two sequences is measured by the dtw distance, which also is computed as the accumulated distance between matched temporal point start pairs along the alignment distance path. traditionally, euclidean metric mathematics is used for distance computation between matched pairs, which ignores the data regularities and might not be optimal for applications at hand. right here we propose to learn multiple mahalanobis metrics, now such concluding that dtw distance becomes the sum of mahalanobis distances. we adapt the large margin nearest neighbor ( lmnn ) framework to our case, and formulate multiple metric learning as a linear programming problem. extensive sequence classification results show that our proposed multiple metrics learning approach is effective, insensitive to the preceding alignment qualities, and reaches the state - of - the - art performances on ucr time series datasets.", "histories": [["v1", "Sat, 11 Jun 2016 21:14:08 GMT  (1128kb,D)", "http://arxiv.org/abs/1606.03628v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jiaping zhao", "zerong xi", "laurent itti"], "accepted": false, "id": "1606.03628"}, "pdf": {"name": "1606.03628.pdf", "metadata": {"source": "CRF", "title": "metricDTW: local distance metric learning in Dynamic Time Warping", "authors": ["Jiaping Zhao", "Zerong Xi", "Laurent Itti"], "emails": ["jiapingz@usc.edu", "zxi@usc.edu", "itti@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "Dynamic time warping (DTW) is an algorithm to align temporal sequences and measure their similarities. DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3]. DTW allows temporal sequences to be locally shifted, contracted and stretched, and it calculates a global optimal alignment path between two given sequences under certain restrictions. Therefore, the similarity between two sequences calculated under the optimal alignment is independent of, to some extent, non-linear variations in the time dimension. The similarity is often quantified by the DTW distance, which is the sum of point-wise distances along the alignment path, i.e., D(P,Q) = \u03a3(i,j)\u2208pd(i, j), where p is the alignment path between two sequences P and Q, (i, j) is a pair of matched points on the alignment path and d(i, j) is the distance (affinity) between i and j. The most widely used point-wise distance d(i, j) is the (squared) Euclidean distance.\nSince DTW distance naturally measures the similarity between time series, it is widely used for time series classification. There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17]. Although 1NN-DTW is competitive and hard to beat, to the best of our knowledge, the DTW distance is often computed as the sum of point-wise (squared) Euclidean distances along the matching path, i.e., D(P,Q) = \u03a3(i,j)\u2208pd(i, j), where d(i, j) =\u2016 i \u2212 j \u20162 is the (squared) Euclidean distance between the matched points i and j. Nevertheless, the performance of kNN significantly depends on the used similarity measures. Although Euclidean distance is simple and sometimes effective, but it is agnostic of domain knowledge and data regularities. Extensive researches have shown that kNN performances can be greatly improved by learning a proper distance metric (e.g., Mahalanobis distance) from labels examples\nar X\niv :1\n60 6.\n03 62\n8v 1\n[ cs\n.L G\n] 1\n1 Ju\nn 20\n[21, 2]. This motives us to learn local distance metrics and calculate DTW distance as the sum of point-wise learned distances, i.e., D\u0302(P,Q) = \u03a3(i,j)\u2208pd\u0302(i, j), where d\u0302(i, j) = (i\u2212 j)TMij(i\u2212 j), Mij is a positive semidefinite matrix to be learned and d\u0302(i, j) is the squared Mahalanobis distance. In the paper, instead of learning one uniform distance metric, we partition the feature space, and learn individual metrics within and between subspaces. When using DTW distance calculated under the learned metrics as the similarity measure, 1NN classifier has the potential to obtain improved performances.\nWe closely follow Large Margin Nearest Neighbor (LMNN) [21] to formulate local metric learning in DTW. In [21], the Mahalanobis metric is learned with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. Mathematically, the authors formulate the metric learning as a semidefinite programming problem. In our case, we use the same max margin framework, with the only difference that: examples in [21] are feature points in some fixed-dimension space, and distances between examples are squared Mahalanobis distances, while in our case, examples are temporal sequences, and distances between examples are DTW distances. We term the local metric learning in DTW as metricDTW. We have to emphasize that although the learned local distances are metric distances, the DTW distance under those metrics is generally not a metric distance since the triangle inequality does not hold.\nBefore computing the DTW distance, we have to align given sequences to obtain the alignment path, along which the DTW distance is defined. In our work, we do not aim to learn to align sequences, instead, we use existing DTW techniques to align sequences first and treat these alignment paths as known. Therefore, the metric learning in DTW is independent of the preceding alignment process, in principle, any sequence alignment algorithms can be used before metric learning.\nIn the paper, different from the tradition, we compute the distance between a matched point pair by the distance between their descriptors. The descriptor of a temporal point is a representation of the\nsubsequence centering on that point, and it represents the structural information around that point (see Fig. 2). In this way, DTW distance is computed as the accumulated descriptor distances along the alignment path. In our case, descriptors are further k-means clustered into groups, then multiple local distance metrics are learned within individual clusters and between any two clusters, such that DTW distances calculated under the learned metrics make kNN neighbors of temporal sequences always come from the same class, while sequences from different classes are separated by a large margin. Our proposed local metrics learning framework is depicted in Fig. 1.\nOur multiple metric learning ends up learning multiple Mahalanobis distance matrices, some magnifying the Euclidean distances between subsequences, while other shrinking the original Euclidean distances. This is equivalent to learn the importance of subsequences automatically, e.g., certain shaped subsequences are discriminative for classification, and their subtle differences may be magnified by the corresponding learned metrics, while certain shaped subsequences are less class-membership defining, and their big differences might be suppressed after metric learning. In this perspective, our local metric learning framework is essentially learning the importance of different subsequences in an automatic and principled way.\nWe extensively test the performance of metricDTW for time series classification on 70 UCR time series datasets [3], and experimental results show that (1) the learned local metrics, compared with the default Euclidean metric, improve the 1NN classification accuracies significantly; (2) given alignment paths of different qualities, the subsequent metric learning consistently boosts classification accuracies significantly, showing that the proposed metric learning approach is invariant to the preceding alignment step; (3) our metric learning algorithm outperforms the state-of-the-art time series classification algorithm (1NN-DTW) significantly on UCR datasets, therefore, we set a new record for future time series classification comparison."}, {"heading": "2 Related work", "text": "As mentioned, our local metric learning framework is essentially learning the importance of different subsequences in an automatic and principled way. There are several prior works focusing on mining representative and discriminative subsequences (image patches) from temporal sequences (images).\nTime series shapelet is introduced in [22], and it is a time series subsequence (patterns) which is discriminative of class-membership. The authors propose to enumerate all possible candidate subsequences, evaluate their qualities using information gain, and build a decision tree classifier out of the top ranked shapelets. Mining shapelets in their case is to search for more important subsequences, while disregarding less important subsequences. In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images. Mid-level visual patch is conceptually similar to shapelet in time series, and it a image patch which is both representative and discriminative for scene categories. They [19, 5] pose the discriminative patch search procedure as a discriminative clustering process, in which they selectively choose important patches but discarding other common patches. We are different from above work in that, we never have to greedily select important subsequences, instead, we take all subsequences into account and automatically learn their importance through metric learning.\nOur work is most similar to and largely inspired by LMNN [21]. In [21], Weinbergre and Saul extend LMNN to learn multiple local distance metrics, which is exploited in our work as well. However, we are still sufficiently different: first the labeled examples in our case are temporal sequences; second, the DTW distance between two examples is jointly defined by multiple metrics, while in [21], distance between two examples are determined by a single metric. In [7], Garreau et al propose to learn a Mahalanobis distance metric to perform DTW sequence alignment. First they need ground-truth alignments, which is not required in our case, and second they focus on alignment, instead of kNN classification."}, {"heading": "3 Local distance metric learning in DTW", "text": "As mentioned above, local metric learning needs sequence alignments as inputs. While in most scenarios, ground-truth sequence to sequence alignments are expensive or impossible to label, in experiments, we use DTW to align sequences first, and use the computed alignments for the subsequent\nmetric learning. In this section, we first briefly review the DTW algorithm for sequence alignment, and then introduce our multiple local metric learning algorithm for time series classification."}, {"heading": "3.1 Dynamic Time Warping", "text": "DTW is an algorithm to align temporal sequences under certain restrictions. Given two sequences P and Q of possible different lengths LP and LQ, namely P = (p1, p2, ..., pLP )T and Q = (q1, q2, ..., qLQ)\nT , and let d(P,Q) \u2208 RLP\u00d7LQ be the pairwise distance matrix, where d(i, j) is the distance between points pi and pj . One widely used distance measure is the squared Euclidean distance, i.e., d(i, j) =\u2016 pi \u2212 qj \u201622. The goal of temporal alignment between P and Q is to find two sequences of indices \u03b1 and \u03b2 of the same length l, which match index \u03b1(i) in the time series P to index \u03b2(i) in the time series Q, such that the total cost along the matching path \u2211l i=1 d(\u03b1(i), \u03b2(i)) is minimized. The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]:\n\u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 ... \u2264 \u03b1(l), \u03b2(1) \u2264 \u03b2(2) \u2264 ... \u2264 \u03b2(l) (\u03b1(i+ 1), \u03b2(i+ 1))\u2212 (\u03b1(i), \u03b2(i)) \u2208 {(1, 0), (1, 1), (0, 1)}\n(1)\nSearching for an optimal alignment path p under the above restrictions is equivalent to solve the following recursive formula:\nD(i, j) = d(i, j) + min{D(i\u2212 1, j \u2212 1), D(i, j \u2212 1), D(i\u2212 1, j)} (2)\nwhere D(i, j) is the accumulated distance from the matched point-pair (p1, q1) to the matched pointpair (pi, qj) along the alignment path, and d(i, j) is the distance between points pi and pj . In all the following alignment experiments, we use the squared Euclidean distance to compute d(i, j). The above formula is a typical dynamic programming recursion, and can be solved efficiently in O(LP\u00d7LQ) time by a dp algorithm [6]. The alignment path p is obtained by back-tracking. Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1)."}, {"heading": "3.2 Local distance metric learning", "text": "After obtaining the alignment path p by DTW , we can compute DTW distance between P and Q in two ways: (1) directly return DTW distance as the accumulated distances between matched pairs along p, i.e., \u2211 (i,j)\u2208p d(pi, qj); (2) to measure the distance between a matched pair (pi, qj), we could use the distance between their descriptors, i.e., d(\u2212\u2192pi ,\u2212\u2192qj ), where \u2212\u2192pi and \u2212\u2192qj are descriptors of points pi and qj respectively. In this way, DTW distance between P and Q is calculated as the accumulated descriptor distances along p, i.e., \u2211 (i,j)\u2208p d(\n\u2212\u2192pi ,\u2212\u2192qj ). Here, the descriptor at some point is a feature vector representation of the subsequence centering at that point, and the descriptor is supposed to capture the neighborhood shape information around the temporal point (see Fig. 2 for the illustration of descriptors). Using their descriptor distance to measure two point similarity (distance) makes much sense since two point similarity is usually better represented by their neighbor structural similarity, instead of by their single point to point distance.\nIn following experiments, we always adopt the second way to define the DTW distance, and we use three shape descriptors, namely the raw-subsequence, HOG-1D [23] and the gradient sequence [12].\nIf the squared Euclidean distance is used, then DTW distance is calculated as D(P,Q) = \u2211\n(i,j)\u2208p \u2016\u2212\u2192pi \u2212 \u2212\u2192qj \u20162, which is essentially a equally weighted sum of distances between descriptors (subsequences), however, as shown in [22], some subsequences are more class-membership predictive, while others are less discriminative. Therefore, it makes more sense, if we calculate the DTW distance as a weighed sum of distances between subsequences, i.e., D(P,Q) = \u2211 (i,j)\u2208p \u03c9ij \u2016\n\u2212\u2192pi \u2212\u2212\u2192qj \u20162, where \u03c9ij is the weight between two subsequences \u2212\u2192pi and \u2212\u2192qi and indicates the importance of subsequences \u2212\u2192pi and \u2212\u2192qi for classification. If we make further generalization, the DTW distance can be calculated as the sum of squared Mahalanobis distances between subsequences, i.e.,\nD(P,Q) = \u2211\n(i,j)\u2208p( \u2212\u2192pi\u2212\u2212\u2192qj )TMcicj (\u2212\u2192pi\u2212\u2212\u2192qj ), whereMcicj is a positive semidefinite Mahalanobis\nmatrix to be learned from the labeled data. Note that, instead of learning a global metric matrix, we learn multiple local metric matrices simultaneously. The intuition behind is that differently-shaped subsequences have different importance for classification, therefore, their between-distances should be computed under different metrics. In experiments, we first k-means partition the descriptors from all training sequences into k clusters, and then learn Mahalanobis distance metrics within individual clusters and between any two different clusters. LetMcici , Mcicj denote the metrics within the cluster ci and between two cluster ci and cj respectively, and then the distance between any two descriptors \u2212\u2192pi and \u2212\u2192qi is (\u2212\u2192pi \u2212\u2212\u2192qj )TMcicj (\u2212\u2192pi \u2212\u2212\u2192qj ), where ci and cj are clusters \u2212\u2192pi and \u2212\u2192qi belong to respectively.\nIn order to learn these local metrics from labeled sequence data, we follow LMNN [21] closely and pose our problem as a max margin problem: the local Mahalanobis metrics are trained such that the k-nearest neighbors of any sequence always belong to the same class while sequences of different classes are separated by a large margin. We use the exact notations in LMNN, and the only place to change is to replace the squared Mahalanobis point-to-point distance in [21] by the DTW distance. The adapted LMNN is as follows:\nMinimize : (1\u2212 \u00b5)\u03a3i,j iD(xi, xj) + \u00b5\u03a3j i,l(1\u2212 yil)\u03beijl Subject to :\n(1)D(xi, xl)\u2212D(xi, xj) \u2265 1\u2212 \u03beijl (2) \u03beijl \u2265 0 (3)Mcicj \u2261Mcjci , Mcicj 0, ci, cj \u2208 {1, 2, ..., k}\n(3)\nNote that we enforce the learned matrices between two clusters ci and cj to be the same, i.e., Mcicj \u2261 Mcjci , which makes distance mapping between ci and cj be a metric. We refer readers to [21] for notation meanings. In our experiments, we further simplify the form of Mahalanobis matrices, and constrain them to be not only diagonal but also with single repeated element on the diagonal, i.e.,Mcicj = \u03c9cicj \u00b7 I. Under this simplification, learning a Mahalanobis matrix reduces to learning a scalar, resulting in (k2 + k)/2 unknown scalars to learn. Under the reduction, the original semidefinite programming (3) reduces to a linear programming. In experiments, the balancing factor \u00b5 is tuned by cross-validation."}, {"heading": "4 Experiments", "text": "In this section, we evaluate the performances of the proposed local metric learning method for time series classification on 70 UCR datasets [3], which provide their standard training/test partitions for performance evaluation.\nWe empirically show below that: (1) whether multiple local metric learning boosts time series classification accuracies of 1NN classifier; (2) how the quality of the preceding alignments affects the subsequent metric learning performances; (3) the influence of hyper-parameter settings on the metric learning performances."}, {"heading": "4.1 Experimental settings", "text": "Sequence alignment: when running DTW (2) to align sequences, we use the default squared Euclidean distances to compute point-to-point distance. The alignment paths are the inputs of the subsequent metric learning step.\nTemporal point descriptors: the descriptor at a temporal point is used to represent its neighborhood structures. When computing the point to point distance in DTW alignment (d(i, j) in (2)), we compute the distance between their descriptors and use it as the distance between the original temporal points. Obviously, the searched optimal alignment path by DTW (2) depends on the used descriptor for point-to-point distance computations. Descriptors are used in the subsequent metric learning as well to define the DTW distance (see Sec. 3.2).\nIn experiments, we use three subsequence descriptors, including raw-subsequence, HOG-1D [23] and the derivative sequence [12]. (1) The raw-subsequences taken from temporal points are fixed to be of length 30; (2) HOG-1D is a representation of the raw subsequence, and we use two non-overlapping intervals, use 8 bins and set \u03c3 = 0.1, resulting in a 16D HOG-1D descriptor; (3) the derivative descriptor is simply the first order derivative sequence of the raw subsequence. We follow [12] exactly to compute derivative at each point, and the derivative descriptor is 30D by definition.\nMetric learning: k in kNN is set to 3. For each training time series, we compute its 3 nearest neighbors of the same class based on the DTW distances, which is computed under the default Euclidean metric. We set k in k-means to be 5, partition the training descriptors into 5 clusters and local distance metrics are defined within and between these 5 clusters. The linear program (3) is solved by the CVX package [10, 9]. During test, we use the label of its nearest neighbor in the training set as the predicted test label. This is consistent with the convention in the time series community, in which they use 1NN as classifier."}, {"heading": "4.2 Effectiveness of local distance metric learning", "text": "First, we fix the alignment, and explore the performances of local metric learning. Then, we analyze the influence of the preceding alignment qualities on the performances of subsequent metric learning.\nWe align time series by DTW under three descriptors, derivative, HOG-1D and raw-subsequence, respectively. Given the computed alignments, we learn local distance metrics under the same descriptor as used in the alignment by solving the LP problem (3), and plot 1NN classification accuracies in Fig. 3. Plots in Fig. 3 are scatter plots showing the comparison between 1NN classifier performances under the Euclidean metric and the learned metrics. Each red dot in the plot indicates one UCR dataset, whose x-mapping and y-mapping are accuracies under the Euclidean metric and the learned metrics respectively. By running the signed rank Wilcoxon test, we obtain p-values 0.038/0.015/0.003 for the descriptor raw-subsequence/HOG-1D/gradient, showing that our proposed metric learning improve the 1NN classifier significantly under the confidence level 5%.\nSince the alignment path is the input for the metric learning step, bad alignments may affect the performance. Nevertheless, we empirically show this is not the case. We perform metric learning under different alignments, and evaluate whether significant improvements can be achieved under all cases. In experiments, we align time series under three descriptors, and then learn metrics under the gradient descriptor. We use boxplot to show the performance improvements, compared with using the default Euclidean metric, in Fig. 4(left). The blue box has two tails: the lower and upper edge of each blue box represent 25th and 75th percentiles, with the red line inside the box marking the median improvement and two tails indicating the best and worst improvements. Under three different alignments, the median improvements are all greater than 0 and the majority of improvements are above 0. By running the signed rank Wilcoxon test between the 1NN performances under the Euclidean metric and the learned metrics, we obtain p-values 0.007/0.029/0.003 under alignments by the descriptor raw-subsequence/HOG-1D/gradient. This empirically indicates that the subsequent metric learning is robust to the preceding alignment qualities.\nTo show different descriptors do have different alignment qualities, we could compare DTW alignment paths under different descriptors against the ground-truth alignments. However, UCR datasets do not have the ground-truth alignments, here we simulate time series alignment pairs by manually scaling and stretching time series, and the ground-truth alignment between the original time series and the stretched one is known by simulation; then we run DTW alignment under different descriptors,\nevaluate the alignment error against the ground truth, and plot the results in 4(right). It shows different algorithms do perform differently. We refer the readers to the supplementary materials for simulation details."}, {"heading": "4.3 Effects of hyper-parameters", "text": "There is one important hyper-parameter in the metric learning: the number of clusters of descriptors. In experiments, we align and learn local metrics both under the gradient descriptor, and during the metric learning, we set different numbers of descriptor clusters, i.e., k = {5, 10, 15, 20, 25, 30}, learn metrics by solving (3), and plot the 1NN performance improvements in Fig. 5. Under different k\u2019s, the majority of the improvements are above 0, and the signed rank Wilcoxon test returns p-values 0.003/ 0.026/ 0.005/ 0.021/ 0.002/ 0.017 under k=5/10/15/20/25/30, showing significant improvements under varied k\u2019s."}, {"heading": "4.4 Comparison with the state of the art algorithm", "text": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat. Here we use 1NN-DTW as the baseline, and compare our algorithms to it. In 1NN-DTW, the alignment is computed by DTW as well, however, no descriptor is used, i.e., the point to point distance is directly computed by the squared Euclidean distance between those two points, instead of by their descriptor distance. The DTW distance between two aligned sequences is\ncomputed as the accumulated squared Euclidean point-to-point distances with no descriptor used as well.\nIn our case, we use the HOG-1D descriptor to align sequences and learn local metrics. We plot the time series classification performances in Fig. 6: our algorithm with (without) metric learning wins/draws/loses the baseline on 48/3/19 (47/3/20) datasets, and the signed rank Wilcoxon test returns p-values 1.1 \u00b7 10\u22124(1.8 \u00b7 10\u22125), showing significant accuracy improvement over 1NN-DTW. We document the classification error rates of three algorithms in the supplementary materials."}, {"heading": "5 Conclusion and discussion", "text": "In this paper, we propose to learn multiple local Mahalanobis distance metrics to perform k-nearest neighbor (kNN) classification of temporal sequences. We showed empirically that the metric learning process always improves the 1NN time series classification accuracies, disregard of the qualities of the preceding DTW alignments. Our algorithm wins the 1NN-DTW algorithm significantly on 70 UCR time series datasets, and sets up a record for further comparison.\nDTW time series classification has two consecutive steps: time series alignment and then classification. In this paper, the metric learning happens after the alignment finishes, and information in the metric learning does not back-prop into the preceding alignment step. An naive extension is to do alignment and metric learning in an iterative process. But as we tried, this deteriorated the classification performances. A future research direction is how to do the alignment and learn metrics in an integrated fashion."}], "references": [{"title": "An experimental evaluation of nearest neighbour time series classification", "author": ["A. Bagnall", "J. Lines"], "venue": "arXiv preprint arXiv:1406.4757,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "The ucr time series classification", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": "www.cs.ucr.edu/~eamonn/time_series_data/", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "What makes paris look like paris", "author": ["C. Doersch", "S. Singh", "A. Gupta", "J. Sivic", "A. Efros"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Dynamic time warp (dtw) in matlab", "author": ["D. Ellis"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Metric learning for temporal sequence alignment", "author": ["D. Garreau", "R. Lajugie", "S. Arlot", "F. Bach"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Computing and visualizing dynamic time warping alignments in r: the dtw package", "author": ["T. Giorgino"], "venue": "Journal of statistical Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Graph implementations for nonsmooth convex programs", "author": ["M. Grant", "S. Boyd"], "venue": "Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version", "author": ["M. Grant", "S. Boyd"], "venue": "//cvxr.com/cvx,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Style translation for human motion", "author": ["E. Hsu", "K. Pulli", "J. Popovi\u0107"], "venue": "ACM Transactions on Graphics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Derivative dynamic time warping", "author": ["E. Keogh", "M. Pazzani"], "venue": "In SDM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Exact indexing of dynamic time warping", "author": ["E. Keogh", "C. Ratanamahatana"], "venue": "Knowledge and information systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Continuous action recognition based on sequence alignment", "author": ["K. Kulkarni", "G. Evangelidis", "J. Cech", "R. Horaud"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Dynamic time warping averaging of time series allows faster and more accurate classification", "author": ["F. Petitjean", "G. Forestier", "G. Webb", "A. Nicholson", "Y. Chen", "E. Keogh"], "venue": "In ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Fundamentals of speech recognition", "author": ["L. Rabiner", "B.-H. Juang"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Searching and mining trillions of time series subsequences under dynamic time warping", "author": ["T. Rakthanmanon", "B. Campana", "A. Mueen", "G. Batista", "B. Westover", "Q. Zhu", "J. Zakaria", "E. Keogh"], "venue": "In SIGKDD,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1978}, {"title": "Unsupervised discovery of mid-level discriminative patches", "author": ["S. Singh", "A. Gupta", "A. Efros"], "venue": "Computer Vision\u2013ECCV", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Experimental comparison of representation methods and distance measures for time series data", "author": ["X. Wang", "A. Mueen", "H. Ding", "G. Trajcevski", "P. Scheuermann", "E. Keogh"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "In SIGKDD,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "DTW has been widely used in speech recognition [16], human motion synthesis [11], human activity recognition [14] and time series classification [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 14, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 19, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 0, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 16, "context": "There is increasing acceptance that the nearest neighbor classifier with the DTW distance as the similarity measure (1NN-DTW) is the choice for most time series classification problems and very hard to beat [15, 20, 1, 17].", "startOffset": 207, "endOffset": 222}, {"referenceID": 20, "context": "We adapt LMNN [21] to formulate our multiple metric learning in DTW.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "[21, 2].", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[21, 2].", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "We closely follow Large Margin Nearest Neighbor (LMNN) [21] to formulate local metric learning in DTW.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "In [21], the Mahalanobis metric is learned with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "In our case, we use the same max margin framework, with the only difference that: examples in [21] are feature points in some fixed-dimension space, and distances between examples are squared Mahalanobis distances, while in our case, examples are temporal sequences, and distances between examples are DTW distances.", "startOffset": 94, "endOffset": 98}, {"referenceID": 2, "context": "We extensively test the performance of metricDTW for time series classification on 70 UCR time series datasets [3], and experimental results show that (1) the learned local metrics, compared with the default Euclidean metric, improve the 1NN classification accuracies significantly; (2) given alignment paths of different qualities, the subsequent metric learning consistently boosts classification accuracies significantly, showing that the proposed metric learning approach is invariant to the preceding alignment step; (3) our metric learning algorithm outperforms the state-of-the-art time series classification algorithm (1NN-DTW) significantly on UCR datasets, therefore, we set a new record for future time series classification comparison.", "startOffset": 111, "endOffset": 114}, {"referenceID": 21, "context": "Time series shapelet is introduced in [22], and it is a time series subsequence (patterns) which is discriminative of class-membership.", "startOffset": 38, "endOffset": 42}, {"referenceID": 18, "context": "In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images.", "startOffset": 57, "endOffset": 67}, {"referenceID": 4, "context": "In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images.", "startOffset": 57, "endOffset": 67}, {"referenceID": 3, "context": "In the vision community, there are several related works [19, 5, 4], all of which are devoted to discovering mid-level visual patches from images.", "startOffset": 57, "endOffset": 67}, {"referenceID": 18, "context": "They [19, 5] pose the discriminative patch search procedure as a discriminative clustering process, in which they selectively choose important patches but discarding other common patches.", "startOffset": 5, "endOffset": 12}, {"referenceID": 4, "context": "They [19, 5] pose the discriminative patch search procedure as a discriminative clustering process, in which they selectively choose important patches but discarding other common patches.", "startOffset": 5, "endOffset": 12}, {"referenceID": 20, "context": "Our work is most similar to and largely inspired by LMNN [21].", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "In [21], Weinbergre and Saul extend LMNN to learn multiple local distance metrics, which is exploited in our work as well.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "However, we are still sufficiently different: first the labeled examples in our case are temporal sequences; second, the DTW distance between two examples is jointly defined by multiple metrics, while in [21], distance between two examples are determined by a single metric.", "startOffset": 204, "endOffset": 208}, {"referenceID": 6, "context": "In [7], Garreau et al propose to learn a Mahalanobis distance metric to perform DTW sequence alignment.", "startOffset": 3, "endOffset": 6}, {"referenceID": 17, "context": "The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]: \uf8f2\uf8f3 \u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 .", "startOffset": 109, "endOffset": 120}, {"referenceID": 12, "context": "The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]: \uf8f2\uf8f3 \u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 .", "startOffset": 109, "endOffset": 120}, {"referenceID": 6, "context": "The alignment path p = (\u03b1, \u03b2) is constrained to satisfies boundary, monotonicity and step-pattern conditions [18, 13, 7]: \uf8f2\uf8f3 \u03b1(1) = \u03b2(1) = 1, \u03b1(l) = LP , \u03b2(l) = LQ \u03b1(1) \u2264 \u03b1(2) \u2264 .", "startOffset": 109, "endOffset": 120}, {"referenceID": 5, "context": "The above formula is a typical dynamic programming recursion, and can be solved efficiently in O(LP\u00d7LQ) time by a dp algorithm [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 17, "context": "Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1).", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1).", "startOffset": 146, "endOffset": 153}, {"referenceID": 7, "context": "Various temporal window constraints [18] can be enforced and we could use more complicated step patterns, such as \u201casymmetric\u201d and \u201crabinerJuang\u201d [16, 8], but here we consider DTW without warping window constraints and taking moving patterns as defined in (1).", "startOffset": 146, "endOffset": 153}, {"referenceID": 11, "context": "In following experiments, we always adopt the second way to define the DTW distance, and we use three shape descriptors, namely the raw-subsequence, HOG-1D [23] and the gradient sequence [12].", "startOffset": 187, "endOffset": 191}, {"referenceID": 21, "context": "If the squared Euclidean distance is used, then DTW distance is calculated as D(P,Q) = \u2211 (i,j)\u2208p \u2016 \u2212 \u2192pi \u2212 \u2212 \u2192qj \u2016, which is essentially a equally weighted sum of distances between descriptors (subsequences), however, as shown in [22], some subsequences are more class-membership predictive, while others are less discriminative.", "startOffset": 230, "endOffset": 234}, {"referenceID": 20, "context": "In order to learn these local metrics from labeled sequence data, we follow LMNN [21] closely and pose our problem as a max margin problem: the local Mahalanobis metrics are trained such that the k-nearest neighbors of any sequence always belong to the same class while sequences of different classes are separated by a large margin.", "startOffset": 81, "endOffset": 85}, {"referenceID": 20, "context": "We use the exact notations in LMNN, and the only place to change is to replace the squared Mahalanobis point-to-point distance in [21] by the DTW distance.", "startOffset": 130, "endOffset": 134}, {"referenceID": 20, "context": "We refer readers to [21] for notation meanings.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "In this section, we evaluate the performances of the proposed local metric learning method for time series classification on 70 UCR datasets [3], which provide their standard training/test partitions for performance evaluation.", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "In experiments, we use three subsequence descriptors, including raw-subsequence, HOG-1D [23] and the derivative sequence [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "We follow [12] exactly to compute derivative at each point, and the derivative descriptor is 30D by definition.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "The linear program (3) is solved by the CVX package [10, 9].", "startOffset": 52, "endOffset": 59}, {"referenceID": 8, "context": "The linear program (3) is solved by the CVX package [10, 9].", "startOffset": 52, "endOffset": 59}, {"referenceID": 14, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}, {"referenceID": 19, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}, {"referenceID": 16, "context": "As shown in [15, 20, 1, 17], 1NN classifier with the DTW distance as the similarity measure (1NNDTW) is very hard to beat.", "startOffset": 12, "endOffset": 27}], "year": 2016, "abstractText": "We propose to learn multiple local Mahalanobis distance metrics to perform knearest neighbor (kNN) classification of temporal sequences. Temporal sequences are first aligned by dynamic time warping (DTW); given the alignment path, similarity between two sequences is measured by the DTW distance, which is computed as the accumulated distance between matched temporal point pairs along the alignment path. Traditionally, Euclidean metric is used for distance computation between matched pairs, which ignores the data regularities and might not be optimal for applications at hand. Here we propose to learn multiple Mahalanobis metrics, such that DTW distance becomes the sum of Mahalanobis distances. We adapt the large margin nearest neighbor (LMNN) framework to our case, and formulate multiple metric learning as a linear programming problem. Extensive sequence classification results show that our proposed multiple metrics learning approach is effective, insensitive to the preceding alignment qualities, and reaches the state-ofthe-art performances on UCR time series datasets.", "creator": "LaTeX with hyperref package"}}}