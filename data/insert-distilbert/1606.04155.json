{"id": "1606.04155", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Rationalizing Neural Predictions", "abstract": "procedural prediction without reinforcement justification has limited applicability. as a symbolic remedy, we simply learn to extract pieces of input text as justifications - - rationales - - that are tailored to be short and coherent, demanding yet sufficient inference for making outcome the same prediction. our approach combines two modular components, generator and encoder, which are trained to operate well together. the interaction generator software specifies a distribution over text fragments as candidate rationales and these are passed through utilizing the encoder for prediction. rationales are never given during constraint training. instead, the model is regularized by desiderata for rationales. we evaluate the pearson approach on multi - aspect sentiment analysis against manually annotated test cases. our approach outperforms attention - based baseline by a significant margin. we also do successfully illustrate integrating the method on computing the question retrieval task.", "histories": [["v1", "Mon, 13 Jun 2016 22:10:23 GMT  (205kb,D)", "http://arxiv.org/abs/1606.04155v1", null], ["v2", "Wed, 2 Nov 2016 20:26:20 GMT  (267kb,D)", "http://arxiv.org/abs/1606.04155v2", "EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["tao lei", "regina barzilay", "tommi s jaakkola"], "accepted": true, "id": "1606.04155"}, "pdf": {"name": "1606.04155.pdf", "metadata": {"source": "CRF", "title": "Rationalizing Neural Predictions", "authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "emails": ["tommi}@csail.mit.edu"], "sections": [{"heading": null, "text": "Prediction without justification has limited applicability. As a remedy, we learn to extract pieces of input text as justifications \u2013 rationales \u2013 that are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by desiderata for rationales. We evaluate the approach on multi-aspect sentiment analysis against manually annotated test cases. Our approach outperforms attention-based baseline by a significant margin. We also successfully illustrate the method on the question retrieval task.1"}, {"heading": "1 Introduction", "text": "Many recent advances in NLP problems have come from formulating and training expressive and elaborate neural models. This includes models for sentiment classification, parsing, and machine translation among many others. The gains in accuracy have, however, come at the cost of interpretability since complex neural models offer little transparency concerning their inner workings. In many applications, such as medicine, predictions are used to drive critical decisions, including treatment options. It is necessary in such cases to be able to verify and under-\n1Code will be made available soon.\nstand the underlying basis for the decisions. Ideally, complex neural models would not only yield improved performance but would also offer interpretable justifications \u2013 rationales \u2013 for their predictions.\nIn this paper, we propose a novel approach to incorporating rationale generation as an integral part of the overall learning problem. We limit ourselves to extractive (as opposed to abstractive) rationales. From this perspective, our rationales are simply subsets of the words from the input text that satisfy two key properties. First, the selected words represent short and coherent pieces of text (e.g., phrases) and, second, the selected words must alone suffice for prediction as a substitute of the original text. More concretely, consider the task of multi-aspect sentiment analysis. Figure 1 illustrates a product review along with user rating in terms of two categories or aspects. If the model in this case predicts five star rating for color, it should also identify the phrase \u201da very pleasant ruby red-amber color\u201d as the rationale underlying this decision.\nIn most practical applications, rationale genera-\nar X\niv :1\n60 6.\n04 15\n5v 1\n[ cs\n.C L\n] 1\n3 Ju\nn 20\ntion must be learned entirely in an unsupervised manner. We therefore assume that our model with rationales is trained on the same data as the original neural models, without access to additional rationale annotations. In other words, target rationales are never provided during training; the intermediate step of rationale generation is guided only by the two desiderata discussed above. Our model is composed of two modular components that we call the generator and the encoder. Our generator specifies a distribution over possible rationales (extracted text) and the encoder maps any such text to task specific target values. They are trained jointly to minimize a cost function that favors short, concise rationales while enforcing that the rationales alone suffice for accurate prediction.\nThe notion of what counts as a rationale may be ambiguous in some contexts and the task of selecting rationales may therefore be challenging to evaluate. We focus on two domains where ambiguity is minimal (or can be minimized). The first scenario concerns with multi-aspect sentiment analysis exemplified by the beer review corpus (McAuley et al., 2012). A smaller test set in this corpus identifies, for each aspect, the sentence(s) that relate to this aspect. We can therefore directly evaluate our predictions on the sentence level with the caveat that our the model makes selections on a finer level, in terms of words, not complete sentences. The second scenario concerns with the problem of retrieving similar questions. The extracted rationales should capture the main purpose of the questions. We can therefore evaluate the quality of rationales as a compressed proxy for the full text in terms of retrieval performance. Our model achieves high performance on both tasks. For instance, on the sentiment prediction task, our model achieves extraction accuracy of 96%, as compared to 38% and 81% obtained by the bigram SVM and a neural attention baseline."}, {"heading": "2 Related Work", "text": "Developing sparse interpretable models holds considerably interest in the broader research community(Letham et al., 2015; Kim et al., 2015). The need for interpretability is even more pronounced with recent neural models. Efforts in this area include analyzing and visualizing state activation (Hermans\nand Schrauwen, 2013; Karpathy et al., 2015; Li et al., 2016), learning sparse interpretable word vectors (Faruqui et al., 2015b), and linking word vectors to semantic lexicons or word properties (Faruqui et al., 2015a; Herbelot and Vecchi, 2015).\nAttention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015). Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as visualization and interpretability (Rush et al., 2015; Rockta\u0308schel et al., 2016; Hermann et al., 2015).\nOur work differs from past approaches in terms of what is meant by interpretable models (generating concise yet sufficient rationales) and how interpretation is derived (rationale generation). We explicitly aim to identify salient portions of the input text to justify predictions. Moreover, architecturally, we detach the rationale generation from how it is used (encoding) so as to be able to directly control types of rationales that are acceptable, and to facilitate broader modular use in other applications.\nFinally, we contrast our work with rationale-based classification (Zaidan et al., 2007) which seeks to reduce supervised annotations by relying on richer annotations in the form of human-provided rationales. In our work, rationales are never given during training, and the goal is to learn to generate them."}, {"heading": "3 Extractive Rationale Generation", "text": "We formalize here the task of extractive rationale generation and illustrate it in the context of neural models. To this end, consider a typical NLP task where we are provided with a sequence of words as input, namely x = {x1, \u00b7 \u00b7 \u00b7 , xl}, where each xt \u2208 Rd denotes the vector representation of the ith word. The learning problem is to map the input sequence x to a target vector in Rm. For example, in multi-aspect sentiment analysis each coordinate of the target vector represents the response or rating pertaining to the associated aspect. In text retrieval, on the other hand, the target vectors are used to induce similarity assessments between input sequences. Broadly speaking, we can solve the associated learning problem by estimating a complex pa-\nrameterized mapping enc(x) from input sequences to target vectors. We call this mapping an encoder. The training signal for these vectors is obtained either directly (e.g., multi-sentiment analysis) or via similarities (e.g., text retrieval). The challenge is that a complex neural encoder enc(x) reveals little about its internal workings and thus offers little in the way of justification for why a particular prediction was made.\nIn extractive rationale generation, our goal is to select a subset of the input sequence as a rationale. In order for the subset to qualify as a rationale it should satisfy two criteria: 1) the selected words should be interpretable and 2) they ought to suffice to reach nearly the same prediction (target vector) as the original input. In other words, a rationale must be short and sufficient. We will assume that a short selection is interpretable and focus on optimizing sufficiency under cardinality constraints.\nWe encapsulate the selection of words as a rationale generator which is another parameterized mapping gen(x) from input sequences to shorter sequences of words. Thus gen(x) must include only a few words and enc(gen(x)) should result in nearly the same target vector as the original input passed through the encoder or enc(x). We can think of the generator as a tagging model where each word in the input receives a binary tag pertaining to whether it is selected to be included in the rationale. In our case, the generator is probabilistic and specifies a distribution over possible selections.\nThe rationale generation task is entirely unsupervised in the sense that we assume no explicit annotations about which words should be included in the rationale. Put another way, the rationale is introduced as a latent variable, a constraint that guides how to interpret the input sequence. The encoder and generator are trained jointly, in an end-to-end fashion so as to function well together."}, {"heading": "4 Encoder and Generator", "text": "We use multi-aspect sentiment prediction as a guiding example to instantiate the two key components \u2013 the encoder and the generator. The framework itself generalizes to other tasks.\nEncoder enc(\u00b7): Given a training instance (x,y) where x = {xt}lt=1 is the input text sequence of\nlength l and y \u2208 [0, 1]m is the target m-dimensional sentiment vector, the neural encoder predicts y\u0303 = enc(x). If trained on its own, the encoder would aim to minimize the discrepancy between the predicted sentiment vector y\u0303 and the gold target vector y. We will use the squared error (i.e. L2 distance) as the sentiment loss function,\nL(x,y) = \u2016y\u0303 \u2212 y\u201622 = \u2016enc(x)\u2212 y\u201622\nThe encoder could be realized in many ways such as a recurrent neural network. For example, let ht = fe(xt,ht\u22121) denote a parameterized recurrent unit mapping input word xt and previous state ht\u22121 to next state ht. The target vector is then generated on the basis of the final state reached by the recurrent unit after processing all the words in the input sequence. Specifically,\nht = fe(xt,ht\u22121), t = 1, . . . , l\ny\u0303 = \u03c3e(W ehl + b e)\nGenerator gen(\u00b7): The rationale generator extracts a subset of text from the original input x to function as an interpretable summary. Thus the rationale for a given sequence x can be equivalently defined in terms of binary variables {z1, \u00b7 \u00b7 \u00b7 , zl} where each zt \u2208 0, 1 indicates whether word xt is selected or not. From here on, we will use z to specify the binary selections and thus (z,x) is the actual rationale generated (selections, input). We will use generator gen(x) as synonymous with a probability distribution over binary selections, i.e., z \u223c gen(x) \u2261 p(z|x) where the length of z varies with the input x.\nIn a simple generator, the probability that the tth word is selected can be assumed to be conditionally independent from other selections given the input x. That is, the joint probability p(z|x) factors according to\np(z|x) = l\u220f\nt=1\np(zt|x) (independent selection)\nThe component distributions p(zt|x) can be modeled using a shared bi-directional recurrent neural network. Specifically, let \u2212\u2192 f () and \u2190\u2212 f () be the for-\nward and backward recurrent unit, respectively, then\n\u2212\u2192 ht = \u2212\u2192 f (xt, \u2212\u2212\u2192 ht\u22121) \u2190\u2212 ht = \u2190\u2212 f (xt, \u2190\u2212\u2212 ht+1)\np(zt|x) = \u03c3z(Wz[ \u2212\u2192 ht; \u2190\u2212 ht] + b z)\nIndependent but context dependent selection of words is often sufficient. However, the model is unable to select phrases or refrain from selecting the same word again if already chosen. To this end, we also introduce a dependent selection of words,\np(z|x) = l\u220f\nt=1\np(zt|x, z1 \u00b7 \u00b7 \u00b7 zt\u22121)\nwhich can be also expressed as a recurrent neural network. To this end, we introduce another hidden state st whose role is to couple the selections. For example,\np(zt|x, z1,t\u22121) = \u03c3z(Wz[ \u2212\u2192 ht; \u2190\u2212 ht; st\u22121] + b z)\nst = fz([ \u2212\u2192 ht; \u2190\u2212 ht; zt], st\u22121)\nJoint objective: A rationale in our definition corresponds to the selected words, i.e., {xk|zk = 1}. We will use (z,x) as the shorthand for this rationale and, thus, enc(z,x) refers to the target vector obtained by applying the encoder to the rationale as the input. Our goal here is to formalize how the rationale can be made short and meaningful yet function well in conjunction with the encoder. Our generator and encoder are learned jointly to interact well but they are treated as independent units for modularity.\nThe generator is guided in two ways during learning. First, the rationale that it produces must suffice as a replacement for the input text. In other words, the target vector (sentiment) arising from the rationale should be close to the gold sentiment. The corresponding loss function is given by\nL(z,x,y) = \u2016enc(z,x)\u2212 y\u201622\nNote that the loss function depends directly (parametrically) on the encoder but only indirectly on the generator via the sampled selection.\nSecond, we must guide the generator to realize short and coherent rationales. It should select only a few words and those selections should form phrases\n(consecutive words) rather than represent isolated, disconnected words. We therefore introduce an additional regularizer over the selections\n\u2126(z) = \u03bb1\u2016z\u2016+ \u03bb2 \u2211 t |zt \u2212 zt\u22121|\nwhere the first term penalizes the number of selections while the second one discourages transitions (encourages continuity of selections). Note that this regularizer also depends on the generator only indirectly via the selected rationale. This is because it is easier to assess the rationale once produced rather than directly guide how it is obtained.\nOur final cost function is the combination of the two, cost(z,x,y) = L(z,x,y) + \u2126(z). Since the selections are not provided during training, we minimize the expected cost:\nmin \u03b8e,\u03b8g \u2211 (x,y)\u2208D Ez\u223cgen(x) [cost(z,x,y)]\nwhere \u03b8e and \u03b8g denote the set of parameters of the encoder and generator, respectively, and D is the collection of training instances. Our joint objective encourages the generator to compress the input text into coherent summaries that work well with the associated encoder it is trained with.\nMinimizing the expected cost is challenging since it involves summing over all the possible choices of rationales z. This summation could potentially be made feasible with additional restrictive assumptions about the generator and encoder. However, we assume only that it is possible to efficiently sample from the generator.\nDoubly stochastic gradient We now derive a sampled approximation to the gradient of the expected cost objective. This sampled approximation is obtained separately for each input text x so as to work well with an overall stochastic gradient method. Consider therefore a training pair (x,y). For the parameters of the generator \u03b8g,\n\u2202Ez\u223cgen(x) [cost(z,x,y)] \u2202\u03b8g\n= \u2211 z cost(z,x,y) \u00b7 \u2202p(z|x) \u2202\u03b8g\n= \u2211 z cost(z,x,y) \u00b7 \u2202p(z|x) \u2202\u03b8g \u00b7 p(z|x) p(z|x)\nUsing the fact (log f(\u03b8))\u2032 = f \u2032(\u03b8)/f(\u03b8), we get\u2211 z cost(z,x,y) \u00b7 \u2202p(z|x) \u2202\u03b8g \u00b7 p(z|x) p(z|x)\n= \u2211 z cost(z,x,y) \u00b7 \u2202 log p(z|x) \u2202\u03b8g \u00b7 p(z|x)\n= Ez\u223cgen(x) [ cost(z,x,y) \u2202 log p(z|x)\n\u2202\u03b8g ] The last term is the expected gradient where the expectation is taken with respect to the generator distribution over rationales z. Therefore, we can simply sample a few rationales z from the generator gen(x) and use the resulting average gradient in an overall stochastic gradient method. A sampled approximation to the gradient with respect to the encoder parameters \u03b8e can be derived similarly,\n\u2202Ez\u223cgen(x) [cost(z,x,y)] \u2202\u03b8e\n= \u2211 z \u2202cost(z,x,y) \u2202\u03b8e \u00b7 p(z|x)\n= Ez\u223cgen(x) [ \u2202cost(z,x,y)\n\u2202\u03b8e ] Choice of recurrent unit We employ recurrent convolution (RCNN), a refinement of local-ngram based convolution. RCNN attempts to learn n-gram features that are not necessarily consecutive, and average features in a dynamic (recurrent) fashion. Specifically, for bigrams (filter width n = 2) RCNN computes ht = f(xt,ht\u22121) as follows\n\u03bbt = \u03c3(W \u03bbxt + U \u03bbht\u22121 + b \u03bb)\nc (1) t = \u03bbt c (1) t\u22121 + (1\u2212 \u03bbt) (W1xt) c (2) t = \u03bbt c (2) t\u22121 + (1\u2212 \u03bbt) (c (1) t\u22121 + W2xt)\nht = tanh(c (2) t + b)\nRCNN has been shown to work remarkably in classification and retrieval applications (Lei et al., 2015a; Lei et al., 2015b) compared to other alternatives such CNNs and LSTMs. We use it for all the recurrent units introduced in our model."}, {"heading": "5 Experiments", "text": "We evaluate the proposed joint model on two NLP applications: (1) multi-aspect sentiment analysis on product reviews and (2) similar text retrieval on AskUbuntu question answering forum."}, {"heading": "5.1 Multi-aspect Sentiment Analysis", "text": "Dataset We use the BeerAdvocate2 review dataset used in prior work (McAuley et al., 2012).3 This dataset contains 1.5 million reviews written by the website users. The reviews are naturally multiaspect \u2013 each of the review contains multiple sentences describing the overall impression or one particular aspect of a beer, including appearance, smell (aroma), palate and the taste. In addition to the written text, the reviewer provides the ratings (on a scale of 0 to 5 stars) for each aspect as well as an overall rating. The ratings can be fractional (e.g. 3.5 stars), so we normalize the scores to [0, 1] and use them as the (only) supervision for regression.\nMcAuley et al. (2012) also provided sentencelevel annotations on around 1,000 reviews. Each sentence is annotated with one (or multiple) aspect label, indicating what aspect this sentence covers. We use this set as our test set to evaluate the precision of words in the extracted rationales.\nTable 1 shows several statistics of the beer review dataset. The sentiment correlation between any pair of aspects (and the overall score) is quite high, getting 63.5% on average and a maximum of 79.1% (between the taste and overall score). If directly training the model on this set, the model can be confused due to such strong correlation. We therefore\n2www.beeradvocate.com 3http://snap.stanford.edu/data/\nweb-BeerAdvocate.html\nMethod Appearance Smell Palate% precision % selected % precision % selected % precision % selected SVM 38.3 13 21.6 7 24.9 7 Attention model 80.6 13 88.4 7 65.3 7 Generator (independent) 94.8 13 93.8 7 79.3 7 Generator (recurrent) 96.3 14 95.1 7 80.2 7\nTable 3: Precision of selected rationales for the first three aspects. The precision is evaluated based on whether the selected words are in the sentences describing the target aspect, based on the sentence-level annotations. Best training epochs are selected based on the objective value on the development set (no sentence annotation is used).\nSVM Attention Gen (independent) Gen (recurrent)\n1 73.9 1 89.1 6 97.4 12 96.5 3 55.9 3 88.1 13 94.9 14 96.3 5 48.5 5 86.4 16 92.9 16 91.2\n7 44.7 7 84.1 9 42.2 9 82.3 11 41.2 11 79.8 13 38.3 13 77.1 15 36.7 15 74.4 17 35.1 17 71.6\n30\n48\n65\n83\n100\n5 7 9 11 13 15 17\nSVM Attention Gen (independent) Gen (recurrent)\n1\nFigure 2: Precision (y-axis) when various percentages of text are extracted as rationales (x-axis) for the appearance aspect.\nperform a preprocessing step, picking \u201cless correlated\u201d examples from the dataset.4 This gives us a de-correlated subset for each aspect, each containing about 80k to 90k reviews. We use 10k as the development set. We focus on three aspects since the fourth aspect taste still gets > 50% correlation with the overall sentiment.\nEncoder Performance Before training the joint model, it is worth assessing the neural encoder separately to check how accurate the neural network predicts the sentiment. To this end, we compare neural encoders with bigram SVM model, training medium and large SVM models using 260k and all 1580k reviews respectively. As shown in Table 2, the recurrent neural network models outperform the SVM model for sentiment prediction and also require less training data to achieve the performance. The LSTM and RCNN units obtain similar test error, getting 0.0094 and 0.0087 mean squared error respectively. The RCNN unit performs slightly better and uses less parameters. Based on the results, we choose the RCNN encoder network with 2 stacking layers and 200 hidden states in the joint model.\n4Specifically, for each aspect we train a simple linear regression model to predict the rating of this aspect given the ratings of the other four aspects. We then keep picking reviews with largest prediction error until the sentiment correlation in the selected subset increases dramatically.\n0 50 100\n0.03\n0.04\n0.05\n0.06\nGen (recurrent) Gen (independent)\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 4: Learning curves of the optimized cost function on the development set and the precision of rationales on the test set. The smell (aroma) aspect is the target aspect.\nRationale Selection To extract the supporting rationales for each aspect, we train the joint encodergenerator model on each de-correlated subset. Similar to the encoder, we use RCNN unit with 200 states as the forward and backward recurrent unit for the generator gen(). The dependent selection generator has one additional recurrent layer. For this layer we use a small RCNN unit with 30 states, so the dependent version still has a number of parameters comparable to the independent version. The two versions of the generator have 358k and 323k parameters respectively. We set the cardinality regularization \u03bb1 between values {2e \u2212 4, 3e \u2212 4, 4e \u2212 4} so the extracted rationale texts are neither too long nor too short. For simplicity, we set \u03bb2 = 2\u03bb1 to encourage local coherency of the extraction.\nFor comparison we use the bigram SVM model and implement an attention-based neural network model. The SVM model successively extracts unigram or bigram (from the test reviews) with the highest feature. The attention-based model learns a normalized attention vector of the input tokens (using similarly the forward and backward RNNs), then the model averages over the encoder states accordingly to the attention, and feed the averaged vector\npoured into a sniBer . produces a small coffee head that reduces quickly . black as night . preAy typical imp . roasted malts hit on the nose . a liAle sweet chocolate follows . big toasty character on the taste . in between i 'm geDng plenty of dark chocolate and some biAer espresso . it finishes with hop biAerness . nice smooth mouthfeel with perfect carbona9on for the style . overall a nice stout i would love to have again , maybe with some age on it .\nto the output layer. Similar to the SVM model, the attention-based model can selects words based on their attention weights.\nTable 3 presents the precision of the extracted rationales calcul ted based on sentence-level aspect annotations. The \u03bb1 regularization hyper-parameter is tuned so the two versions of our model extract similar number of words as rationales. The SVM and attention-based model are constrained similarly for comparison. Figure 2 further shows the precision when different amounts of text are extracted. For our model, this corresponds to changing the \u03bb1 regularization. As shown in the table and the figure, our encoder-generator networks extract text pieces describing the target aspect with high precision, ranging from 80% to 96% across the three aspects appearance, smell and palate. The SVM baseline performs poorly, achieving around 30% accuracy. The attention-based model achieves reasonable but worse performance than the rationale generator, suggesting the potential of directly modeling rationales as explicit extraction.\nFigure 4 shows the learning curves of our model for the smell aspect. In the early training epochs, both the independent and recurrent dependent selection models fail to produce good rationales, getting low precision as a result. After a few epochs of exploration however, the models start to achieve high accuracy. We observe that the recurrent version learns more quickly in general, but both versions ob-\ntain close results in the end. Finally we conduct a qualitative case study on the extracted rationales. Figure 3 presents several reviews, with highlighted rationales predicted by the model. Our rationale generator identifies key phrases or adjectives that indicate the sentiment of a particular aspect."}, {"heading": "5.2 Similar Text Retrieval on QA Forum", "text": "Dataset For our second application, we use the real-world AskUbuntu5 dataset used in recent work (dos Santos et al., 2015; Lei et al., 2015b). This set contains a set of 167k unique questions (each consisting a question title and a body) and 16k user-identified similar question pairs. Following previous work, this data is used to train the neural encoder that learns the vector representation of the input question, optimizing the cosine distance (i.e. cosine similarity) between similar questions against random non-similar ones. We use the \u201cone-versusall\u201d hinge loss (i.e. positive versus other negatives) for the encoder, similar to (Lei et al., 2015b). During development and testing, the model is used to score 20 candidate questions given each query question, and a total of 400\u00d720 query-candidate question pairs are annotated for evaluation6.\nTask/Evaluation Setup The question descriptions are often long and fraught with irrelevant details. In\n5askubuntu.com 6https://github.com/taolei87/askubuntu\ni accidentally removed the ubuntu soBware centre , when i was actually trying to remove my ubuntu one applica9ons . although i do n't remember directly uninstalling the centre , i think dele9ng one of those packages might have triggered it . i can not look at history of applica9on changes , as the soBware centre is missing . please advise on how to install , or rather reinstall , ubuntu soBware centre on my computer . how do i install ubuntu soBware centre applica9on ?\ni know this will be an odd ques9on , but i was wondering if anyone knew how to install the ubuntu installer package in an ubuntu\ninstalla9on . to clarify , when you boot up to an ubuntu livecd , it 's got the installer program available so that you can install ubuntu to\na drive . naturally , this program is not present in the installed ubuntu . is there , though , a way to download and install it like other\npackages ? invariably , someone will ask what i 'm trying to do , and the answer \u2026 install installer package on an installed system ?\nMAP (dev) MAP (test) %words Full title 56.5 60.0 10.1 Full body 54.2 53.0 89.9 Independent 55.7 53.6 9.7 56.3 52.6 19.7 Recurrent 56.1 54.6 11.6 56.5 55.6 32.8\nTable 4: Comparison between rati nale models (middle and bottom rows) and the baselines using full title or body (top row).\nGen (independent) Gen (recurrent)\n0.052 47.08 0.063 50.54 0.058 52.36 0.067 49.48 0.059 46.02 0.07 51.96 0.062 49.76 0.078 51.54 0.064 47.94 0.086 52.55 0.068 48.93 0.095 53.59\n0.07 49.5 0.108 53.15 0.081 52.18 0.112 51.48 0.081 51.84 0.116 54.62 0.094 51.24 0.121 52.12 0.094 52.21 0.137 53 0.097 53.61 0.163 53.2 0.098 54.11 0.179 54.13 0.122 49.03 0.193 52.11 0.133 54.19 0.262 52.32 0.135 50.21 0.277 50.87 0.136 48.22 0.328 53.21 0.145 50.96 0.328 55.61 0.155 52.91 0.347 51 0.173 52.74 0.378 54.93 0.197 52.6\n45.0\n47.8\n50.5\n53.3\n56.0\n5% 9% 13% 16% 20%\nGen (independent) Gen (recurrent)\n1\nFigure 6: Retrieval MAP on the test set when various percentages of the texts are chosen as rationales. Data points correspond to models trained with different hyper-parameters.\nthis set-up, a fraction of the original question text should be sufficient to represent its content, and be used for retrieving similar questions. Therefore, we will evaluate rationales based on the accuracy of the question retrieval task, assuming that better rationales achieve higher performance. To put this performance in context, we also report the accuracy when full body of a question is used, as well as titles alone. The latter constitutes an upper bound on the model performance as in this dataset titles provide short, informative summaries of the question content. We evaluate the rationales using the mean average precision (MAP) of retrieval.\nResults Table 4 presents the results of our rationale model. We explore a range of hyper-parameter values7. We include two runs for each version. The first one achieves the highest MAP on the development set, The second run is selected to compare the models when they use roughly 10% of question text (7 words on average). We also show the results of differ nt runs i Figure 6. The rationales achieve the MAP up to 56.5%, getting close to using the titles. The models also outperform the baseline of using the noisy question bodies, indicating the the models\u2019 capacity of extracting short but important fragments.\nFigure 5 shows the rationales for several qu stions in the AskUbuntu domain, using the recurrent version with around 10% extraction. Interestingly, the model does not always select words from the question title. The reasons are that the question body can contain the same or even complementary information useful for retrieval. Indeed, some rationale fragments shown in the figure are error messages, which are typically not in the titles but very useful to identify similar questions."}, {"heading": "6 Conclusion", "text": "We propose a novel modular neural framework to automatically generate concise yet sufficient text fragments (i.e rationales) to justify predictions made by neural networks. The model can be trained in the end-to-end fashion, requiring no explicit annotation of rationales. We successfully illustrate the capacity of the model in two real-world applications.\n7\u03bb1 \u2208 {.008, .01, .012, .015}, \u03bb2 = {0, \u03bb1, 2\u03bb1}, dropout \u2208 {0.1, 0.2}"}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Abccnn: An attention based convolutional neural network for visual question answering", "author": ["Kan Chen", "Jiang Wang", "Liang-Chieh Chen", "Haoyuan Gao", "Wei Xu", "Ram Nevatia."], "venue": "arXiv preprint arXiv:1511.05960.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1601.06733.", "citeRegEx": "Cheng et al\\.,? 2016", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Learning hybrid representations to retrieve semantically equivalent questions", "author": ["Cicero dos Santos", "Luciano Barbosa", "Dasha Bogdanova", "Bianca Zadrozny."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Santos et al\\.,? 2015", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of NAACL.", "citeRegEx": "Faruqui et al\\.,? 2015a", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of ACL.", "citeRegEx": "Faruqui et al\\.,? 2015b", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Building a shared world: mapping distributional to modeltheoretic semantic spaces", "author": ["Aur\u00e9lie Herbelot", "Eva Maria Vecchi."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Lin-", "citeRegEx": "Herbelot and Vecchi.,? 2015", "shortCiteRegEx": "Herbelot and Vecchi.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen."], "venue": "Advances in Neural Information Processing Systems, pages 190\u2013198.", "citeRegEx": "Hermans and Schrauwen.,? 2013", "shortCiteRegEx": "Hermans and Schrauwen.", "year": 2013}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Fei-Fei Li."], "venue": "arXiv preprint arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Mind the gap: A generative approach to interpretable feature selection and extraction", "author": ["B Kim", "JA Shah", "F Doshi-Velez."], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1565\u20131575, Lisbon, Portugal, September.", "citeRegEx": "Lei et al\\.,? 2015a", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Semi-supervised question retrieval with gated convolutions", "author": ["Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi Jaakkola", "Katerina Tymoshenko", "Alessandro Moschitti", "Lluis Marquez."], "venue": "arXiv preprint arXiv:1512.05726.", "citeRegEx": "Lei et al\\.,? 2015b", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "author": ["Benjamin Letham", "Cynthia Rudin", "Tyler H. McCormick", "David Madigan."], "venue": "Annals of Applied Statistics, 9(3):1350\u20131371.", "citeRegEx": "Letham et al\\.,? 2015", "shortCiteRegEx": "Letham et al\\.", "year": 2015}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky."], "venue": "Proceedings of NAACL.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "From softmax to sparsemax: A sparse model of attention and multi-label classification", "author": ["Andr\u00e9 F.T. Martins", "Ram\u00f3n Fernandez Astudillo."], "venue": "CoRR, abs/1602.02068.", "citeRegEx": "Martins and Astudillo.,? 2016", "shortCiteRegEx": "Martins and Astudillo.", "year": 2016}, {"title": "Learning attitudes and attributes from multi-aspect reviews", "author": ["Julian McAuley", "Jure Leskovec", "Dan Jurafsky."], "venue": "Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020\u20131025. IEEE.", "citeRegEx": "McAuley et al\\.,? 2012", "shortCiteRegEx": "McAuley et al\\.", "year": 2012}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Rockt\u00e4schel et al\\.,? 2016", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko."], "venue": "arXiv preprint arXiv:1511.05234.", "citeRegEx": "Xu and Saenko.,? 2015", "shortCiteRegEx": "Xu and Saenko.", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola."], "venue": "arXiv preprint arXiv:1511.02274.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Using \u201dannotator rationales\u201d to improve machine learning for text categorization", "author": ["Omar Zaidan", "Jason Eisner", "Christine D. Piatko."], "venue": "Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Com-", "citeRegEx": "Zaidan et al\\.,? 2007", "shortCiteRegEx": "Zaidan et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "The first scenario concerns with multi-aspect sentiment analysis exemplified by the beer review corpus (McAuley et al., 2012).", "startOffset": 103, "endOffset": 125}, {"referenceID": 13, "context": "Developing sparse interpretable models holds considerably interest in the broader research community(Letham et al., 2015; Kim et al., 2015).", "startOffset": 100, "endOffset": 139}, {"referenceID": 10, "context": "Developing sparse interpretable models holds considerably interest in the broader research community(Letham et al., 2015; Kim et al., 2015).", "startOffset": 100, "endOffset": 139}, {"referenceID": 8, "context": "Efforts in this area include analyzing and visualizing state activation (Hermans and Schrauwen, 2013; Karpathy et al., 2015; Li et al., 2016), learning sparse interpretable word vectors (Faruqui et al.", "startOffset": 72, "endOffset": 141}, {"referenceID": 9, "context": "Efforts in this area include analyzing and visualizing state activation (Hermans and Schrauwen, 2013; Karpathy et al., 2015; Li et al., 2016), learning sparse interpretable word vectors (Faruqui et al.", "startOffset": 72, "endOffset": 141}, {"referenceID": 14, "context": "Efforts in this area include analyzing and visualizing state activation (Hermans and Schrauwen, 2013; Karpathy et al., 2015; Li et al., 2016), learning sparse interpretable word vectors (Faruqui et al.", "startOffset": 72, "endOffset": 141}, {"referenceID": 5, "context": ", 2016), learning sparse interpretable word vectors (Faruqui et al., 2015b), and linking word vectors to semantic lexicons or word properties (Faruqui et al.", "startOffset": 52, "endOffset": 75}, {"referenceID": 4, "context": ", 2015b), and linking word vectors to semantic lexicons or word properties (Faruqui et al., 2015a; Herbelot and Vecchi, 2015).", "startOffset": 75, "endOffset": 125}, {"referenceID": 6, "context": ", 2015b), and linking word vectors to semantic lexicons or word properties (Faruqui et al., 2015a; Herbelot and Vecchi, 2015).", "startOffset": 75, "endOffset": 125}, {"referenceID": 0, "context": "Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015).", "startOffset": 81, "endOffset": 212}, {"referenceID": 2, "context": "Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015).", "startOffset": 81, "endOffset": 212}, {"referenceID": 15, "context": "Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015).", "startOffset": 81, "endOffset": 212}, {"referenceID": 1, "context": "Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015).", "startOffset": 81, "endOffset": 212}, {"referenceID": 19, "context": "Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015).", "startOffset": 81, "endOffset": 212}, {"referenceID": 20, "context": "Attention based models offer another lens to the inner workings of neural models (Bahdanau et al., 2015; Cheng et al., 2016; Martins and Astudillo, 2016; Chen et al., 2015; Xu and Saenko, 2015; Yang et al., 2015).", "startOffset": 81, "endOffset": 212}, {"referenceID": 18, "context": "Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as visualization and interpretability (Rush et al., 2015; Rockt\u00e4schel et al., 2016; Hermann et al., 2015).", "startOffset": 146, "endOffset": 213}, {"referenceID": 17, "context": "Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as visualization and interpretability (Rush et al., 2015; Rockt\u00e4schel et al., 2016; Hermann et al., 2015).", "startOffset": 146, "endOffset": 213}, {"referenceID": 7, "context": "Such models have been successfully applied to many NLP problems, improving both prediction accuracy as well as visualization and interpretability (Rush et al., 2015; Rockt\u00e4schel et al., 2016; Hermann et al., 2015).", "startOffset": 146, "endOffset": 213}, {"referenceID": 21, "context": "Finally, we contrast our work with rationale-based classification (Zaidan et al., 2007) which seeks to reduce supervised annotations by relying on richer annotations in the form of human-provided rationales.", "startOffset": 66, "endOffset": 87}, {"referenceID": 11, "context": "RCNN has been shown to work remarkably in classification and retrieval applications (Lei et al., 2015a; Lei et al., 2015b) compared to other alternatives such CNNs and LSTMs.", "startOffset": 84, "endOffset": 122}, {"referenceID": 12, "context": "RCNN has been shown to work remarkably in classification and retrieval applications (Lei et al., 2015a; Lei et al., 2015b) compared to other alternatives such CNNs and LSTMs.", "startOffset": 84, "endOffset": 122}, {"referenceID": 16, "context": "Dataset We use the BeerAdvocate2 review dataset used in prior work (McAuley et al., 2012).", "startOffset": 67, "endOffset": 89}, {"referenceID": 12, "context": "Dataset For our second application, we use the real-world AskUbuntu5 dataset used in recent work (dos Santos et al., 2015; Lei et al., 2015b).", "startOffset": 97, "endOffset": 141}, {"referenceID": 12, "context": "positive versus other negatives) for the encoder, similar to (Lei et al., 2015b).", "startOffset": 61, "endOffset": 80}], "year": 2017, "abstractText": null, "creator": "LaTeX with hyperref package"}}}