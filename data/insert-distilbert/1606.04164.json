{"id": "1606.04164", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2016", "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "abstract": "in this paper, moreover we propose a novel finetuning algorithm for the recently introduced multi - agent way, mulitlingual neural rendering machine translate that enables zero - resource machine translation. when used together with novel many - to - one translation strategies, we empirically show importantly that this finetuning algorithm allows the multi - way, multilingual model to translate a zero - resource language pair ( 1 ) as well as provide a single - pair intensive neural translation model trained with up to 1m direct parallel sentences of the same individual language than pair languages and ( 2 ) better than pivot - based translation strategy, while keeping only one additional coherent copy of attention - related parameters.", "histories": [["v1", "Mon, 13 Jun 2016 22:40:33 GMT  (25kb)", "http://arxiv.org/abs/1606.04164v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["orhan firat", "baskaran sankaran", "yaser al-onaizan", "fatos t yarman-vural", "kyunghyun cho"], "accepted": true, "id": "1606.04164"}, "pdf": {"name": "1606.04164.pdf", "metadata": {"source": "CRF", "title": "Zero-Resource Translation with Multi-Lingual Neural Machine Translation", "authors": ["Orhan Firat", "Baskaran Sankaran", "Fatos T. Yarman"], "emails": ["orhan.firat@ceng.metu.edu.tr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n04 16\n4v 1\n[ cs\n.C L\n] 1\n3 Ju"}, {"heading": "1 Introduction", "text": "A recently introduced neural machine translation (Forcada and N\u0303eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research. Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015). Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities other than natural\n\u22c6 Work carried out while the author was at IBM Research.\nlanguage text in translation (Luong et al., 2015a; Caglayan et al., 2016). Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016).\nIn this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016). Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in traditional machine translation research (Wu and Wang, 2007; Utiyama and Isahara, 2007), but we explore a way to use the multi-way, multilingual neural model to translate directly from a source to target language.\nIn doing so, we begin by studying different translation strategies available in the multi-way, multilingual model in Sec. 3\u20134. The strategies include a usual one-to-one translation as well as variants of many-to-one translation for multi-source translation (Zoph and Knight, 2016). We empirically show that the many-to-one strategies significantly outperform the one-to-one strategy.\nWe move on to zero-resource translation by first evaluating a vanilla multi-way, multilingual model on a zero-resource language pair, which revealed that the vanilla model cannot do zero-resource translation in Sec. 6.1. Based on the many-to-one strategies we proposed earlier, we design a novel finetun-\ning strategy that does not require any direct parallel corpus between a target, zero-resource language pair in Sec. 5.2, which uses the idea of generating a pseudo-parallel corpus (Sennrich et al., 2015a). This strategy makes an additional copy of the attention mechanism and finetunes only this small set of parameters.\nLarge-scale experiments with Spanish, French and English show that the proposed finetuning strategy allows the multi-way, multilingual neural translation model to perform zero-resource translation as well as a single-pair neural translation model trained with up to 1M true parallel sentences. This result re-confirms the potential of the multi-way, multilingual model for low/zero-resource language translation, which was earlier argued by Firat et al. (2016)."}, {"heading": "2 Multi-Way, Multilingual", "text": "Neural Machine Translation\nRecently Firat et al. (2016) proposed an extension of attention-based neural machine translation (Bahdanau et al., 2015) that can handle multiway, multilingual translation with a shared attention mechanism. This model was designed to handle multiple source and target languages. In this section, we briefly overview this multi-way, multilingual model. For more detailed exposition, we refer the reader to (Firat et al., 2016)."}, {"heading": "2.1 Model Description", "text": "The goal of multi-way, multilingual model is to build a neural translation model that can translate a source sentence given in one of N languages into one of M target languages. Thus to handle those N source and M target languages, the model consists of N encoders and M decoders. Unlike these language-specific encoders and decoders, only a single attention mechanism is shared across all M \u00d7N language pairs.\nEncoder An encoder for the n-th source language reads a source sentence X = (x1, . . . , xTx) as a sequence of linguistic symbols and returns a set of context vectors Cn = {\nh n 1 , . . . ,hnTx\n}\n. The encoder is usually implemented as a bidirectional recurrent network (Schuster and Paliwal, 1997), and each context vector hnt is a concatenation of the forward and reverse recurrent networks\u2019 hidden states\nat time t. Without loss of generality, we assume that the dimensionalities of the context vector for all source languages are all same.\nDecoder and Attention Mechanism A decoder for the m-th target language is a conditional recurrent language model (Mikolov et al., 2010). At each time step t\u2032, it updates its hidden state by\nz m t\u2032 = \u03d5 m(zmt\u2032\u22121, y\u0303 m t\u2032\u22121, c m t\u2032 ),\nbased on the previous hidden state zmt\u2032\u22121, previous target symbol y\u0303mt\u2032\u22121 and the time-dependent context vector cmt\u2032 . \u03d5\nm is a gated recurrent unit (GRU, (Cho et al., 2014)).\nThe time-dependent context vector is computed by the shared attention mechanism as a weighted sum of the context vectors from the encoder Cn:\nc m t\u2032 = U\nTx \u2211\nt=1\n\u03b1 m,n t,t\u2032 h n t + b, (1)\nwhere\n\u03b1 m,n t,t\u2032 \u221d exp\n(\nfscore(W n h n t ,W m z m t\u2032\u22121, y\u0303 m t\u2032\u22121)\n)\n.\n(2)\nThe scoring function fscore returns a scalar and is implemented as a feedforward neural network with a single hidden layer. For more variants of the attention mechanism for machine translation, see (Luong et al., 2015b).\nThe initial hidden state of the decoder is initialized as\nz m 0 = \u03c6 m init(W n h n t ). (3)\nWith the new hidden state zmt\u2032 , the probability distribution over the next symbol is computed by\np(yt = w|y\u0303<t,Xn) \u221d exp(gmw (zmt , cmt ,Emy [y\u0303t\u22121]), (4)\nwhere gmw is a decoder specific parametric function that returns the unnormalized probability for the next target symbol being w."}, {"heading": "2.2 Learning", "text": "Training this multi-way, multilingual model does not require multi-way parallel corpora but only a\nset of bilingual corpora. For each bilingual pair, the conditional log-probability of a ground-truth translation given a source sentence is maximize by adjusting the relevant parameters following the gradient of the log-probability."}, {"heading": "3 Translation Strategies", "text": ""}, {"heading": "3.1 One-to-One Translation", "text": "In the original paper by Firat et al. (2016), only one translation strategy was evaluated, that is, one-toone translation. This one-to-one strategy works on a source sentence given in one language by taking the encoder of that source language, the decoder of a target language and the shared attention mechanism. These three components are glued together as if they form a single-pair neural translation model and translates the source sentence into a target language.\nWe however notice that this is not the only translation strategy available with the multi-way, multilingual model. As we end up with multiple encoders, multiple decoder and a shared attention mechanism, this model naturally enables us to exploit a source sentence given in multiple languages, leading to a many-to-one translation strategy which was proposed recently by Zoph and Knight (2016) in the context of neural machine translation.\nUnlike (Zoph and Knight, 2016), the multi-way, multilingual model is not trained with multi-way parallel corpora. This however does not necessarily imply that the model cannot be used in this way. In the remainder of this section, we propose two alternatives for doing multi-source translation with the multi-way, multilingual model."}, {"heading": "3.2 Many-to-One Translation", "text": "In this section, we consider a case where a source sentence is given in two languages, X1 and X2. However, any of the approaches described below applies to more than two source languages trivially.\nIn this multi-way, multilingual model, multisource translation can be thought of as averaging two separate translation paths. For instance, in the case of Es+Fr to En, we want to combine Es\u2192En and Fr\u2192En so as to get a better English translation. We notice that there are two points in the multi-way,\nmultilingual model where this averaging may happen.\nEarly Average The first candidate is to averaging two translation paths when computing the timedependent context vector (see Eq. (1).) At each time t in the decoder, we compute a time-dependent context vector for each source language, c1t and c 2 t respectively for the two source languages. In this early averaging strategy, we simply take the average of these two context vectors:\nct = c 1 t + c 2 t\n2 . (5)\nSimilarly, we initialize the decoder\u2019s hidden state to be the average of the initializers of the two encoders:\nz0 = 1\n2\n(\n\u03c6init(\u03c6 1 init(h 1 Tx1 )) + \u03c6init(\u03c6 2 init(h 2 Tx1 )) ) ,\n(6)\nwhere \u03c6init is the decoder\u2019s initializer (see Eq. (3).)\nLate Average Alternatively, we can average those two translation paths (e.g., Es\u2192En and Fr\u2192En) at the output level. At each time t, each translation path computes the distribution over the target vocabulary, i.e., p(yt = w|y<t,X1) and p(yt = w|y<t,X2). We then average them to get the multi-source output distribution:\np(yt = w|y<t,X1,X2) = (7) 1\n2 (p(yt = w|y<t,X1) + p(yt = w|y<t)).\nAn advantage of this late averaging strategy over the early averaging one is that this can work even when those two translation paths were not from a single multilingual model. They can be two separately single-pair models. In fact, if X1 and X2 are same and the two translation paths are simply two different models trained on the same language pair\u2013 direction, this is equivalent to constructing an ensemble, which was found to greatly improve translation quality (Sutskever et al., 2014)\nEarly+Late Average The two strategies above can be further combined by late-averaging the output distributions from the early averaged model and the late averaged one. We empirically evaluate this early+late average strategy as well."}, {"heading": "4 Experiments: Translation Strategies and Multi-Source Translation", "text": "Before continuing on with zero-resource machine translation, we first evaluate the translation strategies described in the previous section on multisource translation, as these translation strategies form a basic foundation on which we extend the multi-way, multilingual model for zero-resource machine translation."}, {"heading": "4.1 Settings", "text": "When evaluating the multi-source translation strategies, we use English, Spanish and French, and focus on a scenario where only En-Es and En-Fr parallel corpora are available."}, {"heading": "4.1.1 Corpora", "text": "En-Es We combine the following corpora to form 34.71m parallel Es-En sentence pairs: UN (8.8m), Europarl-v7 (1.8m), news-commentary-v7 (150k), LDC2011T07-T12 (2.9m) and internal technicaldomain data (21.7m).\nEn-Fr We combine the following corpora to form 65.77m parallel En-Fr sentence pairs: UN (9.7m), Europarl-v7 (1.9m), news-commentary-v7 (1.2m), LDC2011T07-T10 (1.6m), ReutersUN (4.5m), internal technical-domain data (23.5m) and Gigaword R2 (20.66m).\nEvaluation Sets We use newstest-2012 and newstest-2013 from WMT as development and test sets, respectively.\nMonolingual Corpora We do not use any additional monolingual corpus.\nPreprocessing All the sentences are tokenized using the tokenizer script from Moses (Koehn et al., 2007). We then replace special tokens, such as numbers, dates and URL\u2019s with predefined markers, which will be replaced back with the original tokens after decoding. After using byte pair encoding (BPE, (Sennrich et al., 2015b)) to get subword symbols, we end up with 37k, 43k and 45k unique tokens for English, Spanish and French, respectively. For training, we only use sentence pairs in which both sentences are only up to 50 symbols long.\nSee Table 1 for the detailed statistics.\n# Sents Train Dev\u2020 Test\u2021\nEn-Es 34.71m 3003 3000 En-Fr 65.77m 3003 3000\nEn-Es-Fr 11.32m 3003 3000\nTable 1: Data statistics. \u2020: newstest-2012. \u2021: newstest-2013"}, {"heading": "4.2 Models and Training", "text": "We start from the code made publicly available as a part of (Firat et al., 2016). We made two changes to the original code. First, we replaced the decoder with the conditional gated recurrent network with the attention mechanism as outlines in (Firat and Cho, 2016). Second, we feed a binary indicator vector of which encoder(s) the source sentence was processed by to the output layer of each decoder (gmw in Eq. (4)). Each dimension of the indicator vector corresponds to one source language, and in the case of multi-source translation, there may be more than one dimensions set to 1.\nWe train the following models: four single-pair models (Es\u2194En and Fr\u2194En) and one multi-way, multilingual model (Es,Fr,En\u2194Es,Fr,En). As proposed by Firat et al. (2016), we share one attention mechanism for the latter case.\nTraining We closely follow the setup from (Firat et al., 2016). Each symbol is represented as a 620-dimensional vector. Any recurrent layer, be it in the encoder or decoder, consists of 1000 gated recurrent units (GRU, (Cho et al., 2014)), and the attention mechanism has a hidden layer of 1200 tanh units (fscore in Eq. (2)). We use Adam (Kingma and Ba, 2015) to train a model, and the gradient at each update is computed using a minibatch of at most 80 sentence pairs. The gradient is clipped to have the norm of at most 1 (Pascanu et al., 2012). We early-stop any training using the T-B score on a development set."}, {"heading": "4.3 One-to-One Translation", "text": "We first confirm that the multi-way, multilingual translation model indeed works as well as singlepair models on the translation paths that were considered during training, which was the major claim in (Firat et al., 2016). In Table 2, we present the re-\nhttps://github.com/nyu-dl/dl4mt-multi T-B score is defined as TER\u2212BLEU\n2 which we found to be\nmore stable than either TER or BLEU alone for the purpose of early-stopping (Zhao and Chen, 2009).\nsults on four language pair-directions (Es\u2194En and Fr\u2194En).\nIt is clear that the multi-way, multilingual model indeed performs comparably on all the four cases with less parameters (due to the shared attention mechanism.) As observed earlier in (Firat et al., 2016), we also see that the multilingual model performs better when a target language is English."}, {"heading": "4.4 Many-to-One Translation", "text": "We consider translating from a pair of source sentences in Spanish (Es) and French (Fr) to English (En). It is important to note that the multilingual model was not trained with any multi-way parallel corpus. Despite this, we observe that the early averaging strategy improves the translation quality (measured in BLEU) by 3 points in the case of the test set (compare Table 2 (a\u2013b) and Table 3 (a).) We conjecture that this happens as training the multilingual model has implicitly encouraged the model to find a common context vector space across multiple source languages.\nThe late averaging strategy however outperforms the early averaging in both cases of multilingual model and a pair of single-pair models (see Table 3 (b)) albeit marginally. The best quality was observed when the early and late averaging strate-\ngies were combined at the output level, achieving up to +3.5 BLEU (compare Table 2 (a) and Table 3 (c).)\nWe emphasize again that there was no multi-way parallel corpus consisting of Spanish, French and English during training. The result presented in this section shows that the multi-way, multilingual model can exploit multiple sources effectively without requiring any multi-way parallel corpus, and we will rely on this property together with the proposed many-to-one translation strategies in the later sections where we propose and investigate zeroresource translation."}, {"heading": "5 Zero-Resource Translation Strategies", "text": "The network architecture of multi-way, multilingual model suggests the potential for translating between two languages without any direct parallel corpus available. In the setting considered in this paper (see Sec. 4.1,) these translation paths correspond to Es\u2194Fr, as only parallel corpora used for training were Es\u2194En and Fr\u2194En.\nThe most naive approach for translating along a zero-resource path is to simply treat it as any other path that was included as a part of training. This corresponds to the one-to-one strategy from Sec. 3.1. In our experiments, it however turned out that this naive approach does not work at all, as can be seen in Table 4 (a).\nIn this section, we investigate this potential of zero-resource translation with the multi-way, multilingual model in depth. More specifically, we propose a number of approaches that enable zeroresource translation without requiring any additional bilingual corpus."}, {"heading": "5.1 Pivot-based Translation", "text": "The first set of approaches exploits the fact that the target zero-resource translation path can be decomposed into a sequence of highresource translation paths (Wu and Wang, 2007; Utiyama and Isahara, 2007). For instance, in our case, Es\u2192Fr can be decomposed into a sequence of Es\u2192En and En\u2192Fr. In other words, we translate a source sentence (Es) into a pivot language (En) and then translate the English translation into a target language (Fr).\nOne-to-One Translation The most basic approach here is to perform each translation path in the decomposed sequence independently from each other. This one-to-one approach introduces only a minimal computational complexity (the multiplicative factor of two.) We can further improve this oneto-one pivot-based translation by maintaining a set of k-best translations from the first stage (Es\u2192En), but this increase the overall computational complexity by the factor of k, making it impractical in practice. We therefore focus only on the former approach of keeping the best pivot translation in this paper.\nMany-to-One Translation With the multi-way, multilingual model considered in this paper, we can extend the naive one-to-one pivot-based strategy by replacing the second stage (En\u2192Fr) to be many-toone translation from Sec. 4.4 using both the original source language and the pivot language as a pair of source languages. We first translate the source sentence (Es) into English, and use both the original source sentence and the English translation (Es+En) to translate into the final target language (Fr).\nBoth approaches described and proposed above do not require any additional action on an alreadytrained multilingual model. They are simply different translation strategies specifically aimed at zeroresource translation."}, {"heading": "5.2 Finetuning with Pseudo Parallel Corpus", "text": "The failure of the naive zero-resource translation earlier (see Table 4 (a)) suggests that the context vectors returned by the encoder are not compatible with the decoder, when the combination was not included during training. The good translation qualities of the translation paths included in training however imply that the representations learned by the encoders and decoders are good. Based on these two observations, we conjecture that all that is needed for a zero-resource translation path is a simple adjustment that makes the context vectors from the encoder to be compatible with the target decoder. Thus, we propose to adjust this zero-resource translation path however without any additional parallel corpus.\nFirst, we generate a small set of pseudo bilingual pairs of sentences for the zero-resource language pair (Es\u2192Fr) in interest. We randomly select N sentences pairs from a parallel corpus between\nthe target language (Fr) and a pivot language (En) and translate the pivot side (En) into the source language (Es). Then, the pivot side is discarded, and we construct a pseudo parallel corpus consisting of sentence pairs of the source and target languages (EsFr).\nWe make a copy of the existing attention mechanism, to which we refer as target-specific attention mechanism. We then finetune only this targetspecific attention mechanism while keeping all the other parameters of the encoder and decoder intact, using the generated pseudo parallel corpus. We do not update any other parameters in the encoder and decoder, because they are already well-trained (evidenced by high translation qualities in Table 2) and we want to avoid disrupting the well-captured structures underlying each language.\nOnce the model has been finetuned with the pseudo parallel corpus, we can use any of the translation strategies described earlier in Sec. 3 for the finetuned zero-resource translation path. We expect a similar gain by using many-to-one translation, which we empirically confirm in the next section."}, {"heading": "6 Experiments: Zero-Resource Translation", "text": ""}, {"heading": "6.1 Without Finetuning", "text": ""}, {"heading": "6.1.1 Settings", "text": "We use the same multi-way, multilingual model trained earlier in Sec. 4.2 to evaluate the zeroresource translation strategies. We emphasize here that this model was trained only using Es-En and Fr-En bilingual parallel corpora without any Es-Fr parallel corpus.\nWe evaluate the proposed approaches to zeroresource translation with the same multi-way, multilingual model from Sec. 4.1. We specifically select\nthe path from Spanish to French (Es\u2192Fr) as a target zero-resource translation path."}, {"heading": "6.1.2 Result and Analysis", "text": "As mentioned earlier, we observed that the multiway, multilingual model cannot directly translate between two languages when the translation path between those two languages was not included in training (Table 4 (a).) On the other hand, the model was able to translate decently with the pivot-based one-to-one translation strategy, as can be see in Table 4 (b). Unsurprisingly, all the many-to-one strategies resulted in worse translation quality, which is due to the inclusion of the useless translation path (direct path between the zero-resource pair, Es-Fr.) These results clearly indicate that the multi-way, multilingual model trained with only bilingual parallel corpora is not capable of direct zero-resource translation as it is."}, {"heading": "6.2 Finetuning with a Pseudo Parallel Corpus", "text": ""}, {"heading": "6.2.1 Settings", "text": "The proposed finetuning strategy raises a number of questions. First, it is unclear how many pseudo sentence pairs are needed to achieve a decent translation quality. Because the purpose of this finetuning stage is simply to adjust the shared attention mechanism so that it can properly bridge from the sourceside encoder to the target-side decoder, we expect it to work with only a small amount of pseudo pairs. We validate this by creating pseudo corpora of different sizes\u20131k, 10k, 100k and 1m.\nSecond, we want to know how detrimental it is to use the generated pseudo sentence pairs compared to using true sentence pairs between the target language pair. In order to answer this question, we compiled a true multi-way parallel corpus by combining the subsets of UN (7.8m), Europral-v7 (1.8m), OpenSubtitles-2013 (1m), news-commentary-v7 (174k), LDC2011T07 (335k) and news-crawl (310k), and use it to finetune the model. This allows us to evaluate the effect of the pseudo and true parallel corpora on finetuning for zero-resource translation.\nLastly, we train single-pair models translating directly from Spanish to French by using the true parallel corpora. These models work as a baseline\nSee the last row of Table 1.\nagainst which we compare the multi-way, multilingual models.\nTraining Unlike the usual training procedure described in Sec. 4.2, we compute the gradient for each update using 60 sentence pairs only, when finetuning the model with the multi-way parallel corpus (either pseudo or true.)"}, {"heading": "6.2.2 Result and Analysis", "text": "Table 5 summarizes all the result. The most important observation is that the proposed finetuning strategy with pseudo-parallel sentence pairs outperforms the pivot-based approach (using the early averaging strategy from Sec. 4.4) even when we used only 1,000 such pairs (compare (b) and (d).) As we increase the size of the pseudo-parallel corpus, we observe a clear improvement. Furthermore, these models perform comparably to or better than the single-pair model trained with 1M true parallel sentence pairs, although they never saw a single true bilingual sentence pair of Spanish and French (compare (a) and (d).) Even when we trained a single-pair model with 11m true parallel pairs, the model could not match the multilingual model finetuned with 1m true parallel pairs by achieving the translation quality of 24.26 BLEU on the test set.\nAnother interesting finding is that it is only beneficial to use true parallel pairs for finetuning the multi-way, mulitilingual models when there are enough of them (1m or more). When there are only a small number of true parallel sentence pairs, we even found using pseudo pairs to be more beneficial than true ones. This effective as more apparent, when the direct one-to-one translation of the zeroresource pair was considered (see (c) in Table 5.) This applies that the misalignment between the encoder and decoder can be largely fixed by using pseudo-parallel pairs only, and we conjecture that it is easier to learn from pseudo-parallel pairs as they better reflect the inductive bias of the trained model. When there is a large amount of true parallel sentence pairs available, however, our results indicate that it is better to exploit them.\nUnlike we observed with the multi-source translation in Sec. 3.2, we were not able to see any improvement by further averaging the early-averaged and late-average decoding schemes (compare (d)\nPseudo Parallel Corpus True Parallel Corpus Pivot Many-to-1 1k 10k 100k 1m 1k 10k 100k 1m\n(a) Single-Pair Models Dev \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 11.25 21.32 Test \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 10.43 20.35 (b) \u221a No Finetuning Dev: 20.64, Test: 20.4 \u2013\n(c) Dev 0.28 10.16 15.61 17.59 0.1 8.45 16.2 20.59 Test 0.47 10.14 15.41 17.61 0.12 8.18 15.8 19.97\n(d) \u221a\nEarly Dev 19.42 21.08 21.7 21.81 8.89 16.89 20.77 22.08 Test 19.43 20.72 21.23 21.46 9.77 16.61 20.40 21.7\n(e) \u221a Early+ Dev 20.89 20.93 21.35 21.33 14.86 18.28 20.31 21.33\nLate Test 20.5 20.71 21.06 21.19 15.42 17.95 20.16 20.9\nTable 5: Zero-resource translation from Spanish (Es) to French (Fr) with finetuning. When pivot is\n\u221a , English is used as a pivot\nlanguage. Row (a) is from Table 4 (b).\nand (e).) This may be explained by the fact that the context vectors computed when creating a pseudo source (e.g., En from Es when Es\u2192Fr) already contains all the information about the pseudo source. It is simply enough to take those context vectors into account via the early averaging scheme.\nThese results clearly indicate and verify the potential of the multi-way, multilingual neural translation model in performing zero-resource machine translation. More specifically, it has been shown that the translation quality can be improved even without any direct parallel corpus available, and if there is a small amount of direct parallel pairs available, the quality may improve even further."}, {"heading": "7 Conclusion: Implications and Limitations", "text": "Implications There are two main results in this paper. First, we showed that the multi-way, multilingual neural translation model by Firat et al. (2016) is able to exploit common, underlying structures across many languages in order to better translate when a source sentence is given in multiple languages. This confirms the usefulness of positive language transfer, which has been believed to be an important factor in human language learning (Odlin, 1989; Ringbom, 2007), in machine translation. Furthermore, our result significantly expands the applicability of multi-source translation (Zoph and Knight, 2016), as it does not assume the availability of multi-way parallel corpora for training.\nSecond, the experiments on zero-resource translation revealed that it is not necessary to have a direct parallel corpus, or deep linguistic knowledge, between two languages in order to build a machine translation system. Importantly we observed that the proposed approach of zero-resource translation is better both in terms of translation quality and data efficiency than a more traditional pivot-based translation (Wu and Wang, 2007; Utiyama and Isahara, 2007). Considering that this is the first attempt at such zero-resource, or extremely low-resource, translation using neural machine translation, we expect a large progress in near future.\nLimitations Despite the promising empirical results presented in this paper, there are a number of shortcomings that needs to addressed in followup research. First, our experiments have been done only with three European languages\u2013Spanish, French and English. More investigation with a diverse set of languages needs to be done in order to make a more solid conclusion, such as was done in (Firat et al., 2016; Chung et al., 2016). Furthermore, the effect of varying sizes of available parallel corpora on the performance of zero-resource translation must be studied more in the future.\nSecond, although the proposed many-to-one translation is indeed generally applicable to any number of source languages, we have only tested a source sentence in two languages. We expect even higher improvement with more languages, but it must be tested thoroughly in the future.\nLastly, the proposed finetuning strategy requires the model to have an additional set of parameters relevant to the attention mechanism for a target, zeroresource pair. This implies that the number of parameters may grow linearly with respect to the number of target language pairs. We expect future research to address this issue by, for instance, mixing in the parallel corpora of high-resource language pairs during finetuning as well."}, {"heading": "Acknowledgments", "text": "OF thanks Georgiana Dinu and Iulian Vlad Serban for insightful discussions. KC thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Does multimodality help human and machine for translation and image caption", "author": ["Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301aMart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Multi-task learning for multiple language translation", "author": ["Dong et al.2015] Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "DL4MT-Tutorial: Conditional gated recurrent unit with attention mechanism", "author": ["Firat", "Cho2016] Orhan Firat", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Recursive hetero-associative memories for translation", "author": ["Forcada", "\u00d1eco1997] Mikel L Forcada", "Ram\u00f3n P \u00d1eco"], "venue": "In Biological and Artificial Computation: From Neuroscience to Technology,", "citeRegEx": "Forcada et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Forcada et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "The International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Character-based neural machine translation", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "arXiv preprint arXiv:1511.06114", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing, IEEE Transactions", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Barry Haddow", "Alexandra Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Utiyama", "Isahara2007] Masao Utiyama", "Hitoshi Isahara"], "venue": "In HLTNAACL,", "citeRegEx": "Utiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Utiyama et al\\.", "year": 2007}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Wu", "Wang2007] Hua Wu", "Haifeng Wang"], "venue": "Machine Translation,", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters", "author": ["Zhao", "Chen2009] Bing Zhao", "Shengyuan Chen"], "venue": "In HLT-NAACL,", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Multi-source neural translation", "author": ["Zoph", "Knight2016] Barret Zoph", "Kevin Knight"], "venue": "In NAACL", "citeRegEx": "Zoph et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zoph et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "A recently introduced neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research.", "startOffset": 49, "endOffset": 147}, {"referenceID": 2, "context": "A recently introduced neural machine translation (Forcada and \u00d1eco, 1997; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has proven to be a platform for new opportunities in machine translation research.", "startOffset": 49, "endOffset": 147}, {"referenceID": 3, "context": "Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015).", "startOffset": 205, "endOffset": 293}, {"referenceID": 11, "context": "Rather than word-level translation with language-specific preprocessing, neural machine translation has found to work well with statistically segmented subword sequences as well as sequences of characters (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015b; Ling et al., 2015).", "startOffset": 205, "endOffset": 293}, {"referenceID": 1, "context": "language text in translation (Luong et al., 2015a; Caglayan et al., 2016).", "startOffset": 29, "endOffset": 73}, {"referenceID": 4, "context": "Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016).", "startOffset": 174, "endOffset": 236}, {"referenceID": 5, "context": "Furthermore, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive language transfer (Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016).", "startOffset": 174, "endOffset": 236}, {"referenceID": 5, "context": "In this paper, we conduct in-depth investigation into the recently proposed multi-way, multilingual neural machine translation (Firat et al., 2016).", "startOffset": 127, "endOffset": 147}, {"referenceID": 5, "context": "This result re-confirms the potential of the multi-way, multilingual model for low/zero-resource language translation, which was earlier argued by Firat et al. (2016).", "startOffset": 147, "endOffset": 167}, {"referenceID": 0, "context": "(2016) proposed an extension of attention-based neural machine translation (Bahdanau et al., 2015) that can handle multiway, multilingual translation with a shared attention mechanism.", "startOffset": 75, "endOffset": 98}, {"referenceID": 5, "context": "For more detailed exposition, we refer the reader to (Firat et al., 2016).", "startOffset": 53, "endOffset": 73}, {"referenceID": 4, "context": "Recently Firat et al. (2016) proposed an extension of attention-based neural machine translation (Bahdanau et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 15, "context": "Decoder and Attention Mechanism A decoder for the m-th target language is a conditional recurrent language model (Mikolov et al., 2010).", "startOffset": 113, "endOffset": 135}, {"referenceID": 2, "context": "\u03c6 m is a gated recurrent unit (GRU, (Cho et al., 2014)).", "startOffset": 36, "endOffset": 54}, {"referenceID": 5, "context": "In the original paper by Firat et al. (2016), only one translation strategy was evaluated, that is, one-toone translation.", "startOffset": 25, "endOffset": 45}, {"referenceID": 19, "context": "In fact, if X1 and X2 are same and the two translation paths are simply two different models trained on the same language pair\u2013 direction, this is equivalent to constructing an ensemble, which was found to greatly improve translation quality (Sutskever et al., 2014)", "startOffset": 242, "endOffset": 266}, {"referenceID": 10, "context": "Preprocessing All the sentences are tokenized using the tokenizer script from Moses (Koehn et al., 2007).", "startOffset": 84, "endOffset": 104}, {"referenceID": 5, "context": "We start from the code made publicly available as a part of (Firat et al., 2016).", "startOffset": 60, "endOffset": 80}, {"referenceID": 5, "context": "As proposed by Firat et al. (2016), we share one attention mechanism for the latter case.", "startOffset": 15, "endOffset": 35}, {"referenceID": 5, "context": "Training We closely follow the setup from (Firat et al., 2016).", "startOffset": 42, "endOffset": 62}, {"referenceID": 2, "context": "Any recurrent layer, be it in the encoder or decoder, consists of 1000 gated recurrent units (GRU, (Cho et al., 2014)), and the attention mechanism has a hidden layer of 1200 tanh units (fscore in Eq.", "startOffset": 99, "endOffset": 117}, {"referenceID": 16, "context": "The gradient is clipped to have the norm of at most 1 (Pascanu et al., 2012).", "startOffset": 54, "endOffset": 76}, {"referenceID": 5, "context": "We first confirm that the multi-way, multilingual translation model indeed works as well as singlepair models on the translation paths that were considered during training, which was the major claim in (Firat et al., 2016).", "startOffset": 202, "endOffset": 222}, {"referenceID": 5, "context": ") As observed earlier in (Firat et al., 2016), we also see that the multilingual model performs better when a target language is English.", "startOffset": 25, "endOffset": 45}, {"referenceID": 5, "context": "First, we showed that the multi-way, multilingual neural translation model by Firat et al. (2016) is able to exploit common, underlying structures across many languages in order to better translate when a source sentence is given in multiple languages.", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": "More investigation with a diverse set of languages needs to be done in order to make a more solid conclusion, such as was done in (Firat et al., 2016; Chung et al., 2016).", "startOffset": 130, "endOffset": 170}, {"referenceID": 3, "context": "More investigation with a diverse set of languages needs to be done in order to make a more solid conclusion, such as was done in (Firat et al., 2016; Chung et al., 2016).", "startOffset": 130, "endOffset": 170}], "year": 2016, "abstractText": "In this paper, we propose a novel finetuning algorithm for the recently introduced multiway, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel manyto-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters.", "creator": "LaTeX with hyperref package"}}}