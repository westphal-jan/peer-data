{"id": "1606.04199", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "abstract": "neural machine translation ( nmt ) programming aims at solving machine translation ( mt ) problems with purely neural networks and exhibits progressively promising results in only recent years. unfortunately however, most of really the existing functional nmt models are of shallow topology and there remaining is still a performance gap between the single nmt model and the best conventional quantitative mt retrieval system. in this work, we introduce a new type of linear access connections, named fast - forward semantic connections, based on deep long short - program term memory ( lstm ) network, operated together with the interleaved bi - directional way for stacking them. fast - forward connections play an essential logistical role to propagate the gradients, in quickly building the deep topology curve of depth 16. on wmt'14 english - to - english french task, we achieved bleu = 37. 7 7 with single attention model, which outperforms the corresponding single shallow model by 6. 2 bleu points. it is the first time that compiling a single nmt model achieves state - of - the - art performance and certainly outperforms the best conventional model by 0. 7 across bleu processing points. even without considering attention constraint mechanism, we can still achieve bleu = 36. 3. after the special item handling for unknown words and the model ensemble, we obtained the best score on this task with 16 bleu = 40. 4. our models are also verified on the more difficult wmt'14 english - / to - hungarian german task.", "histories": [["v1", "Tue, 14 Jun 2016 03:53:00 GMT  (153kb,D)", "http://arxiv.org/abs/1606.04199v1", "To be published on TACL 2016"], ["v2", "Wed, 15 Jun 2016 04:21:03 GMT  (153kb,D)", "http://arxiv.org/abs/1606.04199v2", "To be published on TACL 2016"], ["v3", "Sat, 23 Jul 2016 13:14:17 GMT  (153kb,D)", "http://arxiv.org/abs/1606.04199v3", "TACL 2016"]], "COMMENTS": "To be published on TACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jie zhou", "ying cao", "xuguang wang", "peng li", "wei xu"], "accepted": true, "id": "1606.04199"}, "pdf": {"name": "1606.04199.pdf", "metadata": {"source": "CRF", "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation", "authors": ["Jie Zhou", "Ying Cao", "Xuguang Wang", "Peng Li", "Wei Xu"], "emails": ["zhoujie01@baidu.com", "caoying03@baidu.com", "wangxuguang@baidu.com", "lipeng17@baidu.com", "wei.xu@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and\nBlunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end form. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2014).\nGenerally, there are two types of NMT topologies named encoder-decoder network (Sutskever et al., 2014) and attention network (Bahdanau et al., 2014). Encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. Attention network considers the representations from all time steps of the input sequence, building a detailed relationship between the target words and input words. Recent results show that the systems based on these models can achieve similar performance with conventional SMT systems (Luong et al., 2015; Jean et al., 2015).\nHowever, single neural models of either of the above types have not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on WMT\u201914 English-to-French task. The best BLEU score from single model which has six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) gives 37.0.\nWe focus on improving the single model per-\nar X\niv :1\n60 6.\n04 19\n9v 1\n[ cs\n.C L\n] 1\n4 Ju\nn 20\nformance by increasing the model depth. Deep topology has been proven to outperform the shallow architecture in computer vision. In the past two years the top positions of the ImageNet contest are always occupied by the systems with more than tens or even hundreds of layers (Szegedy et al., 2015; He et al., 2015). But in NMT, the largest depth used successfully is only six (Luong et al., 2015). We attribute this problem to the properties of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is widely used in NMT. In LSTM, there are more non-linear activations than those in convolution layer. These activations largely decrease the gradient in deep topology, especially when the information propagates in recurrent form. There are also many efforts to increase the depth of LSTM such as (Kalchbrenner et al., 2015), where the shortcuts do not avoid the nonlinear and recurrent computation.\nIn this work, we introduce the new linear connections for multi-layer recurrent networks. These connections which are called fast-forward connections play an essential role in building the deep topology with depth of 16. Besides, we also introduce the interleaved bi-directional way to stack LSTM layers in encoder part. The topology works for both encoder-decoder network and attention network. On WMT\u201914 English-to-French task, this is the deepest NMT topology that has ever been investigated. With our deep attention model, the BLEU score can be improved to 37.7 and outperforms the shallow model which has six layers (Luong et al., 2015) by 6.2 BLEU points. This is also the first time on this task that a single NMT model achieves state-ofthe-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with the improvement of 0.7. Even without considering the attention mechanism, we can still achieve 36.3 with a single model. After model ensembling and unknown word processing, the BLEU score can be further improved to 40.4. When evaluated on the subset of test corpus without unknown words, our model gives 41.4. As a reference, previous work showed that oracle re-scoring the SMT generated 1000-best sequences exhibits the BLEU score of about 45 (Sutskever et al., 2014). Our models are also verified on the more difficult WMT\u201914 Englishto-German task."}, {"heading": "2 Neural Machine Translation", "text": "Neural machine translation aims at generating the target word sequence y = {y1, . . . , yn} given the source word sequence x = {x1, . . . , xm} with neural models. In this task, the likelihood p(y | x,\u03b8) of the target sequence will be maximized (Forcada and N\u0303eco, 1997) with parameter \u03b8 to learn:\np(y | x;\u03b8) = m+1\u220f j=1 p(yj | y0:j\u22121,x;\u03b8) (1)\ny0:j\u22121 is the sub sequence from y0 to yj\u22121. y0 and ym+1 denote the start mark and end mark of target sequence respectively.\nThe process can be explicitly split into encoding part, decoding part and interface between these two parts. In the encoding part, the source sequence is processed and transformed into a group of vectors e = {e1, \u00b7 \u00b7 \u00b7 , em} for each time step. Further operations will be used at the interface part to extract the final representation c of the source sequence from e. At decoding step, the target sequence will be generated from the representation c.\nRecently, there are two types of NMT models which are distinct in the interface part. In the encoder-decoder model (Sutskever et al., 2014), a single vector extracted from e is used as the representation. In the attention model (Bahdanau et al., 2014), c is dynamically obtained according to the relationship between target sequence and source sequence.\nRecurrent neural network (RNN) or its specific form LSTM is generally employed as the basic unit of the encoding and decoding function. However, most of the existing works are of shallow topology. In attention network, the encoding part and decoding part have only one LSTM layer respectively. In encoding-decoding network, people used at most six LSTM layers (Luong et al., 2015). As machine translation is considered to be a difficult problem, we believe more complex encoding and decoding functions are needed for modeling the relationship between the source sequence and target sequence. In this work, we focus on enhancing the complexity of the encoding/decoding functions by increasing the model depth.\nDeep neural models have been studied in a wild range of problems. In computer vision, models\nwith more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015). Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path. Training networks based on LSTM layers which is widely used in language problems is a much more challenging task. Because of the existence of large amount of nonlinear activations and the recurrent computation, gradient values are not stable and generally are smaller. Following the same spirit for convolutional network, a lot of efforts have also been spent into training deep LSTM topologies. Yao et al. (2015) introduced depth-gated shortcuts connecting LSTM cells at adjacent layers to provide a fast way to propagate the gradients. They verified the modification of these shortcuts on MT task and language modeling task, but the best score are from models with three layers. Similarly, Kalchbrenner et al. (2015) extended this topology to be multi-dimensional. They decreased the number of nonlinear activations and path length, but the gradient propagation still relies on the recurrent computation. The investigations were also made on question-answering to encode the questions, where at most two LSTM layers are stacked (Hermann et al., 2015).\nBased on the above considerations, we propose new connections to facilitate gradient propagation in the following section."}, {"heading": "3 Deep Topology", "text": "We build the deep LSTM network with the new proposed linear connections. The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation. We call these connections fastforward connections. Within the deep topology, we also introduce the interleaved bi-directional way to stack the LSTM layers. And the way to extract the source sequence representation is consequently modified."}, {"heading": "3.1 Network", "text": "Our whole deep neural network is shown in Fig. 2. This topology can be divided into three parts: encoder part (P-E) on the left, decoder part (P-D) on\nthe right and the interface between these two parts (P-I) which extracts the representation of the source sequence. We have two instantiations of this topology named Deep-ED and Deep-Att, corresponding to the extension of encoder-decoder network and attention network respectively. Our essential innovation is made for the adjacent stacked recurrent layers and we will start with the basic RNN model for the sake of clarity. Recurrent layer: When an input sequence {x1, . . . , xm} is given to a recurrent layer, the output ht at each time step t can be computed as (see Fig. 1 (a))\nht = \u03c3(Wfxt +Wrht\u22121)\n= RNN (Wfxt, ht\u22121)\n= RNN (ft, ht\u22121), (2)\nwhere bias parameter is not included for simplicity. We use red circle, blue empty square to denote the input and hidden state. Blue square with \u201c-\u201d denotes previous hidden state. Dotted line means that the hidden state is recurrently used. This computation can be equivalently split into two consecutive steps:\n\u2022 Feed-Forward computation: ft = Wfxt. Left part in Fig. 1 (b). \u201cf\u201d block.\n\u2022 Recurrent computation: RNN (ft, ht\u22121). Right part and the sum operation (+) followed by activation in Fig. 1 (b). \u201cr\u201d block.\nFor a deep topology with stacked recurrent layers, the input of each block f at recurrent layer k (denoted by fk) is usually the output of block r at its previous recurrent layer k \u2212 1 (denoted by hk\u22121). In our work, we add fast-forward connections (F-F connections) which connect two feed-forward computation blocks f of adjacent recurrent layers. It means that each block f at recurrent layer k takes both outputs of block f and block r at its previous layer as input (Fig. 1 (c)). F-F connections are denoted by dashed red lines in Fig. 1 (c) and Fig. 2. The path of F-F connections contains neither nonlinear activations nor recurrent computation. It provides a fast way for information to travel, so we call this path fast-forward connections.\nAdditionally, in order to learn more temporal dependencies, the sequences could be processed in\ndifferent directions at each pair of adjacent recurrent layers. This is quantitatively expressed in Eq. 3:\nfkt = W k f \u00b7 [fk\u22121t , h k\u22121 t ], k > 1 fkt = W k f xt k = 1 hkt = RNN k (fkt , h k t+(\u22121)k) (3)\nThe opposite directions are marked by direction term (\u22121)k. At the beginning layer, the block f takes xt as the input. [ , ] denotes the concatenation of the inside vectors. This is shown in Fig. 1 (c). Two variations are summarized here:\n\u2022 We add a connection between fkt and fk\u22121t . Without fk\u22121t , our model will be reduced to the traditional stacked model.\n\u2022 We alternate the RNN direction at different layer k with the direction term (\u22121)k. If we fix the direction term to \u22121, all layers work in the forward direction.\nLSTM layer: Actually, a specific type recurrent layer called LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) is used in our work. LSTM is structurally more complex than basic RNN used in Eq. 2. We define the computation of LSTM as a function which maps the input f and its state-\noutput pair (h, s) at previous time step to a new current state-output pair. The exact computations for (ht, st) = LSTM(ft, ht\u22121, st\u22121) are the following:\n[z, z\u03c1, z\u03c6, z\u03c0] = ft +Wrht\u22121\nst = \u03c3i(z) \u25e6 \u03c3g(z\u03c1 + st\u22121 \u25e6 \u03b8\u03c1) + \u03c3g(z\u03c6 + st\u22121 \u25e6 \u03b8\u03c6) \u25e6 st\u22121 ht = \u03c3o(st) \u25e6 \u03c3g(z\u03c0 + st \u25e6 \u03b8\u03c0) (4)\nwhere [z, z\u03c1, z\u03c6, z\u03c0] is the concatenation of four vectors of equal size, \u25e6 means element-wise multiplication, \u03c3i is the input activation function, \u03c3o is the output activation function, \u03c3g is the activation function for gates, and Wr, \u03b8\u03c1, \u03b8\u03c6, and \u03b8\u03c0 are the parameters of the LSTM. It is slightly different from the standard notation in that we do not have a matrix to multiply with the input f in our notation.\nWith this notation, we can write down the computations for our deep bi-directional LSTM model with F-F connections:\nfkt =W k f \u00b7 [fk\u22121t , h k\u22121 t ], k > 1 fkt =W k f xt, k = 1\n(hkt , s k t ) = LSTM k ( fkt , h k t+(\u22121)k , s k t+(\u22121)k ) (5)\nwhere xt is the input to the deep bi-directional LSTM model. For encoder, xt is the embedding of tth word in the source sentence. For decoder xt is the concatenation of the embedding of the tth word in the target sentence and the encoder representation for step t.\nIn our final model two additional operations are used with Eq. 5, which is shown in Eq. 6. Half(f) means the first half elements of h, and Dr(h) is the dropout operation (Hinton et al., 2012) which randomly set an element of h to zero with a certain probability. The use of Half(\u00b7) is to reduce the parameter size and does not affect on the performance. While we tried to only use the first thirds parameters, the performance degrades considerably.\nfkt =W k f \u00b7 [Half(fk\u22121t ),Dr(h k\u22121 t )], k > 1 (6)\nWith the F-F connections, we build a channel to propagate the gradients in deep topology. F-F connections will accelerate the model convergence and meanwhile improve the performance. Similar idea\nwas also used in (He et al., 2015; Zhou and Xu, 2015).\nEncoder: The LSTM layers are stacked following Eq. 5. We call this type of encoder interleaved bidirectional encoder. Besides, there are two similar columns (a1 and a2) in encoder part. Each column consists of ne stacked LSTM layers. There is no connection between the columns. The first layers of both columns process the word representations of the source sequence in different direction. At the last LSTM layers, there are two groups of vectors representing the source sequence. The group size is the same as the length of the input sequence.\nInterface: Previous encoder-decoder model and attention model are different in how to extract the representations of the source sequences. In our work, as a consequence of the introduced F-F connections, we have 4 output vectors (hkt and f k t of both columns). The representations are modified for both Deep-ED and Deep-Att.\nFor Deep-ED, et is static and consists of four parts. 1: The last time step output hkm of the first column. 2: Max-operation Max(\u00b7) of hkt at all time steps of the second column, denoted by Max(hne,a2t ). Max(\u00b7) means to obtain max value from each dimension over t. 3: Max(fne,a1t ). 4: Max(fne,a2t ). Max-operation and last time step extraction provide complimentary information but does not affect the performance much. et is used as the final representation ct.\nFor Deep-Att, we do not need the above two operations. We only concatenate the 4 output vectors at each time steps to obtain et, and a soft attention mechanism (Bahdanau et al., 2014) is used to calculate the final representation ct from et. et is summarized as:\nDeep-ED: et [hne,a1m ,Max(h ne,a2 t ),Max(f ne,a1 t ),Max(f ne,a2 t )] Deep-Att: et [hne,a1t , h ne,a2 t , f ne,a1 t , f ne,a2 t ] (7)\nNote that the vector dimension of f is four times larger than that of h (see Eq. 4). ct is summarized as:\nDeep-ED: ct = et, (const)\nDeep-Att: ct = m\u2211 t\u2032=1 \u03b1t,t\u2032Wpet\u2032 (8)\n\u03b1t,t\u2032 is the normalized attention weight computed by:\n\u03b1t,t\u2032 = exp(a(Wpet\u2032 , h 1,dec t\u22121 ))\u2211\nt\u2032\u2032 exp(a(Wpet\u2032\u2032 , h 1,dec t\u22121 ))\n(9)\nh1,dect\u22121 is the first hidden layer output in decoding part. a(\u00b7) is an alignment model described in (Bahdanau et al., 2014). For Deep-Att to save the memory cost, we linearly project (with Wp) the concatenated vector et to a vector with 1/4 dimension size, denoted by fc (fully connected) block in Fig. 2.\nDecoder: The decoder follows the Eq. 5 and Eq. 6 with fixed direction term \u22121. At the first layer, we use the following xt:\nxt = [ct, yt\u22121] (10)\nyt\u22121 is the target word embedding at the previous time step and initially y0 is zero. There is a single column of nd stacked LSTM units. We also use the F-F connections as those in encoder and all layers work in forward direction. Note that at the last LSTM layer, we only use ht to make the prediction with a softmax layer."}, {"heading": "3.2 Training technique", "text": "We take the parallel data as the only input without using any monolingual data for neither word representation pre-training nor language modeling. Because of the deep bi-directional structure, we do not need to reverse the sequence order as (Sutskever et al., 2014).\nThe deep topology brings difficulties into the model training, especially when the first order methods such as stochastic gradient descent (SGD) (LeCun et al., 1998) is used. The parameters should be properly initialized and the converging process would be slow. We tried several high order optimization techniques such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014). We found all of them are able to speed up the process a lot compared to simple SGD while no significant difference is observed among them in performance. In this work, we choose Adam for model training and will not compare their detailed behavior.\nDropout (Hinton et al., 2012) is also used to avoid over-fitting. It is utilized on LSTM nodes hkt (See Eq. 5) with ratio of pd for both encoder and decoder.\nDuring the whole model training process, we keep all hyper parameters fixed without any intermediate interruption. The hyper parameters are selected according to the performance on development set. For such a deep and large network, it is not easy to determine the tuning strategy and this would be considered in future work."}, {"heading": "3.3 Generation", "text": "We use the common left-to-right beam-search method for sequence generation. At each time step\nt, the word yt can be predicted by:\ny\u0302t = arg max y P(y|y\u03020:t\u22121,x;\u03b8) (11)\nwhere y\u0302t is the predicted target word. y\u03020:t\u22121 is the generated sequence from time step 0 to t \u2212 1. We keep nb best candidates according to Eq. 11 at each time step, until the end mark is generated. The hypothesis are ranked by the total likelihood of the generated sequence, despite in some works normalized likelihood is considered (Jean et al., 2015)."}, {"heading": "4 Experiment", "text": "We evaluate our method mainly on the widely used WMT\u201914 English-to-French translation task. In order to verify our model on more difficult language pairs, we also give the results of WMT\u201914 Englishto-German translation task."}, {"heading": "4.1 Data sets", "text": "For both tasks, we use the full WMT\u201914 parallel corpus as our training data. The detailed data sets are listed below:\n\u2022 English-to-French: Europarl v7, Common Crawl, UN, News Commentary, Gigaword\n\u2022 English-to-German: Europarl v7, Common Crawl, News Commentary\nIn total, the English-to-French corpus includes 36 million sentence pairs, and English-to-German corpus includes 4.5 million sentence pairs. The newstest-2012 and news-test-2013 are concatenated as our development set, and the news-test-2014 is the test set. Our data set is consistent with the previous works on NMT (Luong et al., 2015; Jean et al., 2015) to ensure fair comparison.\nFor the source language, we select the most frequent 200K words as the input vocabulary. For the target language we select the most frequent 80K French words and 160K German words as the output vocabulary. The full vocabulary of German corpus is larger (Jean et al., 2015), so we select more German words to build the target vocabulary. Outof-vocabulary words are replaced with the unknown symbol \u3008unk\u3009. For complete comparison with previous work on English-to-French task, we also show the results with small vocabulary of 30K input words\nand 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014)."}, {"heading": "4.2 Model settings", "text": "We have two models as described above, named Deep-ED and Deep-Att. Both models have exactly the same configuration and layer size except the interface part P-I.\nWe use 256 dimensional word embeddings for both source and target languages. All LSTM layers, including 2\u00d7ne layers in encoder and nd layers in decoder, have 512 memory cells. The output layer size is the same as the size of the target vocabulary. The dimension of ct is 5120 and 1280 for Deep-ED and Deep-Att respectively. For each LSTM layer, activation functions for gates, inputs and outputs are sigmoid, tanh, and tanh respectively.\nOur network is narrow on word embeddings and LSTM layers. Note that in previous work (Sutskever et al., 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used. We also tried larger scale models but did not obtain further improvements. Consider comparing the computation complexity with Luong et. al. (2015), ours might be much lower than theirs."}, {"heading": "4.3 Optimization", "text": "Note that each LSTM layer includes two parts as described in Eq. 3, feed-forward computation and recurrent computation. Since there are non-linear activations in the recurrent computation, a larger learning rate lr = 5\u00d710\u22124 is used while for feed-forward computation a smaller learning rate lf = 4\u00d710\u22125 is used. Word embeddings and softmax layer also use this learning rate lf . We refer to all parameters not used for recurrent computation as non-recurrent part of the model.\nBecause of the large model size, we use strong L2 regularization to constrain the parameter matrix v in the following way:\nv \u2190 v \u2212 l \u00b7 (g + r \u00b7 v) (12)\nHere r is the regularization strength, l is the corresponding learning rate, g stands for the gradients of v. Two embedding layers are not regularized. All the other layers have the same r = 2.\nParameters of the recurrent computation part are set to be zero at the beginning. All non-recurrent parts are randomly initialized with zero mean and standard deviation of 0.07. A detailed guide for setting hyper-parameters can be found in (Bengio, 2012).\nDropout ratio pd is set to be 0.1. In each batch, there are 500 \u223c 800 sequences in our work. The exact number depends on the sequence lengths and model size. We also find that larger batch size results in better convergence although the improvement is not large. But this is constrained by the GPU memory. We need 4 \u223c 8 GPU machines (each has 4 K40 GPU cards) running for 10 days to train the full model with parallelization at data batch level. It takes nearly 1.5 days for each pass.\nOne thing we want to stress here is that our deep model is not sensitive to these settings. Small variations around this region will not affect the final performance."}, {"heading": "4.4 Results", "text": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015). All reported BLEU scores are computed with the multi-bleu.perl1 script which is also used in the above works. The results are tokenized and case sensitive."}, {"heading": "4.4.1 Single models", "text": "English-to-French: First we list our single model results on English-to-French task in Tab. 1. In the first block we show the results with the full corpus. The previous best single NMT encoder-decoder model (Enc-Dec) with six layers gives BLEU=31.5 (Sutskever et al., 2014). From Deep-ED, we obtain the BLEU score of 36.3, which outperforms Enc-Dec model by 4.8 BLEU points. This result is even better than the ensemble result of eight EncDec models which is 35.6 (Luong et al., 2015). This shows that deep topology can also works well with LSTM layer, besides convolutional layers in computer vision. For Deep-Att, the performance is further improved to be 37.7. We also list the previous state-of-the-art performance from conventional SMT system (Durrani et al., 2014) with BLEU\n1https://github.com/moses-smt/mosesdecoder/blob/master/ scripts/generic/multi- bleu.perl\nof 37.0. This is the first time that a single NMT model trained in end-to-end form beats the best conventional system.\nWe also show the results on the smaller data set with 12M sentence pairs and 30K vocabulary in the second block. Two attention models RNNsearch (Bahdanau et al., 2014) and RNNsearch-LV (Jean et al., 2015) give the BLEU score of 28.5 and 32.7 respectively. Note that RNNsearch-LV uses a large output vocabulary of 500K words based on the standard attention model RNNsearch. We obtain BLEU=35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points. The SMT result from (Schwenk, 2014) is also listed and falls behind our model by 2.6 BLEU points.\nBesides, during the generation process, we obtained the best BLEU score with beam size = 3 (when beam size is 2, there is only 0.1 difference in BLEU score.). This is different from other works listed in Tab. 1, where the beam size is 12 (Jean et al., 2015; Sutskever et al., 2014). We attribute this difference to the improved model performance, where the ground truth generally exists in the top hypothesis. Consequently, with largely decreased beam size, the generation efficiency is significantly improved.\nNext we are going to list the effect of the novel F-F connections in our Deep-Att model of shallow topology in Tab. 2. When ne = 1 and nd = 1, the BLEU scores are 31.2 without F-F and 32.3 with F-F. Note that the model without F-F is exactly the standard attention model (Bahdanau et al., 2014). Since here is only a single layer, F-F connections means at the interface part we include ft into the representation (see Eq. 7). We find F-F connections\nbring 1.1 improvement of BLEU score. After we increase our model depth to ne = 2 and nd = 2, the improvement is enlarged to 1.4. When the model is trained with larger depth without F-F connections, we find that the parameter exploding problem (Bengio et al., 1994) happens too frequently that we could not finish training. This suggests that F-F connections provide a fast way for gradient propagation.\nRemoving F-F connections also reduces the corresponding model size. In order to figure out the effect of F-F between models with the same parameter size, we increase the LSTM layer width of Deep-Att without F-F. In Tab. 3 we show that, after using two times larger LSTM layer width of 1024, we can only obtain BLEU score of 33.8. It is still behind the corresponding Deep-Att with F-F.\nWe also notice that the interleaved bi-directional encoder start to work when the encoder depth is larger than 1. This property is investigated in Tab. 4. For our largest model with ne = 9 and nd = 7, we compared the BLEU scores of interleaved bidirectional encoder and uni-directional encoder (all LSTM layers work in forward direction). We find there is a gap about 1.5 points between these two encoders in both Deep-Att and Deep-ED.\nNext we are going to look into the model depth. In Tab. 5, starting from ne = 1 and nd = 1 and gradually increasing the model depth, we significantly increase BLEU scores. With ne = 9 and nd = 7, the best scores for Deep-Att is 37.7. We tried to increase the LSTM width based on this, but obtained little improvements. As we stated in Sec.2, the complexity of encoding/decoding functions, which is related to the model depth, is more important than model size. We also tried larger depth further, but the results start to get worse. With our topology and training technique, ne = 9 and nd = 7 is the best depth we can achieve.\nThe last line in Tab. 5 shows the BLEU score of 36.6 of our deepest model where only one encoding column (Col = 1) is used. We find 1.1 BLEU points degradation with single encoding column. Note that results in Tab. 4 with uni-direction still have two encoding columns. In order to find out whether this is caused by the decreased parameter size, we use a wider LSTM layer with width of 1024 memory blocks. It is shown in Tab. 6 that there is a minor improvement of only 0.1. We attribute this to the complementary information provided by the double encoding column.\nEnglish-to-German: We also verify our deep\ntopology on English-to-German task. English-toGerman task is considered as a relatively more difficult task, because of the less similarity between this language pair. Since the German vocabulary is much larger than French vocabulary, we select 160K most frequent words as the target vocabulary. All the other hyper parameters are exactly the same as those in English-to-French task.\nWe list our single model Deep-Att performance in Tab. 7. Our single model result with BLEU=20.6 is similar with conventional SMT result of 20.7 (Buck et al., 2014). We also outperform the shallow attention models as shown in the first two lines in Tab. 7. All the results are consistent with those in Englishto-French task."}, {"heading": "4.4.2 Post processing", "text": "Two post processing techniques are used to improve the performance further on English-to-French task.\nFirst, three Deep-Att models are built for ensemble results. They are initialized with different parameters. And the training corpus for these models are shuffled with different random seed. We sum over the distribution of the predicted words and normalize the final distribution to generate the next word. It is shown in Tab. 8 that the model ensemble can improve the performance further to 38.9. In (Luong et al., 2015) and (Jean et al., 2015) there are eight models for the best scores, but we only use three models and we do not obtain further gain from\nmore models.\nSecond, we recover the unknown words in the generated sequences with Positional Unknown (PosUnk) model introduced in (Luong et al., 2015). The full parallel corpus is used to obtain the word mappings (Liang et al., 2006). We find this method provides additional 1.5 BLEU points, consistent with the conclusion in (Luong et al., 2015). We obtain the new BLEU score of 39.2 with single Deep-Att. For the ensemble models of Deep-Att, the BLEU score rises to 40.4. At the last two lines, we list the conventional SMT model (Durrani et al., 2014) and previous best neural models based system Encoding-Decoding (Luong et al., 2015) for comparison. We find our best score outperforms the previous best score by nearly 3 points."}, {"heading": "4.5 Analysis", "text": ""}, {"heading": "4.5.1 Length", "text": "On English-to-French task, we analyze the effect of the source sequence length on our models as shown in Fig. 3. Here we show five curves of our Deep-Att single model, our Deep-Att ensemble model, our Deep-ED model, previous Enc-Dec model with four layers (Sutskever et al., 2014) and SMT model (Durrani et al., 2014). We find our Deep-Att model works better than the previous two models (Enc-Dec and SMT) on nearly all length scales. It is also shown that for very long sequences with length over 70 words, the performance of our Deep-Att does not degrade, when compared with another NMT model Enc-Dec. Our Deep-ED also have much better performance than the shallow Enc-Dec model on nearly all length scales, although for long\nsequences it degrades and starts to fall behind DeepAtt."}, {"heading": "4.5.2 Unknown words", "text": "Next we look into the detail of the effect of unknown words on English-to-French task. We select the subset without unknown words on target sentence from the original test set. There are 1705 sequences (56.8%). We compute the BLEU scores on this subset and the results are shown in Tab. 9. We also list the results from SMT model (Durrani et al., 2014) as comparison .\nWe find the BLEU score of Deep-Att on subset rises to 40.3. Considering the score on full test set is 37.7, we have a gap of 2.4. On this subset, the SMT model gives 37.5, and it should be noted that its score on full set is 37.0. This suggests that the difficulty on this subset is not much different from the full set. So we attribute the larger gap for Deepatt to the existence of unknown words. We also compute the BLEU score on the subset with ensemble model and obtain 41.4. As a reference related to human level, in (Sutskever et al., 2014), it has been tested that the BLEU score of oracle re-scoring the LIUM 1000-best results (Schwenk, 2014) is 45."}, {"heading": "4.5.3 Over-fitting", "text": "Deep models have more parameters, and thus have stronger ability to fit the large data set. Besides, we will also show that deep models are less probable to over-fit the data set.\n\u00a0\nIn Fig. 4, on English-to-French task, we show\nthree results from models with different depth. These three models are evaluated by token error rate, which is defined as the ratio of incorrectly predicted words in the whole target sequence with correct historical input. The curve with square marks corresponds to Deep-Att with ne = 9 and nd = 7. The curve with circle marks corresponds to ne = 5 and nd = 3. The curve with triangle marks corresponds to ne = 1 and nd = 1. We find deep model has better performance on test set when the token error rate is the same as that of shallow models on train set. It shows that, with decreased token error rate, deep model is more advantageous in avoiding the overfitting phenomenon. We only plot the early training stage curves because on late training stage the curves become not smooth."}, {"heading": "4.5.4 Examples", "text": "On English-to-French task, we select two example sequences showing in Tab. 10. For each example, there are four lines corresponding to source language, reference(ref), our ensemble model result with PosUnk and SMT model (Durrani) (Durrani et al., 2014) respectively."}, {"heading": "5 Conclusion", "text": "With the introduction of fast-forward connections to the deep LSTM network, we build a fast path with neither non-linear transformations nor recurrent computation to propagate the gradients from the top to the deep bottom. On this path, gradients decays much slower compared to the standard LSTM network. This enable us to build the deep topology of NMT models.\nWe trained NMT models with depth of 16 including 25 LSTM layers and evaluated them mainly on WMT\u201914 English-to-French translation task. This is the deepest topology that have been investigated in NMT area on this task. We showed that our Deep-Att exhibits 6.2 BLEU points improvement over the previous best single model, achieving 37.7 BLEU score. This single end-to-end NMT model outperforms the conventional best SMT system (Durrani et al., 2014) and achieves the state-ofthe-art performance. After utilizing the unknown word processing and model ensemble of three models, we obtained the BLEU score of 40.4, improved by 2.9 BLEU score over previous best result. When evaluated on the subset of test corpus without unknown words, our model gives 41.4. As a reference number, the work (Sutskever et al., 2014) show that oracle re-scoring the SMT generated 1000-best lists exhibits the BLEU score of about 45. Our model is also verified on the more difficult English-toGerman task.\nBesides, our model is efficient in sequence generation. The best results from both single model and model ensemble are obtained with beam size of 3, much smaller than previous NMT systems where beam size is about 12 (Jean et al., 2015; Sutskever et al., 2014). From our analysis, we find deep models are more advantageous at learning long sequences and the deep topology is resistant to the over-fitting phenomenon.\nWe tried deeper models and did not obtain further improvements with current topology and training techniques. However, 16 is not a large number of model depth when compared to the models in computer vision (He et al., 2015). We believe we can benefit from deeper models with new design of topologies and training techniques which are left as our future work."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Practical recommendations for gradient-based training of deep architectures", "author": ["Yoshua Bengio."], "venue": "arXiv:1206.5533.", "citeRegEx": "Bengio.,? 2012", "shortCiteRegEx": "Bengio.", "year": 2012}, {"title": "N-gram counts and language models from the common crawl", "author": ["Christian Buck", "Kenneth Heafield", "Bas van Ooyen."], "venue": "Proceedings of the Language Resources and Evaluation Conference.", "citeRegEx": "Buck et al\\.,? 2014", "shortCiteRegEx": "Buck et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the Empiricial Methods in Nat-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Edinburghs phrase-based machine translation systems for WMT-14", "author": ["Nadir Durrani", "Barry Haddow", "Philipp Koehn", "Kenneth Heafield."], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97\u2013104, June.", "citeRegEx": "Durrani et al\\.,? 2014", "shortCiteRegEx": "Durrani et al\\.", "year": 2014}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fernandez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Graves et al\\.,? 2009", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "arXiv:1506.03340.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700\u20131709, October.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Volume 1, NAACL \u201903, pages", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, Nov.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Alignment by agreement", "author": ["Percy Liang", "Ben Taskar", "Dan Klein."], "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL \u201906,", "citeRegEx": "Liang et al\\.,? 2006", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan L. Yuille."], "venue": "arXiv:1412.6632.", "citeRegEx": "Mao et al\\.,? 2014", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "http://www-lium.univlemans.fr/\u223cschwenk/cslm joint paper [online; accessed 03-september-2014]. University Le Mans", "author": ["Holger Schwenk"], "venue": null, "citeRegEx": "Schwenk.,? \\Q2014\\E", "shortCiteRegEx": "Schwenk.", "year": 2014}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv:1505.00387.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc Le."], "venue": "Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of the 2015 IEEE Conference on Computer Vision", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Depth-gated LSTM", "author": ["Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer."], "venue": "arXiv:1508.03790.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Empirical study on deep learning models for QA", "author": ["Yang Yu", "Wei Zhang", "Chung-Wei Hang", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv:1510.07526.", "citeRegEx": "Yu et al\\.,? 2015", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Jie Zhou", "Wei Xu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Zhou and Xu.,? 2015", "shortCiteRegEx": "Zhou and Xu.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 131, "endOffset": 210}, {"referenceID": 22, "context": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 131, "endOffset": 210}, {"referenceID": 0, "context": "Neural machine translation (NMT) has attracted a lot of interests in solving the machine translation (MT) problems in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 131, "endOffset": 210}, {"referenceID": 15, "context": "Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end form.", "startOffset": 66, "endOffset": 108}, {"referenceID": 5, "context": "Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end form.", "startOffset": 66, "endOffset": 108}, {"referenceID": 24, "context": "Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al.", "startOffset": 86, "endOffset": 108}, {"referenceID": 26, "context": "Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al.", "startOffset": 137, "endOffset": 154}, {"referenceID": 19, "context": ", 2015) and image caption generation (Mao et al., 2014).", "startOffset": 37, "endOffset": 55}, {"referenceID": 22, "context": "Generally, there are two types of NMT topologies named encoder-decoder network (Sutskever et al., 2014) and attention network (Bahdanau et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 0, "context": ", 2014) and attention network (Bahdanau et al., 2014).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "Recent results show that the systems based on these models can achieve similar performance with conventional SMT systems (Luong et al., 2015; Jean et al., 2015).", "startOffset": 121, "endOffset": 160}, {"referenceID": 11, "context": "Recent results show that the systems based on these models can achieve similar performance with conventional SMT systems (Luong et al., 2015; Jean et al., 2015).", "startOffset": 121, "endOffset": 160}, {"referenceID": 5, "context": "However, single neural models of either of the above types have not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on WMT\u201914 English-to-French task.", "startOffset": 119, "endOffset": 141}, {"referenceID": 18, "context": "5 (Luong et al., 2015) while the conventional method of (Durrani et al.", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": ", 2015) while the conventional method of (Durrani et al., 2014) gives 37.", "startOffset": 41, "endOffset": 63}, {"referenceID": 23, "context": "In the past two years the top positions of the ImageNet contest are always occupied by the systems with more than tens or even hundreds of layers (Szegedy et al., 2015; He et al., 2015).", "startOffset": 146, "endOffset": 185}, {"referenceID": 7, "context": "In the past two years the top positions of the ImageNet contest are always occupied by the systems with more than tens or even hundreds of layers (Szegedy et al., 2015; He et al., 2015).", "startOffset": 146, "endOffset": 185}, {"referenceID": 18, "context": "But in NMT, the largest depth used successfully is only six (Luong et al., 2015).", "startOffset": 60, "endOffset": 80}, {"referenceID": 10, "context": "We attribute this problem to the properties of Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is widely used in NMT.", "startOffset": 77, "endOffset": 111}, {"referenceID": 13, "context": "There are also many efforts to increase the depth of LSTM such as (Kalchbrenner et al., 2015), where the shortcuts do not avoid the nonlinear and recurrent computation.", "startOffset": 66, "endOffset": 93}, {"referenceID": 18, "context": "7 and outperforms the shallow model which has six layers (Luong et al., 2015) by 6.", "startOffset": 57, "endOffset": 77}, {"referenceID": 5, "context": "This is also the first time on this task that a single NMT model achieves state-ofthe-art performance and outperforms the best conventional SMT system (Durrani et al., 2014) with the improvement of 0.", "startOffset": 151, "endOffset": 173}, {"referenceID": 22, "context": "As a reference, previous work showed that oracle re-scoring the SMT generated 1000-best sequences exhibits the BLEU score of about 45 (Sutskever et al., 2014).", "startOffset": 134, "endOffset": 158}, {"referenceID": 22, "context": "In the encoder-decoder model (Sutskever et al., 2014), a single vector extracted from e is used as the representation.", "startOffset": 29, "endOffset": 53}, {"referenceID": 0, "context": "In the attention model (Bahdanau et al., 2014), c is dynamically obtained according to the relationship between target sequence and source sequence.", "startOffset": 23, "endOffset": 46}, {"referenceID": 18, "context": "In encoding-decoding network, people used at most six LSTM layers (Luong et al., 2015).", "startOffset": 66, "endOffset": 86}, {"referenceID": 21, "context": "with more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015).", "startOffset": 113, "endOffset": 177}, {"referenceID": 7, "context": "with more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015).", "startOffset": 113, "endOffset": 177}, {"referenceID": 23, "context": "with more than tens of convolution layers outperform the shallow ones on a series of image tasks in recent years (Srivastava et al., 2015; He et al., 2015; Szegedy et al., 2015).", "startOffset": 113, "endOffset": 177}, {"referenceID": 8, "context": "The investigations were also made on question-answering to encode the questions, where at most two LSTM layers are stacked (Hermann et al., 2015).", "startOffset": 123, "endOffset": 145}, {"referenceID": 7, "context": ", 2015; He et al., 2015; Szegedy et al., 2015). Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path. Training networks based on LSTM layers which is widely used in language problems is a much more challenging task. Because of the existence of large amount of nonlinear activations and the recurrent computation, gradient values are not stable and generally are smaller. Following the same spirit for convolutional network, a lot of efforts have also been spent into training deep LSTM topologies. Yao et al. (2015) introduced depth-gated shortcuts connecting LSTM cells at adjacent layers to provide a fast way to propagate the gradients.", "startOffset": 8, "endOffset": 572}, {"referenceID": 7, "context": ", 2015; He et al., 2015; Szegedy et al., 2015). Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path. Training networks based on LSTM layers which is widely used in language problems is a much more challenging task. Because of the existence of large amount of nonlinear activations and the recurrent computation, gradient values are not stable and generally are smaller. Following the same spirit for convolutional network, a lot of efforts have also been spent into training deep LSTM topologies. Yao et al. (2015) introduced depth-gated shortcuts connecting LSTM cells at adjacent layers to provide a fast way to propagate the gradients. They verified the modification of these shortcuts on MT task and language modeling task, but the best score are from models with three layers. Similarly, Kalchbrenner et al. (2015) extended this topology to be multi-dimensional.", "startOffset": 8, "endOffset": 877}, {"referenceID": 10, "context": "LSTM layer: Actually, a specific type recurrent layer called LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) is used in our work.", "startOffset": 66, "endOffset": 121}, {"referenceID": 6, "context": "LSTM layer: Actually, a specific type recurrent layer called LSTM (Hochreiter and Schmidhuber, 1997; Graves et al., 2009) is used in our work.", "startOffset": 66, "endOffset": 121}, {"referenceID": 9, "context": "Half(f) means the first half elements of h, and Dr(h) is the dropout operation (Hinton et al., 2012) which randomly set an element of h to zero with a certain probability.", "startOffset": 79, "endOffset": 100}, {"referenceID": 7, "context": "was also used in (He et al., 2015; Zhou and Xu, 2015).", "startOffset": 17, "endOffset": 53}, {"referenceID": 28, "context": "was also used in (He et al., 2015; Zhou and Xu, 2015).", "startOffset": 17, "endOffset": 53}, {"referenceID": 0, "context": "We only concatenate the 4 output vectors at each time steps to obtain et, and a soft attention mechanism (Bahdanau et al., 2014) is used to calculate the final representation ct from et.", "startOffset": 105, "endOffset": 128}, {"referenceID": 0, "context": "a(\u00b7) is an alignment model described in (Bahdanau et al., 2014).", "startOffset": 40, "endOffset": 63}, {"referenceID": 22, "context": "Because of the deep bi-directional structure, we do not need to reverse the sequence order as (Sutskever et al., 2014).", "startOffset": 94, "endOffset": 118}, {"referenceID": 16, "context": "The deep topology brings difficulties into the model training, especially when the first order methods such as stochastic gradient descent (SGD) (LeCun et al., 1998) is used.", "startOffset": 145, "endOffset": 165}, {"referenceID": 27, "context": "We tried several high order optimization techniques such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014).", "startOffset": 69, "endOffset": 83}, {"referenceID": 14, "context": "We tried several high order optimization techniques such as AdaDelta (Zeiler, 2012), RMSProp (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014).", "startOffset": 130, "endOffset": 151}, {"referenceID": 9, "context": "Dropout (Hinton et al., 2012) is also used to avoid over-fitting.", "startOffset": 8, "endOffset": 29}, {"referenceID": 11, "context": "The hypothesis are ranked by the total likelihood of the generated sequence, despite in some works normalized likelihood is considered (Jean et al., 2015).", "startOffset": 135, "endOffset": 154}, {"referenceID": 18, "context": "Our data set is consistent with the previous works on NMT (Luong et al., 2015; Jean et al., 2015) to ensure fair comparison.", "startOffset": 58, "endOffset": 97}, {"referenceID": 11, "context": "Our data set is consistent with the previous works on NMT (Luong et al., 2015; Jean et al., 2015) to ensure fair comparison.", "startOffset": 58, "endOffset": 97}, {"referenceID": 11, "context": "The full vocabulary of German corpus is larger (Jean et al., 2015), so we select more German words to build the target vocabulary.", "startOffset": 47, "endOffset": 66}, {"referenceID": 20, "context": "and 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 79, "endOffset": 136}, {"referenceID": 22, "context": "and 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 79, "endOffset": 136}, {"referenceID": 4, "context": "and 30K output words on the sub train set with selected 12M parallel sequences (Schwenk, 2014; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 79, "endOffset": 136}, {"referenceID": 22, "context": "Note that in previous work (Sutskever et al., 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used.", "startOffset": 27, "endOffset": 74}, {"referenceID": 0, "context": "Note that in previous work (Sutskever et al., 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used.", "startOffset": 27, "endOffset": 74}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014), 1000 dimensional word embeddings and 1000 dimensional LSTM layers are used. We also tried larger scale models but did not obtain further improvements. Consider comparing the computation complexity with Luong et. al. (2015), ours might be much lower than theirs.", "startOffset": 8, "endOffset": 255}, {"referenceID": 2, "context": "A detailed guide for setting hyper-parameters can be found in (Bengio, 2012).", "startOffset": 62, "endOffset": 76}, {"referenceID": 22, "context": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015).", "startOffset": 62, "endOffset": 125}, {"referenceID": 18, "context": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015).", "startOffset": 62, "endOffset": 125}, {"referenceID": 11, "context": "We keep the same evaluation rule with most previous NMT works (Sutskever et al., 2014; Luong et al., 2015; Jean et al., 2015).", "startOffset": 62, "endOffset": 125}, {"referenceID": 22, "context": "5 (Sutskever et al., 2014).", "startOffset": 2, "endOffset": 26}, {"referenceID": 18, "context": "6 (Luong et al., 2015).", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": "We also list the previous state-of-the-art performance from conventional SMT system (Durrani et al., 2014) with BLEU", "startOffset": 84, "endOffset": 106}, {"referenceID": 0, "context": "Two attention models RNNsearch (Bahdanau et al., 2014) and RNNsearch-LV (Jean et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 11, "context": ", 2014) and RNNsearch-LV (Jean et al., 2015) give the BLEU score of 28.", "startOffset": 25, "endOffset": 44}, {"referenceID": 20, "context": "The SMT result from (Schwenk, 2014) is also listed and falls behind our model by 2.", "startOffset": 20, "endOffset": 35}, {"referenceID": 11, "context": "1, where the beam size is 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 29, "endOffset": 72}, {"referenceID": 22, "context": "1, where the beam size is 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 29, "endOffset": 72}, {"referenceID": 0, "context": "Note that the model without F-F is exactly the standard attention model (Bahdanau et al., 2014).", "startOffset": 72, "endOffset": 95}, {"referenceID": 1, "context": "When the model is trained with larger depth without F-F connections, we find that the parameter exploding problem (Bengio et al., 1994) happens too frequently that we could not finish training.", "startOffset": 114, "endOffset": 135}, {"referenceID": 3, "context": "7 (Buck et al., 2014).", "startOffset": 2, "endOffset": 21}, {"referenceID": 18, "context": "In (Luong et al., 2015) and (Jean et al.", "startOffset": 3, "endOffset": 23}, {"referenceID": 11, "context": ", 2015) and (Jean et al., 2015) there are eight models for the best scores, but we only use three models and we do not obtain further gain from", "startOffset": 12, "endOffset": 31}, {"referenceID": 18, "context": "Second, we recover the unknown words in the generated sequences with Positional Unknown (PosUnk) model introduced in (Luong et al., 2015).", "startOffset": 117, "endOffset": 137}, {"referenceID": 17, "context": "The full parallel corpus is used to obtain the word mappings (Liang et al., 2006).", "startOffset": 61, "endOffset": 81}, {"referenceID": 18, "context": "5 BLEU points, consistent with the conclusion in (Luong et al., 2015).", "startOffset": 49, "endOffset": 69}, {"referenceID": 5, "context": "At the last two lines, we list the conventional SMT model (Durrani et al., 2014) and previous best neural models based system Encoding-Decoding (Luong et al.", "startOffset": 58, "endOffset": 80}, {"referenceID": 18, "context": ", 2014) and previous best neural models based system Encoding-Decoding (Luong et al., 2015) for comparison.", "startOffset": 71, "endOffset": 91}, {"referenceID": 22, "context": "Here we show five curves of our Deep-Att single model, our Deep-Att ensemble model, our Deep-ED model, previous Enc-Dec model with four layers (Sutskever et al., 2014) and SMT model (Durrani et al.", "startOffset": 143, "endOffset": 167}, {"referenceID": 5, "context": ", 2014) and SMT model (Durrani et al., 2014).", "startOffset": 22, "endOffset": 44}, {"referenceID": 5, "context": "We also list the results from SMT model (Durrani et al., 2014) as comparison .", "startOffset": 40, "endOffset": 62}, {"referenceID": 22, "context": "As a reference related to human level, in (Sutskever et al., 2014), it has been tested that the BLEU score of oracle re-scoring the LIUM 1000-best results (Schwenk, 2014) is 45.", "startOffset": 42, "endOffset": 66}, {"referenceID": 20, "context": ", 2014), it has been tested that the BLEU score of oracle re-scoring the LIUM 1000-best results (Schwenk, 2014) is 45.", "startOffset": 96, "endOffset": 111}, {"referenceID": 5, "context": "For each example, there are four lines corresponding to source language, reference(ref), our ensemble model result with PosUnk and SMT model (Durrani) (Durrani et al., 2014) respectively.", "startOffset": 151, "endOffset": 173}, {"referenceID": 5, "context": "This single end-to-end NMT model outperforms the conventional best SMT system (Durrani et al., 2014) and achieves the state-ofthe-art performance.", "startOffset": 78, "endOffset": 100}, {"referenceID": 22, "context": "As a reference number, the work (Sutskever et al., 2014) show that oracle re-scoring the SMT generated 1000-best lists exhibits the BLEU score of about 45.", "startOffset": 32, "endOffset": 56}, {"referenceID": 11, "context": "The best results from both single model and model ensemble are obtained with beam size of 3, much smaller than previous NMT systems where beam size is about 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 160, "endOffset": 203}, {"referenceID": 22, "context": "The best results from both single model and model ensemble are obtained with beam size of 3, much smaller than previous NMT systems where beam size is about 12 (Jean et al., 2015; Sutskever et al., 2014).", "startOffset": 160, "endOffset": 203}], "year": 2016, "abstractText": "Neural machine translation (NMT) aims at solving machine translation (MT) problems with purely neural networks and exhibits promising results in recent years. However, most of the existing NMT models are of shallow topology and there is still a performance gap between the single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) network, together with the interleaved bidirectional way for stacking them. Fastforward connections play an essential role to propagate the gradients in building the deep topology of depth 16. On WMT\u201914 Englishto-French task, we achieved BLEU=37.7 with single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. It is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. Even without considering attention mechanism, we can still achieve BLEU=36.3. After the special handling for unknown words and the model ensembling, we obtained the best score on this task with BLEU=40.4. Our models are also verified on the more difficult WMT\u201914 English-to-German task.", "creator": "LaTeX with hyperref package"}}}