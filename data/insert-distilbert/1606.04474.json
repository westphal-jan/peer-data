{"id": "1606.04474", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Learning to learn by gradient descent by gradient descent", "abstract": "the rapidly move from hand - designed features to learned features in adaptive machine learning has apparently been wildly successful. in spite of realising this, optimization algorithms are still designed by hand. in producing this paper we show how the design of an optimization algorithm can be cast as a learning programming problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic bidding way. suppose our learned algorithms, implemented by lstms, outperform generic, hand - tools designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar formal structure. we generally demonstrate this on a number form of tasks, including simple convex problems, training neural networks, and incorporating styling images with neural art.", "histories": [["v1", "Tue, 14 Jun 2016 17:49:32 GMT  (3040kb,D)", "http://arxiv.org/abs/1606.04474v1", null], ["v2", "Wed, 30 Nov 2016 16:45:45 GMT  (3270kb,D)", "http://arxiv.org/abs/1606.04474v2", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["marcin andrychowicz", "misha denil", "sergio gomez colmenarejo", "matthew w hoffman", "david pfau", "tom schaul", "nando de freitas"], "accepted": true, "id": "1606.04474"}, "pdf": {"name": "1606.04474.pdf", "metadata": {"source": "CRF", "title": "Learning to learn by gradient descent by gradient descent", "authors": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez"], "emails": ["marcin.andrychowicz@gmail.com", "mdenil@google.com", "sergomez@google.com", "mwhoffman@google.com", "pfau@google.com", "schaul@google.com", "nandodefreitas@google.com"], "sections": [{"heading": "1 Introduction", "text": "Frequently, tasks in machine learning can be expressed as the problem of optimizing an objective function f(\u03b8) defined over some domain \u03b8 \u2208 \u0398. The goal in this case is to find the minimizer \u03b8\u2217 = arg min\u03b8\u2208\u0398 f(\u03b8). While any method capable of minimizing this objective function can be applied, the standard approach for differentiable functions is some form of gradient descent, resulting in a sequence of updates\n\u03b8t+1 = \u03b8t \u2212 \u03b1t\u2207f(\u03b8t) . The performance of vanilla gradient descent, however, is hampered by the fact that it only makes use of gradients and ignores second-order information. Classical optimization techniques correct this behavior by rescaling the gradient step using curvature information, typically via the Hessian matrix of second-order partial derivatives\u2014although other choices such as the generalized Gauss-Newton matrix or Fisher information matrix are possible.\nMuch of the modern work in optimization is based around designing update rules tailored to specific classes of problems, with the types of problems of interest differing between different research communities. For example, in the deep learning community we have seen a proliferation of optimization methods specialized for high-dimensional, non-convex optimization problems. These include momentum [Nesterov, 1983, Tseng, 1998], Rprop [Riedmiller and Braun, 1993], Adagrad [Duchi et al., 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015]. More focused methods can also be applied when more structure of the optimization\nar X\niv :1\n60 6.\n04 47\n4v 1\n[ cs\n.N E\nproblem is known [Martens and Grosse, 2015]. In contrast, communities who focus on sparsity tend to favor very different approaches [Donoho, 2006, Bach et al., 2012]. This is even more the case for combinatorial optimization for which relaxations are often the norm [Nemhauser and Wolsey, 1988].\nThis industry of optimizer design allows different communities to create optimization methods which exploit structure in their problems of interest at the expense of potentially poor performance on problems outside of that scope. Moreover the No Free Lunch Theorems for Optimization [Wolpert and Macready, 1997] show that in the setting of combinatorial optimization, no algorithm is able to do better than a random strategy in expectation. This suggests that specialization to a subclass of problems is in fact the only way that improved performance can be achieved in general.\nIn this work we take a different tack and instead propose to replace hand-designed update rules with a learned update rule, which we call the optimizer g, specified by its own set of parameters \u03c6. This results in updates to the optimizee f of the form\n\u03b8t+1 = \u03b8t + gt(\u2207f(\u03b8t), \u03c6) . (1) A high level view of this process is shown in Figure 1. In what follows we will explicitly model the update rule g using a recurrent neural network (RNN) which maintains its own state and hence dynamically updates as a function of its iterates."}, {"heading": "1.1 Transfer learning and generalization", "text": "The goal of this work is to develop a procedure for constructing a learning algorithm which performs well on a particular class of optimization problems. Casting algorithm design as a learning problem allows us to specify the class of problems we are interested in through example problem instances. This is in contrast to the ordinary approach of characterizing properties of interesting problems analytically and using these analytical insights to design learning algorithms by hand.\nIt is interesting to consider the meaning of generalization in this framework. In ordinary statistical learning we have a particular function of interest, whose behavior is constrained through a data set of example function evaluations. In choosing a model we specify a set of inductive biases about how we think the function of interest should behave at points we have not observed, and generalization corresponds to the capacity to make predictions about the behavior of the target function at novel points.\nIn our setting the examples are themselves problem instances, which means generalization corresponds to the ability to transfer knowledge between different problems. This reuse of problem structure is commonly known as transfer learning, and is often treated as a subject in its own right. However, by taking a meta-learning perspective, we can cast the problem of transfer learning as one of generalization, which is much better studied in the machine learning community.\nOne of the great success stories of deep-learning is that we can rely on the ability of deep networks to generalize to new examples by learning interesting sub-structures. In this work we aim to leverage this generalization power, but also to lift it from simple supervised learning to the more general setting of optimization."}, {"heading": "1.2 Related work and a brief history", "text": "The idea of using learning to learn or meta-learning to acquire knowledge or inductive biases has a long history [Thrun and Pratt, 1998]. More recently, Lake et al. [2016] have argued forcefully for its importance as a building block in artificial intelligence. In general these ideas involve learning which occurs at two different time scales: rapid learning within tasks and more gradual, global learning\nacross many different tasks. In some of the earliest work on meta-learning, Naik and Mammone [1992] use the results from previous training runs to modify the descent direction of backpropagation; however, their update strategy is somewhat more ad-hoc and not directly learned. The work of Santoro et al. [2016] takes an approach similar to ours in that multi-task learning is cast as generalization, however they directly train a base learner rather than a higher-level training algorithm.\nMore closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights. This work was built on by [Younger et al., 2001, Hochreiter et al., 2001] wherein a higher-level network act as a gradient descent procedure, with both levels trained during learning. Earlier work of Runarsson and Jonsson [2000] trains similar feed-forward meta-learning rules using evolutionary strategies. Alternatively Schmidhuber [1992, 1993] considers networks that are able to modify their own behavior and act as an alternative to recurrent networks in meta-learning. Note, however that these earlier works do not directly address the transfer of a learned training procedure to novel problem instances and instead focus on adaptivity in the online setting. Similar work has also been attacked in a filtering context [Feldkamp and Puskorius, 1998, Prokhorov et al., 2002], a line of work that is directly related to simple multi-timescale optimizers [Sutton, 1992, Schraudolph, 1999]. Finally, Daniel et al. [2016] considers using reinforcement learning to train a controller for selecting step-sizes, however this work is much more constrained than ours and still requires hand-tuned features."}, {"heading": "2 Learning to learn with recurrent neural networks", "text": "In this work we consider directly parameterizing the optimizer. As a result, in a slight abuse of notation we will write the final optimizee parameters \u03b8\u2217(\u03c6, f) as a function of the optimizer parameters \u03c6 and the function in question. We can then ask the question: What does it mean for an optimizer to be good? Given a distribution of functions f we will write the expected loss as\nL(\u03c6) = Ef [ f ( \u03b8\u2217(f, \u03c6) )] . (2)\nAs noted earlier, we will take the update steps gt to be the output of a recurrent neural network m, parameterized by \u03c6, whose state we will denote explicitly with ht. Next, while the objective function in (2) depends only on the final parameter value, for training the optimizer it will be convenient to have an objective that depends on the entire trajectory of optimization, for some horizon T,\nL(\u03c6) = Ef [ T\u2211 t=1 wtf(\u03b8t) ] where \u03b8t+1 = \u03b8t + gt ,[\ngt ht+1\n] = m(\u2207t, ht, \u03c6) .\n(3)\nHere wt \u2208 R\u22650 are arbitrary weights associated with each time-step and we will also use the notation \u2207t = \u2207\u03b8h(\u03b8t). This formulation is equivalent to (2) when wt = 1[t = T ], but later we will describe why using different weights can prove useful.\nWe can minimize the value of L(\u03c6) using gradient descent on \u03c6. The gradient estimate \u2202L(\u03c6)/\u2202\u03c6 can be computed by sampling a random function f and applying backpropagation to the computational graph in Figure 2. We allow gradients to flow along the solid edges in the graph, but gradients along the dashed edges are dropped. Ignoring gradients along the dashed edges amounts to making the assumption that the gradients of the optimizee do not depend on the optimizer parameters, i.e. \u2202\u2207t / \u2202\u03c6 = 0. This assumption allows us to avoid computing second derivatives of f .\nExamining the objective in (3) we see that the gradient is non-zero only for terms where wt 6= 0. If we use wt = 1[t = T ] to match the original problem, then gradients of trajectory prefixes are zero and only the final optimization step provides information for training the optimizer. This renders Backpropagation Through Time (BPTT) inefficient. We solve this problem by relaxing the objective such that wt > 0 at intermediate points along the trajectory. This changes the objective function, but allows us to train the optimizer on partial trajectories. For simplicity, in all our experiments we use wt = 1 for every t."}, {"heading": "2.1 Coordinatewise LSTM optimizer", "text": "One challenge in applying RNNs in our setting is that we want to be able to optimize at least tens of thousands of parameters. Optimizing at this scale with a fully connected RNN is not feasible as it would require a huge hidden state and an enormous number of parameters. To avoid this difficulty we will use an optimizer m which operates coordinatewise on the parameters of the objective function, similar to other common update rules like RMSprop and ADAM. This coordinatewise network architecture allows us to use a very small network that only looks at a single coordinate to define the optimizer and share optimizer parameters across different parameters of the optimizee.\nDifferent behavior on each coordinate is achieved by using separate activations for each objective function parameter. In addition to allowing us to use a small network for this optimizer, this setup has the nice effect of making the optimizer invariant to the order of parameters in the network, since the same update rule is used independently on each coordinate.\nWe implement the update rule for each coordinate using a two-layer Long Short Term Memory (LSTM) network [Hochreiter and Schmidhuber, 1997]. The network takes as input the optimizee gradient for a single coordinate as well as the previous hidden state and outputs the update for the corresponding optimizee parameter. We will refer to this architecture, illustrated in Figure 3, as an LSTM optimizer.\nThe use of recurrence allows the LSTM to learn dynamic update rules which integrate information from the history of gradients, similar to momentum. This is known to have many desirable properties in convex optimization [see e.g. Nesterov, 1983] and in fact many recent learning\nprocedures\u2014such as ADAM\u2014use momentum in their updates.\nPreprocessing and postprocessing Optimizer inputs and outputs can have very different magnitudes depending on the class of function being optimized, but neural networks usually work robustly only for inputs and outputs which are neither very small nor very large. In practice rescaling inputs and outputs of an LSTM optimizer using suitable constants (shared across all timesteps and functions f ) is sufficient to avoid this problem. In Appendix A we propose a different method of preprocessing inputs to the optimizer inputs which is more robust and gives slightly better performance."}, {"heading": "2.2 Information sharing between coordinates", "text": "In the previous section we considered a coordinatewise architecture, which corresponds by analogy to a learned version of RMSprop or ADAM. Although diagonal methods are quite effective in practice, we can also consider learning more sophisticated optimizers that take the correlations between coordinates into effect. To this end, we introduce a mechanism allowing different LSTMs to communicate with each other.\nGlobal averaging cells The simplest solution is to designate a subset of the cells in each LSTM layer for communication. These cells operate like normal LSTM cells, but their outgoing activations are averaged at each step across all coordinates. These global averaging cells (GACs) are sufficient to allow the networks to implement L2 gradient clipping [Bengio et al., 2013] assuming that each LSTM can compute the square of the gradient. This architecture is denoted as an LSTM+GAC optimizer.\nNTM-BFGS optimizer We also consider augmenting the LSTM+GAC architecture with an external memory that is shared between coordinates. Such a memory, if appropriately designed could allow the optimizer to learn algorithms similar to (low-memory) approximations to Newton\u2019s method, e.g. (L-)BFGS [see Nocedal and Wright, 2006]. The reason for this interpretation is that such methods can be seen as a set of independent processes working coordinatewise, but communicating through the inverse Hessian approximation stored in the memory. We designed a memory architecture that, in theory, allows the network to simulate (L-)BFGS, however we defer a detailed description of this architecture to Appendix B due to lack of space. We call this architecture an NTM-BFGS optimizer, because its use of external memory is similar to the Neural Turing Machine [Graves et al., 2014]. The pivotal differences between our construction and the NTM are (1) our memory allows only low-rank updates; (2) the controller (including read/write heads) operates coordinatewise."}, {"heading": "3 Experiments", "text": "In all experiments the trained optimizers use two-layer LSTMs with 20 hidden units in each layer. Each optimizer is trained by minimizing Equation 3 using truncated BPTT as described in Section 2. The minimization is performed using ADAM with a learning rate chosen by random search.\nWe use early stopping when training the optimizer in order to avoid overfitting the optimizer. After each epoch (some fixed number of learning steps) we freeze the optimizer parameters and evaluate its performance. We pick the best optimizer (according to the final validation loss) and report its average performance on a number of freshly sampled test problems.\nWe compare our trained optimizers with standard optimizers used in Deep Learning: SGD, RMSprop, ADAM, Adadelta, Adagrad, and Rprop. For each of these optimizer and each problem we try the following learning rates: 10\u22126, 2 \u00b710\u22126, 22 \u00b710\u22126, . . . , 229 \u00b710\u22126. We report results with the learning rate that gives the best final error for each problem. When an optimizer has more parameters than just\na learning rate (e.g. decay coefficients for ADAM) we use the default values from the optim package in Torch7. Initial values of all optimizee parameters were sampled from an IID Gaussian distribution."}, {"heading": "3.1 Quadratic functions", "text": "In this experiment we consider training an optimizer on a simple class of synthetic 10-dimensional quadratic functions. In particular we consider minimizing functions of the form\nf(\u03b8) = \u2016W\u03b8 \u2212 y\u201622 for different 10x10 matrices W and 10-dimensional vectors y whose elements are drawn from an IID Gaussian distribution. Optimizers were trained by optimizing random functions from this family and tested on newly sampled functions from the same distribution. Each function was optimized for 100 steps and the trained optimizers were unrolled for 20 steps. We have not used any preprocessing, nor postprocessing. For LSTM+GAC and NTM-BFGS models we designate 5 of the 20 units in each layer as global averaging cells. NTM-BFGS uses one read head and 3 write heads.\nLearning curves for different optimizers, averaged over many functions, are shown in the left plot of Figure 4. Each curve corresponds to the average performance of one optimization algorithm on many test functions; solid curves show learned optimizer performance and dashed curves show the performance of the standard hand-crafted baselines. It is clear the learned optimizers substantially outperform their generic counterparts in this setting, and also that the LSTM+GAC and NTMBFGS variants, which incorporate global information at each step, are able to outperform the purely coordinatewise LSTM optimizer."}, {"heading": "3.2 Training a small neural network on MNIST", "text": "In this experiment we test whether trainable optimizers can learn to optimize a small neural network on MNIST, and also explore how the trained optimizers generalize to functions beyond those they were trained on. To this end, we train the optimizer to optimize a base network and explore a series of modifications to the network architecture and training procedure at test time.\nIn this setting the objective function f(\u03b8) is the cross entropy of a small MLP with parameters \u03b8. The values of f as well as the gradients \u2202f(\u03b8)/\u2202\u03b8 are estimated using random minibatches of 128 examples. The base network is an MLP with one hidden layer of 20 units using a sigmoid activation function. The only source of variability between different runs is the initial value \u03b80 and randomness in minibatch selection. Each optimization was run for 100 steps and the trained optimizers were unrolled for 20 steps. We used input preprocessing described in Appendix A and rescaled the outputs of the LSTM by the factor 0.1.\nLearning curves for the base network using different optimizers are displayed in the center plot of Figure 4. ADAM performs best among the standard optimizers and the LSTM optimizer outperforms it by a large margin. For clarity, we do not plot the results for LSTM+GAC and NTM-BFGS, because their performance is similar to that of the LSTM optimizer.\nGeneralization to longer horizons The right plot in Figure 4 compares the performance of the LSTM optimizer if it is allowed to run for 200 steps to the hand-crafted baselines, this is despite only having been trained to optimize for 100 steps. In this comparison we re-used the LSTM optimizer from the previous experiment, but the baseline learning rates were re-tuned to optimize performance after the full 200 steps of optimization. In spite of this handicap, the LSTM optimizer still outperforms the baseline optimizers on this task.\nGeneralization to different architectures Figure 5 shows three examples of applying the LSTM optimizer to train networks with different architectures than the base network on which it was trained. The modifications are (from left to right) (1) an MLP with 40 hidden units instead of 20, (2) a network with two hidden layers instead of one, and (3) a network using ReLU activations instead of sigmoid. In the first two cases the LSTM optimizer generalizes well, and continues to outperform the hand-designed baselines despite operating outside of its training regime. However, changing the activation function to ReLU makes the dynamics of the learning procedure sufficiently different that the learned optimizer is no longer able to generalize. In all cases, the baseline learning rates were re-tuned."}, {"heading": "3.3 Training a convolutional network on CIFAR-10", "text": "Next we test the performance of the trained neural optimizers on optimizing classification performance for the CIFAR-10 dataset [Krizhevsky, 2009]. In these experiments we used a model with both convolutional and feed-forward layers. In particular, the model used for these experiments includes three convolutional layers with max pooling followed by a fully-connected layer with 32 hidden units; all non-linearities were ReLU activations with batch normalization.\nThe coordinatewise network decomposition introduced in Section 2.1\u2014and used in the previous experiment\u2014utilizes a single LSTM architecture with shared weights, but separate hidden states, for each optimizee parameter. We found that this decomposition was not sufficient for the model architecture introduced in this section due to the differences between the fully connected and convolutional layers. Instead we modify the optimizer by introducing two LSTMs: one proposes parameter updates for the fully connected layers and the other updates the convolutional layer parameters. Like the previous LSTM optimizer we still utilize a coordinatewise decomposition with shared weights and individual hidden states, however LSTM weights are now shared only between parameters of the same type (i.e. fully-connected vs. convolutional).\nTraining curves for this optimizer are shown in Figure 7, where the left plot shows training set performance. In this figure we also show that the learned optimizer can be applied to an additional dataset by learning on the held-out test set. We can see that the LSTM optimizer learns much more quickly than the standard optimizers and converges to the same value."}, {"heading": "3.4 Neural Art", "text": "The recent work on artistic style transfer using convolutional networks, or Neural Art [Gatys et al., 2015], gives a natural testbed for our method, since each content and style image pair gives rise to a different optimization problem. Each Neural Art problem starts from a a content image, c, and a style image, s, and is given by\nf(\u03b8) = \u03b1Lcontent(c, \u03b8) + \u03b2Lstyle(s, \u03b8) + \u03b3Lreg(\u03b8)\nThe minimizer of f is the styled image. The first two terms try to match the content and style of the styled image to that of their first argument, and the third term is a regularizer that encourages smoothness in the styled image. Details can be found in [Gatys et al., 2015].\nWe train optimizers using only 1 style and 1800 content images taken from ImageNet [Deng et al., 2009]. We randomly select 100 content images for testing and 20 content images for validation of trained optimizers. We train the optimizer on 64x64 content images from ImageNet and one fixed style image. We then test how well it generalizes to a different style image and higher resolution (128x128). Each image was optimized for 128 steps and trained optimizers were unrolled for 32 steps. Figure 6 shows the result of styling two different images using the LSTM optimizer. The LSTM optimizer uses inputs preprocessing described in Appendix A and no postprocessing.\nFigure 8 compares the performance of the LSTM optimizer to standard optimization algorithms. The LSTM optimizer outperforms all standard optimizers if the resolution and style image are the same as the ones on which it was trained. Moreover, it continues to perform very well when both the resolution and style are changed at test time."}, {"heading": "4 Visualizations", "text": "Visualizing optimizers is inherently difficult because their proposed updates are functions of the full optimization trajectory. In this section we try to peek into the decisions made by the LSTM optimizer, trained on the neural art task.\nHistories of updates We select a single optimizee parameter (one color channel of one pixel in the styled image) and trace the updates proposed to this coordinate by the LSTM optimizer over a single trajectory of optimization. We also record the updates that would have been proposed by both SGD and ADAM if they followed the same trajectory of iterates. Figure 9 shows the trajectory of updates for two different optimizee parameters. From the plots it is clear that the trained optimizer makes bigger updates than SGD and ADAM. It is also visible that it uses some kind of momentum, but its\nupdates are more noisy than those proposed by ADAM which may be interpreted as having a shorter time-scale momentum.\nProposed update as a function of current gradient Another way to visualize the optimizer behavior is to look at the proposed update gt for a single coordinate as a function of the current gradient evaluation \u2207t. We follow the same procedure as in the previous experiment, and visualize the proposed updates for a few selected time steps in Figure 10, and more extensively in Appendix C. The shape of this function for the LSTM optimizer is often step-like, which is also the case for ADAM. Surprisingly the step is sometimes in the opposite direction as for ADAM, i.e. the bigger the gradient, the bigger the update."}, {"heading": "5 Conclusion", "text": "We have shown how to cast the design of optimization algorithms as a learning problem, which enables us to train optimizers that are specialized to particular classes of functions. Our experiments have confirmed that learned neural optimizers compare favorably against state-of-the-art optimization methods used in deep learning. We witnessed a remarkable degree of transfer, with for example the LSTM optimizer trained on 12,288 parameter neural art tasks being able to generalize to tasks with 49,152 parameters, different styles, and different content images all at the same time. We observed similar impressive results when transferring to different architectures in the MNIST task.\nThe results on the CIFAR image labeling task show that the LSTM optimizers outperform handengineered optimizers when transferring to datasets drawn from the same data distribution.\nIn future work we plan to continue investigating the design of the NTM-BFGS optimizers. We observed that these outperformed the LSTM optimizers for quadratic functions, but we saw no benefit of using these methods in the other stochastic optimization tasks. Another important direction for future work is to develop optimizers that scale better in terms of memory usage."}, {"heading": "A Gradient preprocessing", "text": "One potential challenge in training optimizers is that different input coordinates (i.e. the gradients w.r.t. different optimizee parameters) can have very different magnitudes. This is indeed the case e.g. when the optimizee is a neural network and different parameters correspond to weights in different layers. This can make training an optimizer difficult, because neural networks naturally disregard small variations in input signals and concentrate on bigger input values.\nTo this aim we propose to preprocess the optimizer\u2019s inputs. One solution would be to give the optimizer (log(|\u2207|), sgn(\u2207)) as an input, where\u2207 is the gradient in the current timestep. This has a problem that log(|\u2207|) diverges for\u2207 \u2192 0. Therefore, we use the following preprocessing formula\n\u2207k \u2192\n{( log(|\u2207|) p , sgn(\u2207) ) if |\u2207| \u2265 e\u2212p\n(\u22121, ep\u2207) otherwise where p > 0 is a parameter controlling how small gradients are disregarded (we use p = 10 in all our experiments).\nWe noticed that just rescaling all inputs by an appropriate constant instead also works fine, but the proposed preprocessing seems to be more robust and gives slightly better results on some problems."}, {"heading": "B NTM-BFGS optimizer", "text": "In this section we describe the construction of the NTM-BFGS optimizer in detail. Its design is motivated by the approximate Newton method BFGS, named for Broyden, Fletcher, Goldfarb, and Shanno. In BFGS an explicit estimate of the full (inverse) Hessian is built up from the sequence of observed gradients. We can write a skeletonized version of the BFGS algorithm, using Mt to represent the inverse Hessian approximation at iteration t, as follows\ngt = read(Mt, \u03b8t)\n\u03b8t+1 = \u03b8t + gt Mt+1 = write(Mt, \u03b8t, gt) .\nHere we have packed up all of the details of the BFGS algorithm into the suggestively named read and write operations, which operate on the inverse Hessian approximation Mt. In BFGS these operations have specific forms, for example read(Mt, \u03b8t) = \u2212Mt\u2207h(\u03b8t) is a specific matrix-vector multiplication and the BFGS write operation corresponds to a particular low-rank update of Mt.\nIn this work we preserve the structure of the BFGS updates, but discard their particular form. More specifically the read operation remains a matrix-vector multiplication but the form of the vector used is learned. Similarly, the write operation remains a low-rank update, but the vectors involved are also learned. Conveniently, this structure of interaction with a large dynamically updated state corresponds in a fairly direct way to the architecture of a Neural Turing Machine (NTM), where Mt corresponds to the NTM memory [Graves et al., 2014].\nOur NTM-BFGS optimizer uses an LSTM+GAC as a controller; however, instead of producing the update directly we attach one or more read and write heads to the controller. Each read head produces a read vector rt which is combined with the memory to produce a read result it which is fed back into the controller at the following time step. Each write head produces two outputs, a left write vector at and a right write vector bt. The two write vectors are used to update the memory state by accumulating their outer product. The read and write operation for a single head is diagrammed in Figure 11 and the way read and write heads are attached to the controller is depicted in Figure 12.\nIn can be shown that NTM-BFGS with one read head and 3 write heads can simulate inverse Hessian BFGS assuming that the controller can compute arbitrary (coordinatewise) functions and have access to 2 GACs.\nNTM-L-BFGS optimizer In cases where memory is constrained we can follow the example of L-BFGS and maintain a low rank approximation of the full memory (vis. inverse Hessian). The simplest way to do this is to store a sliding history of the left and right write vectors, allowing us to form the matrix vector multiplication required by the read operation efficiently. We use this limited-memory approach in MNIST and Neural Art experiments."}, {"heading": "C Gradient Visualizations", "text": "Here we show the proposed updates for the three color channels of a corner pixel from one neural art instance. Figures are explained in Section 4.\nStep 1 \u221210\n0\n10\nStep 2 Step 3 Step 4\nStep 5 \u221210\n0\n10\nStep 6 Step 7 Step 8\nStep 9 \u221210\n0\n10\nStep 10 Step 11 Step 12\nStep 13 \u221210\n0\n10\nStep 14 Step 15 Step 16\nStep 17 \u221210\n0\n10\nStep 18 Step 19 Step 20\nStep 21 \u221210\n0\n10\nStep 22 Step 23 Step 24\nStep 25 \u221210\n0\n10\nStep 26 Step 27 Step 28\n\u2212400 0 400 Step 29\n\u221210\n0\n10\n\u2212400 0 400 Step 30 \u2212400 0 400 Step 31 \u2212400 0 400 Step 32\nStep 1 \u221210\n0\n10\nStep 2 Step 3 Step 4\nStep 5 \u221210\n0\n10\nStep 6 Step 7 Step 8\nStep 9 \u221210\n0\n10\nStep 10 Step 11 Step 12\nStep 13 \u221210\n0\n10\nStep 14 Step 15 Step 16\nStep 17 \u221210\n0\n10\nStep 18 Step 19 Step 20\nStep 21 \u221210\n0\n10\nStep 22 Step 23 Step 24\nStep 25 \u221210\n0\n10\nStep 26 Step 27 Step 28\n\u2212400 0 400 Step 29\n\u221210\n0\n10\n\u2212400 0 400 Step 30 \u2212400 0 400 Step 31 \u2212400 0 400 Step 32\nStep 1 \u221210\n0\n10\nStep 2 Step 3 Step 4\nStep 5 \u221210\n0\n10\nStep 6 Step 7 Step 8\nStep 9 \u221210\n0\n10\nStep 10 Step 11 Step 12\nStep 13 \u221210\n0\n10\nStep 14 Step 15 Step 16\nStep 17 \u221210\n0\n10\nStep 18 Step 19 Step 20\nStep 21 \u221210\n0\n10\nStep 22 Step 23 Step 24\nStep 25 \u221210\n0\n10\nStep 26 Step 27 Step 28\n\u2212400 0 400 Step 29\n\u221210\n0\n10\n\u2212400 0 400 Step 30 \u2212400 0 400 Step 31 \u2212400 0 400 Step 32"}], "references": [{"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bach et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["Y. Bengio", "N. Boulanger-Lewandowski", "R. Pascanu"], "venue": "In International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "URL https://www.flickr.com/photos/fbobolas/ 3822222947", "author": ["F. Bobolas"], "venue": "brain-neurons,", "citeRegEx": "Bobolas.,? \\Q2009\\E", "shortCiteRegEx": "Bobolas.", "year": 2009}, {"title": "Fixed-weight networks can learn", "author": ["N.E. Cotter", "P.R. Conwell"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Cotter and Conwell.,? \\Q1990\\E", "shortCiteRegEx": "Cotter and Conwell.", "year": 1990}, {"title": "Learning step size controllers for robust neural network training", "author": ["C. Daniel", "J. Taylor", "S. Nowozin"], "venue": "In Association for the Advancement of Artificial Intelligence,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification", "author": ["L.A. Feldkamp", "G.V. Puskorius"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Feldkamp and Puskorius.,? \\Q1998\\E", "shortCiteRegEx": "Feldkamp and Puskorius.", "year": 1998}, {"title": "A neural algorithm of artistic style", "author": ["L.A. Gatys", "A.S. Ecker", "M. Bethge"], "venue": "arXiv Report 1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning to learn using gradient descent", "author": ["S. Hochreiter", "A.S. Younger", "P.R. Conwell"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv Report", "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "URL https://www.flickr.com/photos/taylortotz101/ 6280077898", "author": ["T. Maley"], "venue": "Creative Commons Attribution 2.0 Generic", "citeRegEx": "Maley.,? \\Q2011\\E", "shortCiteRegEx": "Maley.", "year": 2011}, {"title": "Optimizing neural networks with Kronecker-factored approximate curvature", "author": ["J. Martens", "R. Grosse"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Martens and Grosse.,? \\Q2015\\E", "shortCiteRegEx": "Martens and Grosse.", "year": 2015}, {"title": "Meta-neural networks that learn by learning", "author": ["D.K. Naik", "R. Mammone"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Naik and Mammone.,? \\Q1992\\E", "shortCiteRegEx": "Naik and Mammone.", "year": 1992}, {"title": "Integer and combinatorial optimization", "author": ["G.L. Nemhauser", "L.A. Wolsey"], "venue": null, "citeRegEx": "Nemhauser and Wolsey.,? \\Q1988\\E", "shortCiteRegEx": "Nemhauser and Wolsey.", "year": 1988}, {"title": "A method of solving a convex programming problem with convergence rate o (1/k2)", "author": ["Y. Nesterov"], "venue": "In Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Adaptive behavior with fixed weights in rnn: an overview", "author": ["D.V. Prokhorov", "L.A. Feldkamp", "I.Y. Tyukin"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Prokhorov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Prokhorov et al\\.", "year": 2002}, {"title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "author": ["M. Riedmiller", "H. Braun"], "venue": "In International Conference on Neural Networks,", "citeRegEx": "Riedmiller and Braun.,? \\Q1993\\E", "shortCiteRegEx": "Riedmiller and Braun.", "year": 1993}, {"title": "Evolution and design of distributed learning rules", "author": ["T.P. Runarsson", "M.T. Jonsson"], "venue": "In IEEE Symposium on Combinations of Evolutionary Computation and Neural Networks,", "citeRegEx": "Runarsson and Jonsson.,? \\Q2000\\E", "shortCiteRegEx": "Runarsson and Jonsson.", "year": 2000}, {"title": "Meta-learning with memoryaugmented neural networks", "author": ["A. Santoro", "S. Bartunov", "M. Botvinick", "D. Wierstra", "T. Lillicrap"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1992}, {"title": "A neural network that embeds its own meta-levels", "author": ["J. Schmidhuber"], "venue": "In International Conference on Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1993\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1993}, {"title": "Local gain adaptation in stochastic gradient descent", "author": ["N.N. Schraudolph"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Schraudolph.,? \\Q1999\\E", "shortCiteRegEx": "Schraudolph.", "year": 1999}, {"title": "Adapting bias by gradient descent: An incremental version of delta-bar-delta", "author": ["R.S. Sutton"], "venue": "In Association for the Advancement of Artificial Intelligence,", "citeRegEx": "Sutton.,? \\Q1992\\E", "shortCiteRegEx": "Sutton.", "year": 1992}, {"title": "Learning to learn", "author": ["S. Thrun", "L. Pratt"], "venue": "Springer Science & Business Media,", "citeRegEx": "Thrun and Pratt.,? \\Q1998\\E", "shortCiteRegEx": "Thrun and Pratt.", "year": 1998}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "An incremental gradient (-projection) method with momentum term and adaptive stepsize rule", "author": ["P. Tseng"], "venue": "Journal on Optimization,", "citeRegEx": "Tseng.,? \\Q1998\\E", "shortCiteRegEx": "Tseng.", "year": 1998}, {"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "Transactions on Evolutionary Computation,", "citeRegEx": "Wolpert and Macready.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert and Macready.", "year": 1997}, {"title": "Fixed-weight on-line learning", "author": ["A.S. Younger", "P.R. Conwell", "N.E. Cotter"], "venue": "Transactions on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Younger et al\\.", "year": 1999}, {"title": "Meta-learning with backpropagation", "author": ["A.S. Younger", "S. Hochreiter", "P.R. Conwell"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Younger et al\\.", "year": 2001}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv Report 1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "These include momentum [Nesterov, 1983, Tseng, 1998], Rprop [Riedmiller and Braun, 1993], Adagrad [Duchi et al.", "startOffset": 60, "endOffset": 88}, {"referenceID": 6, "context": "These include momentum [Nesterov, 1983, Tseng, 1998], Rprop [Riedmiller and Braun, 1993], Adagrad [Duchi et al., 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 98, "endOffset": 118}, {"referenceID": 33, "context": ", 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 18, "endOffset": 32}, {"referenceID": 28, "context": ", 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 42, "endOffset": 69}, {"referenceID": 11, "context": ", 2011], Adadelta [Zeiler, 2012], RMSprop [Tieleman and Hinton, 2012], and ADAM [Kingma and Ba, 2015].", "startOffset": 80, "endOffset": 101}, {"referenceID": 15, "context": "problem is known [Martens and Grosse, 2015].", "startOffset": 17, "endOffset": 43}, {"referenceID": 17, "context": "This is even more the case for combinatorial optimization for which relaxations are often the norm [Nemhauser and Wolsey, 1988].", "startOffset": 99, "endOffset": 127}, {"referenceID": 30, "context": "Moreover the No Free Lunch Theorems for Optimization [Wolpert and Macready, 1997] show that in the setting of combinatorial optimization, no algorithm is able to do better than a random strategy in expectation.", "startOffset": 53, "endOffset": 81}, {"referenceID": 27, "context": "2 Related work and a brief history The idea of using learning to learn or meta-learning to acquire knowledge or inductive biases has a long history [Thrun and Pratt, 1998].", "startOffset": 148, "endOffset": 171}, {"referenceID": 13, "context": "More recently, Lake et al. [2016] have argued forcefully for its importance as a building block in artificial intelligence.", "startOffset": 15, "endOffset": 34}, {"referenceID": 12, "context": "In some of the earliest work on meta-learning, Naik and Mammone [1992] use the results from previous training runs to modify the descent direction of backpropagation; however, their update strategy is somewhat more ad-hoc and not directly learned.", "startOffset": 47, "endOffset": 71}, {"referenceID": 12, "context": "In some of the earliest work on meta-learning, Naik and Mammone [1992] use the results from previous training runs to modify the descent direction of backpropagation; however, their update strategy is somewhat more ad-hoc and not directly learned. The work of Santoro et al. [2016] takes an approach similar to ours in that multi-task learning is cast as generalization, however they directly train a base learner rather than a higher-level training algorithm.", "startOffset": 47, "endOffset": 282}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al.", "startOffset": 57, "endOffset": 83}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights.", "startOffset": 57, "endOffset": 115}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights. This work was built on by [Younger et al., 2001, Hochreiter et al., 2001] wherein a higher-level network act as a gradient descent procedure, with both levels trained during learning. Earlier work of Runarsson and Jonsson [2000] trains similar feed-forward meta-learning rules using evolutionary strategies.", "startOffset": 57, "endOffset": 501}, {"referenceID": 3, "context": "More closely related is the line of work that began with Cotter and Conwell [1990] and later Younger et al. [1999] who showed that due to their hidden state, fixed-weight recurrent neural networks can exhibit dynamic behavior without need to modify their network weights. This work was built on by [Younger et al., 2001, Hochreiter et al., 2001] wherein a higher-level network act as a gradient descent procedure, with both levels trained during learning. Earlier work of Runarsson and Jonsson [2000] trains similar feed-forward meta-learning rules using evolutionary strategies. Alternatively Schmidhuber [1992, 1993] considers networks that are able to modify their own behavior and act as an alternative to recurrent networks in meta-learning. Note, however that these earlier works do not directly address the transfer of a learned training procedure to novel problem instances and instead focus on adaptivity in the online setting. Similar work has also been attacked in a filtering context [Feldkamp and Puskorius, 1998, Prokhorov et al., 2002], a line of work that is directly related to simple multi-timescale optimizers [Sutton, 1992, Schraudolph, 1999]. Finally, Daniel et al. [2016] considers using reinforcement learning to train a controller for selecting step-sizes, however this work is much more constrained than ours and still requires hand-tuned features.", "startOffset": 57, "endOffset": 1194}, {"referenceID": 9, "context": "We implement the update rule for each coordinate using a two-layer Long Short Term Memory (LSTM) network [Hochreiter and Schmidhuber, 1997].", "startOffset": 105, "endOffset": 139}, {"referenceID": 1, "context": "These global averaging cells (GACs) are sufficient to allow the networks to implement L2 gradient clipping [Bengio et al., 2013] assuming that each LSTM can compute the square of the gradient.", "startOffset": 107, "endOffset": 128}, {"referenceID": 12, "context": "3 Training a convolutional network on CIFAR-10 Next we test the performance of the trained neural optimizers on optimizing classification performance for the CIFAR-10 dataset [Krizhevsky, 2009].", "startOffset": 175, "endOffset": 193}, {"referenceID": 8, "context": "4 Neural Art The recent work on artistic style transfer using convolutional networks, or Neural Art [Gatys et al., 2015], gives a natural testbed for our method, since each content and style image pair gives rise to a different optimization problem.", "startOffset": 100, "endOffset": 120}, {"referenceID": 8, "context": "Details can be found in [Gatys et al., 2015].", "startOffset": 24, "endOffset": 44}, {"referenceID": 5, "context": "We train optimizers using only 1 style and 1800 content images taken from ImageNet [Deng et al., 2009].", "startOffset": 83, "endOffset": 102}], "year": 2016, "abstractText": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.", "creator": "LaTeX with hyperref package"}}}