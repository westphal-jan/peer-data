{"id": "1606.04988", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Logarithmic Time One-Against-Some", "abstract": "we create a new online reduction of exact multiclass classification based to measure binary classification measures for which training and prediction time scale logarithmically with the number of classes. compared to previous comparable approaches, we obtain substantially better statistical performance for two reasons : first, we prove a tighter and more complete code boosting theorem, second and second we translate the results more directly into an algorithm. we show that several simple techniques give rise to an algorithm that they can compete with one - against - all in wielding both space and predictive reasoning power while offering exponential improvements reduction in speed when the number of classes is large.", "histories": [["v1", "Wed, 15 Jun 2016 21:27:43 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04988v1", null], ["v2", "Thu, 1 Dec 2016 02:09:04 GMT  (202kb,D)", "http://arxiv.org/abs/1606.04988v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["hal daum\u00e9 iii", "nikos karampatziakis", "john langford", "paul mineiro"], "accepted": true, "id": "1606.04988"}, "pdf": {"name": "1606.04988.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Nikos Karampatziakis"], "emails": ["hal@cs.umd.edu", "nikosk@microsoft.com", "jcl@microsoft.com", "pmineiro@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large. An attractive strategy for picking one of K items efficiently is to use a tree; unfortunately, this often comes at the cost of increased error. If efficiency isn\u2019t a concern, the most common and generally effective representation for multiclass prediction is a one-against-all (OAA) structure. Here, inference consists of computing a score for each class and returning the class with the maximum score.\nA general replacement for the one-against-all approach must satisfying a difficult set of desiderata.\n\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22]. \u2022 High speed at training time and test time: A multiclass classifier must spend at least \u03b8(logK) time [7]) so this is a natural benchmark to optimize against. \u2022 Online operation. Many learning algorithms use either online updates or mini-batch updates. Approaches satisfying this constraint can be easily composed into an end-to-end learning system for solving complex problems like image recognition. Online approaches also easily compose with batch learning algorithms such as traditional decision tree learning. \u2022 Linear space: In order to have a drop-in replacement for OAA, an approach must not take much more space than OAA. Memory is at a premium when K is very large, especially for models trained on GPUs, or deployed to small devices.\nWe use an OAA-like structure to make a final prediction, but instead of scoring every class, we only score a small subset of O(logK) classes. We call this \u201cone-against-some\u201d (OAS). How can you efficiently determine what classes should be scored? We use a dynamically built tree to efficiently\nar X\niv :1\n60 6.\n04 98\n8v 1\n[ st\nat .M\nL ]\n1 5\nInput: Example x; Root Node n Result: Predicted class y\u0302 do\nr\u2190 n.f(x) > 0 ? n.left : n.right if \u0302recall(n) > \u0302recall(r) then\nbreak end n\u2190 r x\u2190 x \u222a {(n : 1)}\nwhile n.leaf is false y\u0302 \u2190 argmax\ny\u2208n.candidates Predicty(x)\n\ud835\udc65\n{1, 4, 7, \u2026 }\n\u2026\n{\ud835\udc601, \ud835\udc604, \ud835\udc607, \u2026 }\n\ud835\udc66 = \ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc60\ud835\udc58\n{1, 4, 7, \u2026 }\n\ud835\udc65\n\ud835\udc65\n\ud835\udc53(\ud835\udc65; 0)\n\ud835\udc53(\ud835\udc65; 1) \ud835\udc53(\ud835\udc65; 2)\nFigure 1: Left: Pseudocode for prediction where n.f(x) evaluates the node\u2019s route, \u0302recall() is a recall bound, and x \u222a {(n : 1)} indicates the addition of a sparse feature with index n and value 1. Right: An example is routed through the tree until it reaches a leaf node associated with a set of eligible classes.\nwhittle down the set of candidate classes. The goal of the tree is to maximize the recall of the candidate set so we call this approach \u201cThe Recall Tree.\u201d\nFigure 1 depicts the inference procedure for the Recall Tree: an example is routed through a tree until termination, and then the set of eligible classes compete to predict the label. We use this inference procedure at training time, to facilitate end-to-end joint optimization of the predictors at each internal node in the tree (the \u201crouters\u201d), the tree structure, and the final OAS predictors.\nThe Recall Tree achieves good accuracy, always improving on previous online approaches [7] and sometimes surpassing the OAA baseline. The training algorithm requires only poly(logK) time during training and testing. In practice, the computational benefits are substantial when K \u2265 1000.1 The Recall Tree constructs a tree and learns parameters in a fully online manner as a reduction allowing composition with systems trained via online updates. All of this requires only a factor of 2 more space than OAA approaches.\nOur contributions are the following:\n\u2022 We propose a new online tree construction algorithm which jointly optimizes the construction of the tree, the routers and the underlying OAS predictors (see section 3.2). \u2022 We analyze elements of the algorithm, including a new boosting bound (see section 3.1) on multiclass classification performance and a representational trick which allows the algorithm to perform well if either a tree representation does well or a OAA representation does well as discussed in section 3.3. \u2022 We experiment with the new algorithm, both to analyze its performance relative to baselines and understand the impact of design decisions via ablation experiments.\nThe net effect is a theoretically motivated algorithm which empirically performs well providing a plausible replacement for the standard one-against-all approach in the large K setting."}, {"heading": "1.1 Prior Work", "text": "The LOMTree[7] is the closest prior work, only missing on space requirements where up to a factor of 64 more space than OAA was used experimentally. Despite working with radically less space we show the Recall Tree provides better predictive performance.\nOther approaches such as hierarchical softmax (HSM) and the the Filter Tree [3] use a fixed tree structure [17]. In domains in which there is no prespecified tree hierarchy, using a random tree structure can lead to considerable underperformance as shown previously [1, 7].\nMost other approaches in extreme classification either do not work online [16, 19] or only focus on speeding up either prediction time or training time but not both. Most of the works that enjoy sublin-\n1Our implementation of baseline approaches, including OAA, involve vectorized computations that increase throughput by a factor of 10 to 20, making them much more difficult to outpace than na\u0131\u0308ve implementations.\nInput: Example (x, y); Root node n Result: Updated tree with root at n do\nupdate router(x, y,n) r\u2190 n.f(x) > 0 ? n.left : n.right update candidates(x, y, r)\nif \u0302recall(n) > \u0302recall(r) then break end n\u2190 r x\u2190 x \u222a {(n : 1)}\nwhile n.leaf is false update predictors(x, y, n.candidates)\nInput: Example (x, y); Node n Result: Update node n H\u0302left, H\u0302 \u2032 left . = entropy(n.left, y) H\u0302right, H\u0302 \u2032 right . = entropy(n.right, y) H\u0302|left . = n.left.totaln.total H\u0302 \u2032 left + n.right.total n.total H\u0302right H\u0302|right . = n.left.totaln.total H\u0302left + n.right.total n.total H\u0302 \u2032 right\n\u2206\u0302Hpost \u2190 H\u0302|left \u2212 H\u0302|right Learnn(x, |\u2206\u0302Hpost|, sign(\u2206\u0302Hpost))\n(b) update router. Here, entropy computes\near inference time (but (super)linear training time) are based on tree decomposition approaches. In [16] the authors try to add tree structure learning to HSM via iteratively clustering the classes. While the end result is a classifier whose inference time scales logarithmically with the number of classes, the clustering steps are batch and scale poorly with the number of classes. Similar remarks apply to [1] where the authors propose to learn a tree by solving an eigenvalue problem after (OAA) training. The work of [25] is similar in spirit to ours, as the authors propose to learn a label filter to reduce the number of candidate classes in an OAA approach. However they learn the tree after training the underlying OAA predictors while in this work we learn and, more crucially, use the tree during training of the OAS predictors. Among the approaches that speed up training time we distinguish exact ones [8, 23] that have only been proposed for particular loss functions and approximate ones such as negative sampling as used e.g. in [24]. Though these techniques do not address inference time, separate procedures for speeding up inference (given a trained model) have been proposed [21]. However, such two step procedures can lead to substantially suboptimal results."}, {"heading": "2 The Recall Tree Algorithm", "text": "Here we present a concrete description of the Recall Tree and defer all theoretical results that motivate our decisions to the next section. The Recall Tree maintains one predictor for each class and a tree whose purpose is to eliminate predictors from consideration. We refer to the per-class predictors as one-against-some (OAS) predictors. The tree to create a high recall set of candidate classes and then leverages the OAS predictors to achieve precision. Crucially, the leaves of the tree do not partition the set of classes: classes can (and do) have support at multiple leaves.\nFigure 2 outlines the learning procedures, which we now describe in more detail. Each node in the tree maintains a set of statistics. First, each node n maintains a router, denoted n.f , that maps an example to either a left or right child. This router is implemented as a binary classifier. Second, each node maintains a histogram of the labels of all training examples that have been routed to, or through, that node. This histogram is used in two ways: (1) the most frequent classes form the competitor set for the OAS predictors; (2) the histogram is used to decide whether the statistics at each node can be trusted. This is a crucial issue with trees because a child node sees fewer data than its parent. Therefore we do not simply rely on the empirical recall (i.e. the observed fraction of labels that fall into the most frequent F labels at this node) of a node since such estimate can have considerable variance at deep nodes. Instead, we use a lower bound of the true recall which we compute via an empirical Bernstein inequality (see Section 3.2).\nLearning the predictors for each class In Figure 2a update predictors updates the candidate set predictors using the standard OAA strategy restricted to the set of eligible classes. If the true label is not in the F most frequent classes at this node then no update occurs.\nLearning the set of candidates in each node In Figure 2a update candidates updates the count of the true label at this node. At each node, the most frequent F labels are the candidate set.\nLearning the routers at each node In Figure 2b update router updates the router at a node by optimizing the decrease in the entropy of the label distribution (the label entropy) due to routing. This is in accordance with our theory (Section 3.1). The label entropy for a node is estimated using the empirical counts of each class label entering the node. These counts are reliable as update router is only called for the root or nodes whose true recall bound is better than their children. The expected label entropy after routing is estimated by averaging the estimated label entropy of each child node, weighted by the fraction of examples routing left or right. Finally, we compute the advantage of routing left vs. right by taking the difference of the expected label entropies for routing left vs. right. The sign of this difference determines the binary label for updating the router.\nTree depth control Although we limit the maximum depth d of the tree, this parameter is typically not operative. Instead, we calculate a lower bound \u0302recall(n) on the true recall of node n (Section 3.2), halting descent as in Figure 2a. As we descend the tree, the bound first increases (empirical recall increases) then declines (variance increases)."}, {"heading": "3 Theoretical Motivation", "text": "Online construction of an optimal logarithmic time predictor for multiclass classification given an arbitrary fixed representation at each node appears deeply intractable. A main difficulty is that decisions have to be hard because we cannot afford to maintain a distribution over all class labels. Choosing a classifier so as to minimize error rate is not just NP-hard, it is so difficult it has been considered for cryptographic primitives [5]. Furthermore, the joint optimization of all predictors does not nicely decompose into independent problems. Solving the above problems requires an implausible break-through in complexity theory which we do not achieve here. Instead, we use learning theory to assist the design by analyzing various simplifications of the problem."}, {"heading": "3.1 Optimization Objective", "text": "Shannon Entropy of class labels is optimized in the router of figure 2b. Why?\nSince the Recall Tree jointly optimizes over many base learning algorithms, the systemic properties of the joint optimization are important to consider. A theory of decision tree learning as boosting [12] provides a way to understand these joint properties in a population limit (or equivalently on a training set iterated until convergence). In essence, the analysis shows that each level of the decision tree boosts the accuracy of the resulting tree with this conclusion holding for several common objectives.\nFor multiclass classification, it is important to achieve a weak dependence on the number of class labels. Shannon Entropy is particularly well-suited to this goal, because it has only a logarithmic dependence on the number of class labels. Let \u03c0i|n be the probability that the correct label is i, conditioned on the corresponding example reaching node n. Then Hn = \u2212 \u2211K i=1 \u03c0i|n log2 \u03c0i|n is the Shannon entropy of class labels reaching node n.\nFor this section, we consider a simplified algorithm which neglects concerns of finite sample analysis, how optimization is done, and the leaf predictors. What\u2019s left is the value of optimizing the router objective. We consider an algorithm which recursively splits the leaf with the largest fraction f of all examples starting at the root and reaching the leaf. The leaf is split into two new leaves to the left l and right r. If fl and fr are the fraction of examples going left and right, the split criterion minimizes the expectation over the leaves of the average class entropy, flHl + frHr. This might be achieved by update router in Figure 2a or by any other means. With this criterion and the next assumption tree learning is effective. Definition 1. (\u03b3-Weak Learning Assumption) For all distributions D(x, y) a learning algorithm using examples (x, y)\u2217 IID from D finds a binary classifier c : X \u2192 {\u22121, 1} satisfying\nPr (x,y)\u223cD (c(x) 6= y) \u2264 1 2 \u2212 \u03b3 .\nAs long as Weak Learning occurs, we can prove the following theorem. Theorem 2. If \u03b3 Weak Learning holds for every node in the tree and nodes with the largest fraction of examples are split first, then after t splits the multiclass error rate of the tree is bounded by:\n\u2264 \u221a H1 exp [ \u2212 \u03b3\n2 blog2 tc 16 dlog2Ke ] where H1 is the entropy of the marginal distribution of class labels.\nThe proof in appendix A reuses techniques from [7, 12].\nThe most important observation from the theorem is that as t (the number of splits) increases, the error rate is increasingly bounded. This rate depends on log2 t agreeing with the intuition that boosting happens level by level in the tree. A careful examination of the proof shows that log2K is an upper bound on the entropy at internal nodes, implying the reality may be somewhat better than the theory suggests. The dependence on the initial entropy H1 shows that skewed marginal class distributions are inherently easier to learn than uniform marginal class distributions, as might be expected. These results are similar to previous results [7, 12] with two advantages. We handle multiclass rather than binary classification [12] and we bound error rates instead of entropy [7]."}, {"heading": "3.2 One-Against-Some Prediction and Recall", "text": "For binary classification, branching programs [14] result in exponentially more succinct representations than decision trees [12] by joining nodes into a directed acyclic graphs. The key observation is that nodes in the same level with a similar distribution over class labels can be joined into one node, implying that the number of nodes at one level is only \u03b8(1/\u03b3) where \u03b3 is the weak learning parameter rather than exponential in the depth. This approach generally fails in the multiclass setting because covering the simplex of multiclass label distributions requires (K \u2212 1)\u03b8(1/\u03b3) nodes. One easy special case exists. When the distribution over class labels is skewed so one label is the majority class, learning a minimum entropy classifier is equivalent to predicting whether the class is the majority or not. There are only K possible OAS predictors of this sort so maintaining one for each class label is computationally tractable.\nUsing OAS classifiers creates a limited branching program structure over predictions. Aside from the space savings generated, this also implies that nodes deep in the tree use many more labeled examples than are otherwise available. In finite sample regimes, which are not covered by these boosting analyses, having more labeled samples implies a higher quality predictor as per standard sample complexity analysis.\nA fundamental issue with a tree-structured prediction is that the number of labeled examples incident on the root is much larger than the number of labeled examples incident on a leaf. This potentially leads to: (1) underfitting toward the leaves; and (2) insufficient representation complexity toward the root. Optimizing recall, rather than accuracy, ameliorates this drawback. Instead of halting at a leaf, we can halt at an internal node n for which the top F most frequent labels contain the true answer with a sufficiently high probability. When F = O(logK) this does not compromise the goal of achieving logarithmic time classification.\nNevertheless, as data gets divided down the branches of the tree, empirical estimates for the \u201ctop F most frequent labels\u201d suffer from a substantial missing mass problem [10]. Thus, instead of computing empirical recall to determine when to halt descent, we use an empirical Bernstein (lower) bound [15], which is summarized by the following proposition. Proposition 1. For all learning problems D and all nodes n in a fixed tree there exists a constant \u03bb > 0 such that with probability 1\u2212 \u03b4\nrn \u2265 r\u0302n \u2212\n\u221a \u03bbr\u0302n(1\u2212 r\u0302n)\nmn \u2212 \u03bb mn\n(1)\nwhere r\u0302n is the empirical frequency amongst mn events that the true label is in the top F labels and rn is the expected value in the population limit.\nReducing the depth of the tree by using a bound on rn and joining labeled examples from many leaves in a one-against-some approach both relieves data sparsity problems and allows greater tolerance of errors by the root node."}, {"heading": "3.3 Path Features", "text": "The relative representational power of different solutions is an important consideration. Are OAA types of representations inherently more or less powerful than a tree based representation? Figure 3 shows two learning problems illustrating two extremes under the assumption of a linear representation. On the left is a learning problem for which OAA is ideal while a tree representation requires at least 3 nodes. Conversely, the right image shows a distribution easily solved by a decision tree yet not solved by OAA predictors.\nBased on this, neither tree-based nor OAA style prediction is inherently more powerful, with the best solution being problem dependent.\nSince we are interested in starting with a tree-based approach and ending with a OAS classifier there is a simple representational trick which provides the best of both worlds. We can add features which record the path through the tree. To be precise, let T be a tree and pathT (x) be a vector with one dimension per node in T which is set to 1 if x traverses the node and 0 otherwise. The following proposition holds. Proposition. For any learning problem D for which a tree T achieves error rate , OAA(x, pathT (x)) with a linear representation can achieve error rate .\nLinear representations are special, because they are tractably analyzed and because they are the fundamental building blocks around which many more complex representations are built. Hence, this representational change eases prediction in many common settings.\nProof. A linear OAA classifier is defined by a matrix wiy where i ranges over the input and y ranges over the labels. Let wiy = 0 by default and 1 when i corresponds to a leaf for which the tree predicts y. Under this representation, the prediction of OAA(x, pathT (x)) is identical to T (x), and hence achieves the same error rate."}, {"heading": "4 Empirical Results", "text": "We study several questions empirically.\n1. What is the benefit of using one-against-some on a recall set? 2. What is the benefit of path features? 3. Is the online nature of the Recall Tree useful on nonstationary problems? 4. How does the Recall Tree compare to one-against-all statistically and computationally? 5. How does the Recall Tree compare to LOMTree statistically and computationally?\nThroughout this section we conduct experiments using learning with a linear representation."}, {"heading": "4.1 Datasets", "text": "Table 1 overviews the data sets used for experimentation. These include the largest datasets where published results are available for LOMTree (Aloi, Imagenet, ODP), plus an additional language\nmodeling data set (LTCB). Implementations of the learning algorithms, and scripts to reproduce the data sets and experimental results, are available at (url redacted). Additional details about the datasets can be found in Appendix B."}, {"heading": "4.2 Comparison with other Algorithms", "text": "In our first set of experiments, we compare Recall Tree with a strong computational baseline and a strong statistical baseline. The computational baseline is LOMTree, the only other online logarithmic-time multiclass algorithm of which we are aware. The statistical baseline is OAA, whose statistical performance we would like to match (or even exceed), and whose linear computational dependence on the number of classes we would like to avoid. Details regarding the experimental methodology are in Appendix C. Results are summarized in Figure 4.\nComparison with LOMTree The Recall Tree typically provides superior statistical performance to LOMTree despite being restricted to use a factor of 32 less state. This typically comes with an additional computational cost since the Recall Tree must evaluate a number of per-class predictors.\nComparison with OAA While it is possible for Recall Tree to surpass OAA, typically there is a gap in statistical performance. Computationally OAA has favorable constant factors since it is highly amenable to vectorization. Conversely, the conditional execution pattern of the Recall Tree frustrates vectorization even with example mini-batching. Thus on ALOI although Recall Tree does on average 50 hyperplane evaluations per example while OAA does 1000, OAA is actually faster: larger numbers of classes are required to experience the asymptotic benefits. For ODP with circa 105 classes, with negative gradient subsampling and using 24 cores in parallel, OAA is about the same wall clock time to train as Recall Tree on a single core.2 Negative gradient sampling does not improve inference times, which are roughly 300 times slower for OAA than Recall Tree on ODP.\n2While not yet implemented, Recall Tree can presumably also leverage multicore for acceleration.\n70 75 80 85 90 95\n100\n10000 100000 1e+06 1e+07 1e+08\nAv er\nag e\nCu m\nul at\niv e\nPr og\nre ss\niv e\nAc cu\nra cy\n(% )\nExamples\nin-order permuted\n(a) When the LTCB dataset is presented in the original order, Recall Tree is able to exploit sequential correlations for improved performance. After all examples are processed, the average progressive accuracy is 73.3% vs. 74.6%.\n10 15 20 25 30 35 40\n1 10 100\nTe st\nE rr\nor (%\n)\nCandidate Set Size\nwith path features without path features\n(b) Test error on ALOI for various candidate set sizes, with or without path features (all other parameters held fixed). Using multiple predictors per leaf and including path features improves performance."}, {"heading": "4.3 Online Operation", "text": "In this experiment we leverage the online nature of the algorithm to exploit nonstationarity in the data to improve results. This is not something that is easily done with batch oriented algorithms, or with algorithms that post-process a trained predictor to accelerate inference.\nWe consider two versions of LTCB. In both versions the task is to predict the next word given the previous 6 tokens. The difference is that in one version, the Wikipedia dump is processed in the original order (\u201cin-order\u201d); whereas in the other version the training data is permuted prior to input to the learning algorithm (\u201cpermuted\u201d). We assess progressive validation loss [6] on the sequence. We anticipate that an online algorithm can exploit the sequential correlations of the in-order variant for improved performance. The result in Figure 5a confirms."}, {"heading": "4.4 Impact of Design Choices", "text": "Two differences between Recall Tree and LOMTree are the use of multiple predictors at each tree node and the augmentation of the example with path features. In this experiment we explore the impact of these design choices using the ALOI dataset.\nFigure 5b shows the effect of these two aspects on statistical performance. As the candidate set size is increased, test error decreases, but with diminishing returns. Disabling path features degrades performance, and the effect is more pronounced as the candidate set size increases. This is (inhindsight) not surprising, as a larger candidate set size decreases the difficulty of obtaining good recall (i.e., a good tree) but increases the difficulty of obtaining good precision (i.e., good class predictors), and path features are only applicable to the latter."}, {"heading": "5 Conclusion", "text": "In this work we proposed the Recall Tree, a reduction of multiclass to binary classification, which operates online and scales logarithmically with the number of classes. Unlike the LOMTree [7], we share classifiers among the nodes of the tree which alleviates data sparsity at deep levels while greatly reducing the required state. We also use a tighter analysis which is more closely followed in the implementation. These features allow us to reduce the statistical gap with OAA while still operating many orders of magnitude faster for large K multiclass datasets. In the future we plan to investigate multiway splits in the tree since O(logK)-way splits will not affect our O(poly logK) running time and they might reduce contention in the root and nodes high in the tree."}, {"heading": "A Proof of theorem 2", "text": "Proof. For the fixed tree at timestep t with a fixed partition function in the nodes, the weighted entropy of class labels is Wt = \u2211 {n\u2208Leaves} fnHn.\nWeak Learning implies entropy improvement according to Lemma 6 of [12] which states: Lemma 3. For all q \u2208 [0, 1], Hn \u2212 ( fl fn Hl + fr fn Hr ) \u2265 \u03b32q(1\u2212 q)\nUsing this, the weak learning assumption implies class entropies Hl and Hr satisfy Hn \u2265 ( fl fn Hl + fr fn Hr ) + \u03b32 4\nwhere \u03b3 is the advantage of the weak learner. Hence,\nWt \u2212Wt+1 = fnHn \u2212 flHl \u2212 frHr \u2265 fn \u03b32\n4 .\nWe can bound fn according to Wt = \u2211\n{n\u2208Leaves} fnHn \u2264 \u2211 {n\u2208Leaves} fn dlog2Ke \u2264 t max {n\u2208Leaves} fn dlog2Ke\nwhich implies max{n\u2208Leaves} fn \u2265 Wt tdlog2Ke . Hence,\nWt \u2212Wt+1 \u2265 \u03b32Wt\n4t dlog2Ke which implies the recursion Wt+1 \u2264Wt ( 1\u2212 \u03b3 2\n4tdlog2Ke\n) . In particular,\nWt+1\n\u2264W1 t\u220f i=1 ( 1\u2212 \u03b3 2 4i dlog2Ke ) \u2264W1 2blog2 tc\u220f i=1 ( 1\u2212 \u03b3 2 4i dlog2Ke )\n= W1 2\u220f i=1 ( 1\u2212 \u03b3 2 4i dlog2Ke ) ... 2j\u220f i=2j\u22121+1 ( 1\u2212 \u03b3 2 4i dlog2Ke ) ... 2blog tc\u220f i=2blog tc+1 ( 1\u2212 \u03b3 2 4i dlog2Ke )\nConsidering an individual j,\n2j\u220f i=2j\u22121+1 ( 1\u2212 \u03b3 2 4i dlog2Ke ) \u2264 2j\u220f i=2j\u22121+1 ( 1\u2212 \u03b3 2 2j+2 dlog2Ke )\n= ( 1\u2212 \u03b3 2\n2j+2 dlog2Ke )2j\u22121 \u2264 e\u2212 \u03b32 8dlog2 Ke\nwhere the last inequality follows because limn\u2192\u221e ( 1 + xn )n approximates ex from below. There are blog2 tc terms so\nWt+1 \u2264W1e\u2212 \u03b32blog2 tc 8dlog2 Ke = H1e \u2212 \u03b3 2blog2 tc 8dlog2 Ke .\nwhere H1 is the marginal Shannon entropy of the class labels.\nTo finish the proof, we bound the multiclass loss in terms of the average entropy. Pick any leaf node n and assign the most likely label, y = arg maxi \u03c0ni so the error rate is = 1\u2212 \u03c0ny . The minimum entropy distribution achieving an error rate of satisfies:\nmin \u03c0 \u2211 i6=y \u03c0ni log 1 \u03c0ni\nsubject to = \u2211 i 6=y \u03c0ni and \u03c0ni \u2264 \u03c0ny . Since log is monotonically increasing the value is minimized when \u03c0ni = \u03c0ny or \u03c0ni = 0, implying\nHn \u2265 (1\u2212 \u03c0ny) log 1\n\u03c0ny = log\n1\n1\u2212 .\nDue to the log 1 nonlinearity, an adversary maximizing the error rates with a minimal class entropy average over leaves has the same error rate at each leaf. Hence, the error rate of the tree must satisfy Wt \u2265 log 11\u2212 \u2265 2 . Plugging things together we get the theorem result:\n\u2264 \u221a H1e \u2212 \u03b3 2blog2 tc 16dlog2 Ke"}, {"heading": "B Datasets", "text": "ALOI [9] is a color image collection of one-thousand small objects recorded for scientific purposes [9]. We use the same train-test split and representation as Choromanska et. al. [7].\nImagenet consists of features extracted from intermediate layers of a convolutional neural network trained on the ILVSRC2012 challenge dataset. This dataset was originally developed to study transfer learning in visual tasks [18]; more details are at http://www.di.ens.fr/willow/ research/cnn/. We utilize a predictor linear in this representation.\nLTCB is the Large Text Compression Benchmark, consisting of the first billion bytes of a particular Wikipedia dump [13]. Originally developed to study text compression, it is now commonly used as a language modeling benchmark where the task is to predict the next word in the sequence. We limit the vocabulary to 80000 words plus a single out-of-vocabulary indicator; utilize a model linear in the 6 previous unigrams, the previous bigram, and the previous trigram; and utilize a 90-10 train-test split on entire Wikipedia articles.\nODP[2] is a multiclass dataset derived from the Open Directory Project. We utilize the same traintest split and labels from [7]. Specifically there is a fixed train-test split of 2:1, the representation of a document is a bag of words, and the class label is the most specific category associated with each document."}, {"heading": "C Experimental Methodology", "text": "Default Performance Methodology Hyperparameter selection can be computationally burdensome for large data sets, which is relevant to any claims of decreased training times. Therefore we report results using the default values indicated in Table 3. For the larger data sets (Imagenet, ODP), we do a single pass over the training data; for the smaller data set (ALOI), we do multiple passes over the training data, monitoring a\n10% held-out portion of the training data to determine when to stop optimizing.\nTuned Performance Methodology For tuned performance, we use random search over hyperparameters, taking the best result over 59 probes. For the smaller data set (ALOI), we optimize validation error on a 10% held-out subset of the training data. For the larger data sets (Imagenet, ODP),\nwe optimize progressive validation loss on the initial 10% of the training data. After determining hyperparameters we retrain with the entire training set and report the resulting test error.\nWhen available we report published LOMtree results, although they utilized a different method for optimizing hyperparameters.\nTiming Measurements All timings are taken from the same 24 core xeon server machine. Furthermore, all algorithms are implemented in the Vowpal Wabbit toolkit and therefore share file formats, parser, and binary classification base learner implying differences are attributable to the different reductions. Our baseline OAA implementation is mature and highly tuned: it always exploits vectorization, and furthermore can optionally utilize multicore training and negative gradient subsampling to accelerate training. For the larger datasets these latter features were necessary to complete the experiments: estimated unaccelerated training times are given, along with wall clock times in parenthesis."}], "references": [{"title": "Label embedding trees for large multi-class tasks", "author": ["Samy Bengio", "Jason Weston", "David Grangier"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Refined experts: improving classification in large taxonomies", "author": ["Paul N Bennett", "Nam Nguyen"], "venue": "In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Error-correcting tournaments", "author": ["Alina Beygelzimer", "John Langford", "Pradeep Ravikumar"], "venue": "In Algorithmic Learning Theory, 20th International Conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Manik Varma", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Cryptographic primitives based on hard learning problems", "author": ["Avrim Blum", "Merrick Furst", "Michael Kearns", "Richard J Lipton"], "venue": "In Advances in cryptologyCRYPTO93,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Beating the hold-out: Bounds for k-fold and progressive cross-validation", "author": ["Avrim Blum", "Adam Kalai", "John Langford"], "venue": "In Proceedings of the Twelfth Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Logarithmic time online multiclass prediction", "author": ["Anna E Choromanska", "John Langford"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "An exploration of softmax alternatives belonging to the spherical loss family", "author": ["Alexandre de Br\u00e9bisson", "Pascal Vincent"], "venue": "arXiv preprint arXiv:1511.05042,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "The Amsterdam library of object images", "author": ["Jan-Mark Geusebroek", "Gertjan J Burghouts", "Arnold WM Smeulders"], "venue": "International Journal of Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "The population frequencies of species and the estimation of population", "author": ["I.J. Good"], "venue": "parameters. Biometrika,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1953}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "On the boosting ability of top-down decision tree learning algorithms", "author": ["Michael Kearns", "Yishay Mansour"], "venue": "In Proceedings of STOC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Boosting using branching programs", "author": ["Yishay Mansour", "David McAllester"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Empirical bernstein bounds and sample variance penalization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "arXiv preprint arXiv:0907.3740,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Andriy Mnih", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning", "author": ["Yashoteja Prabhu", "Manik Varma"], "venue": "In Proceedings of the 20th ACM SIGKDD,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "In defense of one-vs-all classification", "author": ["Ryan Rifkin", "Aldebaro Klautau"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["Pascal Vincent", "Alexandre de Br\u00e9bisson", "Xavier Bouthillier"], "venue": "In NIPS", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "WSABIE: scaling up to large vocabulary image annotation", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "IJCAI", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 3, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 6, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 15, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 17, "context": "Can we effectively predict one of K classes in deeply sublinear time? This question gives rise to the area of extreme multiclass classification [1, 3, 4, 7, 17, 19, 25], in whichK is very large.", "startOffset": 144, "endOffset": 168}, {"referenceID": 18, "context": "\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22].", "startOffset": 218, "endOffset": 226}, {"referenceID": 20, "context": "\u2022 High accuracy: The approach should provide accuracy competitive with OAA, a remarkably strong baseline[20] which is the standard \u201coutput layer\u201d of many learning systems such as recent winners of the imagenet contest [11, 22].", "startOffset": 218, "endOffset": 226}, {"referenceID": 6, "context": "\u2022 High speed at training time and test time: A multiclass classifier must spend at least \u03b8(logK) time [7]) so this is a natural benchmark to optimize against.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "The Recall Tree achieves good accuracy, always improving on previous online approaches [7] and sometimes surpassing the OAA baseline.", "startOffset": 87, "endOffset": 90}, {"referenceID": 6, "context": "The LOMTree[7] is the closest prior work, only missing on space requirements where up to a factor of 64 more space than OAA was used experimentally.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Other approaches such as hierarchical softmax (HSM) and the the Filter Tree [3] use a fixed tree structure [17].", "startOffset": 76, "endOffset": 79}, {"referenceID": 15, "context": "Other approaches such as hierarchical softmax (HSM) and the the Filter Tree [3] use a fixed tree structure [17].", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "In domains in which there is no prespecified tree hierarchy, using a random tree structure can lead to considerable underperformance as shown previously [1, 7].", "startOffset": 153, "endOffset": 159}, {"referenceID": 6, "context": "In domains in which there is no prespecified tree hierarchy, using a random tree structure can lead to considerable underperformance as shown previously [1, 7].", "startOffset": 153, "endOffset": 159}, {"referenceID": 14, "context": "Most other approaches in extreme classification either do not work online [16, 19] or only focus on speeding up either prediction time or training time but not both.", "startOffset": 74, "endOffset": 82}, {"referenceID": 17, "context": "Most other approaches in extreme classification either do not work online [16, 19] or only focus on speeding up either prediction time or training time but not both.", "startOffset": 74, "endOffset": 82}, {"referenceID": 14, "context": "In [16] the authors try to add tree structure learning to HSM via iteratively clustering the classes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Similar remarks apply to [1] where the authors propose to learn a tree by solving an eigenvalue problem after (OAA) training.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "Among the approaches that speed up training time we distinguish exact ones [8, 23] that have only been proposed for particular loss functions and approximate ones such as negative sampling as used e.", "startOffset": 75, "endOffset": 82}, {"referenceID": 21, "context": "Among the approaches that speed up training time we distinguish exact ones [8, 23] that have only been proposed for particular loss functions and approximate ones such as negative sampling as used e.", "startOffset": 75, "endOffset": 82}, {"referenceID": 22, "context": "in [24].", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "Though these techniques do not address inference time, separate procedures for speeding up inference (given a trained model) have been proposed [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "Choosing a classifier so as to minimize error rate is not just NP-hard, it is so difficult it has been considered for cryptographic primitives [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 11, "context": "A theory of decision tree learning as boosting [12] provides a way to understand these joint properties in a population limit (or equivalently on a training set iterated until convergence).", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "The proof in appendix A reuses techniques from [7, 12].", "startOffset": 47, "endOffset": 54}, {"referenceID": 11, "context": "The proof in appendix A reuses techniques from [7, 12].", "startOffset": 47, "endOffset": 54}, {"referenceID": 6, "context": "These results are similar to previous results [7, 12] with two advantages.", "startOffset": 46, "endOffset": 53}, {"referenceID": 11, "context": "These results are similar to previous results [7, 12] with two advantages.", "startOffset": 46, "endOffset": 53}, {"referenceID": 11, "context": "We handle multiclass rather than binary classification [12] and we bound error rates instead of entropy [7].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "We handle multiclass rather than binary classification [12] and we bound error rates instead of entropy [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "For binary classification, branching programs [14] result in exponentially more succinct representations than decision trees [12] by joining nodes into a directed acyclic graphs.", "startOffset": 46, "endOffset": 50}, {"referenceID": 11, "context": "For binary classification, branching programs [14] result in exponentially more succinct representations than decision trees [12] by joining nodes into a directed acyclic graphs.", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "Nevertheless, as data gets divided down the branches of the tree, empirical estimates for the \u201ctop F most frequent labels\u201d suffer from a substantial missing mass problem [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "Thus, instead of computing empirical recall to determine when to halt descent, we use an empirical Bernstein (lower) bound [15], which is summarized by the following proposition.", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "Dataset Task Classes Examples ALOI[9] Visual Object Recognition 1k 10 Imagenet[18] Visual Object Recognition \u2248 20k \u2248 10 LTCB[13] Language Modeling \u2248 80k \u2248 10 ODP[2] Document Classification \u2248 100k \u2248 10", "startOffset": 34, "endOffset": 37}, {"referenceID": 16, "context": "Dataset Task Classes Examples ALOI[9] Visual Object Recognition 1k 10 Imagenet[18] Visual Object Recognition \u2248 20k \u2248 10 LTCB[13] Language Modeling \u2248 80k \u2248 10 ODP[2] Document Classification \u2248 100k \u2248 10", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Dataset Task Classes Examples ALOI[9] Visual Object Recognition 1k 10 Imagenet[18] Visual Object Recognition \u2248 20k \u2248 10 LTCB[13] Language Modeling \u2248 80k \u2248 10 ODP[2] Document Classification \u2248 100k \u2248 10", "startOffset": 161, "endOffset": 164}, {"referenceID": 5, "context": "We assess progressive validation loss [6] on the sequence.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Unlike the LOMTree [7], we share classifiers among the nodes of the tree which alleviates data sparsity at deep levels while greatly reducing the required state.", "startOffset": 19, "endOffset": 22}], "year": 2016, "abstractText": "We create a new online reduction of multiclass classification to binary classification for which training and prediction time scale logarithmically with the number of classes. Compared to previous approaches, we obtain substantially better statistical performance for two reasons: First, we prove a tighter and more complete boosting theorem, and second we translate the results more directly into an algorithm. We show that several simple techniques give rise to an algorithm that can compete with one-against-all in both space and predictive power while offering exponential improvements in speed when the number of classes is large.", "creator": "LaTeX with hyperref package"}}}