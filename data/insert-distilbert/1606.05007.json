{"id": "1606.05007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep Neural Networks", "abstract": "phonemic or phonetic sub - word units are the 5 most commonly used atomic elements to represent multiple speech signals in modern asrs. although however internally they are not the optimal choice due to with several reasons such as : large amount wasted of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under - resourced dialects and languages. here, we propose a data - driven pronunciation estimation and acoustic modeling method which only primarily takes the orthographic transcription to jointly estimate a reading set out of sub - word units electronically and a reliable dictionary. experimental results show that the proposed method which offers is based on semi - professionally supervised training of a deep neural network scheme largely outperforms phoneme based continuous speech recognition on the timit dataset.", "histories": [["v1", "Wed, 15 Jun 2016 23:45:33 GMT  (223kb,D)", "http://arxiv.org/abs/1606.05007v1", "Proc. of 17th Interspeech (2016), San Francisco, California, USA"]], "COMMENTS": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["naoya takahashi", "tofigh naghibi", "beat pfister"], "accepted": false, "id": "1606.05007"}, "pdf": {"name": "1606.05007.pdf", "metadata": {"source": "CRF", "title": "Automatic Pronunciation Generation by Utilizing a Semi-supervised Deep Neural Networks", "authors": ["Naoya Takahashi", "Tofigh Naghibi", "Beat Pfister"], "emails": ["NaoyaA.Takahashi@jp.sony.com,", "pfister}@tik.ee.ethz.ch"], "sections": [{"heading": "1. Introduction", "text": "The three principal resources typically required for developing a phoneme based automatic speech recognizer (ASR) are: transcribed acoustic data for acoustic model estimation, text data for language model estimation, and a pronunciation dictionary to map words to sequences of sub-word units. Manual preparation of such resources requires significant investment and expertise. Therefore, an automatic generation of pronunciation dictionary from the data is clearly required for many dialects and languages.\nDeveloping ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3]. A main challenge to develop ASR for under-resourced domains is to produce a reliable pronunciation dictionary from limited available resources. For major languages, however, a canonical pronunciation dictionary is usually already available. However, such dictionaries may be error-prone due to the fact that they are manually generated and in most cases do not cover pronunciation variants. There were several attempts to tackle these problems [4, 5, 6, 7].\nLu et al. [8] proposed a data-driven dictionary generator to include new pronunciations based on newly coming acoustic evidence. Goel et al. in [9] use a grapheme-to-phoneme approach to guess the pronunciation and iteratively refine the acoustic model and the dictionary. However, these methods still require a high-quality initial pronunciation dictionary created by an expert.\nIn modern ASRs words are represented by smaller subword units such as phonemes and the pronunciation dictionary maps words to sequences of sub-word units. However, sub-\nword units do not essentially need to be linguistically motivated elements. In fact, given a set of acoustic samples, the linguistically defined units are most probably not the optimal ones for speech recognition [10]. For instance telephony speech, where high frequency components have been filtered out, requires a modified dictionary with slightly different set of fricatives than full-bandwidth speech.\nOver the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8]. Bacchiani and Ostendorf [12] proposed an iterative acoustic segmentation and clustering approach to build sub-word units from speech signals and subsequently construct the dictionary based on the estimated subword units. Singh et al. [8] introduced a divide-and-conquer strategy to recursively update sub-word units and dictionary. The dictionary computation was done by means of an n-best type algorithm which is known to produce sub-optimal solutions. Although their approach demonstrates some promising results, the performance is still not comparable with a phoneme based ASR.\nThe main focus of this paper is to design an ASR based on an automatically generated dictionary that outperforms commonly used phoneme based ASRs. While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10]. This approach gives us the maximum-likelihood estimates of the pronunciations. These high-quality pronunciations are one of the key factors to outperform phoneme based ASRs. Moreover, to learn proper sub-word units, we combine the strength of Gaussian mixture models (GMM) and deep neural network (DNN) based acoustic modeling. We formulate this problem as an instance of a semi-supervised self-learning process. By taking advantage of the robustness of hidden Markov models (HMM) with GMM based observation probability distribution against labeling errors, we train the first set of sub-word units and output the first set of pronunciations. We then use this dictionary to re-label the data and employ the higher expressiveness of DNNs to improve the modeling of sub-word units and the dictionary in an iterative process. In each iteration round, a new dictionary is generated and by means of this new dictionary the data is re-labeled. This data is again used to train the DNN. As shown in the experiments, the proposed results achieves more than 10% absolute improvement over the phoneme based approach on TIMIT data in a continuous speech recognition task.\nThe reminder of this paper is organized as follows. The proposed framework and its components for joint sub-word units\nar X\niv :1\n60 6.\n05 00\n7v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n16\nand dictionary learning are introduced in Section 2. In Section 3 the experimental results are demonstrated and finally, conclusions are summarized in Section 4."}, {"heading": "2. Semi-supervised joint Dictionary and Acoustic Model Learning", "text": ""}, {"heading": "2.1. Framework", "text": "In the rest of this paper, we refer to data-driven sub-word units as abstract acoustic elements (AAEs) in contrast to phones. Our goal is to jointly learn the pronunciation dictionary d\u2217 = {\u03c91, \u00b7 \u00b7 \u00b7 , \u03c9L} of L pronunciations \u03c9i and N AAE models \u03bb\u2217 = {A1, \u00b7 \u00b7 \u00b7 , AN} that maximize the joint likelihood:\n\u03bb\u2217, d\u2217 = arg max \u039b,D P (X|T,\u039b, D) (1)\nwhere X = (X1, \u00b7 \u00b7 \u00b7 , XM ) is the set of training utterances, T = (T1, \u00b7 \u00b7 \u00b7 , TM ) is the set of corresponding orthographic transcriptions, M is the number of utterances, \u039b is the universe of all possible sets of N AAEs and D is the universe of all the dictionaries which map words to AAEs sequences. It is hard to find the optimal solution for the optimization problem in (1) due to its complex non-linear nature. It is thus decomposed into two simpler optimization problems which can be solved iteratively.\ndi = arg max D P (X|T, \u03bbi, D) (2)\n\u03bbi+1 = arg max \u039b P (X|T,\u039b, di) (3)\nSince the pronunciation of each word can be estimated independently from other words, the dictionary estimation in (2) can be decomposed into L maximum likelihood estimations as follows:\n\u03c9l = arg max \u03c9 \u220f j\u2208\u2126l max Sj P (Xj , Sj |\u03bb)\nsubject to: Sj \u2208 S\u03c9 (4)\nwhere \u2126l is the set of indices of utterances of word Wl, Sj is a sequence of AAEs and S\u03c9 denotes a set of all possible AAE sequences of the pronunciation \u03c9. For instance in S\u03c9 , if the pronunciation is \u03c9 = A1A2A3, some samples in S\u03c9 may\nbeA1A1A1A2A3, A1A2A2A3A3 andA1A1A2A3A3A3. The constraint in (4) implies that all AAE sequences should be samples of the same pronunciation. For the case where \u03bb is modeled by a left-to-right HMM without skips, which is the most common topology in HMM based ASRs, a solution of (4) has been proposed in [15] (Details are in Section 2.3.). In (3), since the dictionary is fixed, the problem results in a common acoustic model estimation given the dictionary. However, the labels reassigned by using the estimated dictionary are very noisy since the dictionary is automatically estimated from data without any expert supervision. Therefore, a robust model is required at early stage of the training iteration while a more expressive and powerful model such as a DNN [16, 17] can be used after the reliable dictionary is obtained.\nThe joint dictionary and AAE learning framework is illustrated in Figure 1 and summarized as follows:\nAlgorithm 1 Semi-supervised joint AAEs and dictionary learning\n1: i = 0 // Initialize AAE models \u03bb0 (Section 2.2) 2: Clustering the acoustic space. 3: Model each cluster by GMM and set as \u03bb0.\n// Start joint AAEs and dictionary learning 4: while ( Performance is improved ) do 5: Given AAE models \u03bbi, update dictionary di by maxi-\nmizing joint likelihood multiple utterances (Section 2.3).\n6: Given dictionary di, double the number of mixtures and update AAE models \u03bbi+1 (Section 2.4). 7: i\u2190 i+ 1 8: end while 9: Replace GMM by DNN and train AAE model using labels\nobtained by HMM-GMM (Section 2.4). 10: while ( Performance is improved ) do 11: Given AAE models \u03bbi, update dictionary di by maximizing joint likelihood multiple utterances. 12: Given dictionary di, re-train DNN based AAE models \u03bbi+1 (Section 2.4). 13: i\u2190 i+ 1 14: end while"}, {"heading": "2.2. Acoustic Model Initialization", "text": "Initial AAE models can simply be obtained by clustering the acoustic space. The acoustic space can be described by any feature as long as it is informative enough to discriminate between different words. We employed the Linde-Buzo-Gray (LBG) algorithm [18] with a squared-error distortion measure to cluster the acoustic feature vectors. The LBG clustering algorithm tends to assign more codebook vectors to high-density areas which is a useful property in order to obtain discriminative initial AAEs. Each cluster is then modeled by a GMM with a single Gaussian component. These models are used as the initial models for AAEs."}, {"heading": "2.3. Dictionary Generation", "text": "The solution of (4) proposed in [15] is an extension of the standard one-dimensional Viterbi algorithm to K dimensions. The K-dimensional Viterbi algorithm calculates the most probable HMM state sequence which is common to K given utterances. While this algorithm is rigorous, its complexity grows exponentially with the number of utterances, which consequently makes it infeasible to apply it to more than a few utterances. An efficient approximation of the K-dimensional Viterbi algorithm has been proposed in [10] where the problem to find the joint alignment and the optimal common sequence for K utterances is decomposed intoK\u22121 applications of two-dimensional Viterbi algorithm. This approximation starts with finding the best alignment between two utterances. Then, while keeping the alignment between the already processed utterances fixed, the next utterance is aligned with this master utterance. The AAE sequence of the final master utterance is the approximation of the K-dimensional Viterbi pronunciation."}, {"heading": "2.4. Acoustic Modeling", "text": "Once the dictionary is updated, all utterances are decoded based on the new pronunciation of the words in the dictionary and the AAEs are re-estimated according to the new labels. The AAEs can be modeled by commonly used models such as HMM/GMM or HMM/DNN. However, at the beginning of the training iteration, the model and dictionary are not accurate enough and more probable to get stuck in a bad local optimum if the model\u2019s degree of freedom is too high. In order to avoid this situation, we start the training with a simple model, namely one Gaussian component for each AAE with a diagonal covariance matrix. In each iteration, the dictionary gets more accurate. Thus, the number of mixture components are doubled in order to increase the modeling power. Once the performance is saturated the GMM is replaced with the DNN in order to utilize more expressive modeling capability. This process makes the semi-supervised DNN training feasible and prevents it to get stuck in a bad local optimum. The HMM state-level transcription is obtained by force-aligned decoding with optimised HMM-GMM and dictionary. This transcription provides labels for DNN training. The DNN is trained to estimate HMM posterior states by minimizing the cross entropy loss L with l1 regularization using back propagation:\narg min W \u2211 i,j L(xij , y i j ,W ) + \u03c1\u2016W\u20161 (5)\nwhere xij \u2208 Xi is the jth feature vector of the ith utterance, yij is the corresponding label andW is the set of network parameters, respectively. \u03c1 is a constant parameter which is set to 10\u22126 in this work."}, {"heading": "3. Experiments", "text": "We conducted several sets of experiments on the TIMIT corpus [19]. The TIMIT corpus provides a manually prepared dictionary and phone-level transcriptions with 61 phones. As a baseline, 61 phone models were trained using the TIMIT dictionary and the provided transcriptions. We used 12 mel frequency cepstral coefficients (MFCCs) and energy with their deltas and delta-deltas as descriptors of the acoustic space. The speech data was analyzed using a 25 ms Hamming window with a 10 ms frame shift. We evaluated phone based DNN-HMM, GMMHMM and AAE based GMM-HMM model as baselines. The DNN architecture was comprised of 7 hidden layers. The first hidden layer had 2048 nodes, next 5 layers had 1024 nodes and the number of nodes at the last layer was equal to the number of HMM states to be predicted. All hidden layers were equipped with the Rectified Linear Unit (ReLU) non-linearity [20]. The input to the network was 11 contiguous frames of MFCCs. The networks were trained using mini-batch gradient descent based on back propagation with momentum. We applied dropout [16] to all hidden layers with dropout probability 0.5. The batch size was set to 128. HMMs had left-to-right, no-skipping topology with three states for each phoneme as opposed to one state for each AAE. HMMs were trained using a modified version of HTK [21] and DNNs were implemented using Lasagne [22]."}, {"heading": "3.1. Isolated Word Recognition", "text": "The first set of experiments were on the isolated word recognition to test the performance of the proposed methods and investigate the effects of hyper parameters such as the number of mixture components and the number of AAEs. For joint pronunciation estimation and acoustic models training, we collected a pronunciation training set comprising of words with more than 10 utterances from the TIMIT training set. The total number of utterances in the pronunciation training set was 12800. After excluding words with less than 4 characters (e.g., a and the), 339 distinct words were collected from the TIMIT test set for the isolated word speech recognition task, resulting in 3900 utterances in total. The baseline GMM based phone models were trained with 32 mixture components. During the GMM based AAE model training the number of mixtures was doubled for each iteration until it reached 128 mixtures as described in Section 2.4."}, {"heading": "3.1.1. Comparison with phonetic approach", "text": "The word error rates (WER) of each method are shown in Table 1. The results show that the proposed data-driven method clearly outperforms the baseline methods. The proposed AAEDNN method achieved 10.3% and 2.4% improvement over GMM and DNN based phonetic acoustic models, respectively. This suggests that a more accurate dictionary and better acoustic models can be obtained directly from training data without any human expertise. Moreover, AAE-DNN method improves the performance by 3.2% over the AAE-GMM method. This indicates that the DNN was successfully trained in the semisupervised manner and the final model could effectively use the its expressive modeling power."}, {"heading": "3.1.2. Number of AAEs", "text": "Our second experiment focused on the effects of the number of AAEs, i.e. N . We trained the dictionary and AAE models with N = 64, 128, 192, 256, 320, 384, 448. The word error rates of DNN and GMM based AAE models are illustrated in Figure 2.\nThe number of mixtures of the GMMs were determined experimentally as shown in Table 2. For DNN based AAE models, the best result are obtained with 384 AAEs in contrast to with 320 AAEs for the GMM based models. Interestingly, the optimal number of AAE states is far higher than the number of states of the phone models (61 phonemes \u00d7 3 states = 183 states). This is an indication that the proposed data-driven approach to jointly generate the sub-word units and dictionary models the acoustic space more precisely than the linguistically motivated phonetic units and the manually designed dictionary. It is also worthwhile to mention that the optimal number of DNN based AAE models was higher than that of GMM based models. This is perhaps due to the fact that the DNN was trained discriminatively, allowing to efficiently model the interaction between higher number of AAEs."}, {"heading": "3.2. Continuous Speech Recognition", "text": "Unlike phoneme based ASRs, the proposed AAE based approach does not depend on linguistic knowledge. It is therefore interesting to compare these approaches on a real-world continues speech recognition (CSR) task. For this purpose, we used the SX records of the TIMIT corpus which contains 450 sentences spoken by 7 speakers, i.e. 3150 utterances in total. We prepared the test set by randomly selecting and putting aside one speaker for each sentence from the SX recordings and used the remaining samples as the training set (450 sentences \u00d7 6 speaker = 2700 utterances). We also included the SA and SI recordings of the TIMIT corpus in the training set. The number of AAEs was 384. The number of mixture components in the GMM based phone models was 64. The performance was evaluated in two scenarios: with and without language model. The language model employed in the baseline and the proposed methods is a simple bigram model.\nTable 3 shows that the proposed AAE-DNN based approach significantly outperforms baseline methods in both scenarios. The performance improvements over the phone based HMMDNN method in with and without the language model scenar-\nios were 10.68% and 5.11%, respectively. The results suggest that the proposed data-driven dictionary and the AAE models are also useful for CSR and a more accurate representation of speech signals can be learned automatically. We observed that all 384 AAEs were actually used in the trained dictionary, and the dictionary tend to assign 39% more HMM states on average to each word as compare with the TIMIT phonetic dictionary. This means that in AAEs, the stay-in-state probability is smaller resulting in more frequent state transitions. This suggests that by using AAEs, the acoustic space was modeled at a higher resolution. This consequently increased the precision of the word pronunciations."}, {"heading": "4. Conclusions", "text": "In this work we proposed a novel joint dictionary and sub-word unit learning framework for ASRs. The proposed method does not require linguistic expertise, and can automatically create the set of sub-word units and the corresponding pronunciation dictionary. In our method, reliable pronunciations are estimated from multiple utterances by an efficient approximation of Kdimensional Viterbi algorithm which estimates the most probable HMM state sequence common to multiple utterances of a word. Experimental results show that the proposed method significantly outperforms the phone based methods which even get manually prepared dictionary and hand crafted transcriptions as inputs. We further investigated the effects of the number of data-driven sub-word units and showed that the optimal number of sub-word units is much higher than the total number of HMM states of the 61 phones. The future works will be directed towards applying the proposed method to speech recognition for under-resourced languages and large vocabulary continuous speech recognition tasks."}, {"heading": "5. References", "text": "[1] A. Das and M. Hasegawa-Johnson, \u201cCross-lingual transfer learn-\ning during supervised training in low resource scenarios,\u201d in Proc. Interspeech, 2015, pp. 1\u20135.\n[2] Y. Qian, D. Povey, and J. Liu, \u201cState-level data borrowing for lowresource speech recognition based on subspace GMMs,\u201d in Proc. Interspeech, 2011, pp. 553\u2013556.\n[3] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, \u201cAutomatic speech recognition for under-resourced languages : A survey,\u201d Speech Communication, vol. 56, pp. 85\u2013100, 2014.\n[4] M. Sarac\u0327lar, H. Nock, and S. Khudanpur, \u201cPronunciation modeling by sharing Gaussian densities across phonetic models,\u201d Computer Speech & Language, vol. 14, no. 2, pp. 137\u2013160, 2000.\n[5] M. Wester, \u201cPronunciation modeling for ASR - Knowledge-based and data-derived methods,\u201d Computer Speech and Language, vol. 17, no. 1, pp. 69\u201385, 2003.\n[6] T. Hain, \u201cImplicit modelling of pronunciation variation in automatic speech recognition,\u201d Speech Communication, vol. 46, no. 2, pp. 171\u2013188, 2005.\n[7] I. Mcgraw, I. Badr, and J. R. Glass, \u201cLearning lexicons from speech using a pronunciation mixture model,\u201d IEEE Transactions on Audio, Speech and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2013.\n[8] R. Singh, B. Raj, and R. M. Stern, \u201cAutomatic generation of subword units for speech recognition systems,\u201d IEEE Transactions on Speech and Audio Processing, vol. 10, no. 2, pp. 89\u201399, 2002.\n[9] A. Ghoshal, D. Povey, M. Agarwal, P. Akyazi, N. Goel, M. Karafi, A. Rastrow, R. C. Rose, P. Schwarz, S. Thomas, and I. Allahabad, \u201cApproaches to automatic lexicon learning with limited trainging examples,\u201d in Proc. ICASSP, 2010, pp. 5094\u20135097.\n[10] T. Naghibi, S. Hoffmann, and B. Pfister, \u201cAn efficient method to estimate pronunciation from multiple utterances,\u201d in Proc.Interspeech, no. August, 2013, pp. 1951\u20131955.\n[11] T. Holter and T. Svendsen, \u201cCombined optimisation of baseforms and model parameters in speech recognition based on acoustic subword units,\u201d in IEEE Workshop Automatic Speech Recognition, 1997, pp. 199\u2013206.\n[12] M. Bacchiani and M. Ostendorf, \u201cJoint lexicon, acoustic unit inventory and model design,\u201d Speech Communication, vol. 29, no. 2, pp. 99\u2013114, 1999.\n[13] T. Svendsen, \u201cPronunciation modeling for speech technology,\u201d in International Conference on Signal Processing and Communications (SPCOM), 2004, pp. 11\u201316.\n[14] H. Mokbel and D. Jouvet, \u201cDerivation of the optimal set of phonetic transcriptions for a word from its acoustic realizations,\u201d in Speech Communication, vol. 29, no. 1, 1999, pp. 49\u201364.\n[15] M. Gerber, T. Kaufmann, and B. Pfister, \u201cExtended Viterbi algorithm for optimized word HMMs,\u201d in ICASSP, 2011, pp. 4932\u2013 4935.\n[16] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, \u201cDeep neural networks for acoustic modeling in speech recognition,\u201d Signal Processing Magazine, 2012.\n[17] O. Abdel-Hamid, A. Mohamed, H. Jiang, and G. Penn, \u201cApplying convolutional neural networks concepts to hybrid NN-HMM model for Speech Recognition,\u201d in ICASSP, 2012, pp. 4277\u20134280.\n[18] Y. Linde, A. Buzo, and R. M. Gray, \u201cAn algorithm for vector quantizer design,\u201d IEEE Transactions on Communications, vol. 28, no. 1, pp. 84\u201395, 1980.\n[19] W. Fisher, G. Doddington, and K. Goudie-Marshall, \u201cThe DARPA speech recognition research database: Specifications and status,\u201d in Proc. DARPA Workshop on Speech Recognition, 1986, pp. 93\u2013 99.\n[20] G. E. Dahl, T. N. Sainath, and G. E. Hinton, \u201cImproving deep neural networks for LVCSR using rectified linear units and dropout,\u201d in Proc. ICASSP, 2013, pp. 8609\u20138613.\n[21] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland, \u201cThe HTK Book (for HTK Version 3.4.1),\u201d http: //htk.eng.cam.ac.uk, 2009, University of Cambridge, UK.\n[22] E. Battenberg, S. Dieleman, D. Nouri, E. Olson, C. Raffel, J. Schlu\u0308ter, S. K. S\u00f8nderby, D. Maturana, M. Thoma et al., \u201cLasagne: First release.\u201d http://dx.doi.org/10.5281/zenodo.27878, Aug. 2015."}], "references": [{"title": "Cross-lingual transfer learning during supervised training in low resource scenarios", "author": ["A. Das", "M. Hasegawa-Johnson"], "venue": "Proc. Interspeech, 2015, pp. 1\u20135.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "State-level data borrowing for lowresource speech recognition based on subspace GMMs", "author": ["Y. Qian", "D. Povey", "J. Liu"], "venue": "Proc. Interspeech, 2011, pp. 553\u2013556.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic speech recognition for under-resourced languages : A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Communication, vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Pronunciation modeling by sharing Gaussian densities across phonetic models", "author": ["M. Sara\u00e7lar", "H. Nock", "S. Khudanpur"], "venue": "Computer Speech & Language, vol. 14, no. 2, pp. 137\u2013160, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Pronunciation modeling for ASR - Knowledge-based and data-derived methods", "author": ["M. Wester"], "venue": "Computer Speech and Language, vol. 17, no. 1, pp. 69\u201385, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Implicit modelling of pronunciation variation in automatic speech recognition", "author": ["T. Hain"], "venue": "Speech Communication, vol. 46, no. 2, pp. 171\u2013188, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning lexicons from speech using a pronunciation mixture model", "author": ["I. Mcgraw", "I. Badr", "J.R. Glass"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 21, no. 2, pp. 357\u2013366, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic generation of subword units for speech recognition systems", "author": ["R. Singh", "B. Raj", "R.M. Stern"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 10, no. 2, pp. 89\u201399, 2002.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Approaches to automatic lexicon learning with limited trainging examples", "author": ["A. Ghoshal", "D. Povey", "M. Agarwal", "P. Akyazi", "N. Goel", "M. Karafi", "A. Rastrow", "R.C. Rose", "P. Schwarz", "S. Thomas", "I. Allahabad"], "venue": "Proc. ICASSP, 2010, pp. 5094\u20135097.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "An efficient method to estimate pronunciation from multiple utterances", "author": ["T. Naghibi", "S. Hoffmann", "B. Pfister"], "venue": "Proc.Interspeech, no. August, 2013, pp. 1951\u20131955.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Combined optimisation of baseforms and model parameters in speech recognition based on acoustic subword units", "author": ["T. Holter", "T. Svendsen"], "venue": "IEEE Workshop Automatic Speech Recognition, 1997, pp. 199\u2013206.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Joint lexicon, acoustic unit inventory and model design", "author": ["M. Bacchiani", "M. Ostendorf"], "venue": "Speech Communication, vol. 29, no. 2, pp. 99\u2013114, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Pronunciation modeling for speech technology", "author": ["T. Svendsen"], "venue": "International Conference on Signal Processing and Communications (SPCOM), 2004, pp. 11\u201316.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Derivation of the optimal set of phonetic transcriptions for a word from its acoustic realizations", "author": ["H. Mokbel", "D. Jouvet"], "venue": "Speech Communication, vol. 29, no. 1, 1999, pp. 49\u201364.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Extended Viterbi algorithm for optimized word HMMs", "author": ["M. Gerber", "T. Kaufmann", "B. Pfister"], "venue": "ICASSP, 2011, pp. 4932\u2013 4935.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for Speech Recognition", "author": ["O. Abdel-Hamid", "A. Mohamed", "H. Jiang", "G. Penn"], "venue": "ICASSP, 2012, pp. 4277\u20134280.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "An algorithm for vector quantizer design", "author": ["Y. Linde", "A. Buzo", "R.M. Gray"], "venue": "IEEE Transactions on Communications, vol. 28, no. 1, pp. 84\u201395, 1980.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1980}, {"title": "The DARPA speech recognition research database: Specifications and status", "author": ["W. Fisher", "G. Doddington", "K. Goudie-Marshall"], "venue": "Proc. DARPA Workshop on Speech Recognition, 1986, pp. 93\u2013 99.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1986}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Proc. ICASSP, 2013, pp. 8609\u20138613.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "The HTK Book (for HTK Version 3.4.1)", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey", "V. Valtchev", "P. Woodland"], "venue": "http: //htk.eng.cam.ac.uk, 2009, University of Cambridge, UK.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Developing ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 1, "context": "Developing ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 2, "context": "Developing ASRs for dialects and under-resourced languages has attracted growing attention over the past few years [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 4, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 5, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 6, "context": "There were several attempts to tackle these problems [4, 5, 6, 7].", "startOffset": 53, "endOffset": 65}, {"referenceID": 7, "context": "[8] proposed a data-driven dictionary generator to include new pronunciations based on newly coming acoustic evidence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "in [9] use a grapheme-to-phoneme approach to guess the pronunciation and iteratively refine the acoustic model and the dictionary.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "In fact, given a set of acoustic samples, the linguistically defined units are most probably not the optimal ones for speech recognition [10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "Over the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8].", "startOffset": 207, "endOffset": 218}, {"referenceID": 11, "context": "Over the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8].", "startOffset": 207, "endOffset": 218}, {"referenceID": 7, "context": "Over the past few years, there have been several attempts to move beyond phoneme based sub-word units by jointly learn a set of sub-word units and their corresponding dictionary directly from the given data [11, 12, 8].", "startOffset": 207, "endOffset": 218}, {"referenceID": 11, "context": "Bacchiani and Ostendorf [12] proposed an iterative acoustic segmentation and clustering approach to build sub-word units from speech signals and subsequently construct the dictionary based on the estimated subword units.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "[8] introduced a divide-and-conquer strategy to recursively update sub-word units and dictionary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 127, "endOffset": 138}, {"referenceID": 12, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 127, "endOffset": 138}, {"referenceID": 13, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 127, "endOffset": 138}, {"referenceID": 14, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 252, "endOffset": 260}, {"referenceID": 9, "context": "While most of the solutions proposed to find a pronunciation based on multiple utterances of a word are n-best type heuristics [8, 13, 14], in this paper, we employ an approximation of the K-dimensional Viterbi algorithm proposed in our previous works [15, 10].", "startOffset": 252, "endOffset": 260}, {"referenceID": 14, "context": "For the case where \u03bb is modeled by a left-to-right HMM without skips, which is the most common topology in HMM based ASRs, a solution of (4) has been proposed in [15] (Details are in Section 2.", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "Therefore, a robust model is required at early stage of the training iteration while a more expressive and powerful model such as a DNN [16, 17] can be used after the reliable dictionary is obtained.", "startOffset": 136, "endOffset": 144}, {"referenceID": 16, "context": "Therefore, a robust model is required at early stage of the training iteration while a more expressive and powerful model such as a DNN [16, 17] can be used after the reliable dictionary is obtained.", "startOffset": 136, "endOffset": 144}, {"referenceID": 17, "context": "We employed the Linde-Buzo-Gray (LBG) algorithm [18] with a squared-error distortion measure to cluster the acoustic feature vectors.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The solution of (4) proposed in [15] is an extension of the standard one-dimensional Viterbi algorithm to K dimensions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "An efficient approximation of the K-dimensional Viterbi algorithm has been proposed in [10] where the problem to find the joint alignment and the optimal common sequence for K utterances is decomposed intoK\u22121 applications of two-dimensional Viterbi algorithm.", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "We conducted several sets of experiments on the TIMIT corpus [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "All hidden layers were equipped with the Rectified Linear Unit (ReLU) non-linearity [20].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "We applied dropout [16] to all hidden layers with dropout probability 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": "HMMs were trained using a modified version of HTK [21] and DNNs were implemented using Lasagne [22].", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "Phonemic or phonetic sub-word units are the most commonly used atomic elements to represent speech signals in modern ASRs. However they are not the optimal choice due to several reasons such as: large amount of effort required to handcraft a pronunciation dictionary, pronunciation variations, human mistakes and under-resourced dialects and languages. Here, we propose a data-driven pronunciation estimation and acoustic modeling method which only takes the orthographic transcription to jointly estimate a set of sub-word units and a reliable dictionary. Experimental results show that the proposed method which is based on semi-supervised training of a deep neural network largely outperforms phoneme based continuous speech recognition on the TIMIT dataset.", "creator": "LaTeX with hyperref package"}}}