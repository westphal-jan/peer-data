{"id": "1606.05679", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Two Discourse Driven Language Models for Semantics", "abstract": "natural narration language understanding often significantly requires deep semantic knowledge. expanding on previous proposals, we suggest that some uniquely important aspects of understanding semantic knowledge can be modeled as a language model if done partly at an appropriate level of abstraction. we develop two distinct models that capture semantic frame item chains and discourse information while abstracting over the specific mentions of predicates and entities. for each model, we investigate thus four implementations : a \" standard \" n - gram interactive language model and describes three inherently discriminatively trained \" neural \" language mental models that generate embeddings for semantic frames. the quality of the semantic learning language models ( semlm ) is specifically evaluated both very intrinsically, using perplexity and analyzing a narrative cloze test and extrinsically - we show that our semlm helps improve performance on semantic natural narrative language processing tasks such as co - reference resolution and discourse parsing.", "histories": [["v1", "Fri, 17 Jun 2016 21:19:35 GMT  (30kb)", "https://arxiv.org/abs/1606.05679v1", "To appear in ACL 16"], ["v2", "Mon, 27 Jun 2016 06:20:52 GMT  (30kb)", "http://arxiv.org/abs/1606.05679v2", "To appear in ACL 16"]], "COMMENTS": "To appear in ACL 16", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haoruo peng", "dan roth"], "accepted": true, "id": "1606.05679"}, "pdf": {"name": "1606.05679.pdf", "metadata": {"source": "CRF", "title": "Two Discourse Driven Language Models for Semantics", "authors": ["Haoruo Peng"], "emails": ["hpeng7@illinois.edu", "danr@illinois.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n05 67\n9v 2\n[ cs\n.C L\n] 2\n7 Ju\nn 20\n16 To appear in ACL\u201916"}, {"heading": "1 Introduction", "text": "Natural language understanding often necessitates deep semantic knowledge. This knowledge needs to be captured at multiple levels, from words to phrases, to sentences, to larger units of discourse. At each level, capturing meaning frequently requires context sensitive abstraction and disambiguation, as shown in the following example (Winograd, 1972):\nEx.1 [Kevin] was robbed by [Robert]. [He] was arrested by the police. Ex.2 [Kevin] was robbed by [Robert]. [He] was rescued by the police.\nIn both cases, one needs to resolve the pronoun \u201che\u201d to either \u201cRobert\u201d or \u201cKevin\u201d. To make\nthe correct decisions, one needs to know that the subject of \u201crob\u201d is more likely than the object of \u201crob\u201d to be the object of \u201carrest\u201d while the object of \u201crob\u201d is more likely to be the object of \u201crescue\u201d. Thus, beyond understanding individual predicates (e.g., at the semantic role labeling level), there is a need to place them and their arguments in a global context.\nHowever, just modeling semantic frames is not sufficient; consider a variation of Ex.1:\nEx.3 Kevin was robbed by Robert, but the police mistakenly arrested him.\nIn this case, \u201chim\u201d should refer to \u201cKevin\u201d as the discourse marker \u201cbut\u201d reverses the meaning, illustrating that it is necessary to take discourse markers into account when modeling semantics.\nIn this paper we propose that these aspects of semantic knowledge can be modeled as a Semantic Language Model (SemLM). Just like the \u201cstandard\u201d syntactic language models (LM), we define a basic vocabulary, a finite representation language, and a prediction task, which allows us to model the distribution over the occurrence of elements in the vocabulary as a function of their (well-defined) context. In difference from syntactic LMs, we represent natural language at a higher level of semantic abstraction, thus facilitating modeling deep semantic knowledge.\nWe propose two distinct discourse driven language models to capture semantics. In our first semantic language model, the Frame-Chain SemLM, we model all semantic frames and discourse markers in the text. Each document is viewed as a single chain of semantic frames and discourse markers. Moreover, while the vocabulary of discourse markers is rather small, the number of different surface form semantic frames that could appear in the text is very large. To achieve a better level of abstraction, we disambiguate semantic frames and map them to their PropBank/FrameNet represen-\ntation. Thus, in Ex.3, the resulting frame chain is \u201crob.01 \u2014 but \u2014 arrest.01\u201d (\u201c01\u201d indicates the predicate sense).\nOur second semantic language model is called Entity-Centered SemLM. Here, we model a sequence of semantic frames and discourse markers involved in a specific co-reference chain. For each co-reference chain in a document, we first extract semantic frames corresponding to each co-referent mention, disambiguate them as before, and then determine the discourse markers between these frames. Thus, each unique frame contains both the disambiguated predicate and the argument label of the mention. In Ex.3, the resulting sequence is \u201crob.01#obj \u2014 but \u2014 arrest.01#obj\u201d (here \u201cobj\u201d indicates the argument label for \u201cKevin\u201d and \u201chim\u201d respectively). While these two models capture somewhat different semantic knowledge, we argue later in the paper that both models can be induced at high quality, and that they are suitable for different NLP tasks.\nFor both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007). Each model defines its own prediction task. In total, we produce eight different SemLMs. Except for Ngram model, others yield embeddings for semantic frames as they are neural language models.\nIn our empirical study, we evaluate both the quality of all SemLMs and their application to coreference resolution and shallow discourse parsing tasks. Following the traditional evaluation standard of language models, we first use perplexity as our metric. We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.e. randomly removing a token from a sequence and test the system\u2019s ability to recover it. We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al., 2006) ). By comparing the results on these test sets, we show that we do not incur noticeable degradation when building SemLMs using preprocessing tools. Moreover, we show that SemLMs improves the performance of co-reference resolu-\ntion, as well as that of predicting the sense of discourse connectives for both explicit and implicit ones.\nThe main contributions of our work can be summarized as follows: 1) The design of two novel discourse driven Semantic Language models, building on text abstraction and neural embeddings; 2) The implementation of high quality SemLMs that are shown to improve state-of-theart NLP systems."}, {"heading": "2 Related Work", "text": "Our work is related to script learning. Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts. Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016). Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence.\nIn our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009). However, we differ from them in the following aspects: 1) script learning does not generate a probabilistic model on semantic frames1; 2) script learning models semantic frame sequences incompletely as they do not consider discourse information; 3) works in script learning rarely show applications to real NLP tasks.\nSome prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b). However, since they use explicit script schemas either as features or constraints, these works suffer from data sparsity problems. In our work, the\n1Some works may utilize a certain probabilistic framework, but they mainly focus on generating high-quality frames by filtering.\nSemLM abstract vocabulary ensures a good coverage of frame semantics."}, {"heading": "3 Two Models for SemLM", "text": "In this section, we describe how we capture sequential semantic information consisted of semantic frames and discourse markers as semantic units (i.e. the vocabulary)."}, {"heading": "3.1 Semantic Frames and Discourse Markers", "text": "Semantic Frames A semantic frame is composed of a predicate and its corresponding argument participants. Here we require the predicate to be disambiguated to a specific sense, and we need a certain level of abstraction of arguments so that we can assign abstract labels. The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) perfectly fits our needs. They both have a limited set of frames (in the scale of thousands) and each frame can be uniquely represented by its predicate sense. These frames provide a good level of generalization as each frame can be instantiated into various surface forms in natural texts. We use these frames as part of our vocabulary for SemLMs. Formally, we use the notation f to represent a frame. Also, we denote fa , f#Arg when referring to an argument role label (Arg) inside a frame (f). Discourse Markers We use discourse markers (connectives) to model discourse relationships between frames. There is only a limited number of unique discourse markers, such as and, but, however, etc. We get the full list from the Penn Discourse Treebank (Prasad et al., 2008) and include them as part of our vocabulary for SemLMs. Formally, we use dis to denote the discourse marker. Note that discourse relationships can exist without an explicit discourse marker, which is also a challenge for discourse parsing. Since we cannot\nreliably identify implicit discourse relationships, we only consider explicit ones here. More importantly, discourse markers are associated with arguments (Wellner and Pustejovsky, 2007) in text (usually two sentences/clauses, sometimes one). We only add a discourse marker in the semantic sequence when its corresponding arguments contain semantic frames which belong to the same semantic sequence. We call them frame-related discourse markers. Details on generating semantic frames and discourse markers to form semantic sequences are discussed in Sec. 5."}, {"heading": "3.2 Frame-Chain SemLM", "text": "For frame-chain SemLM, we model all semantic frames and discourse markers in a document. We form the semantic sequence by first including all semantic frames in the order they appear in the text: [f1, f2, f3, . . .]. Then we add framerelated discourse markers into the sequence by placing them in their order of appearance. Thus we get a sequence like [f1, dis1, f2, f3, dis2, . . .]. Note that discourse markers do not necessarily exist between all semantic frames. Additionally, we treat the period symbol as a special discourse marker, denoted by \u201co\u201d. As some sentences contain more than one semantic frame (situations like clauses), we get the final semantic sequence like this:\n[f1, dis1, f2, o, f3, o, dis2, . . . , o]"}, {"heading": "3.3 Entity-Centered SemLM", "text": "We generate semantic sequences according to co-reference chains for entity-centered SemLM. From co-reference resolution, we can get a sequence like [m1,m2,m3, . . .], where mentions appear in the order they occur in the text. Each mention can be matched to an argument inside a semantic frame. Thus, we replace each mention with its argument label inside a semantic frame, and get [fa1, fa2, fa3, . . .]. We then add discourse markers exactly in they way we do for frame-chain SemLM, and get the following sequence:\n[fa1, dis1, fa2, fa3, dis2, . . .]\nThe comparison of vocabularies between frame-chain and entity-centered SemLMs is summarized in Table 1."}, {"heading": "4 Implementations of SemLM", "text": "In this work, we experiment with four language model implementations: N-gram (NG), Skip-\nGram (SG), Continuous Bag-of-Words (CBOW) and Log-bilinear (LB) language model. For ease of explanation, we assume that a semantic unit sequence is s = [w1, w2, w3, . . . , wk]."}, {"heading": "4.1 N-gram Model", "text": "For an n-gram model, we predict each token based on its n\u22121 previous tokens, i.e. we directly model the following conditional probability (in practice, we choose n = 3, Tri-gram (TRI) ):\np(wt+2|wt, wt+1).\nThen, the probability of the sequence is\np(s) = p(w1)p(w2|w1) k\u22122\u220f\nt=1\np(wt+2|wt, wt+1).\nTo compute p(w2|w1) and p(w1), we need to back off from Tri-gram to Bi-gram and Uni-gram."}, {"heading": "4.2 Skip-Gram Model", "text": "The SG model was proposed in Mikolov et al. (2013b). It uses a token to predict its context, i.e. we model the following conditional probability:\np(c \u2208 c(wt)|wt, \u03b8).\nHere, c(wt) is the context for wt and \u03b8 denotes the learned parameters which include neural network states and embeddings. Then the probability of the sequence is computed as\nk\u220f\nt=1\n\u220f\nc\u2208c(wt)\np(c|wt, \u03b8)."}, {"heading": "4.3 Continuous Bag-of-Words Model", "text": "In contrast to skip-gram, CBOW (Mikolov et al., 2013a) uses context to predict each token, i.e. we model the following conditional probability:\np(wt|c(wt), \u03b8).\nIn this case, the probability of the sequence is\nk\u220f\nt=1\np(wt|c(wt), \u03b8)."}, {"heading": "4.4 Log-bilinear Model", "text": "LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each token. However, LB associates a token with three components instead of just one vector: a target vector v(w), a context vector v\u2019(w) and a bias b(w). So, the conditional probability becomes:\np(wt|c(wt)) = exp(v(wt) \u22bau(c(wt)) + b(wt))\u2211 w\u2208V exp(v(w)\u22bau(c(wt)) + b(w)) .\nHere, V denotes the vocabulary and we define u(c(wt)) = \u2211 ci\u2208c(wt)\nqi \u2299 v\u2032(ci). Note that \u2299 represents element-wise multiplication and qi is a vector that depends only on the position of a token in the context, which is a also a model parameter.\nSo, the overall sequence probability is\nk\u220f\nt=1\np(wt|c(wt))."}, {"heading": "5 Building SemLMs from Scratch", "text": "In this section, we explain how we build SemLMs from un-annotated plain text."}, {"heading": "5.1 Dataset and Preprocessing", "text": "Dataset We use the New York Times Corpus2 (from year 1987 to 2007) for training. It contains a bit more than 1.8M documents in total. Preprocessing We pre-process all documents with semantic role labeling (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998). We also implement the explicit discourse connective identification module in shallow discourse parsing (Song et al., 2015). Additionally, we utilize within document entity coreference (Peng et al., 2015a) to produce coreference chains. To obtain all annotations, we employ the Illinois NLP tools3."}, {"heading": "5.2 Semantic Unit Generation", "text": "FrameNet Mapping We first directly derive semantic frames from semantic role labeling annotations. As the Illinois SRL package is built upon PropBank frames, we do a mapping to FrameNet frames via VerbNet senses (Schuler, 2005), thus achieving a higher level of abstraction. The mapping file4 defines deterministic mappings. However, the mapping is not complete and there are\n2https://catalog.ldc.upenn.edu/LDC2008T19 3http://cogcomp.cs.illinois.edu/page/software/ 4http://verbs.colorado.edu/verb-index/fn/vn-fn.xml\nremaining PropBank frames. Thus, the generated vocabulary for SemLMs contains both PropBank and FrameNet frames. For example, \u201cplace\u201d and \u201cput\u201d with the VerbNet sense id \u201c9.1-2\u201d are converted to the same FrameNet frame \u201cPlacing\u201d.\nAugmenting to Verb Phrases We apply three heuristic modifications to augment semantic frames defined in Sec. 3.1: 1) if a preposition immediately follows a predicate, we append the preposition to the predicate e.g. \u201ctake over\u201d; 2) if we encounter the semantic role label AM-PRD which indicates a secondary predicate, we also append this secondary predicate to the main predicate e.g. \u201cbe happy\u201d; 3) if we see the semantic role label AM-NEG which indicates negation, we append \u201cnot\u201d to the predicate e.g. \u201cnot like\u201d. These three augmentations can co-exist and they allow us to model more fine-grained semantic frames.\nVerb Compounds We have observed that if two predicates appear very close to each other, e.g. \u201ceat and drink\u201d, \u201cdecide to buy\u201d, they actually represent a unified semantic meaning. Thus, we construct compound verbs to connect them together. We apply the rule that if the gap between two predicates is less than two tokens, we treat them as a unified semantic frame defined by the conjunction of the two (augmented) semantic frames, e.g. \u201ceat.01-drink.01\u201d and \u201cdecide.01-buy.01\u201d.\nArgument Labels for Co-referent Mentions To get the argument role label information for coreferent mentions, we need to match each mention to its corresponding semantic role labeling argument. If a mention head is inside an argument, we regard it as a match. We do not consider singleton mentions.\nVocabulary Construction After generating all semantic units for (augmented and compounded) semantic frames and discourse markers, we merge them together as a tentative vocabulary. In order to generate a sensible SemLM, we filter out rare tokens which appear less than 20 times in the data. We add the Unknown token (UNK) and End-ofSequence token (EOS) to the eventual vocabulary.\nStatistics on the eventual SemLM vocabularies and semantic sequences are shown in Table 2. We also compare frame-chain and entity-centered SemLMs to the usual syntactic language model setting. The statistics in Table 2 shows that they are comparable both in vocabulary size and in the total number of tokens for training. Moreover, entity-centered SemLMs have shorter sequences\nthen frame-chain SemLMs. We also provide several examples of high-frequency augmented compound semantic frames in our generated SemLM vocabularies. All are very intuitive:\nwant.01-know.01, agree.01-pay.01, try.01-get.01, decline.02-comment.01, wait.01-see.01, make.02-feel.01, want.01(not)-give.08(up)"}, {"heading": "5.3 Language Model Training", "text": "NG We implement the N-gram model using the SRILM toolkit (Stolcke, 2002). We also employ the well-known KneserNey Smoothing (Kneser and Ney, 1995) technique. SG & CBOW We utilize the word2vec package to implement both SG and CBOW. In practice, we set the context window size to be 10 for SG while set the number as 5 for CBOW (both are usual settings for syntactic language models). We generate 300- dimension embeddings for both models. LB We use the OxLM toolkit (Paul et al., 2014) with Noise-Constrastive Estimation (Gutmann and Hyvarinen, 2010) for the LB model. We set the context window size to 5 and produce 150- dimension embeddings."}, {"heading": "6 Evaluation", "text": "In this section, we first evaluate the quality of SemLMs through perplexity and a narrative cloze test. More importantly, we show that the proposed SemLMs can help improve the performance of coreference resolution and shallow discourse parsing. This further proves that we successfully capture semantic sequence information which can potentially benefit a wide range of semantic related NLP tasks.\nWe have designed two models for SemLM: frame-chain (FC) and entity-centered (EC). By training on both types of sequences respectively, we implement four different language models: TRI, SG, CBOW, LB. We focus the evaluation efforts on these eight SemLMs."}, {"heading": "6.1 Quality Evaluation of SemLMs", "text": "Datasets We use three datasets. We first randomly sample 10% of the New York Times Corpus documents (roughly two years of data), denoted the NYT Hold-out Data. All our SemLMs are trained on the remaining NYT data and tested on this hold-out data. We generate semantic sequences for the training and test data using the methodology described in Sec. 5.\nWe use PropBank data with gold frame annotations as another test set. In this case, we only generate frame-chain SemLM sequences by applying semantic unit generation techniques on gold frames, as described in Sec 5.2. When we test on Gold PropBank Data with Frame Chains, we use frame-chain SemLMs trained from all NYT data.\nSimilarly, we use Ontonotes data (Hovy et al., 2006) with gold frame and co-reference annotations as the third test set, Gold Ontonotes Data with Coref Chains. We only generate entitycentered SemLMs by applying semantic unit generation techniques on gold frames and gold coreference chains, as described in Sec 5.2. Baselines We use Uni-gram (UNI) and Bi-gram (BG) as two language model baselines. In addition, we use the point-wise mutual information (PMI) for token prediction. Essentially, PMI scores each pair of tokens according to their cooccurrences. It predicts a token in the sequence by choosing the one with the highest total PMI with all other tokens in the sequence. We use the ordered PMI (OP) as our baseline, which is a variation of PMI by considering asymmetric counting (Jans et al., 2012)."}, {"heading": "6.1.1 Perplexity", "text": "As SemLMs are language models, it is natural to evaluate the perplexity, which is a measurement of how well a language model can predict sequences.\nResults for SemLM perplexities are presented in Table 3. They are computed without considering end token (EOS). We apply tri-gram KneserNey Smoothing to CBOW, SG and LB. LB consistently shows the lowest perplexities for both frame-chain and entity-centered SemLMs across\nall test sets. Similar to syntactic language models, perplexities are fast decreasing from UNI, BI to TRI. Also, CBOW and SG have very close perplexity results which indicate that their language modeling abilities are at the same level.\nWe can compare the results of our frame-chain SemLM on NYT Hold-out Data and Gold PropBank Data with Frame Chains, and our entitycentered SemLM on NYT Hold-out Data and Gold Ontonotes Data with Coref Chains. While we see differences in the results, the gap is narrow and the relative ranking of different SemLMs does not change. This indicates that the automatic SRL and Co-reference annotations added some noise but, more importantly, that the resulting SemLMs are robust to this noise as we still retain the language modeling ability for all methods.\nAdditionally, our ablation study removes the \u201cFrameNet Mapping\u201d step in Sec. 5.2 (\u201cFC-FM\u201d and \u201cEC-FM\u201d rows), resulting in only using PropBank frames in the vocabulary. The increase in perplexities shows that \u201cFrameNet Mapping\u201d does produce a higher level of abstraction, which is useful for language modeling."}, {"heading": "6.1.2 Narrative Cloze Test", "text": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009). As Rudinger et al. (2015) points out, the narrative cloze test can be regarded as a language modeling evaluation. In the narrative cloze test, we randomly choose and remove one token from each semantic sequence in the test set. We then use language models to predict the missing token and evaluate the correct-\nness. For all SemLMs, we use the conditional probabilities defined in Sec. 4 to get token predictions. We also use ordered PMI as an additional baseline. The narrative cloze test is conducted on the same test sets as the perplexity evaluation. We use mean reciprocal rank (MRR) and recall at 30 (Recall@30) to evaluate.\nResults are provided in Table 4. Consistent with the results in the perplexity evaluation, LB outperforms other methods for both frame-chain and entity-centered SemLMs across all test sets. It is interesting to see that UNI performs better than BG in this prediction task. This finding is also reflected in the results reported in Rudinger et al. (2015). Though CBOW and SG have similar perplexity results, SG appears to be stronger in the narrative cloze test. With respect to the strongest baseline (UNI), LB achieves close to 20% relative improvement for Recall@30 metric on NYT hold-out data. On gold data, the frame-chain\nSemLMs get a relative improvement of 36.5% for Recall@30 while entity-centered SemLMs get 22.3%. For MRR metric, the relative improvement is around half that of the Recall@30 metric.\nIn the narrative cloze test, we also carry out an ablation study to remove the \u201cFrameNet Mapping\u201d step in Sec. 5.2 (\u201cFC-FM\u201d and \u201cEC-FM\u201d rows). The decrease in MRR and Recall@30 metrics further strengthens the argument that \u201cFrameNet Mapping\u201d is important for language modeling as it improves the generalization on frames.\nWe cannot directly compare with other related works (Rudinger et al., 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics. Rudinger et al. (2015) also use the NYT portion of the Gigaword corpus, but with Concrete annotations; Pichotta and Mooney (2016) use the English Wikipedia as their data, and Stanford NLP tools for pre-processing while we use the Illinois NLP tools. Consequently, the even-\ntual chain statistics are different, which leads to different test instances.5 We counter this difficulty by reporting results on \u201cGold PropBank Data\u201d and \u201cGold Ontonotes Data\u201d. We hope that these two gold annotation datasets can become standard test sets. Rudinger et al. (2015) does share a common evaluation metric with us: MRR. If we ignore the data difference and make a rough comparison, we find that the absolute values of our results are better while Rudinger et al. (2015) have higher relative improvement (\u201cRel-Impr\u201d in Table 4). This means that 1) the discourse information is very likely to help better model semantics 2) the discourse information may boost the baseline (UNI) more than it does for the LB model."}, {"heading": "6.2 Evaluation of SemLM Applications", "text": ""}, {"heading": "6.2.1 Co-reference Resolution", "text": "Co-reference resolution is the task of identifying mentions that refer to the same entity. To help improve its performance, we incorporate SemLM information as features into an existing co-reference resolution system. We choose the state-of-art Illinois Co-reference Resolution system (Peng et al., 2015a) as our base system. It employs a supervised joint mention detection and co-reference\n5Rudinger et al. (2015) is similar to our entity-centered SemLM without discourse information. So, in Table 4, we make a rough comparison between them.\nframework. We add additional features into the mention-pair feature set.\nGiven a pair of mentions (m1,m2) where m1 appears before m2, we first extract the corresponding semantic frame and the argument role label of each mention. We do this by following the procedures in Sec. 5. Thus, we can get a pair of semantic frames with argument information (fa1, fa2). We may also get an additional discourse marker between these two frames, e.g. (fa1, dis, fa2). Now, we add the following conditional probability as the feature from SemLMs:\npc = p(fa2|fa1, dis).\nWe also add p2c , \u221a pc and 1/pc as features. To get the value of pc, we follow the definitions in Sec. 4, and we only use the entity-centered SemLM here as its vocabulary covers frames with argument labels. For the neural language model implementations (CBOW, SG and LB), we also include frame embeddings as additional features.\nWe evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al., 2012). We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008). For CoNLL12 data, we follow the train and test document split from CoNLL-2012 Shared Task. We report CoNLL AVG for results (average of MUC, B3, and CEAFe metrics), using the v7.0 scorer provided by the CoNLL-2012 Shared Task.\nCo-reference resolution results with entitycentered SemLM features are shown in Table 5. Tri-grams with conditional probability features improve the performance by a small margin, while the log-bilinear model achieves a 0.4-0.5 F1 points improvement. By employing log-bilinear model embeddings, we further improve the numbers and we outperform the best reported results on the CoNLL12 dataset (Wiseman et al., 2015).\nIn addition, we carry out ablation studies to remove all discourse makers during the language modeling process. We re-train our models and study their effects on the generated features. Table 5 (\u201cw/o DIS\u201d rows) shows that without discourse information, the SemLM features would hurt the overall performance, thus proving the necessity of considering discourse for semantic language models."}, {"heading": "6.2.2 Shallow Discourse Parsing", "text": "Shallow discourse parsing is the task of identifying explicit and implicit discourse connectives, determine their senses and their discourse arguments. In order to show that SemLM can help improve shallow discourse parsing, we evaluate on identifying the correct sense of discourse connectives (both explicit and implicit ones).\nWe choose Song et al. (2015), which uses a supervised pipeline approach, as our base system. The system extracts context features for potential discourse connectives and applies the discourse connective sense classifier. Consider an explicit connective \u201cdis\u201d; we extract the semantic frames that are closest to it (left and right), resulting in the sequence [f1, dis, f2] by following the procedures described in Sec. 5. We then add the following conditional probabilities as features. Compute\nqc = p(dis|f1, f2).\nand, similar to what we do for co-reference resolution, we add qc, q2c , \u221a qc, 1/qc as conditional probability features, which can be computed following the definitions in Sec. 4. We also include frame embeddings as additional features. We only use frame-chain SemLMs here.\nWe evaluate on CoNLL16 (Xue et al., 2015) test and blind sets, following the train and development document split from the Shared Task, and report F1 using the official shared task scorer.\nTable 6 shows the results for shallow discourse parsing with SemLM features. Tri-gram with conditional probability features improve the performance for both explicit and implicit connective sense classifiers. Log-bilinear model with conditional probability features achieves even better results, and frame embeddings further improve the\nnumbers. SemLMs improve relatively more on explicit connectives than on implicit ones.\nWe also show an ablation study in the same setting as we did for co-reference, i.e. removing discourse information (\u201cw/o DIS\u201d rows). While our LB model can still exhibit improvement over the base system, its performance is lower than the proposed discourse driven version, which means that discourse information improves the expressiveness of semantic language models."}, {"heading": "7 Conclusion", "text": "The paper builds two types of discourse driven semantic language models with four different language model implementations that make use of neural embeddings for semantic frames. We use perplexity and a narrative cloze test to prove that the proposed SemLMs have a good level of abstraction and are of high quality, and then apply them successfully to the two challenging tasks of co-reference resolution and shallow discourse parsing, exhibiting improvements over state-ofthe-art systems. In future work, we plan to apply SemLMs to other semantic related NLP tasks e.g. machine translation and question answering."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Christos Christodoulopoulos and Eric Horn for comments that helped to improve this work. This work is supported by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. This material is also based\nupon work supported by the U.S. Department of Homeland Security under Award Number 2009- ST-061-CCI002-07."}], "references": [{"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe."], "venue": "COLING/ACL, pages 86\u201390.", "citeRegEx": "Baker et al\\.,? 1998", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Generating coherent event schemas at scale", "author": ["N. Balasubramanian", "S. Soderland", "Mausam", "O. Etzioni."], "venue": "EMNLP, pages 1721\u20131731.", "citeRegEx": "Balasubramanian et al\\.,? 2013", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2013}, {"title": "Unsupervised discovery of biographical structure from text", "author": ["D. Bamman", "N.A. Smith."], "venue": "TACL, 2:363\u2013376.", "citeRegEx": "Bamman and Smith.,? 2014", "shortCiteRegEx": "Bamman and Smith.", "year": 2014}, {"title": "Unsupervised discovery of event scenarios from texts", "author": ["C.A. Bejan."], "venue": "FLAIRS Conference, pages 124\u2013129.", "citeRegEx": "Bejan.,? 2008", "shortCiteRegEx": "Bejan.", "year": 2008}, {"title": "Understanding the value of features for coreference resolution", "author": ["E. Bengtson", "D. Roth."], "venue": "EMNLP.", "citeRegEx": "Bengtson and Roth.,? 2008", "shortCiteRegEx": "Bengtson and Roth.", "year": 2008}, {"title": "Jointly combining implicit constraints improves temporal ordering", "author": ["N. Chambers", "D. Jurafsky."], "venue": "EMNLP.", "citeRegEx": "Chambers and Jurafsky.,? 2008a", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative event chains", "author": ["N. Chambers", "D. Jurafsky."], "venue": "ACL, volume 94305, pages 789\u2013797.", "citeRegEx": "Chambers and Jurafsky.,? 2008b", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["N. Chambers", "D. Jurafsky."], "venue": "ACL, volume 2, pages 602\u2013610.", "citeRegEx": "Chambers and Jurafsky.,? 2009", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2009}, {"title": "Event schema induction with a probabilistic entity-driven model", "author": ["N. Chambers."], "venue": "EMNLP, volume 13, pages 1797\u20131807.", "citeRegEx": "Chambers.,? 2013", "shortCiteRegEx": "Chambers.", "year": 2013}, {"title": "Probabilistic frame induction", "author": ["J.C.K. Cheung", "H. Poon", "L. Vanderwende."], "venue": "arXiv:1302.4813.", "citeRegEx": "Cheung et al\\.,? 2013", "shortCiteRegEx": "Cheung et al\\.", "year": 2013}, {"title": "First-order probabilistic models for coreference resolution", "author": ["A. Culotta", "M. Wick", "R. Hall", "A. McCallum."], "venue": "NAACL.", "citeRegEx": "Culotta et al\\.,? 2007", "shortCiteRegEx": "Culotta et al\\.", "year": 2007}, {"title": "A unified bayesian model of scripts, frames and language", "author": ["Francis Ferraro", "Benjamin Van Durme."], "venue": "AAAI.", "citeRegEx": "Ferraro and Durme.,? 2016", "shortCiteRegEx": "Ferraro and Durme.", "year": 2016}, {"title": "A hierarchical bayesian model for unsupervised induction of script knowledge", "author": ["L. Frermann", "I. Titov", "Pinkal. M."], "venue": "EACL.", "citeRegEx": "Frermann et al\\.,? 2014", "shortCiteRegEx": "Frermann et al\\.", "year": 2014}, {"title": "What happens next? event prediction using a compositional neural network model", "author": ["M. Granroth-Wilding", "S. Clark", "M.T. Llano", "R. Hepworth", "S. Colton", "J. Gow", "J. Charnley", "N. Lavra\u010d", "M. \u017dnidar\u0161i\u010d"], "venue": "Perovs\u030cek", "citeRegEx": "Granroth.Wilding et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Granroth.Wilding et al\\.", "year": 2015}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen."], "venue": "AISTATS.", "citeRegEx": "Gutmann and Hyvarinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyvarinen.", "year": 2010}, {"title": "Ontonotes: The 90% solution", "author": ["E. Hovy", "M. Marcus", "M. Palmer", "L. Ramshaw", "R. Weischedel."], "venue": "Proceedings of HLT/NAACL.", "citeRegEx": "Hovy et al\\.,? 2006", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Narrative schema as world knowledge for coreference resolution", "author": ["J. Irwin", "M. Komachi", "Y. Matsumoto."], "venue": "CoNLL Shared Task, pages 86\u201392.", "citeRegEx": "Irwin et al\\.,? 2011", "shortCiteRegEx": "Irwin et al\\.", "year": 2011}, {"title": "Skip n-grams and ranking functions for predicting script events", "author": ["B. Jans", "S. Bethard", "I. Vuli\u0107", "M.F. Moens."], "venue": "EACL, pages 336\u2013344.", "citeRegEx": "Jans et al\\.,? 2012", "shortCiteRegEx": "Jans et al\\.", "year": 2012}, {"title": "From Treebank to PropBank", "author": ["P. Kingsbury", "M. Palmer."], "venue": "Proceedings of LREC-2002.", "citeRegEx": "Kingsbury and Palmer.,? 2002", "shortCiteRegEx": "Kingsbury and Palmer.", "year": 2002}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney."], "venue": "ICASSP.", "citeRegEx": "Kneser and Ney.,? 1995", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "NAACL.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton."], "venue": "ICML, pages 641\u2013648.", "citeRegEx": "Mnih and Hinton.,? 2007", "shortCiteRegEx": "Mnih and Hinton.", "year": 2007}, {"title": "Inducing neural models of script knowledge", "author": ["A. Modi", "I. Titov."], "venue": "CoNLL.", "citeRegEx": "Modi and Titov.,? 2014a", "shortCiteRegEx": "Modi and Titov.", "year": 2014}, {"title": "Learning semantic script knowledge with event embeddings", "author": ["A. Modi", "I. Titov."], "venue": "ICLR Workshop.", "citeRegEx": "Modi and Titov.,? 2014b", "shortCiteRegEx": "Modi and Titov.", "year": 2014}, {"title": "Learning schemata for natural language processing", "author": ["R. Mooney", "G. DeJong"], "venue": null, "citeRegEx": "Mooney and DeJong.,? \\Q1985\\E", "shortCiteRegEx": "Mooney and DeJong.", "year": 1985}, {"title": "Generative event schema induction with entity disambiguation", "author": ["K.-H. Nguyen", "X. Tannier", "O. Ferret", "R. Besan\u00e7on."], "venue": "ACL.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "The ace evaluation plan", "author": ["US NIST."], "venue": "US National Institute for Standards and Technology (NIST).", "citeRegEx": "NIST.,? 2004", "shortCiteRegEx": "NIST.", "year": 2004}, {"title": "Oxlm: A neural language modelling framework for machine translation", "author": ["B. Paul", "B. Phil", "H. Hieu."], "venue": "The Prague Bulletin of Mathematical Linguistics, 102(1):81\u201392.", "citeRegEx": "Paul et al\\.,? 2014", "shortCiteRegEx": "Paul et al\\.", "year": 2014}, {"title": "A joint framework for coreference resolution and mention head detection", "author": ["H. Peng", "K. Chang", "D. Roth."], "venue": "CoNLL.", "citeRegEx": "Peng et al\\.,? 2015a", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Solving hard coreference problems", "author": ["H. Peng", "D. Khashabi", "D. Roth."], "venue": "NAACL.", "citeRegEx": "Peng et al\\.,? 2015b", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Statistical script learning with multi-argument events", "author": ["K. Pichotta", "R.J. Mooney."], "venue": "EACL, volume 14, pages 220\u2013229.", "citeRegEx": "Pichotta and Mooney.,? 2014", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2014}, {"title": "Learning statistical scripts with lstm recurrent neural networks", "author": ["K. Pichotta", "R.J. Mooney."], "venue": "AAAI.", "citeRegEx": "Pichotta and Mooney.,? 2016", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2016}, {"title": "CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes", "author": ["S. Pradhan", "A. Moschitti", "N. Xue", "O. Uryupina", "Y. Zhang."], "venue": "CoNLL.", "citeRegEx": "Pradhan et al\\.,? 2012", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "The penn discourse treebank 2.0", "author": ["Rashmi Prasad", "Nikhil Dinesh", "Alan Lee", "Eleni Miltsakaki", "Livio Robaldo", "Aravind Joshi", "Bonnie Webber"], "venue": "In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "Prasad et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2008}, {"title": "Semantic role labeling via integer linear programming inference", "author": ["V. Punyakanok", "D. Roth", "W. Yih", "D. Zimak."], "venue": "COLING.", "citeRegEx": "Punyakanok et al\\.,? 2004", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2004}, {"title": "Coreference resolution with world knowledge", "author": ["A. Rahman", "V. Ng."], "venue": "ACL.", "citeRegEx": "Rahman and Ng.,? 2011", "shortCiteRegEx": "Rahman and Ng.", "year": 2011}, {"title": "Part of speech tagging using a network of linear separators", "author": ["D. Roth", "D. Zelenko."], "venue": "COLINGACL.", "citeRegEx": "Roth and Zelenko.,? 1998", "shortCiteRegEx": "Roth and Zelenko.", "year": 1998}, {"title": "Script induction as language modeling", "author": ["R. Rudinger", "P. Rastogi", "F. Ferraro", "B. Van Durme."], "venue": "EMNLP.", "citeRegEx": "Rudinger et al\\.,? 2015", "shortCiteRegEx": "Rudinger et al\\.", "year": 2015}, {"title": "Scripts, plans, goals, and understanding: An inquiry into human knowledge structures", "author": ["R.C. Schank", "R.P. Abelson."], "venue": "JMZ.", "citeRegEx": "Schank and Abelson.,? 1977", "shortCiteRegEx": "Schank and Abelson.", "year": 1977}, {"title": "Verbnet: A broad-coverage, comprehensive verb lexicon", "author": ["K.K. Schuler"], "venue": null, "citeRegEx": "Schuler.,? \\Q2005\\E", "shortCiteRegEx": "Schuler.", "year": 2005}, {"title": "Improving a pipeline architecture for shallow discourse parsing", "author": ["Y. Song", "H. Peng", "P. Kordjamshidi", "M. Sammons", "D. Roth."], "venue": "CoNLL Shared Task.", "citeRegEx": "Song et al\\.,? 2015", "shortCiteRegEx": "Song et al\\.", "year": 2015}, {"title": "Srilm-an extensible language modeling toolkit", "author": ["A. Stolcke."], "venue": "INTERSPEECH, volume 2002, page 2002.", "citeRegEx": "Stolcke.,? 2002", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Unsupervised induction of semantic roles within a reconstruction-error minimization framework", "author": ["I. Titov", "E. Khoddam."], "venue": "NAACL.", "citeRegEx": "Titov and Khoddam.,? 2015", "shortCiteRegEx": "Titov and Khoddam.", "year": 2015}, {"title": "Automatically identifying the arguments of discourse connectives", "author": ["Ben Wellner", "James Pustejovsky."], "venue": "Proceedings of the 2007 Joint Conference of EMNLP-CoNLL.", "citeRegEx": "Wellner and Pustejovsky.,? 2007", "shortCiteRegEx": "Wellner and Pustejovsky.", "year": 2007}, {"title": "Understanding natural language", "author": ["T. Winograd."], "venue": "Cognitive psychology, 3(1):1\u2013191.", "citeRegEx": "Winograd.,? 1972", "shortCiteRegEx": "Winograd.", "year": 1972}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["S. Wiseman", "A.M. Rush", "S.M. Shieber", "J. Weston."], "venue": "ACL.", "citeRegEx": "Wiseman et al\\.,? 2015", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "The conll-2015 shared task on shallow discourse parsing", "author": ["N. Xue", "H.T. Ng", "S. Pradhan", "R.P.C. Bryant", "A.T. Rutherford."], "venue": "CoNLL.", "citeRegEx": "Xue et al\\.,? 2015", "shortCiteRegEx": "Xue et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 45, "context": "At each level, capturing meaning frequently requires context sensitive abstraction and disambiguation, as shown in the following example (Winograd, 1972):", "startOffset": 137, "endOffset": 153}, {"referenceID": 21, "context": "For both models of SemLM, we study four language model implementations: N-gram, skipgram (Mikolov et al., 2013b), continuous bagof-words (Mikolov et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 20, "context": ", 2013b), continuous bagof-words (Mikolov et al., 2013a) and log-bilinear language model (Mnih and Hinton, 2007).", "startOffset": 33, "endOffset": 56}, {"referenceID": 22, "context": ", 2013a) and log-bilinear language model (Mnih and Hinton, 2007).", "startOffset": 41, "endOffset": 64}, {"referenceID": 6, "context": "We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.", "startOffset": 46, "endOffset": 128}, {"referenceID": 7, "context": "We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.", "startOffset": 46, "endOffset": 128}, {"referenceID": 38, "context": "We also follow the script learning literature (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009; Rudinger et al., 2015) and evaluate on the narrative cloze test, i.", "startOffset": 46, "endOffset": 128}, {"referenceID": 18, "context": "We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al.", "startOffset": 160, "endOffset": 188}, {"referenceID": 15, "context": "We conduct both evaluations on two test sets: a hold-out dataset from the New York Times Corpus and gold sequence data (for frame-chain SemLMs, we use PropBank (Kingsbury and Palmer, 2002); for entitycentered SemLMs, we use Ontonotes (Hovy et al., 2006) ).", "startOffset": 234, "endOffset": 253}, {"referenceID": 39, "context": "Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts.", "startOffset": 12, "endOffset": 63}, {"referenceID": 25, "context": "Early works (Schank and Abelson, 1977; Mooney and DeJong, 1985) tried to construct knowledge bases from documents to learn scripts.", "startOffset": 12, "endOffset": 63}, {"referenceID": 5, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 3, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 17, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 31, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 13, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 32, "context": "Recent work focused on utilizing statistical models to extract high-quality scripts from large amounts of data (Chambers and Jurafsky, 2008a; Bejan, 2008; Jans et al., 2012; Pichotta and Mooney, 2014; Granroth-Wilding et al., 2015; Pichotta and Mooney, 2016).", "startOffset": 111, "endOffset": 258}, {"referenceID": 8, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 9, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 9, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 1, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 2, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 26, "context": "Other works aimed at learning a collection of structured events (Chambers, 2013; Cheung et al., 2013; Cheung et al., 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al.", "startOffset": 64, "endOffset": 197}, {"referenceID": 24, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 23, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 12, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 43, "context": ", 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015).", "startOffset": 59, "endOffset": 153}, {"referenceID": 1, "context": ", 2013; Balasubramanian et al., 2013; Bamman and Smith, 2014; Nguyen et al., 2015), and several works have employed neural embeddings (Modi and Titov, 2014b; Modi and Titov, 2014a; Frermann et al., 2014; Titov and Khoddam, 2015). Ferraro and Van Durme (2016) presented a unified probabilistic model of syntactic and semantic frames while also demonstrating improved coherence.", "startOffset": 8, "endOffset": 259}, {"referenceID": 7, "context": "In our work, the semantic sequences in the entity-centered SemLMs are similar to narrative schemas (Chambers and Jurafsky, 2009).", "startOffset": 99, "endOffset": 128}, {"referenceID": 16, "context": "Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b).", "startOffset": 75, "endOffset": 136}, {"referenceID": 36, "context": "Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b).", "startOffset": 75, "endOffset": 136}, {"referenceID": 30, "context": "Some prior works have used scripts-related ideas to help improve NLP tasks (Irwin et al., 2011; Rahman and Ng, 2011; Peng et al., 2015b).", "startOffset": 75, "endOffset": 136}, {"referenceID": 18, "context": "The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 0, "context": "The design of PropBank frames (Kingsbury and Palmer, 2002) and FrameNet frames (Baker et al., 1998) perfectly fits our needs.", "startOffset": 79, "endOffset": 99}, {"referenceID": 34, "context": "We get the full list from the Penn Discourse Treebank (Prasad et al., 2008) and include them as part of our vocabulary for SemLMs.", "startOffset": 54, "endOffset": 75}, {"referenceID": 44, "context": "More importantly, discourse markers are associated with arguments (Wellner and Pustejovsky, 2007) in text (usually two sentences/clauses, sometimes one).", "startOffset": 66, "endOffset": 97}, {"referenceID": 20, "context": "The SG model was proposed in Mikolov et al. (2013b). It uses a token to predict its context, i.", "startOffset": 29, "endOffset": 52}, {"referenceID": 20, "context": "In contrast to skip-gram, CBOW (Mikolov et al., 2013a) uses context to predict each token, i.", "startOffset": 31, "endOffset": 54}, {"referenceID": 22, "context": "LB was introduced in Mnih and Hinton (2007). Similar to CBOW, it also uses context to predict each token.", "startOffset": 21, "endOffset": 44}, {"referenceID": 35, "context": "Preprocessing We pre-process all documents with semantic role labeling (Punyakanok et al., 2004) and part-of-speech tagger (Roth and Zelenko, 1998).", "startOffset": 71, "endOffset": 96}, {"referenceID": 37, "context": ", 2004) and part-of-speech tagger (Roth and Zelenko, 1998).", "startOffset": 34, "endOffset": 58}, {"referenceID": 41, "context": "We also implement the explicit discourse connective identification module in shallow discourse parsing (Song et al., 2015).", "startOffset": 103, "endOffset": 122}, {"referenceID": 29, "context": "Additionally, we utilize within document entity coreference (Peng et al., 2015a) to produce coreference chains.", "startOffset": 60, "endOffset": 80}, {"referenceID": 40, "context": "As the Illinois SRL package is built upon PropBank frames, we do a mapping to FrameNet frames via VerbNet senses (Schuler, 2005), thus achieving a higher level of abstraction.", "startOffset": 113, "endOffset": 128}, {"referenceID": 42, "context": "NG We implement the N-gram model using the SRILM toolkit (Stolcke, 2002).", "startOffset": 57, "endOffset": 72}, {"referenceID": 19, "context": "We also employ the well-known KneserNey Smoothing (Kneser and Ney, 1995) technique.", "startOffset": 50, "endOffset": 72}, {"referenceID": 28, "context": "LB We use the OxLM toolkit (Paul et al., 2014) with Noise-Constrastive Estimation (Gutmann and Hyvarinen, 2010) for the LB model.", "startOffset": 27, "endOffset": 46}, {"referenceID": 14, "context": ", 2014) with Noise-Constrastive Estimation (Gutmann and Hyvarinen, 2010) for the LB model.", "startOffset": 43, "endOffset": 72}, {"referenceID": 15, "context": "Similarly, we use Ontonotes data (Hovy et al., 2006) with gold frame and co-reference annotations as the third test set, Gold Ontonotes Data with Coref Chains.", "startOffset": 33, "endOffset": 52}, {"referenceID": 17, "context": "We use the ordered PMI (OP) as our baseline, which is a variation of PMI by considering asymmetric counting (Jans et al., 2012).", "startOffset": 108, "endOffset": 127}, {"referenceID": 6, "context": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009).", "startOffset": 64, "endOffset": 123}, {"referenceID": 7, "context": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009).", "startOffset": 64, "endOffset": 123}, {"referenceID": 5, "context": "We follow the Narrative Cloze Test idea used in script learning (Chambers and Jurafsky, 2008b; Chambers and Jurafsky, 2009). As Rudinger et al. (2015) points out, the narrative cloze test can be regarded as a language modeling evaluation.", "startOffset": 65, "endOffset": 151}, {"referenceID": 38, "context": "8% Rudinger et al. (2015) 0.", "startOffset": 3, "endOffset": 26}, {"referenceID": 38, "context": "This finding is also reflected in the results reported in Rudinger et al. (2015). Though CBOW and SG have similar perplexity results, SG appears to be stronger in the narrative cloze test.", "startOffset": 58, "endOffset": 81}, {"referenceID": 38, "context": "We cannot directly compare with other related works (Rudinger et al., 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics.", "startOffset": 52, "endOffset": 102}, {"referenceID": 32, "context": "We cannot directly compare with other related works (Rudinger et al., 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics.", "startOffset": 52, "endOffset": 102}, {"referenceID": 31, "context": ", 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics. Rudinger et al. (2015) also use the NYT portion of the Gigaword corpus, but with Concrete annotations; Pichotta and Mooney (2016) use the English Wikipedia as their data, and Stanford NLP tools for pre-processing while we use the Illinois NLP tools.", "startOffset": 8, "endOffset": 117}, {"referenceID": 31, "context": ", 2015; Pichotta and Mooney, 2016) because of the differences in data and evaluation metrics. Rudinger et al. (2015) also use the NYT portion of the Gigaword corpus, but with Concrete annotations; Pichotta and Mooney (2016) use the English Wikipedia as their data, and Stanford NLP tools for pre-processing while we use the Illinois NLP tools.", "startOffset": 8, "endOffset": 224}, {"referenceID": 46, "context": "We outperform the state-of-art system (Wiseman et al., 2015), which reports the best results on CoNLL12 dataset.", "startOffset": 38, "endOffset": 60}, {"referenceID": 29, "context": "39 Base (Peng et al., 2015a) 71.", "startOffset": 8, "endOffset": 28}, {"referenceID": 38, "context": "Rudinger et al. (2015) does share a common evaluation metric with us: MRR.", "startOffset": 0, "endOffset": 23}, {"referenceID": 38, "context": "Rudinger et al. (2015) does share a common evaluation metric with us: MRR. If we ignore the data difference and make a rough comparison, we find that the absolute values of our results are better while Rudinger et al. (2015) have higher relative improvement (\u201cRel-Impr\u201d in Table 4).", "startOffset": 0, "endOffset": 225}, {"referenceID": 29, "context": "We choose the state-of-art Illinois Co-reference Resolution system (Peng et al., 2015a) as our base system.", "startOffset": 67, "endOffset": 87}, {"referenceID": 27, "context": "We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al.", "startOffset": 97, "endOffset": 109}, {"referenceID": 33, "context": "We evaluate the effect of the added SemLM features on two co-reference benchmark datasets: ACE04 (NIST, 2004) and CoNLL12 (Pradhan et al., 2012).", "startOffset": 122, "endOffset": 144}, {"referenceID": 10, "context": "We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008).", "startOffset": 120, "endOffset": 167}, {"referenceID": 4, "context": "We use the standard split of 268 training documents, 68 development documents, and 106 testing documents for ACE04 data (Culotta et al., 2007; Bengtson and Roth, 2008).", "startOffset": 120, "endOffset": 167}, {"referenceID": 46, "context": "By employing log-bilinear model embeddings, we further improve the numbers and we outperform the best reported results on the CoNLL12 dataset (Wiseman et al., 2015).", "startOffset": 142, "endOffset": 164}, {"referenceID": 41, "context": "Base (Song et al., 2015) 89.", "startOffset": 5, "endOffset": 24}, {"referenceID": 41, "context": "We choose Song et al. (2015), which uses a supervised pipeline approach, as our base system.", "startOffset": 10, "endOffset": 29}, {"referenceID": 47, "context": "We evaluate on CoNLL16 (Xue et al., 2015) test and blind sets, following the train and development document split from the Shared Task, and report F1 using the official shared task scorer.", "startOffset": 23, "endOffset": 41}], "year": 2016, "abstractText": "Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a \u201cstandard\u201d N-gram language model and three discriminatively trained \u201cneural\u201d language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically \u2013 we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}