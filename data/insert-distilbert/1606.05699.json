{"id": "1606.05699", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Socially-Informed Timeline Generation for Complex Events", "abstract": "existing timeline generation systems for complex events consider only information from traditional media, ignoring the sufficiently rich social context provided by user - generated content that reveals representative public interests or convey insightful opinions. we instead aim to generate socially - informed timelines models that contain both online news article summaries and selected user comments. we present an optimization framework designed to balance topical cohesion between the journal article and comment summaries along with their informativeness and coverage of the event. automatic evaluations on real - world datasets that cover four complex events show that our system produces more informative event timelines than state - of - the - art systems. in human relationships evaluation, the associated comment summaries are furthermore potentially rated more insightful than editor's picks judgments and comments ranked highly appreciated by users.", "histories": [["v1", "Fri, 17 Jun 2016 22:52:09 GMT  (256kb,D)", "http://arxiv.org/abs/1606.05699v1", "NAACL 2015"]], "COMMENTS": "NAACL 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "claire cardie", "galen marchetti"], "accepted": true, "id": "1606.05699"}, "pdf": {"name": "1606.05699.pdf", "metadata": {"source": "CRF", "title": "Socially-Informed Timeline Generation for Complex Events", "authors": ["Lu Wang", "Claire Cardie", "Galen Marchetti"], "emails": ["cardie}@cs.cornell.edu", "gjm97@cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Social media sites on the Internet provide increasingly more, and increasingly popular, means for people to voice their opinions on trending events. Traditional news media \u2014 the New York Times and CNN, for example \u2014 now provide online mechanisms that allow and encourage readers to share reactions, opinions, and personal experiences relevant to a news story. For complex emerging events, in particular, user comments can provide relevant, interesting and insightful information beyond the facts reported in the news. But their large volume and tremendous variation in quality make it impossible\nfor readers to efficiently digest the user-generated content, much less integrate it with reported facts from the dozens or hundreds of news reports produced on the event each day.\nIn this work, we present a socially-informed timeline generation system that jointly generates a news article summary and a user comment summary for each day of an ongoing complex event. A sample (gold standard) timeline snippet for Ukraine Crisis is shown in Figure 1. The event timeline is on the left; the comment summary for March 17th is on the right.\nar X\niv :1\n60 6.\n05 69\n9v 1\n[ cs\n.C L\n] 1\n7 Ju\nn 20\nWhile generating timelines from news articles and summarizing user comments have been studied as separate problems (Yan et al., 2011; Ma et al., 2012), their joint summarization for timeline generation raises new challenges. Firstly, there should be a tight connection between the article and comment portion of the timeline. By definition, users comment on socially relevant events. So the important part of articles and insightful comments should both cover these events. Moreover, good reading experience requires that the article summary and comment summary demonstrate evident connectivity. For example, Comment C in Figure 1 (\u201cSanctions are effective and if done in unison with the EU\u201d) is obscure without knowing the context that \u201csanctions are imposed by U.S\u201d. Simply combining the outputs from a timeline generation system and a comment summarization system may lead to timelines that lack cohesion. On the other hand, articles and comments are from intrinsically different genres of text: articles emphasize facts and are written in a professional style; comments reflect opinions in a less formal way. Thus, it could be difficult to recognize the connections between articles and comments. Finally, it is also challenging to enforce continuity in timelines with many entities and events.\nTo address the challenges mentioned above, we formulate the timeline generation task as an optimization problem, where we maximize topic cohesion between the article and comment summaries while preserving their ability to reflect important concepts and subevents, adequate coverage of mentioned topics, and continuity of the timeline as it is updated with new material each day. We design a novel alternating optimizing algorithm that allows the generation of a high quality article summary and comment summary via mutual reinforcement. We demonstrate the effectiveness of our algorithm on four disparate complex event datasets collected over months from the New York Times, CNN, and BBC. Automatic evaluation using ROUGE (Lin and Hovy, 2003) and gold standard timelines indicates that our system can effectively leverage user comments to outperform state-of-the-art approaches on timeline generation. In a human evaluation via Amazon Mechanical Turk, the comment summaries generated by our method were selected as the best in terms of informativeness and insightfulness in 66.7% and\n51.7% of the evaluations (vs. 26.7% and 30.0% for randomly selected editor\u2019s-picks).\nEspecially, our optimization framework relies on two scoring functions that estimate the importance of including individual article sentences and user comments in the timeline. Based on the observation that entities or events frequently discussed in the user comments can help with identify summaryworthy content, we show that the scoring functions can be learned jointly by utilizing graph-based regularization. Experiments show that our joint learning model outperforms state-of-the-art ranking algorithms and other joint learning based methods when evaluated on sentence ranking and comment ranking. For example, we achieve an NDCG@3 of 0.88 on the Ukraine crisis dataset, compared to 0.77 from Yang et al. (2011) which also conducts joint learning between articles and social context using factor graphs.\nFinally, to encourage continuity in the generated timeline, we propose an entity-centered event threading algorithm. Human evaluation demonstrates that users who read timelines with event threads write more informative answers than users who do not see the threads while answering the same questions. This implies that our system constructed threads can help users better navigate the timelines and collect relevant information in a short time.\nFor the rest of the paper, we first describe data collection (Section 2). We then introduce the joint learning model for importance prediction (Section 3). The full timeline generation system is presented in Section 4, which is followed by evaluations (Section 5). Related work and conclusion are in Sections 6 and 7."}, {"heading": "2 Data Collection and Preprocessing", "text": "We crawled news articles from New York Times (NYT), CNN, and BBC on four trending events: the missing Malaysia Airlines Flight MH370 (MH370), the political unrest in Ukraine (Ukraine), the IsraelGaza conflict (Israel-Gaza), and the NSA surveillance leaks (NSA). For each event, we select a set of key words (usually entities\u2019 name), which are used to filter out irrelevant articles. We collect comments for NYT articles through NYT community API, and comments for CNN articles via Disqus\nAPI. 1 NYT comments come with information on whether a comment is an editor\u2019s-pick. The statistics on the four datasets are displayed in Table 1.2\nWe extract parse trees, dependency trees, and coreference resolution results of articles and comments with Stanford CoreNLP (Manning et al., 2014). Sentences in articles are labeled with timestamps using SUTime (Chang and Manning, 2012).\nWe also collect all articles with comments from NYT in 2013 (henceforth NYT2013) to form a training set for learning importance scoring functions on articles sentences and comments (see Section 3). NYT2013 contains 3, 863 articles and 833, 032 comments."}, {"heading": "3 Joint Learning for Importance Scoring", "text": "We first introduce a joint learning method that uses graph-based regularization to simultaneously learn two functions \u2014 a SENTENCE scorer and a COMMENT scorer \u2014 that predict the importance of including an individual news article sentence or a particular user comment in the timeline.\nWe train the model on the aforementioned NYT2013 dataset, where 20% of the articles and their comments are reserved for parameter tuning. Formally, the training data consists of a set of articles D = {di}|D|\u22121i=0 . Each article di contains a set of sentences xsdi = {xsdi ,j} |sdi |\u22121 j=0 and a set of associated comments xcdi = {xcdi ,k} |cdi |\u22121 k=0 , where |sdi | and |cdi | are the numbers of sentences and comments for di. For simplicity, we use xs or xc to denote a sentence or a comment wherever there is no ambiguity.\nIn addition, each article has a human-written abstract. We use the ROUGE-2 (Lin and Hovy, 2003) score of each sentence computed against the associated abstract as its gold-standard importance score.\n1BBC comment volume is low, so we do not collect it. 2The datasets are available at http://www.cs.\ncornell.edu/\u02dcluwang/data.html.\nEach comment is assigned a gold-standard value of 1.0 if it is an editor\u2019s pick, or 0.0 otherwise.\nThe SENTENCE and COMMENT scorers rely on two classifiers, each designed to handle the special characteristics of news and user comments, respectively; and a graph-based regularizing constraint that encourages similarity between selected sentences and comments. We describe each component below.\nArticle SENTENCE Importance. Each sentence xs in a news article is represented as a k-dimensional feature vector xs \u2208 Rk, with a gold-standard label ys. We denote the training set as a feature matrix X\u0303s, with a label vector Y\u0303s. To produce the SENTENCE scoring function fs(xs) = xs \u00b7 ws, we use ridge regression to learn a vector ws that minimizes ||X\u0303sws \u2212 Y\u0303s||22 + \u03b2s \u00b7 ||ws||22. Features used in the model are listed in Table 2. We also impose the following position-based regularizing constraint to encode the fact that the first sentence in a news article usually conveys the most essential information: \u03bbs \u00b7 \u2211 di \u2211 xsdi ,j\n,j 6=0 ||(xsdi ,0 \u2212 xsdi ,j) \u00b7ws \u2212 (ysdi ,0 \u2212 ysdi ,j)|| 2 2 , where xsdi ,j is the j-th sentence in document di. Term (xsdi ,0 \u2212 xsdi ,j) \u00b7 ws measures the difference in predicted scores between the first sentence and any other sentence. This value is expected be close to the true difference. We further construct X\u0303\u2032s to contain all difference vectors (xsdi ,0 \u2212 xsdi ,j), with Y\u0303\u2032s as label difference vector. The objective function to minimize becomes\nJs(ws) = ||X\u0303sws \u2212 Y\u0303s||22 + \u03bbs \u00b7 ||X\u0303\u2032sws \u2212 Y\u0303\u2032s||22 + \u03b2s \u00b7 ||ws||22 (1)\nUser COMMENT Importance. Similarly, each comment xc is represented as an l\u2212dimensional feature vector xc \u2208 Rl, with label yc. Comments in the training data are denoted with a feature matrix X\u0303c with a label vector Y\u0303c. Likewise, we learn fc(xc) = xc \u00b7wc by minimizing ||X\u0303cwc \u2212 Y\u0303c||22 + \u03b2c \u00b7 ||wc||22. Features are listed in Table 3. We apply a pairwise preference-based regularizing constraint (Joachims, 2002) to incorporate a bias toward editor\u2019s picks: \u03bbc \u00b7 \u2211 di \u2211 xcdi ,j \u2208Edi ,xcdi ,k /\u2208Edi ||(xcdi ,j\u2212xcdi ,k) \u00b7wc\u2212 1||22 , where Edi are the editor\u2019s picks for di. Term (xcdi ,j \u2212 xcdi ,k) \u00b7 wc enforces the separation of editor\u2019s picks from regular comments. We further construct X\u0303\u2032c to contain all the pairwise differences\n(xcdi ,j \u2212 xcdi ,k). Y\u0303 \u2032 c is a vector of same size as X\u0303\u2032c with each element as 1. Thus, the objective function to minimize is:\nJc(wc) = ||X\u0303cwc \u2212 Y\u0303c||22 + \u03bbc \u00b7 ||X\u0303\u2032cwc \u2212 Y\u0303\u2032c||22 + \u03b2c \u00b7 ||wc||22 (2)\nGraph-Based Regularization. The regularizing constraint is based on two mutually reinforcing hypotheses: (1) the importance of a sentence depends partially on the availability of sufficient insightful comments that touch on topics in the sentence; (2) the importance of a comment depends partially on whether it addresses notable events reported in the sentences. For example, we want our model to bias ws to predict a high score for a sentence with high similarity to numerous insightful comments.\nWe first create a bipartite graph from sentences and comments on the same articles, where edge weights are based on the content similarity between a sentence and a comment (TF-IDF similarity is used). Let R\u0303 be an N \u00d7M adjacency matrix, where N and M are the numbers of sentences and comments. Rsc is the similarity between sentence xs and comment xc. We normalize R\u0303 by Q\u0303 = D\u0303\u2212 1 2 R\u0303D\u0303\u2032\n\u2212 12 , where D\u0303 and D\u0303\u2032 are diagonal matrices: D\u0303 \u2208 RN\u00d7N , Di,i = \u2211M j=1Ri,j ; D\u0303 \u2032 \u2208 RM\u00d7M , D\u2032j,j = \u2211N i=1Ri,j . The interplay between the two types of data is encoded in the following regularizing constraint:\nJs,c(ws,wc) = \u03bbsc \u00b7 \u2211 di \u2211 xs\u2208xsdi ,xc\u2208xcdi Qxs,xc \u00b7 (xs \u00b7ws \u2212 xc \u00b7wc)2\n(3)\nFull Objective Function. Thus, the full objective function consists of the three parts discussed above:\nJ(ws,wc) = Js(ws) + Jc(wc) + Js,c(ws,wc) (4)\nFurthermore, using the following notation,\nX\u0303 =\n[ X\u0303s 0\n0 X\u0303c\n] Y\u0303 = [ Y\u0303s Y\u0303c ] X\u0303\u2032 = [ X\u0303\u2032s 0 0 X\u0303\u2032c ] Y\u0303\u2032 = [ Y\u0303\u2032s Y\u0303\u2032c ]\n\u03b2\u0303 = [ \u03b2sIk 0 0 \u03b2cIl ] \u03bb\u0303 = [ \u03bbsI|X\u2032s| 0\n0 \u03bbcI|X\u2032c|\n]\nL\u0303 = [ \u03bbscI|Xs| \u2212\u03bbscQ\u0303 \u2212\u03bbscQ\u0303T \u03bbscI|Xc| ] w = [ ws wc ]\nwe can show a closed form solution to Equation 4 as follows:\nw\u0302 =\n(X\u0303TL\u0303X\u0303 + X\u0303TX\u0303 + X\u0303\u2032T\u03bb\u0303X\u0303\u2032 + \u03b2\u0303)\u22121(X\u0303TY\u0303 + X\u0303\u2032T\u03bb\u0303Y\u0303\u2032)\n(5)"}, {"heading": "4 Timeline Generation", "text": "Now we present an optimization framework for timeline generation. Formally, for each day, our system takes as input a set of sentences Vs and a set of comments Vc to be summarized, and the (automatically generated) timeline T (represented as threads) for days prior to the current day. It then identifies a subset S \u2286 Vs as the article summary and a subset C \u2286 Vc as the comment summary by maximizing the following function:\nZ(S,C; T ) = Squal(S; T )+Cqual(C)+\u03b4X (S,C) (6)\nwhere Squal(S; T ) measures the quality of the article summary S in the context of the historical timeline represented as event threads T ; Cqual(C) computes the quality of the comment summary C; and X (S,C) estimates the connectivity between S and C.\nWe solve this maximization problem using an alternating optimization algorithm which is outlined\nin Section 4.4. In general, we alternately search for a better article summary S with hill climbing search and a better comment summary C with FordFulkerson algorithm until convergence.\nIn the rest of this section, we first describe an entity-centered event threading algorithm to construct event threads T which are used to boost article timeline continuity. Then we explain how to compute Squal(S; T ) and Cqual(C) in Section 4.2, followed by X (S,C) in Section 4.3."}, {"heading": "4.1 Entity-Centered Event Threading", "text": "We present an event threading process where each thread connects sequential events centered on a set of relevant entities. For instance, the following thread connects events about Obama\u2019s action towards the annexation of Crimea by Russia: Day 1: Obama declared sanctions on Russian officials. Day 2: President Obama warned Russian. Day 3: Obama urges Russian to move back its troops. Day 4: Obama condemns Russian aggression in Ukraine.\nWe first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open information extraction system. We retain extractions with confidence scores higher than 0.5. We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns. Each relation contains at least one event-related word (Ritter et al., 2012).\nThe entity-centered event threading algorithm works as follows: on the first day, each sentence in the summary becomes an individual cluster; thereafter, each sentence in the current day\u2019s article summary either gets attached to an existing thread or starts a new thread. The updated threads then become the input to next day\u2019s summary generation process. On day n, we have a set of threads T = {\u03c4 : s1, s2, \u00b7 \u00b7 \u00b7 , sn\u22121} constructed from previous n \u2212 1 days, where si represents the set of sentences attached to thread \u03c4 from day i. The cohesion between a new sentence s \u2208 S and a thread \u03c4 is denoted as cohn(s, \u03c4). s is attached to \u03c4\u0302 if there exists \u03c4\u0302 = max\u03c4\u2208T cohn(s, \u03c4) and cohn(s, \u03c4\u0302) > 0.0. Otherwise, s becomes a new thread. We define cohn(s, \u03c4) = minsi\u2208\u03c4,si 6=\u2205 tfsimi(si, s), where tfsimi(si, s) measures the TF similarity between si and s. We con-\nsider unigrams/bigrams/trigrams generated from the entities of our event extractions."}, {"heading": "4.2 Summary Quality Measurement", "text": "Recall that we learned two separate importance scoring functions for sentences and comments, which will be denoted here as imps(s) and impc(c). With an article summary S and threads T = {\u03c4i}, the article summary quality function Squal(S; T ) has the following form:\nSqual(S; T ) = \u2211 s\u2208S imp(s)\n+\u03b8cov \u2211 s\u2032\u2208Vs min( \u2211 s\u2208S tfidf(s, s \u2032), \u03b1 \u2211 s\u0302\u2208Vs tfidf(s\u0302, s \u2032))\n+ \u03b8cont \u2211 \u03c4\u2208T maxsk\u2208S cohn(sk, \u03c4)\ntfidf(\u00b7, \u00b7) is the TF-IDF similarity function. Squal(S; T ) captures three desired qualities of an article summary: importance (first item), coverage (second item), and the continuity of the current summary to previously generated summaries. The coverage function has been used to encourage summary diversity and reduce redundancy (Lin and Bilmes, 2011; Wang et al., 2014). The continuity function considers how well article summary S can be attached to each event thread, thus favors summaries that can be connected to multiple threads.\nParameters \u03b8cov and \u03b1 are tuned on multidocument summarization dataset DUC 2003 (Over and Yen, 2003). Experiments show that system performance peaks and is stable for \u03b8cont \u2208 [1.0, 5.0]. We thus fix \u03b8cont to 1.0. We discard sentences with more than 80% of content words covered by historical summaries. We use BASIC to denote a system that only optimizes on importance and coverage (i.e. first two items in Squal(S; T )). The system optimizing Squal(S; T ) is henceforth called THREAD.\nThe comment summary quality function simply takes the form Cqual(C) = \u2211 c\u2208C impc(c)."}, {"heading": "4.3 Connectivity Measurement", "text": "We encode two objectives in the connectivity function X (S,C): (1) encouraging topical cohesion (i.e. connectivity) between article summary and comment summary; and (2) favoring comments that cover diversified events.\nLet conn(s, c) measure content similarity between a sentence s \u2208 S and a comment c \u2208 C. Connectivity between article summary S and comment summary C is computed as follows. We build a bipartite graph G between S and C with edge weight as conn(s, c).\nWe then find an edge setM, the best matching of G. X (S,C) is defined as the sum over edge weights in M, i.e. X (S,C) = \u2211 e\u2208M weight(e). An example is illustrated in Figure 2.\nWe consider two options for conn(s, c). One is lexical similarity which is based on TF-IDF vectors. Another is semantic similarity. Let Rs = {(as, rs, bs)} and Rc = {(ac, rc, bc)} be the sets of dependency relations in s and c. conn(s, c) is calculated as:\u2211\n(as,rs,bs)\u2208Rs max(ac,rc,bc)\u2208Rc rs=rc\nsimi(as, ac)\u00d7 simi(bs, bc)\nwhere simi(\u00b7, \u00b7) is a word similarity function. We experiment with shortest path based similarity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al., 2013). Systems using the three metrics that optimize Z(S,C; T ) are henceforth called THREAD+OPTTFIDF, THREAD+OPTWordNet and THREAD+OPTWordVec."}, {"heading": "4.4 An Alternating Optimization Algorithm", "text": "To maximize the full objective function Z(S,C; T ), we design a novel alternating optimization algorithm (Alg. 1) where we alternately find better S and C.\nWe initialize S0 by a greedy algorithm (Lin and Bilmes, 2011) with respect to Squal(S; T ). Notice that Squal(S; T ) is a submodular function, so that the greedy solution is a 1 \u2212 1/e approximation to the optimal solution of Squal(S; T ). Fixing S0, we model the problem of finding C0 that maximizes Cqual(C) + \u03b4X (S0, C) as a maximum-weight bipar-\ntite graph matching problem. This problem can be reduced to a maximum network flow problem, and then be solved by Ford-Fulkerson algorithm (details are discussed in (Kleinberg and Tardos, 2005)). Thereafter, for each iteration, we alternately find a better St with regard to Squal(S; T ) + \u03b4X (S,Ct\u22121) using hill climbing, and an exact solution Ct to Cqual(C)+\u03b4X (St, C) with Ford-Fulkerson algorithm. Iteration stops when the increase of Z(S,C) is below threshold (set to 0.01). System performance is stable when we vary \u03b4 \u2208 [1.0, 10.0], so we set \u03b4 = 1.0.\nInput : sentences Vs, comments Vc, threads T , \u03b4, threshold , functions Z(S,C; T ), Squal(S; T ), Cqual(C), X (S,C) Output: article summary S, comment summary C /* Initialize S and C by greedy algorithm and Ford-Fulkerson algorithm */ S0 \u2190maxS Squal(S; T ); C0 \u2190 maxC Cqual(C) + \u03b4X (S0, C); t\u2190 1; \u2206Z \u2190\u221e; while \u2206Z > do\n/* Step 1: Hill climbing algorithm */ St \u2190 maxS Squal(S; T ) + \u03b4X (S,Ct\u22121); /* Step 2: Ford-Fulkerson algorithm */ Ct \u2190 maxC Cqual(C) + \u03b4X (St, C); \u2206Z = Z(St, Ct; T )\u2212Z(St\u22121, Ct\u22121; T ); t\u2190 t+ 1;\nend Algorithm 1: Generate article summary and comment summary for a given day via alternating optimization .\nAlgorithm 1 is guaranteed to find a solution at least as good as S0 and C0. It progresses only if Step 1 finds St that improves upon Z(St\u22121, Ct\u22121; T ), and Step 2 finds Ct where Z(St, Ct; T ) \u2265 Z(St, Ct\u22121; T )."}, {"heading": "5 Experimental Results", "text": "5.1 Evaluation of SENTENCE and COMMENT Importance Scorers\nWe test importance scorers (Section 3) on single document sentence ranking and comment ranking.\nFor both tasks, we compare with two previous systems on joint ranking and summarization of news articles and tweets. Yang et al. (2011) employ supervised learning based on factor graphs to model content similarity between the two types of data. We use the same features for this model. Gao et al. (2012) summarize by including the complementary information between articles and\ntweets, which is estimated by an unsupervised topic model.3 We also consider two state-of-the-art rankers: RankBoost (Freund et al., 2003) and LambdaMART (Burges, 2010). Finally, we use a position baseline that ranks sentences based on their position in the article, and a rating baseline that ranks comments based on positive user ratings.\nWe evaluate using normalized discounted cumulative gain at top 3 returned results (NDCG@3). Sentences are considered relevant if they have ROUGE-2 scores larger than 0.0 (computed against human abstracts), and comments are considered relevant if they are editor\u2019s picks.4 Figure 3 demonstrates that our joint learning model uniformly outperforms all the other comparisons for both ranking tasks. In general, supervised learning based approaches (e.g. our method, Yang et al. (2011), RankBoost, and LambdaMART) produce better results than unsupervised method (e.g. Gao et al. (2012)).5"}, {"heading": "5.2 Leveraging User Comments", "text": "In this section, we test if our system can leverage comments to produce better article-based summaries for event timelines. We collect gold-standard timelines for each of the four events from the corresponding Wikipedia page(s), NYT topic page, or BBC news page.\nWe consider two existing timeline creation systems that only utilize news articles, and a timeline generated from single-article human abstracts: (1) CHIEU AND LEE (2004) select sentences with high\n3We thank Zi Yang and Peng Li for providing the code. 4We experiment with all articles for sentence ranking, and\nNYT comments (with editor\u2019s picks) for comment ranking. 5Similar results are obtained with mean reciprocal rank.\n\u201cinterestingness\u201d and \u201cburstiness\u201d using a likelihood ratio test to compare word distributions of sentences with articles in neighboring days. (2) YAN ET AL. (2011) design an evolutionary summarization system that selects sentences based on on coverage, coherence, and diversity. (3) We construct a timeline from the human ABSTRACTs provided with each article: we sort them chronologically according to article timestamps and add abstract sentences into each daily summary until reaching the word limit.\nWe test on five variations of our system. The first two systems generate article summaries with no comment information by optimizing Squal(S; T ) using a greedy algorithm: BASIC ignores event threading; THREAD considers the threads. THREAD+OPTTFIDF, THREAD+OPTWordNet and THREAD+OPTWordVec (see Section 4.3) leverage user comments to generate article summaries as well as comment summaries based on alternating optimization of Equation 3. Although comment summaries are generated, they are not used in the evaluation.\nFor all systems, we generate daily article summaries of at most 100 words, and select 5 comments for the corresponding comment summary. We employ ROUGE (Lin and Hovy, 2003) to automatically evaluate the content coverage (in terms of ngrams) of the article-based timelines vs. goldstandard timelines. ROUGE-2 (measures bigram overlap) and ROUGE-SU4 (measures unigram and skip-bigrams separated by up to four words) scores are reported in Table 4. As can be seen, under the alternating optimization framework, our systems, employing both articles and comments, consistently yield better ROUGE scores than the three baseline systems and our systems that do not leverage comments. Though constructed from single-article abstracts, baseline ABSTRACT is found to contain redundant information and thus limited in content coverage. This is due to the fact that different media tend to report on the same important events."}, {"heading": "5.3 Evaluating Socially-Informed Timelines", "text": "We evaluate the full article+comment-based timelines on Amazon Mechanical Turk. Turkers are presented with a timeline consisting of five consecutive days\u2019 article summaries and four variations of the accompanying comment summary:\nRANDOMly selected comments, USER\u2019S-PICKS (ranked by positive user ratings), randomly selected EDITOR\u2019S-PICKS and timelines produced by the THREAD+OPTWordVec version of OUR SYSTEM. We also include one noisy comment summary (i.e. irrelevant to the question) to avoid spam. We display two comments per day for each system.6\nTurkers are asked to rank the comment summary variations according to informativeness and insightfulness. For informativeness, we ask the Turkers to judge based only on knowledge displayed in the timeline, and to rate each comment summary based on how much relevant information they learn from it. For insightfulness, Turkers are required to focus on insights and valuable opinions. They are requested to leave a short explanation of their ranking.\n15 five-day periods are randomly selected. We solicit four distinct Turkers located in the U.S. to evaluate each set of timelines. An inter-rater agreement of Krippendorff\u2019s \u03b1 of 0.63 is achieved for informativeness ranking and \u03b1 is 0.50 for insightfulness ranking.\nTable 5 shows the percentage of times a particular method is selected as producing the best comment portion of the timeline, as well as the microaverage rank of each method, for both informativeness and insightfulness. Our system is selected as the best in 66.7% of the evaluations for informativeness and 51.7% for insightfulness. In both cases, we statistically significantly outperform (p < 0.05 using a Wilcoxon signed-rank test) the editor\u2019s-picks\n6For our system, we select the two comments with highest importance scores from the comment summary.\nand user\u2019s-picks. Turkers\u2019 explanations indicate that they prefer our comment summaries mainly because they are \u201cvery informative and insightful to what was happening\u201d, and \u201cshow the sharpness of the commenter\u201d. Turkers sometimes think the summaries randomly selected from editor\u2019s-picks \u201clack connection\u201d, and characterize user\u2019s-picks as \u201cthe information was somewhat limited\u201d.\nFigure 4 shows part of the timeline generated by our system for the Ukraine crisis."}, {"heading": "5.4 Human Evaluation of Event Threading", "text": "Here we evaluate on the utility of event threads for high-level information access guidance: can event threads allow users to easily locate and absorb information with a specific interest in mind?\nWe first sample a 10-day timeline for each dataset from those produced by the THREAD+OPTWordVec\nvariation of our system. We designed one question for each timeline. Sample questions are: \u201cdescribe the activities for searching for the missing flight MH370\u201d, and \u201cdescribe the attitude and action of Russian Government on Eastern Ukraine\u201d. We recruited 10 undergraduate and graduate students who are native speakers of English. Each student first read one question and its corresponding timeline for 5 minutes. The timeline was then removed, and the student wrote down an answer for the question. We asked each student to answer the question for each of four timelines (one for each event dataset). Two timelines are displayed with threads, and two without threads. We presented threads by adding a thread number in front of each sentence.\nWe then used Amazon Mechanical Turk to evaluate the informativeness of students\u2019 answers. Turkers were asked to read all 10 answers for the same question, with five answers based on timelines with threads and five others based on timelines without threads. After that, they rated each answer with an informativeness score on a 1-to-5 rating scale (1 as \u201cnot relevant to the query\u201d, and 5 as \u201cvery informative\u201d). We also added two quality control questions. Table 6 shows that the average rating for answers written after reading timelines with threads is 3.29 (43% are rated\u2265 4), higher than the 2.58 for the timelines with no thread exhibited (30% are rated \u2265 4)."}, {"heading": "6 Related Work", "text": "There is a growing interest in generating article summaries informed by social context. Existing work focuses on learning users\u2019 interests from comments and incorporates the learned information into a news article summarization system (Hu et al., 2008). Zhao et al. (2013) instead estimate word distributions from tweets, and bias a Page Rank algorithm to give higher restart probability to sentences with similar distributions. Generating tweet+article summaries has been recently investigated in Yang et\nal. (2011). They propose a factor graph to allow sentences and tweets to mutually reinforce each other. Gao et al. (2012) exploit a co-ranking model to identify sentence-tweet pairs with complementary information estimated from a topic model. These efforts handle a small number of documents and tweets, while we target a larger scale of data.\nIn terms of timeline summarization, the Chieu and Lee (2004) system ranks sentences according to \u201cburstiness\u201d and \u201cinterestingness\u201d estimated by a likelihood ratio test. Yan et al. (2011) explore an optimization framework that maximizes the relevance, coverage, diversity, and coherence of the timeline. Neither system has leveraged the social context. Our event threading algorithm is also inspired by work on topic detection and tracking (TDT) (Allan et al., 1998), where efforts are made for document-level link detection and topic tracking. Similarly, Nallapati et al. (2004) investigate event threading for articles, where they predict linkage based on causal and temporal dependencies. Shahaf et al. (2012) instead seek for connecting articles into one coherent graph. To the best of our knowledge, we are the first to study sentence-level event threading."}, {"heading": "7 Conclusion", "text": "We presented a socially-informed timeline generation system, which constructs timelines consisting of article summaries and comment summaries. An alternating optimization algorithm is designed to maximize the connectivity between the two sets of summaries as well as their importance and information coverage. Automatic and human evaluations showed that our system produced more informative timelines than state-of-the-art systems. Our comment summaries were also rated as very insightful."}, {"heading": "Acknowledgments", "text": "We thank John Hessel, Lillian Lee, Moontae Lee, David Lutz, Karthik Raman, Vikram Rao, Yiye Ruan, Xanda Schofield, Adith Swaminathan, Chenhao Tan, Bishan Yang, other members of Cornell NLP group, and the NAACL reviewers for valuable suggestions and advice on various aspects of this work. This work was supported in part by DARPA DEFT Grant FA8750-13-2-0015."}], "references": [{"title": "Topic detection and tracking pilot study: Final report", "author": ["J. Allan", "J. Carbonell", "G. Doddington", "J. Yamron", "Y. Yang"], "venue": "In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop,", "citeRegEx": "Allan et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Allan et al\\.", "year": 1998}, {"title": "From RankNet to LambdaRank to LambdaMART: An Overview", "author": ["Christopher J.C. Burges"], "venue": "Technical report, Microsoft Research", "citeRegEx": "Burges.,? \\Q2010\\E", "shortCiteRegEx": "Burges.", "year": 2010}, {"title": "Sutime: A library for recognizing and normalizing time expressions", "author": ["Chang", "Manning2012] Angel X. Chang", "Christopher Manning"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2012}, {"title": "Query based event extraction along a timeline", "author": ["Chieu", "Lee2004] Hai Leong Chieu", "Yoong Keok Lee"], "venue": "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Chieu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chieu et al\\.", "year": 2004}, {"title": "Identifying relations for open information extraction", "author": ["Fader et al.2011] Anthony Fader", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Fader et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Freund et al.2003] Yoav Freund", "Raj Iyer", "Robert E. Schapire", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Freund et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2003}, {"title": "Joint topic modeling for event summarization across news and social media streams", "author": ["Gao et al.2012] Wei Gao", "Peng Li", "Kareem Darwish"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Gao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2012}, {"title": "Comments-oriented document summarization: Understanding documents with readers", "author": ["Hu et al.2008] Meishan Hu", "Aixin Sun", "Ee-Peng Lim"], "venue": "Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Develop-", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the Eighth ACM SIGKDD International", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "A class of submodular functions for document summarization", "author": ["Lin", "Bilmes2011] Hui Lin", "Jeff Bilmes"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Topic-driven reader comments summarization", "author": ["Ma et al.2012] Zongyang Ma", "Aixin Sun", "Quan Yuan", "Gao Cong"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Open language learning for information extraction", "author": ["Mausam et al.2012] Mausam", "Michael Schmitz", "Robert Bart", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods", "citeRegEx": "Mausam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mausam et al\\.", "year": 2012}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Event threading within news topics", "author": ["Ao Feng", "Fuchun Peng", "James Allan"], "venue": "In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Nallapati et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2004}, {"title": "An introduction to DUC 2003: Intrinsic evaluation of generic news text summarization systems", "author": ["Over", "Yen2003] P. Over", "J. Yen"], "venue": null, "citeRegEx": "Over et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Over et al\\.", "year": 2003}, {"title": "Open domain event extraction from twitter", "author": ["Ritter et al.2012] Alan Ritter", "Mausam", "Oren Etzioni", "Sam Clark"], "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Ritter et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2012}, {"title": "Trains of thought: Generating information maps", "author": ["Shahaf et al.2012] Dafna Shahaf", "Carlos Guestrin", "Eric Horvitz"], "venue": "In Proceedings of the 21st International Conference on World Wide Web, WWW", "citeRegEx": "Shahaf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shahaf et al\\.", "year": 2012}, {"title": "The General Inquirer: A Computer Approach to Content Analysis", "author": ["Dexter C. Dunphy", "Marshall S. Smith", "Daniel M. Ogilvie"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Query-focused opinion summarization for user-generated content", "author": ["Wang et al.2014] Lu Wang", "Hema Raghavan", "Claire Cardie", "Vittorio Castelli"], "venue": "In Proceedings of COLING", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Janyce Wiebe", "Paul Hoffmann"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Evolutionary timeline summarization: A balanced optimization framework via iterative substitution", "author": ["Yan et al.2011] Rui Yan", "Xiaojun Wan", "Jahna Otterbacher", "Liang Kong", "Xiaoming Li", "Yan Zhang"], "venue": "In Proceedings of the 34th International ACM SI-", "citeRegEx": "Yan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2011}, {"title": "Social context summarization", "author": ["Yang et al.2011] Zi Yang", "Keke Cai", "Jie Tang", "Li Zhang", "Zhong Su", "Juanzi Li"], "venue": "In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}, {"title": "Timeline generation with social attention", "author": ["Zhao et al.2013] Xin Wayne Zhao", "Yanwei Guo", "Rui Yan", "Yulan He", "Xiaoming Li"], "venue": "In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "While generating timelines from news articles and summarizing user comments have been studied as separate problems (Yan et al., 2011; Ma et al., 2012), their joint summarization for timeline generation", "startOffset": 115, "endOffset": 150}, {"referenceID": 11, "context": "While generating timelines from news articles and summarizing user comments have been studied as separate problems (Yan et al., 2011; Ma et al., 2012), their joint summarization for timeline generation", "startOffset": 115, "endOffset": 150}, {"referenceID": 24, "context": "77 from Yang et al. (2011) which also conducts joint learning between articles and social context using factor graphs.", "startOffset": 8, "endOffset": 27}, {"referenceID": 12, "context": "coreference resolution results of articles and comments with Stanford CoreNLP (Manning et al., 2014).", "startOffset": 78, "endOffset": 100}, {"referenceID": 8, "context": "preference-based regularizing constraint (Joachims, 2002) to incorporate a bias toward editor\u2019s picks: \u03bbc \u00b7 \u2211", "startOffset": 41, "endOffset": 57}, {"referenceID": 22, "context": "Sentiment Features - num /proportion of positive/negative/neutral words (MPQA (Wilson et al., 2005), General Inquirer (Stone et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 20, "context": ", 2005), General Inquirer (Stone et al., 1966)) - num /proportion of sentiment words", "startOffset": 26, "endOffset": 46}, {"referenceID": 13, "context": "We first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open informa-", "startOffset": 87, "endOffset": 108}, {"referenceID": 4, "context": "We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns.", "startOffset": 46, "endOffset": 66}, {"referenceID": 18, "context": "event-related word (Ritter et al., 2012).", "startOffset": 19, "endOffset": 40}, {"referenceID": 15, "context": "ity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al.", "startOffset": 23, "endOffset": 37}, {"referenceID": 14, "context": "ity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al., 2013).", "startOffset": 101, "endOffset": 123}, {"referenceID": 24, "context": "Yang et al. (2011) employ supervised learning based on factor graphs to model content similarity between the two types of", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Gao et al. (2012) summarize by including the complementary information between articles and", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "3 We also consider two state-of-the-art rankers: RankBoost (Freund et al., 2003) and LambdaMART (Burges, 2010).", "startOffset": 59, "endOffset": 80}, {"referenceID": 1, "context": ", 2003) and LambdaMART (Burges, 2010).", "startOffset": 23, "endOffset": 37}, {"referenceID": 23, "context": "our method, Yang et al. (2011), RankBoost, and LambdaMART) produce better results than unsupervised method (e.", "startOffset": 12, "endOffset": 31}, {"referenceID": 6, "context": "Gao et al. (2012)).", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "work focuses on learning users\u2019 interests from comments and incorporates the learned information into a news article summarization system (Hu et al., 2008).", "startOffset": 138, "endOffset": 155}, {"referenceID": 7, "context": "work focuses on learning users\u2019 interests from comments and incorporates the learned information into a news article summarization system (Hu et al., 2008). Zhao et al. (2013) instead estimate word distributions from tweets, and bias a Page Rank algo-", "startOffset": 139, "endOffset": 176}, {"referenceID": 23, "context": "Generating tweet+article summaries has been recently investigated in Yang et al. (2011). They propose a factor graph to allow sentences and tweets to mutually reinforce each other.", "startOffset": 69, "endOffset": 88}, {"referenceID": 6, "context": "Gao et al. (2012) exploit a co-ranking model to identify sentence-tweet pairs with complementary infor-", "startOffset": 0, "endOffset": 18}, {"referenceID": 23, "context": "Yan et al. (2011) explore an optimization framework that maximizes the relevance,", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Our event threading algorithm is also inspired by work on topic detection and tracking (TDT) (Allan et al., 1998), where efforts are made for document-level", "startOffset": 93, "endOffset": 113}, {"referenceID": 16, "context": "Similarly, Nallapati et al. (2004) investigate event threading for articles, where they predict linkage based on causal and temporal dependencies.", "startOffset": 11, "endOffset": 35}, {"referenceID": 16, "context": "Similarly, Nallapati et al. (2004) investigate event threading for articles, where they predict linkage based on causal and temporal dependencies. Shahaf et al. (2012) instead seek for connecting articles into one coherent", "startOffset": 11, "endOffset": 168}], "year": 2016, "abstractText": "Existing timeline generation systems for complex events consider only information from traditional media, ignoring the rich social context provided by user-generated content that reveals representative public interests or insightful opinions. We instead aim to generate socially-informed timelines that contain both news article summaries and selected user comments. We present an optimization framework designed to balance topical cohesion between the article and comment summaries along with their informativeness and coverage of the event. Automatic evaluations on real-world datasets that cover four complex events show that our system produces more informative timelines than state-of-theart systems. In human evaluation, the associated comment summaries are furthermore rated more insightful than editor\u2019s picks and comments ranked highly by users.", "creator": "LaTeX with hyperref package"}}}