{"id": "1606.05767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2016", "title": "On Reward Function for Survival", "abstract": "obtaining a survival strategy ( policy ) is one of the fundamental problems of biological agents. in this paper, we generalize the formulation of previous simulation research related to the survival of an agent and we formulate the survival retrieval problem as a maximization of the multi - step survival probability amplitude in uncertain future time onset steps. alternatively we introduce a method for converting the maximization of multi - step survival probability into a classical reinforcement learning problem. correctly using this conversion, the reward function ( negative temporal deviation cost function ) is being expressed as the log of the temporal survival probability. and we show that the objective function of the theoretical reinforcement learning in this sense is proportional to lowering the variational lower bound of calculating the conditional original problem. finally, we empirically demonstrate successfully that the agent learns survival behavior weakly by using the reward function introduced in this paper.", "histories": [["v1", "Sat, 18 Jun 2016 15:33:04 GMT  (693kb,D)", "https://arxiv.org/abs/1606.05767v1", "Joint 8th International Conference on Soft Computing and Intelligent Systems and 17th International Symposium on Advanced Intelligent Systems"], ["v2", "Sun, 24 Jul 2016 13:19:23 GMT  (693kb,D)", "http://arxiv.org/abs/1606.05767v2", "Joint 8th International Conference on Soft Computing and Intelligent Systems and 17th International Symposium on Advanced Intelligent Systems"]], "COMMENTS": "Joint 8th International Conference on Soft Computing and Intelligent Systems and 17th International Symposium on Advanced Intelligent Systems", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["naoto yoshida"], "accepted": false, "id": "1606.05767"}, "pdf": {"name": "1606.05767.pdf", "metadata": {"source": "CRF", "title": "On Reward Function for Survival", "authors": ["Naoto Yoshida"], "emails": ["naotoyoshida@pfsl.mech.thoku.ac.jp"], "sections": [{"heading": "I. PRELIMINARIES", "text": "Survival strategies are essential for biological agents. Many researchers have developed various types of survival agents since the early days of artificial intelligence. Ashby developed Homeostat, which dynamically stabilizes the state of the machine [1], and Walter developed simple robotic agents that can explore the environment of a room and automatically recharge their batteries at a recharging station [2]. Toda discussed the survival problem of artificial agents in the natural environment [3], [4]. He speculated about the functional requirements for an autonomous survival agent based on decision theory. Based on Toda\u2019s works, Pfeifer and Scheier pointed out the importance of \u2018complete\u2019 autonomous agents and research on embodied cognitive science [5]. In this sense, they developed a simple self-sufficient autonomous robot. McFarland and Bo\u0308sser discussed the autonomous agent from the perspective of research on animal behavior [6]. They suggested several requirements for intelligent agents through comparison of the market economy with natural selection, and they also developed simple robots that were self-sufficient, autonomous agents [7]. Also, Lin, in a simulation study, compared several reinforcement learning (RL) architectures for a complex survival task in a non-Markovian environment [8]. Sibly and McFarland introduced the state space approach in ethology and suggested that animal behaviors are the consequence of optimal control with respect to the cost function given by the fitness function [9]. Keramati and Gutkin suggested a similar perspective, but they also suggested changes in the distance between the current homeostatic state and the optimal, desired state as the reward function of RL [10]. Konidaris and Barto developed RL architecture that automatically balances multiple required nutrients (protein, fat, water, etc.) through tuning of the reward function, which depends on the agent\u2019s homeostatic state [11]. They tested the architecture in a simulation experiment.\nOgata and Sugino developed a real robot agent intended for survival [12]. Their robot evaluates the sensor signals (motor temperature, battery level, etc.) in real time and learns to avoid\nundesirable stimuli. Doya and Uchibe developed robots called \u201cCyber rodents\u201d intended for the study of learning agents for survival and evolutionary robotics [13]. Cyber rodents can recharge their batteries by touching special battery packs in the environment. The wireless communication modules of robots enable the software evolution of control algorithms [14].\nReward stimuli have been introduced in most of the preceding research studies, with the reward function being necessary for the RL paradigm. However, almost all of the previous studies on survival adopted hand-crafted reward functions that do not guarantee the intended behaviors, which are the survival strategies in this case. The reward and objective function given by the designer may work well in simple RL tasks. However, for more complex tasks and life-long learning settings in which agents must learn for days and months with a large amount of data, a badly hand-crafted reward function may ultimately have serious problems in terms of system performance.\nBased on such considerations, we first focus on the mathematical formulation of the classical survival problem as a maximization problem of the multi-step survival probability. To solve this problem, we introduce an iterative model-based method for maximization of the objective function using an expectation-maximization (EM) algorithm with variational approximations. Surprisingly, after the M-step, the negative free-energy function (variational lower bound) in our algorithm is identical in form to the classical RL objective function with a specific reward function. Therefore, it can be maximized by classical model-free RL algorithms. Our contributions are i) a probabilistic formulation of the classical survival problem, ii) a suggested RL approach to solve this problem and iii) a demonstration that the maximization of multi-step survival probability through RL algorithms is identical to the maximization of the log of that probability from the variational lower bound."}, {"heading": "A. Objective Functions of Reinforcement Learning Problem", "text": "Reinforcement learning (RL) is the field of research that constructs learning agents that obtain an optimal policy through interactions with the environment. We first introduce the general form of the objective function in the POMDP (partially observable Markov decision process) model. For the sake of simplicity, we restrict the discussion to a finite set of actions, states, and observations.\nMany realistic environments for the agent are known to be modeled by the partially observable Markov decision process (POMDP) [15]. The POMDP model consists of the state set S, the action set A, the observation set O, the transition probability to state s\u2032 \u2208 S given a state s \u2208 S and an action a \u2208 A as P (s\u2032|s, a), the observation probability P (o|s), and the reward function r(s, a).\nar X\niv :1\n60 6.\n05 76\n7v 2\n[ cs\n.A I]\n2 4\nJu l 2\n01 6\nAt each time step t, the agent receives an observation ot from the state st by the observation probability and replies with an action at. Then, the state of the environment changes to st+1 and the agent receives a reward rt = r(st, at). In the POMDP setting, since the agent can not gain access to the true state s \u2208 S of the environment, the agent needs to infer the current true state from a sequence of observations and actions. We call the sequence of observations and actions a history ht = {o0, a0, o1, a1, . . . , ot\u22121, at\u22121, ot}. Using the history, how the agent acts in the environment can be expressed by a probability \u03c0(a|h) called the policy. In the POMDP model, the probability of generating the T -step trajectory \u03c4T = {s0, o0, a0, s1, o1, a1, . . . , sT\u22121, oT\u22121, aT\u22121, sT , oT } is\nP (\u03c4T |\u03c0) = P (o0|s0)P (s0)\n\u00d7 T\u22121\u220f t=0 P (ot+1|st+1)P (st+1|st, at)\u03c0(at|ht).\nTherefore, the expectation of the T -step average reward is given by\nJT (\u03c0) = \u2211 \u03c4T P (\u03c4T |\u03c0) [ 1 T T\u22121\u2211 t=0 rt ] .\nWe denote the limit T \u2192 \u221e by J(\u03c0) = limT\u2192\u221e JT (\u03c0). This JT (\u03c0) or J(\u03c0) (or the product with some constant) is the typical objective function in the reinforcement learning literature1. The objective of reinforcement learning is to find the optimal policy \u03c0\u2217 that maximally achieves the objective function, defined above, through interactions with the environment."}, {"heading": "II. SURVIVAL PROBLEM", "text": "In this section, we formulate the survival problem from the models of an animal proposed by Ashby [1] and a similar idea suggested by Sibly and McFarland [9] and McFarland and Bo\u0308user [6] from the view point of ethology. In their model, an animal has several variables that are observed by the animal and have some importance for sustaining life (for example, the water level and the energy level in the body, as shown in Figure 1). We call these variables the \u2018physiological\n1Even though many studies derive the algorithm from another objective function like \u2211T\u22121 t=0 \u03b3\ntrt, the performance of the algorithm is usually evaluated by the total or the average reward criterion.\nstate\u2019. On the other hand, an agent also has a \u2018perceptual state\u2019, which is the perception of the environmental stimuli (vision, touch, etc.). The combined physiological state and perceptual state, which may be represented in the animal\u2019s brain, is called the \u2018motivational state\u2019 [6]2. The animal has one compact manifold, which is defined in the physiological state space. This manifold is called the viability zone [16], and we define the state of the animal as \u2018Alive\u2019 when the current physiological state is in the manifold. The adaptive behavior is expressed as an optimal process that steers the physiological state toward the optimal state using the observed data from outside and inside the body."}, {"heading": "A. Formulation of the Survival Problem", "text": "The assumptions of the problem are as shown in Figure 2. Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19]. A unit of the agent consists of an RL agent and a body, and the RL agent interacts with the world through the body. Because the sensors may not perfectly determine the current situation of the world and the physiological state of the body, the sensor may exhibit only partial observability. Since sensing of the physiological state is a process inside of the body, we may be able to assume that there is no information loss with this sensing.\nWe now formalize the survival problem as an optimization problem. We mostly follow the usual definition of the dynamics in the POMDP model explained in the previous section. Like the POMDP model, the agent interacts with the environment. At the state st \u2208 S, the agent receives the current observation ot \u2208 O with probability P (o|s). Then, the agent takes an action at \u2208 A following some policy \u03c0; the state then changes at the next time step following the probability P (st+1|st, at). Because ot is the observation which consists of the temporal stimulus to the agent at the time step t, we can understand that the observation in POMDP is a generalization of the motivational state. In this definition, we do not explicitly separate the observation caused by external stimuli (the perceptual state; vision, touch, etc.) from the observation\n2The terms \u2018physiological state\u2019, \u2018perceptual state\u2019 and \u2018motivational state\u2019 may be misleading in this paper, because these \u201cstates\u201d do not necessarily follow Markovian dynamics.\ncaused by internal stimuli (the observed physiological state; energy level, water level, etc.).\nIn order to formulate the survival problem, we introduce a binary signal At instead of the reward, which represents the \u201calive flag\u201d of the agent at times step t = 0, 1, 2, 3 \u00b7 \u00b7 \u00b7 . At = 1 represents that the agent is \u2018Alive\u2019 at the time step t, and At = 0 represents \u2018Dead\u2019. To generalize the problem, we soften the definition of the boundary of the viability zone by introducing the temporal survival probability P (At+1 = 1|st) > 0. Because the survival probability is ultimately caused by the physiological state of the agent (animal), we assume that the survival probability is only dependent on the current state. However, the following discussion is directly applicable for other definitions of the temporal survival probability, including P (At+1 = 1|ot), P (At+1 = 1|st, at), and so on. Also, we assume A0 = 1, and this probability is known for the agent unit (either in the body or RL agent).\nA natural interpretation of \u2018survival\u2019 is to stay alive as long as possible in the environment, so the agent requires policy \u03c0 that realizes the signal sequence At = 1 (t = 0, 1, . . . ). Using these definitions, the multi-step survival probability given a policy \u03c0 is expressed by a joint probability\nP (A\u0304T |\u03c0), (1)\nwhere A\u0304T denotes the sequence {A0 = 1, A1 = 1, . . . , AT+1 = 1}. We then define the objective of the agent for the survival problem as the maximization of the probability defined by (1)."}, {"heading": "B. Maximization of Survival Probability by Variational Method", "text": "In the following discussion, we show that the maximization of the objective function (1) through maximization of the variational bound can be reduced to maximization of the conventional objective function of RL.\nFirst, we discuss the situation of the planning problem, in which we search for the optimal policy given the true transition probability P (s\u2032|s, a), the observation probability P (o|s), and the temporal survival probability P (A|s). The logarithm of the objective function (1) can be transformed by introducing the arbitrary probability distribution Q(\u03c4) on the T -step trajectory \u03c4 as\nlogP (A\u0304T |\u03c0) = \u2211 \u03c4 Q(\u03c4) logP (A\u0304T |\u03c0)\n= \u2211 \u03c4 Q(\u03c4) log P (A\u0304T , \u03c4 |\u03c0) P (\u03c4 |A\u0304T , \u03c0) Q(\u03c4) Q(\u03c4)\n= \u2211 \u03c4 Q(\u03c4) ( logP (A\u0304T |\u03c4)\u2212 log Q(\u03c4) P (\u03c4 |\u03c0) ) + \u2211 \u03c4 Q(\u03c4) log Q(\u03c4)\nP (\u03c4 |A\u0304T , \u03c0) = \u2212F (Q(\u00b7), P (\u00b7|\u03c0)) + KL(Q(\u00b7)||P(\u00b7|A\u0304T, \u03c0)).\nThe relationship P (A\u0304T |\u03c0)P (\u03c4 |A\u0304T , \u03c0) = P (A\u0304T , \u03c4 |\u03c0) was used at the second equality. P (\u03c4 |A\u0304T , \u03c0) is the posterior of P (\u03c4 |\u03c0) given A\u0304, which is P (\u03c4 |A\u0304T , \u03c0) = P (A\u0304T |\u03c4)P (\u03c4 |\u03c0)\u2211 \u03c4\u2032 P (A\u0304T |\u03c4 \u2032)P (\u03c4 \u2032|\u03c0) from Bayes\u2019s theorem, and P (A\u0304T , \u03c4 |\u03c0) = P (A\u0304T |\u03c4)P (\u03c4 |\u03c0)\nwas used at the third equality. The first term on the RHS in the last row is\n\u2212F (Q(\u00b7), P (\u00b7|\u03c0)) = \u2211 \u03c4 Q(\u03c4) ( logP (A\u0304T |\u03c4)\u2212 log Q(\u03c4) P (\u03c4 |\u03c0) ) ,\nand F is called the free energy from the analogy with the statistical mechanics. If there is some restriction on the distribution Q(\u03c4) (for example, Q(\u03c4) is given by a specific class of the probability distribution), F is called the variational free energy. KL(Q(\u00b7)||P (\u00b7)) is the Kullback-Leibler (KL) divergence KL(Q(\u00b7)||P (\u00b7)) = \u2211 \u03c4 Q(\u03c4) log Q(\u03c4) P (\u03c4) . The KL divergence is known to be non-negative and zero only if the two probability distributions P and Q are equivalent. Because of the nonnegativity of the KL divergence and the above equality, the log-probability logP (A\u0304T |\u03c0) is lower bounded by \u2212F . Hence, this value is termed the variational (lower) bound.\nAlso, from the equality above\nlogP (A\u0304T |\u03c0) + F (Q(\u00b7), P (\u00b7|\u03c0)) = KL(Q(\u00b7)||P(\u00b7|\u0304rT, \u03c0)), and there is a relationship\nargmin Q [F (Q(\u00b7), P (\u00b7|\u03c0))] = argmin Q [KL(Q(\u00b7)||P(\u00b7|A\u0304T, \u03c0))]\nbecause logP (A\u0304T |\u03c0) is not a function of Q. In the maximization problem of the likelihood logP (A\u0304T |\u03c0), the method that introduces the restricted class of Q(\u03c4) and maximizes the variational bound \u2212F is known to be the variational method and it is widely used in machine learning [20]. The probability distribution of the T -step trajectory \u03c4 = {s0, o0, a0, s1, o1, a1, . . . , sT\u22121, oT\u22121, aT\u22121, sT , oT } given a policy \u03c0 is\nP (\u03c4 |\u03c0) = P (o0|s0)P (s0)\n\u00d7 T\u22121\u220f t=0 P (ot+1|st+1)P (st+1|st, at)\u03c0(at|ht).\nAnd we restrict the distribution Q(\u03c4) with arbitrary policy \u03c0Q(a|h) to Q(\u03c4 |\u03c0Q) = P (o0|s0)P (s0)\n\u00d7 T\u22121\u220f t=0 P (ot+1|st+1)P (st+1|st, at)\u03c0Q(at|ht).\nBy using these distributions, we introduce the EM algorithm as Algorithm 1. The maximization in the M-step is simply the replacement of \u03c0k by \u03c0kQ from the equality\nargmax \u03c0\n[\u2212F (Q(\u00b7|\u03c0Q), P (\u00b7|\u03c0))]\n= argmax \u03c0 [\u2211 \u03c4 Q(\u03c4 |\u03c0Q) ( logP (A\u0304T |\u03c4)\u2212 log Q(\u03c4 |\u03c0Q) P (\u03c4 |\u03c0) )] = argmin\n\u03c0 [\u2211 \u03c4 Q(\u03c4 |\u03c0Q) log Q(\u03c4 |\u03c0Q) P (\u03c4 |\u03c0) ] = argmin\n\u03c0 [KL(Q(\u00b7|\u03c0Q)||P (\u00b7|\u03c0))]\n= \u03c0Q.\nBecause of the restriction on Q(\u03c4 |\u03c0Q), the minimization of the KL divergence in the E-step is a variational sense and may\nAlgorithm 1 1. Set k = 0 and an arbitrary policy \u03c00. 2. (E-step) Obtain \u03c0kQ by optimization\n\u03c0kQ = argmin \u03c0Q [KL(Q(\u00b7|\u03c0Q)||P(\u00b7|A\u0304T;\u03c0k))]\n= argmax \u03c0Q\n[\u2212F (Q(\u00b7|\u03c0Q), P (\u00b7|\u03c0k))].\n3. (M-step) Update \u03c0 by using \u03c0kQ. That is\n\u03c0k+1 = argmax \u03c0 [\u2212F (Q(\u00b7|\u03c0kQ), P (\u00b7|\u03c0))]\n= \u03c0kQ.\n4. if \u03c0k+1 is converged, then 5. return \u03c0k+1 6. else 7. k \u2190 k + 1 and go to E-step. 8. end if\nnot be zero. If the environment is MDP and we assume that \u03c0Q is a Markov policy, Rawlik et al. derived the analytical solution of the E-step and it is given by the softmax policy \u03c0Q(a|s) = exp{\u03a8(s, a)}/Z, where \u03a8(s, a) is some energy function and Z is the normalization term [21].\nIn POMDP, on the other hand, no analytical solution to the E-step is known. To tackle this problem, we may parametrize the policies \u03c0 and \u03c0Q by parameters \u03b8, \u03c6 as \u03c0\u03b8, \u03c0\u03c6. And then, we assume \u03b8 = \u03c6\u21d2 \u03c0\u03b8 = \u03c0\u03c6. Therefore \u03b8 = \u03c6\u21d2 P (\u03c4 |\u03c0\u03b8) = Q(\u03c4 |\u03c0\u03c6) by definition. The variational method that introduces the parametrized variational distribution Q\u03c6 and maximizes the variational bound \u2212F (\u03c6, \u03b8) := \u2212F (Q(\u00b7|\u03c0\u03c6), P (\u00b7|\u03c0\u03b8)) by gradient methods are well known in the neural computing community [22]\u2013[25]. Following this idea, we replace the Estep and M-step in Algorithm 1 by\n\u03c6k = argmax \u03c6 [\u2212F (\u03c6, \u03b8k)] (2)\nand\n\u03b8k+1 = argmax \u03b8 [\u2212F (\u03c6k, \u03b8)] = \u03c6k. (3)\nIf each stage of the algorithm is performed exactly, a monotonic increase of the variational bound\n\u2212F (\u03c60, \u03b81) \u2264 \u2212F (\u03c61, \u03b82) \u2264 \u2212F (\u03c62, \u03b83) \u2264 . . .\nafter the M-step is guaranteed. Moreover, there is a relationship after the M-step\n\u2212F (\u03c6k, \u03b8k+1) = \u2212F (\u03b8k+1, \u03b8k+1) = \u2211 \u03c4 P (\u03c4 |\u03c0\u03b8k+1) logP (A\u0304T |\u03c4)\n= \u2211 \u03c4 P (\u03c4 |\u03c0\u03b8k+1) [T\u22121\u2211 t=0 logP (At+1 = 1|st) ]\n= TJT (\u03c0\u03b8k+1)\nin which JT (\u03c0) denotes the objective function of the reinforcement learning with respect to the reward function\nrt = logP (At+1 = 1|st). Here, the equality logP (A\u0304T |\u03c4) =\u2211T\u22121 t=0 logP (At+1 = 1|st) is used at the third equality. Because T is a constant, an increase of the variational bound is equivalent to the increase of JT (\u03c0\u03b8k+1). Therefore, from the discussion above, the maximization of the log-form of the objective function logP (A\u0304T |\u03c0\u03b8) through the variational bound \u2212F (\u03c6, \u03b8) is reduced to the maximization of JT (\u03c0\u03b8) with respect to the parameter of the agent \u03b8."}, {"heading": "C. Solving the Survival Problem by Reinforcement Learning Algorithms", "text": "Now we consider the survival problem in the reinforcement learning setting; that is, the maximization of the log of the objective function logP (A\u0304T |\u03c0) while the agent cannot access the true environment model P (o|s), P (s\u2032|s, a). In this setting, we can not perform the iterative algorithms described above. However, from the discussion of the second algorithm (equation 2, 3), the variational bound is proportional to JT (\u03c0\u03b8) after each M-step. Then, in order to maximize the variational bound, we can take a direct maximization of JT (\u03c0\u03b8) with respect to \u03b8, instead of the exact execution of the iterative algorithm. Because JT (\u03c0\u03b8) with reward function\nrt = logP (At+1 = 1|st)\nis the conventional objective function of the reinforcement learning paradigm, we can apply the RL algorithms to the maximization of the survival probability."}, {"heading": "III. EXPERIMENT", "text": "In the experiment, we verify the reward setting by evaluating the finite horizon survival probability in the simple grid world domain.\na) Environment: The environment consists of a 3 x 3 grid world (Figure 3). The agent selects an action at each time step from UP, DOWN, RIGHT, LEFT and EAT. When the agent takes the action UP, DOWN, RIGHT or LEFT and if the wall is not in that direction, the agent moves one step in the selected direction. Otherwise, the agent stays at the current position. In the environment, there are two types of objects, A and B, at uniformly random positions, such that the two objects never overlap. The position of the objects changes if the agent selects the EAT action at the corresponding position. Also, the position of B may randomly change at every time step with a probability of 0.01.\nThe agent has a continuous battery level E \u2208 [0, 100] that decreases by 1% at each time step. The battery level is\nrecharged +5 if the agent selects EAT at the position of object A. Therefore, the object A corresponds to the food object.\nThe temporal survival probability of the agent when At = 1 is defined as P (At+1 = 1|st) = P (At+1 = 1|Et, Ct) = f(Et)g(Ct) where Et is the battery level at time t = 1, 2, 3, . . . , Ct \u2208 {0, 1} is the flag bit whether the agent ate the object B (C = 1) or not (C = 0). f and g are defined as\nf(Et) = exp { \u2212 (Et \u2212 60) 2\n1000 } and\ng(Ct) =\n{ 0.5 (Ct = 1)\n1 (Ct = 0).\nThe observation of the agent is defined by the set o = {x, pA, pB , c, E\u0302}, where x is the position of the agent, pA is the position of object A, pB is the position of object B, ct is the type of object that the agent EATs (c = 1 for nothing, c = 2 for A, and c = 3 for B), and E\u0302 is the discrete state of the current battery level. In this experiment, we discretized the continuous battery level [0, 100] into 20 discrete regions, and E\u0302 receives the class of the corresponding region of the current battery level E. Therefore, this environment is a simple POMDP setting because of the discretization of the battery level. In order to survive in this environment, the agent must take the food (A), avoid the poisonous object (B) and regulate its energy level (E). Even though this might be an over simplified model of the biological agent, this kind of situation will occur everywhere in the life of animals. Importantly, in this setting, agents initially do not know which object information (pA, pB) corresponds to \u201cfood\u201d or \u201cpoison\u201d. Then the agent has to associate these objects with changes in the homeostatic values (E and c). Also, agents never receive positive rewards when they take food and the reward values for food-capture depend on the agents\u2019 battery level. Therefore, this experiment is fundamentally different from task-oriented problems like \u201cfood capturing\u201d.\nb) Agent Settings: The Sarsa(\u03bb) agent was used in this task and the action-value function in expressed by the tabular function. In this experiment, the learning rate \u03b1 was 0.1, the discount rate \u03b3, 0.95 and the decay rate of the eligibility trace \u03bb, 0.1. The agent follows the -greedy policy in which the action is almost entirely selected by greedy action selection a = argmaxbQ(s, b) but, with a small probability of = 0.01, the action is selected from the uniformly random distribution over the action set. The training procedure of the agent was as follows. An episode starts at the optimal battery level (that is, Bt = 60) with a random allocation of objects in the environment. At each time step, the alive flag is updated according to the temporal survival probability P (At+1|st). If the agent receives At+1 = 0 after the t-th update, the episode ends and the next episode starts.\nc) Results: Figure 4 shows the evolution of the median survival time along the number of episodes. Evaluation was done by freezing parameters of the agent every 1000 episodes, and the agent is tested in 1000 episodes without learning. The solid line represents the median of the survival time of the Sarsa(\u03bb) agent with the survival reward settings described above. The dashed line represents an agent that randomly\nand uniformly selects one action among the 5 actions. The growth of the lifetime clearly shows that the Sarsa(\u03bb) agent successfully learns the survival strategy during the process. The results of the random agent show that the environment has sufficient complexity that the random agent cannot stay alive longer than 25 time steps. Figure 5 shows the battery level of the agent at its death and the median amount of A (left panel) and B (right panel) consumed in the evaluation process after the corresponding episodes. From these figures, we can know that the battery level is successfully controlled around the desired level (E = 60) and that the amount of food eaten (A) increased. On the other hand, the number of poisonous objects (B) eaten is always zero. This result also supports the successful learning of the survival strategy by Sarsa(\u03bb) with only the survival rewards."}, {"heading": "IV. DISCUSSION", "text": "In our approach, we introduced the first \u201cfundamental\u201d reward function of RL for the general survival problem, which so far has been only heuristically defined in previous studies. The key is to soften the definition of the viability zone with the temporal survival probability, so that the reward function is simply the log of the temporal survival probability. Using this setting, the agents can learn the survival policy with respect to the maximization of the survival probability in the future. The source of the reward function, the temporal survival\nprobability, has an explicit meaning and may be obtainable through the evolutionary process. However, even though our reward setting is fundamental for survival, it may not be the \u201coptimal reward\u201d in terms of learning efficiency for the survival policy. It is known that there are reward settings that have the same optimal policy but a different learning speed for the RL agent [26]. And, recently, the pre-training approach for the RL has been successfully applied to the realrobot domain with direct visual image inputs [27]. Therefore, to speed up learning and achieve a robotic agent that fits the unknown-dynamic environment in its (sometimes physical) lifetime, the survival problem should be examined to determine how to equip an agent with moderate prior knowledge of the environment, including the reward function and the state transition dynamics.\nThe relationship between the planning problem and the inference problem is a hot topic in recent machine learning communities [28]\u2013[30]. Vlassis and Toussaint [31] introduced the perspective that model-free reinforcement learning can be treated as an application of a stochastic EM algorithm to the maximization of the mixture likelihood p(R = 1;\u03c0). Our objective function (1) and its lower bound were briefly introduced by Toussaint [32] in the context of a stochastic control problem. In this context, the goal of our study is the maximization of the joint probability (1) from the beginning. Further, we demonstrated the relationships between its lower bound and the EM-based approach, including the POMDP case."}, {"heading": "V. CONCLUSION", "text": "We have discussed the survival problem of the agent in the environment, and have shown that the survival problem can be reduced to an RL problem in (PO)MDPs with a specific reward function. Because of the popularity of the (PO)MDP assumptions in the research of autonomous agents, especially those concerning models of animals, this formulation may be seen as the basis of a truly autonomous agent for survival."}, {"heading": "ACKNOWLEDGMENT", "text": "I would like to thank Makoto Otsuka and Stephany Nix for helpful comments to improve the quality of this paper."}], "references": [{"title": "Design for a Brain", "author": ["W.R. Ashby"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1960}, {"title": "The living brain", "author": ["W. Walter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1953}, {"title": "The design of a fungus-eater: A model of human behavior in an unsophisticated environment", "author": ["M. Toda"], "venue": "Behavioral Science, vol. 7, no. 2, pp. 164\u2013183, 1962.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1962}, {"title": "robot, and society: Models and speculations", "author": ["Man"], "venue": "M. Nijhoff Pub.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1982}, {"title": "Basic cycles, utility and opportunism in self-sufficient robots", "author": ["D. McFarland", "E. Spier"], "venue": "Robotics and Autonomous Systems, vol. 20, no. 2, pp. 179\u2013190, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["L.-J. Lin"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 293\u2013321, 1992.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "On the fitness of behavior sequences", "author": ["R. Sibly", "D. McFarland"], "venue": "American Naturalist, pp. 601\u2013617, 1976.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1976}, {"title": "A reinforcement learning theory for homeostatic regulation", "author": ["M. Keramati", "B.S. Gutkin"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 82\u201390.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "An adaptive robot motivational system", "author": ["G. Konidaris", "A. Barto"], "venue": "From Animals to Animats 9. Springer, 2006, pp. 346\u2013356.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Emergence of robot behavior based on selfpreservation. research methodology and embodiment of mechanical system", "author": ["T. Ogata", "S. Sugano"], "venue": "Journal of the Robotics Society of Japan, vol. 15, no. 5, pp. 710\u2013721, 1997.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "The cyber rodent project: Exploration of adaptive mechanisms for self-preservation and self-reproduction", "author": ["K. Doya", "E. Uchibe"], "venue": "Adaptive Behavior, vol. 13, no. 2, pp. 149\u2013160, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Biologically inspired embodied evolution of survival", "author": ["S. Elfwing", "E. Uchibe", "K. Doya", "H.I. Christensen"], "venue": "Evolutionary Computation, 2005. The 2005 IEEE Congress on, vol. 3. IEEE, 2005, pp. 2210\u20132216.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "arXiv preprint cs/9605103, 1996.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Simulation of adaptive behavior in animats: Review and prospect", "author": ["J.-A. Meyer", "A. Guillot"], "venue": "In J.-A. Meyer and S.W. Wilson (Eds.) From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, 1991, pp. 2\u201314.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Modeling motivations and emotions as a basis for intelligent behavior", "author": ["D. Ca\u00f1amero"], "venue": "Proceedings of the first international conference on Autonomous agents. ACM, 1997, pp. 148\u2013155.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A.G. Barto", "S. Singh", "N. Chentanez"], "venue": "Proc. 3rd Int. Conf. Development Learn, 2004, pp. 112\u2013119.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning, vol. 37, no. 2, pp. 183\u2013233, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "On stochastic optimal control and reinforcement learning by approximate inference", "author": ["K. Rawlik", "M. Toussaint", "S. Vijayakumar"], "venue": "Proceedings of the Twenty-Third international joint conference on Artificial Intelligence. AAAI Press, 2013, pp. 3052\u20133056.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Varieties of helmholtz machine", "author": ["P. Dayan", "G.E. Hinton"], "venue": "Neural Networks, vol. 9, no. 8, pp. 1385\u20131403, 1996.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1996}, {"title": "Black box variational inference", "author": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "arXiv preprint arXiv:1401.0118, 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "arXiv preprint arXiv:1402.0030, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "ICML, vol. 99, 1999, pp. 278\u2013287.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Autonomous reinforcement learning on raw visual input data in a real world application", "author": ["S. Lange", "M. Riedmiller", "A. Voigtlander"], "venue": "Neural Networks (IJCNN), The 2012 International Joint Conference on. IEEE, 2012, pp. 1\u20138.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic inference for solving (po) mdps", "author": ["M. Toussaint", "S. Harmeling", "A. Storkey"], "venue": "University of Edinburgh, Informatics Research Report 0934, 2006.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "General duality between optimal control and estimation", "author": ["E. Todorov"], "venue": "Decision and Control, 2008. CDC 2008. 47th IEEE Conference on. IEEE, 2008, pp. 4286\u20134292.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimal control as a graphical model inference problem", "author": ["H.J. Kappen", "V. G\u00f3mez", "M. Opper"], "venue": "Machine learning, vol. 87, no. 2, pp. 159\u2013 182, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Model-free reinforcement learning as mixture learning", "author": ["N. Vlassis", "M. Toussaint"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 1081\u20131088.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Robot trajectory optimization using approximate inference", "author": ["M. Toussaint"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 2009, pp. 1049\u20131056.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Ashby developed Homeostat, which dynamically stabilizes the state of the machine [1], and Walter developed simple robotic agents that can explore the environment of a room and automatically recharge their batteries at a recharging station [2].", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Ashby developed Homeostat, which dynamically stabilizes the state of the machine [1], and Walter developed simple robotic agents that can explore the environment of a room and automatically recharge their batteries at a recharging station [2].", "startOffset": 239, "endOffset": 242}, {"referenceID": 2, "context": "Toda discussed the survival problem of artificial agents in the natural environment [3], [4].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Toda discussed the survival problem of artificial agents in the natural environment [3], [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "They suggested several requirements for intelligent agents through comparison of the market economy with natural selection, and they also developed simple robots that were self-sufficient, autonomous agents [7].", "startOffset": 207, "endOffset": 210}, {"referenceID": 5, "context": "Also, Lin, in a simulation study, compared several reinforcement learning (RL) architectures for a complex survival task in a non-Markovian environment [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "Sibly and McFarland introduced the state space approach in ethology and suggested that animal behaviors are the consequence of optimal control with respect to the cost function given by the fitness function [9].", "startOffset": 207, "endOffset": 210}, {"referenceID": 7, "context": "Keramati and Gutkin suggested a similar perspective, but they also suggested changes in the distance between the current homeostatic state and the optimal, desired state as the reward function of RL [10].", "startOffset": 199, "endOffset": 203}, {"referenceID": 8, "context": ") through tuning of the reward function, which depends on the agent\u2019s homeostatic state [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "Ogata and Sugino developed a real robot agent intended for survival [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "Doya and Uchibe developed robots called \u201cCyber rodents\u201d intended for the study of learning agents for survival and evolutionary robotics [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "The wireless communication modules of robots enable the software evolution of control algorithms [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "Many realistic environments for the agent are known to be modeled by the partially observable Markov decision process (POMDP) [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 0, "context": "In this section, we formulate the survival problem from the models of an animal proposed by Ashby [1] and a similar idea suggested by Sibly and McFarland [9] and McFarland and B\u00f6user [6] from the view point of ethology.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "In this section, we formulate the survival problem from the models of an animal proposed by Ashby [1] and a similar idea suggested by Sibly and McFarland [9] and McFarland and B\u00f6user [6] from the view point of ethology.", "startOffset": 154, "endOffset": 157}, {"referenceID": 13, "context": "This manifold is called the viability zone [16], and we define the state of the animal as \u2018Alive\u2019 when the current physiological state is in the manifold.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19].", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19].", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "Similar settings have been suggested elsewhere [1], [11], [17]\u2013 [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 16, "context": "In the maximization problem of the likelihood logP (\u0100T |\u03c0), the method that introduces the restricted class of Q(\u03c4) and maximizes the variational bound \u2212F is known to be the variational method and it is widely used in machine learning [20].", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": "derived the analytical solution of the E-step and it is given by the softmax policy \u03c0Q(a|s) = exp{\u03a8(s, a)}/Z, where \u03a8(s, a) is some energy function and Z is the normalization term [21].", "startOffset": 180, "endOffset": 184}, {"referenceID": 18, "context": "The variational method that introduces the parametrized variational distribution Q\u03c6 and maximizes the variational bound \u2212F (\u03c6, \u03b8) := \u2212F (Q(\u00b7|\u03c0\u03c6), P (\u00b7|\u03c0\u03b8)) by gradient methods are well known in the neural computing community [22]\u2013[25].", "startOffset": 225, "endOffset": 229}, {"referenceID": 21, "context": "The variational method that introduces the parametrized variational distribution Q\u03c6 and maximizes the variational bound \u2212F (\u03c6, \u03b8) := \u2212F (Q(\u00b7|\u03c0\u03c6), P (\u00b7|\u03c0\u03b8)) by gradient methods are well known in the neural computing community [22]\u2013[25].", "startOffset": 230, "endOffset": 234}, {"referenceID": 22, "context": "It is known that there are reward settings that have the same optimal policy but a different learning speed for the RL agent [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "And, recently, the pre-training approach for the RL has been successfully applied to the realrobot domain with direct visual image inputs [27].", "startOffset": 138, "endOffset": 142}, {"referenceID": 24, "context": "The relationship between the planning problem and the inference problem is a hot topic in recent machine learning communities [28]\u2013[30].", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "The relationship between the planning problem and the inference problem is a hot topic in recent machine learning communities [28]\u2013[30].", "startOffset": 131, "endOffset": 135}, {"referenceID": 27, "context": "Vlassis and Toussaint [31] introduced the perspective that model-free reinforcement learning can be treated as an application of a stochastic EM algorithm to the maximization of the mixture likelihood p(R = 1;\u03c0).", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "Our objective function (1) and its lower bound were briefly introduced by Toussaint [32] in the context of a stochastic control problem.", "startOffset": 84, "endOffset": 88}], "year": 2016, "abstractText": "Obtaining a survival strategy (policy) is one of the fundamental problems of biological agents. In this paper, we generalize the formulation of previous research related to the survival of an agent and we formulate the survival problem as a maximization of the multi-step survival probability in future time steps. We introduce a method for converting the maximization of multi-step survival probability into a classical reinforcement learning problem. Using this conversion, the reward function (negative temporal cost function) is expressed as the log of the temporal survival probability. And we show that the objective function of the reinforcement learning in this sense is proportional to the variational lower bound of the original problem. Finally, We empirically demonstrate that the agent learns survival behavior by using the reward function introduced in this paper.", "creator": "LaTeX with hyperref package"}}}