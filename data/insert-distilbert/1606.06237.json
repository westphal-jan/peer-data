{"id": "1606.06237", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Online and Differentially-Private Tensor Decomposition", "abstract": "tensor resolution decomposition is positioned to be a pervasive tool in the era of big data. in this paper, typically we best resolve many questions of the key dimensional algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. naturally we propose simple stability variants of the tensor power method which enjoy these historically strong properties. we present the first guarantees for online tensor power method which has a linear memory requirement. moreover, we present a noise calibrated tensor power efficiency method with efficient privacy update guarantees. at the tender heart of all these guarantees lies a careful perturbation decomposition analysis derived in this paper which improves up on the existing results significantly.", "histories": [["v1", "Mon, 20 Jun 2016 18:30:10 GMT  (205kb,D)", "https://arxiv.org/abs/1606.06237v1", "20 pages, 9 figures"], ["v2", "Sat, 2 Jul 2016 22:30:01 GMT  (205kb,D)", "http://arxiv.org/abs/1606.06237v2", "20 pages, 9 figures"], ["v3", "Sun, 30 Oct 2016 21:56:58 GMT  (209kb,D)", "http://arxiv.org/abs/1606.06237v3", "19 pages, 9 figures. To appear at the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS 2016), to be held at Barcelona, Spain"], ["v4", "Thu, 15 Dec 2016 13:35:22 GMT  (208kb,D)", "http://arxiv.org/abs/1606.06237v4", "19 pages, 9 figures. To appear at the 30th Annual Conference on Advances in Neural Information Processing Systems (NIPS 2016), to be held at Barcelona, Spain. Fix small typos in proofs of Lemmas C.5 and C.6"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yining wang", "anima anandkumar"], "accepted": true, "id": "1606.06237"}, "pdf": {"name": "1606.06237.pdf", "metadata": {"source": "CRF", "title": "Online and Differentially-Private Tensor Decomposition", "authors": ["Yining Wang", "Animashree Anandkumar"], "emails": ["yiningwa@cs.cmu.edu", "a.anandkumar@uci.edu"], "sections": [{"heading": null, "text": "Keywords: Tensor decomposition, tensor power method, online methods, streaming, differential privacy, perturbation analysis."}, {"heading": "1 Introduction", "text": "In recent years, tensor decomposition has emerged as a powerful tool to solve many challenging problems in unsupervised [1], supervised [18] and reinforcement learning [4]. Tensors are higher order extensions of matrices which can reveal far greater information compared to matrices, while retaining most of the efficiencies of matrix operations [1].\nA central task in tensor analysis is the process of decomposing the tensor into its rank-1 components, which is usually referred to as CP (Candecomp/Parafac) decomposition in the literature. While decomposition of arbitrary tensors is NP-hard [13], it becomes tractable for the class of tensors with linearly independent components. Through a simple whitening procedure, such tensors can be converted to orthogonally decomposable tensors. Tensor power method is a popular method for computing the decomposition of an orthogonal tensor. It is simple and efficient to implement, and a natural extension of the matrix power method.\nIn the absence of noise, the tensor power method correctly recovers the components under a random initialization followed by deflation. On the other hand, perturbation analysis of tensor power method is much more delicate compared to the matrix case. This is because the problem of tensor decomposition is NP-hard, and if a large amount of arbitrary noise is added to an orthogonal tensor, the decomposition can again become intractable. In [1], guaranteed recovery of components was proven under bounded noise, and the bound was improved in [2]. In this paper, we significantly improve upon the noise requirements, i.e. the extent of noise that can be withstood by the tensor power method.\nIn order for tensor methods to be deployed in large-scale systems, we require fast, parallelizable and scalable algorithms. To achieve this, we need to avoid the exponential increase in computation and memory requirements with the order of the tensor; i.e. a naive implementation on a 3rd-order d-dimensional tensor would require O(d3) computation and memory. Instead, we analyze the online tensor power method that requires only linear (in d) memory and does not form the entire tensor. This is achieved in settings, where the tensor is an empirical higher order moment, computed from the stream of data samples. We can avoid explicit construction of the tensor by running online tensor\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n06 23\n7v 4\n[ st\nat .M\nL ]\n1 5\nD ec\n2 01\npower method directly on i.i.d. data samples. We show that this algorithm correctly recovers tensor components in time1 O\u0303(nk2d) and O\u0303(dk) memory for a rank-k tensor and n number of data samples. Additionally, we provide efficient sample complexity analysis.\nAs spectral methods become increasingly popular with recommendation system and health analytics applications [29, 17], data privacy is particularly relevant in the context of preserving sensitive private information. Differential privacy could still be useful even if data privacy is not the prime concern [30]. We propose the first differentially private tensor decomposition algorithm with both privacy and utility guarantees via noise calibrated power iterations. We show that under the natural assumption of tensor incoherence, privacy parameters have no (polynomial) dependence on tensor dimension d. On the other hand, straightforward input perturbation type methods lead to far worse bounds and do not yield guaranteed recovery for all values of privacy parameters."}, {"heading": "1.1 Related work", "text": "Online tensor SGD Stochastic gradient descent (SGD) is an intuitive approach for online tensor decomposition and has been successful in practical large-scale tensor decomposition problems [16]. Despite its simplicity, theoretical properties are particularly hard to establish. [11] considered a variant of the SGD objective and proved its correctness. However, the approach in [11] only works for even-order tensors and its sample complexity dependency upon tensor dimension d is poor.\nTensor PCA In the statistical tensor PCA [24] model a d\u00d7d\u00d7d tensor T = v\u22973+E is observed and one wishes to recover component v under the presence of Gaussian random noise E. [24] shows that \u2016E\u2016op = O(d\u22121/2) is sufficient to guarantee approximate recovery of v and [14] further improves the noise condition to \u2016E\u2016op = O(d\u22121/4) via a 4th-order sum-of-squares relaxation. Techniques in both [24, 14] are rather complicated and could be difficult to adapt to memory or privacy constraints. Furthermore, in [24, 14] only one component is considered. On the other hand, [25] shows that \u2016E\u2016op = O(d\u22121/2) is sufficient for recovering multiple components from noisy tensors. However, [25] assumes exact computation of rank-1 tensor approximation, which is NP-hard in general.\nNoisy matrix power methods Our relaxed noise condition analysis for tensor power method is inspired by recent analysis of noisy matrix power methods [12, 6]. Unlike the matrix case, tensor decomposition no longer requires spectral gap among eigenvalues and eigenvectors are usually recovered one at a time [1, 2]. This poses new challenges and requires non-trivial extensions of matrix power method analysis to the tensor case."}, {"heading": "1.2 Notation and Preliminaries", "text": "We use [n] to denote the set {1, 2, \u00b7 \u00b7 \u00b7 , n}. We use bold characters A,T,v for matrices, tensors, vectors and normal characters \u03bb, \u00b5 for scalars. A pth order tensor T of dimensions d1, \u00b7 \u00b7 \u00b7 , dp has d1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 dp elements, each indexed by a p-tuple (i1, \u00b7 \u00b7 \u00b7 , ip) \u2208 [d1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [dp]. A tensor T of dimensions d\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 d is super-symmetric or simply symmetric if Ti1,\u00b7\u00b7\u00b7 ,ip = T\u03c3(i1),\u00b7\u00b7\u00b7 ,\u03c3(ip) for all permutations \u03c3 : [p] \u2192 [p]. For a tensor T \u2208 Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dp and matrices A1 \u2208 Rm1\u00d7d1 , \u00b7 \u00b7 \u00b7 ,Ap \u2208 Rmp\u00d7dp , the multi-linear form T(A1, \u00b7 \u00b7 \u00b7 ,Ap) is a m1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7mp tensor defined as\n[T(A1, \u00b7 \u00b7 \u00b7 ,Ap)]i1,\u00b7\u00b7\u00b7 ,ip = \u2211\nj1\u2208[d1]\n\u00b7 \u00b7 \u00b7 \u2211\njp\u2208[dp]\nTj1,\u00b7\u00b7\u00b7 ,jp [A1]j1,i1 \u00b7 \u00b7 \u00b7 [Ap]jp,ip .\nWe use \u2016v\u20162 = \u221a\u2211 i v 2 i for vector 2-norm and \u2016v\u2016\u221e = maxi |vi| for vector infinity norm. We use \u2016T\u2016op to denote the operator norm or spectral norm of a tensor T, which is defined as \u2016T\u2016op = sup\u2016u1\u20162=\u00b7\u00b7\u00b7\u2016up\u20162=1 T(u1, \u00b7 \u00b7 \u00b7 ,up). An event A is said to occur with overwhelming probability if Pr[A] \u2265 1\u2212 d\u221210. We limit ourselves to symmetric 3rd-order tensors (p = 3) in this paper. The results can be directly extended to asymmetric tensors since they can first be symmetrized using simple matrix operations (see [1]). Extension to higher-order tensors is also straightforward. A symmetric 3rd-order tensor T is rank-1 if it can be written in the form of\nT = \u03bb \u00b7 v \u2297 v \u2297 v = \u03bbv\u22973 \u21d0\u21d2 Ti,j,` = \u03bb \u00b7 v(i) \u00b7 v(j) \u00b7 v(`), (1) 1O\u0303 hides poly-logarithmic factors.\nAlgorithm 1 Robust tensor power method [1]\n1: Input: symmetric d\u00d7 d\u00d7 d tensor T\u0303, number of components k \u2264 d, number of iterations L, R. 2: for i = 1 to k do 3: Initialization: Draw u0 uniformly at random from the unit sphere in Rd. 4: Power iteration: Compute ut = T\u0303(I,ut\u22121,ut\u22121)/\u2016T\u0303(I,ut\u22121,ut\u22121)\u20162 for t = 1, \u00b7 \u00b7 \u00b7 , R. 5: Boosting: Repeat Steps 3 and 4 for L times and obtain u(1)R , \u00b7 \u00b7 \u00b7 ,u (L) R . Let \u03c4 \u2217 =\nargmaxL\u03c4=1T\u0303(u (\u03c4) R ,u (\u03c4) R ,u (\u03c4) R ). Set v\u0302i = u (\u03c4) R and \u03bb\u0302i = T\u0303(u (\u03c4) R ,u (\u03c4) R ,u (\u03c4) R ).\n6: Deflation: T\u0303\u2190 T\u0303\u2212 \u03bb\u0302iv\u0302\u22973i . 7: end for 8: Output: Estimated eigenvalue/Eigenvector pairs {\u03bb\u0302i, v\u0302i}ki=1.\nwhere \u2297 represents the outer product, and v \u2208 Rd is a unit vector (i.e., \u2016v\u20162 = 1) and \u03bb \u2208 R+. 2 A tensor T \u2208 Rd\u00d7d\u00d7d is said to have a CP (Candecomp/Parafac) rank k if it can be (minimally) written as the sum of k rank-1 tensors:\nT = \u2211 i\u2208[k] \u03bbivi \u2297 vi \u2297 vi, \u03bbi \u2208 R+, vi \u2208 Rd. (2)\nA tensor is said to be orthogonally decomposable if in the above decomposition \u3008vi,vj\u3009 = 0 for i 6= j. Any tensor can be converted to an orthogonal tensor through an invertible whitening transform, provided that v1,v2, . . . ,vk are linearly independent [1]. We thus limit our analysis to orthogonal tensors in this paper since it can be extended to this more general class in a straightforward manner.\nTensor Power Method: A popular algorithm for finding the tensor decomposition in (2) is through the tensor power method. The full algorithm is given in Algorithm 1. We first provide an improved noise analysis for the robust power method, improving error tolerance bounds previously established in [1]. We next propose memory-efficient and/or differentially private variants of the robust power method and give performance guarantee based on our improved noise analysis."}, {"heading": "2 Improved Noise Analysis for Tensor Power Method", "text": "When the tensor T has an exact orthogonal decomposition, the power method provably recovers all the components with random initialization and deflation. However, the analysis is more subtle under noise. While matrix perturbation bounds are well understood, it is an open problem in the case of tensors. This is because the problem of tensor decomposition is NP-hard, and becomes tractable only under special conditions such as orthogonality (and more generally linear independence). If a large amount of arbitrary noise is added, the decomposition can again become intractable. In [1], guaranteed recovery of components was proven under bounded noise and we recap the result below.\nTheorem 2.1 ([1] Theorem 5.1, simplified version). Suppose T\u0303 = T+\u2206T , where T = \u2211k i=1 \u03bbiv \u22973 i with \u03bbi > 0 and orthonormal basis vectors{v1, \u00b7 \u00b7 \u00b7 ,vk} \u2286 Rd, d \u2265 k, and noise \u2206T satisfies \u2016\u2206T \u2016op \u2264 . Let \u03bbmax, \u03bbmin be the largest and smallest values in {\u03bbi}ki=1 and {\u03bb\u0302i, v\u0302i}ki=1 be outputs of Algorithm 1. There exist absolute constants K0, C1, C2, C3 > 0 such that if\n\u2264 C1 \u00b7\u03bbmin/d, R = \u2126(log d+log log(\u03bbmax/ )), L = \u2126(max{K0, k} log(max{K0, k})), (3)\nthen with probability at least 0.9, there exists a permutation \u03c0 : [k]\u2192 [k] such that\n|\u03bbi \u2212 \u03bb\u0302\u03c0(i)| \u2264 C2 , \u2016vi \u2212 v\u0302\u03c0(i)\u20162 \u2264 C3 /\u03bbi, \u2200i = 1, \u00b7 \u00b7 \u00b7 , k.\nTheorem 2.1 is the first provably correct result on robust tensor decomposition under general noise conditions. In particular, the noise term \u2206T can be deterministic or even adversarial. However, one important drawback of Theorem 2.1 is that \u2016\u2206T \u2016op must be upper bounded by O(\u03bbmin/d), which is a strong assumption for many practical applications [28]. On the other hand, [2, 24] show that by using smart initializations the robust tensor power method is capable of tolerating O(\u03bbmin/ \u221a d)\n2One can always assume without loss of generality that \u03bb \u2265 0 by replacing v with \u2212v instead.\nmagnitude of noise, and [25] suggests that such noise magnitude cannot be improved if deflation (i.e., successive rank-one approximation) is to be performed. In this paper, we show that the relaxed noise bound O(\u03bbmin/ \u221a d) holds even if the initialization of robust TPM is as simple as a vector uniformly sampled from the d-dimensional sphere (Algorithm 1). Our claim is formalized below:\nTheorem 2.2 (Improved noise tolerance analysis for robust TPM). Assume the same notation as in Theorem 2.1. Let \u2208 (0, 1/2) be an error tolerance parameter. There exist absolute constants K0, C0, C1, C2, C3 > 0 such that if \u2206T satisfies\n\u2016\u2206T (I,u(\u03c4)t ,u (\u03c4) t )\u20162 \u2264 , |\u2206T (vi,u (\u03c4) t ,u (\u03c4) t )| \u2264 min{ / \u221a k,C0\u03bbmin/d} (4)\nfor all i \u2208 [k], t \u2208 [T ], \u03c4 \u2208 [L] and furthermore\n\u2264 C1 \u00b7 \u03bbmin/ \u221a k, R = \u2126(log(\u03bbmaxd/ )), L = \u2126(max{K0, k} log(max{K0, k})), (5)\nthen with probability at least 0.9, there exists a permutation \u03c0 : [k]\u2192 [k] such that\n|\u03bbi \u2212 \u03bb\u0302\u03c0(i)| \u2264 C2 , \u2016vi \u2212 v\u0302\u03c0(i)\u20162 \u2264 C3 /\u03bbi, \u2200i = 1, \u00b7 \u00b7 \u00b7 , k.\nDue to space constraints, proof of Theorem 2.2 is placed in Appendix C. We next make several remarks on our results. In particular, we consider three scenarios with increasing assumptions imposed on the noise tensor \u2206T and compare the noise conditions in Theorem 2.2 with existing results on orthogonal tensor decomposition:\n1. \u2206T does not have any special structure: in this case, we only have |\u2206T (vi,ut,ut)| \u2264 \u2016\u2206T \u2016op and our noise conditions reduces to the classical one: \u2016\u2206T \u2016op = O(\u03bbmin/d). 2. \u2206T is \u201cround\u201d in the sense that |\u2206T (vi,ut,ut)| \u2264 O(1/ \u221a d) \u00b7 \u2016\u2206T (I,ut,ut)\u20162: this is\nthe typical setting when the noise \u2206T follows Gaussian or sub-Gaussian distributions, as we explain in Sec. 3 and 4. Our noise condition in this case is \u2016\u2206T \u2016op = O(\u03bbmin/ \u221a d), strictly improving Theorem 2.1 on robust tensor power method with random initializations and matching the bound for more advanced SVD initialization techniques in [2].\n3. \u2206T is weakly correlated with signal in the sense that \u2016\u2206T (vi, I, I)\u20162 = O(\u03bbmin/d) for all i \u2264 k: in this case our noise condition reduces to \u2016\u2206T \u2016op = O(\u03bbmin/ \u221a k), strictly\nimproving over SVD initialization [2] in the \u201cundercomplete\u201d regime k = o(d). Note that the whitening trick [3, 1] does not attain our bound, as we explain in Appendix B.\nFinally, we remark that the log log(1/ ) quadratic convergence rate in Eq. (3) is worsened to log(1/ ) linear rate in Eq. (5). We are not sure whether this is an artifact of our analysis, because similar analysis for the matrix noisy power method [12] also reveals a linear convergence rate.\nImplications Our bounds in Theorem 2.2 results in sharper analysis of both memory-efficient and differentially private power methods which we propose in Sec. 3, 4. Using the original analysis (Theorem 2.1) for the two applications, the memory-efficient tensor power method would have sample complexity cubic in the dimension d and for differentially private tensor decomposition the privacy level \u03b5 needs to scale as \u2126\u0303( \u221a d) as d increases, which is particularly bad as the quality of privacy protection e\u03b5 degrades exponentially with tensor dimension d. On the other hand, our improved noise condition in Theorem 2.2 greatly sharpens the bounds in both applications: for memory efficient decomposition, we now require only quadratic sample complexity and for differentially private decomposition, the privacy level \u03b5 has no polynomial dependence on d. This makes our results far more practical for high-dimensional tensor decomposition applications.\nNumerical verification of noise conditions and comparison with whitening techniques We verify our improved noise conditions for robust tensor power method on simulation tensor data. In particular, we consider three noise models and demonstrate varied asymptotic noise magnitudes at which tensor power method succeeds. The simulation results nicely match our theoretical findings and also suggest, in an empirical way, tightness of noise bounds in Theorem 2.2. Due to space constraints, simulation results are placed in Appendix A.\nWe also compare our improved noise bound with those obtained by whitening, a popular technique that reduces tensor decomposition to matrix decomposition problems [1, 21, 28]. We show in Appendix B that, without side information the standard analysis of whitening based tensor decomposition leads to worse noise tolerance bounds than what we obtained in Theorem 2.2."}, {"heading": "3 Memory-Efficient Streaming Tensor Decomposition", "text": "Tensor power method in Algorithm 1 requires significant storage to be deployed: \u2126(d3) memory is required to store a dense d \u00d7 d \u00d7 d tensor, which is prohibitively large in many real-world applications as tensor dimension d could be really high. We show in this section how to compute tensor decomposition in a memory efficient manner, with storage scaling linearly in d. In particular, we consider the case when tensor T to be decomposed is a population moment Ex\u223cD[x\u22973] with respect to some unknown underlying data distribution D, and data points x1,x2, \u00b7 \u00b7 \u00b7 i.i.d. sampled fromD are fed into a tensor decomposition algorithm in a streaming fashion. One classical example is topic modeling, where each xi represents documents that come in streams and consistent estimation of topics can be achieved by decomposing variants of the population moment [1, 3].\nAlgorithm 2 displays memory-efficient tensor decomposition procedure on streaming data points. The main idea is to replace the power iteration step T(I,u,u) in Algorithm 1 with a \u201cdata association\u201d step that exploits the empirical-moment structure of the tensor T to be decomposed and evaluates approximate power iterations from stochastic data samples. This procedure is highly efficient, in that both time and space complexity scale linearly with tensor dimension d: Proposition 3.1. Algorithm 2 runs in O(nkdLR) time and O(d(k + L)) memory, with O(nkR) sample complexity (number of data point gone through).\nIn the remainder of this section we show Algorithm 2 recovers eigenvectors of the population moment Ex\u223cD[x\u22973] with high probability and we derive corresponding sample complexity bounds. To facilitate our theoretical analysis we need several assumptions on the data distribution D. The first natural assumption is the low-rankness of the population moment Ex\u223cD[x\u22973] to be decomposed: Assumption 3.1 (Low-rank moment). The mean tensor T = Ex\u223cD[x\u22973] admits a low-rank representation T = \u2211k i=1 \u03bbiv \u22973 i for \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbk > 0 and orthonormal {v1, \u00b7 \u00b7 \u00b7 ,vk} \u2286 Rd.\nWe also place restrictions on the \u201cnoise model\u201d, which imply that the population moment Ex\u223cD[x\u22973] can be well approximated by a reasonable number of samples with high probability. In particular, we consider sub-Gaussian noise as formulated in Definition 3.1 and Assumption 3.2: Definition 3.1 (Multivariate sub-Gaussian distribution, [15]). A D-dimensional random variable x belongs to the sub-Gaussian distribution family SGD(\u03c3) with parameter \u03c3 > 0 if it has zero mean and E [ exp(a>x) ] \u2264 exp { \u2016a\u201622\u03c32/2 } for all a \u2208 RD. Assumption 3.2 (Sub-Gaussian noise). There exists \u03c3 > 0 such that the mean-centered vectorized random variable vec(x\u22973 \u2212 E[x\u22973]) belongs to SGd3(\u03c3) as defined in Definition 3.1.\nWe remark that Assumption 3.2 includes a wide family of distributions that are of practical importance, for example noise that have compact support. Assumption 3.2 also resembles (B, p)-round noise considered in [12] that imposes spherical symmetry constraints onto the noise distribution.\nWe are now ready to present the main theorem that bounds the recovery (approximation) error of eigenvalues and eigenvectors of the streaming robust tensor power method in Algorithm 2: Theorem 3.1 (Analysis of streaming robust tensor power method). Let Assumptions 3.1, 3.2 hold true and suppose < C1\u03bbmin/ \u221a k for some sufficiently small absolute constant C1 > 0. If\nn = \u2126\u0303 ( min { \u03c32d\n2 , \u03c32d2\n\u03bb2min\n}) , R = \u2126(log(\u03bbmaxd/ )), L = \u2126(k log k),\nthen with probability at least 0.9 there exists permutation \u03c0 : [k]\u2192 [k] such that\n|\u03bbi \u2212 \u03bb\u0302\u03c0(i)| \u2264 C2 , \u2016vi \u2212 v\u0302\u03c0(i)\u20162 \u2264 C3 /\u03bbi, \u2200i = 1, \u00b7 \u00b7 \u00b7 , k for some universal constants C2, C3 > 0.\nCorollary 3.1 is then an immediate consequence of Theorem 3.1, which simplifies the bounds and highlights asymptotic dependencies over important model parameters d, k and \u03c3:\nAlgorithm 2 Online robust tensor power method 1: Input: data stream x1,x2, \u00b7 \u00b7 \u00b7 \u2208 Rd, no. of components k, parameters L,R, n. 2: for i = 1 to k do 3: Draw u(1)0 , \u00b7 \u00b7 \u00b7 ,u (L) 0 i.i.d. uniformly at random from the unit sphere Sd\u22121.\n4: for t = 0 to R\u2212 1 do 5: Initialization: Set accumulators u\u0303(1)t+1, \u00b7 \u00b7 \u00b7 , u\u0303 (L) t+1 and \u03bb\u0303\n(1), \u00b7 \u00b7 \u00b7 , \u03bb\u0303(L) to 0. 6: Data association: Read the next n data points; update u\u0303(\u03c4)t+1 \u2190 u\u0303 (\u03c4) t+1 + 1 n (x > ` u (\u03c4) t ) 2xi\nand \u03bb\u0303(\u03c4) \u2190 \u03bb\u0303(\u03c4) + 1n (x > ` u (\u03c4) t ) 3 for each ` \u2208 [n] and \u03c4 \u2208 [L]. 7: Deflation: For each \u03c4 \u2208 [L], update u\u0303(\u03c4)t+1 \u2190 u\u0303 (\u03c4) t+1 \u2212 \u2211i\u22121 j=1 \u03bb\u0302j\u03be 2 j,\u03c4 v\u0302j\nand \u03bb\u0303(\u03c4) \u2190 \u03bb\u0303(\u03c4) \u2212 \u2211i\u22121 j=1 \u03bb\u0302j\u03be 3 j,\u03c4 , where \u03bej,\u03c4 = v\u0302 > j u\u0303 (\u03c4) t .\n8: Normalization: u(\u03c4)t+1 = u\u0303 (\u03c4) t+1/\u2016u\u0303 (\u03c4) t+1\u20162, for each \u03c4 \u2208 [L].\n9: end for 10: Find \u03c4\u2217 = argmax\u03c4\u2208[L]\u03bb\u0303 (\u03c4) and store \u03bb\u0302i = \u03bb\u0303(\u03c4 \u2217), v\u0302i = u (\u03c4\u2217) R . 11: end for 12: Output: approximate eigenvalue and eigenvector pairs {\u03bb\u0302i, v\u0302i}ki=1 of E\u0302x\u223cD[x\u22973].\nCorollary 3.1. Under Assumptions 3.1, 3.2, Algorithm 2 correctly learns {\u03bbi,vi}ki=1 up toO(1/ \u221a d) additive error with O\u0303(\u03c32kd2) samples and O\u0303(dk) memory.\nProofs of Theorem 3.1 and Corollary 3.1 are both deferred to Appendix D. Compared to streaming noisy matrix PCA considered in [12], the bound is weaker with an additional 1/k factor in the term involving and 1/d factor in the term that does not involve . We conjecture this to be a fundamental difficulty of the tensor decomposition problem. On the other hand, our bounds resulting from the analysis in Sec. 2 have a O(1/d) improvement compared to applying existing analysis in [1] directly.\nRemark on comparison with SGD: Our proposed streaming tensor power method is nothing but the projected stochastic gradient descent (SGD) procedure on the objective of maximizing the tensor norm on the sphere. The optimal solution of this coincides with the objective of finding the best rank-1 approximation of the tensor. Here, we can estimate all the components of the tensor through deflation. An alternative method is to run SGD based a combined objective function to obtain all the components of the tensor simultaneously, as considered in [16, 11]. However, the analysis in [11] only works for even-order tensors and has worse dependency (at least d9) on tensor dimension d."}, {"heading": "4 Differentially private tensor decomposition", "text": "The objective of private data processing is to release data summaries such that any particular entry of the original data cannot be reliably inferred from the released results. Formally speaking, we adopt the popular (\u03b5, \u03b4)-differential privacy criterion proposed in [9]: Definition 4.1 ((\u03b5, \u03b4)-differential privacy [9]). Let M denote all symmetric d-dimensional real third order tensors and O be an arbitrary output set. A randomized algorithm A : M \u2192 O is (\u03b5, \u03b4)-differentially private if for all neighboring tensors T,T\u2032 and measurable set O \u2286 O we have\nPr [A(T) \u2208 O] \u2264 e\u03b5 Pr [A(T\u2032) \u2208 O] + \u03b4,\nwhere \u03b5 > 0, \u03b4 \u2208 [0, 1) are privacy parameters and probabilities are taken over randomness in A.\nSince our tensor decomposition analysis concerns symmetric tensors primarily, we adopt a \u201csymmetric\u201d definition of neighboring tensors in Definition 4.1, as shown below: Definition 4.2 (Neighboring tensors). Two d\u00d7d\u00d7d symmetric tensors T,T\u2032 are neighboring tensors if there exists i, j, k \u2208 [d] such that\nT\u2032\u2212T = \u00b1symmetrize(ei\u2297ej\u2297ek) = \u00b1 (ei \u2297 ej \u2297 ek + ei \u2297 ek \u2297 ej + \u00b7 \u00b7 \u00b7+ ek \u2297 ej \u2297 ei) .\nAs noted earlier, the above notions can be similarly extended to asymmetric tensors as well as the guarantees for tensor power method on asymmetric tensors. We also remark that the difference of\nAlgorithm 3 Differentially private robust tensor power method 1: Input: tensor T, no. of components k, number of iterations L,R, privacy parameters \u03b5, \u03b4.\n2: Initialization: D = 0, \u03bd = 6 \u221a 2 ln(1.25/\u03b4\u2032)\n\u03b5\u2032 , \u03b4 \u2032 = \u03b42K , \u03b5 \u2032 = \u03b5\u221a K(4+ln(2/\u03b4)) , K = kL(R+ 1).\n3: for i = 1 to k do 4: Initialization: Draw u(1)0 , \u00b7 \u00b7 \u00b7 ,u (\u03c4) 0 uniformly at random from the unit sphere in Rd. 5: for t = 0 to R\u2212 1 do 6: Power iteration: compute u\u0303(\u03c4)t+1 = (T\u2212D)(I,u (\u03c4) t ,u (\u03c4) t ). 7: Noise calibration: release u\u0304(\u03c4)t+1 = u\u0303 (\u03c4) t+1 + \u03bd\u2016u (\u03c4) t \u20162\u221e \u00b7 z (\u03c4) t , where z (\u03c4) t\ni.i.d.\u223c N (0, Id). 8: Normalization: u(\u03c4)t+1 = u\u0304 (\u03c4) t+1/\u2016u\u0304 (\u03c4) t+1\u20162.\n9: end for 10: Compute \u03bb\u0303(\u03c4) = (T\u2212D)(u(\u03c4)R ,u (\u03c4) R ,u (\u03c4) R ) + \u03bd\u2016u (\u03c4) R \u20163\u221e \u00b7 z\u03c4 and let \u03c4\u2217 = argmax\u03c4 \u03bb\u0303(\u03c4)."}, {"heading": "11: Deflation: \u03bb\u0302i = \u03bb\u0303(\u03c4", "text": "\u2217), v\u0302i = u (\u03c4\u2217) R , D\u2190 D + \u03bb\u0302iv\u0302 \u22973 i . 12: end for 13: Output: eigenvalue/eigenvector pairs {\u03bb\u0302i, v\u0302i}ki=1.\n\u201cneighboring tensors\u201d as defined above has Frobenious norm bounded by O(1). This is necessary because an arbitrary perturbation of a tensor, even if restricted to only one entry, is capable of destroying any utility guarantee possible.\nIn a nutshell, Definitions 4.1, 4.2 state that an algorithm A is differentially private if, conditioned on any set of possible outputs of A, one cannot distinguish with high probability between two \u201cneighboring\u201d tensors T,T\u2032 that differ only in a single entry (up to symmetrization), thus protecting the privacy of any particular element in the original tensor T. Here \u03b5, \u03b4 are parameters controlling the level of privacy, with smaller \u03b5, \u03b4 values implying stronger privacy guarantee as Pr[A(T) \u2208 O] and Pr[A(T\u2032) \u2208 O] are closer to each other. Algorithm 3 describes the procedure of privately releasing eigenvectors of a low-rank input tensor T. The main idea for privacy preservation is the following noise calibration step\nu\u0304t+1 = u\u0303t+1 + \u03bd\u2016ut\u20162\u221e \u00b7 zt, where zt is a d-dimensional standard Normal random variable and \u03bd\u2016ut\u20162\u221e is a carefully designed noise magnitude in order to achieved desired privacy level (\u03b5, \u03b4). One key aspect is that the noise calibration step occurs at every power iteration, which adds to the robustness of the algorithm and achieves sharper bounds. We discuss at the end of this section. Theorem 4.1 (Privacy guarantee). Algorithm 3 satisfies (\u03b5, \u03b4)-differential privacy.\nProof. The only power iteration step of Algorithm 3 can be thought of as K = kL(R+ 1) queries directed to a private data sanitizer which produces f1(T;u) = T(I,u,u) or f2(T;u) = T(u,u,u) each time. The `2-sensitivity of both queries can be separately bounded as\n\u22062f1 = sup T\u2032 \u2016T(I,u,u)\u2212T\u2032(I,u,u)\u20162 \u2264 sup i,j,k 2(|uiuj |+ |uiuk|+ |ujuk|) \u2264 6\u2016u\u20162\u221e;\n\u22062f2 = sup T\u2032 \u2223\u2223T(u,u,u)\u2212T\u2032(u,u,u)\u2223\u2223 = sup i,j,k 6 \u2223\u2223uiujuk\u2223\u2223 \u2264 6\u2016u\u20163\u221e,\nwhere T\u2032 = T + symmetrize(ei \u2297 ej \u2297 ek) is some neighboring tensor of T. Thus, applying the Gaussian mechanism [9] we can (\u03b5, \u03b4)-privately release one output of either f1(u) or f2(u) by\nf`(u) + \u22062f` \u00b7\n\u221a 2 ln(1.25/\u03b4)\n\u03b5 \u00b7w,\nwhere ` = 1, 2 and w \u223c N (0, I) are i.i.d. standard Normal random variables. Finally, applying advanced composition [9] across all K = kL(R+ 1) private releases we complete the proof of this proposition. Note that both normalization and deflation steps do not affect the differential privacy of Algorithm 3 due to the closeness under post-processing property of DP.\nThe rest of the section is devoted to discussing the \u201cutility\u201d of Algorithm 3; i.e., to show that the algorithm is still capable of producing approximate eigenvectors, despite the privacy constraints. Similar to [12], we adopt the following incoherence assumptions on the eigenspace of T:\nAssumption 4.1 (Incoherent basis). Suppose V \u2208 Rd\u00d7k is the stacked matrix of orthonormal component vectors {vi}ki=1. There exists constant \u00b50 > 0 such that\nd k max 1\u2264i\u2264d \u2016V>ei\u201622 \u2264 \u00b50. (6)\nNote that by definition, \u00b50 is always in the range of [1, d/k]. Intuitively, Assumption 4.1 with small constant \u00b50 implies a relatively \u201cflat\u201d distribution of element magnitudes in T. The incoherence level \u00b50 plays an important role in the utility guarantee of Algorithm 3, as we show below: Theorem 4.2 (Guaranteed recovery of eigenvector under privacy requirements). Suppose T =\u2211k\ni=1 \u03bbiv \u22973 i for \u03bb1 > \u03bb2 \u2265 \u03bb3 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk > 0 with orthonormal v1, \u00b7 \u00b7 \u00b7 ,vk \u2208 Rd, and suppose\nAssumption 4.1 holds with \u00b50. Assume \u03bb1\u2212\u03bb2 \u2265 c/ \u221a d for some sufficiently small universal constant c > 0. If R = \u0398(log(\u03bbmaxd)), L = \u0398(k log k) and \u03b5, \u03b4 satisfy\n\u03b5 = \u2126\n( \u00b50k 2 log(\u03bbmaxd/\u03b4)\n\u03bbmin\n) , (7)\nthen with probability at least 0.9 the first eigen pair (\u03bb\u03021, v\u03021) returned by Algorithm 3 satisfies\u2223\u2223\u03bb1 \u2212 \u03bb\u03021\u2223\u2223 = O(1/\u221ad), \u2016v1 \u2212 v\u03021\u20162 = O(1/(\u03bb1\u221ad)). At a high level, Theorem 4.2 states that when the privacy parameter \u03b5 is not too small (i.e., privacy requirements are not too stringent), Algorithm 3 approximately recovers the largest eigenvalue and eigenvector with high probability. Furthermore, when \u00b50 is a constant, the lower bound condition on the privacy parameter \u03b5 does not depend polynomially upon tensor dimension d, which is a much desired property for high-dimensional data analysis. On the other hand, similar results cannot be achieved via simpler methods like input perturbation, as we discuss below:\nComparison with input perturbation Input perturbation is perhaps the simplest method for differentially private data analysis and has been successful in numerous scenarios, e.g. private matrix PCA [10]. In our context, this would entail appending a random Gaussian tensor E directly onto the input tensor T before tensor power iterations. By Gaussian mechanism, the standard deviation \u03c3 of each element in E scales as \u03c3 = \u2126(\u03b5\u22121 \u221a log(1/\u03b4)). On the other hand, noise analysis for tensor decomposition derived in [24, 2] and in the subsequent section of this paper requires \u03c3 = O(1/d) or \u2016E\u2016op = O(1/ \u221a d), which implies \u03b5 = \u2126\u0303(d) (cf. Lemma F.9). That is, the privacy parameter \u03b5 must scale linearly with tensor dimension d to successfully recover even the first principle eigenvector, which renders the privacy guarantee of the input perturbation procedure useless for high-dimensional tensors. Thus, we require a non-trivial new approach for differentially private tensor decomposition.\nFinally, we remark that a more desired utility analysis would bound the approximation error \u2016vi\u2212v\u0302i\u20162 for every component v1, \u00b7 \u00b7 \u00b7 ,vk, and not just the top eigenvector. Unfortunately, our current analysis cannot handle deflation effectively as the deflated vector v\u0302i \u2212 vi may not be incoherent. Extension to deflated tensor decomposition remains an interesting open question."}, {"heading": "5 Conclusion", "text": "We consider memory-efficient and differentially private tensor decomposition problems in this paper and derive efficient algorithms for both online and private tensor decomposition based on the popular tensor power method framework. Through an improved noise condition analysis of robust tensor power method, we obtain sharper dimension-dependent sample complexity bounds for online tensor decomposition and wider range of privacy parameters values for private tensor decomposition while still retaining utility. Simulation results verify the tightness of our noise conditions in principle.\nOne important direction of future research is to extend our online and/or private tensor decomposition algorithms and analysis to practical applications such as topic modeling and community detection, where tensor decomposition acts as one critical step for data analysis. An end-to-end analysis of online/private methods for these applications would be theoretically interesting and could also greatly benefit practical machine learning of important models.\nAcknowledgement A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, ONR Award N00014- 14-1-0665, ARO YIP Award W911NF-13-1-0084 and AFOSR YIP FA9550-15-1-0221."}, {"heading": "A Simulation results", "text": "We verify our main theoretical results in Theorem 2.2 on synthetic tensors. T is taken to be a rank-3 tensor T = v\u229731 + 0.75v \u22973 2 + 0.5v \u22973 3 , where v1 = (1, 0, 0, 0, \u00b7 \u00b7 \u00b7 , ), v2 = (0, 1, 0, 0, \u00b7 \u00b7 \u00b7 ) and v3 = (0, 0, 1, 0, \u00b7 \u00b7 \u00b7 ). The noise tensor E is synthesized according to the following three regimes:\n1. Random Gaussian noise: First generate Eijk i.i.d.\u223c N (0, 1) and then super-symmetrize E. 2. Adversarial Gaussian noise: E = \u2211d i=1 v2 \u2297 ei \u2297 ei + ei \u2297 v2 \u2297 ei + ei \u2297 ei \u2297 v2,\nwhere ei = (0, \u00b7 \u00b7 \u00b7 , 0, 1, 0, \u00b7 \u00b7 \u00b7 , 0) has all zero entries except for the ith one. 3. Weakly correlated noise: Let {v4, \u00b7 \u00b7 \u00b7 ,vd} be an orthonormal basis of the orthogonal\ncomplement of span{v1,v2,v3}. Set E = \u2211d i=4 vi \u2297 vi \u2297 vi.\nIn Fig. 1 we plot the \u201cfailure probability\u201d (measured via 20 independent trials per setting) of the robust tensor power method with random initialization against controlled noise magnitude \u2016E\u2016op = \u03c3. A trial is \u201csuccessful\u201d if for all i \u2208 {1, 2, 3} the recovered eigenvector v\u0302i satisfies v\u0302>i vi \u2265 1/4. To control \u2016E\u2016op, we first compute the operator norm of the generated raw noise tensor by invoking the eig_sshopm routine in Matlab tensor toolbox [5] (algorithm based on [20]) and then re-scale the entries. By inspecting the noise levels at which phase transition of failure probabilities occurs for different tensor dimensions d, ranging from 25 to 200. It is quite clear from Fig. 1 that the phase transitions occur at \u03c3 = O(1/ \u221a d) for random Gaussian noise, \u03c3 = O(1/d) for adversarial noise and \u03c3 = O(1/ log d) for weakly correlated noises, which matches our theoretical findings in Sec. 2 up to logarithmic terms. Our simulation results and explicit construction of an \u201cadversarial\u201d noise matrix also suggests that our analysis for robust tensor power method with random initializations under random Gaussian noise and existing analysis for worst-case noise in [1] are tight."}, {"heading": "B Comparison with whitening and matrix SVD decompositions", "text": "Another popular thread of tensor decomposition techniques involve whitening and reducing the problem to a matrix SVD decomposition, which is very effective at reducing the dimensionality of\nthe problem in the k = o(d) undercomplete settings [1, 21, 28]. We show in this section that without additional side information, a standard application and analysis of tensor decomposition of whitening and matrix SVD techniques leads to worse error bounds than we established in Theorem 2.2.\nWhen only the 3rd-order tensor T is available, one common whitening approach is to randomly \u201cmarginalized out\u201d one view of T\u0303:\nM(\u03b8) := T\u0303(I, I,\u03b8), \u03b8 randomly drawn on the unit d-dimensional sphere;\nand then evaluate top-k eigen-decomposition of M(\u03b8). LetW = span(v1, \u00b7 \u00b7 \u00b7 ,vk) be the span of the true components of T and W\u0302 be the top-k eigenspace of matrix M(\u03b8) obtained by collapsing one view of T\u0303. We then have the following proposition that bounds the perturbation betweenW and W\u0302: Proposition B.1. Suppose T\u0303 = T + \u2206T as in Theorems 2.1, 2.2 and let \u03a0W ,\u03a0W\u0302 be the projection operators ofW and W\u0302 , respectively. Then with probability at least 0.9 over the random draw of \u03b8,\u2225\u2225\u03a0W \u2212\u03a0W\u0302\u2225\u22252 \u2264 O\u0303 (\u221a d\u2016\u2206T \u2016op \u03bbmin ) , if \u2016\u2206T \u2016op = O\u0303 ( \u03bbmin\u221a d ) ,\nProof. First, we decompose M(\u03b8) into two terms:\nM(\u03b8) = k\u2211 i=1 \u03bbi(v > i \u03b8) \u00b7 viv>i + \u2206T (I, I,\u03b8).\nDefine \u03bb\u0304i = \u03bbi(v>i \u03b8) and E\u0304 = \u2206T (I, I,\u03b8). We then have that\nM(\u03b8) = M0 + E\u0304,\nwhere M0 is a d\u00d7 d rank-k matrix with eigenvalues (\u03bb\u03041, \u00b7 \u00b7 \u00b7 , \u03bb\u0304k) and eigenvectors (v1, \u00b7 \u00b7 \u00b7 ,vk), and E\u0304 satisfies \u2016E\u0304\u20162 \u2264 \u2016\u2206T \u2016op. Since \u03b8 is uniformly sampled from the d-dimensional unit sphere, by standard concentration arguments we have that |v>j \u03b8| = \u2126\u0303(1/ \u221a d) with overwhelming probability for all j = 1, \u00b7 \u00b7 \u00b7 , k and hence \u03c3k(M0) = \u2126\u0303(\u03bbmin/ \u221a d),\nwhere \u03c3k(\u00b7) denotes the kth largest singular value of a matrix. Applying Weyl\u2019s theorem (Lemma F.7) we have that\n\u03c3k(M(\u03b8)) \u2265 \u03c3k(M0)\u2212 \u2016E\u0304\u20162 = \u2126\u0303(\u03bbmin/ \u221a d),\nwhere the last inequality is due to the condition imposed on noise magnitude \u2016\u2206T \u2016op and the fact that \u2016E\u0304\u20162 \u2264 \u2016\u2206T \u2016op. Applying Wedin\u2019s theorem (Lemma F.8) with \u03b1 = 0 and \u03b4 = \u03c3k(M(\u03b8)) = \u2126\u0303(\u03bbmin/\n\u221a d) we arrive at\u2225\u2225\u03a0W \u2212\u03a0W\u0302\u2225\u22252 \u2264 \u2016E\u0304\u20162\u03c3k(M(\u03b8)) \u2264 O\u0303 (\u221a d\u2016\u2206T \u2016op \u03bbmin ) .\nThis simple result shows that the whitening trick does not trivially lead to matching noise conditions in Theorem 2.2 under k = o(d) settings."}, {"heading": "C Proof of Theorem 2.2", "text": "C.1 Proof sketch of Theorem 2.2\nIn this section we sketch the proof of Theorem 2.2. Our proof is mostly built upon the analysis in [1] for robust tensor power method. However, we borrow new ideas from [12] to substantially revise the per-iteration analysis (Lemma C.2), which subsequently results in desired relaxation of noise conditions. Some results and arguments in [1], especially those involved with absolute constants, are simplified for accessibility purposes.\nWe start with Lemma C.1 that analyzes random initializations against eigenvectors.\nLemma C.1. Fix j\u2217 \u2208 {1, \u00b7 \u00b7 \u00b7 , k} and \u03b7 \u2208 (0, 1/2). Suppose L satisfies L = \u2126(k/\u03b7). Then with probability at least 1\u2212 \u03b7 there exists a initialization u0 such that\nmax 1\u2264j\u2264k,j 6=j\u2217\n|v>j u0| \u2264 0.5|v>j\u2217u0| and |v>j\u2217u0| \u2265 1/ \u221a d. (8)\nRoughly speaking, Lemma C.1 shows that with L = \u2126(d log d) initializations the initial vector u0 will slightly bias towards one of the directions j\u2217 with overwhelming probability. The lemma is a slight generalization of Lemma B.1 in [1] to the k \u2264 d case and their proofs are similar. For completeness purposes we include its proof in Appendix C.2. Applying a standard boosting argument we have the following corollary, which guarantees exponentially decaying failure probabilities: Corollary C.1. For any \u03b7\u0303 \u2208 (0, 1/2), with L = \u2126(k log(1/\u03b7\u0303)) initializations Eq. (8) holds for at least one initialization with probability at least 1\u2212 \u03b7\u0303.\nThe following lemma is the key lemma that characterizes the recovery of single eigenvectors of the robust tensor power method. Lemma C.2. Suppose \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk \u2265 0 and assume without loss of generality that the conditions in Lemma C.1 hold with respect to j\u2217 = 1. Assume in addition that\n\u2016\u2206T (I,ut,ut)\u20162 \u2264 min { \u0303t, \u03bb1\n40 \u221a d\n} , |\u2206T (vj ,ut,ut)| \u2264 min { \u0303t\u221a k , \u03bb1 8d } , \u0303t \u2264 \u03bb1 200\nfor all t \u2208 [T ] and j such that \u03bbj > 0. We then have that 3\nmax j 6=1\n\u03bbj |v>j ut| \u2264 0.5\u03bb1|v>1 ut|, tan \u03b8(v1,ut) \u2264 0.8 tan \u03b8(v1,ut\u22121) + 8\u0303t/\u03bb1. (9)\nIn addition, if \u03b8(v1,ut) \u2264 \u03c0/3 we have further that\n|v>j ut+1| |v>1 ut+1| \u2264 0.8 |v>j ut| |v>1 ut| + 8\u0303t \u03bb1 \u221a k , \u2200j > 1 and \u03bbj > 0. (10)\nCompared to existing analysis in (Propositions B.1, B.2, Lemmas B.2, B.3, B.4 in [1]), our proof in Appendix C.2 analyzes the two-phase behavior of robust tensor power method in a unified framework and is thus much cleaner. Furthermore, we borrow ideas from [12] to prove shrinkage of the tangent angle between v1 and ut, which subsequently leads to relaxed noise conditions. We also prove additional bounds regarding |v>j ut| for j > 1 to facilitate later deflation analysis. This result is used for relaxing noise conditions only and is hence not proved in previous work [1].\nFinally, we present the following lemma that analyzes the deflation step in the robust noisy power method, in which both \u201celement-wise\u201d and \u201cfull-vector\u201d conditions on the deflated tensor are proved.\nLemma C.3. Let {\u03bb\u0302i, v\u0302i}ki=1 be eigenvalue and (orthonormal) eigenvector pairs that approximates {\u03bbi,vi}ki=1 with \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk > 0 such that for all i \u2208 [k],\n|\u03bb\u0302i \u2212 \u03bbi| \u2264 C , tan \u03b8(vi,vi) \u2264 min{ \u221a 2, C /\u03bbi} |v\u0302>i vj | \u2264 C /(\u03bbi \u221a k), \u2200j > i (11)\nfor some absolute constantC > 0 and error tolerance parameter > 0. Denote Ei = \u03bb\u0302iv\u0302 \u22973 i \u2212\u03bbiv\u22973i as the ith reconstruction error tensor. Let \u03b4 \u2208 (0, 1) be an arbitrary small constant. There exist universal constants C > 0 such that if \u2264 C \u2032\u03bbmin/ \u221a k then the following holds for all t \u2208 [k] and \u2016u\u20162 = 1:\u2225\u2225\u2225\u2225\u2225 t\u2211 i=1 Ei(I,u,u) \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u03bat(u) and \u2223\u2223\u2223\u2223 t\u2211 i=1 Ei(vj ,u,u)\n\u2223\u2223\u2223\u2223 \u2264 \u03bat(u)2 \u221ak , \u2200j > t, (12) where \u03bat(u) = \u221a \u03b4 + C \u2032\u2032 \u2211t i=1 |v>i u|2 and C \u2032\u2032 > 0 is a universal constant.\nWe are now ready to prove the main theorem. 3For notational simplicity, let tan \u03b8(v1,u\u22121) =\u221e.\nProof of Theorem 2.2. We use induction to prove the theorem. For i = 1 all conditions in Lemma C.2 are satisfied with \u0303t = 2 when \u2264 C1\u03bbmin/ \u221a k for some sufficiently small constant C1 > 0. Lemma C.2 then asserts that, with L = \u2126(d log d) initializations and R = \u2126(log(\u03bb1k/ )) iterations, \u2016v\u03021 \u2212 v1\u20162 \u2264 tan \u03b8(v\u03021,v1) \u2264 C2 /\u03bb1 for some universal constant C2 > 0. Furthermore,\n|\u03bb\u03021 \u2212 \u03bb1| = \u2223\u2223T\u0303(v\u03021, v\u03021, v\u03021)\u2212 \u03bb1\u2223\u2223 \u2264 \u2223\u2223\u2206T (v\u03021, v\u03021, v\u03021)\u2223\u2223+ \u2223\u2223T(v\u03021, v\u03021, v\u03021)\u2212 \u03bb1\u2223\u2223\n\u2264 O (\n\u221a k\n) + \u2223\u2223\u2223\u2223\u03bb1|v>1 v\u03021|3 \u2212 \u03bb1 +\u2211 j>1 \u03bbj |v>j v\u03021|3 \u2223\u2223\u2223\u2223\n\u2264 O (\n\u221a k\n) + \u2223\u2223\u2223\u2223\u03bb1 [1 +O( \u03bb1 )] \u2212 \u03bb1 + \u2211 j>1 \u03bbjO ( 3 \u03bb3jk 1.5 )\u2223\u2223\u2223\u2223 \u2264 O( ), if \u2264 C1\u03bbmin/ \u221a k for some sufficiently small constant C1.\nWe next prove the theorem for the case of i+ 1 assuming by induction that the theorem holds for all {\u03bbj ,vj}ij=1. In this case, the \u201cnew\u201d noise tensor \u2206\u0303T comes from both the original noise and also noise introduced by deflations; that is, \u2206\u0303T = T\u0303 + \u2211i j=1 Ei. Invoking Lemma C.3 we have that \u2206\u0303T satisfies conditions in Lemma C.2 with\n\u0303t = ( 1 + max{\u03bai(ut), \u03bai(ut)2} ) ,\nwhere \u03bai(u) = \u221a \u03b4 + C \u2032\u2032 \u2211i j=1 |u>vj |2 as defined in Lemma C.3, provided that \u2264 C1\u03bbmin/ \u221a k for some sufficiently small constant C1. Furthermore, note that for arbitrary \u03b4 \u2208 (0, 1), we can again pick C \u20321 > 0 to be a sufficiently small constant (possibly depending on \u03b4) such that \u2264 C \u20321\u03bbmin/ \u221a k\nwould imply \u0303t \u2264 min{\u03bb1/200, 0.01\u03bbmin \u221a \u03b4/(C \u2032\u2032k)}. Subsequently, by Eq. (9) we know that after\n\u2126(log(\u03bbmaxk/ )) iterations we have that tan \u03b8(ut,vi+1) \u2264 0.1 \u221a \u03b4/(C \u2032\u2032k) and hence for any j \u2264 i,\n|u>t vj | = cos \u03b8(ut,vj) = sin \u03b8(ut,vi+1) \u2264 tan \u03b8(ut,vi+1) \u2264 0.1 \u221a \u03b4/(C \u2032\u2032k). Consequently,\nC \u2032\u2032 \u2211i j=1 |u>t vj |2 \u2264 0.01\u03b4 and therefore \u03bai(ut) \u2264 \u221a 1.01\u03b4 \u2264 1. We then have that \u0303t \u2264 2 and hence the resuling bounds on |\u03bb\u0302i+1 \u2212 \u03bbi+1| and tan \u03b8(ut,vi+1) hold with the same constant C as all previous iterations before i. Finally, applying Lemma C.1 and taking a union bound over all k iterations we complete the proof.\nC.2 Proof of technical lemmas\nProof of Lemma C.1. Let u\u0303(\u03c4)0 i.i.d.\u223c Nd(0, Id\u00d7d) for \u03c4 \u2208 [L] and define Zj,\u03c4 = v>j u\u0303 (\u03c4) 0 for j \u2208 [d] and \u03c4 \u2208 [L]. Without loss of generality, assume j\u2217 = 1. Consider the following sets of events:\nE1 := { Z : max\n\u03c4\u2208[L] |Z1,\u03c4 | \u2265 0.5\n\u221a lnL\u2212 \u221a 2 ln(6/\u03b7) } , (13)\nE2,\u03c4 := { Z\u00b7,\u03c4 : max\n1<j\u2264k |Zj,\u03c4 | \u2264\n\u221a 2 ln k + \u221a 2 ln(3/\u03b7) } , (14)\nE3,\u03c4 := Z\u00b7,\u03c4 : d\u2211\nj=k+1\n|Zj,\u03c4 |2 \u2264 3 ln(3/\u03b7) \u00b7 d+ 2 ln(3/\u03b7)  . (15) Suppose E1 holds with \u03c4\u2217 = argmax\u03c4 |Z1,\u03c4 | and suppose in addition that E2,\u03c4\u2217 and E3,\u03c4\u2217 hold. To derive Eq. (8) we need to show the following inequalities:\n0.5 \u221a lnL\u2212 \u221a 2 ln(6/\u03b7) \u2265 0.5 (\u221a 2 ln k + \u221a 2 ln(3/\u03b7) ) ;\n(0.6 \u221a lnL\u2212 \u221a 2 ln(6/\u03b7))2\nk \u00b7 (0.6 \u221a lnL\u2212 \u221a 2 ln(6/\u03b7))2 + 3 ln(3/\u03b7)d+ 2 ln(3/\u03b7) \u2265 1 d .\nIt can be easily verified that L = \u2126(k/\u03b7) satisfies the above inequalities and hence imply Eq. (8) under E1 \u2229 E2,\u03c4\u2217 \u2229 E3,\u03c4\u2217 .\nThe rest of the proof is to lower bound the probabilities of events E1, E2,\u03c4\u2217 and E3,\u03c4\u2217 . We first consider E1. Because Z1,1, \u00b7 \u00b7 \u00b7 , Z1,L\ni.i.d.\u223c N (0, 1) and f(Z1,1, \u00b7 \u00b7 \u00b7 , Z1,L) = max\u03c4 |Z1,\u03c4 | is a 1-Lipschitz function, applying Lemma F.1 we have that\nPr [ max \u03c4 |Z1,\u03c4 | < \u00b5\u2212 t ] \u2264 2e\u2212t 2/2, (16)\nwhere \u00b5 = E[max\u03c4 |Z1,\u03c4 |]. By Lemma F.2, \u00b5 \u2265 E[max\u03c4 Z1,\u03c4 ] \u2265 \u221a lnL/ \u221a \u03c0 ln 2 \u2265 0.5 \u221a lnL. Setting t = \u221a\n2 ln(6/\u03b7) in Eq. (16) we have that Pr[E1] \u2265 1\u2212 \u03b7/3. Next, suppose E1 holds with \u03c4\u2217 = argmax\u03c4 |Z1,\u03c4 |. Note that E2,\u03c4\u2217 and E3,\u03c4\u2217 are independent regardless of the choice of \u03c4\u2217, because Z1,\u03c4\u2217 , \u00b7 \u00b7 \u00b7 , Zd,\u03c4\u2217 are independent Gaussian random variables. We can then lower bound the probabilities of E2,\u03c4\u2217 and E3,\u03c4\u2217 separately. We consider E2,\u03c4\u2217 first. Because Z2,\u03c4\u2217 , \u00b7 \u00b7 \u00b7 , Zk,\u03c4\u2217 are i.i.d. standard Normal random variables, applying Lemma F.3 we obtain\nPr [ max 2\u2264j\u2264k |Zj,\u03c4\u2217 | > \u221a 2 ln k + \u221a 2t ] \u2264 e\u2212t. (17)\nPutting t = ln(3/\u03b7) in Eq. (17) we have that Pr[E2,\u03c4\u2217 |E1] \u2265 1 \u2212 \u03b7/3. For E3,\u03c4\u2217 , it is obvious by definition that \u2211d j=k+1 |Zj,\u03c4\u2217 |2 is a \u03c72d\u2212k-distributed random variable and is independent of E1 and E2,\u03c4\u2217 . Applying Lemma F.4 the following holds:\nPr  d\u2211 j=k+1 |Zj,\u03c4\u2217 |2 > d+ 2 \u221a dt+ 2t  \u2264 e\u2212t. (18) Putting t = ln(3/\u03b7) in Eq. (18) and noting that \u221a d \u2264 d, t \u2265 1, we conclude that Pr[E3,\u03c4\u2217 |E1] \u2265 1\u2212 \u03b7/3. Finally, applying union bound we have that Pr[E1 \u2229 E2,\u03c4\u2217 \u2229 E3,\u03c4\u2217 ] \u2265 1\u2212 \u03b7.\nProof of Lemma C.2. First, as a consequence of Corollary C.1, we know that |v>1 u0| \u2265 1/ \u221a d. The conditions in Lemma C.2 then imply |\u2206T (vj ,ut,ut)| \u2264 \u03bb1|v>1 u0|2/8. We now use induction to prove Eq. (9). When t = 0 Eq. (9) trivially holds due to Lemma C.1 and the condition that j\u2217 = 1 corresponds to the largest eigenvalue \u03bb1. The objective is then to prove Eq. (9) for the case of t+ 1, assuming it holds for all iterations up to t.\nWe first consider the second part of Eq. (9) concerning tan \u03b8(v1,ut). Let V \u2208 Rd\u00d7(k\u22121) be an orthonormal basis of the complement subspace V\u22a5 = span{v2, \u00b7 \u00b7 \u00b7 ,vk}. Further let \u03b5t = \u2206T (I,ut,ut). Because T(I,ut,ut) lies in the span of columns of V, we have that\ntan \u03b8(v1,ut+1) \u2264 \u2016V>T(I,ut,ut)\u20162 + \u2016\u03b5t\u20162 |v>1 [T(I,ut,ut) + \u03b5t]| \u2264 \u2016V >T(I,ut,ut)\u20162 + \u2016\u03b5t\u20162 |v>1 T(I,ut,ut)| \u2212 |v>1 \u03b5t| .\nIn addition, note that\n\u2016V>T(I,ut,ut)\u20162 = \u221a\u221a\u221a\u221a k\u2211 j=2 \u03bb2j |v>j ut|4 \u2264 max j 6=1 \u03bbj |v>j ut| \u00b7 \u221a\u221a\u221a\u221a k\u2211 j=2 |v>j ut|2,\nwhere the first equality is due to the orthogonality of {v2, \u00b7 \u00b7 \u00b7 ,vk} and in the last inequality we apply H\u2019\u0301older\u2019s inequality. Because \u221a\u2211k j=2 |v>j ut|2 = \u2016V>ut\u20162, we have that\ntan \u03b8(v1,ut+1) \u2264 \u2016V>ut\u20162 \u00b7maxj 6=1 \u03bbj |v>j ut|+ \u2016\u03b5t\u20162\n|v>1 ut| \u00b7 \u03bb1|v>1 ut| \u2212 |v>1 \u03b5t|\n= tan \u03b8(v1,ut)\n[ maxj 6=1 \u03bbj |v>j ut|+ \u2016\u03b5t\u20162/\u2016V>ut\u20162\n\u03bb1|v>1 ut| \u2212 |v>1 \u03b5t|/|v>1 ut|\n]\n\u2264 tan \u03b8(v1,ut) [\n0.5\u03bb1|v>1 ut|+ \u2016\u03b5t\u20162/\u2016V>ut\u20162 \u03bb1|v>1 ut| \u2212 |v>1 \u03b5t|/|v>1 ut|\n] (19)\n= tan \u03b8(v1,ut)\n[ 1\n2\n1\n1\u2212 |v>1 \u03b5t|/(\u03bb1|v>1 ut|2) ] \ufe38 \ufe37\ufe37 \ufe38\n\u03b1\n+ 1\n1\u2212 |v>1 \u03b5t|/(\u03bb1|v>1 ut|2)\ufe38 \ufe37\ufe37 \ufe38 2\u03b1\n\u00b7 \u2016\u03b5t\u20162 \u03bb1|v>1 ut|2\ufe38 \ufe37\ufe37 \ufe38\n\u03b2\n.\nHere in Line 19 we apply the induction hypothesis that maxj 6=1 \u03bbj |v>j ut| \u2264 0.5\u03bb1|v>1 ut|. Before proceeding the analysis we first show that |v>1 u0| \u2264 |v>1 ut|. Applying the induction hypothesis, we have that\ntan \u03b8(v1,ut) \u2264 0.8t tan \u03b8(v1,u0) + 40\u0303t/\u03bb1 \u2264 0.8 tan \u03b8(v1,u0) + 40\u0303t/\u03bb1 \u2264 tan \u03b8(v1,u0), where the last inequality is due to \u0303t \u2264 \u03bb1/200. Subsequently, \u03b8(v1,ut) \u2264 \u03b8(v1,u0) and hence |v>1 ut| = cos \u03b8(v1,ut) \u2265 cos \u03b8(v1,u0) = |v>1 u0|. Now applying |v>1 \u03b5t| \u2264 |v>1 u0|2/4 we obtain\n|v>1 \u03b5t| \u2264 \u03bb1|v>1 u0|2 4 \u2264 \u03bb1|v > 1 ut|2 4 =\u21d2 1 1\u2212 |v>1 \u03b5t|/(\u03bb1|v>1 ut|2) \u2264 3 2 =\u21d2 \u03b1 \u2264 3 4 . (20)\nNext we bound \u03b2 by considering two cases. In the first case of |v>1 ut| \u2264 0.5, we have that\n\u03b2 = tan \u03b8(v1,ut) \u00b7 \u2016V>\u03b5t\u20162 \u03bb1|v>1 ut| \u221a 1\u2212 |v>1 ut|2 \u2264 2\u2016\u03b5t\u20162 \u03bb1|v>1 ut| \u00b7 tan \u03b8(v1,ut) \u2264 0.05 tan \u03b8(v1,ut).\n(21) where the last inequality is due to the condition that \u2016\u03b5t\u20162 \u2264 \u03bb1|v > 1 u0| 40 \u2264 \u03bb1|v>1 ut| 40 . On the other hand, if |v>1 ut| > 0.5 the following holds:\n\u03b2 = \u2016\u03b5t\u20162 \u03bb1|v>1 ut|2 \u2264 4\u2016\u03b5t\u20162 \u03bb1 \u2264 4\u0303t \u03bb1 . (22)\nCombining Eq. (20,21,22) we obtain tan \u03b8(v1,ut+1) \u2264 0.8 tan \u03b8(v1,ut) + 8\u0303t/\u03bb1.\nWe next prove the first part of Eq. (9), namely that maxj 6=1 \u03bbj |v>j ut+1| \u2264 0.5\u03bb1|v>1 ut+1|. For those j with \u03bbj = 0 the bound trivially holds. So we consider only j > 1 with \u03bbj > 0. We then have that\n\u03bb1|v>1 ut+1| \u03bbj |v>j ut+1| = \u03bb1|v>1 [T(I,ut,ut) + \u03b5t]| \u03bbj |v>j [T(I,ut,ut) + \u03b5t]| \u2265 ( \u03bb1|v>1 ut| \u03bbj |v>j ut| )2 \ufe38 \ufe37\ufe37 \ufe38\n\u03b1\u2032\n\u00b7 1\u2212 |v > 1 \u03b5t|/(\u03bb1|v>1 u2t |)\n1 + |v>j \u03b5t|/(\u03bbj |v>j \u03b5t|2)\ufe38 \ufe37\ufe37 \ufe38 \u03b2\u2032\n.\nBy induction hypothesis \u03b1\u2032 \u2265 4. Applying conditions on |v>1 \u03b5t| we get |v>1 \u03b5t| \u2264 \u03bb1|v>1 u0| 2\n4 \u2264 \u03bb1|v>1 ut| 2\n4 and hence |v > 1 \u03b5t|/(\u03bb1|v>1 ut|2) \u2264 1/4. On the other hand,(\n\u03bb1|v>1 ut| \u03bbj |v>j ut|\n)2 [ 1 +\n|v>j \u03b5| \u03bbj |v>j ut|2\n]\u22121 = (\u03bbj |v>j ut| \u03bb1|v>1 ut| )2 + \u03bbj |v>j \u03b5t| \u03bb21|v>1 ut|2 \u22121 \u2265 [1 4 + |v>j \u03b5t| \u03bb1|v>1 ut|2 ]\u22121 .\nBecause |v>j \u03b5t| \u2264 \u03bb1|v>1 u0| 2 8 \u2264 \u03bb1|v>1 ut| 2\n8 , the right-hand side of the above equation is lower bounded by 8/3. Therefore, \u03b1\u2032\u03b2\u2032 \u2265 83 (1\u2212 1 4 ) \u2265 2.\nThe last part of this proof is devoted to showing Eq. (10). Under the condition that \u03b8(v1,ut) \u2264 \u03c0/3 we have that cos \u03b8(v1,ut) = |v>1 ut| \u2265 1/2. Subsequently, for arbitrary j > 1 with \u03bbj > 0 the following holds:\n|v>j ut+1| |v>1 ut+1| \u2264 \u03bbj |v>j ut|2 + |v>j \u03b5t| \u03bb1|v>1 ut|2 \u2212 |v>1 \u03b5t| \u2264 |v>j ut| |v>1 ut| \u00b71 2\n1\n1\u2212 |v>1 \u03b5t|/(\u03bb1|v>1 ut|2)\ufe38 \ufe37\ufe37 \ufe38 \u03b1\n+ |v>j \u03b5t|\n\u03bb1|v>1 ut|2 \u2212 |v>1 \u03b5t|\ufe38 \ufe37\ufe37 \ufe38 \u03b3 .\nBecause |v>1 ut| \u2265 1/2 and |v>1 \u03b5t| \u2264 12\u03bb1|v > 1 u0|2 \u2264 12\u03bb1|v > 1 ut|2, we have \u03b3 \u2264 8|v>j \u03b5t|/\u03bb1 and hence\n|v>j ut+1| \u2264 0.8|v>j ut|+ 8|v>j \u03b5t| \u03bb1 \u2264 0.8|v>j ut|+ 8\u0303t \u03bb1 \u221a k .\nProof of Lemma C.3. The first part of Eq. (12) is a simplified result of Lemma B.5 4 in [1] because \u2016v\u0302i \u2212 vi\u20162 \u2264 tan \u03b8(v\u0302i,vi) when \u2016v\u0302i\u20162 = \u2016vi\u20162 = 1 and \u03b8 < \u03c0/2. The proofs are almost identical.\n4Except that we operate under a k < d regime, which adds no difficulty to the proof.\nSo we focus on proving the second part of Eq. (12) here. Recall that v>j vi = 0 for all j > i. Subsequently, \u2223\u2223\u2223\u2223 t\u2211\ni=1\nEi(vj ,u,u) \u2223\u2223\u2223\u2223 \u2264 t\u2211 i=1 \u03bb\u0302i|u>v\u0302i|2|v>j v\u0302i| \u2264 C \u221a k t\u2211 i=1 \u03bb\u0302i \u03bbi |u>v\u0302i|2.\nDefine v\u0302\u22a5i = v\u0302i \u2212 (v\u0302 > i vi)vi as the difference between v\u0302i and its projection on vi. It is then by definition that \u2016v\u0302\u22a5i \u20162 = \u2016v\u0302i\u20162 sin \u03b8(v\u0302i,vi) \u2264 tan \u03b8(v\u0302i,vi). Subsequently,\nt\u2211 i=1 \u03bb\u0302i \u03bbi |u>v\u0302i|2 \u2264 t\u2211 i=1\n( 1 + |\u03bb\u0302i \u2212 \u03bbi|\n\u03bbi\n) |u>v\u0302i|2 \u2264 ( C\n\u03bbmin + 1 )[ t\u2211 i=1 ( |u>vi|2 + |u>v\u0302\u22a5i |2 )]\n\u2264 ( C\n\u03bbmin + 1\n)[ k\u2016v\u0302\u22a5i \u201622 + t\u2211 i=1 |u>vi|2 ] \u2264 ( C \u03bbmin + 1 ) C2k 2\n\u03bb2min\ufe38 \ufe37\ufe37 \ufe38 a +\nt\u2211 i=1 |u>vi|2.\nHere the second step is due to H\u00f6lder inequality and the fact that max1\u2264i\u2264k |\u03bb\u0302i\u2212\u03bbi| \u03bbi \u2264 C \u03bbmin . For arbitrary \u03b4 \u2208 (0, 1), set \u2264 min{\u03bbminC2 , \u221a \u03b4 2C3 \u03bbmin\u221a k } \u2264 C \u2032\u03bbmin/ \u221a k we have that a \u2264 \u03b4/C, and hence the second part of Eq. (12) holds with C \u2032\u2032 = C."}, {"heading": "D Proof of results for streaming robust tensor power method", "text": "Proof of Theorem 3.1. First, note that if x1, \u00b7 \u00b7 \u00b7 ,xn i.i.d.\u223c P , P \u2208 SGD(\u03c3) then the distribution of the sample mean x\u0304 = 1n \u2211n i=1 xi belongs to SGD(\u03c3/ \u221a n). To see this, fix any a \u2208 RD and one can show that\nE [ exp(a>x\u0304) ] = n\u220f i=1 E [ exp(a>xi/n) ] \u2264 n\u220f i=1 exp(\u2016a\u201622\u03c32/n2) = exp(\u2016a\u201622\u03c32/n),\nwhere the second inequality is due to the fact that xi \u2208 SGD(\u03c3) and \u2016a/n\u201622 = \u2016a\u201622/n2. Under Assumptions 3.1, 3.2 and using the the above arguments, we know that\nvec(\u2206T ) = vec\n[ 1\nn n\u2211 i=1 x\u22973i \u2212T\n] \u2208 SGd3(\u03c3/n)\nNow fix vi,ut \u2208 Rd with unit L2 norms. Applying Lemma F.6 with respect to \u03a3 = vec(vi \u2297 ut \u2297 ut)vec(vi \u2297 ut \u2297 ut)> we obtain that\nPr [\u2223\u2223\u2206T (vi,ut,ut)\u2223\u22232 > (1 + 2\u221at+ t)\u03c32 n ] \u2264 e\u2212t, \u2200t > 0. (23)\nSubsequently, with overwhelming probability (e.g., \u2265 1\u2212 n\u221210) we have that\n\u2016\u2206T (I,ut,ut)\u20162 = O\u0303 ( \u03c3 \u221a d\nn\n) ,\n\u2223\u2223\u2206T (vi,ut,ut)\u2223\u2223 = O\u0303(\u03c3\u221a 1 n ) .\nFinally, with\nn = \u2126\u0303 ( min { \u03c32d\n2 , \u03c32d2\n\u03bb2min }) the conditions in Eq. (4) are satisfied with overwhelming probability and hence the error bounds on |\u03bbi \u2212 \u03bb\u0302\u03c0(i)| and \u2016vi \u2212 v\u0302\u03c0(i)\u20162."}, {"heading": "E Proofs of utility results for differentially private tensor decomposition", "text": "Before proving Theorem 4.2, we first present a lemma that upper bounds \u2016ut\u2016\u221e when the components V \u2208 Rd\u00d7k is incoherent (Assumption 4.1) and Gaussian noise across power updates is added.\nLemma E.1. Suppose T = \u2211k i=1 \u03bbiv \u2297 i 3 and V = (v1, \u00b7 \u00b7 \u00b7 ,vk) satisfies Assumption 4.1 with coherence level \u00b50. Fixu \u2208 Rd with \u2016u\u20162 = 1 and let u\u0304 = T(I,u,u)+\u03c3\u00b7z, where z \u223c N (0, Id\u00d7d) are zero-mean independently distributed Gaussian random variables. We then have that\n\u2016u\u0304\u2016\u221e \u2016u\u0304\u20162 = O\n(\u221a \u00b50k log d\nd\n) .\nwith overwhelming probability.\nProof. We prove this lemma by showing an upper bound for \u2016u\u0304\u2016\u221e and a lower bound on \u2016u\u0304\u20162, both with overwhelming probabilities. For the infinity-norm upper bound, we consider the following decomposition via triangle inequality:\n\u2016u\u0304\u2016\u221e \u2264 \u2016u\u0303\u2016\u221e + \u03c3\u2016z\u2016\u221e,\nwhere u\u0303 = T(I,u,u) and z \u223c N (0, Id\u00d7d). By definition,\n\u2016u\u0303\u2016\u221e = \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03bbi|u>vi|2vi \u2225\u2225\u2225\u2225\u2225 \u221e = max 1\u2264j\u2264d \u2223\u2223\u2223\u03bb>(V>ej)\u2223\u2223\u2223, where \u03bb is a k-dimensional vector defined as \u03bb = (\u03bb1|u>v1|2, \u00b7 \u00b7 \u00b7 , \u03bbk|u>vk|2). By CauchySchwarts inequality, we have that\n\u2016u\u0303\u2016\u221e = max 1\u2264j\u2264d \u2223\u2223\u2223\u03bb>(V>ej)\u2223\u2223\u2223 \u2264 \u2016\u03bb\u20162 \u00b7 max 1\u2264j\u2264d \u2016V>ej\u20162 \u2264 \u221a\u221a\u221a\u221a\u00b50k d ( k\u2211 i=1 \u03bb2i |u>vi|4 ) ,\nwhere the last inequality is due to the condition that V is incoherent with coherence level \u00b50. In addition, \u2016z\u2016\u221e = O( \u221a log d) with overwhelming probability, by applying Lemma F.3. As a result,\n\u2016u\u0304\u2016\u221e \u2264 \u221a\u221a\u221a\u221a2k\u00b50 d ( k\u2211 i=1 \u03bb2i |u>vi|4 ) +O(\u03c3 \u221a log d). (24)\nWe next lower bound the denominator term \u2016u\u0304\u20162. By definition, u\u0304 follows a multi-variate Gaussian distribution with mean u\u0303 and co-variance \u03c32Id\u00d7d. Applying Lemma F.5 with \u00b5 = \u2016u\u0303\u201622/\u03c32 and t = O(log d) we have that \u2016u\u0304\u201622 = \u2126(\u2016u\u0303\u201622 + \u03c32d) with overwhelming probability. Note also that\n\u2016u\u0303\u201622 = \u2225\u2225\u2225\u2225\u2225 k\u2211 i=1 \u03bbi|u>vi|2vi \u2225\u2225\u2225\u2225\u2225 2\n2\n= k\u2211 i=1 \u03bb2i |u>vi|4\nbecause {vi}ki=1 are orthonormal vectors. Consequently,\n\u2016u\u0304\u201622 = \u2126\n \u221a\u221a\u221a\u221a\u03c32d+ k\u2211\ni=1\n\u03bb2i |u>vi|4  . (25) Combining Eqs. (24,25) we obtain\n\u2016u\u0304\u2016\u221e \u2016u\u0304\u20162 \u2264\n\u221a 2\u00b50k d \u2211k i=1 \u03bb 2 i |u>vi|4 +O(\u03c3 \u221a log d)\n\u2126 (\u221a \u03c32d+ \u2211k i=1 \u03bb 2 i |u>vi|4\n) \u2264 O(\u221a\u00b50k d ) +O (\u221a log d d ) = O (\u221a \u00b50k log d d ) .\nWe are now ready to prove Theorem 4.2.\nProof of Theorem 4.2. Applying Lemma E.1 we can with overwhelming probability upper bound the per-coordinate standard deviation of Gaussian noise calibrated in Algorithm 3:\nmax 0\u2264t\u2264T 1\u2264\u03c4\u2264L\n{ \u03bd\u2016u(\u03c4)t \u20162\u221e, \u03bd\u2016u (\u03c4) t \u20163\u221e } \u2264 O\n(\u221a K \u00b7 log(1/\u03b4)\n\u03b5 \u00b7 \u00b50k log d d\n) ,\nwhere K = kL(T + 1) = O\u0303(k2 log(\u03bbmaxd)). Let (\u03c4) t = E(I,u (\u03c4) t ,u (\u03c4) t ) = \u03c3 (\u03c4) t \u00b7 z be the noise vector calibrated, where \u03c3(\u03c4)t = \u03bd\u2016u (\u03c4) t \u20162\u221e. We then have that with overwhelming probability,\n\u2016 (\u03c4)t \u20162 = O ( \u00b50k 2 log(\u03bbmaxd/\u03b4)\n\u03b5 \u221a d\n) and\n\u2223\u2223v>1 (\u03c4)t \u2223\u2223 = O(\u00b50k2 log(\u03bbmaxd/\u03b4)\u03b5d ) .\nEquating the upper bound for |v>1 (\u03c4) t | with O(\u03bbmin/d) we obtain the desired privacy level condition:\n\u03b5 = \u2126\n( \u00b50k 2 log(\u03bbmaxd/\u03b4)\n\u03bbmin\n) .\nIt can also be easily verified that all noise conditions in Theorem 2.2 are satisfied with above lower bound condition on \u03b5."}, {"heading": "F Technical lemmas", "text": "F.1 Tail inequalities\nLemma F.1 (Tail bound of Lipschitz function of Gaussian random variables, [8]). Suppose x \u223c Nd(0, \u03c32Id\u00d7d) are d-dimensional independent Gaussian random variables and let f : Rd \u2192 R be an L-Lipschitz function; that is, |f(x)\u2212f(y)| \u2264 L\u2016x\u2212y\u20162 for all x,y \u2208 Rd. Suppose \u00b5 = Ex[f(x)]. Then for all t > 0, we have that\nPr [\u2223\u2223f(x)\u2212 \u00b5\u2223\u2223 \u2265 t] \u2264 2e\u2212t2/(2\u03c32L2).\nLemma F.2 (Bounds on maximum of Gaussian random variables, [19]). Suppose X1, \u00b7 \u00b7 \u00b7 , Xn i.i.d.\u223c N (0, \u03c32) and let Y = max1\u2264i\u2264nXi. We then have that \u03c3\u221a \u03c0 ln 2 \u221a lnn \u2264 E[Y ] \u2264 \u03c3 \u221a 2 \u221a lnn.\nLemma F.3 (Bounds on maximum absolute values of Gaussian random variables; Theorem 3.12, [23]). Suppose X1, \u00b7 \u00b7 \u00b7 , Xn i.i.d.\u223c N (0, \u03c32) and let Y = max1\u2264i\u2264n |Xi|. We then have that\nPr [ Y \u2265 \u03c3 \u221a 2 lnn+ \u03c3 \u221a 2t ] \u2264 e\u2212t, \u2200t > 0.\nLemma F.4 (Bounds on Chi-square random variables, [22]). Suppose X \u223c \u03c72k; that is, X =\u2211k j=1 Y 2 j for i.i.d. standard Normal random variables Y1, \u00b7 \u00b7 \u00b7 , Yk. We then have that \u2200t > 0,\nPr [ X \u2265 k + 2 \u221a kt+ 2t ] \u2264 e\u2212t, Pr [ X \u2264 k \u2212 2 \u221a kt ] \u2264 e\u2212t.\nLemma F.5 (Bounds on non-central Chi-square random variables, [7]). Suppose X \u223c \u03c72k(\u00b5); that is, X = \u2211k j=1 Y 2 k for independent Normal random variables Y1, \u00b7 \u00b7 \u00b7 , Yk distributed as Yj \u223c N (\u00b5j , 1),\u2211\nj \u00b5j = \u00b5. We then have that Pr [ X \u2265 (k + \u00b5) + 2 \u221a (k + 2\u00b5)t+ 2t ] \u2264 e\u2212t,\nPr [ X \u2264 (k + \u00b5)\u2212 2 \u221a (k + 2\u00b5)t ] \u2264 e\u2212t.\nLemma F.6 (Bounds on quadratic forms of sub-Gaussian random variables, [15]). Suppose X \u223c SGD(\u03c3) and let \u03a3 \u2208 RD\u00d7D be a positive semidefinite matrix. Then for all t > 0 we have that\nPr [ X>\u03a3X > \u03c32 ( tr(\u03a3) + 2 \u221a tr(\u03a32)t+ 2\u2016\u03a3\u2016t )] \u2264 e\u2212t.\nF.2 Matrix perturbation lemmas\nLemma F.7 (Weyl\u2019s theorem; Theorem 4.11, p. 204 in [26]). Let A,E be given m\u00d7n matrices with m \u2265 n. Then\nmax i\u2208[n] \u2223\u2223\u2223\u03c3i(A + E)\u2212 \u03c3i(A)\u2223\u2223\u2223 \u2264 \u2016E\u20162. Lemma F.8 (Wedin\u2019s theorem; Theorem 4.4, pp. 262 in [26]). Let A,E \u2208 Rm\u00d7n be given matrices with m \u2265 n. Let A have the following singular value decomposition U>1U>2\nU>3\nA [ V1 V2 ] = [ \u03a31 00 \u03a32 0 0 ] ,\nwhere U1,U2,U3,V1,V2 have orthonormal columns and \u03a31 and \u03a32 are diagonal matrices. Let"}, {"heading": "A\u0303 = A + E be a perturbed version of A and (U\u03031, U\u03032, U\u03033, V\u03031, V\u03032, \u03a3\u03031, \u03a3\u03032) be analogous singular", "text": "value decomposition of A\u0303. Let \u03a6 be the matrix of canonical angles between Range(U1) and Range(U\u03031) and \u0398 be the matrix of canonical angles between Range(V1) and Range(V\u03031). If there exists \u03b1, \u03b4 > 0 such that\nmin i \u03c3i(\u03a3\u03031) \u2265 \u03b1+ \u03b4 and max i \u03c3i(\u03a32) \u2264 \u03b1,\nthen\nmax{\u2016U1U>1 \u2212 U\u03031U\u0303>1 \u20162, \u2016U1U>1 \u2212 V\u03031V\u0303>1 \u20162} = max{\u2016 sin \u03a6\u20162, \u2016 sin \u0398\u20162} \u2264 \u2016E\u20162 \u03b4 .\nF.3 Lemmas on random tensors\nLemma F.9 (Spectral norm bound of random tensors, [27]). Suppose X is a pth order tensor with dimensions d1, \u00b7 \u00b7 \u00b7 , dp and each element of X is sampled i.i.d. from Gaussian distribution N (0, \u03c32). Then the following upper bound on \u2016X\u2016op holds with probability at least (1\u2212 \u03b4):\n\u2016X\u2016op \u2264 \u221a\u221a\u221a\u221a8\u03c32(( p\u2211 k=1 dp ) ln(2K/K0) + ln(2/\u03b4) ) ,\nwhere K0 = ln(3/2)."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Tensor decomposition is an important tool for big data analysis. In this paper,<lb>we resolve many of the key algorithmic questions regarding robustness, memory<lb>efficiency, and differential privacy of tensor decomposition. We propose simple<lb>variants of the tensor power method which enjoy these strong properties. We present<lb>the first guarantees for online tensor power method which has a linear memory<lb>requirement. Moreover, we present a noise calibrated tensor power method with<lb>efficient privacy guarantees. At the heart of all these guarantees lies a careful<lb>perturbation analysis derived in this paper which improves up on the existing<lb>results significantly.<lb>", "creator": "LaTeX with hyperref package"}}}