{"id": "1606.06630", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "On Multiplicative Integration with Recurrent Neural Networks", "abstract": "we introduce a general and simple structural design test called multiplicative integration ( mi ) to improve recurrent model neural networks ( rnns ). mi changes the way data in which information from difference valued sources includes flows and is integrated in the computational building block of an rnn, while introducing almost substantially no extra parameters. the new structure can safely be easily embedded into many popular rnn standard models, some including lstms and grus. we empirically analyze potentially its learning support behaviour and conduct evaluations on several tasks using different rnn models. our experimental results demonstrate that multiplicative integration can provide a substantial performance boost indeed over many of the existing rnn models.", "histories": [["v1", "Tue, 21 Jun 2016 15:55:29 GMT  (71kb,D)", "http://arxiv.org/abs/1606.06630v1", "11 pages, 2 figures"], ["v2", "Sat, 12 Nov 2016 19:47:10 GMT  (187kb,D)", "http://arxiv.org/abs/1606.06630v2", "10 pages, 2 figures; To appear in NIPS2016"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yuhuai wu", "saizheng zhang", "ying zhang", "yoshua bengio", "ruslan salakhutdinov"], "accepted": true, "id": "1606.06630"}, "pdf": {"name": "1606.06630.pdf", "metadata": {"source": "CRF", "title": "On Multiplicative Integration with Recurrent Neural Networks", "authors": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3]. Most of these designs are derived from popular structures including vanilla RNNs, Long Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5]. Despite of their varying characteristics, most of them share a common computational building block, described by the following equation:\n\u03c6(Wx+Uz + b), (1)\nwhere x \u2208 Rn and z \u2208 Rm are state vectors coming from different information sources, W \u2208 Rd\u00d7n and U \u2208 Rd\u00d7m are state-to-state transition matrices, and b is a bias vector. This computational building block serves as a combinator for integrating information flow from the x and z by a sum operation \u201c+\u201d, followed by a nonlinearity \u03c6. We refer to it as the additive building block. Additive building blocks are widely implemented in various state computations in RNNs (e.g. hidden state computations for vanilla-RNNs, gate/cell computations of LSTMs and GRUs.\nIn this work, we propose an alternative design for constructing the computational building block by changing the procedure of information integration. Specifically, instead of utilizing sum operation \u201c+\", we propose to use the Hadamard product \u201c \u201d to fuse Wx and Uz:\n\u03c6(Wx Uz + b) (2)\nThe result of this modification changes the RNN from first order to second order [6], while introducing no extra parameters. We call this kind of information integration design a form of Multiplicative Integration. The effect of multiplication naturally results in a gating type structure, in which Wx and Uz are the gates of each other. More specifically, one can think of the state-to-state computation Uz (where for example z represents the previous state) as dynamically rescaled by Wx (where for example x represents the input). Such rescaling does not exist in the additive building block, in which Uz is independent of x. This relatively simple modification brings about advantages over the additive building block as it alters RNN\u2019s gradient properties, which we discuss in detail in the next section, as well as verify through extensive experiments.\n\u2217Equal contribution.\nar X\niv :1\n60 6.\n06 63\n0v 1\n[ cs\n.L G\n] 2\n1 Ju\nn 20\nIn the following sections, we first introduce a general formulation of Multiplicative Integration. We then compare it to the additive building block on several sequence learning tasks, including character level language modelling, speech recognition, large scale sentence representation learning using a Skip-Thought model, and teaching a machine to read and comprehend for a question answering task. The experimental results (together with several existing state-of-the-art models) show that various RNN structures (including vanilla RNNs, LSTMs, and GRUs) equipped with Multiplicative Integration provide better generalization and easier optimization. Its main advantages include: (1) it enjoys better gradient properties due to the gating effect. Most of the hidden units are non-saturated; (2) the general formulation of Multiplicative Integration naturally includes the regular additive building block as a special case, and introduces almost no extra parameters compared to the additive building block; and (3) it is a drop-in replacement for the additive building block in most of the popular RNN models, including LSTMs and GRUs. It can also be combined with other RNN training techniques such as Recurrent Batch Normalization [7]. We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10]."}, {"heading": "2 Structure Description and Analysis", "text": ""}, {"heading": "2.1 General Formulation of Multiplicative Integration", "text": "The key idea behind Multiplicative Integration is to integrate different information flows Wx and Uz, by the Hadamard product \u201c \u201d. A more general formulation of Multiplicative Integration includes two more bias vectors \u03b21 and \u03b22 added to Wx and Uz:\n\u03c6((Wx+ \u03b21) (Uz + \u03b22) + b) (3) where \u03b21,\u03b22 \u2208 Rd are bias vectors. Notice that such formulation contains the first order terms as in a additive building block, i.e., \u03b21 Uht\u22121 + \u03b22 Wxt. In order to make the Multiplicative Integration more flexible, we introduce another bias vector \u03b1 \u2208 Rd to gate2 the term Wx Uz, obtaining the following formulation:\n\u03c6(\u03b1 Wx Uz + \u03b21 Uz + \u03b22 Wx+ b), (4) Note that the number of parameters of the Multiplicative Integration is about the same as that of the additive building block, since the number of new parameters (\u03b1, \u03b21 and \u03b22) are negligible compared to total number of parameters. Also, Multiplicative Integration can be easily extended to LSTMs and GRUs3, that adopt vanilla building blocks for computing gates and output states, where one can directly replace them with the Multiplicative Integration. More generally, in any kind of structure where k information flows (k \u2265 2) are involved (e.g. RNN with multiple skip connections [11] or in feedforward models like residual networks [12]), one can implement pairwise Multiplicative Integration for integrating all k information sources."}, {"heading": "2.2 Gradient Properties", "text": "The Multiplicative Integration has different gradient properties compared to the additive building block. For clarity of presentation, we first look at vanilla-RNN and RNN with Multiplicative Integration embedded, referred to as MI-RNN. That is, ht = \u03c6(Wxt + Uht\u22121 + b) versus ht = \u03c6(Wxt Uht\u22121 + b). In a vanilla-RNN, the gradient \u2202ht\u2202ht\u2212n can be computed as follows:\n\u2202ht \u2202ht\u2212n = t\u220f k=t\u2212n+1 UTdiag(\u03c6\u2032k), (5)\nwhere \u03c6\u2032k = \u03c6 \u2032(Wxk +Uhk\u22121 +b). The equation above shows that the gradient flow through time heavily depends on the hidden-to-hidden matrix U, but W and xk appear to play a limited role: they only come in the derivative of \u03c6\u2032 mixed with Uhk\u22121. On the other hand, the gradient \u2202ht\u2202ht\u2212n of a MI-RNN is4:\n\u2202ht \u2202ht\u2212n = t\u220f k=t\u2212n+1 UTdiag(Wxk)diag(\u03c6 \u2032 k), (6)\n2If \u03b1 = 0, the Multiplicative Integration will degenerate to the vanilla additive building block. 3See exact formulations in the Appendix. 4Here we adopt the simplest formulation of Multiplicative Integration for illustration. In the more general\ncase (Eq. 4), diag(Wxk) in Eq. 6 will become diag(\u03b1 Wxk + \u03b21).\nwhere \u03c6\u2032k = \u03c6 \u2032(Wxk Uhk\u22121 + b). By looking at the gradient, we see that the matrix W and the current input xk is directly involved in the gradient computation by gating the matrix U, hence more capable of altering the updates of the learning system. As we show in our experiments, with Wxk directly gating the gradient, the vanishing/exploding problem is alleviated: Wxk dynamically reconciles U, making the gradient propagation easier compared to the regular RNNs. For LSTMs and GRUs with Multiplicative Integration, the gradient propagation properties are more complicated. But in principle, the benefits of the gating effect also persists in these models."}, {"heading": "3 Experiments", "text": "In all of our experiments, we use the general form of Multiplicative Integration (Eq. 4) for any hidden state/gate computations, unless otherwise specified."}, {"heading": "3.1 Exploratory Experiments", "text": "To further understand the functionality of Multiplicative Integration, we take a simple RNN for illustration, and perform several exploratory experiments on the character level language modeling task using Penn-Treebank dataset [13], following the data partition in [14]. The length of the training sequence is 50. All models have a single hidden layer of size 2048, and we use Adam optimization algorithm [15] with learning rate 1e\u22124. Weights are initialized to samples drawn from uniform[\u22120.02, 0.02]. Performance is evaluated by the bits-per-character (BPC) metric, which is log2 of perplexity."}, {"heading": "3.1.1 Gradient Properties", "text": "To analyze the gradient flow of the model, we divide the gradient in Eq. 6 into two parts: 1. the gated matrix products: UTdiag(Wxk), and 2. the derivative of the nonlinearity \u03c6\u2032, We separately analyze the properties of each term compared to the additive building block. We first focus on the gating effect brought by diag(Wxk). In order to separate out the effect of nonlinearity, we chose \u03c6 to be the identity map, hence both vanilla-RNN and MI-RNN reduce to linear models, referred to as lin-RNN and lin-MI-RNN.\nFor each model we monitor the log-L2-norm of the gradient log||\u2202C/\u2202ht||2 (averaged over the training set) after every training epoch, where ht is the hidden state at time step t, and C is the negative log-likelihood of the single character prediction at the final time step (t = 50). Figure. 1 shows the evolution of the gradient norms for small t, i.e., 0, 5, 10, as they better reflect the gradient propagation behaviour. Observe that the norms of lin-MI-RNN (orange) increase rapidly and soon exceed the corresponding norms of lin-RNN by a large margin. The norms of lin-RNN stay close to zero (\u2248 10\u22124) and their changes over time are almost negligible. This observation implies that with the help of diag(Wxk) term, the gradient vanishing of lin-MI-RNN can be alleviated compared to lin-RNN. The final test BPC (bits-per-character) of lin-MI-RNN is 1.48, which is comparable to a vanilla-RNN with stabilizing regularizer [16], while lin-RNN performs rather poorly, achieving a test BPC of over 2.\nNext we look into the nonlinearity \u03c6. We chose \u03c6 = tanh for both vanilla-RNN and MI-RNN. Figure 1 (c) and (d) shows a comparison of histograms of hidden activations over all time steps on the validation set after training. Interestingly, in (c) for vanilla-RNN, most activations are saturated with values around \u00b11, whereas in (d) for MI-RNN, most activations are non-saturated with values around 0. This has a direct consequence in gradient propagation: non-saturated activations imply that diag(\u03c6\u2032k) \u2248 1 for \u03c6 = tanh, which can help gradients propagate, whereas saturated activations imply that diag(\u03c6\u2032k) \u2248 0, resulting in gradients vanishing."}, {"heading": "3.1.2 Scaling Problem", "text": "When adding two numbers at different order of magnitude, the smaller one might be negligible for the sum. However, when multiplying two numbers, the value of the product depends on both regardless of the scales. This principle also applies when comparing Multiplicative Integration to the additive building blocks. In this experiment, we test whether Multiplicative Integration is more robust to the scales of weight values. Following the same models as in Section 3.1.1, we first calculated the norms of Wxk and Uhk\u22121 for both vanilla-RNN and MI-RNN for different k after training. We found that in both structures, Wxk is a lot smaller than Uhk\u22121 in magnitude. This might be due to the fact that xk is a one-hot vector, making the number of updates for (columns of) W be smaller than U. As a result, in vanilla-RNN, the pre-activation term Wxk +Uhk\u22121 is largely controlled by the value of Uhk\u22121, while Wxk becomes rather small. In MI-RNN, on the other hand, the pre-activation term Wxk Uhk\u22121 still depends on the values of both Wxk and Uhk\u22121, due to multiplication.\nWe next tried different initialization of W and U to test their sensitivities to the scaling. For each model, we fix the initialization of U to uniform[\u22120.02, 0.02] and initialize W to uniform[\u2212rW, rW] where rW varies in {0.02, 0.1, 0.3, 0.6}. Table 1, top left panel, shows results. As we increase the scale of W, performance of the vanilla-RNN improves, suggesting that the model is able to better utilize the input information. On the other hand, MI-RNN is much more robust to different initializations, where the scaling has almost no effect on the final performance."}, {"heading": "3.1.3 On different choices of the formulation", "text": "In our third experiment, we evaluated the performance of different computational building blocks, which are Eq. 1 (vanilla-RNN), Eq. 2 (MI-RNN-simple) and Eq. 4 (MI-RNN-general)5. From the validation curves in Figure 1 (b), we see that both MI-RNN, simple and MI-RNN-general yield much better performance compared to vanilla-RNN, and MI-RNN-general has a faster convergence speed compared to MI-RNN-simple. We also compared our results to the previously published models in Table 1, bottom left panel, where MI-RNN-general achieves a test BPC of 1.39, which is to our knowledge the best result for RNNs on this task without complex gating/cell mechanisms."}, {"heading": "3.2 Character Level Language Modeling", "text": "In addition to the Penn-Treebank dataset, we also perform character level language modeling on two larger datasets: text86 and Hutter Challenge Wikipedia7. Both of them contain 100M characters from Wikipedia while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet size of 205. For both datasets, we follow the training protocols in [14] and [1] respectively. We use Adam for optimization with the starting learning rate grid-searched in {0.002, 0.001, 0.0005}. If the validation BPC (bits-per-character) does not decrease for 2 epochs, we half the learning rate.\nWe implemented Multiplicative Integration on both vanilla-RNN and LSTM, referred to as MIRNN and MI-LSTM. The results for the text8 dataset are shown in Table 1, bottom middle panel. All five models, including some of the previously published models, have the same number of\n5We perform hyper-parameter search for the initialization of {\u03b1,\u03b21,\u03b22,b} in MI-RNN-general. 6http://mattmahoney.net/dc/textdata 7http://prize.hutter1.net/\nparameters (\u22484M). For RNNs without complex gating/cell mechanisms (the first three results), our MI-RNN (with {\u03b1,\u03b21,\u03b22,b} initialized as {2, 0.5, 0.5, 0}) performs the best, our MI-LSTM (with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 0.5, 0.5, 0}) outperforms all other models by a large margin8. On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2]. Table 1, bottom right panel, shows results. Despite the simple structure compared to the sophisticated connection designs in GF-LSTM and grid-LSTM, our MI-LSTM outperforms all other models and achieves the new state-of-the-art on this task."}, {"heading": "3.3 Speech Recognition", "text": "We next evaluate our models on Wall Street Journal (WSJ) corpus (available as LDC corpus LDC93S6B and LDC94S13B), where we use the full 81 hour set \u201csi284\u201d for training, set \u201cdev93\u201d for validation and set \u201ceval92\u201d for test. We follow the same data preparation process and model setting as in [20], and we use 59 characters as the targets for the acoustic modelling. Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].\nOur model (referred to as MI-LSTM+CTC+WFST) consists of 4 bidirectional MI-LSTM layers, each with 320 units for each direction. CTC is performed on top to resolve the alignment issue in speech transcription. For comparison, we also train a baseline model (referred to as LSTM+CTC+WFST) with the same size but using vanilla LSTM. Adam with learning rate 0.0001 is used for optimization and Gaussian weight noise with zero mean and 0.05 standard deviation is injected for regularization. We evaluate our models on the character error rate (CER) without language model and the word error rate (WER) with extended trigram language model.\nTable 1, top right panel, shows that MI-LSTM+CTC+WFST achieves quite good results on both CER and WER compared to recent works, and it has a clear improvement over the baseline model. Note that we did not conduct a careful hyper-parameter search on this task, hence one could potentially obtain better results with better decoding schemes and regularization techniques."}, {"heading": "3.4 Learning Skip-Thought Vectors", "text": "Next, we evaluate our Multiplicative Integration on the Skip-Thought model of [25]. Skip-Thought is an encoder-decoder model that attempts to learn generic, distributed sentence representations. The model produces sentence representation that are robust and perform well in practice, as it achieves excellent results across many different NLP tasks. The model was trained on the BookCorpus dataset that consists of 11,038 books with 74,004,228 sentences. Not surprisingly, a single pass through\n8[7] reports better results but they use much larger models (\u224816M) which is not directly comparable.\nthe training data can take up to a week on a high-end GPU (as reported in [25]). Such training speed largely limits one to perform careful hyper-parameter search. However, with Multiplicative Integration, not only the training time is shortened by a factor of two, but the final performance is also significantly improved.\nWe exactly follow the authors\u2019 Theano implementation of the skip-thought model9: Encoder and decoder are single-layer GRUs with hidden-layer size of 2400; all recurrent matrices adopt orthogonal initialization while non-recurrent weights are initialized from uniform distribution. Adam is used for optimization. We implemented Multiplicative Integration only for the encoder GRU (embedding MI into decoder did not provide any substantial gains). We refer our model as MI-uni-skip, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}. We also train a baseline model with the same size, referred to as uni-skip(ours), which essentially reproduces the original model of [25].\nDuring the course of training, we evaluated the skip-thought vectors on the semantic relatedness task, using SICK dataset, every 2500 updates for both MI-uni-skip and the baseline model (each iteration processes a mini-batch of size 64). The results are shown in Figure 2a. Note that MI-uni-skip significantly outperforms the baseline, not only in terms of speed of convergence, but also in terms of final performance. At around 125k updates, MI-uni-skip already exceeds the best performance achieved by the baseline, which takes about twice the number of updates.\nWe also evaluated both models after one week of training, with the best results being reported on six out of eight tasks reported in [25]: semantic relatedness task on SICK dataset, paraphrase detection task on Microsoft Research Paraphrase Corpus, and four classification benchmarks: movie review sentiment (MR), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), and opinion polarity (MPQA). We also compared our results with the results reported on three models in the original skip-thought paper: uni-skip, bi-skip, combine-skip. Uni-skip is the same model as our baseline, bi-skip is a bidirectional model of the same size, and combine-skip takes the concatenation of the vectors from uni-skip and bi-skip to form a 4800 dimension vector for task evaluation. Table 2 shows that MI-uni-skip dominates across all the tasks. Not only it achieves higher performance than the baseline model, but in many cases, it also outperforms the combine-skip model, which has twice the number of dimensions. Clearly, Multiplicative Integration provides a faster and better way to train a large-scale Skip-Thought model."}, {"heading": "3.5 Teaching Machines to Read and Comprehend", "text": "In our last experiment, we show that the use of Multiplicative Integration can be combined with other techniques for training RNNs, and the advantages of using MI still persist. Recently, [7] introduced Batch-Normalization [26] for RNNs. They evaluated their proposed technique on a uni-directional\n9https://github.com/ryankiros/skip-thoughts\nAttentive Reader Model [27] for the question answering task using the CNN corpus10. To test our approach, we evaluated the following four models: 1. A vanilla LSTM attentive reader model with a single hidden layer size 240 (same as [7]) as our baseline, referred to as LSTM (ours), 2. A multiplicative integration LSTM with a single hidden size 240, referred to as MI-LSTM, 3. MILSTM with Batch-Norm, referred to as MI-LSTM+BN, 4. MI-LSTM with Batch-Norm everywhere (as detailed in [7]), referred to as MI-LSTM+BN-everywhere. We compared our models to results reported in [7] (referred to as LSTM, BN-LSTM and BN-LSTM everywhere) 11.\nFor all MI models, {\u03b1,\u03b21,\u03b22,b} were initialized to {1, 1, 1, 0}. We follow the experimental protocol of [7]12 and use exactly the same settings as theirs, except we remove the gradient clipping for MI-LSTMs. Figure. 2b shows validation curves of the baseline (LSTM), MI-LSTM, BN-LSTM, and MI-LSTM+BN, and the final validation errors of all models are reported in Table 2, bottom right panel. Clearly, using Multiplicative Integration results in improved model performance regardless of whether Batch-Norm is used. However, the combination of MI and Batch-Norm provides the best performance and the fastest speed of convergence. This shows the general applicability of Multiplication Integration when combining it with other optimization techniques."}, {"heading": "4 Relationship to Previous Models", "text": ""}, {"heading": "4.1 Relationship to Hidden Markov Models", "text": "One can show that under certain constraints, MI-RNN is effectively implementing the forward algorithm of the Hidden Markov Model(HMM). A direct mapping can be constructed as follows (see [28] for a similar derivation). Let U \u2208 Rm\u00d7m be the state transition probability matrix with Uij = Pr[ht+1 = i|ht = j], W \u2208 Rm\u00d7n be the observation probability matrix with Wij = Pr[xt = i|ht = j]. When xt is a one-hot vector (e.g., in many of the language modelling tasks), multiplying it by W is effectively choosing a column of the observation matrix. Namely, if the jth entry of xt is one, then Wxt = Pr[xt|ht = j]. Let h0 be the initial state distribution with h0 = Pr[h0] and {ht}t\u22651 be the alpha values in the forward algorithm of HMM, i.e., ht = Pr[x1, ..., xt, ht]. Then Uht = Pr[x1, ..., xt, ht+1]. Thus ht+1 = Wxt+1 Uht = Pr[xt+1|ht+1] \u00b7Pr[x1, ..., xt, ht+1] = Pr[x1, ..., xt+1, ht+1]. To exactly implement the forward algorithm using Multiplicative Integration, the matrices W and U have to be probability matrices, and xt needs to be a one-hot vector. The function \u03c6 needs to be linear, and we drop all the bias terms. Therefore, RNN with Multiplicative Integration can be seen as a nonlinear extension of HMMs. The extra freedom in parameter values and nonlinearity makes the model more flexible compared to HMMs."}, {"heading": "4.2 Relations to Second Order RNNs and Multiplicative RNNs", "text": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10]. We first describe the similarities with these two models:\nThe second order RNN involves a second order term st in a vanilla-RNN, where the ith element st,i is computed by the bilinear form: st,i = xTt T (i)ht\u22121, where T (i) \u2208 Rn\u00d7m(1 \u2264 i \u2264 m) is\n10Note that [7] used a truncated version of the original dataset in order to save computation. 11Learning curves and the final result number are obtained by emails correspondence with authors of [7]. 12https://github.com/cooijmanstim/recurrent-batch-normalization.git.\nthe ith slice of a tensor T \u2208 Rm\u00d7n\u00d7m. Multiplicative Integration also involve a second order term st = \u03b1 Wxt Uht\u22121, but in our case st,i = \u03b1i(wi \u00b7 xt)(ui \u00b7 ht\u22121) = xTt (\u03b1wi \u2297 ui)ht\u22121, where wi and ui are ith row in W and U, and \u03b1i is the ith element of \u03b1. Note that the outer product \u03b1iwi \u2297 ui is a rank-1 matrix. The Multiplicative RNN is also a second order RNN, but which approximates T by a tensor decomposition \u2211 x (i) t T (i) = Pdiag(Vxt)Q. For MI-RNN, we can also think of the second order term as a tensor decomposition: \u03b1 Wxt Uht\u22121 = U(xt)ht\u22121 = [diag(\u03b1)diag(Wxt)U]ht\u22121.\nThere are however several differences that make MI a favourable model: (1) Simpler Parametrization: MI uses a rank-1 approximation compared to the second order RNNs, and a diagonal approximation compared to Multiplicative RNN. Moreover, MI-RNN shares parameters across the first and second order terms, whereas the other two models do not. As a result, the number of parameters are largely reduced, which makes our model more practical for large scale problems, while avoiding overfitting. (2) Easier Optimization: In tensor decomposition methods, the products of three different (low-rank) matrices generally makes it hard to optimize [10]. However, the optimization problem becomes easier in MI, as discussed in section 2 and 3. (3) General structural design vs. vanilla-RNN design: Multiplicative Integration can be easily embedded in many other RNN structures, e.g. LSTMs and GRUs, whereas the second order RNN and MRNN present a very specific design for modifying vanilla-RNNs.\nMoreover, we also compared MI-RNN\u2019s performance to the previous HF-MRNN\u2019s results (Multiplicative RNN trained by Hessian-free method) in Table 1, bottom left and bottom middle panels, on Penn-Treebank and text8 datasets. One can see that MI-RNN outperforms HF-MRNN on both tasks."}, {"heading": "4.3 General Multiplicative Integration", "text": "Multiplicative Integration can be viewed as a general way of combining information flows from two different sources. In particular, [29] proposed the ladder network that achieves promising results on semi-supervised learning. In their model, they combine the lateral connections and the backward connections via the \u201ccombinator\u201d function by a Hadamard product. The performance would severely degrade without this product as empirically shown by [30]. [31] explored neural embedding approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions, and compared a variety of embedding models on the link prediction task. Surprisingly, the best results among all bilinear functions is the simple weighted Hadamard product. They further carefully compare the multiplicative and additive interactions and show that the multiplicative interaction dominates the additive one."}, {"heading": "5 Conclusion", "text": "In this paper we proposed to use Multiplicative Integration (MI), a simple Hadamard product to combine information flow in recurrent neural networks. MI can be easily integrated into many popular RNN models, including LSTMs and GRUs, while introducing almost no extra parameters. Indeed, the implementation of MI requires almost no extra work beyond implementing RNN models. We also show that MI achieves state-of-the-art performance on four different tasks or 11 datasets of varying sizes and scales. We believe that the Multiplicative Integration can become a default building block for training various types of RNN models."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Disney research and ONR Grant N00014-14-10232. The authors thank the developers of Theano [32] and Keras [33], and also thank Jimmy Ba for many thought-provoking discussions."}], "references": [{"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.02367,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv preprint arXiv:1507.01526,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "First-order versus second-order single-layer recurrent neural networks", "author": ["Mark W Goudreau", "C Lee Giles", "Srimat T Chakradhar", "D Chen"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Recurrent batch normalization", "author": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "Aaron Courville"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "An inequality with application to statistical estimation for probabilistic functions of markov processes and to a model for ecology", "author": ["LE Baum", "JA Eagon"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1967}, {"title": "Second-order recurrent neural networks for grammatical inference", "author": ["C Lee Giles", "D Chen", "CB Miller", "HH Chen", "GZ Sun", "YC Lee"], "venue": "In Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Architectural complexity measures of recurrent neural networks", "author": ["Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan Salakhutdinov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.08210,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1993}, {"title": "Subword language modeling with neural networks", "author": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink"], "venue": "preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Regularizing rnns by stabilizing activations", "author": ["David Krueger", "Roland Memisevic"], "venue": "arXiv preprint arXiv:1511.08400,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns", "author": ["Awni Y Hannun", "Andrew L Maas", "Daniel Jurafsky", "Andrew Y Ng"], "venue": "arXiv preprint arXiv:1408.2873,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "arXiv preprint arXiv:1507.08240,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Regularization and nonlinearities for neural language models: when are they needed", "author": ["Marius Pachitariu", "Maneesh Sahani"], "venue": "arXiv preprint arXiv:1301.5650,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["Mehryar Mohri", "Fernando Pereira", "Michael Riley"], "venue": "Computer Speech & Language,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Refining hidden markov models with recurrent neural networks", "author": ["T. Wessels", "C.W. Omlin"], "venue": "In Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2000}, {"title": "Semi-supervised learning with ladder network", "author": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Deconstructing the ladder network architecture", "author": ["Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06430,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "arXiv preprint arXiv:1412.6575,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Theano: A python framework for fast computation of mathematical expressions, 2016", "author": ["Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].", "startOffset": 100, "endOffset": 109}, {"referenceID": 1, "context": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].", "startOffset": 100, "endOffset": 109}, {"referenceID": 2, "context": "Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs) [1, 2, 3].", "startOffset": 100, "endOffset": 109}, {"referenceID": 3, "context": "Most of these designs are derived from popular structures including vanilla RNNs, Long Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "Most of these designs are derived from popular structures including vanilla RNNs, Long Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5].", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "The result of this modification changes the RNN from first order to second order [6], while introducing no extra parameters.", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "It can also be combined with other RNN training techniques such as Recurrent Batch Normalization [7].", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 117, "endOffset": 123}, {"referenceID": 5, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 117, "endOffset": 123}, {"referenceID": 9, "context": "We further discuss its relationship to existing models, including Hidden Markov Models (HMMs) [8], second order RNNs [9, 6] and Multiplicative RNNs [10].", "startOffset": 148, "endOffset": 152}, {"referenceID": 10, "context": "RNN with multiple skip connections [11] or in feedforward models like residual networks [12]), one can implement pairwise Multiplicative Integration for integrating all k information sources.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "RNN with multiple skip connections [11] or in feedforward models like residual networks [12]), one can implement pairwise Multiplicative Integration for integrating all k information sources.", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "1 Exploratory Experiments To further understand the functionality of Multiplicative Integration, we take a simple RNN for illustration, and perform several exploratory experiments on the character level language modeling task using Penn-Treebank dataset [13], following the data partition in [14].", "startOffset": 254, "endOffset": 258}, {"referenceID": 13, "context": "1 Exploratory Experiments To further understand the functionality of Multiplicative Integration, we take a simple RNN for illustration, and perform several exploratory experiments on the character level language modeling task using Penn-Treebank dataset [13], following the data partition in [14].", "startOffset": 292, "endOffset": 296}, {"referenceID": 14, "context": "All models have a single hidden layer of size 2048, and we use Adam optimization algorithm [15] with learning rate 1e\u22124.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "48, which is comparable to a vanilla-RNN with stabilizing regularizer [16], while lin-RNN performs rather poorly, achieving a test BPC of over 2.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "For both datasets, we follow the training protocols in [14] and [1] respectively.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "For both datasets, we follow the training protocols in [14] and [1] respectively.", "startOffset": 64, "endOffset": 67}, {"referenceID": 16, "context": "DRNN+CTCbeamsearch [17] 10.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "1 Encoder-Decoder [18] 6.", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "3 LSTM+CTCbeamsearch [19] 9.", "startOffset": 21, "endOffset": 25}, {"referenceID": 19, "context": "7 Eesen [20] - 7.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "RNN [14] 1.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "42 HF-MRNN [14] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "41 RNN+stabalization [16] 1.", "startOffset": 21, "endOffset": 25}, {"referenceID": 20, "context": "RNN+smoothReLu [21] 1.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "55 HF-MRNN [14] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 21, "context": "stacked-LSTM [22] 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "67 GF-LSTM [1] 1.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "58 grid-LSTM [2] 1.", "startOffset": 13, "endOffset": 16}, {"referenceID": 21, "context": "On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 1, "context": "On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit, \u224817M, with {\u03b1,\u03b21,\u03b22,b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers, \u224827M) [22], GF-LSTM (5 layers, \u224820M) [1], and grid-LSTM (6 layers, \u224817M) [2].", "startOffset": 257, "endOffset": 260}, {"referenceID": 19, "context": "We follow the same data preparation process and model setting as in [20], and we use 59 characters as the targets for the acoustic modelling.", "startOffset": 68, "endOffset": 72}, {"referenceID": 22, "context": "Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "Decoding is done with the CTC [23] based weighted finite-state transducers (WFSTs) [24] as proposed by [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "Next, we evaluate our Multiplicative Integration on the Skip-Thought model of [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "[7] reports better results but they use much larger models (\u224816M) which is not directly comparable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "uni-skip [25] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "2872 bi-skip [25] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "2995 combine-skip [25] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "uni-skip [25] 73.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "9 bi-skip [25] 71.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "2 combine-skip [25] 73.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "uni-skip [25] 75.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "9 bi-skip [25] 73.", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "3 combine-skip [25] 76.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "LSTM [7] 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "5033 BN-LSTM [7] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "4951 BN-everywhere [7] 0.", "startOffset": 19, "endOffset": 22}, {"referenceID": 24, "context": "the training data can take up to a week on a high-end GPU (as reported in [25]).", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "We also train a baseline model with the same size, referred to as uni-skip(ours), which essentially reproduces the original model of [25].", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "We also evaluated both models after one week of training, with the best results being reported on six out of eight tasks reported in [25]: semantic relatedness task on SICK dataset, paraphrase detection task on Microsoft Research Paraphrase Corpus, and four classification benchmarks: movie review sentiment (MR), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), and opinion polarity (MPQA).", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Recently, [7] introduced Batch-Normalization [26] for RNNs.", "startOffset": 10, "endOffset": 13}, {"referenceID": 25, "context": "Recently, [7] introduced Batch-Normalization [26] for RNNs.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "LSTM [7]", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "BN-LSTM [7]", "startOffset": 8, "endOffset": 11}, {"referenceID": 26, "context": "Attentive Reader Model [27] for the question answering task using the CNN corpus10.", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "A vanilla LSTM attentive reader model with a single hidden layer size 240 (same as [7]) as our baseline, referred to as LSTM (ours), 2.", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "MI-LSTM with Batch-Norm everywhere (as detailed in [7]), referred to as MI-LSTM+BN-everywhere.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "We compared our models to results reported in [7] (referred to as LSTM, BN-LSTM and BN-LSTM everywhere) 11.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "We follow the experimental protocol of [7]12 and use exactly the same settings as theirs, except we remove the gradient clipping for MI-LSTMs.", "startOffset": 39, "endOffset": 42}, {"referenceID": 27, "context": "A direct mapping can be constructed as follows (see [28] for a similar derivation).", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10].", "startOffset": 42, "endOffset": 48}, {"referenceID": 9, "context": "MI-RNN is related to the second order RNN [9, 6] and the multiplicative RNN (MRNN) [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "Note that [7] used a truncated version of the original dataset in order to save computation.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Learning curves and the final result number are obtained by emails correspondence with authors of [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "(2) Easier Optimization: In tensor decomposition methods, the products of three different (low-rank) matrices generally makes it hard to optimize [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 28, "context": "In particular, [29] proposed the ladder network that achieves promising results on semi-supervised learning.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "The performance would severely degrade without this product as empirically shown by [30].", "startOffset": 84, "endOffset": 88}, {"referenceID": 30, "context": "[31] explored neural embedding approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions, and compared a variety of embedding models on the link prediction task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The authors thank the developers of Theano [32] and Keras [33], and also thank Jimmy Ba for many thought-provoking discussions.", "startOffset": 43, "endOffset": 47}], "year": 2016, "abstractText": "We introduce a general and simple structural design called \u201cMultiplicative Integration\u201d (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.", "creator": "LaTeX with hyperref package"}}}