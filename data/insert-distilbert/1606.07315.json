{"id": "1606.07315", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Nearly-optimal Robust Matrix Completion", "abstract": "in this paper, we consider the problem of robust matrix completion ( rmc ) where the goal is to recover a low - rank matrix by closely observing a small number of its entries out somewhere of which a few can otherwise be arbitrarily corrupted. we propose a simple projected gradient descent method to estimate the low - rank matrix that alternately performs a projected gradient posterior descent clearing step and cleans up a few of the corrupted entries using hard - thresholding. our algorithm naturally solves rmc using nearly optimal number of observations as well if as nearly optimal number of corruptions. our result also implies significant improvement over the existing time complexity bounds for the low - rank matrix completion problem. finally, an application overview of our result to the robust pca problem ( low - rank + sparse matrix separation ) leads to nearly linear time ( in matrix dimensions ) algorithm for the same ; existing state - of - the - art methods require quadratic time. our empirical results corroborate our theoretical results and show notice that even for promising moderate sized problems, our method for robust pca is an an order of magnitude faster than the existing methods.", "histories": [["v1", "Thu, 23 Jun 2016 13:57:56 GMT  (1562kb)", "http://arxiv.org/abs/1606.07315v1", null], ["v2", "Wed, 7 Dec 2016 20:05:13 GMT  (1588kb)", "http://arxiv.org/abs/1606.07315v2", null], ["v3", "Thu, 8 Dec 2016 19:48:40 GMT  (1569kb)", "http://arxiv.org/abs/1606.07315v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["yeshwanth cherapanamjeri", "kartik gupta", "prateek jain"], "accepted": true, "id": "1606.07315"}, "pdf": {"name": "1606.07315.pdf", "metadata": {"source": "CRF", "title": "Nearly-optimal Robust Matrix Completion", "authors": ["Yeshwanth Cherapanamjeri", "Kartik Gupta", "Prateek Jain"], "emails": ["t-yecher@microsoft.com", "t-kagu@microsoft.com", "prajain@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n07 31\n5v 1\n[ cs\n.L G\n] 2\n3 Ju\nn 20"}, {"heading": "1 Introduction", "text": "In this paper, we study the Robust Matrix Completion (RMC) problem where the goal is to recover an underlying low-rank matrix by observing a small number of sparsely corrupted entries from the matrix. Formally, RMC: Find rank-r matrix L\u2217 \u2208 Rm\u00d7n using \u2126 and P\u2126(L\u2217) + S\u2217, (1) where \u2126 \u2286 [m]\u00d7 [n] is the set of observed entries (throughout the paper we assume that m \u2264 n), S\u2217 denotes the sparse corruptions of the observed entries, i.e., Supp(S\u2217) \u2208 \u2126. Sampling operator P\u2126 : Rm\u00d7n \u2192 Rm\u00d7n is defined as:\n(P\u2126(A))ij = Aij , if (i, j) \u2208 \u2126, (P\u2126(A))ij = 0, otherwise. (2)\nRMC is an important problem with several applications such as recommendation systems with outliers. Similarly, the problem is also heavily used to model PCA under gross outliers as well as erasures [JRVS11]. Finally, as we show later, an efficient solution to RMC enables faster solution for the robust PCA (RPCA) problem as well. The goal in RPCA is to find a low-rank matrix L\u2217 and sparse matrix S\u0303\u2217 by observing their sum, i.e., M = L\u2217 + S\u0303\u2217. State-of-the-art results for RPCA shows exact recovery of a rank-r, \u00b5-incoherent L\u2217 (see Assumption 1, Section 3) if at most \u03c1 = 1\n\u00b52r fraction of the entries in each row/column of S\u0303\u2217 are\ncorrupted [HKZ11, NUNS+14].\nHowever, the existing state-of-the-art results for RMC with optimal \u03c1 = 1 \u00b52r fraction of corrupted entries, either require at least a constant fraction of the entries of L\u2217 to be observed [CJSC11, CLMW11] or require restrictive assumptions like support of corruptions S\u0303\u2217 being uniformly random [Li13]. [KLT14] also considers RMC problem but studies the noisy setting and do not provide exact recovery bounds. Moreover, most of the\nexisting methods for RMC use convex relaxation for both low-rank and sparse components, and in general exhibit large time complexity (O(m2n)).\nIn this work, we attempt to answer the following open question (assuming m \u2264 n): Can RMC be solved exactly by using |\u2126| = O(rn log n) observations out of which O( 1\n\u00b52r ) fraction of the\nobserved entries in each row/column are corrupted.\nNote that both |\u2126| (for uniformly random \u2126) and \u03c1 values mentioned in the question above denote the information theoretic limits. Hence, the goal is to solve RMC for nearly-optimal number of samples and nearly-optimal fraction of corruptions.\nUnder standard assumptions on L\u2217, S\u2217, \u2126 and for n = O(m), we answer the above question in affirmative albeit with |\u2126| which is O(r) (ignoring log factors) larger than the optimal sample complexity (see Theorem 1). In particular, we propose a simple projected gradient (PGD) style method for RMC that alternately cleans up corrupted entries by hard-thresholding; our method\u2019s computational complexity is also nearly optimal (O(|\u2126|r+ (m+n)r2 + r3)). Note that our method applies non-convex opeators like low-rank projection and hard-thresholding. Hence, standard convex analysis techniques cannot be used for our algorithm.\nSeveral recent results [JN15, NUNS+14, JTK14, HW14, Blu11] show that under certain assumptions, projection onto non-convex sets indeed lead to provable algorithms with fast convergence to the global optima. However, as explained in Section 3, RMC presents unique set of challenges as we have to perform error analysis with the errors arising due to missing entries as well as sparse corruptions, both of which interact among themselves as well. In fact, our careful error analysis also enables us to improve results for the matrix completion as well as the RPCA problem.\nMatrix Completion (MC): The goal of MC is to find rank-r L\u2217 using P\u2126(L\u2217). State-of-the-art result for MC uses nuclear norm minimization and requires |\u2126| \u2265 \u00b52nr2 log2 n under standard \u00b5-incoherence assumption (see Section 3), but the method requires O(m2n) time in general. The best sample complexity result for a non-convex iterative method (with at most logarithmic dependence on the condition number of L\u2217) achieve exact recovery when |\u2126| \u2265 \u00b56nr5 log2 n and needs O(|\u2126|r) computational steps. In contrast, assuming n = O(m), our method achieves nearly the same sample complexity of trace-norm but with nearly linear time algorithm (O(|\u2126|r)). See Table 1 for a detailed comparison of our result with the existing methods.\nRPCA: Several recent results show that RPCA can be solved if \u03c1 = O( 1 \u00b52r )-fraction of entries in each row and column of L\u2217 are corrupted [NUNS+14, HKZ11] where L\u2217 is assumed to be \u00b5-incoherent. Moreover, St-NcRPCA algorithm [NUNS+14] can solve the problem in time O(mnr2). Corollary 2 shows that by sampling \u2126 uniformly at random, we can solve the problem in time O(nr3) only. That is, we can recover L\u2217 without even observing the entire input matrix. Moreover, if the goal is to recover the sparse corruption as well, then we can obtain a two-pass (over the input matrix) algorithm that solves the RPCA problem exactly. St-NcRPCA algorithm requires r2 log(1/\u01eb) passes over the data. Our method has significantly smaller space complexity as well.\nOur empirical results on synthetic data demonstrates effectiveness of our method. We also apply our method to the foreground background separation problem; our method is an order of magnitude faster than the stateof-the-art method (St-NcRPCA) while achieving similar accuracy.\nIn summary, this paper\u2019s main contributions are: (a) RMC: We propose a nearly linear time method that solves RMC with |\u2126| = O(nr2 log2 n log2 \u2016M\u20162/\u01eb) random entries and with optimal fraction of corruptions (\u03c1 = 1\n\u00b52r ).\n(b) Matrix Completion: Our result improves upon the existing linear time algorithm\u2019s sample complexity by an O(r3) factor, and time complexity by O(r4) factor, although with an extra O(log \u2016L\u2217\u2016/\u01eb) factor in both time and sample complexity. (c) RPCA: We present a nearly linear time (O(nr3)) algorithm for RPCA under optimal fraction of cor-\nruptions, improving upon O(mnr2) time complexity of the existing methods.\nNotations: We assume that M = L\u2217 + S\u0303\u2217 and P\u2126(M) = P\u2126(L\u2217) + S\u2217, i.e., S\u2217 = P\u2126(S\u0303\u2217). \u2016v\u2016p denotes \u2113p norm of a vector v; \u2016v\u2016 denotes \u21132 norm of v. \u2016A\u20162, \u2016A\u2016F , \u2016A\u2016\u2217 denotes the operator, Frobenius, and nuclear norm of A, respectively; by default \u2016A\u2016 = \u2016A\u20162. Operator P\u2126 is given by (2), operators Pk(A) and HT \u03b6(A) are defined in Section 2. \u03c3i(A) denotes i-th singular value of A and \u03c3\u2217i denotes the i-th singular value of L\u2217.\nPaper Organization: We present our main algorithm in Section 2 and our main results in Section 3. We also present the proof of one of our main results in Section 3. Section 4 presents our empirical result. Due to lack of space, we present most of the proofs and useful lemmas in Appendix."}, {"heading": "2 Algorithm", "text": "In this section we present our algorithm for solving the RMC (Robust Matrix Completion) problem: given \u2126 and P\u2126(M) where M = L \u2217 + S\u0303\u2217 \u2208 Rm\u00d7n, rank(L\u2217) \u2264 r, \u2016S\u0303\u2217\u20160 \u2264 s and S\u2217 = P\u2126(S\u0303\u2217), the goal is to recover L\u2217. To this end, we focus on solving the following non-convex optimization problem:\n(L\u2217, S\u2217) = argmin L,S \u2016P\u2126(M)\u2212 P\u2126(L)\u2212 S\u20162F , s.t., rank(L) \u2264 r,P\u2126(S) = S, \u2016S\u20160 \u2264 s. (3)\nFor the above problem, we propose a simple iterative algorithm that combines projected gradient descent (for L) with alternating projections (for S). In particular, we maintain iterates L(t) (with rank k \u2264 r) and sparse S(t). L(t+1) is computed using gradient descent step for objective (3) and then projecting back onto the set of rank k matrices. That is,\nL(t+1) = Pk ( L(t) + 1\np P\u2126(M \u2212 L(t) \u2212 S(t))\n) , (4)\nwhere Pk(A) denotes projection of A onto the set of rank-k matrices and can be computed efficiently using SVD of A, p = |\u2126| mn\n. S(t+1) is computed by projecting the residual P\u2126(M\u2212L(t+1)) onto set of sparse matrices using a hard-thresholding operator, i.e.,\nS(t+1) = HT \u03b6(M \u2212 L(t+1)), (5)\nwhere HT \u03b6 : Rm\u00d7n \u2192 Rm\u00d7n is the hard thresholding operator defined as: (HT \u03b6(A))ij = Aij if |Aij | \u2265 \u03b6 and 0 otherwise. Intuitively, a better estimate of the sparse corruptions for each iteration will reduce the noise of the projected gradient descent step and a better estimate of the low rank matrix will enable better estimation of the sparse corruptions. Hence, under correct set of assumptions, the algorithm should recover L\u2217, S\u0303\u2217 exactly.\nUnfortunately, just the above two simple iterations cannot handle problems where L\u2217 has poor condition number, as the intermediate errors can be significantly larger than the smallest singular values of L\u2217, making recovery of the corresponding singular vectors challenging. To alleviate this issue, we propose an algorithm that proceeds in stages. In the q-th stage, we project L(t) onto set of rank-kq matrices. Rank kq is monotonic w.r.t. q. Under standard assumptions, we show that we can increase kq in a manner such that after each stage\n\u2225\u2225L(t) \u2212 L\u2217 \u2225\u2225 \u221e decreased by at least a constant factor. Hence, the number of stages is only logarithmic\nin the condition number of L\u2217.\nSee Algorithm 1 for a psuedo-code of the algorithm. Our algorithm has an \u201couter loop\u201d (see Line 6) which sets rank kq of iterates L\n(t) appropriately (see Line 7). We then update L(t) and S(t) in the \u201cinner loop\u201d using (4), (5). We set threshold for the hard-thresholding operator using singular values of current gradient descent update (see Line 12). Note that, we divide \u2126 uniformly into Q \u00b7T sets, where Q is an upper bound on the number of outer iterations and T is the number of inner iterations. This division ensures independence\nAlgorithm 1 L\u0302 = PG-RMC (\u2126,P\u2126(M), \u01eb, r, \u00b5, \u03b7) 1: Input: Observed entries \u2126, Matrix P\u2126(M) \u2208 Rm\u00d7n, convergence criterion \u01eb, target rank r, incoherence\nparameter \u00b5, thresholding parameter \u03b7\n2: T \u2190 10 log 10\u00b5 2rn2\u2016P\u2126(M)\u20162\n|\u2126|\u01eb , Q \u2190 T /* Number of inner and outer iterations */ 3: Partition \u2126 into Q \u00b7 T subsets {\u2126q,t : q \u2208 [Q], t \u2208 [T ]} uniformly at random 4: L(0) = 0,M (0) = mn|\u2126|P\u2126(M), \u03b6 \u2190 \u03b7\u03c31(M (0)) /* Initialization */ 5: k0 \u2190 0, q \u2190 0 6: while \u03c3kq+1(M (0)) > \u01eb2\u03b7n do\n7: q \u2190 q + 1, kq \u2190 \u2223\u2223\u2223\u2223{i : \u03c3i(M (0)) \u2265 \u03c3kq\u22121+1(M (0)) 2 } \u2223\u2223\u2223\u2223 /* Setting rank of the q-th stage */\n8: for Iteration t = 0 to t = T do 9: S(t) = HT \u03b6(P\u2126q,t(M \u2212 L(t))) /* Projection onto set of sparse matrices */ 10: M (t) = L(t) \u2212 mn|\u2126q,t|P\u2126q,t(L (t) + S(t) \u2212M) /* Gradient descent update */ 11: L(t+1) = Pkq (M (t)) /* Projected gradient descent update */\n12: Set threshold \u03b6 \u2190 \u03b7 ( \u03c3kq+1(M (t)) + ( 1 2 )t\u22122 \u03c3kq (M (t)) ) 13: end for 14: S(0) = S(T ), L(0) = L(T ), M (0) = M (T ) /* Initialization for the next outer iteration */ 15: end while 16: Return: L(T )\nacross iterates that is critical to application of standard concentration bounds; such division is a standard technique in the matrix completion related literature [JN15, HW14, Rec11]. Also, \u03b7 is a tunable parameter which should be less than one and is smaller for \u201ceasier\u201d problems.\nNote that updating S(t) requires O(|\u2126| \u00b7 r+(m+n) \u00b7 r) computational steps. Computation of L(t+1) requires computing SVD for projection Pr, which can be computed in time O(|\u2126| \u00b7r+(m+n) \u00b7r2+ r3) time (ignoring log factors); see [JMD10] for more details. Hence, the computational complexity of each step of the algorithm is linear in |\u2126|\u00b7r (assuming |\u2126| \u2265 r\u00b7(m+n)). As we show in the next section, the algorithm exhibits geometric convergence rate under standard assumptions and hence the overall complexity is still nearly linear in |\u2126| (assuming r is just a constant).\nRank based Stagewise algorithm: We also provide a rank-based stagewise algorithm where the outer loop increments kq by one at each stage, i.e., the rank is q in the q-th stage. Our analysis extends for this algorithm as well, however, its time and sample complexity trades off a factor of O(log(\u03c31/\u01eb)) from the complexity of PG-RMC with a factor of r (rank of L\u2217). We provide the detailed algorithm in Appendix 5.2 due to lack of space (see Algorithm 2)."}, {"heading": "3 Analysis", "text": "We now present our analysis for both of our algorithms PG-RMC (Algorithm 1) and R-RMC (Algorithm 2). In general the problem of Robust PCA with Missing Entries (3) is harder than the standard Matrix Completion problem and hence is NP-hard [HMRW14]. Hence, we need to impose certain (by now standard) assumptions on L\u2217, S\u0303\u2217, and \u2126 to ensure tractability of the problem: Assumption 1. Rank and incoherence of L\u2217: L\u2217 \u2208 Rm\u00d7n is a rank-r incoherent matrix, i.e.,\u2225\u2225e\u22a4i U\u2217 \u2225\u2225 2 \u2264 \u00b5 \u221a r m , \u2225\u2225e\u22a4j V \u2217 \u2225\u2225 2 \u2264 \u00b5 \u221a r n , \u2200i \u2208 [m], \u2200j \u2208 [n], where L\u2217 = U\u2217\u03a3\u2217(V \u2217)\u22a4 is the SVD of L\u2217. Assumption 2. Sampling (\u2126): In each iteration, \u2126q,t is obtained by sampling each entry with probability p = |\u2126q,t| mn .\nAssumption 3. Sparsity of S\u0303\u2217, S\u2217: We assume that at most \u03c1 \u2264 c \u00b52r fraction of the elements in each row and column of S\u0303\u2217 are non-zero for a small enough constant c. Moreover, we assume that \u2126 is independent of S\u0303\u2217. Hence, S\u2217 = P\u2126(S\u0303\u2217) also has at most p \u00b7 \u03c1 fraction of the entries in expectation. Assumptions 1, 2 are standard assumptions in the provable matrix completion literature [CR09, Rec11, JN15], while Assumptions 1, 3 are standard assumptions in the robust PCA (low-rank+sparse matrix recovery) literature [CSPW11, CLMW11, HKZ11]. Hence, our setting is a generalization of both the standard and popular problems and as we show later in the section, our result can be used to meaningfully improve the state-of-the-art for both these problems.\nWe first present our main result for Algorithm 1 under the assumptions given above. Theorem 1. Let Assumptions 1, 2 and 3 on L\u2217, S\u0303\u2217 and \u2126 hold respectively. Let m \u2264 n, n = O(m), and let the number of samples |\u2126| satisfy:\nE[|\u2126|] \u2265 C\u03b1\u00b54r2n log2 (n) log2 ( \u00b52r\u03c31\n\u01eb\n) ,\nwhere C is a global constant. Then, with probability at least 1\u2212n\u2212 log \u03b12 , Algorithm 1 with \u03b7 = 4\u00b52r m , at most O(log(\u2016M\u20162/\u01eb))) outer iterations and O(log(\u00b5 2r\u2016M\u20162\n\u01eb )) inner iterations, outputs a matrix L\u0302 such that:\n\u2225\u2225\u2225L\u0302\u2212 L\u2217 \u2225\u2225\u2225 F \u2264 \u01eb.\nNote that our number of samples increase with the desired accuracy \u01eb. However, using argument similar to that of [JN15], we should be able to replace \u01eb by \u03c3\u2217min which should modify the \u01eb term to be log\n2 \u03ba where \u03ba = \u03c31(L \u2217)/\u03c3r(L\u2217). We leave ironing out the details for future work.\nNote that the numbe of samples matches information theoretic bound upto O(r logn log2 \u03c3\u22171/\u01eb) factor. Also, the number of allowed corruptions in S\u0303\u2217 also matches the known lower bounds (up to a constant factor) and cannot be improved upon information theoretically.\nWe now present our result for the rank based stagewise algorithm (Algorithm 2). Theorem 2. Under Assumptions 1, 2 and 3 on L\u2217, S\u0303\u2217 and \u2126 respectively and \u2126 satisfying:\nE[|\u2126|] \u2265 C\u03b1\u00b54r3n log2 (n) log ( \u00b52r\u03c31\n\u01eb\n) ,\nfor a large enough constant C, then Algorithm 2 with \u03b7 set to 4\u00b5 2r\nm outputs a matrix L\u0302 such that: \u2225\u2225\u2225L\u0302\u2212 L\u2217 \u2225\u2225\u2225 F \u2264\n\u01eb, w.p. \u2265 1\u2212 n\u2212 log \u03b12 . Notice that the sample complexity of Algorithm 2 has an additional multiplicative factor of O(r) when compared to that of Algorithm 1, but shaves off a factor of O(log(\u03ba)). Similarly, computational complexity of Algorithm 2 also trades off a O(log \u03ba) factor for O(r) factor from the computational complexity of Algorithm 1.\nResult for Matrix Completion: Note that for S\u0303\u2217 = 0, the RMC problem with Assumptions 1,2 is exactly the same as the standard matrix completion problem and hence, we get the following result as a corollary of Theorem 1: Corollary 1 (Matrix Completion). Suppose we observe \u2126 and P\u2126(L \u2217) where Assumptions 1,2 hold for L\u2217 and \u2126. Also, let E[|\u2126|] \u2265 C\u03b12\u00b54r2 log2 n log2 \u03c31/\u01eb and m \u2264 n. Then, w.p. \u2265 1 \u2212 n\u2212 log \u03b1 2 , Algorithm 1 outputs L\u0302 s.t. \u2016L\u0302\u2212 L\u2217\u20162 \u2264 \u01eb. Table 1 compares our sample and time complexity bounds for low-rank MC. Note that our sample complexity is nearly the same as that of nuclear-norm methods while the running time of our algorithm is significantly\nbetter than the existing results that have at most logarithmic dependence on the condition number of L\u2217.\nResult for Robust PCA: Consider the standard Robust PCA problem (RPCA), where the goal is to recover L\u2217 from M = L\u2217 + S\u0303\u2217. For RPCA as well, we can randomly sample |\u2126| entries from M , where \u2126 satisfies the assumption required by Theorem 1. This leads us to the following corollary: Corollary 2 (Robust PCA). Suppose we observe M = L\u2217 + S\u0303\u2217, where Assumptions 1, 3 hold for L\u2217 and S\u0303\u2217. Generate \u2126 \u2208 [m] \u00d7 [n] by sampling each entry uniformly at random with probability p, s.t., E[|\u2126|] \u2265 C\u03b12\u00b54r2 log2 n log2 \u03c31/\u01eb. Let m \u2264 n. Then, w.p. \u2265 1 \u2212 n\u2212 log \u03b1 2 , Algorithm 1 outputs L\u0302 s.t. \u2016L\u0302\u2212 L\u2217\u20162 \u2264 \u01eb. Hence, using Theorem 1, we will still be able to recover L\u2217 but using only the sampled entries. Moreover, the running time of the algorithm is only O(\u00b52nr3 log2 n log2(\u03c31/\u01eb)), i.e., we are able to solve RPCA problem in time linear in n. To the best of our knowledge, the existing state-of-the-art methods for RPCA require at least O(n2r) time to perform the same task [NUNS+14, GWL16]. Similarly, we don\u2019t need to load the entire data matrix in memory, but we can just sample the matrix and work with the obtained sparse matrix with at most linear number of entries. Hence, our method significantly reduces both time and space complexity, and as demonstrated empirically in Section 4 can help scale our algorithm to very large data sets without losing accuracy."}, {"heading": "3.1 Proof of Theorem 1", "text": "We now present our proof for Theorem 1; the proof of Theorem 2 follows similarly. The proofs of all but one of the lemmas used are deferred to the appendix to improve readability. Recall that we assume that M = L\u2217+S\u0303\u2217 and define S\u2217 = P\u2126(S\u0303\u2217). Similarly, we define S\u0303(t) = HT \u03b6(M\u2212L(t)). Critically, S(t) = P\u2126(S\u0303(t)) (see Line 9 of Algorithm 1), i.e., S\u0303(t) is the set of iterates that we \u201ccould\u201d obtain if entire M was observed. Note that we cannot compute S\u0303(t), it is introduced only to simplify our analysis.\nWe first re-write the projected gradient descent step for L(t+1) as described in (4):\nL(t+1) = Pkq ( L\u2217 + (S\u0303\u2217 \u2212 S\u0303(t))\ufe38 \ufe37\ufe37 \ufe38\nE1\n+ ( I \u2212 P\u2126q,t\np\n) ( E2\ufe37 \ufe38\ufe38 \ufe37 (L(t) \u2212 L\u2217)+(S\u0303(t) \u2212 S\u0303\u2217))\n\ufe38 \ufe37\ufe37 \ufe38 E3\n) (6)\nThat is, L(t+1) is obtained by rank-kq SVD of a perturbed version of L \u2217: L\u2217 + E1 + E3. As we perform entrywise thresholding to reduce \u2016S\u0303\u2217 \u2212 S\u0303(t)\u2016\u221e, we need to bound \u2016L(t+1) \u2212 L\u2217\u2016\u221e. To this end, we use techniques from [JN15], [NUNS+14] that explicitly model singular vectors of L(t+1) and argue about the\ninfinity norm error using a Taylor series expansion. However, in our case, such an error analysis requires analyzing the following key quantities (H = E1 + E3):\n\u22001 \u2264 j, s.t., j even : Aj := max q\u2208[n]\n\u2016e\u22a4q ( H\u22a4H ) j 2 V \u2217\u20162, Bj := max\nq\u2208[m] \u2016e\u22a4q\n( HH\u22a4 ) j 2 U\u2217\u20162,\n\u22001 \u2264 j, s.t., j odd : Cj := max q\u2208[n]\n\u2016e\u22a4q H\u22a4 ( HH\u22a4 )\u230a j2 \u230b U\u2217\u20162, Dj := max q\u2208[m] \u2016e\u22a4q H ( H\u22a4H )\u230a j2 \u230b V \u2217\u20162. (7)\nNote that E1 = 0 in the case of standard RPCA which was analyzed in [NUNS +14], while E3 = 0 in the case of standard MC which was considered in [JN15]. In contrast, in our case both E1 and E3 are non-zero. Moreover, E3 is dependent on random variable \u2126. Hence, for j \u2265 2, we will get cross terms between E3 and E1 that will also have dependent random variables which precludes application of standard Bernsteinstyle tail bounds. To this end, we use a technique similar to that of [EKYY13, JN15] to provide a careful combinatorial-style argument to bound the above given quantity. That is, we can provide the following key lemma: Lemma 1. Let L\u2217, \u2126, and S\u0303\u2217 satisfy Assumptions 1, 2 and 3 respectively. Let L\u2217 = U\u2217\u03a3\u2217(V \u2217)\u22a4 be the singular value decomposition of L\u2217. Furthermore, suppose that in the tth iteration of the qth stage, S\u0303(t) defined as HT\u03b6(M \u2212 L(t)) satisfies Supp(S\u0303(t)) \u2286 Supp(S\u0303\u2217), then we have:\nmax{Aa, Ba, Ca, Da} \u2264 \u00b5 \u221a r\nm\n( \u03c1n \u2016E1\u2016\u221e + c \u221a n\np (\u2016E1\u2016\u221e + \u2016E2\u2016\u221e) log n\n)a ,\n\u2200c > 0 w.p \u2265 1\u2212 n\u22122 log c4+4, where E1, E2 and E3 are defined in (6), Aa, Ba, Ca, Da are defined in (7). Remark: We would like to note that even for the standard MC setting, i.e., when E1 = 0, we obtain better bound than that of [JN15] as we can bound maxi \u2016eTi (E3)qU\u20162 directly rather than the weaker\u221a rmaxi \u2016eTi (E3)quj\u2016 bound that [JN15] uses. In the following lemma, we characterize how the progress in the estimation of L\u2217 by L(t) depends on the quantities in 7. Lemma 2. Let L(t) = Pk(L \u2217 +H), where H is any perturbation matrix that satisfies the following:\n1. \u2016H\u20162 \u2264 \u03c3\u2217k 4 2. \u2200i \u2208 [n], a \u2208 \u2308 logn2 \u2309 with \u03c5 \u2264 \u03c3\u2217k 4\n\u2225\u2225\u2225e\u22a4i ( H\u22a4H ) a 2 V \u2217 \u2225\u2225\u2225 2 , \u2225\u2225\u2225e\u22a4i ( HH\u22a4 ) a 2 U\u2217 \u2225\u2225\u2225 2 \u2264 (\u03c5)a\u00b5 \u221a r m when a is even\u2225\u2225\u2225e\u22a4i H\u22a4 ( HH\u22a4 )\u230a a2 \u230b U\u2217 \u2225\u2225\u2225 2 , \u2225\u2225\u2225e\u22a4i H ( H\u22a4H )\u230a a2 \u230b V \u2217 \u2225\u2225\u2225 2 \u2264 (\u03c5)a\u00b5 \u221a r m when a is odd\nwhere \u03c3\u2217k is the k th singular value of L\u2217. Also, let L\u2217 satisfy Assumption 1. Then, the following holds:\n\u2225\u2225\u2225L(t+1) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 \u00b5 2r m ( \u03c3\u2217k+1 + 20 \u2016H\u20162 + 8\u03c5 )\nwhere \u00b5 and r are the rank and incoherence of the matrix L\u2217 respectively.\nIn the next lemma, we show that with the threshold chosen in the algorithm, we show an improvement in the estimation of S\u0303\u2217 by S\u0303(t+1). Lemma 3. In the tth iterate of the qth stage, assume the following holds:\n1. \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e \u2264 2\u00b52r m ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k )\n2. 78 ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k ) \u2264 ( \u03bbk+1 + ( 1 2 )z \u03bbk ) \u2264 98 ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k )\nwhere \u03c3\u2217k and \u03c3 \u2217 k+1 are the k and (k+1) th singular values of L\u2217, \u03bbk and \u03bbk+1 are the k and (k+1)th singular values of M (t) and, r and \u00b5 are the rank and incoherence of the m\u00d7n matrix L\u2217 respectively. Then we have\n1. Supp ( S\u0303(t) ) \u2286 Supp ( S\u0303\u2217 ) 2. \u2225\u2225\u2225S\u0303(t) \u2212 S\u0303\u2217\n\u2225\u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k )\nIn the following lemma, we show that we make progress simultaneously in the estimation of both S\u0303\u2217 and L\u2217 by S\u0303(t) and L(t). We make use of Lemmas 2 and 3 to show how progress in the estimation of one affects the other alternatively. We use Lemma 1 along with Lemma 2 to show improved estimation of L\u2217 by L(t). Lemma 4. Let L\u2217, \u2126, S\u0303\u2217 and S\u0303(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the qth stage of Algorithm 1, S\u0303(t) and L(t) satisfy:\n\u2225\u2225\u2225S\u0303(t) \u2212 S\u0303\u2217 \u2225\u2225\u2225 \u221e \u2264 8\u00b5 2r m (\u2223\u2223\u2223\u03c3\u2217kq+1 \u2223\u2223\u2223+ ( 1 2 )t\u22123 \u2223\u2223\u2223\u03c3\u2217kq \u2223\u2223\u2223 ) ,\nSupp ( S\u0303(t) ) \u2286 Supp ( S\u0303\u2217 ) , and\n\u2225\u2225\u2225L(t) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m (\u2223\u2223\u2223\u03c3\u2217kq+1 \u2223\u2223\u2223+ ( 1 2 )t\u22123 \u2223\u2223\u2223\u03c3\u2217kq \u2223\u2223\u2223 ) .\nwith probability \u2265 1\u2212 ((q \u2212 1)T + t\u2212 1)n\u2212(10+log\u03b1) where T is the number of iterations in the inner loop.\nProof. We prove the lemma by induction on both q and t."}, {"heading": "Base Case: q = 1 and t = 0", "text": "We begin by first proving an upper bound on \u2016L\u2217\u2016\u221e. We do this as follows:\n\u2223\u2223L\u2217ij \u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 r\u2211\nk=1\n\u03c3\u2217ku \u2217 ikv \u2217 jk \u2223\u2223\u2223\u2223\u2223 \u2264 r\u2211\nk=1\n\u03c3\u2217k \u2223\u2223u\u2217ikv\u2217jk \u2223\u2223 \u2264 \u03c3\u22171 r\u2211\nk=1\n\u2223\u2223u\u2217ikv\u2217jk \u2223\u2223 \u2264 \u00b5 2r\u221a mn \u03c3\u22171\nwhere the last inequality follows from Cauchy-Schwartz and the incoherence of U\u2217. This directly proves the third claim of the lemma for the base case. We also note that due to the thresholding step and the incoherence assumption on L\u2217, we have:\n1. \u2225\u2225E(0) \u2225\u2225 \u221e \u2264 8\u00b52r m (\u03c3\u22172 + 2\u03c3 \u2217 1) (\u03b6) \u2264 8\u00b52r m ( 8\u03c3\u2217k1 ) , and 2. Supp ( S\u0303(t) ) = Supp ( S\u0303\u2217 ) .\nwhere (\u03b6) follows from Lemma 5. So the base case of induction is satisfied."}, {"heading": "Induction over t", "text": "We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:\na) \u2225\u2225E(t) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217kq+1 + ( 1 2 )t\u22123 \u03c3\u2217kq ) b) Supp ( S\u0303(t) ) \u2286 Supp ( S\u0303\u2217 ) .\nc) \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e \u2264 2\u00b52r m ( \u03c3\u2217kq+1 + ( 1 2 )t\u22123 \u03c3\u2217kq )\nwith probability 1\u2212 ((q \u2212 1)T + t\u2212 1)n\u2212(10+log\u03b1). Then by Lemma 2, we have: \u2225\u2225\u2225L(t+1) \u2212 L\u2217\n\u2225\u2225\u2225 \u221e \u2264 \u00b5 2r m ( \u03c3\u2217kq+1 + 20 \u2016H\u20162 + 8\u03c5 ) (8)\nFrom Lemma 1, we have:\n\u03c5 \u2264 \u03c1n \u2225\u2225\u2225E(t) \u2225\u2225\u2225 \u221e +8\u03b2\u03b1 logn (\u03b61) \u2264 1 100 ( \u03c3\u2217kq+1 + ( 1 2 )t\u22123 \u03c3\u2217kq ) +8\u03b2\u03b1 logn (\u03b62) \u2264 1 50 ( \u03c3\u2217kq+1 + ( 1 2 )t\u22123 \u03c3\u2217kq ) (9)\nwhere (\u03b61) follows from our assumptions on \u03c1 and our inductive hypothesis on \u2225\u2225E(t) \u2225\u2225 \u221e and (\u03b62) follows\nfrom our assumption on p and by noticing that \u2016D\u2016\u221e \u2264 \u2225\u2225E(t) \u2225\u2225 \u221e + \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e. Recall that D = L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217. From Lemma 11:\n\u2016H\u20162 \u2264 1\n100\n( \u03c3\u2217kq+1 + ( 1\n2\n)t\u22123 \u03c3\u2217kq ) (10)\nwith probability \u2265 1\u2212 n\u2212(10+log\u03b1). From Equations 10, 9 and 8, we have: \u2225\u2225\u2225L\u2217 \u2212 L(t+1)\n\u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m ( \u03c3\u2217kq+1 + ( 1 2 )t\u22122 \u03c3\u2217kq )\nwhich by union bound holds with probability \u2265 1 \u2212 ((q \u2212 1)T + t)n\u2212(10+log\u03b1). Hence, using Lemma 3 and 12 we have:\n1. \u2225\u2225E(t+1) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217kq+1 + ( 1 2 )t\u22122 \u03c3\u2217kq ) 2. Supp ( S\u0303(t)t+ 1 ) \u2286 Supp ( S\u0303\u2217 ) .\nwhich also holds with probability \u2265 1 \u2212 ((q \u2212 1)T + t)n\u2212(10+log\u03b1). This concludes the proof for induction over t."}, {"heading": "Induction Over Stages q", "text": "We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:\n1. \u2225\u2225E(T ) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217kq+1 + ( 1 2 )T \u03c3\u2217kq ) \u2264 8\u00b5 2r\u03c3\u2217kq+1 m + \u01eb10n , and 2. Supp ( S\u0303(T ) ) \u2286 Supp ( S\u0303\u2217 ) .\nwith probability \u2265 1\u2212 (qT \u2212 1)n\u2212(10+log\u03b1). From Lemmas 6 and 11 we get: \u2223\u2223\u2223\u03c3kq+1 ( M (T ) ) \u2212 \u03c3\u2217kq+1 \u2223\u2223\u2223 \u2264 \u2016H\u20162 \u2264 1\n100\n( \u03c3\u2217kq+1 + m\u01eb\n10n\u00b52r\n) (11)\nwith probability 1 \u2212 n\u2212(10+log\u03b1). We know that \u03b7\u03c3kq+1 ( M (t) ) \u2265 \u01eb2n which with 11 implies that \u2223\u2223\u2223\u03c3\u2217kq+1 \u2223\u2223\u2223 >\nm\u01eb 10n\u00b52r .\n\u2225\u2225\u2225L(T+1) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m ( \u03c3\u2217kq+1 + ( 1 2 )T+1 \u03c3\u2217kq ) \u2264 2\u00b5 2r m ( \u03c3\u2217kq+1 +\nm\u01eb\n20n\u00b52rn\n)\n\u2264 2\u00b5 2r\nm\n( \u03c3\u2217kq+1 + \u03c3\u2217kq+1 2 ) \u2264 2\u00b5 2r m ( 2\u03c3\u2217kq+1 ) (\u03b64) \u2264 2\u00b5 2r m ( 8\u03c3\u2217kq+1 )\nwhere (\u03b64) follows from Lemma 5. By union bound this holds with probability \u2265 1\u2212 qTn\u2212(10+log\u03b1). Now, from 12 and 3, we have through a similar series of arguments as above:\n\u2225\u2225\u2225E(t)T + 1 \u2225\u2225\u2225 \u221e \u2264 8\u00b5 2r m ( 8\u03c3\u2217kq+1 ) (12)\nwhich holds with probability \u2265 1\u2212 qTn\u2212(10+log\u03b1).\nIn the following lemma, we show that that \u03c3\u2217kq+1 is sufficiently small compared to \u03c3 \u2217 kq\u22121+1 and \u03c3\u2217kq is sufficiently large compared to \u03c3\u2217kq\u22121+1. The first condition enables us to show that \u03c3 \u2217 kq+1\ndecreases geometrically which ensures that only a small number of \u201couter iterations\u201d are required for the algorithm to converge while the second condition ensures that the error measured by \u2016E1\u2016\u221e and \u2016E3\u2016\u221e is small in comparison to \u03c3\u2217kq which is required for the application of Lemma 3. Lemma 5. Assume that L\u2217, \u2126 and S\u0303\u2217 satisfy Assumptions 1,2 and 3 respectively. Furthermore, suppose at the beginning of the qth stage of algorithm 1:\n1. \u2225\u2225L\u2217 \u2212 L(0) \u2225\u2225 \u221e \u2264 2\u00b52r m ( 2\u03c3\u2217kq\u22121+1 ) 2. \u2225\u2225E(0) \u2225\u2225 \u221e \u2264 8\u00b52r m ( 2\u03c3\u2217kq\u22121+1 )\nThen, the following hold:\n1. \u03c3\u2217kq \u2265 15 32\u03c3 \u2217 kq\u22121+1\n2. \u03c3\u2217kq+1 \u2264 17 32\u03c3 \u2217 kq\u22121+1\nwith probability \u2265 1\u2212 n\u2212(10+log\u03b1)\nWe can now proceed to prove Theorem 1:\nProof of Theorem 1: From Lemma 13 we know that T \u2265 log(3\u00b5 2r\u03c3\u22171 \u01eb\n). Consider the stage q reached at the termination of the algorithm. We know from Lemma 4 that:\n1. \u2225\u2225E(T ) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217kq+1 + ( 1 2 )T \u03c3\u2217kq ) \u2264 8\u00b52r m \u03c3\u2217kq+1 + \u01eb 10n 2. \u2225\u2225L(T ) \u2212 L\u2217\n\u2225\u2225 \u221e \u2264 2\u00b52r m ( \u03c3\u2217kq+1 + ( 1 2 )T \u2223\u2223\u2223\u03c3\u2217kq \u2223\u2223\u2223 ) \u2264 2\u00b52r m \u03c3\u2217kq+1 + \u01eb 10n\nCombining this with Lemmas 6 and 11, we get:\n\u2223\u2223\u03c3kq+1(MT ) \u2223\u2223 \u2265 \u03c3\u2217kq+1 \u2212 1\n100\n( \u03c3\u2217kq+1 + m\u01eb\n10n\u00b52r\n) (13)\nWhen the while loop terminates, \u03b7\u03c3kq+1 ( M (T ) ) < \u01eb2n , which from 13, implies that \u03c3 \u2217 kq+1\n< m\u01eb7n\u00b52r . So we have:\n\u2016L\u2212 L\u2217\u2016\u221e = \u2225\u2225\u2225L(T ) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m \u03c3\u2217kq+1 + \u01eb 10n \u2264 \u01eb 2n .\nWe will now bound the number of iterations required for the PG-RMC to converge.\nFrom claim 2 of Lemma 5, we have \u03c3\u2217kq+1 \u2264 17 32\u03c3 \u2217 kq\u22121+1 \u2200q \u2265 1. By recursively applying this inequality, we get \u03c3\u2217kq+1 \u2264 ( 17 32 )q \u03c3\u22171 . We know that when the algorithm terminates, \u03c3 \u2217 kq+1 < \u01eb7\u00b52r . Since, ( 17 32 )q \u03c3\u22171 is an\nupper bound for \u03c3\u2217kq+1, an upper bound for the number of iterations is 5 log ( 7\u00b52r\u03c3\u22171 \u01eb ) . Also, note that an upper bound to this quantity is used to partition the samples provided to the algorithm. This happens with probability \u2265 1\u2212 T 2n\u2212(10+log\u03b1) \u2265 1\u2212 n\u2212 log\u03b1. This concludes the proof."}, {"heading": "4 Experiments", "text": "In this section we discuss the performance of Algorithm 1 on synthetic data and its use in foreground background separation. The goal of the section is two-fold: a) to demonstrate practicality and effectiveness of Algorithm 1 for the RMC problem, b) to show that Algorithm 1 indeed solves RPCA problem in significantly smaller time than that required by the existing state-of-the-art algorithm (St-NcRPCA [NUNS+14]). To this end, we use synthetic data as well as video datasets where the goal is to perform foreground-background separation [CLMW11].\nWe implemented our algorithm in MATLAB and the results for the synthetic data set were obtained by averaging over 20 runs. We obtained a matlab implementation of St-NcRPCA [NUNS+14] from the authors of [NUNS+14]. Note that if the sampling probability is p = 1, then our method is similar to St-NcRPCA; the key difference being how rank is selected in each stage.\nParameters. The algorithm has three main parameters: 1) threshold \u03bb, 2) incoherence \u00b5 and 3) sampling probability p (E[|\u2126|] = p \u00b7 mn). In the experiments on synthetic data we observed that keeping \u03bb \u223c \u00b5 \u2225\u2225M \u2212 S(t)\n\u2225\u2225 2 / \u221a n speeds up the recovery while for background extraction keeping \u03bb \u223c \u00b5 \u2225\u2225M \u2212 S(t) \u2225\u2225 2 /n\ngives a better quality output. The value of \u00b5 for real world data sets was figured out using cross validation while for the synthetic data the same value was used as used in data generation. The sampling probability for the synthetic data could be kept as low as 2r log2(n)/n while for the real world data set we got good results for p = 0.05. Also, rather than splitting samples, we use entire set of observed entries to perform our updates (see Algorithm 1).\nSynthetic data. We generate M = L\u2217+ S\u0303\u2217 of two sizes, where L\u2217 = UV \u22a4 \u2208 R2000\u00d72000 (and R5000\u00d75000) is a random rank-5 (and rank-10 respectively) matrix with incoherence \u2248 1. S\u0303\u2217 is generated by considering a uniformly random subset of size\n\u2225\u2225\u2225S\u0303\u2217 \u2225\u2225\u2225 0 from [m]\u00d7 [n] where every entry is i.i.d. from the uniform distribution\nin [ r 2 \u221a mn , r\u221a mn ]. This is the same setup as used in [CLMW11]. Figure 1 (a) plots recovery error (\u2016L \u2212 L\u2217\u2016F ) vs computational time for our PG-RMC method (with different sampling probabilities) as well as the St-NcRPCA algorithm. Note that even for very small values of sampling p, we can achieve same recovery error using significantly small values. For example, our method with p = 0.1 achieve 0.01 error (\u2016L\u2212 L\u2217\u2016F ) in \u2248 2.5s while St-NcRPCA method requires \u2248 10s to achieve the same accuracy. Note that we do not compare against the convex relaxation based methods like IALM from [CLMW11], as [NUNS+14] shows that St-NcRPCA is significantly faster than IALM and several other convex relaxation solvers.\nFigure 1 (b) plots time required to achieve different recovery errors (\u2016L\u2212L\u2217\u2016F ) as the sampling probability p increases. As expected, we observe a linear increase in the run-time with p. Interestingly, for very small values of p, we observe an increase in running time. In this regime, \u2016P\u2126(M)\u20162 p becomes very large (as p\ndoesn\u2019t satisfy the sampling requirements). Hence, increase in the number of iterations (T \u2248 log \u2016P\u2126(M)\u20162 p\u01eb\n) dominates the decrease in per iteration time complexity.\nFigure 1 (c), (d) plots computation time required by our method (PG-RMC , Algorithm 1) versus rank and incoherence, respectively. As expected, as these two problem parameters increase, our method requires more time. Note that our run-time dependence on rank seems to be linear, while our existing results require O(r3) time. This hints at the possibility of further improving the computational complexity analysis of our algorithm.\nWe also study phase transition for different values of sampling probability p. Figure 3 (a) in Appendix 5.4 show a phase transition phenomenon where beyond p > .06 the probability of recovery is almost 1 while below it, it is almost 0.\nForeground-background separation. We also applied our technique to the problem of foregroundbackground separation. We use the usual method of stacking up the vectorized video frames to construct a matrix. The background, being static, will form the low rank component while the foreground can be considered to be the noise.\nWe applied our PG-RMC method (with varying p) to several videos. Figure 2 (a), (d) shows one frame each from two videos (a shopping center video, a restaurant video). Figure 2 (b), (d) shows the extracted background from the two videos by using our method (PG-RMC , Algorithm 1) with probability of sampling p = 0.05. Figure 2 (c), (f) compares objective function value for different p values. Clearly, PG-RMC can recover the true background with p as small as 0.05. We also observe an order of magnitude speedup (\u2248 5x) over St-NcRPCA [NUNS+14]. We present results on the video Escalator in Appendix 5.4.\nConclusion. In this work, we studied the Robust Matrix Completion problem. For this problem, we provide exact recovery of the low-rank matrix L\u2217 using nearly optimal number of observations as well as nearly optimal fraction of corruptions in the observed entries. Our RMC result is based on a simple and efficient PGD algorithm that has nearly linear time complexity as well. Our result improves state-of-the-art for the related Matrix Completion as well as Robust PCA problem. For Robust PCA, we provide first nearly linear time algorithm under standard assumptions.\nOur sample complexity depends on \u01eb, the desired accuracy in L\u2217. We believe that the arguments used by [JN15], we should be able to remove the \u01eb dependence as well and leave it for future work. Moreover, improving dependence of sample complexity on r (from r2 to r) also represents an important direction. Finally, similar to foreground background separation, we would like to explore more applications of RMC/RPCA."}, {"heading": "5 Appendix", "text": "We divide this section into five parts. In the first part we prove some common lemmas. In the second part we give the convergence guarantee for PG-RMC . In the third part we give another algorithm which has a sample complexity of O(\u00b54r3n log2 n log \u00b52r\u03c3\u22171\n\u01eb ) and prove its convergence guarantees. In the fourth part we\nprove a generalized form of lemma 1. In the fifth part we present some additional experiments.\nFor the sake of convenience in the following proofs, we will define some notation here.\nWe define p = |\u2126k,t| mn and we consider the following equivalent update step for L(t+1) in the analysis:\nL(t+1) := Pk(M (t)) M (t) := L\u2217 +H H := E(t) + \u03b2G E(t) := S\u0303\u2217 \u2212 S\u0303(t)\nS\u0303(t) := HT \u03b6 ( M (t) \u2212 L(t) ) G := 1\n\u03b2\n( I \u2212 P\u2126q,tp ) D\nD := L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217 \u03b2 := 2 \u221a n \u2016D\u2016\u221e\u221a\np\nThe singular values of L\u2217 are denoted by \u03c3\u22171 , . . . , \u03c3 \u2217 r where |\u03c3\u22171 | \u2265 . . . \u2265 |\u03c3\u2217r | and we will let \u03bb1, . . . , \u03bbn denote the singular values of M (t) where |\u03bb1| \u2265 . . . \u2265 |\u03bbn|."}, {"heading": "5.1 Common Lemmas", "text": "We will begin by restating some lemmas from previous work that we will use in our proofs.\nFirst, we restate Weyl\u2019s perturbation lemma from [Bha97], a key tool in our analysis: Lemma 6. Suppose B = A+E \u2208 Rm\u00d7n matrix. Let \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbk and \u03c31, \u00b7 \u00b7 \u00b7 , \u03c3k be the singular values of B and A respectively such that \u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbk and \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3k. Then:\n|\u03bbi \u2212 \u03c3i| \u2264 \u2016E\u20162 \u2200 i \u2208 [k].\nThis lemma establishes a bound on the spectral norm of a sparse matrix. Lemma 7. Let S \u2208 Rm\u00d7n be a sparse matrix with row and column sparsity \u03c1. Then,\n\u2016S\u20162 \u2264 \u03c1max{m,n} \u2016S\u2016\u221e\nProof. For any pair of unit vectors u and v, we have:\nv\u22a4Su = \u2211\n1\u2264i\u2264m,1\u2264j\u2264n viujSij \u2264\n\u2211\n1\u2264i\u2264m,1\u2264j\u2264n |Sij |\n( v2i + u 2 j\n2\n)\n\u2264 1 2\n  \u2211\n1\u2264i\u2264m v2i\n\u2211\n1\u2264j\u2264n |Sij |+\n\u2211\n1\u2264j\u2264n u2j\n\u2211\n1\u2264i\u2264m |Sij |\n  \u2264 \u03c1max{m,n} \u2016S\u2016\u221e\nLemma now follows by using \u2016S\u20162 = maxu,v,\u2016u\u20162=1,\u2016v\u20162=1 uTSv.\nNow, we define a 0-mean random matrix with small higher moments values. Definition 1 (Definition 7, [JN15]). H is a random matrix of size m \u00d7 n with each of its entries drawn independently satisfying the following moment conditions:\nE[hij ] = 0, |hij | < 1, E[|hij |k] \u2264 1max{m,n} ,\nfor i, j \u2208 [n] and 2 \u2264 k \u2264 2 logn.\nWe now restate two useful lemmas from [JN15]: Lemma 8 (Lemma 12, 13 of [JN15]). We have the following two claims:\n\u2022 Suppose H satisfies Definition 1. Then, w.p. \u2265 1\u2212 1/n10+log\u03b1, we have: \u2016H\u20162 \u2264 3 \u221a \u03b1.\n\u2022 Let A be a m\u00d7n matrix with n \u2265 m. Suppose \u2126 \u2286 [m]\u00d7 [n] is obtained by sampling each element with probability p \u2208 [ 1 4n , 0.5 ] . Then, the following matrix H satisfies Defintion 1:\nH :=\n\u221a p\n2 \u221a n \u2016A\u2016\u221e\n( A\u2212 1\np P\u2126(A)\n) .\nLemma 9 (Lemma 13, [JN15]). Let A \u2208 Rn\u00d7n be a symmetric matrix with eigenvalues \u03c31, \u00b7 \u00b7 \u00b7 , \u03c3n where |\u03c31| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |\u03c3n|. Let B = A + C be a perturbation of A satisfying \u2016C\u20162 \u2264 \u03c3k2 and let Pk(B) = U\u039bU\u22a4 by the rank-k projection of B. Then, \u039b\u22121 exists and we have:\n1. \u2225\u2225A\u2212AU\u039b\u22121U\u22a4A \u2225\u2225 2 \u2264 |\u03c3k|+ 5 \u2016C\u20162, 2. \u2225\u2225AU\u039b\u2212aU\u22a4A \u2225\u2225 2 \u2264 4 ( |\u03c3k| 2 )\u2212a+2 \u2200a \u2265 2.\nWe now provide a lemma that bounds \u2016 \u00b7 \u2016\u221e norm of an incoherent matrix with its operator norm. Lemma 10. Let A \u2208 Rm\u00d7n be a rank r, \u00b5-incoherent matrix. Then for any C \u2208 Rn\u00d7m, we have:\n\u2016ACA\u2016\u221e \u2264 \u00b52r\u221a mn \u2016ACA\u20162\nProof. Let A = U\u03a3V T . Then, ACA = UUTACAV V T . The lemma now follows by using definition of incoherence with the fact that \u2016UTACAU\u20162 \u2264 \u2016ACA\u20162.\nWe now present a lemma that shows improvement in the error \u2016L \u2212 L\u2217\u2016\u221e by using gradient descent on L(t). Lemma 11. Let L\u2217, \u2126, S\u0303\u2217 satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of any stage q:\n1. \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e \u2264 2\u00b52r m ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k ) 2. \u2225\u2225\u2225S\u0303\u2217 \u2212 S\u0303(t)\n\u2225\u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k )\n3. Supp(S\u0303(t)) \u2286 Supp(S\u0303\u2217)\nwhere z \u2265 \u22123 and \u03c3\u2217k and \u03c3\u2217k+1 are the k and (k + 1)th singular values of L\u2217. Also, let E1 = S\u0303(t) \u2212 S\u0303\u2217 and E3 = ( I \u2212 P\u2126q,t\np\n)( L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217 ) be the error terms defined also in (6). Then, the following holds\nw.p \u2265 1\u2212 n\u2212(10+log\u03b1): \u2016E1 + E3\u20162 \u2264 1\n100\n( \u03c3\u2217k+1 + ( 1\n2\n)z \u03c3\u2217k ) (14)\nProof. Note from Lemma 8,\n1 \u03b2 E3 = 1 \u03b2\n( I \u2212 P\u2126q,t\np\n)( L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217 ) ,\nwith \u03b2 = 2 \u221a n\u221a p \u00b7 \u2016L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217\u2016\u221e satisfies definition 1.\nWe now bound the spectral norm of E1 + E3 as follows:\n\u2016E1 + E3\u20162 \u2264 \u2016E1\u20162 + \u03b2 \u00b7 \u2225\u2225\u2225\u2225 1\n\u03b2 E3 \u2225\u2225\u2225\u2225 2 (\u03b61) \u2264 \u03c1n \u2225\u2225\u2225S\u0303(t) \u2212 S\u0303\u2217 \u2225\u2225\u2225 \u221e + 3\u03b2 \u221a \u03b1,\n(\u03b62)\n\u2264 1 200 ( \u03c3\u2217kq+1 + ( 1 2 )z \u03c3\u2217kq ) + 60\u00b52r m \u221a n p \u221a \u03b1 (\u2223\u2223\u2223\u03c3\u2217kq+1 \u2223\u2223\u2223+ ( 1 2 )z \u2223\u2223\u2223\u03c3\u2217kq \u2223\u2223\u2223 ) ,\n(\u03b63)\n\u2264 1 100 ( \u03c3\u2217kq+1 + ( 1 2 )z \u03c3\u2217kq ) .\nwhere (\u03b61) follows from Lemma 7 and 8. (\u03b62) follows by our assumptions on \u03c1, \u2225\u2225L(t) \u2212 L\u2217 \u2225\u2225 \u221e, and \u2225\u2225\u2225S\u0303(t) \u2212 S\u0303\u2217 \u2225\u2225\u2225 \u221e . (\u03b63) follows from our assumption on p.\nIn the following lemma, we prove that the value of the threshold computed using \u03c3k(M (t)) = \u03c3k(L \u2217+E1+E3), where E1, E3 are defined in (6), closely tracks the threshold that we would have gotten had we had access to the true eigenvalues of L\u2217, \u03c3\u2217k. Lemma 12. Let L\u2217, \u2126, S\u0303\u2217 satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of any stage q:\n1. \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e \u2264 2\u00b52r m ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k ) 2. \u2225\u2225\u2225S\u0303\u2217 \u2212 S\u0303(t)\n\u2225\u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217k+1 + ( 1 2 )z \u03c3\u2217k )\n3. Supp(S\u0303(t)) \u2286 Supp(S\u0303\u2217)\nwhere z \u2265 \u22123 and \u03c3\u2217k and \u03c3\u2217k+1 are the k and (k + 1)th singular values of L\u2217. Also, let E1 = S\u0303(t) \u2212 S\u0303\u2217 and E3 = ( I \u2212 P\u2126q,t\np\n)( L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217 ) be the error terms defined also in (6). Then, the following holds\n\u2200z > \u22123 w.p \u2265 1\u2212 n\u2212(10+log\u03b1):\n7\n8\n( \u03c3\u2217k+1 + ( 1\n2\n)z+1 \u03c3\u2217k ) \u2264 ( \u03bbk+1 + ( 1\n2\n)z+1 \u03bbk ) \u2264 9\n8\n( \u03c3\u2217k+1 + ( 1\n2\n)z+1 \u03c3\u2217k ) , (15)\nwhere \u03bbk := \u03c3k(M (t)) = \u03c3k(L \u2217 + E1 + E3) and E1, E3 are defined in (6). Proof. Using Weyl\u2019s inequality (Lemma 6), we have: : |\u03bbk \u2212 \u03c3\u2217k| \u2264 \u2016E1 + E3\u20162 and \u2223\u2223\u03bbk+1 \u2212 \u03c3\u2217k+1\n\u2223\u2223 \u2264 \u2016E1 + E3\u20162 We now proceed to prove the lemma as follows:\n\u2223\u2223\u2223\u2223\u2223\u03bbk+1 + ( 1 2 )z+1 \u03bbk \u2212 \u03c3\u2217k+1 \u2212 ( 1 2 )z+1 \u03c3\u2217k \u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u03bbk+1 \u2212 \u03c3\u2217k+1 \u2223\u2223+ ( 1 2 )z+1 |\u03bbk \u2212 \u03c3\u2217k| ,\n\u2264 \u2016E1 + E3\u20162\n( 1 + ( 1\n2\n)z+1) (\u03b6) \u2264 1\n100\n( \u03c3\u2217k+1 + ( 1\n2\n)z \u03c3\u2217k )( 1 + ( 1\n2\n)z+1) ,\n\u2264 1 8\n( \u03c3\u2217k+1 + ( 1\n2\n)z+1 \u03c3\u2217k ) ,\nwhere (\u03b6) follows from Lemma 11 and the last inequality follows from the assumption that z \u2265 \u22123.\nNext, we show that the projected gradient descent update (6) leads to a better estimate of L\u2217, i.e., we bound \u2016L(t+1) \u2212 L\u2217\u2016\u221e. Under the assumptions of the below given Lemma, the proof follows arguments similar to [NUNS+14] with additional challenge arises due to more involved error terms E1, E3.\nOur proof proceeds by first symmetrizing our matrices by rectangular dilation. We first begin by noting some properties of symmetrized matrices used in the proof of the following lemma.\nRemark 1. Let A be a m\u00d7 n dimensional matrix with singular value decomposition U\u03a3V \u22a4. We denote its symmetrized version be As := [ 0 A\u22a4\nA 0\n] . Then:\n1. The singular value decomposition of As is given by As = Us\u03a3sU \u22a4 s where\nUs := 1\u221a 2 [ V V U \u2212U ] \u03a3s := [ \u03a3 0 0 \u2212\u03a3 ]\n2. P2k (As) = [\n0 Pk(A\u22a4) Pk(A) 0\n]\n3. We have A2js =\n[ (A\u22a4A)j 0\n0 (AA\u22a4)j\n] A2j+1s = [ 0 (A\u22a4A)jA\u22a4\n(AA\u22a4)jA 0\n]\n4. We have\nUs\u03a3 \u2212j s U \u22a4 s =\n[ V \u03a3\u2212jV \u22a4 0\n0 U\u03a3\u2212jU\u22a4\n] when j is even\nUs\u03a3 \u2212j s U \u22a4 s =\n[ 0 V\u03a3\u2212jU\u22a4\nU\u03a3\u2212jV \u22a4 0\n] when j is odd\nProof of Lemma 2: L(t+1)s = P2k (L\u2217s +Hs)\nLet l = m + n. Let \u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbl be the eigenvalues of M (t)s = L\u2217s + Hs with |\u03bb1| \u2265 |\u03bb2| \u00b7 \u00b7 \u00b7 \u2265 |\u03bbl|. Let u1, u2, \u00b7 \u00b7 \u00b7 , ul be the corresponding eigenvectors of M (t)s . Using Lemma 6 along with the assumption on \u2016Hs\u20162, we have: |\u03bb2k| \u2265 3\u03c3\u2217k 4 . Let U\u039bV be the eigen vector decomposition of L(t+1). Let Us\u039bsU \u22a4 s to be the eigen vector decomposition of L (t+1) s . Then, using Remark 1 we have \u2200 i \u2208 [2k]:\n(L\u2217s +Hs)ui = \u03bbiui, i.e.\n( I \u2212 Hs\n\u03bbi\n) ui = L \u2217 sui.\nAs |\u03bb2k| \u2265 3\u03c3 \u2217 k 4 and \u2016Hs\u20162 \u2264 14\u03c3\u2217k, we can apply the Taylor\u2019s series expansion to get the following expression for ui:\nui = 1\n\u03bbi\n I + \u221e\u2211\nj=0\n( Hs \u03bbi )j  L\u2217sui.\nThat is,\nL(t+1)s =\n2k\u2211\ni=1\n\u03bbiuiu \u22a4 i =\n2k\u2211\ni=1\n\u03bb\u22121i \u2211\n0\u2264s,t<\u221e\n( Hs \u03bbi )s L\u2217suiu \u22a4 i L \u2217 s ( Hs \u03bbi )t ,\n= \u2211\n0\u2264s,t<\u221e\n2k\u2211\ni=1\n\u03bb \u2212(s+t+1) i H s sL \u2217 suiu \u22a4 i L \u2217 sH t s =\n\u2211\n0\u2264s,t<\u221e HssL \u2217 sUs\u039b \u2212(s+t+1) s U \u22a4 s L \u2217 sH t s.\nSubtracting L\u2217s on both sides and taking operator norm, we get:\n\u2225\u2225\u2225L(t+1)s \u2212 L\u2217s \u2225\u2225\u2225 \u221e = \u2225\u2225Us\u039bsU\u22a4s \u2212 L\u2217s \u2225\u2225 \u221e = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 0\u2264s,t<\u221e HssL \u2217 sUs\u039b \u2212(s+t+1) s U \u22a4 s L \u2217 sH t s \u2212 L\u2217s \u2225\u2225\u2225\u2225\u2225\u2225 \u221e ,\n\u2264 \u2225\u2225L\u2217sUs\u039b\u22121s U\u22a4s L\u2217s \u2212 L\u2217s \u2225\u2225 \u221e + \u2211\n1\u2264s+t<\u221e\n\u2225\u2225\u2225HssL\u2217sUs\u039b\u2212(s+t+1)s U\u22a4s L\u2217sHts \u2225\u2225\u2225 \u221e . (16)\nWe separately bound the first and the second term of RHS. The first term can be bounded as follows:\n\u2225\u2225L\u2217sUs\u039b\u22121s U\u22a4s L\u2217s \u2212 L\u2217s \u2225\u2225 \u221e (\u03b61) \u2264 \u2225\u2225\u2225\u2225L \u2217 s [ 0 V \u03a3\u22121U\u22a4\nU\u03a3\u22121V \u22a4 0\n] L\u2217s \u2212 L\u2217s \u2225\u2225\u2225\u2225 \u221e\n(17)\n\u2264 \u2225\u2225L\u2217V \u03a3\u22121U\u22a4L\u2217 \u2212 L\u2217 \u2225\u2225 \u221e (\u03b62) \u2264 \u00b5 2r\u221a mn \u2225\u2225L\u2217U\u039b\u22121U\u22a4L\u2217 \u2212 L\u2217 \u2225\u2225 2 (\u03b63) \u2264 \u00b5 2r\u221a mn (\u2223\u2223\u03c3\u2217k+1 \u2223\u2223+ 5 \u2016H\u20162 ) , (18)\nwhere (\u03b61) follows Remark 1, (\u03b62) from Lemma 10 and (\u03b63) follows from Claim 1 of Lemma 9.\nWe now bound second term of RHS of (16) which we again split in two parts. We first bound the terms with s+ t > logn:\n\u2225\u2225\u2225HssL\u2217sUs\u039b\u2212(s+t+1)s U\u22a4s L\u2217sHts \u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225HssL\u2217sUs\u039b\u2212(s+t+1)s U\u22a4s L\u2217sHts \u2225\u2225\u2225 2 (\u03b61) \u2264 \u2016Hs\u2016s+t2 4 ( 2\n\u03c3\u2217k\n)\u2212(s+t\u22121)\n\u2264 4 \u2016H\u20162 ( \u2016H\u20162 2\n\u03c3\u2217k\n)\u2212(s+t\u22121) (\u03b62) \u2264 4\u00b5 2r\nm \u2016H\u20162\n( 1\n2\n)\u2212(s+t\u22121\u2212logn) , (19)\nwhere (\u03b61) follows from the second claim of Lemma 9 and noting that \u2016Hs\u20162 = \u2016H\u20162 and (\u03b62) follows from assumption on \u2016H\u20162 and using the fact that s+ t \u2265 log n. Summing up over all terms with s+ t > logn, we get from 19 and 18:\n\u2225\u2225\u2225L(t+1)s \u2212 L\u2217s \u2225\u2225\u2225 \u221e \u2264 \u00b5 2r\u221a mn (\u2223\u2223\u03c3\u2217k+1 \u2223\u2223+ 20 \u2016H\u20162 ) +\n\u2211\n0<s+t\u2264log n\n\u2225\u2225\u2225HssL\u2217sUs\u039b\u2212(s+t+1)s U\u22a4s L\u2217sHts \u2225\u2225\u2225 \u221e\n(20)\nNow, for terms corresponding to 1 \u2264 s+ t \u2264 logn, we have: \u2225\u2225\u2225HssL\u2217sUs\u039b\u2212(s+t+1)s U\u22a4s L\u2217sHts \u2225\u2225\u2225 \u221e = max q1\u2208[m+n],q2\u2208[m+n] \u2223\u2223\u2223e\u22a4q1H s sL \u2217 sUs\u039b \u2212(s+t+1) s U \u22a4 s L \u2217 sH t seq2 \u2223\u2223\u2223\n\u2264 (\nmax q1\u2208[m+n]\n\u2225\u2225e\u22a4q1H s sU \u2217 s \u2225\u2225 2 )\u2225\u2225\u2225\u03a3\u2217s(U\u2217s )\u22a4Us\u039b\u2212(s+t+1)s U\u22a4s U\u2217s\u03a3\u2217s \u2225\u2225\u2225 2 ( max q2\u2208[m+n] \u2225\u2225e\u22a4q2H tU\u2217s \u2225\u2225 2 )\n(\u03b61) \u2264 \u00b5 2r\nm \u03c5s+t \u2225\u2225\u2225L\u2217sUs\u039b\u2212(s+t+1)s U\u22a4s L\u2217s \u2225\u2225\u2225 2 (\u03b62) \u2264 4\u00b5 2r m \u03c5s+t ( 2\n\u03c3\u2217k\n)s+t\u22121 \u2264 4\u00b5 2r\nm \u03c5\n( 1\n2\n)s+t\u22121 , (21)\nwhere (\u03b61) follows from assumption on H in the lemma statement, (\u03b62) follows from Claim 2 of Lemma 9.\nIt now remains to bound the terms, max q1\u2208[m+n]\n\u2225\u2225e\u22a4q1HssU\u2217s \u2225\u2225 2 . Note from Remark 1.1 that U\u2217s = 1\u221a 2\n[ V \u2217 V \u2217 U\u2217 \u2212U\u2217 ] .\nNow, we have the following cases for Hss :\nHjs =\n[( H\u22a4H ) s 2 0\n0 ( HH\u22a4 ) s 2\n] when s is even Hjs = [ 0 H\u22a4 ( HH\u22a4 )\u230a s2 \u230b\nH ( H\u22a4H )\u230a s2 \u230b 0\n] when s is odd\nIn these two cases, we have:\nHssU \u2217 s = 1\u221a 2\n[( H\u22a4H ) s 2 V \u2217\n( H\u22a4H ) s 2 V \u2217(\nHH\u22a4 ) s 2 U\u2217 \u2212 ( HH\u22a4 ) s 2 U\u2217\n] HssU \u2217 s = 1\u221a 2 [ H\u22a4 ( HH\u22a4 )\u230a s2 \u230b U\u2217 \u2212H\u22a4 ( HH\u22a4 )\u230a s2 \u230b U\u2217 H ( H\u22a4H )\u230a s2 \u230b V \u2217 H ( H\u22a4H )\u230a s2 \u230b V \u2217 ]\nThis leads to the following 4 cases for max q1\u2208[m+n]\n\u2225\u2225e\u22a4q1HssU\u2217s \u2225\u2225 2 :\nfor s even max q\u2032\u2208[n]\n\u2225\u2225\u2225e\u22a4q\u2032 ( H\u22a4H ) s 2 V \u2217 \u2225\u2225\u2225 2\nmax q\u2032\u2208[m]\n\u2225\u2225\u2225e\u22a4q\u2032 ( HH\u22a4 ) s 2 U\u2217 \u2225\u2225\u2225 2\nfor s odd max q\u2032\u2208[n]\n\u2225\u2225\u2225e\u22a4q\u2032H\u22a4 ( HH\u22a4 )\u230a s2 \u230b U\u2217 \u2225\u2225\u2225 2 max q\u2032\u2208[m] \u2225\u2225\u2225e\u22a4q\u2032H ( H\u22a4H )\u230a s2 \u230b V \u2217 \u2225\u2225\u2225 2\nwe get the bound on these terms in Lemma 15. Also, note from the Remark 1.2 that \u2225\u2225\u2225L\u2217s \u2212 L (t+1) s \u2225\u2225\u2225 \u221e\n= \u2225\u2225L\u2217 \u2212 L(t+1)\n\u2225\u2225 \u221e.\nNow, summing up 21 over all 1 \u2264 s+ t \u2264 logn and combining with 20 we get the required result. In the next lemma, we show that with the threshold chosen in the algorithm, we show an improvement in the estimation of S\u0303\u2217 by S\u0303(t+1).\nProof of Lemma 3: We first prove the first claim of the lemma. Consider an index pair (i, j) /\u2208 Supp(S\u0303\u2217).\n\u2223\u2223\u2223Mij \u2212 L(t)ij \u2223\u2223\u2223 \u2264 2\u00b5 2r\nm\n( \u03c3\u2217k+1 + ( 1\n2\n)z \u03c3\u2217k ) (\u03b61) \u2264 16\u00b5 2r\n7m\n( \u03bbk+1 + ( 1\n2\n)z \u03bbk ) \u2264 \u03b7 ( \u03bbk+1 + ( 1\n2\n)z \u03bbk )\nwhere (\u03b61) follows from the second assumption. Hence, we do not threshold any entry that is not corrupted by S\u0303\u2217.\nNow, we prove the second claim of the lemma. Consider an index entry (i, j) \u2208 Supp(S\u0303\u2217). Here, we consider two cases:\n1. The entry (i, j) \u2208 Supp(S\u0303(t)): Here the entry (i, j) is thresholded. We know that L(t)ij + S\u0303 (t) ij = L \u2217 ij+ S\u0303 \u2217 ij\nfrom which we get \u2223\u2223\u2223S\u0303(t)ij \u2212 S\u0303\u2217ij \u2223\u2223\u2223 = \u2223\u2223\u2223L\u2217ij \u2212 L (t) ij \u2223\u2223\u2223 \u2264 \u2225\u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225\u2225 \u221e\n2. The entry (i, j) /\u2208 Supp(S\u0303(t)): Here the entry (i, j) is not thresholded. We know that \u2223\u2223\u2223L\u2217ij + S\u0303\u2217ij \u2212 L (t) ij \u2223\u2223\u2223 \u2264 \u03b6 from which we get\n\u2223\u2223\u2223S\u0303\u2217ij \u2223\u2223\u2223 \u2264 \u03b6 + \u2223\u2223\u2223L\u2217ij \u2212 L (t) ij \u2223\u2223\u2223 (\u03b62)\n\u2264 36\u00b5 2r\n8m\n( \u03c3\u2217k+1 + ( 1\n2\n)z \u03c3\u2217k ) + 2\u00b52r\nm\n( \u03c3\u2217k+1 + ( 1\n2\n)z \u03c3\u2217k )\n\u2264 8\u00b5 2r\nm\n( \u03c3\u2217k+1 + ( 1\n2\n)z \u03c3\u2217k )\nwhere (\u03b62) follows from the second assumption along with the assumption about \u03b7 = \u00b52r m .\nThe above two cases prove the second statement of the lemma. Lemma 13. The number of iteration T in the inner loop of Algorithm 1 and Algorithm 2 satisfy:\nT \u2265 10 log ( 7n2\u00b52r\u03c3\u22171/\u01eb )\nw.p \u2265 1\u2212 n\u2212(10+log\u03b1). Here \u03c3\u22171 is the highest singular value of L\u2217, r is it\u2019s rank and \u00b5 is it\u2019s incoherence.\nProof. We have the bound since\n\u2225\u2225\u2225\u2225 n1n2 |\u2126| P\u2126 ( M \u2212 S\u0303(0) )\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225L \u2217 + ( I \u2212 P\u2126 p )(( S\u0303(0) \u2212 S\u2217 ) \u2212 L\u2217 ) + ( S\u2217 \u2212 S\u0303(0) )\u2225\u2225\u2225\u2225 2\n\u2265 \u03c3\u22171 \u2212 \u2016H\u20162 (\u03b61) \u2265 3 4 \u03c3\u22171\nwhere (\u03b61) follows from Lemma 11.\nWe will now prove Lemma 1 Proof of Lemma 1: Recall the definitions ofE1 = ( S\u0303\u2217 \u2212 S\u0303(t) ) , E2 = ( L(t) \u2212 L\u2217 ) , E3 = ( I \u2212 P\u2126q,t\np\n) (E2 \u2212 E1)\nand \u03b2 = 2 \u221a\nn p \u2016E2 \u2212 E1\u2016\u221e. Recall that H := E1 + E3 From Lemma 8, we have that 1\u03b2E3 satisfies Def-\ninition 1. This implies that the matrix 1 \u03b2 (E1 + E3) satisfies the conditions of Lemma 15. Now, we have \u22001 \u2264 a \u2264 \u2308logn\u2309 and \u2200i \u2208 [n]:\n\u2225\u2225ei(HH\u22a4)aU\u2217 \u2225\u2225 2 = \u03b22a \u2225\u2225\u2225\u2225\u2225ei (( 1 \u03b2 H )( 1 \u03b2 H )\u22a4)a U\u2217 \u2225\u2225\u2225\u2225\u2225 2\n(\u03b6) \u2264 \u03b22a ( \u03c1n\n\u03b2 \u2016E1\u2016\u221e + c logn\n)2a \u00b5 \u221a r\nm \u2264 \u00b5\n\u221a r\nm\n( \u03c1n \u2016E1\u2016\u221e + 2c \u221a n\np (\u2016E1\u2016\u221e + \u2016E2\u2016\u221e) logn\n)2a\nwhere (\u03b6) follows from the application of Lemma 15 along with the incoherence assumption on U\u2217. The other statements of the lemma can be proved in a similar manner by invocations of the different claims of Lemma 15.\nProof of Lemma 5: We know that:\n\u03bbkq \u2264 \u03c3\u2217kq + \u2016H\u20162 , \u03bbkq\u22121+1 \u2265 \u03c3 \u2217 kq\u22121+1 \u2212 \u2016H\u20162 , \u03bbkq \u2265\n\u03bbkq\u22121+1\n2\nCombining the three inequalities, we get:\n\u03c3\u2217kq \u2265 \u03c3\u2217kq\u22121+1 \u2212 3 \u2016H\u20162\n2\nApplying Lemma 11, we get the first claim of the lemma.\nSimilar to the first claim, we have:\n\u03bbkq+1 \u2265 \u03c3\u2217kq+1 \u2212 \u2016H\u20162 , \u03bbkq\u22121+1 \u2264 \u03c3 \u2217 kq\u22121+1 + \u2016H\u20162 , \u03bbkq+1 \u2264\n\u03bbkq\u22121+1\n2\nAgain, combining the three inequalities, we get:\n\u03c3\u2217kq+1 \u2264 \u03c3\u2217kq\u22121+1 + 3 \u2016H\u20162\n2\nAnother application of Lemma 11 gives the second claim.\nAlgorithm 2 L\u0302 = R-RMC(\u2126,P\u2126(M), \u01eb, r, \u03b7): Non-convex Robust Matrix Completion 1: Input: Observed entries \u2126, Matrix P\u2126(M) \u2208 Rm\u00d7n, convergence criterion \u01eb, target rank r, thresholding\nparameter \u03b7\n2: T \u2190 10 log 10\u00b5 2rn2\u2016P\u2126(M)\u20162n\n|\u2126|\u01eb /*Number of inner iterations*/\n3: Partition \u2126 into rT subsets {\u2126q,t : q \u2208 [r], t \u2208 [T ]} uniformly at random 4: L(0) = 0, M (0) \u2190 mn|\u2126|P\u2126(M), \u03b6 \u2190 \u03b7mn |\u2126| \u03c31(P\u2126(M)) 5: q \u2190 0 6: while \u03c3q+1(M\n(0)) > \u01eb2\u03b7m do 7: q \u2190 q + 1 8: for Iteration t = 0 to t = T do 9: S(t) = H\u03b6(P\u2126q,t(M \u2212 L(t))) /*Projection onto set of sparse matrices*/ 10: M (t) = L(t) \u2212 mn|\u2126q,t|P\u2126q,t(L (t) + S(t) \u2212M) /*Gradient Descent Update*/ 11: L(t+1) = Pq(M (t)) /*Projected Gradient Descent step*/\n12: Set threshold \u03b6 \u2190 \u03b7 ( \u03c3q+1(M (t)) + ( 1 2 )t \u03c3q(M (t)) ) 13: end for 14: S(0) = S(T ), L(0) = L(T+1),M (0) = M (T ) 15: end while 16: Return: L(T+1)"}, {"heading": "5.2 Algorithm R-RMC", "text": "Proof of Theorem 2: From Lemma 13 we know that T \u2265 log(3\u00b5 2nr\u03c3\u22171 m\u01eb ).\nConsider the stage q reached at the termination of the algorithm. We know from Lemma 14 that:\n1. \u2225\u2225E(T ) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217q+1 + ( 1 2 )T \u03c3\u2217q ) \u2264 8\u00b52r m \u03c3\u2217q+1 + \u01eb 10n 2. \u2225\u2225L(T ) \u2212 L\u2217 \u2225\u2225 \u221e \u2264 2\u00b52r m ( \u03c3\u2217q+1 + ( 1 2 )T \u03c3\u2217q ) \u2264 2\u00b52r m \u03c3\u2217q+1 + \u01eb 10n\nCombining this with Lemmas 6 and 11, we get:\n\u03c3q+1(M) \u2265 \u03c3\u2217q+1 \u2212 1\n100\n( \u03c3\u2217q+1 + m\u01eb\n10n\u00b52r\n) (22)\nWhen the while loop terminates, \u03b7\u03c3q+1 ( M (T ) ) < \u01eb2n , which from 22, implies that \u03c3 \u2217 q+1 < m\u01eb 7n\u00b52r . So we have:\n\u2016L\u2212 L\u2217\u2016\u221e = \u2225\u2225\u2225L(T ) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m |\u03c3\u2217kq+1|+ \u01eb 10n \u2264 \u01eb 2n .\nAs in the case of the proof of Theorem 1, the following lemma shows that we simultaneously make progress in both the estimation of L\u2217 and S\u0303\u2217 by L(t) and S\u0303(t) respectively. Similar to Lemma 4, we make use of Lemmas 3 and 2 to show how improvement in estimation of one of the quantities affects the other and the other five terms, \u2016H\u20162, max q\u2032\u2208[n] \u2225\u2225\u2225e\u22a4q\u2032 ( H\u22a4H )j V \u2217 \u2225\u2225\u2225 2 , max q\u2032\u2208[m] \u2225\u2225\u2225e\u22a4q\u2032 ( HH\u22a4 )j U\u2217 \u2225\u2225\u2225 2 , max q\u2032\u2208[n] \u2225\u2225\u2225e\u22a4q\u2032H\u22a4 ( HH\u22a4 )j U\u2217 \u2225\u2225\u2225 2 and\nmax q\u2032\u2208[m]\n\u2225\u2225\u2225e\u22a4q\u2032H ( H\u22a4H )j V \u2217 \u2225\u2225\u2225 2 are analyzed the same way:\nLemma 14. Let L\u2217, \u2126, S\u0303\u2217 and S\u0303(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the qth stage of Algorithm 2, S\u0303(t) and L(t) satisfy:\n\u2225\u2225\u2225S\u0303(t) \u2212 S\u0303\u2217 \u2225\u2225\u2225 \u221e \u2264 8\u00b5 2r m ( \u03c3\u2217q+1 + ( 1 2 )t\u22121 \u03c3\u2217q ) ,\nSupp ( S\u0303(t) ) \u2286 Supp ( S\u0303\u2217 ) , and\n\u2225\u2225\u2225L(t) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m ( \u03c3\u2217q+1 + ( 1 2 )t\u22121 \u03c3\u2217q ) .\nwith probability \u2265 1\u2212 ((q \u2212 1)T + t\u2212 1)n\u2212(10+log\u03b1) where T is the number of iterations in the inner loop.\nProof. We prove the lemma by induction on both q and t."}, {"heading": "Base Case: q = 1 and t = 0", "text": "We begin by first proving an upper bound on \u2016L\u2217\u2016\u221e. We do this as follows:\n\u2223\u2223L\u2217ij \u2223\u2223 = \u2223\u2223\u2223\u2223\u2223 r\u2211\nk=1\n\u03c3\u2217ku \u2217 ikv \u2217 jk \u2223\u2223\u2223\u2223\u2223 \u2264 r\u2211\nk=1\n\u2223\u2223\u03c3\u2217ku\u2217ikv\u2217jk \u2223\u2223 \u2264 \u03c3\u22171 r\u2211\nk=1\n\u2223\u2223u\u2217ikv\u2217jk \u2223\u2223 \u2264 \u00b5 2r\nm \u03c3\u22171\nwhere the last inequality follows from Cauchy-Schwartz and the incoherence of U\u2217. This directly proves the third claim of the lemma for the base case. We also note that due to the thresholding step and the incoherence assumption on L\u2217, we have:\n1. \u2225\u2225E(0) \u2225\u2225 \u221e \u2264 8\u00b52r m (\u03c3\u22172 + 2\u03c3 \u2217 1) 2. Supp ( S\u0303(t) ) = Supp ( S\u0303\u2217 ) .\nSo the base case of induction is satisfied."}, {"heading": "Induction over t", "text": "We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:\na) \u2225\u2225E(t) \u2225\u2225 \u221e \u2264 8\u00b52r m ( |\u03c3\u2217q+1|+ ( 1 2 )t\u22121 |\u03c3\u2217q | ) b) Supp ( S\u0303(t) ) \u2286 Supp ( S\u0303\u2217 ) .\nc) \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e \u2264 2\u00b52r m ( |\u03c3\u2217q+1|+ ( 1 2 )t\u22121 |\u03c3\u2217q | )\nwith probability 1\u2212 ((q \u2212 1)T + t\u2212 1)n\u2212(10+log\u03b1). Then by Lemma 2, we have:\n\u2225\u2225\u2225L(t+1) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 \u00b5 2r m ( |\u03c3\u2217kq+1|+ 20 \u2016H\u20162 + 8\u03c5 ) (23)\nFrom Lemma 1, we have:\n\u03c5 \u2264 \u03c1n \u2225\u2225\u2225E(t) \u2225\u2225\u2225 \u221e + 8\u03b2\u03b1 logn (\u03b61) \u2264 1 100 ( \u03c3\u2217q+1 + ( 1 2 )t\u22121 \u03c3\u2217q ) + 8\u03b2\u03b1 logn (\u03b62) \u2264 1 50 ( \u03c3\u2217q+1 + ( 1 2 )t\u22121 \u03c3\u2217q ) (24)\nwhere (\u03b61) follows from our assumptions on \u03c1 and our inductive hypothesis on \u2225\u2225E(t) \u2225\u2225 \u221e and (\u03b62) follows\nfrom our assumption on p and by noticing that \u2016D\u2016\u221e \u2264 \u2225\u2225E(t) \u2225\u2225 \u221e + \u2225\u2225L\u2217 \u2212 L(t) \u2225\u2225 \u221e. Recall that D = L(t) \u2212 L\u2217 + S\u0303(t) \u2212 S\u0303\u2217.\nFrom Lemma 11:\n\u2016H\u20162 \u2264 1\n100\n( \u03c3\u2217q+1 + ( 1\n2\n)t\u22121 \u03c3\u2217q ) (25)\nwith probability \u2265 1\u2212 n\u2212(10+log\u03b1). From Equations 25, 24 and 23, we have:\n\u2225\u2225\u2225L\u2217 \u2212 L(t+1) \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m ( \u03c3\u2217q+1 + ( 1 2 )t \u03c3\u2217q )\nwhich by union bound holds with probability \u2265 1 \u2212 ((q \u2212 1)T + t)n\u2212(10+log\u03b1). Hence, using Lemma 3 and 12 we have:\n1. \u2225\u2225E(t+1) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217q+1 + ( 1 2 )t \u03c3\u2217q ) 2. Supp ( S\u0303(t+1) ) \u2286 Supp ( S\u0303\u2217 ) .\nwhich also holds with probability \u2265 1 \u2212 ((q \u2212 1)T + t)n\u2212(10+log\u03b1). This concludes the proof for induction over t."}, {"heading": "Induction Over Stages q", "text": "We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:\n1. \u2225\u2225E(T ) \u2225\u2225 \u221e \u2264 8\u00b52r m ( \u03c3\u2217q+1 + ( 1 2 )T \u03c3\u2217q ) \u2264 8\u00b5 2r\u03c3\u2217q+1 m + \u01eb10n 2. Supp ( S\u0303(T ) ) \u2286 Supp ( S\u0303\u2217 ) .\nwith probability \u2265 1\u2212 (qT \u2212 1)n\u2212(10+log\u03b1). From Lemmas 6 and 11 we get:\n\u2223\u2223\u2223\u03c3q+1 ( M (T ) ) \u2212 \u03c3\u2217q+1 \u2223\u2223\u2223 \u2264 \u2016H\u20162 \u2264 1\n100\n( \u03c3\u2217q+1 + m\u01eb\n10n\u00b52r\n) (26)\nwith probability 1\u2212n\u2212(10+log\u03b1). We know that \u03b7\u03c3q+1 ( M (t) ) \u2265 \u01eb2n which with 26 implies that \u03c3\u2217q+1 > m\u01eb10n\u00b52r .\n\u2225\u2225\u2225L(T+1) \u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u00b5 2r m ( \u03c3\u2217q+1 + ( 1 2 )T+1 \u03c3\u2217q ) \u2264 2\u00b5 2r m ( \u03c3\u2217q+1 + m\u01eb 20\u00b52rn )\n\u2264 2\u00b5 2r\nm\n( \u03c3\u2217q+1 + \u03c3\u2217q+1 2 ) \u2264 2\u00b5 2r m ( 2\u03c3\u2217q+1 )\nBy union bound this holds with probability \u2265 1\u2212 qTn\u2212(10+log\u03b1). Now, from 12 and 3, we have through a similar series of arguments as above:\n\u2225\u2225\u2225E(T+1) \u2225\u2225\u2225 \u221e \u2264 8\u00b5 2r m ( 2\u03c3\u2217kq+1 ) (27)\nwhich holds with probability \u2265 1\u2212 qTn\u2212(10+log\u03b1)."}, {"heading": "5.3 Proof of a generalized form of Lemma 1", "text": "Lemma 15. Suppose H = H1 + H2 and H \u2208 Rm\u00d7n where H1 satisfies Definition 1 (Definition 7 from [JN15]) and H2 is a matrix with column and row sparsity \u03c1. Let U be a matrix with rows denoted as u1, . . . , um and let V be a matrix with rows denoted as v1, . . . , vn. Let eq be the q\nth vector from standard basis. Let \u03c4 = max{max\ni\u2208[m] \u2016ui\u2016 ,max i\u2208[n] \u2016vi\u2016}. Then, for 0 \u2264 a \u2264 logn:\nmax q\u2208[n]\n\u2225\u2225\u2225e\u22a4q ( H\u22a4H )a V \u2225\u2225\u2225 2 , max q\u2208[m] \u2225\u2225\u2225e\u22a4q ( HH\u22a4 )a U \u2225\u2225\u2225 2 \u2264 (\u03c1n \u2016H2\u2016\u221e + c logn)2a\u03c4\nmax q\u2208[n]\n\u2225\u2225\u2225e\u22a4q H\u22a4 ( HH\u22a4 )a U \u2225\u2225\u2225 2 , max q\u2208[m] \u2225\u2225\u2225e\u22a4q H ( H\u22a4H )a V \u2225\u2225\u2225 2 \u2264 (\u03c1n \u2016H2\u2016\u221e + c logn)2a+1\u03c4\nwith probability n\u22122 log c 4+4.\nProof. Similar to [JN15], we will prove the statement for q = 1 and it can be proved for q \u2208 [n] by taking a union bound over all q. For the sake of brevity, we will prove only the inequality:\nmax q\u2208[n]\n\u2225\u2225\u2225e\u22a4q ( H\u22a4H )a V \u2225\u2225\u2225 2 \u2264 (\u03c1n \u2016H2\u2016\u221e + c logn)2a\u03c4\nThe rest of the lemma follows by applying similar arguments to the appropriate quantities.\nLet \u03c9 : [2a] \u2192 {1, 2} be a function used to index a single term in the expansion of (H\u22a4H)a. We express the term as follows:\n(H\u22a4H)a = \u2211\n\u03c9\na\u220f\ni=1\nH\u22a4\u03c9(2i\u22121)H\u03c9(2i)\nWe will now fix one such term \u03c9 and then bound the length of the following random vector:\nv\u03c9 = e \u22a4 1\na\u220f\ni=1\n(H\u22a4\u03c9(2i\u22121)H\u03c9(2i))V\nLet \u03b1 be used to denote a tuple (i, j) of integers used to index entries in a matrix. Let T (i) be used to denote the parity function computed on i, i.e, 0 if i is divisible by 2 and 1 otherwise. This function indicates if the matrix in the expansion is transposed or not. We now introduce Bp,q(i,j),(k,l), p \u2208 {1, 2}, q \u2208 {0, 1} and Ap(i,j), p \u2208 {1, 2} which are defined as follows:\nAp(i,j) := \u03b4i,1(\u03b4p,1 + \u03b4p,21{(i,j)\u2208Supp(H2)})\nBp,q(i,j),(k,l) := (\u03b4q,1\u03b4j,l + \u03b4q,0\u03b4i,k)(\u03b4p,1 + \u03b4p,21{(k,l)\u2208Supp(H2)})\nwhere \u03b4i,j = 1 if i = j and 0 otherwise. We will subsequently write the random vector v\u03c9 in terms of the individual entries of the matrices. The role of Bp,q(i,j),(k,l) and A p (i,j) is to ensure consistency in the terms used to describe v\u03c9. We will use hi,\u03b1 to refer to (Hi)\u03b1.\nWith this notation in hand, we are ready to describe v\u03c9.\nv\u03c9 = \u2211\n\u03b11,...,\u03b12a \u03b11(1)=1\nA\u03c9(1)\u03b11 B \u03c9(2),T (2) \u03b11\u03b12 . . . B\u03c9(2a),T (2a)\u03b12a\u22121\u03b12a h\u03c9(1),\u03b11 \u00b7 \u00b7 \u00b7h\u03c9(2a),\u03b12av\u03b12a(2)\nWe now write the squared length of v\u03c9 as follows:\nX\u03c9 = \u2211\n\u03b11,...,\u03b12a,\u03b1 \u2032 1,...,\u03b1 \u2032 2a\n\u03b11(1)=1,\u03b1 \u2032 1(1)=1\nA\u03c9(1)\u03b11 B \u03c9(2),T (2) \u03b11\u03b12 . . . B\u03c9(2a),T (2a)\u03b12a\u22121\u03b12a h\u03c9(1),\u03b11 \u00b7 \u00b7 \u00b7h\u03c9(2a),\u03b12a\nA\u03c9(1)\u03b11 B \u03c9(2),T (2) \u03b1\u20321\u03b1 \u2032 2 . . . B \u03c9(2a),T (2a) \u03b1\u20322a\u22121\u03b1 \u2032 2a h\u03c9(1),\u03b1\u20321 \u00b7 \u00b7 \u00b7h\u03c9(2a),\u03b1\u20322a\u3008v\u03b12a(2), v\u03b1\u20322a(2)\u3009\nWe can see from the above equations that the entries used to represent v\u03c9 are defined with respect to paths in a bipartite graph. In the following, we introduce notations to represent entire paths rather than just individual edges:\nLet \u03b1 := (\u03b11, . . . , \u03b12a) and\n\u03b6\u03b1 := A \u03c9(1) \u03b11 B\u03c9(2),T (2)\u03b11\u03b12 . . . B \u03c9(2a),T (2a) \u03b12a\u22121\u03b12a h\u03c9(1),\u03b11 . . . h\u03c9(2a),\u03b12a\nNow, we can write:\nX\u03c9 = \u2211\n\u03b1,\u03b1\u2032\n\u03b11(1)=\u03b1 \u2032 1(1)=1\n\u03b6\u03b1\u03b6\u03b1\u2032\u3008v\u03b12a(2), v\u03b1\u20322a(2)\u3009\nCalculating the kth moment expansion of X\u03c9 for some number k, we obtain:\nE[Xk\u03c9] = \u2211\n\u03b11,...,\u03b12k\nE[\u03b6\u03b11 . . . \u03b6\u03b12k\u3008v\u03b112a(2), v\u03b122a(2)\u3009 . . . \u3008v\u03b12k\u221212a (2), v\u03b12k2a(2)\u3009] (28)\nWe now show how to bound the above moment effectively. Notice that the moment is defined with respect to a collection of 2k paths. We denote this collection by \u2206 := (\u03b11, . . . ,\u03b12k). For each such collection, we define a partition \u0393(\u2206) of the index set {(s, l) : s \u2208 [2k], l \u2208 [2a]} where (s, l) and (s\u2032, l\u2032) are in the same equivalence class if \u03c9(l) = \u03c9(l\u2032) = 1 and \u03b1sl = \u03b1 s\u2032\nl\u2032 . Additionally, each (s, l) such that \u03c9(l) = 2 is in a separate equivalence class.\nWe bound the expression in (28) by partitioning all possible collections of 2k paths based on the partitions defined by them in the above manner. We then proceed to bound the contribution of any one specific path to (28) following a particular partition \u0393, the number of paths satisfying that particular partition and finally, the total number of partitions. Since, H1 is a matrix with 0 mean, any equivalence class containing an index (s, l) such that \u03c9(l) = 1 contains at least two elements.\nWe proceed to bound (28) by taking absolute values:\nE[Xk\u03c9] \u2264 \u2211\n\u03b11,...,\u03b12k\nE[|\u03b6\u03b11 | . . . |\u03b6\u03b12k ||\u3008v\u03b112a(2), v\u03b122a(2)\u3009| . . . |\u3008v\u03b12k\u221212a (2), v\u03b12k2a(2)\u3009|] (29)\nWe now fix one particular partition and bound the contribution to (29) of all collections of paths \u2206 that correspond to a valid partition \u0393.\nWe construct from \u0393 a directed multigraph G. The equivalence classes of \u0393 form the vertex set of G, V (G). There are 4 kinds of edges in G where each type is indexed by a tuple (p, q) where p \u2208 {1, 2}, q \u2208 {0, 1}.\nWe denote the edge sets corresponding to these 4 edge types by E(1,0), E(1,1), E(2,0) and E(2,1) respectively. An edge of type (p, q) exists from equivalence class \u03b31 to equivalence class \u03b32 if there exists (s, l) \u2208 \u03b31 and (s\u2032, l\u2032) \u2208 \u03b32 such that l\u2032 = l + 1, s = s\u2032, \u03c9(s\u2032) = p and T (l\u2032) = q. The summation in 29 can be written as follows:\nE[|\u03b6\u03b11 | . . . |\u03b6\u03b12k | \u2223\u2223\u2223\u3008v\u03b112a(2), v\u03b122a(2)\u3009 \u2223\u2223\u2223 . . . \u2223\u2223\u2223\u3008v\u03b12k\u221212a (2), v\u03b12k2a(2)\u3009 \u2223\u2223\u2223]\n\u2264 \u03c42k ( 2k\u220f\ns=1\nA \u03c9(1) \u03b1s1\n2a\u22121\u220f\nl=1\nB \u03c9(l+1),T (l+1) \u03b1s\nl ,\u03b1s l+1\n) E [( 2k\u220f\ns=1\n2a\u220f\nl=1\n\u2223\u2223\u2223h\u03c9(l),\u03b1s l \u2223\u2223\u2223 )]\n(\u03b61) \u2264 \u03c42k ( 2k\u220f\ns=1\nA \u03c9(1) \u03b1s1\n2a\u22121\u220f\nl=1\nB \u03c9(l+1),T (l+1) \u03b1s\nl ,\u03b1s l+1\n) \u220f\n\u03b3\u2208V1(G)\n1\nn\n\u220f\n\u03b3\u2208V2(G) \u2016H2\u2016\u221e\n= \u03c42k \u2016H2\u2016w2\u221e\nnw1\n( 2k\u220f\ns=1\nA \u03c9(1) \u03b1s1\n2a\u22121\u220f\nl=1\nB \u03c9(l+1),T (l+1) \u03b1s\nl ,\u03b1s l+1\n)\nwhere (\u03b61) follows from the moment conditions on H1. V1(G) and V2(G) are the vertices in the graph corresponding to tuples (i, j) such that \u03c9(j) = 1 and \u03c9(j) = 2 respectively and w1 = |V1(G)|, w2 = |V2(G)|. We first consider an equivalence class \u03b31 such that there exists an index (s, l) \u2208 \u03b31 and l = 1. We form a spanning tree T1 of all the nodes reachable from \u03b31 with \u03b31 as root. We then remove the nodes V (T1) from the graph G and repeat this procedure until we obtain a set of l trees T1, . . . , Tl with roots \u03b31, . . . , \u03b3l such that l\u22c3\ni=1\nV (Gi) = V (G). This happens because every node is reachable from some equivalence class which\ncontains an index of the form (s, 1). Also, each of these trees Ti, \u2200 i \u2208 [l] is disjoint in their vertex sets. Given this decomposition, we can factorize the above product as follows:\nE[Xk\u03c9] \u2264 \u03c42k \u2016H2\u2016w2\u221e\nnw1\nl\u220f\nj=1\n\u2211\n\u03b11,...,\u03b1vj\nA\u03c9(1)\u03b11\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(1,0)(Tj) B1,0\u03b1\u03b3\u03b1\u03b3\u2032 \u220f\n{\u03b3,\u03b3\u2032}\u2208E(1,1)(Tj) B1,1\u03b1\u03b3\u03b1\u03b3\u2032\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(2,0)(Tj) B2,0\u03b1\u03b3\u03b1\u03b3\u2032\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(2,1)(Tj) B2,1\u03b1\u03b3\u03b1\u03b3\u2032 (30)\nFor a single connected component, we can compute the summation bottom up from the leaves. First, notice that:\n\u2211 \u03b1\u03b3\u2032 B2,1\u03b1\u03b3\u03b1\u03b3\u2032 \u2264 \u03c1n \u2211 \u03b1\u03b3\u2032\nB2,0\u03b1\u03b3\u03b1\u03b3\u2032 \u2264 \u03c1n \u2211 \u03b1\u03b3\u2032 B1,1\u03b1\u03b3\u03b1\u03b3\u2032 = n \u2211 \u03b1\u03b3\u2032 B1,0\u03b1\u03b3\u03b1\u03b3\u2032 = n\nWhere the first two follow from the sparsity of H2. Every node in the tree Tj with the exception of the root has a single incoming edge. For the root, \u03b3j , we have:\n\u2211 \u03b11 A \u03c9(1) \u03b11 \u2264 \u03c1n for \u03c9(1) = 2 \u2211 \u03b11 A \u03c9(1) \u03b11 = n for \u03c9(1) = 1\nFrom the above two observations, we have:\n\u2211 \u03b11,...,\u03b1vj A\u03c9(1)\u03b11\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(1,0)(Tj) B1,0\u03b1\u03b3\u03b1\u03b3\u2032\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(1,1)(Tj) B1,1\u03b1\u03b3\u03b1\u03b3\u2032\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(2,0)(Tj) B2,0\u03b1\u03b3\u03b1\u03b3\u2032\n\u220f\n{\u03b3,\u03b3\u2032}\u2208E(2,1)(Tj) B2,1\u03b1\u03b3\u03b1\u03b3\u2032 \u2264 (\u03c1n) w2,jnw1,j\nwhere wk,j represents the number of vertices in the j th component which contain tuples (i, j) such that \u03c9(j) = k for k \u2208 {1, 2}. Plugging the above in (30) gives us\nE[Xk\u03c9(\u0393)] \u2264 \u03c42k \u2016H2\u2016w2\u221e\nnw1 (\u03c1n)\n\u2211 j w2,jn \u2211 j w1,j = \u03c42k \u2016H2\u2016w2\u221e (\u03c1n)w2\nLet a1 and a2 be defined as |{i : \u03c9(i) = 1}| and |{i : \u03c9(i) = 2}| respectively (Note that w2 = 2a2k). Summing up over all possible partitions (there are (2a1k) 2a1k of them), we get our final bound on E [ X\u0302k\u03c9 ] as \u03c42k(\u03c1n \u2016H2\u2016\u221e)2a2k(2a1k)2a1k. Now, we bound the probability that X\u0302\u03c9 is too large. Choosing k = \u2308 logn a1 \u2309 and applying the kth moment Markov inequality, we obtain:\nPr [\u2223\u2223\u2223X\u0302\u03c9 \u2223\u2223\u2223 > (c logn)2a1\u03c42(\u03c1n \u2016H2\u2016\u221e)2a2 ] \u2264 E [\u2223\u2223\u2223X\u0302\u03c9 \u2223\u2223\u2223 k ](\n1 (c logn)2a1\u03c42(\u03c1n \u2016H2\u2016\u221e)2a2 )k\n\u2264 (\n2ka1 c logn\n)2ka1\n\u2264 n\u22122 log c4\nTaking a union bound over all the 2a possible \u03c9, over values of a from 1 to logn and over the n values of q, we get the required result."}, {"heading": "5.4 Additional Experimental Results", "text": "We detail some additional experiments performed with Algorithm 1 in this section. The experiments were performed on synthetic data and real world data sets.\nSynthetic data. We generate a random matrix M \u2208 R2000\u00d72000 in the same way as described in Section 4. In these experiments our aim is to analyze the behavior of the algorithm in extremal cases. We consider two of such cases : 1) sampling probability is very low (Figure 3 (a)), 2) number of corruptions is very large (Figure 3 (b)). In the first case, we see that the we get a reasonably good probability of recovery (\u223c 0.8) even with very low sampling probability (0.07). In the second case, we observe that the time taken to recover seems almost independent of the number of corruptions as long as they are below a certain threshold. In our experiments we saw that on increasing the \u03c1 to 0.2 the probability of recovery went to 0. To compute the probability of recovery we ran the experiment 20 times and counted the number of successful runs.\nForeground-background separation. We present results for one more real world data set in this section. We applied our PG-RMC method (with varying p) to the Escalator video. Figure 4 (a) shows one frame\nfrom the video. Figure 4 (b) shows the extracted background from the video by using our method (PGRMC , Algorithm 1) with probability of sampling p = 0.05. Figure 4 (c) compares objective function value for different p values.\n0.05 0.1 0.15 0.2 0.25 0.3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1e-01 1e-02 1e-03 1e-04 1e-05"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "In this paper, we consider the problem of Robust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of the corrupted entries using hard-thresholding. Our algorithm solves RMC using nearly optimal number of observations as well as nearly optimal number of corruptions. Our result also implies significant improvement over the existing time complexity bounds for the low-rank matrix completion problem. Finally, an application of our result to the robust PCA problem (low-rank+sparse matrix separation) leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our empirical results corroborate our theoretical results and show that even for moderate sized problems, our method for robust PCA is an an order of magnitude faster than the existing methods.", "creator": "LaTeX with hyperref package"}}}