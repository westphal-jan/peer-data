{"id": "1606.07770", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Captioning Images with Diverse Objects", "abstract": "we propose novel object captioner ( noc ), a complementary deep visual semantic captioning model that can describe a large number of object categories not present in existing image - caption datasets. recent captioning models are limited in their ability to scale simultaneously and describe concepts outside of paired dynamic image - text corpora. our model takes advantage of external link sources - labeled images from object recognition datasets, interfaces and semantic knowledge extracted from unannotated text - and combines atop them to generate descriptions about novel objects. we propose minimizing a joint objective which can learn from diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel html objects outside of image - caption datasets. we demonstrate that our model exploits semantic information to generate coherent captions for hundreds of object categories in the imagenet object recognition search dataset that are not observed in image - caption training data, as well as many categories structures that are observed very rarely.", "histories": [["v1", "Fri, 24 Jun 2016 17:53:45 GMT  (1873kb,D)", "http://arxiv.org/abs/1606.07770v1", "9 pages, 3 figures"], ["v2", "Thu, 1 Dec 2016 20:54:17 GMT  (8155kb,D)", "http://arxiv.org/abs/1606.07770v2", "16 pages, 12 figures, 7 tables. New empirical results, more qualitative and quantitative results. Includes human evaluations"], ["v3", "Thu, 20 Jul 2017 18:06:27 GMT  (9001kb,D)", "http://arxiv.org/abs/1606.07770v3", "CVPR 2017 Camera ready version. 17 pages (8 + 9 supplement), 12 figures, 8 tables. Includes project pagethis http URL"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["subhashini venugopalan", "lisa anne hendricks", "marcus rohrbach", "raymond mooney", "trevor darrell", "kate saenko"], "accepted": false, "id": "1606.07770"}, "pdf": {"name": "1606.07770.pdf", "metadata": {"source": "CRF", "title": "Captioning Images with Diverse Objects", "authors": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach"], "emails": ["vsub@cs.utexas.edu", "lisa_anne@berkeley.edu", "rohrbach@berkeley.edu", "mooney@cs.utexas.edu", "trevor@eecs.berkeley.edu", "saenko@cs.uml.edu"], "sections": [{"heading": "1 Introduction", "text": "Modern visual classifiers (He et al., 2016; Simonyan and Zisserman, 2014) can recognize thousands of object categories, some of which are basic or entry-level (e.g. television), and others that are fine-grained and task specific (e.g. dial-phone, cell-phone). However, recent stateof-the-art visual captioning systems (Vinyals et\n\u2217Equal contribution.\nal., 2015; Fang et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Kiros et al., 2015; Mao et al., 2015a) that learn directly from image and description pairs fail in their ability to generalize and describe this vast set of recognizable objects in context. While such systems could be scaled by building larger image/video description datasets, obtaining such captioned data would be expensive and laborious. Furthermore, visual description is challenging because models must not only correctly identify visual concepts contained in an image, but must also compose these concepts into a coherent sentence.\nRecent deep captioning models rely entirely on paired images and captions (to model language and vision) and are hence unable to describe objects which do not explicitly appear in imagecaption datasets. Recent work from Hendricks et al. (2016) shows that, to incorporate the vast knowledge of current visual recognition models without explicit paired captions, models must learn from external sources and learn to compose sentences about visual concepts which are infrequent or non-existent in image-description corpora as in Fig. 1. In this spirit, we propose NOC, an architecture that integrates knowledge\nar X\niv :1\n60 6.\n07 77\n0v 1\n[ cs\n.C V\n] 2\n4 Ju\nn 20\nfrom external visual recognition datasets as well as semantic information from independent unannotated text corpora to generate captions for a variety of objects.\nSpecifically, we introduce auxiliary objectives which allows our network to learn a description model simultaneously with a deep language model and visual recognition system. Unlike previous work, our auxiliary objectives allow our model to learn relevant information from multiple data sources in an end-to-end fashion. Another notable aspect of our model is that it implicitly leverages distributional embeddings trained on external text corpora enabling it to describe unseen object categories based on the descriptions of the training images. Additionally, unlike previous captioning approaches, our model combines textual and visual confidences at the semantic (word) level, fully exploiting the visual classifier\u2019s ability to recognize a wide range of objects for the novel object captioning task. We demonstrate that our model\u2019s auxiliary training objectives along with its ability to leverage visual and semantic information, enable it to generate descriptions for hundreds of objects not present in description datasets. Compared to previous work (Hendricks et al., 2016) for the task, our model is end-to-end trainable and provides improved performance. Additionally, our model directly extends to generate descriptions for objects which appear infrequently/rarely in image description datasets."}, {"heading": "2 Related Work", "text": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training. Deep visual description frameworks first encode an image into a fixed length feature vector and then generate a description by either conditioning text generation on image features (Vinyals et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al.,\n2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word. Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015). Also, in almost all cases, the parameters of the visual pipeline are initialized with weights trained on the ImageNet classification task. On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al., 2015) have also been explored. Our model is similar to the CNN-RNN frameworks in Mao et al. (2015a) and Hendricks et al. (2016).\nNovel object captioning Recently Mao et al. (2015b) proposed an approach, that extends a model\u2019s capability to describe a small set of novel concepts (e.g. quidditch, samisen) from paired training data while retaining its ability to describe older concepts learned previously. On the other hand, Hendricks et al. (2016) introduce a model that can describe many objects already existing in English corpora and object recognition datasets (ImageNet) but not in the caption corpora (e.g. pheasant, otter). Our focus is on the latter case. Hendricks et al. (2016) integrate information from external text and visual sources, and explicitly transfer parameters from objects seen in image-caption data to unseen ImageNet objects to generate descriptions for the new objects. While this works well for many ImageNet classes, their parameter transfer is explicit and cannot be trained end-to-end. Furthermore, their model cannot caption objects for which few paired training examples already exist. Our proposed framework integrates information from external sources implicitly, making it end-to-end trainable, as well as extends directly to caption ImageNet objects with few or no descriptions.\nMulti-modal and Zero-Shot Learning Another closely related line of research takes advantage of distributional semantics to learn a joint embedding space using visual and textual infor-\nmation for zero-shot labeling of novel object categories (Frome et al., 2013; Norouzi et al., 2013), as well as retrieval of images with text (Socher et al., 2014; Lazaridou et al., 2014). Visual description itself can be cast as a multimodal learning problem in which caption words w0, ..., wn\u22121 and an image are projected into a joint embedding space before the next word in a caption, wn, is generated (Mao et al., 2015a; Kiros et al., 2015). Although our approach uses distributional word embeddings, our model differs in the sense that it can be trained with unpaired text and visual data but still combine the semantic information at a later stage during caption generation."}, {"heading": "3 Novel Object Captioner (NOC)", "text": "Our NOC model is illustrated in Fig. 2. It not only learns from paired image-caption data, but also leverages semantic information from unannotated text and integrates it with a visual recognition model. By introducing auxiliary loss functions (objectives) and jointly training different components on multiple data sources, we obtain a visual description model which simultaneously learns an independent object recognition model, as well as a language model. We start by first training a simple LSTMbased language model (LM) (Sundermeyer et al., 2010) for sentence generation. Our LM incorporates dense representations for words from distributional embeddings (GloVe, Pennington et al. (2014)) pre-trained on external text corpora.\nSimultaneously, we also train a state-of-the-art visual recognition network to provide confidences over words in the vocabulary given an image. To generate descriptions conditioned on image content, we combine the predictions of our language and visual recognition networks by summing textual and visual confidences at the word level.\nIn this way, we decompose our model into discrete textual and visual pipelines which can be trained exclusively using unpaired text and unpaired image data (networks on left and right of Fig. 2). When training our description model, we introduce auxiliary image-specific (LIM), and text-specific (LLM) objectives along with the paired image-caption (LCM) loss function. These loss functions when trained jointly influence our model to not only produce reasonable image descriptions, but also predict visual concepts as well as generate cohesive text (language modeling). We first discuss the auxiliary objectives and the joint training, and then discuss how we leverage embeddings trained with external text to compose descriptions about novel objects."}, {"heading": "3.1 Auxiliary Training Objectives", "text": "Our motivation for introducing auxiliary objectives is to learn how to describe images without losing the ability to recognize more objects. Typically, image-captioning models incorporate a visual classifier pre-trained on a source domain (e.g. ImageNet dataset) and then tune it to the target domain (the image-caption dataset). However, important information from the source dataset can be suppressed if similar information is not present when fine-tuning, leading the network to forget (over-write weights) for objects not present in the target domain. This is problematic in our scenario in which the model relies on the source datasets to learn a large variety of visual concepts not present in the target dataset. However, with pre-training as well as the complementary auxiliary objectives the model maintains its ability to recognize a wider variety of objects and is encouraged to describe objects which are not present in the target dataset at test time. For the ease of exposition, we abstract away the details of the language and the visual models and first describe the joint training objectives of the\ncomplete model, i.e. the text-specific loss, the image-specific loss, and the image-caption loss. We will then describe the language and the visual models.\nImage-specific Loss Our visual recognition model (Fig. 2a, left) is a neural network parametrized by \u03b8I and is trained on object recognition datasets. Unlike typical visual recognition models that are trained with a single label on a classification task, for the task of image captioning an image model that has high confidence over multiple visual concepts occurring in an image simultaneously would be preferable. Hence, we choose to train our model using multiple labels (more in Sec. 4.2) with a multi-label loss. If l denotes a label and zl denotes the binary groundtruth value for the label, then the optimization objective for the visual model is given by the cross-entropy loss (LIM ):\nLIM(I; \u03b8I) = \u2212 \u2211 l zllog(Sl(fIM (I; \u03b8I)))\n+(1\u2212 zl)(1\u2212 log(Sl(fIM (I; \u03b8I)))) (1)\nwhere Si(x) is the output of a softmax function over index i and input x, and fIM , is the activation of the final layer of the visual recognition network.\nText-specific Loss Our language model (Fig. 2, right) is based on LSTM Recurrent Neural Networks. We denote the parameters of this network by \u03b8L, and the activation of the final layer of this network by fLM . The language model is trained to predict the next word wt in a given sequence of words w0, ..., wt\u22121. This is optimized using the softmax loss LLM which is equivalent to the maximum-likelihood:\nLLM(w0, ..., wt\u22121; \u03b8L) = \u2212 \u2211 t log(Swt(fLM (w0, ..., wt\u22121; \u03b8L))) (2)\nImage-caption Loss The goal of the image captioning model (Fig. 2, center) is to generate a sentence conditioned on an image (I). NOC predicts the next word in a sequence, wt, conditioned on previously generated words (w0, ..., wt\u22121) and\nan image (I), by summing activations from the deep language model, which operates over previous words, and the deep image model, which operates over an image. We denote these final (summed) activations by fCM . Then, the probability of predicting the next word is given by,\nP (wt|w0, ..., wt\u22121, I) = Swt(fCM (w0, ..., wt\u22121, I; \u03b8))\n=Swt(fLM (w0, ..., wt\u22121; \u03b8L) + fIM (I; \u03b8I))\nGiven pairs of images and descriptions, the caption model optimizes the parameters of the underlying language model (\u03b8L) and image model (\u03b8I) by minimizing the caption model loss LCM :\nLCM(w0, ., wt\u22121, I; \u03b8L, \u03b8I) = \u2212 \u2211 t log(Swt(fCM (w0, ., wt\u22121, I; \u03b8L, \u03b8I))) (3)\nJoint Training with Auxiliary Losses While many previous approaches have been successful on image captioning by pre-training the image and language models and tuning the caption model alone (Eqn. 3), this is insufficent to generate descriptions for objects outside of the image-caption dataset since the model tends to \u201cforget\u201d (over-write weights) for objects only seen in external data sources. To remedy this, we propose to train the image model, language model, and caption model simultaneously on different data sources. The NOC model\u2019s final objective simultaneously minimizes the three individual complementary objectives:\nL = LCM + \u03b1LIM + \u03b2LLM (4)\nwhere \u03b1 and \u03b2 are hyper-parameters which determine the weighting between different losses (we use \u03b1 = 1 and \u03b2 = 1). By sharing the weights of the caption model\u2019s network with the image network and the language network (as depicted in Fig. 2 (a)), the model can be trained simultaneously on independent image-only data, unannotated text data, as well as paired image-caption data. Consequently, co-optimizing different objectives aids the model in recognizing categories outside of the paired image-sentence data."}, {"heading": "3.2 Language Model with Semantic Embeddings", "text": "Our language model consists of the following components: a continuous lower dimensional embedding space for words (Wglove), a single recurrent (LSTM) hidden layer, and two linear transformation layers where the second layer (W Tglove) maps the vectors to the size of the vocabulary. Finally a softmax activation function is used on the output layer to produce a normalized probability distribution. The cross-entropy loss which is equivalent to the maximum-likelihood is used as the training objective.\nIn addition to our joint objective (Eqn.4), we also employ semantic embeddings in our language model to help generate sentences when describing novel objects. Specifically, the initial input embedding space (Wglove) is used to represent the input (one-hot) words into semantically meaningful dense fixed-length vectors. While the final transformation layer (W Tglove) reverses the mapping of a dense vector back to the full vocabulary with the help of a softmax activation function. These distributional embeddings (Mikolov et al., 2013; Pennington et al., 2014) share the property that words that are semantically similar have similar vector representations. The intuitive reason for using these embeddings in the input and output transformation layers is to help the language model treat words unseen in the image-text corpus to (semantically) similar words that have previously been seen so as to encourage compositional sentence generation i.e. encourage it to use new/rare word in a sentence description based on the visual confidence."}, {"heading": "3.3 Visual Classifier", "text": "The other main component of our model is the visual classifier. We employ a recent convolutional neural network (VGG-16 (Simonyan and Zisserman, 2014)) as the visual recognition network for the image classifier. We modify the final layers of the network to incorporate the multi-label loss (Eqn. 1) to predict visual confidence over multiple labels in the vocabulary. The rest of the classification network remains unchanged."}, {"heading": "4 Datasets", "text": "In this section we describe the image description dataset as well as the external text and image datasets used in our experiments."}, {"heading": "4.1 External Text Corpus (WebCorpus)", "text": "We extract sentences from Gigaword, the British National Corpus (BNC), UkWaC, and Wikipedia. Stanford CoreNLP 3.4.2 (Manning et al., 2014) was used to extract tokenizations. This dataset was used to train the LSTM language model. For the dense word representation in the network, we use GloVe (Pennington et al., 2014) pre-trained on 6B tokens of external corpora including Gigaword and Wikipedia. To create our LM vocabulary we identified the 80,000 most frequent tokens from the combined external corpora. We refine this vocabulary further to a set of 72,700 words that also had GloVe embeddings."}, {"heading": "4.2 Image Caption data", "text": "To empirically evaluate the ability of NOC to describe new objects we use the training and test set from Hendricks et al. (2016). The dataset consists of a subset of MSCOCO (Lin et al., 2014) which excludes all sentences that describe one of the following eight objects: bottle, bus, couch, microwave, pizza, racket, suitcase, or zebra. We then evaluate how well NOC can generate descriptions about these eight held-out words."}, {"heading": "4.3 Image data", "text": "We also qualitatively evaluate sentences generated by NOC on approximately 700 different ImageNet (Russakovsky et al., 2014) objects which are not present in the MSCOCO dataset. We choose this set by identifying objects that are present in both ImageNet and our language corpus (vocabulary), but not present in MSCOCO. Chosen words span a variety of categories including fine-grained categories (e.g., \u201cbloodhound\u201d and \u201cchrysanthemum\u201d), adjectives (e.g., \u201cchiffon\u201d), and entry level words (e.g., \u201ctoad\u201d).\nRare objects In order to understand if including additional data enables NOC to better generate sentences about rare words, we select 52 objects which are in ImageNet as well as the\nMSCOCO vocabulary. Selected words occur with varying frequency in the MSCOCO training set, with about 52 mentions on average (median 27) across all training sentences. For example, words such as bonsai only appear 5 times, while some others such as pumpkin appears 58 times, and scarf appears 144 times."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Empirical Evaluation on MSCOCO", "text": "We empirically evaluate the ability of our proposed model to describe novel objects by following the experimental setup of (Hendricks et al., 2016). We optimize each loss in our model with the following datasets: the caption model, which jointly learns the parameters \u03b8L and \u03b8I , is trained only on the subset of MSCOCO without the 8 objects (see section 4.2), the image model, which updates parameters \u03b8I , is optimized using labeled images, and the language model which updates parameters \u03b8L, is trained using the corresponding descriptions. We first present evaluations for the in-domain setting in which the image classifier is trained with all COCO training images and the language model is trained with all sentences in the training set before comparing training from different data sources. We use the F1 metric, which determines if the model mentions the object name in the description of the images containing the object, and the METEOR metric (Denkowski and Lavie, 2014) to evaluate description quality.\nCOCO heldout objects. Results of the models on the 8 held-out COCO objects are presented in Table 1. This compares the F1 score achieved by NOC to the previously existing method, DCC (Hendricks et al., 2016), and demonstrates that NOC is able to integrate new vocabulary words into description better for every\nobject in the held-out dataset except \u201ccouch\u201d and \u201cmicrowave\u201d, outperforming DCC considerably.\nAblations. Table 2 compares how different aspects of training impact the overall performance. Vision Contribution (tuned): The model that does not incorporate Glove or LM pretraining has poor performance (METEOR 15.78, F1 14.41); this ablation shows the contribution of the vision model alone in recognizing and describing the held out objects. Glove contribution: The model trained without the auxiliary objective, performs better with F1 of 25.38 and METEOR of 19.80; this improvement comes largely from the GloVe embeddings which help in captioning novel object classes. Vision Contribution (NoTuning): It\u2019s interesting to note that when we fix the pre-trained classifier\u2019s weights (trained on all objects), before training on the image-caption COCO subset, the F1 increases substantially to 39.70 suggesting that the visual model recognizes many objects but can \u201cforget\u201d objects learned by the classifier when fine-tuned on the imagecaption data (without the 8 objects). Auxiliary Objective: Finally, incorporating the auxiliary objectives F1 improves remarkably to 47.02 and incorporating all aspects does best (F1 50.51, METEOR 20.69) significantly outperforming DCC.\nTraining data source. To study the effect of different data sources, we also evaluate our model in an out-of-domain setting where classifiers for held out objects are trained with images from ImageNet and the language model is trained on text mined from external corpora. Table 3 reports average scores across the eight held-out objects. We compare our NOC model to results from Hendricks et al. (2016) (DCC), as well as a competitive image captioning model - LRCN (Donahue et al., 2015) trained only on the held-out MSCOCO dataset. In the out-of-domain setting (line 2), for\nthe chosen set of 8 objects, NOC performs slightly better on F1 and a bit lower on METEOR compared to DCC. However, we note that in DCC one needs to explicitly identify a set of \u201cseen\u201d object classes from which weights are transferred to the new unseen objects, whereas our model is directly able to caption novel objects without any post-processing. This transfer mechanism not only makes DCC unwieldy to train (it\u2019s not end-to-end) but also leads to peculiar descriptions. E.g., to describe \u201cracket\u201d, DCC transfers parameters from the word \u201ctennis\u201d. Consequently, given a picture of a man holding a tennis racket, DCC produces the caption \u201cA man is playing racket on the court.\u201d where as our NOC model produces the caption \u201cA man is swinging a racket at a ball\u201d (Fig. 3).\nWith COCO image training (line 3), F1 scores of our NOC model improves considerably even with the Web Corpus LM training. Finally in the in-domain setting (line 4) NOC outperforms\nDCC on F1 by over 10 points while maintaining a similar METEOR score. This suggests that NOC is able to associate the objects with captions better with in-domain training, and the auxiliary objectives and embedding help the model to generalize and describe novel objects."}, {"heading": "5.2 Scaling to ImageNet", "text": "Describing Novel words To demonstrate the scalability of our proposed architecture, we describe objects in ImageNet for which no paired image-sentence data exists. Table 4 compares models on 638 novel object categories using the following metrics: (i) Describing novel objects (%) refers to the percentage of the selected ImageNet objects mentioned in descriptions, i.e. for each novel word (e.g., \u201cotter\u201d) the model should incorporate the word (\u201cotter\u201d) into at least one description about an ImageNet image of the object (otter). While DCC is able to recognize and describe 56.85% (363) of the selected ImageNet objects in descriptions, NOC recognizes several more objects and is capable of describing 91.27% (582 of 638) ImageNet objects. (ii) Accuracy refers to the percentage of images from each category where the model is able to correctly identify and describe the category. We report the average accuracy across all categories. DCC incorporates a new word correctly 11.08% of the time, in comparison, NOC improves this appreciably to 24.74%. (iii) F1 score is computed based on precision and recall of mentioning the object in the description. Again, NOC outperforms with average F1 33.76% to DCC\u2019s 14.47%. In short, NOC is both able to describe more categories, and correctly integrates new vocabulary word into descriptions more frequently.\nFig. 3 (column 3) shows examples where NOC describes a large variety of objects. Fig. 3 (right) outlines some errors. Failing to describe a new object is one common error for NOC. E.g. Fig. 3 (top right), NOC incorrectly describes a man holding a \u201csitar\u201d as a man holding a \u201cbaseball bat\u201d. Other common errors include generating non-grammatical or nonsensical phrases (example with \u201caardvark\u201d) and repeating a specific object (\u201cA barracuda ... with a barracuda\u201d).\nDescribing Rare Words In addition to describing novel words, NOC can also help describe words which are mentioned rarely in paired corpora. We demonstrate this by choosing 52 words which are rarely described in paired text corpora including \"teapot\" (30 annotations), \"swan\" (60 annotations), and \"whisk\" (11 an-\nnotations). When tested on ImageNet images containing these concepts, a model trained only with MSCOCO paired data incorporates rare words into sentences 2.93% of the time with an average F1 score of 4.58%. In contrast, when integrating outside data using our NOC model can incorporate rare words into descriptions 35.15% of the time with an average F1 score of 47.58%."}, {"heading": "6 Conclusion", "text": "We present an end-to-end trainable architecture that incorporates auxiliary training objectives and distributional semantics to generate descriptions for object classes not seen in paired imagecaption data. Our model outperforms previous captioning frameworks in recognizing novel objects while generating descriptions of comparable quality. We demonstrate our model\u2019s captioning capabilities on a held-out set of MSCOCO objects as well as several hundred ImageNet objects. We also present an analysis of the contributions from different network modules, training objectives, and data sources. Additionally, our model directly extends to generate captions for ImageNet objects mentioned rarely in the image-caption corpora."}, {"heading": "Acknowledgements", "text": "Lisa Anne Hendricks is supported by the NDSEG Fellowship. Trevor Darrell was supported in part by DARPA; AFRL; DoD MURI award N000141110688; NSF awards IIS-1212798, IIS1427425, and IIS-1536003, and the Berkeley Artificial Intelligence Research (BAIR) Lab. Raymond Mooney and Kate Saenko are supported\nin part by DARPA under AFRL grant FA875013-2-0026 and a Google Grant. Mooney is also supported by ONR ATL Grant N00014-11-1-010."}], "references": [{"title": "Meteor universal: Language specific translation evaluation for any target", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": null, "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al.2015] Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2016] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Hendricks et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li Fei-Fei"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models. TACL", "author": ["Kiros et al.2015] Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Vicente Ordonez", "Tamara L Berg", "UNC Chapel Hill", "Yejin Choi"], "venue": "TACL", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Elia Bruni", "Marco Baroni"], "venue": null, "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning et al.2014] C. Manning", "M Surdeanu", "J Bauer", "J Finkel", "S J Bethard", "Dx McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao et al.2015a] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "In ICLR", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["Mao et al.2015b] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan L. Yuille"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Midge: Generating image descriptions from computer", "author": ["Jesse Dodge", "Amit Goyal", "Kota Yamaguchi", "Karl Stratos", "Xufeng Han", "Alyssa Mensch", "Alexander C. Berg", "Tamara L. Berg", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Norouzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences. TACL", "author": ["Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Lstm neural networks for language modeling", "author": ["Sundermeyer", "R. Schluter", "H. Ney."], "venue": "INTERSPEECH.", "citeRegEx": "Sundermeyer et al\\.,? 2010", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Corpusguided sentence generation of natural images", "author": ["Yang et al.2011] Yezhou Yang", "Ching Lik Teo", "Hal Daum\u00e9 III", "Yiannis Aloimonos"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Modern visual classifiers (He et al., 2016; Simonyan and Zisserman, 2014) can recognize thousands of object categories, some of which are basic or entry-level (e.", "startOffset": 26, "endOffset": 73}, {"referenceID": 4, "context": "Recent work from Hendricks et al. (2016) shows that, to incorporate the vast knowledge of current visual recognition models without explicit paired captions, models must learn from external sources and learn to compose sentences about visual concepts which are infrequent or non-existent in image-description corpora as in Fig.", "startOffset": 17, "endOffset": 41}, {"referenceID": 4, "context": "Compared to previous work (Hendricks et al., 2016) for the task, our model is end-to-end trainable and provides improved performance.", "startOffset": 26, "endOffset": 50}, {"referenceID": 21, "context": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training.", "startOffset": 69, "endOffset": 136}, {"referenceID": 14, "context": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training.", "startOffset": 69, "endOffset": 136}, {"referenceID": 8, "context": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training.", "startOffset": 69, "endOffset": 136}, {"referenceID": 20, "context": "Deep visual description frameworks first encode an image into a fixed length feature vector and then generate a description by either conditioning text generation on image features (Vinyals et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al.", "startOffset": 181, "endOffset": 253}, {"referenceID": 1, "context": "Deep visual description frameworks first encode an image into a fixed length feature vector and then generate a description by either conditioning text generation on image features (Vinyals et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al.", "startOffset": 181, "endOffset": 253}, {"referenceID": 6, "context": ", 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word.", "startOffset": 119, "endOffset": 178}, {"referenceID": 7, "context": ", 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word.", "startOffset": 119, "endOffset": 178}, {"referenceID": 4, "context": "Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015).", "startOffset": 268, "endOffset": 311}, {"referenceID": 6, "context": "On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word. Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015). Also, in almost all cases, the parameters of the visual pipeline are initialized with weights trained on the ImageNet classification task. On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al., 2015) have also been explored. Our model is similar to the CNN-RNN frameworks in Mao et al. (2015a) and Hendricks et al.", "startOffset": 8, "endOffset": 979}, {"referenceID": 1, "context": ", 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word. Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015). Also, in almost all cases, the parameters of the visual pipeline are initialized with weights trained on the ImageNet classification task. On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al., 2015) have also been explored. Our model is similar to the CNN-RNN frameworks in Mao et al. (2015a) and Hendricks et al. (2016).", "startOffset": 8, "endOffset": 1007}, {"referenceID": 10, "context": "Novel object captioning Recently Mao et al. (2015b) proposed an approach, that extends a model\u2019s capability to describe a small set of novel concepts (e.", "startOffset": 33, "endOffset": 52}, {"referenceID": 4, "context": "On the other hand, Hendricks et al. (2016) introduce a model that can describe many objects already existing in English corpora and object recognition datasets (ImageNet) but not in the caption corpora (e.", "startOffset": 19, "endOffset": 43}, {"referenceID": 4, "context": "On the other hand, Hendricks et al. (2016) introduce a model that can describe many objects already existing in English corpora and object recognition datasets (ImageNet) but not in the caption corpora (e.g. pheasant, otter). Our focus is on the latter case. Hendricks et al. (2016) integrate information from external text and visual sources, and explicitly transfer parameters from objects seen in image-caption data to unseen ImageNet objects to generate descriptions for the new objects.", "startOffset": 19, "endOffset": 283}, {"referenceID": 2, "context": "egories (Frome et al., 2013; Norouzi et al., 2013), as well as retrieval of images with text (Socher et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 15, "context": "egories (Frome et al., 2013; Norouzi et al., 2013), as well as retrieval of images with text (Socher et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 18, "context": ", 2013), as well as retrieval of images with text (Socher et al., 2014; Lazaridou et al., 2014).", "startOffset": 50, "endOffset": 95}, {"referenceID": 9, "context": ", 2013), as well as retrieval of images with text (Socher et al., 2014; Lazaridou et al., 2014).", "startOffset": 50, "endOffset": 95}, {"referenceID": 7, "context": ", wn\u22121 and an image are projected into a joint embedding space before the next word in a caption, wn, is generated (Mao et al., 2015a; Kiros et al., 2015).", "startOffset": 115, "endOffset": 154}, {"referenceID": 19, "context": "We start by first training a simple LSTMbased language model (LM) (Sundermeyer et al., 2010) for sentence generation.", "startOffset": 66, "endOffset": 92}, {"referenceID": 16, "context": "Our LM incorporates dense representations for words from distributional embeddings (GloVe, Pennington et al. (2014)) pre-trained on external text corpora.", "startOffset": 91, "endOffset": 116}, {"referenceID": 13, "context": "These distributional embeddings (Mikolov et al., 2013; Pennington et al., 2014) share the property that words that are semantically similar have", "startOffset": 32, "endOffset": 79}, {"referenceID": 16, "context": "These distributional embeddings (Mikolov et al., 2013; Pennington et al., 2014) share the property that words that are semantically similar have", "startOffset": 32, "endOffset": 79}, {"referenceID": 10, "context": "2 (Manning et al., 2014) was used to extract tokenizations.", "startOffset": 2, "endOffset": 24}, {"referenceID": 16, "context": "the dense word representation in the network, we use GloVe (Pennington et al., 2014) pre-trained on 6B tokens of external corpora including Gigaword and Wikipedia.", "startOffset": 59, "endOffset": 84}, {"referenceID": 4, "context": "To empirically evaluate the ability of NOC to describe new objects we use the training and test set from Hendricks et al. (2016). The dataset consists of a subset of MSCOCO (Lin et al.", "startOffset": 105, "endOffset": 129}, {"referenceID": 4, "context": "Table 1: COCO Compositional Captioning: F1 scores of NOC (our model) and DCC (Hendricks et al., 2016)", "startOffset": 77, "endOffset": 101}, {"referenceID": 4, "context": "We empirically evaluate the ability of our proposed model to describe novel objects by following the experimental setup of (Hendricks et al., 2016).", "startOffset": 123, "endOffset": 147}, {"referenceID": 4, "context": "This compares the F1 score achieved by NOC to the previously existing method, DCC (Hendricks et al., 2016), and demonstrates that NOC is able to integrate new vocabulary words into description better for every object in the held-out dataset except \u201ccouch\u201d and \u201cmicrowave\u201d, outperforming DCC considerably.", "startOffset": 82, "endOffset": 106}, {"referenceID": 1, "context": "(2016) (DCC), as well as a competitive image captioning model - LRCN (Donahue et al., 2015) trained only on the held-out MSCOCO dataset.", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": "We compare our NOC model to results from Hendricks et al. (2016) (DCC), as well as a competitive image captioning model - LRCN (Donahue et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 4, "context": "(Hendricks et al., 2016) on % of novel classes described,", "startOffset": 0, "endOffset": 24}], "year": 2016, "abstractText": "We propose Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Recent captioning models are limited in their ability to scale and describe concepts outside of paired image-text corpora. Our model takes advantage of external sources labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text and combines them to generate descriptions about novel objects. We propose minimizing a joint objective which can learn from diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of imagecaption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in imagecaption training data, as well as many categories that are observed very rarely.", "creator": "LaTeX with hyperref package"}}}