{"id": "1606.07950", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Word sense disambiguation via bipartite representation of complex networks", "abstract": "in recent years, concepts and methods of complex narrative networks have been employed to tackle the word wide sense disambiguation ( ~ wsd ) task by representing words as key nodes, which things are connected if they are semantically similar. despite the increasingly number of detailed studies carried out with applying such models, nowadays most of them use networks just to freely represent equally the related data, while the pattern recognition performed elsewhere on the attribute space is performed using traditional learning techniques. in other words, the structural relationship between words have often not been explicitly used in the pattern recognition process. in addition, only a few investigations have probed the suitability of representations based on bipartite networks letters and graphs ( bigraphs ) for the problem, as many approaches consider all possible links between words. in this context, we assess the relevance of a bipartite network model representing both feature words ( i. e. the words contained characterizing the context ) and target ( ambiguous ) words to solve additional ambiguities in uniquely written texts. here, first we firmly focus on the semantical relationships between these two particular type lists of words, disregarding the relationships between all feature word words. in special, the proposed method not only serves to represent texts as graphs, but \" also clearly constructs a structure on which the discrimination properties of senses is accomplished. our provisional results revealed that the proposed learning algorithm in such bipartite networks provides excellent results mostly when topical features are employed to characterize the context. surprisingly, our tentative method even outperformed the structured support vector machine algorithm in particular cases, with the advantage of being robust even if a small training dataset is available. taken together, the results shown obtained here helped show that the proposed representation / classification method typically might be useful to improve the semantical characterization of written multimedia texts.", "histories": [["v1", "Sat, 25 Jun 2016 19:08:19 GMT  (1176kb,D)", "http://arxiv.org/abs/1606.07950v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["edilson a correa jr", "alneu de", "rade lopes", "diego r amancio"], "accepted": false, "id": "1606.07950"}, "pdf": {"name": "1606.07950.pdf", "metadata": {"source": "CRF", "title": "Word sense disambiguation via bipartite representation of complex networks", "authors": ["Edilson A. Correa", "Diego R. Amancio"], "emails": [], "sections": [{"heading": null, "text": "The word sense disambiguation (WSD) task aims at identifying the meaning of words in a given context for specific words conveying multiple meanings. This task plays a prominent role in a myriad of real world applications, such as machine translation, word processing and information retrieval. Recently, concepts and methods of complex networks have been employed to tackle this task by representing words as nodes, which are connected if they are semantically similar. Despite the increasingly number of studies carried out with such models, most of them use networks just to represent the data, while the pattern recognition performed on the attribute space is performed using traditional learning techniques. In other words, the structural relationship between words have not been explicitly used in the pattern recognition process. In addition, only a few investigations have probed the suitability of representations based on bipartite networks and graphs (bigraphs) for the problem, as many approaches consider all possible links between words. In this context, we assess the relevance of a bipartite network model representing both feature words (i.e. the words characterizing the context) and target (ambiguous) words to solve ambiguities in written texts. Here, we focus on the semantical relationships between these two type of words, disregarding the relationships between feature words. In special, the proposed method not only serves to represent texts as graphs, but also constructs a structure on which the discrimination of senses is accomplished. Our results revealed that the proposed learning algorithm in such bipartite networks provides excellent results mostly when topical features are employed to characterize the context. Surprisingly, our method even outperformed the\nar X\niv :1\n60 6.\n07 95\n0v 1\n[ cs\n.C L\n] 2\n5 Ju\nn 20\n16\nsupport vector machine algorithm in particular cases, with the advantage of being robust even if a small training dataset is available. Taken together, the results obtained here show that the proposed representation/classification method might be useful to improve the semantical characterization of written texts. Keywords: complex networks, bipartite graphs, word sense disambiguation, bipartite networks, pattern recognition, semantic analysis, network science"}, {"heading": "1. Introduction", "text": "The word sense disambiguation (WSD) task has been widely studied in the field of Natural Language Processing (NLP) [1]. This task is defined as the ability to computationally detect which sense is being conveyed in a particular context [2]. Although humans solve ambiguities in an effortlessly manner, this matter remains an open problem in computer science, owing to the complexity associated with the representation of human knowledge in computer-based systems [3]. The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11]. In addition, the resolution of ambiguities plays a pivotal role in the development of the so-called semantic web [12].\nMany approaches devised to solve ambiguities in texts employ machine learning methods to automatically extract the best features in specific contexts [2]. Automatic methods commonly use texts as a source of information, and these texts need to be transformed into a structured format. Popular representations are vectors of features, trees and graphs of relations between words [1]. All such representations attempt to grasp, in a particular way, the semantical features related to the context surrounding ambiguous (target) words. Then, the information extracted from the context is used in the learning process. Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.g. [22, 23]). In addition, most of the current network models emphasise the relationship between all words of the document. As a consequence, a minor relevance has been given Preprint submitted to Elsevier June 28, 2016\nto the relationships between feature and target words. In this paper, we propose a different network representation which does not consider the relationship between all words, as described e.g. in [17, 22]. We rather model texts using a bipartite network representation which focus on the relevant information arising from the relationship between feature and target words. This representation is then used as an underlying structure on which the proposed learning algorithm is applied. As we shall show, the combination of this textual representation and the proposed learning technique may improve the classification process when compared with well-known supervised algorithms hinging on traditional text representations. Remarkably, we have also found that our method retains its discriminative power even when a considerable small amount of training instances is available.\nThe remainder of this paper is organized as follows. Section 2 presents a brief review of basic concepts employed in this paper and related works. Section 3 presents the details of the proposed representation and algorithm to undertake the word sense disambiguation task. In Section 4, we discuss the details of the experiments and the results concerning the accuracy and robustness of the proposed method. Finally, we present some perspectives for further works."}, {"heading": "2. Related works", "text": "The word sense disambiguation task can be defined as follows. Given a document represented as a sequence of words T = {w1, w2, ..., wn}, the objective is to assign appropriate sense(s) to all or some of the words wi \u2208 T . In other words, the objective is to find a mapping A from words to senses, such that A(wi) \u2286 SD(wi), where SD(wi) is the set of senses encoded in a dictionary D for the word wi, and A(wi) is the subset of appropriate senses of wi \u2208 T . One of the most popular approaches to tackle the WSD problem is the use of machine learning, since this task can be seen as a supervised classification problem, where senses represent the classes [2]. The attributes used in the learning methods are usually any informative evidence obtained from the topical context and external knowledge sources. The latter approach is usually not common in practice because the creation of\nknowledge datasets demands a time-consuming effort, since the change in domains requires the recreation of new knowledge bases.\nThe generic WSD task can be distinguished into two types: lexical sample and all-words disambiguation. In the former, a WSD system is required to disambiguate a restricted set of target words. This is mostly done by supervised classifiers [2]. In the all-words scenario, the WSD system is expected to disambiguate all open-class words in a text. This task usually requires a wide-coverage of domains, and for this reason a knowledge-based system is usually employed. In this article, only the lexical sample task is considered.\nThe main step in any supervised WSD system is the representation of the context in which target words occur. The set of features employed typically are chosen to characterize the context in a myriad of forms [2]. The most common types of attributes used for this aim are:\n\u2022 local features : the features of an ambiguous concept are a small number of words\nsurrounding target words. The number of words representing the context is defined in terms of the window size \u03c9. For example, if the context of the target word \u03c4\u03c9 is \u201cp\u22123 p\u22122 p\u22121 \u03c4\u03c9 p+1 p+2 p+3\u201d and \u03c9 = 2, then the words p\u22122, p\u22121, p+1 and p+2 are used as features.\n\u2022 topical features : the features are defined as topics of a text or discourse, usually denoted\nin a bag-of-words representation;\n\u2022 syntatical features : the features are syntactic cues and argument-head relations be-\ntween the target word and other words within the same sentence; and\n\u2022 semantical features : the features of a word are any semantic information available,\nsuch as previously established senses or domain indicators.\nUsing the aforementioned set of features, each word occurrence can be converted to a feature vector, which in turn is used as input in supervised classification algorithms. Typical classifiers employed for this task include decision trees [24], bayesian classifiers [24, 25], neural networks [24] and support vector machines [25, 26].\nAnother approach that has been used to address the WSD problem consists in the use of complex networks and graphs [16]. For instance, the HyperLex algorithm [17] connects words co-occurring in paragraphs to establish similarity relations among words appearing in the same context. The frequency of co-occurrences is considered according to the following weighting scheme:\nwij = 1\u2212max{P (wi, wj), P (wj, wi)} (1)\nwhere P (wi, wj) = fij/fi, fi is the frequency of word i in the document and fij is the frequency of the co-occurrence of the words i and j. Then, this network is used to create a tree-like structure via recognition of central concepts, which represent all possible senses. To perform the classification, the distance of context words to the central concepts in the tree structure is computed to identify the most likely sense.\nUsing a different approach, [15] uses the local topological properties of co-occurrence networks to disambiguate target words. In this case, even though a significant performance has been found for particular target words, the optimal discrimination rate was obtained with traditional local features, suggesting thus that the overall discriminability could be improved upon combining features of distinct nature, as suggested by similar approaches [27, 28, 29].\nDespite the numerous studies devoted to the WSD problem, this task remains an open problem in NLP, and currently it is considered one of the most complex problems in Artificial Intelligence [3]. Our contribution in this paper is the proposition of a new representation that is able to focus the sense discrimination analysis on the relationship between features and target words. Unlike previous studies [15, 17], we disregard the links between features words in our bipartite graph representation. Despite its seemingly simplicity, we show that such representation captures, in a artlessly manner, informative properties of target words and their respective senses."}, {"heading": "3. Overview of the technique", "text": "This section presents the approaches to represent local and topic features of target words in a bipartite heterogeneous network. Here we also present the Inductive Model Based on\nBipartite Heterogeneous Network (IMBHN) algorithm, which is responsible for inducing a classification model from the structure of a bipartite network [30, 31]."}, {"heading": "3.1. Modelling word context as a bipartite heterogeneous network", "text": "Traditionally, the context of ambiguous words is represented in a vector space model, so that each target word is characterized by a vector. In this representation, each dimension of the vector corresponds to a specific feature. Alternatively, we may represent the data using a bipartite heterogeneous network. In this model, while the first layer comprises only feature words, the second only stores target words. As mentioned in Section 2, currently, there exists a wide variety of features to tackle the WSD problem. In this paper, we focused on the analysis of local and topical attributes, as such data are readily available on (or derivable from) any corpus. Note that, in this case, we have not used any knowledge dataset.\nIn the proposed strategy based on topical features, we create a set T of topical words. Then, each distinct becomes a distinct feature. As topical words, we considered the most frequent words of the dataset. The number of topical words, i.e. |T |, is a free parameter. Given T , the bipartite network is created by establishing a link between topical and target words whenever they co-occur in the same document.\nIn the proposed representation based on local features, each feature word surrounding the target word represents an attribute. For each instance of the target word in the text, we select the \u03c9 closest surroundings words to become a feature word (see definition in Section 2). The selected words are then connected to the target words by weighted edges."}, {"heading": "3.2. Algorithm description", "text": "The IMBHN algorithm can be used in the context of any text classification task. If the objective is to classify distinct documents in a given number of classes, the bipartite network can be constructed so that nodes represent both terms and documents. In this general scenario, such representation is used to compute the relevance of specific terms for distinct document classes. In a similar fashion, in this study, we compute the relevance of local/topical features for each target word. Then, this relevance is used to infer word senses.\nThe proposed algorithm for sense identification relies upon a network structure with two distinct layers: (i) a layer representing possible feature words (i.e. local or topical features), and (ii) a layer comprising all occurrences of the target word. The two layers are illustrated in Figure 1. Edges are established across layers so that context words and distinct occurrences of the target word are connected. In addition, in the proposed network representation, a weight relating each feature word to each target word is also established. The main components of the model are:\n\u2022 wdk,ti : the weight of the connection linking the k-th target word and the i-th feature\nword. In the strategy based on topical features, this weight is constant along the execution of the algorithm and, for a given document T , is computed as\nwdk,ti = 1\u2212 \u03b4(dk, ti)/l(T ), (2)\nwhere \u03b4(dk, ti) denotes the the distance between two words (i.e. the number of intermediary words) and l(T ) is the length of T (measured in terms of word counts). In the strategy based on local features, the weight of the links is given by the term frequency - inverse document (tf-idf) strategy [1].\n\u2022 fti,cj : let C be the set of possible classes (i.e. word senses). fti,cj represents the current\nrelevance of the i-th feature word (ti \u2208 T ) to the j-th class (cj \u2208 C). This value is initialized using a heuristic and then is updated at each step of the algorithm.\n\u2022 ydk,cj : represents the actual membership of the k-th target word. In other words, this\nis the label provided in the supervised classification scheme. If cj is the class of the k-th target word, then ydk,cj = 1; otherwise, ydk,cj = 0.\n\u2022 \u03c6dk,cj : represents the obtained membership of the k-th target word. If cj is the class\nobtained for the k-th target word, then \u03c6dk,cj = 1; otherwise, \u03c6dk,cj = 0.\n\u2022 dk,cj : denotes the error of the current iteration. It is computed as:\ndk,cj = ydk,cj \u2212 \u03c6dk,cj . (3)\nAs we shall show, this error is used to update weights in f so that, at each new iteration, the distance between ydk,cj and \u03c6dk,cj decreases.\nNote that, in the model illustrated in Figure 1, we only consider the relationship between feature and target words. Differently from traditional models, the relationship between feature words [15] is not explicitly considered in our model.\nThe training phase of the algorithm can be divided into the three following major steps:\n1. Initialization: there are three possible ways of initializing f , i.e. the vector weights of\nfeature words. The most simple strategy is to initialize weights with zeros or random values. A more informed alternative initializes weights using the a priori likelihood of feature words co-occur with senses. This probability can be computed as\nPr = P (fi|dk) = nfi,dk/ndk , (4)\nwhere nfi,dk is the number of times that the i-th feature word appears in the context of the k-th target word and ndk is the total number of occurrences of dk. In our experiments, we report the best results obtained among these three alternatives.\n2. Error calculation: In the error calculation step, firstly, the output vector for each\ntarget word (\u03c6(dk)) is computed. This vector depends upon the presence of the feature word in the context (wdk,ti) and its relevance for the class (fti,cj). Mathematically, the class computed at each new iteration is given by\nC (\u2211 ti\u2208T wdk,tifti,cj ) =  1, if cj = arg max cl\u2208C ( \u2211 ti\u2208T wdk,tifti,cl ) .\n0, otherwise.\n(5)\nAfter updating the classes for each target word, the values of fti,cj are modified. This update is controlled by the correction rate \u03b7:\nf (n+1) ti,cj = f (n) ti,cj + \u03b7 \u2211 dk\u2208D wdk,ti (n) dk,cj , (6)\nwhere the superscipt (n) in f and denotes the value of these quantities computed in the n-th iteration of the algorithm and D is the set of target words. Note that (n)dk,cj is computed as defined in equation 3. The values of dk,cj and fti,cj in equations 3 and 6, respectively, are updated until a stop criterion is reached. In our experiments, we have stopped the algorithm when a minimum error min = 0.01 is obtained. If the minimum error is not reached after nmax = 1, 000 iterations, the algorithm is stopped.\n3. Classification: in the classification phase, the induced values of f are used in the clas-\nsification. The word senses for each ambiguous word of the dataset are then obtained by computing the following linear combination:\nclass(dk) = arg max cj\u2208C (\u2211 ti\u2208T wdk,tifti,cj ) . (7)"}, {"heading": "4. Experimental Evaluation", "text": "This section presents the corpus used in the experiments. In addition, we also detail the experimental configuration of parameters. Finally, we present a robustness analysis to investigate how the performance of the IMBHN varies with the size of the training set."}, {"heading": "4.1. Corpus", "text": "In order to evaluate the proposed algorithm, the SENSEVAL-2 [32] corpus was used. This corpus comprises documents from distinct sources, including the British National Corpus and the Penntreebank portion of the Wall Street Journal. The SENSEVAL-2 corpus encompasses 15, 225 instances of short texts representing the context surrounding ambiguous words. Each word is tagged with its part-of-speech, and the manually annotated senses of four target words is provided. The number of senses and the number of instances of each word used in our experiments is shown in Table 1. In the evaluation process, these four words were considered as the target words. In particular, to characterized the contexts, we have removed stopwords and punctuation marks as such elements do not convey any semantical meaning and, therefore, do not improve the characterization of contexts."}, {"heading": "4.2. Experiment Configuration", "text": "The results obtained by the IMBHN algorithm were compared with four inductive classification algorithms: Naive Bayes (NB) [33], J48 (C4.5 algorithm) [34], IBk (k -Nearest Neighbors) [35] and Support Vector Machine via sequential minimal optimization (SMO) [36]. The parameters of these algorithms have been chosen using the methodology described in [37]. For the IMBHN algorithm, we used the error correction rates \u03b7 = {0.01, 0.05, 0.10, 0.50}. The number of topical features used in the experiments were |T | = {100, 200, 300}. Finally, the window size for the local features were \u03c9 = {1, 2}. The evaluation process was performed via 10-fold cross-validation [38]."}, {"heading": "4.3. Results and discussion", "text": "To analyze the behavior and accuracy of the proposed algorithm, we first studied the WSD task using topical features to characterize the context of target words of our dataset. The obtained results are shown in Table 2. When the number of topical features |T | is set with |T | = 100, the best results occurred for the SMO and J48 techniques. In three cases, the proposed algorithm IMBHN performed worse than the best results achieved with competing techniques.\nIn general, the performance of the classifiers tend to improve when the number of topical features (|T |) increases from 100 to 300. This is clear when one observes that e.g. the best accuracy rate for the word \u201cinterest\u201d goes from 79.77% to 84.71%. The same behavior can be observed for the other target words of the dataset, however, in a minor proportion. Concerning the performance of the proposed technique when |T | = {200, 300}, in most cases, the IMBHN method is outperformed by the SMO technique, which provided the best results for the words \u201cinterest\u201d, \u201cline\u201d and \u201cserve\u201d. The best results for the word \u201chard\u201d was achieved with the J48 classifier.\nWhen analyzing the performance of the classifiers induced with local features, a different pattern of accuracy has been found, as shown in Table 3. For the words \u201cinterest\u201d, \u201cline\u201dand \u201cserve\u201d the IMBHN classifier yielded the best results, for \u03c9 = {1, 2, 3}. Conversely, if we consider the word \u201chard\u201d, the decision tree based algorithm, J48, outperformed all other\nmethods. However, the performance achieved with J48 was very similar to the one obtained with the IMBHN: the maximum difference of accuracy between these two classifiers was 1.09%, when \u03c9 = 3. This observation confirms the suitability of the proposed method for the problem, as optimized results have been found for virtually all words of the dataset.\nThe best results obtained with topical and local features are summarized in Table 4. The proposed algorithm for representing texts and discriminating senses outperformed other methods when considering also distinct types of features. In special, the IMBHN performed significantly better than the SMO method for the word \u201cline\u201d and \u201cserve\u201d. A minor gain in performance has been observed for \u201cinterest\u201d. With regard to the word \u201chard\u201d, the best performance was obtained with the J48 (with topical features). However, a similar accuracy was obtained with the IMBHN (with local features, as shown in Table 3). All in all, these results show, as a proof of principle, that the proposed algorithm may be useful to the word sense disambiguation problem, as optimal or near-optimal performance has been found in the studied corpus.\nA disadvantage associated to the use of supervised methods to undertake the word sense disambiguation problem is the painstaking, time-consuming effort required to build reliable datasets [2]. For this reason, it becomes relevant to analyze the performance of WSD systems when only a few labelled instances are available for training [2]. In this sense, we performed a robustness analysis of the proposed algorithm to investigate how performance is affected when smaller fractions of the dataset are provided for the algorithm. To perform such a robustness analysis the following procedure was adopted. We defined a sampling rate S, representing the percentage of disregarded instances from the original dataset. For each sampling rate, we computed the accuracy \u0393(S) relative to the sampled dataset. The relative accuracy rate for a given S was computed as\n\u0393\u0303(S) = \u0393(S)\n\u0393(0) , (8)\nwhich quantities the percentage of the original accuracy which is preserved when the original dataset is sampled with sampling rate S. For each sampling rate, we generated 50 sampled subsets. The obtained results for the IMBHN in its best configuration (i.e. using local\nfeatures and \u03c9 = 3) are shown in Figure 2. The best scenario occurs for the word \u201chard\u201d, as even when 90% of the original is ignored, in average, more than 95% of the original accuracy (i.e. \u0393(S = 0)) is recovered. Concerning the other words, a good performance was also observed when only a small fraction was available. This is the case of \u201cserve\u201d: when 90% of the dataset is disregarded, 85% of the original accuracy is kept. These results suggest that the IMBHN could be successfully applied in much smaller datasets without a significative loss in performance. We have found similar robustness results for other configurations of parameters (\u03c9) of the IMBHN (results not shown), which reinforces the hypothesis that the resiliency of the method with regard to the total amount of instances in the training phase is stable with varying parameter values. Note that such a robustness, although strongly desired in practical problems, does not naturally arise in all pattern recognition methods. This is evident e.g. when the robustness SMO is verified for \u201cserve\u201d and \u201cinterest\u201d, as shown in Figure 3. Note that when S = 0.9, the accuracy drops to about 60% of its original value."}, {"heading": "5. Conclusion", "text": "The accurate discrimination of word senses plays a pivotal role in information extraction and document classification tasks. While methods based on deep paradigms may perform well in very specific domains, statistical methods based mainly on machine learning have proved useful to undertake the word sense disambiguation task in more general contexts. In this article, we have devised a statistical model to both represent contexts and recognize patterns in written texts. The model hinges on a bipartite network, with layers representing\nfeature words and target words, i.e. words conveying two or more potential senses. We have shown, as a proof of principle, that the proposed model presents a significant performance, mainly when contextual features are modelled via extraction of local words to represent semantical contexts. We have also observed that, in general, our method performs well even if a relatively small amount of data is available for the training process. This is an important property as it may significantly reduce both time and effort required to construct a corpus of labelled data.\nAs future works, we intend to explore further generalizations of the algorithm. Owing to the power of word adjacency networks in extracting relevant semantical features of texts [15], we intend to use such models to improve the characterization of the studied bipartite networks. The word adjacency model could be used, for example, to better represent the relationship between feature and target words by using network similarity measurements [39, 40, 41]. We also intend to extend the present model to consider topological and dynamical measurements of word adjacency networks as local features [15]."}, {"heading": "Acknowledgements", "text": "E.A.C. Jr. and D.R.A. acknowledge financial support from Google (Google Research Awards in Latin America grant). D.R.A. thanks Sa\u0303o Paulo Research Foundation (FAPESP) for support (grant. no. 2014/20830-0). A.A.L. acknowledges support from FAPESP (grant no. 2011/22749-8 and 2015/14228-9) and CNPq (Brazil) (grant no. 302645/2015-2)."}], "references": [{"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Word sense disambiguation: A survey", "author": ["R. Navigli"], "venue": "ACM Computing Surveys (CSUR) 41 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Translation", "author": ["W. Weaver"], "venue": "Machine translation of languages 14 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1955}, {"title": "Commercial applications of natural language processing", "author": ["K.W. Church", "L.F. Rau"], "venue": "Communications of the ACM 38 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Semeval-2007 task 08: Metonymy resolution at semeval-2007", "author": ["K. Markert", "M. Nissim"], "venue": "in: Proceedings of the 4th International Workshop on Semantic Evaluations, Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Understanding the role of conceptual relations in word sense disambiguation", "author": ["D. Fernandez-Amoros", "R. Heradio"], "venue": "Expert Systems with Applications 38 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering filter keywords for company name disambiguation in twitter", "author": ["D. Spina", "J. Gonzalo", "E. Amig\u00f3"], "venue": "Expert Systems with Applications 40 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Identity rank: Named entity disambiguation in the news domain", "author": ["N. Fernandez", "J.A. Fisteus", "L. Sanchez", "G. Lopez"], "venue": "Expert Systems with Applications 39 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "O", "author": ["T. Berners-Lee", "J. Hendler"], "venue": "Lassila, et al., The semantic web, Scientific american 284 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Particle competition and cooperation in networks for semi-supervised learning", "author": ["F. Breve", "L. Zhao", "M. Quiles", "W. Pedrycz", "J. Liu"], "venue": "IEEE Transactions on Knowledge and Data Engineering 24 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Chaotic encryption method based on life-like cellular automata", "author": ["J. Machicao", "A.G. Marco", "O.M. Bruno"], "venue": "Expert Systems with Applications 39 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["D.R. Amancio", "O.N. Oliveira Jr"], "venue": "d. F. Costa, Unveiling the relationship between complex networks metrics and word senses, EPL (Europhysics Letters) 98 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Graph-based natural language processing and information retrieval", "author": ["R. Mihalcea", "D. Radev"], "venue": "Cambridge University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Hyperlex: lexical cartography for information retrieval", "author": ["J. V\u00e9ronis"], "venue": "Computer Speech & Language 18 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Jr", "author": ["D.R. Amancio", "E.G. Altmann", "D. Rybski", "O.N. Oliveira"], "venue": "L. d. F. Costa, Probing the statistical properties of unknown texts: application to the voynich manuscript, PLoS ONE 8 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Authorship attribution through function word adjacency networks", "author": ["S. Segarra", "M. Eisen", "A. Ribeiro"], "venue": "IEEE Transactions on Signal Processing 63 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "The complexity of chinese syntactic dependency networks", "author": ["H. Liu"], "venue": "Physica A 387 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Network properties of written human language", "author": ["A.P. Masucci", "G.J. Rodgers"], "venue": "Phys. Rev. E 74 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Word sense disambiguation via high order of learning in complex networks", "author": ["T.C. Silva", "D.R. Amancio"], "venue": "EPL (Europhysics Letters) 98 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Discriminating word senses with tourist walks in complex networks", "author": ["T.C. Silva", "D.R. Amancio"], "venue": "Eur. Phys. J. B 86 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning", "author": ["R.J. Mooney"], "venue": "arXiv preprint cmp-lg/9612001 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "J", "author": ["G. Escudero", "L. M\u00e0rquez", "G. Rigau"], "venue": "G. Salgado, On the portability and tuning of supervised word sense disambiguation systems ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Analyzing natural human language from the point of view of dynamic of a complex network", "author": ["G.A. Wachs-Lopes", "P.S. Rodrigues"], "venue": "Expert Systems with Applications 45 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "A complex network approach to stylometry", "author": ["D.R. Amancio"], "venue": "PLoS ONE 10 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "O", "author": ["D.R. Amancio", "S.M. Aluisio"], "venue": "N. Oliveira Jr., L. da F. Costa, Complex networks analysis of language complexity, EPL (Europhysics Letters) 100 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "A simple model for self-organization of bipartite networks", "author": ["K. Sneppen", "M. Rosvall", "A. Trusina", "P. Minnhagen"], "venue": "EPL (Europhysics Letters) 67 ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "A", "author": ["R.G. Rossi"], "venue": "de Andrade Lopes, T. de Paulo Faleiros, S. O. Rezende, Inductive model generation for text classification using a bipartite heterogeneous network, Journal of Computer Science and Technology 29 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": "in: Proceedings of the 23rd International Conference on Machine Learning, ICML \u201906, ACM, New York, NY, USA", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Programs for Machine Learning", "author": ["R. Quinlan"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1993}, {"title": "Instance-based learning algorithms", "author": ["D.W. Aha", "D. Kibler", "M.K. Albert"], "venue": "Mach. Learn. 6 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1991}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J. Platt"], "venue": "in: B. Schoelkopf, C. Burges, A. Smola (Eds.), Advances in Kernel Methods - Support Vector Learning, MIT Press", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "L", "author": ["D.R. Amancio", "C.H. Comin", "D. Casanova", "G. Travieso", "O.M. Bruno", "F.A. Rodrigues"], "venue": "d. F. Costa, A systematic comparison of supervised classifiers, PLoS ONE 9 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["R. Kohavi"], "venue": "in: Proceedings of the 14th International Joint Conference on Artificial Intelligence, IJCAI\u201995, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1995}, {"title": "Stability of similarity measurements for bipartite networks", "author": ["J.-G. Liu", "L. Hou", "X. Pan", "Q. Guo", "T. Zhou"], "venue": "Scientific Reports 6 ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Complex systems: features", "author": ["C.H. Comin", "T.K.D. Peron", "F.N. Silva", "D.R. Amancio", "F.A. Rodrigues", "L.F. Costa"], "venue": "similarity and connectivity, arXiv: 1606.05400 ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Vertex similarity in networks", "author": ["E.A. Leicht", "P. Holme", "M.E.J. Newman"], "venue": "Phys. Rev. E 73 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "The word sense disambiguation (WSD) task has been widely studied in the field of Natural Language Processing (NLP) [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "This task is defined as the ability to computationally detect which sense is being conveyed in a particular context [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 2, "context": "The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11].", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11].", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11].", "startOffset": 193, "endOffset": 213}, {"referenceID": 5, "context": "The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11].", "startOffset": 193, "endOffset": 213}, {"referenceID": 6, "context": "The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11].", "startOffset": 193, "endOffset": 213}, {"referenceID": 7, "context": "The importance of the WSD task stems from its essential role in a variety of real world applications, such as machine translation [4], word processing [5], information retrieval and extraction [6, 7, 8, 9, 10, 11].", "startOffset": 193, "endOffset": 213}, {"referenceID": 8, "context": "In addition, the resolution of ambiguities plays a pivotal role in the development of the so-called semantic web [12].", "startOffset": 113, "endOffset": 117}, {"referenceID": 1, "context": "Many approaches devised to solve ambiguities in texts employ machine learning methods to automatically extract the best features in specific contexts [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "Popular representations are vectors of features, trees and graphs of relations between words [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 74, "endOffset": 82}, {"referenceID": 10, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 74, "endOffset": 82}, {"referenceID": 11, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 12, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 13, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 14, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 15, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 16, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 17, "context": "Although graphs have been employed in general pattern recognition methods [13, 14] and, particularly in the analysis of the semantical properties of texts in several ways [15, 16, 17, 18, 19, 20, 21], the use of network models in the learning process has been restricted to a few works (see e.", "startOffset": 171, "endOffset": 199}, {"referenceID": 18, "context": "[22, 23]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[22, 23]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "in [17, 22].", "startOffset": 3, "endOffset": 11}, {"referenceID": 18, "context": "in [17, 22].", "startOffset": 3, "endOffset": 11}, {"referenceID": 1, "context": "One of the most popular approaches to tackle the WSD problem is the use of machine learning, since this task can be seen as a supervised classification problem, where senses represent the classes [2].", "startOffset": 196, "endOffset": 199}, {"referenceID": 1, "context": "This is mostly done by supervised classifiers [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "The set of features employed typically are chosen to characterize the context in a myriad of forms [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 20, "context": "Typical classifiers employed for this task include decision trees [24], bayesian classifiers [24, 25], neural networks [24] and support vector machines [25, 26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "Typical classifiers employed for this task include decision trees [24], bayesian classifiers [24, 25], neural networks [24] and support vector machines [25, 26].", "startOffset": 93, "endOffset": 101}, {"referenceID": 21, "context": "Typical classifiers employed for this task include decision trees [24], bayesian classifiers [24, 25], neural networks [24] and support vector machines [25, 26].", "startOffset": 93, "endOffset": 101}, {"referenceID": 20, "context": "Typical classifiers employed for this task include decision trees [24], bayesian classifiers [24, 25], neural networks [24] and support vector machines [25, 26].", "startOffset": 119, "endOffset": 123}, {"referenceID": 21, "context": "Typical classifiers employed for this task include decision trees [24], bayesian classifiers [24, 25], neural networks [24] and support vector machines [25, 26].", "startOffset": 152, "endOffset": 160}, {"referenceID": 12, "context": "Another approach that has been used to address the WSD problem consists in the use of complex networks and graphs [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "For instance, the HyperLex algorithm [17] connects words co-occurring in paragraphs to establish similarity relations among words appearing in the same context.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Using a different approach, [15] uses the local topological properties of co-occurrence networks to disambiguate target words.", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "In this case, even though a significant performance has been found for particular target words, the optimal discrimination rate was obtained with traditional local features, suggesting thus that the overall discriminability could be improved upon combining features of distinct nature, as suggested by similar approaches [27, 28, 29].", "startOffset": 321, "endOffset": 333}, {"referenceID": 23, "context": "In this case, even though a significant performance has been found for particular target words, the optimal discrimination rate was obtained with traditional local features, suggesting thus that the overall discriminability could be improved upon combining features of distinct nature, as suggested by similar approaches [27, 28, 29].", "startOffset": 321, "endOffset": 333}, {"referenceID": 24, "context": "In this case, even though a significant performance has been found for particular target words, the optimal discrimination rate was obtained with traditional local features, suggesting thus that the overall discriminability could be improved upon combining features of distinct nature, as suggested by similar approaches [27, 28, 29].", "startOffset": 321, "endOffset": 333}, {"referenceID": 11, "context": "Unlike previous studies [15, 17], we disregard the links between features words in our bipartite graph representation.", "startOffset": 24, "endOffset": 32}, {"referenceID": 13, "context": "Unlike previous studies [15, 17], we disregard the links between features words in our bipartite graph representation.", "startOffset": 24, "endOffset": 32}, {"referenceID": 25, "context": "Bipartite Heterogeneous Network (IMBHN) algorithm, which is responsible for inducing a classification model from the structure of a bipartite network [30, 31].", "startOffset": 150, "endOffset": 158}, {"referenceID": 26, "context": "Bipartite Heterogeneous Network (IMBHN) algorithm, which is responsible for inducing a classification model from the structure of a bipartite network [30, 31].", "startOffset": 150, "endOffset": 158}, {"referenceID": 0, "context": "In the strategy based on local features, the weight of the links is given by the term frequency - inverse document (tf-idf) strategy [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "feature words [15] is not explicitly considered in our model.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "The results obtained by the IMBHN algorithm were compared with four inductive classification algorithms: Naive Bayes (NB) [33], J48 (C4.", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "5 algorithm) [34], IBk (k -Nearest Neighbors) [35] and Support Vector Machine via sequential minimal optimization (SMO) [36].", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "5 algorithm) [34], IBk (k -Nearest Neighbors) [35] and Support Vector Machine via sequential minimal optimization (SMO) [36].", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "5 algorithm) [34], IBk (k -Nearest Neighbors) [35] and Support Vector Machine via sequential minimal optimization (SMO) [36].", "startOffset": 120, "endOffset": 124}, {"referenceID": 31, "context": "The parameters of these algorithms have been chosen using the methodology described in [37].", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "The evaluation process was performed via 10-fold cross-validation [38].", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "A disadvantage associated to the use of supervised methods to undertake the word sense disambiguation problem is the painstaking, time-consuming effort required to build reliable datasets [2].", "startOffset": 188, "endOffset": 191}, {"referenceID": 1, "context": "For this reason, it becomes relevant to analyze the performance of WSD systems when only a few labelled instances are available for training [2].", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "texts [15], we intend to use such models to improve the characterization of the studied bipartite networks.", "startOffset": 6, "endOffset": 10}, {"referenceID": 33, "context": "The word adjacency model could be used, for example, to better represent the relationship between feature and target words by using network similarity measurements [39, 40, 41].", "startOffset": 164, "endOffset": 176}, {"referenceID": 34, "context": "The word adjacency model could be used, for example, to better represent the relationship between feature and target words by using network similarity measurements [39, 40, 41].", "startOffset": 164, "endOffset": 176}, {"referenceID": 35, "context": "The word adjacency model could be used, for example, to better represent the relationship between feature and target words by using network similarity measurements [39, 40, 41].", "startOffset": 164, "endOffset": 176}, {"referenceID": 11, "context": "We also intend to extend the present model to consider topological and dynamical measurements of word adjacency networks as local features [15].", "startOffset": 139, "endOffset": 143}], "year": 2016, "abstractText": "The word sense disambiguation (WSD) task aims at identifying the meaning of words in a given context for specific words conveying multiple meanings. This task plays a prominent role in a myriad of real world applications, such as machine translation, word processing and information retrieval. Recently, concepts and methods of complex networks have been employed to tackle this task by representing words as nodes, which are connected if they are semantically similar. Despite the increasingly number of studies carried out with such models, most of them use networks just to represent the data, while the pattern recognition performed on the attribute space is performed using traditional learning techniques. In other words, the structural relationship between words have not been explicitly used in the pattern recognition process. In addition, only a few investigations have probed the suitability of representations based on bipartite networks and graphs (bigraphs) for the problem, as many approaches consider all possible links between words. In this context, we assess the relevance of a bipartite network model representing both feature words (i.e. the words characterizing the context) and target (ambiguous) words to solve ambiguities in written texts. Here, we focus on the semantical relationships between these two type of words, disregarding the relationships between feature words. In special, the proposed method not only serves to represent texts as graphs, but also constructs a structure on which the discrimination of senses is accomplished. Our results revealed that the proposed learning algorithm in such bipartite networks provides excellent results mostly when topical features are employed to characterize the context. Surprisingly, our method even outperformed the 1 ar X iv :1 60 6. 07 95 0v 1 [ cs .C L ] 2 5 Ju n 20 16 support vector machine algorithm in particular cases, with the advantage of being robust even if a small training dataset is available. Taken together, the results obtained here show that the proposed representation/classification method might be useful to improve the semantical characterization of written texts.", "creator": "LaTeX with hyperref package"}}}