{"id": "1606.08089", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "This before That: Causal Precedence in the Biomedical Domain", "abstract": "causal precedence similarity between biochemical rna interactions analyses is crucial in utilizing the biomedical domain, because it transforms spatial collections of individual interactions, e. g., bindings and phosphorylations, into the causal mechanisms needed to first inform meaningful search and inference. here, because we analyze causal precedence in the biomedical domain as distinct phenomena from open - domain, increasing temporal precedence. first, we describe a novel, hand - annotated text corpus of causal precedence in the biomedical domain. second, technique we already use extends this corpus to investigate a battery base of models of precedence, covering rule - based, feature - based, and latent representation models. the highest - performing individual model achieved a maximal micro f1 of 43 points, approaching eliminating the best performers on the simpler existing temporal - only canonical precedence tasks. feature - based and latent representation models each outperform the rule - based models, but their performance is precisely complementary to one another. we apply a sieve - based architecture to capitalize on virtually this lack of overlap, achieving a linear micro f1 score of 46 points.", "histories": [["v1", "Sun, 26 Jun 2016 21:55:40 GMT  (473kb,D)", "http://arxiv.org/abs/1606.08089v1", "To appear in the proceedings of the 2016 Workshop on Biomedical Natural Language Processing (BioNLP 2016)"]], "COMMENTS": "To appear in the proceedings of the 2016 Workshop on Biomedical Natural Language Processing (BioNLP 2016)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["gus hahn-powell", "dane bell", "marco a valenzuela-esc\\'arcega", "mihai surdeanu"], "accepted": false, "id": "1606.08089"}, "pdf": {"name": "1606.08089.pdf", "metadata": {"source": "CRF", "title": "This before That: Causal Precedence in the Biomedical Domain", "authors": ["Gus Hahn-Powell", "Dane Bell", "Marco A. Valenzuela-Esc\u00e1rcega", "Mihai Surdeanu"], "emails": ["hahnpowell@email.arizona.edu"], "sections": [{"heading": "1 Introduction", "text": "In the biomedical domain, an enormous amount of information about protein, gene, and drug interactions appears in the form of natural language across millions of academic papers. There is a tremendous ongoing effort (Ne\u0301dellec et al., 2013; Kim et al., 2012; Kim et al., 2009) to extract individual chemical interactions from these texts, but these interactions are only isolated fragments of larger causal mechanisms such as protein signaling pathways. Nowhere, however, including any\ndatabase, is the complete mechanism described in a form that lends itself to causal search or inference. The absence of such a database is not for lack of trying; Pathway Commons (Cerami et al., 2011) aims to address the need, but its authors estimate it currently covers 1% of the literature due to the high cost of annotation1. This issue only grows more pressing with the yearly growth in biomedical publishing, which presents an otherwise insurmountable challenge for biomedical researchers to query and interpret.\nThe Big Mechanism program (Cohen, 2015) aims to construct exactly such large-scale mechanistic information by reading and assembling protein signaling pathways that are relevant for cancer, and exploit them to generate novel explanatory and treatment hypotheses. Although prior work (Chambers et al., 2014; Mirza, 2016) has addressed the challenging area of temporal precedence in the open domain, the biomedical domain presents very different data and, consequently, requires novel techniques. Precedence in mechanistic biology is causal rather than temporal. Though event temporality is crucial to understanding electronic health records for individual patients (Bethard et al., 2015; Bethard et al., 2016), its contribution to the understanding of biomolecular reactions is less clear as these events and processes may repeat in extremely short cycles, continue without end, or overlap in time. At any level of abstraction, causal precedence encodes mechanistic information and facilitates inference over spotty evidence. For the purpose of this work, precedence is defined for two events, A and B, as\nA precedes B if and only if the output of A is necessary for the successful execution of B.2\n1Personal communication. 2See the \u201cprecedes\u201d examples in Table 1.\nar X\niv :1\n60 6.\n08 08\n9v 1\n[ cs\n.C L\n] 2\n6 Ju\nn 20\n16\nVery little annotated data exists for causal precedence, especially efforts focusing on signaling pathways. BioCause (Miha\u0306ila\u0306 et al., 2013), for instance, is centered on connections between claims and evidence and contains only 51 annotated examples of causal precedence3. Our work4 offers three contributions in aid of automatically extracting causal ordering in biomedical text. First, we provide and describe a dataset of real text examples, manually annotated for causal precedence. Second, we analyze the efficacy of a battery of different models in automatically determining precedence, built on top of the Reach automatic reading system (Valenzuela-Esca\u0301rcega et al., 2015a; Valenzuela-Esca\u0301rcega et al., 2015c) and measured against this novel corpus. In particular, we investigate three classes of models: (a) deterministic rule-based models inspired by the precedence sieves proposed by Chambers et al. (2014), (b) feature-based models, and (c) models that rely on latent representations such as long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). Our analysis indicates that while independently the top-performing model achieves a micro F1 of 43, these models are largely complementary with a combined recall of 58 points. Lastly, we conduct an error analysis of these models to motivate and inform future research."}, {"heading": "2 A Corpus for Causal Precedence in the Biomedical Domain", "text": "Our corpus annotates several types of relations between mentions of biochemical interactions. Following common terminology promoted by the BioNLP shared tasks, we will interchangeably use \u201cevents\u201d to refer to these interactions. To generate candidate events for our planned annotations, we ran the Reach event extraction system (Valenzuela-Esca\u0301rcega et al., 2015a; Valenzuela-Esca\u0301rcega et al., 2015c) over the full text5 of 500 biomedical papers taken from the\n3These are marked in the BioCause corpus as Causality events with Cause and Effect arguments. The remaining 800 annotations are claim-evidence relations.\n4The corpus, tools, and system introduced in this work are publicly available at https://github.com/ myedibleenso/this-before-that\n5We chose to ignore the \u201creferences\u201d, \u201cmaterials\u201d, and \u201cmethods\u201d sections, which generally do not contain mechanistic information.\nOpen Access subset of PubMed6. The events extracted by Reach are biochemical events of two types: simple events such as phosphorylation that modify one or more entities (typically proteins), and nested events (regulations) that have other events as arguments.\nTo improve the likelihood of finding pairs of events with a relevant link, we filtered event pairs by imposing the following requirements for inclusion in the corpus:\n1. Event pairs must share at least one participant. This constraint is based on the observation that interactions that share participants are more likely to be connected.\n2. Event pairs must be within 1 sentence of each other. Similarly, discourse proximity increases the likelihood of two events being related.\n3. Event pairs must not share the same type. This helps to maximize the diversity of the dataset.\n4. Event pairs must not already be contained in an extracted Regulation event. For example, we did not annotate the relation between the binding and the phosphorylation events in \u201cThe binding of X and Y is inhibited\n6http://www.ncbi.nlm.nih.gov/pmc/ tools/openftlist/\nby X phosphorylation\u201d, because it is already captured by most state-of-the-art biomedical event extraction systems.\nAfter applying these constraints, only 1700 event pairs remained. In order to rapidly annotate the event pairs, we developed a browser-based annotation UI that is completely client-side (see Figure 3). Using this tool, we annotated 1000 event pairs for this work; 84 of these were discarded due to severe extraction errors. The annotations include the event spans, event triggers (i.e., the verbal or nominal predicates that indicate the type of interaction such as \u201cbinding\u201d or \u201cphosphorylated\u201d), source document, minimal sentential span encompassing both event mentions, and whether or not the event pair involves coreference for either the event trigger or the event participants. For events requiring coreference resolution, we expanded the encompassing span of text to also capture the antecedent. Note that domain-specific coreference resolution is a component of the event extraction system used here (Bell et al., 2016).\nWhen describing the relations between these event pairs, we refer to the event that occurs first in text as Event 1 (E1) and the event that follows as Event 2 (E2). Each (E1, E2) pair was assigned one of seven labels: \u201cE1 precedes E2\u201d, \u201cE2 precedes E1\u201d, \u201cEquivalent\u201d, \u201cE1 specifies E2\u201d, \u201cE2 specifies E1\u201d, \u201cOther\u201d, or \u201cNone\u201d. Table 1 provides examples for each of these labels. We converged on these labels because they are fundamental to the assembly of causal mechanisms from a collection of events. Collectively, the seven labels address three important assembly tasks: equivalence, i.e., understanding that two event mentions discuss the same event, subsumption, i.e., the two mentions discuss the same event, but one is more specific than the other, and, most importantly, causal precedence, the identification of which is the focus of this work. During the annotation process, we came across examples of other relevant phenomena. We grouped these instances under the label \u201cOther\u201d and leave their analysis for future work.\nThough simplified, the examples in Table 3 illustrate that this is a complex task sensitive to linguistic evidence. For example, the direction of the precedence relations in the first two rows in the table changes based on a single word in the context (\u201cprior\u201d vs. \u201cfollowing\u201d).\nIn terms of the distribution of relations, causal\nprecedence pairs appear more frequently within the same sentence, while cases of the subsumption (\u201cspecifies\u201d) and equivalence relations are far more common across sentences (see Figure 1). Coreference is involved in 10\u201315% of the instances for each relation label (see Figure 2).\nThe annotation process was performed by two linguists familiar with the biomedical domain. To minimize errors, the annotation task was initially performed together at the same workstation.7 On a randomly selected sample of 100 event pairs, the two annotators had a Cohen\u2019s kappa score (Cohen, 1960) of 0.82, indicating \u201calmost perfect\u201d agreement for the precedes labels (Landis and Koch, 1977)."}, {"heading": "3 Models of Causal Precedence", "text": "We have developed both deterministic, interpretable models and automatic, machine-learning models for detecting causal precedence in our dataset. Importantly, the models covered in this work focus solely on causal precedence, which is the most complex relation annotated in the dataset previously introduced. Thus, for all experiments discussed here, we reduce these annotations to three labels: \u201cE1 precedes E2\u201d, \u201cE2 precedes E1\u201d, and Nil, which covers all the other labels in the\n7Similar to pair programming.\ncorpus."}, {"heading": "3.1 Deterministic Models", "text": "The deterministic models are defined by a small number of hand-written rules using the Odin event extraction framework (Valenzuela-Esca\u0301rcega et al., 2015b). The number of rules for each model is shown in Table 2, and sharply contrast with the 92,711 features introduced later (Table 3) that are used by our machine-learning models. In order to avoid overfitting, all of the deterministic models were created without reference to the annotation corpus, using general linguistic expertise and domain knowledge.\nIntra-sentence ordering Within sentences, syntactic regularities can be exploited to cover a large variety of grammatical constructions indicating precedence relations. Rules defined over dependency parses (De Marneffe and Manning, 2008) capture precedence in sentences like those in (1) and (2) as well as many others.\n(1) [The RBD of PI3KC2B binds HRAS]after , when [HRAS is not bound to GTP]before\n(2) [The ubiquitination of A]before is followed by [the phosphorylation of B]after\nOther phrases captured include: \u201cprecedes\u201d, \u201cdue to\u201d, \u201cleads to\u201d, \u201cresults in\u201d, etc.\nInter-sentence ordering Although syntax operates over single sentences, cross-sentence time expressions can indicate ordering, as shown in Examples (3) and (4). We exploit these regularities as well by checking for sentence-initial word combinations.\n(3) [A is phosphorylated by B]before. As a downstream effect, [C is . . . ]after\n(4) [A is phosphorylated by B]before. [C is then . . . ]after\nOther phrases captured include: \u201cLater\u201d, \u201cIn response\u201d, \u201cFor this\u201d, and \u201cUltimately\u201d.\nVerbal tense- and aspect-based (Reichenbach) ordering Following Chambers et al. (2014), we use deterministic rules to establish precedence between events that have certain verbal tense and aspect. These rules are derived from linguistic analysis of tense and aspect by (Reichenbach, 1947; Derczynski and Gaizauskas, 2013). Example (5) illustrates a case in which we can accurately infer order just from this information. Because has been phosphorylated has past tense and perfective\naspect, this model concludes that it precedes share (present tense, simple aspect) and thus the binding of histone H2A.\n(5) These [PTIP] proteins also share the ability to bind histone H2A (or H2AX in mammals) that has been phosphorylated. . . .\nThe logic determining which tense-aspect combinations receive which precedence relations is identical to CAEVO, which is possible because it is open source8. However, CAEVO operates over annotations that include gold tense and aspect values, whereas this model additionally detects tense and aspect using Odin rules before applying this logic."}, {"heading": "3.2 Feature-based Models", "text": "Most instances of causal precedence cannot be captured with deterministic rules, because they lack explicit words, phrases, or syntactic structures that unambiguously mark the relation. Using a combination of the surface, syntactic, and taxonomic features outlined in Table 3, we trained a set of statistical classifiers to detect causal precedence relations between pairs of events in our corpus. For training and testing purposes, we treated any instance not labeled as either \u201cE1 precedes E2\u201d or \u201cE2 precedes E1\u201d as a negative example. We examined the following statistical models: a linear kernel SVM (Chang and Lin, 2011), logistic regression (Fan et al., 2008), and random forest9 (Surdeanu et al., 2014). For the SVM and logistic regression (LR) models, we also compared the effects of L1 and L2 regularization."}, {"heading": "3.3 Latent Representation Models", "text": "Due to the complexity of the task and variety of causal precedence instances encountered during the annotation process, it is unclear whether a linear combination of engineered features is sufficient for broad coverage classification. For this reason, we introduce a latent feature representation model using an LSTM (Hochreiter and Schmidhuber, 1997; Bergstra et al., 2010; Chollet, 2015) to capture underlying semantic features by incorporating long-distance contextual information and selectively persisting memory of previous event pairs to aid in classification.\n8https://github.com/nchambers/caevo 9Abbreviated as RF\nThe basic architecture is shown in Figure 5. The input to this model is the provenance of the relation, i.e., the whole text containing the two events and the text in between. Formally, this is represented as a concatenated sequence of 200 dimensional vectors where each vector in the sequence corresponds to a token in the minimal sentential span encompassing the event pair being classified. Intuitively, this LSTM \u201creads\u201d the text from left to right and outputs a classification label from the set of three when done. We consider two variations of this model: the basic model (LSTM) with the vector weights for each token uninitialized and a second form (LSTM+P) where the vectors are initialized using pre-training. In the pretraining configuration, the vector weights are initialized using word embeddings generated by a word2vec (Mikolov et al., 2013; R\u030cehu\u030ar\u030cek and Sojka, 2010) model trained on the full text of over 1 million biomedical papers taken from the Open Access subset of PubMed. Because the corpus is only 1000 annotations, it was thought that pretraining could improve prediction of causal precedence and guide the model with distributional semantic representations specific to this domain.\nBuilding on this simple blueprint, we designed a three-pronged \u201cpitchfork\u201d (FLSTM) where the span of E1, the span of E2, and the minimal sentential span encompassing E1 and E2 each serve as a separate input, allowing the model to explicitly address each of them as well as discover how these three inputs relate to one another. This architecture is shown in Figure 6. Each input feeds into its own LSTM and corresponding dropout layer before the three forks are merged via a concatenation of tensors. Like the basic model, one version of the \u201cpitchfork\u201d is trained with vector weights initialized using the pre-trained word embeddings (FLSTM+P)."}, {"heading": "4 Results", "text": "We summarize the performance of all these models on the dataset previously introduced in Table 4. We report results using micro precision, recall, and F1 scores for each model. With fewer than 200 instances of causal precedence occurring in 1000 annotations, training and testing for both the feature-based classifiers and latent feature models was performed using stratified 10-fold cross validation. For the latent feature models, training was parameterized using a maximum of 100\nIn addition, binding of nucleotide-free Ras to PI3KC2\u03b2 inhibits its lipid kinase activity. The PI3KC2\u03b2 and Ras complex may then translocate to distal sites such as early endosomes (EE) where ITSN1 then binds to PI3KC2\u03b2 leading to the release of nucleotide-free Ras and activation of the lipid kinase activity of PI3KC2\u03b2.\nepochs with support for early stopping through monitoring of validation loss10. Weight updates were made on batches of 32 examples and all folds completed in fewer than 50 epochs.\nThe table also includes a sieve-based ensemble system, which performs significantly better than the best-performing single model. In this architecture, the sieves are applied in descending order\n10The validation set used for each fold came from a different class-balanced fold.\nof precision, so that the positive predictions of the higher precision sieves will always be preferred to contradictory predictions made by subsequent, lower-precision sieves. Figure 7 illustrates that as sieves are added, the F1 score remains fairly constant, while recall increases at the cost of precision.\nDespite some obvious patterns noted in Table 1, the deterministic models perform the worst due in large part to their rarity in the corpus. An analysis of this result is given in Section 5. Overall, our top-performing model was the linear kernel SVM with L1 regularization. In all cases, the feature-based classifiers outperform the latent feature representations, suggesting that in cases such as this where little data is available, feature-based classifiers capitalizing on high-level linguistic features are able to better generalize and thus outperform latent feature models. However, as our discussion in Section 5.1 will show, our combined model demonstrates that the latent and featurebased models are largely complementary."}, {"heading": "5 Discussion", "text": "Overall, results are promising, particularly in light of the conscious choice to omit (causal) regulation reactions from this task, as they are already captured by the Reach reading system.\nHowever, the deterministic models created so far have extremely low recall, such that it is difficult even to determine their precision. An analysis of the Reichenbach model reveals one source of this low coverage. In short, although writers could describe causal mechanisms using temporal indicators such as tense and aspect, temporal description is rare enough in this domain not to be represented in our randomly sampled database. Table 5 illustrates the lack of overlap with informative tense-aspect combinations; a single tense is used per passage, and no perfective aspect is used.\nSimilarly, the time expressions required by the deterministic intra- and inter-sentence precedence rules are rare enough to make them ineffective on this sample."}, {"heading": "5.1 Model overlap", "text": "As Chambers et al. (2014), Mirza (2016), and many other algorithms have shown, models can be applied sequentially in \u201csieves\u201d to produce higherquality output. Ideally, each model in a sievebased system will capture different portions of the data through a mixture of approaches, distinguishing this method from more naive ensembles in which the contributions of a lone component would be washed out. Figure 8 details this observation by showing the coverage difference between the models described here.\n4\n3\n9\n4\n1\n16\n2\n2\n13\n3 14 0\n1\n6\n6\nLSTM LSTM+P\nFLSTM FLSTM+P\n(a) Overlap of true positive predictions made by LSTM models. Though in Table 4 the models appear to perform similarly, the learned representations are largely distinct and complementary in their coverage."}, {"heading": "5.2 Error analysis", "text": "We performed an analysis of the false positives shared by all feature-based classifiers, in addition to the false negatives shared by all models. Here we limit our discussion to only the most prominent characteristic shared by the majority of false positives.\nDiscourse information More than half of the false positives share contrastive discourse features, suggesting that a model of discourse could improve classifier discrimination. Example (8) demonstrates such a contrastive structure, which whereas introduces a clause (and event) that is contrasted and therefore both temporally and causally distinct from the following clause (and event). The existence of regular cues like whereas indicates that a feature to explicitly model these structures is possible.\n(8) Whereas [PRAS40 inhibits the mTORC1 activity via raptor]E1, DEPTOR was identified to interact directly with mTOR in both [mTORC1 and mTORC2 complexes]E2"}, {"heading": "6 Related Work", "text": "Though focused on temporal ordering, Chambers et al. (2014) adopt a sieve-based approach, with high-precision deterministic sieves preceding and constraining lower-precision, higher-recall machine learning sieves. As with our system, the deterministic sieves were linguistically motivated, and had the additional advantage of operating over time expressions (during, Friday, etc.) as well as events, the former of which are typically lacking in the biomedical domain.\nMirza (2016) implemented a hybrid sieve-based approach for causal relation detection between events that includes a set of causal verb rules and corresponding syntactic dependencies and a feature-based classifier. However, both of these works focus on open-domain texts. To our knowledge, we are the first to investigate causal precedence in the biomedical domain."}, {"heading": "7 Conclusion", "text": "These are the first experiments regarding automatic annotation of causal precedence in the biomedical domain. Although the dearth of temporal expressions and other regular linguistic cues make the task especially difficult in this domain, the initial results are promising, and demonstrate that a sieve-based system of the models tested here improves performance over the top-performing individual component. Both the annotation corpus and the models described here represent large steps toward linking automatic reading to a larger, more informative biological mechanism."}, {"heading": "Acknowledgments", "text": "This work was funded by the Defense Advanced Research Projects Agency (DARPA) Big Mechanism program under ARO contract W911NF-141-0395."}], "references": [{"title": "An investigation of coreference phenomena in the biomedical domain", "author": ["Dane Bell", "Gus Hahn-Powell", "Marco A. ValenzuelaEsc\u00e1rcega", "Mihai Surdeanu."], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation.", "citeRegEx": "Bell et al\\.,? 2016", "shortCiteRegEx": "Bell et al\\.", "year": 2016}, {"title": "Paper available at http://arxiv", "author": ["LREC"], "venue": "org/abs/1603.03758.", "citeRegEx": "LREC,? 2016", "shortCiteRegEx": "LREC", "year": 2016}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Semeval-2015 task 6: Clinical tempeval", "author": ["Steven Bethard", "Leon Derczynski", "Guergana Savova", "Guergana Savova", "James Pustejovsky", "Marc Verhagen."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),", "citeRegEx": "Bethard et al\\.,? 2015", "shortCiteRegEx": "Bethard et al\\.", "year": 2015}, {"title": "Semeval-2016 task 12: Clinical tempeval", "author": ["Steven Bethard", "Guergana Savova", "Wei-Te Chen", "Leon Derczynski", "James Pustejovsky", "Marc Verhagen."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016), San", "citeRegEx": "Bethard et al\\.,? 2016", "shortCiteRegEx": "Bethard et al\\.", "year": 2016}, {"title": "Pathway commons, a web resource for biological pathway data", "author": ["Ethan G Cerami", "Benjamin E Gross", "Emek Demir", "Igor Rodchenkov", "\u00d6zg\u00fcn Babur", "Nadia Anwar", "Nikolaus Schultz", "Gary D Bader", "Chris Sander."], "venue": "Nucleic acids research, 39(suppl", "citeRegEx": "Cerami et al\\.,? 2011", "shortCiteRegEx": "Cerami et al\\.", "year": 2011}, {"title": "Dense event ordering with a multi-pass architecture", "author": ["Nathanael Chambers", "Taylor Cassidy", "Bill McDowell", "Steven Bethard."], "venue": "Transactions of the Association for Computational Linguistics, 2:273\u2013 284.", "citeRegEx": "Chambers et al\\.,? 2014", "shortCiteRegEx": "Chambers et al\\.", "year": 2014}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin."], "venue": "ACM Transactions on Intelligent Systems and Technology, 2:27:1\u201327:27. Software available at http:// www.csie.ntu.edu.tw/ \u0303cjlin/libsvm.", "citeRegEx": "Chang and Lin.,? 2011", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "keras", "author": ["Franois Chollet."], "venue": "https://github. com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "A coefficient of agreement for nominal scale", "author": ["Jacob Cohen."], "venue": "Educational and Psychosocial Measurement, 20:37\u201346.", "citeRegEx": "Cohen.,? 1960", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "DARPA\u2019s Big Mechanism program", "author": ["Paul R. Cohen."], "venue": "Physical Biology, 12(4):045008.", "citeRegEx": "Cohen.,? 2015", "shortCiteRegEx": "Cohen.", "year": 2015}, {"title": "Stanford typed dependencies manual", "author": ["Marie-Catherine De Marneffe", "Christopher D Manning."], "venue": "Technical report, Stanford University.", "citeRegEx": "Marneffe and Manning.,? 2008", "shortCiteRegEx": "Marneffe and Manning.", "year": 2008}, {"title": "Empirical validation of Reichenbach\u2019s tense framework", "author": ["Leon Derczynski", "Robert Gaizauskas."], "venue": "Proceedings of the 10th International Conference on Computational Semantics, pages 71\u201382.", "citeRegEx": "Derczynski and Gaizauskas.,? 2013", "shortCiteRegEx": "Derczynski and Gaizauskas.", "year": 2013}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."], "venue": "J. Mach. Learn. Res., 9:1871\u20131874, June.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Overview of bionlp\u201909 shared task on event extraction", "author": ["Jin-Dong Kim", "Tomoko Ohta", "Sampo Pyysalo", "Yoshinobu Kano", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Workshop on Current Trends", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "The genia event and protein coreference tasks of the bionlp shared task 2011", "author": ["Jin-Dong Kim", "Ngan Nguyen", "Yue Wang", "Junichi Tsujii", "Toshihisa Takagi", "Akinori Yonezawa."], "venue": "BMC bioinformatics, 13(11):1.", "citeRegEx": "Kim et al\\.,? 2012", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "The measurement of observer agreement for categorical data", "author": ["J Richard Landis", "Gary G Koch."], "venue": "biometrics, pages 159\u2013174.", "citeRegEx": "Landis and Koch.,? 1977", "shortCiteRegEx": "Landis and Koch.", "year": 1977}, {"title": "Biocause: Annotating and analysing causality in the biomedical domain", "author": ["Claudiu Mih\u0103il\u0103", "Tomoko Ohta", "Sampo Pyysalo", "Sophia Ananiadou."], "venue": "BMC Bioinformatics, 14(1):1\u201318.", "citeRegEx": "Mih\u0103il\u0103 et al\\.,? 2013", "shortCiteRegEx": "Mih\u0103il\u0103 et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Extracting temporal and causal relations between events", "author": ["Paramita Mirza"], "venue": null, "citeRegEx": "Mirza.,? \\Q2016\\E", "shortCiteRegEx": "Mirza.", "year": 2016}, {"title": "Overview of bionlp shared task 2013", "author": ["Claire N\u00e9dellec", "Robert Bossy", "Jin-Dong Kim", "JungJae Kim", "Tomoko Ohta", "Sampo Pyysalo", "Pierre Zweigenbaum."], "venue": "Proceedings of the BioNLP Shared Task 2013 Workshop, pages 1\u20137.", "citeRegEx": "N\u00e9dellec et al\\.,? 2013", "shortCiteRegEx": "N\u00e9dellec et al\\.", "year": 2013}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u2013 50, Valletta, Malta, May. ELRA. http://is.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Elements of Symbolic Logic", "author": ["Hans Reichenbach."], "venue": "Macmillan, London.", "citeRegEx": "Reichenbach.,? 1947", "shortCiteRegEx": "Reichenbach.", "year": 1947}, {"title": "processors", "author": ["Mihai Surdeanu", "Marco Valenzuela-Esc\u00e1rcega", "Gus Hahn-Powell", "Peter Jansen", "Daniel Fried", "Dane Bell", "Tom Hicks."], "venue": "https:// github.com/clulab/processors.", "citeRegEx": "Surdeanu et al\\.,? 2014", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2014}, {"title": "Reach", "author": ["Marco Valenzuela-Esc\u00e1rcega", "Gus Hahn-Powell", "Dane Bell", "Tom Hicks", "Enrique Noriega", "Mihai Surdeanu."], "venue": "https://github.com/ clulab/reach.", "citeRegEx": "Valenzuela.Esc\u00e1rcega et al\\.,? 2015a", "shortCiteRegEx": "Valenzuela.Esc\u00e1rcega et al\\.", "year": 2015}, {"title": "2015b. Description of the odin event extraction framework and rule language", "author": ["Marco A. Valenzuela-Esc\u00e1rcega", "Gus Hahn-Powell", "Mihai Surdeanu"], "venue": null, "citeRegEx": "Valenzuela.Esc\u00e1rcega et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Valenzuela.Esc\u00e1rcega et al\\.", "year": 2015}, {"title": "A domain-independent rule-based framework for event extraction", "author": ["Marco A. Valenzuela-Esc\u00e1rcega", "Gustave HahnPowell", "Thomas Hicks", "Mihai Surdeanu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational", "citeRegEx": "Valenzuela.Esc\u00e1rcega et al\\.,? 2015c", "shortCiteRegEx": "Valenzuela.Esc\u00e1rcega et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "There is a tremendous ongoing effort (N\u00e9dellec et al., 2013; Kim et al., 2012; Kim et al., 2009) to extract individual chemical interactions from these texts, but these interactions are only isolated fragments of larger causal mechanisms such as protein signaling pathways.", "startOffset": 37, "endOffset": 96}, {"referenceID": 16, "context": "There is a tremendous ongoing effort (N\u00e9dellec et al., 2013; Kim et al., 2012; Kim et al., 2009) to extract individual chemical interactions from these texts, but these interactions are only isolated fragments of larger causal mechanisms such as protein signaling pathways.", "startOffset": 37, "endOffset": 96}, {"referenceID": 15, "context": "There is a tremendous ongoing effort (N\u00e9dellec et al., 2013; Kim et al., 2012; Kim et al., 2009) to extract individual chemical interactions from these texts, but these interactions are only isolated fragments of larger causal mechanisms such as protein signaling pathways.", "startOffset": 37, "endOffset": 96}, {"referenceID": 5, "context": "The absence of such a database is not for lack of trying; Pathway Commons (Cerami et al., 2011) aims to address the need, but its authors estimate it currently covers 1% of the literature due to the high cost of annotation1.", "startOffset": 74, "endOffset": 95}, {"referenceID": 10, "context": "The Big Mechanism program (Cohen, 2015) aims to construct exactly such large-scale mechanistic information by reading and assembling protein signaling pathways that are relevant for cancer, and exploit them to generate novel explanatory and treatment hypotheses.", "startOffset": 26, "endOffset": 39}, {"referenceID": 6, "context": "Although prior work (Chambers et al., 2014; Mirza, 2016) has addressed the challenging area of temporal precedence in the open domain, the biomedical domain presents very different data and, consequently, requires novel techniques.", "startOffset": 20, "endOffset": 56}, {"referenceID": 20, "context": "Although prior work (Chambers et al., 2014; Mirza, 2016) has addressed the challenging area of temporal precedence in the open domain, the biomedical domain presents very different data and, consequently, requires novel techniques.", "startOffset": 20, "endOffset": 56}, {"referenceID": 3, "context": "Though event temporality is crucial to understanding electronic health records for individual patients (Bethard et al., 2015; Bethard et al., 2016), its contribution to the understanding of biomolecular reactions is less clear as these events and processes may repeat in extremely short cycles, continue without end, or overlap in time.", "startOffset": 103, "endOffset": 147}, {"referenceID": 4, "context": "Though event temporality is crucial to understanding electronic health records for individual patients (Bethard et al., 2015; Bethard et al., 2016), its contribution to the understanding of biomolecular reactions is less clear as these events and processes may repeat in extremely short cycles, continue without end, or overlap in time.", "startOffset": 103, "endOffset": 147}, {"referenceID": 18, "context": "BioCause (Mih\u0103il\u0103 et al., 2013), for instance, is centered on connections between claims and evidence and contains only 51 annotated examples of causal precedence3.", "startOffset": 9, "endOffset": 31}, {"referenceID": 25, "context": "Second, we analyze the efficacy of a battery of different models in automatically determining precedence, built on top of the Reach automatic reading system (Valenzuela-Esc\u00e1rcega et al., 2015a; Valenzuela-Esc\u00e1rcega et al., 2015c) and measured against this novel corpus.", "startOffset": 157, "endOffset": 229}, {"referenceID": 27, "context": "Second, we analyze the efficacy of a battery of different models in automatically determining precedence, built on top of the Reach automatic reading system (Valenzuela-Esc\u00e1rcega et al., 2015a; Valenzuela-Esc\u00e1rcega et al., 2015c) and measured against this novel corpus.", "startOffset": 157, "endOffset": 229}, {"referenceID": 14, "context": "(2014), (b) feature-based models, and (c) models that rely on latent representations such as long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 132, "endOffset": 166}, {"referenceID": 6, "context": "In particular, we investigate three classes of models: (a) deterministic rule-based models inspired by the precedence sieves proposed by Chambers et al. (2014), (b) feature-based models, and (c) models that rely on latent representations such as long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997).", "startOffset": 137, "endOffset": 160}, {"referenceID": 25, "context": "To generate candidate events for our planned annotations, we ran the Reach event extraction system (Valenzuela-Esc\u00e1rcega et al., 2015a; Valenzuela-Esc\u00e1rcega et al., 2015c) over the full text5 of 500 biomedical papers taken from the", "startOffset": 99, "endOffset": 171}, {"referenceID": 27, "context": "To generate candidate events for our planned annotations, we ran the Reach event extraction system (Valenzuela-Esc\u00e1rcega et al., 2015a; Valenzuela-Esc\u00e1rcega et al., 2015c) over the full text5 of 500 biomedical papers taken from the", "startOffset": 99, "endOffset": 171}, {"referenceID": 0, "context": "Note that domain-specific coreference resolution is a component of the event extraction system used here (Bell et al., 2016).", "startOffset": 105, "endOffset": 124}, {"referenceID": 9, "context": "7 On a randomly selected sample of 100 event pairs, the two annotators had a Cohen\u2019s kappa score (Cohen, 1960) of 0.", "startOffset": 97, "endOffset": 110}, {"referenceID": 17, "context": "82, indicating \u201calmost perfect\u201d agreement for the precedes labels (Landis and Koch, 1977).", "startOffset": 66, "endOffset": 89}, {"referenceID": 23, "context": "These rules are derived from linguistic analysis of tense and aspect by (Reichenbach, 1947; Derczynski and Gaizauskas, 2013).", "startOffset": 72, "endOffset": 124}, {"referenceID": 12, "context": "These rules are derived from linguistic analysis of tense and aspect by (Reichenbach, 1947; Derczynski and Gaizauskas, 2013).", "startOffset": 72, "endOffset": 124}, {"referenceID": 6, "context": "Verbal tense- and aspect-based (Reichenbach) ordering Following Chambers et al. (2014), we use deterministic rules to establish precedence between events that have certain verbal tense and aspect.", "startOffset": 64, "endOffset": 87}, {"referenceID": 7, "context": "We examined the following statistical models: a linear kernel SVM (Chang and Lin, 2011), logistic regression (Fan et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 13, "context": "We examined the following statistical models: a linear kernel SVM (Chang and Lin, 2011), logistic regression (Fan et al., 2008), and random forest9 (Surdeanu et al.", "startOffset": 109, "endOffset": 127}, {"referenceID": 24, "context": ", 2008), and random forest9 (Surdeanu et al., 2014).", "startOffset": 28, "endOffset": 51}, {"referenceID": 14, "context": "For this reason, we introduce a latent feature representation model using an LSTM (Hochreiter and Schmidhuber, 1997; Bergstra et al., 2010; Chollet, 2015) to capture underlying semantic features by incorporating long-distance contextual information and selectively persisting memory of previous event pairs to aid in classification.", "startOffset": 82, "endOffset": 154}, {"referenceID": 2, "context": "For this reason, we introduce a latent feature representation model using an LSTM (Hochreiter and Schmidhuber, 1997; Bergstra et al., 2010; Chollet, 2015) to capture underlying semantic features by incorporating long-distance contextual information and selectively persisting memory of previous event pairs to aid in classification.", "startOffset": 82, "endOffset": 154}, {"referenceID": 8, "context": "For this reason, we introduce a latent feature representation model using an LSTM (Hochreiter and Schmidhuber, 1997; Bergstra et al., 2010; Chollet, 2015) to capture underlying semantic features by incorporating long-distance contextual information and selectively persisting memory of previous event pairs to aid in classification.", "startOffset": 82, "endOffset": 154}, {"referenceID": 19, "context": "In the pretraining configuration, the vector weights are initialized using word embeddings generated by a word2vec (Mikolov et al., 2013; \u0158eh\u016f\u0159ek and Sojka, 2010) model trained on the full text of over 1 million biomedical papers taken from the Open Access subset of PubMed.", "startOffset": 115, "endOffset": 162}, {"referenceID": 22, "context": "In the pretraining configuration, the vector weights are initialized using word embeddings generated by a word2vec (Mikolov et al., 2013; \u0158eh\u016f\u0159ek and Sojka, 2010) model trained on the full text of over 1 million biomedical papers taken from the Open Access subset of PubMed.", "startOffset": 115, "endOffset": 162}, {"referenceID": 6, "context": "Highlighted cells are tense-aspect combinations that are informative for establishing temporal precedence, following Chambers et al. (2014). All but one event pair fall outside of these informative combinations, and that exceptional pair was a false positive case.", "startOffset": 117, "endOffset": 140}, {"referenceID": 6, "context": "As Chambers et al. (2014), Mirza (2016), and many other algorithms have shown, models can be applied sequentially in \u201csieves\u201d to produce higherquality output.", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": "As Chambers et al. (2014), Mirza (2016), and many other algorithms have shown, models can be applied sequentially in \u201csieves\u201d to produce higherquality output.", "startOffset": 3, "endOffset": 40}, {"referenceID": 6, "context": "Though focused on temporal ordering, Chambers et al. (2014) adopt a sieve-based approach, with high-precision deterministic sieves preceding and constraining lower-precision, higher-recall machine learning sieves.", "startOffset": 37, "endOffset": 60}, {"referenceID": 6, "context": "Though focused on temporal ordering, Chambers et al. (2014) adopt a sieve-based approach, with high-precision deterministic sieves preceding and constraining lower-precision, higher-recall machine learning sieves. As with our system, the deterministic sieves were linguistically motivated, and had the additional advantage of operating over time expressions (during, Friday, etc.) as well as events, the former of which are typically lacking in the biomedical domain. Mirza (2016) implemented a hybrid sieve-based approach for causal relation detection between events that includes a set of causal verb rules and corresponding syntactic dependencies and a feature-based classifier.", "startOffset": 37, "endOffset": 481}], "year": 2016, "abstractText": "Causal precedence between biochemical interactions is crucial in the biomedical domain, because it transforms collections of individual interactions, e.g., bindings and phosphorylations, into the causal mechanisms needed to inform meaningful search and inference. Here, we analyze causal precedence in the biomedical domain as distinct from open-domain, temporal precedence. First, we describe a novel, hand-annotated text corpus of causal precedence in the biomedical domain. Second, we use this corpus to investigate a battery of models of precedence, covering rule-based, feature-based, and latent representation models. The highestperforming individual model achieved a micro F1 of 43 points, approaching the best performers on the simpler temporalonly precedence tasks. Feature-based and latent representation models each outperform the rule-based models, but their performance is complementary to one another. We apply a sieve-based architecture to capitalize on this lack of overlap, achieving a micro F1 score of 46 points.", "creator": "TeX"}}}