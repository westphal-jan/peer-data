{"id": "1606.08104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Content-Based Top-N Recommendation using Heterogeneous Relations", "abstract": "top - $ ~ n $ recommender feedback systems have have been extensively studied. however, the sparsity of user - item routing activities has not necessarily been well resolved. while many hybrid systems were proposed to address the cold - wired start problem, previously the profile information has not been sufficiently actively leveraged. furthermore, the heterogeneity of profiles communicated between users separately and items intensifies the challenge. in this paper, we propose a content - based top - $ n $ recommender system by instantly learning the global term weights prevalent in profiles. to achieve this, we bring in comparison pathsim, which could well measures the node similarity with heterogeneous relations ( between users and items ). starting from the original tf - idf data value, the overlapping global term weights gradually converge, and eventually somehow reflect both profile and activity information. to facilitate training, the derivative is reformulated into matrix form, which could eventually easily subsequently be paralleled. we conduct extensive experiments, which demonstrate the superiority performance of the proposed method.", "histories": [["v1", "Mon, 27 Jun 2016 00:58:16 GMT  (28kb)", "http://arxiv.org/abs/1606.08104v1", "13 pages, 8 figures, ADC 2016"]], "COMMENTS": "13 pages, 8 figures, ADC 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["yifan chen", "xiang zhao", "junjiao gan", "junkai ren", "yang fang"], "accepted": false, "id": "1606.08104"}, "pdf": {"name": "1606.08104.pdf", "metadata": {"source": "CRF", "title": "Content-Based Top-N Recommendation using Heterogeneous Relations", "authors": ["Yifan Chen", "Xiang Zhao", "Junjiao Gan", "Junkai Ren", "Yang Fang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n08 10\n4v 1\n[ cs\n.I R\n] 2\n7 Ju\nn 20"}, {"heading": "1 Introduction", "text": "Recommender systems typically leverage two types of signals to effectively recommend items to users - user activities, and content matching between user and item profiles. Depending on what to use, the recommendation models in literature are usually categorized into collaborative filtering, content-based and hybrid models [1]. In real-world applications, solely employing collaborative filtering or content-based models can not achieve desirable results, as is often the case that single source of information tends to be incomplete.\nTo better illustrate, we motivate the following example in architecture. Recently, Vanke, a leading real-estate corporation in China, started a Uber-ForArchitects project, namely NOSPPP, which tries to match architects with appropriate projects based on previous project information of architects and firms. During the running of NOSPPP, the data collected are two-fold, with the participated projects and the resumes, respectively. In terms of recommender system, the former is named \u201cfeedback\u201d (of users) while the latter is referred as \u201cprofiles\u201d (of items). Due to the sparsity of feedback, collaborative filtering based recommenders would face the cold-start problem, and hence, we have to resort to content-based or hybrid models [9]. However, unlike the applicant-job scenario therein, where the profiles of users and jobs could well match, the profiles of architects and projects describe things in two different worlds. Specifically,\nthe profiles of architects presents the working experience and skills, while the profiles of projects tells the area, the interior and exterior constructions, etc. This is rational, as designing architectures is in the form of art, where it is hard to specify the conditions or requirements through decomposition. In response to applications like NOSPPP, we explore recommendation utilizing both sparse feedback and heterogeneous profiles.\nIn this paper, we exploit a hybrid recommendation method that ensembles both sources of information. For ease of exposition, we consider the case that auxiliary information exists only on the side of items, and propose a item-based top-N recommendation algorithm 1. Classic item-based collaborative filtering uses the direct link information for recommendation without diffusing the influence of other user-item links. By regarding user-item interactions as a bi-type information network, we observe that such influence can be captured by item node similarity, where PathSim [18] via meta-path is served. Moreover, while content matching between heterogeneous profiles of users and items does not produce explicable results, methods including [23,22,19,21,20] suggest high similarity among objects within the same subspace, thus we contend that it can be employed for matching profiles between item-item or user-user, since profiles of same type is naturally homogeneous. A standard way to measure similarity between two profiles is computing the cosine similarity of the two bags of words, and each word is weighted by term frequency tf (within the document) \u00d7 inverted document frequency idf (of the term within the corpus). While the local term frequency could be computed offline, it has been suggested [9] that the global term weights idf requires further optimization to achieve better precision. Thus, we optimize the global term weights with the guidance of the similarity from PathSim.\nIn summary, the major contribution of the paper is a novel hybrid recommendation method, the overview of which is outlined as follows:\n(1) Derive item similarity measured by meta-paths using PathSim;\n(2) Optimize the global term weights guided by PathSim; and\n(3) Recommend top-N items based on nearest neighbor collaborative filtering.\nOrganization. Section 2 discusses related work. We present the method for deriving initial similarity between items in Section 3, and then, introduce the learning method for optimizing global term weights in Section 4. Experiment results are in Section 5, followed by conclusion in Section 6."}, {"heading": "2 Related Work", "text": "Top-N recommender systems have been extensively studied during the last few years, which could be classified into three categories.\n1 Without loss of generality, it is straightforward to extend the idea to the case of auxiliary information on both sides of users and items.\nThe first category is neighborhood-based collaborative filtering, which could be further classified into three classes: item-based and user-based. Given a certain user, user-based-nearest-neighbor (userkNN) [12,7,15] first identifies a set of similar users, and then recommends top-N items based on what items those similar users have purchased. Similarly, item-based-nearest-neighbor (itemkNN) [16] identifies a set of similar items for each of the items that the user has purchased, and then recommends top-N items based on those similar items.There are plenty of ways to measure user/item similarity, e.g., Pearson correlation, cosine similarity, and so forth.\nThe second category is model-based collaborative filtering, in which the latent factor models have achieved the state-of-the-art performance. Cremonesi et.al. proposed a simple model-based algorithm PureSVD [6], where users\u2019 features and items\u2019 features are represented by the principle singular vectors of the user-item matrix. Koren proposed the well-known SVD++ model [6]. Wu applied Regularized Matrix Factorization (RMF), Maximum Margin Matrix Factorization (MMMF), and Nonnegative Matrix Factorization (NMF) to recommender systems [24]. Weighted Regularized Matrix Factorization (WRMF) was introduced by Hu et al. [10]. The key idea of these methods is to factorize the user-item matrix to represent the users preferences and items characteristics in a common latent space, and then estimate the user-item matrix by the dot product of user factors and item factors. All these methods assume that only a few variables impact users preference and items features, which means the low-rank structure of user-item matrix.\nAnother model-based method, SLIM, proposed by Ning et. al. [13], predicts the user-item matrix by multiplying the observed user-item matrix by the aggregation coefficient matrix. SLIM estimates the coefficient matrix by learning from the observed user-item matrix with a simultaneous regression model. Specifically, it introduces sparsity with \u21131-norm regularizer into the regularized optimization and formed an elastic net problem to benefit from the smoothness of \u21132-norm and the sparsity of \u21131-norm. Later, plenty of research has been done based on SLIM. SSLIM [14] integrates the side information. LorSLIM [4] involves the nuclearnorm to induce the low-rank property of SLIM. HOSLIM [5] uses the potential higher-order information to generate better recommendation.\nThe last category is hybrid methods. Hybrid method is used to combine the virtue of different recommender algorithms to generate better performance. A hybrid method was used to deal with the cold-start scenarios by mapping entities (user/item attributes) to latent features of a matrix factorization model [8]."}, {"heading": "3 Initializing Item Similarity", "text": "This section present the method for measuring item similarity."}, {"heading": "3.1 Preliminaries", "text": "Definition 1 (Information Network). An information network is defined as a directed graph G = (V,E) with an object type mapping function \u03c3 : V \u2192 A\nand a link type mapping function \u03d5 : E \u2192 R, where each object v \u2208 V belongs to one particular object type \u03c3(v) \u2208 A, each link e \u2208 E belongs to a particular relation \u03d5(e) \u2208 R, and if two links belong to the same relation type, the two links share the same starting object type as well as the ending object type. Note that, if a relation exists from type A to type B, denoted as ARB, the inverse relation R\u22121 holds naturally for BR\u22121A. R and its inverse R\u22121 are usually not equal, unless the two types are the same and R is symmetric. When the types of objects |A| > 1 or the types of relations |R| > 1, the network is called heterogeneous information network; otherwise, it is a homogeneous information network.\nDefinition 2 (Network Schema). The network schema, denoted as TG = (A,R), is a meta template for a heterogeneous network G = (V,E) with the object type mapping \u03c3 : V \u2192 A and the link mapping \u03c2 : E \u2192 R, which is a directed graph defined over object types A, with edges as relations from R.\nDefinition 3 (Meta-path). A meta-path P is a path defined on the graph of network schema TG = (A,R), and is denoted in the form of A1 R1\u2192A2 R2\u2192 . . . Rl\u2192Al+1. For simplicity, the meta-path can be denoted by the type names if there exist no multiple relations between the same pair of types: P = (A1A2 . . . Al+1). A path p = (a1a2 . . . al+1) between a1 and al+1 is said to follow the meta-path P, if \u2200i, ai \u2208 Ai and each link ei = \u3008aiai+1\u3009 belongs to each relation Ri in P. These paths are called path instances of P, denoted as p \u2208 P.\nIn the following, meta-path is confined to symmetric, namely round trip metapaths in the form of P = (PlP \u22121 l ).\nDefinition 4 (Commuting Matrix). Given a network G = (V,E) and its network schema TG, a commuting matrix M for a meta-path P = (A1A2 . . . Al) is defined as M = WA1A2WA2A3WAl\u22121Al , where WAiAj is the weight matrix between type Ai and type Aj.\nDefinition 5 (PathSim). Given a symmetric meta-path P, PathSim between two objects vi and vj of the same type is:\nsij = 2Mij\nMii +Mjj , (1)\nwhere Mij represents the i th row and jth column element of matrix M ."}, {"heading": "3.2 Measuring Item Similarity", "text": "To measure item similarity through PathSim, we first define the meta-path in the form of Pn = (A(BA)\nn) (the mined frequent patterns [3]). For instance, n = 1 corresponds to P1 = (ABA) and n = 2 corresponds to P2 = (ABABA). It is easy to verify the symmetry of Pn and thus PathSim can be applied. The associated commuting matrix for Pn is M = (WABWBA)\nn and consequently the similarity between item i and item j can be computed by Equation (1).\nSuppose we define N meta-paths P1,P2, . . . ,PN , with the corresponding similarities s1, s2, . . . , sN , the overall similarity should be measured as the weighted aggregation, e.g. s =\n\u2211N n=1 \u03b1nsn, where \u2211N n=1 \u03b1n = 1. As is suggested in [18]\nthat the meta-path with relatively short length is good enough to measure similarity, and a long meta-path may even reduce the quality, we set smaller weights for longer meta-paths. We naturally set the weights as \u03b1n = 2N\u2212n\n2N\u22121 . We further\ndenote Sp for the matrix of PathSim, where spij represents the element of S p in the ith row and jth column."}, {"heading": "4 Optimizing Profile Similarity", "text": "We prompt to measure the item similarity based on the profiles. Prior to the discussion, we first list the notations used in this section in Table 1. Note that vectors and matrices are made bold.\nEach profile contains rich text to describe the feature. Thus more effective content analysis methods and text similarity measures are crucial for the recommendation. Most designed recommender systems involving text similarity measure applied cosine similarity of two bags of words, where each word is weighted by tf \u00d7 idf [2,17]. Nevertheless, it is possible to go beyond the definition of tf \u00d7 idf , where tf represents the local term weights and idf the global term weights. While tf could be derived offline with various methods, idf requires further optimization as suggested by [9]. Thus the global term could be optimized with the guidance of PathSim and the similarity derived from profiles could be calculated by the following Equation:\ns f ij = di \u00b7 dj \u2016di\u20162\u2016dj\u20162 , (2)\nwhere \u2016 \u00b7 \u20162 is the \u21132 norm of a vector, and di represents the term vector, each dimension represents a term, and the value in each dimension represents the weight of the term. di could be decomposed as w l i \u25e6 w g, where wgi denotes\nfor the local weights for item i. wg denotes for the global term weights, which is initially set with the original Inverted Document Frequency, and optimized gradually. \u25e6 is a binary operation, conducting the element-wise product of two vectors, thus the result is also a vector. By letting wk = (w g k)\n2, we could further formalize Equation 2 as follows:\ns f ij =\n\u2211t\nk=1 w l ikw l jkwk\n[\n\u2211t\nk=1(w l ik) 2wk\n] 1\n2\n[\n\u2211t\nk=1(w l jk) 2wk\n] 1\n2\n,\nand the partial derivative could be derived as:\n\u2202s f ij \u2202wk =\n1\n\u2016di\u201622\u2016dj\u2016 2 2\n{\nwlikw l jk\u2016di\u20162\u2016dj\u20162 \u2212\n[\n\u2016dj\u20162 2\u2016di\u20162 (wlik) 2 + \u2016di\u20162 2\u2016dj\u20162 (wljk) 2\n]\ndi \u00b7 dj\n}\n= wlikw l jk\n\u2016di\u20162\u2016dj\u20162 \u2212\ns f ij\n2\n[\n(wlik) 2 \u2016di\u201622 + (wljk) 2 \u2016dj\u201622\n]\n.\nTo optimize the global term weights, we should define the loss function to measure the difference between spij and s f ij . We develop the squared loss function and the associated optimization methods."}, {"heading": "4.1 Squared Error Loss Function", "text": "Due to the sparsity of the user-item information network, we could also expect the sparsity of similarities measured by PathSim. If item i can not reach item j through the bi-type information network, according to Equation 1, sij = 0.\nIn this section, the loss function is defined as the squared error, given by\nL =\nNv \u2211\ni=1\nNv \u2211\nj=1\n(sfij \u2212 s p ij) 2 = \u2016Sf \u2212 Sp\u20162F ,\nbased on which, we could minimize the following objective function to optimize the global term frequency:\nmin w\nJ = 1\n2 \u2016Sf \u2212 Sp\u20162F +\n\u03bb 2 \u2016w\u201622\ns.t. w \u2265 0 (3)\nwhere \u2016 \u00b7 \u2016F is the Frobenius norm, which is actually the squared sum of all elements of the matrix. w stands for the vector of wk, and we penalize \u21132 norm on the global term weights w to avoid over fitting and sparsity result. Sp is denoted for PathSim matrix, whereas Sf for the profile similarity matrix. We reformulate the problem into the following element-wise form, to facilitate the deduction of partial derivative over wk, e.g.,\n\u2202J \u2202wk .\nmin wk\nJ = 1\n2\nNv \u2211\ni=1\nNv \u2211\nj=1\n(sfij \u2212 s p ij)\n2 + \u03bb\n2\nNw \u2211\ni=1\nw2k\nwk \u2265 0, k = 1, . . . , Nw\nSolution. The partial derivative is given in Equation 4.\n\u2202J\n\u2202wk =\nNv \u2211\ni=1\nNv \u2211\nj=1\n(sfij \u2212 s p ij)\n{\nwlikw l jk\n\u2016di\u20162\u2016dj\u20162 \u2212\ns\u0303ij\n2\n[\n(wlik) 2 \u2016di\u201622 + (wljk) 2 \u2016dj\u201622\n]}\n+ \u03bbwk. (4)\nWe further define qij = s f ij \u2212 s p ij , pik = wlij \u2016di\u20162 and rij = (s f ij \u2212 s p ij)s f ij . Thus\nwe have:\nNv \u2211\ni=1\nNv \u2211\nj=1\n(sfij \u2212 s p ij)\nwlikw l jk\n\u2016di\u20162\u2016dj\u20162 =\nNv \u2211\ni=1\nNv \u2211\nj=1\nqijpikpjk = p T kQpk\nNv \u2211\ni=1\nNv \u2211\nj=1\n(sfij \u2212 s p ij)\ns f ij\n2\n[\n(wlik) 2 \u2016di\u201622 + (wljk) 2 \u2016dj\u201622\n]\n=\nNv \u2211\ni=1\nNv \u2211\nj=1\n1 2 rij(p 2 ik + p 2 jk)\n=\nNv \u2211\ni=1\nNv \u2211\nj=1\nrijp 2 ik = p T kRpk.\nwhere pk is a vector of pik, Q is Nv \u00d7 Nv matrix of qij and R is a diagonal matrix with the i-th element of principal diagonal equals \u2211\nj rij . By defining L = Q\u2212R, we find the following close form of derivative:\n\u2202J\n\u2202wk = pTk Lpk + \u03bbwk.\nIt could be further represented into the matrix form:\n\u2202J \u2202w = diag(P TLP ) + \u03bbw, (5)\nwhere diag(\u00b7) extracts the principal diagonal and form as a vector. Following the common practices for top-N recommendation [11], the loss function is computed over all entries of S. The summation above contains n\u00d7n terms, namely all pairwise items in the dataset. To ensure good performance while achieve reasonable training time, the algorithm is paralleled by CUDA."}, {"heading": "5 Experimental Evaluation", "text": "To evaluate our proposed method, extensive experiments have been conducted. However, due to space limitation, we only present part of the results."}, {"heading": "5.1 Experiment Setup", "text": "The results reported in this section is based on the NIPS dataset2. It contains paper-author and paper-word matrices extracted from co-author network at the NIPS conference over 13 volumes. We regard authors as users, papers as items and the contents of papers as the profile of items. Thus the data has 2037 users (authors) and 1740 items (papers), where 13649 words have been extracted from the corpus of item profiles. The content of the papers is preprocessed such that all words are converted to lower case and stemmed and stop-words are removed. One may note that NIPS dataset is very sparse, that is, some author may publish only one or two papers, which shows the importance of properly leveraging side information for recommendation.\nWe applied 5-time Leave-One-Out cross validation (LOOCV) to evaluate our proposed method. In each run, each of the dataset is split into a training set and a testing set by randomly selecting one of the non-zero entries of each user and placing it into the testing set. The training set is used to train a model, then for each user a size-N ranked list of recommended items is generated by the model. We varies N as 5,10,15,20 to compare the result difference. Our method has two parameters, np and \u03bb. np measures the length of meta-path and \u03bb measures the degree of regularization.\nThe recommendation quality is measured using Hit Rate (HR) and Average Reciprocal Hit Rank (ARHR) [7]. HR is defined as\nHR = #hits\n#users ,\nwhere #users is the total number of users and #hits is the number of users whose item in the testing set is recommended (i.e., hit) in the size-N recommendation list. A second measure for evaluation is ARHR, which is defined as\nARHR = 1\n#users\n#hits \u2211\ni=1\n1 pi ,\nwhere if an item of a user is hit, p is the position of the item in the ranked recommendation list. ARHR is a weighted version of HR and it measures how strongly an item is recommended, in which the weight is the reciprocal of the hit position in the recommendation list.\nWe implement our algorithm in C++. As our method involves optimizing the global weights over the whole vocabulary of item profiles, to expedite the training efficiency, the training process is paralleled in GPU and implemented by CUDA3. All experiments are done on a machine with 4-core Intel i7-4790 processor at 3.60GHz and Nvidia GeForce GTX TITAN X graphics card."}, {"heading": "5.2 Effect of Initial Value", "text": "We first evaluate the influence of initial value we set for global weights on the performance. We compare two settings, random and idf. The initial value is randomly set in the first setting while it is set as the value of inverted document frequency in the latter one. Here \u03bb is set as 0.01 and np as 1. The result is reported in Figure 1, which shows the superiority of idf over random. The result demonstrates the usefulness of side information in this dataset and we set the initial value of global term as idf thereafter."}, {"heading": "5.3 Effect of Parameters", "text": "In this set of experiments, the validation is conducted to select the most suitable parameters. \u03bb is varied from 0 to 0.05 and stepped 0.005 and np is set as 1,2,3. We draw the lines in Figure 2. Three lines are drawn to distinguish np = 1 (red line),np = 2 (blue line) and np = 3 (green line) respectively. The result shows that np should be set 1 to achieve better performance. This result is consistent\n2 http://www.cs.nyu.edu/\u02dcroweis/data.html 3 http://www.nvidia.cn/object/cuda-cn.html\nwith [18], which suggests shorter length of meta-path is good enough to measure similarity.\nAs Figure 2 depicts the performance along with \u03bb, we find the best value as 0.01 for np = 1, 0.02 for np = 1 and 0.025 for np = 3. It has also been shown that the method performs more robustly when np = 1 while it varies dramatically with \u03bb when np > 1. Based on the observation, we finally pick np and \u03bb as 1 and 0.01 for the rest of the experiments.\n5.4 Recommendation for Different Top-N\nBy setting the global term as the inverted document frequency, and letting \u03bb = 0.01, we evaluate the top-N recommendation performance, the result of which is illustrated in Figure 3. Obviously, with the increase of N , the performance improves. We also compare the different setting of np, which further demonstrates np = 1 could be a better choice. We also found in this set of experiments that when N increase from 5 to 10, the performance shows relatively higher improvement."}, {"heading": "5.5 Comparison of Algorithms", "text": "We finally compares our method with other algorithms in this set of experiments. As top-N recommendation methods have been extensively studied, we compare only with some state-of-the-art methods, e.g. Slim [13] and LCE [17]. We also incorporate the pure tfidf method to calculate the item similarity for recommendation. To distinguish, we name our proposed method as Mist (Meta path based item similarity to learn global term weights). We depict the result in Figure 4, where all the compared algorithms were optimized to the best settings.\nFigure 4(a) shows the recommendation of Mist is consistently better than other three methods. Note that Slim has the worst performance, this could be\nattributed to the sparsity of dataset. LCE also took advantage of item profiles, thus it has achieved good performance. It is also worth noting that the pure tfidf shows relatively acceptable results and Mist could be regarded as the collaborative optimized tfidf.\nWhen it comes to ARHR, showing in Figure 4(b), Mist also behaves the best, which was followed by LCE, tfidf and Slim. In conclusion, the learned global term weights can well capture both structural and textual information."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a content-based top-N recommender system by leveraging item profiles. We first employed PathSim to measure the item similarity on the top of heterogeneous relations between users and items, and then optimized the global term weights towards the PathSim similarities. To facilitate training, the derivation was reformulated into matrix form, which could easily be paralleled. We conducted extensive experiments, and the experimental results demonstrate the superiority of the proposed method."}], "references": [{"title": "Fab: content-based, collaborative recommendation", "author": ["M. Balabanovi\u0107", "Y. Shoham"], "venue": "Communications of the ACM 40(3), 66\u201372", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Cold-start item and user recommendation with decoupled completion and transduction", "author": ["I. Barjasteh", "R. Forsati", "F. Masrour", "A. Esfahanian", "H. Radha"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, RecSys 2015, Vienna, Austria, September 16-20, 2015. pp. 91\u201398", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards frequent subgraph mining on single large uncertain graphs", "author": ["Y. Chen", "X. Zhao", "X. Lin", "Y. Wang"], "venue": "2015 IEEE International Conference on Data Mining, ICDM 2015, Atlantic City, NJ, USA, November 14-17, 2015. pp. 41\u201350", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "LorSLIM: Low rank sparse linear methods for top-n recommendations", "author": ["Y. Cheng", "L. Yin", "Y. Yu"], "venue": "2014 IEEE International Conference on Data Mining, ICDM 2014, Shenzhen, China, December 14-17, 2014. pp. 90\u201399", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "HOSLIM: higher-order sparse linear method for top-n recommender systems", "author": ["E. Christakopoulou", "G. Karypis"], "venue": "Advances in Knowledge Discovery and Data Mining - 18th Pacific-Asia Conference, PAKDD 2014, Tainan, Taiwan, May 13-16, 2014. Proceedings, Part II. pp. 38\u201349", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "Proceedings of the 2010 ACM Conference on Recommender Systems, RecSys 2010, Barcelona, Spain, September 26-30, 2010. pp. 39\u201346", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Item-based top-N recommendation algorithms", "author": ["M. Deshpande", "G. Karypis"], "venue": "ACM Trans. Inf. Syst. 22(1), 143\u2013177", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning attribute-to-feature mappings for cold-start recommendations", "author": ["Z. Gantner", "L. Drumond", "C. Freudenthaler", "S. Rendle", "L. Schmidt-Thieme"], "venue": "ICDM 2010, The 10th IEEE International Conference on Data Mining, Sydney, Australia, 14-17 December 2010. pp. 176\u2013185", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning global term weights for contentbased recommender systems", "author": ["Y. Gu", "B. Zhao", "D. Hardtke", "Y. Sun"], "venue": "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada, April 11 - 15, 2016. pp. 391\u2013400", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Y. Hu", "Y. Koren", "C. Volinsky"], "venue": "Proceedings of the 8th IEEE International Conference on Data Mining (ICDM 2008), December 15-19, 2008, Pisa, Italy. pp. 263\u2013272", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "FISM: factored item similarity models for top-n recommender systems", "author": ["S. Kabbur", "X. Ning", "G. Karypis"], "venue": "The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013. pp. 659\u2013667", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluation of item-based top-n recommendation algorithms", "author": ["G. Karypis"], "venue": "Proceedings of the 2001 ACM CIKM International Conference on Information and Knowledge Management, Atlanta, Georgia, USA, November 5-10, 2001. pp. 247\u2013 254", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "SLIM: sparse linear methods for top-n recommender systems", "author": ["X. Ning", "G. Karypis"], "venue": "11th IEEE International Conference on Data Mining, ICDM 2011, Vancouver, BC, Canada, December 11-14, 2011. pp. 497\u2013506", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse linear methods with side information for top-n recommendations", "author": ["X. Ning", "G. Karypis"], "venue": "Sixth ACM Conference on Recommender Systems, RecSys \u201912, Dublin, Ireland, September 9-13, 2012. pp. 155\u2013162", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Qualitative analysis of user-based and item-based prediction algorithms for recommendation agents", "author": ["M. Papagelis", "D. Plexousakis"], "venue": "Engineering Applications of Artificial Intelligence 18(7), 781\u2013789", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Grouplens: An open architecture for collaborative filtering of netnews", "author": ["P. Resnick", "N. Iacovou", "M. Suchak", "P. Bergstrom", "J. Riedl"], "venue": "CSCW \u201994, Proceedings of the Conference on Computer Supported Cooperative Work, Chapel Hill, NC, USA, October 22-26, 1994. pp. 175\u2013186", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Item cold-start recommendations: learning local collective embeddings", "author": ["M. Saveski", "A. Mantrach"], "venue": "Eighth ACM Conference on Recommender Systems, RecSys \u201914, Foster City, Silicon Valley, CA, USA - October 06 - 10, 2014. pp. 89\u201396", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "PathSim: Meta path-based top-k similarity search in heterogeneous information networks", "author": ["Y. Sun", "J. Han", "X. Yan", "P.S. Yu", "T. Wu"], "venue": "PVLDB 4(11), 992\u20131003", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Effective multi-query expansions: Robust landmark retrieval", "author": ["Y. Wang", "X. Lin", "L. Wu", "W. Zhang"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multi12  media Conference, MM \u201915, Brisbane, Australia, October 26 - 30, 2015. pp. 79\u201388", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploiting correlation consensus: Towards subspace clustering for multi-modal data", "author": ["Y. Wang", "X. Lin", "L. Wu", "W. Zhang", "Q. Zhang"], "venue": "Proceedings of the ACM International Conference on Multimedia, MM \u201914, Orlando, FL, USA, November 03 - 07, 2014. pp. 981\u2013984", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "LBMCH: learning bridging mapping for cross-modal hashing", "author": ["Y. Wang", "X. Lin", "L. Wu", "W. Zhang", "Q. Zhang"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015. pp. 999\u20131002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust subspace clustering for multi-view data by exploiting correlation consensus", "author": ["Y. Wang", "X. Lin", "L. Wu", "W. Zhang", "Q. Zhang", "X. Huang"], "venue": "IEEE Trans. Image Processing 24(11), 3939\u20133949", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised metric fusion over multiview data by graph random walk-based cross-view diffusion", "author": ["Y. Wang", "W. Zhang", "L. Wu", "X. Lin", "X. Zhao"], "venue": "IEEE Transactions on Neural Networks and Learning Systems", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative filtering via ensembles of matrix factorizations", "author": ["M. Wu"], "venue": "Proceedings of KDD Cup and Workshop. vol. 2007", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Depending on what to use, the recommendation models in literature are usually categorized into collaborative filtering, content-based and hybrid models [1].", "startOffset": 152, "endOffset": 155}, {"referenceID": 8, "context": "Due to the sparsity of feedback, collaborative filtering based recommenders would face the cold-start problem, and hence, we have to resort to content-based or hybrid models [9].", "startOffset": 174, "endOffset": 177}, {"referenceID": 17, "context": "By regarding user-item interactions as a bi-type information network, we observe that such influence can be captured by item node similarity, where PathSim [18] via meta-path is served.", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "Moreover, while content matching between heterogeneous profiles of users and items does not produce explicable results, methods including [23,22,19,21,20] suggest high similarity among objects within the same subspace, thus we contend that it can be employed for matching profiles between item-item or user-user, since profiles of same type is naturally homogeneous.", "startOffset": 138, "endOffset": 154}, {"referenceID": 21, "context": "Moreover, while content matching between heterogeneous profiles of users and items does not produce explicable results, methods including [23,22,19,21,20] suggest high similarity among objects within the same subspace, thus we contend that it can be employed for matching profiles between item-item or user-user, since profiles of same type is naturally homogeneous.", "startOffset": 138, "endOffset": 154}, {"referenceID": 18, "context": "Moreover, while content matching between heterogeneous profiles of users and items does not produce explicable results, methods including [23,22,19,21,20] suggest high similarity among objects within the same subspace, thus we contend that it can be employed for matching profiles between item-item or user-user, since profiles of same type is naturally homogeneous.", "startOffset": 138, "endOffset": 154}, {"referenceID": 20, "context": "Moreover, while content matching between heterogeneous profiles of users and items does not produce explicable results, methods including [23,22,19,21,20] suggest high similarity among objects within the same subspace, thus we contend that it can be employed for matching profiles between item-item or user-user, since profiles of same type is naturally homogeneous.", "startOffset": 138, "endOffset": 154}, {"referenceID": 19, "context": "Moreover, while content matching between heterogeneous profiles of users and items does not produce explicable results, methods including [23,22,19,21,20] suggest high similarity among objects within the same subspace, thus we contend that it can be employed for matching profiles between item-item or user-user, since profiles of same type is naturally homogeneous.", "startOffset": 138, "endOffset": 154}, {"referenceID": 8, "context": "While the local term frequency could be computed offline, it has been suggested [9] that the global term weights idf requires further optimization to achieve better precision.", "startOffset": 80, "endOffset": 83}, {"referenceID": 11, "context": "Given a certain user, user-based-nearest-neighbor (userkNN) [12,7,15] first identifies a set of similar users, and then recommends top-N items based on what items those similar users have purchased.", "startOffset": 60, "endOffset": 69}, {"referenceID": 6, "context": "Given a certain user, user-based-nearest-neighbor (userkNN) [12,7,15] first identifies a set of similar users, and then recommends top-N items based on what items those similar users have purchased.", "startOffset": 60, "endOffset": 69}, {"referenceID": 14, "context": "Given a certain user, user-based-nearest-neighbor (userkNN) [12,7,15] first identifies a set of similar users, and then recommends top-N items based on what items those similar users have purchased.", "startOffset": 60, "endOffset": 69}, {"referenceID": 15, "context": "Similarly, item-based-nearest-neighbor (itemkNN) [16] identifies a set of similar items for each of the items that the user has purchased, and then recommends top-N items based on those similar items.", "startOffset": 49, "endOffset": 53}, {"referenceID": 5, "context": "proposed a simple model-based algorithm PureSVD [6], where users\u2019 features and items\u2019 features are represented by the principle singular vectors of the user-item matrix.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "Koren proposed the well-known SVD++ model [6].", "startOffset": 42, "endOffset": 45}, {"referenceID": 23, "context": "Wu applied Regularized Matrix Factorization (RMF), Maximum Margin Matrix Factorization (MMMF), and Nonnegative Matrix Factorization (NMF) to recommender systems [24].", "startOffset": 161, "endOffset": 165}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13], predicts the user-item matrix by multiplying the observed user-item matrix by the aggregation coefficient matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "SSLIM [14] integrates the side information.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "LorSLIM [4] involves the nuclearnorm to induce the low-rank property of SLIM.", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "HOSLIM [5] uses the potential higher-order information to generate better recommendation.", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "A hybrid method was used to deal with the cold-start scenarios by mapping entities (user/item attributes) to latent features of a matrix factorization model [8].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "2 Measuring Item Similarity To measure item similarity through PathSim, we first define the meta-path in the form of Pn = (A(BA) ) (the mined frequent patterns [3]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 17, "context": "As is suggested in [18] that the meta-path with relatively short length is good enough to measure similarity, and a long meta-path may even reduce the quality, we set smaller weights for longer meta-paths.", "startOffset": 19, "endOffset": 23}, {"referenceID": 1, "context": "Most designed recommender systems involving text similarity measure applied cosine similarity of two bags of words, where each word is weighted by tf \u00d7 idf [2,17].", "startOffset": 156, "endOffset": 162}, {"referenceID": 16, "context": "Most designed recommender systems involving text similarity measure applied cosine similarity of two bags of words, where each word is weighted by tf \u00d7 idf [2,17].", "startOffset": 156, "endOffset": 162}, {"referenceID": 8, "context": "While tf could be derived offline with various methods, idf requires further optimization as suggested by [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 10, "context": "Following the common practices for top-N recommendation [11], the loss function is computed over all entries of S.", "startOffset": 56, "endOffset": 60}, {"referenceID": 6, "context": "The recommendation quality is measured using Hit Rate (HR) and Average Reciprocal Hit Rank (ARHR) [7].", "startOffset": 98, "endOffset": 101}, {"referenceID": 17, "context": "with [18], which suggests shorter length of meta-path is good enough to measure similarity.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "Slim [13] and LCE [17].", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "Slim [13] and LCE [17].", "startOffset": 18, "endOffset": 22}], "year": 2016, "abstractText": "Top-N recommender systems have been extensively studied. However, the sparsity of user-item activities has not been well resolved. While many hybrid systems were proposed to address the cold-start problem, the profile information has not been sufficiently leveraged. Furthermore, the heterogeneity of profiles between users and items intensifies the challenge. In this paper, we propose a content-based top-N recommender system by learning the global term weights in profiles. To achieve this, we bring in PathSim, which could well measures the node similarity with heterogeneous relations (between users and items). Starting from the original TF-IDF value, the global term weights gradually converge, and eventually reflect both profile and activity information. To facilitate training, the derivative is reformulated into matrix form, which could easily be paralleled. We conduct extensive experiments, which demonstrate the superiority of the proposed method.", "creator": "LaTeX with hyperref package"}}}