{"id": "1606.08777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "\"Show me the cup\": Reference with Continuous Representations", "abstract": "one of the most fundamentally basic mental functions of language is to routinely refer to objects in a shared scene. modeling reference with continuous representations is challenging because it requires individuation, i. e., tracking and distinguishing an arbitrary number of referents. we introduce a neural network model that, given a definite user description and a set of objects strictly represented by natural images, points explicitly to the intended object if the conceptual expression has a unique referent, or indicates a failure, if it does not. the model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are judged purely visual, and when they are characterized by a combination of visual performance and linguistic aesthetic properties.", "histories": [["v1", "Tue, 28 Jun 2016 16:31:50 GMT  (6442kb,D)", "http://arxiv.org/abs/1606.08777v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["gemma boleda", "sebastian pad\\'o", "marco baroni"], "accepted": false, "id": "1606.08777"}, "pdf": {"name": "1606.08777.pdf", "metadata": {"source": "CRF", "title": "\u201cShow me the cup\u201d: Reference with Continuous Representations", "authors": ["Gemma Boleda", "Sebastian Pad\u00f3", "Marco Baroni"], "emails": ["firstname.lastname@unitn.it", "sebastian.pado@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "Humans use language to talk about the world, and one of its most basic functions is to refer to objects (Russell, 1905). This makes reference one of the fundamental devices to ground linguistic symbols in extralinguistic reality (Harnad, 1990). 1 For successful reference, the speaker must choose an expression allowing the hearer to pick the right referent. For instance, assume that Adam and Barbara are in the context of Figure 1, and consider the dialogues in (1).\n(1) Adam: Can you please give me. . . a. . . . the mug?\nBarbara: Sure. 1We ignore the thorny philosophical issues of reference, such as its relationship to reality. For an overview and references (no pun intended), see Reimer and Michaelson (2014).\nb. . . . the pencil? Barbara (searching): Ahem, I can\u2019t see any pencil here. . . c. . . . the book? Barbara: Sorry, which one?\nIn dialogue (1-a), reference is successful. It fails in (1-b) and (1-c), but for different reasons: in (1-b), the word \u201cpencil\u201d does not apply to any object in the scene; in (1-c), the use of singular \u201cthe\u201d implies that Adam refers to a unique object, while the scene contains three matching objects. These examples show how reference involves both characterization mechanisms that capture object properties, mainly through the use of content words (e.g. \u201cmug\u201d vs. \u201cpencil\u201d), and individuation mechanisms, prominently encoded in function words and morphology (e.g., \u201cthe\u201d vs. \u201csome\u201d, singular vs. plural), which allow us to track and distinguish referents.\nExisting computational approaches to meaning account for one of these aspects at the expense of the other: Data-driven approaches, including distributional semantic and neural network models, typically model the conceptual level (Turney and Pantel, 2010), accounting well for characterization, but not for in-\nar X\niv :1\n60 6.\n08 77\n7v 1\n[ cs\n.C L\n] 2\n8 Ju\nn 20\ndividuation. The converse holds for logics-based approaches (Bos et al., 2004).\nIn this paper, we propose a neural network model aimed at both aspects of reference, and that can be trained directly on reference acts. Just like in the typical reference scenario, the model works across modalities, looking for the referent of a verbal expression in the visual world, or in a setting in which entities are characterized by joint visual and linguistic information. The model, Point-or-Protest (PoP), behaves like Barbara: It identifies (points to) the image that corresponds to a given linguistic expression, or protests in case of reference failure. While the model is generic and could be extended to other reference types, our starting point in this paper is reference to (concrete) entities using single-entity denoting noun phrases (as in (1)). This case clearly illustrates the joint workings of characterization (reference requires recognizing the right sort of entity in the scene) and individuation (reference succeeds only if there is exactly one entity of the right kind: in (1), Barbara cannot simply recognize the presence of some \u201cpencil mass\u201d, but she must check that there is only one pencil to unambiguously refer to). We show, in two experiments, that PoP is competitive with a state-of-the-art pipeline requiring specific heuristics.2"}, {"heading": "2 Models", "text": "Point-or-Protest Point-or-Protest (PoP) is a feedforward neural network learning from examples how to react to successful and failed reference acts.3 Given a variable-length sequence of objects depicted in images (possibly coupled with other information characterizing them, e.g., verbal attributes) and a natural language query, PoP must either point to the object denoted by the query, returning its index in the sequence, or protest if the query phrase is not an appropriate referring expression. The PoP architecture builds an \u201centity array\u201d whose entries are vectors storing information about the objects in the scene, and uses similarity-based reasoning about the vectors in the array and the query to decide its response. We currently focus on singular definite article semantics, as in (1), with failure if there is no\n2We will make our code and data available. 3For neural network design and training see, e.g., Nielsen\n(2015).\npossible referent (missing-referent anomaly) or if there is more than one (multiple-referent anomaly). We discussed above the linguistic appeal of this case. From a machine-learning perspective, one-entity individuation requires a non-linear separation of the anomalous reference acts (0 or more than 1) from the felicitous ones.\nWe use the diagram in Figure 2 to introduce PoP. In this example, the input set contains a harrier and two cups, with the corresponding linguistic query being cup.4 PoP should thus raise the anomaly flag.\nThe linguistic query is first mapped to a dense space by using pre-compiled cbow embeddings, whereas images are mapped to vector representations by passing them through a pre-trained convolutional neural network (cnn), and extracting the activation patterns in one of the top layers of the network (see Section 4 for further details). If the input consists of objects with linguistic attributes, we simply concatenate the corresponding cnn and cbow vectors to get their input representation, and analogously we concatenate cbow vectors to represent multi-word linguistic phrases. Conceptually, using cbow embeddings means that the listener we model already possesses large amounts of unembodied knowledge about word meaning, as gathered from linguistic cooccurrence patterns independently of reference. This assumption is unrealistic, and we abandon it with the TRPoP model described below.\nPoP maps the input object representations in the sequence to an array of entity vectors by applying a linear transformation. The corresponding mapping matrix V is shared across objects, as the position of objects in the input sequence is arbitrary, and PoP should not learn associations between objects and specific sequence slots (e.g., from the Figure 2 example, it should not learn to associate cups with positions 2 and 3 in general). Each vector in the entity array corresponds to one input object. In parallel, PoP maps the linguistic expression to a \u201cquery\u201d vector through a separate linear transformation L. The query vector lives in the same space as the entity vectors to enable pairwise similarity computations. We can thus interpret the matrices V and L as map-\n4We do not enter the determiner in the query, since it does not vary across data points: our setup is equivalent to always having \u201cthe\u201d in the input. The network learns the intended semantics through training.\nping input vectors into a shared multimodal space, in which it is possible to probe visual (or mixed) entities with linguistic queries. Next, the network takes the dot product of the query with each entry in the entity array. The resulting vector (containing as many dimensions as dot products, and thus objects) encodes the similarity profile of the query with the entity vectors: the larger the value in dimension n, the more likely it is that the n-th object in the input sequence is a good referent for the query.\nPoP also needs to assess whether the reference act was felicitous. The cumulative \u201csimilarity mass\u201d across entity vectors should provide the network with good evidence to reason about anomaly. For the specific aim of modeling singular reference, the network should discover that, when cumulative similarity is too low or too high, the reference is not appropriate for the current sequence: in the first case, because no object matches the query; in the second, because there is more than one object that matches the query. More precisely, along the \u201canomaly pathway\u201d shown in grey in Figure 2, we first pass the similarity vector through a nonlinearity \u03c8 to sharpen the contrasts, particularly zeroing out low similarities. For example, a relu transformation might set all low similarities to 0, making it easier to detect anomalies: in Figure 2, the whitening of the harrier similarity cell is\nmeant to suggest this process. We then sum across all values in the resulting vector, obtaining a cumulative similarity score. We concatenate it with the cardinality of the input sequence and feed them, via a linear transformation Ai, to a vector of \u201canomaly sensor\u201d cells. Cardinality enables the model to take the number of inputs into account when assessing the cumulative similarity score: the same score that looks suspiciously high for two objects is bound to be low for ten objects. More specifically, through cardinality the model can compare the average similarity to arbitrary thresholds, and subsets of anomaly sensor cells can learn different thresholds to pick up anomalies (the presence of multiple anomaly sensor cells allows the model to pick up \u201cnon-linear\u201d patterns, such as the one for single-entity reference we are addressing here). Their output is linearly combined via matrix Ao into a single value. The latter is passed through nonlinearity \u03c6, that is bounding the anomaly score to approximate a discrete yes/no response.\nWe finally concatenate the similarity profile with the cell containing the anomaly score, and pass the resulting vector through a softmax nonlinearity (\u03c0). The model output for an input sequence of n objects will thus contain a probability distribution over n+ 1 indices. We take the index with the maximum value for this distribution as PoP\u2019s response: if it\nis one of the first n indices, then PoP \u201cpointed\u201d at the corresponding object, whereas if PoP assigned maximum probability to the n+ 1th cell, that means that it \u201cprotested\u201d. In the figure diagram, PoP has correctly raised the anomaly flag. We train PoP by backpropagating the error of the log-likelihood cost function when comparing its output (either the index of the correct object, or the anomaly flag) with the ground truth for the training reference acts.\nPipeline As a strong competitor, we implemented a method that performs our task by manual pipelining of a set of separately trained/tuned components. The Pipeline first induces a set of multimodal embeddings by optimizing similarities between matched pairs of queries and objects, compared to random confounders. It uses a max-margin cost function forcing query representations to be (much) more similar to the objects they denote than to irrelevant ones. This has been shown to produce excellent multimodal embeddings (Frome et al., 2013; Lazaridou et al., 2015a; Weston et al., 2011). Once these embeddings have been separately trained, the model computes similarities between the query and each of the objects in each referential act in our test sets, picking the object with largest similarity as candidate object to point at. Then, two separately-tuned heuristics are used to catch anomalous acts: Missing reference is predicted if no query-object similarity is above an (optimized) threshold. Multiple reference is guessed if the difference between the two largest similarities is below another optimized threshold.\nConvolutional Neural Network Since PoP uses input image embeddings based on a pre-trained convolutional neural network (CNN), we also test a model matching the categorical labels produced by the same CNN for the input images against the query. For the example of Figure 2, it would pass each of the images through the full CNN, obtaining 3 labels. We take a lax approach to label matching, in which the model scores a hit even when, e.g., the gold label is a substring of the model-predicted one. Anomaly detection is straightforward (although again implemented ad hoc): CNN deems a reference act anomalous if no produced label matches the query, or if more than one does. Thus, the CNN would be successful if it predicted a synonym of cup for both image 2 and 3.\nTabula Rasa PoP Through the cbow vectors, PoP can rely on pre-acquired text-induced word similarity knowledge. The assumption that word meanings are first learned separately, purely from language statistics, and then fine-tuned in the referential setup, is unrealistic. Ideally, we would want a model that learns word representations in parallel from reference acts and language statistics. For the time being, we consider instead the other extreme, where word representations are entirely induced from the reference acts during training. The \u201cTabula Rasa\u201d PoP model (TRPoP) is identical to the one in Figure 2, except that input query representations (and attributes in the Object+Attribute setup explained below) are one-hot vectors. This model will thus induce distributed representations from scratch when estimating the weights of matrices L and V. Such representations will then depend entirely on the role of words as queries or attributes in the referential acts we model."}, {"heading": "3 Data", "text": "We test our model in two experiments, for each of which we have automatically created a large-scale dataset. Both datasets contain 40,000 sequences for training, 5,000 for validation and 10,000 for testing, each with 15% missing-referent and 15% multireferent anomalies. The sequences are of varying length, from 2 to 5 candidate referents. The supplementary materials contain the algorithms used to generate the datasets as well as detailed statistics.\nObject-Only Experiment. Our first experiment represents a base case of reference, namely matching noun phrases consisting of single nouns with visually represented entities. Figure 3 shows two examples. The objects and images are sampled uniformly at random from a set of 2,000 objects and 50 ImageNet5 images per object, itself sampled from a larger dataset used in Lazaridou et al. (2015b). As the examples show, we use natural objects and images, which makes the task very challenging (even humans might wonder which image in the second row depicts a darling). We generate data with an algorithm sampling sets of sequences with uniform distributions over sequence lengths (2 to 5) and indices of the queried object within a sequence.\n5http://imagenet.stanford.edu/\nObject+Attribute Experiment. Our second experiment, illustrated in Figure 4, goes one step further in testing the model\u2019s individuation capabilities. In the scene from Section 1, imagine that Adam points to the book on top and says \u201cI recommend this book\u201d. This linguistically conveyed information will be associated to Barbara\u2019s representation of the entity, together with its visual features. Crucially, it can be used to identify the first book if later on Adam asks her \u201cCan you bring the book I recommended?\u201d. We test this situation in a simplified form. Each referent is associated with both an image and a linguisticallyexpressed attribute, more specifically a verb (the only word class from which we could sample a sufficient number of attributes with the characteristics outlined below). The query and the sequence items are all pairs like spend:bill, where we interpret the attribute analogously to an object relative clause, that is, a bill that is being spent (we ignore tense for simplicity).\nWe restrict the attributes under consideration for each object to the 500 highest-associated syntactic neighbors of the object according to the DM resource (Baroni and Lenci, 2010), such that the attributes be compatible with the objects (to exclude nonsensical combinations such as repair:dog). Of these, we retain only verbs taking the target item as direct object, in line with the \u201crelative clause\u201d interpretation sketched above. Moreover, we focus on (relatively) abstract verbs, for two reasons. First, a concrete verb is more likely than an abstract one to have strong visual correlates that do not match what is actually depicted in an image (cf. groom:dog vs. like:dog). Second, successful reference routinely mixes concrete and abstract cues (e.g., a noun referring to a\nconcrete object combined with a modifier recording an event associated to it: the book I lent you), and we are interested in simulating this scenario. We thus filter verbs through the concreteness norms of Brysbaert et al. (2014), retaining only those with a concreteness score of at most 2.5 (on a 1\u20135 scale).\nThe object-attribute structure of the stimuli in this experiment also enables us to introduce challenging confounders into the sequences \u2013 namely, pairs that share either the attribute or the object with the query. For each sequence, we start by picking an attributeobject query. Given the query, we generate two more compatible attributes for the query object, and alternative objects compatible to these attributes as well as the initial attribute. Starting from all attribute-object combinations, we randomly drop as many as necessary to obtain the final sequences of 2 to 5 items. A consequence of this design is that the objects within sequences tend to be somewhat related since they share compatible attributes, and vice versa. The first sequence in Fig. 4 illustrates the effect: For the query\nobject bartender, we generate the confounder object soldier, connected through the attributes instruct and inform. The full sequence also includes the confounder object emperor, not shown in the figure."}, {"heading": "4 Experiments", "text": "Method PoP and Pipeline\u2019s input word representations are 400-dimensional cbow embeddings from Baroni et al. (2014), trained on about 2.8 billion tokens of unannotated text. These models, as well as TRPoP, use 4096-dimensional vectors as input visual representations, which are produced by passing images through the pre-trained VGG 19-layer CNN of Simonyan and Zisserman (2015) (trained on the ILSVRC-2012 data), and extracting the corresponding activations on the topmost fully connected layer.6 The same pre-trained network was used to generate the labels of our CNN competitor model. The parameters of PoP/TRPoP and of the Pipeline maxmargin embeddings are estimated by online stochastic gradient descent on the training portions of the two datasets. For Pipeline, we extract all possible pairs of positive and negative query-object tuples from each reference act in the relevant training data. Details on model hyperparameter tuning are in the supplementary material. We consider three baselines for both experiments. Random assigns all labels randomly. Majority assigns the most frequent output label, namely anomaly, accounting for 30% of the sequences (the non-anomalous labels are distributed among predicted indices). Probability randomly assigns labels based on their relative frequency in the training data.\nExperiment 1: Object-Only Results are reported on the left-hand side of Table 1. Besides overall accuracy (Total), we show accuracy itemized by successful reference acts (Pointing), missing-referent (MissRef ) and multiple-referent anomalies (MultRef ).\n(TR)PoP and Pipeline are clearly above the baselines (Majority and Probability reach deceptively high anomaly-detection scores by over-raising the anomaly flag, at the cost of pointing performance). PoP\u2019s absolute performance is close to that of the manually-crafted Pipeline. By jointly learning to point and handling anomalies in reference acts, PoP\n6We use the MatConvNet toolkit, http://www.vlfeat. org/matconvnet/\nloses some performance in pointing, but in exchange it does better on anomaly detection. As could be expected, MissRef is more difficult than MultRef for all three models. Interestingly, TRPoP, which does not rely on pre-trained word embeddings, performs comparably to PoP (but it requires more than twice as many epochs to converge, see supplementary materials). This suggests that useful representations of word meaning can be learned solely from examples of successful and failed reference acts.\nCNN performance is barely above baseline, and, like Majority and Probability, it trivially reaches high performance on anomaly cases because it raises the anomaly flag whenever it fails to produce the name of the target object (and it rarely produces the right label). For instance, in the first example in Figure 3, CNN can get a hit as long as it doesn\u2019t produce \u201ccup\u201d. As for its extremely low pointing performance, note that CNN, unlike PoP, cannot make reasonable pointing guesses for objects it did not see during training. The large performance asymmetry between these two models sharing the same visual processing network shows that PoP generalizes well beyond the knowledge it inherited from this pre-trained network. Importantly, PoP reasons about similarity in multimodal space, rather than assigning hard labels. For example, the CNN, when presented with an image associated to the out-of-training query academician, tags it as academic gown \u2013 not unreasonably but incorrectly. PoP points to the correct slot because its multimodal academician query vector is most similar to the correct entity vector than to the other candidates, with no need to perform explicit label matching. Intriguingly, even when considering the subset of test data that CNN is trained on, we still observe an asymmetry: CNN reaches 58% accuracy, while PoP\u2019s performance is at 67%. This suggests that reference-based training has fine-tuned better representations also for the objects the CNN was explicitly trained for.\nExperiment 2: Object+Attribute Results are shown on the right side of Table 1. CNN is not tested here, as it does not handle attributes. PoP\u2019s results are slightly higher than in the previous experiment, while those of TRPoP and Pipeline are slightly lower, such that now PoP is clearly above them. The three models are exploiting both visual and verbal information, as shown by their comparison to two additional\nbaselines, shown at the bottom of the table. AttrRandom randomly picks one of the objects that shares the attribute with the query, if any, and raises the anomaly flag otherwise. This baseline has, by construction, 0% accuracy on MultRef anomalies, and it performs at random in MissRef detection. However, even in the pointing case, its performance is still well below that of the models. ImgShuffle is a variant of PoP trained after shuffling image vectors, so that each image ID is (consistently) associated with the CNN representation of another image (mostly depicting objects that do not match the image label). The only reliable signal that this baseline can then exploit is attribute information. Again, its performance is clearly below that of the models.\nAs for anomaly handling, while PoP still finds MissRef easier than MultRef, this time TRPoP and Pipeline actually perform worse on MissRef. Comparing the MissRef cases in which the models failed to raise the anomaly flag, we observe that Pipeline and TRPoP wrongly pointed to an entity sharing the query attribute much more often than PoP (943 and 883 vs. 629). They are thus over-relying on matching attributes, assigning too high a similarity to pairs that are simply sharing the verbal attribute. This also explains their higher performance on MultRef: Attribute sharing makes both repeated referents very similar to the query, triggering the relevant heuristic. Thus, PoP seems better at integrating verbal and visual cues than them. Compared to TRPoP, PoP has an important prior in the semantics encoded in its pre-trained word embeddings, which helps it discover systematic relations between words and objects while\nkeeping the attribute information apart from that of the head noun. Compared to Pipeline, by jointly learning to point and to spot anomalies, it might be able to attain a better balance between visual and verbal information.\nTo conclude, our model PoP and its variant TRPoP can learn to refer directly from examples. While PoP is not clearly superior to Pipeline, it has a fundamental advantage: It learns to refer in one integrated architecture. Pipeline (as well as CNN) learns to characterize objects (e.g., to recognize cups as referents for \u201ccup\u201d), but uses an ad hoc strategy, needing a manually coded heuristic, to simulate individuating capabilities (distinguishing cases where there are several or no cups). As soon as the referential setup gets more complex, as in the Object+Attribute experiment in which visual and verbal information need to be combined, the heuristics break down."}, {"heading": "5 Related work", "text": "Modeling. The PoP model \u201creasons\u201d about the similarity between a query and a set of candidates in vector space, akin to soft attention mechanisms in recent neural network architectures (Bahdanau et al., 2015; Xu et al., 2015). While attention is standardly used to retrieve auxiliary information when producing an output, we directly expose the similarity vector as (part of the) output, in order to obtain a model that learns to point robustly across input sequence orders and lengths. The idea of exposing an attention mechanism functioning as a pointer over the input has recently been employed by Vinyals et al. (2015) in the context of sequence-to-sequence RNNs. PoP\u2019s\nentity array emulates traditional memory locations within a fully differentiable architecture. This is akin to the memory vectors of the recently proposed Memory Networks framework (Sukhbaatar et al., 2015; Weston et al., 2015). However, the Memory Networks array has fixed size, whereas our entity array adapts to input object cardinality.\nMultimodal reference resolution. Our task is a special case of reference resolution. Various studies in this area have proposed multimodal approaches jointly handling vision and language (Gorniak and Roy, 2004; Larsson, 2015; Matuszek et al., 2014; Steels and Belpaeme, 2005, a.o.). These papers focus on aspects of the resolution process we are not currently addressing, such as full compositionality or gesture, but they work with very limited perceptual input, such as simple shapes and colours. Probably the most relevant study in this area is the one by Kennington and Schlangen (2015). They consider visual scenes with more objects than our sequences, but more limited in nature (tables with 36 puzzle pieces). They handle spatial relations and flexible compositionality. However, they must train a separate classifier for each word in their set, which means that their method can\u2019t process unseen words, and would probably perform badly for words that are not observed frequently enough during training. Moreover, they do not present an integrated architecture for the whole resolution process, as we do, but separate components that are manually combined. Crucially, they assume referring expressions are always felicitous. We are not aware of prior work that, like our Object+Attribute setup, considers referents disambiguated by a mixture of perceptual and verballyexpressed abstract properties.\nReferring expression generation and other related work. The task of referring expression generation (Krahmer and van Deemter, 2012) has recently received new impulses from the study of multimodal language/vision scenarios. The task is converse to ours: given a scene, generate the optimal linguistic expression to pick out a given referent. The focus is generally on considerably more complex (but artificial or heavily controlled) scenes than our sequences, and correspondingly on linguistically more complex referring expressions. Some recent efforts collect and analyze large corpora of referring expressions\nfor multimodal tasks (Kazemzadeh et al., 2014; Tily and Piantadosi, 2009). A method to generate unambiguous referring expressions for objects in natural images has been recently proposed by Mao et al. (2016). Our task is more distantly related to visual question answering (Geman et al., 2015; Malinowski and Fritz, 2014; Ren et al., 2015), in the sense that we model one specific type of question that could easily be asked about an image. Even more generally, our approach fits into the multimodal distributional semantics paradigm. See Baroni (2016) for a discussion of how the problem of reference is addressed in that line of work. There is of course a large body of work on modeling reference with symbolic/logical methods (Abbott, 2010), that provides the framework for our problem, but is not directly relevant to our empirical aims. Our task can finally also be seen as a special case of the much broader problem of content-based image retrieval (Datta et al., 2008)."}, {"heading": "6 Conclusion and outlook", "text": "PoP is a neural network model that, given a linguistic expression and a set of objects represented by natural images, either resolves reference by pointing to the object denoted by the expression, or flags the reference act as anomalous if the linguistic expression is not adequate. The model consists of an integrated and generic architecture that can be directly trained on examples of successful and failed reference acts. The model is competitive with a pipeline manually engineered to perform the same task. PoP successfully accounts for characterization because it is able to relate entity properties (visual, and linguistically conveyed) to linguistic expressions, via its distributed representations. It has some individuating capabilities because it builds entity representations and reasons about them.\nAlthough our experiments concentrated on singular definite descriptions (\u201cshow me THE cup\u201d, the PoP architecture is general enough that should be able, given appropriate training data, to learn to respond to other reference acts, e.g., corresponding to all X or many X (Sorodoc et al., 2016). We intend to pursue this direction in future work. We also intend to progressively remove various artificial characteristics of our simulations, such as the fact that our sequences do not form natural scenes,\nand that our linguistic expressions are very limited. Most importantly, we are currently assuming one-toone mapping between images and entity vectors. In real-life reference, however, possible referents might re-appear or be mentioned at different times and in different places, and we might, over time, acquire further knowledge characterizing them. PoP must thus eventually be able to learn when to initialize a new entity vector (thus maintaining distinct, individuated representations of the entities in ongoing discourse), and when to update an existing one with new information (furthering the characterization of the entity). Recent work on fully differentiable architectures controlling discrete data structures that can grow and shrink, such as stacks (Joulin and Mikolov, 2015), should make such extensions feasible."}, {"heading": "Acknowledgments", "text": "We are grateful to Elia Bruni for the CNN baseline idea, and to Angeliki Lazaridou for providing us with the visual vectors used in the paper. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES); DFG (SFB 732, Project D10); and Spanish MINECO (grant FFI201341301-P)."}, {"heading": "1 Data Creation for the Object-Only Dataset (Experiment 1)", "text": "The process to generate a object sequence is shown in Algorithm 1. We start with an empty sequence and sample the length of the sequence uniformly at random from the permitted sequence lengths (l. 2). We fill the sequence with objects and images sampled uniformly at random (l. 4/5). We assume, without loss of generality, that the the object that we will query for, q, is the first one (l. 6). Then we sample whether the current sequence should be an anomaly (l. 7). If it should be a missing-anomaly (i.e., no matches for the query), we overwrite the target object and image with a new random draw from the pool (l. 9/10). If we decide to turn it into a multipleanomaly (i.e., with multiple matches for the query), we randomly select another position in the sequence and overwrite it with the query object and a new image (l. 12/13). Finally, we shuffle the sequence so that the query is assigned a random position (l. 14)."}, {"heading": "2 Data Creation for the Object+Attribute Dataset (Experiment 2)", "text": "Figure 1 shows the intuition for sampling the Object+Attribute dataset. Arrows indicate compatibility constraints in sampling. We start from the query pair (object 1 \u2013 attribute 1). Then we sample two more attributes that are both compatible with object\nAlgorithm 1 Creation of Object-Only dataset Input: Sequence length interval [i \u2265 2, j]; Set of objects\nO = {o1, . . . , on} and sets of associated images I(o) for each object o; probability of missing-anomalies P0; probability of multiple-anomalies Pm.\nOutput: \u3008 object query q, object-image sequence S\u3009 1: S \u2190 [] 2: l \u223c U (i, j) 3: for k = 1 to l do 4: o \u223c O, i \u223c I(o) 5: S[k] = \u3008o, i\u3009 6: q \u2190 S[1] 7: r0 \u223c Bern(p0), rm \u223c Bern(p0 + pm) 8: if r0 then 9: o\u2032 \u223c O, i\u2032 \u223c I(o) so that o\u2032 6= q 10: S[0]\u2190 \u3008o, i\u3009 11: else if rm then 12: i \u223c U (1, l) 13: S[i]\u2190 \u3008o, i\u2032\u3009 where S[1] = \u3008o, i\u3009, i\u2032 \u223c I(o) 14: shuffle(S)\n1. Finally, we sample two more objects that are compatible both with the original attribute 1 and one of the two attributes.\nAlgorithm 2 defines the sampling procedure formally. We sample the first triple randomly (l. 2). Then we sample two two compatible attributes for this object (l. 3), and one more object for each attribute (l. 4). This yields a set of six confounders (l. 5\u201310). After sampling the length of the final sequence l (l. 11), we build the sequence from the first triple and l \u2212 1 confounders (l. 12\u201313), with the first triple as query (l. 14). The treatment of the anomalies is exactly as before.\nar X\niv :1\n60 6.\n08 77\n7v 1\n[ cs\n.C L\n] 2\n8 Ju\nn 20\n16"}, {"heading": "3 Statistics on the Datasets", "text": "Table 1 shows statistics on the dataset. The first line covers the Object-Only dataset. Objects occur on average 90 times in the train portion of ObjectOnly, specific images only twice; the numbers for the test set are commensurately lower. While all objects in the test set are seen during training, 23% of the images are not. Due to the creation by random sampling, a minimal number of sequences is repeated (5 sequences occur twice in the training set, 1 four times) and shared between training and validation set (1 sequence). All other sequences occur just once.\nThe second line covers the Object+Attribute dataset. The average frequencies for objects and object images mirror those in Object-Only quite closely. The new columns on object-attribute (O+A) and object-attribute-image (O+A+I) combinations show that object-attribute combinations occur relatively infrequently (each object is paired with many attributes) but that the combination is considerably restricted (almost no combinations are new in the test set). The full entity representations (object-attributeimage triples), however, are very infrequent (average frequency just above 1), and more than 80% of these are unseen in the test set. A single sequence occurs twice in the test set, all others once; one sequence is shared between train and test.\nAlgorithm 2 Creation of Object+Attribute dataset Input: Sequence length interval [i \u2265 2, j]; Set of objects\nO = {o1, . . . , on}, sets of associated images I(o) and associated abstract attributes A(o) for each object o; probability of missing-anomalies P0; probability of\nmultiple-anomalies Pm. Output: \u3008object-attribute query q, object-image-attribute\nsequence S\u3009 1: S \u2190 [], Sc \u2190 [] 2: o1 \u223c O, a1 \u223c A(o1), i1 \u223c I(o1) 3: a2, a3 \u223c A(o1) so that a1 6= a2 6= a3 4: o2 \u223c A\u22121(m2), o3 \u223c A\u22121(m3) 5: Sc[1]\u2190 \u3008a2, o1, i \u223c I(o1)\u3009 6: Sc[2]\u2190 \u3008a1, o2, i \u223c I(o2)\u3009 7: Sc[3]\u2190 \u3008a2, o2, i \u223c I(o2)\u3009 8: Sc[4]\u2190 \u3008a3, o1, i \u223c I(o1)\u3009 9: Sc[5]\u2190 \u3008a1, o3, i \u223c I(o3)\u3009\n10: Sc[6]\u2190 \u3008a3, o3, i \u223c I(o3)\u3009 11: l \u223c U (i, j) 12: S[1]\u2190 \u3008o1, a2, i1\u3009 13: S[2..l] \u2190 sample candidates from Sc w.o. replace-\nment 14: q \u2190 S[1] 15: r0 \u223c Bern(p0), rm \u223c Bern(p0 + pm) 16: if r0 then 17: o\u2032 \u223c O, a\u2032 \u223c A(o), i\u2032 \u223c I(o) so that \u3008o\u2032, a\u2032\u3009 6= q 18: S[0]\u2190 \u3008a\u2032, o\u2032, i\u3009 19: else if rm then 20: i \u223c U (1, l) 21: S[i]\u2190 \u3008a, o, i\u2032\u3009 where S[1] = \u3008a, o, i\u3009, i\u2032 \u223c I(o) 22: shuffle(S)"}, {"heading": "4 Hyperparameter Tuning", "text": "We tuned the following hyperparameters on the Object-Only validation set and re-used them for Object+Attribute without further tuning (except for the Pipeline heuristics\u2019 thresholds). Chosen values are given in parentheses.\n\u2022 PoP: multimodal embedding size (300), anomaly sensor size (100), nonlinearities \u03c8 (relu) and \u03c6 (sigmoid), learning rate (0.09), epoch count (14).\n\u2022 TRPoP: same settings, except epoch count (36).\n\u2022 Pipeline: multimodal embedding size (300), margin size (0.5), learning rate (0.09), maximum similarity threshold (0.1 for Object-Only, 0.4 for Object+Attribute), top-two similarity difference threshold (0.05 and 0.07).\nMomentum was set to 0.09, learning rate decay to 1E-4 for all models, based on informal preliminary experimentation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR Conference Track,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. contextpredicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Grounding distributional semantics in the visual world", "author": ["Marco Baroni"], "venue": "Language and Linguistics Compass,", "citeRegEx": "Baroni.,? \\Q2016\\E", "shortCiteRegEx": "Baroni.", "year": 2016}, {"title": "Wide-coverage semantic representations from a CCG parser", "author": ["Bos et al.2004] Johan Bos", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "In Proceedings of the COLING,", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Concreteness ratings for 40 thousand generally known English word lemmas", "author": ["Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior Research Methods,", "citeRegEx": "Brysbaert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "Image retrieval: ideas, influences, and trends of the new age", "author": ["Datta et al.2008] Ritendra Datta", "Dhiraj Joshi", "Jia Li", "James Wang"], "venue": "ACM Computing Surveys,", "citeRegEx": "Datta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Datta et al\\.", "year": 2008}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Visual Turing test for computer vision systems", "author": ["Geman et al.2015] Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Grounded semantic composition for visual scenes", "author": ["Gorniak", "Roy2004] Peter Gorniak", "Deb Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gorniak et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gorniak et al\\.", "year": 2004}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Mikolov2015] Armand Joulin", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "ReferItGame: Referring to objects in photographs of natural scenes", "author": ["Vicente Ordonez", "Mark Matten", "Tamara Berg"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kazemzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution", "author": ["Kennington", "Schlangen2015] Casey Kennington", "David Schlangen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kennington et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kennington et al\\.", "year": 2015}, {"title": "Computational generation of referring expressions: A survey", "author": ["Krahmer", "van Deemter2012] Emiel Krahmer", "Kees van Deemter"], "venue": "Computational Linguistics,", "citeRegEx": "Krahmer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krahmer et al\\.", "year": 2012}, {"title": "Formal semantics for perceptual classification", "author": ["Staffan Larsson"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Larsson.,? \\Q2015\\E", "shortCiteRegEx": "Larsson.", "year": 2015}, {"title": "2015a. Hubness and pollution: Delving into cross-space mapping for zero-shot learning", "author": ["Georgiana Dinu", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Generation and comprehension of unambiguous object descriptions", "author": ["Mao et al.2016] Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan Yuille", "Kevin Murphy"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Mao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2016}, {"title": "Learning from unscripted deictic gesture and language for human-robot interactions", "author": ["Liefeng Bo", "Luke Zettlemoyer", "Dieter Fox"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Matuszek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2014}, {"title": "Neural Networks and Deep Learning. Determination Press, New York. Published online: http:// neuralnetworksanddeeplearning.com", "author": ["Michael Nielsen"], "venue": null, "citeRegEx": "Nielsen.,? \\Q2015\\E", "shortCiteRegEx": "Nielsen.", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2015] Karen Simonyan", "Andrew Zisserman"], "venue": "In Proceedings of ICLR Conference Track,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Look, some green circles!\u201d", "author": ["Sandro Pezzelle", "Angeliki Lazaridou", "Aur\u00e9lie Herbelot", "Gemma Boleda", "Raffa Bernardi"], "venue": null, "citeRegEx": "Sorodoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "Coordinating perceptually grounded categories through language: A case study for colour", "author": ["Steels", "Belpaeme2005] Luc Steels", "Tony Belpaeme"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "Steels et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Steels et al\\.", "year": 2005}, {"title": "Endto-end memory networks. http://arxiv.org/ abs/1503.08895", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Refer efficiently: Use less informative expressions for more predictable meanings", "author": ["Tily", "Piantadosi2009] Harry Tily", "Steven Piantadosi"], "venue": "In Proceedings of the CogSci Workshop on the Production of Referring Expressions,", "citeRegEx": "Tily et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tily et al\\.", "year": 2009}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Turney", "Pantel2010] Peter D Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "WSABIE: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In Proceedings of ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "fundamental devices to ground linguistic symbols in extralinguistic reality (Harnad, 1990).", "startOffset": 76, "endOffset": 90}, {"referenceID": 4, "context": "The converse holds for logics-based approaches (Bos et al., 2004).", "startOffset": 47, "endOffset": 65}, {"referenceID": 21, "context": ", Nielsen (2015). possible referent (missing-referent anomaly) or if there is more than one (multiple-referent anomaly).", "startOffset": 2, "endOffset": 17}, {"referenceID": 7, "context": "This has been shown to produce excellent multimodal embeddings (Frome et al., 2013; Lazaridou et al., 2015a; Weston et al., 2011).", "startOffset": 63, "endOffset": 129}, {"referenceID": 29, "context": "This has been shown to produce excellent multimodal embeddings (Frome et al., 2013; Lazaridou et al., 2015a; Weston et al., 2011).", "startOffset": 63, "endOffset": 129}, {"referenceID": 16, "context": "The objects and images are sampled uniformly at random from a set of 2,000 objects and 50 ImageNet5 images per object, itself sampled from a larger dataset used in Lazaridou et al. (2015b). As the examples show, we use natural objects and images, which makes the task very challenging (even humans might wonder which image in the second row depicts", "startOffset": 164, "endOffset": 189}, {"referenceID": 5, "context": "We thus filter verbs through the concreteness norms of Brysbaert et al. (2014), retaining only those with a concreteness score of at most 2.", "startOffset": 55, "endOffset": 79}, {"referenceID": 1, "context": "Method PoP and Pipeline\u2019s input word representations are 400-dimensional cbow embeddings from Baroni et al. (2014), trained on about 2.", "startOffset": 94, "endOffset": 115}, {"referenceID": 15, "context": "Various studies in this area have proposed multimodal approaches jointly handling vision and language (Gorniak and Roy, 2004; Larsson, 2015; Matuszek et al., 2014; Steels and Belpaeme, 2005, a.o.). These papers focus on aspects of the resolution process we are not currently addressing, such as full compositionality or gesture, but they work with very limited perceptual input, such as simple shapes and colours. Probably the most relevant study in this area is the one by Kennington and Schlangen (2015). They consider visual scenes with more objects than our sequences, but more limited in nature (tables with 36 puzzle pieces).", "startOffset": 126, "endOffset": 506}, {"referenceID": 12, "context": "Some recent efforts collect and analyze large corpora of referring expressions for multimodal tasks (Kazemzadeh et al., 2014; Tily and Piantadosi, 2009).", "startOffset": 100, "endOffset": 152}, {"referenceID": 8, "context": "Our task is more distantly related to visual question answering (Geman et al., 2015; Malinowski and Fritz, 2014; Ren et al., 2015), in the sense that we model one specific type of question that could easily be asked about an image.", "startOffset": 64, "endOffset": 130}, {"referenceID": 22, "context": "Our task is more distantly related to visual question answering (Geman et al., 2015; Malinowski and Fritz, 2014; Ren et al., 2015), in the sense that we model one specific type of question that could easily be asked about an image.", "startOffset": 64, "endOffset": 130}, {"referenceID": 6, "context": "Our task can finally also be seen as a special case of the much broader problem of content-based image retrieval (Datta et al., 2008).", "startOffset": 113, "endOffset": 133}, {"referenceID": 3, "context": "See Baroni (2016) for a discussion of how the problem of reference is addressed in that line of work.", "startOffset": 4, "endOffset": 18}, {"referenceID": 24, "context": ", corresponding to all X or many X (Sorodoc et al., 2016).", "startOffset": 35, "endOffset": 57}], "year": 2016, "abstractText": "One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties.", "creator": "LaTeX with hyperref package"}}}