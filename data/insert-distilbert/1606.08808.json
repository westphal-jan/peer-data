{"id": "1606.08808", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Adaptive Training of Random Mapping for Data Quantization", "abstract": "data quantization learns encoding results of data with certain requirements, and provides a broad descriptive perspective point of many real - world applications to learn data network handling. nevertheless, the results of encoder is again usually limited to multivariate inputs with the compressed random mapping, and side variable information of binary codes are hardly to mostly depict executing the original data patterns as possible. in the literature, cosine based random quantization has attracted much more attentions due to its intrinsic bounded results. nevertheless, it usually suffers from the uncertain outputs, and information of original data results fails to be fully preserved in the reduced control codes. in this work, a novel binary embedding method, termed adaptive training quantization ( atq ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. as an adaptive learning schemes idea, the reduced phase mapping is adaptively jointly calculated with idea of data type group, while the bias of random transform is to be improved simply to hold with most biased matching information. experimental comparative results show that both the proposed method is able to obtain considerable outstanding performance compared with other random trait quantization methods.", "histories": [["v1", "Tue, 28 Jun 2016 18:15:32 GMT  (64kb)", "https://arxiv.org/abs/1606.08808v1", "6 pages, 5 figures, 15.8"], ["v2", "Fri, 26 May 2017 15:24:26 GMT  (66kb)", "http://arxiv.org/abs/1606.08808v2", "6 pages, 5 figures, 15.8"]], "COMMENTS": "6 pages, 5 figures, 15.8", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["miao cheng", "ah chung tsoi"], "accepted": false, "id": "1606.08808"}, "pdf": {"name": "1606.08808.pdf", "metadata": {"source": "CRF", "title": "Adaptive Training of Random Mapping for Data Quantization", "authors": ["Miao Cheng", "Ah Chung Tsoi"], "emails": ["cheng@outlook.com", "actsoi@must.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n08 80\n8v 2\n[ cs\n.L G\n] 2\n6 M\nay 2\nI. Introduction\nMany information processing problems refers to efficient data representation as a basic procedure of pattern analysis. As advancement in information technology, there emerges much demands on useful handling tools of big data, and resulting solutions to data proceeding. To fast retrieve required information, data quantization methods have been widely applied to find matched data with binary codes, and plays a key bridge between query and feedback in systems [1][2]. It firstly learns the most outstanding features of each data, and then transforms the intrinsic patterns into binary codes as a global description. For each query, it is straightforward to find the best matched binary codes among data set with simple \u2019xor\u2019 comparisons, which usually temps to learn the data of similar category. And computational efficiency and processing performance are able to be calculated on, with such logical bits in comparison with other metric measures. In this work, the unsupervised learning based quantization is considered, which holds ubiquitously applicable advantages over other methods, due to the fact that no label information is required.\nTill now, many the-state-of-arts unsupervised quantization solutions have been widely applied to data hashing. Among these methods, some of them learn the binary codes beginning with reduced generation of inputs for further binarization or data matching, as a basic step of random approaches.\nIn the literature, many quantization method adopts wellknown principal component analysis (PCA) [3] to find the appropriate projections for binary embedding. As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions. Spectral Hashing (SH) [6] encodes input data into low-dimensional binary ones by preserving of local structures. And the similar idea is also used to find the best quantization with iterative procedures in other methods [7]. The fact is that, the resulting codes are always falling into binary formalism as belief results of cognitive contents. In other words, the obtained codes are of sequences of binary codes {0, 1}, whatever the range of inputs are given for 0-1 replacement. Furthermore, there needs a necessary threshold to constrain and filter inputs from original value extents, and positive / negative opinion has been logically employed as a natural deterministic decision on data distribution.\nFurthermore, some specific methods, e.g., randomised-based hashing, has attracted more and more attentions in recent years as encoding features in randomised manners. To address the binarization of arbitrarily given data, random function based hashing is devised to avoid fussy deduction and calculation burden of deterministic results [8][9]. And it is believed that, such random mapping holds an intrinsic relationship with Fourier features [10]. In couple with inherence of binary codes by erasing periodic cycles of inputs, some works treat the hashing learning of data naturally with projected function values range in [\u22121, 1], which makes the bounding values naturally for binary encoders. Generally, one of its popular definitions can be referred as:\nh (x) = sgn ( f (x)) = sgn ( cos ( wT x + b )) , (1)\nwhere sgn (\u00b7) and cos (\u00b7) respectively denotes the sign function and the cosine function, w denotes the random mapping vector, while b denotes the linear offset [9]. The inspired motivation behind such items can be referred to shift-invariant transform [10], e.g.,\nk (x, y) = k (x \u2212 y) =\n\u222b\nRd p (w) e\u2212iw T (x\u2212y)dw. (2)\nThen, the random features can be approximated to cosine formalism with well-known properties of functions of a complex variable,\ncos z \u2261 eiz + e\u2212iz\n2 . (3)\nAs a general outlet, the original shift-invariant kernels are to approximate the Gaussian kernel k (x, y) = exp (\n\u2212t \u2016x \u2212 y\u201622\n)\nwith normal distribution of w. Nevertheless, the binary coding of shift-invariant kernels are considered in this work for data quantization.\nAs a result, the mapped values of each data feature are\ncalculated as\nx = [ cos ( wT1 x + b ) , cos ( wT2 x + b ) , \u00b7 \u00b7 \u00b7 , cos ( wTr x + b )]T , (4)\nthen the encoded data are obtained via the binary quantization. Obviously, the final inputs to sign function is constrained by certain cosine function mapping values. In certain works, it is also regard as a specifical hashing function with sift-invariant mapping [9], if each feature is reformed into two angular transforms. For convenience, it is simply referred to cosine quantization (CQ) in this context, associated with respective transforms of each reduced features. There are also some existing works conducting the idea of randomized selection of data samples, which however, beyond the scope of this work.\nMostly, the original w and b are randomly generated in accordance with normal gaussian distribution as a common sense. Nevertheless, there is no existing discussion on alternative choices of random mapping, and optimal solutions to learn with binary balances are unavailable for further development. In this work, the adaptive trained quantization of random inputs will be discussed, and a variant of trigonometric function based quantization is proposed to learn the optimal mapping of initial inputs in light of 2-way clustering, while the offset rotations of random mapping are to be improved with maximum binary side information.\nThe following contents of this paper is organized as follows: section 2 will present the proposed improved hashing learning with adaptive cycle rotation, section 3 will evaluate performance of the proposed method with several random hashing methods. In section 4, we draw the conclusion of this work.\nII. Adaptive Training Quantization\nSupposed there given a set of data samples, X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] \u2208 R d\u00d7n, ATQ aims to learn the linear mapping matrix W = [w1,w2, \u00b7 \u00b7 \u00b7 ,wr] \u2208 R d\u00d7r and offset b to transform the original data into the r-dimensional reduced features. As discussed foregoing, it is straightforward to randomly map the inputs into some low-dimensional space, and usually the ranged phase of results are out of ensured disciplinarian.\nActually, the outputs of angular based mapping function provides the transformed results between [\u22121, 1], and naturally matches the finally reduced binary codes, e.g., cos ( wT x + b ) \u2208 [\u22121, 1]. However, the randomly selected w and b usually fall into a random result of data distributed range, and ideal quantization of cosine-like mapping is out of solution for further optimization. Here, a two-stage modified approach is\ndevised to improve the resulting random mapping, so that more acceptable outputs are optimized to obtain the ideal quantization of CQ. To simplify depiction of discussions, Cos (\u00b7) and S in (\u00b7) are employed to denote the resulting whole vector / matrix of cosine / sine transforms of each data feature, while cos (\u00b7) and sin (\u00b7) indicate the transforms of single input."}, {"heading": "A. Adaptive Training of Linear Mapping", "text": "In terms of linear transform in pre-transform, the mapping is randomly generated for final binary coding. Nevertheless, it is considerable whether the two-side groups can be preserved in the reduced features as possible. If the class label information is ignored, the original problem may come down to a simple problem of 2-way groups. Without the supervised sense, the whole data is to be grouped into certain distribution with low self-cognitive nature in reduced space. In the literature, PCA has been widely adopted to seek for principal components of global distribution of data, and is able to preserve the main information of original data [3]. In addition, it is also competent to learn the ideal data groups of feature structures as a data decomposition method, and informative contents of data models can be preserved.\nTo address the adaptive training of linear transform, the 2- way groups based objective function is devised to improve ideal mapping of original transform. Here, the offset of linear mapping is ignored firstly, so that handling of the first stage could be smooth, however, it is to be conducted in the following step. Without loss of generality, the objective can be defined as\nJ = argmin w\nCos ( wT X ) \u03a0 ( Cos ( wT X ))T , (5)\nwhere \u03a0 denotes the alignment matrix of data set, also known as Laplacian matrix. It is generally calculated as \u03a0 = I\u2212 1 n eeT , of which I \u2208 Rn\u00d7n denotes the identity matrix and e \u2208 Rn\u00d71 denotes the vector with all \u20191\u2019 as its elements.\nThis objective is actually a kind of problems of data clusters in the principal subspaces, and has been widely applied to many self-taught applications. The original problem conducts the 2-way clustering of data, and seeks for solutions with principal decomposition of data [11]. Nevertheless, it is different from the PCA based clustering methods with the general objective function, and traditional eigen-solution is unavailable for the further results. And also, there have been some works involving iterative approach to solve optimization problem of pattern analysis [12][13]. To optimize the involved objective, the Conjugate Gradient (CG) [14] is adopted to seek for the local optimum for the general data mapping. The most advantages of CG method over other methods, e.g., Newton method [15][14], is that it does not require to calculate the second-order gradient \u22072J, and optimized steps and directions are adaptively searched in iterations [13]. More specifically, the conjugate direction of CG method is obtained by applying the Gram-Schmidt procedure to the gradient results iteratively, and step size is decided according to gradient values of the current and previous iterations.\nFor the referred objective function, its gradient can be\ncalculated as\n\u2207J = \u22122X [ Diag(S in ( wT X ) ) ] \u03a0 [ Cos ( wT X )] . (6)\nHere, Diag (\u00b7) denotes the diagonal matrix with elements of input vector on its diagonal. By re-defining the t-th gradient of objective function as gt, the CG method selects the suitable step size in the conjugate direction st \u2208 R d\u00d7r with previously obtained conjugate direction st\u22121, which is calculated as\nst = \u2212gt + \u03b8t\u22121st\u22121. (7)\nHere, \u03b8t\u22121 is computed based on gradients of original objective function gt and gt\u22121. In this work, the well-known FletcherReeves formula is adopted to calculate the (t \u2212 1)-th \u03b8t\u22121, which is defined as \u03b8t\u22121 = \u2016gt\u2016 2 / \u2016gt\u22121\u2016 2. Hence, W is updated as W=W+\u03b1st with deduced step size \u03b1 in each iteration.\nIt is well-known that step size \u03b1 decides the reduction of the objective function with updated variable and in each iteration, and make the decrease sufficient. In the literature, a popular condition of sufficient decrease can be referred to the Armijo condition [14],\nJ ( W (t+1) ) \u2212 J ( W (t) )\n\u2264 \u03bbgTt\n( W (t+1) \u2212 W (t) ) , (8)\nwhere W (t) denotes the obtained W in the t-th optimization, and 0 < \u03bb < 1. On the other hand, the stopping rule plays an important role in iterative optimization problems, and an appropriate one is able to reduce overall computational cost. For general objective optimization problems, though the global minimum is difficult to achieve, it is feasible for the limit points to be stationary, while the local minimum can be found. In the literature, gradient based stopping rule has been a successful tool for stopping iterative search, it is mainly applied to quadratic optimization, while diseased learning problem is usually unavoidable in general objective functions [13].\nTo tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13]. With the values of objective function, it can be defined as\nJ ( W (t) ) \u2212 J ( W (t+1) )\n\u01ebmax k<t\nJ ( W (k) ) \u2212 J ( W (k+1) ) , (9)\nwhere 0 < \u01eb < 1 denotes the constant of stopping rule. Obviously, the above stopping criterion focuses on the reduction of objective function value, while no gradient is needed to decide the stopping of optimization. Compared with gradient based stopping rule, it is more component for the iterative optimization of general objective function. In addition, further improved efficiency can be attainable if simple calculation is adopted for this stage, if no specific demand is required. And an alternative choice could be reached by using the randomized nonlinear component analysis methods. But the two-way group idea is considered in this work to demonstrate the possibility of adaptive training of random mapping."}, {"heading": "B. Adaptive Training of Offset", "text": "In this subsection, the choice of offset constant of linear transform in cosine random mapping is to be discussed, so that appropriate rotation of angular is possible for acceptable results of binary embedding. To the best knowledge of ours, there has been no existing work conducting such topic, and a random constant is usually referred. Indeed, the offset constant plays a weak impact on final quantization as usual concept. Nevertheless, the randomly selected offset actually cannot match the optimal projective direction with cycle rounding generally as illustrated in Fig. 1.\nAs the original bias of cosine function is determined by randomly selected offsets, the final hashing outputs may lay to an unbalanced hyperplane as a result. Then, the resulting mapping fails to maximally depict the projective values in the binary embedding as possible. In other words, the random biases are unable to make an optimal side decision of binary information for input data. This problem has been an intrinsic limitation of random hashing, and intuitively, the projective extension is required to be enlarged on horizontal direction as deterministic cosine hashing with maximum probability. This motivation has inspired this work as an initial attempt at ideal choice of mapping bias, and devise an adaptive learning of offset decision for believable binary embedding.\nGenerally, there are several projective directions are selected for linear mapping, and each of them afford the representation of data patterns in one reduced dimension. In terms of this, the original bias of linear mapping can be adjusted in every projective hyperplane, and most distinctive binary codes can be obtained in the final thresholding. To discuss conveniently,\na simple linear mapping function is referred here, and further more combination can be implementation following the similar deduction. With an optimally chosen offset b of linear mapping, the mapping function is defined as\nf (x) = cos ( wT x + b ) . (10)\nAs discussed, the ideal offset holds the efficiency of maximizing extension on projective directions. On the other hand, it only adjusts the results on every horizontal hyperplanes and pushes affective impacts for each linear mapping. That is, only the associated mapping results have the correspondence with the given offset.\nWith a given data set X = [x1, x2, \u00b7 \u00b7 \u00b7 , xn] \u2208 R d\u00d7n and rotated bias b, it is feasible to consider the related mapping side information in a square sum of row results, e.g.,\nJ (b) = argmax b\nn \u2211\ni=1\n( cos ( wT xi + b ))2\n(11)\nWith respect to such objective function, it is difficult to handle the square items, and reach available solution directly with amounts of data. In order to optimize the above objective, the objective function can be simplified by unfolding each item of the original one,\nJ (b) = argmax b\n1 2\n[\n\u2211n i=1 cos\n( 2wT xi )] cos 2b\n\u2212 1 2\n[\n\u2211n i=1 sin\n( 2wT xi )] sin 2b + n 2 .\n(12)\nBy absorbing two summation sets of trigonometric functions of wT xi and removing halfway constant, it can be further capsulated as\nJ (b) = argmax b cos (2b + arctan \u00b5). (13)\nHere, \u00b5 is the value of the ratio\n\u00b5 =\n\u2211n i=1 sin\n( 2wT xi )\n\u2211n i=1 cos ( 2wT xi ) . (14)\nClearly, the ideal b is able to be calculated directly, and deterministic result is confidently assured.\nIn accordance with the reduced formalism of optimization,\nthe ideal offset can be calculated as b = \u230a \u2212 \u00b5\n2\n\u230b\nwith cycle\nadjustment of tangent function. With the learned offset, the side information of hashing codes are able to be maximized matching with binary embedding of trigonometric function. Though such idea is pushed intuitively, the performed results are improved for most cases. The suspicion comes from the fact that two phases of adaptive learning aim to search reverse directions along total objective cost. Nevertheless, it is fine for quantization as chosed offset can only be modified with minor rotation of spherical angular, and actually, periodically repeated results is hardly to make definite conclusion with ideal exploitation of quantization of uncertain data.\nIII. Simulation Evaluation\nIn this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ. Two data sets, CIFAR10 image dataset [18] and MINIST digit database [19], are used in these experiments. Furthermore, the mean average precision (mAP) is computed in each experiments to measure performance of different algorithms. In iterative learning of W, the initial step \u03b1 is set to 1, and alterant step size \u03b2 is set to 0.5. And the constants, \u03bb of Armijo condition and \u01eb of stopping rule are set to 0.01.\nFor images in CIFAR-10 dataset, a set of 512 dimensional GIST descriptors [20] and 128 dimensional SIFT descriptors [21] are learned from every tiny image, so that each data is represented as a 640 dimensional sample vector. In the experiments, the different batches in CIFAR-10 dataset is combined to one by ignoring their groups and several subsets are randomly selected to form the aforehand hashing data,\nquery data and sequential data. For each data in MINIST, 784 dimensional gray features are used to describe its visual handwritten pattern. Similarly, the whole data are combined while the original order of data is disordered, and then lots of subsets are randomly selected to be involved into experiments.\nIn the experiment, the search performance of encoders are evaluated with two databases. For the CIFAR-10 data set, 10000 data are randomly selected and encoded with binary embedding, and then 1000 data are randomly selected to push forward matching query. For the MINIST data set, the training and testing data sets are combined into one set, respective 10000 and 1000 data are randomly selected to form the original and query data. It is noticeable that no supervised / semisupervised corporation between them is involved, and only self-taught hashing is considered for adaptive learning. The first experiment evaluates the query performance of different algorithms with fixed 50 neighbors in mAP measure, and the search results with different hashing bits are shown in Fig. 1 (a) and (b).\nAccording to the experimental results, the proposed ATQ and CQ are able to outperform all the-state-of-arts random quantization methods for the CIFAR-10 data set. And the obtained results of CQ and ATQ are quite similar to each other as increasing of encoding bits, and the tendency has been kept for large fluent data length. Other hashing methods give their results arounding mAP = 0.1, while involved data bits are used for binary embedding. LSH and SH can give better results compared with ITQ, if enough bits are used for query matching. And SH and ITQ present strong intensity of vibration with short bits, which is ameliorated with increased data bits.\nFor the MINIST data set, the results of most algorithms have vibrated strongly with different data bits. The similar dilemma also occurs in LSH, which also presents vibration for changing bits. Nevertheless, both CQ and ATQ always present stable results and the involved bits have no affection on their performance. Furthermore, the proposed ATQ method gives better results compared with original CQ, if limited binary bits are adopted to depict the data information.\nAnother experiment evaluates the performance of different algorithms with the obtained results against different number of samples used in mAP matching, and the results are shown in Fig. 2 (c) and (d). The experiments are proceeded while fixing the length of hashing bits and involved neighbors in mAP by turns. In details, different neighbors are used to calculate mAP, while 16 fixed bits are used to generate results.\nObviously, the results of different algorithms change with strong vibrations, as the involved samples in mAP is increasing stepwise. Nevertheless, SH and CQ can present much fluent results of stable performance compared with other methods. In terms of the results of CIFAR-10 data set, the ATQ and CQ work better than other methods in a higher rank, and ATQ takes the best results for most cases of different mAP. Compared with LSH and ITQ, SH gives much better results in most cases of different neighbors involved in matching query. And LSH presents the most fluctuant results with changing bits\nof quantization, among which best and worst performance is available.\nOn the other hand, the results from MINIST data set are quite different from the ones of CIFAR-10. SH presents the best performance though there is a large vibration for limited data samples used for calculation of mAP. The reason for this, can be considered as the reconstruction desire of MINIST data set with respect to intrinsic characteristics of SH. Furthermore, there are also large vibrations occurring in results of LSH and ITQ, following the ones of SH. Distinctively, CQ and ATQ are able to hold their results with stable performance with different number of involved samples in mAP. And it seems that, ATQ can give some better results over CQ if more neighbors are considered for data matching.\nIV. Conclusion and FutureWork\nAs data quantization has been quite popular with data matching and efficient query, the related data encoding technologies have been studied broad in recent years. Specially, randomized quantization has been widely studied for binary\nhashing technologies, which usually considers the linear mapping as a random transformation. As a fascinating attempt, cosine-based mapping has been widely applied to find the binary codes of linear mapping, and it is also known as a variant of Fourier features if certain data unfolding is adopted. As a novel topic, it has attract much attentions with simple idea of binary embedding. Nevertheless, it usually confronts the limitation of uncertain projections, and obtained codes usually fails to fully describe the distinctive information of original data.\nIn this work, the limitation of devised cosine hashing is tackled, of which hashing is proceeded with random linear mapping. As the projective directions and offsets are generated randomly, there may exist unbalanced results in the low-dimensional results for further cosine thresholding. As a result, an adaptive training of data quantization is proposed to improve the intrinsic limitation of cosine based encoder. In terms of linear mapping, two stage training approach is devised to learn the linear transformation and offset respectively.\nIn the first stage, the linear mapping is searched with the minimum objective of 2-way data groups. The fact is that, binary coding of data is similar to data grouping problems, e.g., clustering, more or less. And PCA has been widely applied to find the group indicator of original data as binary index if 2-way group is referred. In our work, such idea is adopted to construct the objective function so that selforganized data groups are available under an unsupervised view. For the offset of linear mapping, it presents a weak impact on cosine rotation of random mapping as a common sense. Nevertheless, it is disclosed that it can affect the final binary results as a adjustment of global results. And it is necessary to find an appropriate offset for improved results of binary embedding. In light of characteristics of cycle transformation, it is feasible to select the offset ideally so that binary side information could be preserved in the reduced codes. As a result, the obtained horizontal projections are able to preserve the maximum side information with the searched offset. According to experimental results, the proposed method is able to give an improvement on cosine random hashing, and output performance is derived with comparable results.\nWith respect to future work, it is valuable to exploit the latent performance of other trigonometric functions for encoder of linear mapping, and the computational efficiency is to be studied as a main issue. And the difference and combination of different functions are also a fascinating topic for further development on data unfolding based pattern analysis. Furthermore, the intrinsic relationship between trigonometric functions and frequency analysis of information is to be discussed with its applications to data processing. Based on this consideration, and latent solution to adaptive learning may be available for further study of data analysis.\nAcknowledgment\nThis work was supported by Natural Science Foundation Project of Macau S.A.R., research committee of Macau University of Science and Technology, and Qingdao University.\nReferences\n[1] A. Andoni and P. Indyk, \u201cNear-optimal hashing algorithms for approximate nearest neighbor in high dimensions,\u201d Communications of The ACM, vol. 51, no. 1, pp. 117\u2013122, 2008. [2] H. Jegou, M. Douze, and C. Schmid, \u201cProduct quantization for nearest neighbor search,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 117\u2013128, 2011. [3] M. Turk and A. Pentland, \u201cEigenfaces for recognition,\u201d Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 71\u201386, 1991. [4] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni, \u201cLocality-sensitive hashing scheme based on p-stable distributions,\u201d in Proceedings of the ACM Symposium on Computational Geometry, 2004. [5] M. S. Charikar, \u201cSimilarity estimation techniques from rounding algorithms,\u201d in STOC, 2002, pp. 380\u2013388. [6] Y. Weiss, A. Torralba, and R. Fergus, \u201cSpectral hashing,\u201d in Neural Information Processing Systems, vol. 21, 2009, pp. 1753\u20131760. [7] Y. Gong and S. Lazebnik, \u201cIterative quantization: A procrustean approach to learning binary codes,\u201d in IEEE International Conference on Computer Vision and Pattern Recognition, 2011. [8] A. Rahimi and B. Recht, \u201cRandom features for large-scale kernel machines,\u201d in Neural Information Processing Systems, 2007. [9] M. Raginsky and S. Lazebnik, \u201cLocality-sensitive binary codes from shift-invariant kernels,\u201d in Neural Information Processing Systems, 2009. [10] D. Lopez-Paz, S. Sra, A. J. Smola, Z. Ghahramani, and B. Scho\u0308lkopf, \u201cRandomized nonlinear component analysis,\u201d in Proceedings of International Conference on Machine Learning, 2014. [11] C. Ding and X. He, \u201cK-means clustering via principal component analysis,\u201d in Proceedings of International Conference on Machine Learning, 2004. [12] N. Quadrianto and C. H. Lampert, \u201cLearning multi-view neighborhood preserving projections,\u201d in Proceedings of International Conference on Machine Learning, 2011. [13] M. Cheng, C.-M. Pun, and Y. Y. Tang, \u201cNonnegative class-specific entropy component analysis with adaptive step search criterion,\u201d Pattern Analysis and Applications, vol. 17, no. 1, pp. 113\u2013127, 2014. [14] D. P. Bertsekas, Nonlinear Programming, A. Scientific, Ed. Belmont, 1999. [15] C. T. Kelley, Iterative Methods for Optimization, ser. Frountiers in Applied Mathematics. SIAM, 1999. [16] J. J. More\u0301 and G. Toraldo, \u201cOn the solution of large quadratic programming problems with bound constraints,\u201d SIAM Journal of Optimization, vol. 1, no. 1, pp. 93\u2013113, 1991. [17] R. Zdunek and A. Cichocki, \u201cNonnegative matrix factorization with constrained second-order optimization,\u201d Signal Processing, vol. 87, no. 8, pp. 1904\u20131916, 2007. [18] A. Krizhevsky, \u201cLearning multiple layers of features from tiny images,\u201d University of Toronto, Tech. report, 2009. [19] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d in Proceedings of the IEEE, vol. 86, 1998, pp. 2278\u20132324. [20] A. Oliva and A. Torralba, \u201cModeling the shape of the scene: a holistic representation of the spatial envelope,\u201d International Journal of Computer Vision, vol. 42, no. 3, pp. 145\u2013175, 2001. [21] D. G. Lowe, \u201cDistinctive image features from scale-invariant keypoints,\u201d International Journal of Computer Vision, vol. 60, no. 2, pp. 91\u2013110, 2004."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Communications of The ACM, vol. 51, no. 1, pp. 117\u2013122, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 117\u2013128, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 71\u201386, 1991.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "Proceedings of the ACM Symposium on Computational Geometry, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC, 2002, pp. 380\u2013388.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Neural Information Processing Systems, vol. 21, 2009, pp. 1753\u20131760.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE International Conference on Computer Vision and Pattern Recognition, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Neural Information Processing Systems, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Neural Information Processing Systems, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Randomized nonlinear component analysis", "author": ["D. Lopez-Paz", "S. Sra", "A.J. Smola", "Z. Ghahramani", "B. Sch\u00f6lkopf"], "venue": "Proceedings of International Conference on Machine Learning, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "K-means clustering via principal component analysis", "author": ["C. Ding", "X. He"], "venue": "Proceedings of International Conference on Machine Learning, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning multi-view neighborhood preserving projections", "author": ["N. Quadrianto", "C.H. Lampert"], "venue": "Proceedings of International Conference on Machine Learning, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonnegative class-specific entropy component analysis with adaptive step search criterion", "author": ["M. Cheng", "C.-M. Pun", "Y.Y. Tang"], "venue": "Pattern Analysis and Applications, vol. 17, no. 1, pp. 113\u2013127, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Iterative Methods for Optimization, ser", "author": ["C.T. Kelley"], "venue": "Frountiers in Applied Mathematics. SIAM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "On the solution of large quadratic programming problems with bound constraints", "author": ["J.J. Mor\u00e9", "G. Toraldo"], "venue": "SIAM Journal of Optimization, vol. 1, no. 1, pp. 93\u2013113, 1991.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Nonnegative matrix factorization with constrained second-order optimization", "author": ["R. Zdunek", "A. Cichocki"], "venue": "Signal Processing, vol. 87, no. 8, pp. 1904\u20131916, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1904}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "University of Toronto, Tech. report, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, 1998, pp. 2278\u20132324.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision, vol. 42, no. 3, pp. 145\u2013175, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "To fast retrieve required information, data quantization methods have been widely applied to find matched data with binary codes, and plays a key bridge between query and feedback in systems [1][2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 1, "context": "To fast retrieve required information, data quantization methods have been widely applied to find matched data with binary codes, and plays a key bridge between query and feedback in systems [1][2].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "In the literature, many quantization method adopts wellknown principal component analysis (PCA) [3] to find the appropriate projections for binary embedding.", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "Spectral Hashing (SH) [6] encodes input data into low-dimensional binary ones by preserving of local structures.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "And the similar idea is also used to find the best quantization with iterative procedures in other methods [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "To address the binarization of arbitrarily given data, random function based hashing is devised to avoid fussy deduction and calculation burden of deterministic results [8][9].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "To address the binarization of arbitrarily given data, random function based hashing is devised to avoid fussy deduction and calculation burden of deterministic results [8][9].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "And it is believed that, such random mapping holds an intrinsic relationship with Fourier features [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "where sgn (\u00b7) and cos (\u00b7) respectively denotes the sign function and the cosine function, w denotes the random mapping vector, while b denotes the linear offset [9].", "startOffset": 161, "endOffset": 164}, {"referenceID": 9, "context": "The inspired motivation behind such items can be referred to shift-invariant transform [10], e.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "In certain works, it is also regard as a specifical hashing function with sift-invariant mapping [9], if each feature is reformed into two angular transforms.", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "In the literature, PCA has been widely adopted to seek for principal components of global distribution of data, and is able to preserve the main information of original data [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 10, "context": "The original problem conducts the 2-way clustering of data, and seeks for solutions with principal decomposition of data [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "And also, there have been some works involving iterative approach to solve optimization problem of pattern analysis [12][13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "And also, there have been some works involving iterative approach to solve optimization problem of pattern analysis [12][13].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": ", Newton method [15][14], is that it does not require to calculate the second-order gradient \u2207J, and optimized steps and directions are adaptively searched in iterations [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": ", Newton method [15][14], is that it does not require to calculate the second-order gradient \u2207J, and optimized steps and directions are adaptively searched in iterations [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "In the literature, gradient based stopping rule has been a successful tool for stopping iterative search, it is mainly applied to quadratic optimization, while diseased learning problem is usually unavoidable in general objective functions [13].", "startOffset": 240, "endOffset": 244}, {"referenceID": 14, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 220, "endOffset": 224}, {"referenceID": 0, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 184, "endOffset": 187}, {"referenceID": 3, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 195, "endOffset": 198}, {"referenceID": 6, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 204, "endOffset": 207}, {"referenceID": 16, "context": "Two data sets, CIFAR10 image dataset [18] and MINIST digit database [19], are used in these experiments.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Two data sets, CIFAR10 image dataset [18] and MINIST digit database [19], are used in these experiments.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "For images in CIFAR-10 dataset, a set of 512 dimensional GIST descriptors [20] and 128 dimensional SIFT descriptors [21] are learned from every tiny image, so that each data is represented as a 640 dimensional sample vector.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "For images in CIFAR-10 dataset, a set of 512 dimensional GIST descriptors [20] and 128 dimensional SIFT descriptors [21] are learned from every tiny image, so that each data is represented as a 640 dimensional sample vector.", "startOffset": 116, "endOffset": 120}], "year": 2017, "abstractText": "Data quantization learns encoding results of data with certain requirements, and provides a broad perspective of many real-world applications to data handling. Nevertheless, the results of encoder is usually limited to multivariate inputs with the random mapping, and side information of binary codes are hardly to mostly depict the original data patterns as possible. In the literature, cosine based random quantization has attracted much attentions due to its intrinsic bounded results. Nevertheless, it usually suffers from the uncertain outputs, and information of original data fails to be fully preserved in the reduced codes. In this work, a novel binary embedding method, termed adaptive training quantization (ATQ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. As an adaptive learning idea, the reduced mapping is adaptively calculated with idea of data group, while the bias of random transform is to be improved to hold most matching information. Experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods.", "creator": "LaTeX with hyperref package"}}}