{"id": "1606.09058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "A Distributional Semantics Approach to Implicit Language Learning", "abstract": "in the present paper we show that distributional information is particularly important when then considering concept availability under implicit language reward learning conditions. when based now on results measured from different behavioural experiments we argue that the implicit learnability dependent of semantic regularities depends on the degree to which the relevant concept is reflected in language processing use. in our simulations, we train a vector - space model drawn on either an english or a chinese corpus and then feed the resulting representations to a feed - forward neural network. the task of the classical neural network was placed to find a mapping between the word representations and the novel words. using explicit datasets from four language behavioural theory experiments, which used different semantic manipulations, we were able to instantly obtain learning patterns very similar to those obtained by humans.", "histories": [["v1", "Wed, 29 Jun 2016 12:08:51 GMT  (676kb,D)", "http://arxiv.org/abs/1606.09058v1", "5 pages, 7 figures, NetWords 2015"]], "COMMENTS": "5 pages, 7 figures, NetWords 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["dimitrios alikaniotis", "john n williams"], "accepted": false, "id": "1606.09058"}, "pdf": {"name": "1606.09058.pdf", "metadata": {"source": "CRF", "title": "A Distributional Semantics Approach to Implicit Language Learning", "authors": ["Dimitrios Alikaniotis", "John N. Williams"], "emails": ["da352@cam.ac.uk", "jnw12@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Vector-space models of semantics (VSMs) derive word representations by keeping track of the cooccurrence patterns of each word when found in large linguistic corpora. By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al., 2006). A common criticism addressed towards such models is that those co-occurrence patterns do not explicitly encode specific semantic features unlike more traditional models of semantic memory (Collins and Quillian, 1969; Rogers and McClelland, 2004). Recently, however, corpus studies (Bresnan and Hay, 2008; Hill et al., 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as ani-\nmacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al., 2013a).\nIn the present paper we argue that distributional characteristics of words are particularly important when considering concept availability under implicit language learning conditions. Studies on implicit learning of form-meaning connections have highlighted that during the learning process a restricted set of conceptual distinctions are available such as those involving animacy and concreteness. For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne. They were explicitly told that they functioned like the article \u2018the\u2019 but that gi and ro were used with near objects and ro and ne with far objects. What they were not told was that gi and ul were used with living things and ro and ne with non-living things. Participants were exposed to grammatical determinernoun combinations in a training task and afterwards given novel determiner-noun combinations to test for generalisation of the hidden regularity. W and L&W report such a generalisation effect even in participants who remained unaware of the relevance of animacy to article usage \u2013 semantic implicit learning. Paciorek and Williams (2015) (P&W) report similar effects for a system in which novel verbs (rather than determiners) collocate with either abstract or concrete nouns. However, certain semantic constraints on semantic implicit learning have been obtained. In P&W generalisation was weaker when tested with items that were of relatively low semantic similarity to the exemplars received in training. In L&W Chinese participants showed implicit generalisation of a system in which determiner usage was governed by whether the noun referred to a long or flat object (corresponding to the Chinese classifier system) whereas there was no such implicit gen-\nCopyright c\u00a9 by the paper\u2019s authors. Copying permitted for private and academic purposes. In Vito Pirrelli, Claudia Marzi, Marcello Ferro (eds.): Word Structure and Word Usage. Proceedings of the NetWordS Final\nConference, Pisa, March 30-April 1, 2015, published at http://ceur-ws.org\nar X\niv :1\n60 6.\n09 05\n8v 1\n[ cs\n.C L\n] 2\n9 Ju\nn 20\n16\neralisation in native English speakers. Based on this evidence we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. By forming semantic representations of words based on their distributional characteristics we may be able to predict what would be learnable under implicit learning conditions."}, {"heading": "2 Simulation", "text": "We obtained semantic representations using the skip-gram architecture (Mikolov et al., 2013) provided by the word2vec package,1 trained with hierarchical softmax on the British National Corpus or on a Chinese Wikipedia dump file of comparable size. The parameters used were as follows: window size: B5A5, vector dimensionality: 300, subsampling threshold: t = e\u22123 only for the English corpus.\nThe skip-gram model encapsulates the idea of distributional semantics introduced above by learning which contexts are more probable for a given word. Concretely, it uses a neural network architecture, where each word from a large corpus is presented in the input layer and its context\n1https://code.google.com/p/word2vec/\n(i.e. several words around it) in the output layer. The goal of the network is to learn a configuration of weights such that when a word is presented in the input layer the nodes in the output that become more activated correspond to those words in the vocabulary, which had appeared more frequently as its context.\nAs argued above, the resulting representations will carry, by means of their distributional patterns, semantic information such as concreteness or animacy. Consistent with the above hypotheses, we predict that given a set of words in the training phase, the degree to which one can generalise to novel nouns will depend on how much the relevant concepts are reflected in the former words. If, for example, the words used during the training session do not encode animacy based on their co-occurrence statistics, albeit denoting animate nouns, then generalising to other animate nouns would be more difficult.\nIn order to examine this prediction, we fed the\nresulting semantic representations to a non-linear classifier (a feedforward neural network) the task of which was to learn to associate noun representations to determiners or verbs, depending on the study in question. During the training phase, the neural network received as input the semantic vectors of the nouns and the corresponding determiners/verbs (coded as 1-in-N binary vectors, where N is the number of novel non-words)2 in the output vector. Using backpropagation with stochastic gradient descent as the learning algorithm, the goal of the network was to learn to discriminate between grammatical and ungrammatical noun \u2013 determiner/verb combinations. We hypothesise that this could be possible if either specific features of the input representation or a combination of them contained the relevant concepts. Considering the distributed nature of our semantic representations, we explore the latter option by adding a tanh hidden layer, the purpose of which was to\n2All the studies reported use four novel non-words.\nextract non-linear combinations of features of the input vector. We then recorded the generalisation ability through time (epochs) of our classifier by simply asking what would be the probability of encountering a known determiner k with a novel word ~w by taking the softmax function:"}, {"heading": "3 Results and Discussion", "text": "If the model has been successful in learning that \u2018gi\u2019 should be activated more given animate concepts then the probability P (y = gi|~wlion) would be higher than P (y = ro|~wlion). Fig. 1 shows the performance of the classifier on the testing set of W where, in the behavioural data, selection of the grammatical item was significantly above chance in a two alternative forced choice task for the unaware group. The slopes of the gradients clearly show that on such a task the model would favour grammatical combinations as well.\nFigures 2-3 plot the results of two experiments from P&W which focused on the abstract/concrete distinction. P&W used a false memory task in the generalisation phase, measuring learning by comparing the endorsement rates between novel grammatical and novel ungrammatical verb-noun pairs. It was reasoned that if the participants had some knowledge of the system they would endorse more novel grammatical sequences. Expt 1 (Fig. 2) used generalisation items that were higher in semantic similarity to trained items than was the case in Expt 4 (Fig. 3). The behavioural results from the unaware groups (bottom rows) show that this manipulation resulted in larger grammaticality effects on familiarity judgements in Expt 1 than Expt 4, and also higher endorsements for concrete items in general in Expt 1. Our simulation was able to capture both of these effects.\nL&W Expt 3 examined the learnability of a system based on a long/flat distinction, which is reflected in the distributional patterns of Chinese but not of English. In Chinese, nouns denoting long objects have to be preceded by a specific classifier while flat object nouns by another. L&W\u2019s training phase consisted of showing to participants combinations of thin/flat objects with novel determiners, asking them to judge whether the noun was thin or flat. After a period of exposure, participants were introduced to novel determiner \u2013 noun combinations, which either followed the grammatical system (control trials) or did not (violation trials). Participants had significantly lower reaction times (Fig. 4, bottom row) when presented with a novel grammatical sequence than an ungrammatical sequence, an effect not observed in the RTs of the English participants. The corresponding re-\nsults of our simulations plotted in Fig. 4 show that indeed the regularity was learnable when the semantic model had only experienced a Chinese text, but not when it experienced the English corpus.\nWhile more direct evidence is needed to support our initial hypothesis, our results seem to point to the direction that semantic information encoded by the distributional characteristics of words when found in large corpora can be important in determining what could be implicitly learnable."}, {"heading": "Acknowledgments", "text": "The first author is supported by the Onassis Foundation. We would like to thank the three anonymous reviewers for their valuable feedback."}], "references": [{"title": "Gradient grammar: An effect of animacy on the syntax of give in New Zealand and American English", "author": ["J. Bresnan", "J. Hay"], "venue": "Lingua, 118(2):245\u2013259.", "citeRegEx": "Bresnan and Hay,? 2008", "shortCiteRegEx": "Bresnan and Hay", "year": 2008}, {"title": "Retrieval time from semantic memory", "author": ["A.M. Collins", "M.R. Quillian"], "venue": "Journal of Verbal Learning and Verbal Behavior, 8(2):240\u2013247.", "citeRegEx": "Collins and Quillian,? 1969", "shortCiteRegEx": "Collins and Quillian", "year": 1969}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Concreteness and Corpora: A Theoretical and Practical Analysis", "author": ["F. Hill", "D. Kiela", "A. Korhonen"], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75\u201383.", "citeRegEx": "Hill et al\\.,? 2013a", "shortCiteRegEx": "Hill et al\\.", "year": 2013}, {"title": "A Quantitative Empirical Analysis of the Abstract/Concrete Distinction", "author": ["F. Hill", "A. Korhonen", "C. Bentz"], "venue": "Cognitive Science, 38(1):162\u2013177.", "citeRegEx": "Hill et al\\.,? 2013b", "shortCiteRegEx": "Hill et al\\.", "year": 2013}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review, 104(2):211\u2013240.", "citeRegEx": "Landauer and Dumais,? 1997", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Crosslinguistic Differences in Implicit Language Learning", "author": ["J.H.C. Leung", "J.N. Williams"], "venue": "Studies in Second Language Acquisition, 36(4):733\u2013755.", "citeRegEx": "Leung and Williams,? 2014", "shortCiteRegEx": "Leung and Williams", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semantic generalisation in implicit language learning", "author": ["A. Paciorek", "J. Williams"], "venue": "Journal of Experimental Psychology: Learning, Memory and Cognition.", "citeRegEx": "Paciorek and Williams,? 2015", "shortCiteRegEx": "Paciorek and Williams", "year": 2015}, {"title": "Semantic Cognition: A Parallel Distributed Processing Approach", "author": ["T.T. Rogers", "J.L. McClelland"], "venue": "MIT Press.", "citeRegEx": "Rogers and McClelland,? 2004", "shortCiteRegEx": "Rogers and McClelland", "year": 2004}, {"title": "An improved model of semantic similarity based on lexical co-occurrence", "author": ["D. Rohde", "L.M. Gonnerman", "D.C. Plaut"], "venue": "Communications of the ACM.", "citeRegEx": "Rohde et al\\.,? 2006", "shortCiteRegEx": "Rohde et al\\.", "year": 2006}, {"title": "Learning without awareness", "author": ["J.N. Williams"], "venue": "Studies in Second Language Acquisition, 27:269\u2013304.", "citeRegEx": "Williams,? 2005", "shortCiteRegEx": "Williams", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al.", "startOffset": 77, "endOffset": 91}, {"referenceID": 5, "context": "By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al., 2006).", "startOffset": 164, "endOffset": 211}, {"referenceID": 10, "context": "By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al., 2006).", "startOffset": 164, "endOffset": 211}, {"referenceID": 1, "context": "A common criticism addressed towards such models is that those co-occurrence patterns do not explicitly encode specific semantic features unlike more traditional models of semantic memory (Collins and Quillian, 1969; Rogers and McClelland, 2004).", "startOffset": 188, "endOffset": 245}, {"referenceID": 9, "context": "A common criticism addressed towards such models is that those co-occurrence patterns do not explicitly encode specific semantic features unlike more traditional models of semantic memory (Collins and Quillian, 1969; Rogers and McClelland, 2004).", "startOffset": 188, "endOffset": 245}, {"referenceID": 0, "context": "Recently, however, corpus studies (Bresnan and Hay, 2008; Hill et al., 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as animacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 4, "context": "Recently, however, corpus studies (Bresnan and Hay, 2008; Hill et al., 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as animacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 3, "context": ", 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as animacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al., 2013a).", "startOffset": 183, "endOffset": 203}, {"referenceID": 9, "context": "For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne.", "startOffset": 27, "endOffset": 43}, {"referenceID": 6, "context": "For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne.", "startOffset": 51, "endOffset": 77}, {"referenceID": 6, "context": "For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne. They were explicitly told that they functioned like the article \u2018the\u2019 but that gi and ro were used with near objects and ro and ne with far objects. What they were not told was that gi and ul were used with living things and ro and ne with non-living things. Participants were exposed to grammatical determinernoun combinations in a training task and afterwards given novel determiner-noun combinations to test for generalisation of the hidden regularity. W and L&W report such a generalisation effect even in participants who remained unaware of the relevance of animacy to article usage \u2013 semantic implicit learning. Paciorek and Williams (2015) (P&W) report similar effects for a system in which novel verbs (rather than determiners) collocate with either abstract or concrete nouns.", "startOffset": 51, "endOffset": 820}, {"referenceID": 11, "context": "A ct iv at io n Williams (2005)", "startOffset": 16, "endOffset": 32}, {"referenceID": 11, "context": "Figure 1: Generalisation gradients obtained from the Williams (2005) dataset.", "startOffset": 53, "endOffset": 69}, {"referenceID": 7, "context": "We obtained semantic representations using the skip-gram architecture (Mikolov et al., 2013) provided by the word2vec package,1 trained with hierarchical softmax on the British National Corpus or on a Chinese Wikipedia dump file of comparable size.", "startOffset": 70, "endOffset": 92}, {"referenceID": 11, "context": "A ct iv at io n Paciorek & Williams (2015), exp.", "startOffset": 27, "endOffset": 43}, {"referenceID": 8, "context": "Figure 2: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp.", "startOffset": 74, "endOffset": 103}, {"referenceID": 8, "context": "Figure 2: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp. 1. The hyperparameters used were the same as in the simulation of Williams (2005).", "startOffset": 74, "endOffset": 191}, {"referenceID": 11, "context": "A ct iv at io n Paciorek & Williams (2015), exp.", "startOffset": 27, "endOffset": 43}, {"referenceID": 8, "context": "Figure 3: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp.", "startOffset": 74, "endOffset": 103}, {"referenceID": 8, "context": "Figure 3: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp. 4. The hyperparameters used were the same as in the simulation of Williams (2005).", "startOffset": 74, "endOffset": 191}, {"referenceID": 11, "context": "A ct iv at io n Leung & Williams (2014), exp.", "startOffset": 24, "endOffset": 40}, {"referenceID": 6, "context": "Figure 4: Results from Leung and Williams (2014), exp.", "startOffset": 23, "endOffset": 49}, {"referenceID": 6, "context": "Figure 4: Results from Leung and Williams (2014), exp. 3. See text for more info on the measures used. The gradients for the ungrammatical combinations are (1\u2212 grammatical). The value of the weight decay was set to \u03b3 = 0.05 while the rest of the hyperparameters used were the same as in the simulation of Williams (2005).", "startOffset": 23, "endOffset": 321}], "year": 2016, "abstractText": "In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a VectorSpace model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.", "creator": "LaTeX with hyperref package"}}}