{"id": "1606.09184", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Disease Trajectory Maps", "abstract": "medical researchers are coming to finally appreciate that many diseases are in fact complex, heterogeneous syndromes particularly composed of subpopulations that express different variants of a related complication. time series data extracted from individual electronic health records ( ehr ) offer analysts an exciting new analytic way to study subtle differences in the way these diseases progress over time. in this paper, we focus on answering two questions that can be asked using fully these comprehensive databases of time series. first, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of of freedom that account for differences in trajectories across the population. second, we want to understand how personally important clinical outcomes are associated with disease trajectories. to answer these questions, we propose the disease trajectory map ( dtm ), a novel probabilistic model that learns low - dimensional representations of sparse and irregularly sampled time series. we propose a stochastic variational mathematical inference algorithm for learning the dtm that allows the model to scale to date large modern medical datasets. to demonstrate the dtm, we analyze data collected on patients with the complex autoimmune disease, scleroderma. we find that dtm learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.", "histories": [["v1", "Wed, 29 Jun 2016 17:06:45 GMT  (1245kb,D)", "http://arxiv.org/abs/1606.09184v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.AP", "authors": ["peter schulam", "raman arora"], "accepted": true, "id": "1606.09184"}, "pdf": {"name": "1606.09184.pdf", "metadata": {"source": "CRF", "title": "Disease Trajectory Maps", "authors": ["Peter Schulam"], "emails": ["pschulam@cs.jhu.edu", "arora@cs.jhu.edu"], "sections": [{"heading": "1 Introduction", "text": "Time series data is becoming increasingly important in medical research and practice. This is due, in part, to the growing adoption of electronic health records (EHRs), which capture snapshots of an individual\u2019s state over time. These snapshots include clinical observations (apparent symptoms and vital sign measurements), laboratory test results, and treatment information. In parallel, medical researchers are beginning to recognize and appreciate that many diseases are in fact complex, highly heterogeneous syndromes [Craig, 2008] and that individuals may belong to disease subpopulations or subtypes that express similar sets of symptoms over time (see e.g. Saria and Goldenberg [2015]). Examples of such diseases include asthma [L\u00f6tvall et al., 2011], autism [Wiggins et al., 2012], and COPD [Castaldi et al., 2014]. The data captured in EHRs can help better understand these complex diseases. EHRs contain a multitude of types of observations and the ability to track their progression can help bring in to focus the subtle differences across individual disease expression.\nIn this paper, we focus on two exploratory questions that we can begin to answer using repositories of biomedical time series data. First, we want to discover whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences across a heterogeneous population. A better understanding of the types of trajectories and how they differ can yield insights into the biological underpinnings of the disease. In turn, this may motivate new targeted therapies. In the clinic, physicians can analyze an individual\u2019s clinical history to better understand the \u201cflavor\u201d of the disease being expressed and can use this knowledge to make more accurate prognoses and guide treatment decisions. Second, we would like to know whether individuals with similar clinical outcomes (e.g. death, severe organ damage, or development of comorbidities) have similar disease trajectories. In complex diseases, individuals are often at risk of developing a number of severe complications and clinicians rarely have access to accurate prognostic biomarkers. Discovering associations between target outcomes and trajectory patterns\nar X\niv :1\n60 6.\n09 18\n4v 1\n[ st\nat .M\nL ]\n2 9\nJu n\n20 16\nmay both generate new hypotheses regarding the causes of these outcomes and help clinicians to better anticipate the event using an individual\u2019s clinical history.\nContributions. Our approach to simultaneously answering these questions is to embed individual disease trajectories into a low-dimensional vector space wherein similarity in the embedded space implies that two individuals have similar trajectories. Such an embedding would naturally answer our first question, and the results could also be used to answer the second by comparing distributions over embeddings across groups defined by different outcomes. To produce such an embedding, we introduce a novel probabilistic model of biomedical time series data, which we term the Disease Trajectory Map (DTM). In particular, the DTM models the trajectory of a single clinical marker, which is an observation or measurement recorded over time by clinicians that are used to track the progression of a disease (see e.g. Schulam et al. [2015]). Examples of clinical markers are pulmonary function tests or creatinine laboratory test results, which track lung and kidney function respectively. The DTM discovers low-dimensional (2D or 3D) latent representations of clinical marker trajectories that are easy to visualize. Moreover, the model learns an expressive family of distributions over trajectories that is parameterized by the low-dimensional representation. This allows the DTM to capture a wide variety of trajectory shapes, making it suitable for studying complex diseases where expression varies widely across the population. We describe a stochastic variational inference algorithm for estimating the posterior distribution over the parameters and individual-specific representations, which allows our model to be easily applied to large biomedical datasets. To demonstrate the DTM, we analyze clinical marker data collected on individuals with the complex autoimmune disease scleroderma (see e.g. Allanore et al. [2015]). We find that the learned representations capture interesting subpopulations consistent with previous findings, and that the representations suggest associations with important clinical outcomes."}, {"heading": "1.1 Background and Related Work", "text": "Clinical marker data extracted from EHRs is a by-product of an individual\u2019s interactions with the healthcare system. As a result, the time series are often irregularly sampled (the time between samples varies within and across individuals), and may be extremely sparse (it is not unusual to have a single observation for an individual). To aid the following discussion, we briefly introduce notation for this type of data. We use m to denote the number of individual disease trajectories recorded in a given dataset. For each individual, we use ni to denote the number of observations. We collect the observation times for subject i into a column vector ti (sorted in non-decreasing order) and the corresponding measurements into a column vector yi: ti , [ti1, . . . , tini ]\n> and yi , [yi1, . . . , yini ]\n>. Our goal is to embed the pair (ti,yi) into a low-dimensional vector space wherein similarity between two embeddings (xi,xj) implies that the trajectories have similar shapes. This is commonly done using basis representations of the trajectories. Fixed basis representations. In the statistics literature, trajectory data is often referred to as unbalanced longitudinal data, and it is commonly analyzed in that community using linear mixed models (LMMs) [Verbeke and Molenberghs, 2009]. In their simplest form, LMMs assume the following probabilistic model:\nwi | \u03a3 \u223c N (0,\u03a3) , yi | Bi,wi, \u00b5, \u03c32 \u223c N (\u00b5+ Biwi, \u03c32Ini). (1)\nThe matrix Bi \u2208 Rni\u00d7d is known as the design matrix, and can be used to capture non-linear relationships between the observation times ti and measurements yi. Its rows are comprised of d-dimensional basis expansions of each observation time Bi = [b(ti1), \u00b7 \u00b7 \u00b7 ,b(tini)]>. Common choices of b(\u00b7) include polynomials, splines, wavelets, and Fourier series. The particular basis used is often carefully crafted by the analyst depending on the nature of the trajectories and on the desired structure (e.g. invariance to translations and scaling) in the representation [Brillinger, 2001]. The design matrix can therefore make the LMM remarkably flexible despite its simple parametric probabilistic assumptions. Moreover, the prior over wi and the conjugate likelihood make it straightforward to fit \u00b5, \u03a3, and \u03c32 using EM or Bayesian posterior inference.\nAfter estimating the model parameters, we can estimate the coefficients wi of a given clinical marker trajectory using the posterior distribution, which embeds the trajectory in a Euclidean space. To flexibly capture complex trajectory shapes, however, the basis must be high-dimensional, which makes interpretability of the embeddings challenging. We can use low-dimensional summaries such as the projection on to a principal subspace, but these are not necessarily substantively meaningful. Indeed, much research has gone into developing principal direction post-processing techniques (e.g. Kaiser [1958]) or alternative estimators that enhance interpretability (e.g. Carvalho et al. [2012]).\nData-adaptive basis representations. A set of related, but more flexible, techniques comes from functional data analysis where observations are functions (i.e. trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation. The mixed effect model often suffers from numerical instability of covariance matrices in high dimensions; James et al. [2000] addressed this by constraining the rank of the covariance matrices\u2014this is often referred to as the reduced rank model. The reduced rank model was further extended by Zhou et al. [2008] to a two-dimensional sparse principal component model. Although the reduced rank model embeds trajectories using a data-driven basis, the basis is restricted to lie in a linear subspace of a fixed basis, which may be overly restrictive. Other approaches to learning a functional basis include Bayesian estimation of B-spline parameters (e.g. [Bigelow and Dunson, 2012]) and placing priors over reproducing kernel Hilbert spaces (e.g. [MacLehose and Dunson, 2009]). Although flexible, these two approaches do not learn a compact representation. Cluster-based representations. Mixture models and clustering approaches are also commonly used to represent and discover structure in time series data. Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership. LMMs have also been extended to have nonparametric Dirichlet process priors over the coefficients (e.g. Kleinman and Ibrahim [1998]), which implicitly induce clusters in the data. Although these approaches flexibly model trajectory data, the structure they recover is a partition, which does not allow us to compare all trajectories in a coherent way as we can in a vector space. Lexicon-based representations. Another line of research has investigated the discovery of motifs or repeated patterns in continuous time-series data for the purposes of succinctly representing the data as a string of elements of the discovered lexicon. These include efforts in the speech processing community to identify sub-word units (parts of the words at the same level as phonemes) in a datadriven manner [Varadarajan et al., 2008, Levin et al., 2013]. In computational healthcare, Saria et al. [2011] propose a method for discovering deformable motifs that are repeated in continuous time-series data. These methods are, in spirit, similar to discretization approaches such as symbolic aggregate approximation (SAX) [Lin et al., 2007] and piecewise aggregate approximation (PAX) [Keogh et al., 2001] that are popular in data mining, and aim to find compact description of sequential data, primarily for the purposes of indexing, search, anomaly detection, and information retrieval. The focus in this paper is to learn representations for entire trajectories rather than discover a lexicon. Furthermore, we are interested in learning a representation for individuals and capturing latent similarities between two patients in terms of Euclidean distances in the learnt representation."}, {"heading": "2 Disease Trajectory Maps", "text": "To motivate Disease Trajectory Maps, we begin from the reduced-rank formulation of linear mixed models as proposed by James et al. [2000]. In particular, let \u00b5 \u2208 R be the marginal mean of the observations, F \u2208 Rd\u00d7q be a rank-q matrix, and \u03c32 be the variance of measurement errors. As a reminder, yi \u2208 Rni denotes the vector of observed trajectory measurements, Bi \u2208 Rni\u00d7d denotes the subject\u2019s design matrix, and xi \u2208 Rq denotes the representation or embedding of the subject. We begin with the reduced-rank conditional model: yi | Bi,xi, \u00b5,F, \u03c32 \u223c N (\u00b5 + BiFxi, \u03c32Ini). In the reduced-rank model, we assume an isotropic normal prior over xi and marginalize to obtain the observed-data log-likelihood, which is then optimized with respect to F. Here, just as was done by Lawrence [2004] to derive the GPLVM, we swap the marginalization for optimization and vice versa. By assuming a normal prior N (0, \u03b1Ik) over the rows of F and marginalizing we obtain: yi | Bi,xi, \u00b5, \u03c32, \u03b1 \u223c N (\u00b5, \u03b1\u3008xi,xi\u3009BiB>i + \u03c32Ini). (2) Note that by marginalizing over F, we induce a joint distribution over all trajectories in the dataset. Moreover, this joint distribution is a Gaussian process with mean \u00b5 and the following covariance\nfunction defined across trajectories:\nCov(yi,yj | Bi,Bj ,xi,xj , \u00b5, \u03c32, \u03b1) = \u03b1\u3008xi,xj\u3009BiB>j + I[i = j] (\u03c32Ini). (3)\nThis reformulation of the reduced-rank LMM suggests a natural alternative to learning the representations xi of the subjects. Just as in the GPLVM, we can maximize the log-probability of all trajectories with respect to the hyperparameters {\u00b5, \u03c32, \u03b1} and the representations {xi : i \u2208 [m]}. More importantly, however, the reformulation allows us to relate the representation to the basis coefficients non-linearly by using the \u201ckernel trick\u201d to reparameterize the covariance function. Let k(\u00b7, \u00b7) denote a non-linear kernel defined over the representations with parameters \u03b8, then we have:\nCov(yi,yj | Bi,Bj ,xi,xj , \u00b5, \u03c32,\u03b8) = k(xi,xj)BiB>j + I[i = j] (\u03c32Ini). (4)\nLet y , [y>1 , . . . ,y > m] > denote the column vector obtained by concatenating the measurement vectors from each trajectory. The joint distribution over y is a multivariate normal:\ny | B1:m,x1:m, \u00b5, \u03c32,\u03b8 \u223c N (\u00b5,\u03a3DTM + \u03c32In), (5)\nwhere \u03a3DTM is a full-rank covariance matrix that depends on the design matrices B1:m, the representations x1:m, and the kernel k(\u00b7, \u00b7). In particular, \u03a3DTM is a block-structured matrix with m row blocks and m column blocks. The block at the ith row and jth column is the covariance between yi and yj defined above. We complete the model by placing isotropic Gaussian priors over xi. Note that this model is similar to the Bayesian GPLVM [Titsias and Lawrence, 2010], but models functional data instead of finite-dimensional vectors."}, {"heading": "2.1 Learning and Inference in the DTM", "text": "As formulated, the model will scale poorly to large datasets. Inference within each iteration of an optimization algorithm, for example, requires storing and inverting \u03a3DTM, which requires O(n2) space andO(n3) time respectively, where n , \u2211m i=1 ni is the number of clinical marker observations. For modern datasets, where n can be in the hundreds of thousands or millions, this is unacceptable. In this section, we approximate the log-likelihood using techniques from Hensman et al. [2013] that allows us to apply stochastic variational inference (SVI) [Hoffman et al., 2013].\nRecent work in scaling Gaussian processes to large datasets focuses on the idea of inducing points [Snelson and Ghahramani, 2005, Titsias, 2009], which are a relatively small number of artificial observations of the Gaussian process that act as a bottleneck and approximately capture the information contained in the training data. Let f \u2208 Rm denote observations of the GP at inputs {xi}mi=1 and u \u2208 Rp denote inducing points at inputs {zi} p i=1. Titsias [2009] constructs the inducing points as variational parameters by introducing an augmented probability model:\nu \u223c N (0,Kpp) , f | u \u223c N (KmpK\u22121pp u, K\u0303mm), (6)\nwhere Kpp is the Gram matrix between inducing points, Kmm is the Gram matrix between observations, Kmp is the cross Gram matrix between observations and inducing points, and K\u0303mm , Kmm \u2212 KmpK\u22121pp Kpm. Titsias [2009] then marginalizes over u to construct a low-rank approximate covariance matrix, which is computationally cheaper to invert using the Woodbury identity. Hensman et al. [2013] extends these ideas by maintaining a variational distribution over u that d-separates the observations and satisfies the conditions required to apply SVI [Hoffman et al., 2013]. Let yf = f + where is iid Gaussian noise with variance \u03c3\n2, then the key result from Hensman et al. [2013] that we use here is the following bound:\nlog p(yf | u) \u2265 \u2211m i=1 Ep(fi|u)[log p(yfi | fi)]. (7)\nIn the interest of space, we refer the interested reader to Hensman et al. [2013] for details.\nDTM evidence lower bound. When marginalizing over the rows of F, we induced a Gaussian process over the trajectories, but by doing so we implicitly induced a Gaussian process over the subject-specific basis coefficients. Let wi , Fxi \u2208 Rd denote the curve weights implied by the mapping F and representation xi, and let w:,k for k \u2208 [d] denote the kth coefficient of all subjects in the dataset. After marginalizing the kth row of F and applying the kernel trick, we see that the vector of coefficients w:,k has a Gaussian process distribution with mean 0 and covariance function: Cov(wik, wjk) = \u03b1k(xi,xj). Moreover, the Gaussian processes across coefficients are statistically independent of one another. To lower bound the DTM log-likelihood, we introduce p inducing points uk for each vector of coefficients w:,k with shared inducing point inputs {zi}pi=1. To refer to all\ninducing points simultaneously, we will use U , [u1, . . . ,ud] and u to denote the \u201cvectorized\u201d form of U obtained by stacking its columns. Applying the bound in (7) we have:\nlogp(y | u,x1:m) \u2265 m\u2211 i=1 Ep(wi|u,xi)[log p(yi | wi)]\n= m\u2211 i=1 logN (yi | \u00b5+ BiU>K\u22121pp ki, \u03c32Ini)\u2212 k\u0303ii 2\u03c32 Tr[B>i Bi] , m\u2211 i=1 log p\u0303(yi | u,xi), (8)\nwhere ki , [k(xi, z1), . . . , k(xi, zp)]> and k\u0303ii is the ith diagonal element of K\u0303mm. We can then construct the variational lower bound on log p(y):\nlog p(y) \u2265 Eq(u,x1:m)[log p(y | u,x1:m)]\u2212KL(q(u,x1:m) \u2016 p(u,x1:m)) (9)\n\u2265 m\u2211 i=1 Eq(u,xi)[log p\u0303(yi | u,xi)]\u2212KL(q(u,x1:m) \u2016 p(u,x1:m)), (10)\nwhere we use the lower bound in (8). Finally, to make the lower bound concrete we specify the variational distribution q(u,x1:m) to be a product of independent multivariate normal distributions:\nq(u,x1:M ) , N (u |m,S) \u220fm i=1N (xi |mi,Si), (11)\nwhere the variational parameters to be fit are m, S, and {mi,Si}mi=1.\nStochastic optimization of the lower bound. To apply SVI, we must be able to compute the gradient of the expected value of log p\u0303(yi | u,xi) under the variational distributions. Because u and xi are assumed to be independent in the variational posteriors, we can analyze the expectation in either order. Fix xi, then we see that log p\u0303(yi | u,xi) depends on u only through the mean of the Gaussian density, which is a quadratic term in log likelihood. Because q(u) is multivariate normal, we can compute the expectation in closed form.\nEq(u)[log p\u0303(yi | u,xi)] = Eq(U)[logN (yi | \u00b5+ (Bi \u2297 k > i K \u22121 pp )u, \u03c3 2Ini)]\u2212 k\u0303ii 2\u03c32 Tr[B>i Bi]\n= logN (yi | \u00b5+ Cim, \u03c32Ini)]\u2212 1\n2\u03c32 Tr[SC>i Ci]\u2212 k\u0303ii 2\u03c32 Tr[B>i Bi],\nwhere we have defined Ci , (Bi\u2297k>i K\u22121pp ) to be the extended design matrix and\u2297 is the Kronecker product. We now need to compute the expectation of this expression with respect to q(xi), which entails computing the expectations of ki (a vector) and kik>i (a matrix). In this paper, we assume an RBF kernel, and so the elements of the vector and matrix are all exponentiated quadratic functions of xi. This makes the expectations straightforward to compute given that q(xi) is multivariate normal.1 We therefore see that the expected value of log p\u0303(yi) can be computed in closed form under the assumed variational distribution.\nWe use the standard SVI algorithm to optimize the lower bound. We subsample the data, optimize the likelihood of each example in the batch with respect to the variational parameters over the representation (mi, Si), and compute approximate gradients of the global variational parameters (m, S) and the hyperparameters. The likelihood term is conjugate to the prior over u, and so we can compute the natural gradients with respect to the global variational parameters m and S [Hoffman et al., 2013, Hensman et al., 2013]. Additional details on the approximate objective and the gradients required for SVI are given in the supplement. We provide details on initialization, minibatch selection, and learning rates for our experiments in Section 3.\nInference on new trajectories. The variational distribution over the inducing point values u can be used to approximate a posterior process over the basis coefficients wi [Hensman et al., 2013]. Therefore, given a representation xi, we have that\nwik | xi,m,S \u223c N (k>i K\u22121pp mk, k\u0303ii + k > i K \u22121 pp SkkK \u22121 pp ki), (12)\nwhere mk is the approximate posterior mean of the kth column of U and Skk is its covariance. The approximate joint posterior distribution over all coefficients can be shown to be multivariate normal.\n1Other kernels can be used instead, but the expectations may not have closed form expressions.\nLet \u00b5(xi) be the mean of this distribution given representation xi and \u03a3(xi) be the covariance, then the posterior predictive distribution over a new trajectory y\u2217 given the representation x\u2217 is\ny\u2217 | x\u2217 \u223c N (\u00b5+ B\u2217\u00b5(x\u2217),B\u2217\u03a3(x\u2217)B>\u2217 + \u03c32In\u2217 . (13)\nWe can then approximately marginalize with respect to the prior over x\u2217 or a variational approximation of the posterior given a partial trajectory using a Monte Carlo estimate."}, {"heading": "3 Experiments", "text": "We now use DTM to analyze clinical marker trajectories of individuals with the autoimmune disease, scleroderma [Allanore et al., 2015]. Scleroderma is a heterogeneous and complex chronic autoimmune disease. It can potentially affect many of the visceral organs, such as the heart, lungs, kidneys, and vasculature. Any given individual may experience only a subset of complications, and the timing of the symptoms relative to disease onset can vary considerably across individuals. Moreover, there are no known biomarkers that accurately predict an individual\u2019s disease course. Clinicians and medical researchers are therefore interested in characterizing and understanding disease progression patterns. Moreover, there are a number of clinical outcomes responsible for the majority of morbidity among patients with scleroderma. These include congestive heart failure, pulmonary hypertension and pulmonary arterial hypertension, gastrointestinal complications, and myositis [Varga et al., 2012]. We use the DTM to study associations between these outcomes and disease trajectories.\nWe study two scleroderma clinical markers. The first is the percent of predicted forced vital capacity (PFVC): a pulmonary function test result measuring lung function. PFVC is recorded as percentage points, and a higher value (near 100) indicates that the individual\u2019s lungs are functioning as expected. The second clinical marker that we study is the total modified Rodnan skin score (TSS). Scleroderma is named after its effect on the skin, which becomes hard and fibrous during periods of high disease activity. Because it is the most clinically apparent symptom, many of the current sub-categorizations of scleroderma depend on an individual\u2019s pattern of skin disease activity over time [Varga et al., 2012]. To systematically monitor skin disease activity, clinicians use the TSS which is a quantitative score between 0 and 55 computed by evaluating skin thickness at 17 sites across the body (higher scores indicate more active skin disease)."}, {"heading": "3.1 Experimental Setup", "text": "For our experiments, we extract trajectories from one of nation\u2019s largest scleroderma patient registries. For both PFVC and TSS, we study the trajectory from the time of first symptom until ten years of follow-up. The PFVC dataset contains trajectories for 2,323 individuals and the TSS dataset contains 2,239 individuals. The median number of observations per individuals is 3 for the PFVC data and 2 for the TSS data. The maximum number of observations is 55 and 22 for PFVC and TSS respectively.\nWe present two sets of results. In the first, we visualize groups of similar trajectories obtained by clustering the representations learned by DTM. Although not quantitative, we use these visualizations as a way to check that the DTM uncovers subpopulations that are consistent with what is currently known about scleroderma. In the second set of results, we use the learned representations of trajectories obtained using the LMM, the reduced-rank model (FPCA) as described by James et al. [2000], and the DTM to statistically test for relationships between important clinical outcomes and learned disease trajectory representations.\nFor all experiments and all models, we use a common 5-dimensional B-spline basis composed of degree-2 polynomials (see e.g. Chapter 20 in Gelman et al. [2014]). We choose knots using the percentiles of observation times across the entire training set [Ramsay et al., 2002]. For the LMM and FPCA models, we use EM to fit model parameters. To fit the DTM, we use the LMM estimate to set the mean \u00b5 , noise \u03c32, and average the diagonal elements of \u03a3 to set the kernel scale \u03b1. Length-scales ` are set to 1. For these experiments, we do not learn the hyperparameters during optimization. We initialize the variational means over xi using the first two unit-scaled principal components of wi and set the variational covariances to be diagonal with standard deviation 0.1. For both PFVC and TSS, we use minibatches of size 25 and learn for a total of five epochs (passes over the training data). The initial learning rate for m and S is 0.1 and decays as t\u22121 for each epoch t."}, {"heading": "3.2 Qualitative Analysis of Representations", "text": "The DTM returns approximate posteriors over the representations xi for all individuals in the training set. We examine these posteriors for both the PFVC and TSS datasets to check for consistency with what is currently known about scleroderma disease trajectories. In Figure 1 (A) we show\ngroups of trajectories uncovered by clustering the learned representations, which are plotted in Figure 1 (B). Many of the groups shown here align with other work on scleroderma lung disease subtypes (e.g. Schulam et al. [2015]). In particular, we see rapidly declining trajectories (group [5]), slowly declining trajectories (group [22]), recovering trajectories (group [23]), and stable trajectories (group [34]). Surprisingly, we also see a group of individuals who we describe as \u201clate decliners\u201d (group [28]). These individuals are stable for the first 5-6 years, but begin to decline thereafter. This is surprising because the onset of scleroderma-related lung disease is currently thought to occur early in the disease course [Varga et al., 2012]. In Figure 2 (A) we show clusters of TSS trajectories and the corresponding color-coded representations in Figure 2 (B). These trajectories corroborate what is currently known about skin disease in scleroderma. In particular, we see individuals who have minimal activity (e.g. group [1]) and individuals with early activity that later stabilizes (e.g. group [11]), which correspond to what is known as the limited and diffuse variants of scleroderma [Varga et al., 2012]. We also find that there are a number of individuals with increasing activity over time (group [6]) and some whose activity remains high over the ten year period (group [19]). These patterns are not currently considered to be canonical trajectories and warrant further investigation."}, {"heading": "3.3 Associations between Representations and Clinical Outcomes", "text": "To quantitatively evaluate the low-dimensional representations learned by the DTM, we statistically test for relationships between the representations of clinical marker trajectories and important clinical outcomes. We compare the inferences of the hypothesis test with those made using representations derived from the LMM and FPCA baselines. For the LMM, we project wi into its 2-dimensional principal subspace. For FPCA, we learn a rank-2 covariance, which recovers 2-dimensional embeddings. To establish that the models are all equally expressive and achieve comparable generalization error, we present held-out data log-likelihoods in Table 1, which are estimated using 10-fold cross-validation. We see that the models are roughly equivalent with respect to generalization error.\nTo test associations between clinical outcomes and learned representations, we use a kernel density estimator test [Duong et al., 2012] to test the null hypothesis that the distributions across subgroups with and without the outcome are equivalent. The p-values obtained are listed in Table 2. As a point of reference, we include two clinical outcomes that should be clearly related to the two clinical\nLMM -17.59 (\u00b1 1.18) -3.95 (\u00b1 0.04) -13.63 (\u00b1 1.41) -3.47 (\u00b1 0.05) FPCA -17.89 (\u00b1 1.19) -4.03 (\u00b1 0.02) -13.76 (\u00b1 1.42) -3.47 (\u00b1 0.05) DTM -17.74 (\u00b1 1.23) -3.98 (\u00b1 0.03) -13.25 (\u00b1 1.38) -3.32 (\u00b1 0.06)\nmarkers. Interstitial lung disease is the most common cause of lung damage in scleroderma [Varga et al., 2012], and so we confirm that the null hypothesis is rejected for all three models. Similarly, for TSS we expect ulcers and gangrene to be associated with severe skin disease. In this case, only the representations learned by DTM reveal this relationship. For the remaining outcomes, we see that FPCA and DTM reveal similar associations, but that only DTM suggests a relationship with pulmonary arterial hypertension (PAH). Presence of fibrosis (which drives lung disease progression) has been shown to be a risk factor in the development of PAH (see Chapter 36 of Varga et al. [2012]), but only the representations learned by DTM corroborate this finding (see Figure 3)."}, {"heading": "4 Conclusion", "text": "We present the Disease Trajectory Map (DTM), a novel probabilistic model that learns lowdimensional embeddings of sparse and irregularly sampled clinical time series data. The DTM is a reformulation of the LMM that places an emphasis on the representations that the model learns. This view is comparable to that taken by Lawrence [2004] in deriving the Gaussian process latent variable model (GPLVM) from probabilistic principal component analysis (PPCA) [Tipping and Bishop, 1999], and indeed the DTM can be interpreted as a \u201ctwin kernel\u201d GPLVM (briefly discussed in the concluding paragraphs) over functional observations. The DTM can also be viewed as an LMM with a \u201cwarped\u201d Gaussian prior over the random effects (see e.g. Damianou et al. [2015] for a discussion of distributions induced by mapping Gaussian random variables through non-linear maps). We demonstrate the model by analyzing data extracted from one of the nation\u2019s largest scleroderma patient registries, and found that the DTM induces structure among trajectories that is consistent with previous findings and also uncovers several surprising disease trajectory shapes. We also explore associations between important clinical outcomes and the DTM\u2019s representations and found statistically significant differences in representations between outcome-defined groups that were not uncovered by two sets of baseline representations."}, {"heading": "A Derivation of Evidence Lower Bound", "text": "When marginalizing over the rows of F, we induced a Gaussian process over the trajectories, but by doing so we implicitly induced a Gaussian process over the subject-specific basis coefficients. Let wi , Fxi \u2208 Rd denote the curve weights implied by the mapping F and representation xi, and let w:,k for k \u2208 [d] denote the kth coefficient of all subjects in the dataset. After marginalizing the kth row of F and applying the kernel trick, we see that the vector of coefficients w:,k has a Gaussian process distribution with mean 0 and covariance\nCov(wik, wjk) = \u03b1k(xi,xj). (14)\nMoreover, the Gaussian processes across coefficients are mutually statistically independent of one another. To construct our approximate objective, we first approximate each of the d coefficient Gaussian processes by introducing p inducing points (see e.g. Snelson and Ghahramani [2005], Titsias [2009]) with values uk \u2208 Rp for each k \u2208 [d] observed at common inputs zi \u2208 Rq for i \u2208 [p]. We assume that each w:,k and uk are sampled from a common Gaussian process, which implies the joint distribution:\nuk | \u03b8 \u223c N (0,Kpp) (15) wk | uk,\u03b8 \u223c N (KmpK\u22121pp uk, K\u0303mm). (16)\nwhere Kpp is the Gram matrix between inducing points, Kmm is the Gram matrix between subjects (based on their representations xi), Kmp is the cross Gram matrix between subjects and inducing points, and K\u0303mm , Kmm \u2212KmpK\u22121pp Kpm.\nNow, we stack the inducing point values u1:d into the columns of a matrix U , [u1, . . . ,ud]. We will use u to denote the \u201cvectorization\u201d of U obtained by stacking the columns. Each row i of U can be thought of as the vector of coefficients belonging to a single inducing subject which has an associated representation zi \u2208 Rq . Let y , [y>1 , . . . ,y>m]> be the vector of concatenated trajectories and W be the matrix containing subject i\u2019s coefficients wi in each row, then following the derivation of Hensman et al. [2013], we can lower bound the conditional log-probability of y given u and x1:m:\nlog p(y | u,x1:m) = log \u222b p(y |W)p(W | u,x1:m)dW (17)\n= log \u222b m\u220f i=1 p(yi | wi)p(W | u,x1:m)dW (18)\n\u2265 \u222b p(W | u,x1:m) m\u2211 i=1 log p(yi | wi)dW (19)\n= m\u2211 i=1 Ep(wi|u,xi)[log p(yi | wi)]. (20)\nThe expectation in each summand is easy to calculate because the mean of yi is linearly dependent on wi and because the conditional distribution wi given u is multivariate normal. Specifically, we have that\nwi | u,xi \u223c N (U>K\u22121pp ki, k\u0303iiId), (21)\nwhere ki is a column vector filled with the ith row of Kmp and k\u0303ii is the ith diagonal element of K\u0303mm. Together with the conditional distribution of yi given wi, we have that each summand can be written as\nEp(wi|u,xi)[log p(yi | wi)] (22)\n= \u2212ni 2 log 2\u03c0\u03c32 \u2212 1 2\u03c32 Ep(wi|u,xi)[(yi \u2212 \u00b5\u2212 Biwi) >(yi \u2212 \u00b5\u2212 Biwi)] (23)\n= logN (yi | \u00b5+ BiU>K\u22121pp ki, \u03c32Ini)\u2212 k\u0303ii 2\u03c32 Tr[B>i Bi] (24)\n, log p\u0303(yi | u,xi). (25)\nWe can now write the lower bound on the conditional log-probability as\nlog p(y | u,x1:m) \u2265 m\u2211 i=1 log p\u0303(yi | u,xi) , log p\u0303(y | u,x1:m). (26)\nTo complete the derivation of the approximate objective, we use the lower bound on log p(y | u,x1:m) to create a variational lower bound on the marginal log-probability of the trajectories\nlog p(y) = log \u222b p(y | u,x1:m)p(u,x1:m)du (27)\n\u2265 \u222b q(u,x1:m) (log p(y | u,x1:m)\u2212 log q(u,x1:m) + log p(u,x1:m)) dudx1:m (28)\n\u2265 \u222b q(u,x1:m) (log p\u0303(y | u,x1:m)\u2212 log q(u,x1:m) + log p(u,x1:m)) dudx1:m (29)\n, log p\u0303(y). (30)\nWe assume that u, x1, . . . ,xm are all mutually independent in the variational posterior. We use a multivariate normal variational approximation for each xi with variational parameters mi and Si.\nFixing xi, to find the the optimal form for q(u), note that each log p\u0303(yi | u,xi) is composed of a log-likelihood plus an additive term that is independent of u. Therefore, the terms that depend on u can be written as:\nEq(u) [ m\u2211 i=1 logN (yi | \u00b5+ BiU>K\u22121pp ki, \u03c32Ini) ] \u2212KL(q\u2016p). (31)\nNow, note that the mean in any of the log-likelihood terms can be rewritten as\n\u00b5+ BiU >K\u22121pp ki = \u00b5+ (Bi \u2297 k > i K \u22121 pp )u, (32)\nLet Ci , (Bi \u2297 k>i K\u22121pp ) denote the extended design matrix obtained through this rewriting, and recall that each column uk is normally distributed with mean zero and covariance Kpp. The prior over the vectorized matrix u is therefore also multivariate normal. The expression above is maximized when q(u) is equal to the posterior over u given the observed trajectories. Because the prior is multivariate normal and the mean of the likelihood depends linearly on u, the posterior must also be multivariate normal. Moreover, we know its exact form:\nm\u2217 = S\u2217\n( \u03c3\u22122\nm\u2211 i=1 C>i (yi \u2212 \u00b5)\n) , S\u2217 = ( \u03c3\u22122\nm\u2211 i=1 C>i Ci + (Id \u2297K\u22121pp )\n)\u22121 . (33)\nWe therefore parameterize q(u) as a multivariate normal distribution with variational parameters m and S.\nWe now derive a closed-form expression for the expectation of log p\u0303(yi | u,xi) under variational posterior distribution. Because u and xi are assumed to be independent in the variational posteriors, we can analyze the expectation in either order. Fix xi, then we see that log p\u0303(yi | u,xi) depends on u only through the mean of the Gaussian density, which is a quadratic term in log likelihood. Because q(u) is multivariate normal, we can compute the expectation in closed form.\nEq(u)[log p\u0303(yi | u,xi)] = Eq(U)[logN (yi | \u00b5+ (Bi \u2297 k > i K \u22121 pp )u, \u03c3 2Ini)]\u2212 k\u0303ii 2\u03c32 Tr[B>i Bi]\n= logN (yi | \u00b5+ Cim, \u03c32Ini)]\u2212 1\n2\u03c32 Tr[SC>i Ci]\u2212 k\u0303ii 2\u03c32 Tr[B>i Bi],\nWe can compute the expectation of Eq(u)[log p\u0303(yi | u,xi)] in closed form by noting that we need only compute expectations of ki and kik>i . Specifically, we have that\nEq(xi)[k(xi, zj)] = \u03b1\n|Si|1/2|A|1/2 exp\n{ 1\n2 (B>A\u22121b\u2212 c)\n} , (34)\nwhere A = S\u22121i + ` \u22122Iq, b = S\u22121i mi + ` \u22122zj , and c = m>i S \u22121 i m + ` \u22122z>j zj . Similarly, for the expected outer product, we have\nEq(xi)[k(xi, zj)k(xi, zk)] = \u03b1\n|Si|1/2|A|1/2 exp\n{ 1\n2 (B>A\u22121b\u2212 c)\n} , (35)\nwhere A = S\u22121i +2` \u22122Iq , b = S\u22121i mi+` \u22122zj +` \u22122zk, and c = m>i S \u22121 i m+` \u22122z>j zj +` \u22122z>k zk. Importantly, we can simply substitute these expectations into Eq(u)[log p\u0303(yi | u,xi)] and the form of the lower bound does not change (it is still a Gaussian log-likelihood plus the additional trace terms)."}, {"heading": "B Optimizing the Evidence Lower Bound", "text": "To formulate the complete objective, we use the lower bound derived above and place priors on the observation noise \u03c32, and the hyperparameters of the kernel k(\u00b7, \u00b7). In this section and in our experiments we assume that the kernel is a radial basis function (RBF) with scale \u03b1 and length-scale (or bandwidth) `. We assumelog normal distributions over \u03c32, \u03b1, and ` with mean parameters ms, ma, m` respectively and precision parameters \u03c1s, \u03c1a, and \u03c1` respectively. Our objective is therefore\nJSA-DTM(m,S,m1:m,S1:m, \u00b5, \u03c32, \u03b1, `) = (36) m\u2211 i=1 \u2212ni 2 log 2\u03c0\u03c32 \u2212 1 2\u03c32 Eq(xi)[\u2016yi \u2212 \u00b5\u2212 (Bi \u2297 k > i K \u22121 pp )m\u201622] (37)\n+ m\u2211 i=1 \u2212 1 2\u03c32 Tr[S(B>i Bi \u2297K\u22121pp Eq(xi)[kik > i ]K \u22121 pp )] (38)\n+ m\u2211 i=1 \u2212 1 2\u03c32 Tr[B>i Bi](\u03b1\u2212 Eq(xi)[k > i K \u22121 pp ki]) (39)\n\u2212 m\u2211 i=1 1 2 ( Tr[Si + mim > i ]\u2212 q \u2212 log |Si| ) (40)\n\u2212 1 2\n( Tr[(S + mm>)(Id \u2297K\u22121pp )]\u2212 pd+ log |Kpp|d\n|S|\n) (41)\n\u2212 \u03c1s 2 \u2016 log \u03c32 \u2212ms\u201622 \u2212 \u03c1a 2 \u2016 log\u03b1\u2212ma\u201622 \u2212 \u03c1` 2 \u2016 log `\u2212m`\u201622. (42)\nNote that the last three lines above can be seen as regularizers (log priors for the hyperparameters and a KL divergence between the variational distribution q and the prior p). The first four lines can be decomposed across subjects, suggesting that we can use stochastic approximation of the objective and its gradients to derive a scalable algorithm for optimizing the objective.\nWe define an iterative first-order optimization algorithm. In broad strokes, within each iteration we will sample a single subject i (or a batch of patients), maximize the objective with respect to mi and Si while holding the global variables fixed, compute the approximate gradients of the objective, and take a small step in the direction of each gradient for each parameter (the step size is determined by a learning schedule, which may be specific to each global variable). We discuss each step in detail below. We do so assuming a single sampled subject i, although in principle we can sample a batch of subjects to reduce variance in the gradient estimate.\nMaximizing wrt local variables (mi,Si). Before computing gradients of the approximate objective with respect to the global parameters, we first do a block coordinate optimization over the local variational parameters of subject i. We optimize:\nJi(xi) = (43)\n\u2212 ni 2 log 2\u03c0\u03c32 \u2212 1 2\u03c32 Eq(xi)[\u2016yi \u2212 \u00b5\u2212 (Bi \u2297 k > i K \u22121 pp )m\u201622] (44)\n\u2212 1 2\u03c32 Tr[S(B>i Bi \u2297K\u22121pp Eq(xi)[kik > i ]K \u22121 pp )] (45)\n\u2212 1 2\u03c32 Tr[B>i Bi](\u03b1\u2212 Eq(xi)[k > i K \u22121 pp ki]). (46)\nWe can optimize this expression using a gradient-based optimizer. We use the scaled conjugate gradients algorithm.\nEstimating gradients of global variables. Having sampled subject i and refit her local variational parameters, we now want to approximate the gradient of the full objective with respect to the global variables m, S, \u00b5, \u03c32, \u03b1, and `. We first look at the approximate gradient with respect to m.\n\u2207\u0302JSA-DTM(m) = Eq(xi)[ m \u03c32 (B>i \u2297K\u22121pp ki)(yi \u2212 \u00b5\u2212 (B\u2297 k > i K \u22121 pp )m)]\u2212 (Id \u2297K\u22121pp )m. (47)\nThe approximate gradient with respect to S is\n\u2207\u0302JSA-DTM(S) =\u2212 m\n2\u03c32 Tr[(B>i Bi \u2297K\u22121pp Eq(xi)[kik > i ]K \u22121 pp )] (48)\n\u2212 1 2 Tr[(Id \u2297K\u22121pp )] + 1 2 Tr[S\u22121]. (49)\nNote that if we set these approximate gradients to 0, we obtain the following estimates of m and S:\nm\u0302 = S\u0302 (m \u03c32 (B>i \u2297K\u22121pp Eq(xi)[ki])(y \u2212 \u00b5) )\n(50) S\u0302 = (m \u03c32 (B>i Bi \u2297K\u22121pp Eq(xi)[kik > i ]K \u22121 pp ) + (Id \u2297K\u22121pp ) )\u22121 (51)\nWe can improve the rate of convergence of our algorithm by taking the geometry of the space of distributions parameterized by m and S into account. We do so by using the natural gradients for these two parameters instead of the approximations above. Let \u03b81 and \u03b82 denote the canonical parameterization of the variational multivariate normal, then the gradient updates at time t are Hoffman et al. [2013]:\n\u03b8t1 = \u03b8 t\u22121 1 + \u03bbt(\u03b7 t\u22121 1 \u2212 \u03b8 t\u22121 1 ) (52) \u03b8t2 = \u03b8 t\u22121 2 + \u03bbt(\u03b7 t\u22121 2 \u2212 \u03b8 t\u22121 2 ), (53)\nwhere\n\u03b7t\u221211 = m \u03c32 (B>i \u2297K\u22121pp Eq(xi)[ki])(y \u2212 \u00b5) (54) \u03b7t\u221212 = \u2212 m\n2\u03c32 (B>i Bi \u2297K\u22121pp Eq(xi)[kik > i ]K \u22121 pp ) (55)\nTo update the hyperparamters, we need to compute the gradients with respect to \u00b5, \u03c32, \u03b1, and `. We parameterize \u03c32, \u03b1, and ` using their logarithms, and so present gradients with respect to that representation. To make the expressions more clear, we present the gradients as differentials with respect to the kernel, which can be completed using the chain rule. The estimate of the gradient with respect to \u00b5 is\n\u2207\u0302JSA-DTM(\u00b5) = m\n\u03c32 (yi \u2212 \u00b5\u2212 (Bi \u2297 Eq(xi)[k\n> i ]K \u22121 pp )m) >1ni . (56)\nThe estimate of the gradient with respect to log \u03c32 is\n\u2207\u0302JSA-DTM(log \u03c32) =\u2212 mni\n2 +\nm\n2\u03c32 Eq(xi)[\u2016yi \u2212 \u00b5\u2212 (Bi \u2297 k\n> i K \u22121 pp )m\u201622] (57)\n+ m\n2\u03c32 Tr[S(B>i Bi \u2297K\u22121pp Eq(xi)[kix > i ]K \u22121 pp )] (58)\n+ m\n2\u03c32 Tr[B>i B](\u03b1\u2212 Tr[K\u22121pp Eq(xi)[kix > i ]]) (59) \u2212 \u03c1s(log \u03c32 \u2212ms). (60)\nThe estimate of the gradient with respect to log\u03b1 is\n\u2207\u0302SA-DTM(log\u03b1) = (61) m\n\u03c32 Eq(xi)[(yi \u2212 \u00b5\u2212 Cm)\n>(Bi \u2297 \u2202k>i K\u22121pp \u2212 k > i K \u22121 pp \u2202KppK \u22121 pp )m] (62)\n\u2212 m \u03c32 Eq(xi)[Tr[SC > i (Bi \u2297 \u2202k > i K \u22121 pp \u2212 k > i K \u22121 pp \u2202KppK \u22121 pp )]] (63) \u2212 m 2\u03c32 Tr[B>i Bi]\u03b1 (64) + m\n2\u03c32 Tr[B>i Bi](2Eq(xi)[k > i ]K \u22121 pp \u2202Eq(xi)[ki]\u2212 Tr[K \u22121 pp \u2202KppK \u22121 pp Eq(xi)[kix > i ]]) (65)\n+ 1\n2\n( Tr[(S + mm>)(Id \u2297K\u22121pp \u2202KppK\u22121pp )]\u2212 dTr[K\u22121pp \u2202Kpp] ) . (66)\nThe estimate of the gradient with respect to log ` is\n\u2207\u0302SA-DTM(log `) = (67) m\n\u03c32 Eq(xi)[(yi \u2212 \u00b5\u2212 Cm)\n>(Bi \u2297 \u2202k>i K\u22121pp \u2212 k > i K \u22121 pp \u2202KppK \u22121 pp )m] (68)\n\u2212 m \u03c32 Eq(xi)[Tr[SC > i (Bi \u2297 \u2202k > i K \u22121 pp \u2212 k > i K \u22121 pp \u2202KppK \u22121 pp )]] (69) + m\n2\u03c32 Tr[B>i Bi](2Eq(xi)[k > i ]K \u22121 pp \u2202Eq(xi)[ki]\u2212 Tr[K \u22121 pp \u2202KppK \u22121 pp Eq(xi)[kix > i ]]) (70)\n+ 1\n2\n( Tr[(S + mm>)(Id \u2297K\u22121pp \u2202KppK\u22121pp )]\u2212 dTr[K\u22121pp \u2202Kpp] ) . (71)"}], "references": [{"title": "Systemic sclerosis. Nature Reviews Disease Primers, page", "author": ["Allanore"], "venue": null, "citeRegEx": "Allanore,? \\Q2015\\E", "shortCiteRegEx": "Allanore", "year": 2015}, {"title": "Time series: data analysis and theory, volume 36", "author": ["Carvalho"], "venue": "American Statistical Association,", "citeRegEx": "Carvalho,? \\Q2012\\E", "shortCiteRegEx": "Carvalho", "year": 2012}, {"title": "The varimax criterion for analytic rotation in factor analysis", "author": ["E. Keogh"], "venue": null, "citeRegEx": "Keogh,? \\Q2000\\E", "shortCiteRegEx": "Keogh", "year": 2000}, {"title": "Experiencing SAX: a novel symbolic representation", "author": ["Jessica Lin", "Eamonn Keogh", "Li Wei", "Stefano Lonardi"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2013}, {"title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering", "author": ["B.M. Marlin"], "venue": null, "citeRegEx": "Marlin,? \\Q2009\\E", "shortCiteRegEx": "Marlin", "year": 2009}, {"title": "Applied functional data analysis: methods and case studies", "author": ["James Ramsay"], "venue": "Proc. ACM SIGHIT International Health Informatics Symposium,", "citeRegEx": "Ramsay,? \\Q2012\\E", "shortCiteRegEx": "Ramsay", "year": 2012}, {"title": "Subtyping: What it is and its role in precision medicine", "author": ["S. Saria", "A. Goldenberg"], "venue": "Int. Sys.,", "citeRegEx": "Saria and Goldenberg.,? \\Q2015\\E", "shortCiteRegEx": "Saria and Goldenberg.", "year": 2015}, {"title": "Clustering longitudinal clinical marker trajectories from electronic health", "author": ["P. Schulam", "F. Wigley", "S. Saria"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Schulam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulam et al\\.", "year": 2015}, {"title": "Applications to phenotyping and endotype discovery", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "In AAAI,", "citeRegEx": "Snelson and Ghahramani.,? \\Q2005\\E", "shortCiteRegEx": "Snelson and Ghahramani.", "year": 2005}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "author": ["B. Varadarajan"], "venue": "Series B (Statistical Methodology),", "citeRegEx": "Varadarajan,? \\Q1999\\E", "shortCiteRegEx": "Varadarajan", "year": 1999}, {"title": "Linear mixed models for longitudinal data", "author": ["G. Verbeke", "G. Molenberghs"], "venue": null, "citeRegEx": "Verbeke and Molenberghs.,? \\Q2009\\E", "shortCiteRegEx": "Verbeke and Molenberghs.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Saria and Goldenberg [2015]).", "startOffset": 0, "endOffset": 28}, {"referenceID": 6, "context": "Schulam et al. [2015]).", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Allanore et al. [2015]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "In the statistics literature, trajectory data is often referred to as unbalanced longitudinal data, and it is commonly analyzed in that community using linear mixed models (LMMs) [Verbeke and Molenberghs, 2009].", "startOffset": 179, "endOffset": 210}, {"referenceID": 1, "context": "Carvalho et al. [2012]).", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation.", "startOffset": 130, "endOffset": 943}, {"referenceID": 2, "context": "trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation. The mixed effect model often suffers from numerical instability of covariance matrices in high dimensions; James et al. [2000] addressed this by constraining the rank of the covariance matrices\u2014this is often referred to as the reduced rank model.", "startOffset": 130, "endOffset": 1186}, {"referenceID": 2, "context": "trajectories) assumed to be sampled from a stochastic process and the goal is to find a parsimonious representation for the data [Ramsay et al., 2002]. Functional principal component analysis (FPCA), one of the most standard techniques in functional data analysis, expresses functional data in the orthonormal basis given by the eigenfunctions of the auto-covariance operator. This representation is optimal in the sense that no other representation captures more variation [Ramsay, 2006]. The idea itself can be traced back to early independent work by Karhunen and Loeve and is also referred to as the Karhunen-Loeve expansion [Watanabe, 1965]. While numerous variants of FPCA have been proposed, the one that is most relevant to the problem at hand is that of sparse FPCA [Castro et al., 1986, Rice and Wu, 2001] where we allow sparse irregularly sampled data as in longitudinal data analysis. To deal with the sparsity, Rice and Wu [2001] proposed the mixed effect model which leverages statistical strength from all observations for function estimation. The mixed effect model often suffers from numerical instability of covariance matrices in high dimensions; James et al. [2000] addressed this by constraining the rank of the covariance matrices\u2014this is often referred to as the reduced rank model. The reduced rank model was further extended by Zhou et al. [2008] to a two-dimensional sparse principal component model.", "startOffset": 130, "endOffset": 1372}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership.", "startOffset": 0, "endOffset": 154}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership. LMMs have also been extended to have nonparametric Dirichlet process priors over the coefficients (e.g. Kleinman and Ibrahim [1998]), which implicitly induce clusters in the data.", "startOffset": 0, "endOffset": 425}, {"referenceID": 2, "context": "Marlin et al. [2012] cluster time series data from the ICU using a mixture model and use cluster membership to predict outcomes. Schulam and Saria [2015] describe a probabilistic model that represents trajectories using a hierarchy of features, which includes \u201csubtype\u201d or cluster membership. LMMs have also been extended to have nonparametric Dirichlet process priors over the coefficients (e.g. Kleinman and Ibrahim [1998]), which implicitly induce clusters in the data. Although these approaches flexibly model trajectory data, the structure they recover is a partition, which does not allow us to compare all trajectories in a coherent way as we can in a vector space. Lexicon-based representations. Another line of research has investigated the discovery of motifs or repeated patterns in continuous time-series data for the purposes of succinctly representing the data as a string of elements of the discovered lexicon. These include efforts in the speech processing community to identify sub-word units (parts of the words at the same level as phonemes) in a datadriven manner [Varadarajan et al., 2008, Levin et al., 2013]. In computational healthcare, Saria et al. [2011] propose a method for discovering deformable motifs that are repeated in continuous time-series data.", "startOffset": 0, "endOffset": 1181}, {"referenceID": 8, "context": "Recent work in scaling Gaussian processes to large datasets focuses on the idea of inducing points [Snelson and Ghahramani, 2005, Titsias, 2009], which are a relatively small number of artificial observations of the Gaussian process that act as a bottleneck and approximately capture the information contained in the training data. Let f \u2208 R denote observations of the GP at inputs {xi}i=1 and u \u2208 R denote inducing points at inputs {zi} p i=1. Titsias [2009] constructs the inducing points as variational parameters by introducing an augmented probability model: u \u223c N (0,Kpp) , f | u \u223c N (KmpK pp u, K\u0303mm), (6)", "startOffset": 100, "endOffset": 460}, {"referenceID": 7, "context": "Schulam et al. [2015]).", "startOffset": 0, "endOffset": 22}], "year": 2016, "abstractText": "Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Time series data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of time series. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled time series. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories that the representations are significantly associated with important clinical outcomes.", "creator": "LaTeX with hyperref package"}}}