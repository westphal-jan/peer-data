{"id": "1606.09637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Lifted Region-Based Belief Propagation", "abstract": "due to the intractable nature of exact local lifted inference, research has recently focused on the discovery of accurate and efficient approximate inference algorithms in statistical relational models ( srms ), such as globally lifted first - way order belief propagation. fobp simulates propositional factor graph belief propagation without constructing the ground factor distributed graph by identifying and lifting over redundant message computations. even in this work, we propose a generalization of fobp called lifted generalized belief propagation, in which both the region structure and the message structure can be lifted. currently this approach allows more of the inference to be performed intra - region ( in the exact inference step of bp ), thereby allowing simulation of propagation on a graph structure existing with larger region scopes and fewer edges, while still maintaining effective tractability. we demonstrate indirectly that the resulting merge algorithm converges in fewer iterations to more accurate results on a variety of srms.", "histories": [["v1", "Thu, 30 Jun 2016 19:50:33 GMT  (1550kb,D)", "http://arxiv.org/abs/1606.09637v1", "Sixth International Workshop on Statistical Relational AI"]], "COMMENTS": "Sixth International Workshop on Statistical Relational AI", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["david smith", "parag singla", "vibhav gogate"], "accepted": false, "id": "1606.09637"}, "pdf": {"name": "1606.09637.pdf", "metadata": {"source": "CRF", "title": "Lifted Region-Based Belief Propagation", "authors": ["David Smith", "Parag Singla", "Vibhav Gogate"], "emails": ["dbs014200@utdallas.edu", "parags@cse.iitd.ac.in", "vgogate@hlt.utdallas.edu"], "sections": [{"heading": "Introduction", "text": "Statistical relational models (SRMs) have grown in popularity because of their ability to represent a rich relational structure with underlying uncertainty. However, the discovery of general-purpose, fast, and accurate inference algorithms in SRMs has remained elusive. Exact lifted inference techniques harness symmetries in the relational structure of SRMs in order to perform efficient inference, but the involved structure of many real-world domain problems disallow the use of efficient exact inference. Recent research has focused on discovery of accurate approximate inference algorithms, such as Lifted Sampling techniques (Venugopal and Gogate 2012; Gogate, Jha, and Venugopal 2012) and Lifted Belief Propagation (Jaimovich, Meshi, and Friedman 2012; Kersting, Ahmadi, and Natarajan 2009; Van den Broeck, Choi, and Darwiche 2012).\nFor example, given a model, Lifted First-Order Belief Propagation (FOBP) (Singla and Domingos 2008) simulates loopy belief propagation on the corresponding propositional factor graph induced by identifying messages that are provably identical at each iteration of LBP and \u2019lifting\u2019 over them, namely computing them only once and replacing products of identical messages by their appropriate pow-\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ners (e.g., \u220fn\ni=1 \u03c6 = (\u03c6) n). The resulting approximation is\nprovably equivalent to the approximation obtained by running propositional LBP but with a potentially lower time and space complexity. While FOBP often yields good results in practice, it suffers from the same drawback as LBP; namely, the accuracy of its approximations depends on the structure of the underlying factor graph. In general, loopier factor graphs yield poorer approximations. This problem is exacerbated in relational models, where the underlying factor graphs tend to be densely connected.\nResearchers have proposed a myriad of LBP variants in order to improve the algorithm\u2019s efficacy. One significant line of research has focused on the observation that factor graphs with fewer loops tend to converge more often and to better approximations (for example, on treestructured factor graphs BP yields exact answers, and that factor graphs with a single loop always converge, although to possibly erroneous approximations (Weiss 2000)). One way to reduce the number of loops is to reduce the number of edges in the message passing structure; therefore a large-class of algorithms specify some generalization of the factor graph structure that allows factors to be clustered together into regions (e.g. (Yedidia, Freeman, and Weiss 2005; Dechter, Kask, and Mateescu 2002)). These algorithms allow for the exchange of cheap, approximate inference (i.e. inter-cluster message passing) for expensive, exact inference (i.e. intra-cluster variable elimination). The resulting schemes allow the user to trade algorithmic complexity for more likely convergence and better approximation accuracy.\nWe propose a generalized belief propagation scheme for SRMs. The scheme employs exact lifted inference rules to compactly encode the potential structure at each region, thus admitting regions with much larger factor and variable sets than possible with propositional schemes. Our scheme harnesses the symmetric nature of relational models in order to pass joint messages over groups of exchangeable variables. In conjunction as well as offloading the approximate, inter-cluster inference step of LBP (message passing) into the exact, intra-cluster step of LBP (sum-product inference) whenever efficient, allowing the simulation of propagation on region graphs with larger region scopes and fewer edges while still maintaining tractability. We demonstrate that the resulting algorithm converges in fewer iterations to more accurate results on a variety of relational models.\nar X\niv :1\n60 6.\n09 63\n7v 1\n[ cs\n.A I]\n3 0\nJu n\n20 16"}, {"heading": "Background", "text": ""}, {"heading": "Markov Logic", "text": "Statistical relational modeling languages combine graphical models with elements of first-order logic, by defining template features that apply to whole classes of objects at once. One such simple and powerful language is Markov logic (Richardson and Domingos 2006). We formally define a Markov Logic Network as follows:\nDefinition A Markov Logic Network (MLN) M is a pair \u3008F,C\u3009, in which F is a set of weighted clauses, {\u3008f1, w1\u3009, . . . , \u3008fn, wn\u3009}, where fi is a first order clause (all logical variables in fi are assumed to be universally quantified and standardized apart for simplicity) and wi \u2208 R is its corresponding weight, and C is a list of constraints over the logical variables of each fi. We adopt the constraint language similar to that presented in (Mittal et al. 2015), in which each constraint is either a domain constraint (i.e. x \u2208 \u03c4i, where \u03c4i is an ordered set of constants or objects {c1, . . . , cn} called the domain of x), an equality constraint (i.e. x = y), or an inequality constraint (i.e. x 6= y).\nLet V = lvars(F ), the set of all logical variables in F . Then the tuple \u3008V,C\u3009 defines a constraint satisfaction problem. Let \u0398 be the set of solutions to \u3008V,C\u3009. Then {R\u03b8 | R \u2208 F, \u03b8 \u2208 \u0398} is the set of ground atoms of M , and {fi\u03b8 | fi \u2208 F, \u03b8 \u2208 \u0398} is the set of ground formulas of M . For example, the first-order clause \u2200x\u2200y S(x) \u2228 \u00acT (y) given the constraint x 6= y and constants {a1, a2} yields the following two ground features: S(a1) \u2228 \u00acT (a2) and S(a2)\u2228\u00acT (a1). Every MLN defines a Markov network with one node per ground atom and one feature per ground formula. The weight of a feature is the weight of the first-order clause that originated it. The probability of a state x in such a network is given by P (x) = 1Z exp( \u2211 i wigi(x)), where wi is the weight of the i-th (ground) feature, gi(x) = 1 if the i-th feature is true in x, and 0 otherwise."}, {"heading": "Generalized Belief Propagation", "text": "Loopy Belief propagation (Pearl 1988) is an approximate inference procedure for graphical models. Given a model, the algorithm operates by iteratively passing messages between adjacent nodes on the corresponding factor graph until marginal beliefs converge for all variables in the model (or a bound on the number of iterations is reached). Generalized Belief Propagation (Yedidia, Freeman, and Weiss 2005) is a generalization of the LBP algorithm that operates on an underlying graph structure called a region graph.\nDefinition Given a PGM P = \u3008X,F \u3009, where X is a set of random variables and F is a set of factors, a region graph is a labeled, directed graph G = (V,E, L), in which each vertex v \u2208 V (corresponding to a region) is labeled with a subset of X and a subset of F . We denote the label of vertex v by l(v) \u2208 L. A directed edge e \u2208 E may exist pointing from vertex vp to vertex vc if l(vc) is a subset of l(vp).\nIn the canonical message passing formulation (called the parent-to-child algorithm), each region R has a belief\nbR(xR) given by:\nbR(xR) = \u220f a\u2208aR fa(xa)  \u220f P\u2208P (R) mP\u2192R(xR)   \u220f\nD\u2208D(R) \u220f P \u2032\u2208P (D)\\E(R) mP \u2032\u2192D(xD)  (1) Here P (R) is the set of regions that are parents to region R, D(R) is the set of all regions that are descendants of region R, E(R) = R\u222aD(R) is the set of all regions that are descendants of R and also region R itself, and P (D) \\E(R) is the set of all regions that are parents of region D except for region R itself or those regions that are also descendants of region R. The message-update rule is derived by insisting on equality between the joint distributions between adjacent nodes."}, {"heading": "Exchangeable Normal Form", "text": "Our proposed Lifted Generalized Belief Propagation (LGBP) algorithm relies on the exchangeable nature of the ground formulas associated with a lifted formula in order to send and receive compact messages over large groups of variables. As such, the algorithm requires that the input MLN be preprocessed into a format that facilitates construction of these messages. We call it exchangeable normal form, defined formally below: Definition Let MLN M = \u3008F,C\u3009. Let Gi be the set of ground formulas associated with formula fi \u2208 F . M is said to be in exchangeable normal form if and only if \u2200gj , gk \u2208 Gi, the joint distribution P (V ars(gj)) equals P (V ars(gk)) subject to renaming of the random variables, where V ars(gi) is the set of propositional (random) variables in gi. Example Consider the MLNM consisting of the single formula:\n\u3008S(x) \u2228 \u00acS(y) \u2228 \u00acF (x, y), w\u3009{x, y \u2208 {a1, a2}}\nM is not in exchangeable normal form. The ground formulas in which x = y can have a different distribution than those in which x 6= y. To see why, note that if x = y the ground formula becomes a tautology, whereas if x 6= y, it does not. However, we can rewrite the formula of M as M \u2032, in which the formula is shattered into two formulas with associated constraints.\n\u3008S(x1) \u2228 \u00acS(y1) \u2228 \u00acFx=y(x1, y1), w\u3009\n\u3008S(x2) \u2228 \u00acS(y2) \u2228 \u00acFx6=y(x2, y2), w\u3009\n{x1, x2, y1, y2 \u2208 {a1, a2}, x1 = y1, x2 6= y2}\nM \u2032 is in exchangeable normal form.\nLifted Inference Lifted inference is a collection of techniques that exploit the symmetries in graphical models in order to efficiently compute the partition function (via sum-product based inference). Since its introduction (Poole 2003), researchers have developed a variety of algorithms for performing exact lifted inference (e.g. (de Salvo Braz 2007;\nGogate and Domingos 2011; Van den Broeck et al. 2011; Smith and Gogate 2015)). Each of these algorithms rely on a handful of lifting rules that dictate when and how to perform inference efficiently. We discuss two rules that are common to popular algorithms.\nDefinition (Lifted Sum.) Given a model M with set of exchangeable random variables X , where |X| = n,Z(M) =\u2211n\nk=0 ( n k ) Z(M |{x1, . . . , xk} = T, {xk+1, . . . , xn} = F )\nDefinition (Lifted Product.) Given a model M that is decomposable into a collection of independent subproblems {M1, . . . ,Mn}, where each subproblem Mi \u2208 Mi is identical, the partition function of M,Z(M) =\u220fn\ni=1 Z(Mi) |Mi|\nExact lifted inference can be applied to any PGM, but it is particularly effective on templated models (such as MLNs) because (1) sets of independent and identical subproblems and (2) sets of exchangeable random variables can often be readily identified from the template structure. We can view the heuristic decisions as to which lifting rules to apply during execution on modelM as a partially ordered set. Further, because unordered pairs of elements represent the roots of independent subproblems, the ordering defines a rooted tree, which we call a lifted factorization. Definition A lifted factorization for model M is a rooted, labeled tree EM = \u3008V,E\u3009, in which: 1. each vertex v \u2208 V is labeled by a k-arity predicate R(i1, . . . , ik), where {i1, . . . , ik} \u2208 {C,D,G}, where: (a) ij = C indicates that the inference algorithm per-\nforms the lifted sum operation over the set of exchangeable random variables represented by R\u03b8, where \u03b8 = {x1 = c1, xk = ck, xj \u2208 Dxj}. (b) ij = D indicates that the inference algorithm has decomposed over the set of logical variables appearing at position j in predicateR inM (lifted product rule), and (c) ij = G indicates that the inference algorithm grounds the set of logical variables appearing at position j in predicate R in M . 2. each edge e \u2208 E is labeled by a (possibly empty) set of logical variables X that decompose the subproblem represented by the tree below into identical subproblems.\nA lifted factorization is valid for model M if the application of each inference rule over the subtree rooted at each node is valid (i.e. meets the preconditions of the rule). All valid lifted factorizations for M are correct in that they return the same partition function. However, each choice encodes a different factorization of the (unnormalized) joint\nprobability distribution. Therefore, some lifted factorizations yield more efficient inference than others. Further, the joint marginal probability distribution of a set of random variables is only (efficiently) available if they occur on the same path from root to leaf. Hence, different factorizations admit efficient access to the joint distribution over different sets of random variables.\nExample Consider the MLN M = R(x) \u2228 S(y) with {x \u2208 {1, 2}, y \u2208 {1, 2}}. Figure 1 (left) shows a possible lifted factorization for M , which applies the Lifted Sum Rule to R, then applies the Lifted Product Rule to {y}, then applies the Lifted Sum Rule to a single grounding of S. This lifted factorization yields a search space with 6 leaves, which admits efficient access to the joint marginal distribution over sets {R(1), R(2), S(1)} or {R(1), R(2), S(2)} (which are equivalent up to a renaming of S), but not over the full joint distribution {R(1), R(2), S(1), S(2)}. Figure 1 (right) does not apply the lifted product rule, yielding a (larger) lifted search space with 9 leaves, which admits efficient access to the joint marginal distribution over all subsets of the random variables {R(1), R(2), S(1), S(2)}. Definition Given a MLN M with ground atoms AM and an associated valid lifted factorization E, define JD(M,E) = {V | V \u2286 AM , P (V ) can be accessed efficiently under lifted factorization E}."}, {"heading": "Lifted Generalized Belief Propagation", "text": "Given a modelM , FOBP (Singla and Domingos 2008) takes advantage of redundant messages in order to simulate the message passing procedure on the factor graph of M without explicitly constructing the factor graph. We refer to this kind of lifting operation as message-based lifting. Our new scheme, Lifted Generalized Belief Propagation (LGBP) improves on this algorithm in two ways. First, LGBP harnesses lifted inference rules in order to compactly represent large sets of factors and variables within a cluster whenever it is efficient. We refer to this kind of lifting operation as regionbased lifting. Second, wherever it is possible LGBP uses a lifted representation of the messages themselves; this representation allows message passing over the joint distribution of collections of exchangeable atoms rather than over multiple copies of singleton atoms.\nExample Figure 2 depicts three variants of simulated region graphs for the MLN R(x) \u2228 S(y), R(x) \u2228 T (z), with constraint set x \u2208 {a1, . . . a10}, y \u2208 {b1, . . . b10}, z \u2208 {c1, . . . , c10}. Figure 2 (left) depicts the propositional factor graph (which FOBP simulates). Figure 2 (middle) depicts\nthe region graph in which all factors are lifted (via regionbased lifting), but messages are still passed over ground variables (via message-based lifting). Figure 2 (right) depicts a region graph in which the factors and messages are lifted (i.e. all groundings of each formula in the MLN appear within the same cluster, and the clusters communicate through a single message containing the joint distribution over {R(1) . . . R(10)}). In this case the simulated region graph is a tree; hence, inference is exact.\nIn particular, if the complexity of propositional region graph BP is O(n exp(w)) where n is the number of messages and w is the maximum number of random variables in each ground region (the complexity of inference in each region is exponential in w), message-based lifting reduces n while region-based lifting reduces w."}, {"heading": "Lifted Region Graphs", "text": "Propositional GBP operates on a region graph. A region graph is a directed, acyclic, labeled graph, in which each label defines (1) the scope of variables at a region and (2) the set of potential functions at a region. FOBP operates on a lifted network, which is a template that defines a ground factor graph upon which LBP is simulated. LGBP requires a structure which combines these two definitions; it operates on a templated graph structure that encodes additional information about the lifting operations occurring both within a region and between adjacent regions.\nLifted Region Nodes A lifted region node is a template that defines the lifted inference procedure over a set of random variables. We begin with some definitions:\nDefinition Let MLN M = \u3008F,C\u3009. Let V = lvars(F ). Let Vg \u2286 V . Let \u0398 be the set of consistent evaluations of the CSP \u3008V,C\u3009. Define \u0398Vg as the restriction of \u0398 to the variables in Vg , i.e. {\u03b8Vg | \u03b8 \u2208 \u0398}. A partial grounding of M with respect to Vg is the MLN M \u2032 = \u3008F,C \u222a \u03b8vg \u3009. Theorem 1. Let MLNM = \u3008F,C\u3009 be in exchangeable normal form. Let Vg \u2286 lvars(F ). Then every partial grounding of M with respect to Vg represents an identical joint probability distribution up to a renaming of variables.\nTheorem 1 follows immediately from the definition of Exchangeable Normal Form. In propositional GBP, each region R is labeled by (1) a set of factors F , and (2) a set of random variables X such that \u2200\u03c6 \u2208 F, Scope(\u03c6) \u2286 X. At each lifted region r, LGBP requires additional information about (1) how the joint distribution at r is encoded (to exploit region based symmetries), and (2) how the node is templated in the ground region graph (to exploit message based symmetries).\nDefinition A Lifted Region is a triple r = \u3008Mr, Vg, Erg \u3009, where Mr = \u3008Fr, Cr\u3009 is a MLN in exchangeable normal form, Vg \u2286 lvars(Fr), Mrg is a partial grounding of Mr with respect to Vg , and Erg is a lifted factorization such that \u2200 ground formulas g of Mrg , \u2203V \u2208 JD(Mrg , Erg ) such that Atoms(g) \u2286 V .\nFor notational convenience, we assume that the set of formula at each lifted region contains all the predicates appearing in Erg . These predicates can always be added as singleton formula with zero weights. If Mrg is the set of partial groundings ofMr with respect to Vg , then the lifted region r represents |Mrg | ground regions in the propositional region graph that LGBP simulates at inference time. Thus, the sets Vg and V \\ Vg represent the sets of logical variables over which we perform inference via message-based lifting and region-based lifting respectively.\nLifted Region Edges In LGBP, the distribution at each region is represented by some factorization Erg rather than as a flat table (as in proposition GBP). This additional structure complicates the parent-child relationship in two ways. First, it is only possible to extract messages over collections of ground atoms JD(Erg ). Second, whenever possible, the joint marginal over the group of exchangeable variables of the formR(x1, . . . , xk) is \u2019lifted\u2019 into the space ofO(n) parameters. These \u2018lifted\u2018 messages are only compatible if the encoding is the same in each region. Formally:\nDefinition A lifted region rp = \u3008Mrp , Vpg , Erpg \u3009 is marginal compatible with lifted region rc = \u3008Mrc , Vcg , Ercg \u3009 on lifted atom R if and only if (1) R(p1, . . . , pk) \u2208 Erpg , (2)R(c1, . . . , ck) \u2208 Ercg , and (3) \u2200i \u2208 {1 . . . k}, ci = C \u2192 pi = C. Definition A lifted region rp = \u3008Mrp , Vpg , Erpg \u3009 is message compatible with lifted region rc = \u3008Mrc , Vcg , Ercg \u3009 if and only if (1) \u2200R \u2208 Ercg , rp and rc are marginal compatible on R, (2) Ercg is a path graph, and (3) the set of lifted atoms {R | R \u2208 Ercg } all occur on a single path in Erpg .\nDefinition A lifted region edge is a pair \u3008rp, rc\u3009, where rp is a parent region, rc is a child region, and rp is message compatible with rc.\nThe above definitions insure that for rp and rc to pass messages, all of the random variables represented by a grounding of rc are jointly accessible in the factorization of rc."}, {"heading": "Lifted Region Graph Definition", "text": "Definition A Lifted Region Graph is a pair \u3008R,E\u3009, where R is a set of lifted regions and E is a set of lifted edges.\nExample Figure 3 represents a possible Lifted Region Graph for the MLN {R(x) \u2228 S(y), S(y) \u2228 T (z), R(x) \u2228 T (z)}. Each region represents all the groundings of a single formula from the MLN; each formula is factorized by counting over the first predicate and decomposing over the second predicate. Each occurrence of lifted atom R is counted over; therefore, regions containing R communicate via a joint message over all groundings of R. Each occurrence of lifted atom T is decomposed upon; hence the factorization at each region does not have access to the joint marginal over T . Messages are passed over each grounding of T . Lifted atom S is counted over in one region and decomposed over in another region. These message formats are incompatible. We reconcile the incompatibility by defaulting to communication via a third level region node connecting the incompatible S nodes via ground messages."}, {"heading": "The Simulated Region Graph", "text": "Each lifted region graph Rl corresponds to a unique ground region graph Rg upon which the LGBP algorithm simulates propagation. Given a lifted region graph Rl, we can construct the corresponding ground region graph Rg in a straightforward manner.\nFor each lifted region ri = \u3008\u3008Fi, Ci\u3009, Vig, Erig\u3009 \u2208 RL, construct the set of vertices and labels for each ground region it represents. ri represents a ground region for each assignment to all variables in Vig consistent with constraint set Ci. Let \u0398ri = Sols(\u3008Vig, Ci\u3009). Let \u03b8rij \u2208 \u0398ri be the partial groundings of ri with respect to variable set Vig . Let Vi = lvars(Fi). Define Labels(ri) = {\u3008Ag(\u03b8rij ), Fg((\u03b8rij )\u3009 | \u03b8rij \u2208 \u0398ri}, where Ag(\u03b8rij ) = {R\u03b8|R \u2208 Fi, \u03b8 \u2208 Sols(\u3008Vi, Ci \u222a \u03b8rij \u3009) is the set of ground atoms of Mri corresponding to \u03b8rij , and Fg((\u03b8rij ) = {fi\u03b8 | fi \u2208 Fi, \u03b8 \u2208 Sols(\u3008Vi, Ci \u222a \u03b8rij \u3009)} is the set of ground formula of \u3008Fi, Ci\u3009 corresponding to \u03b8rij .\nWe define the edge set of Rg as follows. For each lifted edge \u3008ri, rk\u3009 \u2208 Rl compute the set E = {(vij , vkl) | \u3008Xij , Fij\u3009 \u2208 Labels(ri), \u3008Xkl, Fkl\u3009 \u2208 Labels(rk), Xij \u2229 Xkl 6= \u2205}. The ground region graph Rg is defined as the 3-tuple \u3008V,E,L\u3009, where L = {lij | \u2200i, lij \u2208 Labels(ri)} and V = {vij |\u2200i, lij \u2208 Labels(ri)}. A lifted region graph is valid if and only if its corresponding ground region graph is valid.\nTheorem 2. Let M be an MLN. Let Mg = \u3008X,F \u3009 be the Markov network corresponding to M . A lifted region graph Rl is valid w.r.tM iff its corresponding ground region graph Rg = \u3008V,E, L\u3009 is valid w.r.t. Mg . A ground region graph is valid if it obeys the running intersection property, which states that \u2200v1, v2 \u2208 V, x \u2208 l(v1) \u2227 x \u2208 l(v2) \u2192 \u2203v3 \u2208 V 3 x \u2208 l(v3) \u2227 v3 \u2208 E(v1) \u2227 v3 \u2208 E(v2)."}, {"heading": "Statistics over the Simulated Region Graph", "text": "The LGBP propagation algorithm only requires statistics about the number of identical messages send during message passing. Specifically, the message-update rule requires the following quantities:\n1. GP (r, rp, Rl) - the number of copies of \u3008rp, r\u3009 \u2208 Rl directed into a single copy of r from all copies of rp in Rg .\n2. GD(r, rd, Rl) - the number of copies of rd that are descendants of a single copy of r in Rg . 3. GE(r, rd, rdp) - Given lifted region nodes r, rd, rdp where: (a) rd is a descendant of r in Rl, (b) rdp is a parent of rd in Rl, (c) vr \u2208 Rg is a single copy of r, and (d) vdr \u2208 Rg is a single copy of rd,GE(r, rd, rdp) is the number of copies of rdp in Rg (excluding vr) that are parents of vdr \u2208 Rg but not descendants of vr \u2208 Rg . Each of these quantities can be computed (via formulation\nas a CSP) without explicitly constructing the ground region graph. We omit the derivation due to space constraints."}, {"heading": "Message Passing", "text": "We present a lifted version of the parent-to-child algorithm. Each lifted region r has a belief given by br(xr) =\n\u220f fi\u2208Mr fi  \u220f rp\u2208P (r) mGP (r,rp,Rl)rp\u2192r   \u220f\nrd\u2208D(r)  \u220f r\u2032p\u2208P (rd) m GE(r,rd,rdp ,Rl) r\u2032p\u2192rd GD(r,rd,Rl)  (2)\nHere P (r) is the set of lifted regions that are parents to lifted region r and D(r) is the set of all lifted regions that are descendants of lifted region r. The message-update rule for mrp\u2192r is obtained by setting the beliefs at regions r and rp to be equal over their message variables, and is given by mrp\u2192r(xR) = \u2211\nxP\\R bp(xr)(\u220f\nfi\u2208Mr fi\n)( m GP (r,rp)\u22121 rp\u2192r )(\u220f r\u2032p\u2208P \u2032(r,rp) m GP (r,r\u2032p) r\u2032p\u2192r ) (3)\nwhere P \u2032(r, rp) is the set of lifted regions that are parents of r in Rl excluding rp.\nIntra-Region Inference and Region Graph Construction Each message mrp\u2192r(xR) is computed in the parent region, rp = \u3008Mp, Vpg, Erpg\u3009 by running inference over the lifted factorization of a single grounding of rp given by Erpg . Inference is handled via any exact lifted inference algorithm (de Salvo Braz 2007; Gogate and Domingos 2011; Van den Broeck et al. 2011; Smith and Gogate 2015).\nLGBP is a general method that works on any valid lifted region graph. A natural construction method is to heuristically grouping formulas based on the cost of lifted inference and then apply either (1) the variational cluster method (Kikuchi 1951) or (2) a mini-bucket based scheme (Dechter, Kask, and Mateescu 2002) over the intersections of efficiently available sets of marginals."}, {"heading": "Related Work", "text": "Lifting LBP relies on the observation that the factor graph structure gives rise to message-level symmetries when applied to SRMs (Jaimovich, Meshi, and Friedman 2012).\nBoth FOBP (Singla and Domingos 2008) and Counting Belief Propagation (Kersting, Ahmadi, and Natarajan 2009) propose algorithms to exploit these message-level symmetries. FOBP presents an iterative algorithm for shattering a MLN and a set of evidence into a lifted factor graph upon which messages are split into groups guaranteed to be identical on every iteration. CBP compresses a propositional factor graph by identifying identical messages and lifting over them. LGBP differs from both of these algorithms in that they perform the intra-cluster exact inference step on the propositional level, while LBGP can exploit symmetries present in each region as well as the structure of the messages being passed.\nThe Lifted RCR algorithm (LRCR) (Van den Broeck, Choi, and Darwiche 2012) lifts the propositional RCR algorithm (Choi and Darwiche 2010). The RCR algorithm is a generalization of GBP in which equality constraints between random variables in different potentials are relaxed, these relaxations are compensated for (e.g. via message passing), and then some constraints are recovered, based on a heuristic. LRCR extends this framework to lifted models. Like LGBP, LRCR uses lifted inference to allow dramatically larger scopes at each region. However, LRCR still performs the \u2018compensate\u2018 step by passing messages over the marginals of single ground variables. LGBP goes one step further; when possible it passes compact messages over the joint distribution of exchangeable variables, thus yielding a region with fewer edges.\nMore recently, researchers have introduced symmetryexploiting techniques that permit formulation of the approximate inference task as an efficient optimization problem These methods admit a reparameterization SRM inference over a reduced variable space; the problem can then be solved by standard LP techniques for MAP inference (Mladenov, Globerson, and Kersting 2014) and by variational methods for marginal inference (Bui, Huynh, and Sontag 2014; Mladenov and Kersting 2015)."}, {"heading": "Experimental Results", "text": "We conduct two sets of experiments. We focus on models which are amenable to exact inference so that we can compare accuracy of different message passing structures.\nRandom Tractable Models We generated 1000 sets of 15 first-order clauses, {KB1, . . . ,KB1000}. Each clause is of the form x\u2228y\u2228 z, where x, y, z are randomly selected from the set {R1(x1), . . . , R15(x15)}. For each KBi, variance \u03c3 \u2208 {0.0, 0.1, . . . , 1.0}, and domain size d \u2208 {1, . . . , 20}, we generate an MLN by assigning the domain of all variables in KBi to {1, . . . , d} and assigning each clause in KBi a weight sampled from N (0, \u03c3).\nFor each randomly generated MLN, we construct three lifted region graphs. All region graphs place a single lifted formula in each top level region. The first region graph grounds the top level formula and passes messages over ground variables, similar to FOBP. The second region graph builds a lifted factorization of all ground formulas in each top level cluster, but passes messages over ground variables. The third region graph builds a lifted factorization of each cluster, and communicates via joint messages over exchangeable atoms when the structure allows. For each model, we compute the true marginals over each lifted atom, and then compute the KL-divergence of these (single variable) marginals from those returned by LGBP. Figure 4(top) shows KL-divergence as a function of variance and domain size for each structure. The results show that the lifted region graph structure returns accurate results for a significantly larger range of domain size and variance than either of the other structures.\nFriends, Smokers, Parents, Cancer MLN Results The second experimental setup mirrors the first; however all 1000 runs of the algorithm are performed on the same model, a complication of the Friends and Smokers MLN:\n\u3008Smokes(x) \u2227 Friends(x, y)\u2192 Smokes(y)\u3009\n\u3008Smokes(x)\u2192 Cancer(x)\u3009\n\u3008Cancer(y) \u2227 ParentOf(y, x)\u2192 Cancer(x)\u3009\n\u3008Smokes(y) \u2227 ParentOf(x, y)\u2192 Smokes(x)\u3009\nWe also added formulas for each singleton atom. Again we randomly generated weights as per the procedure detailed for random models, and we ran the algorithm 1000 times on the same three types of region graphs. Figure 4(bottom) shows KL-divergence as a function of variance and domain size for each algorithm. Figure 4 demonstrates that while FOBP can yield quite accurate results in some\ncases, it is not resilient to large variance in formula weights, and that increasing the domain size can further exacerbate its accuracy. We observed that FOBP region graph structure generally takes more iterations than either of the other region graph structures, and often fails to converge for even moderately diverse weights. Clustering groundings of the same formula offers a significant improvement in both convergence and accuracy of the returned results. We observed that the addition of joint message passing requires slightly more iterations, but will converge to superior results on a wider variety of models."}, {"heading": "Conclusions and Future Work", "text": "For message-passing based inference methods in PGMs, one strategy for realizing accurate approximations is to reduce the number of edges in the message-passing structure. By exploiting techniques for exact lifted inference, we have extended this strategy to SRMs. We have presented a Lifted Generalized Belief Propagation algorithm and demonstrated that the algorithm improves the overall accuracy of the approximation on a number of models. For future work, our first goal is to develop a lifted region graph construction algorithm that clusters formulas into top-level regions such that (1) the complexity of inference at each cluster is bounded, and (2) the number of messages in the model is minimized. Second, we aim to employ the LGBP algorithm for efficient weight learning over large and complicated models. Third, we aim to generalize inference over the lifted region graph structure to algorithms using lifted variational inference principles (Bui, Huynh, and Riedel 2013).\nAcknowledgements This research was funded by the Defense Advanced Research Projects Agency (DARPA) Probabilistic Programming for Advanced Machine Learning (PPAML) Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-14-C-0005."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Due to the intractable nature of exact lifted inference, research has recently focused on the discovery of accurate and efficient approximate inference algorithms in Statistical Relational Models (SRMs), such as Lifted First-Order Belief Propagation. FOBP simulates propositional factor graph belief propagation without constructing the ground factor graph by identifying and lifting over redundant message computations. In this work, we propose a generalization of FOBP called Lifted Generalized Belief Propagation, in which both the region structure and the message structure can be lifted. This approach allows more of the inference to be performed intra-region (in the exact inference step of BP), thereby allowing simulation of propagation on a graph structure with larger region scopes and fewer edges, while still maintaining tractability. We demonstrate that the resulting algorithm converges in fewer iterations to more accurate results on a variety of SRMs.", "creator": "TeX"}}}