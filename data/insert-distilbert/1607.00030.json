{"id": "1607.00030", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "HUME: Human UCCA-Based Evaluation of Machine Translation", "abstract": "human evaluation of machine translation normally uses sentence - level measures such as relative ranking or adequacy scales. however, these provide no insight into possible errors, and don't scale well with sentence length. we argue for a semantics - specification based evaluation, which captures what meaning. components are retained in the mt output, providing a more fine - grained context analysis of translation quality, understanding and enables comparing the construction and tuning of semantics - based mt. we present a novel human semantic evaluation measure, human ucca - based mt evaluation ( hume ), building on the ucca semantic representation scheme. hume covers a wider range of general semantic related phenomena than previous methods and does not rely on semantic annotation of the potentially garbled mt output. we fully experiment with four language pairs, demonstrating hume's broad lexi applicability, and report good inter - annotator agreement rates and positive correlation with human adequacy scores.", "histories": [["v1", "Thu, 30 Jun 2016 20:35:47 GMT  (1078kb,D)", "https://arxiv.org/abs/1607.00030v1", null], ["v2", "Tue, 27 Sep 2016 13:39:42 GMT  (1126kb,D)", "http://arxiv.org/abs/1607.00030v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexandra birch", "omri abend", "ondrej bojar", "barry haddow"], "accepted": true, "id": "1607.00030"}, "pdf": {"name": "1607.00030.pdf", "metadata": {"source": "CRF", "title": "HUME: Human UCCA-Based Evaluation of Machine Translation", "authors": ["Alexandra Birch", "Omri Abend", "Ond\u0159ej Bojar", "Barry Haddow"], "emails": ["a.birch@ed.ac.uk,", "oabend@cs.huji.ac.il", "bojar@ufal.mff.cuni.cz,", "bhaddow@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Human judgement should be the ultimate test of the quality of an MT system. Nevertheless, common measures for human MT evaluation, such as adequacy and fluency judgements or the relative ranking of possible translations, are problematic in two ways. First, as the quality of translation is multi-faceted, it is difficult to quantify the quality of the entire sentence in a single number. This is indeed reflected in the diminishing inter-annotator agreement (IAA) rates of human ranking measures\n\u2217 All authors contributed equally to this work.\nwith the sentence length (Bojar et al., 2011). Second, a sentence-level quality score does not indicate what parts of the sentence are badly translated, and so cannot inform developers in repairing these errors.\nThese problems are partially addressed by measures that decompose over parts of the evaluated translation, often words or n-grams (see \u00a72 for a brief survey of previous work). A promising line of research decomposes metrics over semantically defined units, quantifying the similarity of the output and the reference in terms of their verb argument structure; the most notable of these measures is HMEANT (Lo and Wu, 2011).\nWe propose the HUME metric, a human evaluation measure that decomposes over UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its cross-linguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their inter-relations.\nHUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We are thus avoiding the semantic annotation of machinegenerated text, which is often garbled or semantically unclear. This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence and avoids relying on reference translations, which have been shown to bias annotators (?).\nAfter a brief review (\u00a72), we describe HUME in detail (\u00a73). Our experiments with four language pairs: English to Czech, German, Polish and Romanian (\u00a74) document HUME\u2019s interannotator agreement and efficiency (time of annotation). We further empirically compare HUME\nar X\niv :1\n60 7.\n00 03\n0v 2\n[ cs\n.C L\n] 2\n7 Se\np 20\n16\nwith direct assessment of human adequacy ratings (\u00a75), and conclude by discussing the differences with HMEANT (\u00a76)."}, {"heading": "2 Background", "text": "MT Evaluation. Human evaluation is generally done by ranking the outputs of multiple systems e.g., in the WMT tasks (Bojar et al., 2015), or by assigning adequacy/fluency scores to each translation, a procedure recently improved by Graham et al. (2015b) under the title Direct Assessment. We use this latter method to compare and contrast with HUME later in the paper. HTER (Snover et al., 2006) is another widely used human evaluation metric which uses edit distance metrics to compare a translation and its human post-edition. HTER suffers from the problem that small edits in the translation could in fact be serious flaws in accuracy, e.g., deleting a negation. Some manual measures ask annotators to explicitly mark errors, but this has been found to have even lower agreement than ranking (Lommel et al., 2014).\nHowever, while providing the gold standard for MT evaluation, human evaluation is not a scalable solution. Scalability is addressed by employing automatic and semi-automatic approximations of human judgements. Commonly, such scores decompose over the sub-parts of the translation, and quantify how many of these sub-parts appear in a manually created reference translation. This decomposition allows system developers to localize the errors. The most commonly used measures decompose over n-grams or individual words, e.g., BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005). Another common approach is to determine the similarity between the reference and translation in terms of string edits (Snover et al., 2006). While these measures stimulated much progress in MT research by allowing the evaluation of massivescale experiments, the focus on words and n-grams does not provide a good estimate of semantic correctness, and may favour shallow string-based MT models.\nIn order to address this shortcoming, more recent work quantified the similarity of the reference and translation in terms of their structure. Liu and Gildea (2005) took a syntactic approach, using dependency grammar, and Owczarzak et al. (2007) took a similar approach using Lexical Functional Grammar structures. Gim\u00e9nez\nand M\u00e0rquez (2007) proposed to combine multiple types of information, capturing the overlap between the translation and reference in terms of their semantic (predicate-argument structures), lexical and morphosyntactic features. Mach\u00e1c\u030cek and Bojar (2015) divided the source sentences into shorter segments, defined using a phrase structure parse, and applied human ranking to the resulting segments.\nPerhaps the most notable attempt at semantic MT evaluation is MEANT and its human variant HMEANT (Lo and Wu, 2011), which quantifies the similarity between the reference and translation in terms of the overlap in their verbal argument structures and associated semantic roles. We discuss the differences between HMEANT and HUME in \u00a76.\nSemantic Representation. UCCA (Universal Conceptual Cognitive Annotation) (Abend and Rappoport, 2013) is a cross-linguistically applicable scheme for semantic annotation. Formally, an UCCA structure is a directed acyclic graph (DAG), whose leaves correspond to the words of the text. The graph\u2019s nodes, called units, are either terminals or several elements jointly viewed as a single entity according to some semantic or cognitive consideration. Edges bear a category, indicating the role of the sub-unit in the structure the unit represents.\nUCCA\u2019s basic inventory of distinctions (its foundational layer) focuses on argument structures (adjectival, nominal, verbal and others) and relations between them. The most basic notion is the Scene, which describes a movement, an action or a state which persists in time. Each Scene contains one main relation and zero or more participants. For example, the sentence \u201cAfter graduation, Tom moved to America\u201d contains two\nScenes, whose main relations are \u201cgraduation\u201d and \u201cmoved\u201d. The participant \u201cTom\u201d is a part of both Scenes, while \u201cAmerica\u201d only of the latter (Figure 1). Further categories account for inter-scene relations and the sub-structures of participants and relations.\nThe use of UCCA for semantic MT evaluation has several motivations. First, UCCA\u2019s foundational layer can be annotated by non-experts after a short training (Abend and Rappoport, 2013; Marinotti, 2014). Second, UCCA is crosslinguistically applicable, seeking to represent what is shared between languages by building on linguistic typological theory (Dixon, 2010b; Dixon, 2010a; Dixon, 2012). Its cross-linguistic applicability has so far been tested in annotations of English, French, German and Czech. Third, the scheme has been shown to be stable across translations: UCCA annotations of translated text usually contain the same set of relations (Sulem et al., 2015), indicating that UCCA reflects a layer of representation that in a correct translation is mostly shared between the translation and the source.\nThe Abstract Meaning Representation (AMR) (Banarescu et al., 2013) shares UCCA\u2019s motivation for defining a more complete semantic annotation. However, using AMR is not optimal for defining a decomposition of a sentence into semantic units as it does not anchor its semantic symbols in the text, and thus does not provide clear decomposition of the sentence into subspans. Also, AMR is more fine-grained than UCCA and consequently harder to annotate. Other approaches represent semantic structures as bilexical dependencies (Sgall et al., 1986; Hajic\u030c et al., 2012; Oepen and L\u00f8nning, 2006), which are indeed anchored in the text, but are less suitable for MT evaluation as they require linguistic expertise for their annotation."}, {"heading": "3 The HUME Measure", "text": ""}, {"heading": "3.1 Annotation Procedure", "text": "This section summarises the manual annotation procedure used to compute the HUME measure. We denote the source sentence as s and the translation as t. The procedure involves two manual steps: (1) UCCA-annotating s, (2) HUMEannotation: human judgements as to the translation quality of each semantic unit of s relative to t, where units are defined according to the UCCA\nannotation. UCCA annotation is performed once for every source sentence, irrespective of the number of its translations we wish to evaluate, and requires proficiency in the source language only. HUME annotation requires the employment of bilingual annotators.1\nUCCA Annotation. We begin by creating UCCA annotations for the source sentence, following the UCCA guidelines.2 A UCCA annotation for a sentence s is a labeled DAG G, whose leaves are the words of s. For every node in G, we define its yield to be its leaf descendants. The semantic units for s according to G are the yields of nodes in G.\nTranslation Evaluation. HUME annotation is done by traversing the semantic units of the source sentence, which correspond to the arguments and relations expressed in the text, and marking the extent to which they have been correctly translated. HUME aggregates the judgements of the users into a composite score, which reflects the overall extent to which the semantic content of s is preserved in t.\nAnnotation of the semantic units requires first deciding whether a unit is structural, i.e., has meaning-bearing sub-units in the target language, or atomic. In most cases, atomic units correspond to individual words, but they may also correspond to multi-word expressions that translate as one unit. For instance, the expression \u201ctook a shower\u201d is translated into the German \u201cduschte\u201d, while its individual words do not correspond to any sub-part of the German translation, motivating the labeling the entire expression as an atomic node. When a multi-word unit is labeled as atomic, its sub-units\u2019 annotations are ignored in the evaluation.\nAtomic units can be labelled as \u201cGreen\u201d (G, correct), \u201cOrange\u201d (O, partially correct) and \u201cRed\u201d (R, incorrect). Green means that the meaning of the word or phrase has been largely preserved. Orange means that the essential meaning of the unit has been preserved, but some part of the translation is wrong. This is often be due to the translated word having the wrong inflection, in a way that impacts little on the understandability of the sentence. Red means that the essential\n1Where bilingual annotators are not available, the evaluation could be based on the UCCA structure for the reference translation. See discussion in \u00a76.\n2All UCCA-related resources can be found here: http: //www.cs.huji.ac.il/~oabend/ucca.html\nmeaning of the unit has not been captured. Structural units have sub-units (children in the UCCA graph), which are themselves atomic or structural. Structural units are labeled as \u201cAdequate\u201d (A) or \u201cBad\u201d (B), meaning that the relation between the sub-units went wrong3. We will use the example \u201cman bites dog\u201d to illustrate typical examples of why a structural node should be labelled as \u201cBad\u201d: incorrect ordering (\u201cdog bites man\u201d), deletion (\u201cman bites\u201d) and insertion (\u201cman bites biscuit dog\u201d).\nHUME labels reflect adequacy, rather than fluency judgements. Specifically, annotators are instructed to label a unit as Adequate if its translation is understandable and preserves the meaning of the source unit, even if its fluency is impaired.\nFigure 2 presents an example of a HUME annotation, where the translation is in English for ease of comprehension. When evaluating \u201cto America\u201d the annotator looks at the translation and sees the word \u201cstateside\u201d. This word captures the whole phrase and so we mark this non-leaf node with an atomic label. Here we choose Orange since it approximately captures the meaning in this context. The ability to mark non-leaves with atomic labels allows the annotator to account for translations which only correspond at the phrase level. Another feature highlighted in this example is that by separating structural and atomic units, we are able to define where an error occurs, and localise the error to its point of origin. The linker \u201cAfter\u201d is translated incorrectly as \u201cby\u201d which changes the meaning of the entire sentence. This error is captured at the atomic level, and it is labelled Red. The sentence still contains two Scenes and\n3 Three labels are used with atomic units, as opposed to two labels with structural units, as atomic units are more susceptible to slight errors.\na Linker and therefore we mark the root node as structurally correct, Adequate."}, {"heading": "3.2 Composite Score", "text": "We proceed to detailing how judgements on the semantic units of the source are aggregated into a composite score. We start by taking a very simple approach and compute an accuracy score. Let Green(s, t), Adequate(s, t) and Orange(s, t) be the number of Green, Adequate and Orange units, respectively. Let Units(s) be the number of units marked with any of the labels. Then HUME\u2019s composite score is:\nHUME(s, t) = Green(s, t) + Adequate(s, t) + 0.5 \u00b7 Orange(s, t)\nUnits(s)"}, {"heading": "3.3 Annotation Interface", "text": "Figure 3 shows the HUME annotation interface4. One source sentence and one translation are presented at a time. The user is asked to select a label for each source semantic unit, by clicking the \u201cA\u201d, \u201cB\u201d, Green, Orange, or Red buttons to the right of the unit\u2019s box. Units with multiple parents (as with \u201cTom\u201d in Figure 2) are displayed twice, once under each of their parents, but are only annotatable in one of their instances, to avoid double counting.\nThe interface presents, for each unit, the translation segment aligned with it. This allows the user, especially in long sentences, to focus her attention on the parts that are most likely to be relevant for her judgement. As the alignments are automatically derived, and therefore noisy, the annotator is instructed to treat the aligned text is a cue, but to ignore the alignment if it is misleading, and instead make a judgement according to the full translation. Concretely, let s be a source sentence, t a translation, and A \u2282 2s \u00d7 2t a many-to-many word alignment. If u is a semantic unit in s, whose yield is yld(u), we define the aligned text in t to be \u22c3 (xs,xt)\u2208A\u2227xs\u2229yld(u)6=\u2205 xt.\nWhere the aligned text is discontinuous in t, words between the left and right boundaries which are not contained in it (intervening words) are presented in a smaller red font. Intervening words are likely to change the meaning of the translation of u, and thus should be attended to when considering whether the translation is correct or not.\nFor example, in Figure 3, \u201congoing pregnancy\u201d is translated to \u201cSchwangerschaft ... laufenden\u201d\n4A demo of HUME can be found in www.cs.huji. ac.il/~oabend/hume_demo.html\n(lit. \u201cpregnancy ... ongoing\u201d). This alone seems acceptable but the interleaving words in red notify the annotator to check the whole translation, in which the meaning of the expression is not preserved5. The annotator should thus mark this structural node as Bad."}, {"heading": "4 Experiments", "text": "In order to validate the HUME metric, we ran an annotation experiment with one source language (English), and four target languages (Czech, German, Polish and Romanian), using text from the public health domain. Semantically accurate translation is paramount in this domain, which makes it particularly suitable for semantic MT evaluation. HUME is evaluated in terms of its consistency (inter-annotator agreement), efficiency (time of annotation) and validity (by comparing it with crowd-sourced adequacy judgements)."}, {"heading": "4.1 Datasets and Translation Systems", "text": "For each of the four language pairs under consideration we built phrase-based MT systems using Moses (Koehn et al., 2007). These were trained on large parallel data sets extracted from OPUS (Tiedemann, 2009), and the data sets released for the WMT14 medical translation task (Bojar et al., 2014), giving between 45 and 85 million sentences of training data, depending on the language pair. These translation systems were used to translate texts derived from both NHS 246\n5The interleaving words are \u201c... und beide berichtet berichteten ...\u201d (lit. \u201c... and both report reported ...\u201d), which doesn\u2019t form any coherent relation with the rest of the sentence.\n6http://www.nhs24.com/\nand Cochrane7 into the four languages. NHS 24 is a public body providing healthcare and healthservice related information in Scotland; Cochrane is an international NGO which provides independent systematic reviews on health-related research. NHS 24 texts come from the \u201cHealth A-Z\u201d section in the NHS Inform website, and Cochrane texts come from their plain language summaries and abstracts."}, {"heading": "4.2 HUME Annotation Statistics", "text": "The source sentences are all in English, and their UCCA annotation was performed by four computational linguists and one linguist. For the annotation of the MT output, we recruited two annotators for each of German, Romanian and Polish and one main annotator for Czech. For computing Czech IAA, several further annotators worked on a small number of sentences each. We treat these further annotators as one annotator, resulting in two annotators for each language pair. The annotators were all native speakers of the respective target languages and fluent in English. They completed a three hour on-line training session which included a description of UCCA and the HUME task, followed by walking through a few examples.\nTable 1 shows the total number of sentences and units annotated by each annotator. Not all units in all sentences were annotated, often due to the annotator accidentally missing a node.\nEfficiency. We estimate the annotation time using the timestamps provided by the annotation tool, which are recorded whenever an annotated sentence is submitted. Annotators are not able to re-open a sentence once submitted. To esti-\n7http://www.cochrane.org/\nmate the annotation time, we compute the time difference between successive sentences, and discard outlying times, assuming annotation was not continuous in these cases. From inspection of histograms of annotation times, we set the upper threshold at 500 seconds. Median annotation times are presented in Table 2, indicating that the annotation of a sentence takes around 2\u20134 minutes, with some variation between annotators.\nInter-Annotator Agreement. In order to assess the consistency of the annotation, we measure the Inter-Annotator Agreement (IAA) using Cohen\u2019s Kappa on the multiply-annotated units. Table 3 reports the number of units which have two annotations from different annotators and the corresponding Kappas. We report the overall Kappa, as well as separate Kappas on atomic units (annotated as Red, Orange or Green) and structural units (annotated as Adequate or Bad). As expected and confirmed by confusion matrices in Figure 4, there is generally little confusion between the two types of units. This results in the Kappa for all units being considerably higher than the Kappa over the atomic units or structural units, where there is more internal confusion.\nTo assess HUME reliability for long sentences, we binned the sentences according to length and measured Kappa on each bin (Figure 5). We see no discernible reduction of IAA with sentence length. Table 3 also shows that the overall IAA is similar for all languages, presenting good agreement (0.6\u20130.7). However, there are differences observed when we break down by node type. Specifically, we see a contrast between Czech and Polish, where the IAA is higher for atomic than for structural units, and German and Romanian, where the reverse is true. We also observe low IAA (around\n0.3) in the cases of German atomic units, and Polish and Czech structural units.\nLooking more closely at the areas of disagreement, we see that for the Polish structural units, the proportion of As was quite different between the two annotators (53% vs. 71%), whereas for other languages the annotators agree in the proportions. We believe that this was because one of the Polish annotators did not fully understand the guidelines for structural units, and percolated errors up the tree, creating more Bs. For German atomic and Czech structural units, where Kappa is also around 0.3, the proportion of such units being marked as \u201ccorrect\u201d is relatively high, meaning that the class distribution is more skewed, so the expected agreement used in the Kappa calculation is high, lowering Kappa. Finally we note some evidence of domain-specific disagreements, for instance the German MT system normally translated \u201creview\u201d (as in \u201csystematic review\u201d \u2013 a frequent term in the Cochrane texts) as \u201c\u00dcberpr\u00fcfung\u201d, which one annotator marked correct, and the other (a Cochrane employee) as incorrect."}, {"heading": "5 Comparison with Direct Assessment", "text": "Recent research (Graham et al., 2015b; Graham et al., 2015a; Graham, 2015) has proposed a new approach for collecting accuracy ratings, direct assessment (DA). Statistical interpretation of a large number of crowd-sourced adequacy judgements for each candidate translation on a fine-grained scale of 0 to 100 results in reliable aggregate scores, that correlate very strongly with one another.\nWe attempted to follow Graham et al. (2015b) but struggled to get enough crowd-sourced judgements for our target languages. We ended up with 10 adequacy judgements on most of the HUME annotated translations for German and Romanian but insufficient data for Czech and Polish. We see this as a severe practical limitation of DA.\nFigure 6 plots the HUME score for each sentence against its DA score. HUME and Direct Assessment scores correlate reasonably well. The Pearson correlation for en-ro (en-de) is 0.70 (0.58), or 0.78 (0.74) if only doubly HUMEannotated points are considered. This confirms that HUME is consistent with an accepted human evaluation method, despite their conceptual differences. While DA is a valuable tool, HUME has two advantages: it returns fine-grained semantic information about the quality of translations and it only requires very few annotators. Direct assessment returns a single opaque score, and (as also noted by Graham et al.) requires a large crowd which may not be available or reliable.\nFigure 7 presents an analysis of HUME\u2019s correlations with DA by HUME unit type, an analysis enabled by HUME\u2019s semantic decomposition. For both target languages, correlation is highest in the \u2019all\u2019 case, supporting our claim for the value of aggregating over a wide range of semantic phenomena. Some types of nodes predict the DA scores better than others. HUME scores on As correlate more strongly with DA than scores on Scene Main Relations (P+S). Center nodes (C) are also more correlated than elaborator nodes (E), which is expected given that Centers are defined to be more semantically dominant. Future work will construct an aggregate HUME score which weights the different node types according to their semantic prominence.\nFigure 8 presents an example of a doubly HUME-annotated English-German example accompanied by 10 raw DA scores assigned to that sentence. The figure illustrates the conceptual dif-\nference between the measures: while DA standardises and averages scores across annotators to denoise the crowd-sourced raw data, thus obtaining a single aggregate score, HUME decomposes over a combinatorial structure, thus allowing to localize the translation errors. We now turn to comparing HUME to a more conceptually-related measure, namely HMEANT."}, {"heading": "6 Comparison with HMEANT", "text": "HMEANT is a human MT evaluation metric that measures the overlap between the translation a reference in terms of their SRL annotations. In\nthis section we present a qualitative comparison between HUME and HMEANT, using examples from our experimental data.\nVerbal Structures Only? HMEANT focuses on verbal argument structures, ignoring other pervasive phenomena such as non-verbal predicates and inter-clausal relations. Consider the following example:\nSource a coronary angioplasty may not be technically possible Transl. eine koronare Angioplastie kann nicht technisch m\u00f6glich Gloss a coronary angioplasty can not technically possible\nThe German translation is largely correct, except that the main verb \u201csein\u201d (\u201cbe\u201d) is omitted. While this may be interpreted as a minor error, HMEANT will assign the sentence a very low score, as it failed to translate the main verb.\nIt is also relatively common that verbal constructions are translated as non-verbal ones or vice versa. Consider the following example:\nSource ... tend to be higher in saturated fats Transl. ... in der Regel h\u00f6her in ges\u00e4ttigte Fette Gloss ... as a rule higher in saturated fats The German translation is largely correct, despite the grammatical divergence, namely that the English verb \u201ctend\u201d is translated into the German prepositional phrase \u201cin der Regel\u201d (\u201cas a rule\u201d). HMEANT will consider the translation to be of poor quality as there is no German verb to align with the English one.\nWe conducted an analysis of the English UCCA Wikipedia corpus (5324 sentences) in order to as-\nsess the pervasiveness of three phenomena that are not well supported by HMEANT.8 First, copula clauses are treated in HMEANT simply as instances of the main verb \u201cbe\u201d, which generally does not convey the meaning of these clauses. They appear in 21.7% of the sentences, according to conservative estimates that only consider non-auxiliary instances of \u201cbe\u201d. Second, nominal argument structures, ignored by HMEANT, are in fact highly pervasive, appearing in 48.7% of the sentences. Third, linkers that express interrelations between clauses (mainly discourse markers and conjunctions) appear in 56% of the sentences, but are again ignored by HMEANT. For instance, linkers are sometimes omitted in translation, but these omissions are not penalized by HMEANT. The following is such an example from our experimental dataset:\nSource However, this review was restricted to ... Transl. Diese \u00dcberpr\u00fcfung bescr\u00e4nkte sich auf ... Gloss This review was restricted to ...\nWe note that some of these issues were already observed in previous applications of HMEANT to languages other than English. See Birch et al. (2013) for German, Bojar and Wu (2012) for Czech and Chuchunkov et al. (2014) for Russian.\nOne Structure or Two. HUME only annotates the source, while HMEANT relies on two independently constructed structural annotations, one for the reference and one for the translation. Not annotating the translation is appealing as it is often impossible to assign a semantic structure to a low quality translation. On the other hand, HUME may be artificially boosting the perceived understandability of the translation by allowing access to the source.\nAlignment. In HMEANT, the alignment between the reference and translation structures is a key part of the manual annotation. If the alignment cannot be created, the translation is heavily penalized. Bojar and Wu (2012) and Chuchunkov et al. (2014) argue that the structures of the reference and of an accurate translation may still diverge, for instance due to a different interpretation of a PP-attachment, or the verb having an additional modifier in one of the structures. It would\n8Argument structures and linkers are explicitly marked in UCCA. Non-auxiliary instances of \u201cbe\u201d and nouns are identified using the NLTK standard tagger. Nominal argument structures are here Scenes whose Main Relation is headed by a noun.\nbe desirable to allow modifications to the SRL annotations at the alignment stage, to avoid unduly penalizing such spurious divergences.\nThe same issue is noted by Lo and Wu (2014): the IAA on SRL dropped from 90% to 61% when the two aligned structures were from two different annotators. HUME uses automatic (wordlevel) alignment, which only serves as a cue for directing the attention of the annotators. The user is expected to mentally correct the alignment as needed, thus circumventing this difficulty.\nMonolingual vs. Bilingual Evaluation. HUME diverges from HMEANT and from shallower measures like BLEU, in not requiring a reference. Instead, it directly compares the source and the translation. This requires the employment of bilingual annotators, but has the benefit of avoiding using a reference, which is never uniquely defined, and may thus lead to unjustly low scores where the translation is a paraphrase of the reference. If only monolingual annotators are available, the HUME evaluation could be performed with a reference sentence instead of with the source. This, however, would risk inaccurate judgements due to the naturally occurring differences between the source and its reference translations."}, {"heading": "7 Conclusion", "text": "We have introduced HUME, a human semantic MT evaluation measure which addresses a wide range of semantic phenomena. We have shown that it can be reliably and efficiently annotated in multiple languages, and that annotation quality is robust to sentence length. Comparison to direct assessments further support HUME\u2019s validity. We believe that HUME, and a future automated version of HUME, allows for a finer-grained analysis of translation quality, and will be useful in informing the development of a more semantically aware approach to MT.\nAll annotation data gathered in this project, together with analysis scripts, is available online9."}, {"heading": "Acknowledgments", "text": "This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 644402 (HimL).\n9https://github.com/bhaddow/ hume-emnlp16"}], "references": [{"title": "Universal conceptual cognitive annotation (ucca)", "author": ["Abend", "Rappoport2013] Omri Abend", "Ari Rappoport"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Abend et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abend et al\\.", "year": 2013}, {"title": "Martha Palmer", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn"], "venue": "and Nathan Schneider.", "citeRegEx": "Banarescu et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "author": ["Banerjee", "Lavie2005] Satanjeev Banerjee", "Alon Lavie"], "venue": "In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Mea-", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "Christian Buck", "author": ["Alexandra Birch", "Barry Haddow", "Ulrich Germann", "Maria Nadejde"], "venue": "and Philipp Koehn.", "citeRegEx": "Birch et al.2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards a Predicate-Argument Evaluation for MT", "author": ["Bojar", "Wu2012] Ond\u0159ej Bojar", "Dekai Wu"], "venue": "In Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Bojar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2012}, {"title": "and Omar F", "author": ["Ond\u0159ej Bojar", "Milo\u0161 Ercegov\u010devi\u0107", "Martin Popel"], "venue": "Zaidan.", "citeRegEx": "Bojar et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Lucia Specia", "author": ["Ondrej Bojar", "Christian Buck", "Christian Federmann", "Barry Haddow", "Philipp Koehn", "Johannes Leveling", "Christof Monz", "Pavel Pecina", "Matt Post", "Herve Saint-Amand", "Radu Soricut"], "venue": "and Ale\u0161 Tamchyna.", "citeRegEx": "Bojar et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Alexander Tarelkin", "author": ["Alexander Chuchunkov"], "venue": "and Irina Galinskaya.", "citeRegEx": "Chuchunkov et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic evaluation of machine translation quality using n-gram co-occurrence statistics", "author": ["George Doddington"], "venue": "In Proceedings of the second international conference on Human Language Technology Research,", "citeRegEx": "Doddington.,? \\Q2002\\E", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Linguistic features for automatic evaluation of heterogenous mt systems", "author": ["Gim\u00e9nez", "M\u00e0rquez2007] Jes\u00fas Gim\u00e9nez", "Llu\u00eds M\u00e0rquez"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation,", "citeRegEx": "Gim\u00e9nez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gim\u00e9nez et al\\.", "year": 2007}, {"title": "Can machine translation systems be evaluated by the crowd alone", "author": ["Timothy Baldwin", "Alistair Moffat", "Justin Zobel"], "venue": "Natural Language Engineering,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Accurate evaluation of segment-level machine translation metrics", "author": ["Nitika Mathur", "Timothy Baldwin"], "venue": "In Proc. of NAACL-HLT,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Improving evaluation of machine translation quality estimation", "author": ["Yvette Graham"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-", "citeRegEx": "Graham.,? \\Q2015\\E", "shortCiteRegEx": "Graham.", "year": 2015}, {"title": "Nicola Bertoldi", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico"], "venue": "et al.", "citeRegEx": "Koehn et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Syntactic features for evaluation of machine translation", "author": ["Liu", "Gildea2005] Ding Liu", "Daniel Gildea"], "venue": "In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,", "citeRegEx": "Liu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2005}, {"title": "Structured vs. flat semantic role representations for machine translation evaluation", "author": ["Lo", "Wu2011] Chi-kiu Lo", "Dekai Wu"], "venue": "In Proceedings of the Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation,", "citeRegEx": "Lo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2011}, {"title": "On the Reliability and Inter-Annotator Agreement of Human Semantic MT Evaluation via HMEANT", "author": ["Lo", "Wu2014] Chi-Kiu Lo", "Dekai Wu"], "venue": null, "citeRegEx": "Lo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lo et al\\.", "year": 2014}, {"title": "Maja Popovic", "author": ["Arle Richard Lommel"], "venue": "and Aljoscha Burchardt.", "citeRegEx": "Lommel et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Evaluating Machine Translation Quality Using Short Segments Annotations", "author": ["Mach\u00e1\u010dek", "Bojar2015] Matou\u0161 Mach\u00e1\u010dek", "Ond\u0159ej Bojar"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Mach\u00e1\u010dek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mach\u00e1\u010dek et al\\.", "year": 2015}, {"title": "Measuring semantic preservation in machine translation with HCOMET: human cognitive metric for evaluating translation", "author": ["Pedro Marinotti"], "venue": null, "citeRegEx": "Marinotti.,? \\Q2014\\E", "shortCiteRegEx": "Marinotti.", "year": 2014}, {"title": "Discriminant-based mrs banking", "author": ["Oepen", "L\u00f8nning2006] Stephan Oepen", "Jan Tore L\u00f8nning"], "venue": "In Proceedings of LREC,", "citeRegEx": "Oepen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Oepen et al\\.", "year": 2006}, {"title": "Josef van Genabith", "author": ["Karolina Owczarzak"], "venue": "and Andy Way.", "citeRegEx": "Owczarzak et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu.", "citeRegEx": "Papineni et al.2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Eva Haji\u010dov\u00e1", "author": ["Petr Sgall"], "venue": "and Jarmila Panevov\u00e1.", "citeRegEx": "Sgall et al.1986", "shortCiteRegEx": null, "year": 1986}, {"title": "Linnea Micciulla", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz"], "venue": "and John Makhoul.", "citeRegEx": "Snover et al.2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Omri Abend", "author": ["Elior Sulem"], "venue": "and Ari Rappoport.", "citeRegEx": "Sulem et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "News from OPUS \u2013 a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann"], "venue": "In Recent Advances in Natural Language Processing,", "citeRegEx": "Tiedemann.,? \\Q2009\\E", "shortCiteRegEx": "Tiedemann.", "year": 2009}], "referenceMentions": [], "year": 2016, "abstractText": "Human evaluation of machine translation normally uses sentence-level measures such as relative ranking or adequacy scales. However, these provide no insight into possible errors, and do not scale well with sentence length. We argue for a semantics-based evaluation, which captures what meaning components are retained in the MT output, thus providing a more fine-grained analysis of translation quality, and enabling the construction and tuning of semantics-based MT. We present a novel human semantic evaluation measure, Human UCCA-based MT Evaluation (HUME), building on the UCCA semantic representation scheme. HUME covers a wider range of semantic phenomena than previous methods and does not rely on semantic annotation of the potentially garbled MT output. We experiment with four language pairs, demonstrating HUME\u2019s broad applicability, and report good inter-annotator agreement rates and correlation with human adequacy scores.", "creator": "LaTeX with hyperref package"}}}