{"id": "1607.00122", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Less-forgetting Learning in Deep Neural Networks", "abstract": "a catastrophic forgetting problem partially makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light weight conditions. this paper, presents a new method for alleviating the catastrophic forgetting problem. unlike, previous research, our method does not therefore use any forget information from the source domain. surprisingly, our method is very effective - to forget less of the information in the source domain, and we show the effectiveness of our method using several later experiments. furthermore, we observed that the forgetting problem occurs between mini - batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. we also try to finally solve this problem using the proposed method. finally, we show our less - forgetting learning method is also helpful to well improve the performance of deep neural networks in terms of recognition rates.", "histories": [["v1", "Fri, 1 Jul 2016 06:50:17 GMT  (204kb,D)", "http://arxiv.org/abs/1607.00122v1", "5 pages, 4 figures"]], "COMMENTS": "5 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["heechul jung", "jeongwoo ju", "minju jung", "junmo kim"], "accepted": false, "id": "1607.00122"}, "pdf": {"name": "1607.00122.pdf", "metadata": {"source": "CRF", "title": "Less-forgetting Learning in Deep Neural Networks", "authors": ["Heechul Jung", "Minju Jung", "Junmo Kim"], "emails": ["junmo.kim}@kaist.ac.kr.", "veryju@kaist.ac.kr"], "sections": [{"heading": null, "text": "Index Terms\u2014Transfer learning, domain adaptation, catastrophic forgetting problem\nI. INTRODUCTION\nDEEP neural networks (DNNs) have grown to nearlyhuman levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4]. Despite this advancement of deep learning, remaining issues still exist; a catastrophic forgetting problem is the one of these remaining issues [5]. The problem is an important issue in DNNs since this enables an improvement in the performance of DNNs in several important applications such as domain adaptation and incremental learning.\nA catastrophic forgetting phenomenon is often observed when performing domain adaptation because the distribution of source data is significantly different from the distribution of target data. For example, consider the traditional transfer learning protocol (weight copy \u2192 fine-tuning) for adapting to a new domain. Usually, the network pre-trained using the original data (source domain) is used as initial weights for adapting to the new data (target domain) [6], [7]. During learning new data from the target domain, it is natural that the network forgets the previously learned information from the source domain. Even if source and target domains are nearly homogeneous, the network forgets the information about source data.\nSeveral researches have been performed for alleviating such problem. Srivastava et al. proposed a local winnertake-all (LWTA) activation function that helps to prevent the\nHeechul Jung, Minju Jung and Junmo Kim are with the School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea, e-mail: {heechul, alswn0925, junmo.kim}@kaist.ac.kr. Jungwoo Ju is with the Division of Future Vehicle, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea, e-mail: veryju@kaist.ac.kr\ncatastrophic forgetting [8]. This activation function has the effectiveness of implicit long-term memory. Recently, several experiments of a catastrophic forgetting problem in DNNs were empirically performed in [5]. The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information. However, these kinds of methods are not explicitly used for the unforgetting of previously learned information, which means that it does not guarantee the unforgetting ability.\nAn unsupervised approach was proposed in [12]. Goodrich et. al extended this method to a recurrent neural network [13]. These methods compute cluster centroids while learning the training data in source domain. They also use the computed centroids for the target domain. Consequently, these are not generally applicable for the pre-trained model, and these are not adequate for our problem settings. [14] and [15] have similar issues mentioned above.\nIn this paper, we try to solve a catastrophic forgetting problem in DNNs by using a new learning method. The proposed method has the ability to maintain the original feature space of the source domain, even if the network does not see any previous training data. Therefore, the method forgets less of the information obtained from the source data compared to the traditional transfer learning method. Figure 1 shows the feature spaces of the traditional transfer learning method and the proposed method. In the proposed method, features of the same source class and target data are well clustered, even if re-training only using the target data is finished.\nOur proposed method is also applicable to the learning method from scratch, which is a general training protocol using stochastic gradient descent methods. We observed that the forgetting problem also arises between mini-batches because mini-batches are small datasets subsampled from a large amount of whole data. We also deal with such problems. We\nar X\niv :1\n60 7.\n00 12\n2v 1\n[ cs\n.L G\n] 1\nJ ul\n2 01\n6\nsummarize our main contributions as follows: \u2022 We propose a less-forgetting learning method to alleviate\na catastrophic forgetting problem in DNNs. \u2022 We observe that a catastrophic forgetting problem also\noccurs between mini-batches when using a stochastic gradient descent learning. \u2022 Furthermore, we show our less-forgetting learning is effective to solve the problem, and it gives better generalization performance."}, {"heading": "A. Less-forgetting Problem", "text": "To theoretically show the less-forgetting problem, we assume that we have the weight parameters of the pre-trained model for the source domain, and training data are given for the target domain (target data). Consequently, data for the source domain (source data) are not accessible. For more clarity, we explain the problem using mathematical expressions as follows.\nWe denote that the dataset for the source domain is D(s) = {(x(s)i , y (s) i )} ns i=1, and the dataset for the target domain is D(t) = {(x(t)i , y (t) i )} nt i=1, where ns and nt are the number of datasets of the source domain and target domain, respectively. Furthermore, x(\u00b7)i is the training data, and y (\u00b7) i is the corresponding label. These two datasets are mutually exclusive, and each dataset has both the training and validation datasets as follows: D(s) = D(s)t \u222a D (s) v ,D (s) t \u2229 D (s) v = \u2205,D(t) = D (t) t \u222a D (t) v and D (t) t \u2229 D (t) v = \u2205, where D(\u00b7)t and D (\u00b7) v are training and validation datasets, respectively. The source network F(x; \u03b8(s)) for the source domain is trained using D(s)t , where \u03b8 (s) is a weight parameter set. The initial values of the weights are initialized randomly with N (0, \u03c32), a normal distribution. The trained weight parameters \u03b8(s) for the source domain are obtained by using the dataset D\n(s) t . The target network F(x; \u03b8 (t)) for the target domain is trained using the dataset D(t)t . Finally, we obtain the updated weight parameters \u03b8(t). In order to satisfy the less-forgetting condition, F(x; \u03b8(t)) \u2248 F(x; \u03b8(s)) for x \u2208 D(s)."}, {"heading": "II. LESS-FORGETTING LEARNING", "text": "In DNNs, the lower layer is considered as a feature extractor, and the top layer is regarded as a linear classifier. This means that the weights of the softmax function represent a decision boundary for classifying the features. Due to the linear classifier on the top layer, the features extracted from the top hidden layer are usually linearly separable. Using this knowledge, we propose a new learning scheme that satisfies the following two properties to reduce the forgetting problem of the information learned from the source domain:\nProperty 1. The decision boundaries should be unchanged.\nProperty 2. The features extracted from source data by the target network should be present in a position close to the features extracted from source data by the source network.\nWe build a less-forgetting learning algorithm based on two properties. The first property is easily implemented by setting\nthe learning rates of the boundary to zero, but satisfying the second property is not trivial since we cannot access to the source data. Instead of using the source data, we use the target data and show it is helpful to satisfy Property 2. Figure 2 briefly explains our algorithm, and the details are as follows.\nInitially, like a traditional transfer learning method, we reuse the weights of the source network as the initial weights of the target network. Next, we freeze the weights of the softmax layer to maintain the boundaries of the classifier. Then, we train the network where to simultaneously minimize the total loss function as follows:\nLt(x; \u03b8(s), \u03b8(t)) = \u03bbcLc(x; \u03b8(t)) + \u03bbeLe(x; \u03b8(s), \u03b8(t)), (1)\nwhere Lt, Lc, and Le are the total, cross-entropy and Euclidean loss functions, respectively. Further, \u03bbc and \u03bbe are the tuning parameters, and x \u2208 D(t). Usually, parameter \u03bbe has a smaller value than the value of \u03bbc. The cross-entropy loss Lc is defined as follows:\nLc(x; \u03b8(t)) = \u2212 C\u2211 i=1 ti log(oi(x; \u03b8 (t))), (2)\nwhere ti is the i-th value of the ground truth label, and o (t) i is the i-th output value of the softmax of the target network. C is the total number of class. In other words, this loss function helps the network to classify the input data x correctly. In order to satisfy the second property, Le is defined as follows:\nLe(x; \u03b8(s), \u03b8(t)) = 1\n2 ||fL\u22121(x; \u03b8(s))\u2212 fL\u22121(x; \u03b8(t))||22, (3)\nwhere L is the total number of hidden layers, and fL\u22121 is a feature vector of layer L \u2212 1. Using the loss function, the target network learns to extract features which are similar to the features extracted by source network.\nFinally, we build a less-forgetting learning algorithm, as shown in Algorithm 1. Ni and Nb in the algorithm denote the number of iterations and the size of mini-batch."}, {"heading": "III. LESS-FORGETTING FOR GENERAL LEARNING CASES", "text": "It is well known the forgetting problem occurs when learning a new task [5]. In addition, we show that the forgetting problem also occurs when performing general learning process\nAlgorithm 1 Less-forgetting Learning (LF) Input: \u03b8(s),D(t)t , Ni, Nb Output: \u03b8(t)\n1: \u03b8(t) \u2190 \u03b8(s) // initial weights 2: Freeze the weights of the softmax layer. 3: for i=1,. . .,Ni // training iteration 4: Select mini-batch set B from D(t)t . (|B| = Nb) 5: Update \u03b8(t) using backpropagation with B to minimize total loss Lt(x; \u03b8(s), \u03b8(t)). 6: end for 7: Return \u03b8(t).\nusing gradient descent methods. To observe the forgetting phenomenon of the network, we probed the training loss values of a particular mini-batch set, as shown in Figure 3. The observing procedure is as follows:\n1) We select a particular mini-batch set B that will be observed, from the training set Dt. (|B| = 100, |Dt| = 50000 and B \u2282 Dt). 2) Perform a standard stochastic gradient descent-based backpropagation with Dt and without shuffling on each iteration using three methods such as traditional learning method, less-forgetting method in Algorithm 1 and modified less-forgetting method in Algorithm 2. (total number of epoch = 30) 3) We capture the loss value with B every 50 iterations (= 0.1 epoch).\nUsing the experiment, we could identify the forgetting problem when performing traditional learning, as shown in Figure 3. It shows periodically oscillating signals, and the one cycle of the signal is exactly the same to one epoch. In other words, the valley is observed at the moment when the network sees B. At the next captured point (after 50 iterations), the loss value was increased. We say that this phenomenon is due to the forgetting problem. In order to satisfy the condition of less-forgetting between mini-batches, the gap between valley and peak points might be narrow.\nFigure 3 shows our less-forgetting method has more smooth graph than traditional learning method. It seems to alleviate the forgetting problem, but the value of training loss is higher than traditional learning method. This is due to the first property in Section II, and freezing the boundary is an obstacle to learn new data. In the modified less-forgetting algorithm, we often unfreeze the boundary of the network. In addition, we update parameters of the source network using the parameters of the target network. Finally, as shown in Figure 3, the green line is\nAlgorithm 2 Less-forgetting for General Learning Cases Input: Dt, Ns, Nf , Ni, Nb Output: \u03b8(t)\n1: \u03b8(t) \u223c N (0, \u03c32) 2: for i=1,. . .,Ni // training iteration 3: if i mod Ns = 1 4: \u03b8(s) \u2190 \u03b8(t) 5: end if 6: if i mod Nf = 1 7: if bi/Nf c = 0 or even number 8: Unfreeze the weights of the softmax layer of the target network.\n9: else 10: Freeze the weights of the softmax layer of the target network. 11: end if 12: end if 13: Select mini-batch set B from Dt. (|B| = Nb) 14: Update \u03b8(t) using backpropagation with B to minimize total loss Lt(x; \u03b8(s), \u03b8(t)) in Equation 1. 15: end for 16: Return \u03b8(t).\nmore smooth than the black line, and the green line has lower loss values than the red line.\nUsing this observation, we present a less-forgetting algorithm for general learning cases, as shown in Algorithm 2. Our algorithm has two main parts (line 3\u223c5 and line 6\u223c12). The first part is to switch the parameters of source network which means the network that we want to forget less, and the second part is to unfreeze repeatedly the boundary of the network. If the value of Ns is large, the network does not adapt a new data well. Further, the parameter of Nf plays a role similar to Ns. As a result, our algorithm has an ability to forget less the information learned previously. We set the value of Ns is smaller than Nf , and we set Ns = 100 and Nf = 1000 for all the experiments."}, {"heading": "IV. EXPERIMENTS", "text": "We establish two different recognition experiments: unforgetting and generalization tests for Algorithm 1 and 2, respectively. In the unforgetting experiment, we manually set up a source domain and a target domain using an object recognition dataset (CIFAR-10) and considered two different digit number recognition datasets (MNIST, SVHN) to belong to the source domain and the target domain, respectively. In the generalizaton experiment, we used CIFAR-10 dataset. In addition, we used a dropout method [10] for Maxout and LWTA experiments because we want to raise their maximum performance."}, {"heading": "A. Unforgetting Test", "text": "In order to examine how much forgetting less prevents a DNN from losing its source domain information while also performing on the target domain, we manipulate CIFAR-10 whereas MNIST and SVHN remain intact. First, we split a training set of CIFAR-10 images, Dt, into two sets so that they belong to the source and the target domains, D(s)t ,D (t) t ; they contain 40,000 and 10,000 images, respectively. Second, we convert the pixel values of the images in D(t)t into new ones based on the equation R+G+B3 , where R,G,B are the\npixel values of each channel, and the same conversion is conducted on a test set, Dv where the converted set becomes D (t) v . However, in the digit unforgetting test, we use MNIST dataset to source domain, SVHN dataset to target domain. Table I includes recognition rates of source networks, which are trained only on source domain and tested\u2014with different activation functions\u2014on each domain separately. The large accuracy gap between them certainly makes sense, since the DNNs has never seen examples from the target domain during training. As opposed to source network, we resume training with different methods on the target domain after learning is finished on the source domain nunder the constraint that during, target domain learning, DNNs cannot access examples from source domain. The results are also listed in Table I. Two \u03bbe values are selected based on observing Figure 4. Our method predicts true labels more accurately than previous works, such as the traditional transfer learning method, LWTA, and Maxout. In addition, this results provide compelling evidence that an alternating activation function is not the proper way to prevent a network from forget as little previously learned information as possible.\nNext, we observe the relation between target accuracy and domain accuracy in accordance with a value of \u03bbe, as displayed in Figure 4. It should be noted that the closer the curve comes to the top right corner, the better performance. Note that the top target recognition rate of less forgetting is even higher than that of transfer learning, LWTA, and Maxout in CIFAR10. Therfore, it could be inferred that dissimilarity between source(color) and target domain(grayscale) is not that much, and remembering previous domain help more generlization."}, {"heading": "B. Generalization Test", "text": "As explained in Section III, we adopt Algorithm 2 on the object recognition and make a discovery that less forgetting learning shows better performance, i.e. more generalization. Table II shows the accuracy of original, batch normalization, batch normalization plus less forgetting, and less forgetting learning as the number of iteratioin increase. It is obvious that\nall cases equipped with less forgetting show an improvement over ones without it."}, {"heading": "V. CONCLUSION", "text": "We proposed a less-forgetting learning method to alleviate a catastrophic forgetting problem in DNNs. Also, we observed that the forgetting phenomenon occurs when performing general learning method like a stochastic gradient descent method. Our method is also effective to mitigate this problem. Finally, we showed that our method is useful for improving generalization ability of DNNs."}], "references": [{"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 1701\u20131708.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical investigation of catastrophic forgeting in gradient-based neural networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6211, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual domain adaptation: A survey of recent advances", "author": ["V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Signal Processing Magazine, IEEE, vol. 32, no. 3, pp. 53\u201369, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345\u2013 1359, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2310\u20132318.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "2012, technical Report arXiv:1207.0580.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1929}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML), 2013.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. X, XXXX XXXX  5", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised neuron selection for mitigating catastrophic forgetting in neural networks", "author": ["B. Goodrich", "I. Arel"], "venue": "Circuits and Systems (MWSCAS), 2014 IEEE 57th International Midwest Symposium on. IEEE, 2014, pp. 997\u20131000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Mitigating catastrophic forgetting in temporal difference learning with function approximation", "author": ["Goodrich", "I. Arel"], "venue": "2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Neuron clustering for mitigating catastrophic forgetting in feedforward neural networks", "author": ["B. Goodrich", "I. Arel"], "venue": "Computational Intelligence in Dynamic and Uncertain Environments (CIDUE), 2014 IEEE Symposium on. IEEE, 2014, pp. 62\u201368.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequential covariance-matrix estimation with application to mitigating catastrophic forgetting", "author": ["T. Lancewicki", "B. Goodrich", "I. Arel"], "venue": "Machine Learning and Applications and Workshops (ICMLA), 2015 14th International Conference on. IEEE, 2015, pp. 618\u2013629.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "Despite this advancement of deep learning, remaining issues still exist; a catastrophic forgetting problem is the one of these remaining issues [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Usually, the network pre-trained using the original data (source domain) is used as initial weights for adapting to the new data (target domain) [6], [7].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "Usually, the network pre-trained using the original data (source domain) is used as initial weights for adapting to the new data (target domain) [6], [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 15, "context": "Visualization of the feature space for ten classes using t-SNE [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "catastrophic forgetting [8].", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Recently, several experiments of a catastrophic forgetting problem in DNNs were empirically performed in [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "An unsupervised approach was proposed in [12].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "al extended this method to a recurrent neural network [13].", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "[14] and [15] have similar issues mentioned above.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and [15] have similar issues mentioned above.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "It is well known the forgetting problem occurs when learning a new task [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "In addition, we used a dropout method [10] for Maxout and LWTA experiments because we want to raise their maximum performance.", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "Source network (Maxout) [5] 99.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Source network (LWTA) [8] 99.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "SVHN Transfer (Maxout) [5] 64.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Transfer (LWTA) [8] 58.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "Source network (Maxout) [5] 78.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Source network (LWTA) [8] 76.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "\u2193 Transfer (Maxout) [5] 71.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "CIFAR10 Transfer (LWTA) [8] 68.", "startOffset": 24, "endOffset": 27}], "year": 2016, "abstractText": "A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we show the effectiveness of our method using several experiments. Furthermore, we observed that the forgetting problem occurs between mini-batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. We also try to solve this problem using the proposed method. Finally, we show our less-forgetting learning method is also helpful to improve the performance of deep neural networks in terms of recognition rates.", "creator": "LaTeX with hyperref package"}}}