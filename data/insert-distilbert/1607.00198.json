{"id": "1607.00198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Sharing Network Parameters for Crosslingual Named Entity Recognition", "abstract": "most state of the art approaches for named entity recognition rely on hand crafted features and annotated corpora. recently devised neural network based models have been proposed which don't require handcrafted features manually but still require annotated corpora. however, such annotated corpora may not be available for many languages. mainly in this paper, we propose supporting a neural network based model which allows anyone sharing the decoder as well as word and character level parameters between all two languages thereby allowing a resource fortunate language to aid a resource deprived language. specifically, we focus on the case when limited annotated corpora is available in one query language ( $ l _ 1 $ ) whilst and abundant annotated corpora is available in another language ( $ l _ 2 $ ). sharing the network architecture and parameters between $ l _ 1 $ and $ l _ 2 $ leads to improved program performance in $ l _ 1 $. further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. we experiment themselves with 4 language pairs and show that indeed in a resource constrained setup ( lesser annotated corpora ), a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language.", "histories": [["v1", "Fri, 1 Jul 2016 10:35:59 GMT  (133kb,D)", "http://arxiv.org/abs/1607.00198v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rudra murthy v", "mitesh khapra", "pushpak bhattacharyya"], "accepted": false, "id": "1607.00198"}, "pdf": {"name": "1607.00198.pdf", "metadata": {"source": "CRF", "title": "Sharing Network Parameters for Crosslingual Named Entity Recognition", "authors": ["Rudra Murthy", "Mitesh Khapra"], "emails": ["rudra@cse.iitb.ac.in", "mikhapra@in.ibm.com", "pb@cse.iitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Named Entity Recognition (NER) plays a crucial role in several downstream applications such as Information Extraction, Question Answering, Machine Translation etc.. Existing state of the art\nsystems for NER are typically supervised systems which require sufficient annotated corpora for training (Ando and Zhang, 2005; Collobert et al., 2011; Turian et al., 2010). In addition, they rely on language-specific handcrafted features (such as capitalization of first character in English). Some of these features rely on knowledge resources in the form of gazetteers (Florian et al., 2003) and other NLP tools such as POS taggers which in turn require their own training data. This requirement of resources in the form of training data, gazetteers, tools, feature engineering, etc. makes it hard to apply these approaches to resource deprived languages.\nRecently, several Neural Network based approaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc. They directly learn meaningful feature representations from the training data itself and can also benefit from large amounts of unannotated corpora in the language. However, they still require sufficient data for training the network and thus only partially address the problem of resource scarcity.\nVery recently Gillick et al. (2015) proposed an encoder decoder based model for sequence labeling which takes a sequence of bytes (characters) as input instead of words and outputs spans as well as labels for these spans. For example, in the case of part-of-speech tagging the span could identify one word and the associated label would be the part-ofspeech tag of that word. Since the input consists of character sequences, the network can be jointly\nar X\niv :1\n60 7.\n00 19\n8v 1\n[ cs\n.C L\n] 1\nJ ul\n2 01\ntrained using annotated corpora from multiple languages by sharing the vocabulary (characters, in this case) and associated parameters. They show that such a jointly trained model can perform better than the same model trained on monolingual data. However, they do not focus on the resource constrained setup where one of the languages has very little annotated corpora. Further, the best results in their joint training setup are poor when compared even to the monolingual results reported in this paper.\nIn this paper, we propose a neural network based model which allows sharing of character dependent, word dependent and output dependent parameters. Specifically, given a sequence of words, we employ LSTMs at word level and CNNs at character level to extract complementary feature representations. The word level LSTMs can capture contextual information and the character level CNNs can encode morphological information. At the output layer we use a feedforward network to predict NER tags. Similar to Gillick et al. (2015), our character dependent parameters are shared across languages (which use the same character set). However, unlike Gillick et al. (2015) we do not use an encoder decoder architecture. Further, our model also employs word level features which can be shared across languages by using jointly learned bilingual word embeddings from parallel corpora (Gouws et al., 2015). Since the NER tags are same across languages, even the output layer of our model is shared across languages.\nWe experiment with 4 language pairs, viz., English-Spanish, English-German, SpanishGerman and Dutch-German using standard NER datasets released as part of the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) and German NER data by Faruqui and Pado\u0301 (2010). We artificially constrain the amount of training data available in one language and show that the network can still benefit from abundant annotated corpora in another language by jointly learning the shared parameters. Further, in the monolingual setup we report state of the art results for two out of three languages without using any handcrafted features or gazetteers.\nThe remainder of this paper is organized as follows:"}, {"heading": "2 Related Work", "text": "In this section we present a quick overview of (i) neural network based approaches for NER which now report state of the art results and (ii) approaches catering to multilingual NER.\nNeural networks were first explored in the context of named entity recognition by Hammerton (2003) but, Collobert et al. (2011) were the first to successfully use neural networks for several NLP tasks including NER. Unlike existing supervised systems, they used minimal handcrafted features and instead relied on automatically learning word representations from large unannotated corpora. The output layer was a CRF layer which modeled the entire sequence likelihood. They also used the idea of sharing network parameters across different tasks (but not between different languages).\nThis idea was further developed by (Santos and Zadrozny, 2014; dos Santos et al., 2015) to include character level information in addition to word level information. They used Convolutional Neural Networks (CNNs) with fixed filter width to extract relevant character level information. The combined character features and word embeddings were fed to a time delay neural network as in Collobert et al. (2011) and used for Spanish and Portuguese NER.\nThere are a few works which use Bidirectional Long Short Term Memory (Bi-LSTMs) (Schuster and Paliwal, 1997) for encoding word sequence information for sequence tagging. For examples Huang et al. (2015) use LSTMs for encoding word sequences and then use CRFs for decoding tag sequences. Chiu and Nichols (2015) use a combination of Bi-LSTMs with CNNs for NER. The decoder is still a CRF which was trained to maximize the entire sequence likelihood. Both these approaches also use some handcrafted features. Very recently Lample et al. (2016) proposed Hierarchical Bi-LSTMs as an alternative to CNN-Bi-LSTMs wherein they first use a character level Bi-LSTMs followed by a word level Bi-LSTMs, thus forming a hierarchy of LSTMs. They also used CRF at the output layer. The model was tested on English, Spanish, Dutch, and German languages. They reported state-of-theart results when systems with no handcrafted feature engineering are considered.\nVery recently Gillick et al. (2015) proposed a\nnovel encoder-decoder architecture for language independent sequence tagging. Even more recently, Yang et al. (2016) extended Lample et al. (2016) and focused on both multi-task and multilingual setting. In the multi-task scenario, except for the output CRF layer, the rest of the network parameters were shared. In the multilingual setting only the character-level features were shared across languages. Though they reported some improvements in the multilingual setting, their model is not suitable in a resource constrained setup (limited training data) because knowledge sharing between languages happens only through character-level features.\nMultilingual training of NER systems was explored dating back to (Babych and Hartley, 2003). Usually these systems train a language dependent NER tagger by (i) enforcing tag constraints along the aligned words in parallel tagged corpora (Chen et al., 2010; Li et al., 2012) or untagged parallel corpus (Wang et al., 2013a; Wang and Manning, 2014; Wang and Manning, 2014; Wang et al., 2013b) and/or (ii) use cross-lingual features (Li et al., 2012; Ta\u0308ckstro\u0308m et al., 2012; Che et al., 2013).\nUnlike existing methods, our proposed deep learning model allows sharing of different parameters across languages and can be jointly trained without the need for any annotated parallel corpus or any handcrafted features."}, {"heading": "3 Model", "text": "In this section, we describe our model which encodes both character level as well as word level information for Named Entity Recognition. As shown in Figure 3, our model consists of three components, viz., (i) a convolutional layer for extracting character-level features, (ii) a bi-directional LSTM for encoding input word sequences and (iii) a feedforward output layer for predicting the tags."}, {"heading": "3.1 Character level Convolutional Layer", "text": "The input to our model is a sequence of words X = {x1, . . . , xn}. We consider each word wi to be further composed of a sequence of characters, i.e., xi = {ci1, ci2, ..., cik} where k is the number of characters in the word. Each character ci1 is represented as a one hot vector \u2208 R|C| where |C| is the number of characters in the language. These one-hot\nrepresentations of all the characters in the word are stacked to a form a matrix M \u2208 Rk\u00d7|C|. We then apply several filters of one dimensional convolution to this matrix. The width of these filters varies from 1 to n, i.e., these filters look at 1 to n-gram character sequences. The intuition is that a filter of length 1 could look at unigram characters and hopefully learn to distinguish between upper case and lowercase characters. Similarly, a filter of length 4 could learn that a sequence \u201dson$\u201d at the end of a word indicates a PERSON (as in Thomson, Johnson, Jefferson, etc).\nThe convolutional operation is followed by a max-pooling operation to pick the most relevant feature (for example, as shown in Figure 1, the maxpooling layer picks up the feature corresponding to capitalization). Further, since there could be multiple relevant n-grams of the same length we define multiple filters of each width. For example, each of the 4-gram sequences son$, corp, ltd. is relevant for NER and different filters of width 4 could capture the information encoded in these different 4-gram sequences. In other words, we have k1, k2, ..., kn filters on width 1,2, ..., n. If we have a total of d1 such filters (d1 = k1 + k2 + ... + kn) then we get a d1 dimensional representation of the word denoted by hcnn(xi)."}, {"heading": "3.2 Bi-directional LSTM", "text": "The input to the bi-directional LSTM is a sequence of words where each word is represented by the following concatenated vector.\nh(xi) = [hemb(xi), hcnn(xi)] (1)\nhemb(xi) is simply the embedding of the word which can be pre-trained (say, using word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b)) and then fine tuned while training our model. The second part, i.e., hcnn(xi) encodes character level information as described in the previous sub-section.\nThe forward LSTM reads this sequence of word representations from left to right whereas the backward LSTM does the same from right to left. This results in a hidden representation for each word which contains two parts.\ngi = [fi(x1, . . . , xi), bi(xn, . . . , xi)] (2)\nwhere, fi and bi are the forward and backward LSTM\u2019s outputs respectively at time-step (position) i. We use the standard definitions of the LSTM functions fi and bi as described in Gillick et al. (2015)."}, {"heading": "3.3 Decoder", "text": "Given a training set D = (X,Y ) where X = (x1, . . . , xn) is a sequence of words and Y = (y1, . . . , yn) is a corresponding sequence of entity tags, our goal is to maximize the log-likelihood of the training data as in equation 3.\nmaximize \u03b8 \u2211 \u2200(X,Y )\u2208D logP (Y |X) (3)\nwhere \u03b8 are the parameters of the network. The log conditional probability P (Y |X) can be decomposed as in equation 4,\nlogP (Y |X) = n\u2211 i=1 logP (yi|x1, . . . , xn, yi\u22121) (4) We model logP (yi|x1, . . . , xn, yi\u22121) using the\nfollowing equation:\nlogP (yi|x1, . . . , xn, yi\u22121) =Wyigi +Ayiyi\u22121\u2212 log \u2211 k\u2208T exp(Wykgi +Aykyi\u22121) (5)\nwhere, Wyi is a parameter vector w.r.t tag yi which when multiplied with gi gives a score for assigning the tag yi. Matrix A can be viewed as a transition matrix where the entry Ayiyi\u22121 gives the transition score from tag yi\u22121 to tag yi. T is the set of all possible output tags.\nIn simple words, our decoder computes the probabilities of the entity tags by passing the output representations computed by LSTM at each position i and the previous tag yi\u22121 through a linear layer followed by a softmax layer. In this sense, our model is a complete neural network based solution as opposed to existing models which use CRFs at the output."}, {"heading": "3.4 Sharing parameters across languages", "text": "As shown in Figure 3, our model contains the following parameters: (i) convolutional filters (ii) word embeddings (iii) LSTM parameters and (iv) decoder parameters. The convolutional filters operate on character sequences and hence can be shared between languages which share a common character set. This is true for many European languages and we consider some of these languages for our experiments (English, Spanish, Dutch and German). Recently there has been a lot of interest in jointly learning bilingual word representations. The aim here is to project words across languages into a common space such that similar words across languages lie very close to each other in this space. In this paper, we experiment with Bilbowa bilingual word embeddings which allows us to share the space of word embeddings across languages. Similarly, we also share the output layer across languages since all languages have the same entity tagset. Finally, we also share\nthe LSTM parameters across languages. Thus, irrespective of whether the model sees a Spanish training instance or an English training, the same set of filters, LSTM parameters and output parameters get updated based on the loss function (and of course the word embeddings corresponding to the words present in the sentence also get updated)."}, {"heading": "4 Experimental Setup", "text": "In this section we describe the following: (i) the datasets used for our experiments (ii) publicly available word embeddings used for different languages and (iii) the hyperparameters considered for all our experiments."}, {"heading": "4.1 Dataset", "text": "For English, Spanish and Dutch we use the the datasets which were released as part of CoNLL Shared Tasks on NER. Specifically, for English we use the data released as part of the CoNLL 2003 English NER Shared Task (Tjong Kim Sang and De Meulder, 2003). For Spanish and Dutch we used the data released as part of the CoNLL 2002 Shared Task (Tjong Kim Sang, 2002). The following entity tags are considered in these Shared Tasks : Person, Location, Organization and Miscellaneous. For all the three languages, the official splits are used as training, development and test files.\nApart from these three languages we also evaluate our models on German. However, we did not have access to the German data from CoNLL (as it requires a special license). Instead we used the publicly available German NER data released by Faruqui and Pado\u0301 (2010). This data was constructed by manually annotating the first two German Europarl session transcripts with NER labels following the CoNLL 2003 annotation guidelines. We use the first session to create train and valid splits. Table 1 summarizes the dataset statistics. Note that the\nGerman data is different from the English, Spanish and Dutch data which use News articles (as opposed to parliamentary proceedings). Note that the German NER data is in IO format so, for all our experiments involving German we convert the data in other languages also to IO format. For the remaining NER experiments, data is converted to IOBES format (Ratinov and Roth, 2009)."}, {"heading": "4.2 Word Embeddings", "text": "We used pre-trained Spectral word embeddings (Dhillon et al., 2015) for English, Spanish, German and Dutch. All the word embeddings are of 200 dimensions. We update these pre-trained word embeddings during training. We convert all words to lowercase before obtaining the corresponding word embedding. However, note that we preserve the case information when sending the character sequence through the CNN layer (as the case information is important for the character filters). Word embeddings for different languages lie in different feature spaces (unless we use bilingual word embeddings which are trained to reside in the same feature space). These word embeddings cannot be directly given as input to our model (as unrelated words from the 2 languages can have similar word embeddings i.e., similar features). We use a language dependent linear layer to map the words from the 2 languages to a common feature space in a task specific setting (common features w.r.t named entity task) and then fed these as input to the LSTM layer."}, {"heading": "4.3 Resource constrained setup", "text": "In the resource constrained setup we assume that we have ample training data in one source language and only limited training data in the target language.\nIn all our resource constrained experiments the LSTM parameters are always shared between the source and target language. In addition, we share one or more of the following: (i) convolutional filters (ii) space of word embeddings and (iii) decoder\nparameters. By sharing the space of word embeddings, we mean that instead of using individually trained monolingual Spectral embeddings for the source and target language, we use jointly trained word embeddings which project the words in a common space. We use off-the-shelf Bilbowa algorithm (Gouws et al., 2015) with default settings to train these bilingual word embeddings. Bilbowa takes both monolingual and bilingual corpora as input. For bilingual corpora, we use the relevant sourcetarget portion of Europarl corpus (Koehn, 2005) and Opus (Skadin\u0327s\u030c et al., 2014). For monolingiual corpora, we obtain short abstracts for each of the 4 languages from Dbpedia (Lehmann et al., 2014).\nDuring training, we combine the training set of the source and target languages. Specifically, we merge all sentences from the training corpus of each language and randomly shuffle them to obtain a bilingual training set. This procedure is similarly repeated for the development set."}, {"heading": "4.4 Hyper-parameters", "text": "Our model contains the following hyper-parameters: (i) LSTM size, (ii) maximum width of CNN filters (iii) number of filters per width (i.e., number of filters for the same width n) and (iv) the learning rate. All the hyper-parameters were tuned by doing a grid search and evaluating the error on the development set. For the LSTM size we considered values from 100 to 300 in steps of 50, for the maximum width of the CNN filters we considered values from k = 4 to 9 (i.e., we use all filters of width 1 to k). We varied the number of filters per width from 10 to 30 in steps of 5 and the learning rate from 0.05 to 0.50 in steps of 0.05."}, {"heading": "5 Results", "text": "In this section we report our experimental results."}, {"heading": "5.1 Monolingual NER", "text": "The main focus of this work is to see if a resource constrained language can benefit from a resource rich language. However, before reporting results in this setup, we would like to check how well our model performs for monolingual NER (i.e., training and testing in the same language). Table 2 compares our results with some very recently published stateof-the art systems. We observe that our model gives\nstate of the art results for Dutch and English and comparable results in Spanish. This shows that a completely neural network based approach can also perform at par with approaches which use a combination of Neural Networks and CRFs (Yang et al., 2016; Lample et al., 2016)."}, {"heading": "5.2 A naturally resource constrained scenario", "text": "We now discuss our results in the resource constrained setup. In our primary experiments, we treat German as the target language and English, Spanish and Dutch as the source language. The reason for choosing German as the target language is that the NER data available for German is indeed very small as compared to the English, Spanish and Dutch datasets (thus naturally forming a pair of resource rich (English, Dutch, Spanish) and resource poor (German) languages). We train our model jointly using the entire source (English or Dutch or Spanish) and target (German) data. We report separate results for the case when (i) the convolutional filters are shared (ii) the decoder is shared and (iii) both are shared. We compare these results with the case when we train a model using only the target (German) data. The results are summarized in Table 3a (DE: German, EN: English, NL: Dutch, ES: Spanish).\nWe observe that sharing of parameters between the two languages helps achieve better results compared to the monolingual setting. Sharing of decoder between English and German helps the most. On the other hand, for German and Dutch we get best results when sharing both character level filters as\nwell as decoder parameters. For German and Spanish sharing the filters helps achieve better results.\nNext, we intend to use a common word embedding space for the source and target languages where related words across the two languages have similar embeddings. The intuition here is if a source word is seen at training time but the corresponding target word (translation) is only seen at test time, the model could still be able to generalize since the embeddings of the source and target words are similar. For this, we use the jointly trained Bilbowa word embeddings as described in section 4.3. In addition, the decoder and character filters are also shared between the two languages. These results are summarized in Table 3b. We observe that we get larger gains when combining the source and target language data. However, the overall results are still poorer when using monolingual Spectral embeddings (as reported in Table 3a). This is mainly because the monolingual corpora used for training Bilbowa word embeddings was much smaller as compared to that used for training Spectral embeddings. For example, the English Spectral embeddings were trained on a larger GigaWord corpus (>1 billion words) whereas the Bilbowa embeddings were trained on a smaller corpus comprising of Dbpedia abstracts (around 400 million words). Given the promising gains obtained by using these bilingual word embeddings it would be interesting to train them on larger corpora. We leave this as future work."}, {"heading": "5.3 A simulated resource constrained scenario", "text": "To help us analyze our model further we perform one more experiment using English as the source and Spanish as the target language. Since sufficient annotated corpora is available in Spanish, we artificially simulate a resource constrained setup by varying the amount of training data in Spanish from 10% to 90% in steps of 10%. These results are summarized in Figure 4a. We see an improvement of around 0.73% to 1.87% when the amount of Spanish data is between 30% to 80%. The benefit of adding English data would of course taper off as more and more Spanish data is available. We hoped that the English data would be more useful when a smaller amount of Spanish data (< 30%) is available but this is not the case. We believe this happens because at lower Spanish data sizes, the English data dom-\ninates the training process which perhaps prevents the model from learning certain Spanish-specific characteristics. Finally, Figure 4b summarizes the results obtained when using a common word embedding space (i.e., using Bilbowa word embeddings) and sharing the decoder and character filters. Once again we see larger improvements but the overall results are lower than those obtained by Spectral embedding due to reasons explained above."}, {"heading": "6 Analysis", "text": "We did some error analysis to understand the effect of sharing different network parameters. Although our primary experiments were on English-German, Spanish-German and Dutch-German, we restricted our error analysis to English-Spanish since we could understand these two languages."}, {"heading": "6.1 Shared Decoder", "text": "Intuitively, sharing the decoder should allow one language to benefit from the tag sequence patterns learned from another language. Of course, this would not happen in the two languages having very\ndifferent word orders (for example, English-Hindi) but this is not the case for English & Spanish. Indeed, we observed that the Spanish model was able to benefit from certain tag sequences which were not frequently seen in the Spanish training data but were seen in the English training data. For example the tag sequence pattern ( O w LOC is frequently confused and tagged as ( O w ORG by the Spanish monolingual model. Here, the symbol \u201d(\u201d is tagged as Others and w is a place-holder for some word. However, this tag pattern was frequently observed in the English training data. For example, such patterns were observed in English Sports news articles: \u201cRonaldo ( O Brazil LOC ) scored 2 goals in the match.\u201d. The joint model could benefit from this information coming from the English data and was thus able to reduce some of the errors made by the Spanish model."}, {"heading": "6.2 Shared Character Filters", "text": "We observed that sharing character filters also helps in generalization by extracting language independent named entity features. For example, many location names begin with an upper-case character and end with the suffix ia as in Australia, Austria, Columbia, India, Indonesia, Malaysia, etc.. There were many such location named entities in the English corpus compared to the Spanish training corpus. We observed that Spanish benefited from this\nin the joint training setup and made fewer mistakes on such names (which it was otherwise confusing with Organization tag in the monolingual setting)"}, {"heading": "7 Conclusion", "text": "In this work, we focused on the problem of improving NER in a resource deprived language by using additional annotated corpora from another language. To this end, we proposed a neural network based architecture which allows sharing of various parameters between the two languages. Specifically, we share the decoder, the filters used for extracting character level features and a shared space comprising of bilingual word embeddings. Since the parameters are shared the model can be jointly trained using annotated corpora available in both languages. Our experiments involving 4 language pairs suggest that such joint training indeed improves the performance in a resource deprived language.\nThere are a few interesting research directions that we would like to pursue in the future. Firstly, we observed that we get much larger gains when the space of word embeddings is shared. However, due to poorer quality of the bilingual embeddings the overall results are not better as compared to the case when we use monolingual word embeddings. We would like to see if training the bilingual word embeddings on a larger corpus would help in correcting this situation. Further, currently the word\nembeddings are trained independently of the NER task and then fine tuned during training. It would be interesting to design a model which allows to jointly embed words and predict tags in multiple languages. Finally, in this work we used only two languages at a time. We would like to see if jointly training with multiple languages could give better results."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Improving machine translation quality with automatic named entity recognition", "author": ["Babych", "Hartley2003] Bogdan Babych", "Anthony Hartley"], "venue": "In Proceedings of the 7th International EAMT Workshop on MT and Other Language Technology", "citeRegEx": "Babych et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Babych et al\\.", "year": 2003}, {"title": "Named entity recognition with bilingual constraints", "author": ["Che et al.2013] Wanxiang Che", "Mengqiu Wang", "Christopher D. Manning", "Ting Liu"], "venue": null, "citeRegEx": "Che et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Che et al\\.", "year": 2013}, {"title": "On jointly recognizing and aligning bilingual named entities", "author": ["Chen et al.2010] Yufeng Chen", "Chengqing Zong", "Keh-Yih Su"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason P.C. Chiu", "Eric Nichols"], "venue": null, "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Dean P. Foster", "Lyle H. Ungar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dhillon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2015}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Training and evaluating a german named entity recognizer with semantic generalization", "author": ["Faruqui", "Pad\u00f32010] Manaal Faruqui", "Sebastian Pad\u00f3"], "venue": "In Proceedings of KONVENS", "citeRegEx": "Faruqui et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2010}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Multilingual language processing from bytes. CoRR, abs/1512.00103", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "Proceedings of the 32nd International Conference on Machine Learn-", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer"], "venue": "In In proceedings of NAACL-HLT (NAACL 2016).,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "DBpedia - a large-scale, multilingual knowledge base", "author": ["Lehmann et al.2014] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Chris Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2014}, {"title": "Joint bilingual name tagging for parallel corpora", "author": ["Li et al.2012] Qi Li", "Haibo Li", "Heng Ji", "Wen Wang", "Jing Zheng", "Fei Huang"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLTNAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] M. Schuster", "Kuldip K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Billions of parallel words for free: Building and using the eu bookshop corpus", "author": ["J\u00f6rg Tiedemann", "Roberts Rozis", "Daiga Deksne"], "venue": "In Proceedings of the 9th International Conference on Language Resources and Eval-", "citeRegEx": "Skadi\u0146\u0161 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Skadi\u0146\u0161 et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the conll-2002 shared task: Languageindependent named entity recognition", "author": [], "venue": "In Proceedings of the 6th Conference on Natural Language Learning Volume 20,", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Cross-lingual projected expectation regularization for weakly supervised learning", "author": ["Wang", "Manning2014] Mengqiu Wang", "Christopher D. Manning"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Effective bilingual constraints for semi-supervised learning of named entity recognizers", "author": ["Wang et al.2013a] Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning"], "venue": "Proceedings of the Twenty-Seventh", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Joint word alignment and bilingual named entity recognition using dual decomposition", "author": ["Wang et al.2013b] Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Yang et al.2016] Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen"], "venue": "arXiv preprint arXiv:1603.06270", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Some of these features rely on knowledge resources in the form of gazetteers (Florian et al., 2003) and other", "startOffset": 77, "endOffset": 99}, {"referenceID": 5, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 13, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 15, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 31, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 10, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 10, "context": "Very recently Gillick et al. (2015) proposed an encoder decoder based model for sequence labeling which takes a sequence of bytes (characters) as input instead of words and outputs spans as well as labels for these spans.", "startOffset": 14, "endOffset": 36}, {"referenceID": 10, "context": "Similar to Gillick et al. (2015), our character dependent parameters are shared across languages (which use the same character set).", "startOffset": 11, "endOffset": 33}, {"referenceID": 10, "context": "Similar to Gillick et al. (2015), our character dependent parameters are shared across languages (which use the same character set). However, unlike Gillick et al. (2015) we do not use an encoder decoder ar-", "startOffset": 11, "endOffset": 171}, {"referenceID": 11, "context": "Further, our model also employs word level features which can be shared across languages by using jointly learned bilingual word embeddings from parallel corpora (Gouws et al., 2015).", "startOffset": 162, "endOffset": 182}, {"referenceID": 26, "context": ", English-Spanish, English-German, SpanishGerman and Dutch-German using standard NER datasets released as part of the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) and German NER data by Faruqui and Pad\u00f3 (2010). We artificially constrain the amount of training data available in one language and show that the network can still benefit from abundant annotated corpora in another language by jointly learning the shared parameters.", "startOffset": 147, "endOffset": 243}, {"referenceID": 11, "context": "Neural networks were first explored in the context of named entity recognition by Hammerton (2003) but, Collobert et al.", "startOffset": 82, "endOffset": 99}, {"referenceID": 5, "context": "Neural networks were first explored in the context of named entity recognition by Hammerton (2003) but, Collobert et al. (2011) were the first to successfully use neural networks for several NLP tasks including NER.", "startOffset": 104, "endOffset": 128}, {"referenceID": 5, "context": "The combined character features and word embeddings were fed to a time delay neural network as in Collobert et al. (2011) and used for Spanish and Portuguese NER.", "startOffset": 98, "endOffset": 122}, {"referenceID": 13, "context": "For examples Huang et al. (2015) use LSTMs for encoding word", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "Very recently Lample et al. (2016) proposed Hierarchical Bi-LSTMs as an alternative to CNN-Bi-LSTMs wherein they first use a character level Bi-LSTMs followed by a word level Bi-LSTMs, thus forming a hierarchy of LSTMs.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "Very recently Gillick et al. (2015) proposed a", "startOffset": 14, "endOffset": 36}, {"referenceID": 30, "context": "Even more recently, Yang et al. (2016) extended Lample et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 15, "context": "(2016) extended Lample et al. (2016) and focused on both multi-task and multilingual setting.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "the aligned words in parallel tagged corpora (Chen et al., 2010; Li et al., 2012) or untagged parallel corpus (Wang et al.", "startOffset": 45, "endOffset": 81}, {"referenceID": 17, "context": "the aligned words in parallel tagged corpora (Chen et al., 2010; Li et al., 2012) or untagged parallel corpus (Wang et al.", "startOffset": 45, "endOffset": 81}, {"referenceID": 10, "context": "We use the standard definitions of the LSTM functions fi and bi as described in Gillick et al. (2015).", "startOffset": 80, "endOffset": 102}, {"referenceID": 6, "context": "We used pre-trained Spectral word embeddings (Dhillon et al., 2015) for English, Spanish, German", "startOffset": 45, "endOffset": 67}, {"referenceID": 11, "context": "We use off-the-shelf Bilbowa algorithm (Gouws et al., 2015) with default settings to train these bilingual word embeddings.", "startOffset": 39, "endOffset": 59}, {"referenceID": 14, "context": "For bilingual corpora, we use the relevant sourcetarget portion of Europarl corpus (Koehn, 2005) and Opus (Skadi\u0146\u0161 et al.", "startOffset": 83, "endOffset": 96}, {"referenceID": 23, "context": "For bilingual corpora, we use the relevant sourcetarget portion of Europarl corpus (Koehn, 2005) and Opus (Skadi\u0146\u0161 et al., 2014).", "startOffset": 106, "endOffset": 128}, {"referenceID": 16, "context": "For monolingiual corpora, we obtain short abstracts for each of the 4 languages from Dbpedia (Lehmann et al., 2014).", "startOffset": 93, "endOffset": 115}, {"referenceID": 31, "context": "This shows that a completely neural network based approach can also perform at par with approaches which use a combination of Neural Networks and CRFs (Yang et al., 2016; Lample et al., 2016).", "startOffset": 151, "endOffset": 191}, {"referenceID": 15, "context": "This shows that a completely neural network based approach can also perform at par with approaches which use a combination of Neural Networks and CRFs (Yang et al., 2016; Lample et al., 2016).", "startOffset": 151, "endOffset": 191}, {"referenceID": 10, "context": "English Gillick et al. (2015) 86.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "English Gillick et al. (2015) 86.50 Yang et al. (2016) 90.", "startOffset": 8, "endOffset": 55}, {"referenceID": 10, "context": "English Gillick et al. (2015) 86.50 Yang et al. (2016) 90.94 Lample et al. (2016) 90.", "startOffset": 8, "endOffset": 82}, {"referenceID": 10, "context": "Spanish Gillick et al. (2015) 82.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "Spanish Gillick et al. (2015) 82.95 Yang et al. (2016) 84.", "startOffset": 8, "endOffset": 55}, {"referenceID": 10, "context": "Spanish Gillick et al. (2015) 82.95 Yang et al. (2016) 84.69 Lample et al. (2016) 85.", "startOffset": 8, "endOffset": 82}, {"referenceID": 10, "context": "Dutch Gillick et al. (2015) 82.", "startOffset": 6, "endOffset": 28}, {"referenceID": 10, "context": "Dutch Gillick et al. (2015) 82.84 Yang et al. (2016) 85.", "startOffset": 6, "endOffset": 53}, {"referenceID": 10, "context": "Dutch Gillick et al. (2015) 82.84 Yang et al. (2016) 85.00 Lample et al. (2016) 81.", "startOffset": 6, "endOffset": 80}], "year": 2016, "abstractText": "Most state of the art approaches for Named Entity Recognition rely on hand crafted features and annotated corpora. Recently Neural network based models have been proposed which do not require handcrafted features but still require annotated corpora. However, such annotated corpora may not be available for many languages. In this paper, we propose a neural network based model which allows sharing the decoder as well as word and character level parameters between two languages thereby allowing a resource fortunate language to aid a resource deprived language. Specifically, we focus on the case when limited annotated corpora is available in one language (L1) and abundant annotated corpora is available in another language (L2). Sharing the network architecture and parameters between L1 and L2 leads to improved performance in L1. Further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. We experiment with 4 language pairs and show that indeed in a resource constrained setup (lesser annotated corpora), a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language.", "creator": "LaTeX with hyperref package"}}}