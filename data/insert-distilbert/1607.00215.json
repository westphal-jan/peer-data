{"id": "1607.00215", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?", "abstract": "computational results demonstrate that posterior sampling for reinforcement learning ( psrl ) dramatically outperforms algorithms apparently driven by optimism, such as ucrl2. hence we provide crucial insight into the extent of this performance boost and the phenomenon that otherwise drives it. we leverage note this detailed insight requirement to easily establish given an $ \\ tilde { o } ( h \\ sqrt { sat } ) $ expected regret posterior bound for psrl in finite - horizon episodic markov decision processes, set where $ h $ is describing the horizon, $ s $ is the number proportion of states, $ a $ is the number of actions and $ h t $ is the time elapsed. this improves upon presenting the best previous bound of $ \\ tilde { o } ( h s \\ sqrt { at } ) $ for any reinforcement learning algorithm.", "histories": [["v1", "Fri, 1 Jul 2016 11:58:28 GMT  (3027kb,D)", "http://arxiv.org/abs/1607.00215v1", null], ["v2", "Fri, 22 Jul 2016 22:43:10 GMT  (3222kb,D)", "http://arxiv.org/abs/1607.00215v2", null], ["v3", "Tue, 13 Jun 2017 15:54:51 GMT  (5658kb,D)", "http://arxiv.org/abs/1607.00215v3", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["ian osband", "benjamin van roy"], "accepted": true, "id": "1607.00215"}, "pdf": {"name": "1607.00215.pdf", "metadata": {"source": "CRF", "title": "Why is Posterior Sampling Better than Optimism for Reinforcement Learning?", "authors": ["Ian Osband", "Benjamin Van Roy"], "emails": ["iosband@stanford.edu", "bvr@stanford.edu"], "sections": [{"heading": null, "text": "\u221a SAT ) expected regret bound for PSRL in finite-horizon episodic\nMarkov decision processes, where H is the horizon, S is the number of states, A is the number of actions and T is the time elapsed. This improves upon the best previous bound of O\u0303(HS \u221a AT ) for any reinforcement learning algorithm."}, {"heading": "1 Introduction", "text": "We consider a well-studied reinforcement learning problem in which an agent interacts with a Markov decision process with the aim of maximizing expected cumulative reward [25, 6]. Our focus is on the tabula rasa case, in which the agent has virtually no prior information about the MDP. As such, the agent is unable to generalize across state-action pairs and may have to gather data at each in order to learn an effective decision policy. Key to performance is how the agent balances between exploration to acquire information of long-term benefit and exploitation to maximize expected near-term rewards. In principle, dynamic programming can be applied to compute the so-called Bayes-optimal solution to this problem [12]. However, this is computationally intractable for anything beyond the simplest of toy problems. As such, researchers have proposed and analyzed a number of heuristic reinforcement learning algorithms. The literature on efficient reinforcement learning offers statistical efficiency guarantees for computationally tractable algorithms. These provably efficient algorithms (see, e.g., [16, 5, 15, 23, 26, 14, 8]) predominantly address the exploration-exploitation trade-off via optimism in the face of uncertainty (OFU): when at a state, the agent assigns to each action an optimistically biased while statistically plausible estimate of future value and selects the action with the greatest estimate. If a selected action is not near-optimal, the estimate must be overly optimistic, in which case the agent learns from the experience. Efficiency relative to less sophisticated exploration strategies stems from the fact that the agent avoids actions that neither yield high value nor informative data. An alternative approach, based on Thompson sampling [27], involves sampling a statistically plausibly set of action values and selecting the maximizing action. These values can be generated, for example, by sampling from the posterior distribution over MDPs and computing the state-action value function of the sampled MDP. This approach, originally proposed in [24], is called posterior sampling for reinforcement learning (PSRL). Computational results in [20] demonstrate that PSRL dramatically outperforms algorithms based on OFU. The primary aim of this paper is to provide insight into the extent of this performance boost and the phenomenon that drives it. ar X iv :1 60 7. 00 21 5v 1\n[ st\nat .M\nL ]\n1 J\nul 2\n01 6\nWe argue that applying OFU in a manner that competes with PSRL in terms of statistical efficiency would require intractable computation. As such, OFU-based algorithms presented in the literature sacrifice statistical efficiency to attain computational tractability. We will explain how these algorithms are statistically inefficient. We will also leverage this insight to produce an O\u0303(H \u221a SAT ) expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where H is the horizon, S is the number of states, A is the number of actions and T is the time elapsed. This improves upon the best previous bound of O\u0303(HS \u221a AT ) for any reinforcement learning algorithm. We discuss why we believe PSRL satisfies a tighter O\u0303( \u221a HSAT ), though we have not proved that. We present computational results chosen to enhance insight on how learning times scale with problem parameters. These empirical scalings match our theoretical predictions."}, {"heading": "2 Problem formulation", "text": "We consider the problem of learning to optimize a random finite-horizon MDP M\u2217= (S,A,R\u2217,P \u2217,H,\u03c1) over episodes of interaction, where S = {1, .., S} is the state space, A = {1, .., A} is the action space, H is the horizon, and \u03c1 is the initial state distribution. In each time period h = 1, ..,H within an episode, the agent observes state sh \u2208 S, selects action ah \u2208 A, receives a reward rh \u223c R\u2217(sh, ah), and transitions to a new state sh+1 \u223c P \u2217(sh, ah). A policy \u00b5 is a mapping from state s \u2208 S and period h = 1, ..,H to action a \u2208 A. For each MDP M = (S,A, RM, PM, H, \u03c1) and policy \u00b5 we define the state-action value function for each period h:\nQM\u00b5,h(s, a) := EM,\u00b5  H\u2211 j=h rM (sj , aj) \u2223\u2223\u2223sh = s, ah = a  , (1) where rM (s, a) = E[r|r \u223c RM (s, a)]. The subscript \u00b5 indicates that actions over periods h+ 1, . . . ,H are selected according to the policy \u00b5. Let VM\u00b5,h(s) := QM\u00b5,h(s, \u00b5(s, h)). A policy \u00b5M is optimal for the MDP M if \u00b5M \u2208 arg max\u00b5 VM\u00b5,h(s) for all s \u2208 S and h = 1, . . . ,H. We will use \u00b5M to denote such an optimal policy. Let Ht = (s1, a1, r1, . . . , st\u22121, at\u22121, rt\u22121) denote the history of observations made prior to time t. To highlight this time evolution within episodes, with some abuse of notation, we let skh = st for t = (k \u2212 1)H + h, so that skh is the state in period h of episode k. We define Hkh analogously. An RL algorithm is a deterministic sequence {\u03c0k|k = 1, 2, . . .} of functions, each mapping Hk1 to a probability distribution \u03c0k(Hk1) over policies, from which the agent samples a policy \u00b5k for use over the kth episode. We define the regret incurred by a reinforcement learning algorithm \u03c0 up to time T to be\nRegret(T, \u03c0,M\u2217) := dT/He\u2211 k=1 \u2206k, (2)\nwhere \u2206k denotes regret over the kth episode, defined with respect to true MDP M\u2217 by\n\u2206k := \u2211 S \u03c1(s)(VM \u2217 \u00b5\u2217,1(s)\u2212 VM \u2217 \u00b5k,1(s)) (3)\nwith \u00b5\u2217 = \u00b5M\u2217 . Note that regret is random since it depends on the random MDP M\u2217, on the random sampling of policies, and, through the history Hk1, on previous transitions and rewards. We will assess and compare algorithm performance in terms of expected regret. In a longer paper, we would like to provide some more detailed discussion between the similarities and differences between this formulation and other common problems in the literature. This would include some discussion of finite vs infinite horizon problems [4, 3], regret vs sample complexity [15] and Bayesian vs frequentist guarantees[17]. However, this is outside the scope of this paper."}, {"heading": "3 Why are optimistic algorithms inefficient?", "text": "Algorithm 1 conveys the typical structure of an RL algorithm based on OFU. Before each episode, it constructs a confidence set to represent the range of MDPs that are statistically plausible given prior knowledge and observations. Then, a policy is selected by maximizing value simultaneously over policies and MDPs in this set. The agent then follows this policy over the episode. It is interesting to contrast this approach against PSRL, which is presented as Algorithm 2. Instead of maximizing over a confidence set, PSRL samples a single statistically plausible MDP and selects a policy that maximizes value for that MDP.\nAlgorithm 1 OFU RL 1: Input: confidence set constructor \u03a6 2: for episode k=1,2,.. do 3: construct confidence setMk = \u03a6(Hk1) 4: compute \u00b5k \u2208 argmax\n\u00b5\nmaxM\u2208MkV M \u00b5,1\n5: for time h=1,2,..,H do 6: take action akh=\u00b5k(skh,h) 7: observe rkh and skh+1 8: update Hkh=Hkh \u222a (akh,rkh,skh+1) 9: end for 10: end for\nAlgorithm 2 PSRL 1: Input: prior distribution \u03c6 2: for episode k=1,2,.. do 3: sample MDP Mk \u223c \u03c6(\u00b7|Hk1) 4: compute \u00b5k \u2208 argmax\n\u00b5\nV Mk \u00b5,1\n5: for time h=1,2,..,H do 6: take action akh=\u00b5k(skh,h) 7: observe rkh and skh+1 8: update Hkh=Hkh \u222a (akh,rkh,skh+1) 9: end for 10: end for\nThe literature on efficient reinforcement learning offers statistical efficiency guarantees for a number of OFU-based algorithms [16, 5, 15, 23, 26, 14, 8]. It is worth noting that some of these algorithms do not explicitly construct an MDP confidence set but instead directly generate optimistic MDP value functions, which can be thought of as value functions for MDPs that maximizes values over an implicit confidence sets. In this section, we discuss how and why these algorithms forgo the level of statistical efficiency enjoyed by PSRL."}, {"heading": "3.1 Confidence sets and statistical efficiency", "text": "To understand when and why optimistic algorithms sacrifice statistical efficiency, it is useful to review an example along the lines of one discussed in [22]. Our example involves a maximal-reward path problem and offers a simple context that facilitates illustration of a basic phenomenon. Just like the RL problem, this maximal-reward path problem involves learning to solve a random MDP M\u2217 = (S,A, R\u2217, P \u2217, H, \u03c1). Let S be deterministic with H = A = S. The transitions PM (s, a) assign probability one to state a so that the action dictates the state transition. The mean rewards are distributed rM\u2217(s, a) \u223c N(1{s 6= a}, 1) independently across state-action pairs, and R\u2217(s, a) assign probability one to rM\u2217(s, a). The initial distribution \u03c1 assign probability one to s = 1. This environment can be thought of as a graph with S densely-connected vertices and edges that offer normally distributed rewards. As with our reinforcement learning formulation, we consider learning to solve this problem over repeated episodes, but we introduce one difference. Instead of observing a sample from R\u2217(skh, akh) when action akh is selected at state skh, the agent only observes a noise-corrupted version of the cumulative reward in an episode. In particular, at the end of the kth episode, the agent observes \u2211H h=1 r\nM\u2217(skh, akh) + wk, where wk \u223c N(0, 1). Proposition 3 of [22], translated to our current context, establishes that for our maximalreward path problem, the expected regret of PSRL satisfies\nE [ Regret(T, \u03c0PSRL,M\u2217) ] = O\u0303( \u221a HSAT ). (4)\nOne can design an OFU-based algorithm that obeys the same bound, as established in [7]. Such an algorithm would, before each episode, construct an ellipsoidal confidence set characterized by a level set of the posterior distribution over mean rewards, which is a normal distribution over S \u00d7A variables. Then, it would select the policy and combination of mean rewards from this set to maximize expected reward over the next episode. Such an algorithm would enjoy expected regret competitive with that of PSRL but, as observed in [7], it requires solving an NP-hard optimization problem at the start of each episode. PSRL, on the other hand, is computationally efficient since it only involves solving a single known MDP.\nAn alternative and computationally efficient OFU algorithm generates an independent confidence interval for each edge\u2019s mean reward and combines these to give hyper-rectangular confidence sets. Using these confidence sets, you can establish a regret bound of the form\nE [ Regret(T, \u03c0OFUrect,M\u2217) ] = O\u0303(HS \u221a AT ), (5)\nwhich sacrifices a factor of \u221a HS relative to ellipsoidal bounds in (4). The reason for this loss in statistical efficiency is intuitive, as we now explain. A hyper-rectangular confidence set supplies a separate confidence interval for each edge\u2019s mean reward. Each interval has to be wide enough to include all statistically plausible values. When these intervals are combined, the resulting hyper-rectangle will contain combinations of edge rewards that are not statistically plausible. In particular, though having one of the mean rewards at the boundary of its confidence interval may be statistically plausible, having mean rewards of all edges simultaneously at their respective boundaries should not be. As such, hyperrectangular confidence sets force OFU-based algorithms to be overly conservative in ruling out combinations of mean rewards, which impedes learning. A similar issue arrises in building confidence sets of MDPs for general reinforcement learning. To facilitate computational tractability, OFU-based reinforcement learning algorithms maintain separate confidence sets for uncertainties associated with separate state-action pairs. These are sometimes confidence sets of transition probabilities and mean rewards and are sometimes confidence sets of state-action values. In either case, the independence of these confidence sets introduces statistical inefficiencies akin to those we discussed in the context of our maximal-reward path problem. A more statistically efficient OFU-based algorithm would construct confidence sets that correspond to level sets of a posterior distribution over MDPs. This would, however, likely give rise to computationally intractable optimization problems since the confidence bounds are coupled across each state and action through time. PSRL naturally samples from the properly calibrated posterior distribution which enables its relative statistical efficiency, but at a computational cost no greater than solving a single known MDP. By comparison even statistically loose hyper-rectangular approximations such as UCRL2 are S-times more computationally expensive [14]."}, {"heading": "3.2 Computational illustration", "text": "In this section, we present some computational illustration that the confidence sets used in common OFU-based algorithms lead to extremely inefficient estimates as S or H grow. We highlight these failures via some trivial MDPs with only one action which we describe inFigure 1. In each of these environments the true value from the initial state 0 is Q(0, 1) = 12 (H \u2212 1). The simplicity of the examples means it is easy to see how to design more effective confidence sets in these contexts. Nonetheless, we will see that confidence sets suggested by OFU-RL can become incredibly mis-calibrated as H and S grow.\nIn each experiment we sample K = 1000 episodes of data from the MDP and then examine the optimistic/sampled Q-values for UCRL2 and PSRL. We implement a version of UCRL2 optimized for finite horizon MDPs and implement PSRL with a uniform Dirichlet prior over the inital dynamics P (0, 1) = (p1, .., p2N ) and a N(0, 1) prior over rewards updating as if rewards had N(0, 1) noise. For both algorithms, if we say that R or P are known then we mean that we use the true R or P inside this part of UCRL2 or PSRL. We present three simple experiments. Figure 2a shows the effect of increasing N in Figure 1a on an agent with full reward knowledge but unknown transitions. Figure 2b shows the effect of increasing N in Figure 1a on an agent with full transition knowledge but unknown rewards.\nFigure 2c shows the effect of increasing H in Figure 1b on an agent with unknown rewards and transitions. We compare the estimates of PSRL to a version of UCRL2 modified for finite horizon settings. In each experiment, the estimates guided by OFU become extremely mis-calibrated, even for relatively small values of S,H. PSRL remains stable.\nThe results of Figure 2b are particularly revealing. They demonstrates the potential pitfalls of OFU-RL even when the underlying transition dynamics entirely known. Several OFU algorithms have been proposed to remedy the loose UCRL-style L1 concentration from transitions [9, 1, 8] but none of these address the inefficiency from hyper-rectangular confidence sets. These loose confidence sets lead to extremely poor performance. We push these results to Appendix A along with comparison to several other OFU approaches."}, {"heading": "4 Posterior sampling for reinforcement learning", "text": "We believe that PSRL offers comparable performance to the computationally intractable OFU-RL algorithm that uses appropriately designed confidence sets to overcome shortcomings discussed in the previous section. Importantly, PSRL has computational cost no greater than solving a single known MDP. In this section, we develop insight into how PSRL accomplishes this and the extent of the resulting performance improvement. This improvement can be formalized through regret bounds. For the context of a finite horizon episodic MDP, prior analyses of UCRL2 [14] and PSRL [20] lead to an expected regret bound:\nE [Regret(T, \u03c0,M\u2217)] = O\u0303(HS \u221a AT ).\nIn fact, we believe that PSRL satisfies a tighter regret bound:\nE [Regret(T, \u03c0,M\u2217)] = O\u0303( \u221a HSAT ).\nThe next subsections shed light on the reduction of S to \u221a S and the reduction of H to \u221a H.\n4.1 From S to \u221a S\nThe following result establishes that PSRL satisfies a stronger expected regret bound than previous analysis [20], reducing the scaling with S to \u221a S.\nTheorem 1. For any prior over rewards with r \u2208 [0, 1] and sub Gaussian noise and any independent Dirichlet prior over transitions, PSRL satisfies\nE [ Regret(T, \u03c0PSRL,M\u2217) ] = O\u0303 ( H \u221a SAT ) (6)\nOur proof of Theorem 1 begins with the standard analysis of optimistic algorithms to add and subtract the imagined optimal reward VMk\u00b5k,1.\n\u2206k = VM \u2217 \u00b5\u2217,1 \u2212 VM \u2217 \u00b5k,1\n= VM \u2217 \u00b5\u2217,1 \u2212 V Mk \u00b5k,1\ufe38 \ufe37\ufe37 \ufe38\n\u2206opt k\n+VMk\u00b5k,1 \u2212 V M\u2217 \u00b5k,1\ufe38 \ufe37\ufe37 \ufe38 \u2206conc\nk\n(7)\nWhere \u2206optk is the regret from optimism and \u2206conck is the regret from concentration. We then use posterior sampling to assert that, conditional on any past data [22, 20]\nE[\u2206optk |Hk1] = E[V \u2217 \u22171 \u2212 V kk1|Hk1] = 0.\nTo consdense our notation we write xkh := (skh, akh) and V kk,h := V Mk \u00b5k,h . We let r\u0302k(x) := E[r\u2217(x)|Hk1], P\u0302k(x) := E[P \u2217(x)|Hk1] and introduce wR(x) := rk(x) \u2212 r\u0302k(x) and wPh := (Pk(xk1)\u2212 P\u0302k(xk1))TV kkh+1. We note that, conditional upon the data Hk1 the true reward and transitions are independent of the rewards and transitions sampled by PSRL, so that E[r\u2217(x)|Hk1] = r\u0302k(x),E[P \u2217(x)|Hk1] = P\u0302k(x) for any x. However, E[wR(x)|Hk1] and E[wPh (x)|Hk1] are generally non-zero, since the agent chooses its policy to optimize its reward under Mk. We can rewrite the regret from concentration via the Bellman operator (section 5.2 of [20]),\nE [ V kk1\u2212V \u2217k1|Hk1 ] = E [ rk(xk1)\u2212r\u2217(xk1)+Pk(xk1)TV kk2\u2212P \u2217(xk1)TV \u2217k2 |Hk1 ] = E [ rk(xk1)\u2212r\u2217(xk1)+ ( Pk(xk1)\u2212P\u0302k(xk1) )T V kk2 + E [( V kk2\u2212V \u2217k2 ) (s\u2032)|s\u2032\u223cP \u2217(xk1) ] |Hk1\n] = ... = E [\u2211H h=1{rk(xk1)\u2212r\u0302 \u2217(xk1)}+ \u2211H h=1 {( Pk(xkh)\u2212P\u0302k(xkh) )T V kkh } |Hk1\n] \u2264 E [\u2211H h=1|w R(xkh)|+ \u2211H h=1|w P h (xkh)| |Hk1 ] . (8)\nBounding the contribution from |wR| is relatively straightforward. The key step to our proof is a new result that bounds |wPh | in terms of a Gaussian posterior, even though the underlying posterior distribution is not Gaussian. At the root of our argument is the notion of stochastic optimism [21], which introduces a partial ordering over random variables. Definition 1. For any X and Y real-valued random variables we say that X is stochastically optimistic for Y if for any u:R\u2192R convex and increasing E[u(X)] \u2265 E[u(Y )]. We will use the notation X <so Y to express this relation.\nAt a high level, we establish that posterior sampling with a correct prior can be approximated by posterior sampling with an incorrect prior if the mis-specified prior and all possible posteriors are stochastically optimistic the corresponding true prior and posteriors. This allows us to approximate PSRL with a Dirichlet prior over transition probabilities via a Gaussian prior over probability-weighted next-state values, based on the following result [21]: Lemma 1. For all V \u2208 [0, 1]N and \u03b1 \u2208 [0,\u221e)N with \u03b1T1 \u2265 2, if X \u223c N(\u03b1>V/\u03b1>1, 1/\u03b1>1) and Y = PTV for P \u223c Dirichlet(\u03b1) then X <so Y .\nIt turns out that analysis of a conceptual algorithm that makes use of this Gaussian approximation rather than Dirichlet distributions is simpler, and this allows us to establish the regret bound of Theorem 1. We call this algorithm Gaussian PSRL and describe the algorithm in Appendix C. This result allows us to prove that |wPh (x)| concentrates at a rate O\u0303(H/ \u221a n(x)), rather than previous analysis O\u0303(H \u221a S/n(x)) where n(x) is the number of prior observations at x Prior work has explored other ways designing OFU-based algorithms that reduce how regret or learning time scales with S. MORMAX [26] and delayed Q-learning [23], in particular, come with sample complexity bounds that are linear in S, which may be the best dependence on S that one can hope for. But even in terms of sample complexity, these algorithms are not necessarily an improvement over UCRL2 or its variants with similar bounds [8]. We can compare these bounds in terms of T\u03c0( ) := \u201ctime until algorithm \u03c0 has expected average regret less than \u201d:\nTDelayQ( ) = O\u0303 ( H9SA\n4\n) , TMORMAX( ) = O\u0303 ( H7SA\n2\n) , TUCRL2( ) = O\u0303 ( H2S2A\n2\n) . (9)\nTheorem 1 implies TPSRL( ) = O\u0303(H 2SA 2 ). MORMAX and delayed Q-learning reduces the S-dependence of UCRL2, but this comes at the expense of worse dependence on H, and the resulting algorithms are not practical.\n4.2 From H to \u221a H\nThe results of [4] adapted to finite horizon MDPs would suggest a lower bound \u2126(H \u221a SAT ) on the worst-case regret for any algorithm. However, the associated proof is incorrect. The strongest lower bound with a correct proof is \u2126( \u221a HSAT ) [14]. It remains an open question whether such a lower bound applies to expected regret, though we believe it does. Recent analyses [18, 8] suggest that reducing the dependence of H to \u221a H is possible. They note that \u201clocal value variance\u201d satisfies a Bellman equation. Intuitively this captures that if we transition to a bad state V ' 0, then we cannot transition anywhere much worse during this episode. This relation means that \u2211H h=1 w P h (xkh) should behave more as if they were\nindependent and grow O( \u221a H), unlike our analysis which crudely upper bounds them each in turn O(H). We present a sketch towards an analysis of Conjecture 1 in Appendix D. Conjecture 1. For any prior over rewards with r \u2208 [0, 1] and sub Gaussian noise and any independent Dirichlet prior over transitions, we conjecture that\nE [ Regret(T, \u03c0PSRL,M\u2217) ] = O\u0303 (\u221a HSAT ) , (10)\nand that this matches the lower bounds for any algorithm up to logarithmic factors.\nWe note that, unlike our proof of Theorem 1 these arguments would not hold for Gaussian PSRL. Interestingly, we find that our experimental evaluation is consistent with O\u0303(HS \u221a AT ), O\u0303(H \u221a SAT ) and O\u0303( \u221a HSAT ) for UCRL2, Gaussian PSRL and PSRL respectively. These results help to suggest that it might be possible to formalize the arguments in our conjecture and maybe even in terms of the problem-specific frequentist regret [11]."}, {"heading": "4.2.1 An empirical investigation", "text": "We now discuss a computational study designed to illustrate how learning times of reinforcement learning algorithms scale with H, and in particular, to support the conjecture that PSRL reduces regret by a factor of \u221a H relative to UCRL2. The class of MDPs we consider involves a long chain of states with S = H = N and with two actions: left and right. Each episode the agent begins in state 1. The optimal policy is to head right at every timestep, all other policies have zero expected reward. Inefficient strategies for exploration will take \u2126(2N ) episodes to learn the optimal policy [21].\nWe evaluate several learning algorithms from five random seeds and N = 2, .., 100 for one million episodes each. Our goal is to investigate their empirical performance and scaling. We highlight results for three algorithms with O\u0303( \u221a T ) regret bounds: UCRL2, Gaussian PSRL and PSRL. We include more details for these experiments including a link to their source code and experimental results for several other algorithms in Appendix E. We implement UCRL2 with confidence sets optimized for finite horizon MDPs. For the Bayesian algorithms we use a uniform Dirichlet prior for transitions and N(0, 1) prior for rewards. Figure 4 display the regret curves for these algorithms for N \u2208 {5, 10, 30, 50}. As suggested by our analysis, PSRL outperforms Gaussian PSRL which itself outperforms UCRL2. These differences seems to scale with the length of the chain N and that even for relatively small MDPs, PSRL is many orders of magnitude more efficient than UCRL2.\nWe investigate the empirical scaling of these algorithms with respect to N and compare these results to our analysis. Suppose for some B \u2208 R we can bound Regret(T ) \u2264 \u221a BT ). This means we that T\u0303 ( ) := \u201c#episodes until the average regret per episode \u2264 \u201d= O\u0303(B2H/ 2). The results of Theorem 1 and Conjecture 1 would suggest that up to o(log(N)):\nlog(T\u0303UCRL2) ' 5 log(N), log(T\u0303GaussPSRL) ' 4 log(N), log(T\u0303PSRL) ' 3 log(N).\nFigure 5 displays T\u0303 (0.1) for UCRL2, Gaussian PSRL and PSRL. Along side these results we also draw dotted lines with slope 5, 4 and 3 respectively, which are the scalings associated with bounds O\u0303(HS \u221a AT ), O\u0303(H \u221a SAT ) and O\u0303( \u221a HSAT ) respectively. We find that the emprical results match the results of Theorem 1 and provide support to Conjecture 1."}, {"heading": "5 Conclusion", "text": "PSRL is orders of magnitude more statistically efficient than UCRL and S-times less computationally expensive. In the future, we believe that analysts will be able to formally specify an OFU approach to RL whose statistical efficiency matches PSRL. However, we argue that the resulting confidence sets which address both the coupling over H and S will result in a computationally intractable optimization problem. For this reason, computationally efficient approaches to OFU RL will sacrifice statistical efficiency; this is why posterior sampling is better than optimism for reinforcement learning."}, {"heading": "A Computational illustration", "text": "In this section we expand upon the simple examples given by Section 3.2 to a full decision problem with two actions. We define an MDP similar to Figure 1 but now with two actions. The first action is identical to Figure 1, but the second action modifies the transition probabilities to favour the rewarding states with probability 0.6/N and assigning only 0.4/N to the non-rewarding states. We now investigate the regret of several learning algorithms which we adapt to this setting. These algorithms are based upon BEB [17], BOLT [1], -greedy with = 0.1, Gaussian PSRL (see Appendix C), Optimistic PSRL (which takes K = 10 samples and takes the maximum over sampled Q-values similar to BOSS [2]), PSRL [24], UCFH [8] and UCRL2 [14]. We link to the full code for implementation in Appendix E.\nWe see that the loose estimates in OFU algorithms from Figure 2 lead to bad performance in a decision problem. This poor scaling with the number of successor states N occurs when either the rewards or the transition function is unknown. We note that in stochastic environments the PAC-Bayes algorithm BOLT, which relies upon optimistic fake prior data, can sometimes concentrate too quickly and so incur the maximum linear regret. In general, although BOLT is PAC-Bayes, it concentrates too fast to be PAC-MDP just like BEB [17]. In Figure 8 we see a similar effect as we increase the episode length H. We note the second order UCFH modification improves upon UCRL2\u2019s miscalibration with H, as is reflected in their bounds [8]. We note that both BEB and BOLT scale poorly with the horizon H."}, {"heading": "B Proof of Theorem 1", "text": "The contributions from rewards wR(xkh) = O\u0303( \u221a\n1/n) are relatively simple to bound for any posterior distribution [20]. The key piece of our analysis is modified form of Lemma 1 to bound the contribution from transitions wPh (xkh) under any future value function V kkh+1. Before we can approach this result we need to look more deeply into the notion of stochastic optimism. Stochastic optimism is closely related to second-order stochastic dominance: X <so Y if and only if \u2212Y second-order stochastically dominates \u2212X [13]. This characterization of optimism for random variables affords us several equivalences via integration by parts [19]. Lemma 2. The following are equivalent to X <so Y :\n1. For any random variable Z independent of X and Y , E[max(X,Z)] \u2265 E[max(Y,Z)] 2. X =D Y +A+W for A \u2265 0 and E [W |Y +A] = 0 for all values y + a. 3. For any \u03b1 \u2208 R, \u222b\u221e \u03b1 {P(X \u2265 s)\u2212 P(Y \u2265 s)} ds \u2265 0.\nLemma 3.1 implies that the maximum of optimistic variables is optimistic, 3.2 provides some more clear intuition for this condition and previous work [21] uses 3.3 to establish Lemma 1. At first glance Lemma 1, may look obstruse and the full proof [21] which we reproduce in Appendix. Nevertheless, the ideas behind this argument are simple and the result is quite deep. Lemma 1 compares the posterior distribution of the Q-values for a problem similar Figure 1 when updating a under Gaussian posterior for the rewards as if the N(\u00b5, 1) (X) and the PSRL update for Dirichlet transitions (Y ). Lemma 1 says that, for any possible data, the resulting posteriors will be ordered X <so Y . Lemma 1 will allow us to bound the concentration of the next step value function PTV directly, rather than via some H\u00f6lder decomposition as per Section 3. This is crucial to our analysis to provide O\u0303(H \u221a SAT ) bounds upon the expected regret. Lemma 3. For any history Hk1 then with probability as least 1\u2212 1T E [\u2211H h=1 { |wR(xkh)|+ |wPh (xkh)| } | Hk1 ] \u2264 \u2211H h=1 (H + 1) \u221a 2 log(2SAT ) max(nk(xkh)\u22122,1) . (11)\nNote the result which we claim in Lemma 3 requires a little more care because V kkh+1 may also depend upon the sampled Pk(xkh), however it poses no real extra difficulty. The key observation is that Lemma 1 already relies upon a sequence of worst-case bounds for any V \u2208 [0, 1]S . We observe that, for a finite horizon MDP, if wPh is positive then it must place more transition mass to higher value states in V kkh+1. We use Lemma 2 to note that the setting considered in Lemma 1 is already a worst-case for future value V kkh+1. We replace Lemma 5 in [21] with a relaxation for V \u2208 {0, h}S . This, together with some elementary concentration results for sub-Gaussian random variables completes the proof of Lemma 3. We can now use Lemma 3 to complete our proof of Theorem\n1 using a pigeonhole principle, E [ Regret(T, \u03c0PSRL,M\u2217) ] \u2264 \u2211dT/He k=1 \u2211H h=1(H + 1) \u221a 2 log(2SAT ) nk(xkh) + 2SA+ 1\n\u2264 5H \u221a SAT log(2SAT ).\nNote at no point in our proof did we require the exact posterior distribution for Mk, our only requirement was that the sampled value function was stochastically optimistic V kk1 <so V \u2217\u22171. This suggests that a similar O\u0303(H \u221a SAT ) bound for a posterior sampling algorithm with a mis-specified Gaussian posterior. We call this Gaussian PSRL and describe this algorithm in Appendix C. It should be clear that we believe Gaussian PSRL is an inferior algorithm to PSRL. However, Gaussian PSRL may be more amenable to analysis and provides some insight into what kinds of frequentist robustness is avaiable to PSRL."}, {"heading": "C Gaussian PSRL", "text": "Our proof for PSRL actually worked by comparing the (correct) posterior estimates for PSRL with a posterior as if the underlying system were Gaussian. This is essentially idential to RLSVI with a tabular basis functions, but without the complications from the shrinkage towards zero [21]. Importantly, RLSVI in that form treats the MDP as if it were timeinhomogeneous, which leads it to have E[Regret(T, \u03c0RLSVI,M\u2217)] = O\u0303( \u221a H3SAT ) rather than Gaussian PSRL which is E[Regret(T, \u03c0GaussPSRL,M\u2217)] = O\u0303( \u221a H2SAT ).\nAlgorithm 3 Gaussian PSRL 1: Input: Reward prior \u03c6 and Dirichlet transition prior counts \u03b10(s, a) \u2208 RS+ for each (s, a). 2: Initialize: H = \u2205, \u03b1(s, a)\u2190 \u03b10(s, a) for all s, a 3: for episode k = 1, 2, .. do 4: Qkh+1(s, a)\u2190 0 for all s, a 5: sample rk(s, a) \u223c \u03c6(\u00b7|H) for all s, a 6: sample wPk (s, a) \u223c N ( \u00b5 = 0, \u03c32 = H 2\n\u03b1(s,a)\n) for all s, a\n7: for time h = H,H \u2212 1, .., 1 do 8: V kh+1 \u2190 max\u03b1Qkh+1(s, \u03b1) for all s 9: Qkh(s, a)\u2190 rk(s, a) + \u03b1(s,a)T V k h+1 \u03b1(s,a)T 1 + w P k (s, a) for all s, a\n10: end for 11: for time h = 1, 2, .., H do 12: take action akh \u2208 arg maxaQkh(skh, a) 13: observe rkh and skh+1 14: update reward history H = H \u222a (skh, akh, rkh) 15: update transition \u03b1(skh, akh)[skh+1] ++ 16: end for 17: end for\nThe proof of these regret bounds is precisely the same as the argument we give for PSRL and Theorem 1. That said, we actually believe that PSRL will satisfy a stronger result O\u0303( \u221a HSAT ) as presented in Conjecture 1."}, {"heading": "D Towards a tight analysis of PSRL", "text": "The key remaining loose piece of our analysis concerns the summation \u2211H h=1 w P h (xkh). Our\ncurrent proof of Theorem 1 bounds each wPh (xkh) indepedently. Each term is O\u0303( \u221a H nk(xkh) )\nand we bound the resulting sum O\u0303(H \u221a\nH nk(xkh) ). However, this approach is very loose. In\nparticular, to repeat our geometric intuition, we have assumed a worst-case hyper-rectangle over all timesteps H when the actual geometry should be an ellipse. It is not possible to sequentially get the \u201cworst-case\u201d transitions O(H) at each and every timestep during an episode, since once your sample gets one such transition then there will\nbe no more future value to deplete. A very similar observation is used by recent analyses in the sample complexity setting [18] and also finite horizon MDPs [8]. This seems to suggest that it should be possible to combine the insights of Lemma 1 with, for example, Lemma 4 of [8] to remove both the \u221a s and the \u221a H from our bounds to prove Conjecture 1.\nWe note that this informal argument would not apply Gaussian PSRL, since it generates wP from some Gaussian posterior which does not satsify the Bellman operators. Therefore, we should be able to find some evidence for this conjecture if we find domains where UCRL, Gaussian PSRL and PSRL all demonstrate their (unique) predicted scalings."}, {"heading": "E Chain experiments", "text": "All of the code and experiments used in this paper are available in full at https://github. com/iosband/TabulaRL. We hope that researchers will find this simple codebase useful for quickly prototyping and experimenting in tabular reinforcement learning simulations. In addition to the results already presented we also investigate the scaling of similar Bayesian learning algorithms BEB [17] and BOLT [1]. We see that neither algorithms scale as gracefully as PSRL, although BOLT comes close. However, as observed in Appendix A, BOLT can perform poorly in more highly stochastic environments. BOLT also requires S-times more computational cost than PSRL or BEB. We include these algorithms in Figure 9.\nE.1 Rescaling confidence sets\nIt is well known that provably-efficient OFU algorithms can perform poorly in practice. In response to this observation, many practicioners suggest rescaling confidence sets to obtain better empirical performance [26, 1, 17]. In Figure 10 we present the performance of several algorithms with confidence sets rescaled \u2208 {0.01, 0.03, 0.1, 0.3, 1}. We can see that rescaling for tighter confidence sets can sometimes give better empirical performance. However, it does not change the fundamental scaling of the algorithm. Also, for aggressive scalings some seeds may not converge at all.\nE.2 Prior sensitivities\nWe ran all of our Bayesian algorithms with uninformative independent priors for rewards and transitions. For rewards, we use r(s, a) \u223c N(0, 1) and updated as if the observed noise were Gaussian with precision \u03c4 = 1\u03c32 = 1. For transitions, we use a uniform Dirichlet prior P (s, a) \u223c Dirchlet(\u03b1). In Figures 11 and 12 we examine the performance of Gaussian PSRL and PSRL on a chain of length N = 10 as we vary \u03c4 and \u03b1 = \u03b101.\nWe find that the algorithms are extremely robust over several orders of magnitude. Only large values of \u03c4 (which means that the agent updates it reward prior too quickly) caused problems for some seeds in this environment. Going forward, developing a more clear frequentist analysis of these Bayesian algorithms is a direction for important future research.\nE.3 Optimistic posterior sampling\nWe compare our implementation of PSRL with a similar optimistic variant which samples K \u2265 1 samples from the posterior and forms the optimistic Q-value over the envelope of sampled Q-values. This algorithm is sometimes called \u201coptimistic posterior sampling\u201d [10]. We experiment with this algorithm over several values of K but find that the resultant algorithm performs very similarly to PSRL, but at an increased computational cost. We display this effect over several magnitudes of K in Figures 13 and 14. This algorithm \u201cOptimistic PSRL\u201d is spiritually very similar to BOSS [2] and previous work had suggested that K > 1 could lead to improved performance. We believe that an important\ndifference is that PSRL, unlike Thompson sampling, should not resample every timestep but previous implementations had compared to this faulty benchmark [10]."}, {"heading": "F Gaussian Dirichlet dominance", "text": "These results are are available in Appendix G of earlier work [21]."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Computational results demonstrate that posterior sampling for reinforcement<lb>learning (PSRL) dramatically outperforms algorithms driven by optimism,<lb>such as UCRL2. We provide insight into the extent of this performance boost<lb>and the phenomenon that drives it. We leverage this insight to establish<lb>an \u00d5(H<lb>\u221a<lb>SAT ) expected regret bound for PSRL in finite-horizon episodic<lb>Markov decision processes, where H is the horizon, S is the number of<lb>states, A is the number of actions and T is the time elapsed. This improves<lb>upon the best previous bound of \u00d5(HS<lb>\u221a<lb>AT ) for any reinforcement learning<lb>algorithm.", "creator": "LaTeX with hyperref package"}}}