{"id": "1607.00225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource", "abstract": "word embeddings have recently seen a strong increase in interest as a result of strong performance gains on attempting a variety of tasks. however, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language - specific tasks. still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing ability for unique observations into language prediction use mechanisms and variability. in this paper we demonstrate the performance of multiple types of embeddings, created with suitable both count and prediction - based architectures written on a variety of corpora, in two opposing language - specific tasks : relation evaluation, and dialect identification. for the latter, we compare unsupervised methods with a traditional, hand - crafted dictionary. with this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, analyzing and then demonstrate again how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task.", "histories": [["v1", "Fri, 1 Jul 2016 12:48:35 GMT  (167kb,D)", "http://arxiv.org/abs/1607.00225v1", "in LREC 2016"]], "COMMENTS": "in LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["st\\'ephan tulkens", "chris emmery", "walter daelemans"], "accepted": false, "id": "1607.00225"}, "pdf": {"name": "1607.00225.pdf", "metadata": {"source": "CRF", "title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource", "authors": ["St\u00e9phan Tulkens"], "emails": ["first.lastname@uantwerpen.be"], "sections": [{"heading": "1 Introduction", "text": "The strong variability of language use within, and across textual media (Collins et al., 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al., 2011). Formal and informal varieties, as well as an abundance of deviations from grammar and spelling conventions in the latter, drastically complicate computationally interpreting the meaning of, and relations between words. This task of understanding lies at\nthe heart of natural language processing (NLP). Neural-network-based language models such as the models in word2vec have recently gained strong interest in NLP due to the fact that they improved state-of-the-art performance on a variety of tasks in the field. Given these developments, we found it surprising that only one set of word embeddings has been publicly released for Dutch (Al-Rfou et al., 2013), which does not offer sufficiently large dimensionality for state-of-the-art performance. The primary goal of this research is thus evaluating word embeddings derived from several popular Dutch corpora and the impact of these sources on their quality, specifically focusing on problems characteristic for Dutch. Word embeddings\u2014being an unsupervised technique\u2014 cannot be easily evaluated without comparing performance in some downstream task. Therefore, we present two novel benchmarking tasks of our own making: a relation identification task analogous to previous evaluations on English, in which the quality of different kinds of word embeddings is measured, and a dialect identification task which measures the usefulness of word embeddings as a linguistic resource for Dutch in particular. In the literature, there has been some debate on the effectiveness of prediction-based embeddings when compared to more classical count-based embedding models (Baroni et al., 2014). As such, we train both count- (SPPMI) and prediction-based (SGNS) models, and compare them to previous efforts in both Dutch and English. Additionally, we make the trained embeddings, the means to construct these models on new corpora, as well as the materials to evaluate their quality available to the research community1.\n1Code and data are accessible via github.com/ clips/dutchembeddings.\nar X\niv :1\n60 7.\n00 22\n5v 1\n[ cs\n.C L\n] 1\nJ ul\n2 01\n6"}, {"heading": "2 Related Work", "text": "An idea mostly brought forward by the earlier distributional semantic models (DSMs), is that the context in which words occur (the distribution of the words surrounding them) can serve as a representation of their meaning, also known as the distributional hypothesis (Harris, 1954). Count based DSMs include LSA (Deerwester et al., 1989, 1990), PLSA (Hofmann, 1999) and LDA (Blei et al., 2003), which first create an explicit matrix of occurrence counts for a number of documents, and then factor this matrix into a lowdimensional, dense representation using Singular Value Decomposition (SVD) (Schu\u0308tze and Silverstein, 1997). A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al., 2006). Over the years, these DSMs have proven adequate as a semantic representation in a variety of NLP tasks.\nAn alternative to these count-based methods can be found in models predicting word identity from a given sentence context. Rather than deriving meaning from the representation of an entire corpus, these construct word representations one sentence at a time. In attempting to predict the current word through its context, the model will learn that words which occur in similar sentence contexts are semantically related. These representations are projected into n-dimensional vector spaces in which more similar words are closer together, and are therefore referred to as word embeddings. Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents. Matrix-based sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful\nand most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a,b). While word2vec often referred to as a single model, it is actually a collection of two different architectures, SkipGram (SG) and Continuous Bag of Words (CBoW), and two different training methods, hierarchical skipgram (HS) and negative sampling (NS). Levy et al. (2015) show that one of the architectures in the word2vec toolkit, SkipGram with Negative Sampling (SGNS) implicitly factorizes a co-occurrence matrix which has been shifted by a factor of log(k), where k is the number of negative samples. Negative samples, in this case, are noise words which do not belong to the context currently being modelled. Subsequently, the authors propose SPPMI, which is the explicit, count-based version of SGNS, i.e. it explicitly creates a co-occurrence matrix, and then shifts all cells in the matrix by log(k). SPPMI is therefore a count-based model which is theoretically equivalent to SGNS. When compared to other methods, such as GloVe (Pennington et al., 2014), SPPMI has showed increased performance (Levy et al., 2015)."}, {"heading": "3 Data", "text": "In our research, we used four large corpora, as well as a combination of three of these corpora to train both SPPMI and word2vec. Additionally, we retrieved a dataset of region-labeled Dutch social media posts, as well as hand-crafted dictionaries for the dialect identification task (see 4.3)."}, {"heading": "3.1 Corpora", "text": "Roularta The Roularta corpus (Roularta Consortium, 2011) was compiled from a set of articles from the Belgian publishing consortium Roularta2. Hence, the articles in this corpus display more characteristics of formal language than the other corpora.\nWikipedia We created a corpus of a Wikipedia dump3. The raw dump was then parsed using a Wikipedia parser, wikiextractor4, and tokenized using Pattern (De Smedt and Daelemans, 2012).\n2www.roularta.be/en 3The 2015.07.03 dump, available at: dumps. wikimedia.org/nlwiki/20150703, retrieved on the 29/06/2015.\n4github.com/attardi/wikiextractor\nSoNaR The SoNaR corpus (Oostdijk et al., 2013) is compiled from a large number of disparate sources, including newsletters, press releases, books, magazines and newspapers. It therefore displays a high amount of variance in terms of word use and style. Unlike the COW corpus (see below), some spelling variation in the SoNaR corpus is automatically corrected, and the frequency of other languages in the corpus is reduced through the use of computational methods.\nCOW The COW corpus (Scha\u0308fer and Bildhauer, 2012) is a 4 billion word corpus which was automatically retrieved from domains from the .be and .nl top level domains in 2011 and 2014 (Scha\u0308fer and Bildhauer, 2012). As such, there is considerable language variability in the corpus. The corpus was automatically tokenized, although we did perform some extra pre-processing (see 3.2).\nSocial Media Dataset The social media dataset was retrieved from several Dutch Facebook pages which all had the peculiarities of a specific dialect or province as their subject. As such, these pages contain a high percentage of dialect language utterances specific to that province or city. For each of these Facebook pages, the region of the page was determined, and all posts on these pages were then labelled as belonging to this region, resulting in a corpus of 96,000 posts. Tokenization and lemmatization of each post was performed using Frog (Bosch et al., 2007). This dataset is noisy in nature, and weakly labelled, as people might use standard language when talking about their province or home town, or will not use the \u2018correct\u2019 dialect on the designated page. This will prove the robustness of our models, and specifically that of our methods for ranking dialects.\nCombined In addition to these corpora, we also created a Combined corpus, which consists of the concatenation of the Roularta, Wikipedia and SoNaR corpora, as described above. We created the Combined corpus to test whether adding more data would improve performance, and to observe whether the pattern of performance on our relation task would change as a result of the concatenation."}, {"heading": "3.2 Preprocessing", "text": "Given that all corpora were already tokenized, all tokens were lowercased, and those solely consisting of non-alphanumeric characters were ex-\ncluded. Furthermore, sentences that were shorter than five tokens were removed, as these do not contain enough context words to provide meaningful results. Some additional preprocessing was performed on the COW corpus: as a side-effect of adapting the already tokenized version of the corpus, the Dutch section contains some incorrectly tokenized plurals, e.g. regio\u2019s, tokenized as regi + o + \u2019 + s. Given this, we chose to remove all tokens that only consisted of one character, except the token u, which is a Dutch pronoun indicating politeness."}, {"heading": "3.3 Dictionaries", "text": "To compare our embeddings to a hand-crafted linguistic resource, we collected a dictionary containing dialect words and sentences, as well as one for standard Dutch. The dialect dictionary was retrieved from MWB (Mijn WoordenBoek)5, which offers user-submitted dialect words, sentences and\n5www.mijnwoordenboek.nl/dialecten, retrieved on 05/10/2015.\nsayings, and their translations. Only the dialect part was retained, and split in single words, which were then stored according to the region it was assigned to by MWB, and the province this region is part of. Any overlapping words across dialects were removed. As a reference dictionary for standard Dutch, the OpenTaal word list6 was used. Additionally, it was used to remove any general words from the dialect dictionaries, i.e. if a word occurred in both the Dutch reference dictionary and a dialect, it was deleted from the dialect. While employing hand-crafted dictionaries can be beneficial in many tasks, producing such resources is expensive, and often takes expert knowledge. Techniques able to use unlabelled data would not only avoid this, but could also prove to be more effective."}, {"heading": "4 Experiments", "text": "For the evaluation of our Dutch word embeddings, we constructed both a novel benchmark task and downstream task, which can be used to evaluate the performance of new embeddings for Dutch."}, {"heading": "4.1 Parameter Estimation", "text": "For each corpus, we trained models using the word2vec implementation (R\u030cehu\u030ar\u030cek and Sojka, 2010; Mikolov et al., 2013a) from gensim7. In order to determine optimal settings for the hyperparameters, several models were trained with different parameter values in parallel and were evaluated in the relation evaluation task (see below). For word2vec the SGNS architecture with a negative sampling of 15, a vector size of 320, and a window size of 11 maximized the quality across all corpora. For the SPPMI models, we created embeddings for the 50,000 most frequent words, experimenting with window sizes of 5 and 10, and shift constants of 1, 5 and 10. For all models, a shift constant of 1 and a window size of 5 produced the best results, the exception being the model based on the Roularta corpus, which performed best with a shift constant of 5 and a window size of 5. Relying on only one set of hyperparameters, as well as the performance of the relation task, could be seen as a point of contention. However, we argue in line with Schnabel et al. (2015) that \u2018true\u2019 performance across unre-\n6Retrieved from www.opentaal.org/bestanden on 19/10/2015, version dated 24/08/2011.\n7radimrehurek.com/gensim/\nlated downstream tasks is complicated to assess. Nevertheless, we regard our approach to be satisfactory for the research presented here. Finally, in addition to our own models, we use the Polyglot embeddings (Al-Rfou et al., 2013) as a baseline, as this is currently the only available set of embeddings for Dutch."}, {"heading": "4.2 Relation Identification", "text": "This task is based on the well-known relation identification dataset which was included with the original word2vec toolkit8, and which includes approximately 20,000 relation identification questions, each of the form: \u201cIf A has a relation to B, which word has the same relation to D?\u201d. As such, it uses the fact that vectors are compositional. For example, given man, woman, and king, the answer to the question should be queen, the relation here being \u2018gender\u2019. In the original set, these questions were divided into several categories, some based on semantic relations, e.g. \u2018opposites\u2019 or \u2018country capitals\u2019, and some based on syntactic relations, e.g. \u2018past tense\u2019. Mirroring this, we created a similar evaluation set for Dutch. Considering the categories used, we aimed to replicate the original evaluation set as closely as possible, while also including some interesting syntactic phenomena in Dutch that are not present in English, such as the formation of diminutives. This resulted in 11 categories; 6 syntactic and 5 semantic. See Table 3 for an overview of the categories and an example for each category. Subsequently, we created a set of words occurring in all corpora\u2014not taking into account word frequency\u2014and retrieved applicable tuples of words from this vocabulary which fit the categories. By only taking words\n8 code.google.com/archive/p/word2vec/\nfrom the intersection of the vocabulary of all models, it is guaranteed that no model is unfairly penalized, assuring that every model is able to produce an embedding for each word in the evaluation set. After selecting approximately 20 tuples of words for each category, the 2-permutation of each set was taken separately, resulting in approximately 10,000 predicates.\nAs an evaluation measure the following procedure was performed for each set of embeddings: For each 2-permutation of tuples in the predicate evaluation set (A,B),(C,D), where A, B, C, and D are distinct words, the following test was performed:\nargmax v\u2208V (sim(v,A\u2212B+D)) (1)\nWhere sim is the cosine similarity:\nsim(w1,w2) =\n\u2192 w1. \u2192 w2\n\u2016w1\u2016\u2016w2\u2016 (2)\nThe objective is thus to find the word v in the vocabulary V which maximizes the similarity score with the vector (A\u2212B+D)."}, {"heading": "4.3 Dialect Identification", "text": "The relationship evaluation set above is a test of the quality of different embeddings. However, this does not prove the effectiveness of word embeddings as a linguistic resource. To counteract this, we created a task in which we try to detect dialectal variation in social media posts. The goal is to measure whether a resource that is equivalent to a hand-crafted resource can be created without any supervision. This identification of text containing\ndialect has been of interest to researchers across different languages such as Spanish (Gonc\u0327alves and Sa\u0301nchez, 2014), German (Scheffler et al., 2014), and Arabic (Lin et al., 2014). The task, then, is to correctly map text with dialect-specific language to the region of origin.\nTo test if the embeddings provide richer information regarding dialects than hand-crafted dictionaries, performance for both approaches needs to be compared. The amount of dialect groups for this task was determined based on the correspondence between those in the dialect dictionaries and a social media test set described in Section 3.3, which resulted in an identification task of at total 16 Dutch and Flemish provinces. For classification of dialect using the embeddings, we use each word in a document to rank the dialects for that document using two simple methods:\nPROV using this method, we classify social media posts as belonging to a province by computing the similarity (as defined in Eq. 2) of every word in the post with all province names, and label the post with the province that was most similar to the highest amount of words. As such, we assume that the province which is most similar to a given word in n-dimensional space is the province to which that word belongs.\nCO like PROV, but including countries, i.e. \u2018Nederland\u2019 and \u2018Belgie\u0308\u2019 as possible targets. Hence, any words closer to either of the country names will not be assigned a province. This has a normalizing effect, as words from the general Dutch vocabulary will not get assigned a province.\nWe tested both these methods for SPPMI and SGNS models. For the dictionary the procedure was largely similar, but instead of distance a lookup through the dictionaries was used."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Relation Identification", "text": "The results of the experiment on the relation identification are presented in Table 3, which shows that all models obtain higher performance on the syntactic categories when compared to the semantic categories, the exception being the \u2018gender\u2019 category, on which all models did comparatively well. Furthermore, performance on \u2018currency\u2019 and \u2018opposites\u2019 was consistently low, the former of which could be explained through low occurrence of currencies in our data. All models outperform the baseline embeddings, which is made all the more problematic by the fact that the vocabulary of the baseline model was fairly small; only 6000 out of the 10,000 predicates were in vocabulary for the model. While it is not possible to estimate how the model would have performed on OOV (Out Of Vocabulary) words, this does demonstrate that it performs well even given a large variety of words.\nComparing different SGNS models, it is safe to say that the biggest determiner of success is corpus size: the model based on the largest corpus obtains the highest score in 7 out of 11 categories, and is also the best scoring model overall. The Roularta embeddings, which are based on the smallest corpus, obtained the lowest score in 7 categories, and the lowest score overall. More interesting is the fact that the Combined corpus, does not manage to outperform the SoNaR corpus individually. This shows that combining corpora can cause interference, and diminish performance.\nGiven the purported equivalence of SPPMI and SGNS, it is surprising that the performance of the SPPMI models was consistently lower than the performance of the SGNS models, although the SPPMI COW model did obtain the best performance on plurals. None of the SPPMI models\nseem to capture information about superlatives or nationalities reliably, with all scores for superlatives close to 0, and (with the exception of the COW corpus) very low scores for nationality.\nFinally, Mikolov et al. (2013a) report comparable performance (51.3 average) on the English variant of the relation dataset. While this does not reveal anything about the relative difficulty of the predicates in the dataset, it does show that our Dutch set yields comparable performance for a similar architecture."}, {"heading": "5.2 Dialect Identification", "text": "As the models based on the COW corpus obtained the best results on the previous task, we used these in the dialect identification task. To determine the validity of using these models on our test data, we report coverage percentages for the models and dictionaries with regards to the test data vocabulary. The dialect part of our hand-crafted dictionaries had a coverage of 11.6%, which shows that the test set includes a large part of dialect words, as expected. The Dutch part of the dictionary covered 23.1% of the corpus. The SGNS model had a coverage of 68.3%, while the SPPMI model had a coverage of 24.4%, which is fairly low when compared to the SGNS model, but still more than either of the dictionaries in separation.\nAs our methods provide a ranking of provinces, both accuracy and mean reciprocal rank (MRR) were used to evaluate classification performance. While accuracy provides us with a fair measure of how well a dialect can be predicted for a downstream task, MRR can indicate if the correct dialect is still highly ranked. As summarized in Table 5, SPPMI obtained the highest accuracy score when countries were included as targets. When MRR was used as a metric, SGNS obtained the highest performance.\nPerformance per dialect is shown in Figure 1. Here, SGNS embeddings outperform the dictionaries in 7 out of 13 cases, and the SPPMI models outperform both the SGNS and dictionary models on several provinces. Regarding SPPMI, the figure reveals a more nuanced pattern of performance: for both tasks, the SPPMI model obtains surprisingly high performance on the ANT dialect, while having good performance on several other dialects. This is offset, however, by the fact that the model attains a score of 0% on 6 provinces, and a very low score on 2 others. An explana-\ntion for this effect is that, being derived from a very large co-occurrence matrix, SPPMI is less able to generalize and more prone to frequency effects. To find support for this claim, we assessed the corpus frequencies of the province names in the COW corpus, and found that the names of all 6 provinces on which the SPPMI models obtained a score of 0 had a corpus frequency which was lower than 700. To illustrate; the name of the first high-frequent province, Overijssel, for which we do not have labeled data, has a frequency of 35218. Conversely, the provinces of Utrecht (UTR), Groningen (GRO), and Antwerpen (ANT) are all very high-frequent, and these are exactly the provinces on which the SPPMI model obtains comparably high performance. While the SGNS model showed a similar pattern of performance, it scored better on provinces whose names have a high corpus frequency, showing that it is influenced by frequency, but still is able to generalize beyond these frequency effects."}, {"heading": "6 Conclusion", "text": "In this paper, we provided state-of-the-art word embeddings for Dutch derived from four corpora, comparing two different algorithms. Having high dimensionality, and being derived from large corpora, we hypothesized they were able to serve as a helpful resource in downstream tasks. To compare the efficiency of the embeddings and the algorithms used for deriving them, we performed two\nseparate tasks: first, a relation identification task, highly similar to the relation identification task presented with the original word2vec toolkit, but adapted to specific phenomena present in the Dutch language. Here we showed to obtain better performance than the baseline model, comparable to that of the English word2vec results for this task. Secondly, a downstream dialect identification task, in which we showed that both methods we use for deriving word embeddings outperform expensive hand-crafted dialect resources using a simple unsupervised procedure."}, {"heading": "7 Acknowledgements", "text": "We would like to thank TextGain for making the corpora of social media posts available to us, and A\u0301kos Ka\u0301da\u0301r for helpful remarks on our work. Part of this research was carried out in the framework of the Accumulate IWT SBO project, funded by the government agency for Innovation by Science and Technology (IWT)."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bul-", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "context-predicting semantic vectors. In ACL (1), pages 238\u2013247.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "Sen\u00e9cal", "J.-S.", "F. Morin", "Gauvain", "J.-L."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "An efficient memorybased morphosyntactic tagger and parser for dutch", "author": ["Bosch", "A. v. d.", "B. Busser", "S. Canisius", "W. Daelemans"], "venue": "LOT Occasional Series, 7:191\u2013206.", "citeRegEx": "Bosch et al\\.,? 2007", "shortCiteRegEx": "Bosch et al\\.", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior research methods, 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy,? 2007", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "Inference in text understanding", "author": ["A. Collins", "J.S. Brown", "K.M. Larkin"], "venue": "BBN report; no. 3684.", "citeRegEx": "Collins et al\\.,? 1977", "shortCiteRegEx": "Collins et al\\.", "year": 1977}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Pattern for python", "author": ["T. De Smedt", "W. Daelemans"], "venue": "The Journal of Machine Learning Research, 13(1):2063\u20132067.", "citeRegEx": "Smedt and Daelemans,? 2012", "shortCiteRegEx": "Smedt and Daelemans", "year": 2012}, {"title": "Computer information retrieval using latent semantic structure", "author": ["S.C. Deerwester", "S.T. Dumais", "G.W. Furnas", "R.A. Harshman", "T.K. Landauer", "K.E. Lochbaum", "L.A. Streeter"], "venue": "US Patent 4,839,853.", "citeRegEx": "Deerwester et al\\.,? 1989", "shortCiteRegEx": "Deerwester et al\\.", "year": 1989}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R.A. Harshman"], "venue": "Journal of the American society for information science, 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Crowdsourcing dialect characterization through twitter", "author": ["B. Gon\u00e7alves", "D. S\u00e1nchez"], "venue": "PloS one, 9(11):e112074.", "citeRegEx": "Gon\u00e7alves and S\u00e1nchez,? 2014", "shortCiteRegEx": "Gon\u00e7alves and S\u00e1nchez", "year": 2014}, {"title": "Contextual bearing on linguistic variation in social media", "author": ["S. Gouws", "D. Metzler", "C. Cai", "E. Hovy"], "venue": "Proceedings of the Workshop on Languages in Social Media, pages 20\u201329. Association for Computational Linguis-", "citeRegEx": "Gouws et al\\.,? 2011", "shortCiteRegEx": "Gouws et al\\.", "year": 2011}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357. ACM.", "citeRegEx": "Hofmann,? 1999", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg,? 2014", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg", "I. Ramat-Gan"], "venue": "CoNLL-2014, page 171.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "The cmu submission for the shared task on language identification in code-switched data", "author": ["Lin", "C.-C.", "W. Ammar", "L. Levin", "C. Dyer"], "venue": "EMNLP 2014, page 80.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "The Written Language Bias in Linguistics", "author": ["P. Linell"], "venue": "Link\u00f6ping University Electronic Press.", "citeRegEx": "Linell,? 1982", "shortCiteRegEx": "Linell", "year": 1982}, {"title": "Producing highdimensional semantic spaces from lexical cooccurrence", "author": ["K. Lund", "C. Burgess"], "venue": "Behavior Research Methods, Instruments, & Computers, 28(2):203\u2013208.", "citeRegEx": "Lund and Burgess,? 1996", "shortCiteRegEx": "Lund and Burgess", "year": 1996}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton,? 2009", "shortCiteRegEx": "Mnih and Hinton", "year": 2009}, {"title": "The construction of a 500million-word reference corpus of contemporary written dutch", "author": ["N. Oostdijk", "M. Reynaert", "V. Hoste", "I. Schuurman"], "venue": "Essential speech and language technology for Dutch, pages 219\u2013247. Springer.", "citeRegEx": "Oostdijk et al\\.,? 2013", "shortCiteRegEx": "Oostdijk et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta,", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka", "year": 2010}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["P. Resnik"], "venue": "J. Artif. Intell. Res.(JAIR), 11:95\u2013 130.", "citeRegEx": "Resnik,? 1999", "shortCiteRegEx": "Resnik", "year": 1999}, {"title": "An improved model of semantic similarity based on lexical co-occurrence", "author": ["D.L. Rohde", "L.M. Gonnerman", "D.C. Plaut"], "venue": "Communications of the ACM, 8:627\u2013633.", "citeRegEx": "Rohde et al\\.,? 2006", "shortCiteRegEx": "Rohde et al\\.", "year": 2006}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["R. Rosenfeld"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rosenfeld,? \\Q2000\\E", "shortCiteRegEx": "Rosenfeld", "year": 2000}, {"title": "Building large corpora from the web using a new efficient tool chain", "author": ["R. Sch\u00e4fer", "F. Bildhauer"], "venue": "LREC, pages 486\u2013493.", "citeRegEx": "Sch\u00e4fer and Bildhauer,? 2012", "shortCiteRegEx": "Sch\u00e4fer and Bildhauer", "year": 2012}, {"title": "Mapping german tweets to geographic regions", "author": ["T. Scheffler", "J. Gontrum", "M. Wegel", "S. Wendler"], "venue": "Proceedings of the NLP4CMC Workshop at Konvens.", "citeRegEx": "Scheffler et al\\.,? 2014", "shortCiteRegEx": "Scheffler et al\\.", "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "Proc. of EMNLP.", "citeRegEx": "Schnabel et al\\.,? 2015", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Projections for efficient document clustering", "author": ["H. Sch\u00fctze", "C. Silverstein"], "venue": "ACM SIGIR Forum, volume 31, pages 74\u201381. ACM.", "citeRegEx": "Sch\u00fctze and Silverstein,? 1997", "shortCiteRegEx": "Sch\u00fctze and Silverstein", "year": 1997}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "The strong variability of language use within, and across textual media (Collins et al., 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 72, "endOffset": 108}, {"referenceID": 20, "context": "The strong variability of language use within, and across textual media (Collins et al., 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 72, "endOffset": 108}, {"referenceID": 28, "context": ", 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 136, "endOffset": 167}, {"referenceID": 30, "context": ", 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 136, "endOffset": 167}, {"referenceID": 13, "context": ", 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al., 2011).", "startOffset": 215, "endOffset": 235}, {"referenceID": 0, "context": "Given these developments, we found it surprising that only one set of word embeddings has been publicly released for Dutch (Al-Rfou et al., 2013), which does not offer sufficiently large dimensionality for state-of-the-art performance.", "startOffset": 123, "endOffset": 145}, {"referenceID": 1, "context": "In the literature, there has been some debate on the effectiveness of prediction-based embeddings when compared to more classical count-based embedding models (Baroni et al., 2014).", "startOffset": 159, "endOffset": 180}, {"referenceID": 14, "context": "An idea mostly brought forward by the earlier distributional semantic models (DSMs), is that the context in which words occur (the distribution of the words surrounding them) can serve as a representation of their meaning, also known as the distributional hypothesis (Harris, 1954).", "startOffset": 267, "endOffset": 281}, {"referenceID": 15, "context": ", 1989, 1990), PLSA (Hofmann, 1999) and LDA (Blei et al.", "startOffset": 20, "endOffset": 35}, {"referenceID": 3, "context": ", 1989, 1990), PLSA (Hofmann, 1999) and LDA (Blei et al., 2003), which first create an explicit matrix of occurrence counts for a number of documents, and then factor this matrix into a lowdimensional, dense representation using Singular Value Decomposition (SVD) (Sch\u00fctze and Silverstein, 1997).", "startOffset": 44, "endOffset": 63}, {"referenceID": 34, "context": ", 2003), which first create an explicit matrix of occurrence counts for a number of documents, and then factor this matrix into a lowdimensional, dense representation using Singular Value Decomposition (SVD) (Sch\u00fctze and Silverstein, 1997).", "startOffset": 208, "endOffset": 239}, {"referenceID": 21, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 5, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al.", "startOffset": 290, "endOffset": 342}, {"referenceID": 16, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al.", "startOffset": 290, "endOffset": 342}, {"referenceID": 29, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al., 2006).", "startOffset": 354, "endOffset": 374}, {"referenceID": 2, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 7, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 24, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 23, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 26, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 35, "context": ", 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP.", "startOffset": 31, "endOffset": 97}, {"referenceID": 8, "context": ", 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP.", "startOffset": 31, "endOffset": 97}, {"referenceID": 1, "context": ", 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP.", "startOffset": 31, "endOffset": 97}, {"referenceID": 26, "context": "When compared to other methods, such as GloVe (Pennington et al., 2014), SPPMI has showed increased performance (Levy et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 17, "context": ", 2014), SPPMI has showed increased performance (Levy et al., 2015).", "startOffset": 48, "endOffset": 67}, {"referenceID": 1, "context": ", 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents.", "startOffset": 8, "endOffset": 135}, {"referenceID": 1, "context": ", 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents. Matrix-based sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful and most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a,b). While word2vec often referred to as a single model, it is actually a collection of two different architectures, SkipGram (SG) and Continuous Bag of Words (CBoW), and two different training methods, hierarchical skipgram (HS) and negative sampling (NS). Levy et al. (2015) show that one of the architectures in the word2vec toolkit, SkipGram with Negative Sampling (SGNS) implicitly factorizes a co-occurrence matrix which has been shifted by a factor of log(k), where k is the number of negative samples.", "startOffset": 8, "endOffset": 794}, {"referenceID": 25, "context": "SoNaR The SoNaR corpus (Oostdijk et al., 2013) is compiled from a large number of disparate sources, including newsletters, press releases, books, magazines and newspapers.", "startOffset": 23, "endOffset": 46}, {"referenceID": 31, "context": "COW The COW corpus (Sch\u00e4fer and Bildhauer, 2012) is a 4 billion word corpus which was automatically retrieved from domains from the .", "startOffset": 19, "endOffset": 48}, {"referenceID": 31, "context": "nl top level domains in 2011 and 2014 (Sch\u00e4fer and Bildhauer, 2012).", "startOffset": 38, "endOffset": 67}, {"referenceID": 4, "context": "Tokenization and lemmatization of each post was performed using Frog (Bosch et al., 2007).", "startOffset": 69, "endOffset": 89}, {"referenceID": 27, "context": "For each corpus, we trained models using the word2vec implementation (\u0158eh\u016f\u0159ek and Sojka, 2010; Mikolov et al., 2013a) from gensim7.", "startOffset": 69, "endOffset": 117}, {"referenceID": 22, "context": "For each corpus, we trained models using the word2vec implementation (\u0158eh\u016f\u0159ek and Sojka, 2010; Mikolov et al., 2013a) from gensim7.", "startOffset": 69, "endOffset": 117}, {"referenceID": 22, "context": "For each corpus, we trained models using the word2vec implementation (\u0158eh\u016f\u0159ek and Sojka, 2010; Mikolov et al., 2013a) from gensim7. In order to determine optimal settings for the hyperparameters, several models were trained with different parameter values in parallel and were evaluated in the relation evaluation task (see below). For word2vec the SGNS architecture with a negative sampling of 15, a vector size of 320, and a window size of 11 maximized the quality across all corpora. For the SPPMI models, we created embeddings for the 50,000 most frequent words, experimenting with window sizes of 5 and 10, and shift constants of 1, 5 and 10. For all models, a shift constant of 1 and a window size of 5 produced the best results, the exception being the model based on the Roularta corpus, which performed best with a shift constant of 5 and a window size of 5. Relying on only one set of hyperparameters, as well as the performance of the relation task, could be seen as a point of contention. However, we argue in line with Schnabel et al. (2015) that \u2018true\u2019 performance across unre-", "startOffset": 95, "endOffset": 1055}, {"referenceID": 0, "context": "Finally, in addition to our own models, we use the Polyglot embeddings (Al-Rfou et al., 2013) as a baseline, as this is currently the only available set of embeddings for Dutch.", "startOffset": 71, "endOffset": 93}, {"referenceID": 12, "context": "This identification of text containing dialect has been of interest to researchers across different languages such as Spanish (Gon\u00e7alves and S\u00e1nchez, 2014), German (Scheffler et al.", "startOffset": 126, "endOffset": 155}, {"referenceID": 32, "context": "This identification of text containing dialect has been of interest to researchers across different languages such as Spanish (Gon\u00e7alves and S\u00e1nchez, 2014), German (Scheffler et al., 2014), and Arabic (Lin et al.", "startOffset": 164, "endOffset": 188}, {"referenceID": 19, "context": ", 2014), and Arabic (Lin et al., 2014).", "startOffset": 20, "endOffset": 38}, {"referenceID": 22, "context": "Finally, Mikolov et al. (2013a) report comparable performance (51.", "startOffset": 9, "endOffset": 32}], "year": 2016, "abstractText": "Word embeddings have recently seen a strong increase in interest as a result of strong performance gains on a variety of tasks. However, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language-specific tasks. Still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing for unique observations into language use and variability. In this paper we demonstrate the performance of multiple types of embeddings, created with both count and prediction-based architectures on a variety of corpora, in two language-specific tasks: relation evaluation, and dialect identification. For the latter, we compare unsupervised methods with a traditional, handcrafted dictionary. With this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task.", "creator": "LaTeX with hyperref package"}}}