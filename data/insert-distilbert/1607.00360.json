{"id": "1607.00360", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "A scaled Bregman theorem with applications", "abstract": "bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. this paper ref explores the use of standard bregman divergences to establish reductions between such algorithms and support their analyses. we present by a new scaled partial isodistortion theorem directly involving bregman interval divergences ( scaled bregman theorem for short ) which shows it that certain \" bregman distortions'\" ( employing a potentially non - convex generator ) may probably be exactly re - written as a formally scaled bregman divergence representation computed over transformed data. admissible distortions include geodesic distances on curved manifolds and projections without or gauge - normalisation, while defined admissible data operations include scalars, vectors and matrices.", "histories": [["v1", "Fri, 1 Jul 2016 19:27:28 GMT  (1273kb,D)", "http://arxiv.org/abs/1607.00360v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["richard nock", "aditya krishna menon", "cheng soon ong"], "accepted": true, "id": "1607.00360"}, "pdf": {"name": "1607.00360.pdf", "metadata": {"source": "CRF", "title": "A scaled Bregman theorem with applications", "authors": ["Richard Nock", "Aditya Krishna Menon", "Cheng Soon Ong"], "emails": ["chengsoon.ong}@data61.csiro.au"], "sections": [{"heading": null, "text": "Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning."}, {"heading": "1 Introduction: Bregman divergences as a reduction tool", "text": "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. In recent years, Bregman divergences have arisen in procedures for convex optimisation [Beck and Teboulle, 2003], online learning [Cesa-Bianchi and Lugosi, 2006, Chapter 11] clustering [Banerjee et al., 2005], matrix approximation [Dhillon and Tropp, 2008], class-probability estimation [Buja et al., 2005, Nock and Nielsen, 2009, Reid and Williamson, 2010, 2011], density ratio estimation [Sugiyama et al., 2012], boosting [Collins et al., 2002], variational inference [Hern\u00e1ndez-Lobato et al., 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions \u03d5 with derivative \u03d5\u2032, conjugate \u03d5?, and divergence D\u03d5:\n\u2022 the triangle equality: D\u03d5(x\u2016y) +D\u03d5(y\u2016z)\u2212D\u03d5(x\u2016z) = (\u03d5\u2032(z)\u2212 \u03d5\u2032(y))(x\u2212 y);\n\u2022 the dual symmetry property: D\u03d5(x\u2016y) = D\u03d5?(\u03d5\u2032(y)\u2016\u03d5\u2032(x));\n\u2022 the right-centroid (population minimizer) is the average: arg min\u00b5 E[D\u03d5(X\u2016\u00b5)] = E[X].\nCasting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence.\nAnother intriguing possibility is that one may derive reductions amongst learning problems by connecting their underlying Bregman minimisations. Menon and Ong [2016] recently established how (binary) density ratio estimation (DRE) can be exactly reduced to class-probability estimation (CPE). This was facilitated by interpreting CPE as a Bregman minimisation [Buja et al., 2005, Section 19], and a new property of Bregman divergences \u2014\nar X\niv :1\n60 7.\n00 36\n0v 1\n[ cs\n.L G\n] 1\nJ ul\n2 01\n6\nMenon and Ong [2016, Lemma 2] showed that for any twice differentiable scalar convex \u03d5, for g(x) = 1 + x and \u03d5\u2020(x) . = g(x) \u00b7 \u03d5(x/g(x)), g(x) \u00b7D\u03d5(x/g(x)\u2016y/g(y)) = D\u03d5\u2020(x\u2016y). (1) Since the binary class-probability function \u03b7(x) = Pr(Y = 1|X = x) is related to the class-conditional density ratio r(x) = Pr(X = x|Y = 1)/Pr(X = x|Y = \u22121) via Bayes\u2019 rule as \u03b7(x) = r(x)/g(r(x)), any \u03b7\u0302 with small D\u03d5(\u03b7\u2016\u03b7\u0302) implicitly produces an r\u0302 with low D\u03d5\u2020(r\u2016r\u0302) i.e. a good estimate of the density ratio. The Bregman property of Equation 1 thus establishes a reduction from DRE to CPE. Two natural questions arise from this analysis: can we generalise Equation 1 to other g(\u00b7), and if so, can we similarly relate other problems to each other?\nThis paper presents a new Bregman identity (Theorem 1), the scaled Bregman theorem, a significant generalisation of Menon and Ong [2016, Lemma 2]. It shows that general distortions D\u03d5\u2020 \u2013 which are not necessarily convex, positive, bounded or symmetric \u2013 may be re-expressed as a Bregman divergence D\u03d5 computed over transformed data, where this transformation can be as simple as a projection or normalisation by a gauge, or more involved like the exponential map on lifted coordinates for a curved manifold. Interestingly, candidate distortions include geodesic distances on curved manifolds. Equivalently, Theorem 1 shows various distortions can be \u201creverse engineered\u201d as Bregman divergences (despite appearing prima facie to be a very different object), and thus inherit their good properties. Hence, Bregman divergences can embed several distances in a different \u2014 and arguably less involved \u2014 way than the transformations known to date [Acharyya et al., 2013].\nAs with the aforementioned key properties of Bregman divergences, Theorem 1 has potentially wide applicability. We present three such novel applications (see Table 1) to vastly different problems:\n\u2022 a reduction of multiple density ratio estimation to multiclass-probability estimation (\u00a73), generalising the results of Menon and Ong [2016] for the binary label case,\n\u2022 a projection-free yet norm-enforcing mirror gradient algorithm (enforced norms are those of mirrored vectors and of the offset) with guarantees for adaptive filtering (\u00a74), and\n\u2022 a seeding approach for clustering on positively or negatively (constant) curved manifolds based on a popular seeding for flat manifolds and with the same approximation guarantees (\u00a75).\nExperiments on each of these domains (\u00a76) validate our analysis. The Supplementary Material details the proofs of all results, provides the experimental results in extenso and some additional (nascent) applications of the scaled Bregman theorem to exponential families and computational geometry."}, {"heading": "2 Main result: the scaled Bregman theorem", "text": "In the remaining, [k] .= {0, 1, ..., k} and [k]\u2217 .= {1, 2, ..., k} for k \u2208 N. For any differentiable (but not necessarily convex) \u03d5 : X\u2192 R, we define the Bregman \u201cdistortion\u201d D\u03d5 as\nD\u03d5(x\u2016y) .= \u03d5(x)\u2212 \u03d5(y)\u2212 (x\u2212 y)>\u2207\u03d5(y) . (2)\nWhen \u03d5 is convex, D\u03d5 is the familiar Bregman divergence with generator \u03d5.\nWithout further ado, we present our main result.\nTheorem 1 Let, \u03d5 : X\u2192 R be convex differentiable, and g : X\u2192 R\u2217 be differentiable. Then,\ng(x) \u00b7D\u03d5 ( x/g(x) \u2225\u2225 y/g(y) ) = D\u03d5\u2020 ( x \u2225\u2225 y ) ,\u2200x,y \u2208 X , (3)\nwhere \u03d5\u2020(x) .= g(x) \u00b7 \u03d5 (x/g(x)) , (4)\nif and only if (i) g is affine on X, and/or (ii) for every z \u2208 Xg .= {(1/g(x)) \u00b7 x : x \u2208 X},\n\u03d5 (z) = z>\u2207\u03d5(z) . (5)\nTable 2 presents some examples of (sometimes involved) triplets (D\u03d5, D\u03d5\u2020 , g) for which Equation 3 holds; related proofs are in Appendix C. If we fold g into D\u03d5 in the left hand-side of eq. (3), then Theorem 1 states a scaled isodistortion (sometimes it turns out to be equivalently an adaptive scaled isometry, see Appendix I) property between X and Xg . Because D\u03d5 is such an important object, we do not perform this folding and refer to Theorem 1 as the scaled Bregman theorem for short.\nRemark. If Xg is a vector space, \u03d5 satisfies Equation 5 if and only if it is positive homogeneous of degree 1 on Xg (i.e. \u03d5(\u03b1z) = \u03b1 \u00b7 \u03d5(z) for any \u03b1 > 0) from Euler\u2019s homogenous function theorem. When Xg is not a vector space, this only holds for \u03b1 such that \u03b1z \u2208 Xg as well. We thus call the gradient condition of Equation 5 \u201crestricted positive homogeneity\u201d for simplicity.\nRemark. Appendix D gives a \u201cdeep composition\u201d extension of Theorem 1.\nFor the special case where X = R, and g(x) = 1 + x, Theorem 1 is exactly Menon and Ong [2016, Lemma 2] (c.f. Equation 1). We wish to highlight a few points with regard to our more general result. First, the \u201cdistortion\u201d generator \u03d5\u2020 may be1 non-convex, as the following illustrates.\nExample. Suppose \u03d5(x) = 12\u2016x\u201622 corresponds to the generator for squared Euclidean distance. Then, for g(x) = 1 + 1>x, we have \u03d5\u2020(x) = 12 \u00b7 \u2016x\u201622 1+1>x , which is non-convex on X = R d.\nWhen \u03d5\u2020 is non-convex, the right hand side in Equation 3 is an object that ostensibly bears only a superficial similarity to a Bregman divergence; it is somewhat remarkable that Theorem 1 shows this general \u201cdistortion\u201d between a pair (x,y) to be entirely equivalent to a (scaling of a) Bregman divergence between some transformation of the points. Second, when g is linear, Equation 3 holds for any convex \u03d5. (This was the case considered in Menon and Ong [2016].) When g is non-linear, however, \u03d5 must be chosen carefully so that (\u03d5, g) satisfies the restricted homogeneity conditon2 of Equation 5. In general, given a convex \u03d5, one can \u201creverse engineer\u201d a suitable g to guarantee this conditon, as illustrated by the following example.\nExample. Suppose3 \u03d5(x) = (1 + \u2016x\u201622)/2. Then, Equation 5 requires that \u2016x\u201622 = 1 for every x \u2208 Xg , i.e. Xg is (a subset of) the unit sphere. This is afforded by the choice g(x) = \u2016x\u20162. Third, Theorem 1 is not merely a mathematical curiosity: we now show that it facilitates novel results in three very different domains, namely estimating multiclass density ratios, constrained online optimisation, and clustering data on a manifold with non-zero curvature. We discuss nascent applications to exponential families and computational geometry in Appendices E and F."}, {"heading": "3 Multiclass density-ratio estimation via class-probability estimation", "text": "Given samples from a number of densities, density ratio estimation concerns estimating the ratio between each density and some reference density. This has applications in the covariate shift problem wherein the train and test distributions over instances differ [Shimodaira, 2000]. Our first application of Theorem 1 is to show how density ratio estimation can be reduced to class-probability estimation [Buja et al., 2005, Reid and Williamson, 2010].\n1Evidently, \u03d5\u2020 is convex iff g is non-negative, by Equation (3) and the fact that a function is convex iff its Bregman \u201cdistortion\u201d is nonnegative [Boyd and Vandenberghe, 2004, Section 3.1.3].\n2We stress that this condition only needs to hold on Xg \u2286 X.; it would not be really interesting in general for \u03d5 to be homogeneous everywhere in its domain, since we would basically have \u03d5\u2020 = \u03d5.\n3The constant 1/2 added in \u03d5 does not change D\u03d5, since a Bregman divergence is invariant to affine terms; removing this however would make the divergences D\u03d5 and D\u03d5\u2020 differ by a constant.\nTo proceed, we fix notation. For some integer C \u2265 1, consider a distribution P(X,Y) over an (instance, label) space X \u00d7 [C]. Let ({Pc}Cc=1,\u03c0) be densities giving P(X|Y = c) and P(Y = c) respectively, and M giving P(X) accordingly. Fix c\u2217 \u2208 [C] a reference class, and suppose for simplicity that c\u2217 = C. Let \u03c0\u0303 \u2208 4C\u22121 such that \u03c0\u0303c . = \u03c0c/(1 \u2212 \u03c0C). Density ratio estimation [Sugiyama et al., 2012] concerns inferring the vector r(x) \u2208 RC\u22121 of density ratios relative to C, with rc(x) .= P(X = x|Y = c)/P(X = x|Y = C) , while classprobability estimation [Buja et al., 2005] concerns inferring the vector \u03b7(x) \u2208 RC\u22121 of class-probabilities, with \u03b7c(x) . = P(Y = c|X = x)/\u03c0\u0303c . In both cases, we estimate the respective quantities given an iid sample S \u223c P(X,Y)N . The genesis of the reduction from density ratio to class-probability estimation is the fact that r(x) = (\u03c0C/(1\u2212 \u03c0C)) \u00b7 \u03b7(x)/\u03b7C(x). In practice one will only have an estimate \u03b7\u0302, typically derived by minimising a suitable loss on the given S [Williamson et al., 2014], with a canonical example being multiclass logistic regression. Given \u03b7\u0302, it is natural to estimate the density ratio via:\nr\u0302(x) = \u03b7\u0302(x)\n\u03b7\u0302C(x) . (6)\nWhile this estimate is intuitive, to establish a formal reduction we must relate the quality of r\u0302 to that of \u03b7\u0302. Since the minimisation of a suitable loss for class-probability estimation is equivalent to a Bregman minimisation [Buja et al., 2005, Section 19], [Williamson et al., 2014, Proposition 7], this is however immediate by Theorem 1, as shown below.\nLemma 2 Given a class-probability estimator \u03b7\u0302 : X \u2192 [0, 1]C\u22121, let the density ratio estimator r\u0302 be as per Equation 6. Then for any convex differentiable \u03d5 : [0, 1]C\u22121 \u2192 R,\nEX\u223cM [D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))] = (1\u2212 \u03c0C) \u00b7 EX\u223cPC [ D\u03d5\u2020(r(X)\u2016r\u0302(X)) ] (7)\nwhere \u03d5\u2020 is as per Equation 4 with g(x) .= \u03c0C/(1\u2212 \u03c0C) + \u03c0\u0303>x .\nLemma 2 generalises Menon and Ong [2016, Proposition 3], which focussed on the binary case with \u03c0 = 1/2. (See Appendix G for a review of that result.) Unpacking the Lemma, the LHS in Equation 7 represents the object minimised by some suitable loss for class-probability estimation. Since g is affine, we can use any convex, differentiable \u03d5, and so can use any suitable class-probability loss to estimate \u03b7\u0302. Lemma 2 thus implies that producing \u03b7\u0302 by minimising any class-probability loss equivalently produces an r\u0302 as per Equation 6 that minimises a Bregman divergence to the true r. Thus, Theorem 1 provides a reduction from density ratio to multiclass probability estimation.\nWe now detail two applications where g(\u00b7) is no longer affine, and \u03d5 must be chosen more carefully."}, {"heading": "4 Dual norm mirror descent: projection-free online learning on Lp balls", "text": "A substantial amount of work in the intersection of machine learning and convex optimisation has focused on constrained optimisation within a ball [Shalev-Shwartz et al., 2007, Duchi et al., 2008]. This optimisation is typically via projection operators that can be expensive to compute [Hazan and Kale, 2012, Jaggi, 2013]. We now show that gauge functions can be used as an inexpensive alternative, and that Theorem 1 easily yields guarantees for this procedure in online learning.\nWe consider the adaptive filtering problem, closely related to the online least squares problem with linear predictors [Cesa-Bianchi and Lugosi, 2006, Chapter 11]. Here, over a sequence of T rounds, we observe some xt \u2208 X. We must the predict a target value y\u0302t = w>t\u22121xt using our current weight vectorwt\u22121. The true target yt = u\n>xt + t is then revealed, where t is some unknown noise, and we may update our weight to wt. Our goal is to minimise the regret of the sequence {wt}Tt=0,\nR(w1:T |u) .= T\u2211\nt=1\n( u>xt \u2212w>t\u22121xt )2 \u2212 T\u2211\nt=1\n( u>xt \u2212 yt )2 . (8)\nLet q \u2208 (1, 2] and p be such that 1/p + 1/q = 1. For \u03d5 .= 12 \u00b7 \u2016x\u20162q and loss `t(w) = 12 \u00b7 (yt \u2212 w>xt)2, the p-LMS algorithm [Kivinen et al., 2006] employs the stochastic mirror gradient updates\nwt . = argmin w \u03b7t \u00b7 `t(w) +D\u03d5(w\u2016wt\u22121) = (\u2207\u03d5)\u22121 (\u2207\u03d5(wt\u22121)\u2212 \u03b7t \u00b7 \u2207`t) , (9)\nwhere \u03b7t is a learning rate to be specified by the user. Kivinen et al. [2006, Theorem 2] shows that for appropriate \u03b7t, one has R(w1:T |u) \u2264 (p\u2212 1) \u00b7maxx\u2208X \u2016x\u20162p \u00b7 \u2016u\u20162q . The p-LMS updates do not provide any explicit control on \u2016wt\u2016, i.e. there is no regularisation. Experiments (\u00a76) suggest that leaving \u2016wt\u2016 uncontrolled may not be a good idea as the increase of the norm sometimes prevents (significant) updates (9). Also, the wide success of regularisation in machine learning calls for regularised variants that retain the regret guarantees and computational efficiency of p-LMS. (Adding a projection step to Equation 9 would not achieve both.) We now do just this. For fixed W > 0, let \u03d5 .= (1/2)(W 2 + \u2016x\u20162q), a translation of that used in p-LMS. Invoking Theorem 1 with the admissible gq(x) = ||x||q/W yields \u03d5\u2020 .= \u03d5\u2020q = W\u2016x\u2016q (see Table 2). Using the fact that Lp and Lq norms are dual of each other, we replace Equation 9 by:\nwt . = \u2207\u03d5\u2020p ( \u2207\u03d5\u2020q(wt\u22121)\u2212 \u03b7t \u00b7 \u2207`t ) . (10)\nSee Lemma 6 of the Appendix for the simple forms of \u2207\u03d5\u2020{p,q}. We call update (10) the dual norm p-LMS (DN-pLMS) algorithm, noting that the dual refers to the polar transform of the norm, and g stems from a gauge normalization for Bq(W ), the closed Lq ball with radiusW > 0. Namely, we have \u03b3GAU(x) = W/\u2016x\u2016q = g(x)\u22121 for the gauge \u03b3GAU(x) . = sup{z \u2265 0 : z \u00b7 x \u2208 Bq(W )}, so that \u03d5\u2020q implicitly performs gauge normalisation of the data. This update is no more computationally expensive than Equation 9 \u2014 we simply need to compute the p- and q-norms of appropriate terms \u2014 but, crucially, automatically constrains the norms of wt and its image by\u2207\u03d5\u2020q .\nLemma 3 For the update in Equation 10, \u2016wt\u2016q = \u2016\u2207\u03d5\u2020q(wt)\u2016p = W, \u2200t > 0.\nLemma 3 is remarkable, since nowhere in Equation 10 do we project onto the Lq ball. Nonetheless, for the DN-pLMS updates to be principled, we need a similar regret guarantee to the original p-LMS. Fortunately, this may be done using Theorem 1 to exploit the original proof of Kivinen et al. [2006]. For any u \u2208 Rd, define the q-normalised regret of {wt}Tt=0 by\nRq(w1:T |u) .= T\u2211\nt=1\n( (1/gq(u)) \u00b7 u>xt \u2212w>t\u22121xt )2 \u2212 T\u2211\nt=1\n( (1/gq(u)) \u00b7 u>xt \u2212 yt )2 . (11)\nWe have the following bound on Rq for the DN-pLMS updates. (We cannot expect a bound on the unnormalised R(\u00b7) of Equation 8, since by Lemma 3 we can only compete against norm W vectors.)\nLemma 4 Pick any u \u2208 Rd, p, q satisfying 1/p + 1/q = 1 and p > 2, and W > 0. Suppose \u2016xt\u2016p \u2264 Xp and |yt| \u2264 Y,\u2200t \u2264 T . Let {wt} be as per Equation 10, using learning rate\n\u03b7t . = \u03b3t \u00b7\nW\n4(p\u2212 1) max{W,Xp}XpW + |yt \u2212w>t\u22121xt|Xp , (12)\nfor any desired \u03b3t \u2208 [1/2, 1]. Then,\nRq(w1:T |u) \u2264 4(p\u2212 1)X2pW 2 + (16p\u2212 8) max{W,Xp}X2pW + 8Y X2p . (13)\nSeveral remarks can be made. First, the bound depends on the maximal signal value Y , but this is the maximal signal in the observed sequence, so it may not be very large in practice; if it is comparable to W , then our bound is looser than Kivinen et al. [2006] by just a constant factor. Second, the learning rate is adaptive in the sense that its choice depends on the last mistake made. There is a nice way to represent the \u201coffset\u201d vector \u03b7t \u00b7 \u2207`t in eq. (10), since we have, for Q\u2032\u2032 .= 4(p\u2212 1) max{W,Xp}XpW ,\n\u03b7t \u00b7 \u2207`t = W \u00b7 |yt \u2212w>t\u22121xt|Xp\nQ\u2032\u2032 + |yt \u2212w>t\u22121xt|Xp \u00b7 sign(yt \u2212w>t\u22121xt) \u00b7\n( 1 Xp \u00b7 x ) , (14)\nso the Lp norm of the offset is actually equal to W \u00b7Q, where Q \u2208 [0, 1] is all the smaller as the vector w. gets better. Hence, the update in eq. (10) controls in fact all norms (that of w., its image by\u2207\u03d5\u2020q and the offset). Third, because of the normalisation of u, the bound actually does not depend on u, but on the radius W chosen for the Lq ball."}, {"heading": "5 Clustering on a manifold via data transformation", "text": "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011]. We consider two fundamental manifolds investigated by Galperin [1993] to compute centers of mass from relativistic theory: the sphere Sd and the hyperboloid Hd, the former being of positive curvature, and the latter of negative curvature. Applications involving these specific manifolds are numerous in text processing, computer vision, geometric modelling, computer graphics, to name a few [Buss and Fillmore, 2001, Dhillon and Modha, 2001, Endo and Miyamoto, 2015, Kuang et al., 2014, Rong et al., 2010, Shahani et al., 2015, Straub et al., 2015a,b,c, 2014]. We emphasize the fact that the clustering problem has significant practical impact for d as small as 2 in computer vision [Straub et al., 2014].\nThe problem is non-trivial for two separate reasons. First, the ambient space, i.e. the space of registration of the input data, is often implicitly Euclidean and therefore not the manifold [Dhillon and Modha, 2001]: if the mapping to the manifold is not carefully done, then geodesic distances measured on the manifold may be inconsistent with respect to the ambient space. Second, the fact that the manifold has non-zero curvature essentially prevents the direct use of Euclidean optimization algorithms [Zhang and Sra, 2016] \u2014 put simply, the average of two points\nthat belong to a manifold does not necessarily belong to the manifold, so we have to be careful on how to compute centroids for hard clustering [Galperin, 1993, Nock et al., 2016, Rong et al., 2010, Schwander and Nielsen, 2013].\nWhat we show now is that Riemannian manifolds with constant sectional curvature may be clustered with the k-means++ seeding for flat manifolds [Arthur and Vassilvitskii, 2007], without even touching a line of the algorithm. To formalise the problem, we need three key components of Riemannian geometry: tangent planes, exponential map and geodesics [Amari and Nagaoka, 2000]. We assume that the ambient space is a tangent plane to the manifold M, which conveniently makes it look Euclidean (see Figure 1). The point of tangency is called q, and the tangent plane TqM. The exponential map, expq : TqM\u2192M, performs a distance preserving mapping: the geodesic length between q and expq(x) in M is the same as the Euclidean length between q and x in TqM. Our clustering objective is to find C .= {c1, c2, ...ck} \u2282M such that Drec(S : C) = infC\u2032\u2282M,|C\u2032|=kDrec(S,C\u2032), with\nDrec(S,C) . = \u2211 i\u2208[m]\u2217 minj\u2208[k]\u2217 Drec(expq(xi), cj) , (15)\nwhere Drec is a reconstruction loss, a function of the geodesic distance between expq(xi) and cj . We use two loss functions defined from Galperin [1993] and used in machine learning for more than a decade [Dhillon and Modha, 2001]:\nR+ 3 Drec(y, c) .= {\n1\u2212 cosDG(y, c) for M = Sd coshDG(y, c)\u2212 1 for M = Hd . (16)\nHere, DG(y, c) is the corresponding geodesic distance of M between y and c. Figure 1 shows that Drec(y, c) is the orthogonal distance between TcM and y when M = Sd. The solution to the clustering problem in eq. (15) is therefore the one that minimizes the error between tangent planes defined at the centroids, and points on the manifold.\nIt turns out that both distances in 16 can be engineered as Bregman divergences via Theorem 1, as seen in Table 2. Furthermore, they imply the same \u03d5, which is just the generator of Mahalanobis distortion, but a different g. The construction involves a third party, a lifting map (lift(.)) that increases the dimension by one. The Sphere lifting map Rd 3 x 7\u2192 xS \u2208 Rd+1 is indicated in Table 3 (left). The new coordinate depends on the norm of x. The Hyperbolic lifting map, Rd 3 x 7\u2192 xH \u2208 Rd \u00d7 C, involves a pure imaginary additional coordinate, is indicated in in Table 3 (right, with a slight abuse of notation) and Figure 1. Both xS and xH live on a d-dimensional manifold, depicted in Figure 1. When they are scaled by the corresponding g.(.), they happen to be mapped to Sd or Hd, respectively, by what happens to be the manifold\u2019s exponential map for the original x (see Appendix C).\nTheorem 1 is interesting in this case because \u03d5 corresponds to a Mahalanobis distortion: this shows that kmeans++ seeding [Arthur and Vassilvitskii, 2007, Nock et al., 2008] can be used directly on the scaled coordinates (g\u22121{S,H}(x\n{S,H}) \u00b7 x{S,H}) to pick centroids that yield an approximation of the global optimum for the clustering problem on the manifold which is just as good as the original Euclidean approximation bound [Arthur and Vassilvitskii, 2007].\nLemma 5 The expected potential of Sk-means++ seeding over the random choices of C+ satisfies:\nE[Drec(S : C)] \u2264 8(2 + log k) \u00b7 inf C\u2032\u2208Sd Drec(S : C \u2032) . (17)\nThe same approximation bounds holds for Hk-means++ seeding on the hyperboloid (C\u2032,C+ \u2208 Hd).\nLemma 5 is notable since it was only recently shown that such a bound is possible for the sphere [Endo and Miyamoto, 2015], and to our knowledge, no such approximation quality is known for clustering on the hyperboloid [Rong et al., 2010, Schwander and Nielsen, 2013]. Notice that Lloyd iterations on non-linear manifolds would require repetitive renormalizations to keep centers on the manifold [Dhillon and Modha, 2001], an additional disadvantage compared to clustering on flat manifolds that {G,K}-means++ seedings do not bear."}, {"heading": "6 Experimental validation", "text": "We present some experiments validating our theoretical analysis for the applications above.\nMultiple density ratio estimation. See Appendix H.1 for experiments in this domain.\nDual norm p-LMS (DN-p-LMS). We ran p-LMS and the DN-pLMS of \u00a74 on the experimental setting of Kivinen et al. [2006]. We refer to that paper for an exhaustive description of the experimental setting, which we briefly summarize: it is a noisy signal processing setting, involving a dense or a sparse target. We compute, over the signal received, the error of our predictor on the signal. We keep all parameters as they are in [Kivinen et al., 2006], except for one: we make sure that data are scaled to fit in a Lp ball of prescribed radius, to test the assumption related in [Kivinen et al., 2006] that fixing the learning rate \u03b7t is not straightforward in p-LMS. Knowing the true value of Xp, we then scale it by a misestimation factor \u03c1, typically in [0.1, 1.7]. We use the same misestimation in DN-p-LMS. Thus, both algorithms suffer the same source of uncertainty. Also, we periodically change the signal (each 1000 iterations), to assess the performances of the algorithms in tracking changes in the signal.\nExperiments, given in extenso in Appendix H.2, are sumarized in Table 4. The following trends emerge: in the mid to long run, DN-p-LMS is never beaten by p-LMS by more than a fraction of percent. On the other hand, DN-p-LM can beat p-LMS by very significant differences (exceeding 40%), in particular when p < 2, i.e. when we are outside the regime of the proof of Kivinen et al. [2006]. This indicates that significantly stronger and more general results than the one of Lemma 4 may be expected. Also, it seems that the problem of p-LMS lies in an \u201cexploding\u201d norm problem: in various cases, we observe that \u2016wt\u2016 (in any norm) blows up with t, and this correlates with a very significant degradation of its performances. Clearly, DN-p-LMS does not have this problem since all relevant norms are under tight control. Finally, even when the norm does not explode, DN-p-LMS can still beat p-LMS, by less important differences though. Of course, the output of p-LMS can repeatedly be normalised, but the normalisation would escape the theory of Kivinen et al. [2006] and it is not clear which kind of normalisation would bring the best results.\nClustering on the sphere. For k \u2208 [50]\u2217, we simulate on T0S2 a mixture of spherical Gaussian and uniform densities with 2k components. We run three algorithms: (i) SKM [Dhillon and Modha, 2001] on the data embedded on S2 with random (Forgy) initialization, (ii), Sk-means++ and (iii) SKM with Sk-means++ initialisation. Results are averaged over the algorithms\u2019 runs.\nTable 5 (left) displays that using Sk-means++ as initialization for SKM brings a very significant leverage over SKM alone, since we almost divide the k-means potential by a factor 2 on some runs. The right plot of Table 5 shows that S-k-means++ consistently reduces the k-means potential by at least a factor 2 over Forgy. The left plot in Table 6 displays that even when it has converged, SKM does not necessarily beat Sk-means++. Finally, the center+right plots in Table 6 display that even when it does beat Sk-means++ when it has converged, the iteration number after which SKM beats Sk-means++ increases with k, and in the worst case may exceed the average number of iterations needed for SKM to converge (we stopped SKM if relative improvement is not above 1o/oo)."}, {"heading": "7 Conclusion", "text": "We presented a new scaled Bregman identity (Theorem 1), and used it to derive novel results in multiple density ratio estimation, adaptive filtering, and clustering on curved manifolds. We believe that, like other established properties of Bregman divergences, there is potential for several other applications of the result; Appendix E, F present preliminary thoughts in this direction."}, {"heading": "A Additional helper lemmas", "text": "We begin with some helper lemmas that will be used in some of the proofs. In what follows, let\n\u03d5q(w) = (1/2)(W 2 + \u2016w\u20162q) \u03d5\u2020q(w) = W \u00b7 \u2016w\u2016q\nfor some W > 0 and p, q \u2208 (1,\u221e) such that 1/p+ 1/q = 1.\nA.1 Properties of \u03d5q and \u03d5\u2020q\nWe use the following properties of \u03d5,\u03d5\u2020.\nLemma 6 For any w,\n\u2207\u03d5q(w) = \u2016w\u20162\u2212qq \u00b7 sign(w)\u2297 |w|q\u22121 \u2207\u03d5\u2020q(w) = W \u00b7 \u2016w\u20161\u2212qq \u00b7 sign(w)\u2297 |w|q\u22121,\nwhere \u2297 denotes Hadamard product.\nProof The first identity was shown in Kivinen et al. [2006, Example 1]. The second identity follows from a simple calculation.\nThis implies the follows useful relation between the gradients of \u03d5q and \u03d5\u2020q .\nCorollary 7 For any w,\n\u2207\u03d5q(w) = (\u2016w\u2016q/W ) \u00b7 \u2207\u03d5\u2020q(w) \u2016\u2207\u03d5\u2020q(w)\u2016p = W \u2016\u2207\u03d5q(w)\u2016p = \u2016w\u2016q.\nProof [Proof of Corollary 7] The proof follows by direct application of Lemma 6 and the definition of p, q. Note the third identity was shown in Kivinen et al. [2006, Appendix I].\nAs a consequence, we conclude that the gradients of \u03d5 and \u03d5\u2020 coincide when considering vectors on the W -sphere.\nLemma 8 For any \u2016w\u2016q = W , \u2207\u03d5q (w) = \u2207\u03d5\u2020q (w) .\nProof This follows from the relation between\u2207\u03d5q and \u2207\u03d5\u2020q from Lemma 6.\nFinally, we have the following result about the composition of gradients.\nLemma 9 For any w,\n\u2207\u03d5q \u25e6 \u2207\u03d5\u2020p(w) = \u2207\u03d5\u2020q \u25e6 \u2207\u03d5\u2020p(w) = W\n\u2016w\u2016p \u00b7w.\nProof For the first identity, applying Lemma 6 twice,\n\u2207\u03d5q \u25e6 \u2207\u03d5\u2020p(w) = 1 \u2016\u2207\u03d5\u2020p(w)\u2016q\u22122q \u00b7 sign(\u2207\u03d5\u2020p(w))\u2297 |\u2207\u03d5\u2020p(w)|q\u22121\n= 1 W q\u22122q \u00b7 sign(w)\u2297 W\nq\u22121\n\u2016w\u2016(p\u22121)(q\u22121)p \u00b7 |w|(p\u22121)(q\u22121)\n= W\n\u2016w\u2016p \u00b7w . (18)\nFor the second identity, use Corollary 7 to conclude that\n\u2207\u03d5\u2020q \u25e6 \u2207\u03d5\u2020p(w) = W \u2016\u2207\u03d5\u2020p(w)\u2016q \u00b7 \u2207\u03d5q(\u2207\u03d5\u2020p(w))\n= W \u00b7 w\u2016w\u2016p .\nA.2 Bound on successive iterate divergence\nThe following Lemma extends [Kivinen et al., 2006, Appendix I] to \u03d5\u2020.\nLemma 10 For any w and \u03b4,\nD\u03d5\u2020q\n( w\u2016\u2207\u03d5\u2020p ( \u2207\u03d5\u2020q(w) + \u03b4 ))\n\u2264 (p\u2212 1)\u2016w\u2016qW 2 \u00b7 \u2225\u2225\u2225\u2225\u2225\n1 \u2016\u2207\u03d5\u2020q(w) + \u03b4\u2016p \u00b7 ( \u2207\u03d5\u2020q(w) + \u03b4 ) \u2212 1 W \u00b7 \u2207\u03d5\u2020q (w) \u2225\u2225\u2225\u2225\u2225 2\np\n. (19)\nProof [Proof of Lemma 10] In this proof, \u25e6 denotes composition and \u2297 is Hadamard product. The key step in the proof is the use of Theorem 1 to \u201cbranch\u201d on the proof of [Kivinen et al., 2006, Appendix I] on the first following identity (letting \u03d5q(w) . = (1/2) \u00b7 (W 2 + \u2016w\u20162q)). We also make use of the dual symmetry of Bregman divergences and we obtain third identity of:\nD\u03d5\u2020q\n( w\u2016\u2207\u03d5\u2020p ( \u2207\u03d5\u2020q(w) + \u03b4 ))\n= \u2016w\u2016q W \u00b7D\u03d5q\n( W \u2016w\u2016q \u00b7w \u2225\u2225\u2225\u2225\u2225\nW\n\u2016\u2207\u03d5\u2020p(\u2207\u03d5\u2020q(w) + \u03b4)\u2016q \u00b7 \u2207\u03d5\u2020p\n( \u2207\u03d5\u2020q(w) + \u03b4\n))\n= \u2016w\u2016q W\n\u00b7D\u03d5q ( W\n\u2016w\u2016q \u00b7w \u2225\u2225\u2207\u03d5\u2020p ( \u2207\u03d5\u2020q(w) + \u03b4\n)) (20)\n= \u2016w\u2016q W\n\u00b7D\u03d5p ( \u2207\u03d5q \u25e6 \u2207\u03d5\u2020p ( \u2207\u03d5\u2020q(w) + \u03b4 ) \u2225\u2225\u2225\u2225\u2207\u03d5q ( W \u2016w\u2016q \u00b7w )) by dual symmetry\n= \u2016w\u2016q W \u00b7D\u03d5p\n( W\n\u2016\u2207\u03d5\u2020q(w) + \u03b4\u2016p \u00b7 ( \u2207\u03d5\u2020q(w) + \u03b4\n) \u2225\u2225\u2225\u2225 W\n\u2016w\u2016q \u00b7 \u2207\u03d5q (w)\n) (21)\n= \u2016w\u2016q W \u00b7D\u03d5p\n( W\n\u2016\u2207\u03d5\u2020q(w) + \u03b4\u2016p \u00b7 ( \u2207\u03d5\u2020q(w) + \u03b4\n) \u2225\u2225\u2207\u03d5\u2020q (w) )\n(22)\n= \u2016w\u2016qW \u00b7D\u03d5p\n( 1\n\u2016\u2207\u03d5\u2020q(w) + \u03b4\u2016p \u00b7 ( \u2207\u03d5\u2020q(w) + \u03b4\n) \u2225\u2225\u2225\u2225 1\nW \u00b7 \u2207\u03d5\u2020q (w)\n) . (23)\nEquations (20) \u2013 (22) hold because of Corollary 7. We now use Appendix I4 in Kivinen et al. [2006] on Equation (23) and obtain\nD\u03d5\u2020q\n( w\u2016\u2207\u03d5\u2020p ( \u2207\u03d5\u2020q(w) + \u03b4 ))\n\u2264 (p\u2212 1)\u2016w\u2016qW 2 \u00b7 \u2225\u2225\u2225\u2225\u2225\n1 \u2016\u2207\u03d5\u2020q(w) + \u03b4\u2016p \u00b7 ( \u2207\u03d5\u2020q(w) + \u03b4 ) \u2212 1 W \u00b7 \u2207\u03d5\u2020q (w) \u2225\u2225\u2225\u2225\u2225 2\np\n,\nas claimed.\nA.3 Bound on successive iterate divergence to target\nIn what follows, we write the DN-pLMS updates as wt = \u2207\u03d5\u2020p(\u03b8t), where\n\u03b8t . = \u2207\u03d5\u2020q(wt\u22121)\u2212\u2206t\nfor \u2206t = \u03b7t \u00b7 (w>t\u22121xt \u2212 yt) \u00b7 xt. Further, for notational ease, we write\nu\u0304 . =\nu\ngq(u)\nand \u03b8\u0304t . =\n\u03b8t \u2016\u03b8t\u2016p .\nWe have the following preliminary bound on the distance from iterates of DN-pLMS to the (normalised) target.\nLemma 11 Fix any learning rate sequence {\u03b7t}Tt=1. Pick any u, and consider iterates {wt}Tt=0 as per Equation 10. Denote st\n. = (u\u0304\u2212wt\u22121)>xt, rt .= u\u0304>xt \u2212 yt, and \u03b1t .= W\u2016\u03b8t\u2016p . Suppose \u2016xt\u2016p \u2264 Xp. Then,\nD\u03d5q (u\u0304 \u2016wt\u22121 )\u2212D\u03d5q (u\u0304 \u2016wt ) \u2265 Q+R+ S + T ,\nwith\nQ . =\n\u03b1t\n2 \u03b7t(s\n2 t \u2212 r2t ),\nR . = (1\u2212 \u03b1t) \u00b7 ( W 2 \u2212 u\u0304>\u2207\u03d5\u2020q(wt\u22121) ) \ufe38 \ufe37\ufe37 \ufe38\n\u2208[0,2W 2]\n,\nS . = p\u2212 1 2 \u00b7 ( 2\u03b12t\u03b7 2 t (st \u2212 rt)2X2p \u2212 \u2225\u2225(st \u2212 rt)\u03b7t\u03b1t \u00b7 xt \u2212 (1\u2212 \u03b1t) \u00b7 \u2207\u03d5\u2020q(wt\u22121) \u2225\u22252 p )\n\ufe38 \ufe37\ufe37 \ufe38 \u2265\u22122(1\u2212\u03b1t)2W 2\n,\nT . =\n\u03b1t\n2 \u03b7t(st \u2212 rt)2\n( 1\u2212 2(p\u2212 1)\u03b7t\u03b1tX2p ) .\nProof [Proof of Lemma 11] The Bregman triangle equality (also called the three points property) [Boissonnat et al., 2010, Property 5], [Cesa-Bianchi and Lugosi, 2006, Lemma 11.1] brings:\nD\u03d5q (u\u0304 \u2016wt\u22121 )\u2212D\u03d5q (u\u0304 \u2016wt ) = (u\u0304\u2212wt\u22121)> (\u2207\u03d5q (wt)\u2212\u2207\u03d5q (wt\u22121))\u2212D\u03d5q (wt\u22121 \u2016wt ) = (u\u0304\u2212wt\u22121)> ( \u2207\u03d5\u2020q (wt)\u2212\u2207\u03d5\u2020q (wt\u22121) ) \u2212D\u03d5\u2020q (wt\u22121 \u2016wt ) by Lemmas 3, 8 .\n4This result is stated as a bound on D\u03d5q (w\u2016(\u2207\u03d5q)\u22121(\u2207\u03d5q(w) + \u03b4)), which by the Bregman dual symmetry property is equivalent to a bound on D\u03d5p (\u2207\u03d5q(w) + \u03b4\u2016\u2207\u03d5q(w)).\nWe now have\n\u2207\u03d5\u2020q (wt) = \u2207\u03d5\u2020q \u25e6 \u2207\u03d5\u2020p(\u03b8t) = W \u00b7 \u03b8\u0304t\nby Corollary 7. We get\nD\u03d5q (u\u0304 \u2016wt\u22121 )\u2212D\u03d5q (u\u0304 \u2016wt )\n\u2265 (u\u0304\u2212wt\u22121)> ( W \u00b7 \u03b8\u0304t \u2212\u2207\u03d5\u2020q(wt\u22121) ) \u2212 (p\u2212 1)W 2 2 \u00b7 \u2225\u2225\u2225\u2225\u03b8\u0304t \u2212 1 W \u00b7 \u2207\u03d5\u2020q (wt\u22121) \u2225\u2225\u2225\u2225 2\np\n= (u\u0304\u2212wt\u22121)> ( W \u00b7 \u03b8\u0304t \u2212\u2207\u03d5\u2020q(wt\u22121) ) \u2212 p\u2212 1 2 \u00b7 \u2225\u2225W \u00b7 \u03b8\u0304t \u2212\u2207\u03d5\u2020q (wt\u22121) \u2225\u22252 p .\nNow, note that \u03b8t = \u2207\u03d5\u2020q(wt\u22121) + \u03b7t \u00b7 (st \u2212 rt) \u00b7 xt.\nWe can thus rewrite the above as\nD\u03d5q (u\u0304 \u2016wt\u22121 )\u2212D\u03d5q (u\u0304 \u2016wt ) \u2265 st(st \u2212 rt)\u03b7t\u03b1t + (1\u2212 \u03b1t) ( w>t\u22121\u2207\u03d5\u2020q(wt\u22121)\u2212 u\u0304>\u2207\u03d5\u2020q(wt\u22121) )\n\u2212p\u2212 1 2 \u00b7 \u2225\u2225(st \u2212 rt)\u03b7t\u03b1t \u00b7 xt \u2212 (1\u2212 \u03b1t) \u00b7 \u2207\u03d5\u2020q(wt\u22121) \u2225\u22252 p\n= st(st \u2212 rt)\u03b7t\u03b1t + (1\u2212 \u03b1t) ( W 2 \u2212 u\u0304>\u2207\u03d5\u2020q(wt\u22121) )\n\u2212p\u2212 1 2 \u00b7 \u2225\u2225(st \u2212 rt)\u03b7t\u03b1t \u00b7 xt \u2212 (1\u2212 \u03b1t) \u00b7 \u2207\u03d5\u2020q(wt\u22121) \u2225\u22252 p\nby definition of \u2207\u03d5\u2020q = Q+R+ S + T .\nWe can show that the sum R+ S + T \u2265 0. This proof involves chaining together multiple simple inequalities. We give a high level overview in Figure 2.\nLemma 12 Let R,S, T be as per Lemma 11. Suppose we fix\n\u03b7t = \u03b3 \u00b7 W\n4(p\u2212 1)MXpW + |yt \u2212w>t\u22121xt|Xp , (24)\nfor any \u03b3 \u2208 [1/2, 1], and M .= max{W,Xp}. Then, T +R+ S \u2265 0.\nProof The triangle inequality and the fact that \u2016\u2207\u03d5\u2020q(wt\u22121)\u2016p = W brings\n\u03b1t \u2208 [\nW\n\u2016\u2207\u03d5\u2020q(wt\u22121)\u2016p + \u03b7t|st \u2212 rt| \u00b7 \u2016xt\u2016p ,\nW\n\u2016\u2207\u03d5\u2020q(wt\u22121)\u2016p \u2212 \u03b7t|st \u2212 rt| \u00b7 \u2016xt\u2016p\n]\n\u2286 [\nW\nW + \u03b7t|st \u2212 rt|Xp ,\nW\nW \u2212 \u03b7t|st \u2212 rt| \u00b7Xp\n] , (25)\nassuming that \u03b7t is chosen so that\n\u03b7t \u2264 W\n|st \u2212 rt| \u00b7Xp , (26)\nso that the right bound is non negative. To indeed ensure this, suppose that for some 0 < t \u2264 1/2, we fix\n\u03b7t \u2264 t 1\u2212 t \u00b7 W|st \u2212 rt|Xp . (27)\nWe would in addition obtain from Equation (25) that \u03b1t \u2208 [1\u2212 t, 1 + t]. Suppose \u03b7t is also fixed to ensure\n\u03b7t \u2208 [\nW\n2(p\u2212 1)\u03batXpW 2 \u2212 |st \u2212 rt|Xp ,\nW\n4(p\u2212 1)XpW 2 + |st \u2212 rt|Xp\n] , (28)\nfor some \u03bat such that\n\u03bat \u2265 2 + |st \u2212 rt|\n(p\u2212 1)W 2 . (29)\nNotice that constraint on \u03bat makes the interval non empty and its left bound strictly positive. Assuming (28) holds, we would have\n\u03b1t\u03b7t \u2208 [\n1\n2(p\u2212 1)\u03batX2p ,\n1\n4(p\u2212 1)X2p\n] . (30)\nThe left bound of (30) holds because\n\u03b1t\u03b7t \u2265 \u03b7t \u00b7 W\nW + \u03b7t|st \u2212 rt|Xp \u2265 1\n2(p\u2212 1)\u03batX2p . (31)\nThe first inequality holds because of (25) and the second one holds because of (28). The right bound of (30) holds because of (25), and so\n\u03b1t\u03b7t \u2264 \u03b7t \u00b7 W\nW \u2212 \u03b7t|st \u2212 rt|Xp \u2264 1\n4(p\u2212 1)X2p , (32)\nwhere the last inequality is due to (28).\nEquation (30) makes that T (\u03b7t\u03b1t) is at least its value when \u03b1t\u03b7t attains the lower bound of (31), that is,\nT (\u03b7t\u03b1t) \u2265 \u03bat \u2212 1 \u03bat \u00b7 (st \u2212 rt) 2 8(p\u2212 1)X2p . (33)\nNow, to guarantee \u03b1t \u2208 [1\u2212 t, 1 + t], it is sufficient that the right-hand side of inequality (27) belongs to interval (28) and we pick \u03b7t within the interval [left bound (28), right-hand side (27)]. To guarantee that the right-hand side of inequality (27) falls in interval (28), we need first,\nW\n2(p\u2212 1)\u03batXpW 2 \u2212 |st \u2212 rt|Xp \u2264 t 1\u2212 t \u00b7 W|st \u2212 rt|Xp , (34)\nthat is,\n\u03bat \u2265 1 t \u00b7 |st \u2212 rt| 2(p\u2212 1)W 2 . (35)\nTo guarantee that the right-hand side of inequality (27) falls in interval (28) we need then\nW\n4(p\u2212 1)XpW 2 + |st \u2212 rt|Xp \u2265 t 1\u2212 t \u00b7 W|st \u2212 rt|Xp , (36)\nthat is,\nt \u2264 |st \u2212 rt|\n2|st \u2212 rt|+ 4(p\u2212 1)W 2 . (37)\nTo summarize, if we pick any strictly positive t following inequality (37) (note t < 1) and\n\u03bat . = 2 +\n1 t \u00b7 |st \u2212 rt| (p\u2212 1)W 2 , (38)\nthen we shall have both \u03b1t \u2208 [1\u2212 t, 1 + t] and inequality (33) holds as well. In this case, we shall have\nT +R+ S \u2265  1\u2212 1\n2 + 1 t \u00b7 |st\u2212rt| (p\u22121)W 2\n  \u00b7 (st \u2212 rt) 2\n8(p\u2212 1)X2p \u2212 2 tW 2 \u2212 (p\u2212 1) 2tW 2\n\u2265 (\n1\u2212 1 2\n) \u00b7 (st \u2212 rt) 2\n8(p\u2212 1)X2p \u2212 2 tW 2 \u2212 (p\u2212 1) 2tW 2\n= (st \u2212 rt)2\n16(p\u2212 1)X2p \u2212 2 tW 2 \u2212 (p\u2212 1) 2tW 2 . (39)\nTo finish up, we want to solve for t the right-hand side such that it is non negative, and we find that t has to satisfy\nt \u2264 1 p\u2212 1 \u00b7 ( 1 + \u221a 1 + (st \u2212 rt)2 16X2pW 2 ) . (40)\nSince \u221a 1 + x \u2265 \u221ax, a sufficient condition is\nt \u2264 |st \u2212 rt|\n4(p\u2212 1)XpW . (41)\nTo ensure this and inequality (37), it is sufficient that we fix\nt . = |st \u2212 rt| 2|st \u2212 rt|+ 4(p\u2212 1)WM , (42)\nwhere M .= max{W,Xp}. With this expression for t, we get from (38),\n\u03bat . = 2 +\n4M\nW + 2|st \u2212 rt| (p\u2212 1)W 2 . (43)\nFor these choices, Lemma 13 implies that the given \u03b7t is feasible.\nLemma 13 Suppose t satisfies (42) and \u03bat satisfies (43). Then, a sufficient condition for \u03b7t to satisfy both (27) and (28) is\n\u03b7t = \u03b3 \u00b7 W\n4(p\u2212 1)MXpW + |yt \u2212w>t\u22121xt|Xp ,\nfor any \u03b3 \u2208 [1/2, 1].\nProof [Proof of Lemma 13] Notice the range of values authorized for \u03b7t: \u03b7t \u2208 [ W\n2(p\u2212 1)\u03batXpW 2 \u2212 |st \u2212 rt|Xp , t 1\u2212 t \u00b7 W|st \u2212 rt|Xp\n]\n=\n  W\n2(p\u2212 1) (\n2 + 4MW + 2|st\u2212rt| (p\u22121)W 2 ) XpW 2 \u2212 |st \u2212 rt|Xp\n, W\n4(p\u2212 1)MXpW + |st \u2212 rt|Xp\n \n=\n[ W\n2(2(p\u2212 1)W 2 + 4M(p\u2212 1)W + 2|st \u2212 rt|)Xp \u2212 |st \u2212 rt|Xp ,\nW\n4(p\u2212 1)MXpW + |st \u2212 rt|Xp\n]\n=\n[ W\n4(p\u2212 1)XpW 2 + 8(p\u2212 1)MXpW + 3|st \u2212 rt|Xp ,\nW\n4(p\u2212 1)MXpW + |st \u2212 rt|Xp\n]\n\u2283 [\nW\n8(p\u2212 1)MXpW + 2|st \u2212 rt|Xp ,\nW\n4(p\u2212 1)MXpW + |st \u2212 rt|Xp\n] . (44)\nA sufficient condition for \u03b7t to fall in interval (44) is\n\u03b7t = \u03b3 \u00b7 W\n4(p\u2212 1)MXpW + |yt \u2212w>t\u22121xt|Xp ,\nfor any \u03b3 \u2208 [1/2, 1].\nLemma 14 Suppose we fix the learning rate as per (24). Pick any u, and consider iterates {wt}Tt=0 as per Equation 10. Suppose \u2016xt\u2016p \u2264 Xp and |yt| \u2264 Y,\u2200t \u2264 T . Then, for any t,\nD\u03d5q (u\u0304 \u2016wt\u22121 )\u2212D\u03d5q (u\u0304 \u2016wt ) \u2265 1\n4(p\u2212 1) (\n2 + 4MW + 2(Y+XpW ) (p\u22121)W 2 ) X2p \u00b7 (s2t \u2212 r2t )\nwhere st . = (u\u0304\u2212wt\u22121)>xt, rt .= u\u0304>xt \u2212 yt.\nProof [Proof of Lemma 14] We start from the bound of Lemma 11:\nD\u03d5q (u\u0304 \u2016wt\u22121 )\u2212D\u03d5q (u\u0304 \u2016wt ) \u2265 Q+R+ S + T \u2265 Q by Lemma 12 = \u03b1t\n2 \u03b7t(s\n2 t \u2212 r2t ) by definition\n\u2265 1 4(p\u2212 1)\u03batX2p \u00b7 (s2t \u2212 r2t )\n\u2265 1 4(p\u2212 1) ( 2 + 4MW + 2 maxt |yt\u2212w>t\u22121xt| (p\u22121)W 2 ) X2p \u00b7 (s2t \u2212 r2t )\n\u2265 1 4(p\u2212 1) ( 2 + 4MW + 2(Y+XpW ) (p\u22121)W 2 ) X2p \u00b7 (s2t \u2212 r2t ) . (45)\nThe last constraint to check for this bound to be valid is our t in (42) has to be < 1/2 from inequality (26), which trivially holds since 4(p\u2212 1)WM \u2265 0. We conclude by noting Lemma 13 provides a feasible value of \u03b7t.\nA.4 Gauge normalisation\nThe following lemma about the gauge of x will be useful.\nLemma 15 Let gq(x) = \u2016x\u2016q/W for some W > 0. Then, for the iterates {wt} as per Equation 9, gq(wt) = 1.\nProof We have\ngq(wt) = \u2016wt\u2016q W\n= W\nW by Lemma 3\n= 1 ,\u2200t \u2265 1 . (46)"}, {"heading": "B Proofs of results in main body", "text": "We present proofs of all results in the main body.\nProof [Proof of Theorem 1] Let J : X \u2192 Xg denote the Jacobian of h : x 7\u2192 (1/g(x)) \u00b7 x. By an elementary calculation, g(x) \u00b7 J = Id \u2212 (1/g(x)) \u00b7 x\u2207g(x)>, which by the chain rule brings the following expression for the gradient of \u03d5\u2020(y) = g(y) \u00b7 (\u03d5 \u25e6 h)(y):\n\u2207\u03d5\u2020 (y) = \u2207g(y) \u00b7 (\u03d5 \u25e6 h)(y) + g(y) \u00b7 \u2207(\u03d5 \u25e6 h)(y) = \u2207g(y) \u00b7 (\u03d5 \u25e6 h)(y) + g(y) \u00b7 J>\u2207\u03d5(h(y)) = \u2207g(y) \u00b7 (\u03d5 \u25e6 h)(y) +\u2207\u03d5(h(y))\u2212 (1/g(y)) \u00b7 \u2207g(y)y>\u2207\u03d5(h(y))\n= \u2207\u03d5 ( 1 g(y) \u00b7 y ) + ( \u03d5 ( 1 g(y) \u00b7 y ) \u2212 1 g(y) \u00b7 y>\u2207\u03d5 ( 1 g(y) \u00b7 y )) \u00b7 \u2207g(y) . (47)\nFor simplicity, let u = x/g(x) and v = y/g(y), so that \u03d5\u2020(x) = g(x) \u00b7 \u03d5(u) and \u03d5\u2020(y) = g(y) \u00b7 \u03d5(v). The above then reads\n\u2207\u03d5\u2020 (y) = \u2207\u03d5 (v) + ( \u03d5 (v)\u2212 v>\u2207\u03d5 (v) ) \u00b7 \u2207g(y). (48)\nNow, the LHS of Equation (3) is\ng(x) \u00b7D\u03d5 ( 1\ng(x) \u00b7 x \u2225\u2225 1 g(y)\n\u00b7 y ) = g(x) \u00b7D\u03d5 (u\u2016v)\n= g(x) \u00b7 \u03d5(u)\u2212 g(x) \u00b7 \u03d5(v)\u2212 g(x) \u00b7 \u2207\u03d5(v)>(u\u2212 v) = \u03d5\u2020(x)\u2212 g(x) \u00b7 \u03d5(v)\u2212\u2207\u03d5(v)>(x\u2212 g(x) \u00b7 v) = \u03d5\u2020(x)\u2212 g(x) \u00b7 (\u03d5(v)\u2212\u2207\u03d5(v)>v)\u2212\u2207\u03d5(v)>x,\nwhile the RHS is\nD\u03d5\u2020 ( x \u2225\u2225 y )\n= \u03d5\u2020(x)\u2212 \u03d5\u2020(y)\u2212\u2207\u03d5\u2020(y)>(x\u2212 y) = \u03d5\u2020(x)\u2212 g(y) \u00b7 \u03d5(v)\u2212\u2207\u03d5 (v)> (x\u2212 y)\u2212 ( \u03d5 (v)\u2212 v>\u2207\u03d5 (v) ) \u00b7 \u2207g(y)>(x\u2212 y).\nCancelling the common \u03d5\u2020(x) and \u2207\u03d5 (v)> y terms, the difference \u2206 = RHS\u2212 LHS is\n\u2206 = g(x) \u00b7 (\u03d5(v)\u2212\u2207\u03d5(v)>v)\u2212 g(y) \u00b7 \u03d5(v) +\u2207\u03d5 (v)> y \u2212 ( \u03d5 (v)\u2212 v>\u2207\u03d5 (v) ) \u00b7 \u2207g(y)>(x\u2212 y)\n= g(x) \u00b7 (\u03d5(v)\u2212\u2207\u03d5(v)>v)\u2212 g(y) \u00b7 \u03d5(v) + g(y) \u00b7 \u2207\u03d5 (v)> v \u2212 ( \u03d5 (v)\u2212 v>\u2207\u03d5 (v) ) \u00b7 \u2207g(y)>(x\u2212 y) = g(x) \u00b7 (\u03d5(v)\u2212\u2207\u03d5(v)>v)\u2212 g(y) \u00b7 ( \u03d5(v)\u2212\u2207\u03d5 (v)> v ) \u2212 ( \u03d5 (v)\u2212 v>\u2207\u03d5 (v) ) \u00b7 \u2207g(y)>(x\u2212 y)\n= (\u03d5(v)\u2212\u2207\u03d5(v)>v) \u00b7 (g(x)\u2212 g(y)\u2212\u2207g(y)>(x\u2212 y)) = (\u03d5(v)\u2212\u2207\u03d5(v)>v) \u00b7Bg(x\u2016y).\nThus, the identity holds, if and only if either \u03d5(v) = \u2207\u03d5(v)>v for every v \u2208 Xg, or Bg(x\u2016y) = 0. The latter is true if and only if g is affine from Equation 2. The result follows.\nIt is easy to check that Theorem 1 in fact holds for separable (matrix) trace divergences [Kulis et al., 2009] of the form\nD\u03d5(X\u2016Y) .= \u03d5(X)\u2212 \u03d5(Y)\u2212 tr ( \u2207\u03d5(Y)>(X \u2212 Y) ) , (49)\nwith \u03d5, g : S(d) \u2192 R (for S(d) the set of symmetric real matrices), with \u03d5 convex. In this case, the restricted positive homogeneity property becomes\n\u03d5 (U) = tr ( \u2207\u03d5(U)>U ) ,\u2200U \u2208 Xg . (50)\nProof [Proof of Lemma 2] Note that by construction, g(r(x)) = P(X = x)/((1\u2212 \u03c0C) \u00b7 P(X = x|Y = C)), and so (\n1\ng(r(x)) \u00b7 r(x)\n)\nc\n= (1\u2212 \u03c0C) \u00b7 P(X = x|Y = C) P(X = x) \u00b7 P(X = x|Y = c) P(X = x|Y = C)\n= (1\u2212 \u03c0C) \u03c0c \u00b7 \u03c0cP(X = x|Y = c)\nP(X = x) = \u03b7(x) . (51)\nFurthermore,\nP(X = x) = C\u2211\nc=1\n\u03c0cP(X = x|Y = c)\n= (1\u2212 \u03c0C) \u00b7 (\n\u03c0C 1\u2212 \u03c0C\n+ \u2211\nc<C\n\u03c0c 1\u2212 \u03c0C \u00b7 P(X = x|Y = c) P(X = x|Y = C)\n) \u00b7 P(X = x|Y = C)\n= (1\u2212 \u03c0C) \u00b7 g(r(x)) \u00b7 P(X = x|Y = C) . (52) Now let\nr\u0302(x) = 1\n\u03b7\u0302C(x) \u00b7 \u03b7\u0302(x).\nIt then comes\nEM [D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))] = (1\u2212 \u03c0C) \u00b7 EPC [g(r(x)) \u00b7D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))]\n= (1\u2212 \u03c0C) \u00b7 EPC [ g(r(x)) \u00b7D\u03d5 ( 1\ng(r(x)) \u00b7 r(x)\n\u2225\u2225\u2225\u2225 \u03b7\u0302(X) )]\n= (1\u2212 \u03c0C) \u00b7 EPC [ g(r(x)) \u00b7D\u03d5 ( 1\ng(r(x)) \u00b7 r(x)\n\u2225\u2225\u2225\u2225 1\ng(r\u0302(X)) \u00b7 r\u0302(X)\n)]\n= (1\u2212 \u03c0C) \u00b7 EPC [ D\u03d5\u2020(r(X)\u2016r\u0302(X)) ] ,\nas claimed.\nProof [Proof of Lemma 3] For any x, \u2016\u2207\u03d5\u2020p(x)\u2016q = W by Corollary 7. Since wt = \u2207\u03d5\u2020p(\u03b8t\u22121) for suitable \u03b8t\u22121, the result follows. The result for \u2016\u2207\u03d5\u2020q(wt)\u2016p follows similarly by Corollary 7. Note that while \u2016wt\u2016q = \u2016\u2207\u03d5(wt)\u2016p for the standard p-LMS update [Kivinen et al., 2006, Appendix I], these norms may vary with each iteration i.e. wt may not lie in the Lq ball.\nProof [Proof of Lemma 4] Similarly to the proof of Lemma 10, a key to the proof of Lemma 4 relies on branching on Kivinen et al. [2006] through the use of Theorem 1. We first note that D\u03d5\u2020q (u\u2016w0) = W \u00b7 \u2016u\u2016q since w0 = 0, and D\u03d5\u2020q (u\u2016wT+1) \u2265 0, and so\nW \u00b7 \u2016u\u2016q \u2265 D\u03d5\u2020q (u\u2016w0)\u2212D\u03d5\u2020q (u\u2016wT+1)\n=\nT\u2211\nt=1\n{ D\u03d5\u2020q (u\u2016wt\u22121)\u2212D\u03d5\u2020q (u\u2016wt) } by telescoping property\n= gq(u) \u00b7 T\u2211\nt=1\n{ D\u03d5q ( u\ngq(u)\n\u2225\u2225\u2225\u2225 wt\u22121\ngq(wt\u22121)\n) \u2212D\u03d5q ( u\ngq(u)\n\u2225\u2225\u2225\u2225 wt\ngq(wt)\n)} by Theorem 1\n= gq(u) \u00b7 T\u2211\nt=1\n{ D\u03d5q ( u\ngq(u) \u2016wt\u22121\n) \u2212D\u03d5q ( u\ngq(u) \u2016wt\n)} by Lemma 15 . (53)\nRecall from Lemma 14 that\nD\u03d5q\n( u\ngq(u) \u2016wt\u22121\n) \u2212D\u03d5q ( u\ngq(u) \u2016wt\n) \u2265 1\n4(p\u2212 1) (\n2 + 4MW + 2(Y+XpW ) (p\u22121)W 2 ) X2p \u00b7 (s2t \u2212 r2t )\nwhere\nst . = ((1/gq(u)) \u00b7 u\u2212wt\u22121)>xt rt . = (1/gq(u)) \u00b7 u>xt \u2212 yt.\nNote that Rq(w1:T |u) = \u2211T t=1(s 2 t \u2212 r2t ) by definition. Summing the above for t = 1, 2, ..., T and telescoping sums yields\nRq(w1:T |u) \u2264 4(p\u2212 1) ( 2 + 4M\nW +\n2(Y +XpW ) (p\u2212 1)W 2 ) X2pW 2\n= 4(p\u2212 1)X2pW 2 + 16(p\u2212 1)MX2pW + 8(Y +XpW )X2p \u2264 4(p\u2212 1)X2pW 2 + (16p\u2212 8)MX2pW + 8Y X2p . (54)\nSee Figure 3 for some geometric intuition about the updates.\nProof [Proof of Lemma 5] We start by the sphere. Let \u03d5(x) .= (1/2) \u00b7 \u2016x\u201622. Since a Bregman divergence is invariant to linear transformation, it comes from Table 7 that\nD\u03d5\n( xS\ngS(xS)\n\u2225\u2225 c S\ngS(cS)\n) =\n1\ngS(cS) \u00b7D\u03d5\u2020(x\u2016c) = 1\u2212 cosDG(x, c),\nwhere we recall that DG denotes the geodesic distance on the sphere (see Figure 1 and Appendix C). Equivalently, \u2225\u2225\u2225\u2225 1\ngS(xS) \u00b7 xS \u2212 1 gS(cS) \u00b7 cS\n\u2225\u2225\u2225\u2225 2\n2\n= 1\u2212 cosDG(x, c) . (55)\nThis equality allows us to use k-means++ using the LHS of (55) to compute the distribution that picks a center. The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.2 and 3.3) can be carried out without modification to yield the same approximation ratio as that of Arthur and Vassilvitskii [2007] if the distortion at hand is the squared Euclidean distance, which turns out to be Drec(. : .) from eq. (55). The case of the hyperboloid follows the exact same path, but starts from the fact that Table 7 now brings\nD\u03d5\n( xH\ngH(xH)\n\u2225\u2225 c H\ngH(cH)\n) = coshDG(y, c)\u2212 1 = \u2225\u2225\u2225\u2225 1\ngH(xH) \u00b7 xH \u2212 1 gH(cH) \u00b7 cH\n\u2225\u2225\u2225\u2225 2\n2\n.\nTo finish, in the same way as for the Sphere, we just need the existence of a coordinate system for which the centroid is an average of the cluster points, which can be obtained from hyperbolic barycentric coordinates [Ungar, 2014, Section 18]."}, {"heading": "C Working out examples of Table 7", "text": "We fill in the details justifying each of the examples of Equation 3 provided in Table 2. We also provide the form of the corresponding divergences D\u03d5 and distortions D\u03d5\u2020 in the augmented Table 7.\nRow I \u2014 for X = Rd, consider \u03d5(x) = (1 + \u2016x\u201622)/2 and g(x) = \u2016x\u20162 (we project on the Euclidean sphere). It comes\n\u03d5\u2020(x) = \u2016x\u20162 \u00b7   1 + \u2225\u2225\u2225 1\u2016x\u20162 \u00b7 x \u2225\u2225\u2225 2 2\n2\n  = \u2016x\u20162 . (56)\ng is not linear (but it is homogeneous of degree 1), but we have\n\u03d5(x) = 1 = x>\u2207\u03d5(x) ,\u2200x : \u2016x\u20162 = 1 , (57)\nso \u03d5 is 1-homogeneous on the Euclidean sphere, and we can apply Theorem 1. We have\ng(x) \u00b7D\u03d5 ( 1\ng(x) \u00b7 x\u2016 1 g(y) \u00b7 y ) = \u2016x\u20162 2 \u00b7 \u2225\u2225\u2225\u2225 1 \u2016x\u20162 \u00b7 x\u2212 1\u2016y\u20162 \u00b7 y \u2225\u2225\u2225\u2225 2\n2\n= \u2016x\u20162 \u00b7 ( 1\u2212 x >y\n\u2016x\u20162\u2016y\u20162\n) = \u2016x\u20162 \u00b7 (1\u2212 cos(x,y)) , (58)\nand we also have\nD\u03d5\u2020 (x\u2016y) = \u2016x\u20162 \u2212 \u2016y\u20162 \u2212 1 \u2016y\u20162 \u00b7 (x\u2212 y)>y\n= \u2016x\u20162 \u2212 \u2016y\u20162 \u2212 x>y \u2016y\u20162 + \u2016y\u20162 (59) = \u2016x\u20162 \u00b7 ( 1\u2212 x >y\n\u2016x\u20162\u2016y\u20162\n) = \u2016x\u20162 \u00b7 (1\u2212 cos(x,y)) , (60)\nwhich is equal to Equation (58), so we check that Theorem 1 applies in this case. D\u03d5\u2020 has some properties. One is a weak form of triangle inequality.\nLemma 16 D\u03d5\u2020 (x\u2016y) +D\u03d5\u2020 (y\u2016z) \u2264 D\u03d5\u2020 (x\u2016z), \u2200x,y, z such that \u2016y\u20162 \u2264 \u2016x\u20162.\nProof\nD\u03d5\u2020 (x\u2016y) +D\u03d5\u2020 (y\u2016z) = \u2016x\u20162 \u00b7 (1\u2212 cos(x,y)) + \u2016y\u20162 \u00b7 (1\u2212 cos(y, z)) = \u2016x\u20162 \u00b7 ((1\u2212 cos(x,y)) + (1\u2212 cos(y, z))) + (\u2016y\u20162 \u2212 \u2016x\u20162) \u00b7 (1\u2212 cos(y, z)) \u2264 \u2016x\u20162 \u00b7 (1\u2212 cos(x, z)) + (\u2016y\u20162 \u2212 \u2016x\u20162) \u00b7 (1\u2212 cos(y, z)) \u2264 D\u03d5\u2020 (x\u2016z) + (\u2016y\u20162 \u2212 \u2016x\u20162) \u00b7 (1\u2212 cos(y, z)) \u2264 D\u03d5\u2020 (x\u2016z) , (61)\nsince \u2016y\u20162 \u2264 \u2016x\u20162. We have used the fact that (1 \u2212 cos(x,y)) is half the Euclidean distance between unitnormalized vectors.\nIt turns out that D\u03d5\u2020(x\u2016\u00b5) can be related to the log-likelihood of a von Mises-Fisher distribution with expected direction \u00b5, which is useful in text analysis [Reisinger et al., 2010].\nRow II \u2014 Let \u03d5(x) .= (1/2) \u00b7 (u2 + \u2016x\u20162q), for q > 1 [Kivinen et al., 2006]. We have\n\u03d5\n( 1 g(x) \u00b7 x ) = u2 2 + 1 2 \u00b7 \u2225\u2225\u2225\u2225 1 g(x) \u00b7 x \u2225\u2225\u2225\u2225 2\nq\n= u2\n2 +\n1\n2g2(x) \u00b7 \u2016x\u20162q . (62)\nWe also have\n( 1 g(x) \u00b7 x )> \u2207\u03d5 ( 1 g(x) \u00b7 x ) = 1 g(x) \u00b7 \u2211\ni\nxi \u00b7 sign ( 1 g(x) \u00b7 xi ) \u2223\u2223\u2223 1g(x) \u00b7 xi \u2223\u2223\u2223 q\u22121\n\u2225\u2225\u2225 1g(x) \u00b7 x \u2225\u2225\u2225 q\u22122\nq\n= \u2211\ni\n\u2223\u2223\u2223 1g(x) \u00b7 xi \u2223\u2223\u2223 q\n\u2225\u2225\u2225 1g(x) \u00b7 x \u2225\u2225\u2225 q\u22122\nq\n= 1\ng2(x) \u00b7 \u2016x\u20162q . (63)\nTo have the condition of Theorem 1 satisfied, we therefore need\n\u2016x\u2016q = ug(x) , (64)\nSo we use g(x) = \u2016x\u2016q/W and u = W , observing that \u03d5 is 1-homogeneous on the Lp sphere. We check that\n\u03d5\u2020(x) = W \u00b7 \u2016x\u2016q . (65)\nand we obtain\nD\u03d5\u2020(w\u2016w\u2032) = W \u00b7 \u2016w\u2016q \u2212W \u00b7 \u2211\ni\nwi \u00b7 sign(w\u2032i) \u00b7 |w\u2032i|q\u22121 \u2016w\u2032\u2016q\u22121q . (66)\nRow III \u2014 As in Buss and Fillmore [2001], we assume \u2016x\u20162 \u2264 \u03c0, or we renormalize or change the radius of the ball) We first lift the data points using the Sphere lifting map Rd 3 x 7\u2192 xS \u2208 Rd+1:\nxS . = [x1 x2 \u00b7 \u00b7 \u00b7 xd rx cot rx]> , (67)\nwhere rx . = \u2016x\u20162 is the Euclidean norm of x. Notice that the last coordinate is a coordinate of the Hessian of the geodesic distance to the origin on the sphere [Buss and Fillmore, 2001]. We then let g(xS) .= rx/ sin rx (notice that g is computed uding the first d coordinates). Finally, for X = Rd+1 and u > 1, consider \u03d5(xS) = (u2 +\u2016xS\u201622)/2. The set of points for which \u03d5(xS) = (xS)>\u2207\u03d5(xS) is equivalently the subset Xg \u2286 Rd+1 such that\nXg . = {xS : g2(xS) = u2} . (68)\nSo \u03d5 satisfies the restricted positive homogeneity of degree 1 on Xg and we can apply Theorem 1. We first remark that:\n\u2016xS\u201622 = r2x + r2x cot2 rx\n= r2x\nsin r2x = g2(xS) , (69)\nand\n\u03d5\u2020(xS) = rx sin rx \u00b7 \u03d5 ( sin rx rx \u00b7 xS ) = rx sin rx \u00b7 ( sin rx rx )2 \u00b7 \u2016xS\u201622 = \u2016xS\u20162 , (70)\nand finally, because of the spherical law of cosines,\nsin rx sin ry cos(x,y) + cos rx cos ry = cosDG(x,y) , (71)\nwhere we recall from eq. (16) that DG(x,y) is the geodesic distance between the image of the exponential maps of x and y on the sphere.\nWe then derive\ng(xS) \u00b7D\u03d5 ( 1\ng(xS) \u00b7 xS\u2016 1 g(yS) \u00b7 yS\n)\n= rx 2 sin rx \u00b7 \u2225\u2225\u2225\u2225 sin rx rx \u00b7 xS \u2212 sin ry ry \u00b7 yS \u2225\u2225\u2225\u2225 2\n2\n= rx 2 sin rx \u00b7 ( sin2 rx \u2016x\u201622 \u00b7 \u2016xS\u201622 + sin2 ry \u2016y\u201622 \u00b7 \u2016yS\u201622 \u2212 2 \u00b7 sin rx rx \u00b7 sin ry ry \u00b7 (xS)>yS )\n= rx sin rx \u00b7 ( 1\u2212 sin rx rx \u00b7 sin ry ry \u00b7 (xS)>yS )\n(72)\n= rx sin rx \u00b7 ( 1\u2212 sin rx rx \u00b7 sin ry ry \u00b7 ( x>y + rxry cot rx cot ry )) (73) = rx\nsin rx \u00b7 (1\u2212 sin rx sin ry \u00b7 (cos(x,y) + cot rx cot ry))\n= rx\nsin rx \u00b7 (1\u2212 (sin rx sin ry cos(x,y) + cos rx cos ry))\n= rx\nsin rx \u00b7 (1\u2212 cosDG(x,y)) . (74)\nIn Equation (72), we use Equation (69), and we use Equation (71) in Equation (74). We also check\nD\u03d5\u2020(x S\u2016yS) = \u2016xS\u20162 \u2212 \u2016yS\u20162 \u2212\n1\n\u2016yS\u20162 \u00b7 (xS \u2212 yS)>yS\n= \u2016xS\u20162 \u2212 1 \u2016yS\u20162 \u00b7 (xS)>yS = \u2016xS\u20162 \u00b7 ( 1\u2212 (x S)>yS\n\u2016xS\u20162\u2016yS\u20162\n) (75)\n= rx\nsin rx \u00b7 (1\u2212 cosDG(x,y)) . (76)\nTo obtain (76), we use the fact that\n(xS)>yS\n\u2016xS\u20162\u2016yS\u20162 = sin rx rx \u00b7 sin ry ry\n\u00b7 ( x>y + rxry cot rx cot ry ) , (77)\nand then plug it into Equation (76), which yields the identity between Equation (73) (and thus (74)) and (76). So Theorem 1 holds in this case as well.\nWe remark that (1/g(xS)) \u00b7 xS = exp0(x) is the exponential map for the sphere [Buss and Fillmore, 2001].\nRow IV \u2014 In the same way as we did for row IV, we first create a lifting map, but this time complex valued, the Hyperboloid lifting map H: Rd 3 x 7\u2192 xH \u2208 Rd \u00d7 C. With an abuse of notation, it is given by\nxH . = [x1 x2 \u00b7 \u00b7 \u00b7 xd irx coth rx]> , (78)\nand we let g(xH) .= \u2212rx/ sinh rx, with coth and sinh defining respectively the hyperbolic cotangent and hyperbolic sine. We let 0 coth 0 = 0/ sinh 0 = 1. Notice that the complex number is pure imaginary and so H defines a d dimensional manifold that lives in Rd+1 assuming that the last coordinate is the imaginary axis. Let expq(x) . = (1/g(xH) \u00b7 xH . Notice that\n\u2225\u2225expq(x) \u2225\u22252\n2 = sinh2 rx r2x \u00b7 ( r2x + i 2r2x coth 2 rx )\n= sinh2 rx + i 2 cosh2 rx = sinh2 rx \u2212 cosh2 rx = \u22121 , (79)\nso expq(x) defines a lifting map from Rd to the hyperboloid model Hd of hyperbolic geomety Galperin [1993]. In fact, it defines the exponential map for the plane TqHd tangent to Hd in point q . = [0 0 \u00b7 \u00b7 \u00b7 0 i] = 0H . To see this, remark that we can express the geodesic distance DG with the hyperbolic metric between xH and yH as\nDG(x H ,yH) . = cosh\u22121(\u2212(xH)>yH) , (80)\nwhere cosh\u22121 is the inverse hyperbolic cosine. So, for any x \u2208 TqHd, since rx = \u2016x\u2212 0\u20162, we have\nDG(expq(x), q) = cosh \u22121(\u2212(xH)>0H)\n= cosh\u22121(\u2212i2 cosh rx) = rx = \u2016x\u2212 0\u20162 , (81)\nand expq(x) is indeed the exponential map for TqHd. Now, remark that\nexpq(x) > expq(y) = sinh rx rx \u00b7 sinh ry ry \u00b7 (xH)>yH\n= sinh rx rx \u00b7 sinh ry ry\n\u00b7 ( x>y + i2rxry coth rx coth ry )\n= sinh rx sinh ry \u00b7 (cos(x,y)\u2212 coth rx coth ry) = sinh rx sinh ry cos(x,y)\u2212 cosh rx cosh ry = \u2212 coshDG(xH ,yH) . (82)\nEq. (82) holds by the hyperbolic law of cosines. Now, we let \u03d5(xH) = (u2 + \u2016xH\u201622)/2 and\nXg . = {xH : \u2016xH\u201622 = u2} . (83)\nWe check that \u03d5(xH) = u2 = (xH)>\u2207\u03d5(xH) for any xH \u2208 Xg, so we can apply Theorem 1. We then use eqs. (79) and (82) and derive\ng(xH) \u00b7D\u03d5 ( 1\ng(xH) \u00b7 xH\u2016 1 g(yH) \u00b7 yH\n)\n= \u2212 rx 2 sinh rx\n\u00b7 \u2225\u2225expq(x)\u2212 expq(y) \u2225\u22252 2\n= \u2212 rx 2 sinh rx\n\u00b7 (\u2225\u2225expq(x) \u2225\u22252 2 + \u2225\u2225expq(y) \u2225\u22252 2 \u2212 2 expq(x)> expq(y) )\n= \u2212 rx sinh rx\n\u00b7 ( coshDG(x H ,yH)\u2212 1 ) . (84)\nNote that eq. (84) is a negative-valued and concave distortion.\nRow V \u2014 for X = Rd+\u2217, consider \u03d5(x) = \u2211 i xi log xi \u2212 xi and g(x) = 1>x (we normalize on the simplex). Since g is linear, we do not need to check for the homogeneity of \u03d5, and we directly obtain:\ng(x) \u00b7D\u03d5 ( 1\ng(x) \u00b7 x\u2016 1 g(y) \u00b7 y ) = \u2211\ni\nxi log xi \u2212 (1>x) log(1>x)\u2212 1>x\n\u22121 >x 1>y \u00b7 \u2211\ni\nyi log yi \u2212 (1>x) log(1>y) + 1>x\n\u2212(1>x) \u00b7 \u2211\ni\n( xi\n1>x \u2212 yi 1>y\n) \u00b7 log yi\n1>y\n= \u2211\ni\nxi log xi yi \u2212 (1>x) \u00b7 log 1 >x 1>y . (85)\nFurthermore,\n\u03d5\u2020(x) = 1>x \u00b7 (\u2211\ni\nxi 1>x \u00b7 log xi 1>x\n\u2212 1 ) = \u2211\ni\nxi log xi \u2212 (1>x) log(1>x)\u2212 1>x . (86)\nNoting that \u03d5\u2020(x) is the sum of three terms, one of which is linear and can be removed for the divergence, so the divergence is just the sum of the two divergences with the two generators, which is found to be Equation (85) as well. Remark that while the KL divergence is convex in its both arguments, D\u03d5\u2020(x\u2016y) may not be (jointly) convex. Indeed, its Hessian in y equals:\nHy(D\u03d5\u2020) = Diag({xi/y2i }i)\u2212 1>x\n(1>y)2 \u00b7 11> , (87)\nwhich may be indefinite.\nRow VI \u2014 for X = Rd+\u2217, consider \u03d5(x) = \u2212d\u2212 \u2211 i log xi and g(x) = (\u03c0x) 1/d, where we let \u03c0x . = \u220f i xi (we normalize with the geometric average). It comes\n\u03d5\u2020(x) = (\u03c0x) 1/d \u00b7 ( \u2212d\u2212 \u2211\ni\nlog xi\n(\u03c0x)1/d\n) = \u2212d \u00b7 (\u03c0x)1/d . (88)\ng is not linear (but it is homogeneous of degree 1), and we have\n\u03d5(x) = \u2212d = x>\u2207\u03d5(x) ,\u2200x : \u220f\ni\nxi = 1 , (89)\nso \u03d5 is 1-homogeneous on Xg , and we can apply Theorem 1. We have\ng(x) \u00b7D\u03d5 ( 1\ng(x) \u00b7 x\u2016 1 g(y) \u00b7 y )\n= (\u03c0x) 1/d \u00b7\n\u2211\ni\n( xi(\u03c0y) 1/d\nyi(\u03c0x)1/d \u2212 log xi(\u03c0y)\n1/d\nyi(\u03c0x)1/d\n) \u2212 d(\u03c0x)1/d\n= \u2211\ni\nxi(\u03c0y) 1/d\nyi \u2212 d(\u03c0x)1/d log(\u03c0x)1/d \u2212 (\u03c0x)1/d log \u03c0y + (\u03c0x)1/d log \u03c0y\n+d(\u03c0x) 1/d log(\u03c0x) 1/d \u2212 d(\u03c0x)1/d\n= \u2211\ni\nxi(\u03c0y) 1/d\nyi \u2212 d(\u03c0x)1/d . (90)\nWe also have\n\u2202\n\u2202xi \u03d5\u2020(x) = \u2212(1/xi) \u00b7 (\u03c0x)1/d , (91)\nand so\nD\u03d5\u2020 (x\u2016y) = \u2212d(\u03c0x)1/d + d(\u03c0y)1/d + \u00b7 \u2211\ni\n(xi \u2212 yi) \u00b7 (\u03c0y)\n1/d\nyi\n= \u2212d(\u03c0x)1/d + d(\u03c0y)1/d + \u2211\ni\nxi(\u03c0y) 1/d\nyi \u2212 d(\u03c0y)1/d\n= \u2211\ni\nxi(\u03c0y) 1/d\nyi \u2212 d(\u03c0x)1/d , (92)\nwhich is equal to Equation (90), so we check that Theorem 1 applies in this case.\nRow VII \u2014 We use the following fact Kulis et al. [2009]. Let X = ULU> and Y = VTV> be the eigendecomposition of symmetric positive definite matrices X and Y, with L .= Diag(l), T .= Diag(t), and U .= [u1|u2| \u00b7 \u00b7 \u00b7 |ud], V .= [v1|v2| \u00b7 \u00b7 \u00b7 |vd] orthonormal; let \u03d5 = tr (X log X \u2212 X). Then we have\nD\u03d5 (X\u2016Y) = \u2211\ni,j\n(u>i vj) 2 \u00b7D\u03d52(li\u2016tj) , (93)\nwith \u03d52(x) = x log x\u2212 x. We pick g(X) = tr (X) = \u2211 i li, which brings from Equation (85)\ng(X) \u00b7D\u03d5 ( 1\ng(X) \u00b7 X\u2016 1 g(Y) \u00b7 Y )\n= \u2211\ni,j\n(u>i vj) 2 \u00b7 tr (X) \u00b7D\u03d52\n( li\ntr (X) \u2016 tj tr (Y)\n)\n= \u2211\ni,j\n(u>i vj) 2 \u00b7 tr (X) \u00b7\n( li\ntr (X) \u00b7 log li \u00b7 tr (Y) tj \u00b7 tr (X) \u2212 li tr (X) + tj tr (Y)\n)\n= \u2211\ni,j\n(u>i vj) 2 \u00b7 ( li log\nli tj \u2212 li + tj\n) + log ( tr (Y)\ntr (X)\n) \u00b7 \u2211\ni,j\n(u>i vj) 2 \u00b7 li\n+ tr (X) tr (Y) \u00b7 \u2211\ni,j\n(u>i vj) 2 \u00b7 tj \u2212\n\u2211\ni,j\n(u>i vj) 2 \u00b7 tj . (94)\nBecause U, V are orthonormal, we also get \u2211 i,j(u > i vj) 2 \u00b7 li = \u2211 i li \u2211 j cos 2(ui,vj) = \u2211 i li = tr (X) and\u2211\ni,j(u > i vj) 2 \u00b7 tj = tr (y), and so Equation (94) becomes\ng(X) \u00b7D\u03d5 ( 1\ng(X) \u00b7 X\u2016 1 g(Y) \u00b7 Y )\n= tr (X log X \u2212 X log Y)\u2212 tr (X) + tr (Y) + tr (X) \u00b7 log ( tr (Y)\ntr (X)\n) + tr (X)\u2212 tr (Y)\n= tr (X log X \u2212 X log Y)\u2212 tr (X) \u00b7 log ( tr (X)\ntr (Y)\n) . (95)\nWe also check that\n\u03d5\u2020(X) = tr (X) \u00b7 tr ( 1\ntr (X) \u00b7 X log\n( 1 tr (X) \u00b7 X ) \u2212 1 tr (X) \u00b7 X )\n= tr ( X log ( 1 tr (X) \u00b7 X )) \u2212 tr (X) , (96)\nand\nX log\n( 1 tr (X) \u00b7 X ) = ULU>U log ( 1 1>l \u00b7 L ) U>\n= UL log\n( 1 1>l \u00b7 L ) U> (97)\n= UL log LU> \u2212 log tr (X) \u00b7 ULU> , (98)\nso that \u03d5\u2020(X) = tr (X log X \u2212 X) \u2212 tr (X) \u00b7 log tr (X). Let \u03d53(X) .= tr (X) \u00b7 log tr (X). We have \u2207\u03d53(X) = (1 + log tr (X)) \u00b7 I. Since a (Bregman) divergence involving a sum of generators is the sum of (Bregman) divergences, we get\nD\u03d5\u2020 (X\u2016Y) = tr (X log X \u2212 X log Y \u2212 X + Y)\u2212 tr (X) \u00b7 log tr (X) + tr (Y) \u00b7 log tr (Y) +(1 + log tr (Y)) \u00b7 tr (X \u2212 Y)\n= tr (X log X \u2212 X log Y)\u2212 tr (X) \u00b7 log tr (X) + tr (X) \u00b7 log tr (Y) = tr (X log X \u2212 X log Y)\u2212 tr (X) \u00b7 log ( tr (X)\ntr (Y)\n) , (99)\nwhich is Equation (95).\nRow VIII \u2014 We have the same property as for Row V, but this time with \u03d52 = \u2212d\u2212 log x [Kulis et al., 2009]. We check that whenever det(X) = 1, we have\n\u03d5(X) = \u2212d\u2212 log det(X) = \u2212d = \u2212 det(X)tr (I) = tr ( det(X)X\u22121X ) = tr ( \u2207\u03d5(X)>X ) . (100)\nFor g(X) .= det X1/d, we get:\n\u03d5\u2020(X) = det X1/d \u00b7 ( \u2212d\u2212 log det ( 1 det X1/d \u00b7 X ))\n= det X1/d \u00b7 ( \u2212d\u2212 log 1\ndet X \u00b7 det X\n) = \u2212d \u00b7 det X1/d , (101)\nand furthermore\n\u2207\u03d5\u2020(X) = \u2212d \u00b7 \u2207(det X1/d)(X) = \u2212det(X1/d) \u00b7 X\u22121 (102)\nSo,\nD\u03d5\u2020 (X\u2016Y) = \u2212d \u00b7 det X1/d + d \u00b7 det Y1/d + tr ( det(Y1/d) \u00b7 Y\u22121(X \u2212 Y) )\n= \u2212d \u00b7 det X1/d + d \u00b7 det Y1/d + det(Y1/d)tr ( XY\u22121 ) \u2212 d \u00b7 det Y1/d = det(Y1/d)tr ( XY\u22121 ) \u2212 d \u00b7 det X1/d . (103)\nWe check that it is equal to:\ng(X) \u00b7D\u03d5 ( 1\ng(X) \u00b7 X\u2016 1 g(Y) \u00b7 Y )\n= det X1/d \u00b7 \u2211\ni,j\n(u>i vj) 2 \u00b7 ( li det Y 1/d\ntj det X1/d \u2212 log li det Y\n1/d tj det X1/d \u2212 d ) . (104)\nTo check it, we use the fact that, since U and V are orthonormal,\n\u2211\ni,j\n(u>i vj) 2 \u00b7 log li det Y\n1/d\ntj det X1/d\n= \u2211\ni,j\n(u>i vj) 2 \u00b7 log li \u2212\n\u2211\ni,j\n(u>i vj) 2 \u00b7 log det X1/d\n+ \u2211\ni,j\n(u>i vj) 2 \u00b7 log det Y1/d \u2212\n\u2211\ni,j\n(u>i vj) 2 \u00b7 log tj\n= \u2211\ni\nlog li \u2212 d \u00b7 log det X1/d\n\ufe38 \ufe37\ufe37 \ufe38 =0\n+ d \u00b7 log det Y1/d \u2212 \u2211\nj\nlog lj\n\ufe38 \ufe37\ufe37 \ufe38 =0\n= 0 , (105)\nwhich yields\ng(X) \u00b7D\u03d5 ( 1\ng(X) \u00b7 X\u2016 1 g(Y) \u00b7 Y ) = det X1/d \u00b7 \u2211\ni,j\n(u>i vj) 2 \u00b7 ( li det Y 1/d tj det X1/d \u2212 d )\n= det Y1/d \u00b7 \u2211\ni,j\n(u>i vj) 2 \u00b7 li tj \u2212 d \u00b7 det X1/d\n= det Y1/d \u00b7 tr ( XY\u22121 ) \u2212 d \u00b7 det X1/d , (106)\nwhich is equal to Equation (103)."}, {"heading": "D Going deep: higher-order identities", "text": "We can generalize Theorem 1 to higher order identities. For this, consider k > 0 an integer, and let g1, g2, ..., gk : X \u2192 R\u2217 be a sequence of differentiable functions. For any `, `\u2032 \u2208 [k]\u2217 such that ` \u2264 `\u2032, we let g\u0303`,`\u2032 be defined recursively as:\ng\u0303`,`\u2032(x) . =\n{ g\u0303`\u22121,`\u2032(x) \u00b7 g`\u2032\u2212(`\u22121) ( 1 g\u0303`\u22121,`\u2032 (x) \u00b7 x )\nif 1 < ` \u2264 `\u2032 , g`\u2032(x) if ` = 1 ,\n(107)\nand, for any ` \u2208 [k],\n\u03d5\u2020(`)(x) . =\n{ g`(x) \u00b7 \u03d5\u2020(`\u22121) ( 1 g`(x) \u00b7 x )\nif 0 < ` \u2264 k , \u03d5(x) if ` = 0 .\n(108)\nNotice that even when all g. are linear, this does not guarantee that some g\u0303`,`\u2032 for ` 6= 1 is going to be linear. However, if for example g`\u2032 is linear and all \u201cpreceeding\u201d g` (` \u2264 `\u2032) are homogeneous of degree 1, then all g\u0303`,`\u2032 (\u2200` \u2264 `\u2032) are linear.\nCorollary 17 For any k \u2208 N\u2217, let \u03d5 : X\u2192 R be convex differentiable, and g` : X\u2192 R\u2217 (` \u2208 [k]) a sequence of k differentiable functions. Then the following relationship holds, for any `, `\u2032 \u2208 [k]\u2217 with ` \u2264 `\u2032:\ng\u0303`,`\u2032(x) \u00b7D\u03d5\u2020(`\u2032\u2212`) ( 1\ng\u0303`,`\u2032(x) \u00b7 x\u2016 1 g\u0303`,`\u2032(y) \u00b7 y ) = D\u03d5\u2020(`\u2032) (x\u2016y) ,\u2200x,y \u2208 X , (109)\nwith g\u0303`,`\u2032 defined as in Equation (107) and \u03d5\u2020(` \u2032) defined as in Equation (108), if and only if at least one of the two following conditions hold:\n(i) g\u0303`,`\u2032 is linear on X;\n(ii) \u03d5\u2020(` \u2032\u2212`) is positive homogeneous of degree 1 on X`,`\u2032 . = {(1/g\u0303`,`\u2032(x)) \u00b7 x : x \u2208 X}.\nWe check that whenever \u03d5 is convex and all g. are non-negative, then all \u03d5\u2020(`) are convex (\u2200` \u2208 [k]). To prove this, we choose `\u2032 = ` and rewrite Equation (3), which brings, since \u03d5\u2020(` \u2032\u2212`) = \u03d5\u2020(0) = \u03d5,\ng\u0303`,`(x) \u00b7D\u03d5 ( 1\ng\u0303`,`(x) \u00b7 x\u2016 1 g\u0303`,`(y) \u00b7 y ) = D\u03d5\u2020(`) (x\u2016y) ,\u2200x,y \u2208 X . (110)\nSince \u03d5 is convex, a sufficient condition to prove our result is to show that g\u0303`,` is non-negative \u2014 which will prove that the right hand side of (110) is non-negative, and therefore \u03d5\u2020(`) is convex \u2014. This can easily be proven by induction from the expression of g\u0303`,`\u2032 in (107) and the fact that all g. are non-negative.\nOne interesting candidate for simplification is when all g. are the same linear function, say g`(x) = a>x+ b,\u2200` \u2208 [k]. In this case, we have indeed:\ng\u0303`,`\u2032(x) = a >x+ b \u00b7 g\u0303`\u22121,`\u2032(x)\n= b` + a>x \u00b7 `\u22121\u2211\nj=1\nbj , (111)\n\u03d5\u2020(` \u2032)(x) =  b`\u2032 + a>x \u00b7 `\u2032\u22121\u2211\nj=1\nbj   \u00b7 \u03d5 ( 1 b`\u2032 + a>x \u00b7\u2211`\u2032\u22121j=1 bj \u00b7 x ) . (112)\nProof [Proof of Corollary 17] To check eq. (4), we first remark (`\u2032 being fixed) that it holds for ` = 1 (this is eq. (4)), and then proceed by an induction from the induction base hypothesis that, for some ` \u2264 `\u2032,\n\u03d5\u2020(` \u2032)(x) = g\u0303`,`\u2032(x) \u00b7 \u03d5\u2020(`\n\u2032\u2212`) ( 1 g\u0303`,`\u2032(x) \u00b7 x ) . (113)\nWe now have\n\u03d5\u2020(` \u2032)(x)\n= g\u0303`+1,`\u2032(x)\ng`\u2032\u2212` (\n1 g\u0303`,`\u2032 (x)\n\u00b7 x ) \u00b7 \u03d5\u2020(`\u2032\u2212`)\n( 1 g\u0303`,`\u2032(x) \u00b7 x )\n(114)\n= g\u0303`+1,`\u2032(x)\ng`\u2032\u2212` (\n1 g\u0303`,`\u2032 (x)\n\u00b7 x ) \u00b7 g`\u2032\u2212`\n( 1 g\u0303`,`\u2032(x) \u00b7 x ) \u00b7 \u03d5\u2020(`\u2032\u2212(`+1))   1 g\u0303`,`\u2032(x)g`\u2032\u2212` ( 1 g\u0303`,`\u2032 (x) \u00b7 x ) \u00b7 x   (115)\n= g\u0303`+1,`\u2032(x) \u00b7 \u03d5\u2020(` \u2032\u2212(`+1))   1 g\u0303`,`\u2032(x)g`\u2032\u2212` ( 1 g\u0303`,`\u2032 (x) \u00b7 x ) \u00b7 x  \n= g\u0303`+1,`\u2032(x) \u00b7 \u03d5\u2020(` \u2032\u2212(`+1))\n( 1 g\u0303`+1,`\u2032(x) \u00b7 x ) . (116)\nEq. (114) comes from eq. (113) and the definition of g\u0303` in (107), eq. (115) comes from the definition of \u03d5\u2020(` \u2032\u2212`) in (108), eq. (116) is a second use of the definition of g\u0303` in (107).\nNotice the eventual high non-linearities introduced by the composition in eqs (107,108), which justifies the \"deep\" characterization."}, {"heading": "E Additional application: exponential families", "text": "Let \u03d5 be the cumulant function of a regular \u03d5-exponential family with pdf p\u03d5(.|\u03b8), where \u03b8 \u2208 X is its natural parameter. Let \u2126(.) be a norm on X. Let X\u2126 be the image of the application from X onto the \u2126-ball of unit norm defined by x 7\u2192 (1/\u2126(x)) \u00b7 x. Let \u03b8\u2126 be the image of \u03b8 \u2208 X. For any two \u03b8,\u03b8\u2032 \u2208 X, let\nKL\u03d5(\u03b8\u2016\u03b8\u2032) .= \u222b p\u03d5(x|\u03b8) log\np\u03d5(x|\u03b8) p\u03d5(x|\u03b8\u2032) dx (117)\nbe the KL divergence between the two densities p\u03d5(.|\u03b8) and p\u03d5(.|\u03b8\u2032).\nLemma 18 For any convex \u03d5 which is restricted positive 1-homogeneous on X\u2126, the KL-divergence between two members of the same \u03d5-exponential family satisfies:\nKL\u03d5(\u03b8\u2126\u2016\u03b8\u2032\u2126) = 1 \u2126(\u03b8) \u00b7D\u03d5\u2020(\u03b8\u2032\u2016\u03b8) . (118)\nProof We know that KL(\u03b8\u2016\u03b8\u2032) = D\u03d5(\u03b8\u2032\u2016\u03b8) Boissonnat et al. [2010]. Hence,\nD\u03d5\u2020(\u03b8 \u2032\u2016\u03b8) = \u2126(\u03b8) \u00b7D\u03d5\n( 1\n\u2126(\u03b8) \u00b7 \u03b8\u2032\u2016 1 \u2126(\u03b8\u2032) \u00b7 \u03b8 )\n= \u2126(\u03b8) \u00b7D\u03d5(\u03b8\u2032\u2126\u2016\u03b8\u2126) = \u2126(\u03b8) \u00b7KL\u03d5(\u03b8\u2126\u2016\u03b8\u2032\u2126) , (119)\nas claimed.\nThe interest in Lemma 18 is to provide an integral-free expression of the KL-divergence when natural parameters are scaled by non-trivial transformations. Furthermore, even when D\u03d5\u2020 may not be a Bregman divergence, it still bears the same analytical form, which still may be useful for formal derivations."}, {"heading": "F Additional application: computational geometry, nearest neighbor rules", "text": "Two important objects of central importance in (computational) geometry are balls and Voronoi diagrams induced by a distortion, with which we can characterize the topological and computational aspects of major structures (Voronoi diagrams, triangulations, nearest neighbor topologies, etc.) [Boissonnat et al., 2010].\nSince a Bregman divergence is not necessarily symmetric, there are two types of (dual) balls that can be defined, the first or second types, where the variable x is respectively placed in the left or right position. The first type Bregman balls are convex while the second type are not necessarily convex. A Bregman ball of the second type (with center c and \"radius\" r) is defined as:\nB\u2032(c, r|X, \u03d5) .= {x \u2208 X : D\u03d5(c\u2016x) \u2264 r} . (120)\nIt turns out that any divergence D\u03d5\u2020 induces a ball of the second type, which is not necessarily analytically a Bregman ball (when \u03d5\u2020 is not convex), but turns out to define the same ball as a Bregman ball over transformed coordinates. In other words and to be a little bit more specific,\n\"any x belongs to the ball of the second type induced by D\u03d5\u2020 over X iff (1/g(x)) \u00b7 x (\u2208 Xg) belongs to the Bregman ball of the second type induced by D\u03d5 over Xg .\"\nTheorem 19 Let (\u03d5, g, \u03d5\u2020) satisfy the conditions of Theorem 1, with g non negative. Then\nB\u2032(c, r|\u03d5\u2020,X) = B\u2032 ( c, r\ng(c)\n\u2225\u2225\u2225\u2225\u03d5,Xg ) . (121)\nProof From Theorem 1, we have\nD\u03d5\u2020(c\u2016x) \u2264 r (122)\niff\nD\u03d5\n( 1 g(c) \u00b7 c \u2225\u2225\u2225\u2225 1 g(c) \u00b7 x ) \u2264 1 g(c) \u00b7 r . (123)\nHence,\nB\u2032(c, r|\u03d5\u2020,X) = {x \u2208 Xg : D\u03d5(c/g(c)\u2016x/g(x)) \u2264 r/g(c)} = B\u2032(c, r/g(c)|\u03d5,Xg) , (124)\nas claimed.\nThis property is not true for balls of the first type. What Theorem 19 says is that the topology induced by D\u03d5\u2020 over X is just no different from that induced by D\u03d5 over Xg .\nLet us now investigate Bregman Voronoi diagrams. In the same way as there exists two types of Bregman balls, we can define two types of Bregman Voronoi diagrams that depend on the equation of the Bregman bisector [Boissonnat et al., 2010]. Of particular interest is the Bregman bisector of the first type:\nBB\u03d5(x,y|X) = {z \u2208 X : D\u03d5(z\u2016x) = D\u03d5(z\u2016y)} . (125)\nIt turns out that any divergence D\u03d5\u2020 induces a bisector of the first type which is not necessarily analytically a Bregman bisector (when \u03d5\u2020 is not convex), but turns out to define the same bisector as a Bregman bisector over transformed coordinates. Again, we get more precisely\n\"any x belongs to a Bregman bisector of the first type induced by D\u03d5\u2020 over X iff (1/g(x)) \u00b7 x (\u2208 Xg) belongs to the corresponding Bregman bisector of the first type induced by D\u03d5 over Xg .\"\nTheorem 20 Let (\u03d5, g, \u03d5\u2020) satisfy the conditions of Theorem 1. Then\nBB\u03d5\u2020(x,y|X) = BB\u03d5(x,y|Xg) . (126)\n(proof similar to Theorem 19) This property is not true for Bregman bisectors of the second type. Theorems 19, 20 have several important algorithmic consequences, some of which are listed now:\n\u2022 the Voronoi diagram (resp. Delaunay triangulation) of the first type associated to \u03d5\u2020 can be constructed via the Voronoi diagram (resp. Delaunay triangulation) of the first type associated to \u03d5 [Boissonnat et al., 2010];\n\u2022 range search using ball trees on D\u03d5\u2020 can be efficiently implemented using Bregman divergence D\u03d5 on Xg [Cayton, 2009];\n\u2022 the minimum enclosing ball problem, the one-class clustering problem (an important problem in machine learning), with balls of the second type on D\u03d5\u2020 can be solved via the minimum Bregman enclosing ball problem on D\u03d5 [Nock and Nielsen, 2005]."}, {"heading": "G Review: binary density ratio estimation", "text": "For completeness, we quickly review the central result of Menon and Ong [2016, Proposition 3]. Let (P,Q, \u03c0) be densities giving P(X|Y = 1),P(X = x|Y = \u22121),P(Y = 1) respectively, and M giving P(X = x) accordingly. Let r(x) .= P(X = x|Y = 1)/P(X = x|Y = \u22121) be the density ratio of the class-conditional densities, and \u03b7(x) . = P[Y = 1|X = x] be the class-probability function. Then, we have the following, which extends [Menon and Ong, 2016, Proposition 6] for the case \u03c0 6= 12 .\nLemma 21 Given a class-probability estimator \u03b7\u0302 : X\u2192 [0, 1], let the density ratio estimator r\u0302 be\nr\u0302(x) = 1\u2212 \u03c0 \u03c0 \u00b7 \u03b7\u0302(x) 1\u2212 \u03b7\u0302(x) . (127)\nThen for any convex differentiable \u03d5 : [0, 1]\u2192 R,\nEX\u223cM [D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))] = \u03c0 \u00b7 EX\u223cQ [ D\u03d5\u2020(r(X)\u2016r\u0302(X)) ] . (128)\nwhere \u03d5\u2020 is as per Equation 4 with g(z) .= 1\u2212\u03c0\u03c0 + z .\nProof [Proof of Lemma 21] Note that\n1 g(r(x)) \u00b7 r(x) = \u03c0P(X = x|Y = \u22121) P(X = x) \u00b7 P(X = x|Y = 1) P(X = x|Y = \u22121)\n= \u03c0P(X = x|Y = 1) P(X = x) = \u03b7(x) , (129)\nand furthermore\nP(X = x) = (1\u2212 \u03c0)P(X = x|Y = \u22121) + \u03c0P(X = x|Y = 1)\n= \u03c0 \u00b7 (\n1\u2212 \u03c0 \u03c0 + P(X = x|Y = 1) P(X = x|Y = \u22121)\n) \u00b7 P(X = x|Y = \u22121)\n= \u03c0 \u00b7 g(r(x)) \u00b7 P(X = x|Y = \u22121) . (130)\nSo,\nEM [D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))] = \u03c0 \u00b7 EQ [g(r(X)) \u00b7D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))] (131)\n= \u03c0 \u00b7 EQ [ g(r(X)) \u00b7D\u03d5 ( 1\ng(r(X)) \u00b7 r(X)\n\u2225\u2225\u2225\u2225 \u03b7\u0302(X) )]\n(132)\n= \u03c0 \u00b7 EQ [ g(r(X)) \u00b7D\u03d5 ( 1\ng(r(X)) \u00b7 r(X)\n\u2225\u2225\u2225\u2225 1\ng(r\u0302(X)) \u00b7 r\u0302(X)\n)] (133)\n= \u03c0 \u00b7 EQ [ D\u03d5\u2020(r(X)\u2016r\u0302(X)) ] , (134)\nas claimed. Equation (131) comes from (130), Equation (132) comes from (129), Equation (133) comes from (127) and the definition of g. Equation (134) comes from Theorem 1, noting that g is linear."}, {"heading": "H Additional experiments", "text": "H.1 Multiclass density ratio experiments\nWe consider a synthetic multiclass density ratio estimation problem. We fix X = R2, and consider C = 3 classes. We consider a distribution where the class-conditionals Pr(X|Y = c) are multivariate Gaussians with means \u00b5c and covariance \u03c32c \u00b7 Id. As the class-conditionals have a closed form, we can explicitly compute \u03b7, as well the density ratio r to the reference class c\u2217 = C.\nFor fixed class prior \u03c0 = Pr(Y = c), we draw NTr samples from Pr(X,Y). From this, we estimate the classprobability \u03b7\u0302 using multiclass logistic regression. This can be seen as minimising EX\u223cM [D\u03d5(\u03b7(X)\u2016\u03b7\u0302(X))] where \u03d5(z) = \u2211 i zi log zi is the generator for the KL-divergence.\nWe then use Equation 6 to estimate the density ratios r\u0302 from \u03b7\u0302. On a fresh sample of Nte instances from Pr(X,Y), we estimate the right hand side of Lemma 2, viz. EX\u223cPC [ D\u03d5\u2020(r(X)\u2016r\u0302(X)) ] , where \u03d5\u2020 uses the g as specified in Lemma 2. From the result of Lemma 2, we expect this divergence to be small when \u03b7\u0302 is a good estimator of \u03b7.\nWe perform the above for sample sizes N \u2208 {44, 45, . . . , 410}, with NTr = 0.8N and NTe = 0.2N . For each sample size, we perform T = 25 trials, where in each trial we randomly draw \u03c0 uniformly over (1/C)1 + (1\u2212 1/C) \u00b7 [0, 1]C , \u00b5c from 0.1 \u00b7N (0, 1), and \u03c3c uniformly from [0.5, 1]. Figure 4 summarises the mean divergence across the T trials for each sample size. We see that, as expected, with more training samples the divergence decreases in a monotone fashion.\nH.2 Adaptive filtering experiments\nTables 8 \u2013 13 present in extenso the experiments of p-LMS vs DN-p-LMS, as a function of (p, q), whether target u is sparse or not, and the misestimation factor \u03c1 for Xp. We refer to Kivinen et al. [2006] for the formal definitions used for sparse / dense targets as well as for the experimental setting, which we have reproduced with the sole difference that the signal changes periodically each 1 000 iterations."}, {"heading": "I Comment: Theorem 1 is a scaled isometry in disguise (sometimes)", "text": "Theorem 1 states in fact an isometry under some conditions, but an adaptive one in the sense that metrics involved rely on all parameters, and in particular on the points involved in the divergences (See Figure 5). Indeed, a simple Taylor expansion of the equation (2) (main file) shows that any such Bregman distortion with a twice differentiable generator can be expressed as:\nD\u03d5(x\u2016y) = 1 2 \u00b7 (x\u2212 y)>H\u03d5(x\u2212 y) , (135)\nfor some value of the Hessian H\u03d5 depending on x,y (see for example [Kivinen et al., 2006, Appendix I], [Amari and Nagaoka, 2000]). Hence, under the constraint that both \u03d5 and \u03d5\u2020 are twice differentiable, eq. (3) becomes\ng(x) \u00b7 ( 1\ng(x) \u00b7 x\u2212 1 g(y) \u00b7 y )> H\u03d5 ( 1 g(x) \u00b7 x\u2212 1 g(y) \u00b7 y ) = (x\u2212 y)>H\u03d5\u2020(x\u2212 y) . (136)\nAssuming g non-negative (which, by the way, enforces the convexity of \u03d5\u2020), we get by taking square roots,\n\u221a g(x) \u00b7 \u2225\u2225\u2225\u2225 1 g(x) \u00b7 x\u2212 1 g(y) \u00b7 y \u2225\u2225\u2225\u2225\nH\u03d5 = \u2016x\u2212 y\u2016H \u03d5\u2020 , (137)\nwhich is a scaled isometry relationship between Xg (left) and X (right), but again the metrics involved depend on the arguments. Nevertheless, eq. (137) displays a sophisticated relationship between distances in Xg and in X which may prove useful in itself."}], "references": [{"title": "Bregman divergences and triangle inequality", "author": ["S. Acharyya", "A. Banerjee", "D. Boley"], "venue": "In SDM,", "citeRegEx": "Acharyya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharyya et al\\.", "year": 2013}, {"title": "Loss functions for binary class probability estimation and classification", "author": ["J.-D. Boissonnat", "F. Nielsen", "R. Nock"], "venue": "Bregman Voronoi diagrams. DCG,", "citeRegEx": "Boissonnat et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Boissonnat et al\\.", "year": 2003}, {"title": "Spherical averages and applications to spherical splines and interpolation", "author": ["S.-R. Buss", "J.-P. Fillmore"], "venue": "Structure and applications,", "citeRegEx": "Buss and Fillmore.,? \\Q2005\\E", "shortCiteRegEx": "Buss and Fillmore.", "year": 2005}, {"title": "Spherical k-means++ clustering", "author": ["Y. Endo", "S. Miyamoto"], "venue": "In Proc. of the 12 MDAI, pages 103\u2013114,", "citeRegEx": "Endo and Miyamoto.,? \\Q2008\\E", "shortCiteRegEx": "Endo and Miyamoto.", "year": 2008}, {"title": "Revisiting Frank-Wolfe: Projection-free sparse convex optimization", "author": ["M. Jaggi"], "venue": "ICML,", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Linking losses for class-probability and density ratio estimation", "author": ["Menon", "C.-S. Ong"], "venue": "In ICML,", "citeRegEx": "Menon and Ong.,? \\Q2009\\E", "shortCiteRegEx": "Menon and Ong.", "year": 2009}, {"title": "Bregman divergences and surrogates for learning", "author": ["R. Nock", "F. Nielsen"], "venue": "Machine Learning,", "citeRegEx": "Nock and Nielsen.,? \\Q2005\\E", "shortCiteRegEx": "Nock and Nielsen.", "year": 2005}, {"title": "Information, divergence and risk for binary experiments", "author": ["M. Reid", "R. Williamson"], "venue": "JMLR, 12:731\u2013817,", "citeRegEx": "Reid and Williamson.,? \\Q2011\\E", "shortCiteRegEx": "Reid and Williamson.", "year": 2011}, {"title": "Hyperbolic centroidal Voronoi tessellation", "author": ["G. Rong", "M. Jin", "X. Guo"], "venue": "th ACM SPM,", "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2010}, {"title": "The dynamics of coarsening in highly anisotropic systems: Si particles in Al\u2212Si liquids", "author": ["A.-J. Shahani", "E.-B. Gulsoy", "V.-J. Roussochatzakis", "J.-W. Gibbs", "J.-L. Fife", "P.-W. Voorhees"], "venue": "Acta Materialia,", "citeRegEx": "Shahani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahani et al\\.", "year": 2015}, {"title": "A mixture of Manhattan frames: Beyond the Manhattan world", "author": ["J. Straub", "G. Rosman", "O. Freifeld", "J.-J. Leonard", "J.-W. Fisher III"], "venue": "In Proc. of the 27 IEEE CVPR,", "citeRegEx": "Straub et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2014}, {"title": "Real-time Manhattan world rotation estimation in 3d", "author": ["J. Straub", "N. Bhandari", "J.-J. Leonard", "J.-W. Fisher III"], "venue": "In Proc. of the 27 IROS,", "citeRegEx": "Straub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2015}, {"title": "Small-variance nonparametric clustering on the hypersphere", "author": ["J. Straub", "T. Campbell", "J.-P. How", "J.-W. Fisher III"], "venue": "In Proc. of the 28 IEEE CVPR,", "citeRegEx": "Straub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2015}, {"title": "A Dirichlet process mixture model for spherical data", "author": ["J. Straub", "J. Chang", "O. Freifeld", "J.-W. Fisher III"], "venue": "In Proc. of the 18 AISTATS,", "citeRegEx": "Straub et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straub et al\\.", "year": 2015}, {"title": "Subspace clustering", "author": ["R. Vidal"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Vidal.,? \\Q2011\\E", "shortCiteRegEx": "Vidal.", "year": 2011}, {"title": "First-order methods for geodesically convex optimization", "author": ["H. Zhang", "S. Sra"], "venue": null, "citeRegEx": "Zhang and Sra.,? \\Q2016\\E", "shortCiteRegEx": "Zhang and Sra.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": ", 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions \u03c6 with derivative \u03c6\u2032, conjugate \u03c6, and divergence D\u03c6: \u2022 the triangle equality: D\u03c6(x\u2016y) +D\u03c6(y\u2016z)\u2212D\u03c6(x\u2016z) = (\u03c6\u2032(z)\u2212 \u03c6\u2032(y))(x\u2212 y); \u2022 the dual symmetry property: D\u03c6(x\u2016y) = D\u03c6?(\u03c6\u2032(y)\u2016\u03c6\u2032(x)); \u2022 the right-centroid (population minimizer) is the average: arg min\u03bc E[D\u03c6(X\u2016\u03bc)] = E[X]. Casting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence.", "startOffset": 37, "endOffset": 799}, {"referenceID": 1, "context": ", 2016], and computational geometry [Boissonnat et al., 2010]. Despite these being very different applications, many of these algorithms and their analyses basically rely on three beautiful analytic properties of Bregman divergences, properties that we summarize for differentiable scalar convex functions \u03c6 with derivative \u03c6\u2032, conjugate \u03c6, and divergence D\u03c6: \u2022 the triangle equality: D\u03c6(x\u2016y) +D\u03c6(y\u2016z)\u2212D\u03c6(x\u2016z) = (\u03c6\u2032(z)\u2212 \u03c6\u2032(y))(x\u2212 y); \u2022 the dual symmetry property: D\u03c6(x\u2016y) = D\u03c6?(\u03c6\u2032(y)\u2016\u03c6\u2032(x)); \u2022 the right-centroid (population minimizer) is the average: arg min\u03bc E[D\u03c6(X\u2016\u03bc)] = E[X]. Casting a problem as a Bregman minimisation allows one to employ these properties to simplify analysis; for example, by interpreting mirror descent as applying a particular Bregman regulariser, Beck and Teboulle [2003] relied on the triangle equality above to simplify its proof of convergence. Another intriguing possibility is that one may derive reductions amongst learning problems by connecting their underlying Bregman minimisations. Menon and Ong [2016] recently established how (binary) density ratio estimation (DRE) can be exactly reduced to class-probability estimation (CPE).", "startOffset": 37, "endOffset": 1041}, {"referenceID": 0, "context": "Hence, Bregman divergences can embed several distances in a different \u2014 and arguably less involved \u2014 way than the transformations known to date [Acharyya et al., 2013].", "startOffset": 144, "endOffset": 167}, {"referenceID": 0, "context": "Hence, Bregman divergences can embed several distances in a different \u2014 and arguably less involved \u2014 way than the transformations known to date [Acharyya et al., 2013]. As with the aforementioned key properties of Bregman divergences, Theorem 1 has potentially wide applicability. We present three such novel applications (see Table 1) to vastly different problems: \u2022 a reduction of multiple density ratio estimation to multiclass-probability estimation (\u00a73), generalising the results of Menon and Ong [2016] for the binary label case, \u2022 a projection-free yet norm-enforcing mirror gradient algorithm (enforced norms are those of mirrored vectors and of the offset) with guarantees for adaptive filtering (\u00a74), and \u2022 a seeding approach for clustering on positively or negatively (constant) curved manifolds based on a popular seeding for flat manifolds and with the same approximation guarantees (\u00a75).", "startOffset": 145, "endOffset": 509}, {"referenceID": 5, "context": "For the special case where X = R, and g(x) = 1 + x, Theorem 1 is exactly Menon and Ong [2016, Lemma 2] (c.f. Equation 1). We wish to highlight a few points with regard to our more general result. First, the \u201cdistortion\u201d generator \u03c6\u2020 may be1 non-convex, as the following illustrates. Example. Suppose \u03c6(x) = 2\u2016x\u20162 corresponds to the generator for squared Euclidean distance. Then, for g(x) = 1 + 1>x, we have \u03c6\u2020(x) = 12 \u00b7 \u2016x\u201622 1+1>x , which is non-convex on X = R . When \u03c6\u2020 is non-convex, the right hand side in Equation 3 is an object that ostensibly bears only a superficial similarity to a Bregman divergence; it is somewhat remarkable that Theorem 1 shows this general \u201cdistortion\u201d between a pair (x,y) to be entirely equivalent to a (scaling of a) Bregman divergence between some transformation of the points. Second, when g is linear, Equation 3 holds for any convex \u03c6. (This was the case considered in Menon and Ong [2016].) When g is non-linear, however, \u03c6 must be chosen carefully so that (\u03c6, g) satisfies the restricted homogeneity conditon2 of Equation 5.", "startOffset": 73, "endOffset": 930}, {"referenceID": 14, "context": "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011].", "startOffset": 241, "endOffset": 254}, {"referenceID": 10, "context": "We emphasize the fact that the clustering problem has significant practical impact for d as small as 2 in computer vision [Straub et al., 2014].", "startOffset": 122, "endOffset": 143}, {"referenceID": 15, "context": "Second, the fact that the manifold has non-zero curvature essentially prevents the direct use of Euclidean optimization algorithms [Zhang and Sra, 2016] \u2014 put simply, the average of two points", "startOffset": 131, "endOffset": 152}, {"referenceID": 6, "context": "Our final application can be related to two problems that have received a steadily growing interest over the past decade in unsupervised machine learning: clustering on a non-linear manifold [Dhillon and Modha, 2001], and subspace custering [Vidal, 2011]. We consider two fundamental manifolds investigated by Galperin [1993] to compute centers of mass from relativistic theory: the sphere S and the hyperboloid H, the former being of positive curvature, and the latter of negative curvature.", "startOffset": 242, "endOffset": 326}, {"referenceID": 3, "context": "The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.", "startOffset": 325, "endOffset": 447}, {"referenceID": 3, "context": "The key to using the approximation property of k-means++ relies on the existence of a coordinate system on the sphere for which the cluster centroid is just the average of the cluster points (polar coordinates), an average that eventually has to be rescaled if the coordinate system is not that one [Dhillon and Modha, 2001, Endo and Miyamoto, 2015]. The existence of this coordinate system makes that the proof of Arthur and Vassilvitskii [2007] (and in particular the key Lemmata 3.2 and 3.3) can be carried out without modification to yield the same approximation ratio as that of Arthur and Vassilvitskii [2007] if the distortion at hand is the squared Euclidean distance, which turns out to be Drec(.", "startOffset": 325, "endOffset": 616}, {"referenceID": 2, "context": "Row III \u2014 As in Buss and Fillmore [2001], we assume \u2016x\u20162 \u2264 \u03c0, or we renormalize or change the radius of the ball) We first lift the data points using the Sphere lifting map R 3 x 7\u2192 x \u2208 R: x .", "startOffset": 16, "endOffset": 41}, {"referenceID": 1, "context": "Proof We know that KL(\u03b8\u2016\u03b8\u2032) = D\u03c6(\u03b8\u2032\u2016\u03b8) Boissonnat et al. [2010]. Hence, D\u03c6\u2020(\u03b8 \u2032\u2016\u03b8) = \u03a9(\u03b8) \u00b7D\u03c6 ( 1 \u03a9(\u03b8) \u00b7 \u03b8\u2032\u2016 1 \u03a9(\u03b8\u2032) \u00b7 \u03b8 )", "startOffset": 39, "endOffset": 64}, {"referenceID": 6, "context": ", 2010]; \u2022 range search using ball trees on D\u03c6\u2020 can be efficiently implemented using Bregman divergence D\u03c6 on Xg [Cayton, 2009]; \u2022 the minimum enclosing ball problem, the one-class clustering problem (an important problem in machine learning), with balls of the second type on D\u03c6\u2020 can be solved via the minimum Bregman enclosing ball problem on D\u03c6 [Nock and Nielsen, 2005].", "startOffset": 348, "endOffset": 372}], "year": 2016, "abstractText": "Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms. This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses. We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain \u201cBregman distortions\u201d (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices. Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning.", "creator": "LaTeX with hyperref package"}}}