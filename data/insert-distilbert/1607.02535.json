{"id": "1607.02535", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2016", "title": "Learning from Multiway Data: Simple and Efficient Tensor Regression", "abstract": "tensor regression has shown used to be advantageous processes in learning tasks with multi - directional goal relatedness. given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. in this academic paper, we introduce weak subsampled tensor projected gradient to solve completing the problem. conversely our algorithm is impressively : simple faster and efficient. it is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for easy further acceleration. theoretical analysis shows that our algorithm converges equally to the first correct solution in fixed number of iterations. the memory requirement grows only linearly fast with the size of the problem. we demonstrate superior empirical performance on both multi - linear multi - task learning and spatio - temporal applications.", "histories": [["v1", "Fri, 8 Jul 2016 21:40:44 GMT  (447kb,D)", "http://arxiv.org/abs/1607.02535v1", "10 pages, Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016"]], "COMMENTS": "10 pages, Proceedings of the 33rd International Conference on Machine Learning (ICML-16), 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rose yu", "yan liu"], "accepted": true, "id": "1607.02535"}, "pdf": {"name": "1607.02535.pdf", "metadata": {"source": "CRF", "title": "Learning from Multiway Data: Simple and Efficient Tensor Regression", "authors": ["Rose Yu", "Yan Liu"], "emails": ["QIYU@USC.EDU", "YANLIU.CS@USC.EDU"], "sections": [{"heading": "1. Introduction", "text": "Massive multiway data emerge from many fields: spacetime measurements on several variables in climate dynamics (Hasselmann, 1997), multichannel EEG signals in neurology (Acar et al., 2007) and natural images sequences in computer vision (Vasilescu & Terzopoulos, 2002). Tensor provides a natural representation for multiway data. In particular, tensor decomposition has been a popular technique for exploratory data analysis (Kolda & Bader, 2009) and has been extensively studied. In contrast, tensor regression, which aims to learn a model with multi-linear parameters, is especially suitable for applications with multi-directional relatedness, but has not been fully examined. For example, in a task that predicts multiple climate variables at different locations and time, the data can be indexed by variable \u00d7 location \u00d7 time. Tensor regression provides us with a concise way of modeling complex structures in multiway data.\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nMany tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis. These methods share the assumption that the model parameters form a high order tensor and there exists a lowdimensional factorization for the model tensor. They can be summarized into two types of approaches: (1) alternating least square (ALS) sequentially finds the factor that minimizes the loss while keeping others fixed; (2) spectral regularization approximates the original non-convex problem with a convex surrogate loss, such as the nuclear norm of the unfolded tensor.\nA clear drawback of all the algorithms mentioned above is high computational cost. ALS displays unstable convergence properties and outputs sub-optimal solutions (Cichocki et al., 2009). Trace-norm minimization suffers from slow convergence (Gandy et al., 2011). Moreover, those methods face the memory bottleneck when dealing with large-scale datasets. ALS, for example, requires the matricization of entire data tensor at every mode. In addition, most existing algorithms are largely constrained by the specific tensor regression model. Adapting one algorithm to a new regression model involves derivations for all the updating steps, which can be tedious and sometimes non-trivial.\nIn this paper, we introduce subsampled Tensor Projected Gradient (TPG), a simple and fast recipe to address the challenge. It is an efficient solver for a variety of tensor regression problems. The memory requirement grows linearly with the size of the problem. Our algorithm is based upon projected gradient descent (Calamai & More\u0301, 1987) and can also be seen as a tensor generalization of iterative hard thresholding algorithm (Blumensath & Davies, 2009). At each projection step, our algorithm iteratively returns a set of leading singular vectors of the model, avoiding full singular value decomposition (SVD). To handle large sample size, we employ randomized sketching for subsampling and noise reduction to further accelerate the process.\nWe provide theoretical analysis of our algorithm, which is guaranteed to find the correct solution under the Restricted\nar X\niv :1\n60 7.\n02 53\n5v 1\n[ cs\n.L G\n] 8\nJ ul\n2 01\n6\nIsometry Property (RIP) assumption. In fact, the algorithm only needs a fixed number of iterations, depending solely on the logarithm of signal to noise ratio. It is also robust to noise, with estimation error depending linearly on the size of the observation error. The proposed method is simple and easy to implement. At the same time, it enjoys fast convergence rate and superior robustness. We demonstrate the empirical performance on two example applications: multilinear multi-task learning and multivariate spatio-temporal forecasting. Experiment results show that the proposed algorithm significantly outperforms existing approaches in both prediction accuracy and speed."}, {"heading": "2. Preliminary", "text": "Across the paper, we use calligraphy font for tensors, such as X ,Y , bold uppercase letters for matrices, such as X,Y, and bold lowercase letters for vectors, such as x,y.\nTensor Unfolding Each dimension of a tensor is a mode. An n-mode unfolding of a tensor W along mode n transforms a tensor into a matrix W(n) by treating n as the first mode of the matrix and cyclically concatenating other modes. The indexing follows the convention in (Kolda & Bader, 2009). It is also known as tensor matricization.\nN-Mode Product The n-mode product between tensorW and matrix U on mode n is represented asW \u00d7n U and is defined as (W \u00d7n U)(n) = UW(n) .\nTucker Decomposition Tucker decomposition factorizes a tensorW intoW = S \u00d71 U1 \u00b7 \u00b7 \u00b7 \u00d7n Un, where {Un} are all unitary matrices and the core tensor satisfies that S(n) is row-wise orthogonal for all n = 1, 2, . . . , N ."}, {"heading": "3. Related Work", "text": "Several algorithms have been proposed for tensor regression. For example, (Zhou et al., 2013) proposes to use Alternating least square (ALS) algorithm. (Romera-Paredes et al., 2013) employs ALS as well as an Alternating Direction Method of Multiplier (ADMM) technique to solve the nuclear norm regularized optimization problem. (Signoretto et al., 2014) proposes a more general version of ADMM based on Douglas-Rachford splitting method. Both ADMM-based algorithms try to solve a convex relaxation of the original optimization problem, using singular value soft-thresholding. To address the scalability issue of these methods, (Yu et al., 2014) proposes a greedy algorithm following the Orthogonal Matching Pursuit (OMP) scheme. Though significantly faster, it requires the matricization of the data tensor, and thus would face memory bottleneck when dealing with large sample size.\nOur work is closely related to iterative hard thresholding in compressive sensing (Blumensath & Davies, 2009), spar-\nsified gradient descent in sparse recovery (Garg & Khandekar, 2009) or singular value projection method in lowrank matrix completion (Jain et al., 2010). We generalize the idea of iterative hard thresholding to tensors and utilize several tensor specific properties to achieve acceleration. We also leverage randomized sampling technique, which concerns how to sample data to accelerate the common learning algorithms. Specifically, we employ count sketch (Clarkson & Woodruff, 2013) as a pre-processing step to alleviate the memory bottleneck for large dataset."}, {"heading": "4. Simple and Efficient Tensor Regression", "text": "We start by describing the problem of tensor regression and our proposed algorithm in details. We use three-mode tensor for ease of explanation. Our method and analysis directly applies to higher order cases."}, {"heading": "4.1. Tensor Regression", "text": "Given a predictor tensor X and a response tensor Y , tensor regression targets at the following problem:\nW? = argmin W L(W;X ,Y)\ns.t. rank(W) \u2264 R (1)\nThe problem aims to estimate a model tensor W \u2208 RD1\u00d7D2\u00d7D3 that minimizes the empirical loss L, subject to the constraint that the Tucker rank of W is at most R. Equivalent, the model tensor W has a low-dimensional factorization W = S \u00d71 U1 \u00d72 U2 \u00d73 U3 with core S \u2208 RR1\u00d7R2\u00d7R3 and orthonormal projection matrices {Un \u2208 RDn\u00d7Rn}. The dimensionality of S is at most R. The reason we favor Tucker rank over others is due to the fact that it is a high order generalization of matrix SVD, thus is computational tractable, and carries nice properties that we later would utilize.\nMany existing tensor regression models are special cases of the problem in (1). For example, in multi-linear multitask learning (Romera-Paredes et al., 2013), given the predictor and response for each task (Xt,Yt), the empirical loss is defined as the summarization of the loss for all the tasks, i.e., L(W;X ,Y) = \u2211T t=1 \u2016Yt \u2212 Xtwt\u20162F , with wt as the tth column of W(1). For the univariate GLM model in (Zhou et al., 2013), the model is defined as Y = vec(X )T vec(W)+E . Table 1 summarizes existing tensor regression models, their algorithms as well as main application domains. In this work, we use the simple linear regression model Y = \u3008X ,W\u3009 + E to illustrate our idea, where X \u2208 RT\u00d7D1\u00d7D3 , Y \u2208 RT\u00d7D2\u00d7D3 with sample size T , and E as i.i.d Gaussian noise. The tensor inner product \u3008X ,W\u3009 is defined as the matrix multiplication on each slice independently, i.e., \u3008X ,W\u3009 = \u2211M m=1 X:,:,m,W:,:,m. Our methodology can be easily extended to handle more\ncomplex regression models."}, {"heading": "4.2. Tensor Projected Gradient", "text": "To solve problem (1), we propose a simple and efficient tensor regression algorithm: subsampled Tensor Projected Gradient (TPG). TPG is based upon the prototypical method of projected gradient descent and can also be seen as a tensor generalization of iterative hard thresholding algorithm (Blumensath & Davies, 2009). For the projection step, we resort to tensor power iterations to iteratively search for the leading singular vectors of the model tensor. We further leverage randomized sketching (Clarkson & Woodruff, 2013) to address the memory bottleneck and speed up the algorithm.\nAs shown in Algorithm 1, subsampled Tensor Projected Gradient (TPG) combines a gradient step with a proximal point projection step (Rockafellar, 1976). The gradient step treats (1) as an unconstrained optimization ofW . As long as the loss function is differentiable in a neighborhood of current solution, standard gradient descent methods can be applied. For our case, computing the gradient under linear model is trivial: OL(W;X ,Y) = \u3008X T ,Y \u2212 \u3008X ,W\u3009\u3009. After the gradient step, the subsequent proximal point step aims to find a projection PR(W) : RD1\u00d7D2\u00d7D3 \u2192 RD1\u00d7D2\u00d7D3 satisfying:\nPR(Wk) = argmin W (\u2016Wk \u2212W\u20162F )\ns.t. W \u2208 C(R) = {W : rank(W) \u2264 R} (2)\nThe difficulty of solving the above problem mainly comes from the non-convexity of the set of low-rank tensors. A common approach is to use nuclear norm as a convex surrogate to approximate the rank constraint (Gandy et al., 2011). The resulting problem can either be solved by SemiDefinite Programming (SDP) or soft-thresholding of the singular values. However, both algorithms are computational expensive. Soft-thresholding, for example, requires a full SVD for each unfolding of the tensor.\nAlgorithm 1 Subsampled Tensor Projected Gradient 1: Input: predictor X , response Y , rank R 2: Output: model tensorW \u2208 RD1\u00d7D2\u00d7D3 3: Compute count sketch S 4: Sketch Y\u0303 \u2190 Y \u00d71 S, X\u0303 \u2190 X \u00d71 S 5: InitializeW0 as zero tensor 6: repeat 7: W\u0303k+1 =Wk \u2212 \u03b7OL(Wk; X\u0303 , Y\u0303) 8: Wk+1 = ITP(W\u0303k+1) 9: until Converge\nIterative hard thresholding, on the other hand, avoids full SVD. It takes advantage of the general Eckart-YoungMirsky theorem (Eckart & Young, 1936) for matrices,\nAlgorithm 2 Iterative Tensor Projection (ITP) 1: Input: model W\u0303 , predictor X , response Y , rank R 2: Output: projectionW \u2208 RD1\u00d7D2\u00d7D3 3: Initialize {U0n} with R left singular vectors ofW(n) 4: while i \u2264 R do 5: repeat 6: uk+11 \u2190 W\u0303 \u00d72 uk2 T \u00d73 uk3 T\n7: uk+12 \u2190 W\u0303 \u00d71 uk1 T \u00d73 uk3 T 8: uk+13 \u2190 W\u0303 \u00d71 uk1 T \u00d72 uk2 T\n9: until Converge to {u1,u2,u3} 10: Update {Un} with {un} 11: W \u2190 W\u0303 \u00d71 U1U1T \u00d72 U2U2T \u00d73 U3U3T 12: if L(W;X ,Y) \u2264 then 13: RETURN 14: end if 15: end while\nwhich allows the Euclidean projection to be efficiently computed with thin SVD. Iterative hard thresholding algorithm has been shown to be memory efficient and robust to noise. Unfortunately, it is well-known that EckartYoung-Mirsky theorem no long applies to higher order tensors (Kolda & Bader, 2009). Therefore, computing highorder singular value decomposition (HOSVD) (De Lathauwer et al., 2000b) and discarding small singular values do not guarantee optimality of the projection.\nTo address the challenge, we note that for tensor Tucker model, we have : W = S \u00d71 U1 \u00d72 U2 \u00d73 U3. And the projection matrices {Un} happen to be the left singular vectors of the unfolded tensor, i.e., Un\u03a3nVTn =W(n). This property allows us to compute each projection matrix efficiently with thin SVD. By iterating over all factors, we can obtain a local optimal solution that is guaranteed to have rank at mostR. We want to emphasize that there is no known algorithm that can guarantee the convergence to the global optimal solution. However, in the Tucker model, different local optimas are highly concentrated, thus the choice of local optima does not really matter (Ishteva et al., 2011b).\nWhen the model parameter tensor W is very large, performing thin SVD itself can be expensive. In our problem, the dimensionality of the model is usually much larger than its rank. With this observation, we utilize another property of Tucker model Un = W \u00d71 \u00b7 \u00b7 \u00b7 \u00d7n\u22121 UTn\u22121 \u00d7n+1 UTn+1 \u00b7 \u00b7 \u00b7 \u00d7N UN . This property implies that instead of performing thin SVD on the original tensor, we can trade cheap tensor matrix multiplication to avoid expensive large matrix SVD. This leads to the Iterative Tensor Projection (ITP) procedure as described in Algorithm 2. Denote {un} as row vectors of {Un}, ITP uses power iteration to find one leading singular vector at a time. The algorithm stops\neither when hitting the rank upper bound R or when the loss function value decreases below a threshold .\nITP is significantly faster than traditional tensor regression algorithms especially when the model is low-rank. It guarantees that the proximal point projection step can be solved efficiently. If we initialize our solution with the top R left singular vectors of tensor unfoldings, the projection iteration can start from a close neighborhood of the stationary point, thus leading to faster convergence. In tensor regression, our main focus is to minimize the empirical loss. Sequentially finding the rank-1 subspace allows us to evaluate the performance as the algorithm proceeds. The decrease of empirical loss would call for early stop of the thin SVD procedure.\nAnother acceleration trick we employ is randomized sketching. This trick is particularly useful when we are encountered with ultra high sample size or extremely sparse data. Online algorithms, such as stochastic gradient descent or stochastic ADMM are common techniques to deal with large samples and break the memory bottleneck. However, from a subsampling point of view, online algorithms make i.i.d assumptions of the data and uniformly select samples. It usually fails to leverage the data sparsity.\nIn our framework, the convergence of the TPG algorithm, as will be discussed in a later section, depends only on the logarithm of signal to noise ratio. Randomized sketching instantiates the mechanism to reduce noise by duplicating data instances and combining the outputs. This mechanism provides TPG with considerable amount of boost. Its performance therefore increases linearly if the noise is decreased. We resort to count sketch (Clarkson & Woodruff, 2013) as a subsampling step before feeding data into TPG. A count sketch matrix of size M \u00d7 N , denoted as S, is generated as follows: start with a zero matrix RM\u00d7N , for each column j, uniformly pick a row i \u2208 {1, 2, \u00b7 \u00b7 \u00b7M} and assign {\u22121, 1} with equal probability to Si,j . In practice, we find count sketch works well with TPG, even when the sample size is very small."}, {"heading": "5. Theoretical Analysis", "text": "We now analyze theoretical properties of the proposed algorithm. We prove that TPG guarantees optimality of the estimated solution, under the assumption that the predictor tensor satisfies Restricted Isometry Property (RIP) (Candes et al., 2006). With carefully designed step size, the algorithm converges to the correct solution in constant number of iterations, and the achievable estimation error depends linearly on the size of the observation error.\nWe assume the predictor tensor satisfies Restricted Isometry Property in the following form:\nDefinition 5.1. (Restricted Isometry Property) The isom-\netry constant of X is the smallest number \u03b4R such as the following holds for allW with Tucker rank at most R.\n(1\u2212 \u03b4R)\u2016W\u20162F \u2264 \u2016\u3008X ,W\u3009\u20162F \u2264 (1 + \u03b4R)\u2016W\u20162F\nNote that even though we make the RIP analogy for tensor X , we actually impose the RIP assumption w.r.t. matrix rank instead of tensor rank. Similar assumption can be found in (Rauhut et al., 2015).\nThe proposed solution TPG as in Algorithm 1 is built upon projected gradient method. To prove the convergence, we first guarantee the optimality (local) of the proximal point step, obtained by ITP in Algorithm 2. The following lemma guarantees the correctness of the solution from ITP. Lemma 5.2. (Tensor Projection) The projection step in Algorithm 2, defined as PR : RD1\u00d7D2\u00d7D3 \u2192 RD1\u00d7D2\u00d7D3 computes a proximal point PR(W\u0303k+1) = Wk+1, whose Tucker rank is at most R. Formally,\nWk+1 = argmin W \u2016W \u2212 W\u0303k+1\u20162F s.t rank(W) \u2264 R\nthe projected solutionWk+1 follows a Tucker model, written as Wk+1 = S \u00d71 U1 \u00d72 U2 \u00d73 U3, where each dimension of the core S is upper bounded by R.\nProof. Minimizing \u2016W \u2212 W\u0303k+1\u20162F given S is equivalent to minimizing the following problem\n\u2016S \u00d71 U1 \u00d72 U2 \u00d73 U3 \u2212 W\u0303k+1\u20162F = {\u2016S \u00d71 U1 \u00d72 U2 \u00d73 U3\u20162F \u2212 2\u3008S \u00d71 U1 \u00d72 U2 \u00d73 U3, W\u0303k+1\u3009+ \u2016W\u0303k+1\u20162F } = {\u2016S\u20162F + \u2016W\u0303k+1\u20162F \u2212 2\u3008S, W\u0303k+1 \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 \u3009} = \u2212\u2016S\u20162F + \u2016W\u0303k+1\u20162F\nGiven projection matrices {Un}, we have:\nS = W\u0303k+1 \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 Thus, the minimizer of \u2016W\u2212W\u0303k+1\u20162F generates projection matrices that maximize the following objective function:\n{Un} = argmax {Un} \u2016W\u0303k+1 \u00d71 UT1 \u00d72 UT2 \u00d73 UT3 \u20162F (3)\nEach row vector un of Un can be solved independently with power iteration. Therefore, repeating this procedure for different modes leads to an optimal (local) minimizer near a neighborhood of W\u0303k+1. In fact, for Tucker tensor, convergence to a saddle point or a local maximum is only observed in artificially designed numerical examples (Ishteva et al., 2011a).\nNext we prove our main theorem, which states that TPG converges to the correct solution in constant number of iterations with isometry constant \u03b42R < 1/3.\nTheorem 5.3. (Main) For tensor regression model Y = \u3008X ,W\u3009+ E , suppose the predictor tensor X satisfies RIP condition with isometry constant \u03b42R < 1/3. Let W? be the optimal tensor of Tucker rank at most R. Then tensor projected gradient (TPG) algorithm with step-size \u03b7 = 11+\u03b4R computes a feasible solution W such that the estimation error \u2016W \u2212 W?\u20162F \u2264 11\u2212\u03b42R \u2016E\u2016 2 F in at most d 1log(1/\u03b1) log( \u2016Y\u20162F \u2016E|2F\n)e iterations for an universal constance \u03b1 that is independent of problem parameters.\nProof. : The decrease in loss\nL(Wk+1)\u2212 L(Wk) (4) = 2\u3008OL(Wk),Wk+1 \u2212Wk\u3009+ \u2016\u3008X ,Wk+1 \u2212Wk\u3009\u20162F \u2264 2\u3008OL(Wk),Wk+1 \u2212Wk\u3009+ (1 + \u03b42R)\u2016Wk+1 \u2212Wk \u20162F\nHere the inequality follows from RIP condition. And isometry constant of \u03b42R follows from the subadditivity of rank.\nDefine upper bound\nu(W) := 2\u3008OL(Wk),W \u2212Wk\u3009+ (1 + \u03b42R)\u2016W \u2212Wk \u20162F = (1 + \u03b42R){\u2016W \u2212 W\u0303k+1\u20162F \u2212 \u3008OL(Wk),W \u2212Wk\u3009}\nwhere the second equality follows from the definition of gradient step W\u0303k+1 =Wk \u2212 \u03b7OL(Wk) From Equation (4) and Lemma 5.2,\nL(Wk+1)\u2212 L(Wk) \u2264 u(Wk+1) \u2264 u(W?) (5) = 2\u3008OL(Wk),W? \u2212Wk\u3009+ (1 + \u03b42R)\u2016W? \u2212Wk \u20162F = 2\u3008OL(Wk),W? \u2212Wk\u3009+ 2\u03b42R\u2016W? \u2212Wk \u20162F + (1\u2212 \u03b42R)\u2016W? \u2212Wk \u20162F \u2264 2\u3008OL(Wk),W? \u2212Wk\u3009+ 2\u03b42R\u2016W? \u2212Wk \u20162F + \u2016\u3008X ,W? \u2212Wk\u3009\u20162F (6) = L(W?)\u2212 L(Wk) + 2\u03b42R\u2016W? \u2212Wk \u20162F \u2264 L(W?)\u2212 L(Wk) + 2\u03b42R\n1\u2212\u03b42R \u2016\u3008X ,W? \u2212Wk\u3009\u20162F (7)\nIn short,\nL(Wk+1) \u2264 L(W?) + 2\u03b42R 1\u2212 \u03b42R \u2016\u3008X ,W? \u2212Wk\u3009\u20162F (8)\nThe inequality (6) and (7) follows from RIP condition. Given model assumption Y \u2212 \u3008X ,W?\u3009 = E , we have\n\u2016\u3008X ,W? \u2212Wk\u3009\u20162F = \u2016Y \u2212 \u3008X ,Wk\u3009 \u2212 E\u20162F = L(Wk)\u2212 2\u3008E ,Y \u2212 \u3008X ,Wk\u3009\u3009+ \u2016E\u20162F\n\u2264 L(Wk) + 2 C L(Wk) + 1 C2 L(Wk) = (1 + 1\nC )2L(Wk) (9)\nas long as the noise satisfies C2\u2016E\u20162F \u2264 L(Wk).\nFollowing Equation (8),\nL(Wk+1) \u2264 \u2016E\u20162F + 2\u03b42R\n1\u2212 \u03b42R (1 +\n1 C )2L(Wk)\n\u2264 [ 1 C2 + 2\u03b42R 1\u2212 \u03b42R (1 + 1 C )2]L(Wk) = \u03b1L(Wk) (10)\nWith the assumption that \u03b42R < 1/3, select C > 1+\u03b42R1\u22123\u03b42R , we have [ 1C2 + 2\u03b42R 1\u2212\u03b42R (1 + 1 C )\n2] < 1. The above inequality implies that the algorithm enjoys a globally geometric convergence rate and the loss decreases multiplicatively.\nFor the simplest case, with initial point as zero, we have L(W0) = \u2016Y\u20162F .\nIn order to obtain a loss value that is small enough\nL(WK) \u2264 \u03b1KL(W0) \u2264 \u2016E\u20162F (11)\nthe algorithm requires at least K \u2265 1log(1/\u03b1) log( \u2016Y\u20162F \u2016E|2F\n) iterations.\n\u2016WK \u2212W?\u20162F \u2264 1\n1\u2212 \u03b42R \u2016\u3008X ,WK \u2212W?\u3009\u20162F (12)\n\u2264 1 1\u2212 \u03b42R (1 + 1 C )2L(WK) \u2264 1 1\u2212 \u03b42R \u2016E\u20162F\nwhere inequality (12) follows from RIP.\nTheorem 5.3 shows that under RIP assumption, TGP converges to an approximate solution in O( 1log(1/\u03b1) log( \u2016Y\u20162F \u2016E|2F\n)) number of iterations, which depends solely on the logarithm of signal to noise ratio. The achievable estimation error depends linearly on the size of the observation error.\nAs a pre-processing step, our proposed algorithm employs l2-subspace embedding (count sketch) to subsample the data instances.\nDefinition 5.4. (l2-subspace embedding) A (1 \u00b1 ) l2subspace embedding for the column space of a N \u00d7D matrix A is a matrix S for which for all x \u2208 RD\n\u2016SAx\u201622 = (1\u00b1 )\u2016Ax\u201622\nSubsampling step solves an unconstrained optimization problem which is essentially a least square problem, the approximation error can be upper-bounded as follows:\nLemma 5.5. (Approximation Guarantee) For any 0 < \u03b4 < 1, given a sparse l2-subspace embedding matrix S with K = O((D1D2D3)2/\u03b4 2) rows, then with probability (1 \u2212 \u03b4), we can achieve (1 + )-approximation. The sketch X \u00d71 S can be computed in O(nnz(X )) time, and Y \u00d71 S can be computed in O(nnz(Y)) time.\nThe result follows directly from (Clarkson & Woodruff, 2013). The randomized sketching leads to a (1 + ) approximation of the original tensor regression solution. It also serves as a noise reduction step, facilitating fast convergence of subsequent TPG procedure."}, {"heading": "6. Applications of Tensor Regression", "text": "Tensor regression finds applications in many domains. We present two examples: one is the multi-linear multi-task learning problem in machine learning community, and the other is the spatio-temporal forecasting problem in time series analysis domain."}, {"heading": "6.1. Multi-linear Multi-task Learning", "text": "Multi-linear multi-task learning (Romera-Paredes et al., 2013; Wimalawarne et al., 2014) tackles the scenario where the tasks to be learned are references by multiple indices, thus contain multi-modal relationship. Given the predictor and response for each task: (Xt \u2208 Rmt\u00d7dt ,Yt \u2208 Rmt\u00d71), traditional multi-task learning concatenate parameter vector wt \u2208 Rdt\u00d71 into a matrix. Here, with additional information about task indices, the model stacks the coefficient vectors into a model tensor W . The empirical loss is defined as the summarization of the least square loss for all the tasks, i.e L(W;X ,Y) = \u2211T t=1 \u2016Yt \u2212Xtwt\u20162F , with wt as the t th column ofW(1). The multi-linear multi-task learning problem can be described as follows:\nW\u0302 = argmin W { T\u2211 t=1 \u2016Yt \u2212Xtwt\u20162F } s.t. rank(W) \u2264 R (13)"}, {"heading": "6.2. Spatio-temporal Forecasting", "text": "Spatio-temporal forecasting (Cressie & Wikle, 2015) is to predict the future values given their historical measurements. Suppose we are given access to measurements X \u2208 RT\u00d7P\u00d7M of T timestamps of M variables over P locations as well as the geographical coordinates of P locations. We can model the time series with a Vector Auto-regressive (VAR) model of lag L, where we assume the generative process as Xt,:,m = Xt,mW:,:,m + Et,:,m, for m = 1, . . . ,M and t = L + 1, . . . , T . Here Xt,m = [X>t\u22121,:,m, . . . ,X>t\u2212L,:,m] denotes the concatenation of L-lag historical data before time t. We learn a model coefficient tensorW \u2208 RPL\u00d7P\u00d7M to forecast multiple variables simultaneously. The forecasting task can be formulated as follows:\nW\u0302 = argmin W\n{ \u2016X\u0302 \u2212 X\u20162F + \u00b5 M\u2211 m=1 tr(X\u0302>:,:,mLX\u0302:,:,m) }\ns.t. X\u0302 = Xt,mW:,:,m, s.t. rank(W) \u2264 R (14)\nwhere rank constraint imposes structures such as spatial clustering and temporal periodicity on the model. The Laplacian regularizer L is constructed from the kernel using the geographical information, which accounts for the spatial proximity of observations."}, {"heading": "7. Experiments", "text": "We conduct a set of experiments on one synthetic dataset and two real world applications. In this section, we present and analyze the results obtained. We compare TGP with following baseline methods:\n\u2022 OLS: ordinary least square estimator without lowrank constraint\n\u2022 THOSVD (De Lathauwer et al., 2000b): a two-step heuristic that first solves the least square and then performs truncated singular value decomposition\n\u2022 Greedy (Yu et al., 2014): a fast tensor regression solution that sequentially estimates rank one subspace based on Orthogonal Matching Pursuit\n\u2022 ADMM (Gandy et al., 2011): alternating direction method of multipliers for nuclear norm regularized optimization"}, {"heading": "7.1. Synthetic Dataset", "text": "We construct a model coefficient tensor W of size 30 \u00d7 30 \u00d7 20 with Tucker rank equals to 2 for all modes. Then, we generate the observations Y and X according to multivariate regression model Y:,:,m = X:,:,mW:,:,m + E:,:,m for m = 1, . . . ,M , where E is a noise tensor with zero mean Gaussian elements. We generate the time series with 30, 000 time stamps and repeat the procedure for 10 times.\nFigure 1(a) and 1(b) shows the parameter estimation RMSE and the run time error bar with respect to the sketching size. Since the true model is low-rank, simple OLS suffers from poor performance. Other methods are able to converge to the correct solution. The main difference occurs for small sketch size scenario. TPG demonstrates its impressively robustness to noise while others shows high variance in estimation RMSE. ADMM obtains reasonable accuracy and is relatively robust, but is very slow.\nWe also investigate the impact of sketching scheme on TPG. We compare count sketch (Count) with sketch with i.i.d Gaussian entries (Gaussian) and sparse random projection (Sparse) (Achlioptas, 2003). Figure 1(c) and 1(d) shows the parameter estimation RMSE and the run time errorbar for TPG combined with different random sketching algorithm. TPG with count sketch achieves best performance, especially for small sketch size. The results justify\nthe metrit of leveraging count sketch for noise reduction, in order to accelerate the convergence of TPG algorithm."}, {"heading": "7.2. Real Data", "text": "In this section, we test the described approaches with two real world application datasets: multi-linear multi-task learning and spatio-temporal forecasting problem."}, {"heading": "7.2.1. MULTI-LINEAR MULTI-TASK LEARNING", "text": "We compare TPG with state-of-art multi-linear multi-task baseline. Our evaluation follows the same experiment setting in (Romera-Paredes et al., 2013) on the restaurant & consumer dataset, provided by the authors in the paper. The data set contains consumer ratings given to different restaurants. The data has 138 consumers gave 3 type of scores for restaurant service. Each restaurant has 45 attributes for rating prediction. The total number of instances for all the tasks were 3483. The problem results in a model tensor of dimension 45\u00d7 138\u00d7 3.\nWe split the training and testing set with different ratio ranging from 0.1 to 0.9 and randomly select the training data instances. When the training size is small, many tasks contained no training example. We also select 200 instances as the validation set. We compare with MLMTLC and multi-task feature learning baselines in the original paper. MTL-L21 uses L21 regularizer and MTL-Trace is the trace-norm regularized multi-task learning algorithm. The model parameters are selected by minimizing the mean squared error on the validation set.\nFigure 2 demonstrates the prediction performance in terms of MSE and runtime with respect to different training size. Compared with MLMTL-C, TPG is around 10% \u2212 25% more accurate and at least 20 times faster. MTL-L21 or MTL-Trace runs faster than MLMTL-C but also sacrifices accuracy. The difference is more noticeable in the case of small training size. These results are not surprising. Given limited samples and highly correlated tasks in the restaurant score prediction, the model parameters demonstrate low-rank property. In fact, we found that rank 1 is the opti-\nmal setting for this data during the experiments."}, {"heading": "7.2.2. SPATIO-TEMPORAL FORECASTING", "text": "For the spatio-temporal forecasting task, we experiment with following two datasets.\nFoursquare Check-In The Foursquare check-in data set contains the users check-in records in Pittsburgh area from Feb 24 to May 23, 2012, categorized by different venue types such as Art & Entertainment, College & University, and Food. We extract hourly check-in records of 739 users in 34 different venues categories over 3, 474 hours time period as well as users\u2019 friendship network.\nUSHCN Measurements The U.S. Historical Climatology Network (USHCN) daily (http://cdiac.ornl. gov/ftp/ushcn_daily/) contains daily measurements for 5 climate variables for more than 100 years. The five climate variables correspond to temperature max, temperature min, precipitation, snow fall and snow depth. The records were collected across more than 1, 200 locations and spans over 45, 384 time stamps.\nWe split the data along the temporal dimension into 80% training set and 20% testing set. We choose VAR (3) model and use 5-fold cross-validation to select the rank during the training phase. For both datasets, we normalize each individual time series by removing the mean and dividing by standard deviation. Due to the memory constraint of the Greedy algorithm, evaluations are conducted on downsampled datasets.\nTable 2 presents the best forecasting performance (w.r.t sketching size) and the corresponding run time for each of the methods. TPG outperforms baseline methods with higher accuracy. Greedy shows similar accuracy, but TPG converges in very few iterations. For USHCN, TPG achieves much higher accuracy with significantly shorter run time. Those results demonstrate the efficiency of our proposed algorithm for spatio-temporal forecasting tasks.\nWe further investigate the learned structure of TPG algorithm from USHCN data. Figure 3 shows the spatialtemporal dependency graph on the terrain of California.\n1cov(X ,W) : Y\u00b7\u00b7\u00b7 ,in\u22121in+1\u00b7\u00b7\u00b7 ,jn\u22121jn+1,\u00b7\u00b7\u00b7 = \u2211 in XinWin\nEach velocity vector represents the aggregated weight learned by TPG from one location to the other. The graph provides an interesting illustration of atmospheric circulation. For example, near Shasta-Trinity National forecast in northern California, the air flow into the forecasts. On the east side along Rocky mountain area, there is a strong atmospheric pressure, leading to wind moving from south east to north west passing the bay area. Another notable atmospheric circulation happens near Salton sea at the border of Utah, caused mainly by the evaporation of the sea."}, {"heading": "8. Discussion", "text": "The implication of our approach has several interesting aspects that might shed light upon future algorithmic design.\n(1) The projection step in TPG does not depends on data, thus it connects to tensor decomposition techniques such as high order orthogonal iteration (HOOI) (De Lathauwer et al., 2000a). However, there is subtle difference in that the regression would call for early stop of iterative projection as it sequentially search for the orthogonal subspaces.\n(2) TPG behaves similarly as first order methods. The convergence rate can be further improved with second order Newton method. This can be done easily by replacing the gradient with Hessian. This modification does not affect the theoretical properties of the proposed algorithm, but would lead to significant empirical improvement (Jain et al., 2010)."}, {"heading": "9. Conclusion", "text": "In this paper, we study tensor regression as a tool to analyze multiway data. We introduce Tensor Projected Gradient to solve the problem. Our approach is built upon projected gradient method, generalizing iterative hard thresholding technique to high order tensors. The algorithm is very sim-\nple and general, which can be easily applied to many tensor regression models. It also shares the efficiency of iterative hard thresholding method. We prove that the algorithm converges within a constant number of iterations and the achievable estimation error is linear to the size of the noise. We evaluate our method on multi-linear multi-task learning as well as spatio-temporal forecasting applications. The results show that the our method is significantly faster and is impressively robust to noise."}, {"heading": "10. Acknowledgment", "text": "This work is supported in part by the U. S. Army Research Office under grant number W911NF-15-1-0491, NSF Research Grant IIS- 1254206 and IIS-1134990. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government."}], "references": [{"title": "Multiway analysis of epilepsy", "author": ["Acar", "Evrim", "Aykut-Bingol", "Canan", "Bingol", "Haluk", "Bro", "Rasmus", "Yener", "B\u00fclent"], "venue": "tensors. Bioinformatics,", "citeRegEx": "Acar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2007}, {"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Achlioptas", "Dimitris"], "venue": "Journal of computer and System Sciences,", "citeRegEx": "Achlioptas and Dimitris.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas and Dimitris.", "year": 2003}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["Blumensath", "Thomas", "Davies", "Mike E"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath et al\\.", "year": 2009}, {"title": "Projected gradient methods for linearly constrained problems", "author": ["Calamai", "Paul H", "Mor\u00e9", "Jorge J"], "venue": "Mathematical programming,", "citeRegEx": "Calamai et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Calamai et al\\.", "year": 1987}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["Candes", "Emmanuel J", "Romberg", "Justin K", "Tao", "Terence"], "venue": "Communications on pure and applied mathematics,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation", "author": ["Cichocki", "Andrzej", "Zdunek", "Rafal", "Phan", "Anh Huy", "Amari", "Shun-ichi"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "author": ["Clarkson", "Kenneth L", "Woodruff", "David P"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Clarkson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2013}, {"title": "Statistics for spatiotemporal data", "author": ["Cressie", "Noel", "Wikle", "Christopher K"], "venue": null, "citeRegEx": "Cressie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cressie et al\\.", "year": 2015}, {"title": "On the best rank-1 and rank-(r 1, r 2,..., rn) approximation of higher-order tensors", "author": ["De Lathauwer", "Lieven", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "A multilinear singular value decomposition", "author": ["De Lathauwer", "Lieven", "De Moor", "Bart", "Vandewalle", "Joos"], "venue": "SIAM journal on Matrix Analysis and Applications,", "citeRegEx": "Lathauwer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2000}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Eckart", "Carl", "Young", "Gale"], "venue": null, "citeRegEx": "Eckart et al\\.,? \\Q1936\\E", "shortCiteRegEx": "Eckart et al\\.", "year": 1936}, {"title": "Tensor completion and low-n-rank tensor recovery via convex optimization", "author": ["Gandy", "Silvia", "Recht", "Benjamin", "Yamada", "Isao"], "venue": "Inverse Problems,", "citeRegEx": "Gandy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gandy et al\\.", "year": 2011}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Garg", "Rahul", "Khandekar", "Rohit"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Garg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Garg et al\\.", "year": 2009}, {"title": "Multi-pattern fingerprint method for detection and attribution of climate change", "author": ["Hasselmann", "Klaus"], "venue": "Climate Dynamics,", "citeRegEx": "Hasselmann and Klaus.,? \\Q1997\\E", "shortCiteRegEx": "Hasselmann and Klaus.", "year": 1997}, {"title": "Best low multilinear rank approximation of higher-order tensors, based on the riemannian trust-region scheme", "author": ["Ishteva", "Mariya", "Absil", "P-A", "Van Huffel", "Sabine", "De Lathauwer", "Lieven"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Ishteva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ishteva et al\\.", "year": 2011}, {"title": "Tucker compression and local optima", "author": ["Ishteva", "Mariya", "Absil", "P-A", "Van Huffel", "Sabine", "De Lathauwer", "Lieven"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Ishteva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ishteva et al\\.", "year": 2011}, {"title": "Guaranteed rank minimization via singular value projection", "author": ["Jain", "Prateek", "Meka", "Raghu", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Jain et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2010}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara G", "Bader", "Brett W"], "venue": "SIAM review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Tensor completion in hierarchical tensor representations", "author": ["Rauhut", "Holger", "Schneider", "Reinhold", "Stojanac", "\u017deljka"], "venue": "In Compressed Sensing and its Applications,", "citeRegEx": "Rauhut et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rauhut et al\\.", "year": 2015}, {"title": "Monotone operators and the proximal point algorithm", "author": ["Rockafellar", "R Tyrrell"], "venue": "SIAM journal on control and optimization,", "citeRegEx": "Rockafellar and Tyrrell.,? \\Q1976\\E", "shortCiteRegEx": "Rockafellar and Tyrrell.", "year": 1976}, {"title": "Multilinear multitask learning", "author": ["Romera-Paredes", "Bernardino", "Aung", "Hane", "Bianchi-Berthouze", "Nadia", "Pontil", "Massimiliano"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Romera.Paredes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Romera.Paredes et al\\.", "year": 2013}, {"title": "Learning with tensors: a framework based on convex optimization and spectral regularization", "author": ["Signoretto", "Marco", "Dinh", "Quoc Tran", "De Lathauwer", "Lieven", "Suykens", "Johan AK"], "venue": "Machine Learning,", "citeRegEx": "Signoretto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2014}, {"title": "Multilinear analysis of image ensembles: Tensorfaces", "author": ["Vasilescu", "M Alex O", "Terzopoulos", "Demetri"], "venue": "VisionECCV", "citeRegEx": "Vasilescu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Vasilescu et al\\.", "year": 2002}, {"title": "Multitask learning meets tensor factorization: task imputation via convex optimization", "author": ["Wimalawarne", "Kishan", "Sugiyama", "Masashi", "Tomioka", "Ryota"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "Fast multivariate spatio-temporal analysis via low rank tensor learning", "author": ["Yu", "Rose", "Bahadori", "Mohammad Taha", "Liu", "Yan"], "venue": null, "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Accelerated online low rank tensor learning for multivariate spatiotemporal streams", "author": ["Yu", "Rose", "Cheng", "Dehua", "Liu", "Yan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Multilinear subspace regression: An orthogonal tensor decomposition approach", "author": ["Zhao", "Qibin", "Caiafa", "Cesar F", "Mandic", "Danilo P", "Zhang", "Liqing", "Ball", "Tonio", "Schulze-Bonhage", "Andreas", "Cichocki", "Andrzej"], "venue": "In NIPS,", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["Zhou", "Hua", "Li", "Lexin", "Zhu", "Hongtu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Massive multiway data emerge from many fields: spacetime measurements on several variables in climate dynamics (Hasselmann, 1997), multichannel EEG signals in neurology (Acar et al., 2007) and natural images sequences in computer vision (Vasilescu & Terzopoulos, 2002).", "startOffset": 169, "endOffset": 188}, {"referenceID": 26, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 27, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 20, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 23, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 21, "context": "Many tensor regression methods have been proposed (Zhao et al., 2011; Zhou et al., 2013; Romera-Paredes et al., 2013; Wimalawarne et al., 2014; Signoretto et al., 2014), leading to a broad range of successful applications ranging from neural science to climate research to social network analysis.", "startOffset": 50, "endOffset": 168}, {"referenceID": 5, "context": "ALS displays unstable convergence properties and outputs sub-optimal solutions (Cichocki et al., 2009).", "startOffset": 79, "endOffset": 102}, {"referenceID": 11, "context": "Trace-norm minimization suffers from slow convergence (Gandy et al., 2011).", "startOffset": 54, "endOffset": 74}, {"referenceID": 27, "context": "For example, (Zhou et al., 2013) proposes to use Alternating least square (ALS) algorithm.", "startOffset": 13, "endOffset": 32}, {"referenceID": 20, "context": "(Romera-Paredes et al., 2013) employs ALS as well as an Alternating Direction Method of Multiplier (ADMM) technique to solve the nuclear norm regularized optimization problem.", "startOffset": 0, "endOffset": 29}, {"referenceID": 21, "context": "(Signoretto et al., 2014) proposes a more general version of ADMM based on Douglas-Rachford splitting method.", "startOffset": 0, "endOffset": 25}, {"referenceID": 24, "context": "To address the scalability issue of these methods, (Yu et al., 2014) proposes a greedy algorithm following the Orthogonal Matching Pursuit (OMP) scheme.", "startOffset": 51, "endOffset": 68}, {"referenceID": 16, "context": "Our work is closely related to iterative hard thresholding in compressive sensing (Blumensath & Davies, 2009), sparsified gradient descent in sparse recovery (Garg & Khandekar, 2009) or singular value projection method in lowrank matrix completion (Jain et al., 2010).", "startOffset": 248, "endOffset": 267}, {"referenceID": 20, "context": "For example, in multi-linear multitask learning (Romera-Paredes et al., 2013), given the predictor and response for each task (X,Y), the empirical loss is defined as the summarization of the loss for all the tasks, i.", "startOffset": 48, "endOffset": 77}, {"referenceID": 27, "context": "For the univariate GLM model in (Zhou et al., 2013), the model is defined as Y = vec(X ) vec(W)+E .", "startOffset": 32, "endOffset": 51}, {"referenceID": 11, "context": "A common approach is to use nuclear norm as a convex surrogate to approximate the rank constraint (Gandy et al., 2011).", "startOffset": 98, "endOffset": 118}, {"referenceID": 4, "context": "We prove that TPG guarantees optimality of the estimated solution, under the assumption that the predictor tensor satisfies Restricted Isometry Property (RIP) (Candes et al., 2006).", "startOffset": 159, "endOffset": 180}, {"referenceID": 18, "context": "Similar assumption can be found in (Rauhut et al., 2015).", "startOffset": 35, "endOffset": 56}, {"referenceID": 20, "context": "Multi-linear multi-task learning (Romera-Paredes et al., 2013; Wimalawarne et al., 2014) tackles the scenario where the tasks to be learned are references by multiple indices, thus contain multi-modal relationship.", "startOffset": 33, "endOffset": 88}, {"referenceID": 23, "context": "Multi-linear multi-task learning (Romera-Paredes et al., 2013; Wimalawarne et al., 2014) tackles the scenario where the tasks to be learned are references by multiple indices, thus contain multi-modal relationship.", "startOffset": 33, "endOffset": 88}, {"referenceID": 24, "context": "\u2022 Greedy (Yu et al., 2014): a fast tensor regression solution that sequentially estimates rank one subspace based on Orthogonal Matching Pursuit", "startOffset": 9, "endOffset": 26}, {"referenceID": 11, "context": "\u2022 ADMM (Gandy et al., 2011): alternating direction method of multipliers for nuclear norm regularized optimization", "startOffset": 7, "endOffset": 27}, {"referenceID": 20, "context": "Our evaluation follows the same experiment setting in (Romera-Paredes et al., 2013) on the restaurant & consumer dataset, provided by the authors in the paper.", "startOffset": 54, "endOffset": 83}, {"referenceID": 16, "context": "This modification does not affect the theoretical properties of the proposed algorithm, but would lead to significant empirical improvement (Jain et al., 2010).", "startOffset": 140, "endOffset": 159}], "year": 2016, "abstractText": "Tensor regression has shown to be advantageous in learning tasks with multi-directional relatedness. Given massive multiway data, traditional methods are often too slow to operate on or suffer from memory bottleneck. In this paper, we introduce subsampled tensor projected gradient to solve the problem. Our algorithm is impressively simple and efficient. It is built upon projected gradient method with fast tensor power iterations, leveraging randomized sketching for further acceleration. Theoretical analysis shows that our algorithm converges to the correct solution in fixed number of iterations. The memory requirement grows linearly with the size of the problem. We demonstrate superior empirical performance on both multi-linear multi-task learning and spatio-temporal applications.", "creator": "LaTeX with hyperref package"}}}