{"id": "1607.04423", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jul-2016", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "abstract": "cloze - style queries are representative problems in reading comprehension. over the past few months, we have seen much progress seeing that utilizing neural network approach to solve cloze - style questions. in this work, we present a novel model scenario for cloze - styled style reading comprehension tasks, called attention - over - matter attention reader. our model specifically aims to place rather another attention mechanism over the document - level attention, and induces \" attended attention \" for final read predictions. unlike the previous works, our neural tracking network model requires less pre - defined term hyper - formal parameters approaches and still uses an elegant architecture for modeling. experimental results show that the proposed attention - over - attention model nearly significantly outperforms various state - of - the - art systems by a uniformly large margin in public datasets, such as cnn and cnn children's book index test datasets.", "histories": [["v1", "Fri, 15 Jul 2016 09:10:11 GMT  (69kb,D)", "http://arxiv.org/abs/1607.04423v1", "8+1 pages. arXiv admin note: substantial text overlap witharXiv:1607.02250"], ["v2", "Mon, 18 Jul 2016 09:46:02 GMT  (68kb,D)", "http://arxiv.org/abs/1607.04423v2", "8+1 pages. some typos and descriptions fixed, closely related toarXiv:1607.02250"], ["v3", "Thu, 4 Aug 2016 06:17:42 GMT  (72kb,D)", "http://arxiv.org/abs/1607.04423v3", "8+1 pages. fixed errors in Fig.1 and Table.3, closely related toarXiv:1607.02250"], ["v4", "Tue, 6 Jun 2017 02:51:54 GMT  (251kb,D)", "http://arxiv.org/abs/1607.04423v4", "8+2 pages. accepted as a conference paper at ACL2017 (long paper)"]], "COMMENTS": "8+1 pages. arXiv admin note: substantial text overlap witharXiv:1607.02250", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["yiming cui", "zhipeng chen", "si wei", "shijin wang", "ting liu", "guoping hu"], "accepted": true, "id": "1607.04423"}, "pdf": {"name": "1607.04423.pdf", "metadata": {"source": "CRF", "title": "Attention-over-Attention Neural Networks for Reading Comprehension", "authors": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "emails": ["ymcui@iflytek.com", "zpchen@iflytek.com", "siwei@iflytek.com", "sjwang3@iflytek.com", "gphu@iflytek.com", "tliu@ir.hit.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "To read and comprehend the human languages are challenging tasks for the machines, which requires that the understanding of natural languages and the ability to do reasoning over various clues. Reading comprehension is a general problem in the real world, which aims to read and comprehend a given article or context, and answer the questions based on it. Among various types of reading comprehension problems, the Cloze-style queries are the fundamental ones and has become a starter in tackling machine comprehensions. Similar to the general reading comprehension problems, the Cloze-style queries (Taylor, 1953) are raised based on the nature of the document, while the answer is a single word inside of the document.\nIn order to teach the machine to do Cloze-style reading comprehensions, large-scale training data is necessary for learning relationships between the given document and query. By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary. Hill et al. (2015) released the Children\u2019s Book Test (CBT) dataset afterwards, where the training samples are generated through automatic approaches. Also, Cui et al. (2016) has released the Chinese reading comprehension datasets for future research. As we can see that, automatically generating large-scale training data for neural network training is essential for reading comprehension. Furthermore, more complicated problems, such as reasoning or summarization of context, need much more data to learn the higher-level interactions.\nIn this paper, we present a novel and elegant neural network architecture, called attention-overattention model. As we can understand the meaning literally, our model aims to place another attention mechanism on the existing document-level attention. Unlike the previous works, that are using heuristic merging functions (Cui et al., 2016), or setting various hyper-parameters (Trischler et al., 2016), our model could automatically generate an \u201cattended attention\u201d over the various document-level attentions, and make a mutual look not only from query-to-document but also document-to-query, which will benefit from the interactive information.\nThis work was done by the Joint Laboratory of HIT and iFLYTEK (HFL).\nar X\niv :1\n60 7.\n04 42\n3v 1\n[ cs\n.C L\n] 1\n5 Ju\nl 2 01\n6\nTo sum up, the main contributions of our work are listed as follows.\n\u2022 To our knowledge, this is the first time that the attention-over-attention mechanism is introduced.\n\u2022 Unlike the previous works on introducing complex architectures or many non-trainable hyperparameters to the model, our model is simple and without any burdens on hyper-parameter tuning, but outperforms various state-of-the-art systems by a large margin on the public datasets.\n\u2022 As our model is generalized, we believe the attention-over-attention mechanism can be used to other tasks as well.\nThe rest of our paper will be organized as follows. In Section 2, we will give an introduction to the Cloze-style reading comprehension task, and then talk about the related public datasets. Then the proposed attention-over-attention Reader will be presented in detail in Section 3. The experimental results will be given in Section 4. Related work will be discussed in Section 5, and we make a brief conclusion of our work at the end of this paper."}, {"heading": "2 Cloze-style Reading Comprehension", "text": "In this section, we will give a brief introduction to the Cloze-style reading comprehension task at the beginning. And then, several existing public datasets will be described in detail."}, {"heading": "2.1 Task Description", "text": "Among various reading comprehension problems, the Cloze-style reading comprehension is a primary and representative type of question. The Cloze-style reading comprehension problem (Taylor, 1953) aims to comprehend the given context or document, and then answer the questions based on the nature of the document, while the answer is a single word in the document. Formally, a general Cloze-style reading comprehension problem can be illustrated as a triple:\n\u3008D,Q,A\u3009\nwhere D is the document, Q is the query and A is the answer to the query."}, {"heading": "2.2 Existing Public Datasets", "text": "Large-scale training data is essential for training neural networks. Recently, several public datasets for the Cloze-style reading comprehension are readily available. CNN/Daily Mail.1 Hermann et al. (2015) have firstly published two datasets: CNN and Daily Mail news data. They construct these datasets with web-crawled CNN and Daily Mail news data. One of the characteristics of these datasets is that the news article is often associated with a summary. So they first regard the main body of the news article as the Document, and the Query is formed by the summary of the article, where one entity word is replaced by a special placeholder to indicate the missing word. And finally, the replaced entity word will be the Answer of the Query. Also, they have proposed to anonymize the named entity tokens in the data, and re-shuffle the entity tokens for every sample to exploit general relationships between anonymized named entities, rather than the common knowledge. But as Chen et al. (2016)\u2019s studies on these datasets showed that the anonymization of the entity word is less useful than expected. Here is an example of CNN news dataset shown in the below.\nDocument\n@entity2 celebrates his late strike as @entity5 retained their unbeaten @entity4 record with a 1 - 1 draw against @entity9 . @entity9 captain @entity14 had given the visitors a deserved first - half leadbut @entity2 \u2019s strike two minutes from time maintained @entity5 \u2019s nine - point lead at the top . @entity9 needed to win to breathe new life into the title race but they were\n1The pre-processed CNN and Daily Mail datasets are available at http://cs.nyu.edu/\u02dckcho/DMQA/\ndealt a cruel blow as @entity25 defender @entity24 was sent - off late on for two bookings in quick succession.\n. . . . . .\n@entity92 are up to fourth after they defeated @entity93 1 - 0 at home thanks to a goal from @entity96 forward @entity97 . e-mail to a friend\nQuery\nthe result keeps @placeholder nine points clear and retains their unbeaten league run\nAnswer\n@entity5\nChildren\u2019s Book Test. 2 There was also a dataset called the Children\u2019s Book Test (CBTest) released by Hill et al. (2015), which is built on the children\u2019s book story through Project Gutenberg. Different from the previously published CNN/Daily Mail datasets, they formed the Document with 20 consecutive sentences in the book and regarded the 21st sentence as the Query, where one word is blanked with a special placeholder. Based on the type of missing word\u2019s part-of-speech, the datasets are divided into four types: Named Entities (NE), Common Nouns (CN), Verbs and Prepositions. As the verbs and prepositions are less dependent with the document and fairly easy to answer than the others, most of the studies are focusing on the NE and CN datasets."}, {"heading": "3 Attention-over-Attention Reader", "text": "In this section, we will introduce our attention-based neural network model for Cloze-style reading comprehension task, namely Attention-over-Attention Reader (AoA Reader). Our model is primarily motivated by Kadlec et al., (2016), which aims to directly estimate the answer from the document-level attention instead of calculating blended representations of the document. Some researchers also suggested that by just concatenating the final representations of the query RNN states is not enough for representing the full information of query (Cui et al., 2016). In this paper, we propose a novel work that placing another attention over the primary attention, to indicate the \u201cimportance\u201d of each attentions.\nNow, we will give a formal description of our proposed model. Given a set of training triple \u3008D,Q,A\u3009, the proposed model will be constructed in the following steps. Contextual Embedding. We first transform every word in the document D and query Q into one-hot representations, and then convert them into continuous representations with a shared embedding matrix We. As the query is typically shorter than the document, by sharing the embedding weights between the document and query, the representations of query can be benefited from the embedding learning in the document side, which is better than separating embedding matrices individually. After that, we use two bi-directional RNNs to get contextual representations of the document and query individually, where representation of each word is formed by concatenating the forward and backward hidden states. In our implementation, we utilized the bi-directional Gated Recurrent Unit (GRU) (Cho et al., 2014).\ne(x) =We \u2217 x, where x \u2208 D,Q (1) \u2212\u2212\u2212\u2192 hs(x) = \u2212\u2212\u2212\u2192 GRU(e(x)) ; \u2190\u2212\u2212\u2212 hs(x) = \u2190\u2212\u2212\u2212 GRU(e(x)) (2)\nhs(x) = [ \u2212\u2212\u2212\u2192 hs(x); \u2190\u2212\u2212\u2212 hs(x)] (3)\nWe take hdoc \u2208 R|D|\u2217d and hquery \u2208 R|Q|\u2217d to represent the contextual representations of document and query, where d is the dimension of GRUs. Pair-wise Matching Score. After obtaining the contextual embeddings of the document hdoc and query hquery, we calculate a pair-wise matching matrix, which indicate the pair-wise matching degree of one document word and one query word. Formally, when given ith word of document and jth word of query, we can compute a matching score their dot product.\n2The CBTest datasets are available at http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\nM(i, j) = hdoc(i) hquery(j) (4)\nIn this way, we can calculate every pair-wise matching score between the document and query, forming a matrix M \u2208 R|D|\u2217|Q|, where the value of ith row and jth column is filled by M(i, j). Individual Attentions. After getting the pair-wise matching matrixM , we apply a column-wise softmax function to get probability distributions in each column, where each column is an individual documentlevel attention when considering a single query word. We denote \u03b1(t) \u2208 R|D| as the document-level attention regarding query word at time t, which can be seen as a query-to-document attention.\n\u03b1(t) = softmax(M(1, t), ...,M(|D|, t)) (5)\n\u03b1 = [\u03b1(1), \u03b1(2), ..., \u03b1(|Q|)] (6)\nAttention-over-Attention. Different from Cui et al. (2016), we use a more \u201cwise\u201d way to combine these document-level attentions into a final attention, while the previous work used naive heuristics, such as summing or averaging over individual attention \u03b1(t).\nFirst, we calculate a reversed attention, that is, for every document word at time t, we calculate the \u201cimportance\u201d distribution on the query, to indicate which query words are more important given a single document word. We apply a row-wise softmax function to the pair-wise matching matrixM to get querylevel attentions. We denote \u03b2(t) \u2208 R|Q| as the query-level attention regarding document word at time t, which can be seen as a document-to-query attention.\n\u03b2(t) = softmax(M(t, 1), ...,M(t, |Q|)) (7)\nSo far, we have obtained both query-to-document attention \u03b1 and document-to-query attention \u03b2. Our motivation is to exploit mutual informations between the document and query. However, most of the previous works are only relying on query-to-document attention, that is, only calculate a document-level attention when considering the query.\nThen we average all the \u03b2(t) to get an averaged query-level attention \u03b2. Note that, we do not apply another softmax to the \u03b2, because averaging individual attentions do not break the normalizing condition.\n\u03b2 = 1\nn |D|\u2211 t=1 \u03b2(t) (8)\nFinally, we calculate dot product of \u03b1 and \u03b2 to get the \u201cattended document-level attention\u201d s \u2208 R|D|. Intuitively, this operation is calculating a weighted sum of each individual document-level attention \u03b1(t) when looking at query word at time t. In this way, the contributions by each query word can be learned explicitly, and the final decision (document-level attention) is made through the voted result by the importance of each query word.\ns = \u03b1 \u03b2 (9) Final Predictions. Following Kadlec et al. (2016), we map the attention results s to the vocabulary space V , and sum the attention value where the candidate word occurs in different places of the document.\nP (w|D,Q) = \u2211\ni\u2208I(w,D)\nsi, w \u2208 V (10)\nwhere I(w,D) indicate the positions that word w appear in the document D. The proposed neural network architecture is depicted in Figure 1. Note that, as our model mainly adds limited steps of calculations to the AS Reader (Kadlec et al., 2016) and do not employ any additional weights, the computational complexity is similar to the AS Reader."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Experimental Setups", "text": "The general settings of our neural network model are detailed below.\nDimensions of embedding and hidden layer for each task are listed in Table 1. We trained each model for several epochs and choose the best one according to the performance of validation set. All models are trained on Tesla K40 GPU. Implementation is done with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015)."}, {"heading": "4.2 Results", "text": "To verify the effectiveness of our proposed model, we compared our model with other state-of-the-art systems on public datasets. Our evaluation is carried out on CNN news datasets (Hermann et al., 2015) and CBTest NE/CN datasets (Hill et al., 2015). The statistics of these datasets are listed in Table 2. Note that, no special pre-processing is done with these datasets. We evaluate the model in terms of its accuracy. Due to the time limitations, we are not able to evaluate our model in the ensemble, and only tested single model performance. We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper. The experimental results are given in Table 3.\nCNN News. The results on CNN news datasets shows that our AoA Reader gives competitive results among various state-of-the-art baselines, including those cutting-edge systems. When compared to the previous best result (EpiReader), our model gives similar results, where 0.4% improvements in test set and gives slight drop in validation set by 0.3%. Except for EpiReader, our model shows an absolute advantage over all other models by a significant margin.\nTo investigate the effectiveness of employing attention-over-attention mechanism, we also compared our model to CAS Reader, where the latter used pre-defined heuristics. From the result, we can see that getting rid of those heuristics and letting the model to explicitly learn the weights between individual attentions could give a significant boost, where 4.1% and 3.7% improvements can be made in validation and test set.\nAlso, the Stanford AR (Chen et al., 2016) and GA Reader (Dhingra et al., 2016) utilized pre-trained word embeddings for initialization, while our model does not adopt any pre-trained model for initialization. Furthermore, we do not optimize for a certain type of dataset, unlike the Stanford AR only normalized the probabilities over the named entities, rather than all the words in the document, which demonstrate that our model is more general and powerful than previous works. We have also noticed that it is fairly hard for a single model to reach above 75%, as indicated in Chen et al. (2016)\u2019s study showed that the coreference errors (roughly takes up 25%) make the questions \u201cunanswerable\u201d even for the humans. CBTest NE/CN. In CBTest NE dataset, our AoA Reader outperforms all the state-of-the-art systems by a large margin, where a 2.5% and 2.3% absolute accuracy improvements over the most recent state-ofthe-art system EpiReader in the validation and test set respectively. We have also noticed that, though we haven\u2019t tried our model in ensemble, our single model could stay on par with the previous best system in the ensemble, and even we have an absolute improvement of 0.9% beyond the best ensemble model (Iterative Attention) in the validation set. This demonstrates that our model is powerful enough to compete with the ensemble models of previous works, and introducing ensembles in our model may have another boost in the performance, though we haven\u2019t tried in this paper.\nIn CBTest CN dataset, our model gives modest improvements over the state-of-the-art systems. When compared with Iterative Attention model, our model shows a similar result, with slight improvements on validation and test set. But when compared to EpiReader, our model could give a significant improvement with 0.3% and 1.7% gains respectively, and even larger gains over GA Reader and all of the previous works, which also demonstrate the effectiveness of our model."}, {"heading": "5 Related Work", "text": "Cloze-style reading comprehension tasks have been widely investigated. To take a look at these models, we can find that almost all of them are attention-based neural networks, which indicate that the attentionbased model is powerful and essential in machine comprehensions. We will take a brief revisit to the previous works.\nHermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples. By using their method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network to study the relationships inside of these triples. They proposed to use an attention-based neural network for tackling this task. Evaluation on CNN/DailyMail datasets showed that their approach is effective than traditional baselines.\nHill et al. (2015) also proposed a similar approach for large-scale training data collections for children\u2019s book reading comprehension task. They proposed to use window-based memory network and self-supervision heuristics have been investigated. The results showed that their model had surpassed all other methods in predicting named entities(NE) and common nouns(CN) on both the CBT and the CNN QA benchmark.\nKadlec et al. (2016) proposed to use a simple model that using the document-level attention result to directly pick the answer from the document, rather than computing the weighted sum representation of the document like the previous works did. The proposed model is typically motivated by Pointer Network (Vinyals et al., 2015). This model aims to solve one particular task, where the answer is only a single\nword and should appear in the document at least once. Experimental results showed that their model outperformed previously proposed models by a large margin in public datasets (both CBTest NE/CN and CNN/DailyMail datasets).\nLiu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind their approach is to automatically generate largescale pseudo training data and then using the neural network model to resolve zero pronouns. They also propose a two-step training: a pre-training phase and an adaptation phase, and this can also be applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus is encouraging, and the proposed approach significantly outperforms the state-of-the-art methods.\nWe also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al., 2016), during our writing of this article. Sordoni et al. (2016) have proposed an iterative alternating attention mechanism and gating strategies to accumulatively optimize the attention after several hops, where the number of hops is defined heuristically. Trischler et al. (2016) adopted a re-ranking strategy into the neural networks and used a joint-training method to optimize the neural network. The final prediction is determined by both the Extractor (Attention Sum Reader) and the Re-ranker (Reasoner). The works mentioned above, both outperformed the state-of-the-art systems.\nOur work is primarily inspired by Cui et al. (2016) and Kadlec et al. (2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016). Unlike the CAS Reader proposed by Cui et al. (2016), we do not assume any heuristics to our model, such as using merge functions: sum, avg etc. We used a mechanism called \u201cattention-over-attention\u201d to explicitly calculate the weights between different individual document-level attentions, and get the final attention by computing the weighted sum of them. Also, we find that our model is typically general and simple than the recently proposed model, and our model brings significant improvements over these cutting edge systems."}, {"heading": "6 Conclusion", "text": "In this paper, we introduce a novel attention-based neural network to tackle the Cloze-style reading comprehension problems. The proposed Attention-over-Attention model aims to compute the attentions not only for the document but also the query side, which will benefit from the mutual information. Then a weighted sum of attention is carried out to get an attended attention over the document for the final predictions. Among several public datasets, our model could give consistent and significant improvements over various state-of-the-art systems by a large margin. A highlight in our model is that we propose to use the attention-over-attention mechanism to get \u201cattended attention\u201d. Besides this, our model is elegant and easy to carry out but shows promising results on this task.\nThe future work will be carried out in the following aspects. We believe that our model is general and may apply to other tasks as well, so firstly we are going to fully investigate the usage of this architecture in other tasks. Second, we are planning to investigate hybrid reading comprehension models to tackle the problems that rely on comprehensive reasoning on several sentences."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724\u20131734. Association for Computational Linguistics.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github.com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Consensus attention-based neural networks for chinese reading comprehension", "author": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu."], "venue": "arXiv preprint arXiv:1607.02250.", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Generating and exploiting large-scale pseudo training data for zero pronoun resolution", "author": ["Ting Liu", "Yiming Cui", "Qingyu Yin", "Shijin Wang", "Weinan Zhang", "Guoping Hu."], "venue": "arXiv preprint arXiv:1606.01603.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML (3), 28:1310\u20131318.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1606.02245.", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u2013 1958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor."], "venue": "Journalism and Mass Communication Quarterly, 30(4):415.", "citeRegEx": "Taylor.,? 1953", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688, May.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Natural language comprehension with the epireader", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1606.02270.", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems, pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Similar to the general reading comprehension problems, the Cloze-style queries (Taylor, 1953) are raised based on the nature of the document, while the answer is a single word inside of the document.", "startOffset": 79, "endOffset": 93}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data.", "startOffset": 54, "endOffset": 77}, {"referenceID": 4, "context": "Unlike the previous works, that are using heuristic merging functions (Cui et al., 2016), or setting various hyper-parameters (Trischler et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 17, "context": ", 2016), or setting various hyper-parameters (Trischler et al., 2016), our model could automatically generate an \u201cattended attention\u201d over the various document-level attentions, and make a mutual look not only from query-to-document but also document-to-query, which will benefit from the interactive information.", "startOffset": 45, "endOffset": 69}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary.", "startOffset": 55, "endOffset": 205}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary. Hill et al. (2015) released the Children\u2019s Book Test (CBT) dataset afterwards, where the training samples are generated through automatic approaches.", "startOffset": 55, "endOffset": 371}, {"referenceID": 0, "context": "By adopting attention-based neural network approaches (Bahdanau et al., 2014), the machine can learn these patterns in large-scale training data. To create large-scale training data, Hermann et al. (2015) published the CNN/Daily Mail news corpus for Cloze-style reading comprehensions, where the content is formed by the news articles and its summary. Hill et al. (2015) released the Children\u2019s Book Test (CBT) dataset afterwards, where the training samples are generated through automatic approaches. Also, Cui et al. (2016) has released the Chinese reading comprehension datasets for future research.", "startOffset": 55, "endOffset": 526}, {"referenceID": 15, "context": "The Cloze-style reading comprehension problem (Taylor, 1953) aims to comprehend the given context or document, and then answer the questions based on the nature of the document, while the answer is a single word in the document.", "startOffset": 46, "endOffset": 60}, {"referenceID": 5, "context": "1 Hermann et al. (2015) have firstly published two datasets: CNN and Daily Mail news data.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": "But as Chen et al. (2016)\u2019s studies on these datasets showed that the anonymization of the entity word is less useful than expected.", "startOffset": 7, "endOffset": 26}, {"referenceID": 7, "context": "2 There was also a dataset called the Children\u2019s Book Test (CBTest) released by Hill et al. (2015), which is built on the children\u2019s book story through Project Gutenberg.", "startOffset": 80, "endOffset": 99}, {"referenceID": 4, "context": "Some researchers also suggested that by just concatenating the final representations of the query RNN states is not enough for representing the full information of query (Cui et al., 2016).", "startOffset": 170, "endOffset": 188}, {"referenceID": 2, "context": "In our implementation, we utilized the bi-directional Gated Recurrent Unit (GRU) (Cho et al., 2014).", "startOffset": 81, "endOffset": 99}, {"referenceID": 6, "context": "Our model is primarily motivated by Kadlec et al., (2016), which aims to directly estimate the answer from the document-level attention instead of calculating blended representations of the document.", "startOffset": 36, "endOffset": 58}, {"referenceID": 4, "context": "Different from Cui et al. (2016), we use a more \u201cwise\u201d way to combine these document-level attentions into a final attention, while the previous work used naive heuristics, such as summing or averaging over individual attention \u03b1(t).", "startOffset": 15, "endOffset": 33}, {"referenceID": 8, "context": "Following Kadlec et al. (2016), we map the attention results s to the vocabulary space V , and sum the attention value where the candidate word occurs in different places of the document.", "startOffset": 10, "endOffset": 31}, {"referenceID": 8, "context": "Note that, as our model mainly adds limited steps of calculations to the AS Reader (Kadlec et al., 2016) and do not employ any additional weights, the computational complexity is similar to the AS Reader.", "startOffset": 83, "endOffset": 104}, {"referenceID": 14, "context": "1 (Srivastava et al., 2014).", "startOffset": 2, "endOffset": 27}, {"referenceID": 16, "context": "While we were implementing the AS Reader, we observed that it is easy to overfit the training data within two epochs, where a similar conclusion was also made in Trischler et al. (2016). For regularization purpose and handling overfitting problems, we adopted l2-regularization to 0.", "startOffset": 162, "endOffset": 186}, {"referenceID": 12, "context": "\u2022 Hidden Layer: The GRU units are initialized with random orthogonal matrices (Saxe et al., 2013).", "startOffset": 78, "endOffset": 97}, {"referenceID": 11, "context": "To prevent the gradient exploding problem, we set gradient clipping threshold to 5 in our experiments (Pascanu et al., 2013) .", "startOffset": 102, "endOffset": 124}, {"referenceID": 9, "context": "\u2022 Optimization: We used the ADAM update rule (Kingma and Ba, 2014) with an initial learning rate lr = 0.", "startOffset": 45, "endOffset": 66}, {"referenceID": 3, "context": "Implementation is done with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015).", "startOffset": 77, "endOffset": 92}, {"referenceID": 6, "context": "Table 2: Statistics of public Cloze-style reading comprehension datasets: CNN news data (Hermann et al., 2015) and CBTest NE(Named Entites) / CN(Common Nouns) (Hill et al.", "startOffset": 88, "endOffset": 110}, {"referenceID": 7, "context": ", 2015) and CBTest NE(Named Entites) / CN(Common Nouns) (Hill et al., 2015).", "startOffset": 56, "endOffset": 75}, {"referenceID": 6, "context": "Our evaluation is carried out on CNN news datasets (Hermann et al., 2015) and CBTest NE/CN datasets (Hill et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 7, "context": ", 2015) and CBTest NE/CN datasets (Hill et al., 2015).", "startOffset": 34, "endOffset": 53}, {"referenceID": 5, "context": "We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper.", "startOffset": 47, "endOffset": 115}, {"referenceID": 13, "context": "We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper.", "startOffset": 47, "endOffset": 115}, {"referenceID": 17, "context": "We also added pretty new works to compare with (Dhingra et al., 2016; Sordoni et al., 2016; Trischler et al., 2016), which came to our attention when we are writing this paper.", "startOffset": 47, "endOffset": 115}, {"referenceID": 6, "context": "Results marked with 1 are taken from (Hermann et al., 2015), and 2 are taken from (Hill et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 7, "context": ", 2015), and 2 are taken from (Hill et al., 2015), and 3 are taken from (Kadlec et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 8, "context": ", 2015), and 3 are taken from (Kadlec et al., 2016), and 4 are taken from (Cui et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 4, "context": ", 2016), and 4 are taken from (Cui et al., 2016), and 5 are taken from (Chen et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 1, "context": ", 2016), and 5 are taken from (Chen et al., 2016), and 6 are taken from (Dhingra et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": ", 2016), and 6 are taken from (Dhingra et al., 2016), and 7 are taken from (Trischler et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 17, "context": ", 2016), and 7 are taken from (Trischler et al., 2016), and 8 are taken from (Sordoni et al.", "startOffset": 30, "endOffset": 54}, {"referenceID": 13, "context": ", 2016), and 8 are taken from (Sordoni et al., 2016).", "startOffset": 30, "endOffset": 52}, {"referenceID": 1, "context": "Also, the Stanford AR (Chen et al., 2016) and GA Reader (Dhingra et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 5, "context": ", 2016) and GA Reader (Dhingra et al., 2016) utilized pre-trained word embeddings for initialization, while our model does not adopt any pre-trained model for initialization.", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "Also, the Stanford AR (Chen et al., 2016) and GA Reader (Dhingra et al., 2016) utilized pre-trained word embeddings for initialization, while our model does not adopt any pre-trained model for initialization. Furthermore, we do not optimize for a certain type of dataset, unlike the Stanford AR only normalized the probabilities over the named entities, rather than all the words in the document, which demonstrate that our model is more general and powerful than previous works. We have also noticed that it is fairly hard for a single model to reach above 75%, as indicated in Chen et al. (2016)\u2019s study showed that the coreference errors (roughly takes up 25%) make the questions \u201cunanswerable\u201d even for the humans.", "startOffset": 23, "endOffset": 598}, {"referenceID": 18, "context": "The proposed model is typically motivated by Pointer Network (Vinyals et al., 2015).", "startOffset": 61, "endOffset": 83}, {"referenceID": 6, "context": "Hermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples.", "startOffset": 0, "endOffset": 22}, {"referenceID": 6, "context": "Hermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples. By using their method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network to study the relationships inside of these triples. They proposed to use an attention-based neural network for tackling this task. Evaluation on CNN/DailyMail datasets showed that their approach is effective than traditional baselines. Hill et al. (2015) also proposed a similar approach for large-scale training data collections for children\u2019s book reading comprehension task.", "startOffset": 0, "endOffset": 516}, {"referenceID": 6, "context": "Hermann et al. (2015) have proposed a methodology for obtaining large quantities of \u3008D,Q,A\u3009 triples. By using their method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network to study the relationships inside of these triples. They proposed to use an attention-based neural network for tackling this task. Evaluation on CNN/DailyMail datasets showed that their approach is effective than traditional baselines. Hill et al. (2015) also proposed a similar approach for large-scale training data collections for children\u2019s book reading comprehension task. They proposed to use window-based memory network and self-supervision heuristics have been investigated. The results showed that their model had surpassed all other methods in predicting named entities(NE) and common nouns(CN) on both the CBT and the CNN QA benchmark. Kadlec et al. (2016) proposed to use a simple model that using the document-level attention result to directly pick the answer from the document, rather than computing the weighted sum representation of the document like the previous works did.", "startOffset": 0, "endOffset": 929}, {"referenceID": 13, "context": "We also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al.", "startOffset": 43, "endOffset": 65}, {"referenceID": 17, "context": ", 2016) and (Trischler et al., 2016), during our writing of this article.", "startOffset": 12, "endOffset": 36}, {"referenceID": 13, "context": "(2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 138}, {"referenceID": 17, "context": "(2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 138}, {"referenceID": 4, "context": "(2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016).", "startOffset": 74, "endOffset": 138}, {"referenceID": 8, "context": "Liu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "Liu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind their approach is to automatically generate largescale pseudo training data and then using the neural network model to resolve zero pronouns. They also propose a two-step training: a pre-training phase and an adaptation phase, and this can also be applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus is encouraging, and the proposed approach significantly outperforms the state-of-the-art methods. We also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al., 2016), during our writing of this article. Sordoni et al. (2016) have proposed an iterative alternating attention mechanism and gating strategies to accumulatively optimize the attention after several hops, where the number of hops is defined heuristically.", "startOffset": 0, "endOffset": 737}, {"referenceID": 8, "context": "Liu et al. (2016) proposed an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind their approach is to automatically generate largescale pseudo training data and then using the neural network model to resolve zero pronouns. They also propose a two-step training: a pre-training phase and an adaptation phase, and this can also be applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus is encouraging, and the proposed approach significantly outperforms the state-of-the-art methods. We also have noticed two very recent works (Sordoni et al., 2016) and (Trischler et al., 2016), during our writing of this article. Sordoni et al. (2016) have proposed an iterative alternating attention mechanism and gating strategies to accumulatively optimize the attention after several hops, where the number of hops is defined heuristically. Trischler et al. (2016) adopted a re-ranking strategy into the neural networks and used a joint-training method to optimize the neural network.", "startOffset": 0, "endOffset": 954}, {"referenceID": 4, "context": "Our work is primarily inspired by Cui et al. (2016) and Kadlec et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 4, "context": "Our work is primarily inspired by Cui et al. (2016) and Kadlec et al. (2016) , where the latter model is widely applied to many follow-up works (Sordoni et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 4, "context": "Our work is primarily inspired by Cui et al. (2016) and Kadlec et al. (2016) , where the latter model is widely applied to many follow-up works (Sordoni et al., 2016; Trischler et al., 2016; Cui et al., 2016). Unlike the CAS Reader proposed by Cui et al. (2016), we do not assume any heuristics to our model, such as using merge functions: sum, avg etc.", "startOffset": 34, "endOffset": 262}], "year": 2017, "abstractText": "Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader. Our model aims to place another attention mechanism over the document-level attention, and induces \u201cattended attention\u201d for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attentionover-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\u2019s Book Test datasets.", "creator": "TeX"}}}