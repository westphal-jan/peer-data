{"id": "1607.07057", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2016", "title": "Latent Tree Language Model", "abstract": "it this related paper proposes we introduce latent tree language model ( ltlm ), a novel approach to language modeling that encodes syntax techniques and semantics of a given sentence as a tree of word roles.", "histories": [["v1", "Sun, 24 Jul 2016 15:40:36 GMT  (178kb,D)", "https://arxiv.org/abs/1607.07057v1", "Submitted to EMNLP 2016"], ["v2", "Mon, 29 Aug 2016 12:35:24 GMT  (178kb,D)", "http://arxiv.org/abs/1607.07057v2", "Accepted to EMNLP 2016"], ["v3", "Mon, 5 Sep 2016 14:47:18 GMT  (172kb,D)", "http://arxiv.org/abs/1607.07057v3", "Accepted to EMNLP 2016"]], "COMMENTS": "Submitted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tomas brychcin"], "accepted": true, "id": "1607.07057"}, "pdf": {"name": "1607.07057.pdf", "metadata": {"source": "CRF", "title": "Latent Tree Language Model", "authors": ["Tom\u00e1\u0161 Brychc\u0131\u0301n"], "emails": ["brychcin@kiv.zcu.cz"], "sections": [{"heading": null, "text": "The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms.\nWe combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model."}, {"heading": "1 Introduction", "text": "Language modeling is one of the core disciplines in natural language processing (NLP). Automatic speech recognition, machine translation, optical character recognition, and other tasks strongly depend on the language model (LM). An improvement in language modeling often leads to better performance of the whole task. The goal of language modeling is to determine the joint probability of a sentence. Currently, the dominant approach is n-gram language modeling, which decomposes\nthe joint probability into the product of conditional probabilities by using the chain rule. In traditional n-gram LMs the words are represented as distinct symbols. This leads to an enormous number of word combinations.\nIn the last years many researchers have tried to capture words contextual meaning and incorporate it into the LMs. Word sequences that have never been seen before receive high probability when they are made of words that are semantically similar to words forming sentences seen in training data. This ability can increase the LM performance because it reduces the data sparsity problem. In NLP a very common paradigm for word meaning representation is the use of the Distributional hypothesis. It suggests that two words are expected to be semantically similar if they occur in similar contexts (they are similarly distributed in the text) (Harris, 1954). Models based on this assumption are denoted as distributional semantic models (DSMs).\nRecently, semantically motivated LMs have begun to surpass the ordinary n-gram LMs. The most commonly used architectures are neural network LMs (Bengio et al., 2003; Mikolov et al., 2010; Mikolov et al., 2011) and class-based LMs. Classbased LMs are more related to this work thus we investigate them deeper.\nBrown et al. (1992) introduced class-based LMs of English. Their unsupervised algorithm searches classes consisting of words that are most probable in the given context (one word window in both directions). However, the computational complexity of this algorithm is very high. This approach was later extended by (Martin et al., 1998; Whit-\nar X\niv :1\n60 7.\n07 05\n7v 3\n[ cs\n.C L\n] 5\nS ep\n2 01\ntaker and Woodland, 2003) to improve the complexity and to work with wider context. Deschacht et al. (2012) used the same idea and introduced Latent Words Language Model (LWLM), where word classes are latent variables in a graphical model. They apply Gibbs sampling or the expectation maximization algorithm to discover the word classes that are most probable in the context of surrounding word classes. A similar approach was presented in (Brychc\u0131\u0301n and Konop\u0131\u0301k, 2014; Brychc\u0131\u0301n and Konop\u0131\u0301k, 2015), where the word clusters derived from various semantic spaces were used to improve LMs.\nIn above mentioned approaches, the meaning of a word is inferred from the surrounding words independently of their relation. An alternative approach is to derive contexts based on the syntactic relations the word participates in. Such syntactic contexts are automatically produced by dependency parse-trees. Resulting word representations are usually less topical and exhibit more functional similarity (they are more syntactically oriented) as shown in (Pado\u0301 and Lapata, 2007; Levy and Goldberg, 2014).\nDependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years (Ku\u0308bler et al., 2009). Popel and Marec\u030cek (2010) showed that these methods are promising direction of improving LMs. Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Marec\u030cek and Straka, 2013) offering new possibilities even for poorly-resourced languages.\nIn this work we introduce a new DSM that uses tree-based context to create word roles. The word role contains the words that are similarly distributed over similar tree-based contexts. The word role encodes the semantic and syntactic properties of a word. We do not rely on parse trees as a prior knowledge, but we jointly learn the tree structures and word roles. Our model is a soft clustering, i.e. one word may be present in several roles. Thus it is theoretically able to capture the word polysemy. The learned structure is used as a LM, where each word role is conditioned on its parent role. We present the unsupervised algorithm that discovers the tree structures only from the distribution of words in a training corpus (i.e. no labeled data or external sources of in-\nformation are needed). In our work we were inspired by class-based LMs (Deschacht et al., 2012), unsupervised dependency parsing (Marec\u030cek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).\nThis paper is organized as follows. We start with the definition of our model (Section 2). The process of learning the hidden sentence structures is explained in Section 3. We introduce two algorithms for searching the most probable tree for a given sentence (Section 4). The experimental results on English and Czech corpora are presented in Section 6. We conclude in Section 7 and offer some directions for future work."}, {"heading": "2 Latent Tree Language Model", "text": "In this section we describe Latent Tree Language Model (LTLM). LTLM is a generative statistical model that discovers the tree structures hidden in the text corpus.\nLet L be a word vocabulary with total of |L| distinct words. Assume we have a training corpus w divided into S sentences. The goal of LTLM or other LMs is to estimate the probability of a text P (w). Let Ns denote the number of words in the s-th sentence. The s-th sentence is a sequence of words ws = {ws,i}Nsi=0, where ws,i \u2208 L is a word at position i in this sentence and ws,0 = < s > is an artificial symbol that is added at the beginning of each sentence.\nEach sentence s is associated with the dependency graph Gs. We define the dependency graph as a labeled directed graph, where nodes correspond to the words in the sentence and there is a label for each node that we call role. Formally, it is a triple Gs = (V s,Es, rs) consisting of:\n\u2022 The set of nodes V s = {0, 1, ..., Ns}. Each token ws,i is associated with node i \u2208 V s.\n\u2022 The set of edges Es \u2286 V s \u00d7 V s.\n\u2022 The sequence of roles rs = {rs,i}Nsi=0, where 1 \u2264 rs,i \u2264 K for i \u2208 V s. K is the number of roles.\nThe artificial word ws,0 = < s > at the beginning of the sentence has always role 1 (rs,0 = 1). Analogously to w, the sequence of all rs is denoted as r and sequence of allGs asG.\nEdge e \u2208 Es is an ordered pair of nodes (i, j). We say that i is the head or the parent and j is the dependent or the child. We use the notation i \u2192 j for such edge. The directed path from node i to node j is denoted as i \u2217\u2192 j.\nWe place a few constraints on the graphGs.\n\u2022 The graphGs is a tree. It means it is the acyclic graph (if i \u2192 j then not j \u2217\u2192 i), where each node has one parent (if i \u2192 j then not k \u2192 j for every k 6= i).\n\u2022 The graph Gs is projective (there are no cross edges). For each edge (i, j) and for each k between i and j (i.e. i < k < j or i > k > j) there must exist the directed path i \u2217\u2192 k.\n\u2022 The graphGs is always rooted in the node 0.\nWe denote these graphs as the projective dependency trees. Example of such a tree is on Figure 1. For the treeGs we define a function\nhs(j) = i, when (i, j) \u2208 Es (1)\nthat returns the parent for each node except the root. We use graph Gs as a representation of the Bayesian network with random variables Es and rs. The roles rs,i represent the node labels and the edges express the dependences between the roles. The conditional probability of the role at position i given its parent role is denoted as P (rs,i|rs,hs(i)). The conditional probability of the word at position i in the sentence given its role rs,i is denoted as P (ws,i|rs,i).\nWe model the distribution over words in the sentence s as the mixture\nP (ws) = P (ws|rs,0) = Ns\u220f i=1 K\u2211 k=1 P (ws,i|rs,i = k)P (rs,i = k|rs,hs(i)). (2)\nThe root role is kept fixed for each sentence (rs,0 = 1) so P (ws) = P (ws|rs,0).\nWe look at the roles as mixtures over child roles and simultaneously as mixtures over words. We can represent dependency between roles with a set of K multinomial distributions \u03b8 over K roles, such that P (rs,i|rs,hs(i) = k) = \u03b8 (k) rs,i . Simultaneously, dependency of words on their roles can be represented as a set of K multinomial distributions \u03c6 over |L| words, such that P (ws,i|rs,i = k) = \u03c6(k)ws,i . To make predictions about new sentences, we need to assume a prior distribution on the parameters \u03b8(k) and \u03c6(k).\nWe place a Dirichlet prior D with the vector of K hyper-parameters \u03b1 on a multinomial distribution \u03b8(k) \u223c D(\u03b1) and with the vector of |L| hyperparameters \u03b2 on a multinomial distribution \u03c6(k) \u223c D(\u03b2). In general, D is not restricted to be Dirichlet distribution. It could be any distribution over discrete children, such as logistic normal. In this paper, we focus only on Dirichlet as a conjugate prior to the multinomial distribution and derive the learning algorithm under this assumption.\nThe choice of the child role depends only on its parent role, i.e. child roles with the same parent are mutually independent. This property is especially important for the learning algorithm (Section 3) and also for searching the most probable trees (Section 4). We do not place any assumption on the length of the sentence Ns or on how many children the parent node is expected to have."}, {"heading": "3 Parameter Estimation", "text": "In this section we present the learning algorithm for LTLM. The goal is to estimate \u03b8 and \u03c6 in a way that maximizes the predictive ability of the model (generates the corpus with maximal joint probability P (w)).\nLet \u03c7k(i,j) be an operation that changes the treeGs toG\u2032s\n\u03c7k(i,j) : Gs \u2192 G \u2032 s, (3)\nsuch that the newly created tree G\u2032(V \u2032s,E \u2032 s, r \u2032 s) consists of:\n\u2022 V \u2032s = V s.\n\u2022 E\u2032s = (Es \\ {(hs(i), i)}) \u222a {(j, i)}. \u2022 r\u2032s,a = { rs,a for a 6= i k for a = i , where 0 \u2264 a \u2264 Ns.\nIt means that we change the role of the selected node i so that rs,i = k and simultaneously we change the parent of this node to be j. We call this operation a partial change.\nThe newly created graph G\u2032 must satisfy all conditions presented in Section 2, i.e. it is a projective dependency tree rooted in the node 0. Thus not all partial changes \u03c7k(i,j) are possible to perform on graphGs.\nClearly, for the sentence s there is at most Ns(1+Ns)\n2 parent changes 1.\nTo estimate the parameters of LTLM we apply Gibbs sampling and gradually sample \u03c7k(i,j) for trees Gs. For doing so we need to determine the posterior predictive distribution2\nG\u2032s \u223c P (\u03c7k(i,j)(Gs)|w,G), (4) from which we will sample partial changes to update the trees. In the equation, G denote the sequence of all trees for given sentences w and G\u2032s is a result of one sampling. In the following text we derive this equation under assumptions from Section 2.\nThe posterior predictive distribution of Dirichlet multinomial has the form of additive smoothing that is well known in the context of language modeling. The hyper-parameters of Dirichlet prior determine how much is the predictive distribution smoothed. Thus the predictive distribution for the word-in-role distribution can be expressed as\nP (ws,i|rs,i,w\\s,i, r\\s,i) = n (ws,i|rs,i) \\s,i + \u03b2\nn (\u2022|rs,i) \\s,i + |L|\u03b2\n, (5)\n1The most parent changes are possible for the special case of the tree, where each node i has parent i \u2212 1. Thus for each node i we can change its parent to any node j < i and keep the projectivity of the tree. That is Ns(1+Ns)\n2 possibilities.\n2The posterior predictive distribution is the distribution of an unobserved variable conditioned by the observed data, i.e. P (Xn+1|X1, ..., Xn), where Xi are i.i.d. (independent and identically distributed random variables).\nwhere n(ws,i|rs,i)\\s,i is the number of times the role rs,i has been assigned to the word ws,i, excluding the position i in the s-th sentence. The symbol \u2022 represents any word in the vocabulary so that n (\u2022|rs,i) \\s,i = \u2211 l\u2208L n (l|rs,i) \\s,i . We use the symmetric Dirichlet distribution for the word-in-role probabilities as it could be difficult to estimate the vector of hyper-parameters \u03b2 for large word vocabulary. In the above mentioned equation, \u03b2 is a scalar.\nThe predictive distribution for the role-by-role distribution is\nP ( rs,i|rs,hs(i), r\\s,i ) = n (rs,i|rs,hs(i)) \\s,i + \u03b1rs,i\nn (\u2022|rs,hs(i)) \\s,i + K\u2211 k=1 \u03b1k . (6)\nAnalogously to the previous equation, n (rs,i|rs,hs(i)) \\s,i denote the number of times the role rs,i has the parent role rs,hs(i), excluding the position i in the s-th sentence. The symbol \u2022 represents any possible role to make the probability distribution summing up to 1. We assume an asymmetric Dirichlet distribution.\nWe can use predictive distributions of above mentioned Dirichlet multinomials to express the joint probability that the role at position i is k (rs,i = k) with parent at position j conditioned on current values of all variables, except those in position i in the sentence s\nP (rs,i = k, j|w, r\\s,i) \u221d P (ws,i|rs,i = k,w\\s,i, r\\s,i)\n\u00d7 P (rs,i = k|rs,j , r\\s,i) \u00d7 \u220f a:hs(a)=i P (rs,a|rs,i = k, r\\s,i). (7)\nThe choice of the node i role affects the word that is produced by this role and also all the child roles of the node i. Simultaneously, the role of the node i depends on its parent j role. Formula 7 is derived from the joint probability of a sentence s and a tree Gs, where all probabilities which do not depend on the choice of the role at position i are removed and equality is replaced by proportionality (\u221d).\nWe express the final predictive distribution for sampling partial changes \u03c7k(i,j) as\nP (\u03c7k(i,j)(Gs)|w,G) \u221d P (rs,i = k, j|w, r\\s,i) P (rs,i, hs(i)|w, r\\s,i)\n(8) that is essentially the fraction between the joint probability of rs,i and its parent after the partial change and before the partial change (conditioned on all other variables). This fraction can be interpreted as the necessity to perform this partial change.\nWe investigate two strategies of sampling partial changes:\n\u2022 Per sentence: We sample a single partial change according to Equation 8 for each sentence in the training corpus. It means during one pass through the corpus (one iteration) we perform S partial changes.\n\u2022 Per position: We sample a partial change for each position in each sentence. We perform in totalN = \u2211S s=1Ns partial changes during one\npass. Note that the denominator in Equation 8 is constant for this strategy and can be removed.\nWe compare both training strategies in Section 6. After enough training iterations, we can estimate the conditional probabilities \u03c6(k)l and \u03b8 (p) k from actual samples as\n\u03c6 (k) l \u2248\nn(ws,i=l|rs,i=k) + \u03b2 n(\u2022|rs,i=k) + |L|\u03b2 (9)\n\u03b8 (p) k \u2248\nn(rs,i=k|rs,hs(i)=p) + \u03b1k\nn(\u2022|rs,hs(i)=p) + K\u2211\nm=1 \u03b1m\n. (10)\nThese equations are similar to equations 5 and 6, but here the counts n do not exclude any position in a corpus.\nNote that in the Gibbs sampling equation, we assume that the Dirichlet parameters \u03b1 and \u03b2 are given. We use a fixed point iteration technique described in (Minka, 2003) to estimate them."}, {"heading": "4 Inference", "text": "In this section we present two approaches for searching the most probable tree for a given sentence assuming we have already estimated the parameters \u03b8 and \u03c6."}, {"heading": "4.1 Non-deterministic Inference", "text": "We use the same sampling technique as for estimating parameters (Equation 8), i.e. we iteratively sample the partial changes \u03c7k(i,j). However, we use equations 9 and 10 for predictive distributions of Dirichlet multinomials instead of 5 and 6. In fact, these equations correspond to the predictive distributions over the newly added wordws,i with the role rs,i into the corpus, conditioned on w and r. This sampling technique rarely finds the best solution, but often it is very near."}, {"heading": "4.2 Deterministic Inference", "text": "Here we present the deterministic algorithm that guarantees to find the most probable tree for a given sentence. We were inspired by Cocke-YoungerKasami (CYK) algorithm (Lange and Lei\u00df, 2009).\nLet T ns,a,c denote the subtree of Gs (subgraph of Gs that is also a tree) containing subsequence of nodes {a, a + 1, ..., c}. The superscript n denotes the number of children the root of this subtree has. We denote the joint probability of a subtree from position a to position c with the corresponding words conditioned by the root role k as Pn({ws,i}ci=a,T ns,a,c|k). Our goal is to find the tree Gs = T 1+s,0,Ns that maximizes probability P (ws,Gs) = P\n1+({ws,i}Nsi=0,T 1+ s,0,Ns |0). Similarly to CYK algorithm, our approach fol-\nlows bottom-up direction and goes through all possible subsequences for a sentence (sequence of words). At the beginning, the probabilities for subsequences of length 1 (i.e. single words) are calculated as P 1+({ws,a},T 1+s,a,a|k) = P (ws,a|rs,a = k). Once it has considered subsequences of length 1, it goes on to subsequences of length 2, and so on.\nThanks to mutual independence of roles under the same parent, we can find the most probable subtree with the root role k and with at least two root children according to\nP 2+({ws,i}ci=a,T 2+s,a,c|k) = max b:a<b<c\n[P 1+({ws,i}bi=a,T 1+s,a,b|k)\u00d7\nP 1+({ws,i}ci=b+1,T 1+s,b+1,c|k)]. (11)\nIt means we merge two neighboring subtrees with the same root role k. This is the reason why the new subtree has at least two root children. This formula is visualized on Figure 2a. Unfortunately, this does not cover all subtree cases. We find the most probable tree with only root child as follows\nP 1({ws,i}ci=a,T 1s,a,c|k) = max b,m:a\u2264b\u2264c,1\u2264m\u2264K\n[P (ws,b|rs,b = m)\u00d7 P (rs,b = m|k)\u00d7 P 1+({ws,i}b\u22121i=a ,T 1+ s,a,b\u22121|m)\u00d7\nP 1+({ws,i}ci=b+1,T 1+s,b+1,c|m)]. (12)\nThis formula is visualized on Figure 2b. To find the most probable subtree no matter how many children the root has, we need to take the maximum from both mentioned equations P 1+ = max(P 2+, P 1).\nThe algorithm has complexity O(N3sK2), i.e. it has cubic dependence on the length of the sentence Ns."}, {"heading": "5 Side-dependent LTLM", "text": "Until now, we presented LTLM in its simplified version. In role-by-role probabilities (role conditioned on its parent role) we did not distinguish whether the role is on the left side or the right side of the parent. However, this position keeps important information about the syntax of words (and their roles).\nWe assume separate multinomial distributions \u03b8\u0307 for roles that are on the left and \u03b8\u0308 for roles on the right. Each of them has its own Dirichlet prior with hyper-parameters \u03b1\u0307 and \u03b1\u0308, respectively. The process of estimating LTLM parameters is almost the same. The only difference is that we need to redefine the predictive distribution for the role-by-role distribution (Equation 6) to include only counts of roles on the appropriate side. Also, every time the role-by-role probability is used we need to distinguish sides:\nP (rs,i|rs,hs(i)) =\n{ \u03b8\u0307 (rs,hs(i)) rs,i for i < hs(i))\n\u03b8\u0308 (rs,hs(i)) rs,i for i > hs(i))\n.\n(13) In the following text we always assume the side-\ndependent LTLM."}, {"heading": "6 Experimental Results and Discussion", "text": "In this section we present experiments with LTLM on two languages, English (EN) and Czech (CS).\nAs a training corpus we use CzEng 1.0 (Bojar et al., 2012) of the sentence-parallel Czech-English corpus. We choose this corpus because it contains multiple domains, it is of reasonable length, and it is parallel so we can easily provide comparison between both languages. The corpus is divided into 100 similarly-sized sections. We use parts 0\u201397 for training, the part 98 as a development set, and the last part 99 for testing.\nWe have removed all sentences longer than 30 words. The reason was that the complexity of the learning phase and the process of searching most probable trees depends on the length of sentences. It has led to removing approximately a quarter of all sentences. The corpus is available in a tokenized form so the only preprocessing step we use is lowercasing. We keep the vocabulary of 100,000 most frequent words in the corpus for both languages. The less frequent words were replaced by the symbol <unk>. Statistics for the final corpora are shown in Table 1.\nWe measure the quality of LTLM by perplexity that is the standard measure used for LMs. Perplexity is a measure of uncertainty. The lower perplexity means the better predictive ability of the LM.\nDuring the process of parameter estimation we measure the perplexity of joint probability of sentences and their trees defined as PPX(P (w,G)) = N \u221a 1\nP (w,G) , where N is the number of all words in the training data w.\nAs we describe in Section 3, there are two approaches for the parameter estimation of LTLM. During our experiments, we found that the perposition strategy of training has the ability to converge faster, but to a worse solution compared to the per-sentence strategy which converges slower, but to a better solution.\nWe train LTLM by 500 iterations of the perposition sampling followed by another 500 iterations of the per-sentence sampling. This proves to be effi-\ncient in both aspects, the reasonable speed of convergence and the satisfactory predictive ability of the model. The learning curves are showed on Figure 3. We present the models with 10, 20, 50, 100, 200, 500, and 1000 roles. The higher role cardinality models were not possible to create because of the very high computational requirements. Similarly to the training of LTLM, the non-deterministic inference uses 100 iterations of per-position sampling followed by 100 iterations of per-sentence sampling.\nIn the following experiments we measure how well LTLM generalizes the learned patterns, i.e. how well it works on the previously unseen data. Again, we measure the perplexity, but of probability P (w) for mutual comparison with different LMs that are based on different architectures (PPX(P (w)) = N \u221a 1\nP (w) ).\nTo show the strengths of LTLM we compare it with several state-of-the-art LMs. We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011)3, and with LWLM (Deschacht et al., 2012)4. We have also created syntactic dependency tree based LM (denoted as STLM). Syntactic dependency trees for both languages are provided within CzEng corpus and are based on\n3Implementation is available at http://rnnlm.org/. Size of the hidden layer was set to 300 in our experiments. It was computationally intractable to use more neurons.\n4Implementation is available at http://liir.cs. kuleuven.be/software.php.\nMST parser (McDonald et al., 2005). We use the same architecture as for LTLM and experiment with two approaches to represent the roles. Firstly, the roles are given by the part-of-speech tag (denoted as PoS STLM). No training is required, all information come from CzEng corpus. Secondly, we learn the roles using the same algorithm as for LTLM. The only difference is that the trees are kept unchanged. Note that both deterministic and non-deterministic inference perform almost the same in this model so we do not distinguish between them.\nWe combine baseline 4-gram MKN model with other models via linear combination (in the tables denoted by the symbol +) that is simple but very efficient technique to combine LMs. Final probability is then expressed as\nP (w) = S\u220f\ns=1 Ns\u220f i=1 [ \u03bbP LM1 + (\u03bb\u2212 1)P LM2 ] . (14)\nIn the case of MKN the probability PMKN is the probability of a word ws,i conditioned by 3 previous words with MKN smoothing. For LTLM or STLM this probability is defined as\nP LTLM(ws,i|rs,hs(i)) = K\u2211 k=1 P (ws,i|rs,i = k)P (rs,i = k|rs,hs(i)). (15)\nWe use the expectation maximization algorithm (Dempster et al., 1977) for the maximum likelihood estimate of \u03bb parameter on the development part of the corpus. The influence of the number of roles on the perplexity is shown in Table 3 and the final\nresults are shown in Table 2. Note that these perplexities are not comparable with those on Figure 3 (PPX(P (w)) vs. PPX(P (w,G))). Weights of LTLM and STLM when interpolated with MKN LM are shown on Figure 4.\nFrom the tables we can see several important findings. Standalone LTLM performs worse than MKN on both languages, however their combination leads to dramatic improvements compared with other LMs. Best results are achieved by 4- gram MKN interpolated with 1000 roles LTLM and the deterministic inference. The perplexity was improved by approximately 46% on English and 49% on Czech compared with standalone MKN. The deterministic inference outperformed the nondeterministic one in all cases. LTLM also signifi-\ncantly outperformed STLM where the syntactic dependency trees were provided as a prior knowledge. The joint learning of syntax and semantics of a sentence proved to be more suitable for predicting the words.\nAn in-depth analysis of semantic and syntactic properties of LTLM is beyond the scope of this paper. For better insight into the behavior of LTLM, we show the most probable word substitutions for one selected sentence (see Table 4). We can see that the original words are often on the front positions. Also it seems that LTLM is more syntactically oriented, which confirms claims from (Levy and Goldberg, 2014; Pado\u0301 and Lapata, 2007), but to draw such conclusions a deeper analysis is required. The properties of the model strongly depends on the number of distinct roles. We experimented with maximally 1000 roles. To catch the meaning of various words in natural language, more roles may be needed. However, with our current implementation, it was intractable to train LTLM with more roles in a reasonable time. Training 1000 roles LTLM took up to two weeks on a powerful computational unit."}, {"heading": "7 Conclusion and Future Work", "text": "In this paper we introduced the Latent Tree Language Model. Our model discovers the latent tree structures hidden in natural text and uses them to predict the words in a sentence. Our experiments with English and Czech corpora showed dramatic improvements in the predictive ability compared with standalone Modified Kneser-Ney LM. Our Java implementation is available for research purposes at https://github.com/brychcin/LTLM.\nIt was beyond the scope of this paper to explicitly test the semantic and syntactic properties of the model. As the main direction for future work we plan to investigate these properties for example by comparison with human-assigned judgments. Also, we want to test our model in different NLP tasks (e.g. speech recognition, machine translation, etc.).\nWe think that the role-by-role distribution should depend on the distance between the parent and the child, but our preliminary experiments were not met with success. We plan to elaborate on this assumption. Another idea we want to explore is to use different distributions as a prior to multinomials. For example, Blei and Lafferty (2006) showed that the logistic-normal distribution works well for topic modeling because it captures the correlations between topics. The same idea might work for roles."}, {"heading": "Acknowledgments", "text": "This publication was supported by the project LO1506 of the Czech Ministry of Education, Youth and Sports. Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the programme \u201dProjects of Large Research, Development, and Innovations Infrastructures\u201d. Lastly, we would like to thank the anonymous reviewers for their insightful feedback."}], "references": [{"title": "A neural probabilistic lan", "author": ["Christian Janvin"], "venue": null, "citeRegEx": "Janvin.,? \\Q2003\\E", "shortCiteRegEx": "Janvin.", "year": 2003}, {"title": "Correlated topic models", "author": ["David M. Blei", "John D. Lafferty."], "venue": "In Proceedings of the 23rd International Conference on Machine Learning, pages 113\u2013 120. MIT Press.", "citeRegEx": "Blei and Lafferty.,? 2006", "shortCiteRegEx": "Blei and Lafferty.", "year": 2006}, {"title": "The joy of parallelism with czeng 1.0", "author": ["Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd", "Ond\u0159ej Du\u0161ek", "Petra Galu\u0161\u010d\u00e1kov\u00e1", "Martin Majli\u0161", "David Mare\u010dek", "Ji\u0159\u0131\u0301 Mar\u0161\u0131\u0301k", "Michal Nov\u00e1k", "Martin Popel", "Ale\u0161 Tamchyna"], "venue": "In Proceedings of the Eight International Conference", "citeRegEx": "Bojar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2012}, {"title": "Classbased n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai."], "venue": "Computational Linguistics, 18:467\u2013479.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Semantic spaces for improving language modeling", "author": ["Tom\u00e1\u0161 Brychc\u0131\u0301n", "Miloslav Konop\u0131\u0301k"], "venue": "Computer Speech & Language,", "citeRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.,? \\Q2014\\E", "shortCiteRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.", "year": 2014}, {"title": "Latent semantics in language models", "author": ["Tom\u00e1\u0161 Brychc\u0131\u0301n", "Miloslav Konop\u0131\u0301k"], "venue": "Computer Speech & Language,", "citeRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.,? \\Q2015\\E", "shortCiteRegEx": "Brychc\u0131\u0301n and Konop\u0131\u0301k.", "year": 2015}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua T. Goodman."], "venue": "Technical report, Computer Science Group, Harvard University.", "citeRegEx": "Chen and Goodman.,? 1998", "shortCiteRegEx": "Chen and Goodman.", "year": 1998}, {"title": "Logistic normal priors for unsupervised probabilistic grammar induction", "author": ["Shay B. Cohen", "Kevin Gimpel", "Noah A. Smith."], "venue": "Advances in Neural Information Processing Systems 21, pages 1\u20138.", "citeRegEx": "Cohen et al\\.,? 2009", "shortCiteRegEx": "Cohen et al\\.", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Arthur P. Dempster", "N.M. Laird", "D.B. Rubin."], "venue": "Journal of the Royal Statistical Society. Series B, 39(1):1\u201338.", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "The latent words language model", "author": ["Koen Deschacht", "Jan De Belder", "Marie-Francine Moens."], "venue": "Computer Speech & Language, 26(5):384\u2013409.", "citeRegEx": "Deschacht et al\\.,? 2012", "shortCiteRegEx": "Deschacht et al\\.", "year": 2012}, {"title": "Distributional structure", "author": ["Zellig Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Improving unsupervised dependency parsing with richer contexts and smoothing", "author": ["William P. Headden III", "Mark Johnson", "David McClosky."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of", "citeRegEx": "III et al\\.,? 2009", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Dependency parsing", "author": ["Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre."], "venue": "Synthesis Lectures on Human Language Technologies, 2(1):1\u2013127.", "citeRegEx": "K\u00fcbler et al\\.,? 2009", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "To cnf or not to cnf? an efficient yet presentable version of the cyk algorithm", "author": ["Martin Lange", "Hans Lei\u00df."], "venue": "Informatica Didactica, 8.", "citeRegEx": "Lange and Lei\u00df.,? 2009", "shortCiteRegEx": "Lange and Lei\u00df.", "year": 2009}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302\u2013308, Baltimore, Maryland, June. Association for Computa-", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing", "author": ["David Mare\u010dek", "Milan Straka."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Mare\u010dek and Straka.,? 2013", "shortCiteRegEx": "Mare\u010dek and Straka.", "year": 2013}, {"title": "Algorithms for bigram and trigram word clustering", "author": ["Sven Martin", "Jorg Liermann", "Hermann Ney."], "venue": "Speech Communication, 24(1):19\u201337.", "citeRegEx": "Martin et al\\.,? 1998", "shortCiteRegEx": "Martin et al\\.", "year": 1998}, {"title": "Non-projective dependency parsing using spanning tree algorithms", "author": ["Ryan McDonald", "Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1\u0161 Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u00fd", "Sanjeev Khudanpur."], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, pages", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Estimating a dirichlet distribution", "author": ["Thomas P. Minka."], "venue": "Technical report.", "citeRegEx": "Minka.,? 2003", "shortCiteRegEx": "Minka.", "year": 2003}, {"title": "Dependencybased construction of semantic space models", "author": ["Sebastian Pad\u00f3", "Mirella Lapata."], "venue": "Computational Linguistics, 33(2):161\u2013199, June.", "citeRegEx": "Pad\u00f3 and Lapata.,? 2007", "shortCiteRegEx": "Pad\u00f3 and Lapata.", "year": 2007}, {"title": "Perplexity of n-gram and dependency language models", "author": ["Martin Popel", "David Mare\u010dek."], "venue": "Proceedings of the 13th International Conference on Text, Speech and Dialogue, TSD\u201910, pages 173\u2013180, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "Popel and Mare\u010dek.,? 2010", "shortCiteRegEx": "Popel and Mare\u010dek.", "year": 2010}, {"title": "Unsupervised dependency parsing without gold part-of-speech tags", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Angel X. Chang", "Daniel Jurafsky."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281\u20131290, Ed-", "citeRegEx": "Spitkovsky et al\\.,? 2011", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2011}, {"title": "Language modelling for russian and english using words and classes", "author": ["Edward W.D. Whittaker", "Philip C. Woodland."], "venue": "Computer Speech & Language, 17(1):87\u2013104.", "citeRegEx": "Whittaker and Woodland.,? 2003", "shortCiteRegEx": "Whittaker and Woodland.", "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "It suggests that two words are expected to be semantically similar if they occur in similar contexts (they are similarly distributed in the text) (Harris, 1954).", "startOffset": 146, "endOffset": 160}, {"referenceID": 18, "context": "The most commonly used architectures are neural network LMs (Bengio et al., 2003; Mikolov et al., 2010; Mikolov et al., 2011) and class-based LMs.", "startOffset": 60, "endOffset": 125}, {"referenceID": 19, "context": "The most commonly used architectures are neural network LMs (Bengio et al., 2003; Mikolov et al., 2010; Mikolov et al., 2011) and class-based LMs.", "startOffset": 60, "endOffset": 125}, {"referenceID": 4, "context": "A similar approach was presented in (Brychc\u0131\u0301n and Konop\u0131\u0301k, 2014; Brychc\u0131\u0301n and Konop\u0131\u0301k, 2015), where the word clusters derived from various semantic spaces were used to improve LMs.", "startOffset": 36, "endOffset": 96}, {"referenceID": 5, "context": "A similar approach was presented in (Brychc\u0131\u0301n and Konop\u0131\u0301k, 2014; Brychc\u0131\u0301n and Konop\u0131\u0301k, 2015), where the word clusters derived from various semantic spaces were used to improve LMs.", "startOffset": 36, "endOffset": 96}, {"referenceID": 7, "context": "Deschacht et al. (2012) used the same idea and introduced Latent Words Language Model (LWLM), where word classes are latent variables in a graphical model.", "startOffset": 0, "endOffset": 24}, {"referenceID": 21, "context": "Resulting word representations are usually less topical and exhibit more functional similarity (they are more syntactically oriented) as shown in (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 194}, {"referenceID": 14, "context": "Resulting word representations are usually less topical and exhibit more functional similarity (they are more syntactically oriented) as shown in (Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014).", "startOffset": 146, "endOffset": 194}, {"referenceID": 12, "context": "Dependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years (K\u00fcbler et al., 2009).", "startOffset": 105, "endOffset": 126}, {"referenceID": 7, "context": "Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Mare\u010dek and Straka, 2013) offering new possibilities even for poorly-resourced languages.", "startOffset": 69, "endOffset": 191}, {"referenceID": 23, "context": "Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Mare\u010dek and Straka, 2013) offering new possibilities even for poorly-resourced languages.", "startOffset": 69, "endOffset": 191}, {"referenceID": 15, "context": "Recently, unsupervised algorithms for dependency parsing appeared in (Headden III et al., 2009; Cohen et al., 2009; Spitkovsky et al., 2010; Spitkovsky et al., 2011; Mare\u010dek and Straka, 2013) offering new possibilities even for poorly-resourced languages.", "startOffset": 69, "endOffset": 191}, {"referenceID": 10, "context": "Dependency-based methods for syntactic parsing have become increasingly popular in NLP in the last years (K\u00fcbler et al., 2009). Popel and Mare\u010dek (2010) showed that these methods are promising direction of improving LMs.", "startOffset": 106, "endOffset": 153}, {"referenceID": 9, "context": "In our work we were inspired by class-based LMs (Deschacht et al., 2012), unsupervised dependency parsing (Mare\u010dek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).", "startOffset": 48, "endOffset": 72}, {"referenceID": 15, "context": ", 2012), unsupervised dependency parsing (Mare\u010dek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).", "startOffset": 41, "endOffset": 67}, {"referenceID": 14, "context": ", 2012), unsupervised dependency parsing (Mare\u010dek and Straka, 2013), and tree-based DSMs (Levy and Goldberg, 2014).", "startOffset": 89, "endOffset": 114}, {"referenceID": 20, "context": "We use a fixed point iteration technique described in (Minka, 2003) to estimate them.", "startOffset": 54, "endOffset": 67}, {"referenceID": 13, "context": "We were inspired by Cocke-YoungerKasami (CYK) algorithm (Lange and Lei\u00df, 2009).", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "0 (Bojar et al., 2012) of the sentence-parallel Czech-English corpus.", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al.", "startOffset": 59, "endOffset": 83}, {"referenceID": 18, "context": "We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011)3, and with LWLM (Deschacht et al.", "startOffset": 126, "endOffset": 170}, {"referenceID": 19, "context": "We experiment with Modified Kneser-Ney (MKN) interpolation (Chen and Goodman, 1998), with Recurrent Neural Network LM (RNNLM) (Mikolov et al., 2010; Mikolov et al., 2011)3, and with LWLM (Deschacht et al.", "startOffset": 126, "endOffset": 170}, {"referenceID": 9, "context": ", 2011)3, and with LWLM (Deschacht et al., 2012)4.", "startOffset": 24, "endOffset": 48}, {"referenceID": 17, "context": "MST parser (McDonald et al., 2005).", "startOffset": 11, "endOffset": 34}, {"referenceID": 8, "context": "We use the expectation maximization algorithm (Dempster et al., 1977) for the maximum likelihood estimate of \u03bb parameter on the development part of the corpus.", "startOffset": 46, "endOffset": 69}, {"referenceID": 14, "context": "Also it seems that LTLM is more syntactically oriented, which confirms claims from (Levy and Goldberg, 2014; Pad\u00f3 and Lapata, 2007), but to draw such conclusions a deeper analysis is required.", "startOffset": 83, "endOffset": 131}, {"referenceID": 21, "context": "Also it seems that LTLM is more syntactically oriented, which confirms claims from (Levy and Goldberg, 2014; Pad\u00f3 and Lapata, 2007), but to draw such conclusions a deeper analysis is required.", "startOffset": 83, "endOffset": 131}, {"referenceID": 1, "context": "For example, Blei and Lafferty (2006) showed that the logistic-normal distribution works well for topic modeling because it captures the correlations between topics.", "startOffset": 13, "endOffset": 38}], "year": 2016, "abstractText": "In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles. The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms. We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.", "creator": "TeX"}}}