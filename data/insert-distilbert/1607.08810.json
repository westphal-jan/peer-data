{"id": "1607.08810", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2016", "title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms", "abstract": "polynomial networks and statistical factorization machines are two recently - proposed models that can often efficiently use inherent feature interactions in classification and regression assignment tasks. in this paper, we revisit both models from a unified perspective. based on this new view, we primarily study the properties of specific both models and already propose new efficient training algorithms. key to our approach is to cast parameter learning as a low - rank symmetric tensor estimation problem, which we happily solve by multi - convex scaling optimization. we demonstrate our approach on regression and recommender system tasks.", "histories": [["v1", "Fri, 29 Jul 2016 13:54:51 GMT  (664kb,D)", "http://arxiv.org/abs/1607.08810v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["mathieu blondel", "masakazu ishihata", "akinori fujino", "naonori ueda"], "accepted": true, "id": "1607.08810"}, "pdf": {"name": "1607.08810.pdf", "metadata": {"source": "META", "title": "Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms", "authors": ["Mathieu Blondel", "Masakazu Ishihata", "Akinori Fujino", "Naonori Ueda"], "emails": ["MATHIEU.BLONDEL@LAB.NTT.CO.JP", "ISHIHATA.MASAKAZU@LAB.NTT.CO.JP", "FUJINO.AKINORI@LAB.NTT.CO.JP", "UEDA.NAONORI@LAB.NTT.CO.JP"], "sections": [{"heading": "1. Introduction", "text": "Interactions between features play an important role in many classification and regression tasks. One of the simplest approach to leverage such interactions consists in explicitly augmenting feature vectors with products of features (monomials), as in polynomial regression. Although fast linear model solvers can be used (Chang et al., 2010; Sonnenburg & Franc, 2010), an obvious drawback of this kind of approach is that the number of parameters to estimate scales as O(dm), where d is the number of features and m is the order of interactions considered. As a result, it is usually limited to second or third-order interactions.\nAnother popular approach consists in using a polynomial kernel so as to implicitly map the data via the kernel trick. The main advantage of this approach is that the number of parameters to estimate in the model is actually independent of d and m. However, the cost of storing and evaluating the model is now proportional to the number of training instances. This is sometimes called the curse of kernelization (Wang et al., 2010). Common ways to address the issue in-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nclude the Nystro\u0308m method (Williams & Seeger, 2001), random features (Kar & Karnick, 2012) and sketching (Pham & Pagh, 2013; Avron et al., 2014).\nIn this paper, in order to leverage feature interactions in possibly very high-dimensional data, we consider models which predict the output y \u2208 R associated with an input vector x \u2208 Rd by\ny\u0302K(x;\u03bb,P ) := k\u2211 s=1 \u03bbsK(ps,x), (1)\nwhere \u03bb = [\u03bb1, . . . , \u03bbk]T \u2208 Rk, P = [p1, . . . ,pk] \u2208 Rd\u00d7k, K is a kernel and k is a hyper-parameter. More specifically, we focus on two specific choices of K which allow us to use feature interactions: the homogeneous polynomial and the ANOVA kernels. Our contributions are as follows. We show (Section 3) that choosing one kernel or the other allows us to recover polynomial networks (PNs) (Livni et al., 2014) and, surprisingly, factorization machines (FMs) (Rendle, 2010; 2012). Based on this new view, we show important properties of PNs and FMs. Notably, we show for the first time that the objective function of arbitrary-order FMs is multi-convex (Section 4). Unfortunately, the objective function of PNs is not multiconvex. To remedy this problem, we propose a lifted approach, based on casting parameter estimation as a lowrank tensor estimation problem (Section 5.1). Combined with a symmetrization trick, this approach leads to a multiconvex problem, for both PNs and FMs (Section 5.2). We demonstrate our approach on regression and recommender system tasks.\nNotation. We denote vectors, matrices and tensors using lower-case, upper-case and calligraphic bold, e.g., w, W\nand W . We denote the set of m times\ufe37 \ufe38\ufe38 \ufe37\nd\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 d real tensors by Rdm and the set of symmetric real tensors by Sdm . We use \u3008\u00b7, \u00b7\u3009 to denote vector, matrix and tensor inner product. Given x, we define a symmetric rank-one tensor by\nar X\niv :1\n60 7.\n08 81\n0v 1\n[ st\nat .M\nL ]\n2 9\nJu l 2\nx\u2297m := x \u2297 \u00b7 \u00b7 \u00b7 \u2297 x \u2208 Sdm , where (x\u2297m)j1,j2,...,jm = xj1xj2 . . . xjm . We use [d] to denote the set {1, . . . , d}."}, {"heading": "2. Related work", "text": ""}, {"heading": "2.1. Polynomial networks", "text": "Polynomial networks (PNs) (Livni et al., 2014) of degree m = 2 predict the output y \u2208 R associated with x \u2208 Rd by\ny\u0302PN(x;w,\u03bb,P ) := \u3008w,x\u3009+ \u3008\u03c3(PTx),\u03bb\u3009, (2) where w \u2208 Rd, P \u2208 Rd\u00d7k, \u03bb \u2208 Rk and \u03c3(u) := u2 is evaluated element-wise. Intuitively, the right-hand term can be interpreted as a feedforward neural network with one hidden layer of k units and with activation function \u03c3(u). Livni et al. (2014) also extend (2) to the case m = 3 and show theoretically that PNs can approximate feedforward networks with sigmoidal activation. A similar model was independently shown to perform well on dependency parsing (Chen & Manning, 2014). Unfortunately, the objective function of PNs is non-convex. In Section 5, we derive a multi-convex objective based on low-rank symmetric tensor estimation, suitable for training arbitrary-order PNs."}, {"heading": "2.2. Factorization machines", "text": "One of the simplest way to leverage feature interactions is polynomial regression (PR). For example, for second-order interactions, in this approach, we compute predictions by\ny\u0302PR(x;w,W ) := \u3008w,x\u3009+ \u2211 j\u2032>j W j,j\u2032xjxj\u2032 ,\nwhere w \u2208 Rd and W \u2208 Rd2 . Obviously, model size in PR does not scale well w.r.t. d. The main idea of (secondorder) factorization machines (FMs) (Rendle, 2010; 2012) is to replaceW with a factorized matrix PPT:\ny\u0302FM(x;w,P ) := \u3008w,x\u3009+ \u2211 j\u2032>j (PPT)jj\u2032xjxj\u2032 ,\nwhere P \u2208 Rd\u00d7k. FMs have been increasingly popular for efficiently modeling feature interactions in highdimensional data, see (Rendle, 2012) and references therein. In Section 4, we show for the first time that the objective function of arbitrary-order FMs is multi-convex."}, {"heading": "3. Polynomial and ANOVA kernels", "text": "In this section, we show that the prediction functions used by polynomial networks and factorization machines can be written using (1) for a specific choice of kernel.\nThe polynomial kernel is a popular kernel for using combinations of features. The kernel is defined as\nPm\u03b3 (p,x) := (\u03b3 + \u3008p,x\u3009)m,\nwherem \u2208 N is the degree and \u03b3 > 0 is a hyper-parameter. We define the homogeneous polynomial kernel by\nHm(p,x) := Pm0 (p,x) = \u3008p,x\u3009m.\nLet p = [p1, . . . , pd]T and x = [x1, . . . , xd]T. Then,\nHm(p,x) = d\u2211\nj1=1\n. . . d\u2211 jm=1 pj1xj1 . . . pjmxjm .\nWe thus see thatHm uses all monomials of degree m (i.e., all combinations of features with replacement).\nA much lesser known kernel is the ANOVA kernel (Stitson et al., 1997; Vapnik, 1998). Following (Shawe-Taylor & Cristianini, 2004, Section 9.2), the ANOVA kernel of degree m, where 2 \u2264 m \u2264 d, can be defined as\nAm(p,x) := \u2211\njm>\u00b7\u00b7\u00b7>j1\npj1xj1 . . . pjmxjm . (3)\nAs a result,Am uses only monomials composed of distinct features (i.e., feature combinations without replacement). For later convenience, we also define A0(p,x) := 1 and A1(p,x) := \u3008p,x\u3009. With Hm and Am defined, we are now in position to state the following lemma.\nLemma 1 Expressing PNs and FMs using kernels\nLet y\u0302K(x;\u03bb,P ) be defined as in (1). Then,\ny\u0302PN(x;w,\u03bb,P ) = \u3008w,x\u3009+ y\u0302H2(x;\u03bb,P ) y\u0302FM(x;w,P ) = \u3008w,x\u3009+ y\u0302A2(x; 1,P ).\nThe relation easily extends to higher orders. This new view allows us to state results that will be very useful in the next sections. The first one is that Hm and Am are homogeneous functions, i.e., they satisfy\n\u03bbmK(p,x) = K(\u03bbp,x) \u2200\u03bb \u2208 R,\u2200m \u2208 N+.\nAnother key property of Am(p,x) is multi-linearity.1\nLemma 2 Multi-linearity of Am(p,x) w.r.t. p1, . . . , pd Let p,x \u2208 Rd, j \u2208 [d] and 1 \u2264 m \u2264 d. Then,\nAm(p,x) = Am(p\u00acj ,x\u00acj) + pjxj Am\u22121(p\u00acj ,x\u00acj)\nwhere p\u00acj denotes the (d\u2212 1)-dimensional vector with pj removed and similarly for x\u00acj .\nThat is, everything else kept fixed, Am(p,x) is an affine function of pj , \u2200j \u2208 [d]. Proof is given in Appendix B.1.\n1A function f(\u03b81, . . . , \u03b8k) is called multi-linear (resp. multiconvex) if it is linear (resp. convex) w.r.t. \u03b81, . . . , \u03b8k separately.\nAssuming p is dense and x sparse, the cost of naively computingAm(p,x) by (3) is O(nz(x)m), where nz(x) is the number of non-zero features in x. To address this issue, we will make use of the following lemma for computing Am in nearly O(mnz(x)) time when m \u2208 {2, 3}.\nLemma 3 Efficient computation of ANOVA kernel\nA2(p,x) = 1 2\n[ H2(p,x)\u2212D2(p,x) ] A3(p,x) = 1\n6\n[ H3(p,x)\u2212 3D2,1(p,x) + 2D3(p,x) ] where we defined Dm(p,x) := \u2211dj=1(pjxj)m and Dm,n(p,x) := Dm(p,x)Dn(p,x).\nSee Appendix B.2 for a derivation."}, {"heading": "4. Direct approach", "text": "Let us denote the training set by X = [x1, . . . ,xn] \u2208 Rd\u00d7n and y = [y1, . . . , yn]T \u2208 Rn. The most natural approach to learn models of the form (1) is to directly choose \u03bb and P so as to minimize some error function\nDK(\u03bb,P ) := n\u2211 i=1 ` (yi, y\u0302K(xi;\u03bb,P )) , (4)\nwhere `(yi, y\u0302i) is a convex loss function. Note that (4) is a convex objective w.r.t. \u03bb regardless of K. However, it is in general non-convex w.r.t. P . Fortunately, when K = Am, we can show that (4) is multi-convex. Theorem 1 Multi-convexity of (4) when K = Am\nDAm is convex in \u03bb and in each row of P separately.\nProof is given in Appendix B.3. As a corollary, the objective function of FMs of arbitrary order is thus multi-convex. Theorem 1 suggests that we can minimize (4) efficiently when K = Am by solving a succession of convex problems w.r.t. \u03bb and the rows of P . We next show that when m is odd, we can just fix \u03bb = 1 without loss of generality.\nLemma 4 When is it useful to fit \u03bb?\nLet K = Hm or Am. Then min\n\u03bb\u2208Rk,P\u2208Rd\u00d7k DK(\u03bb,P ) \u2264 min P\u2208Rd\u00d7k DK(1,P ) if m is even\nmin \u03bb\u2208Rk,P\u2208Rd\u00d7k DK(\u03bb,P ) = min P\u2208Rd\u00d7k DK(1,P ) if m is odd.\nThe result stems from the fact that Hm and Am are homogeneous functions. If we define v := sign(\u03bb) m \u221a |\u03bb|p, then we obtain \u03bbHm(p,x) = Hm(v,x) \u2200\u03bb if m is odd, and similarly for Am. That is, \u03bb can be absorbed into v without loss of generality. When m is even, \u03bb < 0 cannot be absorbed unless we allow complex numbers. Because FMs fix \u03bb = 1, Lemma 4 shows that the class of functions that FMs can represent is possibly smaller than our framework."}, {"heading": "5. Lifted approach", "text": ""}, {"heading": "5.1. Conversion to low-rank tensor estimation problem", "text": "If we set K = Hm in (4), the resulting optimization problem is neither convex nor multi-convex w.r.t. P . In (Blondel et al., 2015), for m = 2, it was proposed to cast parameter estimation as a low-rank symmetric matrix estimation problem. A similar idea was used in the context of phase retrieval in (Cande\u0300s et al., 2013). Inspired by these works, we propose to convert the problem of estimating\u03bb andP to that of estimating a low-rank symmetric tensor W \u2208 Sdm . Combined with a symmetrization trick, this approach leads to an objective that is multi-convex, for both K = Am and K = Hm (Section 5.2). We begin by rewriting the kernel definitions using rank-one tensors. ForHm(p,x), it is easy to see that\nHm(p,x) = \u3008p\u2297m,x\u2297m\u3009. (5)\nFor Am(p,x), we need to ignore irrelevant monomials. For convenience, we introduce the following notation:\n\u3008W ,X \u3009> := \u2211\njm>\u00b7\u00b7\u00b7>j1\nWj1,...,jmX j1,...,jm \u2200W ,X \u2208 Sd m .\nWe can now concisely rewrite the ANOVA kernel as\nAm(p,x) = \u3008p\u2297m,x\u2297m\u3009>. (6)\nOur key insight is described in the following lemma.\nLemma 5 Link between tensors and kernel expansions\nLet W \u2208 Sdm have a symmetric outer product decomposition (Comon et al., 2008)\nW = k\u2211 s=1 \u03bbsp \u2297m s . (7)\nLet \u03bb = [\u03bb1, . . . , \u03bbk]T and P = [p1, . . . ,pk]. Then,\n\u3008W ,x\u2297m\u3009 = y\u0302Hm(x;\u03bb,P ) (8) \u3008W ,x\u2297m\u3009> = y\u0302Am(x;\u03bb,P ). (9)\nThe result follows immediately from (5) and (6), and from the linearity of \u3008\u00b7, \u00b7\u3009 and \u3008\u00b7, \u00b7\u3009>. Given W \u2208 Sdm , let us define the following objective functions\nLHm(W) := n\u2211 i=1 ` ( yi, \u3008W ,x\u2297mi \u3009 ) LAm(W) :=\nn\u2211 i=1 ` ( yi, \u3008W ,x\u2297mi \u3009> ) .\nIf W is decomposed as in (7), then from Lemma 5, we obtain LK(W) = DK(\u03bb,P ) for K = Hm or Am. This\nsuggests that we can convert the problem of learning \u03bb and P to that of learning a symmetric tensor W of (symmetric) rank k. Thus, the problem of finding a small number of bases p1, . . . ,pk and their associated weights \u03bb1, . . . , \u03bbk is converted to that of learning a low-rank symmetric tensor. Following (Cande\u0300s et al., 2013), we call this approach lifted. Intuitively, we can think of W as a tensor that contains the weights for predicting y of monomials of degree m. For instance, when m = 3, Wi,j,k is the weight corresponding to the monomial xixjxk."}, {"heading": "5.2. Multi-convex formulation", "text": "Estimating a low-rank symmetric tensor W \u2208 Sdm for arbitrary integer m \u2265 2 is in itself a difficult non-convex problem. Nevertheless, based on a symmetrization trick, we can convert the problem to a multi-convex one, which we can easily minimize by alternating minimization. We first present our approach for the case m = 2 to give intuitions then explain how to extend it to m \u2265 3. Intuition with the second-order case. For the case m = 2, we need to estimate a low-rank symmetric matrix W \u2208 Sd2 . Naively parameterizing W = P diag(\u03bb)PT and solving for \u03bb and P does not lead to a multi-convex formulation for the case K = H2. This is due to the fact that \u3008P diag(\u03bb)PT,x\u22972\u3009 is quadratic in P . Our key idea is to parametrize W = S(UV T) where U ,V \u2208 Rd\u00d7r and S(M) := 12 (M + M\nT) \u2208 Sd2 is the symmetrization of M \u2208 Rd2 . We then minimize LK(S(UV T)) w.r.t. U ,V . The main advantage is that both \u3008S(UV T), \u00b7\u3009 and \u3008S(UV T), \u00b7\u3009> are bi-linear inU and V . This implies that LK(S(UV T)) is bi-convex in U and V and can therefore be efficiently minimized by alternating minimization. Once we obtained W = S(UV T), we can optionally compute its eigendecomposition W = P diag(\u03bb)PT, with k = rank(W ) and r \u2264 k \u2264 2r, then apply (8) or (9) to obtain the model in kernel expansion form.\nExtension to higher-order case. For m \u2265 3, we now estimate a low-rank symmetric tensor W = S(M) \u2208 Sdm , where M \u2208 Rdm and S(M) is the symmetrization of M (cf. Appendix A.2). We decompose M using m matrices of size d\u00d7 r. Let us call these matrices {U t}mt=1 and their columns uts = [u t 1s, . . . , u t ds]\nT. Then the decomposition of M can be expressed as a sum of rank-one tensors\nM = r\u2211 s=1 u1s \u2297 \u00b7 \u00b7 \u00b7 \u2297 ums . (10)\nDue to multi-linearity of (10) w.r.t. U1, . . . ,Um, the objective function LK is multi-convex in U1, . . . ,Um.\nComputing predictions efficiently. When K = Hm, predictions are computed by \u3008W ,x\u2297m\u3009. To compute them efficiently, we use the following lemma.\nLemma 6 Symmetrization does not affect inner product\n\u3008S(M),X \u3009 = \u3008M,X \u3009 \u2200M \u2208 Rdm ,X \u2208 Sdm ,m \u2265 2. (11)\nProof is given in Appendix A.2. Using x\u2297m \u2208 Sdm , W = S(M) and (10), we then obtain\n\u3008W ,x\u2297m\u3009 = \u3008M,x\u2297m\u3009 = r\u2211 s=1 m\u220f t=1 \u3008uts,x\u3009.\nAs a result, we never need to explicitly compute the symmetrized tensor. For the case K = A2, cf. Appendix D.3."}, {"heading": "6. Regularization", "text": "In some applications, the number of bases or the rank constraint are not enough for obtaining good generalization performance and it is necessary to consider additional form of regularization. For the lifted objective with K = H2 or A2, we use the typical Frobenius-norm regularization\nL\u0303K(U ,V ) := LK(S(UV T))+ \u03b2\n2 (\u2016U\u20162F+\u2016V \u20162F ), (12)\nwhere \u03b2 > 0 is a regularization hyper-parameter. For the direct objective, we introduce the new regularization\nD\u0303K(\u03bb,P ) := DK(\u03bb,P ) + \u03b2 k\u2211 s=1 |\u03bbs| \u2016ps\u20162. (13)\nThis allows us to regularize \u03bb and P with a single hyperparameter. Let us define the following nuclear norm penalized objective:\nL\u0304K(M) := LK(S(M)) + \u03b2\u2016M\u2016\u2217. (14) We can show that (12), (13) and (14) are equivalent in the following sense. Theorem 2 Equivalence of regularized problems\nLet K = H2 or A2, then min M\u2208Rd2\nL\u0304K(M) = min U\u2208Rd\u00d7r V \u2208Rd\u00d7r L\u0303K(U ,V ) = min \u03bb\u2208Rk P\u2208Rd\u00d7k D\u0303K(\u03bb,P )\nwhere rank(M\u2217) \u2264 r = k andM\u2217 \u2208 argmin M\u2208Rd2 L\u0304K(M).\nProof is given in Appendix C. Our proof relies on the variational form of the nuclear norm and is thus limited to m = 2. One of the key ingredients of the proof is to show that the minimizer of (14) is always a symmetric matrix. In addition to Theorem 2, from (Abernethy et al., 2009), we also know that every local minimum U ,V of (12) gives a global solution UV T of (14) provided that rank(M\u2217) \u2264 r. Proving a similar result for (13) is a future work. When m \u2265 3, as used in our experiments, a squared Frobenius norm penalty on P (direct objective) or on {U t}mt=1 (lifted objective) works well in practice, although we lose the theoretical connection with the nuclear norm."}, {"heading": "7. Coordinate descent algorithms", "text": "We now describe how to learn the model parameters by coordinate descent, which is a state-of-the-art learningrate free solver for multi-convex problems (e.g., Yu et al. (2012)). In the following, we assume that ` is \u00b5-smooth.\nDirect objective with K = Am for m \u2208 {2, 3}. First, we note that minimizing (13) w.r.t. \u03bb can be reduced to a standard `1-regularized convex objective via a simple change of variable. Hence we focus on minimization w.r.t. P .\nLet us denote the elements of P by pjs. Then, our algorithm cyclically performs the following update for all s \u2208 [k] and j \u2208 [d]:\npjs \u2190 pjs \u2212 \u03b7\u22121 [\nn\u2211 i=1 `\u2032(yi, y\u0302i) \u2202y\u0302i \u2202pjs\n+ 2\u03b2|\u03bbs|pjs ] ,\nwhere \u03b7 := \u00b5 \u2211n i=1 ( \u2202y\u0302i \u2202pjs )2 + 2\u03b2|\u03bbs|. Note that when ` is the squared loss, the above is equivalent to a Newton update and is the exact coordinate-wise minimizer.\nThe key challenge to use CD is computing \u2202y\u0302i\u2202pjs = \u03bbs \u2202Am(ps,xi)\n\u2202pjs efficiently. Let us denote the elements of\nX by xji. Using Lemma 3, we obtain \u2202A2(ps,xi)\n\u2202pjs =\n\u3008ps,xi\u3009xji \u2212 pjsx2ji and \u2202A 3(ps,xi) \u2202pjs = A2(ps,xi)xji \u2212 pjsx 2 ji\u3008ps,xi\u3009 + p2jsx3ji. If for all i \u2208 [n] and for s fixed, we maintain \u3008ps,xi\u3009 and A2(ps,xi) (i.e., keep in sync after every update of pjs), then computing \u2202y\u0302i\u2202pjs takes O(m) time. Hence the cost of one epoch, i.e. updating all elements of P once, is O(mknz(X)). Complete details and pseudo code are given in Appendix D.1.\nTo our knowledge, this is the first CD algorithm capable of training third-order FMs. Supporting arbitrarym \u2208 N is an important future work.\nLifted objective with K = Hm. Recall that we want to learn the matrices {U t}mt=1, whose columns we denote by uts = [u t 1s, . . . , u t ds]\nT. Our algorithm cyclically performs the following update for all t \u2208 [m], s \u2208 [r] and j \u2208 [d]:\nutjs \u2190 utjs \u2212 \u03b7\u22121 [ n\u2211 i=1 `\u2032(yi, y\u0302i) \u2202y\u0302i \u2202utjs + \u03b2utjs ] ,\nwhere \u03b7 := \u00b5 \u2211n i=1 ( \u2202y\u0302i \u2202utjs )2 + \u03b2. The main difficulty is computing \u2202y\u0302i \u2202utjs = \u220f t\u2032 6=t\u3008ut \u2032\ns ,xi\u3009xji efficiently. If for all i \u2208 [n] and for t and s fixed, we maintain \u03bei :=\u220f t\u2032 6=t\u3008ut \u2032\ns ,xi\u3009, then the cost of computing \u2202y\u0302i\u2202utjs is O(1). Hence the cost of one epoch is O(mrnz(X)), the same as SGD. Complete details are given in Appendix D.2.\nConvergence. The above updates decrease the objective monotonically. Convergence to a stationary point is guar-\nanteed following (Bertsekas, 1999, Proposition 2.7.1)."}, {"heading": "8. Inhomogeneous polynomial models", "text": "The algorithms presented so far are designed for homogeneous polynomial kernelsHm andAm. These kernels only use monomials of the same degree m. However, in many applications, we would like to use monomials of up to some degree. In this section, we propose a simple idea to do so using the algorithms presented so far, unmodified. Our key observation is that we can easily turn homogeneous polynomials into inhomogeneous ones by augmenting the dimensions of the training data with dummy features.\nWe begin by explaining how to learn inhomogeneous polynomial models using Hm. Let us denote p\u0303T := [\u03b3,pT] \u2208 Rd+1 and x\u0303T := [1,xT] \u2208 Rd+1. Then, we obtain\nHm(p\u0303, x\u0303) = \u3008p\u0303, x\u0303\u3009m = (\u03b3 + \u3008p,x\u3009)m = Pm\u03b3 (p,x).\nTherefore, if we prepare the augmented training set x\u03031, . . . , x\u0303n, the problem of learning a model of the form\u2211k s=1 \u03bbsPm\u03b3s(ps,x) can be converted to that of learning a rank-k symmetric tensor W \u2208 S(d+1)m using the method presented in Section 5. Note that the parameter \u03b3s is automatically learned from data for each basis ps.\nNext, we explain how to learn inhomogeneous polynomial models usingAm. Using Lemma 2, we immediately obtain for 1 \u2264 m \u2264 d:\nAm(p\u0303, x\u0303) = Am(p,x) + \u03b3Am\u22121(p,x). (15)\nFor instance, when m = 2, we obtain\nA2(p\u0303, x\u0303) = A2(p,x)+\u03b3A1(p,x) = A2(p,x)+\u03b3\u3008p,x\u3009.\nTherefore, if we prepare the augmented training set x\u03031, . . . , x\u0303n, we can easily learn a combination of linear kernel and second-order ANOVA kernel using methods presented in Section 4 or Section 5. Note that (15) only states the relation between two ANOVA kernels of consecutive degrees. Fortunately, we can also apply (15) recursively. Namely, by adding m \u2212 1 dummy features, we can sum the kernels from Am down to A1 (i.e., linear kernel)."}, {"heading": "9. Experimental results", "text": "In this section, we present experimental results, focusing on regression tasks. Datasets are described in Appendix E. In all experiments, we set `(y, y\u0302) to the squared loss."}, {"heading": "9.1. Direct optimization: is it useful to fit \u03bb?", "text": "As explained in Section 4, there is no benefit to fitting \u03bb when m is odd, since Am and Hm can absorb \u03bb into P . This is however not the case when m is even: Am andHm\ncan absorb absolute values but not negative signs (unless complex numbers are allowed for parameters). Therefore, when m is even, the class of functions we can represent with models of the form (1) is possibly smaller if we fix \u03bb = 1 (as done in FMs).\nTo check that this is indeed the case, on the diabetes dataset, we minimized (13) with m = 2 as follows:\na) minimize w.r.t. both \u03bb and P alternatingly, b) fix \u03bbs = 1 for s \u2208 [k] and minimize w.r.t. P , c) fix \u03bbs = \u00b11 with proba. 0.5 and minimize w.r.t. P .\nWe initialized elements of P by pjs \u223c N (0, 0.01) for all j \u2208 [d], s \u2208 [k]. Our results are shown in Figure 1. For K = A2, we use CD and for K = H2, we use L-BFGS. Note that since (13) is convex w.r.t. \u03bb, a) is insensitive to the initialization of \u03bb as long as we fit \u03bb beforeP . Not surprisingly, fitting \u03bb allows us to achieve a smaller objective value. This is especially apparent when K = H2. However, the difference is much smaller when K = A2. We give intuitions as to why this is the case in Section 10.\nWe emphasize that this experiment was designed to confirm that fitting \u03bb does indeed improve representation power of the model when m is even. In practice, it is possible that fixing \u03bb = 1 reduces overfitting and thus improves generalization error. However, this highly depends on the data."}, {"heading": "9.2. Direct vs. lifted optimization", "text": "In this section, we compare the direct and lifted optimization approaches on high-dimensional data when m = 2. To compare the two approaches fairly, we propose the following initialization scheme. Recall that, at the end of the day, both approaches are essentially learning a low rank symmetric matrix: W = S(UV T) for lifted and W = P diag(\u03bb)PT for direct optimization. This suggests that we can easily convert the matricesU ,V \u2208 Rd\u00d7r used for initializing lifted optimization to P \u2208 Rd\u00d7k and \u03bb \u2208 Rd\u00d7k by computing the (reduced) eigendecomposition of S(UV T). Note that because we solve the lifted optimization problem by coordinate descent, UV T is never symmetric and therefore the rank of S(UV T) is usually twice that of UV T. Hence, in practice, we have that r = k/2. In our experiment, we compared four methods: lifted objective solved by CD, direct objective solved by CD, LBFGS and SGD. For lifted optimization, we initialized the elements of U and V by sampling from N (0, 0.01). For direct optimization, we obtained P and \u03bb as explained. Results on the E2006-tfidf high-dimensional dataset are shown in Figure 2. For K = A2, we find that Lifted (CD) and Direct (CD) have similar convergence speed and both outperform Direct (L-BFGS). For K = H2, we find that Lifted (CD) outperforms both Direct (L-BFGS) and Direct (SGD). Note that we did not implement Direct (CD) for K = H2 since the direct optimization problem is not coordinate-wise convex, as explained in Section 5."}, {"heading": "9.3. Recommender system experiment", "text": "To confirm the ability of the proposed framework to infer the weights of unobserved feature interactions, we conducted experiments on Last.fm and Movielens 1M, two standard recommender system datasets. Following (Rendle, 2012), matrix factorization can be reduced to FMs by creating a dataset of (xi, yi) pairs where xi contains the one-hot encoding of the user and item and yi is the corresponding rating (i.e., number of training instances equals number of ratings). We compared four models:\na) K = A2 (augment): y\u0302 = y\u0302A2(x\u0303), with x\u0303T := [1,xT], b) K = A2 (linear combination): y\u0302 = \u3008w,x\u3009+ y\u0302A2(x), c) K = H2 (augment): y\u0302 = y\u0302H2(x\u0303) and d) K = H2 (linear combination): y\u0302 = \u3008w,x\u3009+ y\u0302H2(x),\nwhere w \u2208 Rd is a vector of first-order weights, estimated from training data. Note that b) and d) are exactly the same as FMs and PNs, respectively. Results are shown in Figure 3. We see that A2 tends to outperform H2 on these tasks. We hypothesize that this the case because features are binary (cf., discussion in Section 10). We also see that simply augmenting the features as suggested in Section 8 is comparable or better than learning additional first-order feature weights, as done in FMs and PNs."}, {"heading": "9.4. Low-budget non-linear regression experiment", "text": "In this experiment, we demonstrate the ability of the proposed framework to reach good regression performance with a small number of bases k. We compared:\na) Proposed with K = H3 (with augmented features), b) Proposed with K = A3 (with augmented features), c) Nystro\u0308m method with K = P31 and d) Random Selection: choose p1, . . . ,pk uniformly at\nrandom from training set and use K = P31 . For a) and b) we used the lifted approach. For fair comparison in terms of model size (number of floats used), we set r = k/3. Results on the abalone, cadata and cpusmall datasets are shown in Figure 4. We see that i) the proposed framework reaches the same performance as kernel ridge regression with much fewer bases than other methods and ii) H3 tends to outperform A3 on these tasks. Similar trends were observed when using K = H2 or A2."}, {"heading": "10. Discussion", "text": "Ability to infer weights of unobserved interactions. In our view, one of the strengths of PNs and FMs is their\nability to infer the weights of unobserved feature interactions, unlike traditional kernel methods. To see why, recall that in kernel methods, predictions are computed by y\u0302 =\u2211n i=1 \u03b1iK(xi,x). When K = Hm or Am, by Lemma 5, this is equivalent to y\u0302 = \u3008W\u0303 ,x\u2297m\u3009 or \u3008W\u0303 ,x\u2297m\u3009> if we set W\u0303 := \u2211ni=1 \u03b1ix\u2297mi . Thus, in kernel methods, the weight associated with xj1 . . . xjm can be written as a linear combination of the training data\u2019s monomials:\nW\u0303j1,...,jm = n\u2211 i=1 \u03b1ixj1i . . . xjmi.\nAssuming binary features, the weights of monomials that were never observed in the training set are zero. In contrast, in PNs and FMs, we have W = \u2211ks=1 \u03bbip\u2297ms and therefore the weight associated with xj1 . . . xjm becomes\nWj1,...,jm = k\u2211 s=1 \u03bbspj1s . . . pjms.\nBecause parameters are shared across monomials, PNs and FMs are able to interpolate the weights of monomials that were never observed in the training set. This is the key property which makes it possible to use them on recommender system tasks. In future work, we plan to apply PNs and FMs to biological data, where this property should be\nvery useful, e.g., for inferring higher-order interactions between genes.\nANOVA kernel vs. polynomial kernel. One of the key properties of the ANOVA kernel Am(p,x) is multilinearity w.r.t. elements of p (Lemma 2). This is the key difference with Hm(p,x) which makes the direct optimization objective multi-convex when K = Am (Theorem 1). However, because we need to ignore irrelevant monomials, computing the kernel and its gradient is more challenging. Deriving efficient training algorithms for arbitrary m \u2208 N is an important future work. In our experiments in Section 9.1, we showed that fixing \u03bb = 1 works relatively well when K = A2. To see intuitively why this is the case, note that fixing \u03bb = 1 is equivalent to constraining the weight matrix W to be positive semidefinite, i.e., \u2203P s.t. W = PPT. Next, observe that we can rewrite the prediction function as\ny\u0302A2(x; 1,P ) = \u3008PPT,x\u22972\u3009> = \u3008U(PPT),x\u22972\u3009,\nwhere U(M) is a mask which sets diagonal and lowerdiagonal elements of M to zero. We therefore see that when using K = A2, we are learning a strictly uppertriangular matrix, parametrized by PPT. Importantly, the matrix U(PPT) is not positive semidefinite. This is what gives the model some degree of freedom, even though PPT is positive semidefinite. In contrast, when using K = H2, if we fix \u03bb = 1, then we have that\ny\u0302H2(x; 1,P ) = \u3008PPT,x\u22972\u3009 = xTPPTx \u2265 0\nand therefore the model is unable to predict negative values.\nEmpirically, we showed in Section 9.4 that Hm outperforms Am for low-budget non-linear regression. In contrast, we showed in Section 9.3 that Am outperforms Hm for recommender systems. The main difference between the two experiments is the nature of the features used: continuous for the former and binary for the latter. For binary features, squared features x21, . . . , x 2 d are redundant with\nx1, . . . , xd and are therefore not expected to help improve accuracy. On the contrary, they might introduce bias towards first-order features. We hypothesize that the ANOVA kernel is in general a better choice for binary features, although this needs to be verified by more experiments, for instance on natural language processing (NLP) tasks.\nDirect vs. lifted optimization. The main advantage of direct optimization is that we only need to estimate \u03bb \u2208 Rk and P \u2208 Rd\u00d7k and therefore the number of parameters to estimate is independent of the degree m. Unfortunately, the approach is neither convex nor multi-convex when using K = Hm. In addition, the regularized objective (13) is non-smooth w.r.t. \u03bb. In Section 5, we proposed to reformulate the problem as one of low-rank symmetric tensor estimation and used a symmetrization trick to obtain a multiconvex smooth objective function. Because this objective involves the estimation ofmmatrices of size d\u00d7r, we need to set r = k/m for fair comparison with the direct objective in terms of model size. When K = Am, we showed that the direct objective is readily multi-convex. However, an advantage of our lifted objective when K = Am is that it is convex w.r.t. larger block of variables than the direct objective."}, {"heading": "11. Conclusion", "text": "In this paper, we revisited polynomial networks (Livni et al., 2014) and factorization machines (Rendle, 2010; 2012) from a unified perspective. We proposed direct and lifted optimization approaches and showed their equivalence in the regularized case for m = 2. With respect to PNs, we proposed the first CD solver with support for arbitrary integerm \u2265 2. With respect to FMs, we made several novel contributions including making a connection with the ANOVA kernel, proving important properties of the objective function and deriving the first CD solver for third-order FMs. Empirically, we showed that the proposed algorithms achieve excellent performance on non-linear regression and recommender system tasks."}, {"heading": "Acknowledgments", "text": "This work was partially conducted as part of \u201cResearch and Development on Fundamental and Applied Technologies for Social Big Data\u201d, commissioned by the National Institute of Information and Communications Technology (NICT), Japan. We also thank Vlad Niculae, Olivier Grisel, Fabian Pedregosa and Joseph Salmon for their valuable comments."}, {"heading": "A. Symmetric tensors", "text": "A.1. Background\nLet Rd1\u00d7\u00b7\u00b7\u00b7\u00d7dm be the set of d1\u00d7\u00b7 \u00b7 \u00b7\u00d7dm realm-order tensors. In this paper, we focus on cubical tensors, i.e., d1 = \u00b7 \u00b7 \u00b7 = dm = d. We denote the set of m-order cubical tensors by Rd m\n. We denote the elements of M \u2208 Rdm by Mj1,...,jm , where j1, . . . , jm \u2208 [d]. Let \u03c3 = [\u03c31, . . . , \u03c3m] be a permutation of {1, . . . ,m}. Given M \u2208 Rdm , we define M\u03c3 \u2208 Rdm as the tensor such that\n(M\u03c3)j1,...,jm := Mj\u03c31 ,...,j\u03c3m \u2200j1, . . . , jm \u2208 [d].\nIn other words M\u03c3 is a copy of M with its axes permuted. This generalizes the concept of transpose to tensors. Let Pm be the set of all permutations of {1, . . . ,m}. We say that a tensor X \u2208 Rdm is symmetric if and only if\nX\u03c3 = X \u2200\u03c3 \u2208 Pm.\nWe denote the set of symmetric tensors by Sdm .\nGiven M \u2208 Rdm , we define the symmetrization of M by\nS(M) = 1 m! \u2211 \u03c3\u2208Pm M\u03c3.\nNote that when m = 2, then S(M) = 12 (M +M T). Given x \u2208 Rd, we define a symmetric rank-one tensor by x\u2297m := x\u2297 \u00b7 \u00b7 \u00b7 \u2297 x\ufe38 \ufe37\ufe37 \ufe38 m times \u2208 Sdm , i.e., (x\u2297m)j1,j2,...,jm = xj1xj2 . . . xjm . We denote the symmetric outer product decomposition (Comon et al., 2008) of W \u2208 Sd m by\nW = k\u2211 s=1 \u03bbsp \u2297m s ,\nwhere k is called the symmetric rank of W . This generalizes the concept of eigendecomposition to tensors. These two concepts are illustrated in Figure 5.\nA.2. Proof of Lemma 6\nAssume M \u2208 Rdm and X \u2208 Sdm . Then,\n\u3008S(M),X \u3009 = 1 m! \u2211 \u03c3\u2208Pm \u3008M\u03c3,X \u3009 by definition of S(M) and by linearity\n= 1\nm! \u2211 \u03c3\u2208Pm \u3008(M\u03c3)\u03c3\u22121 ,X\u03c3\u22121\u3009 since \u3008A,B\u3009 = \u3008A\u03c3,B\u03c3\u3009 \u2200A,B \u2208 Rd m ,\u2200\u03c3 \u2208 Pm\n= 1\nm! \u2211 \u03c3\u2208Pm \u3008M,X\u03c3\u22121\u3009 by definition of inverse permutation\n= 1\nm! \u2211 \u03c3\u2208Pm \u3008M,X \u3009 since X \u2208 Sdm\n= \u3008M,X \u3009."}, {"heading": "B. Proofs related to ANOVA kernels", "text": "B.1. Proof of multi-linearity (Lemma 2)\nFor m = 1, we have\nA1(p,x) = d\u2211 j=1 pjxj\n= \u2211 k 6=j pkxk + pjxj\n= A1(p\u00acj ,x\u00acj) + pjxj A0(p\u00acj ,x\u00acj)\nwhere we used A0(p,x) = 1. For 1 < m \u2264 d, first notice that we can rewrite (3) as\nAm(p,x) = \u2211\njm>\u00b7\u00b7\u00b7>j1\npj1xj1 . . . pjmxjm jk \u2208 [d], k \u2208 [m]\n= d\u2212m+1\u2211 j1=1 d\u2212m+2\u2211 j2=j1+1 \u00b7 \u00b7 \u00b7 d\u2211 jm=jm\u22121+1 pj1xj1 . . . pjmxjm .\nThen,\nAm(p,x) = d\u2212m+1\u2211 j1=1 d\u2212m+2\u2211 j2=j1+1 \u00b7 \u00b7 \u00b7 d\u2211 jm=jm\u22121+1 pj1xj1pj2xj2 . . . pjmxjm\n= d\u2212m+2\u2211 j2=j1+1 \u00b7 \u00b7 \u00b7 d\u2211 jm=jm\u22121+1 p1x1pj2xj2 . . . pjmxjm+\nd\u2212m+1\u2211 j1=2 d\u2212m+2\u2211 j2=j1+1 \u00b7 \u00b7 \u00b7 d\u2211 jm=jm\u22121+1 pj1xj1pj2xj2 . . . pjmxjm\n= p1x1Am\u22121(p\u00ac1,x\u00ac1) +Am(p\u00ac1,x\u00ac1).\nWe can always permute the elements of p and x without changing Am(p,x). It follows that\nAm(p,x) = pjxjAm\u22121(p\u00acj ,x\u00acj) +Am(p\u00acj ,x\u00acj) \u2200j \u2208 [d].\nB.2. Efficient computation when m \u2208 {2, 3} Using the multinomial theorem, we can expand the homogeneous polynomial kernel as\nHm(p,x) = \u3008p,x\u3009m = \u2211\nk1+\u00b7\u00b7\u00b7+kd=m\n( m\nk1, . . . , kd ) d\u220f j=1 (pjxj) kj (16)\nwhere ( m\nk1, . . . , kd\n) :=\nm!\nk1! . . . kd! is the multinomial coefficient and kj \u2208 {0, 1, . . . ,m}. Intuitively, (\nm k1,...,kd\n) is the weight of the monomial\n(p1x1) k1 . . . (pdxd) kd in the expansion. For instance, if p,x \u2208 R3, then the weight of p1x1p23x23 is ( 3 1,0,2 ) = 3. The main observation is that monomials where all k1, . . . , kd are in {0, 1} correspond to monomials of (3). If we can compute all other monomials efficiently, then we just need to subtract them from the homogeneous kernel in order to obtain (3).\nTo simplify notation, we define the shorthands\n\u03c1j := pjxj , Dm(p,x) := d\u2211 j=1 \u03c1mj and Dm,n(p,x) := Dm(p,x)Dn(p,x).\nCase m = 2\nFor m = 2, the possible monomials are of the form \u03c12j for all j and \u03c1i\u03c1j for j > i. Applying (16), we obtain\nH2(p,x) = d\u2211 j=1 \u03c12j + 2 \u2211 j>i \u03c1i\u03c1j\n= D2(p,x) + 2A2(p,x)\nand therefore A2(p,x) = 1\n2\n[ H2(p,x)\u2212D2(p,x) ] .\nThis formula was already mentioned in (Stitson et al., 1997). It was also rediscovered in (Rendle, 2010; 2012), although the connection with the ANOVA kernel was not identified.\nCase m = 3\nFor m = 3, the possible monomials are of the form \u03c13j for all j, \u03c1i\u03c1 2 j for i 6= j and \u03c1i\u03c1j\u03c1k for k > j > i. Applying (16), we obtain\nH3(p,x) = d\u2211 j=1 \u03c13j + 3 \u2211 i6=j \u03c1i\u03c1 2 j + 6 \u2211 k>i>i \u03c1i\u03c1j\u03c1k\n= D3(p,x) + 3 \u2211 i6=j \u03c1i\u03c1 2 j + 6A3(p,x).\nWe can compute the second term efficiently by using\n\u2211 i6=j \u03c1i\u03c1 2 j = d\u2211 i,j=1 \u03c1i\u03c1 2 j \u2212 d\u2211 j=1 \u03c13j\n= D2,1(p,x)\u2212D3(p,x).\nWe therefore obtain A3(p,x) = 1\n6\n[ H3(p,x)\u2212D3(p,x)\u2212 3 ( D2,1(p,x)\u2212D3(p,x) )] = 1\n6\n[ H3(p,x)\u2212 3D2,1(p,x) + 2D3(p,x) ] .\nB.3. Proof of multi-convexity (Theorem 1)\nLet us denote the rows of P by p\u03041, . . . , p\u0304d \u2208 Rk. Using Lemma 2, we know that there exists constants as and bs such that for all j \u2208 [d]\ny\u0302Am(x;\u03bb,P ) = k\u2211 s=1 \u03bbsAm(ps,x)\n= k\u2211 s=1 \u03bbs(pjsxjas + bs)\n= k\u2211 s=1 pjs\u03bbsxjas + const\n= \u3008p\u0304j , \u00b5\u0304j\u3009+ const where \u00b5\u0304j := [\u03bb1xja1, . . . , \u03bbkxjak]T. Hence y\u0302Am(x;\u03bb,P ) is an affine function of p\u03041, . . . , p\u0304d. The composition of a convex loss function and an affine function is convex. Therefore, (4) is convex in p\u0304j \u2200j \u2208 [d]. Convexity w.r.t. \u03bb is obvious."}, {"heading": "C. Proof of equivalence between regularized problems (Theorem 2)", "text": "First, we are going to prove that the optimal solution of the nuclear norm penalized problem is a symmetric matrix. For that, we need the following lemma.\nLemma 7 Upper-bound on nuclear norm of symmetrized matrix\n\u2016S(M)\u2016\u2217 \u2264 \u2016M\u2016\u2217 \u2200M \u2208 Rd 2\nProof.\n\u2016S(M)\u2016\u2217 = \u2016 1\n2 (M +MT)\u2016\u2217\n= 1\n2 (\u2016M +MT\u2016\u2217)\n\u2264 1 2\n(\u2016M\u2016\u2217 + \u2016MT\u2016\u2217) = \u2016M\u2016\u2217,\nwith equality in the third line holding if and only if M = MT. The second and third lines use absolute homogeneity and subadditivity, two properties that matrix norms satisfy. The last line uses the fact that \u2016M\u2016\u2217 = \u2016MT\u2016\u2217.\nLemma 8 Symmetry of optimal solution of nuclear norm penalized problem\nargmin M\u2208Rd2\nL\u0304K(M) := LK(S(M)) + \u03b2\u2016M\u2016\u2217 \u2208 Sd 2\nProof. From any (possibly asymmetric) square matrix A \u2208 Rd2 , we can construct M = S(A). We obviously have LK(S(A)) = LK(S(M)). Combining this with Lemma 7, we have that L\u0304K(M) \u2264 L\u0304K(A). Therefore we can always achieve the smallest objective value by choosing a symmetric matrix.\nNext, we recall the variational formulation of the nuclear norm based on the SVD.\nLemma 9 Variational formulation of nuclear norm based on SVD\n\u2016M\u2016\u2217 = min U ,V\nM=UV T\n1 2 (\u2016U\u20162F + \u2016V \u20162F ) \u2200M \u2208 Rd 2\n(17)\nThe minimum above is attained at \u2016M\u2016\u2217 = 12 (\u2016U\u20162F + \u2016V \u20162F ), where U \u2208 Rd\u00d7r and V \u2208 Rd\u00d7r, r = rank(M), are formed from the reduced SVD ofM , i.e., U = Adiag(\u03c3) 1 2 and V = B diag(\u03c3) 1 2 whereM = Adiag(\u03c3)BT.\nFor a proof, see for instance (Mazumder et al., 2010, Section A.5).\nNow, we give a specialization of the above for symmetric matrices, based on the eigendecomposition instead of SVD.\nLemma 10 Variational formulation of nuclear norm based on eigendecomposition\n\u2016M\u2016\u2217 = min \u03bb,P\nM=P diag(\u03bb)PT\nk\u2211 s=1 |\u03bbs| \u2016ps\u20162 \u2200M \u2208 Sd 2 , (18)\nwhere k = rank(M). The minimum above is attained by the reduced eigendecomposition M = P diag(\u03bb)PT and \u2016M\u2016\u2217 = \u2016\u03bb\u20161.\nProof. LetA diag(\u03c3)BT and P diag(\u03bb)PT be the reduced SVD and eigendecomposition ofM \u2208 Sd2 , respectively. The relation between the SVD and the eigendecomposition is given by\n\u03c3s = |\u03bbs| as = sign(\u03bbs)ps\nbs = ps.\nFrom Lemma 9, we therefore obtain\nus = \u221a \u03c3sas = \u221a |\u03bbs| sign(\u03bbs)ps vs = \u221a \u03c3sbs = \u221a |\u03bbs|ps.\nNow, computing 12 ( \u2211 s \u2016us\u20162 + \u2016vs\u20162) gives \u2211k s=1 |\u03bbs| \u2016ps\u20162. The minimum value \u2016M\u2016\u2217 = \u2016\u03bb\u20161 follows from the fact that P is orthonormal and hence \u2016ps\u20162 = 1 \u2200s \u2208 [k]. We now have all the tools to prove our result. The equivalence between (12) and (14) when r = rank(M\u2217) is a special case of (Mazumder et al., 2010, Theorem 3). From Lemma 8, we know that the optimal solution of (14) is symmetric. This allows us to substitute (17) with (18), and therefore, (13) is equivalent to (14) with k = rank(M\u2217). As discussed in (Mazumder et al., 2010), the result also holds when r = k is larger than rank(M\u2217)."}, {"heading": "D. Efficient coordinate descent algorithms", "text": "D.1. Direct approach, K = Am for m \u2208 {2, 3} As stated in Theorem 1, the direct optimization objective is multi-convex whenK = Am. This allows us to easily minimize the objective by solving a succession of coordinate-wise convex problems. In this section, we develop an efficient algorithm for minimizing (13) withm \u2208 {2, 3}. It is easy to see that minimization w.r.t. \u03bb can be reduced to a standard `1-regularized convex objective via a simple change of variable. We therefore focus our attention to minimization w.r.t. P .\nAs a reminder, we want to minimize\nf := n\u2211 i=1 `(yi, y\u0302i) + \u03b2 k\u2211 s=1 |\u03bbs|\u2016ps\u20162\nwhere\ny\u0302i := k\u2211 s=1 \u03bbsAm(ps,xi).\nAfter routine calculation, we obtain\n\u2202A2(ps,xi) \u2202pjs = \u3008ps,xi\u3009xji \u2212 pjsx2ji = (\u3008ps,xi\u3009 \u2212 pjsxji)xji \u2202A3(ps,xi) \u2202pjs = 1 2 \u3008ps,xi\u30092xji \u2212 pjsx2ji\u3008ps,xi\u3009 \u2212 1 2 xjiD2(ps,xi) + p2jsx3ji\n= A2(ps,xi)xji \u2212 pjsx2ji\u3008ps,xi\u3009+ p2jsx3ji \u2202Am(ps,xi)\n\u2202p2js = 0 \u2200m \u2208 N\n\u2202y\u0302i \u2202pjs = \u03bbs \u2202Am(ps,xi)\n\u2202pjs \u2202y\u0302i \u2202p2js = 0 \u2200j \u2208 [d], s \u2208 [k].\nThe fact that the second derivative is null is a consequence of the multi-linearity of Am. Using the chain rule, we then obtain\n\u2202f\n\u2202pjs = n\u2211 i=1 `\u2032(yi, y\u0302i) \u2202y\u0302i \u2202pjs + 2\u03b2|\u03bbs|pjs\n\u2202f\n\u2202p2js = n\u2211 i=1\n[ `\u2032\u2032(yi, y\u0302i) ( \u2202y\u0302i \u2202pjs )2 + `\u2032(y\u0302i, yi) \u2202y\u0302i \u2202p2js ] + 2\u03b2|\u03bbs|\n= n\u2211 i=1 `\u2032\u2032(yi, y\u0302i) ( \u2202y\u0302i \u2202pjs )2 + 2\u03b2|\u03bbs|.\nAssuming that ` is \u00b5-smooth, its second derivative is upper-bounded by \u00b5 and therefore we have\n\u2202f\n\u2202p2js \u2264 \u03b7js where \u03b7js := \u00b5 n\u2211 i=1 ( \u2202y\u0302i \u2202pjs )2 + 2\u03b2|\u03bbs|.\nThen the update\npjs \u2190 pjs \u2212 \u03b7\u22121js \u2202f\n\u2202pjs\nguarantees that the objective value is monotonically decreasing except at the coordinate-wise minimum. Note that in the case of the squared loss `(y, y\u0302) = 12 (y \u2212 y\u0302)2, the above update is equivalent to a Newton step and is the exact minimizer of the coordinate-wise objective. An epoch consists in updating all variables once, for instance in cyclic order.\nFor an efficient implementation, we need to maintain y\u0302i \u2200i \u2208 [n] and statistics that depend on ps. For the former, we need O(n) memory. For the latter, we need O(kmn) memory for an implementation with full cache. However, this requirement is not realistic for a large training set. In practice, the memory requirement can be reduced to O(mn) if we recompute the quantities then sweep through p1s, . . . , pds for s fixed. Overall the cost of one epoch is O(knz(X)). A similar implementation technique is described for factorization machines with m = 2 in (Rendle, 2012).\nD.2. Lifted approach, K = Hm\nWe present an efficient coordinate descent solver for the lifted approach withK = Hm, for arbitrary integer m \u2265 2. Recall that our goal is to learn W = S(M) \u2208 Sdm by factorizing M \u2208 Rdm using m matrices of size d \u00d7 r. Let us call these\nAlgorithm 1 CD algorithm for direct obj. with K = A{2,3}\nInput: \u03bb, initial P , \u00b5-smooth loss function `, regularization parameter \u03b2, number of bases k, degree m, tolerance Pre-compute y\u0302i := y\u0302Am(xi;\u03bb,P ) \u2200i \u2208 [n] Set \u2206\u2190 0 for s := 1, . . . , k do\nPre-compute \u3008ps,xi\u3009 and A2(ps,xi) \u2200i \u2208 [n] for j := 1, . . . , d do\nCompute inv. step size \u03b7 := \u00b5 \u2211n\ni=1 ( \u2202y\u0302i \u2202pjs )2 + 2\u03b2|\u03bbs|\nCompute \u03b4 := \u03b7\u22121 [\u2211n\ni=1 ` \u2032(yi, y\u0302i) \u2202y\u0302i \u2202pjs\n+ 2\u03b2|\u03bbs|pjs ]\nUpdate pjs \u2190 pjs \u2212 \u03b4; Set \u2206\u2190 \u2206 + |\u03b4| Synchronize y\u0302i, \u3008ps,xi\u3009 and A2(ps,xi) \u2200i s.t. xji 6= 0\nend for end for If \u2206 \u2264 stop, otherwise repeat Output: P\nAlgorithm 2 CD algorithm for lifted objective with K = Hm\nInput: initial {U t}mt=1, \u00b5-smooth loss function `, regularization parameter \u03b2, rank r, degree m, tolerance Pre-compute y\u0302i := \u2211r s=1 \u220fm t=1\u3008u t s,xi\u3009 \u2200i \u2208 [n] Set \u2206\u2190 0 for t := 1, . . . ,m and s := 1, . . . , r do\nPre-compute \u03bei := \u220f t\u2032 6=t\u3008u t\u2032 s ,xi\u3009 \u2200i \u2208 [n] for j := 1, . . . , d do Compute inv. step size \u03b7 := \u00b5 \u2211n i=1 \u03be 2 i x 2 ji + \u03b2\nCompute \u03b4 := \u03b7\u22121 [\u2211n\ni=1 ` \u2032(yi, y\u0302i)\u03beixji + \u03b2u t js ] Update utjs \u2190 utjs \u2212 \u03b4; Set \u2206\u2190 \u2206 + |\u03b4| Synchronize y\u0302i \u2200i s.t. xji 6= 0\nend for end for If \u2206 \u2264 stop, otherwise repeat Output: {U t}mt=1\nmatricesU1, . . . ,Um and their columns uts = [u t 1s, . . . , u t ds] T with t \u2208 [m] and s \u2208 [r]. The decomposition of M can be expressed as a sum of rank-one tensors\nM = r\u2211 s=1 u1s \u2297 \u00b7 \u00b7 \u00b7 \u2297 ums .\nUsing (11) we obtain\ny\u0302i := \u3008W ,x\u2297mi \u3009 = \u3008M,x\u2297mi \u3009 = r\u2211 s=1 m\u220f t=1 \u3008uts,xi\u3009.\nThe first and second coordinate-wise derivatives are given by\n\u2202y\u0302i \u2202utjs = \u220f t\u2032 6=t \u3008ut\u2032s ,xi\u3009xji and \u2202y\u0302i \u2202(utjs) 2 = 0.\nWe consider the following regularized objective function\nf := n\u2211 i=1 `(yi, y\u0302i) + \u03b2 2 m\u2211 t=1 r\u2211 s=1 \u2016uts\u20162.\nUsing the chain rule, we obtain\n\u2202f\n\u2202utjs = n\u2211 i=1 `\u2032(yi, y\u0302i) \u2202y\u0302i \u2202utjs + \u03b2utjs and \u2202f \u2202(utjs) 2 = n\u2211 i=1 `\u2032\u2032(yi, y\u0302i) ( \u2202y\u0302i \u2202utjs )2 + \u03b2.\nAssuming that ` is \u00b5-smooth, its second derivative is upper-bounded by \u00b5 and therefore we have\n\u2202f\n\u2202(utjs) 2 \u2264 \u03b7tjs where \u03b7tjs := \u00b5 n\u2211 i=1 ( \u2202y\u0302i \u2202utjs )2 + \u03b2.\nThen the update\nutjs \u2190 utjs \u2212 (\u03b7tjs)\u22121 \u2202f\n\u2202utjs\nguarantees that the objective value is monotonically decreasing, except at the coordinate-wise minimum. Note that in the case of the squared loss `(y, y\u0302) = 12 (y \u2212 y\u0302)2, the above update is equivalent to a Newton step and is the exact minimizer of the coordinate-wise objective. An epoch consists in updating all variables once, for instance in cyclic order.\nFor an efficient implementation, the two quantities we need to maintain are y\u0302i \u2200i \u2208 [n] and \u220f t\u2032 6=t\u3008ut \u2032\ns ,xi\u3009 \u2200i \u2208 [n], s \u2208 [r], t \u2208 [m]. For the former, we need O(n) memory. For the latter, we need O(rmn) memory for an implementation with full cache. However, this requirement is not realistic for a large training set. In practice, the memory requirement can be reduced to O(mn) if we recompute the quantity then sweep through ut1s, . . . , u t ds for t and s fixed. Overall the cost of one epoch is O(mrnz(X)).\nD.3. Lifted approach, K = A2\nFor \u3008\u00b7, \u00b7\u3009>, efficient computations are more involved since we need to ignore irrelevant monomials. Nevertheless, we can also compute the predictions directly without explicitly symmetrizing the model. For m = 2, it suffices to subtract the effect of squared features. It is easy to verify that we then obtain\n\u3008S(UV T),x\u22972\u3009> = 1\n2\n[ \u3008UTx,V Tx\u3009 \u2212\nr\u2211 s=1 \u3008us \u25e6 x,vs \u25e6 x\u3009\n] ,\nwhere \u25e6 indicates element-wise product. The coordinate-wise derivatives are given by\n\u2202yi \u2202ujs = 1 2\n[ \u3008vs,x\u3009xji \u2212 vjsx2ji ] and\n\u2202yi \u2202vjs = 1 2\n[ \u3008us,x\u3009xji \u2212 ujsx2ji ] .\nGeneralizing this to arbitrary m is a future work."}, {"heading": "E. Datasets", "text": "For regression experiments, we used the following public datasets.\nDataset n (train) n (test) d Description abalone 3,132 1,045 8 Predict the age of abalones from physical measurements cadata 15,480 5,160 8 Predict housing prices from economic covariates\ncpusmall 6,144 2,048 12 Predict a computer system activity from system performance measures diabetes 331 111 10 Predict disease progression from baseline measurements\nE2006-tfidf 16,087 3,308 150,360 Predict volatility of stock returns from company financial reports\nThe diabetes dataset is available in scikit-learn (Pedregosa et al., 2011). Other datasets are available from http://www. csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/.\nFor recommender system experiments, we used the following two public datasets.\nDataset n d Movielens 1M 1,000,209 (ratings) 9,940 = 6,040 (users) + 3,900 (movies)\nLast.fm 108,437 (tag counts) 24,078 = 12,133 (artists) + 11,945 (tags)\nFor Movielens 1M, the task is to predict ratings between 1 and 5 given by users to movies, i.e., y \u2208 {1, . . . , 5}. For Last.fm, the task is to predict the number of times a tag was assigned to an artist, i.e., y \u2208 N. The design matrix X was constructed following (Rendle, 2010; 2012). Namely, for each rating yi, the corresponding xi is set to the concatenation of the one-hot encodings of the user and item indices. Hence the number of samples n is the number of ratings and the number of features is equal to the sum of the number of users and items. Each sample contains exactly two non-zero features. It is known that factorization machines are equivalent to matrix factorization when using this representation (Rendle, 2010; 2012).\nWe split samples uniformly at random between 75% for training and 25% for testing."}], "references": [{"title": "A new approach to collaborative filtering: Operator estimation with spectral regularization", "author": ["Abernethy", "Jacob", "Bach", "Francis", "Evgeniou", "Theodoros", "Vert", "Jean-Philippe"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Subspace embeddings for the polynomial kernel", "author": ["Avron", "Haim", "Nguyen", "Huy", "Woodruff", "David"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Avron et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Avron et al\\.", "year": 2014}, {"title": "Convex factorization machines", "author": ["Blondel", "Mathieu", "Fujino", "Akinori", "Ueada", "Naonori"], "venue": "In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD),", "citeRegEx": "Blondel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2015}, {"title": "Phase retrieval via matrix completion", "author": ["Cand\u00e8s", "Emmanuel J", "Eldar", "Yonina C", "Strohmer", "Thomas", "Voroninski", "Vladislav"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2013}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Chang", "Yin-Wen", "Hsieh", "Cho-Jui", "Kai-Wei", "Ringgaard", "Michael", "Lin", "Chih-Jen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Danqi", "Manning", "Christopher D"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Symmetric tensors and symmetric tensor rank", "author": ["Comon", "Pierre", "Golub", "Gene", "Lim", "Lek-Heng", "Mourrain", "Bernard"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Comon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Comon et al\\.", "year": 2008}, {"title": "Random feature maps for dot product kernels", "author": ["Kar", "Purushottam", "Karnick", "Harish"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Kar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2012}, {"title": "On the computational efficiency of training neural networks", "author": ["Livni", "Roi", "Shalev-Shwartz", "Shai", "Shamir", "Ohad"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Livni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Livni et al\\.", "year": 2014}, {"title": "Spectral regularization algorithms for learning large incomplete matrices", "author": ["Mazumder", "Rahul", "Hastie", "Trevor", "Tibshirani", "Robert"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mazumder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mazumder et al\\.", "year": 2010}, {"title": "Fast and scalable polynomial kernels via explicit feature maps", "author": ["Pham", "Ninh", "Pagh", "Rasmus"], "venue": "In Proceedings of the 19th KDD conference,", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Factorization machines", "author": ["Rendle", "Steffen"], "venue": "In Proceedings of International Conference on Data Mining,", "citeRegEx": "Rendle and Steffen.,? \\Q2010\\E", "shortCiteRegEx": "Rendle and Steffen.", "year": 2010}, {"title": "Factorization machines with libfm", "author": ["Rendle", "Steffen"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Rendle and Steffen.,? \\Q2012\\E", "shortCiteRegEx": "Rendle and Steffen.", "year": 2012}, {"title": "Kernel Methods for Pattern Analysis", "author": ["Shawe-Taylor", "John", "Cristianini", "Nello"], "venue": null, "citeRegEx": "Shawe.Taylor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor et al\\.", "year": 2004}, {"title": "Coffin: A computational framework for linear svms", "author": ["Sonnenburg", "S\u00f6ren", "Franc", "Vojtech"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2010}, {"title": "Support vector regression with anova decomposition kernels", "author": ["Stitson", "Mark", "Gammerman", "Alex", "Vapnik", "Vladimir", "Vovk", "Volodya", "Watkins", "Chris", "Weston", "Jason"], "venue": null, "citeRegEx": "Stitson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Stitson et al\\.", "year": 1997}, {"title": "Statistical learning theory", "author": ["Vapnik", "Vladimir"], "venue": null, "citeRegEx": "Vapnik and Vladimir.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Vladimir.", "year": 1998}, {"title": "Multi-class pegasos on a budget", "author": ["Z. Wang", "K. Crammer", "S. Vucetic"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML),", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["Williams", "Christopher K. I", "Seeger", "Matthias"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2001}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Yu", "Hsiang-Fu", "Hsieh", "Cho-Jui", "Si", "Dhillon", "Inderjit S"], "venue": "In ICDM,", "citeRegEx": "Yu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Although fast linear model solvers can be used (Chang et al., 2010; Sonnenburg & Franc, 2010), an obvious drawback of this kind of approach is that the number of parameters to estimate scales as O(d), where d is the number of features and m is the order of interactions considered.", "startOffset": 47, "endOffset": 93}, {"referenceID": 17, "context": "This is sometimes called the curse of kernelization (Wang et al., 2010).", "startOffset": 52, "endOffset": 71}, {"referenceID": 1, "context": "clude the Nystr\u00f6m method (Williams & Seeger, 2001), random features (Kar & Karnick, 2012) and sketching (Pham & Pagh, 2013; Avron et al., 2014).", "startOffset": 104, "endOffset": 143}, {"referenceID": 8, "context": "We show (Section 3) that choosing one kernel or the other allows us to recover polynomial networks (PNs) (Livni et al., 2014) and, surprisingly, factorization machines (FMs) (Rendle, 2010; 2012).", "startOffset": 105, "endOffset": 125}, {"referenceID": 8, "context": "Polynomial networks Polynomial networks (PNs) (Livni et al., 2014) of degree m = 2 predict the output y \u2208 R associated with x \u2208 R by \u0177PN(x;w,\u03bb,P ) := \u3008w,x\u3009+ \u3008\u03c3(Px),\u03bb\u3009, (2) where w \u2208 R, P \u2208 Rd\u00d7k, \u03bb \u2208 R and \u03c3(u) := u is evaluated element-wise.", "startOffset": 46, "endOffset": 66}, {"referenceID": 8, "context": "Polynomial networks Polynomial networks (PNs) (Livni et al., 2014) of degree m = 2 predict the output y \u2208 R associated with x \u2208 R by \u0177PN(x;w,\u03bb,P ) := \u3008w,x\u3009+ \u3008\u03c3(Px),\u03bb\u3009, (2) where w \u2208 R, P \u2208 Rd\u00d7k, \u03bb \u2208 R and \u03c3(u) := u is evaluated element-wise. Intuitively, the right-hand term can be interpreted as a feedforward neural network with one hidden layer of k units and with activation function \u03c3(u). Livni et al. (2014) also extend (2) to the case m = 3 and show theoretically that PNs can approximate feedforward networks with sigmoidal activation.", "startOffset": 47, "endOffset": 414}, {"referenceID": 15, "context": "A much lesser known kernel is the ANOVA kernel (Stitson et al., 1997; Vapnik, 1998).", "startOffset": 47, "endOffset": 83}, {"referenceID": 2, "context": "In (Blondel et al., 2015), for m = 2, it was proposed to cast parameter estimation as a low-rank symmetric matrix estimation problem.", "startOffset": 3, "endOffset": 25}, {"referenceID": 3, "context": "A similar idea was used in the context of phase retrieval in (Cand\u00e8s et al., 2013).", "startOffset": 61, "endOffset": 82}, {"referenceID": 6, "context": "Lemma 5 Link between tensors and kernel expansions Let W \u2208 Sdm have a symmetric outer product decomposition (Comon et al., 2008)", "startOffset": 108, "endOffset": 128}, {"referenceID": 3, "context": "Following (Cand\u00e8s et al., 2013), we call this approach lifted.", "startOffset": 10, "endOffset": 31}, {"referenceID": 0, "context": "In addition to Theorem 2, from (Abernethy et al., 2009), we also know that every local minimum U ,V of (12) gives a global solution UV T of (14) provided that rank(M\u2217) \u2264 r.", "startOffset": 31, "endOffset": 55}, {"referenceID": 19, "context": ", Yu et al. (2012)).", "startOffset": 2, "endOffset": 19}, {"referenceID": 8, "context": "Conclusion In this paper, we revisited polynomial networks (Livni et al., 2014) and factorization machines (Rendle, 2010; 2012) from a unified perspective.", "startOffset": 59, "endOffset": 79}], "year": 2016, "abstractText": "Polynomial networks and factorization machines are two recently-proposed models that can efficiently use feature interactions in classification and regression tasks. In this paper, we revisit both models from a unified perspective. Based on this new view, we study the properties of both models and propose new efficient training algorithms. Key to our approach is to cast parameter learning as a low-rank symmetric tensor estimation problem, which we solve by multi-convex optimization. We demonstrate our approach on regression and recommender system tasks.", "creator": "LaTeX with hyperref package"}}}