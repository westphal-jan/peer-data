{"id": "1608.04042", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2016", "title": "Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?", "abstract": "previous studies have proposed image - based clutter measures that correlate with human search times and / 6 or eye movements. however, most models don't take into account the fact that the parasitic effects of clutter deprivation interact appropriately with the foveated nature of the human visual system : visual clutter further from the fovea has an increasing detrimental influence on perception. here, we introduce a new parallel foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. we use feature congestion ( rosenholtz et al. ) as our non foveated clutter spatial model, and we stack a peripheral architecture on top of constant feature congestion for our foveated model. we introduce the peripheral integration feature congestion ( pifc ) coefficient, as a fundamental ingredient of our model that modulates clutter as a non - linear processing gain contingent on eccentricity. we finally show that foveated feature congestion ( ffc ) clutter scores inflammation r ( 44 ) = - 0. 82 correlate better with target detection ( hit rate ) than regular feature congestion r ( 44 ) = - 0. 19 in forced fixation search. thus, our model allows us to enrich proper clutter perception research by computing fixation specific clutter maps. a toolbox for creating peripheral architectures : piranhas : peripheral architectures for natural, hybrid and artificial systems will be made available.", "histories": [["v1", "Sun, 14 Aug 2016 01:07:29 GMT  (6782kb,D)", "http://arxiv.org/abs/1608.04042v1", "Pre-Print to be presented at NIPS 2016 in Barcelona, Spain"]], "COMMENTS": "Pre-Print to be presented at NIPS 2016 in Barcelona, Spain", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.HC", "authors": ["arturo deza", "miguel p eckstein"], "accepted": true, "id": "1608.04042"}, "pdf": {"name": "1608.04042.pdf", "metadata": {"source": "CRF", "title": "Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?", "authors": ["Arturo Deza", "Miguel P. Eckstein"], "emails": ["deza@dyns.ucsb.edu", "eckstein@psych.ucsb.edu"], "sections": [{"heading": "1 Introduction", "text": "What is clutter? While it seems easy to make sense of a cluttered desk vs an uncluttered desk at a glance, it is hard to quantify clutter with a number. Is a cluttered desk, one stacked with papers? Or is an uncluttered desk, one that is more organized irrelevant of number of items? An important goal in clutter research has been to develop an image based computational model that outputs a quantitative measure that correlates with human perceptual behavior [19, 12, 23]. Previous studies have created models that output global or regional metrics to measure clutter perception. Such measures are aimed to predict the influence of clutter on perception. However, one important aspect of human visual perception is that it is not space invariant: the fovea processes visual information with high spatial detail while regions away from the central fovea have access to lower spatial detail. Thus, the influence of clutter on perception can depend on the retinal location of the stimulus and such influences will likely interact with the information content in the stimulus.\nThe goal of the current paper is to develop a foveated clutter model that can successfully predict the interaction between retinal eccentricity and image content in modulating the influence of clutter on perceptual behavior. We introduce a foveated mechanism based on the peripheral architecture proposed by Freeman and Simoncelli [9] and stack it into a current clutter model (Feature Congestion [22, 23]) to generate a clutter map that arises from a calculation of information loss with retinal eccentricity but is multiplicatively modulated by the original unfoveated clutter score. The new\n1https://github.com/ArturoDeza/Piranhas\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 8.\n04 04\n2v 1\n[ cs\n.C V\n] 1\n4 A\nug 2\nmeasure is evaluated in a gaze-contingent psychophysical experiment measuring target detection in complex scenes as a function of target retinal eccentricity. We show that the foveated clutter models that account for loss of information in the periphery correlates better with human target detection (hit rate) across retinal eccentricities than non-foveated models. Although the model is presented in the context of Feature Congestion, the framework can be extended to any previous or future clutter metrics that produce clutter scores that are computed from a global pixel-wise clutter map."}, {"heading": "2 Previous Work", "text": "Previous studies have developed general measures of clutter computed for an entire image and do not consider the space-variant properties of the human visual system. Because our work seeks to model and assess the interaction between clutter and retinal location, experiments manipulating the eccentricity of a target while observers hold fixation (gaze contingent forced fixation) are most appropriate to evaluate the model. To our knowledge there has been no systematic evaluation of fixation dependent clutter models with forced fixation target detection in scenes. In this section, we will give an overview of state-of-the-art clutter models, metrics and evaluations."}, {"heading": "2.1 Clutter Models", "text": "Feature Congestion: Feature Congestion, initially proposed by [22, 23] produces both a pixel-wise clutter score map as a well as a global clutter score for any input image or Region of Interest (ROI). Each clutter map is computed by combining a Color map in CIELab space, an orientation map [14], and a local contrast map at multiple scales through Gaussian Pyramids [5]. One of the main advantages Feature Congestion has is that each pixel-wise clutter score (Fig. 1) and global score can be computed in less than a second. Furthermore, this is one of the few models that can output a specific clutter score for any pixel or ROI in an image. This will be crucial for developing a foveated model as explained in Section 4. Edge Density: Edge Density computes a ratio after applying an Edge Detector on the input image [19]. The final clutter score is the ratio of edges to total number of pixels present in the image. The intuition for this metric is straightforward: \u201cthe more edges, the more clutter\u201d (due to objects for example). Subband Entropy: The Subband Entropy model begins by computing N steerable pyramids [24] at K orientations across each channel from the input image in CIELab color space. Once each N \u00d7K subband is collected for each channel, the entropy for each oriented pyramid is computed pixelwise and they are averaged separately. Thus, Subband Entropy wishes to measure the entropy of each spatial frequency and oriented filter response of an image. Scale Invariance: The Scale Invariant Clutter Model proposed by Farid and Bravo [4] uses graphbased segmentation [8] at multiple k scales. A scale invariant clutter representation is given by the power law coefficient that matches the decay of number of regions with the adjusted scale parameter.\nProtoObject Segmentation: ProtoObject Segmentation proposes an unsupervised metric for clutter scoring [26, 27]. The model begins by converting the image into HSV color space, and then proceeds to segment the image through superpixel segmentation [17, 16, 1]. After segmentation, mean-shift [11] is applied on all cluster (superpixel) medians to calculate the final amount of representative colors present in the image. Next, superpixels are merged with one another contingent on them being adjacent, and being assigned to the same mean-shift HSV cluster. The final score is a ratio between initial number of superpixels and final number of superpixels. Crowding Model: The Crowding Model developed by van der Berg et al. [25] is the only model to have used losses in the periphery due to crowding as a clutter metric. It decomposes the image into 3 different scales in CIELab color space. It then produces 6 different orientation maps for each scale given the luminance channel; a contrast map is also obtained by difference of Gaussians on the previously mentioned channel. All feature maps are then pooled with Gaussian kernels that grow linearly with eccentricity, KL-divergence is then computed between the pre and post pooling feature maps to get information loss coefficients, all coefficients are averaged together to produce a final clutter score. We will discuss the differences of this model to ours in the Discussion (Section 5). Texture Tiling Model: The Texture Tiling Model (TTM) is a recent perceptual model that accounts for losses in the periphery [21, 13] through psyhophysical experiments modelling visual search [7]: feature search, conjunction search, configuration search and asymmetric search. In essence, the Mongrels proposed by Rosenholtz et al. that simulate peripheral losses are very similar to the Metamers proposed by Freeman & Simoncelli [9]. We do not include comparisons to the TTM model since it requires additional psychophysics on the Mongrel versions of the images."}, {"heading": "2.2 Clutter Metrics", "text": "Global Clutter Score: The most basic clutter metric used in clutter research is the original clutter score that every model computes over the entire image. Edge Density & Proto-Object Segmentation output a ratio, while Subband Entropy and Feature Congestion output a score. However, Feature Congestion is the only model that outputs a dense pixelwise clutter map before computing a global score (Fig. 1). Thus, we use Feature Congestion clutter maps for our foveated clutter model. Clutter ROI: The second most used clutter metric is ROI (Region of Interest)-based, as shown in the work of Asher et al. [3]. This metric is of interest when an observer is engaging in target search, vs making a human judgement (Ex: \u201crate the clutter of the following scenes\u201d)."}, {"heading": "2.3 Clutter Evaluations", "text": "Human Clutter Judgements: Multiple studies of clutter, correlate their metrics with rankings/ratings of clutter provided by human participants. Ideally, if clutter model A is better than clutter model B, then the correlation of model scores and human rankings/ratings should be higher for model A than for model B. [27, 19, 25]\nResponse Time: Highly cluttered images will require more time for target search, hence more time to arrive to a decision of target present/absent. Under the previous assumption, a high correlation value between response time and clutter score are a good sign for a clutter model. [23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image. [23, 3, 12]"}, {"heading": "3 Methods & Experiments", "text": ""}, {"heading": "3.1 Experiment 1: Forced Fixation Search", "text": "A total of 13 subjects participated in a Forced Fixation Search experiment where the goal was to detect a target in the subject\u2019s periphery and identify if there was a target (person) present or absent. Participants had variable amounts of time (100, 200, 400, 900, 1600 ms) to view each clip that was presented in a random order at a variable degree of eccentricities that the subjects were not aware of (1 deg, 4 deg, 9 deg, 15 deg). They were then prompted with a Target Detection rating scale where they had to rate from a scale from 1-10 by clicking on a number reporting how confident they were on detecting the target. Participants have unlimited time for making their judgements, and they did not take more than 10 seconds per judgment. There was no response feedback after each trial. Trials were aborted when subjects broke fixation outside of a 1 deg radius around the fixation cross.\nEach subject did 12 sessions that consisted of 360 unique images. Every session also presented the images with aerial viewpoints from different vantage points (Example: session 1 had the target at 12 o\u2019clock, while session 2 had the target at 3 o\u2019clock). To control for any fixational biases, all subjects had a unique fixation point for every trial for the same eccentricity values. All images were rendered with variable levels of clutter. Each session took about an hour to complete. The target was of size 0.5 deg\u00d70.5 deg, 1 deg\u00d71 deg, 1.5 deg\u00d71.5 deg, depending on zoom level. For our analysis, we only used the low zoom and 100 ms time condition since there was less ceiling effects across all eccentricities.\nStimuli Creation: A total of 273 videos were created each with a total duration of 120 seconds, where a \u2018birds eye\u2019 point-of-view camera rotated slowly around the center. While the video was in rotating motion, there was no relative motion between any parts of the video. From the original videos, a total of 360\u00d7 4 different clips were created. Half of the clips were target present, while the other half were target absent. These short and slowly rotating clips were used instead of still images in our experiment, to simulate slow real movement from a pilot point of view. All clips were shown to participants in random order.\nApparatus: An EyeLink 1000 system (SR Research) was used to collect Eye Tracking data at a frequency of 1000Hz. Each participant was at a distance of 76 cm from a LCD screen on gamma display, so that each pixel subtended a visual angle of 0.022 deg /px. All video clips were rendered at 1024\u00d7 760 pixels (22.5 deg\u00d716.7 deg) and a frame rate of 24fps. Eye movements with velocity over 22 deg /s and acceleration over 4000 deg /s2 were qualified as saccades. Every trial began with a fixation cross, where each subject had to fixate the cross with a tolerance of 1 deg."}, {"heading": "4 Foveated Feature Congestion", "text": "A regular Feature Congestion clutter score is computed by taking the mean of the Feature Congestion map of the image or of a target ROI [12]. We propose a Foveated Feature Congestion (FFC) model that outputs a score which takes into account two main terms: 1) a regular Feature Congestion (FC) score and 2) a Peripheral Integration Feature Congestion (PIFC) coefficient that accounts the lower spatial resolution of the visual periphery that are detrimental for target detection. The first term is independent of fixation, while the second term will act as a non-linear gain that will either reduce or amplify the clutter score depending on fixation distance from the target.\nIn this Section we will explain how to compute a PIFC, which will require creating a human-like peripheral architecture as explained in Section 4.1. We then present our Foveated Feature Congestion (FFC) clutter model in Section 4.2. Finally, we conclude by making a quantiative evaluation of the FFC (Section 4.3) in its ability to predict variations of target detectability across images and retinal eccentricity of the target."}, {"heading": "4.1 Creating a Peripheral Architecture", "text": "We used the Piranhas Toolkit [6] to create a Freeman and Simoncelli [9] peripheral architecture. This biologically inspired model has been tested and used to model V1 and V2 responses in human and non-human primates with high precision for a variety of tasks [20, 10, 18, 2]. It is described by a set of pooling (linear) regions that increase in size with retinal eccentricity. Each pooling region is separable with respect to polar angle hn(\u03b8) and log eccentricity gn(e), as described in Eq. 2 and Eq. 3 respectively. These functions are multiplied for every angle and eccentricity (\u03b8, e) and are plotted in log polar coordinates to create the peripheral architecture as seen in Fig. 3.\nf(x) =  cos2(\u03c02 ( x\u2212(t0\u22121)/2 t0\n)); (\u22121 + t0)/2 < x \u2264 (t0 \u2212 1)/2 1; (t0 \u2212 1)/2 < x \u2264 (1\u2212 t0)/2 \u2212cos2(\u03c02 ( x\u2212(1+t0)/2 t0 )) + 1; (1\u2212 t0)/2 < x \u2264 (1 + t0)/2 (1)\nhn(\u03b8) = f (\u03b8 \u2212 (w\u03b8n+ w\u03b82 )\nw\u03b8\n) ;w\u03b8 = 2\u03c0\nN\u03b8 ;n = 0, ..., N\u03b8 \u2212 1 (2)\ngn(e) = f ( log(e)\u2212 [log(e0) + we(n+ 1)]\nwe\n) ;we =\nlog(er)\u2212 log(e0) Ne ;n = 0, ..., Ne \u2212 1 (3)\nThe parameters we used match a V1 architecture with a scale of s = 0.25 , a visual radius of er = 24deg, a fovea of 2 deg, with e0 = 0.25 deg 2, and t0 = 1/2. The scale defines the number of eccentricities Ne, as well as the number of polar pooling regions N\u03b8 from \u30080, 2\u03c0]. Although observers saw the original stimuli at 0.022 deg/pixel, with image size 1024 \u00d7 760; for modelling purposes: we rescaled all images to half their size so the peripheral architecture could fit all images under any fixation point. To preserve stimuli size in degrees after rescaling our images, our foveal model used an input value of 0.044 deg/pixel (twice the value of experimental settings). Resizing the image to half its size also allows the peripheral architecture to consume less CPU computation time and memory."}, {"heading": "4.2 Creating a Foveated Feature Congestion Model", "text": "Intuitively, a foveated clutter model that takes into account target search should score very low when the target is in the fovea (near zero), and very high when the target is in the periphery. Thus, an observer should find a target without difficulty, achieving a near perfect hit rate in the fovea, yet the observer should have a lower hit rate in the periphery given crowding effects. Note that in the\n2We remove regions with a radius smaller than the foveal radius, since there is no pooling in the fovea.\nperiphery, not only should it be harder to detect a target, but it is also likely to confuse the target with another object or region affine in shape, size, texture and/or pixel value (false alarms). Under this assumption, we wish to modulate a clutter score (Feature Congestion) by a multiplicative factor, given the target and fixation location. We call this multiplicative term: the PIFC coefficient, which is defined over a 6 deg\u00d76 deg ROI around the location of target t. The target itself was removed when processing the clutter maps since it indirectly contributes to the ROI clutter score [3]. The PIFC aims at quantifying the information loss around the target region due to peripheral processing.\nTo compute the PIFC, we use the before mentioned ROI, and calculate a mean difference from the foveated clutter map with respect to the original non-foveated clutter map. If the target is foveated, there should be little to no difference between a foveated map and the original map, thus setting the PIFC coefficient value to near zero. However, as the target is farther away from the fovea, the PIFC coefficient should be higher given pooling effects in the periphery. To create a foveated map, we use Feature Congestion and apply max pooling on each pooling region after the peripheral architecture has been stacked on top of the Feature Congestion map. Note that the FFC map values will depend on the fixation location as shown in Fig. 4. The PIFC map is the result of subtracting the foveated map from the unfoveated map in the ROI, and the score is a mean distance value between these two maps (we use L1-norm, L2-norm or KL-divergence). Computational details can be seen in Algorithm 1. Thus, we can resume our model in Eq. 4:\nFFCf,tI = FCI \u00d7 PIFC f ROI(t) (4)\nwhere FCI is the Feature Congestion score [23] of image I which is computed by the mean of the Feature Congestion map RFC , and FFC f,t I is the Foveated Feature Congestion score of the image I , depending on the point of fixation f and the location of the target t."}, {"heading": "4.3 Foveated Feature Congestion Evaluation", "text": "A visualization of each image and its respective Hit Rate vs Clutter Score across both foveated and unfoveated models can be visualized in Fig 5. Qualitatively, it shows the importance of a PIFC weighting term to the total image clutter score when performing our forced fixation search experiment. Futhermore, a quantitative bootstrap correlation analysis comparing classic metrics (Image, Target, ROI) against foveal metrics (FFC1, FFC2 and FFC3) shows that hit rate vs clutter scores are greater for those foveated models with a PIFC: Image: (r(44) = \u22120.19 \u00b1 0.13, p = 0.0774), Target: (r(44) = \u22120.03\u00b1 0.14, p = 0.4204), ROI: (r(44) = \u22120.25\u00b1 0.14, p = 0.0392), FFC1 (L1-norm):\nAlgorithm 1 Computation of Peripheral Integration Feature Congestion (PIFC) Coefficient 1: procedure COMPUTE PIFC OF ROI OF IMAGE I ON FIXATION f 2: Create a Peripheral Architecture A : (N\u03b8, Ne) 3: Offset image I in Peripheral Architecture by fixation f : (fx, fy). 4: Compute Regular Feature Congestion (RFC) map of image I 5: Set Peripheral Feature Congestion (P fFC) \u2282 R 2 + map to zero.\n6: Copy Feature Congestion values in fovea r0: P fFC(r0) = (RFC(r0)) 7: for each pooling region ri overlapping I , s.t. 1 \u2264 i \u2264 N\u03b8 \u00d7Ne do 8: Get Regular Feature Congestion (FC) values in ri 9: Set Peripheral FC value to max Regular FC value: P fFC(ri) = max(RFC(ri))\n10: end for 11: Crop PIFC map to ROI: pfFC = P f FC(ROI) 12: Crop FC map to ROI: rFC = RFC(ROI) 13: Choose Distance metric D between rFC and pfFC map 14: Compute Coefficient = mean(D(rFC , pfFC)) 15: return Coefficient 16: end procedure\n(r(44) = \u22120.82\u00b10.04, p < 0.0001), FFC2 (L2-norm): (r(44) = \u22120.79\u00b10.06, p < 0.0001), FFC3 (KL-divergence): (r(44) = \u22120.82\u00b1 0.04, p < 0.0001). Notice that there is no difference in correlations between using the L1-norm, L2-norm or KLdivergence distance for each model in terms of the correlation with hit rate. Table 1(Supp. Mat.) also shows the highest correlation with a 6 \u00d7 6 deg ROI window across all metrics. Note that the same analysis can not be applied to false alarms, since it is indistinguishable to separate a false alarm at 1 deg from 15 deg (the target is not present, so there is no real eccentricity away from fixation). However as mentioned in the Methods section, fixation location for target absent trials in the experiment were placed assuming a location from its matching target present image. It is important that target present and absent fixations have the same distributions for each eccentricity."}, {"heading": "5 Discussion", "text": "In general, images that have low Feature Congestion have less gain in PIFC coefficients as eccentricity increases. While images with high clutter have higher gain in PIFC coefficients. Consequently, the difference of FFC between different images increases nonlinearly with eccentricity, as observed in Fig. 6. This is our main contribution, as these differences in clutter score as a function of eccentricity do not exist for regular Feature Congestion, and these differences in scores should be able to correlate with human performance in target detection.\nOur model is also different from the van der Berg et al. [25] model since our peripheral architecture uses: a biologically inspired peripheral architecture with log polar regions that provide anisotropic pooling [15] rather than isotropic gaussian pooling as a linear function of eccentricity [25]; we used region-based max pooling for each final feature map instead of pixel-based mean pooling (gaussians) per each scale (which allows for stronger differences); this final difference also makes our model computationally more efficient running at 700ms per image, vs 180s per image for the Crowding model (\u00d7250 speed up). A home-brewed Crowding Model applied to our forced fixation experiment resulted in a correlation of (r(44) = \u22120.23\u00b1 0.13, p = 0.0469), equivalent to using a non foveated metric such as regular Feature Congestion (r(44) = \u22120.19\u00b1 0.13, p = 0.0774). We finally extended our model to create foveated(FoV) versions of Edge Density(ED) [19], Subband Entropy(SE) [24, 23] and ProtoObject Segmentation(PS) [27] showing that correlations for all foveated versions are stronger than non-foveated versions for the same task: rED = \u22120.21, rED+FoV = \u22120.76, rSE = \u22120.19, rSE+FoV = \u22120.77, rPS = \u22120.30, but rPS+FoV = \u22120.74. Note that the highest foveated correlation is FC: rFC+FoV = \u22120.82, despite rFC = \u22120.19 under a L1-norm loss of the PIFC. Feature Congestion has a dense representation, is more bio-inspired than the other models, and outperforms in the periphery. See Figure 7. An overview of creating dense and foveated versions for previously mentioned models can be seen in the Supp. Material."}, {"heading": "6 Conclusion", "text": "In this paper we have introduced a peripheral architecture that shows detrimental effects of different eccentricities on target detection, that helps us model clutter for forced fixation experiments. We introduced a forced fixation experimental design for clutter research; we defined a biologically inspired peripheral architecture that pools features in V1; and we stacked the previously mentioned peripheral architecture on top of a Feature Congestion map to create a Foveated Feature Congestion (FFC) model \u2013 and we extended this pipeline to other clutter models. We showed that the FFC model better explains loss in target detection performance as a function of eccentricity through the introduction of the Peripheral Integration Feature Congestion (PIFC) coefficient which varies non linearly."}, {"heading": "Acknowledgements", "text": "We would like to thank Miguel Lago and Aditya Jonnalagadda for useful proof-reads and revisions, as well as Mordechai Juni, N.C. Puneeth, and Emre Akbas for useful suggestions. This work was supported by the Institute for Collaborative Biotechnologies through grant 2 W911NF-09-0001 from the U.S. Army Research Office."}], "references": [{"title": "Slic superpixels", "author": ["R. Achanta", "A. Shaji", "K. Smith", "A. Lucchi", "P. Fua", "S. S\u00fcsstrunk"], "venue": "Technical report", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Object detection through exploration with a foveated visual field", "author": ["E. Akbas", "M.P. Eckstein"], "venue": "arXiv preprint arXiv:1408.0814", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Regional effects of clutter on human target detection performance", "author": ["M.F. Asher", "D.J. Tolhurst", "T. Troscianko", "I.D. Gilchrist"], "venue": "Journal of vision, 13(5):25\u201325", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "A scale invariant measure of clutter", "author": ["M.J. Bravo", "H. Farid"], "venue": "Journal of Vision, 8(1):23\u201323", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "The laplacian pyramid as a compact image code", "author": ["P.J. Burt", "E.H. Adelson"], "venue": "Communications, IEEE Transactions on, 31(4):532\u2013540", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1983}, {"title": "Visual search: A retrospective", "author": ["M.P. Eckstein"], "venue": "Journal of Vision, 11(5):14\u201314", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient graph-based image segmentation", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "International Journal of Computer Vision, 59(2):167\u2013181", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Metamers of the ventral stream", "author": ["J. Freeman", "E.P. Simoncelli"], "venue": "Nature neuroscience, 14(9):1195\u20131201", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "A functional and perceptual signature of the second visual area in primates", "author": ["J. Freeman", "C.M. Ziemba", "D.J. Heeger", "E.P. Simoncelli", "J.A. Movshon"], "venue": "Nature neuroscience, 16(7):974\u2013981", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "The estimation of the gradient of a density function", "author": ["K. Fukunaga", "L.D. Hostetler"], "venue": "with applications in pattern recognition. Information Theory, IEEE Transactions on, 21(1):32\u201340", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1975}, {"title": "The influence of clutter on real-world scene search: Evidence from search efficiency and eye movements", "author": ["J.M. Henderson", "M. Chanceaux", "T.J. Smith"], "venue": "Journal of Vision, 9(1):32\u201332", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Pooling of continuous features provides a unifying account of crowding", "author": ["S. Keshvari", "R. Rosenholtz"], "venue": "Journal of Vision, 16(39)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Texture segregation and orientation gradient", "author": ["M.S. Landy", "J.R. Bergen"], "venue": "Vision research, 31(4):679\u2013691", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1991}, {"title": "Visual crowding", "author": ["D.M. Levi"], "venue": "Current Biology, 21(18):R678\u2013R679", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Turbopixels: Fast superpixels using geometric flows", "author": ["A. Levinshtein", "A. Stere", "K.N. Kutulakos", "D.J. Fleet", "S.J. Dickinson", "K. Siddiqi"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(12):2290\u20132297", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Entropy rate superpixel segmentation", "author": ["M.-Y. Liu", "O. Tuzel", "S. Ramalingam", "R. Chellappa"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 2097\u20132104. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Representation of naturalistic image structure in the primate visual cortex", "author": ["J.A. Movshon", "E.P. Simoncelli"], "venue": "Cold Spring Harbor symposia on quantitative biology, volume 79, pages 115\u2013122. Cold Spring Harbor Laboratory Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["J. Portilla", "E.P. Simoncelli"], "venue": "International Journal of Computer Vision, 40(1):49\u201370", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "A summary statistic representation in peripheral vision explains visual search", "author": ["R. Rosenholtz", "J. Huang", "A. Raj", "B.J. Balas", "L. Ilie"], "venue": "Journal of vision, 12(4):14\u201314", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature congestion: a measure of display clutter", "author": ["R. Rosenholtz", "Y. Li", "J. Mansfield", "Z. Jin"], "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems, pages 761\u2013770. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Measuring visual clutter", "author": ["R. Rosenholtz", "Y. Li", "L. Nakano"], "venue": "Journal of vision, 7(2):17\u201317", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "The steerable pyramid: A flexible architecture for multi-scale derivative computation", "author": ["E.P. Simoncelli", "W.T. Freeman"], "venue": "icip, page 3444. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "A crowding model of visual clutter", "author": ["R. van den Berg", "F.W. Cornelissen", "J.B. Roerdink"], "venue": "Journal of Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Modeling clutter perception using parametric protoobject partitioning", "author": ["C.-P. Yu", "W.-Y. Hua", "D. Samaras", "G. Zelinsky"], "venue": "Advances in Neural Information Processing Systems, pages 118\u2013126", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Modeling visual clutter perception using proto-object segmentation", "author": ["C.-P. Yu", "D. Samaras", "G.J. Zelinsky"], "venue": "Journal of vision, 14", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 10, "context": "Is a cluttered desk, one stacked with papers? Or is an uncluttered desk, one that is more organized irrelevant of number of items? An important goal in clutter research has been to develop an image based computational model that outputs a quantitative measure that correlates with human perceptual behavior [19, 12, 23].", "startOffset": 307, "endOffset": 319}, {"referenceID": 20, "context": "Is a cluttered desk, one stacked with papers? Or is an uncluttered desk, one that is more organized irrelevant of number of items? An important goal in clutter research has been to develop an image based computational model that outputs a quantitative measure that correlates with human perceptual behavior [19, 12, 23].", "startOffset": 307, "endOffset": 319}, {"referenceID": 7, "context": "We introduce a foveated mechanism based on the peripheral architecture proposed by Freeman and Simoncelli [9] and stack it into a current clutter model (Feature Congestion [22, 23]) to generate a clutter map that arises from a calculation of information loss with retinal eccentricity but is multiplicatively modulated by the original unfoveated clutter score.", "startOffset": 106, "endOffset": 109}, {"referenceID": 19, "context": "We introduce a foveated mechanism based on the peripheral architecture proposed by Freeman and Simoncelli [9] and stack it into a current clutter model (Feature Congestion [22, 23]) to generate a clutter map that arises from a calculation of information loss with retinal eccentricity but is multiplicatively modulated by the original unfoveated clutter score.", "startOffset": 172, "endOffset": 180}, {"referenceID": 20, "context": "We introduce a foveated mechanism based on the peripheral architecture proposed by Freeman and Simoncelli [9] and stack it into a current clutter model (Feature Congestion [22, 23]) to generate a clutter map that arises from a calculation of information loss with retinal eccentricity but is multiplicatively modulated by the original unfoveated clutter score.", "startOffset": 172, "endOffset": 180}, {"referenceID": 20, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Feature Congestion: Feature Congestion, initially proposed by [22, 23] produces both a pixel-wise clutter score map as a well as a global clutter score for any input image or Region of Interest (ROI).", "startOffset": 62, "endOffset": 70}, {"referenceID": 20, "context": "Feature Congestion: Feature Congestion, initially proposed by [22, 23] produces both a pixel-wise clutter score map as a well as a global clutter score for any input image or Region of Interest (ROI).", "startOffset": 62, "endOffset": 70}, {"referenceID": 12, "context": "Each clutter map is computed by combining a Color map in CIELab space, an orientation map [14], and a local contrast map at multiple scales through Gaussian Pyramids [5].", "startOffset": 90, "endOffset": 94}, {"referenceID": 4, "context": "Each clutter map is computed by combining a Color map in CIELab space, an orientation map [14], and a local contrast map at multiple scales through Gaussian Pyramids [5].", "startOffset": 166, "endOffset": 169}, {"referenceID": 21, "context": "Subband Entropy: The Subband Entropy model begins by computing N steerable pyramids [24] at K orientations across each channel from the input image in CIELab color space.", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "Scale Invariance: The Scale Invariant Clutter Model proposed by Farid and Bravo [4] uses graphbased segmentation [8] at multiple k scales.", "startOffset": 80, "endOffset": 83}, {"referenceID": 6, "context": "Scale Invariance: The Scale Invariant Clutter Model proposed by Farid and Bravo [4] uses graphbased segmentation [8] at multiple k scales.", "startOffset": 113, "endOffset": 116}, {"referenceID": 23, "context": "ProtoObject Segmentation: ProtoObject Segmentation proposes an unsupervised metric for clutter scoring [26, 27].", "startOffset": 103, "endOffset": 111}, {"referenceID": 24, "context": "ProtoObject Segmentation: ProtoObject Segmentation proposes an unsupervised metric for clutter scoring [26, 27].", "startOffset": 103, "endOffset": 111}, {"referenceID": 15, "context": "The model begins by converting the image into HSV color space, and then proceeds to segment the image through superpixel segmentation [17, 16, 1].", "startOffset": 134, "endOffset": 145}, {"referenceID": 14, "context": "The model begins by converting the image into HSV color space, and then proceeds to segment the image through superpixel segmentation [17, 16, 1].", "startOffset": 134, "endOffset": 145}, {"referenceID": 0, "context": "The model begins by converting the image into HSV color space, and then proceeds to segment the image through superpixel segmentation [17, 16, 1].", "startOffset": 134, "endOffset": 145}, {"referenceID": 9, "context": "After segmentation, mean-shift [11] is applied on all cluster (superpixel) medians to calculate the final amount of representative colors present in the image.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "[25] is the only model to have used losses in the periphery due to crowding as a clutter metric.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Texture Tiling Model: The Texture Tiling Model (TTM) is a recent perceptual model that accounts for losses in the periphery [21, 13] through psyhophysical experiments modelling visual search [7]: feature search, conjunction search, configuration search and asymmetric search.", "startOffset": 124, "endOffset": 132}, {"referenceID": 11, "context": "Texture Tiling Model: The Texture Tiling Model (TTM) is a recent perceptual model that accounts for losses in the periphery [21, 13] through psyhophysical experiments modelling visual search [7]: feature search, conjunction search, configuration search and asymmetric search.", "startOffset": 124, "endOffset": 132}, {"referenceID": 5, "context": "Texture Tiling Model: The Texture Tiling Model (TTM) is a recent perceptual model that accounts for losses in the periphery [21, 13] through psyhophysical experiments modelling visual search [7]: feature search, conjunction search, configuration search and asymmetric search.", "startOffset": 191, "endOffset": 194}, {"referenceID": 7, "context": "that simulate peripheral losses are very similar to the Metamers proposed by Freeman & Simoncelli [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[27, 19, 25]", "startOffset": 0, "endOffset": 12}, {"referenceID": 22, "context": "[27, 19, 25]", "startOffset": 0, "endOffset": 12}, {"referenceID": 20, "context": "[23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "[23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image.", "startOffset": 0, "endOffset": 18}, {"referenceID": 22, "context": "[23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image.", "startOffset": 0, "endOffset": 18}, {"referenceID": 2, "context": "[23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image.", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "[23, 4, 25, 3, 12] Target Detection (Hit Rate, False Alarms, Performance): In general, when engaging in target search for a fixed amount of time across all trial conditions, an observer will have a lower hit rate and higher false alarm rate for a highly cluttered image than an uncluttered image.", "startOffset": 0, "endOffset": 18}, {"referenceID": 20, "context": "[23, 3, 12]", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "[23, 3, 12]", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "[23, 3, 12]", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "A regular Feature Congestion clutter score is computed by taking the mean of the Feature Congestion map of the image or of a target ROI [12].", "startOffset": 136, "endOffset": 140}, {"referenceID": 7, "context": "Figure 3: Construction of a Peripheral Architecture a la Freeman & Simoncelli [9] using the functions described in Section 4.", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "We used the Piranhas Toolkit [6] to create a Freeman and Simoncelli [9] peripheral architecture.", "startOffset": 68, "endOffset": 71}, {"referenceID": 17, "context": "This biologically inspired model has been tested and used to model V1 and V2 responses in human and non-human primates with high precision for a variety of tasks [20, 10, 18, 2].", "startOffset": 162, "endOffset": 177}, {"referenceID": 8, "context": "This biologically inspired model has been tested and used to model V1 and V2 responses in human and non-human primates with high precision for a variety of tasks [20, 10, 18, 2].", "startOffset": 162, "endOffset": 177}, {"referenceID": 16, "context": "This biologically inspired model has been tested and used to model V1 and V2 responses in human and non-human primates with high precision for a variety of tasks [20, 10, 18, 2].", "startOffset": 162, "endOffset": 177}, {"referenceID": 1, "context": "This biologically inspired model has been tested and used to model V1 and V2 responses in human and non-human primates with high precision for a variety of tasks [20, 10, 18, 2].", "startOffset": 162, "endOffset": 177}, {"referenceID": 2, "context": "The target itself was removed when processing the clutter maps since it indirectly contributes to the ROI clutter score [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 20, "context": "where FCI is the Feature Congestion score [23] of image I which is computed by the mean of the Feature Congestion map RFC , and FFC f,t I is the Foveated Feature Congestion score of the image I , depending on the point of fixation f and the location of the target t.", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "[25] model since our peripheral architecture uses: a biologically inspired peripheral architecture with log polar regions that provide anisotropic pooling [15] rather than isotropic gaussian pooling as a linear function of eccentricity [25]; we used region-based max pooling for each final feature map instead of pixel-based mean pooling (gaussians) per each scale (which allows for stronger differences); this final difference also makes our model computationally more efficient running at 700ms per image, vs 180s per image for the Crowding model (\u00d7250 speed up).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[25] model since our peripheral architecture uses: a biologically inspired peripheral architecture with log polar regions that provide anisotropic pooling [15] rather than isotropic gaussian pooling as a linear function of eccentricity [25]; we used region-based max pooling for each final feature map instead of pixel-based mean pooling (gaussians) per each scale (which allows for stronger differences); this final difference also makes our model computationally more efficient running at 700ms per image, vs 180s per image for the Crowding model (\u00d7250 speed up).", "startOffset": 155, "endOffset": 159}, {"referenceID": 22, "context": "[25] model since our peripheral architecture uses: a biologically inspired peripheral architecture with log polar regions that provide anisotropic pooling [15] rather than isotropic gaussian pooling as a linear function of eccentricity [25]; we used region-based max pooling for each final feature map instead of pixel-based mean pooling (gaussians) per each scale (which allows for stronger differences); this final difference also makes our model computationally more efficient running at 700ms per image, vs 180s per image for the Crowding model (\u00d7250 speed up).", "startOffset": 236, "endOffset": 240}, {"referenceID": 21, "context": "We finally extended our model to create foveated(FoV) versions of Edge Density(ED) [19], Subband Entropy(SE) [24, 23] and ProtoObject Segmentation(PS) [27] showing that correlations for all foveated versions are stronger than non-foveated versions for the same task: rED = \u22120.", "startOffset": 109, "endOffset": 117}, {"referenceID": 20, "context": "We finally extended our model to create foveated(FoV) versions of Edge Density(ED) [19], Subband Entropy(SE) [24, 23] and ProtoObject Segmentation(PS) [27] showing that correlations for all foveated versions are stronger than non-foveated versions for the same task: rED = \u22120.", "startOffset": 109, "endOffset": 117}, {"referenceID": 24, "context": "We finally extended our model to create foveated(FoV) versions of Edge Density(ED) [19], Subband Entropy(SE) [24, 23] and ProtoObject Segmentation(PS) [27] showing that correlations for all foveated versions are stronger than non-foveated versions for the same task: rED = \u22120.", "startOffset": 151, "endOffset": 155}], "year": 2016, "abstractText": "Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We finally show that Foveated Feature Congestion (FFC) clutter scores (r(44) = \u22120.82 \u00b1 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = \u22120.19 \u00b1 0.13, p = 0.0774) in forced fixation search. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. A toolbox for creating peripheral architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and Artificial Systems will be made available1.", "creator": "LaTeX with hyperref package"}}}