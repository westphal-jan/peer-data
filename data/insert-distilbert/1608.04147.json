{"id": "1608.04147", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Aug-2016", "title": "Numerically Grounded Language Models for Semantic Error Correction", "abstract": "semantic error detection approaches and fault correction is an important task for applications such as fact checking, speech - translation to - text or grammatical error correction. current approaches generally focus on relatively shallow semantics and don't account for numeric quantities. our approach overwhelmingly uses language models grounded in fixed numbers within the text. such groundings are furthermore easily achieved for recurrent nonlinear neural language model architectures, which can be further conditioned on incomplete background knowledge bases. approaching our evaluation level on clinical reports shows that good numerical grounding finally improves perplexity by 33 % and f1 for semantic error correction by 5 points when compared to ungrounded approaches. conditioning on identifying a knowledge base yields further improvements.", "histories": [["v1", "Sun, 14 Aug 2016 22:34:22 GMT  (256kb,D)", "http://arxiv.org/abs/1608.04147v1", "accepted to EMNLP 2016"]], "COMMENTS": "accepted to EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["georgios p spithourakis", "isabelle augenstein", "sebastian riedel"], "accepted": true, "id": "1608.04147"}, "pdf": {"name": "1608.04147.pdf", "metadata": {"source": "CRF", "title": "Numerically Grounded Language Models for Semantic Error Correction", "authors": ["Georgios P. Spithourakis"], "emails": ["s.riedel}@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In many real world scenarios it is important to detect and potentially correct semantic errors and inconsistencies in text. For example, when clinicians compose reports, some statements in the text may be inconsistent with measurements taken from the patient (Bowman, 2013). Error rates in clinical data range from 2.3% to 26.9% (Goldberg et al., 2008) and many of them are number-based errors (Arts et al., 2002). Likewise, a blog writer may make statistical claims that contradict facts recorded in databases (Munger, 2008). Numerical concepts constitute 29% of contradictions in Wikipedia and GoogleNews (De Marneffe et al., 2008) and 8.8% of contradictory pairs in entailment datasets (Dagan et al., 2006).\nThese inconsistencies may stem from oversight, lack of reporting guidelines or negligence. In fact they may not even be errors at all, but point to interesting outliers, or to errors in a reference database. In all cases, it is important to spot and possibly correct such inconsistencies. This task is known as semantic error correction (SEC) (Dahlmeier and Ng, 2011).\nIn this paper, we propose a SEC approach to support clinicians with writing patient reports. A SEC system reads a patient\u2019s structured background information from a knowledge base (KB) and their clinical report. Then it recommends improvements to the text of the report for semantic consistency. An example of an inconsistency is shown in Figure 1.\nar X\niv :1\n60 8.\n04 14\n7v 1\n[ cs\n.C L\n] 1\n4 A\nug 2\nThe SEC system has been trained on a dataset of records and learnt that the phrases \u201cnon dilated\u201d and \u201cseverely dilated\u201d correspond to high and low values for \u201cEF\u201d (abbreviation for \u201cejection fraction\u201d, a clinical measurement), respectively. If then the system is presented with the phrase \u201cnon dilated\u201d in the context of a low value, it will detect a semantic inconsistency and correct the text to \u201cseverely dilated\u201d.\nOur contributions are: 1) a straightforward extension to recurrent neural network (RNN) language models for grounding them in numbers available in the text; 2) a simple method for modelling text conditioned on an incomplete KB by lexicalising it; 3) our evaluation on a semantic error correction task for clinical records shows that our method achieves F1 improvements of 5 and 6 percentage points with grounding and KB conditioning, respectively, over an ungrounded approach (F1 of 49%)."}, {"heading": "2 Methodology", "text": "Our approach to semantic error correction (Figure 1) starts with training a language model (LM), which can be grounded in numeric quantities mentioned inline with text (Subsection 2.1) and/or conditioned on a potentially incomplete KB (Subsection 2.2). Given a document for semantic checking, a hypothesis generator proposes corrections, which are then scored using the trained language model (Subsection 2.3). A final decision step involves accepting the best scoring hypothesis."}, {"heading": "2.1 Numerically grounded language modelling", "text": "Let {w1, ..., wT } denote a document, where wt is the one-hot representation of the t-th token and V is the vocabulary size. A neural LM uses a matrix, Ein \u2208 RD\u00d7V , to derive word embeddings, ewt = Einwt. A hidden state from the previous time step, ht\u22121, and the current word embedding, ewt , are sequentially fed to an RNN\u2019s recurrence function to produce the current hidden state, ht \u2208 RD. The conditional probability of the next word is estimated as softmax(Eoutht), where Eout \u2208 RV\u00d7D is an output embeddings matrix.\nWe propose concatenating a representation, ent , of the numeric value of wt to the inputs of the RNN\u2019s recurrence function at each time step. Through this numeric representation, the model can generalise\nto out-of-vocabulary numbers. A straightforward representation is defining ent = float(wt), where float(.) is a numeric conversion function that returns a floating point number constructed from the string of its input. If conversion fails, it returns zero. The proposed mechanism for numerical grounding is shown in Figure 2. Now the probability of each next word depends on numbers that have appeared earlier in the text. We treat numbers as a separate modality that happens to share the same medium as natural language (text), but can convey exact measurements of properties of the real world. At training time, the numeric representations mediate to ground the language model in the real world."}, {"heading": "2.2 Conditioning on incomplete KBs", "text": "The proposed extension can also be used in conditional language modelling of documents given a knowledge base. Consider a set of KB tuples accompanying each document and describing its attributes in the form < attribute, value >, where attributes are defined by a KB schema. We can lexicalise the KB by converting its tuples into textual statements of the form \u201dattribute : value\u201d. An example of how we lexicalise the KB is shown in Figure 2. The generated tokens can then be interpreted for their word embeddings and numeric representations. This approach can incorporate KB tuples flexibly, even\nwhen values of some attributes are missing."}, {"heading": "2.3 Semantic error correction", "text": "A statistical model chooses the most likely correction from a set of possible correction choices. If the model scores a corrected hypothesis higher than the original document, the correction is accepted.\nA hypothesis generator function, G, takes the original document, H0, as input and generates a set of candidate corrected documents G(H0) = {H1, ...,HM}. A simple hypothesis generator uses confusion sets of semantically related words to produce all possible substitutions.\nA scorer model, s, assigns a score s(Hi) \u2208 R to a hypothesis Hi. The scorer is based on a likelihood ratio test between the original document (null hypothesis, H0) and each candidate correction (alternative hypotheses, Hi), i.e. s(Hi) =\np(Hi) p(H0) . The assigned score represents how much more probable a correction is than the original document.\nThe probability of observing a document, p(Hi), can be estimated using language models, or grounded and conditional variants thereof."}, {"heading": "3 Data", "text": "Our dataset comprises 16,003 clinical records from the London Chest Hospital (Table 1). Each patient record consists of a text report and accompanying structured KB tuples. The latter describe 20 possible numeric attributes (age, gender, etc.), which are also partly contained in the report. On average, 7.7 tuples\nare completed per record. Numeric tokens constitute only a small proportion of each sentence (4.3%), but account for a large part of the unique tokens vocabulary (>40%) and suffer high OOV rates.\nTo evaluate SEC, we generate a \u201ccorrupted\u201d dataset of semantic errors from the test part of the \u201ctrusted\u201d dataset (Table 1, last column). We manually build confusion sets (Table 2) by searching the development set for words related to numeric quantities and grouping them if they appear in similar contexts. Then, for each document in the trusted test set we generate an erroneous document by sampling a substitution from the confusion sets. Documents with no possible substitution are excluded. The resulting \u201ccorrupted\u201d dataset is balanced, containing 2,926 correct and 2,926 incorrect documents."}, {"heading": "4 Results and discussion", "text": "Our base LM is a single-layer long short-term memory network (LSTM, Hochreiter and Schmidhuber (1997) with all latent dimensions (internal matrices, input and output embeddings) set to D = 50. We extend this baseline to a conditional variant by conditioning on the lexicalised KB (see Section 2.2). We also derive a numerically grounded model by concatenating the numerical representation of each token to the inputs of the base LM model (see Section 2.1). Finally, we consider a model that is both grounded and conditional (g-conditional).\nThe vocabulary contains the V = 1000 most frequent tokens in the training set. Out-of-vocabulary tokens are substituted with <num unk>, if numeric, and <unk>, otherwise. We extract the numerical representations before masking, so that the grounded models can generalise to out-ofvocabulary numbers. Models are trained to minimise token cross-entropy, with 20 epochs of backpropagation and adaptive mini-batch gradient de-\nscent (AdaDelta) (Zeiler, 2012). For SEC, we use an oracle hypothesis generator that has access to the groundtruth confusion sets (Table 2). We estimate the scorer (Section 2.3) using the trained base, conditional, grounded or g-conditional LMs. As additional baselines we consider a scorer that assigns random scores from a uniform distribution and always (never) scorers that assign the lowest (highest) score to the original document and uniformly random scores to the corrections."}, {"heading": "4.1 Experiment 1: Numerically grounded LM", "text": "We report perplexity and adjusted perplexity (Ueberla, 1994) of our LMs on the test set for all tokens and token classes (Table 3). Adjusted perplexity is not sensitive to OOV-rates and thus allows for meaningful comparisons across token classes. Perplexities are high for numeric tokens because they form a large proportion of the vocabulary. The grounded and g-conditional models achieved a 33.3% and 36.9% improvement in perplexity, respectively, over the base LM model. Conditioning without grounding yields only slight improvements, because most of the numerical values from the lexicalised KB are out-of-vocabulary.\nThe qualitative example in Figure 3 demonstrates how numeric values influence the probability of tokens given their history. We select a document from the development set and substitute its numeric values as we vary EF (the rest are set by solving\na known system of equations). The selected exact values were unseen in the training data. We calculate the probabilities for observing the document with different word choices {\u201cnon\u201d, \u201cmildly\u201d, \u201cseverely\u201d} under the grounded LM and find that \u201cnon dilated\u201d is associated with higher EF values. This shows that it has captured semantic dependencies on numbers."}, {"heading": "4.2 Experiment 2: Semantic error correction", "text": "We evaluate SEC systems on the corrupted dataset (Section 3) for detection and correction.\nFor detection, we report precision, recall and F1 scores in Table 4. Our g-conditional model achieves the best results, a total F1 improvement of 2 points over the base LM model and 7 points over the best baseline. The conditional model without grounding performs slightly worse in the F1 metric than the base LM. Note that with more hypotheses the random baseline behaves more similarly to always. Our hypothesis generator generated on average 12 hypotheses per document. The results of never are zero as it fails to detect any error.\nFor correction, we report mean average precision (MAP) in addition to the same metrics as for detection (Table 5). The former measures the position of the ranking of the correct hypothesis. The always (never) baseline ranks the correct hypothesis at the top (bottom). Again, the g-conditional model yields the best results, achieving an improvement of\n6 points in F1 and 5 points in MAP over the base LM model and an improvement of 47 points in F1 and 9 points in MAP over the best baseline. The conditional model without grounding has the worst performance among the LM-based models."}, {"heading": "5 Related Work", "text": "Grounded language models represent the relationship between words and the non-linguistic context they refer to. Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015). However, no previous approach has explored in-line numbers as a source of grounding.\nOur language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014). They similarly derive confusion sets of semantically related words, substitute the target words with alternatives and score them with an LM. Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011). So far, SEC approaches focus on short distance semantic agreement, whereas our approach can detect errors which require to resolve long-range dependencies. Work on GEC and SEC shows that language models are useful for error correction, however they neither ground in numeric quantities nor incorporate background KBs."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a simple technique to model language in relation to numbers it refers to, as well as conditionally on incomplete knowledge bases. We found that the proposed techniques lead to performance improvements in the tasks of language modelling, and semantic error detection and correction. Numerically grounded models make it possible to capture semantic dependencies of content words on numbers.\nIn future work, we will plan to apply numerically grounded models to other tasks, such as numeric error correction. We will explore alternative ways for deriving the numeric representations, such as accounting for verbal descriptions of numbers. For SEC, a trainable hypothesis generator can potentially improve the coverage of the system."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their insightful comments. We also thank Steffen Petersen for providing the dataset and advising us on the clinical aspects of this work. This research was supported by the Farr Institute of Health Informatics Research."}], "references": [{"title": "Defining and improving data quality in medical registries: a literature review, case study, and generic framework", "author": ["Nicolette F De Keizer", "Gert-Jan Scheffer"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Arts et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Arts et al\\.", "year": 2002}, {"title": "Impact of Electronic Health Record Systems on Information Integrity: Quality and Safety Implications", "author": ["Sue Bowman"], "venue": "Perspectives in Health Information Management,", "citeRegEx": "Bowman.,? \\Q2013\\E", "shortCiteRegEx": "Bowman.", "year": 2013}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "The pascal recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment", "author": ["Dagan et al.2006] Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 2006}, {"title": "Correcting Semantic Collocation Errors with L1-induced Paraphrases", "author": ["Dahlmeier", "Ng2011] Daniel Dahlmeier", "Hwee Tou Ng"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Dahlmeier et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dahlmeier et al\\.", "year": 2011}, {"title": "Finding Contradictions in Text", "author": ["Anna N Rafferty", "Christopher D Manning"], "venue": "In ACL,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Grammatical error correction using hybrid systems and type filtering", "author": ["Zheng Yuan", "\u00d8istein E Andersen", "Helen Yannakoudakis", "Ekaterina Kochmar"], "venue": "In CoNLL Shared Task,", "citeRegEx": "Felice et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Felice et al\\.", "year": 2014}, {"title": "Grounded Language Modeling for Automatic Speech Recognition of Sports Video", "author": ["Fleischman", "Roy2008] Michael Fleischman", "Deb Roy"], "venue": "In Proceedings of ACL,", "citeRegEx": "Fleischman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fleischman et al\\.", "year": 2008}, {"title": "Analysis of data errors in clinical research databases", "author": ["Andrzej Niemierko", "Alexander Turchin"], "venue": "In AMIA. Citeseer", "citeRegEx": "Goldberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2008}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception", "author": ["Kiela", "Clark2015] Douwe Kiela", "Stephen Clark"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Grounding Semantics in Olfactory Perception", "author": ["Kiela et al.2015] Douwe Kiela", "Luana Bulat", "Stephen Clark"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Error Detection in Content Word Combinations", "author": ["Ekaterina Kochmar"], "venue": "Ph.D. thesis,", "citeRegEx": "Kochmar.,? \\Q2016\\E", "shortCiteRegEx": "Kochmar.", "year": 2016}, {"title": "A bayesian model of grounded color semantics. Transactions of the Association for Computational Linguistics, 3:103\u2013115", "author": ["McMahan", "Stone2015] Brian McMahan", "Matthew Stone"], "venue": null, "citeRegEx": "McMahan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McMahan et al\\.", "year": 2015}, {"title": "Blogging and political information: truth or truthiness? Public Choice, 134(1-2):125\u2013138", "author": ["Michael C Munger"], "venue": null, "citeRegEx": "Munger.,? \\Q2008\\E", "shortCiteRegEx": "Munger.", "year": 2008}, {"title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "author": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2013}, {"title": "Learning Grounded Meaning Representations with Autoencoders", "author": ["Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Grounded Compositional Semantics for Finding and Describing Images with Sentences", "author": ["Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Analysing a simple language model\u00b7 some general conclusions for language models for speech recognition", "author": ["Joerg Ueberla"], "venue": "Computer Speech & Language,", "citeRegEx": "Ueberla.,? \\Q1994\\E", "shortCiteRegEx": "Ueberla.", "year": 1994}, {"title": "Linear) Maps of the Impossible: Capturing semantic anomalies in distributional space", "author": ["Marco Baroni", "Roberto Zamparelli"], "venue": "In Proceedings of the Workshop on Distributional Semantics and Compositionality,", "citeRegEx": "Vecchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vecchi et al\\.", "year": 2011}, {"title": "ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "For example, when clinicians compose reports, some statements in the text may be inconsistent with measurements taken from the patient (Bowman, 2013).", "startOffset": 135, "endOffset": 149}, {"referenceID": 8, "context": "9% (Goldberg et al., 2008) and many of them are number-based errors (Arts et al.", "startOffset": 3, "endOffset": 26}, {"referenceID": 0, "context": ", 2008) and many of them are number-based errors (Arts et al., 2002).", "startOffset": 49, "endOffset": 68}, {"referenceID": 14, "context": "Likewise, a blog writer may make statistical claims that contradict facts recorded in databases (Munger, 2008).", "startOffset": 96, "endOffset": 110}, {"referenceID": 3, "context": "8% of contradictory pairs in entailment datasets (Dagan et al., 2006).", "startOffset": 49, "endOffset": 69}, {"referenceID": 20, "context": "scent (AdaDelta) (Zeiler, 2012).", "startOffset": 17, "endOffset": 31}, {"referenceID": 18, "context": "We report perplexity and adjusted perplexity (Ueberla, 1994) of our LMs on the test set for all tokens and token classes (Table 3).", "startOffset": 45, "endOffset": 60}, {"referenceID": 2, "context": "Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 17, "context": "Previous work grounds language on vision (Bruni et al., 2014; Socher et al., 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 10, "context": ", 2014; Silberer and Lapata, 2014), audio (Kiela and Clark, 2015), video (Fleischman and Roy, 2008), colour (McMahan and Stone, 2015), and olfactory perception (Kiela et al., 2015).", "startOffset": 160, "endOffset": 180}, {"referenceID": 15, "context": "Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014).", "startOffset": 105, "endOffset": 143}, {"referenceID": 6, "context": "Our language modelling approach to SEC is inspired by LM approaches to grammatical error detection (GEC) (Ng et al., 2013; Felice et al., 2014).", "startOffset": 105, "endOffset": 143}, {"referenceID": 12, "context": "Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al.", "startOffset": 128, "endOffset": 143}, {"referenceID": 19, "context": "Existing semantic error correction approaches aim at correcting word error choices (Dahlmeier and Ng, 2011), collocation errors (Kochmar, 2016), and semantic anomalies in adjective-noun combinations (Vecchi et al., 2011).", "startOffset": 199, "endOffset": 220}], "year": 2016, "abstractText": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.", "creator": "LaTeX with hyperref package"}}}