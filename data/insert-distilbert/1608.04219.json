{"id": "1608.04219", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition With Groebner Bases", "abstract": "cylindrical compression algebraic decomposition ( cad ) is possibly a key tool in computational algebraic geometry, particularly for quantifier elimination over real - closed fields. ) however, it can be expensive, with worst case complexity doubly relatively exponential in the size representation of the corresponding input. hence it very is important to formulate the problem in the best manner for the cad algorithm. one possibility is to precondition otherwise the resulting input polynomials using groebner basis ( gb ) theory. previous experiments have shown that while this can often be very beneficial to the specific cad algorithm, for some problems it can significantly worsen the cad performance.", "histories": [["v1", "Mon, 15 Aug 2016 09:44:29 GMT  (168kb,D)", "http://arxiv.org/abs/1608.04219v1", null]], "reviews": [], "SUBJECTS": "cs.SC cs.LG", "authors": ["zongyan huang", "matthew england", "james h davenport", "lawrence c paulson"], "accepted": false, "id": "1608.04219"}, "pdf": {"name": "1608.04219.pdf", "metadata": {"source": "CRF", "title": "Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition With Groebner Bases", "authors": ["Zongyan Huang", "Matthew England", "James H. Davenport", "Lawrence C. Paulson"], "emails": ["rubyhuang87@gmail.com;", "lp15@cam.ac.uk", "Matthew.England@coventry.ac.uk", "J.H.Davenport@bath.ac.uk"], "sections": [{"heading": null, "text": "In the present paper we investigate whether machine learning, specifically a support vector machine (SVM), may be used to identify those CAD problems which benefit from GB preconditioning. We run experiments with over 1000 problems (many times larger than previous studies) and find that the machine learned choice does better than the human-made heuristic.\nI. INTRODUCTION"}, {"heading": "A. Cylindrical Algebraic Decomposition", "text": "A Cylindrical Algebraic Decomposition (CAD) is a decomposition of ordered Rn space into cells. These are arranged cylindrically, meaning the projections of any pair with respect to the given ordering are either equal or disjoint. In this definition algebraic is actually short for semi-algebraic as each CAD cell can be described with a finite sequence of polynomial constraints. A CAD is produced to be invariant for input: sign- (or order-) invariant for input polynomials or truth-invariant for input formulae.\nCADs and the first algorithm to compute them were introduced by Collins in 1975 [19]. CAD usually has two stages: projection where an operator is applied recursively on the input to derive corresponding problems in lower dimensions; and lifting where CADs are built incrementally by dimension according to the polynomials identified in projection [1]. The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.\nCAD has worst case complexity doubly exponential in the number of variables [23] applicable whatever the data structure\n[12]. For some applications there exist algorithms with better complexity [3], but CAD implementations still remain the best general purpose approach for many. This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42]. Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5]. For a more detailed introduction to CAD see for example Bradford et al. [7]."}, {"heading": "B. Preconditioning with Groebner Bases", "text": "A Groebner Basis G is a particular generating set of an ideal I defined with respect to a monomial ordering. One definition is that the ideal generated by the leading terms of I is generated by the leading terms of G. Groebner Bases (GB) allow properties of the ideal to be deduced such as dimension and number of zeros and so are one of the main practical tools for working with polynomial systems. Their properties and an algorithm to derive a GB for any ideal was introduced by Buchberger in his PhD thesis of 1965 [14].\nLike CAD, there has been much research to improve and optimise GB calculation, with the F5 algorithm [31] perhaps the most used approach currently. However, also like CAD the calculation of GB is necessarily doubly exponential in the worst case [46] (when using a lexicographic monomial ordering). Despite this, the computation of GB can often be done very quickly and would almost certainly be a superior tool to CAD for any problem involving only polynomial equalities. From this arises the natural question: is the process of replacing a conjunction of polynomial equalities in a CAD problem by their GB a useful precondition for CAD?\nI.e. let E = {e1, e2, . . . } be a set of polynomials; G = {g1, g2, . . . } a GB for E; and B any Boolean combination of constraints, fi \u03c3i 0, where \u03c3i \u2208 {<,>,\u2264,\u2265, 6=,=}) and F = {f1, f2, . . . } is another set of polynomials. Then\n\u03a6 = (e1 = 0 \u2227 e2 = 0 \u2227 . . . ) \u2227B and \u03a8 = (g1 = 0 \u2227 g2 = 0 \u2227 . . . ) \u2227B\nar X\niv :1\n60 8.\n04 21\n9v 1\n[ cs\n.S C\n] 1\n5 A\nug 2\n01 6\nare equivalent and a CAD truth-invariant for either could be used to solve problems involving \u03a6 (such as eliminating any quantifiers applied to \u03a6). So is it worth producing G?\nThe first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]). Of the ten test problems studied: 6 were improved by the GB preconditioning, with the speed-up varying from 2-fold to 1700-fold; 1 problem resulted in a 10-fold slow-down; 1 timed out when GB preconditioning was applied, while it would complete without it; and the other 2 were intractable both for CAD alone and the GB preconditioning step.\nThe problem was recently revisited by Wilson et al. [57]. The authors recreated the experiments of Buchberger and Hong [15] using QEPCAD-B for the CAD and MAPLE 16 for the GB. As we may expect, there had been a big decrease in the computation timings, especially the GB: the two test problems previously intractable [15] could now have the GB calculated quickly. However, two of the CAD problems were still hindered by GB preconditioning. The experiments were then extended to: a wider example set (an additional 12 problems); the alternative CAD implementation in MAPLE-16 [18]; and the case where we further precondition by reducing inequalities of the system (the set F above) with respect to the GB. The key conclusion remained that GB preconditioning would in general benefit CAD (sometimes significantly) but could on occasion hinder it (to the point of making a tractable CAD problem intractable). The authors defined a metric to assist with the decision of when to precondition, the Total Number of Indeterminates (TNoI) of a set of polynomials A,\nTNoI(A) = \u2211 a\u2208A NoI(a) (1)\nwhere NoI(a) is the number of indeterminates in a polynomial a. Then their heuristic was to build a CAD for the preconditioned polynomials only if the TNoI decreased following preconditioning. For most of their test problems the heuristic made the correct choice, but there were examples to the contrary and little correlation between the change in TNoI and level of speed-up / slow-down."}, {"heading": "C. Contribution and plan", "text": "In this paper we consider whether machine learning can be applied to the decision of whether preconditioning CAD input with GB is beneficial for a particular problem. We work on the reasonable assumption that GB computation is cheap for the problems on which CAD is tractable (in fact as shown in [29] the CAD will compute resultants which overestimate the GB). Hence we use algebraic features of both the input problem and the GB itself to decide whether we want to use the GB.\nIn Section II we describe the dataset and computer algebra computations used for the experiment and in Section III we describe the set of features identified to train the machine learning algorithm: a Support Vector Machine (SVM). Then in Section IV we describe the initial machine learning experiment and its results, before running feature selection experiments in Section V. Finally, we compare the machine learned decision with the human developed TNoI-based heuristic, draw our conclusions and discuss future work in Section VI.\nThis is the second paper of the present authors to consider the application of SVMs to CAD optimisation. We previously studied the choice of variable ordering for CAD in [41]. In that paper 3 existing heuristics were evaluated against a machine learned choice of which to use. The latter outperformed each individually and suggested a greater role for machine learning in such decisions, motivating the present study. The only other application of machine learning to computer algebra that the authors are aware of is by Kobayashi et al. [44] who applied a SVM to decide the order of sub-formulae solving for QE."}, {"heading": "II. DATASET AND COMPUTER ALGEBRA", "text": ""}, {"heading": "A. Computer Algebra", "text": "All the computer algebra computations were conducted in MAPLE-17. The CAD algorithm used was an implementation of [18]. This is part of the RegularChains Library1 [16], [17] whose CAD procedures differ from the traditional projection and lifting framework of Collins, instead first decomposing Cn cylindrically and then refining to a CAD of Rn. Previous experiments [57] showed this implementation has the same issues of GB preconditioning as the traditional approach. The default MAPLE GB implementation was used: a meta algorithm calling multiple GB implementations. The GBs were computed with a purely lexicographical ordering of monomials based on the same variable ordering as the CAD.\nAll computations were performed on a 2.4GHz Intel processor, however this is not relevant as we evaluated the CAD performance using cell counts instead of timings, i.e. by comparing the numbers of cells in the final outputted CADs produced with and without GB preconditioning. Numerous previous studies have shown this to be closely correlated to timings and it has the advantage of being discrete, machine and (up the theory used) implementation independent. It also correlates with the cost of any post-processing of the CAD."}, {"heading": "B. Dataset", "text": "A key difficulty in applying machine learning techniques to computer algebra is the lack of suitable datasets. CAD problem sets such as [56] do not have anywhere near a sufficient number of problems to perform the experiment. In our previous study on choosing the variable ordering for CAD [41] we used the nlsat-dataset [58], which although developed for non-linear arithmetic SAT-solvers, contained many suitable problems.\nFor the present experiment we need problems that are expressed with a conjunction of at least two equalities in order to build a non-trivial GB. From the nlsat dataset 493 three-variable problems and 403 four-variable problems fit this criteria, which should have been a sufficient number. GB preconditioning was applied to each problem and cell counts from computing the CAD with the original polynomials and their replacement with the GB were computed and compared. For each one of these problems the GB preconditioning was beneficial or made no difference; surprising as the experiments on much smaller datasets [15], [57] had shown much greater volatility. This points to an undetected uniformity within the current nlsat dataset. It would need to be widened if it is to be used more extensively for computer algebra research.\n1http://www.regularchains.org\nSince existing datasets were not suitable for the present experiment, we had no choice but to generate our own problems. The generation process aimed for an unbiased data set which would be computationally feasible for computing multiple CADs, and have some comparable structure (number of terms and polynomials) to existing CAD problems.\nIn total, 1200 problems were generated using the random polynomial generator randpoly in MAPLE-17. Each problem has two sets of three polynomials; the first to represent conjoined equalities and the second for the other polynomial constraints (respectively E and F from the description in Section I-B). The number of variables was at most 3, labelled x, y, z and under ordering x \u227a y \u227a z; the number of terms per polynomial at most 2; the coefficients were restricted to integers in [\u221220, 20]; and the total degree was varied between 2, 3 and 4 (with 400 problems generated for each).\nA time limit of 300 CPU seconds was set for each CAD computation (all GB computations completed quickly) from which 1062 problems finished to constitute the final dataset. Of these, 75% benefited from GB preconditioning. So our randomly generated dataset matched the previously found results of [15], [57] with most problems benefiting but not all and so is suitable for the purpose of this experiment."}, {"heading": "III. PROBLEM FEATURES", "text": "Table I shows the 28 problem features we identified (guided by previous work). Here (x, y, z) are the three variable labels used in the problems and proportion means the percentage of the total. The features were chosen as easily computable metrics that may affect the cell count of the CAD. They fall into two sets: those generated from the polynomials in the original problem and those obtained after applying GB preconditioning. The abbreviations tds and stds stand for maximum total degree and sum of total degrees respectively:\ntds(F ) = max f\u2208F tds(f), stds(F ) = \u2211 f\u2208F tds(f).\nWe also make use of the metric TNoI (see equation (1)) [57]. Finally we included the base 2 logarithm of the ratio of differences between some of the key metrics. All features could be calculated immediately within MAPLE.\nWe note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28]. This is because stds measures the input polynomials only, while sotd measures the full set of CAD projection polynomials, and so is much more expensive.\nIn addition to training a classifier using all the features in Table I, we trained classifiers using two subsets: one containing the features labelled 1 \u2212 12 concerning the original set of polynomials; and one the features labelled 13\u221225 concerning the polynomials after GB preconditioning. The latter set has one extra feature, the number of polynomials, as this varies after GB calculation but was always 6 to start with. We refer to the first subset as before features, the second as after features and the full set as all features.\nExample: Consider sets of polynomials\nE := {\u221212yz \u2212 3z, 17x2 \u2212 6, \u22122yz + 5x} F := {\u22122yz \u2212 9y, \u221215x2 \u2212 19y, 6xz + 3}.\nThe GB computed for E is\nG := {17x2 \u2212 6, 4y + 1, z + 10x}\nand the all features vector becomes[ 12, 12, 2, 2, 1, 1, 23 , 2 3 , 2 3 , 1 3 , 5 12 , 5 12 ,\n6, 10, 10, 2, 2, 1, 1, 23 , 1 2 , 1 2 , 1 3 , 1 3 , 1 4 , 0.263, 2.263, 0 ] ,\nwith the before features and after features vectors formed from the first and second line respectively.\nThe feature generation process was applied to create the three training sets separately (although the feature labels used were all as in Table I). Each problem was labelled +1 if Groebner basis preconditioning is beneficial for CAD construction, or \u22121 otherwise. After feature generation the training data was standardised so each feature had zero mean and unit variance across the training set. The same standardisation was then applied to features in the test set."}, {"heading": "IV. MACHINE LEARNED CHOICES", "text": "A. Introduction\nMachine learning deals with the design of programs that learn rules from data. This is an attractive alternative to manually constructing them when the underlying functional relationship is complex, as appears to be the case here.\nIn the last decade, the use of machine learning has spread rapidly following the invention of the Support Vector Machine (SVM) (see for example [51]). This gives a powerful and robust method for both: Classification, the assignment of input examples into a given set of classes; and Regression, a supervised pattern analysis in which the output is real-valued. The standard SVM classifier takes a set of input data and predicts one of two possible classes from the input. Given a set of examples, each marked as belonging to one of two classes,\nan SVM training algorithm builds a model that assigns new examples into one of the classes. The examples used to fit the model are called training examples. An important concept in the SVM theory is the use of a kernel function to map data into a high dimensional feature space and then separate samples in the transformed space [53]. Kernel functions enable operations in feature space without ever computing the coordinates of the data in that space, rather they compute the inner products between all pairs of data vectors, which is generally cheaper.\nFor our experiment we used SVM-LIGHT2 [43]; an implementation of SVMs in C."}, {"heading": "B. Cross-validation and grid-search", "text": "The 1062 problems were partitioned into 80% training (849 problems) and 20% test (213 problems), stratified to maintain relative proportions of positive and negative examples. The classification was done using the radial basis function (RBF) kernel. This was chosen after earlier experiments applying machine learning to an automated theorem prover found the RBF kernel to perform well with similar simple algebraic features [9]. The RBF function is defined as:\nK(x, x\u2032) = exp ( \u2212\u03b3||x\u2212 x\u2032||2 ) (2)\nwhere x and x\u2032 are feature vectors. The process depends on kernel parameter \u03b3 and another parameter C which governs the trade-off between margin and training error, and finding the optimal values of these is not trivial. Matthews\u2019 Correlation Coefficient (MCC) [45], [2] is often used to evaluate choices. This takes into account true and false positives and negatives (labelled tp, fp, tn and fn accordingly):\nMCC = tp \u2217 tn\u2212 fp \u2217 fn\u221a\n(tp + fp)(tp + fn)(tn + fp)(tn + fn) . (3)\nIn the case where one of the terms in the denominator is zero the entire denominator is set to 1. The MCC measure has the value 1 if perfect prediction is attained, 0 if the classifier is performing as a random classifier, and \u22121 if the classifier exactly disagrees with the data.\nA grid-search optimisation procedure along with a fivefold stratified cross validation was used, involving a search over a range of (\u03b3,C) values to find the pair which would maximize equation (3). We tested a commonly used range of values in our grid search process [40]: \u03b3 varied between {2\u221215, 2\u221214, 2\u221213, . . . , 23}; and C varied between {2\u22125, 2\u22124, 2\u22123, . . . , 215}. Following the completion of the grid-search, the values giving optimal MCC results were selected. This procedure was repeated for the three feature sets."}, {"heading": "C. Results for the three feature sets", "text": "The classification accuracy was used to measure the efficacy of the machine learning selection process under the 3 feature sets. The test set of 213 problems contained 159 positive samples and 54 negative samples (i.e. 75% of the test problems benefited from GB preconditioning). The results of the machine learned choices are summarised in Table II. First we note that when making a choice based on the before features training set 75% of the problems were predicted\n2http://svmlight.joachims.org\naccurately. I.e. making a decision based on these features results in no more correct decisions than blindly deciding to GB precondition each and every time. However, the other two feature sets resulted in superior decisions. Although only a small improvement on preconditioning blindly, we recall that the wrong choice can give large changes to the size of the CAD or even change the tractability of the problem [57].\nThe results indicate that the features of the GB itself are required to decide whether to use the preconditioning. However, we cannot conclude this directly: earlier research shows that a variable completely useless by itself can provide a significant performance improvement when taken in conjunction with others [34]. To be confident about which features were significant and which were superfluous, further feature selection experiments are required and we will see that the optimal feature subset must contain features from both before and after the GB computation."}, {"heading": "V. FEATURE SELECTION", "text": "There is a strong indication that not all features contribute to the machine learning process. Moreover, a reduced feature set can be beneficial for better understanding the underlying connections. Consequently, we applied some feature selection methods. Both filter and wrapper methods were applied as discussed in the following subsections.\nThe feature selection experiments were conducted with WEKA (Waikato Environment for Knowledge Analysis) [35], a Java machine learning library which supports tasks such as data preprocessing, clustering, classification, regression and feature selection. Each data point is also represented as a fixed number of features. The inputs are samples of 29 features, where the first 28 are the real-valued features from Table I, and the final one is a nominal feature denoting its class."}, {"heading": "A. The filter method", "text": "A correlation based feature selection method, was applied as described in [37]. Unlike other filter methods [36], these measure the rank of feature subsets instead of individual features. A feature subset which contains features highly correlated with the class but uncorrelated with each other is preferred. The metric below is used to measure the quality of a feature subset, and takes into account feature-class correlation as well as feature-feature correlation.\nGs = krci\u221a\nk + k(k \u2212 1)rii\u2032 (4)\nHere, k is the number of features in the subset, rci denotes the average feature-class correlation of feature i, and rii\u2032 the average feature-feature correlation between feature i and i\u2032. The numerator of equation (4) indicates how much relevance there is between the class and a set of features, while the denominator measures the redundancy among the features. The higher Gs, the better the feature subset.\nTo apply this heuristic we must calculate the correlations. With the exception of the class attribute all 28 features are continuous, so in order to have a common measure for computing the correlations we first discretize using the method of Fayyad and Irani [32]. After that, a correlation measure based on the information-theoretical concept of entropy is used, which is a measure of the uncertainty of a random variable. We define the entropy of a variable X [52] as\nH(X) = \u2212 \u2211 i p(xi) log2 ( p(xi) ) . (5)\nThe entropy of X after observing values of another variable Y is then defined as\nH(X|Y ) = \u2212 \u2211 j p(yj) \u2211 i p(xi|yj) log2 ( p(xi|yj) ) , (6)\nwhere p(xi) is the prior probabilities for all values of X , and p(xi|yi) is the posterior probabilities of X given the values of Y . The information gain (IG) [50] measures the amount by which the entropy of X decreases by additional information about X provided by Y , and it is given by\nIG(X,Y ) = H(X)\u2212H(X|Y ). (7)\nThe symmetrical uncertainty (SU) (a modified information gain measure) is then used to measure the correlation between two discrete variables (X and Y) [49]:\nSU(X,Y ) = 2.0\u00d7 ( H(X)\u2212H(X|Y ) H(X) +H(Y ) ) . (8)\nTreating each feature as well as the class as random variables, we can apply this as our correlation measure. More specifically, we simply use SU(c, i) to measure the correlation between a feature i and a class c, and SU(i, i \u2032 ) to measure the correlation between features i and i\u2032. These values are then substituted as rci and rii\u2032 in equation (4).\nRecall that our aim here is to find the optimal subset of features which maximises the metric given in equation (4). The size of our feature set is 28 meaning there are 228 \u2212 1 ' 2.7 \u00d7 108 possible subsets, too many for exhaustive search. Instead a greedy stepwise forward selection search strategy was used for searching the space of feature subsets, which works by adding the current best feature at each round. The search begins with the empty set, and in each step the metric, as defined in equation (4), is computed for every single feature addition, and the feature with the best score improvement is added. If at some step none of the remaining features provide an improvement, the algorithm stops, and the current feature set is returned. The best feature subset found with this method (which may not be the absolute optimal subset of features) is shown in Table III, ordered by importance."}, {"heading": "B. The wrapper method", "text": "The wrapper feature selection method evaluates attributes using accuracy estimates provided by the target learning algorithm. Evaluation of each feature set was conducted with a learning scheme (a SVM with RBF kernel function). The SVM algorithm is run on the dataset, with the same data partitions as described in Section IV-B. Similarly, a five-fold cross validation was carried out. The feature subset with the\nhighest average accuracy was chosen as the final set on which to run the SVM algorithm.\nIn each training / validation fold, starting with an empty set of features: each feature was added; a model was fitted to the training data set; the classifier was then tested on the validation set. This was done on all the features, resulting in a score for each where the score reflects the accuracy of the classifier. The final score for each feature was its average over the five folds. Having obtained a score for all features in the manner above, the feature with the highest score was then added in the feature set. Then, the same greedy procedure as described for the filter method in Section V-A was applied to obtain the best feature subset.\nDue to the large number of cases, the parameters (C, \u03b3) were selected from an optimised sub range instead of the full grid search used in Section IV-B. The reduced range suffices to demonstrate the performance of a reduced feature set. In those previous experiments we found that C taken from {25, 26, 27, 28, 29, 210} and \u03b3 taken from {2\u22125, 2\u22126, 2\u22127, 2\u22128, 2\u22129, 2\u221210} provided good classifier performance.\nThe 36 pairs of (C, \u03b3) values were tested and an optimal feature subset with the highest accuracy was found for each. Then the one with the highest accuracy was selected as the final set, which is shown in Table IV ordered by importance. We see that most of the features selected (9, 12 and 22) related to variable z. Recall that the projection order used in the CAD was always x \u227a y \u227a z, i.e. the variable z is projected first. Hence it makes sense that this variable would have the greatest effect and thus be identified in the feature selection.\nWe examined the performance on further reduced feature sets, obtained by the feature ranking of the wrapper method. Figure 1 shows the overall prediction accuracies. For instance, the predictor obtained from only using a single feature (the best ranked feature was TNoI after GB for both filter and wrapper methods) achieved an accuracy score of 0.756 in that run, with the performance steadily increasing with the size of the feature set until the fifth feature. Taking any sixth feature into the set did not improve the performance noticeably, and hence resulted in the cut-off chosen by the wrapper method.\nAs the wrapper method identified only a few features an error analysis on the misclassified data points is feasible. Figure 2 shows 40 misclassified points and their features 4 and 14, while Figure 3 shows the remaining features. It is interesting that feature 4 of all misclassified samples is either 1 or 2, when for the whole data set roughly a third of samples had this feature value 3 or 4. This indicates that the algorithm performs better on instances with a higher maximum degree of x among all polynomials before GB preconditioning."}, {"heading": "C. Results with reduced feature sets", "text": "Having obtained the reduced feature sets, we ran the experiment again to evaluate the new choices. The data set was again repartitioned into 80% training and 20% test set, stratified to maintain relative class proportions in both training and test partitions. Again, a five-fold cross validation and a finer grid-search optimisation procedure over the range of (C, \u03b3) pairs was conducted as described previously. The\nclassifier with maximum averaged MCC was selected and the resulting classifier was then evaluated. The testing data was also reduced to contain only the features selected. The classification accuracy was used to measure the performance of the classifier. In order to better estimate the generalisation performance of classifiers with reduced feature sets, the data was permuted and partitioned into 80% training and 20% test again and the whole process was repeated 50 times. For each run, each training set was standardised to have zero mean and unit variance, with the same offset and scaling applied subsequently to the corresponding test partition.\nFigure 4 shows boxplots of the accuracies generated by 50 runs of the five-fold cross validation. Both reduced feature sets generated similar results and show a large improvement on the base case where Groebner basis preconditioning is always used before CAD construction. The average overall prediction accuracy of the filter subset and the wrapper subset is 79% and 78% respectively (we note that Figure 1 shows a higher rate but that was just for one sample run). All 50 runs of the wrapper subset performed above the base line, while the top three quartiles of the results of both sets achieve higher than 77% percentage accuracy."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": ""}, {"heading": "A. Comparison with human developed heuristic", "text": "We may compare the machine learned choice with the human developed TNoI heuristic [57], whose performance on the 213 test problems is shown in Table V. It correctly predicted whether GB preconditioning was beneficial for 118 examples, only 55%. So for this dataset it would have been better on average to precondition blindly than to make a decision using TNoI alone. The TNoI heuristic performed better in the experiments by Wilson et al. [57]. Those experiments involved only 22 problems (compared to 213 in the test set here)\nbut they were human constructed to have certain geometric properties while the ones here were random.\nWe also note that the TNoI heuristic actually performed quite differently for positive and negative examples of our dataset, as shown by the separated data in Table V. It was able to identify most of the cases where GB-preconditioning is detrimental but failed to identify many of the cases where it was beneficial. The TNoI after GB was identified as important by both feature methods, but it seems to need to be used in conjunction with other features to be effective here."}, {"heading": "B. Summary", "text": "We investigated the application of machine learning to the problem of predicting when GB preconditioning is beneficial to CAD. We had to create a new dataset of random polynomials for the experiment. We acknowledge that it would be preferable to run such a test on an established dataset but this was not available. Of course, such a random datasets could be enlarged to increase variety almost indefinitely, but we needed to keep the experiment within computationally feasible boundaries. We emphasise the interesting initial finding in Section II-B that supposedly varied established sets can have hidden uniformity; and highlight that our generated dataset matched previously reported results [15], [57] for the topic of study with most, but not all, benefiting from GB preconditioning.\nA machine learned choice on whether to precondition was found to yield better results than either always preconditioning blindly, or using the previously human developed TNoI heuristic [57]. Two feature selection experiments showed that a small feature subset could be used. The two subsets identified were different but both needed features from before and after the GB preconditioning. For one, having fewer features actually improved the learning efficiency to 79%. Although a modest improvement on applying GB preconditioning blindly, we recall that the wrong choice can give large changes to the size of the CAD or even change the tractability of the problem."}, {"heading": "C. Future Work", "text": "There are many possible extensions to this project:\n\u2022 To see how the learned choice performs on a nonrandom, dataset. There is a large set derived from university mathematics entrance exams [44], which is not yet publicly available but may be in the future.\n\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.\n\u2022 In the present paper the variable ordering for CAD and the monomial ordering for GB were fixed. In reality, such decisions may also need to be made in tandem and it is an open problem as to how best to do this. The variable ordering can affect the choice of whether to use GB preconditioning and vice versa.\nFinally, we note there are other algorithm optimisation decisions for CAD, and indeed elsewhere in computer algebra."}, {"heading": "Acknowledgements", "text": "Thanks to David Wilson and James Bridge, our collaborators on [41], for useful conversations on the topic of machine learning to optimise computer algebra. This work was supported by EPSRC grant EP/J003247/1 and EU H2020FETOPEN-2016-2017-CSA project SC2 (712689)."}], "references": [{"title": "Cylindrical algebraic decomposition I: The basic algorithm", "author": ["D. Arnon", "G.E. Collins", "S. McCallum"], "venue": "SIAM J. Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1984}, {"title": "Assessing the accuracy of prediction algorithms for classification: An overview", "author": ["P. Baldi", "S. Brunak", "Y. Chauvin", "C.A.F. Andersen", "H. Nielsen"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Algorithms in Real Algebraic Geometry", "author": ["S. Basu", "R. Pollack", "M.F. Roy"], "venue": "Vol. 10 of Algorithms & Comp. in Math. Springer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Gr\u00f6bner bases using SAC2", "author": ["W. B\u00f6ge", "R. Gebauer", "H. Kredel"], "venue": "In EUROCAL \u201985,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1985}, {"title": "Truth table invariant cylindrical algebraic decomposition by regular chains", "author": ["R. Bradford", "C. Chen", "J.H. Davenport", "M. England", "M. Moreno Maza", "D. Wilson"], "venue": "In Computer Algebra in Scientific Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Cylindrical algebraic decompositions for boolean combinations", "author": ["R. Bradford", "J.H. Davenport", "M. England", "S. McCallum", "D. Wilson"], "venue": "In Proc. ISSAC", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Truth table invariant cylindrical algebraic decomposition", "author": ["R. Bradford", "J.H. Davenport", "M. England", "S. McCallum", "D. Wilson"], "venue": "J. Symbolic Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Optimising problem formulations for cylindrical algebraic decomposition", "author": ["R. Bradford", "J.H. Davenport", "M. England", "D. Wilson"], "venue": "In Intelligent Computer Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Machine learning for firstorder theorem proving", "author": ["J.P. Bridge", "S.B. Holden", "L.C. Paulson"], "venue": "J. Automated Reasoning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improved projection for cylindrical algebraic decomposition", "author": ["C.W. Brown"], "venue": "J. Symbolic Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Constructing a single open cell in a cylindrical algebraic decomposition", "author": ["C.W. Brown"], "venue": "In Proc. ISSAC", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "The complexity of quantifier elimination and cylindrical algebraic decomposition", "author": ["C.W. Brown", "J.H. Davenport"], "venue": "In Proc. ISSAC", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Algorithmic methods for investigating equilibria in epidemic modelling", "author": ["C.W. Brown", "M. El Kahoui", "D. Novotni", "A. Weber"], "venue": "J. Symbolic Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "An algorithm for finding the basis elements of the residue class ring of a zero dimensional polynomial ideal", "author": ["B. Buchberger"], "venue": "J. Symbolic Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1965}, {"title": "Speeding up quantifier elimination by Gr\u00f6bner bases", "author": ["B. Buchberger", "H. Hong"], "venue": "Tech. Report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1991}, {"title": "Real quantifier elimination in the RegularChains library", "author": ["C. Chen", "M. Moreno Maza"], "venue": "In Mathematical Software \u2013 ICMS 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Quantifier elimination by cylindrical algebraic decomposition based on regular chains", "author": ["C. Chen", "M. Moreno Maza"], "venue": "J. Symbolic Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Computing cylindrical algebraic decomposition via triangular decomposition", "author": ["C. Chen", "M. Moreno Maza", "B. Xia", "L. Yang"], "venue": "In Proc. ISSAC", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Quantifier elimination for real closed fields by cylindrical algebraic decomposition", "author": ["G.E. Collins"], "venue": "In Proc. 2nd GI Conference on Automata Theory and Formal Languages,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1975}, {"title": "The SAC-2 computer algebra system", "author": ["G.E. Collins"], "venue": "In EUROCAL \u201985,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1985}, {"title": "Partial cylindrical algebraic decomposition for quantifier elimination", "author": ["G.E. Collins", "H. Hong"], "venue": "J. Symbolic Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Program verification in the presence of complex numbers, functions with branch cuts etc", "author": ["J.H. Davenport", "R. Bradford", "M. England", "D. Wilson"], "venue": "In Proc. SYNASC", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Real quantifier elimination is doubly exponential", "author": ["J.H. Davenport", "J. Heintz"], "venue": "J. Symbolic Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Need polynomial systems be doubly exponential", "author": ["J.H. Davenport", "M. England"], "venue": "In Mathematical Software \u2013 ICMS 2016,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Efficient projection orders for CAD", "author": ["A. Dolzmann", "A. Seidl", "T. Sturm"], "venue": "In Proc. ISSAC", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Problem formulation for truth-table invariant cylindrical algebraic decomposition by incremental triangular decomposition", "author": ["M. England", "R. Bradford", "C. Chen", "J.H. Davenport", "M. Moreno Maza", "D. Wilson"], "venue": "In Intelligent Computer Mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Improving the use of equational constraints in cylindrical algebraic decomposition", "author": ["M. England", "R. Bradford", "J.H. Davenport"], "venue": "In Proc. ISSAC", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Choosing a variable ordering for truth-table invariant cylindrical algebraic decomposition by incremental triangular decomposition", "author": ["M. England", "R. Bradford", "J.H. Davenport", "D. Wilson"], "venue": "ICMS", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "The complexity of cylindrical algebraic decomposition with respect to polynomial degree", "author": ["M. England", "J.H. Davenport"], "venue": "To appear In: Computer Algebra in Scientific Computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Synthesis of optimal numerical algorithms using real quantifier elimination (Case Study: Square root computation)", "author": ["M. Erascu", "H. Hong"], "venue": "In Proc. ISSAC", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A new efficient algorithm for computing groebner bases without reduction to zero (F5)", "author": ["J.C. Faug\u00e8re"], "venue": "In Proc. ISSAC", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Multi-interval discretization of continuousvalued attributes for classification learning", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "In Proc. of the International Joint Conference on Uncertainty in AI, pages 1022\u20131027,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Nonlinear parametric optimization using cylindrical algebraic decomposition", "author": ["I.A. Fotiou", "P.A. Parrilo", "M. Morari"], "venue": "In Proc. CDC-ECC", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations Newsletter,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Correlation-based feature selection for discrete and numeric class machine learning", "author": ["M.A. Hall"], "venue": "In Proc. of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Benchmarking attribute selection techniques for discrete class data mining", "author": ["M.A. Hall", "G. Holmes"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Constructing fewer open cells by gcd computation in CAD projection", "author": ["J. Han", "L. Dai", "B. Xia"], "venue": "In Proc. ISSAC", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "An improvement of the projection operator in cylindrical algebraic decomposition", "author": ["H. Hong"], "venue": "In Proc. ISSAC", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1990}, {"title": "A practical guide to support vector classification", "author": ["C. Hsu", "C. Chang", "C. Lin"], "venue": "Tech. Report, Department of Computer Science, National Taiwan Uni.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Applying machine learning to the problem of choosing a heuristic to select the variable ordering for cylindrical algebraic decomposition", "author": ["Z. Huang", "M. England", "D. Wilson", "J.H. Davenport", "L. Paulson", "J. Bridge"], "venue": "In Intelligent Computer Mathematics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "An effective implementation of a symbolic-numeric cylindrical algebraic decomposition for quantifier elimination", "author": ["H. Iwane", "H. Yanami", "H. Anai", "K. Yokoyama"], "venue": "In Proc. SNC", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1999}, {"title": "Efficient subformula orders for real quantifier elimination of non-prenex formulas", "author": ["M. Kobayashi", "H. Iwane", "T. Matsuzaki", "H. Anai"], "venue": "In Mathematical Aspects of Computer and Information Sciences (MACIS \u201915),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme", "author": ["B.W. Matthews"], "venue": "Biochimica et Biophysica Acta (BBA)- Protein Structure,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1975}, {"title": "The complexity of the word problems for commutative semigroups and polynomial ideals", "author": ["E.W. Mayr", "A.R. Meyer"], "venue": "Advances in Mathematics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1982}, {"title": "An improved projection operation for cylindrical algebraic decomposition", "author": ["S. McCallum"], "venue": "Symbolic Computation,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1998}, {"title": "Metitarski: Past and future", "author": ["L.C. Paulson"], "venue": "In Interactive Theorem Proving,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Numerical Recipes in C (2nd Ed.): The Art of Scientific Computing", "author": ["W.H. Press", "S.A. Teukolsky", "W.T. Vetterling", "B.P. Flannery"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1992}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1986}, {"title": "Kernel methods in computational biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "J.-P. Vert"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "A mathematical theory of communication", "author": ["Claude E. Shannon"], "venue": "Mobile Computing and Communications Review,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Cylindrical algebraic decomposition using validated numerics", "author": ["A. Strzebo\u0144ski"], "venue": "J. Symbolic Computation,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2006}, {"title": "Cylindrical algebraic decomposition using local projections", "author": ["A. Strzebo\u0144ski"], "venue": "In Proc. ISSAC", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "A repository for CAD examples", "author": ["D.J. Wilson", "R.J. Bradford", "J.H. Davenport"], "venue": "ACM Comm. Computer Algebra,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}, {"title": "Speeding up cylindrical algebraic decomposition by Gr\u00f6bner bases", "author": ["D.J. Wilson", "R.J. Bradford", "J.H. Davenport"], "venue": "In Intelligent Computer Mathematics (LNCS", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "CADs and the first algorithm to compute them were introduced by Collins in 1975 [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "CAD usually has two stages: projection where an operator is applied recursively on the input to derive corresponding problems in lower dimensions; and lifting where CADs are built incrementally by dimension according to the polynomials identified in projection [1].", "startOffset": 261, "endOffset": 264}, {"referenceID": 32, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 162, "endOffset": 166}, {"referenceID": 47, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 184, "endOffset": 188}, {"referenceID": 21, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 228, "endOffset": 232}, {"referenceID": 29, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 273, "endOffset": 277}, {"referenceID": 22, "context": "CAD has worst case complexity doubly exponential in the number of variables [23] applicable whatever the data structure [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "CAD has worst case complexity doubly exponential in the number of variables [23] applicable whatever the data structure [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "For some applications there exist algorithms with better complexity [3], but CAD implementations still remain the best general purpose approach for many.", "startOffset": 68, "endOffset": 71}, {"referenceID": 38, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 161, "endOffset": 165}, {"referenceID": 46, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 167, "endOffset": 171}, {"referenceID": 9, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 173, "endOffset": 177}, {"referenceID": 37, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 179, "endOffset": 183}, {"referenceID": 20, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 231, "endOffset": 235}, {"referenceID": 53, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 274, "endOffset": 278}, {"referenceID": 41, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 280, "endOffset": 284}, {"referenceID": 5, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 26, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 122, "endOffset": 126}, {"referenceID": 54, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Their properties and an algorithm to derive a GB for any ideal was introduced by Buchberger in his PhD thesis of 1965 [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": "Like CAD, there has been much research to improve and optimise GB calculation, with the F5 algorithm [31] perhaps the most used approach currently.", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "However, also like CAD the calculation of GB is necessarily doubly exponential in the worst case [46] (when using a lexicographic monomial ordering).", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 20, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 210, "endOffset": 214}, {"referenceID": 56, "context": "[57].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The authors recreated the experiments of Buchberger and Hong [15] using QEPCAD-B for the CAD and MAPLE 16 for the GB.", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "As we may expect, there had been a big decrease in the computation timings, especially the GB: the two test problems previously intractable [15] could now have the GB calculated quickly.", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "The experiments were then extended to: a wider example set (an additional 12 problems); the alternative CAD implementation in MAPLE-16 [18]; and the case where we further precondition by reducing inequalities of the system (the set F above) with respect to the GB.", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "We work on the reasonable assumption that GB computation is cheap for the problems on which CAD is tractable (in fact as shown in [29] the CAD will compute resultants which overestimate the GB).", "startOffset": 130, "endOffset": 134}, {"referenceID": 40, "context": "We previously studied the choice of variable ordering for CAD in [41].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "[44] who applied a SVM to decide the order of sub-formulae solving for QE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "The CAD algorithm used was an implementation of [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "This is part of the RegularChains Library1 [16], [17] whose CAD procedures differ from the traditional projection and lifting framework of Collins, instead first decomposing C cylindrically and then refining to a CAD of R.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "This is part of the RegularChains Library1 [16], [17] whose CAD procedures differ from the traditional projection and lifting framework of Collins, instead first decomposing C cylindrically and then refining to a CAD of R.", "startOffset": 49, "endOffset": 53}, {"referenceID": 56, "context": "Previous experiments [57] showed this implementation has the same issues of GB preconditioning as the traditional approach.", "startOffset": 21, "endOffset": 25}, {"referenceID": 55, "context": "CAD problem sets such as [56] do not have anywhere near a sufficient number of problems to perform the experiment.", "startOffset": 25, "endOffset": 29}, {"referenceID": 40, "context": "In our previous study on choosing the variable ordering for CAD [41] we used the nlsat-dataset [58], which although developed for non-linear arithmetic SAT-solvers, contained many suitable problems.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "For each one of these problems the GB preconditioning was beneficial or made no difference; surprising as the experiments on much smaller datasets [15], [57] had shown much greater volatility.", "startOffset": 147, "endOffset": 151}, {"referenceID": 56, "context": "For each one of these problems the GB preconditioning was beneficial or made no difference; surprising as the experiments on much smaller datasets [15], [57] had shown much greater volatility.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "So our randomly generated dataset matched the previously found results of [15], [57] with most problems benefiting but not all and so is suitable for the purpose of this experiment.", "startOffset": 74, "endOffset": 78}, {"referenceID": 56, "context": "So our randomly generated dataset matched the previously found results of [15], [57] with most problems benefiting but not all and so is suitable for the purpose of this experiment.", "startOffset": 80, "endOffset": 84}, {"referenceID": 56, "context": "We also make use of the metric TNoI (see equation (1)) [57].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 117, "endOffset": 120}, {"referenceID": 25, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 128, "endOffset": 132}, {"referenceID": 50, "context": "In the last decade, the use of machine learning has spread rapidly following the invention of the Support Vector Machine (SVM) (see for example [51]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 52, "context": "An important concept in the SVM theory is the use of a kernel function to map data into a high dimensional feature space and then separate samples in the transformed space [53].", "startOffset": 172, "endOffset": 176}, {"referenceID": 42, "context": "For our experiment we used SVM-LIGHT2 [43]; an implementation of SVMs in C.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "This was chosen after earlier experiments applying machine learning to an automated theorem prover found the RBF kernel to perform well with similar simple algebraic features [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 44, "context": "Matthews\u2019 Correlation Coefficient (MCC) [45], [2] is often used to evaluate choices.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "Matthews\u2019 Correlation Coefficient (MCC) [45], [2] is often used to evaluate choices.", "startOffset": 46, "endOffset": 49}, {"referenceID": 39, "context": "We tested a commonly used range of values in our grid search process [40]: \u03b3 varied between {2\u221215, 2\u221214, 2\u221213, .", "startOffset": 69, "endOffset": 73}, {"referenceID": 56, "context": "Although only a small improvement on preconditioning blindly, we recall that the wrong choice can give large changes to the size of the CAD or even change the tractability of the problem [57].", "startOffset": 187, "endOffset": 191}, {"referenceID": 33, "context": "However, we cannot conclude this directly: earlier research shows that a variable completely useless by itself can provide a significant performance improvement when taken in conjunction with others [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 34, "context": "The feature selection experiments were conducted with WEKA (Waikato Environment for Knowledge Analysis) [35], a Java machine learning library which supports tasks such as data preprocessing, clustering, classification, regression and feature selection.", "startOffset": 104, "endOffset": 108}, {"referenceID": 36, "context": "A correlation based feature selection method, was applied as described in [37].", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "Unlike other filter methods [36], these measure the rank of feature subsets instead of individual features.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "With the exception of the class attribute all 28 features are continuous, so in order to have a common measure for computing the correlations we first discretize using the method of Fayyad and Irani [32].", "startOffset": 199, "endOffset": 203}, {"referenceID": 51, "context": "We define the entropy of a variable X [52] as", "startOffset": 38, "endOffset": 42}, {"referenceID": 49, "context": "The information gain (IG) [50] measures the amount by which the entropy of X decreases by additional information about X provided by Y , and it is given by", "startOffset": 26, "endOffset": 30}, {"referenceID": 48, "context": "The symmetrical uncertainty (SU) (a modified information gain measure) is then used to measure the correlation between two discrete variables (X and Y) [49]:", "startOffset": 152, "endOffset": 156}, {"referenceID": 56, "context": "We may compare the machine learned choice with the human developed TNoI heuristic [57], whose performance on the 213 test problems is shown in Table V.", "startOffset": 82, "endOffset": 86}, {"referenceID": 56, "context": "[57].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We emphasise the interesting initial finding in Section II-B that supposedly varied established sets can have hidden uniformity; and highlight that our generated dataset matched previously reported results [15], [57] for the topic of study with most, but not all, benefiting from GB preconditioning.", "startOffset": 206, "endOffset": 210}, {"referenceID": 56, "context": "We emphasise the interesting initial finding in Section II-B that supposedly varied established sets can have hidden uniformity; and highlight that our generated dataset matched previously reported results [15], [57] for the topic of study with most, but not all, benefiting from GB preconditioning.", "startOffset": 212, "endOffset": 216}, {"referenceID": 56, "context": "A machine learned choice on whether to precondition was found to yield better results than either always preconditioning blindly, or using the previously human developed TNoI heuristic [57].", "startOffset": 185, "endOffset": 189}, {"referenceID": 56, "context": "THE PERFORMANCE OF THE TNOI-BASED HEURISTIC [57]", "startOffset": 44, "endOffset": 48}, {"referenceID": 43, "context": "There is a large set derived from university mathematics entrance exams [44], which is not yet publicly available but may be in the future.", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.", "startOffset": 92, "endOffset": 96}, {"referenceID": 40, "context": "Thanks to David Wilson and James Bridge, our collaborators on [41], for useful conversations on the topic of machine learning to optimise computer algebra.", "startOffset": 62, "endOffset": 66}], "year": 2016, "abstractText": "Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. However, it can be expensive, with worst case complexity doubly exponential in the size of the input. Hence it is important to formulate the problem in the best manner for the CAD algorithm. One possibility is to precondition the input polynomials using Groebner Basis (GB) theory. Previous experiments have shown that while this can often be very beneficial to the CAD algorithm, for some problems it can significantly worsen the CAD performance. In the present paper we investigate whether machine learning, specifically a support vector machine (SVM), may be used to identify those CAD problems which benefit from GB preconditioning. We run experiments with over 1000 problems (many times larger than previous studies) and find that the machine learned choice does better than the human-made heuristic.", "creator": "LaTeX with hyperref package"}}}