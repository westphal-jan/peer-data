{"id": "1608.04320", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Correlated-PCA: Principal Components' Analysis when Data and Noise are Correlated", "abstract": "given a matrix of observed differential data, principal components analysis ( pca ) one computes a given small number of orthogonal correlation directions that contain most of solely its variability. provably accurate solutions for pca variance have been in use for ten decades. however, to the best of our knowledge, constructing all existing theoretical guarantees for it assume requires that the data and the corrupting signal noise are mutually mutually independent, or at least uncorrelated. this is valid in practice often, but not always. in this paper, we study the complex pca problem in the setting where the data and noise can effectively be correlated. such noise is often best referred to as \" data - dependent quantum noise \". we obtain both a correctness result for the standard eigenvalue decomposition ( evd ) sample based solution to pca under simple assumptions on the data - noise correlation. we also develop and analyze a generalization of evd, called cluster - evd, and argue collectively that it reduces the sample complexity of evd in designing certain regimes.", "histories": [["v1", "Mon, 15 Aug 2016 16:32:57 GMT  (202kb)", "https://arxiv.org/abs/1608.04320v1", "Under submission to IEEE Transactions on Signal Processing, A part of this work will appear at NIPS 2016"], ["v2", "Wed, 2 Nov 2016 17:55:02 GMT  (57kb)", "http://arxiv.org/abs/1608.04320v2", "NIPS 2016 (to appear). Longer version submitted to IEEE Trans. Sig. Proc. is atthis http URL"]], "COMMENTS": "Under submission to IEEE Transactions on Signal Processing, A part of this work will appear at NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["namrata vaswani", "han guo"], "accepted": true, "id": "1608.04320"}, "pdf": {"name": "1608.04320.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["namrata@iastate.edu", "hanguo@iastate.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n04 32\n0v 2\n[ cs\n.L G\n] 2\nN ov"}, {"heading": "1 Introduction", "text": "Principal Components Analysis (PCA) is among the most frequently used tools for dimension reduction. Given a matrix of data, it computes a small number of orthogonal directions that contain all (or most) of the variability of the data. The subspace spanned by these directions is the \u201cprincipal subspace\u201d. To use PCA for dimension reduction, one projects the observed data onto this subspace. The standard solution to PCA is to compute the reduced singular value decomposition (SVD) of the data matrix, or, equivalently, to compute the reduced eigenvalue decomposition (EVD) of the empirical covariance matrix of the data. If all eigenvalues are nonzero, a threshold is used and all eigenvectors with eigenvalues above the threshold are retained. This solution, which we henceforth refer to as simple EVD, or just EVD, has been used for many decades and is well-studied in literature, e.g., see [1] and references therein. However, to the best of our knowledge, all existing results for it assume that the true data and the corrupting noise in the observed data are independent, or, at least, uncorrelated. This is valid in practice often, but not always. Here, we study the PCA problem in the setting where the data and noise vectors may be correlated (correlated-PCA). Such noise is sometimes called \u201cdata-dependent\u201d noise.\nContributions. (1) Under a boundedness assumption on the true data vectors, and some other assumptions, for a fixed desired subspace error level, we show that the sample complexity of simpleEVD for correlated-PCA scales as f2r2 logn where n is the data vector length, f is the condition number of the true data covariance matrix and r is its rank. Here \u201csample complexity\u201d refers to the number of samples needed to get a small enough subspace recovery error with high probability (whp). The dependence on f2 is problematic for datasets with large condition numbers, and, especially in the high dimensional setting where n is large. (2) To address this, we also develop and analyze a generalization of simple-EVD, called cluster-EVD. Under an eigenvalues\u2019 \u201cclustering\u201d assumption, cluster-EVD weakens the dependence on f .\nTo our best knowledge, the correlated-PCA problem has not been explicitly studied. We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5]. The version of correlated-PCA studied here is motivated\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nby these works. Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.\nNotation. We use the interval notation [a, b] to mean all of the integers between a and b, inclusive, and similarly for [a, b) etc. We use \u2016 \u00b7 \u2016 to denote the l2 norm of a vector or the induced l2 norm of a matrix. For other lp norms, we use \u2016 \u00b7 \u2016p. For a set T , IT refers to an n\u00d7 |T | matrix of columns of the identity matrix indexed by entries in T . For a matrix A, AT := AIT . A tall matrix with orthonormal columns is referred to as a basis matrix. For basis matrices P\u0302 and P , we quantify the subspace error (SE) between their range spaces using\nSE(P\u0302 ,P ) := \u2016(I \u2212 P\u0302 P\u0302 \u2032)P \u2016. (1)"}, {"heading": "1.1 Correlated-PCA: Problem Definition", "text": "We are given a time sequence of data vectors, yt, that satisfy\nyt = \u2113t +wt, with wt = Mt\u2113t and \u2113t = Pat (2)\nwhere P is an n \u00d7 r basis matrix with r \u226a n. Here \u2113t is the true data vector that lies in a low dimensional subspace of Rn, range(P ); at is its projection into this r-dimensional subspace; and wt is the data-dependent noise. We refer to Mt as the correlation / data-dependency matrix. The goal is to estimate range(P ). We make the following assumptions on \u2113t and Mt.\nAssumption 1.1. The subspace projection coefficients, at, are zero mean, mutually independent and bounded random vectors (r.v.), with a diagonal covariance matrix \u039b. Define \u03bb\u2212 := \u03bbmin(\u039b), \u03bb+ := \u03bbmax(\u039b) and f := \u03bb +\n\u03bb\u2212 . Since the at\u2019s are bounded, we can also define a finite constant\n\u03b7 := maxj=1,2,...r maxt (at)\n2 j\n\u03bbj . Thus, (at)2j \u2264 \u03b7\u03bbj .\nFor most bounded distributions, \u03b7 will be a small constant more than one, e.g., if the distribution of all entries of at is iid zero mean uniform, then \u03b7 = 3. From Assumption 1.1, clearly, the \u2113t\u2019s are also zero mean, bounded, and mutually independent r.v.\u2019s with a rank r covariance matrix \u03a3 EVD = P\u039bP \u2032. In the model, for simplicity, we assume \u039b to be fixed. However, even if we replace \u039b by \u039bt and define \u03bb\u2212 = mint \u03bbmin(\u039bt) and \u03bb+ = \u03bbmax(\u039bt), all our results will still hold.\nAssumption 1.2. Decompose Mt as Mt = M2,tM1,t. Assume that\n\u2016M1,tP \u2016 \u2264 q < 1, \u2016M2,t\u2016 \u2264 1, (3) and, for any sequence of positive semi-definite Hermitian matrices, At, the following holds\nfor a \u03b2 < \u03b1,\n\u2225 \u2225 \u2225 \u2225 \u2225 1 \u03b1 \u03b1 \u2211\nt=1\nM2,tAtM2,t \u2032\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 \u03b2 \u03b1 max t\u2208[1,\u03b1] \u2016At\u2016. (4)\nWe will need the above to hold for all \u03b1 \u2265 \u03b10 and for all \u03b2 \u2264 c0\u03b1 with a c0 \u226a 1. We set \u03b10 and c0 in Theorems 2.1 and 3.3; both will depend on q. Observe that, using (3), \u2016wt\u2016\u2016\u2113t\u2016 \u2264 q, and so q is an upper bound on the signal-to-noise ratio (SNR).\nTo understand the assumption on M2,t, notice that, if we allow \u03b2 = \u03b1, then (4) always holds and is not an assumption. Let B denote the matrix on the LHS of (4). One example situation when (4) holds with a \u03b2 \u226a \u03b1 is if B is block-diagonal with blocks At. In this case, it holds with \u03b2 = 1. In fact, it also holds with \u03b2 = 1 if B is permutation-similar to a block diagonal matrix. The matrix B will be of this form if M2,t = ITt with all the sets Tt being mutually disjoint. More generally, if B is permutation-similar to a block-diagonal matrix with blocks given by the summation of At\u2019s over at most \u03b20 < \u03b1 time instants, then (4) holds with \u03b2 = \u03b20. This will happen if M2,t = ITt with Tt = T [k] for at most \u03b20 time instants and if sets T [k] are mutually disjoint for different k. Finally, the T [k]\u2019s need not even be mutually disjoint. As long as they are such that B is a matrix with nonzero blocks on only the main diagonal and on a few diagonals near it, e.g., if it is block tri-diagonal, it can be shown that the above assumption holds. This example is generalized in Assumption 1.3 given below."}, {"heading": "1.2 Examples of correlated-PCA problems", "text": "One key example of correlated-PCA is the PCA with missing data (PCA-missing) problem. Let Tt denote the set of missing entries at time t. Suppose, we set the missing entries of yt to zero. Then,\nyt = \u2113t \u2212 ITtITt \u2032\u2113t. (5)\nIn this case M2,t = ITt and M1,t = \u2212ITt \u2032. Thus, q is an upper bound on \u2016ITt \u2032P \u2016. Clearly, it will be small if the columns of P are dense vectors. For the reader familiar with low-rank matrix completion (MC), e.g., [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix. This would, of course, be much more expensive than directly solving PCA-missing and would need more assumptions.\nAnother example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with \u2113t. Let Tt denote the support set of wt and let xt be the |Tt|-length vector of its nonzero entries. If we assume linear dependency of xt on \u2113t, we can write out yt as\nyt = \u2113t + ITtxt = \u2113t + ITtMs,t\u2113t. (6) Thus M2,t = ITt and M1,t = Ms,t and so q is an upper bound on \u2016Ms,tP \u2016. In the rest of the paper, we refer to this problem is \u201cPCA with sparse data-dependent corruptions (PCA-SDDC)\u201d. One key application where it occurs is in foreground-background separation for videos consisting of a slow changing background sequence (modeled as lying close to a low-dimensional subspace) and a sparse foreground image sequence consisting typically of one or more moving objects [14]. The PCA-SDDC problem is to estimate the background sequence\u2019s subspace. In this case, \u2113t is the background image at time t, Tt is the support set of the foreground image at t, and xt is the difference between foreground and background intensities on Tt. An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L. However, as shown in Sec. 5, Table 1, this approach will be much slower; and it will work only if its required incoherence assumptions hold. For example, if the columns of P are sparse, it fails.\nFor both problems above, a solution for PCA will work only when the corrupting noise wt is small compared to \u2113t. A sufficient condition for this is that q is small.\nA third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5]. We refer the reader to [18] to understand this application.\nIn all three of the above applications, the assumptions on the data-noise correlation matrix given in Assumption 1.2 hold if there are enough changes of a certain type in the set of missing or corrupted entries, Tt. One example where this is true is in case of a 1D object of length s or less that remains static for at most \u03b2 frames at a time. When it moves, it moves by at least a certain fraction of s pixels. The following assumption is inspired by the object\u2019s support. Assumption 1.3. Let l denote the number of times the set Tt changes in the interval [1, \u03b1] (or in any given interval of length \u03b1 in case of dynamic robust PCA). So 0 \u2264 l \u2264 \u03b1 \u2212 1. Let t0 := 1; let tk, with tk < tk+1, denote the time instants in this interval at which Tt changes; and let T [k] denote the distinct sets. In other words, Tt = T [k] for t \u2208 [tk, tk+1), for each k = 1, 2, . . . , l. Assume that the following hold with a \u03b2 < \u03b1:\n1. (tk+1 \u2212 tk) \u2264 \u03b2\u0303 and |T [k]| \u2264 s;\n2. \u03c12\u03b2\u0303 \u2264 \u03b2 where \u03c1 is the smallest positive integer so that, for any 0 \u2264 k \u2264 l, T [k] and T [k+\u03c1] are disjoint;\n3. for any k1, k2 satisfying 0 \u2264 k1 < k2 \u2264 l, the sets (T [k1] \\ T [k1+1]) and (T [k2] \\ T [k2+1]) are disjoint.\nAn implicit assumption for condition 3 to hold is that \u2211l k=0 |T [k] \\ T [k+1]| \u2264 n. Observe that conditions 2 and 3 enforce an upper bound on the maximum support size s.\nTo connect Assumption 1.3 with the moving object example given above, condition 1 holds if the object\u2019s size is at most s and if it moves at least once every \u03b2\u0303 frames. Condition 2 holds, if, every time it moves, it moves in the same direction and by at least s\n\u03c1 pixels. Condition 3 holds if, every\ntime it moves, it moves in the same direction and by at most d0 \u2265 s\u03c1 pixels, with d0\u03b1 \u2264 n (or, more generally, the motion is such that, if the object were to move at each frame, and if it started at the top of the frame, it does not reach the bottom of the frame in a time interval of length \u03b1).\nThe following lemma [4] shows that, with Assumption 1.3 on Tt, M2,t = ITt satisfies the assumption on M2,t given in Assumption 1.2. Its proof generalizes the discussion below Assumption 1.2.\nLemma 1.4. [[4], Lemmas 5.2 and 5.3] Assume that Assumption 1.3 holds. For any sequence of |Tt| \u00d7 |Tt| symmetric positive-semi-definite matrices At,\n\u2016 \u03b1 \u2211\nt=1\nITtAtITt \u2032\u2016 \u2264 (\u03c12\u03b2\u0303) max t\u2208[1,\u03b1] \u2016At\u2016 \u2264 \u03b2 max t\u2208[1,\u03b1] \u2016At\u2016\nThus, if \u2016ITt \u2032P \u2016 \u2264 q < 1, then the PCA-missing problem satisfies Assumption 1.2. If \u2016Ms,tP \u2016 \u2264 q < 1, then the PCA-SDDC problem satisfies Assumption 1.2.\nAssumption 1.3 is one model on Tt that ensures that, if M2,t = ITt , the assumption on M2,t given in Assumption 1.2 holds. For its many generalizations, see Supplementary Material, Sec. 7, or [4]."}, {"heading": "2 Simple EVD", "text": "Simple EVD computes the top eigenvectors of the empirical covariance matrix, 1\n\u03b1\n\u2211\u03b1 t=1 ytyt \u2032, of\nthe observed data. The following can be shown.\nTheorem 2.1 (simple-EVD result). Let P\u0302 denote the matrix containing all the eigenvectors of 1 \u03b1 \u2211\u03b1 t=1 ytyt\n\u2032 with eigenvalues above a threshold, \u03bbthresh, as its columns. Pick a \u03b6 so that r\u03b6 \u2264 0.01. Suppose that yt\u2019s satisfy (2) and the following hold.\n1. Assumption 1.1 on \u2113t holds. Define\n\u03b10 := C\u03b7 2 r\n211 logn\n(r\u03b6)2 max(f, qf, q2f)2, C :=\n32\n0.012 .\n2. Assumption 1.2 on Mt holds for any \u03b1 \u2265 \u03b10 and for any \u03b2 satisfying \u03b2\n\u03b1 \u2264\n( 1\u2212 r\u03b6 2\n)2\nmin\n(\n(r\u03b6)2\n4.1(qf)2 , (r\u03b6) q2f\n)\n3. Set algorithm parameters \u03bbthresh = 0.95\u03bb\u2212 and \u03b1 \u2265 \u03b10.\nThen, with probability at least 1\u2212 6n\u221210, SE(P\u0302 ,P ) \u2264 r\u03b6. Proof: The proof involves a careful application of the sin \u03b8 theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin \u03b8 bound. It is given in the Supplementary Material, Section 8.\nConsider the lower bound on \u03b1. We refer to this as the \u201csample complexity\u201d. Since q < 1, and \u03b7 is a small constant (e.g., for the uniform distribution, \u03b7 = 3), for a fixed error level, r\u03b6, \u03b10 simplifies to cf2r2 logn. Notice that the dependence on n is logarithmic. It is possible to show that the sample complexity scales as logn because we assume that the \u2113t\u2019s are bounded r.v.s. As a result we can apply the matrix Hoeffding inequality [20] to bound the perturbation between the observed data\u2019s empirical covariance matrix and that of the true data. The bounded r.v. assumption is actually a more practical one than the usual Gaussian assumption since most sources of data have finite power.\nBy replacing matrix Hoeffding by Theorem 5.39 of [21] in places where one can apply a concentration of measure result to \u2211\nt atat \u2032/\u03b1 (which is at r \u00d7 r matrix), and by matrix Bernstein [20] else-\nwhere, it should be possible to further reduce the sample complexity to cmax((qf)2r logn, f2(r+ logn)). It should also be possible remove the boundedness assumption and replace it by a Gaussian (or a sub-Gaussian) assumption, but, that would increase the sample complexity to c(qf)2n.\nConsider the upper bound on \u03b2/\u03b1. Clearly, the smaller term is the first one. This depends on 1/(qf)2. Thus, when f is large and q is not small enough, the bound required may be impractically small. As will be evident from the proof (see Remark 8.3 in Supplementary Material), we get this bound because wt is correlated with \u2113t and this results in E[\u2113twt\u2032] 6= 0. If wt and \u2113t were uncorrelated, qf would get replaced by \u03bbmax(Cov(wt)) \u03bb\u2212\nin the upper bound on \u03b2/\u03b1 as well as in the sample complexity.\nApplication to PCA-missing and PCA-SDDC. By Lemma 1.4, the following is immediate. Corollary 2.2. Consider the PCA-missing model, (5), and assume that maxt \u2016ITt \u2032P \u2016 \u2264 q < 1; or consider the PCA-SDDC model, (6), and assume that maxt \u2016Ms,tP \u2016 \u2264 q < 1. Assume that everything in Theorem 2.1 holds except that we replace Assumption 1.2 by Assumption 1.3. Then, with probability at least 1\u2212 6n\u221210, SE(P\u0302 ,P ) \u2264 r\u03b6."}, {"heading": "3 Cluster-EVD", "text": "To try to relax the strong dependence on f2 of the result above, we develop a generalization of simple-EVD that we call cluster-EVD. This requires the clustering assumption."}, {"heading": "3.1 Clustering assumption", "text": "To state the assumption, define the following partition of the index set {1, 2, . . . r} based on the eigenvalues of \u03a3. Let \u03bbi denote its i-th largest eigenvalue. Definition 3.1 (g-condition-number partition of {1, 2, . . . r}). Define G1 = {1, 2, . . . r1} where r1 is the index for which \u03bb1\n\u03bbr1 \u2264 g and \u03bb1 \u03bbr1+1 > g. In words, to define G1, start with the index of the first\n(largest) eigenvalue and keep adding indices of the smaller eigenvalues to the set until the ratio of the maximum to the minimum eigenvalue first exceeds g.\nFor each k > 1, define Gk = {r\u2217+1, r\u2217+2, . . . , r\u2217+rk} where r\u2217 = ( \u2211k\u22121 i=1 ri), rk is the index for which \u03bbr\u2217+1 \u03bbr\u2217+rk \u2264 g and \u03bbr\u2217+1 \u03bbr\u2217+rk+1\n> g. In words, to define Gk , start with the index of the (r\u2217 + 1)-th eigenvalue, and repeat the above procedure.\nStop when \u03bbr\u2217+rk+1 = 0, i.e., when there are no more nonzero eigenvalues. Define \u03d1 = k as the number of sets in the partition. Thus {G1,G2, . . . ,G\u03d1} is the desired partition.\nDefine G0 = [.], Gk := (P )Gk , \u03bb + k := maxi\u2208Gk \u03bbi (\u039b), \u03bb \u2212 k := mini\u2208Gk \u03bbi (\u039b) and\n\u03c7 := max k=1,2,...,\u03d1 \u03bb+k+1 \u03bb\u2212k .\n\u03c7 quantifies the \u201cdistance\u201d between consecutive sets of the above partition. Moreover, by definition, \u03bb +\nk \u03bb \u2212\nk\n\u2264 g. Clearly, g \u2265 1 and \u03c7 \u2264 1 always. We assume the following.\nAssumption 3.2. For a 1 \u2264 g+ < f and a 0 \u2264 \u03c7+ < 1, assume that there exists a g satisfying 1 \u2264 g \u2264 g+ and a \u03c7 satisfying 0 \u2264 \u03c7 \u2264 \u03c7+, for which we can define a g-condition-number partition of {1, 2, . . . r} that satisfies \u03c7 \u2264 \u03c7+. The number of sets in the partition is \u03d1. When g+ and \u03c7+ are small, we say that the eigenvalues are \u201cwell-clustered\u201d with \u201cclusters\u201d, Gk.\nThis assumption can be understood as a generalization of the eigen-gap condition needed by the block power method, which is a fast algorithm for obtaining the k top eigenvectors of a matrix [22]. We expect it to hold for data that has variability across different scales. The large scale variations would result in the first (largest eigenvalues\u2019) cluster and the smaller scale variations would form the later clusters. This would be true, for example, for video \u201ctextures\u201d such as moving waters or waving trees in a forest. We tested this assumption on some such videos. We describe our conclusions here for three videos - \u201clake\u201d (video of moving lake waters), \u201cwaving-tree\u201d (video consisting of waving trees), and \u201ccurtain\u201d (video of window curtains moving due to the wind). For each video, we first made it low-rank by keeping the eigenvectors corresponding to the smallest number of eigenvalues that contain at least 90% of the total energy and projecting the video onto this subspace. For the low-rankified lake video, f = 74 and Assumption 3.2 holds with \u03d1 = 6 clusters, g+ = 2.6 and \u03c7+ = 0.7. For the waving-tree video, f = 180 and Assumption 3.2 holds with \u03d1 = 6, g+ = 9.4 and \u03c7+ = 0.72. For the curtain video, f = 107 and the assumption holds \u03d1 = 3, g+ = 16.1 and \u03c7+ = 0.5. We show the clusters of eigenvalues in Fig. 1."}, {"heading": "3.2 Cluster-EVD algorithm", "text": "The cluster-EVD approach is summarized in Algorithm 1. I Its main idea is as follows. We start by computing the empirical covariance matrix of the first set of \u03b1 observed data points, D\u03021 := 1 \u03b1 \u2211\u03b1 t=1 ytyt \u2032. Let \u03bb\u0302i denote its i-th largest eigenvalue. To estimate the first cluster, G\u03021, we start with the index of the first (largest) eigenvalue and keep adding indices of the smaller eigenvalues\nAlgorithm 1 Cluster-EVD Parameters: \u03b1, g\u0302, \u03bbthresh. Set G\u03020 \u2190 [.]. Set the flag Stop \u2190 0. Set k \u2190 1. Repeat\n1. Let G\u0302det,k := [G\u03020, G\u03021, . . . G\u0302k\u22121] and let \u03a8k := (I \u2212 G\u0302det,kG\u0302det,k\u2032). Notice that \u03a81 = I. Compute\nD\u0302k = \u03a8k\n\n\n1\n\u03b1\nk\u03b1 \u2211\nt=(k\u22121)\u03b1+1 ytyt\n\u2032\n\n\u03a8k\n2. Find the k-th cluster, G\u0302k: let \u03bb\u0302i = \u03bbi(D\u0302k); (a) find the index r\u0302k for which \u03bb\u03021\n\u03bb\u0302r\u0302k \u2264 g\u0302 and either \u03bb\u03021 \u03bb\u0302r\u0302k+1 > g\u0302 or \u03bb\u0302r\u0302k+1 < \u03bbthresh;\n(b) set G\u0302k = {r\u0302\u2217 + 1, r\u0302\u2217 + 2, . . . , r\u0302\u2217 + r\u0302k} where r\u0302\u2217 := \u2211k\u22121 j=1 r\u0302j ;\n(c) if \u03bb\u0302r\u0302k+1 < \u03bbthresh, update the flag Stop \u2190 1 3. Compute G\u0302k \u2190 eigenvectors(D\u0302k, r\u0302k); increment k\nUntil Stop == 1. Set \u03d1\u0302 \u2190 k. Output P\u0302 \u2190 [G\u03021 \u00b7 \u00b7 \u00b7 G\u0302\u03d1\u0302]. eigenvectors(M, r) returns a basis matrix for the span of the top r eigenvectors of M.\nto it until the ratio of the maximum to the minimum eigenvalue exceeds g\u0302 or until the minimum eigenvalue goes below a \u201czero threshold\u201d, \u03bbthresh. Then, we estimate the first cluster\u2019s subspace, range(G1) by computing the top r\u03021 eigenvectors of D\u03021. To get the second cluster and its subspace, we project the next set of \u03b1 yt\u2019s orthogonal to G\u03021 followed by repeating the above procedure. This is repeated for each k > 1. The algorithm stops when \u03bb\u0302r\u0302k+1 < \u03bbthresh.\nAlgorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS. The one introduced in [3] assumed that the clusters were known to the algorithm (which is unrealistic). The one studied in [5] has an automatic cluster estimation approach, but, one that needs a larger lower bound on \u03b1 compared to what Algorithm 1 needs."}, {"heading": "3.3 Main result", "text": "We give the performance guarantee for Algorithm 1 here. Its parameters are set as follows. We set g\u0302 to a value that is a little larger than g. This is needed to allow for the fact that \u03bb\u0302i is not equal to the i-th eigenvalue of \u039b but is within a small margin of it. For the same reason, we need to also use a nonzero \u201czeroing\u201d threshold, \u03bbthresh, that is larger than zero but smaller than \u03bb\u2212. We set \u03b1 large enough to ensure that SE(P\u0302 ,P ) \u2264 r\u03b6 holds with a high enough probability. Theorem 3.3 (cluster-EVD result). Consider Algorithm 1. Pick a \u03b6 so that r2\u03b6 \u2264 0.0001, and r2\u03b6f \u2264 0.01. Suppose that yt\u2019s satisfy (2) and the following hold.\n1. Assumption 1.1 and Assumption 3.2 on \u2113t hold with \u03c7+ satisfying \u03c7+ \u2264 min(1 \u2212 r\u03b6 \u2212 0.08 0.25 , g+\u22120.0001 1.01g++0.0001 \u2212 0.0001). Define\n\u03b10 := C\u03b7 2 r\n2(11 logn+ log\u03d1)\n(r\u03b6)2 max(g+, qg+,\nq2f, q(r\u03b6)f, (r\u03b6)2f, q \u221a fg+, (r\u03b6) \u221a fg+)2, C := 32 \u00b7 16 0.012 .\n2. Assumption 1.2 on Mt holds with \u03b1 \u2265 \u03b10 and with \u03b2 satisfying \u03b2\n\u03b1 \u2264\n( (1\u2212 r\u03b6 \u2212 \u03c7+) 2\n)2\nmin\n(\n(rk\u03b6) 2\n4.1(qg+)2 , (rk\u03b6) q2f\n)\n.\n3. Set algorithm parameters g\u0302 = 1.01g+ + 0.0001, \u03bbthresh = 0.95\u03bb\u2212 and \u03b1 \u2265 \u03b10.\nThen, with probability at least 1\u2212 12n\u221210, SE(P\u0302 ,P ) \u2264 r\u03b6.\nProof: The proof is given in Section 9 in Supplementary Material.\nWe can also get corollaries for PCA-missing and PCA-SDDC for cluster-EVD. We have given one specific value for g\u0302 and \u03bbthresh in Theorem 3.3 for simplicity. One can, in fact, set g\u0302 to be anything that satisfies (12) given in Supplementary Material and one can set \u03bbthresh to be anything satisfying 5r\u03b6\u03bb\u2212 \u2264 \u03bbthresh \u2264 0.95\u03bb\u2212. Also, it should be possible to reduce the sample complexity of clusterEVD to cmax(q2(g+)2r logn, (g+)2(r + logn)) using the approach explained in Sec. 2."}, {"heading": "4 Discussion", "text": "Comparing simple-EVD and cluster-EVD. Consider the lower bounds on \u03b1. In the cluster-EVD (c-EVD) result, Theorem 3.3, if q is small enough (e.g., if q \u2264 1/\u221af ), and if (r2\u03b6)f \u2264 0.01, it is clear that the maximum in the max(., ., ., .) expression is achieved by (g+)2. Thus, in this regime, c-EVD needs \u03b1 \u2265 C r 2(11 logn+log \u03d1)\n(r\u03b6)2 g 2 and its sample complexity is \u03d1\u03b1. In the EVD result\n(Theorem 2.1), g+ gets replaced by f and \u03d1 by 1, and so, its sample complexity, \u03b1 \u2265 C r211 logn(r\u03b6)2 f2. In situations where the condition number f is very large but g+ is much smaller and \u03d1 is small (the clustering assumption holds well), the sample complexity of c-EVD will be much smaller than that of simple-EVD. However, notice that, the lower bound on \u03b1 for simple-EVD holds for any q < 1 and for any \u03b6 with r\u03b6 < 0.01 while the c-EVD lower bound given above holds only when q is small enough, e.g., q = O(1/ \u221a f), and \u03b6 is small enough, e.g., r\u03b6 = O(1/f). This tighter bound on \u03b6 is needed because the error of the k-th step of c-EVD depends on the errors of the previous steps times f . Secondly, the c-EVD result also needs \u03c7+ and \u03d1 to be small (clustering assumption holds well), whereas, for simple-EVD, by definition, \u03c7+ = 0 and \u03d1 = 1. Another thing to note is that the constants in both lower bounds are very large with the c-EVD one being even larger.\nTo compare the upper bounds on \u03b2, assume that the same \u03b1 is used by both, i.e., \u03b1 = max(\u03b10(EVD), \u03b10(c-EVD)). As long as rk is large enough, \u03c7+ is small enough, and g is small enough, the upper bound on \u03b2 needed by the c-EVD result is significantly looser. For example, if \u03c7+ = 0.2, \u03d1 = 2, rk = r/2, then c-EVD needs \u03b2 \u2264 (0.5 \u00b7 0.79 \u00b7 0.5)2 (r\u03b6) 2\n4.1q2g2\u03b1 while simple-EVD\nneeds \u03b2 \u2264 (0.5 \u00b7 0.99)2 (r\u03b6) 2\n4.1q2f2\u03b1. If g = 3 but f = 100, clearly the c-EVD bound is looser.\nComparison with other results for PCA-SDDC and PCA-missing. To our knowledge, there is no other result for correlated-PCA. Hence, we provide comparisons of the corollaries given above for the PCA-missing and PCA-SDDC special cases with works that also study these or related problems. An alternative solution for either PCA-missing or PCA-SDDC is to first recover the entire matrix L and then compute its subspace via SVD on the estimated L. For the PCA-missing problem, this can be done by using any of the low-rank matrix completion techniques, e.g., nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23]. Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].\nHowever, as explained earlier doing the above has two main disadvantages. The first is that it is much slower (see Sec. 5). The difference in speed is most dramatic when solving the matrix-sized convex programs such as NNM or PCP, but even the Alt-Min methods are slower. If we use the time complexity from [17], then finding the span of the top k singular vectors of an n \u00d7m matrix takes O(nmk) time. Thus, if \u03d1 is a constant, both simple-EVD and c-EVD need O(n\u03b1r) time, whereas, Alt-Min-RPCA needs O(n\u03b1r2) time per iteration [17]. The second disadvantage is that the above methods for MC or RPCA need more assumptions to provably correctly recover L. All the above methods need an incoherence assumption on both the left singular vectors, P , and the right singular vectors, V , of L. Of course, it is possible that, if one studies these methods with the goal of only recovering the column space of L correctly, the incoherence assumption on the right singular vectors is not needed. From simulation experiments (see Sec. 5), the incoherence of the left singular vectors is definitely needed. On the other hand, for the PCA-SDDC problem, simple-EVD or c-EVD do not even need the incoherence assumption on P .\nThe disadvantage of both EVD and c-EVD, or in fact of any solution for the PCA problem, is that they work only when q is small enough (the corrupting noise is small compared to \u2113t)."}, {"heading": "5 Numerical Experiments", "text": "We use the PCA-SDDC problem as our case study example. We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code\nfrom the authors\u2019 webpage). For both PCP and Alt-Min-RPCA, P\u0302 is recovered as the top r eigenvectors of of the estimated L. To show the advantage of EVD or c-EVD, we let \u2113t = Pat with columns of P being sparse. These were chosen as the first r = 5 columns of the identity matrix. We generate at\u2019s iid uniformly with zero mean and covariance matrix \u039b = diag(100, 100, 100, 0.1, 0.1). Thus the condition number f = 1000. The clustering assumption holds with \u03d1 = 2, g+ = 1 and \u03c7+ = 0.001. The noise wt is generated as wt = ITtMs,t\u2113t with Tt generated to satisfy Assumption 1.3 with s = 5, \u03c1 = 2, and \u03b2\u0303 = 1; and the entries of Ms,t being iid N (0, q2) with q = 0.01. We used n = 500. EVD and c-EVD (Algorithm 1) were implemented with \u03b1 = 300, \u03bbthresh = 0.095, g\u0302 = 3. 10000-time Monte Carlo averaged values of SE(P\u0302 ,P ) and execution time are shown in the first row of Table 1. Since the columns of P are sparse, both PCP and Alt-Min-RPCA fail. Both have average SE close to one whereas the average SE of c-EVD and EVD is 0.0908 and 0.0911 respectively. Also, both EVD and c-EVD are much faster than the other two. We also did an experiment with the settings of this experiment, but with P dense. In this case, EVD and c-EVD errors were similar, but PCP and Alt-Min-RPCA errors were less than 10\u22125.\nFor our second experiment, we used images of a low-rankified real video sequence as \u2113t\u2019s. We chose the escalator sequence from http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html since the video changes are only in the region where the escalator moves (and hence can be modeled as being sparse). We made it exactly low-rank by retaining its top 5 eigenvectors and projecting onto their subspace. This resulted in a data matrix L of size n \u00d7 r with n = 20800 and r = 5. We overlaid a simulated moving foreground block on it. The intensity of the moving block was controlled to ensure that q is small. We estimated P\u0302 using EVD, c-EVD, PCP and Alt-Min-RPCA. We let P be the eigenvectors of the low-rankified video with nonzero eigenvalues and computed SE(P\u0302 ,P ). The errors and execution time are displayed in the second row of Table 1. Since n is very large, the difference in speed is most apparent in this case.\nThus c-EVD outperforms PCP and AltMinRPCA when columns of P are sparse. It also outperforms EVD but the advantage in mean error is not as much as our theorems predict. One reason is that the constant in the required lower bounds on \u03b1 is very large. It is hard to pick an \u03b1 that is this large and still only O(log n) unless n is very large. Secondly, both guarantees are only sufficient conditions."}, {"heading": "6 Conclusions and Future Work", "text": "We studied the problem of PCA in noise that is correlated with the data (data-dependent noise). We obtained sample complexity bounds for the most commonly used PCA solution, simple EVD. We also developed and analyzed a generalization of EVD, called cluster-EVD, that has lower sample complexity under extra assumptions. We provided a detailed comparison of our results with those for other approaches to solving its example applications - PCA with missing data and PCA with sparse data-dependent corruptions.\nWe used the matrix Hoeffding inequality [20] to obtain our results. As explained in Sec. 2, it should be possible to improve the sample complexity bounds if this is replaced by [21, Theorem 5.39] or matrix Bernstein. Moreover, as done in [5] (for ReProCS), the mutual independence of \u2113t\u2019s can be easily replaced by a more practical assumption of \u2113t\u2019s following autoregressive model with almost no change to our assumptions. Thirdly, by generalizing the proof techniques developed here, we can also study the problem of correlated-PCA with partial subspace knowledge. This is done in [25]. The solution to the latter problem helps to greatly simplify the proof of correctness of ReProCS for online dynamic RPCA [18]. Fourthly, the boundedness assumption on \u2113t\u2019s can be replaced by a Gaussian or a well-behaved sub-Gaussian assumption but this will increase the sample complexity to O(n). Finally, an open-ended question is how we relax Assumption 1.2 on Mt and still get results similar to Theorem 2.1 or Theorem 3.3."}, {"heading": "7 More examples of Assumption 1.2", "text": "Assumption 1.3 is one simple example of a support change model that ensures that, if M2,t = ITt , the assumption on M2,t given in Assumption 1.2 holds. If instead of one object, there are k objects, and each of their supports satisfies Assumption 1.3, then again, with some modifications, it is possible to show that both the PCA-missing and PCA-SDDC problems satisfy Assumption 1.2. Moreover, notice that Assumption 1.3 does not require the entries in Tt to be contiguous at all (they need not correspond to the support of one or a few objects). Similarly, we can replace the condition that Tt be constant for at most \u03b2\u0303 time instants in Assumption 1.3 by |{t : Tt = T [k]}| \u2264 \u03b2\u0303. Thirdly, the requirement of the object(s) always moving in one direction may seem too stringent. As explained in [4, Lemma 9.4], a Bernoulli-Gaussian \u201cconstant velocity with random acceleration\u201d motion model will also work whp. It allows the object to move at each frame with probability p and not move with probability 1 \u2212 p independent of past or future frames; when the object moves, it moves with an iid Gaussian velocity that has mean 1.1s/\u03c1 and variance \u03c32; \u03c32 needs to be upper bounded and p needs to be lower bounded.\nLastly, if s < c1\u03b1 for c1 \u226a 1, another model that works is that of an object of length s or less moving by at least one pixel and at most b pixels at each time [4, Lemma 9.5]."}, {"heading": "8 Proof of Theorem 2.1", "text": "This result also follows as a corollary of Theorem 3.3. We prove it separately first since its proof is short and and less notation-ally intensive. It will help understand the proof of Theorem 3.3 much more easily. Both results rely on the sin \u03b8 theorem reviewed next. 8.1 sin \u03b8 theorem Davis and Kahan\u2019s sin \u03b8 theorem [19] studies the rotation of eigenvectors by perturbation.\nTheorem 8.1 (sin \u03b8 theorem [19]). Consider two Hermitian matrices D and D\u0302. Suppose that D can be decomposed as\nD = [ E E\u22a5 ]\n[\nA 0 0 A\u22a5\n] [ E\u2032\nE\u22a5 \u2032\n]\nwhere [E E\u22a5] is an orthonormal matrix. Suppose that D\u0302 can be decomposed as\nD\u0302 = [ F F\u22a5 ]\n[\n\u039b 0 0 \u039b\u22a5\n] [ F \u2032\nF\u22a5 \u2032\n]\nwhere [F F\u22a5] is another orthonormal matrix and is such that rank(F ) = rank(E). Let H := D\u0302 \u2212D denote the perturbation. If \u03bbmin(A) > \u03bbmax(\u039b\u22a5), then\n\u2016(I \u2212 FF \u2032)E\u2016 \u2264 \u2016H\u2016 \u03bbmin(A)\u2212 \u03bbmax(\u039b\u22a5) .\nLet r = rank(E). Suppose that F is the matrix of top r eigenvectors of D\u0302. Then \u039b and \u039b\u22a5 are diagonal and \u03bbmax(\u039b\u22a5) = \u03bbr+1(D\u0302) \u2264 \u03bbr+1(D) + \u2016H\u2016. The inequality follows using Weyl\u2019s inequality. Suppose also that \u03bbmin(A) > \u03bbmax(A\u22a5). Then, (i) \u03bbr(D) = \u03bbmin(A) and \u03bbr+1(D) = \u03bbmax(A\u22a5) and (ii) range(E) is equal to the span of the top r eigenvectors ofD. Thus, \u03bbmax(\u039b\u22a5) \u2264 \u03bbmax(A\u22a5) + \u2016H\u2016. With this we have the following corollary. Corollary 8.2. Consider a Hermitian matrix D and its perturbed version D\u0302. Suppose that D can be decomposed as\nD = [ E E\u22a5 ]\n[\nA 0 0 A\u22a5\n] [ E\u2032\nE\u22a5 \u2032\n]\nwhere E is a basis matrix. Let F denote the matrix containing the top rank(E) eigenvectors of D\u0302. Let H := D\u0302 \u2212D denote the perturbation. If \u03bbmin(A)\u2212 \u03bbmax(A\u22a5)\u2212 \u2016H\u2016 > 0, then\n\u2016(I \u2212 FF \u2032)E\u2016 \u2264 \u2016H\u2016 \u03bbmin(A)\u2212 \u03bbmax(A\u22a5)\u2212 \u2016H\u2016 .\nand range(E) is equal to the span of the top rank(E) eigenvectors of D."}, {"heading": "8.2 Proof of Theorem 2.1", "text": "We use the sin \u03b8 theorem [19] from Corollary 8.2. Apply it with D\u0302 = 1\n\u03b1\n\u2211 t ytyt \u2032 and\nD = 1 \u03b1\n\u2211 t \u2113t\u2113t \u2032. Thus, F = P\u0302 . Recall that at = P \u2032\u2113t. Then, D can be decomposed as\nP ( 1 \u03b1\n\u2211 t atat \u2032)P \u2032 + P\u22a50P \u2032\u22a5, and so we have E = P , A = 1 \u03b1 \u2211 t atat \u2032 and A\u22a5 = 0. Moreover,\nit is easy to see that the perturbation H := 1 \u03b1\n\u2211 t ytyt \u2032 \u2212 1 \u03b1 \u2211 t \u2113t\u2113t \u2032 satisfies\nH = 1\n\u03b1\n\u2211\nt\n\u2113tw \u2032 t +\n1\n\u03b1\n\u2211\nt\nwt\u2113 \u2032 t +\n1\n\u03b1\n\u2211\nt\nwtw \u2032 t. (7)\nThus,\nSE(P\u0302 ,P )\n\u2264 2\u2016 1 \u03b1\n\u2211 t \u2113tw \u2032 t\u2016+ \u2016 1\u03b1 \u2211 t wtw \u2032 t\u2016\n\u03bbr( 1 \u03b1\n\u2211 t \u2113t\u2113 \u2032 t)\u2212 (2\u2016 1\u03b1 \u2211 t \u2113tw \u2032 t\u2016+ \u2016 1\u03b1 \u2211 t wtw \u2032 t\u2016)\nif the denominator is positive. Remark 8.3. Because wt is correlated with \u2113t, the \u2113tw\u2032t terms are the dominant ones in the perturbation expression given in (7). If they were uncorrelated, these two terms would be close to zero whp due to law of large numbers and the wtw\u2032t term would be the dominant one.\nIn the next lemma, we bound the terms in the bound on SE(P\u0302 ,P ) using the matrix Hoeffding inequality [20]. Lemma 8.4. Let \u01eb = 0.01r\u03b6\u03bb\u2212.\n1. With probability at least 1\u2212 2n exp ( \u2212\u03b1 \u01eb232(\u03b7rq\u03bb+)2 ) ,\n\u2016 1 \u03b1 \u2211\nt\n\u2113twt \u2032\u2016 \u2264 q\u03bb+\n\u221a\n\u03b2 \u03b1 + \u01eb = [qf\n\u221a\n\u03b2 \u03b1 + 0.01r\u03b6]\u03bb\u2212\n2. With probability at least 1\u2212 2n exp(\u2212 \u03b1\u01eb232(\u03b7rq2\u03bb+)2 ),\n\u2016 1 \u03b1 \u2211\nt\nwtwt \u2032\u2016 \u2264 \u03b2\n\u03b1 q2\u03bb+ + \u01eb = [q2f\n\u03b2 \u03b1 + 0.01r\u03b6]\u03bb\u2212\n3. With probability at least 1\u2212 2n exp(\u2212 \u03b1\u01eb232(\u03b7r\u03bb+)2 ),\n\u03bbr( 1\n\u03b1\n\u2211\nt\n\u2113t\u2113 \u2032 t) \u2265 (1\u2212 (r\u03b6)2)\u03bb\u2212 \u2212 \u01eb\nProof. This follows by using Lemma 9.6 given later with Gcur \u2261 P , Gdet \u2261 [.], Gundet \u2261 [.], \u03b6det \u2261 0, r\u03b6 \u2261 0, rcur = r, g \u2261 f , \u03c7 \u2261 0, \u03d1 \u2261 1. \u22a0\nUsing this lemma to bound the subspace error terms, followed by using the bounds on \u03b2/\u03b1 and \u03b6, we conclude the following: w.p. at least 1 \u2212 2n exp (\n\u2212\u03b1 \u01eb232(\u03b7rq\u03bb+)2 ) \u2212 2n exp(\u2212 \u03b1\u01eb232(\u03b7rq2\u03bb+)2 ) \u2212 2n exp(\u2212 \u03b1\u01eb232(\u03b7r\u03bb+)2 ),\nSE(P\u0302 ,P )\n\u2264 2qf\n\u221a\n\u03b2 \u03b1 + q2f \u03b2 \u03b1 + 0.03r\u03b6\n1\u2212 (r\u03b6)2 \u2212 0.01r\u03b6 \u2212 (2qf \u221a\n\u03b2 \u03b1 + q2f \u03b2 \u03b1 + 0.03r\u03b6)\n\u2264 0.75(1\u2212 r\u03b6)r\u03b6 + 0.03r\u03b6 1\u2212 r\u03b6 < r\u03b6\nUsing the bound \u03b1 \u2265 \u03b10 from the theorem, the probability of the above event is at least 1\u2212 6n\u221210. We get this by bounding each of the three negative terms in the probability expression by \u22122n\u221210. We work this out for the first term: \u03b1 \u01eb 2\n32(\u03b7rq\u03bb+)2 \u2265 32\u00b711(0.01)2 \u03b72r2(logn) (r\u03b6)2 (qf) 2 (0.01r\u03b6\u03bb \u2212)2 32\u03b72r2q2\u03bb+2 = 11 logn.\nThus, 2n exp ( \u2212\u03b1 \u01eb232(\u03b7rq\u03bb+)2 ) \u2264 2n exp(\u221211 logn) \u2264 2n\u221210."}, {"heading": "9 Proof of Theorem 3.3", "text": "We explain the overall idea of the proof next. In Sec. 9.2, we give a sequence of lemmas in generalized form (so that they can apply to various other problems). The proof of Theorem 3.3 is given in Sec. 9.3 and follows easily by applying these. One of the lemmas of Sec. 9.2 is proved in Sec. 10 while the others are proved there itself."}, {"heading": "9.1 Overall idea", "text": "We need to bound SE(P\u0302 ,P ). From Algorithm 1, P\u0302 = [G\u03021, G\u03022, . . . , G\u0302\u03d1] where G\u0302k is the matrix of top r\u0302k eigenvectors of D\u0302k defined in Algorithm 1. Also, P = [G1,G2, . . . ,G\u03d1] where Gk is a basis matrix with rk columns. Definition 9.1. Define \u03b6k := SE([G\u03021, G\u03022, . . . , G\u0302k],Gk) and \u03b60 = 0. Define \u03b6+k := rk\u03b6. Let r0 = 0.\nIt is easy to see that\nSE(P\u0302 ,P ) \u2264 \u03d1 \u2211\nk=1\nSE(P\u0302 ,Gk)\n\u2264 \u03d1 \u2211\nk=1\nSE([G\u03021, G\u03022, . . . , G\u0302k],Gk) =\n\u03d1 \u2211\nk=1\n\u03b6k (8)\nThe first inequality is triangle inequality, the second follows because [G\u03021, G\u03022, . . . , G\u0302k] is orthogonal to [G\u0302k+1, . . .G\u03d1]. Since r = \u2211\nk rk, if we can show that \u03b6k \u2264 \u03b6+k = rk\u03b6 for all k we will be done. We bound \u03b6k using induction. The base case is easy and follows just from the definition, \u03b60 = SE([.], [.]) = 0 = r0\u03b6. For bounding \u03b6k, assume that for all i = 1, 2, . . . , k \u2212 1, \u03b6i \u2264 ri\u03b6. This implies that\nSE([G\u03021, G\u03022, . . . , G\u0302k\u22121], [G1,G2, . . . ,Gk\u22121])\n\u2264 k\u22121 \u2211\ni=1\nSE([G\u03021, G\u03022, . . . , G\u0302k\u22121],Gi)\n\u2264 k\u22121 \u2211\ni=1\n\u03b6i \u2264 k\u22121 \u2211\ni=1\nri\u03b6 \u2264 r\u03b6 (9)\nUsing this, we will first show that r\u0302k = rk, and then we will use this and the sin \u03b8 result to bound \u03b6k.\nBefore proceeding further, we simplify notation.\nDefinition 9.2.\n1. Let\nGdet := [G1,G2, . . . ,Gk\u22121], Gcur := Gk,\nGundet := [G\u0302k+1, . . .G\u03d1]\n2. Similarly, let G\u0302det := [G\u03021, G\u03022, . . . , G\u0302k\u22121], G\u0302cur := G\u0302k.\n3. Let Gdet := G1 \u222a G2 \u00b7 \u00b7 \u00b7 \u222a Gk\u22121 and Gcur = Gk. 4. Let rcur := rk = rank(Gk) and r\u0302cur := r\u0302k.\n5. Let \u03bb+cur := \u03bb + k , \u03bb \u2212 cur := \u03bb \u2212 k , \u03bb + undet := \u03bb + k+1\n6. Let t\u2217 = k\u03b1."}, {"heading": "9.2 Main lemmas - generalized form", "text": "In this section, we give a sequence of lemmas that apply to a generic problem where yt = \u2113t + wt = \u2113t + Mt\u2113t with \u2113t satisfying Assumption 1.1; Mt satisfying Assumption 1.2; and with P split into three parts as P = [Gdet,Gcur,Gundet]. We can correspondingly split \u039b as \u039b = diag(\u039bdet,\u039bcur,\u039bundet).\nWe are given G\u0302det that was computed using (some or all) yt\u2019s for t \u2264 t\u2217 and that satisfies \u03b6det \u2264 r\u03b6. The goal is to estimate range(Gcur) and bound the estimation error. This is done by first estimating r\u0302cur and then computing G\u0302cur as the top r\u0302cur eigenvectors of\nD\u0302 := 1\n\u03b1\nt\u2217+\u03b1 \u2211\nt=t\u2217+1\n\u03a8ytyt \u2032 \u03a8. (10)\nTo bound the estimation error, we first show that, whp, r\u0302cur = rcur and so G\u0302cur = Gcur; and then we use this to show that \u03b6cur \u2264 rcur\u03b6. Definition 9.3.\n1. Define \u03a8 := I \u2212 G\u0302detG\u0302det\u2032.\n2. Define \u03b6det := SE(G\u0302det,Gdet) = \u2016\u03a8Gdet\u2016 and \u03b6+det = r\u03b6\n3. Define \u03b6cur := SE([G\u0302det, G\u0302cur],Gcur).\n4. Let (\u03a8Gcur) QR = EcurRcur denote its reduced QR decomposition. Thus Ecur is a basis\nmatrix whose span equals that of (\u03a8Gcur) and Rcur is a square upper triangular matrix with \u2016Rcur\u2016 = \u2016\u03a8Gcur\u2016 \u2264 1.\n5. Let \u03bb+cur = \u03bbmax(\u039bcur), \u03bb \u2212 cur = \u03bbmin(\u039bcur), \u03bb + undet = \u03bbmax(\u039bundet).\n6. Let rcur = rank(Gcur). Clearly, rcur \u2264 r. Remark 9.4. In special cases, Gdet (and hence G\u0302det) could be empty; and/or Gundet could be empty.\n\u2022 Since \u039b contains eigenvalues in decreasing order, when Gundet is not empty, \u03bb\u2212 \u2264 \u03bb+undet \u2264 \u03bb\u2212cur \u2264 \u03bb+cur \u2264 \u03bb+. \u2022 When Gundet is empty, \u03bb+undet = 0 and \u03bb\u2212 \u2264 \u03bb\u2212cur \u2264 \u03bb+cur \u2264 \u03bb+.\nUsing \u2016Rcur\u2016 = \u2016\u03a8Gcur\u2016 \u2264 1, \u03b6cur = \u2016(I \u2212 G\u0302curG\u0302cur\u2032)\u03a8Gcur\u2016\n= \u2016(I \u2212 G\u0302curG\u0302cur\u2032)EcurRcur\u2016 \u2264 \u2016(I \u2212 G\u0302curG\u0302cur\u2032)Ecur\u2016 = SE(G\u0302cur,Ecur).\nThus, to bound \u03b6cur we need to bound SE(G\u0302cur,Ecur). G\u0302cur is the matrix of top r\u0302cur eigenvectors of D\u0302. From its definition, Ecur is a basis matrix with rcur columns. Suppose for a moment that r\u0302cur = rcur. Then, in order to bound SE(G\u0302cur,Ecur), we can use the sin \u03b8 result, Corollary 8.2. To do this, we need to define a matrix D so that, under appropriate assumptions, the span of its top rcur eigenvectors equals range(Ecur). For the simple EVD proof, we used 1\u03b1 \u2211t\u2217+\u03b1 t=t\u2217+1 \u03a8\u2113t\u2113 \u2032 t\u03a8 as the matrix D. However, this will not work now since Ecur is not orthonormal to \u03a8Gdet or to \u03a8Gundet. But, instead we can use\nD = EcurAEcur \u2032 +Ecur,\u22a5A\u22a5Ecur,\u22a5 \u2032, where\nA := Ecur \u2032( 1\n\u03b1\nt\u2217+\u03b1 \u2211\nt=t\u2217+1\n\u03a8\u2113t\u2113 \u2032 t\u03a8)Ecur and\nA\u22a5 := Ecur,\u22a5 \u2032( 1\n\u03b1\nt\u2217+\u03b1 \u2211\nt=t\u2217+1\n\u03a8\u2113t\u2113 \u2032 t\u03a8)Ecur,\u22a5 (11)\nNow, by construction, D is in the desired form.\nWith the above choice of D, H := D\u0302 \u2212 D satisfies1 H = term1 + term1\u2032 + term2 + term3 + term3\u2032 where term1 := 1\n\u03b1\n\u2211 t \u03a8\u2113tw \u2032 t\u03a8, term2 := 1 \u03b1 \u2211 t \u03a8wtw \u2032 t\u03a8 and term3 =\nEcurEcur \u2032( 1\n\u03b1\n\u2211 t \u03a8\u2113t\u2113 \u2032 t\u03a8)Ecur,\u22a5Ecur,\u22a5 \u2032.\n1This follows easily by writing H = (D\u0302 \u2212 1 \u03b1 \u2211 t \u03a8\u2113t\u2113 \u2032 t\u03a8) + ( 1 \u03b1 \u2211 t \u03a8\u2113t\u2113 \u2032\nt\u03a8 \u2212 D) and using the fact that M = (EE\u2032 +E\u22a5E\u22a5\u2032)M(EE\u2032 +E\u22a5E\u22a5\u2032) for 1\n\u03b1 \u2211 t \u03a8\u2113t\u2113 \u2032 t\u03a8.\nThus, using the above along with Corollary 8.2, we can conclude the following. Fact 9.5.\n1. If r\u0302cur = rcur, and \u03bbmin(A)\u2212 \u03bbmax(A\u22a5)\u2212 \u2016H\u2016 > 0,\n\u03b6cur \u2264 SE(G\u0302cur,Ecur) \u2264 \u2016H\u2016\n\u03bbmin(A) \u2212 \u03bbmax(A\u22a5)\u2212 \u2016H\u2016 .\n2. Let Q := EcurEcur\u2032( 1\u03b1 \u2211 t \u03a8\u2113t\u2113 \u2032 t\u03a8)Ecur,\u22a5Ecur,\u22a5 \u2032. We have\n\u2016H\u2016 \u2264 2\u2016 1 \u03b1 \u2211\nt\n\u03a8\u2113tw \u2032 t\u2016+ \u2016\n1\n\u03b1\n\u2211\nt\nwtw \u2032 t\u2016+ 2\u2016Q\u2016.\nThe next lemma bounds the RHS terms in the above lemma and a few other quantities needed for showing r\u0302cur = rcur.\nLemma 9.6. (1) Assume that yt = \u2113t+wt = \u2113t+Mt\u2113t with \u2113t satisfying Assumption 1.1 and Mt satisfying Assumption 1.2.\n(2) Assume that we are given G\u0302det that was computed using (some or all) yt\u2019s for t \u2264 t\u2217 and that satisfies \u03b6det \u2264 r\u03b6. Define g := \u03bb+cur/\u03bb \u2212 cur, \u03c7 := \u03bb + undet/\u03bb \u2212 cur. Set \u01eb := 0.01rcur\u03b6\u03bb \u2212 cur.\nThen, the following hold:\n1. Let p1 := 2n exp(\u2212 \u03b1\u01eb 2\n32b2 prob\nwhere bprob := \u03b7rq((r\u03b6)\u03bb+ + \u03bb+cur + (r\u03b6) \u221a \u03bb+\u03bb+cur + \u221a\n\u03bb+\u03bb+cur). Conditioned on {\u03b6det \u2264 r\u03b6}, with probability at least 1\u2212 p1\n\u2016 1 \u03b1 \u2211\nt\n\u03a8\u2113twt \u2032\u2016 \u2264 q((r\u03b6)\u03bb+ + \u03bb+cur)\n\u221a\n\u03b2 \u03b1 + \u01eb\n\u2264 [q(r\u03b6)f \u221a \u03b2\n\u03b1 + qg\n\u221a\n\u03b2 \u03b1 + 0.01rcur\u03b6]\u03bb \u2212 cur.\n2. Let p2 := 2n exp(\u2212 \u03b1\u01eb 2 32(q2\u03b7r\u03bb+)2 ). Conditioned on {\u03b6det \u2264 r\u03b6}, with probability (w.p.) at least 1\u2212 p2,\n\u2016 1 \u03b1 \u2211\nt\nwtwt \u2032\u2016 \u2264 \u03b2 \u03b1 q2\u03bb+ + \u01eb \u2264 [\u03b2 \u03b1 q2f + 0.01rcur\u03b6]\u03bb \u2212 cur.\n3. Let p3 := 2n exp(\u2212 \u03b1\u01eb 2\n32b2 prob\n) with bprob := \u03b7r((r\u03b6)2\u03bb+ + \u03bb+cur + 2(r\u03b6) \u221a \u03bb+\u03bb+cur). Condi-\ntioned on {\u03b6det \u2264 r\u03b6}, with probability at least 1\u2212 p3,\n\u2016EcurEcur\u2032( 1\n\u03b1 \u03a8\u2113t\u2113\n\u2032 t\u03a8)Ecur,\u22a5Ecur,\u22a5 \u2032\u2016\n\u2264 (r\u03b6)2\u03bb+ + (r\u03b6) 2 \u221a\n1\u2212 (r\u03b6)2 \u03bb+undet + \u01eb\n\u2264 [(r\u03b6)2f + (r\u03b6) 2 \u221a\n1\u2212 (r\u03b6)2 \u03c7+ 0.01rcur\u03b6]\u03bb\n\u2212 cur.\n4. Conditioned on {\u03b6det \u2264 r\u03b6}, w.p. at least 1\u2212 p3, \u03bbmin(A) \u2265 (1\u2212 (r\u03b6)2)\u03bb\u2212cur \u2212 \u01eb = [1\u2212 (r\u03b6)2 \u2212 0.01rcur\u03b6]\u03bb\u2212cur 5. Conditioned on {\u03b6det \u2264 r\u03b6}, w.p. at least 1\u2212 p3,\n\u03bbmax(A\u22a5) \u2264 ((r\u03b6)2\u03bb+ + \u03bb+undet) + \u01eb \u2264 [(r\u03b6)2f + \u03c7+ 0.01rcur\u03b6]\u03bb\u2212cur.\n6. Conditioned on {\u03b6det \u2264 r\u03b6}, with probability at least 1\u2212 p3,\n\u03bbmax(A\u22a5) \u2265 (1\u2212 (r\u03b6)2 \u2212 (r\u03b6)2 \u221a\n1\u2212 (r\u03b6)2 )\u03bb+undet \u2212 \u01eb.\n7. Conditioned on {\u03b6det \u2264 r\u03b6}, w.p. at least 1\u2212 p3, \u03bbmax(A) \u2265 (1\u2212 (r\u03b6)2)\u03bb+cur \u2212 \u01eb\n= [(1\u2212 (r\u03b6)2)g \u2212 0.01rcur\u03b6]\u03bb\u2212cur.\n8. Conditioned on {\u03b6det \u2264 r\u03b6}, w.p. at least 1\u2212 p3,\n\u03bbmax(A) \u2264 \u03bb+cur + (r\u03b6)2\u03bb+ + 1\n1\u2212 r2\u03b62 (r\u03b6) 2\u03bb+undet + \u01eb\n\u2264 [g + (r\u03b6)2f + (r\u03b6) 2\n1\u2212 (r\u03b6)2 \u03c7+ 0.01rcur\u03b6]\u03bb \u2212 cur.\nProof. The proof is in Section 10. \u22a0\nCorollary 9.7. Consider the setting of Lemma 9.6. Assume\n1. r(r\u03b6) \u2264 0.0001, and r(r\u03b6)f \u2264 0.01. Since rcur \u2264 r, this implies that rcur\u03b6 \u2264 0.0001, and\n2. \u03b2 \u2264 ( (1\u2212rcur\u03b6\u2212\u03c7) 2\n)2\nmin ( (rcur\u03b6) 2\n4.1q2g2 , (rcur\u03b6) q2f\n)\n\u03b1.\nUsing these and using g \u2265 1, g \u2264 f , \u03c7 \u2264 1 (these hold by definition), with probability at least 1\u2212 p1 \u2212 p2 \u2212 4p3,\n\u2016H\u2016 \u2264 [2.02qg \u221a \u03b2\n\u03b1 +\n\u03b2 \u03b1 q2f + 0.08rcur\u03b6]\u03bb \u2212 cur\n\u2264 [0.75(1\u2212 r\u03b6 \u2212 \u03c7)rcur\u03b6 + 0.08rcur\u03b6]\u03bb\u2212cur \u2264 0.83rcur\u03b6\u03bb\u2212cur,\n\u03bbmax(A\u22a5) \u2264 [\u03c7+ 0.02rcur\u03b6]\u03bb\u2212cur, \u03bbmax(A\u22a5) \u2265 [\u03c7\u2212 0.02rcur\u03b6]\u03bb\u2212cur, \u03bbmin(A) \u2265 [1\u2212 0.0101rcur\u03b6]\u03bb\u2212cur, \u03bbmax(A) \u2264 [g + 0.0202rcur\u03b6]\u03bb\u2212cur, \u03bbmax(A) \u2265 [g \u2212 0.02rcur\u03b6]\u03bb\u2212cur.\nLemma 9.8. Consider the setting of Corollary 9.7. In addition, also assume that\n1. g\u0302 = 1.01g + 0.0001 and\n2. \u03c7 \u2264 min ( g\u22120.0001 1.01g+0.0001 \u2212 0.0001, 1\u2212 rcur\u03b6 \u2212 0.080.25 ) .\nLet \u03bb\u0302i := \u03bbi(D\u0302). Then, with probability at least 1\u2212 p1 \u2212 p2 \u2212 4p3, the following hold.\n1. When Gundet is not empty: \u03bb\u03021 \u03bb\u0302rcur \u2264 g\u0302, \u03bb\u03021 \u03bb\u0302rcur+1 > g\u0302, and \u03bb\u0302rcur+1 \u2265 \u03bbthresh.\n2. When Gundet is empty: \u03bb\u03021 \u03bb\u0302rcur \u2264 g\u0302 and \u03bb\u0302rcur+1 < \u03bbthresh < \u03bb\u0302rcur .\n3. If r\u0302cur = rcur, then \u03b6cur \u2264 \u2016H\u2016\u03bbmin(A)\u2212\u03bbmax(A\u22a5)\u2212\u2016H\u2016 \u2264 0.75rcur\u03b6 + 0.08rcur\u03b6 (1\u2212rcur\u03b6\u2212\u03c7) \u2264 rcur\u03b6.\nProof.\nFact 9.9. From the bound on \u03c7, \u03c7 \u2264 1\u22120.0001 \u2264 1\u2212rcur\u03b6. Thus, using Corollary 9.7, \u03bbmin(A) > \u03bbmax(A\u22a5) and so \u03bbrcur(D) = \u03bbmin(A), \u03bbrcur+1(D) = \u03bbmax(A\u22a5), and \u03bb1(D) = \u03bbmax(A). Recall: \u03bb1(.) is the same as \u03bbmax(.).\nProof of item 1. Recall that D\u0302 and D are defined in (10) and (11). Using Weyl\u2019s inequality, Fact 9.9, and Corollary 9.7, with the probability given there,\n\u03bb\u03021 \u03bb\u0302rcur \u2264 \u03bbmax(A) + \u2016H\u2016 \u03bbmin(A)\u2212 \u2016H\u2016 \u2264 g + 0.86rcur\u03b6 1\u2212 0.85rcur\u03b6\nand \u03bb\u03021\n\u03bb\u0302rcur+1 > \u03bbmax(A) \u2212 \u2016H\u2016 \u03bbmax(A\u22a5) + \u2016H\u2016 > g \u2212 0.85rcur\u03b6 \u03c7+ 0.85rcur\u03b6\nThus, if\ng + 0.85rcur\u03b6 1\u2212 0.85rcur\u03b6 \u2264 g\u0302 \u2264 g \u2212 0.85rcur\u03b6 \u03c7+ 0.85rcur\u03b6 (12)\nholds, we will be done. The above requires \u03c7 to be small enough so that the lower bound is not larger than the upper bound and it requires g\u0302 to be appropriately set. Both are ensured by the assumptions in the lemma.\nSince Gundet is not empty, \u03bb + undet = \u03c7\u03bb \u2212 cur > \u03bb \u2212 Thus, using Weyl\u2019s inequality followed by Corollary 9.7, with the probability given there,\n\u03bb\u0302rcur+1 \u2265 \u03bbrcur+1(D)\u2212 \u2016H\u2016 = \u03bbmax(A\u22a5)\u2212 \u2016H\u2016 \u2265 [\u03c7\u2212 0.02rcur\u03b6]\u03bb\u2212cur \u2212 0.83rcur\u03b6\u03bb\u2212cur \u2265 (1\u2212 0.85rcur\u03b6)\u03bb\u2212 > \u03bbthresh\nProof of item 2. Since Gundet is empty, \u03bb + undet = 0 and so \u03c7 = 0. Thus, using Corollary 9.7, with probability given there,\n\u03bb\u0302rcur+1 \u2264 \u03bbrcur+1(D) + \u2016H\u2016 = \u03bbmax(A\u22a5) + \u2016H\u2016 \u2264 0 + 0.02rcur\u03b6\u03bb\u2212 + \u2016H\u2016 \u2264 0.85rcur\u03b6\u03bb\u2212 < \u03bbthresh,\n\u03bb\u0302rcur \u2265 \u03bbrcur(D)\u2212 \u2016H\u2016 = \u03bbmin(A)\u2212 \u2016H\u2016 \u2265 \u03bb\u2212cur \u2212 0.085rcur\u03b6\u03bb\u2212cur \u2265 (1\u2212 0.85rcur\u03b6)\u03bb\u2212 > \u03bbthresh,\nand \u03bb\u03021\n\u03bb\u0302rcur \u2264 \u03bbmax(A) + \u2016H\u2016 \u03bbmin(A)\u2212 \u2016H\u2016 \u2264 g + 0.85rcur\u03b6 1\u2212 0.85rcur\u03b6 \u2264 g\u0302\nProof of item 3. Using Fact 9.5 and Corollary 9.7, since r\u0302cur = rcur is assumed, we get\n\u03b6cur \u2264 [0.75(1\u2212 rcur\u03b6 \u2212 \u03c7)rcur\u03b6 + 0.08rcur\u03b6]\u03bb\u2212cur\n\u03bb\u2212cur[1\u2212 0.0101rcur\u03b6 \u2212 \u03c7\u2212 0.02rcur\u03b6 \u2212 0.83r\u03b6]\n\u2264 0.75(1\u2212 r\u03b6 \u2212 \u03c7)rcur\u03b6 + 0.08rcur\u03b6 (1\u2212 rcur\u03b6 \u2212 \u03c7) \u2264 rcur\u03b6 (13)\nThe last inequality used the bound on \u03c7. \u22a0"}, {"heading": "9.3 Proof of Theorem 3.3", "text": "The theorem is a direct consequence of using (9) and applying Lemma 9.8 for each of the k steps with the substitutions given in Definition 9.2; along with picking \u03b1 appropriately. A detailed proof is in Sec. 11."}, {"heading": "10 Proof of Hoeffding lemma, Lemma 9.6", "text": "The following lemma, which is a modification of [3, Lemma 8.15], will be used in our proof. It is proved in Sec. 11. The proof uses [3, Lemma 2.10].\nLemma 10.1. Given \u03b6det \u2264 r\u03b6.\n1. \u2016\u03a8Gdet\u2016 \u2264 r\u03b6 and \u2016\u03a8Gcur\u2016 \u2264 1.\n2. \u221a 1\u2212 (r\u03b6)2 \u2264 \u03c3i(Rcur) = \u03c3i(\u03a8Gcur) \u2264 1 and \u221a 1\u2212 (r\u03b6)2 \u2264 \u03c3i(\u03a8Gundet) \u2264 1\n3. \u2016Ecur\u2032\u03a8Gundet\u2016 \u2264 (r\u03b6)2 \u221a\n1\u2212 (r\u03b6)2\n4.\n\u03a8\u03a3\u03a8 = [\u03a8Gdet \u03a8Gcur \u03a8Gundet] [\n\u039bdet 0 0\n0 \u039bcur 0 0 \u039bundet\n][\n\u03a8Gdet \u03a8Gcur \u03a8Gundet\n]\u2032\nwith \u03bbmax(\u039bdet) \u2264 \u03bb+, \u03bb\u2212cur \u2264 \u03bbmin(\u039bcur) \u2264 \u03bbmax(\u039bcur) \u2264 \u03bb+cur, \u03bbmax(\u039bundet) \u2264 \u03bb+undet.\n5. Using the first four claims, it is easy to see that\n(a) \u2016Ecur,\u22a5\u2032\u03a8\u03a3\u03a8Ecur,\u22a5\u2016 \u2264 (r\u03b6)2\u03bb+ + \u03bb+undet (b) \u2016Ecur,\u22a5\u2032\u03a8\u03a3\u03a8Ecur\u2016 \u2264 (r\u03b6)2\u03bb+ + (r\u03b6)\n2\u221a 1\u2212(r\u03b6)2 \u03bb+undet\n(c) \u2016\u03a8\u03a3\u2016 \u2264 (r\u03b6)\u03bb+ + \u03bb+cur and \u2016\u03a8\u03a3M1,t\u2032\u2016 \u2264 q((r\u03b6)\u03bb+ + \u03bb+cur) (d) \u2016M1,t\u03a3\u2016 \u2264 q\u03bb+ and \u2016M1,t\u03a3M1,t\u2032\u2016 \u2264 q2\u03bb+\nIf G\u0302det = Gdet = [.], then all the terms containing (r\u03b6) disappear.\n6. \u03bbmin(A+B) \u2265 \u03bbmin(A) + \u03bbmin(B) 7. Let at := P \u2032\u2113t, at,det := Gdet\u2032\u2113t, at,cur := Gcur\u2032\u2113t and at,undet := Gundet\u2032\u2113t. Also let\nat,rest := [at,cur \u2032,at,undet\u2032]\u2032. Then \u2016at,rest\u20162 \u2264 r\u03b7\u03bb+cur and \u2016at,det\u20162 \u2264 \u2016at\u20162 \u2264 r\u03b7\u03bb+.\n8. \u03c3min(Ecur,\u22a5\u2032\u03a8Gundet)2 \u2265 1\u2212 (r\u03b6)2 \u2212 (r\u03b6) 2\u221a\n1\u2212(r\u03b6)2 .\nThe following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.\nCorollary 10.2. Given an \u03b1-length sequence {Zt} of random Hermitian matrices of size n \u00d7 n, a r.v. X , and a set C of values that X can take. For all X \u2208 C, (i) Zt\u2019s are conditionally independent given X; (ii) P(b1I Zt b2I|X) = 1 and (iii) b3I E[ 1\u03b1 \u2211\nt Zt|X ] b4I. For any \u01eb > 0, for all X \u2208 C,\nP\n(\n\u03bbmax\n(\n1\n\u03b1\n\u2211\nt\nZt\n)\n\u2264 b4 + \u01eb \u2223 \u2223 \u2223X\n)\n\u2265 1\u2212 n exp ( \u2212\u03b1\u01eb2 8(b2 \u2212 b1)2 ) ,\nP\n(\n\u03bbmin\n(\n1\n\u03b1\n\u2211\nt\nZt\n)\n\u2265 b3 \u2212 \u01eb \u2223 \u2223 \u2223 X\n)\n\u2265 1\u2212 n exp ( \u2212\u03b1\u01eb2 8(b2 \u2212 b1)2 ) .\nCorollary 10.3. Given an \u03b1-length sequence {Zt} of random matrices of size n1 \u00d7 n2. For all X \u2208 C, (i) Zt\u2019s are conditionally independent given X; (ii) P(\u2016Zt\u2016 \u2264 b1|X) = 1 and (iii) \u2016E[ 1\n\u03b1\n\u2211\nt Zt|X ]\u2016 \u2264 b2. For any \u01eb > 0, for all X \u2208 C,\nP\n(\n\u2225 \u2225 \u2225 \u2225 1\n\u03b1\n\u2211\nt\nZt\n\u2225 \u2225 \u2225 \u2225 \u2264 b2 + \u01eb \u2223 \u2223 \u2223X\n)\n\u2265 1\u2212 (n1 + n2) exp (\u2212\u03b1\u01eb2 32b1 2 ) .\nProof of Lemma 9.6. Recall that we are given G\u0302det that was computed using (some or all) yt\u2019s for t \u2264 t\u2217 and that satisfies \u03b6det \u2264 r\u03b6. From (2), yt is a linear function of \u2113t. Thus, we can let X := {\u21131, \u21132, . . . \u2113t\u2217} denote all the random variables on which the event {\u03b6det \u2264 r\u03b6} depends. In each item of this proof, we need to lower bound the probability of the desired event conditioned on \u03b6det \u2264 r\u03b6. To do this, we first lower bound the probability of the event conditioned on X that is such that X \u2208 {\u03b6det \u2264 r\u03b6}. We get a lower bound that does not depend on X as long as X \u2208 {\u03b6det \u2264 r\u03b6}. Thus, the same probability lower bound holds conditioned on {\u03b6det \u2264 r\u03b6}. Fact 10.4. For an event E and random variable X , P(E|X) \u2265 p for all X \u2208 C implies that P(E|X \u2208 C) \u2265 p.\nProof of Lemma 9.6, item 1. Let\nterm := 1\n\u03b1\n\u2211\nt\n\u03a8\u2113twt \u2032 =\n1\n\u03b1\n\u2211\nt\n\u03a8\u2113t\u2113 \u2032 tM1,t \u2032M2,t \u2032\nSince \u03a8 is a function of X , since \u2113t\u2019s used in the summation above are independent of X and E[\u2113t\u2113t \u2032] = \u03a3,\nE[term|X ] = 1 \u03b1 \u2211\nt\n\u03a8\u03a3M1,t \u2032M2,t \u2032\nNext, we use Cauchy-Schwartz for matrices: \u2225\n\u2225 \u2225 \u2225 \u2225\n\u03b1 \u2211\nt=1\nXtYt \u2032\n\u2225 \u2225 \u2225 \u2225 \u2225 2 \u2264 \u03bbmax ( \u03b1 \u2211\nt=1\nXtXt \u2032 ) \u03bbmax ( \u03b1 \u2211\nt=1\nYtYt \u2032 )\n(14)\nUsing (14), with Xt = \u03a8\u03a3M1,t \u2032 and Yt = M2,t, followed by using\n\u221a\n\u2016 1 \u03b1\n\u2211 t XtX \u2032 t\u2016 \u2264\nmaxt \u2016Xt\u2016, Assumption 1.2 with At \u2261 I, and Lemma 10.1,\n\u2016E[term|X ]\u2016 \u2264 max t\n\u2016\u03a8\u03a3M1,t\u2032\u2016 \u221a \u03b2\n\u03b1\n\u2264 q((r\u03b6)\u03bb+ + \u03bb+cur) \u221a \u03b2\n\u03b1\nfor all X \u2208 {\u03b6det \u2264 r\u03b6}. To bound \u2016\u03a8\u2113tw\u2032t\u2016, rewrite it as \u03a8\u2113tw\u2032t = [\u03a8Gdetat,det + \u03a8Grestat,rest][a \u2032 t,detG \u2032 det + a \u2032 t,restG \u2032 rest]M \u2032 1,tM \u2032 2,t. Thus, using \u2016M2,t\u2016 \u2264 1, \u2016M1,tP \u2016 \u2264 q < 1, and Lemma 10.1,\n\u2016\u03a8\u2113tw\u2032t\u2016 \u2264 qr\u03b7((r\u03b6)\u03bb+ + \u03bb+cur + (r\u03b6) \u221a \u03bb+\u03bb+cur + \u221a \u03bb+\u03bb+cur)\nholds w.p. one when {\u03b6det \u2264 r\u03b6}. Finally, conditioned on X , the individual summands in term are conditionally independent. Using matrix Hoeffding, Corollary 10.3, followed by Fact 10.4, the result follows.\nProof of Lemma 9.6, item 2.\nE[ 1\n\u03b1\n\u2211\nt\nwtw \u2032 t|X ] =\n1\n\u03b1\n\u2211\nt\nM2,tM1,t\u03a3M1,t \u2032M2,t \u2032\nBy Lemma 10.1, \u2016M1,t\u03a3M1,t\u2032\u2016 \u2264 q2\u03bb+. Thus, using Assumption 1.2 with At \u2261 M1,t\u03a3M1,t\u2032,\n\u2016E[ 1 \u03b1 \u2211\nt\nwtw \u2032 t|X ]\u2016 \u2264\n\u03b2 \u03b1 q2\u03bb+.\nUsing Assumption 1.2 and Lemma 10.1,\n\u2016wtw\u2032t\u2016 = \u2016M2,tM1,tPat\u20162 \u2264 q2\u03b7r\u03bb+. Conditional independence of the summands holds as before. Thus, using Corollary 10.3 and Fact 10.4, the result follows.\nProof of Lemma 9.6, item 3.\nE[ 1\n\u03b1\n\u2211\nt\nEcurEcur \u2032 \u03a8\u2113t\u2113t \u2032 \u03a8Ecur,\u22a5Ecur,\u22a5 \u2032\u2016|X ]\n= EcurEcur \u2032 \u03a8\u03a3\u03a8Ecur,\u22a5Ecur,\u22a5 \u2032\nUsing Lemma 10.1, \u2016EcurEcur\u2032\u03a8\u03a3\u03a8Ecur,\u22a5Ecur,\u22a5\u2032\u2016 \u2264 (r\u03b6)2\u03bb++ (r\u03b6) 2\u221a 1\u2212(r\u03b6)2 \u03bb+undet when {\u03b6det \u2264 r\u03b6}. Also, \u2016Ecur\u2032\u03a8\u2113t\u2113t\u2032\u03a8Ecur,\u22a5\u2016 \u2264 \u2016\u03a8\u2113t\u2113t\u2032\u03a8\u2016 \u2264 \u03b7r((r\u03b6)2\u03bb+ + \u03bb+cur + 2(r\u03b6) \u221a\n\u03bb+\u03bb+cur) := bprob holds w.p. one when {\u03b6det \u2264 r\u03b6}. In the above bound, the first inequality is used to get a loose bound, but one that will also apply for the proofs of the later items given below. The rest is the same as in the proofs of the earlier parts.\nProof of Lemma 9.6, item 4. Using Ostrowski\u2019s theorem,\n\u03bbmin(E[A|X ]) = \u03bbmin(Ecur\u2032\u03a8(\u03a3)\u03a8Ecur) \u2265 \u03bbmin(Ecur\u2032\u03a8Gcur\u039bcurGcur\u2032\u03a8Ecur) = \u03bbmin(Rcur\u039bcurRcur \u2032)\n\u2265 \u03bbmin(RcurRcur\u2032)\u03bbmin(\u039bcur) \u2265 (1\u2212 (r\u03b6)2)\u03bb\u2212cur for all X \u2208 {\u03b6det \u2264 r\u03b6}. Ostrowski\u2019s theorem is used to get the second-last inequality, while Lemma 10.1 helps get the last one.\nAs in the proof of item 3, \u2016Ecur\u2032\u03a8\u2113t\u2113t\u2032\u03a8Ecur\u2016 \u2264 \u2016\u03a8\u2113t\u2113t\u2032\u03a8\u2016 \u2264 bprob holds w.p. one when {\u03b6det \u2264 r\u03b6}. Conditional independence of the summands holds as before. Thus, by matrix Hoeffding, Corollary 10.2, the result follows.\nProof of Lemma 9.6, item 5. By Lemma 10.1,\n\u03bbmax(E[A\u22a5|X ]) = \u03bbmax(Ecur,\u22a5\u2032\u03a8\u03a3\u03a8Ecur,\u22a5) \u2264 ((r\u03b6)2\u03bb+ + \u03bb+undet)\nwhen {\u03b6det \u2264 r\u03b6}. The rest of the proof is the same as that of the previous part. Proof of Lemma 9.6, item 6. Using Ostrowski\u2019s theorem, \u03bbmax(E[A\u22a5|X ]) \u2265 \u03bbmax(Ecur,\u22a5\u2032\u03a8Gundet\u039bundetGundet\u2032\u03a8Ecur,\u22a5) \u2265 \u03bbmin(Ecur,\u22a5 \u2032 \u03a8GundetGundet \u2032 \u03a8Ecur,\u22a5)\u03bbmax(\u039bundet). By definition, \u03bbmax(\u039bundet) = \u03bb + undet. By Lemma 10.1, \u03bbmin(Ecur,\u22a5 \u2032 \u03a8GundetGundet \u2032 \u03a8Ecur,\u22a5) = \u03c3min(Ecur,\u22a5 \u2032 \u03a8Gundet)\n2 \u2265 (1\u2212 (r\u03b6)2 \u2212 (r\u03b6)\n2\u221a 1\u2212(r\u03b6)2 ) when {\u03b6det \u2264 r\u03b6}. The rest of the proof is the same as above.\nProof of Lemma 9.6, item 7. Using Ostrowski\u2019s theorem and Lemma 10.1, \u03bbmax(E[A|X ]) \u2265 \u03bbmax(Ecur \u2032 \u03a8Gcur\u039bcurGcur \u2032 \u03a8Ecur) \u2265 \u03bbmin(RcurRcur\u2032)\u03bbmax(\u039bcur) \u2265 (1 \u2212 (r\u03b6)2)\u03bb+cur when {\u03b6det \u2264 r\u03b6}. The rest of the proof is the same as above. \u22a0"}, {"heading": "11 Detailed Proof of Theorem 3.3 and Proof of Lemma 10.1", "text": "Proof of Theorem 3.3. Recall that we need to show that \u03b6k \u2264 rk\u03b6. Assume the substitutions given in Definition 9.2. We will use induction.\nConsider a k < \u03d1. For the k-th step, assume that \u03b6i \u2264 ri\u03b6 for i = 1, 2, . . . , k \u2212 1. Thus, using (9), \u03b6det \u2264 r\u03b6 and so Lemma 9.8 is applicable. We first show that r\u0302k = rk and that Algorithm 1 does not stop (proceeds to (k+1)-th step). From Algorithm 1, r\u0302k = rk if \u03bb\u03021\n\u03bb\u0302rk \u2264 g\u0302, and \u03bb\u03021 \u03bb\u0302rk+1 > g\u0302. Also\nit will not stop if \u03bb\u0302rk+1 \u2265 \u03bbthresh. Since k < \u03d1, Gundet is not empty. Thus, item 1 of Lemma 9.8 shows that all these hold. Hence r\u0302k = rk and algorithm does not stop w.p. at least 1\u2212p1\u2212p2\u22124p3. Thus, by item 3 of the same lemma, with the same probability, \u03b6k \u2264 rk\u03b6. Now consider k = \u03d1. We first show r\u0302k = rk and that Algorithm 1 does stop, i.e., \u03d1\u0302 = \u03d1. This will be true if \u03bb\u03021 \u03bb\u0302rk\n\u2264 g\u0302 and \u03bb\u0302rk+1 < \u03bbthresh. For k = \u03d1, Gundet is empty. Thus, item 2 of Lemma 9.8 shows that this holds w.p. at least 1\u2212 p1 \u2212 p2 \u2212 4p3. Thus, by item 3 of the same lemma, with the same probability, \u03b6k \u2264 rk\u03b6.\nThus, using the union bound, w.p. at least 1 \u2212 \u03d1(p1 + p2 + 4p3), r\u0302k = rk and \u03b6k \u2264 rk\u03b6 for all k. Using (8), this implies that SE \u2264 r\u03b6 with the same probability. Finally, the choice \u03b1 \u2265 \u03b10, implies that p1 \u2264 1\u03d12n\u221210, p2 \u2264 1\u03d12n\u221210, p3 \u2264 1\u03d12n\u221210. Hence SE \u2264 r\u03b6 w.p. at least 1\u2212 12n\u221210. We work this out for p1 below. The others follow similarly. Recall that p1 = 2n exp(\u2212\u03b1 \u01eb 2\n32b2 prob\n), \u01eb = 0.01(r\u03b6)\u03bb\u2212 and bprob = \u03b7rq((r\u03b6)\u03bb+ + \u03bb+cur +\n(r\u03b6) \u221a \u03bb+\u03bb+cur + \u221a \u03bb+\u03bb+cur). Thus, b2prob (\u03bb\u2212)2 \u2264 (4\u03b7rmax(q(r\u03b6)f, qg, q\n\u221a fg, q(r\u03b6) \u221a fg))2 \u2264\n16\u03b72r2 max(q(r\u03b6)f, qg, q \u221a fg)2\nThus, \u03b1 \u01eb 2\n32b2 prob\n\u2265 32\u00b716(0.01)2 \u03b72r2(11 logn+log\u03d1) (r\u03b6)2 max(q(r\u03b6)f, qg, q \u221a fg) (0.01(r\u03b6)) 2 32\u00b716\u03b72r2 max(q(r\u03b6)f,qg,q \u221a fg)2\n\u2265 11 logn+ log\u03d1. Thus, p1 \u2264 1\u03d12n\u221210. \u22a0\nProof of Lemma 10.1. The first claim is obvious. The next two claims follow using the following lemma:\nLemma 11.1 ([3], Lemma 2.10). Suppose that P , P\u0302 and Q are three basis matrices. Also, P and P\u0302 are of the same size, Q\u2032P = 0 and \u2016(I \u2212 P\u0302 P\u0302 \u2032)P \u2016 = \u03b6\u2217. Then,\n1. \u2016(I \u2212 P\u0302 P\u0302 \u2032)PP \u2032\u2016 = \u2016(I \u2212 PP \u2032)P\u0302 P\u0302 \u2032\u2016 = \u2016(I \u2212 PP \u2032)P\u0302 \u2016 = \u2016(I \u2212 P\u0302 P\u0302 \u2032)P \u2016 = \u03b6\u2217 2. \u2016PP \u2032 \u2212 P\u0302 P\u0302 \u2032\u2016 \u2264 2\u2016(I \u2212 P\u0302 P\u0302 \u2032)P \u2016 = 2\u03b6\u2217 3. \u2016P\u0302 \u2032Q\u2016 \u2264 \u03b6\u2217\n4. \u221a 1\u2212 \u03b62\u2217 \u2264 \u03c3i ( (I \u2212 P\u0302 P\u0302 \u2032)Q ) \u2264 1\nUse item 4 of Lemma 11.1 and the fact that Gdet\u2032Gcur = 0 and Gdet\u2032Gundet = 0 to get the second claim.\nFor the third claim, notice that Ecur\u2032\u03a8Gundet = R\u22121curGcur \u2032 \u03a8Gundet = R\u22121curGcur \u2032G\u0302detG\u0302det\u2032Gundet. since \u03a82 = \u03a8 and Gcur\u2032Gundet = 0. Using the second claim, \u2016R\u22121cur\u2016 \u2264 1\u03c3min(Rcur) \u2264 1 1\u2212(r\u03b6)2 . Use item 3 of Lemma 11.1 and the facts that Gcur \u2032Gdet = 0 and Gundet \u2032Gdet = 0 to bound \u2016Gcur\u2032G\u0302det\u2016 and \u2016G\u0302det\u2032Gundet\u2016 respectively.\nThe fourth claim just uses the definitions. The fifth claim uses the previous claims and the assumptions on Mt from Assumption 1.2. The sixth claim follows using Weyl\u2019s inequality.\nThe second last claim: We show how to bound at,rest: \u2016at,rest\u20162 = \u2016at,cur\u20162 + \u2016at,undet\u20162 \u2264 \u2211\nj\u2208Gcur \u03b7\u03bbj + \u2211 j\u2208Gundet \u03b7\u03bbj \u2264 r\u03b7\u03bb+cur (since \u03bbj \u2264 \u03bb+cur for all the j\u2019s being summed over). The other bounds follow similarly.\nLast claim:\n\u03c3min(Ecur,\u22a5 \u2032 \u03a8Gundet) 2\n= \u03bbmin(Gundet \u2032 \u03a8Ecur,\u22a5Ecur,\u22a5 \u2032 \u03a8Gundet) = \u03bbmin(Gundet \u2032 \u03a8(I \u2212EcurEcur\u2032)\u03a8Gundet)\n\u2265 \u03bbmin(Gundet\u2032\u03a8\u03a8Gundet)\u2212 \u03bbmax(Gundet \u2032 \u03a8EcurEcur \u2032 \u03a8Gundet)\n= \u03c3min(\u03a8Gundet) 2 \u2212 \u2016Ecur\u2032\u03a8Gundet\u2016\n\u2265 1\u2212 (r\u03b6)2 \u2212 (r\u03b6) 2 \u221a\n1\u2212 (r\u03b6)2 .\nThe last inequality follows using the second and the third claim. \u22a0"}], "references": [{"title": "Finite sample approximation results for principal component analysis: A matrix perturbation approach", "author": ["B. Nadler"], "venue": "The Annals of Statistics, vol. 36, no. 6, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time robust principal components\u2019 pursuit", "author": ["C. Qiu", "N. Vaswani"], "venue": "Allerton Conf. on Communication, Control, and Computing, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Recursive robust pca or recursive sparse recovery in large but structured noise", "author": ["C. Qiu", "N. Vaswani", "B. Lois", "L. Hogben"], "venue": "IEEE Trans. Info. Th., pp. 5007\u20135039, August 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Online matrix completion and online robust pca", "author": ["B. Lois", "N. Vaswani"], "venue": "IEEE Intl. Symp. Info. Th. (ISIT), 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Online (and Offline) Robust PCA: Novel Algorithms and Performance Guarantees", "author": ["J. Zhan", "B. Lois", "H. Guo", "N. Vaswani"], "venue": "Intnl. Conf. Artif. Intell. and Stat. (AISTATS), 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Stochastic optimization of pca with capped msg", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 1815\u20131823.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A stochastic pca and svd algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "arXiv:1409.2848, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Online principal components analysis", "author": ["C. Boutsidis", "D. Garber", "Z. Karnin", "E. Liberty"], "venue": "Proc. ACM-SIAM Symposium on Discrete Algorithms (SODA), 2015, pp. 887\u2013901.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The fast convergence of incremental pca", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 3174\u20133182.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Online pca with spectral bounds", "author": ["Z. Karnin", "E. Liberty"], "venue": "Proce. Conference on Computational Learning Theory (COLT), 2015, pp. 505\u2013509.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory limited, streaming pca", "author": ["I. Mitliagkas", "C. Caramanis", "P. Jain"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2013, pp. 2886\u20132894.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix rank minimization with applications", "author": ["M. Fazel"], "venue": "PhD thesis, Stanford Univ, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Found. of Comput. Math, , no. 9, pp. 717\u2013772, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust principal component analysis", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of ACM, vol. 58, no. 3, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM Journal on Optimization, vol. 21, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust matrix decomposition with sparse corruptions", "author": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "IEEE Trans. Info. Th., Nov. 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-convex robust pca", "author": ["P. Netrapalli", "U N Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain"], "venue": "Neural Info. Proc. Sys. (NIPS), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Provably correct recursive projected compressive sensing (reprocs) for dynamic robust pca: A correlated-pca reformulation", "author": ["N. Vaswani", "B. Lois", "P. Narayanamurthy"], "venue": "http://www.ece.iastate.edu/long_RobSubTrack_3.pdf, submitted to ICASSP 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "The rotation of eigenvectors by a perturbation. iii", "author": ["C. Davis", "W.M. Kahan"], "venue": "SIAM J. Numer. Anal., vol. 7, pp. 1\u201346, Mar. 1970.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1970}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.A. Tropp"], "venue": "Found. Comput. Math., vol. 12, no. 4, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["R. Vershynin"], "venue": "Compressed sensing, pp. 210\u2013268, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Eigenvalue computation in the 20th century", "author": ["G.H. Golub", "H.A. Van der Vorst"], "venue": "Journal of Computational and Applied Mathematics, vol. 123, no. 1, pp. 35\u201365, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["P. Netrapalli", "P. Jain", "S. Sanghavi"], "venue": "Symposium on Theory of Computing (STOC), 2013.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Alternating direction algorithms for l1 problems in compressive sensing", "author": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "Tech. Rep., University of Illinois at Urbana-Champaign, November 2009. 9", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": ", see [1] and references therein.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 2, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 3, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 4, "context": "We first encountered it while solving the dynamic robust PCA problem in the Recursive Projected Compressive Sensing (ReProCS) framework [2, 3, 4, 5].", "startOffset": 136, "endOffset": 148}, {"referenceID": 5, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 49, "endOffset": 55}, {"referenceID": 7, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 8, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 9, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 10, "context": "Some other somewhat related recent works include [6, 7] that study stochastic optimization based techniques for PCA; and [8, 9, 10, 11] that study online PCA.", "startOffset": 121, "endOffset": 135}, {"referenceID": 11, "context": ", [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": ", [12, 13], PCA-missing can also be solved by first solving the low-rank matrix completion problem to recoverL, followed by PCA on the completed matrix.", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with lt.", "startOffset": 98, "endOffset": 110}, {"referenceID": 14, "context": "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with lt.", "startOffset": 98, "endOffset": 110}, {"referenceID": 15, "context": "Another example where correlated-PCA occurs is that of robust PCA (low-rank + sparse formulation) [14, 15, 16] when the sparse component\u2019s magnitude is correlated with lt.", "startOffset": 98, "endOffset": 110}, {"referenceID": 13, "context": "One key application where it occurs is in foreground-background separation for videos consisting of a slow changing background sequence (modeled as lying close to a low-dimensional subspace) and a sparse foreground image sequence consisting typically of one or more moving objects [14].", "startOffset": 281, "endOffset": 285}, {"referenceID": 13, "context": "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.", "startOffset": 117, "endOffset": 125}, {"referenceID": 14, "context": "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.", "startOffset": 117, "endOffset": 125}, {"referenceID": 16, "context": "An alternative solution approach for PCA-SDDC is to use an RPCA solution such as principal components\u2019 pursuit (PCP) [14, 15] or Alternating-Minimization (Alt-Min-RPCA) [17] to first recover the matrix L followed by PCA on L.", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "A third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 4, "context": "A third example where correlated-PCA and its generalization, correlated-PCA with partial subspace knowledge, occurs is in the subspace update step of Recursive Projected Compressive Sensing (ReProCS) for dynamic robust PCA [3, 5].", "startOffset": 223, "endOffset": 229}, {"referenceID": 17, "context": "We refer the reader to [18] to understand this application.", "startOffset": 23, "endOffset": 27}, {"referenceID": 3, "context": "The following lemma [4] shows that, with Assumption 1.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "[[4], Lemmas 5.", "startOffset": 1, "endOffset": 4}, {"referenceID": 3, "context": "7, or [4].", "startOffset": 6, "endOffset": 9}, {"referenceID": 18, "context": "Proof: The proof involves a careful application of the sin \u03b8 theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin \u03b8 bound.", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Proof: The proof involves a careful application of the sin \u03b8 theorem [19] to bound the subspace error, followed by using matrix Hoeffding [20] to obtain high probability bounds on each of the terms in the sin \u03b8 bound.", "startOffset": 138, "endOffset": 142}, {"referenceID": 19, "context": "As a result we can apply the matrix Hoeffding inequality [20] to bound the perturbation between the observed data\u2019s empirical covariance matrix and that of the true data.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "39 of [21] in places where one can apply a concentration of measure result to \u2211 t atat \u2032/\u03b1 (which is at r \u00d7 r matrix), and by matrix Bernstein [20] elsewhere, it should be possible to further reduce the sample complexity to cmax((qf)r logn, f(r+ logn)).", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "39 of [21] in places where one can apply a concentration of measure result to \u2211 t atat \u2032/\u03b1 (which is at r \u00d7 r matrix), and by matrix Bernstein [20] elsewhere, it should be possible to further reduce the sample complexity to cmax((qf)r logn, f(r+ logn)).", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "This assumption can be understood as a generalization of the eigen-gap condition needed by the block power method, which is a fast algorithm for obtaining the k top eigenvectors of a matrix [22].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "Algorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS.", "startOffset": 84, "endOffset": 90}, {"referenceID": 4, "context": "Algorithm 1 is related to, but significantly different from, the ones introduced in [3, 5] for the subspace deletion step of ReProCS.", "startOffset": 84, "endOffset": 90}, {"referenceID": 2, "context": "The one introduced in [3] assumed that the clusters were known to the algorithm (which is unrealistic).", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "The one studied in [5] has an automatic cluster estimation approach, but, one that needs a larger lower bound on \u03b1 compared to what Algorithm 1 needs.", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": ", nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23].", "startOffset": 34, "endOffset": 38}, {"referenceID": 22, "context": ", nuclear norm minimization (NNM) [13] or alternating minimization (Alt-Min-MC) [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 148, "endOffset": 160}, {"referenceID": 14, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 148, "endOffset": 160}, {"referenceID": 15, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 148, "endOffset": 160}, {"referenceID": 16, "context": "Similarly, for PCA-SDDC, this can be done by solving any of the recent provably correct RPCA techniques such as principal components\u2019 pursuit (PCP) [14, 15, 16] or alternating minimization (Alt-Min-RPCA) [17].", "startOffset": 204, "endOffset": 208}, {"referenceID": 16, "context": "If we use the time complexity from [17], then finding the span of the top k singular vectors of an n \u00d7m matrix takes O(nmk) time.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "Thus, if \u03b8 is a constant, both simple-EVD and c-EVD need O(n\u03b1r) time, whereas, Alt-Min-RPCA needs O(n\u03b1r) time per iteration [17].", "startOffset": 124, "endOffset": 128}, {"referenceID": 14, "context": "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "We compare EVD and cluster-EVD (c-EVD) with PCP [15], solved using [24], and with Alt-Min-RPCA [17] (implemented using code", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "We used the matrix Hoeffding inequality [20] to obtain our results.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "Moreover, as done in [5] (for ReProCS), the mutual independence of lt\u2019s can be easily replaced by a more practical assumption of lt\u2019s following autoregressive model with almost no change to our assumptions.", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "The solution to the latter problem helps to greatly simplify the proof of correctness of ReProCS for online dynamic RPCA [18].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "References [1] B.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] B.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] O.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Z.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "1 sin \u03b8 theorem Davis and Kahan\u2019s sin \u03b8 theorem [19] studies the rotation of eigenvectors by perturbation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "1 (sin \u03b8 theorem [19]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "1 We use the sin \u03b8 theorem [19] from Corollary 8.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "In the next lemma, we bound the terms in the bound on SE(P\u0302 ,P ) using the matrix Hoeffding inequality [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "The following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "The following corollaries of the matrix Hoeffding inequality [20], proved in [3], will be used in the proof.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "1 ([3], Lemma 2.", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "Given a matrix of observed data, Principal Components Analysis (PCA) computes a small number of orthogonal directions that contain most of its variability. Provably accurate solutions for PCA have been in use for a long time. However, to the best of our knowledge, all existing theoretical guarantees for it assume that the data and the corrupting noise are mutually independent, or at least uncorrelated. This is valid in practice often, but not always. In this paper, we study the PCA problem in the setting where the data and noise can be correlated. Such noise is often also referred to as \u201cdata-dependent noise\u201d. We obtain a correctness result for the standard eigenvalue decomposition (EVD) based solution to PCA under simple assumptions on the data-noise correlation. We also develop and analyze a generalization of EVD, cluster-EVD, that improves upon EVD in certain regimes.", "creator": "LaTeX with hyperref package"}}}