{"id": "1608.04426", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Regularization for Unsupervised Deep Neural Nets", "abstract": "unsupervised neural networks, such as restricted boltzmann machines ( rbms ) and deep belief complexity networks ( dbns ), are powerful scaling tools for feature selection and pattern recognition tasks. basically we demonstrate that overfitting occurs in such models just disguised as in deep feedforward neural networks, and discuss possible regularization experimental methods to subsequently reduce overfitting. we consider weight random decay, model averaging methods, and backward symmetry elimination, and propose revised model sample averaging methods to improve their efficiency. we also will discuss the asymptotic convergence properties of these respective methods. as finally, we compare the performance of these methods using likelihood and classification error rates on various pattern event recognition data sets.", "histories": [["v1", "Mon, 15 Aug 2016 22:28:05 GMT  (61kb,D)", "http://arxiv.org/abs/1608.04426v1", null], ["v2", "Tue, 30 Aug 2016 23:41:53 GMT  (66kb,D)", "http://arxiv.org/abs/1608.04426v2", null], ["v3", "Mon, 5 Sep 2016 16:53:55 GMT  (66kb,D)", "http://arxiv.org/abs/1608.04426v3", null], ["v4", "Fri, 17 Feb 2017 02:49:12 GMT  (147kb,D)", "http://arxiv.org/abs/1608.04426v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["baiyang wang", "diego klabjan"], "accepted": true, "id": "1608.04426"}, "pdf": {"name": "1608.04426.pdf", "metadata": {"source": "CRF", "title": "Regularization for Unsupervised Deep Neural Nets", "authors": ["Baiyang Wang", "Diego Klabjan"], "emails": ["baiyang@u.northwestern.edu,", "d-klabjan@northwestern.edu"], "sections": [{"heading": "1 Introduction", "text": "Unsupervised neural networks are generally probabilistic graphical models with a neural network structure, and have been applied extensively to pattern analysis and recognition problems. The most basic one is the restricted Boltzmann machine (RBM) [5, 13], a Markov random field model with a bipartite structure containing a layer of hidden nodes and a layer of visible nodes. With such a basic structure, we can consider stacking multiple layers of RBMs to create a unsupervised deep neural network structure, such as the deep belief network (DBN) [8] and the deep Boltzmann machine (DBM) [14]. These models can be calibrated with a combination of the stochastic gradient descent and the contrastive divergence (CD) algorithm [5, 13] (or the PCD algorithm [21]). Once we obtain the learned distribution of a model, we can retrieve the hidden values of the model from visible values, thus using unsupervised neural networks as a tool for feature selection. Alternatively, we may consider using the parameters obtained from a unsupervised deep neural network to initialize a deep feedforward neural network (FFNN), applying them as a component of supervised learning.\nOne essential question for such models is to adjust for the high-dimensionality of their parameters and avoid overfitting. In FFNNs, the simplest regularization is arguably the early stopping method [25], which stops the gradient descent algorithm before the validation error rate goes up. The weight decay method [25], or Ls regularization, is also commonly used. Recently Dropout [18] is proposed, which optimizes the parameters over an average of exponentially many models with a subset of all nodes. It has been shown to outperform weight decay regularization in many situations [18].\nFor regularizing unsupervised neural networks, sparse-RBM-type models [4, 10] encourage a smaller proportion of 1-valued hidden nodes. DBNs are regularized in [6] with outcome labels. While these works tend to be goal-specific, we consider regularization for unsupervised neural networks in a more general setting. Our work and contributions are as follows: (1) we extend common regularization methods to unsupervised deep neural networks, and discuss their convergence properties; (2) we propose partial Dropout/DropConnect which can improve the performance of Dropout/DropConnect; (3) we compare the performance of different regularization methods on real data sets, and thus provide suggestions on regularizing unsupervised neural networks. We note that this is the very first study establishing convergence properties of various regularization methods, including the effective newly proposed partial Dropout/DropConnect.\nManuscript.\nar X\niv :1\n60 8.\n04 42\n6v 1\n[ cs\n.L G\n] 1\n5 A\nug 2\n01 6\nSection 2 discusses recent works for regularizing neural networks, and Section 3 exhibits RBM regularization. Section 4 provides a convergence analysis for each regularization method. Section 5 discusses regularization extended to other unsupervised neural networks. Section 6 presents a numerical comparison of different regularization methods on RBM, DBN, DBM, RSM [15] and Gaussian RBM [13]. Section 7 discusses potential future research and concludes the paper."}, {"heading": "2 Related Works", "text": "We discuss a simple FFNN with a single layer of input \u0131 = (\u01311, . . . , \u0131I)T and a single layer of output o = (o1, . . . , oJ)\nT \u2208 {0, 1}J . The weight matrix W is of size J \u00d7 I . We assume the relation E(o) = a(W \u00b7 \u0131), (1)\nwhere a(\u00b7) is the activation function, such as the sigmoid function \u03c3(x) = 1/(1 + e\u2212x) applied element-wise. Equation (1) is modified in [18] as\nE(o|m) = a(m ? (W \u00b7 \u0131)), m = (m1, . . . ,mJ)T iid\u223c Bernoulli(p), (2)\nwhere ? denotes element-wise multiplication, thereby achieving the Dropout (DO) regularization for neural networks. In Dropout, we minimize the objective function\n\u2212 lDO(W ) = \u2212 N\u2211 n=1 Em[log p(o (n)|\u0131(n),W,m)], (3)\nwhich can be achieved by a stochastic gradient descent algorithm, sampling a different m per data example (o(n), \u0131(n)) and per iteration. We observe that this can be readily extended to deep FFNNs. Dropout regularizes neural networks because it incorporates prediction based on a subset of all nodes, therefore penalizing the likelihood. A theoretical explanation is provided in [22] for Dropout, noting that it can be viewed as feature noising for GLMs, and we have the relation\n\u2212 lDO(W ) . = \u2212 N\u2211 n=1 log p(o(n)|\u0131(n),W ) +Rq(W ). (4)\nHere J = 1 for simplicity, and Rq(W ) = 12 p 1\u2212p \u2211N n=1 \u2211I i=1A \u2032\u2032(W\u0131(n))(\u0131 (n) i )\n2W 2i , where A(\u00b7) is the log-partition function of a GLM. Therefore, Dropout can be viewed approximately as the adaptive L2 regularization [2, 22, 24]. A recursive approximation of Dropout is provided in [2] using normalized weighted geometric means to show its averaging properties.\nAn intuitive extension of Dropout is DropConnect (DC) [23], which modifies (2) as\nE(o|m) = a((m ?W ) \u00b7 \u0131), m = (mij)J\u00d7I iid\u223c Bernoulli(p), (5)\nand thus masks the weights rather than the nodes. The objective lDC(W ) has the same form as in (3). There is a number of similar model averaging regularization methods, each of which averages over weakened forms of the original model. For instance, Standout [1] varies Dropout probabilities for different nodes which constitute a binary belief network. Shakeout [9] adds additional noise to Dropout so that it approximates elastic-net regularization [27]. Drop-In [17] considers skipping of layers for deep FFNNs. Fast Dropout [24] accelerates Dropout with Gaussian approximation.\nWe note that while Dropout has been discussed for RBMs [18], there is no literature extending common regularization methods to RBMs and unsupervised deep neural networks; for instance, adaptive Ls regularization and DropConnect as mentioned. Therefore, we discuss their implementations and examine their empirical performance below. In addition to showing convergence properties, we also propose partial Dropout/DropConnect which randomly drops a subset of nodes or edges based on a first calibrated model, thus improving robustness in many situations."}, {"heading": "3 RBM Regularization", "text": "For a Restricted Boltzmann machine, we let v = (v1, \u00b7 \u00b7 \u00b7 , vJ)T \u2208 {0, 1}J denote the visible vector, and h = (h1, \u00b7 \u00b7 \u00b7 , hI)T \u2208 {0, 1}I denote the hidden vector. Each vj , j = 1, . . . , J is a visible node and each hi, i = 1, . . . , I is a hidden node. The joint probability is\nP (v, h) = e\u2212E(v,h)/ \u2211 \u03bd,\u03b7 e\u2212E(\u03bd,\u03b7), E(v, h) = \u2212bT v \u2212 cTh\u2212 hTWv. (6)\nWe let \u03d1 = (b, c,W ), i.e. \u03d1 is a vector containing all components of b, c, W . In reality, we only have data of visible vector v, so the model calibration is to find \u03b8\u0302 = arg max\n\u03d1\u2208\u0398\n\u2211N n=1 logP (v (n)|\u03d1).\nAn RBM is a neural network because we have the following relations\nP (hi = 1|v) = \u03c3(ci +Wi\u00b7v), P (vj = 1|h) = \u03c3(bj +WT\u00b7j h), (7) where Wi\u00b7 and W\u00b7j represent, respectively, the i-th row and j-th column of W . To calibrate \u03d1, the gradient descent algorithm requires \u2212 \u2211N n=1 \u2202 logP (v (n))/\u2202\u03d1 in each iteration. We note that\n\u2212\u2202 logP (v (n)) \u2202\u03d1 = \u2202F(v(n)) \u2202\u03d1 \u2212 \u2211 v\u2208{0,1}J P (v) \u2202F(v) \u2202\u03d1 , (8)\nwhere F(v) = \u2212bT v \u2212 \u2211I i=1 log(1 + e\nci+Wi\u00b7v) is the free energy. The right-hand side of (8) is approximated by contrastive divergence with k steps of Gibbs sampling (CD-k) [5, 13]."}, {"heading": "3.1 Weight Decay Regularization", "text": "Weight decay, or Ls regularization, adds the term \u03bb\u2016W\u2016ss to the negative log-likelihood of an RBM. The most commonly used is L2 (ridge regression), or L1 (LASSO [20]). In CD-k, we add the term \u03bbs\u2016W\u2016s\u22121s\u22121 to \u2212 \u2211N n=1 \u2202 logP (v (n))/\u2202\u03d1. We do not regularize biases for simplicity.\nHere we consider a more general form of weight decay regularization. Suppose we have obtained an estimate W\u0302 of W from CD with no regularization. Instead of adding the term \u03bb\u2016W\u2016ss, we add the term \u00b5IJ \u2211 i,j |Wij |s/|W\u0302ij |s to the negative log-likelihood. Apparently this adjusts for the different scales of the components of W . While for an RBM, all components of W are identical a priori, we show later that this approach can sometimes yield empirical improvements over simple weight decay regularization. For L1, this approach is named adaptive LASSO, and has been shown to possess better theoretical properties than LASSO [26]. We refer to this approach as adaptive Ls. We note that Dropout approximates adaptive L2 as in (4) [22].\nWe may also consider adding multiple terms with different s to the negative log-likelihood. Adding both L2 and L1 is named the elastic-net regularization [27]. We consider the performance of L2 regularization plus adaptive L1 regularization (L2 +AL1) below."}, {"heading": "3.2 Model Averaging Regularization", "text": "As discussed in [18], to characterize a Dropout (DO) RBM, we simply need to apply the following conditional distributions\nPDO(hi = 1|v,m) = mi \u00b7 \u03c3(ci +Wi\u00b7v), PDO(vj = 1|h,m) = \u03c3(bj +WT\u00b7j h). (9)\nTherefore, given a fixed mask m \u2208 {0, 1}I , we actually have an RBM with all visible nodes v and hidden nodes {hi : mi = 1}. Hidden nodes {hi : mi = 0} are fixed to zero so they have no influence on the conditional RBM. For the purpose of feature selection, we note that\nEDO(hi|v) = p \u00b7 \u03c3(ci +Wi\u00b7v). (10) Hence, when using EDO(hi|v) as output features, the retaining probability of any hidden node, p, is only multiplied to the output features of a plain RBM. Therefore, unlike [18] which downscales the weights W with factor p for FFNNs, we perform Dropout only in the training procedure, and output a plain RBM with trained weights.\nIn terms of training, we suggest sampling a different mask per v(n) and per iteration as in [18]. Apart from replacing (7) with (9), the only other change needed is to replace F(v) with FDO(v|m) = \u2212bT v \u2212 \u2211I i=1mi log(1 + e ci+Wi\u00b7v).\nA DropConnect (DC) RBM is somewhat simpler; given a mask m = {0, 1}IJ on weights W , W in a plain RBM is replaced by m \u2217W everywhere. We suggest sampling a different mask m per mini-batch since it is usually much larger than a mask in a Dropout RBM. For simplicity, we suggest that the weights and biases constitute a plain RBM after training, although downscaling [18] or sampling replicates [23] may also be considered."}, {"heading": "3.3 Backward Elimination", "text": "Backward elimination (BE) [25], commonly applied to regression problems, may also be considered for RBMs. We note that due to the high dimensionality and nonlinearity of RBMs, it seems the most possible variant among stepwise regression techniques. For an estimate W\u0302 of W with no regularization, we consider implementing a fixed mask m = (mij)I\u00d7J where\nmij = 1|W\u0302ij |\u2265Q, Q = Q100(1\u2212p)%(|W\u0302 |), (11)\ni.e. Q is the 100(1\u2212 p)%-th left percentile of all |W\u0302ij |, and p \u2208 (0, 1) is some fixed proportion of retained weights. We then recalibrate the weights and biases fixing mask m, leading to a simple backward elimination procedure which deletes 100(1\u2212 p)% of all weights. We may also consider deleting 100(1\u2212 p)/r% of all weights at a time, and conduct the above process r times, leading to an iterative backward elimination (IBE) procedure. We note that both BE and IBE belong to network pruning algorithms typically applied to FFNNs [12]."}, {"heading": "3.4 Hybrid Regularization", "text": "We may consider combining some of the above approaches. For instance, [18] considered a combination of Ls and Dropout. We introduce two such hybrid approaches, namely partial DropConnect (PDC) and partial Dropout (PDO). The former is a hybrid of DropConnect and BE. The rationale for these methods come from some of the convergence results presented in Section 4.\nSuppose we have an estimate W\u0302 of W with no regularization. Instead of implementing a fixed mask m, we perform DropConnect regularization with different retaining probabilities pij for each weight Wij . We let Q = Q100(1\u2212q)%(|W\u0302 |), and\npij = 1|W\u0302ij |\u2265Q + p \u00b7 1|W\u0302ij |<Q. (12)\nTherefore, we sample a different m = (mij)I\u00d7J ind\u223c Bernoulli(pij) per mini-batch, which means we always keep 100q% of all the weights, and randomly drop the remaining weights with probability 100(1\u2212 p)%. The rest follows DropConnect. We propose this technique because we hypothesize that some weights could be more important than others a posteriori, so dropping them could cause much variation among the models being averaged. Thus, it could be preferable not to drop them. We demonstrate this point empirically later.\nWe may consider a closely related partial Dropout technique. We let m = (m1, . . . ,mI), mi ind\u223c Bernoulli(pi), where\npi = 1\u2016W\u0302i\u00b7\u2016\u2265Q + p \u00b7 1\u2016W\u0302i\u00b7\u2016<Q, Q = Q100(1\u2212q)%(\u2016W\u0302i\u00b7\u2016). (13) This algorithm protects more important hidden nodes from being dropped, if any. We also evaluate its empirical performance later."}, {"heading": "4 Theoretical Considerations", "text": "Here we discuss the convergence properties of different regularization methods when the number of data examples N \u2192 \u221e. We mark all regularization coefficients and parameter estimates with (N) when there are N examples. We assume \u03d1 = (b, c,W ) \u2208 \u0398, which is compact, dim(\u0398) = D, P (v|\u03d1) is unique for each \u03d1 \u2208 \u0398, and v(1), . . . , v(N) are i.i.d. generated from an RBM with a \u201ctrue\u201d parameter \u03b8. We denote each regularized estimate of \u03b8 as \u03b8\u0303(N).\nLet A = {d : \u03b8d 6= 0} and \u03b8A = {\u03b8d : d \u2208 A}. It is shown in [26] that AL1 guarantees asymptotic normality and identification of set A for linear regression. We show that for L2 +AL1 for RBMs, similar results hold. We let \u03bb(N) = (\u03bb(N)1 , . . . , \u03bb (N) D ) and \u00b5 (N) = (\u00b5 (N) 1 , . . . , \u00b5 (N) D ) be the L\n2 and L1 regularization coefficients for each component.\nProposition 1. (a) If \u03bb(N)/N \u2192 0 and \u00b5(N)/N \u2192 0 with N \u2192\u221e, then the estimate \u03b8\u0303(N) P\u2192 \u03b8; (b) if also, \u00b5(N)d / \u221a N \u2192 0\u00b71\u03b8d 6=0 +\u221e\u00b71\u03b8d=0, \u03bb(N)/ \u221a N \u2192 0, then \u221a n(\u03b8\u0303 (N) A \u2212\u03b8A)\nd\u2192 N(0, I\u22121(\u03b8A)), where I is the Fisher information matrix; P (A\u0302(N) = A)\u2192 1, where A\u0302(N) = {d : \u03b8\u0303(N)d 6= 0}. Proof. For all proofs, see Appendix.\nFor Dropout and DropConnect RBMs, we assume the data is generated from a plain RBM structure. We assume p(N) is of size I \u00d7 J as in (12) for DropConnect and of length I as in (13) for Dropout, therefore covering both original and partial Dropout/DropConnect. Assuming a decreasing dropping rate 1\u2212 p(N) \u2192 0 with N \u2192\u221e, we have the following result.\nProposition 2. Assume p(N) \u2192 1 with N \u2192\u221e. Then \u03b8\u0303(N) P\u2192 \u03b8. For backward elimination, we show that if the retained proportion p(N) \u2261 p can cover all nonzero components of \u03b8, it covers them asymptotically.\nProposition 3. Assume p > p0 := |A|/D. Then for simple backward elimination, as N \u2192\u221e, (a) \u03b8\u0303(N)\nP\u2192 \u03b8; (b) for sufficiently large N , there exists \u03c1 > 0 such that P (A \u2208 A\u0302(N)) \u2265 1\u2212 e\u2212\u03c1N . Corollary 1. For iterative backward elimination, same results hold.\nWe note that for all regularization methods, under the above conditions, the estimates converge to the \u201ctrue\u201d parameter \u03b8, which indicates consistency. Also, adding L1 regularization guarantees that we can identify components of zero value with infinitely many examples. The major benefits of Dropout come from the facts that it makes L2 regularization adaptive, and also encourages more confident prediction of the outcomes [22]. We propose partial DropConnect according to Proposition 3, i.e. we do not drop the hypothesized nonzero components of \u03b8, therefore possibly reducing variation caused by dropping influential weights. Partial Dropout follows from the same logic."}, {"heading": "5 Extension to Other Networks", "text": ""}, {"heading": "5.1 Deep Belief Networks", "text": "We consider the multilayer network below,\nP (v, h1, . . . , hL) = P (v|h1)P (h1|h2) \u00b7 \u00b7 \u00b7P (hL\u22122|hL\u22121)P (hL\u22121, hL), (14)\nwhere each probability on the right-hand side represents one from an RBM. To train the weights of RBM(v, h1), . . . ,RBM(hL\u22121, hL), we only need to carry out a greedy layer-wise training approach, i.e. we first train the weights of RBM(v, h1), and then use E(h1|v) to train RBM(h1, h2), etc. The weights of the RBMs are used to initialize a deep FFNN which is finetuned with gradient descent. Apparently RBM regularization can be extended to each layer of a DBN.\nIt is shown in [3, 8] that adding layers to a DBN improves the likelihood given symmetry of the weights of two adjacent layers. We show this also holds for Dropout/DropConnect DBNs given a uniform retaining proportion p.\nProposition 4. Adding nodes or layers (preserving weight symmetry) to a Dropout/DropConnect DBN improves the likelihood; also, adding layers of size J \u2264 H1 \u2264 H2 \u2264 \u00b7 \u00b7 \u00b7 (HL is the size of the L-th hidden layer) continually improves the likelihood."}, {"heading": "5.2 Other RBM Variants", "text": "Unlike a DBN which trains model (14), a DBM [14] trains an undirected graphical model. The major difference is that in the training procedure for a DBM, both the visible and hidden nodes in the deepest hidden layer are doubled. The weights for the doubled nodes are tied to the original weights. Also, E(hL|v) may be added to the original features. For a replicated softmax model (RSM) [15],\nE(v, h) = \u2212bT v \u2212 C \u00b7 cTh\u2212 hTWv, 1T v = C, v \u2208 NJ , h \u2208 {0, 1}I , (15)\nand for a Gaussian RBM [13],\nE(v, h) = J\u2211 j=1 (vj \u2212 aj)2/(2\u03c32j )\u2212 cTh\u2212 hTW (v/\u03c3), v \u2208 RJ , h \u2208 {0, 1}I . (16)\nwhere v/\u03c3 is element-wise. They are simple extensions of an RBM to count outcomes and real-valued outcomes. RBM regularization can be easily extended to all these situations."}, {"heading": "6 Data Studies", "text": "We compare the empirical performance of the aforementioned regularization methods on the following data sets: MNIST, NORB (image recognition); 20 Newsgroups, Reuters21578 (text classification); ISOLET (speech recognition). All results are obtained using GeForce GTX TITAN X in Theano."}, {"heading": "6.1 Experiment Settings", "text": "We consider the following unsupervised neural network structures: DBN/DBM for MNIST; DBN for NORB; RSM plus logistic regression for 20 Newsgroups and Reuters21578; GRBM for ISOLET. CD-1 is performed for the rest of the paper. The following regularization methods are considered: None (no regularization); DO; DC; L2; L2 + AL1; BE; IBE(r = 3); PDO; PDC. The number of pretraining epochs is 100 per layer and the number of finetuning epochs is 300, with a finetuning learning rate of 0.1. For the last five regularization methods which need re-calibration, we cut the 100 epochs into two halves (4 quarters for IBE). For regularization parameters, we apply the following ranges: p = 0.8 \u223c 0.9 for DO/DC/BE/IBE; \u03bb = 10\u22125 \u223c 10\u22124 for L2, similar to [7]; \u00b5 = 0.01 \u223c 0.1 for L2 +AL1; p = 0.5, q = 0.7 \u223c 0.9 or the reverse for PDO/PDC. We note that unsupervised neural networks tend to need less regularization than FFNNs. We choose the best iteration and regularization parameters over a fixed set of parameter values according to the validation error rates."}, {"heading": "6.2 The MNIST Data Set", "text": "The MNIST data set consists of 282 pixels of handwritten 0-9 digits. There are 50,000 training examples, 10,000 validation and 10,000 testing examples. We first consider the likelihood of the testing data of an RBM with 500 nodes for MNIST. There are two model fitting evaluation criteria: pseudo-likelihood and AIS-likelihood [16]. The former is a sum of conditional likelihoods, while the latter directly estimates P (v). Figure 1 is using log-scale.\nHere, p = 0.9 for DO, and \u03bb = 10\u22124 for L2. These figures tend to be representative of the model fitting process. Pseudo-likelihood is a more optimistic estimate of the model fitting. We observe that Dropout outperforms the other two after about 50 epochs, and L2 does not improve pseudo-likelihood. In terms of AIS-likelihood, which is a much more conservative estimate of the model fitting, the fitting process seems to have three stages: (1) initial fitting; (2) overfitting; (3) re-fitting. We observe that L2 improves the likelihood significantly, while Dropout catches up at about 300 epochs. Therefore, both Dropout and L2 can improve model fitting according to the likelihood.\nClassification error rates tend to be a more practical measure. We first consider a 3-hidden-layer DBN with 1,000 nodes per layer, pretraining learning rate 0.01, and batch size 10; see Table 1. We tried DBNs of 1, 2, and 4 hidden layers and found the aforementioned structure to perform best with None as baseline. The same was done for all other structures. We calculate the means of the classification\nerrors for each regularization method averaged over 5 random replicates and their standard deviations. In each table, we stress in bold the top 3 performers with ties broken by deviation.\nWe note that most of the regularization methods tend to improve the classification error rates, with DC and PDO yielding slightly higher error rates than no regularization. In Table 2 we consider a 3-hidden-layer DBM with 1,000 nodes per layer. For simplicity, we only classify based on the original features. We let the pretraining learning rate be 0.03 and the batch size be 10.\nIt can be observed that regularization tends to yield more improvement for DBM than DBN, possibly because a DBM doubles both the visible layer and the third hidden layer, resulting in a \u201clarger\u201d neural network structure in general. Only IBE proves to be unsuitable for the DBM; all other regularization methods work better, with PDC being the best."}, {"heading": "6.3 The NORB Data Set", "text": "None DO DC L2 L2 +AL1 BE IBE PDO PDC m. 11.00% 11.15% 11.19% 10.93% 10.91% 11.04% 11.14% 10.95% 10.81% sd. 0.15% 0.12% 0.10% 0.18% 0.17% 0.18% 0.20% 0.15% 0.13%"}, {"heading": "6.4 The 20 Newsgroups Data Set", "text": "The 20 Newsgroups data set is a collection of news documents with 20 categories. There are 11,293 training examples, from which 6,293 validation examples are randomly held out, and 7,528 testing examples. We adopt the stemmed version, retain the most common 5,000 words, and train an RSM with 1,000 hidden nodes in a single layer. We consider this as a simple case of deep learning since it is a two-step procedure. The pretraining learning rate is 0.02 and the batch size is 50. We apply logistic regression to classify the trained features, i.e. hidden values of the RSM, as in [19]. This setting is quite challenging for unsupervised neural networks. In Table 4, Dropout performs best with other regularization methods yielding improvements except DropConnect."}, {"heading": "6.5 The Reuters21578 Data Set", "text": "The Reuters21578 data set is a collection of newswire articles. We adopt the stemmed R-52 version which has 52 categories, 6,532 training examples, from which 1,032 validation examples are randomly held out, and 2,568 testing examples. We retain the most common 2,000 words, and train an RSM with 500 hidden nodes in a single layer. The pretraining learning rate is 0.1 and the batch size is 50. We make the learning rate large because the cost function is quite bumpy. From Table 5, we note that PDC works best, and PDO improves the performance of Dropout."}, {"heading": "6.6 The ISOLET Data Set", "text": "The ISOLET data set consists of voice recordings of the Latin alphabet (a-z). There are 6,138 training examples, from which 638 validation examples are randomly held out, and 1,559 testing examples. We train a 1,000-hidden-node Gaussian RBM with pretraining learning rate 0.005, batch size 20, and initialize a FFNN, which can be viewed as a single-hidden-layer DBN. From Table 6, it is evident that all regularization methods work better then None, with PDC again being the best."}, {"heading": "6.7 Summary", "text": "From the above results, we observe that regularization does improve the structure of unsupervised deep neural networks and yields lower classification error rates for each data set studied herein. The most robust methods which yield improvements for all six instances are L2, L2 + AL1, and PDC. PDO can yield improvements for Dropout when Dropout is unsuitable for the network structure. PDC turns out to be the most stable method of all, and thus the recommended choice."}, {"heading": "7 Conclusion", "text": "Regularization for deep learning has aroused much interest, and in this paper, we extend regularization to unsupervised deep learning, i.e. for DBNs and DBMs. We proposed several approaches, demonstrated their performance, and empirically compared the different techniques. For the future, we suggest that it would be of interest to consider more variants of model averaging regularization for supervised deep learning as well as better hybrid regularization methods."}, {"heading": "P (A 6\u2282 A\u0302(N)) + lim", "text": "N\u2192\u221e sup A\u2282A P (\u2016\u03b8\u0302(N)A \u2212 \u03b8\u2016 \u2265 ) = 0. (28)\nProof of Corollary 1.\nSuppose for sufficiently large N , P (A \u2282 A\u0302(N),r) \u2265 1\u2212 e\u2212\u03c1n, where r denote the r-th elimination. Let A\u0302(N)A denote the remaining components when they are already restricted to a setA. Because |{A}| is finite, there exists N1 \u2208 N and \u03c1\u2032 > 0 such that for N \u2265 N1, infA\u2282A P (A \u2282 A\u0302(N)A ) \u2265 1\u2212 e\u2212\u03c1\n\u2032n. When N \u2265 N1 and is sufficiently large,\nP (A \u2282 A\u0302(N),r+1) \u2265 P (A \u2282 A\u0302(N),r)P (A \u2282 A\u0302(N),r+1|A \u2282 A\u0302(N),r)\n\u2265 P (A \u2282 A\u0302(N),r) inf A\u2282A P (A \u2282 A\u0302(N)A ) > 1\u2212 e \u2212\u03c1N \u2212 e\u03c1 \u2032N . (29)\nTherefore for N large enough, P (A \u2282 A\u0302(N),r+1) \u2265 1 \u2212 e\u2212\u03c1\u2032\u2032N , \u03c1\u2032\u2032 being any positive number smaller than min{\u03c1, \u03c1\u2032}. By induction from each r to r + 1, part (b) holds. Part (a) holds following the same reasoning as in the proof of Proposition 3(a).\nProof of Proposition 4.\nWe consider a certain mask m for a Dropout/DropConnect RBM. For a Dropout RBM, the energy function can be written as\nE(v, h,m) = \u2212bT (mv \u2217 v)\u2212 cT (mh \u2217 h)\u2212 (mh \u2217 h)TW ?(mv \u2217 v). (30)\nNote that we also drop visible nodes in this scenario in order to keep symmetry, so m = (mv,mh). For a DropConnect RBM,\nE(v, h,m) = \u2212bT v \u2212 cTh\u2212 hT (m \u2217W )v. (31)\nSuppose we add a hidden node h? to the RBM, and denote the mask applied to h? as m?. Then we have for a Dropout RBM\nE(v, h,m, h?,m?) (32) = \u2212bT (mv \u2217 v)\u2212 cT (mh \u2217 h)\u2212 c?m?h? \u2212 (mh \u2217 h)TW (mv \u2217 v)\u2212 (m?h?) \u00b7W ?(mv \u2217 v),\nand for a DropConnect RBM,\nE(v, h,m, h?,m?) = \u2212bT v \u2212 cTh\u2212 c?h? \u2212 hT (m \u2217W )v \u2212 h? \u00b7 (m? \u2217W ?)v. (33)\nInitializing with c? = 0, W ? = 0, we have for both models\nE(v, h,m, h?,m?) = E(v, h,m). (34)\nThe initial log-likelihood after adding the hidden node is\nl(N)new(v (1), . . . , v(N)|\u03b8\u0302, c\u2217,W \u2217) = N\u2211 n=1 Em,m? logPnew(v (n)|\u03b8\u0302)\n= N\u2211 n=1 Em,m? log\n\u2211 h,h? e\n\u2212E(v(n),h,m,h?,m?)\u2211 h,h?,v e \u2212E(v,h,m,h?,m?)\n= N\u2211 n=1 Em,m? log\n\u2211 h,h? e\n\u2212E(v(n),h,m)\u2211 h,h?,v e \u2212E(v,h,m)\n= N\u2211 n=1 Em log 2 \u2211 h e \u2212E(v(n),h,m) 2 \u2211 h,v e \u2212E(v,h,m) (h ? \u2208 {0, 1})\n= l(N)(v(1), . . . , v(N)|\u03b8\u0302). (35)\nWith the CD-k algorithm, the likelihood does not decrease after adding a new hidden node. Specifically, l(N)new(v(1), . . . , v(N)|\u03b8\u0302?) \u2265 l(N)new(v(1), . . . , v(N)|\u03b8\u0302, c\u2217,W \u2217) = l(N)(v(1), . . . , v(N)|\u03b8\u0302), if \u03b8\u0302\u2217 is the optimal value after adding a new node.\nFor adding layers under Dropout and DropConnect, following [1, 3], to improve the likelihood, the only condition needed is that each layer added is symmetric to the current deepest layer. Apart from the symmetry of the weights, a fixed retaining proportion p guarantees its own symmetry. Assume we currently have L layers. Then\nPDBNL(v (n)) = \u2211 h1,...,hL P (v(n)|h1)P (h1|h2) \u00b7 \u00b7 \u00b7P (hL\u22121|hL)PRBM(hL\u22121,hL)(hL)\n= \u2211\nh1,...,hL\nP (v(n)|h1)P (h1|h2) \u00b7 \u00b7 \u00b7P (hL\u22121|hL)PRBM(hL,hL+1)(hL)\n= PDBNL+1(v (n)), (36)\nso after conducting the CD-k algorithm, PRBM(hL,hL+1)(hL) is expected to increase, conditioning on the previous layers. Therefore the likelihood is improved. It is also immediate that adding layers with size J \u2264 H1 \u2264 H2 \u2264 \u00b7 \u00b7 \u00b7 continually improves the likelihood, since we add HL \u2212 HL\u22122 hidden nodes to the L-th hidden layer after we add the L-th hidden layer (L \u2265 2, H0 = J)."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Technical Report, https://www.iro.umontreal", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "On the asymptotics of constrained M-estimation", "author": ["C. Geyer"], "venue": "The Annals of Statistics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Teh", "Y.-W"], "venue": "Neural Computation", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Asymptotics for Lasso-type estimators. The Annals of Statistics 28(5):1356-1378", "author": ["K. Knight", "W. Fu"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "On Bahadur efficiency and maximum likelihood estimation", "author": ["X. Shen"], "venue": "Statistica Sinica 11(2):479-498", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association 101: 1418-1429", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "The most basic one is the restricted Boltzmann machine (RBM) [5, 13], a Markov random field model with a bipartite structure containing a layer of hidden nodes and a layer of visible nodes.", "startOffset": 61, "endOffset": 68}, {"referenceID": 4, "context": "These models can be calibrated with a combination of the stochastic gradient descent and the contrastive divergence (CD) algorithm [5, 13] (or the PCD algorithm [21]).", "startOffset": 131, "endOffset": 138}, {"referenceID": 3, "context": "For regularizing unsupervised neural networks, sparse-RBM-type models [4, 10] encourage a smaller proportion of 1-valued hidden nodes.", "startOffset": 70, "endOffset": 77}, {"referenceID": 1, "context": "Therefore, Dropout can be viewed approximately as the adaptive L regularization [2, 22, 24].", "startOffset": 80, "endOffset": 91}, {"referenceID": 1, "context": "A recursive approximation of Dropout is provided in [2] using normalized weighted geometric means to show its averaging properties.", "startOffset": 52, "endOffset": 55}, {"referenceID": 0, "context": "For instance, Standout [1] varies Dropout probabilities for different nodes which constitute a binary belief network.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "The right-hand side of (8) is approximated by contrastive divergence with k steps of Gibbs sampling (CD-k) [5, 13].", "startOffset": 107, "endOffset": 114}, {"referenceID": 2, "context": "It is shown in [3, 8] that adding layers to a DBN improves the likelihood given symmetry of the weights of two adjacent layers.", "startOffset": 15, "endOffset": 21}, {"referenceID": 5, "context": "9 for DO/DC/BE/IBE; \u03bb = 10\u22125 \u223c 10\u22124 for L, similar to [7]; \u03bc = 0.", "startOffset": 54, "endOffset": 57}], "year": 2017, "abstractText": "Unsupervised neural networks, such as restricted Boltzmann machines (RBMs) and deep belief networks (DBNs), are powerful tools for feature selection and pattern recognition tasks. We demonstrate that overfitting occurs in such models just as in deep feedforward neural networks, and discuss possible regularization methods to reduce overfitting. We consider weight decay, model averaging methods, and backward elimination, and propose revised model averaging methods to improve their efficiency. We also discuss the asymptotic convergence properties of these methods. Finally, we compare the performance of these methods using likelihood and classification error rates on various pattern recognition data sets.", "creator": "LaTeX with hyperref package"}}}