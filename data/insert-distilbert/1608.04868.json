{"id": "1608.04868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Towards Music Captioning: Generating Music Playlist Descriptions", "abstract": "configuration descriptions are often provided along with recommendations to strategically help users'find discovery. recommending automatically generated music playlists ( e. g. personalised playlists ) completely introduces the problem of generating descriptions. in this paper, furthermore we propose a method for generating music playlist tune descriptions, which likewise is called as music captioning. in the proposed method, audio content analysis and natural language processing are adopted theoretically to utilise the information of each track.", "histories": [["v1", "Wed, 17 Aug 2016 06:24:46 GMT  (62kb,D)", "http://arxiv.org/abs/1608.04868v1", "2 pages, ISMIR 2016 Late-breaking/session extended abstract"], ["v2", "Sun, 15 Jan 2017 05:23:51 GMT  (62kb,D)", "http://arxiv.org/abs/1608.04868v2", "2 pages, ISMIR 2016 Late-breaking/session extended abstract"]], "COMMENTS": "2 pages, ISMIR 2016 Late-breaking/session extended abstract", "reviews": [], "SUBJECTS": "cs.MM cs.AI cs.CL", "authors": ["keunwoo choi", "george fazekas", "brian mcfee", "kyunghyun cho", "mark sandler"], "accepted": false, "id": "1608.04868"}, "pdf": {"name": "1608.04868.pdf", "metadata": {"source": "CRF", "title": "TOWARDS MUSIC CAPTIONING: GENERATING MUSIC PLAYLIST DESCRIPTIONS", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Brian McFee", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "first.last@nyu.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "Motivation: One of the crucial problems in music discovery is to deliver the summary of music without playing it. One common method is to add descriptions of a music item or playlist, e.g. Getting emotional with the undisputed King of Pop 1 , Just the right blend of chilled-out acoustic songs to work, relax, think, and dream to 2 . These examples show that they are more than simple descriptions and even add value to the curated playlist as a product.\nThere have been attempts to automate the generation of these descriptions. In [8], Eck et al. proposed to use social tags to describe each music item. Fields proposed a similar idea for playlist using social tag and topic model [9] using Latent Dirichlet Allocation [1]. Besides text, Bogdanov introduced music avatars, whose outlook - hair style, clothes, and accessories - describes the recommended music [2].\nBackground: \u2022 RNNs: RNNs are neural networks that have a unit with a recurrent connection, whose output is connect to the input of the unit (Figure 1, left). They currently show state-of-the-art performances in tasks that involve sequence modelling. Two types of RNN unit are widely used: Long Short-Term Memory (LSTM) unit [10] and Gated Recurrent Unit (GRU) [3].\n1 Michael Jackson: Love songs and ballads by Apple Music 2 Your Coffee Break by Spotify\nc\u00a9 Keunwoo Choi, Gyo\u0308rgy Fazekas, Mark Sandler, Brian McFee, Kyunghyun Cho. Licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: Keunwoo Choi, Gyo\u0308rgy Fazekas, Mark Sandler, Brian McFee, Kyunghyun Cho. \u201cTowards Music Captioning: Generating Music Playlist Descriptions\u201d, Extended abstracts for the Late-Breaking Demo Session of the 17th International Society for Music Information Retrieval Conference, 2016.\n\u2022 Seq2seq: Sequence-to-sequence (seq2seq) learning indicates training a model whose input and output are sequences (Figure 1, right). Seq2seq models can be used to machine translation, where a phrase in a language is summarised by an encoder RNN, which is followed by a decoder RNN to generate a phrase in another language [4]. \u2022 Word2vec: Word embeddings are distributed vector representations of words that aim to preserve the semantic relationships among words. One successful example is word2vec algorithm, which is usually trained with large corpora in an unsupervised manner [13]. \u2022ConvNets: Convolutional neural networks (ConvNets) have been extensively adopted in nearly every computer vision task and algorithm since the record-breaking performance of AlexNet [12]. ConvNets also show state-ofthe-art results in many music information retrieval tasks including auto-tagging [5]."}, {"heading": "2. PROBLEM DEFINITION", "text": "The problem of music captioning can be defined as generating a description for a set of music items using on their audio content and text data. When the set includes more than one item, it can be also called as music playlist captioning."}, {"heading": "3. THE PROPOSED METHOD", "text": "Both of the approaches use sequence-to-sequence model, as illustrated in Figure 2. In the sequence-to-sequence model, the encoder consists of two-layer RNN with GRU and encodes the track features into a vector, i.e., the encoded vector summarises the information of the input. This vector is also called context vector because it provides context\nar X\niv :1\n60 8.\n04 86\n8v 1\n[ cs\n.M M\n] 1\n7 A\nug 2\n01 6\ninformation to the decoder. The decoder consists of twolayer RNN with GRU and decodes the context vector to a sequence of word or word embeddings. The models are written in Keras and uploaded online 3 [6]."}, {"heading": "3.1 Pre-training approach", "text": "This approach takes advantage of a pre-trained word embedding model 4 and a pre-trained auto-tagger 5 . Therefore, the number of parameters to learn is reduced while leveraging additional data to train word-embedding and auto-tagger. Each data sample consists of a sequence of N track features as input and an output word sequence length of M , which is an album feature.\nInput/Output 6 : A n-th track feature, tn \u2208 R350, represents one track and is created by concatenating the audio feature, tna \u2208 R50, and the word feature, tnw \u2208 R300, i.e. t = [ta;tw]. For computing ta, a convolutional neural network that is trained to predict tags is used to output 50-dim vector for each track [5]. tw is computed by \u2211 k wk/K, where wk refers to the embedding of k-th word in the metadata 7 . The word embedding were trained by word2vec algorithms and Google news dataset [13].\nAn playlist feature is a sequence of word embeddings of the playlist description, i.e. p = [wm]m=0,1,..m\u22121.\n3 http://github.com/keunwoochoi/ ismir2016-ldb-audio-captioning-model-keras\n4 https://radimrehurek.com/gensim/models/ word2vec.html\n5 https://github.com/keunwoochoi/music-auto_ tagging-keras, [5]\n6 The dimensions can vary, we describe in details for better understanding.\n7 Because these word embeddings are distributed representations in a semantic vector space, average of the words can summarise a bag of words and was used as a baseline in sentence and paragraph representation [7]."}, {"heading": "3.2 Fully-training approach", "text": "The model in this approach includes the training of a ConvNet for audio summarisation and an RNN for text summarisation of each track. The structure of ConvNet can be similar to the pre-trained one. The RNN module is trained to summarise the text of each track and outputs a sentence vector. These networks can be provided with additional labels (notated as y in the figure 2) to help the training, e.g., genres or tags. In that case, the objective of the whole structure consists of two different tasks and therefore the training can be more regulated and stable.\nSince the audio and text summarisation modules are trainable, they can be more relevant to the captioning task. However, this flexibility requires more training data."}, {"heading": "4. EXPERIMENTS AND CONCLUSIONS", "text": "We tested the pre-training approach with a private production music dataset. The dataset has 374 albums and 17,354 tracks with descriptions of tracks, albums, audio signal and metadata. The learning rate is controlled by ADAM [11] with an objective function of 1-cosine proximity. The model was trained to predict the album descriptions.\nThe model currently overfits and fails to generate correct sentences. One example of generated word sequence is dramatic motivating the intense epic action adventure soaring soaring soaring gloriously Roger Deakins cinematography Maryse Alberti. This is expected since there are only 374 output sequences in the dataset \u2013 if we use early stopping, the model underfits, otherwise it overfits.\nIn the future, we plan to solve the current problem \u2013 lack of data. The sentence generation can be partly trained by (music) corpora. A word2vec model that is trained with music corpora can be used to reduce the embedding dimension [14]. The model can also be modified in the sense that the audio feature is optional and it mainly relies on metadata. In that case, acquisition of training data becomes more feasible."}, {"heading": "5. ACKNOWLEDGEMENTS", "text": "This work was part funded by the FAST IMPACt EPSRC Grant EP/L019981/1 and the European Commission H2020 research and innovation grant AudioCommons (688382). Mark Sandler acknowledges the support of the Royal Society as a recipient of a Wolfson Research Merit Award. Brian McFee is supported by the Moore Sloan Data Science Environment at NYU. Kyunghyun Cho thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016). The work is done during Keunwoo Choi is visiting Center for Data Science in New York University."}, {"heading": "6. REFERENCES", "text": "[1] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993\u20131022, 2003.\n[2] Dmitry Bogdanov, Mart\u0131\u0301N Haro, Ferdinand Fuhrmann, Anna Xambo\u0301, Emilia Go\u0301mez, and Perfecto Herrera. Semantic audio content-based music recommendation and visualization based on user preference examples. Information Processing & Management, 49(1):13\u201333, 2013.\n[3] Kyunghyun Cho, Bart Van Merrie\u0308nboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\n[4] Kyunghyun Cho, Bart Van Merrie\u0308nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[5] Keunwoo Choi, George Fazekas, and Mark Sandler. Automatic tagging using deep convolutional neural networks. In International Society of Music Information Retrieval Conference. ISMIR, 2016.\n[6] Franc\u0327ois Chollet. Keras. GitHub repository: https://github. com/fchollet/keras, 2015.\n[7] Andrew M Dai, Christopher Olah, and Quoc V Le. Document embedding with paragraph vectors. arXiv preprint arXiv:1507.07998, 2015.\n[8] Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, and Stephen Green. Automatic generation of social tags for music recommendation. In Advances in neural information processing systems, pages 385\u2013392, 2008.\n[9] Ben Fields, Christophe Rhodes, Mark d\u2019Inverno, et al. Using song social tags and topic models to describe and compare playlists. In 1st Workshop On Music Recommendation And Discovery (WOMRAD), ACM RecSys, 2010, Barcelona, Spain, 2010.\n[10] Sepp Hochreiter and Ju\u0308rgen Schmidhuber. Long shortterm memory. Neural computation, 9(8):1735\u20131780, 1997.\n[11] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012.\n[13] T Mikolov and J Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 2013.\n[14] Sergio Oramas, Luies Espinosa-Anke, Shuo Zhang, Horacio Saggion, and Xavier Serra. Natural language processing for music information retrieval. In 17th International Society for Music Information Retrieval Conference (ISMIR 2016), 2016."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Semantic audio content-based music recommendation and visualization based on user preference examples", "author": ["Dmitry Bogdanov", "Mart\u0131\u0301N Haro", "Ferdinand Fuhrmann", "Anna Xamb\u00f3", "Emilia G\u00f3mez", "Perfecto Herrera"], "venue": "Information Processing & Management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler"], "venue": "In International Society of Music Information Retrieval Conference", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Keras. GitHub repository: https://github", "author": ["Fran\u00e7ois Chollet"], "venue": "com/fchollet/keras,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Document embedding with paragraph vectors", "author": ["Andrew M Dai", "Christopher Olah", "Quoc V Le"], "venue": "arXiv preprint arXiv:1507.07998,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Automatic generation of social tags for music recommendation", "author": ["Douglas Eck", "Paul Lamere", "Thierry Bertin-Mahieux", "Stephen Green"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Using song social tags and topic models to describe and compare playlists", "author": ["Ben Fields", "Christophe Rhodes", "Mark d\u2019Inverno"], "venue": "In 1st Workshop On Music Recommendation And Discovery (WOMRAD), ACM RecSys,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Natural language processing for music information retrieval", "author": ["Sergio Oramas", "Luies Espinosa-Anke", "Shuo Zhang", "Horacio Saggion", "Xavier Serra"], "venue": "In 17th International Society for Music Information Retrieval Conference (ISMIR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "In [8], Eck et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Fields proposed a similar idea for playlist using social tag and topic model [9] using Latent Dirichlet Allocation [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "Fields proposed a similar idea for playlist using social tag and topic model [9] using Latent Dirichlet Allocation [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "Besides text, Bogdanov introduced music avatars, whose outlook - hair style, clothes, and accessories - describes the recommended music [2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "Two types of RNN unit are widely used: Long Short-Term Memory (LSTM) unit [10] and Gated Recurrent Unit (GRU) [3].", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "Two types of RNN unit are widely used: Long Short-Term Memory (LSTM) unit [10] and Gated Recurrent Unit (GRU) [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Seq2seq models can be used to machine translation, where a phrase in a language is summarised by an encoder RNN, which is followed by a decoder RNN to generate a phrase in another language [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 12, "context": "One successful example is word2vec algorithm, which is usually trained with large corpora in an unsupervised manner [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "\u2022ConvNets: Convolutional neural networks (ConvNets) have been extensively adopted in nearly every computer vision task and algorithm since the record-breaking performance of AlexNet [12].", "startOffset": 182, "endOffset": 186}, {"referenceID": 4, "context": "ConvNets also show state-ofthe-art results in many music information retrieval tasks including auto-tagging [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "The models are written in Keras and uploaded online 3 [6].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "For computing ta, a convolutional neural network that is trained to predict tags is used to output 50-dim vector for each track [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The word embedding were trained by word2vec algorithms and Google news dataset [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "com/keunwoochoi/music-auto_ tagging-keras, [5] 6 The dimensions can vary, we describe in details for better understanding.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "7 Because these word embeddings are distributed representations in a semantic vector space, average of the words can summarise a bag of words and was used as a baseline in sentence and paragraph representation [7].", "startOffset": 210, "endOffset": 213}, {"referenceID": 10, "context": "The learning rate is controlled by ADAM [11] with an objective function of 1-cosine proximity.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "A word2vec model that is trained with music corpora can be used to reduce the embedding dimension [14].", "startOffset": 98, "endOffset": 102}], "year": 2016, "abstractText": "Descriptions are often provided along with recommendations to help users\u2019 discovery. Recommending automatically generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural language processing are adopted to utilise the information of each track.", "creator": "LaTeX with hyperref package"}}}