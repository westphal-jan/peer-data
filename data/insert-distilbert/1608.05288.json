{"id": "1608.05288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "Accelerating Exact and Approximate Inference for (Distributed) Discrete Optimization with GPUs", "abstract": "discrete optimization is a central problem in artificial intelligence. the computational optimization of the aggregated cost of a network of cost functions arises in a variety of problems including ( w ) csp, dcop, as well as optimization in stochastic variants platforms such as bayesian networks. inference - based pursuit algorithms are powerful techniques for solving several discrete optimization problems, which can be used independently continuously or in systematic combination with other evolutionary techniques. however, their applicability factor is then often only limited by their deep compute processing intensive nature and their space requirements. this paper proposes the design and implementation of a novel inference - flow based technique, which exploits their modern massively parallel architectures, such as those found in graphical computing processing units ( gpus ), to speed up the resolution of exact and approximated inference - based constraint algorithms for discrete optimization. the paper studies the proposed algorithm in both centralized and distributed optimization contexts. the main paper demonstrates improvements that the use of gpus provides significant advantages in terms especially of global runtime and scalability, successfully achieving up compared to two orders of magnitude in speedups and showing a considerable reduction in execution time ( up to 345 times faster ) with respect to a sequential version.", "histories": [["v1", "Thu, 18 Aug 2016 15:14:37 GMT  (2568kb,D)", "http://arxiv.org/abs/1608.05288v1", null], ["v2", "Fri, 16 Jun 2017 02:14:06 GMT  (1010kb,D)", "http://arxiv.org/abs/1608.05288v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["ferdinando fioretto", "enrico pontelli", "william yeoh", "rina dechter"], "accepted": false, "id": "1608.05288"}, "pdf": {"name": "1608.05288.pdf", "metadata": {"source": "CRF", "title": "Accelerating Exact and Approximate Inference for (Distributed) Discrete Optimization with GPUs", "authors": ["Ferdinando Fioretto", "Rina Dechter"], "emails": ["ffiorett@cs.nmsu.edu", "epontell@cs.nmsu.edu", "wyeoh@cs.nmsu.edu", "dechter@ics.uci.edu"], "sections": [{"heading": null, "text": "This paper proposes the design and implementation of a novel inference-based technique, which exploits modern massively parallel architectures, such as those found in Graphical Processing Units (GPUs), to speed up the resolution of exact and approximated inference-based algorithms for discrete optimization. The paper studies the proposed algorithm in both centralized and distributed optimization contexts.\nThe paper demonstrates that the use of GPUs provides significant advantages in terms of runtime and scalability, achieving up to two orders of magnitude in speedups and showing a considerable reduction in execution time (up to 345 times faster) with respect to a sequential version.\nKeywords GPU \u00b7WCSP \u00b7 DCOP \u00b7 (Mini-)Bucket Elimination \u00b7 (A)DPOP\n? This journal article is an extended version of an earlier conference paper [26]. It includes (i) a parallelized design and implementation of Mini-Bucket Elimination with GPUs on WCSPs; (ii) a more detailed description of the GPU operations to ease reproducibility; (iii) a significantly more comprehensive empirical evaluation with additional WCSP benchmarks and different GPU devices.\nFerdinando Fioretto, Enrico Pontelli, William Yeoh Computer Science, New Mexico State University E-mail: {ffiorett,epontell,wyeoh}@cs.nmsu.edu\nRina Dechter School of Information and Computer Science, University of California, Irvine E-mail: dechter@ics.uci.edu\nar X\niv :1\n60 8.\n05 28\n8v 1\n[ cs\n.A I]\n1 8\nA ug\n2 01"}, {"heading": "1 Introduction", "text": "The importance of constraint optimization is outlined by the impact of its application in a wide range of domains, such as supply-chain management (e.g., [49,29]), roster scheduling (e.g., [1,11]), combinatorial auctions (e.g., [52]), bioinformatics (e.g., [2, 13,24]), multi-agent systems (e.g., [21]) and probabilistic reasoning (e.g, [44]).\nIn Constraint Satisfaction Problems (CSPs), the goal is to find a value assignment for a set of variables that satisfies a set of constraints [4,50]. The assignments satisfying the problem constraints are called solutions. In Weighted Constraint Satisfaction Problems (WCSPs) the goal is that of finding an optimal solution, given a set of preferences expressed by means of cost functions [54,53,8]. When resources are distributed among a set of autonomous agents and communication among the agents is restricted, WCSPs take the form of Distributed Constraint Optimization Problems (DCOPs) [41,47,60]. In this context, agents coordinate their value assignments to minimize the overall sum of resulting constraint costs. DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30]. We will refer to WCSPs and DCOPs as discrete optimization problems.\nAlgorithms to solve discrete optimization problems can be classified as exact and approximated. Exact algorithms are guaranteed to find optimal solutions. However, since solving WCSPs and DCOPs is NP-hard [50], optimally solving these problems results in prohibitive runtime and/or use of resources, such as memory or network load. In contrast, approximated algorithms trade solution optimality for shorter runtime and a more efficient use of the available resources.\nFurthermore, discrete optimization algorithms can adopt two main paradigms: search or inference. Search-based methods rely on the use of non-deterministic branching rules to explore different value assignments to variables. These rules are applied recursively until all problem variables are assigned. This process defines a search tree (typically traversed in a depth-first fashion), which has the advantage of requiring only polynomial space. However, the practical efficiency of these methods relies on their ability to prune redundant or sub-optimal subtrees. Inference-based methods are inspired from dynamic programming (DP) techniques. These methods apply a sequence of transformations to reduce the problem size at each step while preserving its semantics. A well known inference-based approach is Bucket Elimination (BE) [15]. BE iterates over the variables of the problem, reducing the size of the problem at each step, by replacing a variable and its related cost functions with a single new function, derived by optimizing over the possible values of the eliminated variable. The Dynamic Programming Optimization Protocol (DPOP) [47] is one of the most efficient inference-based DCOP solvers, and it can be seen as a distributed version of BE, where agents exchange newly introduced cost functions via messages.\nThe importance of inference-based approaches arises in several optimization fields including constraint programming [4,50]. For example, several propagators adopt DP-based techniques to establish constraint consistency. For instance, (1) the knapsack constraint propagator proposed by Trick applies DP techniques to establish arc consistency on the constraint [58]; (2) the propagator for the regular constraint establishes arc consistency using a specific digraph representation of the DFA, which\nhas similarities to dynamic programming [45]; (3) the context free grammar constraint makes use of a propagator based on the CYK parser that uses DP to enforce generalized arc consistency [48].\nThe main drawback of inference-based methods, including BE and DPOP, is that each transformation may introduce cost functions with large arities, requiring exponential time and space in a key structural parameter of a problem, called induced width. While inference-based approaches may not always be appropriate to solve discrete optimization problems, as their time and space requirements may be prohibitive, they may be very effective in problems with particular structures, such as problems where their underlying constraint graphs have small induced widths or distributed problems where the number of messages is crucial for performance, despite the size of the messages. Additionally, approximated inference methods can be effectively used to derive lower bounds, which are important components of branch and bound algorithms, as they can be used to prune parts of the search space by detecting dominated solutions\u2014i.e., solutions whose cost can provably not be lower than the best cost found so far. Mini-Bucket Elimination (MBE) is an approximated variant of BE that can be used for this purpose.\nRecent developments on external-memory algorithms have shown that the use of large secondary data storage can be effective to extend the applicability of memory intensive approaches [22,38,33,56]. However, the computational solving runtime remains a bottleneck.\nTo contrast this background, we note that the structure exploited by inferencebased approaches in constructing solutions makes it suitable to exploit a novel class of massively parallel platforms that are based on the Single Instruction Multiple Thread (SIMT) paradigm\u2014where multiple threads may concurrently operate on different data, but are all executing the same instruction at the same time. The SIMT-based paradigm is widely used in modern Graphical Processing Units (GPUs) for general purpose parallel computing. Several libraries and programming environments (e.g., the Compute Unified Device Architecture (CUDA)) have been made available to allow programmers to exploit the parallel computing power of GPUs.\nIn this paper, we propose the design and implementation of both an exact and an approximated inference-based algorithm that exploits parallel computation using GPUs to solve WCSPs and DCOPs. Our proposal aims at employing GPU hardware to speed up the inference process, thus providing an alternative way to enhance the performance of inference-based discrete optimization approaches.\nThis paper makes the following contributions: (1) We propose a novel design and implementation of a centralized and a distributed exact inference-based algorithm, inspired by BE and DPOP, to optimally solve WCSPs and DCOPs, which harnesses the computational power offered by parallel platforms based on GPUs; (2) We introduce an approximated version of the GPU-based inference-based algorithm, inspired by MBE; (3) We report an extensive empirical analysis that shows significant improvements in performance with respect to the sequential CPU-based algorithms, reporting an average speedup of two order of magnitude; and (4) We show the generality of our approach through empirical evaluations on three different GPU architectures, all providing significant speedups."}, {"heading": "2 Background: Weighted Constraint Satisfaction Problems", "text": "A weighted constraint satisfaction problem (WCSP) [36,54] is a tuple \u3008X,D,C\u3009, where X = {x1, . . . , xn} is a finite set of variables, D = {Dx1 , . . . , Dxn} is a set of finite domains for the variables in X, with Dxi being the set of possible values for the variable xi, and C is a set of weighted constraints (or cost functions). A weighted constraint fi \u2208 C is a function that maps tuples defined on the set of variables relevant to fi into N \u222a {\u221e}, where \u221e is a special value denoting that a given combination of values is not allowed. The set of variables relevant to fi is referred to as the scope of fi, and denoted as xi \u2286 X. Formally, fi : \"xj\u2208xi Dxj \u2192 N \u222a {\u221e}.1 A solution is a value assignment for a subset \u03c1 of variables from X that is consistent with their respective domains; i.e., it is a partial function \u03b8 : X \u2192 \u22c3n i=1Dxi such that, for each xj \u2208 X, if \u03b8(xj) is defined (i.e., xj \u2208 \u03c1), then \u03b8(xj) \u2208 Dxj . The cost of an assignment \u03c1 is the sum of the evaluation of the constraints involving all the variables in \u03c1. A solution is complete if it assigns a value to each variable in X and has finite cost (i.e., different from\u221e). We will use the notation \u03c3 to denote a complete solution, and, for a set of variables V = {xi1 , . . . , xih} \u2286 X, \u03c3V = \u3008\u03c3(xi1), . . . , \u03c3(xih)\u3009 is the projection of \u03c3 to the variables in V, where i1 < \u00b7 \u00b7 \u00b7 < ih. The goal of a WCSP is to find a complete solution \u03c3\u2217 with minimal cost, i.e.,\n\u03c3\u2217=argmin \u03c3\u2208\u03a3 \u2211 fi\u2208C fi(\u03c3xi), (1)\nwhere \u03a3 is the state space, defined as the set of all possible complete solutions. Given a WCSP P , GP =(X, EC) is the constraint graph of P , where {x, y} \u2208 EC iff \u2203fi \u2208 C such that {x, y} \u2286 xi. Given an ordering o on X, we say that a variable xi has lower priority w.r.t. a variable xj , denoted xi \u227ao xj , if xi precedes xj in o.\nDefinition 1 (Induced Graph, Induced Width [16]) Given the constraint graphGP and an ordering o on its nodes, the induced graph G\u2217P on o is the graph obtained by connecting nodes, processed in descending order of priority, to all their preceding neighbors. Given a graph and an ordering of its nodes, the width of a node is the number of edges connecting it to its preceding nodes in the ordering. The induced width w\u2217o of GP is maximum width over all nodes of G \u2217 P along the ordering o.\nExample 1 Fig. 1(a) illustrates the constraint graph of a simple WCSP instance with 4 binary variables, x1, x2, x3, and x4, and 5 constraints f12(x1, x2), f14(x1, x4), f23(x2, x3), f24(x2, x4), f34(x3, x4). Fig. 1(b) illustrates the constraints costs of the WCSP, which associate a cost value for each combination of values for the variables in the scope of the constraints. Fig. 1(c) shows the induced graph G\u2217P obtained along the ordering o = \u3008x1, x2, x3, x4\u3009. Its induced width is 3.\nDefinition 2 (Pseudo-tree [17]) Given a constraint graph GP , a DFS pseudo-tree arrangement for GP is a spanning tree T = \u3008X, ET \u3009 of GP such that if fi \u2208C and {x, y} \u2286 xi, then x and y appear in the same branch of T . Edges of GP that are\n1 For simplicity, we assume that tuples of variables are built according to a predefined ordering.\nin (resp. out of) ET are called tree edges (resp. backedges). The tree edges connect parent-child nodes, while backedges connect a node with its pseudo-parents and its pseudo-children.\nExample 2 Fig. 1(c) shows one possible pseudo-tree T = \u3008X, ET \u3009 associated to the constraint graph shown in Fig. 1(a), with ET = {f12, f23, f23}. The nodes labeled x1 and x2 have one pseudo-child node: x4. The solid lines describe tree edges, while the dotted lines represent backedges.\nDefinition 3 (Projection) The projection of a cost function fi on a set of variables V \u2286 xi is a new cost function fi|V : V \u2192 N \u222a {\u221e}, such that for each possible assignment \u03b8 \u2208 \"xj\u2208VDxj , fi|V(\u03b8) = min\n\u03c3\u2208\u03a3,\u03c3V=\u03b8 fi(\u03c3xi).2\nIn other words, fi|V is constructed from the tuples of fi, removing the values of the variable that do not appear in V and removing duplicate values by keeping the minimum cost of the original tuples in fi.\nDefinition 4 (Concatenation) Let us consider two assignments \u03b8\u2032, defined for variables V , and \u03b8\u2032\u2032, defined for variables W , such that for each x \u2208 V \u2229W we have that \u03b8\u2032(x) = \u03b8\u2032\u2032(x). Their concatenation is an assignment \u03b8\u2032 \u00b7 \u03b8\u2032\u2032 defined for V \u222aW , such as for each x \u2208 V (resp. x \u2208 W ) we have that \u03b8\u2032 \u00b7 \u03b8\u2032\u2032(x) = \u03b8\u2032(x) (resp. \u03b8\u2032 \u00b7 \u03b8\u2032\u2032(x) = \u03b8\u2032\u2032(x)).\nWe define two operations on cost functions: \u2022 The aggregation of two functions fi and fj , is a function fi + fj : xi \u222a xj \u2192\nN\u222a{\u221e}, such that \u2200\u03b8\u2032 \u2208 \"xk\u2208xi Dxk and \u2200\u03b8\u2032\u2032 \u2208 \"xk\u2208xj Dxk , if \u03b8\u2032 \u00b7 \u03b8\u2032\u2032 is defined, then we have that\n(fi + fj)(\u03b8 \u2032 \u00b7 \u03b8\u2032\u2032)=fi(\u03b8\u2032) + fj(\u03b8\u2032\u2032).\n2 For simplicity, we also use \u03b8 to represent the tuple \u3008\u03b8(xi1 ), . . . , \u03b8(xih )\u3009 where {xi1 , . . . , xih} is the domain of \u03b8.\n\u2022 The elimination of a variable xj \u2208 xi from a function fi, denoted as \u03c0\u2212xj (fi), produces a new function with scope xi \\ {xj}, and defined as the projection of fi on xi \\ {xj}, i.e.,\n\u03c0\u2212xj (fi)=fi|xir{xj}.\nAlgorithm 1: BUCKET ELIMINATION /* Variable Elimination Phase */ 1 for i\u2190 n downto 1 do 2 Bi \u2190 {fj \u2208 C | xi \u2208 xj \u2227 i = min{k | xk \u2208 xj}} 3 f\u0302i \u2190 \u03c0\u2212xi (\u2211 fj\u2208Bi fj\n) 4 X\u2190 X \\ {xi} 5 C\u2190 (C \\Bi) \u222a {f\u0302i} /* Value Assignment Phase */ 6 for i\u2190 1 to n do 7 xi \u2190 di s.t. di \u2208 Dxi and di is the best extension of x1, . . . , xi\u22121 w.r.t. Bi 8 return f\u03021\n2.1 Bucket Elimination\nBucket Elimination (BE) [15,16] is a complete inference algorithm that can be used to find all optimal solutions of a WCSP. Algorithm 1 illustrates its pseudocode. BE operates in the following two phases: \u2022 Variable Elimination Phase. BE operates from the highest to lowest priority vari-\nable. When operating on variable xi, it creates a bucket Bi, which is the set of all cost functions that involve xi as the highest priority variable in their scope (line 2). The algorithm then computes a new cost function f\u0302i by aggregating the functions in Bi and eliminating xi (line 3). Thus, xi can be removed from the set of variables X to be processed (line 4) and the new function f\u0302i replaces in C all the cost functions that appear in Bi (line 5). In this work, we refer to the f\u0302i functions as the bucket functions. \u2022 Value Assignment Phase. Once the variable with the lowest priority has been processed, the algorithm considers variables in increasing order of priority. For each variable xi, it generates an optimal assignment by selecting a value di \u2208 Dxi that minimizes the cost of the functions in Bi given the assignments of all the other variables appearing in the scope of the functions in Bi. As a byproduct, and without additional overhead, BE can compute the number of optimal solutions of the problem (see [15], for details). The time and space complexity of BE is exponential on the induced width of the underlying constraint graph, which captures the maximum arity of the f\u0302i functions (line 3).\nExample 3 In our WCSP example of Fig. 1, during the Variable Elimination Phase, BE operates, in order, on the variables x4, x3, x2, and x1. When x4 is processed,\nthe bucket B4 = {f14, f24, f34} is generated, and highlighted in Fig. 2(a)(top) by red edges. The resulting bucket function f\u03024 is shown in Fig. 2(a)(bottom), where the rightmost column shows the values for x4 after its elimination. BE, hence, updates the sets X= {x1, x2, x3} and C= {f12, f23, f\u03024}, as shown in the constraint graph of Fig. 2(b)(top), where the function f\u03024 is displayed as a dotted line. When x3 is processed, B3={f23, f\u03024}, and f\u03023 is shown in Fig. 2(b)(bottom). Thus, X={x1, x2} and C = {f12, f\u03023}, as shown in Fig. 2(c)(top). Next, x2 is processed, and B2 = {f12, f\u03023}, and f\u03022 is illustrated in Fig. 2(c)(bottom). Thus, X= {x1} and C= {f\u03022}, as shown in Fig. 2(d)(top). Lastly, the algorithm processes x1, sets B1={f\u03022}, and f\u03021 is minimized when x1=0, as shown in Fig. 2(d)(bottom). Next, BE starts the Value Assignment Phase, which operates, in order, on the variables x1, x2, x3, and x4. First, it selects the value that minimizes f\u03021, (x1=0). Thus, it processes x2, and selects the value x2 = 1, as it minimizes f\u03022 when x1 = 0, as illustrated in Fig. 2(c)(bottom). Similarly, when BE processes x3, it selects the value x3=0, as it minimizes f\u03023 when x1 = 0 and x2 = 1, illustrated in Fig. 2(b)(bottom). Finally, BE processes the last variable x4 and assigns it the value 1, since it minimizes f\u03024 when x1=0, x2=1, and x3 =0, illustrated in Fig. 2(a)(bottom). Thus, \u03c3\u2217= \u30080, 1, 0, 1\u3009 is an optimal solution to the problem, with cost 4.\n2.2 Mini-Buckets\nThe memory complexity and time complexity of BE depend on the arity of the functions f\u0302 produced during the variable elimination step. Such requirements can quickly become infeasible for problems with large induced widths. To overcome this limitation, Dechter and Rish proposed an incomplete version of the Bucket Elimination [19]. The Mini-Bucket Elimination (MBE) is an approximated version of the\nAlgorithm 2: MINI-BUCKET ELIMINATION(z) /* Variable Elimination Phase */\n9 for i\u2190 n downto 1 do 10 Bi \u2190 {fj \u2208 C | xi \u2208 xj \u2227 i = min{k | xk \u2208 xj}} 11 Let {Bi1 , . . . , Bim} be a partition of Bi s.t. \u2223\u2223\u2223\u22c3fj\u2208Bik xj\u2223\u2223\u2223 \u2264 z, for each k = 1, . . . ,m 12 foreach k \u2208 {1, . . . ,m} do 13 f\u0302ik \u2190 \u03c0\u2212xi (\u2211 fj\u2208Bik fj\n) 14 C\u2190 (C \\Bik ) \u222a {f\u0302ik} 15 X\u2190 X \\ {xi}\n/* Value Assignment Phase */ 16 for i\u2190 1 to n do 17 xi \u2190 di s.t. di \u2208 Dxi and di is the best extension of x1, . . . , xi\u22121 w.r.t. Bi 18 return f\u03021\nBE that allows one to bound the arity of the functions f\u0302i generated during the Variable Elimination Phase. Its pseudocode is illustrated in Algorithm 2. Similarly to BE, MBE operates in the following two phases: \u2022 Variable Elimination Phase. As in BE, during the variable elimination phase,\nMBE operates on the problem variables in decreasing order of priority. However, rather than creating a single bucket function f\u0302i whose scope is the union of the scope of each function in the bucket Bi, it partitions Bi in a set of m \u201cmini\u201dbuckets {Bi1 , . . . , Bim}, such that the size of the scope of the bucket function f\u0302ik , obtained by aggregating the functions inBik , is bounded by a parameter z, for each k \u2208 {1, . . . ,m} (line 11). Thus, MBE considers each mini-bucket independently, and computesm new bucket functions f\u0302ik , by aggregating the functions inBik and eliminating xi (line 13). These functions replace in C all the functions that appear in Bik (line 14), and the set of variables is updated as in BE (line 15).\n\u2022 Value Assignment Phase. This phase is analogous to that of BE (lines 16-17). Consider the elimination step for a variable xi \u2208 X. Since:\nm\u2211 k=1 \u03c0\u2212xi( \u2211 fj\u2208Bik fj ) \u2264 \u03c0\u2212xi( \u2211 fj\u2208Bi fj ) eliminating xi using mini-buckets produces a lower bound on the optimal cost for the bucket Bi. Thus, MBE produces a lower bound on the optimal solution cost. Running the Value Assignment Phase might hence return a sub-optimal solution, whose evaluation will be an upper bound on the optimal solution cost.\nThe time and space complexity of MBE is exponential on the maximal arity of the aggregated functions in the mini-buckets (line 13), and thus it is bounded by the parameter z.\nExample 4 Consider the WCSP of Fig. 1 solved with MBE using z = 1. As in BE, during the Variable Elimination Phase MBE operates, in order, on the variables x4, x3, x2, and x1. When x4 is processed, the bucket B4 = {f14, f24, f34}\u2014\nillustrated by the red edges in Fig. 2(a) top\u2014would result in aggregated bucket function whose arity is 3, and thus exceeds the maximal arity allowed. Thus, MBE creates a partition {B41 , B42 , B43} for B4, whose sets consists of the functions, respectively, f14, f24, and f34. The resulting functions f\u030241 , f\u030242 , and f\u030243 have arity 1, as illustrated in Fig. 2(a) bottom. Then, MBE updates the sets X to {x1, x2, x3} and C to {f12, f23, f\u030241 , f\u030242 , f\u030243}, as shown in the constraint graph of Fig. 2(b) top. When x3 is processed, B3 = {f23, f\u030243}, marked red in Fig. 2(b) top, and the mini-bucket B31 = B3. The resulting bucket function f\u030231 is shown in Fig. 2(b) bottom. Thus, X = {x1, x2} and C = {f12, f\u030241 , f\u030242 f\u030231}. Next, x2 is processed; B2=B21 ={f12, f\u030242 , f\u030231}, and f\u030221 is illustrated in Fig. 2(c) bottom. Thus, X={x1} and C= {f\u030241 , f\u030221}. Lastly, the algorithm processes x1, sets B1 =B11 = {f\u030241 , f\u030221}, and f\u030211 is minimized when x1=1, as shown in Fig. 2(d) bottom. The Value Assignment Phase is analogous to the process carried by BE, except that when processing variable x4 MBE assigns it the value 1, since it minimizes f\u030241 + f\u030242 + f\u030243 when x1 =0, x2 =1, and x3 =0 (Fig. 2(a) bottom). Thus, \u03c3\u2217 = \u30080, 1, 0, 1\u3009 is the reported solution to the problem, with a lower bound cost of 2."}, {"heading": "3 Background: Distributed Constraint Optimization Problems (DCOPs)", "text": "In a Distributed Constraint Optimization Problem (DCOP) [41,47,60], the variables, domains, and cost functions of a WCSP are distributed among a collection of agents. A DCOP is defined as \u3008X,D,C,A, \u03b1\u3009, where X,D, and C are defined as in a WCSP, A = {a1, . . . , ap} is a set of agents, and \u03b1 : X \u2192 A maps each variable to one agent. Following common conventions, we assume that \u03b1 is a bijection:\nEach agent controls exactly one variable. Thus, we will use the terms \u201cvariable\u201d and \u201cagent\u201d interchangeably and assume that \u03b1(xi)=ai. In DCOPs, solutions are defined as for WCSPs, and many solution approaches emulate those proposed in the WCSPs literature. For example, ADOPT [41] is a distributed version of Iterative Deepening Depth First Search, and DPOP [47] is a distributed version of BE. The main difference is in the way the information is shared among agents. Typically, a DCOP agent knows exclusively its domain and the functions involving its variable. It can communicate exclusively with its neighbors (i.e., agents directly connected to it in the constraint graph3), and the exchange of information takes the form of messages. Given a DCOP P , and a DFS pseudo-tree T for the constraint graph GP , we use N(ai)= {aj \u2208A | {xi, xj}\u2208EC} to denote the neighbors of agent ai; and sep(ai) to denote the separator of agent ai, which is the set of ancestor agents that are constrained (i.e., they are linked inGP ) with agent ai or with one of its descendant agents in the pseudo-tree T .\nExample 5 Fig. 1(a\u2013b) illustrate an example of a DCOP instance with 4 agents, ai (i \u2208 {1 . . . , 4}), each controlling one variable, xi. The problem variables, domains and constraints are analogous to those of the WCSP of Example 1. Fig. 1(c) shows one possible pseudo-tree for the DCOP instance, where the agents a1 and a2 have one pseudo-child: a4. The dotted lines represent backedges.\n3.1 Dynamic Programming Optimization Protocol (DPOP)\nDPOP [47] is a dynamic programming based DCOP algorithm that is composed of three phases: \u2022 Pseudo-tree Generation Phase. In this phase the agents coordinate to construc-\ntion of a pseudo-tree, realized through existing distributed pseudo-tree construction algorithms [31]. \u2022 UTIL Propagation Phase. Each agent, starting from the leaves of the pseudo-tree, computes the optimal sum of costs in its subtree for each value combination of variables in its separator set. The agent does so by aggregating the costs of its functions with the variables in its separator and the costs in the UTIL messages received from its child agents, and then eliminating its own variable. \u2022 VALUE Propagation Phase: Each agent, starting from the root of the pseudo-tree, determines the optimal value for its variable. The root agent does so by choosing the value of its variable from its UTIL computations\u2014selecting the value with the minimal cost. It sends the selected value to its children in a VALUE message. Each agent, upon receiving a VALUE message, determines the value for its variable that results in the minimum cost given the variable assignments (of the agents in its separator) indicated in the VALUE message. Once determined, such assignment is further propagated to the children via VALUE messages.\nExample 6 In our example problem, after coordinating to construct the pseudo-tree (Fig. 1(c)), agent a4, being the leaf of the pseudo-tree, starts the UTIL propagation\n3 The constraint graph of a DCOP is equivalent to that of the corresponding WCSP.\nphase, by computing the optimal cost for each value combination of variables x1, x2, and x3 (Fig. 2(a)(bottom)), and sending the costs to its parent agent a3 in a UTIL message. Upon receiving the UTIL messages from each of its children, agents a3 and a2 follow an analogous process. When the root agent a1 receives the UTIL message from each of its children, it computes the minimum cost of the entire problem, and starts the VALUE propagation phase. It selects the value for its variable that minimizes the problem cost (Fig. 2(d)(bottom)) and sends this value down to the pseudo-tree to its child, a3, in a VALUE message. Upon receiving a VALUE message from its parent, each agents follows the same process.\nThe complexity of DPOP is dominated by the UTIL Propagation Phase, which is exponential in the size of the largest separator set sep(ai) for all ai \u2208A. The other two phases require a polynomial number of linear sized messages (in the number of variables of the problem), and the complexity of the local operations is at most linear in the size of the domain.\nObserve that the UTIL Propagation Phase of DPOP emulates the Variable Elimination Phase of BE in a distributed context [10]. Given a pseudo-tree and its ordering o, the UTIL message generated by each DPOP agent ai is equivalent to the aggregated and projected function f\u0302i in BE when xi is processed according to the ordering o.\n3.2 Approximate Distributed Pseudotree Optimization\nAnalogously to how DPOP emulates BE in the distributed context, the Approximate Distributed Pseudotree Optimization (ADPOP) algorithm emulates MBE to solve DCOPs [46]. ADPOP has the same three phases as DPOP, and given a pseudo-tree and its ordering o, the content of the UTIL messages generated by each ADPOP agent ai is equivalent to the bucket functions f\u0302ij (j \u2208 {1, . . . , im}) in MBE when xi is processed according to the ordering o.\nThe complexity of ADPOP is exponential in the input parameter z, while its VALUE Propagation Phase has the same order complexity of the VALUE Propagation Phase in DPOP."}, {"heading": "4 Background: Graphical Processing Units (GPUs)", "text": "Modern Graphics Processing Units (GPUs) are massive parallel architectures, offering thousands of computing cores and a rich memory hierarchy to support graphical processing (e.g., DirectX and OpenGL APIs). NVIDIA\u2019s Compute Unified Device Architecture (CUDA) [51] aims at enabling the use of the multiple cores of a graphic card to accelerate general purpose (non-graphical) applications by providing programming models and APIs that enable the full programmability of the GPU. The computational model supported by CUDA is Single-Instruction Multiple-Threads (SIMT), where multiple threads perform the same operation on multiple data points simultaneously.\nA GPU is constituted by a series of Streaming MultiProcessors (SMs), whose number depends on the specific characteristics of each class of GPU. For example, the\nHOST GLOBAL MEMORY\nCONSTANT MEMORY\nShared memory\nThread Thread\nregs regs\nBlock\nShared memory\nThread Thread\nregs regs\nBlock GRID S M S M S M S M S M S M S M S M\nL2 Cache\nS M S M S M S M S M S M S M S M\nDR AM\nDR AM\nHo st\nIn te\nrfa ce\nDR AM\nDR AM DR AM\nDR AM\nIn st\nru ct\nio n\nCa ch\ne W AR P sc he du\nle r\nW AR P sc he du\nle r\nRe gi\nst er\ns (3\n2K )\ncore core core core core core core core core core core core core core core core\ncore core core core core core core core core core core core core core core core\nSh ar\ned M\nem or y L1 C ac he (6 4K B) SFU SFU SFU SFU HOST\nGLOBAL MEMORY\nCONSTANT MEMORY\nShared memory\nThread Thread\nregs regs\nBlock\nShared memory\nThread Thread\nregs regs\nBlock GRID S M S M S M S M S M S M S M S M\nL2 Cache\nS M S M S M S M S M S M S M S M\nDR AM\nDR AM\nHo st\nIn te\nrfa ce\nDR AM\nDR AM DR AM\nDR AM\nIn st\nru ct\nio n\nCa ch\ne W AR P sc he du\nle r\nW AR P sc he du\nle r\nRe gi\nst er\ns (3\n2K )\ncore core core core core core core core core core core core core core core core\ncore core core core core core core core core core core core core core core core\nSh ar\ned M\nem or y L1 C ac he (6 4K B) SFU SFU SFU SFU\nFig. 4: Fermi Hardware Architecture (left) and CUDA Logical Architecture (right)\nFermi architecture provides 16 SMs, as illustrated in Fig. 4(left). Each SM contains a number of computing cores, each of which incorporate an ALU and a floating-point processing unit. Fig. 4(right) shows a typical CUDA logical architecture. A CUDA program is a C/C++ program that includes parts meant for execution on the CPU (referred to as the host) and parts meant for parallel execution on the GPU (referred as the device). A parallel computation is described by a collection of GPU kernels, where each kernel is a function to be executed by several threads. When mapping a kernel to a specific GPU, CUDA schedules groups of threads (blocks) on the SMs. In turn, each SM partitions the threads within a block in warps4 for execution, which represents the smallest work unit on the device. Each thread instantiated by a kernel can be identified by a unique, sequential, identifier (Tid), which allows to differentiate both the data read by each thread and code to be executed.\n4.1 Memory Organization\nGPU and CPU are, in general, separate hardware units with physically distinct memory types connected by a system bus. Thus, in order for the device to execute some computation invoked by the host and to return the results back to the caller, a data flow needs to be enforced from the host memory to the device memory and vice versa. The device memory architecture is quite different from that of the host, in that it is organized in several levels differing to each other for both physical and logical characteristics.\nEach thread can utilize a small number of registers,5 which have thread lifetime and visibility. Threads in a block can communicate by reading and writing a common area of memory, called shared memory. The total amount of shared memory per block is typically 48KB. Communication between blocks and communication between the blocks and the host is realized through a large global memory. The data stored in the global memory has global visibility and lifetime. Thus, it is visible to all\n4 A warp is typically composed of 32 threads. 5 In modern devices, each SM allots 64KB for registers space.\nthreads within the application (including the host), and lasts for the duration of the host allocation.\nApart from lifetime and visibility, different memory types have also different dimensions, bandwidths, and access times. Registers have the fastest access memory, typically consuming zero clock cycles per instruction, while the global memory is the slowest but largest memory accessible by the device, with access times ranging from 300 to 600 clock cycles. The shared memory is partitioned into 32 logical banks, each serving exactly one request per cycle. Shared memory has an extremely small access latency, provided that multiple thread memory accesses are mapped to different memory banks.\n4.2 Bottlenecks and Common Optimization Practices\nWhile it is relatively simple to develop correct GPU programs (e.g., by incrementally modifying an existing sequential program), it is nevertheless challenging to design an efficient solution. Several factors are critical in gaining performance. In this section, we discuss a few common practice that are important for the design of efficient CUDA programs.\nMemory bandwidth is widely considered to be an important bottleneck for the performance of GPU applications. Accessing global memory is relatively slow compared to accessing shared memory in a CUDA kernel. However, even if not cached, global accesses covering a contiguous 128 Bytes data are fetched at once. Thus, most of the global memory access latency can be hidden if the GPU kernel employs a coalesced memory access pattern. Fig. 5(left) illustrates an example of coalesced memory access pattern, in which aligned threads in a warp accesses aligned entries in a memory segment, which results in a single transaction. Thus, coalesced memory accesses allow the device to reduce the number of fetches to global memory for every thread in a warp. In contrast, when threads adopt a scattered data accesses (Fig. 5(right)), the device serializes the memory transaction, drastically increasing its access latency.\nData transfers between the host and device memory is performed through a system bus, which translates to slow transactions. Thus, in general, it is convenient to store the data onto the device memory. Additionally, batching small memory transfers into a large one will reduce most of the per-transfer processing overhead [51].\nThe organization of the data in data structures and data access patterns play a fundamental role in the efficiency of the GPU computations. Due to the computational model employed by the GPU, it is important that each thread in a warp executes the same branch of execution. When this condition is not satisfied (e.g., two threads execute different branches of a conditional construct), the degree of concurrency typically decreases, as the execution of threads performing separate control flows can be serialized. This is referred to as branch divergence, a phenomenon that has been intensely analyzed within the High Performance Computing (HPC) community [32, 14,20]."}, {"heading": "5 GPU-based (Distributed) Bucket Elimination (GPU-(D)BE)", "text": "Our GPU-based (Distributed) Bucket Elimination (GpuBE) framework, extends BE and MBE (DPOP and ADPOP, respectively) by exploiting GPU parallelism within the aggregation and elimination operations. These operations are responsible for the creation of the functions f\u0302i in BE and f\u0302ik in MBE (lines 3 and 13 of Algorithms 1 and 2, respectively) and the UTIL tables in DPOP and ADPOP (UTIL Propagation Phase), and they dominate the complexity of the algorithms. Thus, we focus on the details of the design and the implementation relevant to such operations. The key observation that allows us to parallelize these operations is that the computation of the cost for each value combination in a bucket function is independent of the computation in the other combinations. The use of a GPU architecture allows us to exploit such independence, by concurrently exploring several value combinations of the bucket function, computed by the aggregation operator, as well as concurrently eliminating out variables.\nDue to the equivalence of BE (resp. MBE) and DPOP (resp. ADPOP), we will refer to the bucket functions f\u0302 and UTIL tables resulted by the aggregation and elimination operations of Algorithms 1 and 2, as well as variables and agents, interchangeably.\n5.1 GPU Data Structures\nIn order to fully capitalize on the parallel computational power of GPUs, the data structures need to be designed in such a way to limit the amount of information exchanged between the CPU host and the GPU device, minimizing the accesses to the (slow) device global memory, while ensuring that the data access pattern enforced is coalesced. To do so, we store into the device global memory exclusively the minimal information required to compute the bucket functions, which are communicated to the GPU once at the beginning of the computation of each bucket or mini-bucket. This allows the GPU kernels to communicate with the CPU host exclusively to exchange the results of the aggregation and elimination processes.\nWe introduce the following concept:\nDefinition 5 (Bucket-table) A bucket-table is a 4-tuple, T = \u3008S,R, \u03c7,\u227a\u3009, where: \u2022 S \u2286 X, is a list of variables denoting the scope of T .\n\u2022 R is a list of tuples of values, each tuple having length |S|. Each element in this list (called row of T ) specifies an assignment of values for the variables in S that is consistent with their domains. We denote with R[i] the tuple of values corresponding to the i-th row in R, for i = {1, . . . , |R|}. \u2022 \u03c7 is a list of length |R| of cost values corresponding to the costs of the assignments\nin R. In particular, the element \u03c7[i] represents the cost of the assignment R[i] for the variables in S, with i = {1, . . . , |R|}. \u2022 \u227a denotes an ordering relation used to sort the variables in the list S. In turn, the\nvalue assignments, and cost values, in each row of R and \u03c7, respectively, obey to the same ordering.\nAs a technical note, a bucket table T is mapped onto the GPU device to store exclusively the cost values \u03c7, not the associated variables values. We assume that the rows of R are sorted in lexicographic order\u2014thus, the i-th entry \u03c7[i] is associated with the i-th permutation R[i] of the variable values in S, in lexicographic order. This strategy allows us to employ a simple perfect hashing to efficiently associate row numbers with variables\u2019 values. We will elaborate on this topic in Section 5.3. Additionally, all the data stored on the GPU global memory is organized in mono-dimensional arrays, so as to facilitate coalesced memory accesses.\nAlgorithm 3: GPUBE(z) /* Variable Ordering Phase (Pseudo-Tree Construction) */ 19 X\u0304\u2190 Sort X w.r.t. \u227aT ordering /* Variable Elimination Phase */ 20 for i\u2190 n downto 1, with xi \u2208 X\u0304 do 21 Bi \u2190 CPU::CONSTRUCTBUCKET(C, xi, z) 22 Let {Bi1 , . . . , Bim} be a partition of Bi s.t.\n\u2223\u2223\u2223\u22c3fj\u2208Bik xj\u2223\u2223\u2223 \u2264 z, for each k = 1, . . . ,m 23 foreach k \u2208 {1, . . . ,m} do 24 Tik = \u3008Bik ,Rik , \u03c7ik ,\u227aT \u3009\u21d4 GPU::RESERVE(|Rik |) 25 foreach fj \u2208 Bik do 26* Tj = \u3008xfj ,Rj , \u03c7j ,\u227aT \u3009 D\u2190H \u21d4 GPU::RESERVE(|Rj |) 27* Tik \u21d4 GPU::AGGREGATE(Tik , Tj) 28 f\u0302ik H\u2190D \u21d4 GPU::ELIMINATE(Tik , xi)\n/* Variable Assignment Phase */\n29 for i\u2190 1 to n, with xi \u2208 X\u0304 do 30 xi \u2190 CPU::FINDBESTASSIGNMENT(x1, . . . , xi\u22121)\n31 return f\u03021\n5.2 Algorithm Overview\nAlgorithm 3 illustrates the pseudocode of GpuBE, where z is an input parameter denoting the maximal mini-bucket size to be processed. We use the following notations:\n\u2013 Starred line numbers denote those instructions that are executed concurrently by both the CPU and the GPU. \u2013 The symbols\u2190 and \u21d4 denote sequential and parallel (i.e., multiple GPU threads) operations, respectively. \u2013 If a parallel operation requires a copy from host (device) to device (host), we write D\u2190H \u21d4 ( H\u2190D \u21d4 ). Host to device (device to host) memory transfers are performed\nimmediately before (after) the execution of the GPU kernel.\nGpuBE is composed of three phases: (1) Variable Ordering, (2) Variable Elimination, and (3) Variable Assignment. Let us considerN(xi)={xj \u2208X |{xi, xj}\u2208EC}, defined analogously as for the agents\u2019 case. During the first phase (line 19), the problem variables are sorted according to a pseudo-tree ordering relation; in particular, we apply the following heuristics in the construction of the pseudo-tree: xi \u227aT xj iff |N(xi)| < |N(xj)|, for every xi, xj \u2208 X. For the distributed case, this phase is identical to that of (A)DPOP, where the agents coordinate the construction of a pseudo-tree, using an off-the-shelf message-passing algorithm [31].\nIn the second phase, the algorithm processes each variable, in descending order, according to the relation \u227aT , and proceeds as in (M)BE: \u2022 The function CPU::CONSTRUCTBUCKET constructs the bucketBi as illustrated in\nAlgorithm 1, line 2. The algorithm proceeds in creating a partition of this bucket, if required (i.e., if z < w\u2217). This phase differs slightly in the distributed case, where each agent, upon receiving a new bucket function from its descendant agents, inserts it into its bucket set Bi. \u2022 For each mini-bucket Bik (k = 1, . . . ,m), GpuBE determines and reserves the amount of global memory to be assigned to each associated bucket-table Tik (line 24). After the GPU::RESERVE function is invoked, a space sufficient to store the bucket-table is allocated, and its cost values \u03c7ik are initialized to 0. \u2022 Thus, GpuBE aggregates the bucket-table Tj associated to each function fj in the mini-bucket with the bucket-table Tik (lines 25\u201327). To do so, it first creates a bucket-table Tj that encodes the cost values of the bucket function fj , reordering them, if necessary, according to the order on its scope specified by the pseudotree relation \u227aT (line 26). This procedure requires a memory transfer from the CPU host to the GPU device global memory. Then, it adds the values \u03c7j of the aggregating bucket-table Tj into the corresponding entries of the bucket-table Tik (line 27). We will further discuss the details of this function, as well as the other kernel functions, in the next sections. \u2022 Finally, the algorithm invokes a GPU call to eliminate the variable xi from the bucket-table Tik , thereby constructing the bucket function f\u0302ik , which is, finally, copied back to the CPU host memory (line 28).\nIn the distributed case, each agent processes lines 21\u201324 in parallel without prior coordination. Starting from the leaves of the pseudo-tree, the agents build their UTIL messages containing the bucket functions (lines 23\u201328), and send them to their parents. Thus, each agent waits to receive the UTIL messages from all of its children before performing the aggregation and elimination operations (lines 25\u201327 and line 28, respectively) for each mini-bucket. By the end of this phase (line 28), the root agent knows the overall cost for each values of its variable xi. Thus, it chooses the value\nthat results in the minimum cost, and it starts the third phase by sending to each child agent the value of its variable xi.\nIn the centralized case, when space is not a concern, there is no need of copying the bucket tables back to the host, after the variable elimination step (line 28). Thus, two memory transfer transactions are avoided for each variable being processed.\nIn the third phase, the algorithm proceeds analogously to as done in (M)BE. For the distributed case, the agents select the values for their variables that minimize their bucket functions costs, given the assignments of their ancestor agents, and send them in VALUE messages to their children. These operations are repeated by every agent receiving a VALUE message until the leaf agents are reached.\nWhile we described the case in which the underlying problem constraint graph is connected, our implementation allows us to handle disconnected graphs. This is done by solving the sub-problems in each connected subgraph independently from other subproblems, and retrieving the problem cost by aggregating the costs stored in the root of each pseudo-tree associated to the connected graphs.\n5.3 GPU-based Constraint Aggregation\nWe now describe the implementation of the constraint aggregation GPU kernel. This operation, takes in input two bucket-tables: Tik and Tj , and aggregates the cost values in \u03c7j to those of \u03c7ik for all the corresponding assignments of the shared variables in the scope of the two bucket-tables. We refer to Tik and Tj as to the output and input bucket-tables, respectively.\nConsider the example in Fig. 6, the cost values \u03c7j of the input bucket-table Tj (right) are aggregated to the cost values \u03c7ik of the output bucket-table Tik (left)\u2014 which where initialized to 0. The rows of the two tables with identical value assignments for the shared variables x2 and x3 are shaded with the same color.\nTo optimize performance of the GPU operations and to avoid unnecessary data transfer to/from the GPU global memory, we only transfer the list of cost values \u03c7 for each bucket-table that need to be aggregated, and employ a simple perfect hashing function to efficiently associate row numbers with variables\u2019 values. This allow us to\ncompute the indices of the cost vector of the input bucket-table relying exclusively on the information of the thread ID and, thus, avoiding accessing the scope S and assignment vectors R of the input and output bucket-tables.\nWe now discuss how this process can be efficiently handled on the GPU kernels. Let T out = \u3008Sout,Rout, \u03c7out,\u227aout\u3009 be the output bucket-table, whose scope is Sout = {xout1 , . . . , xoutm }. Let T in = \u3008Sin,Rin, \u03c7in,\u227ain\u3009 be the input bucket-table, whose scope is Sin = {xin1 , . . . , xins }, and such that Sin v Sout, where A v B denotes that A is a subsequence of B, and with s \u2264 m. Additionally, let xoutm = xins , i.e., the last variable of the input and output bucket-table scopes coincides. The latter is the variable to be eliminated; We will explain this design choice in the next section, where we will discuss the variable elimination process on a GPU. Finally, let \u03c6out : N \u2192 N be a mapping from input bucket-table scope variables indices to output bucket-table scope variable indices, such that \u03c6out(i) = j iff xini = x out j . For instance, in our example of Fig. 6, \u03c6out(0) = 1, as the variable Sj [0] = Sik [1] = x2. Hence, given a row index rout for the output bucket-table \u03c7out, the corresponding row index rin associated to the input bucket-table cost array \u03c7in is given by:\nrin = s\u22121\u2211 k=1   s\u220f j=k+1 |Dxinj |  \ufe38 \ufe37\ufe37 \ufe38 mul[k] \u00b7  b\nrout m\u220f\nj=\u03c6out(k)+1 |Dxoutj |\ufe38 \ufe37\ufe37 \ufe38 div [k]\nc mod |Dxin k |\ufe38 \ufe37\ufe37 \ufe38\nmod[k]   + rout mod |Dxins |\ufe38 \ufe37\ufe37 \ufe38 mod[s] (2)\nEach term in the summation of Equation (2) represents the contribution of the k-th variable\u2019s value in Rout[rout], as an offset to the index rin in the array Rin.\nThe vectors mul , div , and mod are data structures employed to compute efficiently the rin indices on the GPU. The values mul [k], div [k], and mod [k] (and mod [s]) can be efficiently computed in O(s), O(n), and O(1), respectively, for each k = {1, . . . , s \u2212 1}, and copied onto the GPU global memory with one copy transaction\u2014we allocate them as a single mono-dimensional array.\nIn order to exploit the highest degree of parallelism offered by the GPU device, we (1) map one GPU thread Tid to one element of the output bucket-table rout and (2) adopt the ordering relation \u227aT for each input and output bucket-table processed. Adopting such techniques allows each thread to be responsible of performing exactly two reads and one write from/to the GPU global memory. Additionally, the ordering relation enforced on the bucket-tables allows us to exploit the locality of data and to encourage coalesced data accesses. As illustrated in Fig. 6, this paradigm allows threads (whose IDs are identified in red by their Tid\u2019s) to operate on contiguous chunks of data and, thus, minimizes the number of actual read (from the input bucket-table, on the right) and write (onto the output bucket-table, on the left) operations from/to the global memory performed by a group of threads with a single data transaction.6\n6 Accesses to the GPU global memory are cached into cache lines of 128 Bytes, and can be fetched by all requiring threads in a warp.\nProcedure GPU::AGGREGATE(Tik , Tj ) 32 rik \u2190 the thread\u2019s entry ID (Tid) 33 rj \u2190 0 /* holds the value of the index entry of \u03c7j */ 34 s\u2190 |Sj | 35 \u3008mul , div ,mod\u3009 \u2190 COPYTOSHAREDMEMORY() 36 for `\u2190 (1 . . . s\u22121) do 37 rj \u2190 rj +mul [`] \u00b7 ( b rik\ndiv [`] c)%mod [`] ) 38 rj \u2190 rj + ( rik%mod [s]\n) 39 \u03c7ik [rik ]\u2190 \u03c7ik [rik ] + \u03c7j [rj ]\nThe constraint aggregation GPU kernel is described in Procedure Gpu::Aggregate, which is computed in parallel by a number of threads equal to the number of rows of the output bucket-table. Each thread identifies its row index rik within the output bucket-table cost values array \u03c7ir based on its thread ID (line 32), and it initializes a variable that will contain the input bucket-table row index to 0 (line 33). It then copies into the shared memory the static entities mul , div , and mod associated to the aggregation of the the bucket-tables being processed (line 35). A further inspection to the Gpu::Aggregate procedure reveals how it makes use of the auxiliary data structures above to efficiently implement the hash function of equation (2), and retrieve the entry index of the input bucket-table associated to the variables value permutation of the output bucket-table Rik [rik ] (lines 36\u201338). Finally, the instruction in line 39 aggregates the corresponding input bucket-table value to the output bucket-table \u03c7ik [rik ].\nNote that this algorithm highly fits the SIMT paradigm adopted by GPUs; the thread ID and the auxiliary mul , div , and mod arrays are used to retrieve and update all the data necessary to compute the output bucket-table. Additionally, the accesses to the global memory are minimized, as the auxiliary arrays are copied into the shared memory.\nWe illustrate the above process in the following example.\nExample 7 Consider the operation of aggregating the input bucket-table Tj with the bucket-table Tik of Fig. 6 corresponding, respectively, to the bucket-table representing the constraint f23 and the bucket-table f\u03023 (before eliminating the variable x3) in Fig. 2(b). With the Equation (2) notation, s = 2, m = 3 and, thus, the index k of the summation ranges from 1 to s\u2212 1 = 1. Therefore:\nmul [0] = \u220f2 j=2 |Dxj | = 2 div [0] = \u220f2 j=\u03c6out(1)+1=2 |Dxj | = 2 mod [0] = |Dx2 | = 2 mod [1] = |Dx3 | = 2\nTherefore, the mapping from the thread IDs (or, equivalently, the output bucket-table row indices rik ) to the input bucket-table row indices rj is:\nTid = 0 \u21d2 rj = 2 \u00b7 (b 02c) + 0 mod 2 = 0 Tid = 1 \u21d2 rj = 2 \u00b7 (b 12c) + 1 mod 2 = 1 Tid = 2 \u21d2 rj = 2 \u00b7 (b 22c) + 2 mod 2 = 2\nTid = 3 \u21d2 rj = 2 \u00b7 (b 32c) + 3 mod 2 = 3 Tid = 4 \u21d2 rj = 2 \u00b7 (b 42c) + 4 mod 2 = 0 Tid = 5 \u21d2 rj = 2 \u00b7 (b 52c) + 5 mod 2 = 1 Tid = 6 \u21d2 rj = 2 \u00b7 (b 62c) + 6 mod 2 = 2 Tid = 7 \u21d2 rj = 2 \u00b7 (b 72c) + 7 mod 2 = 3\nAs a technical detail, the bucket-tables are created and processed so that the variables in their scope are sorted according to the order \u227aT . This means that the variables with the highest priority appear first in the scope list, while the variable to be eliminated always appear last. We will see, in the next section, that such detail allow us to efficiently encode the elimination operation on the GPU.\nTo fully capitalize on the use of the GPU, we exploit an additional level of parallelism, achieved by running GPU kernels and CPU computations concurrently (lines 26\u201327 of Algorithm 3). This is possible when the Tj bucket-tables can be partitioned in multiple chunks. Fig. 7 illustrates this operation. After transferring the first bucket-table chunk (Tj #1) into the device memory, the process starts the execution of the Gpu::Aggregate() kernel, which operates on this portion of the bucket table (called Kernel #1 in Fig. 7). Thus, the control immediately returns to the CPU host, which enforces the next data transfer onto the device memory, through a call to a GPU::RESERVE(Tj #2). A host-device synchronization point is imposed after each memory transfer (except the first one), to ensure that no overlapping Gpu::Aggregate() GPU kernels are enforced.\n5.4 GPU-based Variable Elimination\nWe now describe the implementation of the variable elimination GPU kernel. This operation takes as input a bucket-table Tik and a variable xi \u2208 Sik and removes this variable from the bucket-table\u2019s scope, optimizing over its cost rows. As a result, the output bucket-table rows list the unique assignments for the value combinations of Sik \\ {xi} in the input bucket-table Rik which minimizes the costs values for each d \u2208 Dxi .\nFig. 8 illustrates this process, where the variable x3 is eliminated from the buckettable Tik . The column being eliminated is highlighted yellow in the input buckettable. The different row colors identify the unique assignments for the remaining variables x1, x2, and exposes the high degree of parallelization that is associated to such operation. To exploit this level of parallelization, we adopt a paradigm similar to that employed in the aggregation operation on GPU, where each thread is responsible of the computation of a single output element.\nProcedure GPU::ELIMINATE(Tik , xi) 40 rik \u2190 the thread\u2019s entry ID (Tid) 41 rj \u2190 rik \u00b7 |Dxi | /* holds the value of the index entry of \u03c7ik */ 42 c\u2217 \u2190 \u03c7ik [rj ] 43 for `\u2190 (1 . . . |Dxi |\u22121) do 44 c\u2217 \u2190 min{c\u2217, \u03c7ik [rj + `]} 45 \u03c7ik [rik ]\u2190 c\u2217\nThe variable elimination GPU kernel is described in Procedure Gpu::Eliminate, which is computed in parallel by a number of threads equal to the number of rows of the output bucket-table. Each thread identifies its row index rik within the output bucket-table cost values \u03c7ik (line 40), given its thread ID. It hence sets an input row index rj to the value of the first \u03c7ik input bucket-table row to analyze (line 41), and it stores in c\u2217 its associated cost. Note that, as the variable to eliminate is listed last in the scope of the bucket-table, it is possible to retrieve each unique assignment for the projected output bucket table, simply by offsetting rik by the size of Dxi . Additionally, all elements listed in \u03c7ik [rj ], . . . , \u03c7ik [rj + |Dxi |] differ exclusively on the value assignment to the variable xi (see Fig. 8). Thus, the GPU kernel evaluates the input bucket-table cost values associated to each element in the domain of xi, by incrementing the row index rj , |Dxi | \u2212 1 times, and chooses the minimum cost value (lines 43\u201344). At last, it saves to the associated output row the best cost found (line 45).\nNote that each thread reads |Dxi | adjacent values of the vector \u03c7ik , and writes one value in the same vector. Thus, this algorithm (1) perfectly fits the SIMT paradigm, (2) minimizes the accesses to the global memory as it encourages a coalesced data access pattern, and (3) uses a relatively small amount of global memory, as it recycles the memory area allocated for the input bucket-table, to output the cost values for the output bucket-table.\nThe ordering \u227aT adopted by the bucket-tables makes this procedure effective, by forcing the variables to be eliminated to be always listed as last. Additionally, we note that reordering the bucket-tables scope may be necessary exclusively when constructing the bucket-table associated to the constraints in C. Indeed, the buckettables constructed by the algorithm preserve this ordering over their scope, since all the problem variables are processed according to the same ordering relation\u227aT , guaranteeing that the variables being eliminated are those with lower priority with respect to \u227aT . Therefore, no reordering will be required in the bucket functions during the process.\nFinally, to reduce the memory transfer time, in addition to the technique described in the previous section, we unrolled the for-loop in lines 25\u201327 of Algorithm 3. Doing so allows us to process all the bucket-tables within a mini-bucket in a single GPU kernel and to copy them to the device using a single transaction."}, {"heading": "6 Theoretical Analysis", "text": "We report below a theoretical analysis on the runtime and memory complexity of our GpuBE(z) algorithms. For the distributed case, we report results on the network load and messages size complexity provided by the proposed algorithms. The network load and messages size are defined, respectively, as the total number of messages exchanged by the agents and as the size of the largest message exchanged by the agents during problem resolution. Since our algorithms rely on an inference-based procedure, the agent\u2019s complexity (i.e., the maximal number of operations performed by the agents while solving the problem) is equivalent to the size of the largest message exchanged. In turn, the latter corresponds to the memory complexity of the algorithm. We use GpuBE(w\u2217) and GpuDBE(w\u2217) to denote our GPU versions of BE and DPOP, respectively, and GpuBE(z) and GpuDBE(z) to denote our GPU versions of MBE and ADPOP, respectively, with mini-bucket size z.\nTheorem 1 For a problem P , given an ordering \u227aT on the constraint graph GP , the (mini-)bucket tables (resp. UTIL messages) constructed by GpuBE(z) (resp. the GpuDBE(z) agents) are identical to those constructed by (M)BE (resp. the (A)DPOP agents), for z \u2264 w\u2217.\nProof The proof follows from the observation that GpuBE(z) and (M)BE are executed on the same induced graph G\u2217P . Thus, the problem variables are processed in the same order by both versions of the algorithms\u2014lines 1 and 6 (9 and 16) for (M)BE, and lines 20 and 29 for GpuBE(z). Analogously, in GpuDBE(z) and (A)DPOP, agents operate on the same pseudo-tree ordering.\nFor the centralized case, during the Variable Elimination Phase, the bucket construction and mini-bucket partitioning operations of GpuBE(z) (lines 21\u201322) are identical to those of MBE (lines 10\u201311). For each mini-bucket Bij in MBE, the operations to create the bucket function f\u0302ik are identical in both algorithms: the effect of invoking the Gpu::Aggregate(Tik , Tj) routine, in GpuBE, for each bucket-table Tik , corresponding to the bucket function fik (lines 25\u201327), is analogous to the aggregation operations performed in MBE: F = \u2211 fj\u2208Bik\nfj (line 13, in parenthesis), and the effect of the Gpu::Eliminate(Tik , xi) routine, which projects the variable xi onto the scope of Tij , produces the bucket function f\u0302ik , which in turn correspond to the elimination operation performed by MBE: \u03c0\u2212xi(F ) (line 13). For the distributed cases, both ADPOP and GpuDBE(z) agents perform the same operations described above\u2014during the UTIL Propagation Phase\u2014and populate the UTIL messages they send to their parent. The equivalence between the Variable Elimination and UTIL Propagation Phases of BE and DPOP, with the respective phases in GpuBE(w\u2217) and GpuDBE(w\u2217), respectively, follows from the process described above differing exclusively in that partitioning Bi produces a single bucket with the same functions as those listed in Bi.\nThe operations performed during the Variable Assignment Phases for (M)BE and GpuBE(z) (lines 5-7 for BE, lines 16\u201317 for MBE, and lines 29\u201330 for GpuBE(z)) are identical. Additionally, the variables are processed in the same order in both algorithms. Thus, the solution assignment for the problem variables returned by (M)BE and GpuBE are identical. Similarly, for the distributed case, (A)DPOP and GpuDBE(z) agents perform the same VALUE Propagation phase.\nCorollary 1 The time and memory (message size) requirements of Gpu(D)BE(z) is, in the worst case, exponential in z, for z \u2264 w\u2217, i.e., is in O(dz), where d = maxxi\u2208XDxi .\nProof This result follows from the equivalence of the Variable Elimination Phases of (M)BE and GpuBE(z), and of the UTIL Propagation Phases of (A)DPOP and GpuDBE(z). During these phases, the construction of the (mini)-buckets requires to save, in the worst case, all possible combinations for the value assignments of the bucket-function with bounded arity z. Thus, they require O(dz) space. Similarly, for the distributed case, due to the equivalence of (A)DPOP and GpuDBE(z), the largest message exchanged by the agents has size O(dz).\nAdditionally, the total amount of operations (or, equivalently, bucket-tables rows) that can be processed in parallel during the GPU-based Constraint Aggregation and GPU-based Variable Elimination steps, is bounded by a constant value which depends on the GPU card characteristic. Thus, the time complexity of GpuDBE(z) is exponential in z.\nCorollary 2 The network load required for GpuDBE(z) is equivalent to the network load required by (A)DPOP.\nProof This result follow from the equivalence of DPOP with GpuDBE(w\u2217) and ADPOP(z) with GpuBE(z) (Theorem 1). Since (A)DPOP requires each agent to send one UTIL message to its parent and one VALUE message to each of its children, there\nare a total of n\u2212 1 UTIL/VALUE messages exchanged\u2014one through each tree-edge of the pseudo-tree TP . Thus, the network load required by (A)DPOP and GpuDBE(z) is in O(n).\nCorollary 3 Gpu(D)BE is correct and complete.\nProof The correctness and completeness of GPU-(D)BE(w\u2217) follow from the correctness and completeness of BE [15] and DPOP [47], and Theorem 1."}, {"heading": "7 Experimental Results", "text": "In this section, we evaluate our GPU implementations of BE and MBE (GpuBE) as well as our GPU implementations of DPOP and ADPOP (GpuDBE) and compare them with their CPU counterparts.7\nExperiments for GpuDBE and (A)DPOP are conducted using a multi-agent DCOP simulator that simulates the concurrent activities of multiple agents, whose actions are activated upon receipt of a message. All algorithms use the same variable ordering in the centralized case and pseudo-tree in the distributed case. Performance of the centralized algorithms are evaluated using the algorithms\u2019 wallclock runtime, while the performance of distributed algorithms are evaluated using the simulated runtime metric [57]. The main focus of the evaluation is on runtime and speedup achieved by the GPU implementations with respect to their CPU counterparts. Additionally, to compare the quality of the solution bounds reported by the incomplete algorithms, we also report the best solution quality found within the given time limits by toulbar2 [3], an optimized, exact centralized solver for WCSPs. Toulbar2 is a state-of-the-art solver that uses a depth-first branch-and-bound process to identify a minimum cost assignment and employs the notion of soft local consistency to prune the search space using the problem lower bound.\nOur experiments are conducted on an AMD Opteron 6276 with a 2.3GHz CPU and is equipped with a GPU device GeForce GTX TITAN with 14 multiprocessors, 2688 cores, with a clock rate of 837MHz, and 6GB of global memory.\nWe performed our experiments on both randomly generated instances on different networks topologies and on standard WCSP benchmarks.8 We first analyze the runtimes of the CPU and GPU versions of BE and DPOP on randomly generated instances, where we report the runtimes and lower bounds of the GPU and CPU versions of MBE and ADPOP at varying of the bucket size z. Then, to ensure that the speedups are not due to a specific GPU device configuration, we compare the CPU and GPU speedups achieved on 3 distinct GPU architectures, characterized by different clock rates, number of SMs, and memory sizes. Finally, we report the solving time and lower bounds of our GpuBE on an extensive set of WCSP benchmarks to verify the generality of the speedups across different domains. Each solver has 1-hour timeout of wallclock time in the centralized case and a 1-hour timeout of simulated\n7 Our source code is available at https://github.com/nandofioretto/GpuBE, and https://github.com/nandofioretto/GpuDBE\n8 Downloadable from http://costfunction.org/en/benchmark/ and http: //graphmod.ics.uci.edu/group/Repository\ntime in the distributed case. Additionally, they have a memory limit of 32GB to solve each problem instance. Results are averaged over all instances. If a solver fails to solve an instance is due to either memory limits (labeled oom) or timeout (labeled oot).\n7.1 Binary Random Networks\nThe instances for each binary network topology are generated as follows: \u2022 Random: We create an n-node network, whose density p1 produces bn (n\u22121) p1c\nedges in total. We do not bound the tree-width, which is based on the underlying graph and randomly generated. \u2022 Scale-free: We create an n-node network based on the Barabasi-Albert model [6]. Starting from a connected 2-node network, we repeatedly add a new node, randomly connecting it to two existing nodes. In turn, these two nodes are selected with probabilities that are proportional to the numbers of their connected edges. The total number of edges is 2 (n\u2212 2) + 1. \u2022 Grid: We create an n-node network arranged as a rectangular grid, where each internal node is connected to four neighboring nodes, while nodes on the grid perimeter are connected to three neighboring nodes unless they are at the corner of the grid, in which case they are connected to two neighboring nodes.\nWe generate 50 instances for each topology, ensuring that the underlying graph is connected. The cost functions are generated using random integer costs in [0, 100], and the constraint tightness (i.e., ratio of entries in the cost table that have a cost of \u221e) p2 is set to 0.5 for all experiments. We set the following as default parameters: For the random and scale-free topology, n=10, d=maxDi\u2208D |Di|=10, and p1 =0.3, and for the grid topology, \u221a n=10.\nTables 1\u20133 report the runtime, in seconds, for random, scale-free, and grid topologies, respectively, varying the number of variables (resp. agents) for the centralized (resp. distributed) algorithms, the size of the variables domains, and the constraint tightness of the constraint graph. The first four (three) columns of Table 1, (2 and 3) describe the problem setting adopted for each experiment. The induced width w\u2217 is averaged across all instances. All other columns report the average runtime and GPU vs. CPU speedup in parenthesis. We make the following observations: \u2022 The GPU-based inference-algorithms are consistently faster that their CPU coun-\nterparts, with speedups of up to 307x. Only two exceptions arise for the random networks, where in the small instances with n = 10, d = 5, p1 = 0.3, and n = 10, d = 10, p1 = 0.2, the GPU versions of the algorithms are slower than their CPU counterparts. \u2022 The speedup increases with the problem size. In particular, the speedup increases with increasing induced width and with increasing domain size of the problem variables. Both these factors influence the size of the bucket-tables to be processed.9\nThis observation corroborates the effectiveness of the GPU parallelism exploited in the construction of these tables.\n9 Recall that BE needs to process bucket-tables whose number of rows is in O(dw \u2217 )."}, {"heading": "10 10 0.3 2.9 0.019 0.002 10.5 0.007 0.001 7.20", "text": ""}, {"heading": "11 10 0.3 3.2 0.031 0.002 13.6 0.013 0.001 13.0", "text": ""}, {"heading": "12 10 0.3 3.6 0.069 0.003 25.7 0.028 0.001 28.5", "text": ""}, {"heading": "13 10 0.3 4.3 0.413 0.005 79.4 0.210 0.002 116", "text": ""}, {"heading": "14 10 0.3 4.4 0.631 0.006 98.6 0.214 0.002 134", "text": ""}, {"heading": "15 10 0.3 5.3 4.190 0.026 158 1.609 0.009 187", "text": ""}, {"heading": "16 10 0.3 5.8 32.29 0.189 171 9.848 0.049 202", "text": ""}, {"heading": "17 10 0.3 6.4 65.41 0.328 200 28.14 0.138 204", "text": ""}, {"heading": "18 10 0.3 7.5 206.1 0.944 218 103.0 0.483 213", "text": ""}, {"heading": "19 10 0.3 8.0 602.1 2.541 237 470.2 2.019 233", "text": ""}, {"heading": "20 10 0.3 8.5 675.3 3.145 215 508.9 2.160 236", "text": ""}, {"heading": "10 5 0.3 3.0 0.001 0.002 0.56 0.001 0.001 0.80", "text": ""}, {"heading": "10 10 0.3 2.9 0.019 0.002 10.5 0.007 0.001 7.20", "text": ""}, {"heading": "10 25 0.3 2.8 0.227 0.004 55.4 0.092 0.001 92.3", "text": ""}, {"heading": "10 50 0.3 2.9 24.81 0.095 262 13.99 0.048 291", "text": ""}, {"heading": "10 100 0.3 2.9 67.59 0.220 308 35.22 0.118 299", "text": ""}, {"heading": "10 10 0.2 2.0 0.001 0.001 0.62 0.001 0.001 0.94", "text": ""}, {"heading": "10 10 0.3 2.9 0.019 0.002 10.5 0.007 0.001 7.20", "text": ""}, {"heading": "10 10 0.4 3.8 0.094 0.002 40.7 0.042 0.001 42.5", "text": ""}, {"heading": "10 10 0.5 4.5 0.525 0.005 105 0.234 0.002 130", "text": ""}, {"heading": "10 10 0.6 5.4 3.378 0.019 176 1.941 0.011 176", "text": ""}, {"heading": "10 10 0.7 5.9 14.86 0.072 205 10.00 0.053 189", "text": ""}, {"heading": "10 10 0.8 6.7 56.23 0.246 228 31.29 0.147 213", "text": ""}, {"heading": "10 10 0.9 7.6 72.32 0.312 232 42.47 0.201 211", "text": "\u2022 As expected, the inference-based algorithms are unable to process instances characterized by large induced widths or large domain sizes, as the size of the buckettables become intractable with the memory limitations. This is evident in the scalefree and grid networks, where the solvers run out of memory for instances with n \u2265 16 and d \u2265 50, respectively. \u2022 The simulated runtimes of the DCOP algorithms are consistently smaller than the wallclock runtimes of the WCSPs ones. This is due to the fact that agents in different branches of the pseudo-tree can compute their bucket-tables independently from each other. \u2022 Finally, the speedup trends of the distributed algorithms are similar to those of the centralized algorithms.\nNext, we analyze the performance of the individual kernels that implement the constraint aggregation and the variable elimination processes described, respectively, in Sections 5.3 and 5.4. Figure 9 illustrates the average speedup obtained by the GPUbased constraint aggregation, and the GPU-based variable elimination with respect to their CPU-based counterparts when considering the largest bucket processed in each instance of the random, scale-free, and grid network instances. The reported average speedup for the constraint aggregation operations range from 363x (in scale free networks) to 613x (in random networks). The variable elimination operations achieve an even higher speedup, ranging from 830x (for grid networks) to 911x (for"}, {"heading": "10 10 6.3 22.99 0.111 207 13.78 0.064 215", "text": ""}, {"heading": "11 10 6.0 25.57 0.120 212 13.21 0.057 231", "text": ""}, {"heading": "12 10 6.0 27.96 0.132 212 14.60 0.072 203", "text": ""}, {"heading": "13 10 5.9 80.14 0.370 217 36.21 0.174 208", "text": ""}, {"heading": "14 10 6.9 78.36 0.339 231 32.50 0.145 223", "text": "scale free networks). This is due to the high locality of data exploited by the GPUbased variable elimination kernel, which encourages coalesced data accesses, and through memory reuse, where we overwrite the input bucket-table of the variable elimination process with the resulting bucket-table from the same process.\nNext, we compare our centralized and distributed versions of GpuBE with MBE [15] and ADPOP [47] at varying of the mini-bucket size z \u2208 {2, . . . , 10}, on binary constraint networks with random, scale-free, and grid topologies, using\nthe same settings described in the previous section. The instances for each topology are generated as described above. Fig. 10(a\u2013c) illustrate the speedup of the CPU and GPU versions of MBE, respectively, on random networks with n = 20, d = 25, p1 = 0.3, on scale-free networks with n = 20, d = 25, and on grid networks with\u221a n = 10, d = 25. The intensity of the color illustrates the solution quality of the bound returned (darker color denotes better solution quality). We make the following observations: \u2022 The speedup obtained by the GPU vs. CPU solvers increases as the size of the mini-\nbuckets increases. This observation is consistent with the previous observation that the speedup increases with increasing induced widths. \u2022 The speedup saturates when z = 7 in all benchmarks, reporting maximal speedups of 235x, 274x, and 156x, for random, scale-free, and grid networks, respectively. This phenomena occurs when the maximum concurrent number of GPU threads are scheduled and executed simultaneously by all the GPU SMs\u2014i.e., when there is enough work to saturate the GPU maximal occupancy. \u2022 As for the previous experiment, the speedup trends of the distributed algorithms are similar to those of the centralized algorithms. The correlation10 of the CPU vs. GPU speedup between the centralized and the distributed solutions are 0.93, 0.95, and 0.99, respectively for the grid, random, and scale-free network topologies.\nTable 5 illustrates a comparison of the speedups obtained with three different GPU hardware configurations: TESLA M2075, GeForce GTX Titan and GeForce GTX Titan X, whose specifics are summarized in Table 4.11\n10 We use the Pearson product-moment correlation coefficient. 11 In all other experiments we used the GeForce GTX Titan, as this is the best, most affordable card at\nour disposal.\nAmong the three GPU devices, the TESLA M2075 achieve the lowest maximal speedups, which range from 117.8x to 213,7x. Additionally, the speedup saturates when z = 5 for grid networks and z = 7 for random and scale-free networks. This is due to the fact that this card can schedule the smallest number of cores per each SMs (32). Since each core can run concurrently a wrap (32 threads), its maximal level of concurrency is 14\u00d7 32\u00d7 32 = 14, 336 threads (and is thus the maximum number of parallel aggregation operations). In contrast, the speedup obtained by our GpuBE is the highest on the GeForce GTX Titan X\u2014obtaining a maximal speedup of 646.9x\u2014 and saturates when z = 8 in all networks. The maximum number of threads that can run concurrently on this card is 24 \u00d7 128 \u00d7 32 = 98, 304. The speedups obtained by our solver on the GeForce GTX Titan, used in the rest for the experiments in this paper, are larger than those obtained on the TESLA but smaller than those obtained\non the GeForce GTX Titan X. This card can run up to 86, 016 threads. In addition to the number of threads than can run concurrently, the GPU clock rate and L2 cache size play a substantial role in the GPU performance.\nFinally, Fig. 10(d) illustrates the time spent by the GPU devices while executing the kernel functions (in white) in contrast to the time used for memory transfers and allocations (in blue), at varying mini-bucket size z = {4, 6, 8}. These times are averaged among all instances for the three network topologies examined and are normalized with the respect to the wallclock runtime. The results show that the time spent by the device in performing actual computations increases, with the respect to the memory transfer time, as the mini-bucket size increase. Allocations and memory transfers on the Titan device are slower than on the TESLA and the GTX Titan X. Finally, these times account for the 36% to 55%, 18% to 34%, and 8% to 18% of the total time, respectively for the mini-bucket sizes 4, 6, and 8.\n7.2 WCSPs Benchmarks\nWe now report the evaluation of our GpuBE on the following standard WCSPs benchmarks: \u2022 Coloring: Graph coloring instances cast into minimum coloring instances. \u2022 Celar: Radio link frequency assignment problems. \u2022 Iscas89: WCSPs derived from digital circuits.\n\u2022 Spot: Instances of the daily photograph scheduling problem of Earth observation satellites. \u2022 Pedigree: Instances from the genetic linkage analysis domain that is associated with the task of haplotyping.\nTables 7\u20139, tabulate the results for the above benchmarks. In each table and for each instance, we report, in order, the instance name\u2014as appearing in the original benchmark\u2014the number of variables n of the problem, the maximum size of their domains d, the number of constraints c, the graph density p1, and the induced width w\u2217 of the underlying constraint graph. In each table, the top row shows the runtimes in seconds of GpuBE(z) at varying bucket size z and GpuBE. The bottom row shows the returned solutions\u2019 qualities, where for GpuBE(z), we report the lower bound it returned. When GpuBE failed to report a solution (due to memory limits), we report the solution quality found by toulbar2 (shown in parenthesis) or a dash symbol, if toulbar2 did not terminate within the time limit. The speedup of GpuBE(z) and GpuBE w.r.t. their CPU counterparts are shown in parentheses. For each instance, we vary the bucket size z from 2 to 20, and report the minimum bucket size zmin, which is the largest constraint arity of the instance, the maximum bucket size zmax = min{wz, 20}, where wz is defined as the maximal bucket size that can be processed within the hardware memory limits, and the intermediate bucket sizes z2 = zmin + 1 3 (zmax \u2212 zmin) and z3 = zmin + 2 3 (zmax \u2212 zmin).\nConsistent with our previous observations, the algorithms\u2019 speedups and solution qualities increase as the bucket size increases. Additionally, for several large problems instances (e.g., scen06-24reduc\u2014scen06reduc in the Celar benchmark), our GPU implementation of MBE can report good lower bounds quickly (within a few seconds), whereas solving the entire problem with the most competitive soft consistency technique in toulbar2 requires from 6 to 48 minutes. For other large instances (e.g., in the Spot benchmark), we observe that toulbar2 ran out of time for the majority of the instances, while our GpuBE(z) can quickly find lower bounds, which could be used in a AND-and-OR search type as proposed by Marinescu and Dechter [40]."}, {"heading": "8 Related Work", "text": "The use of GPUs to solve difficult combinatorial problems has been explored by several proposals in different areas of constraint optimization. For instance, Meyer et al. [35] proposed a multi-GPU implementation of the simplex tableau algorithm that relies on a vertical problem decomposition to reduce communication between GPUs. In constraint programming, Arbelaez and Codognet [5] proposed a GPU-based version of the Adaptive Search algorithm, which explores several large neighborhoods in parallel, resulting in a speedup factor of 17. Campeotto et al. [12] proposed a GPUbased framework that exploits both parallel propagation and parallel exploration of several large neighborhoods using local search techniques, leading to a speedup factor of up to 38. The combination of GPUs with dynamic programming has also been explored to solve different combinatorial optimization problems. For instance, Boyer et al. [9] proposed the use of GPUs to compute the classical DP recursion step for the knapsack problem, which led to a speedup factor of 26. Paw\u0142owski et al. [43] pre-\nsented a DP-based solution for the coalition structure formation problem on GPUs, reporting up to two orders of magnitude of speedup. In a very recent work, Bistaffa et al. [7] study the parallelization of an inference-based algorithm to solve COPs using GPUs, albeit exclusively in the centralized case.12 Silberstein et al. [55] study a GPU-based kernel for the sum-product operations that arise in marginalize a product\n12 This paper is accepted at ECAI 2016 and has not been published in a formal proceeding yet; we received a copy of the camera-ready version from the authors.\nof functions (MPF) problems. The authors report an average speedup factor of 15 for random benchmarks and Bayesian networks and higher average speedups (up to two orders of magnitude) for log domains due to the difference in performance of the log2f and exp2f functions on the CPU and GPU.\nIn the distributed constraint optimization context, GPU parallelism has been applied to speed up several DCOP solving techniques. Fioretto et al. [28] proposed a multi-variable agent decomposition strategy to solve general DCOPs with complex local subproblems, which makes use of GPUs to implement a search-based and a sampling-based algorithm to speed up the agents\u2019 local subproblems resolution. Le et al. [37] studied a GPU accelerated algorithm in the context of stochastic DCOPs\u2014 DCOPs where the values of the cost tables are stochastic. The authors used SIMTstyle parallelism on a DP-based approach, which resulted in a speedup of up to two orders of magnitude. Recently, a combination of GPUs with Markov Chain Monte\nCarlo (MCMC) sampling algorithms has been proposed in the context of solving DCOPs [27], where the authors adopted GPUs to accelerate the computation of the normalization constants used in the MCMC sampling process as well as to compute several samples in parallel, resulting in a speedup of up to one order of magnitude.\nDifferently from other proposals, our approach aims at using GPUs to exploit SIMT-style parallelism from DP-based methods to solve general, exact and approximated, WCSPs and DCOPs."}, {"heading": "9 Conclusions and Discussions", "text": "Inference-based algorithms are powerful tools for solving discrete optimization problems. However, their applicability is limited by their high time and space requirements. Motivated by the increasing availability of GPUs, in this paper, we proposed a scheme to speed up the resolution of inference-based methods for centralized and distributed constraint optimization by exploiting SIMT-style parallelism. We introduced an exact algorithm and an approximated algorithm that are inspired by BE and MBE for WCSPs and by DPOP and ADPOP for DCOPs. These procedures make use of multiple threads in the GPU cards to parallelize the aggregation and elimination procedures, which are responsible for the high complexity in the inference-based approaches. Additionally, we detailed the design of the data structures adopted to process cost functions with GPUs, and of the mapping adopted to associate GPU threads to cost functions\u2019 entries, which allow us to efficiently exploit the data parallelism (SIMT) supported by GPUs.\nFinally, we reported an extensive experimental evaluation of our inference-based GPU implementations on both centralized and distributed benchmarks. We showed that the use of GPUs provides significant advantages in terms of runtime and scalability, achieving speedups of up to two order of magnitude, showing a considerable reduction in runtime (up to 345 times faster) with respect to the serialized version, and that the speedups increase with the induced width of the problem and with the size of the domain of the problem\u2019s variables.\nThe proposed results are significant\u2014the wide availability of GPUs provides access to parallel computing solutions that can be used to improve efficiency of WCSPs and DCOP solvers. Furthermore, GPUs are renowned for their complex architectures (multiple memory levels with very different size and speed characteristics; relatively slow cores), which often create challenges to the effective exploitation of parallelism from irregular applications. The strong experimental results indicate that the proposed algorithms are well-suited to GPU architectures.\nThese results hint that our approach could be exploited in the the context of a dynamic search that makes use of the mini-bucket elimination method as an heuristic to infer bounds on the solution quality, potentially allowing dynamic variable orderings. Indeed, the main drawback of this type of search (and various look-ahead methods) is that, since the heuristic is (re)computed in various nodes during search process, the time invested in the heuristic computation may not be cost-effective. Leveraging the use of GPUs to infer bounds faster during the dynamic search, may therefore produce dramatic speedup to the whole search process.\nWhile this paper describes the applicability of our approach to (M)BE and (A)DPOP, we believe that analogous techniques can be derived and applied to other inference-based approaches to solve discrete optimization problems (e.g., to implement the logic of inference-based propagators) and optimization on graphical models\u2014(e.g., in solving Maximum A Posteriori (MAP/MRF) problems and finding Maximum Probability Explanation (MPE) in Bayesian networks). We also envision that this technology could open the door to efficiently enforcing higher forms of consistencies than domain consistency (e.g., path consistency [42], adaptive con-\nsistency [18], or the more recently proposed branch consistency for DCOPs [25]), especially when the cost functions need to be represented explicitly.\nAcknowledgements This research is partially supported by the National Science Foundation under grants 1345232, 1550662, 1458595, and 1401639. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the sponsoring organizations, agencies, or the U.S. government."}], "references": [{"title": "Nurse Scheduling using Constraint Logic Programming", "author": ["S. Abdennadher", "H. Schlenker"], "venue": "Proceedings of the Conference on Innovative Applications of Artificial Intelligence (IAAI), pp. 838\u2013843", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Computational protein design as an optimization problem", "author": ["D. Allouche", "I. Andr\u00e9", "S. Barbe", "J. Davies", "S. de Givry", "G. Katsirelos", "B. O\u2019Sullivan", "S.D. Prestwich", "T. Schiex", "S. Traor\u00e9"], "venue": "Artificial Intelligence 212, 59\u201379", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "ToulBar2 to solve Weighted Partial Max-SAT", "author": ["D. Allouche", "S. de Givry", "H. Nguyen", "T. Schiex"], "venue": "Tech. rep., INRA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Principles of constraint programming", "author": ["K. Apt"], "venue": "Cambridge University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A GPU implementation of parallel constraint-based local search", "author": ["A. Arbelaez", "P. Codognet"], "venue": "Proceedings of the Euromicro International Conference on Parallel, Distributed and network-based Processing (PDP), pp. 648\u2013655", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Emergence of scaling in random networks", "author": ["A.L. Barab\u00e1si", "R. Albert"], "venue": "Science 286(5439), 509\u2013512", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "CUBE: A CUDA approach for bucket elimination on GPUs", "author": ["F. Bistaffa", "N. Bomberi", "A. Farinelli"], "venue": "Proceedings of the European Conference on Artificial Intelligence (ECAI), p. to appear", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Semiring-based constraint satisfaction and optimization", "author": ["S. Bistarelli", "U. Montanari", "F. Rossi"], "venue": "Journal of the ACM 44(2), 201\u2013236", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Solving knapsack problems on GPU", "author": ["V. Boyer", "D. El Baz", "M. Elkihel"], "venue": "Computers & Operations Research 39(1), 42\u201347", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving DPOP with function filtering", "author": ["I. Brito", "P. Meseguer"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 141\u2013158", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "The state of the art of nurse rostering", "author": ["E.K. Burke", "P. De Causmaecker", "G.V. Berghe", "H. Van Landeghem"], "venue": "Journal of scheduling 7(6), 441\u2013499", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A GPU implementation of large neighborhood search for solving constraint optimization problems", "author": ["F. Campeotto", "A. Dovier", "F. Fioretto", "E. Pontelli"], "venue": "Proceedings of the European Conference on Artificial Intelligence (ECAI), pp. 189\u2013194", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "A constraint solver for flexible protein model", "author": ["F. Campeotto", "A.D. Pal\u00f9", "A. Dovier", "F. Fioretto", "E. Pontelli"], "venue": "Journal of Artificial Intelligence Research 48, 953\u20131000", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Reducing thread divergence in a GPUaccelerated branch-and-bound algorithm", "author": ["I. Chakroun", "M.S. Mezmaz", "N. Melab", "A. Bendjoudi"], "venue": "Concurrency and Computation: Practice and Experience 25(8), 1121\u20131136", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence 113(1), 41\u201385", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Reasoning with probabilistic and deterministic graphical models: Exact algorithms", "author": ["R. Dechter"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning 7(3), 1\u2013191", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Network-based heuristics for constraint-satisfaction problems", "author": ["R. Dechter", "J. Pearl"], "venue": "Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1988}, {"title": "Mini-buckets: A general scheme for bounded inference", "author": ["R. Dechter", "I. Rish"], "venue": "Journal of the ACM 50(2), 107\u2013153", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "SIMD re-convergence at thread frontiers", "author": ["G.F. Diamos", "B. Ashbaugh", "S. Maiyuran", "A. Kerr", "H. Wu", "S. Yalamanchili"], "venue": "Proceedings of the Annual IEEE/ACM International Symposium on Microarchitecture, pp. 477\u2013488", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Autonomous agents coordination: Action languages meet CLP() and Linda", "author": ["A. Dovier", "A. Formisano", "E. Pontelli"], "venue": "Theory and Practice of Logic Programming 13(2), 149\u2013173", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "External A", "author": ["S. Edelkamp", "S. Jabbar", "S. Schr\u00f6dl"], "venue": "Advances in Artificial Intelligence: 27th Annual German Conference on AI, (KI) 2004, pp. 226\u2013240", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Decentralised coordination of low-power embedded devices using the Max-Sum algorithm", "author": ["A. Farinelli", "A. Rogers", "A. Petcu", "N. Jennings"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 639\u2013646", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained community-based gene regulatory network inference", "author": ["F. Fioretto", "A. Dovier", "E. Pontelli"], "venue": "ACM Trans. Model. Comput. Simul. 25(2), 11", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving DPOP with branch consistency for solving distributed constraint optimization problems", "author": ["F. Fioretto", "T. Le", "W. Yeoh", "E. Pontelli", "T.C. Son"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP), pp. 307\u2013323", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting GPUs in solving (distributed) constraint optimization problems with dynamic programming", "author": ["F. Fioretto", "T. Le", "W. Yeoh", "E. Pontelli", "T.C. Son"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP), pp. 121\u2013139", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A dynamic programming-based MCMC framework for solving DCOPs with GPUs", "author": ["F. Fioretto", "W. Yeoh", "E. Pontelli"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP), p. to appear", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Multi-Variable Agent Decomposition for DCOPs", "author": ["F. Fioretto", "W. Yeoh", "E. Pontelli"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 2480\u20132486", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed search for supply chain coordination", "author": ["J. Gaudreault", "J.M. Frayret", "G. Pesant"], "venue": "Computers in Industry 60(6), 441\u2013451", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling microgrid islanding problems as DCOPs", "author": ["S. Gupta", "W. Yeoh", "E. Pontelli", "P. Jain", "S.J. Ranade"], "venue": "North American Power Symposium (NAPS), pp. 1\u20136. IEEE", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed intelligent backtracking", "author": ["Y. Hamadi", "C. Bessi\u00e8re", "J. Quinqueton"], "venue": "Proceedings of the European Conference on Artificial Intelligence (ECAI), pp. 219\u2013223", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Reducing Branch Divergence in GPU Programs", "author": ["T.D. Han", "T.S. Abdelrahman"], "venue": "Proceedings of the Fourth Workshop on General Purpose Processing on Graphics Processing Units, pp. 3:1\u20133:8. ACM Press, New York, NY", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Beem: bucket elimination with external memory", "author": ["K. Kask", "R. Dechter", "A.E. Gelfand"], "venue": "arXiv preprint arXiv:1203.3487", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed constraint optimization with structured resource constraints", "author": ["A. Kumar", "B. Faltings", "A. Petcu"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 923\u2013930", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi GPU implementation of the simplex algorithm", "author": ["M.E. Lalami", "D. El Baz", "V. Boyer"], "venue": "Proceedings of the International Conference on High Performance Computing and Communication (HPCC), vol. 11, pp. 179\u2013186", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Node and arc consistency in weighted csp", "author": ["J. Larrosa"], "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pp. 48\u201353", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2002}, {"title": "ER-DCOPs: A Framework for Distributed Constraint Optimization with Uncertainty in Constraint Utilities", "author": ["T. Le", "F. Fioretto", "W. Yeoh", "T.C. Son", "E. Pontelli"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 605\u2013614", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Scaling up map search in bayesian networks using external memory", "author": ["H. Lim", "C. Yuan", "E.A. Hansen"], "venue": "on Probabilistic Graphical Models p. 177", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Taking DCOP to the real world: Efficient complete solutions for distributed event scheduling", "author": ["R. Maheswaran", "M. Tambe", "E. Bowring", "J. Pearce", "P. Varakantham"], "venue": "Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 310\u2013317", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Evaluating the impact of and/or search on 0-1 integer linear programming", "author": ["R. Marinescu", "R. Dechter"], "venue": "Constraints 15(1), 29\u201363", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "ADOPT: Asynchronous distributed constraint optimization with quality guarantees", "author": ["P. Modi", "W.M. Shen", "M. Tambe", "M. Yokoo"], "venue": "Artificial Intelligence 161(1\u20132), 149\u2013180", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "Networks of constraints: Fundamental properties and applications to picture processing", "author": ["U. Montanari"], "venue": "Information sciences 7, 95\u2013132", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1974}, {"title": "Coalition structure generation with the graphic processor unit", "author": ["K. Paw\u0142owski", "K. Kurach", "T. Michalak", "T. Rahwan"], "venue": "Tech. Rep. CS-RR-13-07, Department of Computer Science, University of Oxford", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2104}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1988}, {"title": "A regular language membership constraint for finite sequences of variables", "author": ["G. Pesant"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP), pp. 482\u2013495", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximations in distributed optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP), pp. 802\u2013806", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "A scalable method for multiagent constraint optimization", "author": ["A. Petcu", "B. Faltings"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 1413\u20131420", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Global grammar constraints", "author": ["C.G. Quimper", "T. Walsh"], "venue": "Proceedings of the International Conference on Principles and Practice of Constraint Programming (CP), pp. 751\u2013755. Springer", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "Enhancing Supply Chain Decisions Using Constraint Programming: A Case Study", "author": ["L. Rodrigues", "L. Magatao"], "venue": "MICAI 2007: Advances in Artificial Intelligence, vol. LNCS 4827, pp. 1110\u20131121. Springer Verlag", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Handbook of Constraint Programming", "author": ["F. Rossi", "P. van Beek", "Walsh", "T. (eds."], "venue": "Elsevier", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "CUDA by Example", "author": ["J. Sanders", "E. Kandrot"], "venue": "An Introduction to General-Purpose GPU Programming. Addison Wesley", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithm for optimal winner determination in combinatorial auctions", "author": ["T. Sandholm"], "venue": "Artificial intelligence 135(1), 1\u201354", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}, {"title": "Valued constraint satisfaction problems: Hard and easy problems", "author": ["T. Schiex", "H. Fargier", "G Verfaillie"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI) 95, 631\u2013639", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1995}, {"title": "Structural descriptions and inexact matching", "author": ["L.G. Shapiro", "R.M. Haralick"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 3(5), 504\u2013519", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1981}, {"title": "Efficient computation of sumproducts on gpus through software-managed cache", "author": ["M. Silberstein", "A. Schuster", "D. Geiger", "A. Patney", "J.D. Owens"], "venue": "Proceedings of the 22nd annual international conference on Supercomputing, pp. 309\u2013318. ACM", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Minimizing writes in parallel external memory search", "author": ["N.R. Sturtevant", "M.J. Rutherford"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "On modeling multiagent task scheduling as a distributed constraint optimization problem", "author": ["E. Sultanik", "P.J. Modi", "W.C. Regli"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 1531\u20131536", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "A dynamic programming approach for consistency and propagation for knapsack constraints", "author": ["M.A. Trick"], "venue": "Annals of Operations Research 118(1-4), 73\u201384", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "BnB-ADOPT: An asynchronous branch-and-bound DCOP algorithm", "author": ["W. Yeoh", "A. Felner", "S. Koenig"], "venue": "Journal of Artificial Intelligence Research 38, 85\u2013133", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed problem solving", "author": ["W. Yeoh", "M. Yokoo"], "venue": "AI Magazine 33(3), 53\u201365", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed constraint optimization for teams of mobile sensing agents", "author": ["R. Zivan", "H. Yedidsion", "S. Okamoto", "R. Glinton", "K. Sycara"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems 29(3), 495\u2013536", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "? This journal article is an extended version of an earlier conference paper [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 48, "context": ", [49,29]), roster scheduling (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 28, "context": ", [49,29]), roster scheduling (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": ", [1,11]), combinatorial auctions (e.", "startOffset": 2, "endOffset": 8}, {"referenceID": 10, "context": ", [1,11]), combinatorial auctions (e.", "startOffset": 2, "endOffset": 8}, {"referenceID": 51, "context": ", [52]), bioinformatics (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": ", [2, 13,24]), multi-agent systems (e.", "startOffset": 2, "endOffset": 12}, {"referenceID": 12, "context": ", [2, 13,24]), multi-agent systems (e.", "startOffset": 2, "endOffset": 12}, {"referenceID": 23, "context": ", [2, 13,24]), multi-agent systems (e.", "startOffset": 2, "endOffset": 12}, {"referenceID": 20, "context": ", [21]) and probabilistic reasoning (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 43, "context": "g, [44]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "In Constraint Satisfaction Problems (CSPs), the goal is to find a value assignment for a set of variables that satisfies a set of constraints [4,50].", "startOffset": 142, "endOffset": 148}, {"referenceID": 49, "context": "In Constraint Satisfaction Problems (CSPs), the goal is to find a value assignment for a set of variables that satisfies a set of constraints [4,50].", "startOffset": 142, "endOffset": 148}, {"referenceID": 53, "context": "In Weighted Constraint Satisfaction Problems (WCSPs) the goal is that of finding an optimal solution, given a set of preferences expressed by means of cost functions [54,53,8].", "startOffset": 166, "endOffset": 175}, {"referenceID": 52, "context": "In Weighted Constraint Satisfaction Problems (WCSPs) the goal is that of finding an optimal solution, given a set of preferences expressed by means of cost functions [54,53,8].", "startOffset": 166, "endOffset": 175}, {"referenceID": 7, "context": "In Weighted Constraint Satisfaction Problems (WCSPs) the goal is that of finding an optimal solution, given a set of preferences expressed by means of cost functions [54,53,8].", "startOffset": 166, "endOffset": 175}, {"referenceID": 40, "context": "When resources are distributed among a set of autonomous agents and communication among the agents is restricted, WCSPs take the form of Distributed Constraint Optimization Problems (DCOPs) [41,47,60].", "startOffset": 190, "endOffset": 200}, {"referenceID": 46, "context": "When resources are distributed among a set of autonomous agents and communication among the agents is restricted, WCSPs take the form of Distributed Constraint Optimization Problems (DCOPs) [41,47,60].", "startOffset": 190, "endOffset": 200}, {"referenceID": 59, "context": "When resources are distributed among a set of autonomous agents and communication among the agents is restricted, WCSPs take the form of Distributed Constraint Optimization Problems (DCOPs) [41,47,60].", "startOffset": 190, "endOffset": 200}, {"referenceID": 38, "context": "DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30].", "startOffset": 104, "endOffset": 112}, {"referenceID": 58, "context": "DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30].", "startOffset": 104, "endOffset": 112}, {"referenceID": 22, "context": "DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30].", "startOffset": 134, "endOffset": 141}, {"referenceID": 60, "context": "DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30].", "startOffset": 134, "endOffset": 141}, {"referenceID": 33, "context": "DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30].", "startOffset": 181, "endOffset": 188}, {"referenceID": 29, "context": "DCOPs have been employed to model various distributed optimization problems, such as meeting scheduling [39, 59], resource allocation [23,61], and power network management problems [34,30].", "startOffset": 181, "endOffset": 188}, {"referenceID": 49, "context": "However, since solving WCSPs and DCOPs is NP-hard [50], optimally solving these problems results in prohibitive runtime and/or use of resources, such as memory or network load.", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "A well known inference-based approach is Bucket Elimination (BE) [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 46, "context": "The Dynamic Programming Optimization Protocol (DPOP) [47] is one of the most efficient inference-based DCOP solvers, and it can be seen as a distributed version of BE, where agents exchange newly introduced cost functions via messages.", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "The importance of inference-based approaches arises in several optimization fields including constraint programming [4,50].", "startOffset": 116, "endOffset": 122}, {"referenceID": 49, "context": "The importance of inference-based approaches arises in several optimization fields including constraint programming [4,50].", "startOffset": 116, "endOffset": 122}, {"referenceID": 57, "context": "For instance, (1) the knapsack constraint propagator proposed by Trick applies DP techniques to establish arc consistency on the constraint [58]; (2) the propagator for the regular constraint establishes arc consistency using a specific digraph representation of the DFA, which", "startOffset": 140, "endOffset": 144}, {"referenceID": 44, "context": "has similarities to dynamic programming [45]; (3) the context free grammar constraint makes use of a propagator based on the CYK parser that uses DP to enforce generalized arc consistency [48].", "startOffset": 40, "endOffset": 44}, {"referenceID": 47, "context": "has similarities to dynamic programming [45]; (3) the context free grammar constraint makes use of a propagator based on the CYK parser that uses DP to enforce generalized arc consistency [48].", "startOffset": 188, "endOffset": 192}, {"referenceID": 21, "context": "Recent developments on external-memory algorithms have shown that the use of large secondary data storage can be effective to extend the applicability of memory intensive approaches [22,38,33,56].", "startOffset": 182, "endOffset": 195}, {"referenceID": 37, "context": "Recent developments on external-memory algorithms have shown that the use of large secondary data storage can be effective to extend the applicability of memory intensive approaches [22,38,33,56].", "startOffset": 182, "endOffset": 195}, {"referenceID": 32, "context": "Recent developments on external-memory algorithms have shown that the use of large secondary data storage can be effective to extend the applicability of memory intensive approaches [22,38,33,56].", "startOffset": 182, "endOffset": 195}, {"referenceID": 55, "context": "Recent developments on external-memory algorithms have shown that the use of large secondary data storage can be effective to extend the applicability of memory intensive approaches [22,38,33,56].", "startOffset": 182, "endOffset": 195}, {"referenceID": 35, "context": "A weighted constraint satisfaction problem (WCSP) [36,54] is a tuple \u3008X,D,C\u3009, where X = {x1, .", "startOffset": 50, "endOffset": 57}, {"referenceID": 53, "context": "A weighted constraint satisfaction problem (WCSP) [36,54] is a tuple \u3008X,D,C\u3009, where X = {x1, .", "startOffset": 50, "endOffset": 57}, {"referenceID": 15, "context": "Definition 1 (Induced Graph, Induced Width [16]) Given the constraint graphGP and an ordering o on its nodes, the induced graph GP on o is the graph obtained by connecting nodes, processed in descending order of priority, to all their preceding neighbors.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "Definition 2 (Pseudo-tree [17]) Given a constraint graph GP , a DFS pseudo-tree arrangement for GP is a spanning tree T = \u3008X, ET \u3009 of GP such that if fi \u2208C and {x, y} \u2286 x, then x and y appear in the same branch of T .", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "Bucket Elimination (BE) [15,16] is a complete inference algorithm that can be used to find all optimal solutions of a WCSP.", "startOffset": 24, "endOffset": 31}, {"referenceID": 15, "context": "Bucket Elimination (BE) [15,16] is a complete inference algorithm that can be used to find all optimal solutions of a WCSP.", "startOffset": 24, "endOffset": 31}, {"referenceID": 14, "context": "As a byproduct, and without additional overhead, BE can compute the number of optimal solutions of the problem (see [15], for details).", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "To overcome this limitation, Dechter and Rish proposed an incomplete version of the Bucket Elimination [19].", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "In a Distributed Constraint Optimization Problem (DCOP) [41,47,60], the variables, domains, and cost functions of a WCSP are distributed among a collection of agents.", "startOffset": 56, "endOffset": 66}, {"referenceID": 46, "context": "In a Distributed Constraint Optimization Problem (DCOP) [41,47,60], the variables, domains, and cost functions of a WCSP are distributed among a collection of agents.", "startOffset": 56, "endOffset": 66}, {"referenceID": 59, "context": "In a Distributed Constraint Optimization Problem (DCOP) [41,47,60], the variables, domains, and cost functions of a WCSP are distributed among a collection of agents.", "startOffset": 56, "endOffset": 66}, {"referenceID": 40, "context": "For example, ADOPT [41] is a distributed version of Iterative Deepening Depth First Search, and DPOP [47] is a distributed version of BE.", "startOffset": 19, "endOffset": 23}, {"referenceID": 46, "context": "For example, ADOPT [41] is a distributed version of Iterative Deepening Depth First Search, and DPOP [47] is a distributed version of BE.", "startOffset": 101, "endOffset": 105}, {"referenceID": 46, "context": "DPOP [47] is a dynamic programming based DCOP algorithm that is composed of three phases: \u2022 Pseudo-tree Generation Phase.", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "In this phase the agents coordinate to construction of a pseudo-tree, realized through existing distributed pseudo-tree construction algorithms [31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "Observe that the UTIL Propagation Phase of DPOP emulates the Variable Elimination Phase of BE in a distributed context [10].", "startOffset": 119, "endOffset": 123}, {"referenceID": 45, "context": "Analogously to how DPOP emulates BE in the distributed context, the Approximate Distributed Pseudotree Optimization (ADPOP) algorithm emulates MBE to solve DCOPs [46].", "startOffset": 162, "endOffset": 166}, {"referenceID": 50, "context": "NVIDIA\u2019s Compute Unified Device Architecture (CUDA) [51] aims at enabling the use of the multiple cores of a graphic card to accelerate general purpose (non-graphical) applications by providing programming models and APIs that enable the full programmability of the GPU.", "startOffset": 52, "endOffset": 56}, {"referenceID": 50, "context": "Additionally, batching small memory transfers into a large one will reduce most of the per-transfer processing overhead [51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 31, "context": "This is referred to as branch divergence, a phenomenon that has been intensely analyzed within the High Performance Computing (HPC) community [32, 14,20].", "startOffset": 142, "endOffset": 153}, {"referenceID": 13, "context": "This is referred to as branch divergence, a phenomenon that has been intensely analyzed within the High Performance Computing (HPC) community [32, 14,20].", "startOffset": 142, "endOffset": 153}, {"referenceID": 19, "context": "This is referred to as branch divergence, a phenomenon that has been intensely analyzed within the High Performance Computing (HPC) community [32, 14,20].", "startOffset": 142, "endOffset": 153}, {"referenceID": 30, "context": "For the distributed case, this phase is identical to that of (A)DPOP, where the agents coordinate the construction of a pseudo-tree, using an off-the-shelf message-passing algorithm [31].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "[1]", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4]", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6]", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] x2 x3 !", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1]", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 0 0 2 0 1 0 1 0 1 1 1 3 Tik Tj j ik", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "6, \u03c6out(0) = 1, as the variable Sj [0] = Sik [1] = x2.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "mod [0] = |Dx2 | = 2 mod [1] = |Dx3 | = 2", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "[1]", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4]", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5]", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6]", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] 0 0 0 5 0 0 1 3 0 1 0 4 0 1 1 7 1 0 0 5 1 0 1 4 1 1 0 5 1 1 1 6 Tik !ik x1 x2", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1]", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2]", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] 0 0 3 0 1 4 1 0 4 1 1 5 Tik Tid", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Proof The correctness and completeness of GPU-(D)BE(w\u2217) follow from the correctness and completeness of BE [15] and DPOP [47], and Theorem 1.", "startOffset": 107, "endOffset": 111}, {"referenceID": 46, "context": "Proof The correctness and completeness of GPU-(D)BE(w\u2217) follow from the correctness and completeness of BE [15] and DPOP [47], and Theorem 1.", "startOffset": 121, "endOffset": 125}, {"referenceID": 56, "context": "Performance of the centralized algorithms are evaluated using the algorithms\u2019 wallclock runtime, while the performance of distributed algorithms are evaluated using the simulated runtime metric [57].", "startOffset": 194, "endOffset": 198}, {"referenceID": 2, "context": "Additionally, to compare the quality of the solution bounds reported by the incomplete algorithms, we also report the best solution quality found within the given time limits by toulbar2 [3], an optimized, exact centralized solver for WCSPs.", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "\u2022 Scale-free: We create an n-node network based on the Barabasi-Albert model [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 14, "context": "Next, we compare our centralized and distributed versions of GpuBE with MBE [15] and ADPOP [47] at varying of the mini-bucket size z \u2208 {2, .", "startOffset": 76, "endOffset": 80}, {"referenceID": 46, "context": "Next, we compare our centralized and distributed versions of GpuBE with MBE [15] and ADPOP [47] at varying of the mini-bucket size z \u2208 {2, .", "startOffset": 91, "endOffset": 95}, {"referenceID": 39, "context": ", in the Spot benchmark), we observe that toulbar2 ran out of time for the majority of the instances, while our GpuBE(z) can quickly find lower bounds, which could be used in a AND-and-OR search type as proposed by Marinescu and Dechter [40].", "startOffset": 237, "endOffset": 241}, {"referenceID": 34, "context": "[35] proposed a multi-GPU implementation of the simplex tableau algorithm that relies on a vertical problem decomposition to reduce communication between GPUs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "In constraint programming, Arbelaez and Codognet [5] proposed a GPU-based version of the Adaptive Search algorithm, which explores several large neighborhoods in parallel, resulting in a speedup factor of 17.", "startOffset": 49, "endOffset": 52}, {"referenceID": 11, "context": "[12] proposed a GPUbased framework that exploits both parallel propagation and parallel exploration of several large neighborhoods using local search techniques, leading to a speedup factor of up to 38.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] proposed the use of GPUs to compute the classical DP recursion step for the knapsack problem, which led to a speedup factor of 26.", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "[43] pre-", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] study the parallelization of an inference-based algorithm to solve COPs using GPUs, albeit exclusively in the centralized case.", "startOffset": 0, "endOffset": 3}, {"referenceID": 54, "context": "[55] study a GPU-based kernel for the sum-product operations that arise in marginalize a product", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] proposed a multi-variable agent decomposition strategy to solve general DCOPs with complex local subproblems, which makes use of GPUs to implement a search-based and a sampling-based algorithm to speed up the agents\u2019 local subproblems resolution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] studied a GPU accelerated algorithm in the context of stochastic DCOPs\u2014 DCOPs where the values of the cost tables are stochastic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Carlo (MCMC) sampling algorithms has been proposed in the context of solving DCOPs [27], where the authors adopted GPUs to accelerate the computation of the normalization constants used in the MCMC sampling process as well as to compute several samples in parallel, resulting in a speedup of up to one order of magnitude.", "startOffset": 83, "endOffset": 87}, {"referenceID": 41, "context": ", path consistency [42], adaptive con-", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "sistency [18], or the more recently proposed branch consistency for DCOPs [25]), especially when the cost functions need to be represented explicitly.", "startOffset": 9, "endOffset": 13}, {"referenceID": 24, "context": "sistency [18], or the more recently proposed branch consistency for DCOPs [25]), especially when the cost functions need to be represented explicitly.", "startOffset": 74, "endOffset": 78}], "year": 2017, "abstractText": "Discrete optimization is a central problem in artificial intelligence. The optimization of the aggregated cost of a network of cost functions arises in a variety of problems including (W)CSP, DCOP, as well as optimization in stochastic variants such as Bayesian networks. Inference-based algorithms are powerful techniques for solving discrete optimization problems, which can be used independently or in combination with other techniques. However, their applicability is often limited by their compute intensive nature and their space requirements. This paper proposes the design and implementation of a novel inference-based technique, which exploits modern massively parallel architectures, such as those found in Graphical Processing Units (GPUs), to speed up the resolution of exact and approximated inference-based algorithms for discrete optimization. The paper studies the proposed algorithm in both centralized and distributed optimization contexts. The paper demonstrates that the use of GPUs provides significant advantages in terms of runtime and scalability, achieving up to two orders of magnitude in speedups and showing a considerable reduction in execution time (up to 345 times faster) with respect to a sequential version.", "creator": "LaTeX with hyperref package"}}}