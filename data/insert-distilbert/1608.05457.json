{"id": "1608.05457", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "abstract": "we finally have constructed a new \" who - did - ya what \" dataset of over 200, 000 fill - in - the - gap ( cloze ) multiple choice reading comprehension problems constructed from the ldc english gigaword packard newswire corpus. the wdw dataset often has a variety of novel features. first, in contrast with the cnn and ibm daily mail datasets ( hermann et al., 2015 ) we avoid using article summaries for question formation. instead, covering each problem is formed from two explicitly independent articles - - - an article given as the passage to be read and a separate article on documenting the same events used to form the question. second, we avoid anonymization - - - each choice is a person named entity. third, the problems have been filtered to remove a fraction that are easily solved automatically by providing simple baselines, while remaining 84 % solvable by humans. we report performance benchmarks of standard systems and propose the wdw dataset as a challenge task for the community.", "histories": [["v1", "Fri, 19 Aug 2016 00:13:10 GMT  (27kb)", "http://arxiv.org/abs/1608.05457v1", "To appear at EMNLP 2016. Our dataset is available at tticnlp.github.io/who_did_what"]], "COMMENTS": "To appear at EMNLP 2016. Our dataset is available at tticnlp.github.io/who_did_what", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["takeshi onishi", "hai wang", "mohit bansal", "kevin gimpel", "david a mcallester"], "accepted": true, "id": "1608.05457"}, "pdf": {"name": "1608.05457.pdf", "metadata": {"source": "CRF", "title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "authors": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal Kevin Gimpel", "David McAllester"], "emails": ["tonishi@ttic.edu", "haiwang@ttic.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "mcallester@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 8.\n05 45\n7v 1\n[ cs\n.C L\n] 1\n9 A"}, {"heading": "1 Introduction", "text": "Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016). Reading comprehension is more difficult than knowledge-based or IR-based question answering in two ways. First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase (Bollacker et al., 2008) or the Google Knowledge Graph (Singhal, 2012).\n1Available at tticnlp.github.io/who_did_what\nSecond, machine comprehension systems cannot exploit the large level of redundancy present on the web to find statements that provide a strong syntactic match to the question (Yang et al., 2015). In contrast, a machine comprehension system must use the single phrasing in the given passage, which may be a poor syntactic match to the question.\nIn this paper, we describe the construction of a new reading comprehension dataset that we refer to as \u201cWho-did-What\u201d. Two typical examples are shown in Table 1.2 The process of forming a problem starts with the selection of a question article from the English Gigaword corpus. The question is formed by deleting a person named entity from the first sentence of the question article. An information retrieval system is then used to select a passage with high overlap with the first sentence of the question article, and an answer choice list is generated from the person named entities in the passage.\nOur dataset differs from the CNN and Daily Mail comprehension tasks (Hermann et al., 2015) in that it forms questions from two distinct articles rather than summary points. This allows problems to be derived from document collections that do not contain manually-written summaries. This also reduces the syntactic similarity between the question and the relevant sentences in the passage, increasing the need for deeper semantic analysis.\nTo make the dataset more challenging we selectively remove problems so as to suppress four simple baselines \u2014 selecting the most mentioned person,\n2The passages here only show certain salient portions of the passage. In the actual dataset, the entire article is given. The correct answers are (3) and (2).\nthe first mentioned person, and two language model baselines. This is also intended to produce problems requiring deeper semantic analysis.\nThe resulting dataset yields a larger gap between human and machine performance than existing ones. Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN (Chen et al., 2016) and 82% for the CBT named entities task (Hill et al., 2016). In spite of this higher level of human performance, various existing readers perform significantly worse on our dataset than they do on the CNN dataset. For example, the Attentive Reader (Hermann et al., 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al., 2016) achieves 70% on CNN but only 59% on Whodid-What.\nIn summary, we believe that our Who-did-What dataset is more challenging, and requires deeper semantic analysis, than existing datasets."}, {"heading": "2 Related Work", "text": "Our Who-did-What dataset is related to several recently developed datasets for machine comprehension. The MCTest dataset (Richardson et al., 2013) consists of 660 fictional stories with 4 multiple choice questions each. This dataset is too small to train systems for the general problem of reading\ncomprehension.\nThe bAbI synthetic question answering dataset (Weston et al., 2016) contains passages describing a series of actions in a simulation followed by a question. For this synthetic data a logical algorithm can be written to solve the problems exactly (and, in fact, is used to generate ground truth answers).\nThe Children\u2019s Book Test (CBT) dataset, created by Hill et al. (2016), consists of 113,719 cloze-style named entity problems. Each problem consists of 20 consecutive sentences from a children\u2019s story, a 21st sentence in which a word has been deleted, and a list of ten choices for the deleted word. The CBT dataset tests story completion rather than reading comprehension. The next event in a story is often not determined \u2014 surprises arise. This may explain why human performance is lower for CBT than for our dataset \u2014 82% for CBT vs. 84% for Who-did-What. The 16% error rate for humans on Who-did-What seems to be largely due to noise in problem formation introduced by errors in named entity recognition and parsing. Reducing this noise in future versions of the dataset should significantly improve human performance. Another difference compared to CBT is that Who-did-What has shorter choice lists on average. Random guessing achieves only 10% on CBT but 32% on Who-did-What. The reduction in the number of choices seems likely to be responsi-\nble for the higher performance of an LSTM system on Who-did-What \u2013 contextual LSTMs (the attentive reader of Hermann et al., 2015) improve from 44% on CBT (as reported by Hill et al., 2016) to 55% on Who-did-What.\nAbove we referenced the comprehension datasets created from CNN and Daily Mail articles by Hermann et al. (2015). The CNN and Daily Mail datasets together consist of 1.4 million questions constructed from approximately 300,000 articles. Of existing datasets, these are the most similar to Who-did-What in that they consists of cloze-style question answering problems derived from news articles. As discussed in Section 1, our Who-did-What dataset differs from these datasets in not being derived from article summaries, in using baseline suppression, and in yielding a larger gap between machine and human performance. The Who-did-What dataset also differs in that the person named entities are not anonymized, permitting the use of external resources to improve performance while remaining difficult for language models due to suppression."}, {"heading": "3 Dataset Construction", "text": "We now describe the construction of our Who-didWhat dataset in more detail. We sketch the procedure below and provide more specific details in the appendix. To generate a problem we first generate the question by selecting a random article \u2014 the \u201cquestion article\u201d \u2014 from the Gigaword corpus and taking the first sentence of that article \u2014 the \u201cquestion sentence\u201d \u2014 as the source of the cloze question. The hope is that the first sentence of an article contains prominent people and events which are likely to be discussed in other independent articles. To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system (Finkel et al., 2005) and parse the sentence using the Stanford PCFG parser (Klein and Manning, 2003).\nThe person named entities are candidates for deletion to create a cloze problem. For each person named entity we then identify a noun phrase in the automatic parse that is headed by that person. For example, if the question sentence is \u201cPresident Obama met yesterday with Apple Founder Steve Jobs\u201d we identify the two person noun\nphrases \u201cPresident Obama\u201d and \u201cApple Founder Steve Jobs\u201d. When a person named entity is selected for deletion, the entire noun phrase is deleted. For example, when deleting the second named entity, we get \u201cPresident Obama met yesterday with XXX\u201d rather than \u201cPresident Obama met yesterday with Apple founder XXX\u201d. This increases the difficulty of the problems because systems cannot rely on descriptors and other local contextual cues. About 700,000 question sentences are generated from Gigaword articles (8% of the total number of articles).\nOnce a cloze question has been formed we select an appropriate article as a passage. The article should be independent of the question article but should discuss the people and events mentioned in the question sentence. To find a passage we search the Gigaword dataset using the Apache Lucene information retrieval system (McCandless et al., 2010), using the question sentence as the query. The named entity to be deleted is included in the query and required to be included in the returned article. We also restrict the search to articles published within two weeks of the date of the question article. Articles containing sentences too similar to the question in word overlap and phrase matching near the blanked phrase are removed. We select the best matching article satisfying our constraints. If no such article can be found, we abort the process and move on to a new question. See the appendix for details.\nGiven a question and a passage we next form the list of choices. We collect all person named entities in the passage except unblanked person named entities in the question. Choices that are subsets of longer choices are eliminated. For example the choice \u201cObama\u201d would be eliminated if the list also contains \u201cBarack Obama\u201d. We also discard ambiguous cases where a part of a blanked NE appears in multiple candidate answers, e.g., if a passage has \u201cBill Clinton\u201d and \u201cHillary Clinton\u201d and the blanked phrase is \u201cClinton\u201d. We found this simple coreference rule to work well in practice since news articles usually employ full names for initial mentions of persons. If the resulting choice list contains fewer than two or more than five choices, the process is aborted and we move on to a new question.3\n3The maximum of five helps to avoid sports articles contain-\nAfter forming an initial set of problems we then remove \u201cduplicated\u201d problems. Duplication arises because Gigaword contains many copies of the same article or articles where one is clearly an edited version of another. Our duplication-removal process ensures that no two problems have very similar questions. Here, similarity is defined as the ratio of the size of the bag of words intersection to the size of the smaller bag.\nIn order to focus our dataset on the most interesting problems, we remove some problems to suppress the performance of the following simple baselines:\n\u2022 First person in passage: Select the person that appears first in the passage.\n\u2022 Most frequent person: Select the most frequent person in the passage.\n\u2022 n-gram: Select the most likely answer to fill the blank under a 5-gram language model trained on Gigaword minus articles which are too similar to one of the questions in word overlap and phrase matching.\n\u2022 Unigram: Select the most frequent last name using the unigram counts from the 5-gram model.\nTo minimize the number of questions removed we solve an optimization problem defined by limiting the performance of each baseline to a specified target value while removing as few problems as possible, i.e.,\nmax \u03b1(C)\n\u2211\nC\u2208{0,1}|b|\n\u03b1(C)|T (C)| (1)\nsubject to\n\u2200i \u2211\nC:Ci=1\n\u03b1(C)|T (C)|\nN \u2264 k\nN = \u2211\nC\u2208{0,1}|b|\n\u03b1(C)|T (C)| (2)\nwhere T (C) is the subset of the questions solved by the subset C of the suppressed baselines, and \u03b1(C) is a keeping rate for question set T (C). Ci = 1 indicates i-th baseline is in the subset and |b| is a number of baselines. Then N is a total number of questions and k is an upper bound for the baselins after suppression. k is set to the random performance. The\ning structured lists of results.\nperformance of these baselines before and after suppression are shown in Table 2. The suppression removed 49.9% of the questions.\nTable 3 shows statistics of our dataset after suppression. We split the final dataset into train, validation, and test by taking the validation and test to be a random split of the most recent 20,000 problems as measured by question article date. In this way there is very little overlap in semantic subject matter between the training set and either validation or test. We also provide a larger \u201crelaxed\u201d training set formed by applying less baseline suppression (a larger value of k in the optimization). The relaxed training set then has a slightly different distribution from the train, validation, and test sets which are all fully suppressed."}, {"heading": "4 Performance Benchmarks", "text": "We report the performance of several systems to characterize our dataset:\n\u2022 Word overlap: Select the choice c inserted to the question q which is the most similar to any sentence s in the passage, i.e., CosSim(bag(c + q),bag(s)).\n\u2022 Sliding window and Distance baselines (and their combination) from Richardson et al. (2013).\n\u2022 Semantic features: NLP feature based system from Wang et al. (2015).\n\u2022 Attentive Reader: LSTM with attention mechanism (Hermann et al., 2015).\n\u2022 Stanford Reader: An attentive reader modified with a bilinear term (Chen et al., 2016).\n\u2022 Attention Sum (AS) Reader: GRU with a pointattention mechanism (Kadlec et al., 2016).\n\u2022 Gated-Attention (GA) Reader: Attention Sum Reader with gated layers (Dhingra et al., 2016).\nTable 4 shows the performance of each system on the test data. For the Attention and Stanford Readers, we anonymized the Who-did-What data by replacing named entities with entity IDs as in the CNN and Daily Mail datasets.\nWe see consistent reductions in accuracy when moving from CNN to our dataset. The Attentive and Stanford Reader drop by up to 10% and the AS and GA reader drop by up to 17%. The ranking of the systems also changes. In contrast to the Attentive/Stanford readers, the AS/GA readers explicitly leverage the frequency of the answer in the passage, a heuristic which appears beneficial for the CNN and Daily Mail tasks. Our suppression of the mostfrequent-person baseline appears to more strongly affect the performance of these latter systems."}, {"heading": "5 Conclusion", "text": "We presented a large-scale person-centered cloze dataset whose scalability and flexibility is suitable for neural methods. This dataset is different in a variety of ways from existing large-scale cloze datasets and provides a significant extension to the training and test data for machine comprehension."}, {"heading": "6 Appendix", "text": "We include pseudocode for generating questions (Alg. 1) and multiple choice answer sets (Alg. 2).\nData: an article A Result: either null , if no question can be\nformed from A, or a cloze question q and the deleted person named entity (NE) t.\ns \u21d0 the first sentence of the article A. if not 10 \u2264 |s| \u2264 120 then return null E \u21d0 The set of person NEs e in s such that e contains no more than three words. Named entities sharing a word with an earlier named entity are deleted. if |E| < 2 then return null T \u21d0 constituent parse tree of s for e \u2208 E starting from the end of s do\nb \u21d0 The node in T for person NE e. while b.category \u2208 {NP, NNP, NNPS} and b.head = e and no element of b.descendant has category SBAR do\nb \u21d0 b.parent end if no element of b.descendant has head word \u201cand\u201d then\nreturn (q, e) where q is the cloze question formed from deleting b from s.\nend end return null\nAlgorithm 1: Question Formation. Named entities are recognized by the Stanford NER system and parse trees are generated by the Stanford PCFG parser. Here X.category is the syntactic category of parse node X, X.parent is the parent-node of the node X, X.descendant is the set of descendants of X and X.head is the head word of X.\nData: a pair (q, e) returned by Algorithm 1. Result: either null , if no appropriate passage can be\nfound, or a passage a and multiple choice answer set C.\np \u21d0 null for a \u2208 RankedArticles(q, e) do\nC \u21d0 The set of person NEs in a different from e and not in q. Named entities appearing as sub-part of an earlier named entity are deleted. if 2 \u2264 |E| \u2264 5 then return (a, C)\nend return null\nRankedArticles(q, e){ Ar \u21d0 \u2205 for t \u2208 {1, 3, 7, 14} do\nA \u21d0 Articles(q, e, t) for a \u2208 A do\nif isValid(a, q) then Ar \u21d0 Ar followed by a\nend end\nend return Ar}\nArticles(q, e, t){ Result: articles containing the person NE e,\npublished within t days of the article from which q was taken, and ranked by Apache Lucene.}\nisValid(a, q){ a is a valid passage for q if the following hold:\n\u2022 no sentence in a shares more than 78% of its words with the question q. \u2022 no sentence in a contains the sequence of five words to the left of the blank in q, and similarly for the sequence to the right. \u2022 a contains at least one of the person NEs in q. (All person NEs in q are different from e. Two named entities are considered the same if they share some words.) }\nAlgorithm 2: Passage Selection"}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A thorough examination of the CNN/Daily Mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "Proceedings of the", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "2016), and result marked IV is from (Dhingra", "author": ["Kadlec"], "venue": null, "citeRegEx": "Kadlec,? \\Q2016\\E", "shortCiteRegEx": "Kadlec", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov."], "venue": "CoRR, abs/1606.01549.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1684\u20131692.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The Goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of International Conference on Learning Representations.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ond\u0159ej Bajgar", "Jan Kleindienst."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 908\u2013918.", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Introducing the knowledge graph", "author": ["Amit Singhal"], "venue": null, "citeRegEx": "Singhal.,? \\Q2012\\E", "shortCiteRegEx": "Singhal.", "year": 2012}, {"title": "Towards AI-complete ques", "author": ["Tomas Mikolov"], "venue": null, "citeRegEx": "Mikolov.,? \\Q2016\\E", "shortCiteRegEx": "Mikolov.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation.", "startOffset": 56, "endOffset": 78}, {"referenceID": 5, "context": "Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016).", "startOffset": 111, "endOffset": 152}, {"referenceID": 6, "context": "Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016).", "startOffset": 111, "endOffset": 152}, {"referenceID": 0, "context": "First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase (Bollacker et al., 2008) or the Google Knowledge Graph (Singhal, 2012).", "startOffset": 148, "endOffset": 172}, {"referenceID": 8, "context": ", 2008) or the Google Knowledge Graph (Singhal, 2012).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "Our dataset differs from the CNN and Daily Mail comprehension tasks (Hermann et al., 2015) in that it forms questions from two distinct articles rather than summary points.", "startOffset": 68, "endOffset": 90}, {"referenceID": 1, "context": "Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN (Chen et al., 2016) and 82% for the CBT named entities task (Hill et al.", "startOffset": 109, "endOffset": 128}, {"referenceID": 6, "context": ", 2016) and 82% for the CBT named entities task (Hill et al., 2016).", "startOffset": 48, "endOffset": 67}, {"referenceID": 5, "context": "For example, the Attentive Reader (Hermann et al., 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al.", "startOffset": 34, "endOffset": 56}, {"referenceID": 7, "context": ", 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al., 2016) achieves 70% on CNN but only 59% on Whodid-What.", "startOffset": 85, "endOffset": 106}, {"referenceID": 6, "context": "The Children\u2019s Book Test (CBT) dataset, created by Hill et al. (2016), consists of 113,719 cloze-style named entity problems.", "startOffset": 51, "endOffset": 70}, {"referenceID": 5, "context": "Above we referenced the comprehension datasets created from CNN and Daily Mail articles by Hermann et al. (2015). The CNN and Daily Mail datasets together consist of 1.", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system (Finkel et al., 2005) and parse the sentence using the Stanford PCFG parser (Klein and Manning, 2003).", "startOffset": 116, "endOffset": 137}, {"referenceID": 5, "context": "\u2022 Attentive Reader: LSTM with attention mechanism (Hermann et al., 2015).", "startOffset": 50, "endOffset": 72}, {"referenceID": 1, "context": "\u2022 Stanford Reader: An attentive reader modified with a bilinear term (Chen et al., 2016).", "startOffset": 69, "endOffset": 88}, {"referenceID": 7, "context": "\u2022 Attention Sum (AS) Reader: GRU with a pointattention mechanism (Kadlec et al., 2016).", "startOffset": 65, "endOffset": 86}, {"referenceID": 3, "context": "\u2022 Gated-Attention (GA) Reader: Attention Sum Reader with gated layers (Dhingra et al., 2016).", "startOffset": 70, "endOffset": 92}], "year": 2016, "abstractText": "We have constructed a new \u201cWho-did-What\u201d dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles \u2014 an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization \u2014 each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.1", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}