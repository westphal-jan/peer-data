{"id": "1608.06027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "recurrent neural nets are widely used for predicting temporal data. their inherent deep feedforward structure allows learning about complex sequential patterns. it is believed that top - down feedback might be an critically important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. in this paper we introduce surprisal - driven recurrent numerical networks, which take into mind account past error information when making new predictions. this objective is achieved globally by sequential continuously monitoring comparing the discrepancy between early most recent predictions and the actual observations. furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving nearly 1. 39 bpc.", "histories": [["v1", "Mon, 22 Aug 2016 01:42:45 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v1", "v1, ICLR format"], ["v2", "Fri, 2 Sep 2016 17:26:02 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v2", "v2, ICLR format, added new results (feedback + zoneout)"], ["v3", "Mon, 5 Sep 2016 04:42:02 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v3", "ICLR format, added new results (feedback + zoneout)"], ["v4", "Wed, 19 Oct 2016 04:32:46 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v4", "ICLR 2017 submission, fixed some equations"]], "COMMENTS": "v1, ICLR format", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kamil m rocki"], "accepted": false, "id": "1608.06027"}, "pdf": {"name": "1608.06027.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kamil Rocki"], "emails": ["kmrocki@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Based on human performance on the same task, it is believed that an important ingredient which is missing in state-of-the-art variants of recurrent networks is top-down feedback. Despite evidence of its existence, it is not entirely clear how mammalian brain might implement such a mechanism. It is important to understand what kind of top-down interaction contributes to improved prediction capability in order to tackle more challenging AI problems requiring interpretation of deeper contextual information. Furthermore, it might provide clues as what makes human cognitive abilities so unique. Existing approaches which consider top-down feedback in neural networks are primarily focused on stacked layers of neurons, where higher-level representations constitute a top-down signal source. In this paper, we propose that the discrepancy between most recent predictions and observations might be effectively used as a feedback signal affecting further predictions. It is very common to use such a discrepancy during learning phase as the error which is subject to minimization, but not during inference. We show that is also possible to use such top-down signal without losing generality of the algorithm and that it improves generalization capabilities when applied to Long-Short Term Memory (Hochreiter & Schmidhuber, 1997) architecture. It is important to point out that the feedback idea presented here applies only to temporal data."}, {"heading": "1.1 SUMMARY OF CONTRIBUTIONS", "text": "The main contributions of this work are:\n\u2022 the introduction of a novel way of incorporating most recent misprediction measure as an additional input signal\n\u2022 extending state-of-the-art performance on character-level text modeling using Hutter Wikipedia dataset."}, {"heading": "1.2 RELATED WORK", "text": "There exist other approaches which attempted to introduce top-down input for improving predictions. One such architecture is Gated-Feedback RNN (Chung et al., 2015). An important difference between architecture proposed here and theirs is the source of the feedback signal. In GF-RNN it is assumed that there exist higher level representation layers and they constitute the feedback source.\nar X\niv :1\n60 8.\n06 02\n7v 1\n[ cs\n.L G\n] 2\n2 A\nug 2\n01 6\nOn the other hand, here, feedback depends directly on the discrepancy between past predictions and current observation and operates even within a single layer. Another related concept is Ladder Networks (Rasmus et al., 2015), where top-down connections contribute to improved semi-supervised learning performance."}, {"heading": "2 FEEDBACK: MISPREDICTION-DRIVEN PREDICTION", "text": ""}, {"heading": "2.1 NOTATION", "text": "The following notation is used throughout the section:\nx - inputs h - hidden units y - outputs p - output probabilities (normalized y) s - surprisal t - time step W - feedforward x\u2192 h connection matrix U - recurrent h\u2192 h connection matrix V - feedback s\u2192 h connection matrix S - truncated BPTT length M - number of inputs N - number of hidden units\n\u00b7 denotes matrix multiplication denotes elementwise multiplication \u03c3(\u00b7), tanh(\u00b7) - elementwise nonlinearities \u03b4x = \u2202E\u2202x\nIn case of LSTM, the following concatenated representations are used:\ngt =  itftot ut  b = b i bf bo bu U = U i Uf Uo Uu W = W i Wf Wo Wu V = V i Vf Vo Vu  (1)"}, {"heading": "2.2 SIMPLE RNN WITHOUT FEEDBACK", "text": "First, we show a simple recurrent neural network architecture without feedback which serves as a basis for demonstrating our approach. It is illustrated in Fig. 2 and formulated as follows:\nht = tanh(W \u00b7 xt + U \u00b7 ht\u22121 + b) (2)"}, {"heading": "2.3 FEEDBACK AUGMENTED RECURRENT NETWORKS", "text": "Figure 3 presents the main idea of surprisal-driven feedback in recurrent networks. In addition to feedforward and recurrent connections W and U , we added one additional matrix V . One more input signal, namely V \u00b7 st is being considered when updating hidden states of the network. We propose that the discrepancy st between most recent predictions pt\u22121 and observations xt might be effectively used as a feedback signal affecting further predictions. Such information is usually used during learning phase as an error signal, but not during inference. Our hypothesis is that it represents an important source of information which can be used during the inference phase, should be used and that it bring benefits in the form of improved generalization capability. Figure 1 presents examples of feedback signal being considered. Intuitively, when surprisal is near zero, the sum of input signals is the same as in a typical RNN. Next subsections provide mathematical description of the feedback architecture in terms of forward and backward passes for the Back Propagation Through Time (BPTT) (Werbos, 1990) algorithm."}, {"heading": "2.4 FORWARD PASS", "text": "Set h0, c0 to zero and p0 to uniform distribution or carry over the last state to emulate full BPTT.\n\u2200i, pi0 = 1\nM , i \u2208 {0, 1, ..,M \u2212 1}, t = 0 (3)\nfor t = 1:1:S-1\nI. Surprisal part\nst = \u2212 i\u2211 log pit\u22121 xit (4)\nIIa. Computing hidden activities, Simple RNN\nht = tanh(W \u00b7 xt + U \u00b7 ht\u22121 + V \u00b7 st + b) (5)\nIIb. Computing hidden activities, LSTM (to be used instead of IIa)\nft = \u03c3(Wf \u00b7 xt + Uf \u00b7 ht\u22121 + bf ) (6)\nit = \u03c3(W i \u00b7 xt + U i \u00b7 ht\u22121 + bi) (7)\not = \u03c3(Wo \u00b7 xt + Uo \u00b7 ht\u22121 + bo) (8)\nut = tanh(Wu \u00b7 xt + Uu \u00b7 ht\u22121 + bu) (9)\nct = (1\u2212 ft) ct\u22121 + it ut (10)\nc\u0302t = tanh(ct) (11)\nht = ot c\u0302t (12)\nIII. Outputs\nyit =Wy \u00b7 ht + by (13)\nSoftmax normalization\npit = ey i t\u2211i ey i t\n(14)"}, {"heading": "2.5 BACKWARD PASS", "text": "for t = S-1:-1:1\nI. Backprop through predictions\nBackprop through softmax, cross-entropy error, accumulate\n\u2202Et \u2202yt = \u2202Et \u2202yt + pt\u22121 \u2212 xt (15)\n\u03b4y \u2192 \u03b4Wy, \u03b4by \u2202E\n\u2202Wy =\n\u2202E\n\u2202Wy + hTt \u00b7 \u2202Et \u2202yt\n(16)\n\u2202E \u2202by = \u2202E \u2202by + M\u2211 i=1 \u2202Eit \u2202yit\n(17)\n\u03b4y \u2192 \u03b4h \u2202Et \u2202ht = \u2202Et \u2202ht + \u2202Et \u2202yt \u00b7WTy (18)\nIIa. Backprop through hidden nonlinearity (simple RNN version)\n\u2202Et \u2202ht = \u2202Et \u2202ht + \u2202Et \u2202ht tanh\u2032(ht) (19)\n\u2202Et \u2202gt = \u2202Et \u2202ht\n(20)\nIIb. Backprop through c, h, g (LSTM version)\nBackprop through memory cells, (keep gradients from the previous iteration)\n\u2202Et \u2202ct = \u2202Et \u2202ct + \u2202Et \u2202ht ot tanh\u2032(c\u0302t) (21)\nCarry error over to \u2202Et\u2202ct\u22121\n\u2202Et \u2202ct\u22121 = \u2202Et \u2202ct\u22121 + \u2202Et \u2202ct (1\u2212 ft) (22)\nPropagate error through the gates\n\u2202Et \u2202ot = \u2202Et \u2202ht c\u0302t \u03c3\u2032(ot) (23)\n\u2202Et \u2202it = \u2202Et \u2202ct ut \u03c3\u2032(it) (24)\n\u2202Et \u2202ft = \u2212\u2202Et \u2202ct ct\u22121 \u03c3\u2032(ft) (25)\n\u2202Et \u2202ut = \u2202Et \u2202ct it tanh\u2032(ut) (26)\nCarry error over to \u2202Et\u2202ht\u22121 \u2202Et \u2202ht\u22121 = \u2202Et \u2202gt \u00b7 UT (27)\nIII. Backprop through linearities\n\u2202Et \u2202b = \u2202Et \u2202b + N\u2211 i=1 \u2202Et \u2202git\n(28)\n\u2202E \u2202U = \u2202E \u2202U + hTt\u22121 \u00b7 \u2202Et \u2202gt\n(29)\n\u2202E \u2202W = \u2202E \u2202W + xTt \u00b7 \u2202Et \u2202gt\n(30)\n\u2202E \u2202x = \u2202E \u2202x + \u2202Et \u2202gt \u00b7WT (31)\nIV. Surprisal part\n\u2202E \u2202V = \u2202E \u2202V + sTt \u00b7 \u2202Et \u2202gt\n(32)\n\u2202E \u2202st = \u2202E \u2202gt \u00b7 V T (33)\n\u2202Et \u2202pt\u22121 = \u2202Et \u2202st xt (34)\nAdjust \u2202Et\u2202pt\u22121 according to the sum of gradients and carry over to \u2202Et \u2202yt\u22121\n\u2202Et \u2202yt\u22121 = \u2202Et \u2202pt\u22121 \u2212 pt\u22121 M\u2211 i=1 \u2202Et \u2202pit\u22121\n(35)"}, {"heading": "3 EXPERIMENTS", "text": "We ran experiments on the enwik8 dataset. It constitutes first 108 bytes of English Wikipedia dump (with all extra symbols present in XML), also known as Hutter Prize challenge dataset1. First 90% of each corpus was used for training, the next 5% for validation and the last 5% for reporting test accuracy. In each iteration sequences of length 10000 were randomly selected. The learning algorithm used was Adagrad1 with a learning rate of 0.001. Weights were initialized using so-called Xavier initialization Glorot & Bengio (2010). Sequence length for BPTT was 100 and batch size 128, states were carried over for the entire sequence of 10000 emulating full BPTT. Forget bias was set initially to 1. Other parameters set to zero. The algorithm was written in C++ and CUDA 8 and ran on GTX Titan GPU for up to 10 days. Table 1 presents results comparing existing state-of-theart approaches to the introduced Feedback LSTM algorithm which outperforms all other methods despite not having any regularizer."}, {"heading": "4 SUMMARY", "text": "We introduced feedback recurrent network architecture, which takes advantage of temporal nature of the data and monitors the discrepancy between predictions and observations. This prediction error information, also known as surprisal, is used when making new guesses. We showed that combining commonly used feedforward, recurrent and such feedback signals improves generalization capabilities of Long-Short Term Memory network. It outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction achieving 1.39 BPC.\n1with a modification taking into consideration only recent window of gradient updates 1http://mattmahoney.net/dc/text.html 2our implementation"}, {"heading": "5 FURTHER WORK", "text": "It is still an open question what the feedback should really constitute as well as how it should interact with lower-level neurons (additive, multiplicative or another type of connection). Further improvements may be possible with the addition of regularization. Another research direction is incorporating sparsity in order improve disentangling sources of variation in temporal data."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work has been supported in part by the Defense Advanced Research Projects Agency (DARPA)."}], "references": [{"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Semisupervised learning with ladder", "author": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "venue": "network. CoRR,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1606.06630,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent highway networks, 2016", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutnk", "Jrgen Schmidhuber"], "venue": null, "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "One such architecture is Gated-Feedback RNN (Chung et al., 2015).", "startOffset": 44, "endOffset": 64}, {"referenceID": 4, "context": "Another related concept is Ladder Networks (Rasmus et al., 2015), where top-down connections contribute to improved semi-supervised learning performance.", "startOffset": 43, "endOffset": 64}, {"referenceID": 7, "context": "Next subsections provide mathematical description of the feedback architecture in terms of forward and backward passes for the Back Propagation Through Time (BPTT) (Werbos, 1990) algorithm.", "startOffset": 164, "endOffset": 178}, {"referenceID": 6, "context": "BPC mRNN(Sutskever et al., 2011) 1.", "startOffset": 8, "endOffset": 32}, {"referenceID": 0, "context": "60 GF-RNN (Chung et al., 2015) 1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 3, "context": "58 Grid LSTM (Kalchbrenner et al., 2015) 1.", "startOffset": 13, "endOffset": 40}, {"referenceID": 8, "context": "45 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "44 Recurrent Highway Networks (Zilly et al., 2016) 1.", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "42 Array LSTM (Rocki, 2016) 1.", "startOffset": 14, "endOffset": 27}], "year": 2017, "abstractText": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.39 BPC.", "creator": "LaTeX with hyperref package"}}}