{"id": "1608.06608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Infinite-Label Learning with Semantic Output Codes", "abstract": "we develop a new statistical machine learning paradigm, named infinite - label learning, to annotate a data point with more than one relevant labels from a successful candidate set, which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels. now the infinite - label learning fundamentally expands roughly the scope of pure conventional multi - label learning, and better accurately models the practical requirements in various real - world applications, such as image tagging, ads - query association, and article categorization. however, how can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? to answer the question, we next seek some clues from the recent work on zero - shot learning, where the key digit is drawn to represent a class / label by a vector of semantic codes, as opposed to treating them as atomic labels. we validate the infinite - label learning task by defining a pac bound in theory and some empirical studies on both synthetic and real data.", "histories": [["v1", "Tue, 23 Aug 2016 19:14:47 GMT  (257kb,D)", "http://arxiv.org/abs/1608.06608v1", null], ["v2", "Wed, 18 Oct 2017 02:47:58 GMT  (322kb,D)", "http://arxiv.org/abs/1608.06608v2", null], ["v3", "Sat, 21 Oct 2017 00:56:08 GMT  (321kb,D)", "http://arxiv.org/abs/1608.06608v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang zhang", "rupam acharyya", "ji liu", "boqing gong"], "accepted": false, "id": "1608.06608"}, "pdf": {"name": "1608.06608.pdf", "metadata": {"source": "CRF", "title": "Infinite-Label Learning with Semantic Output Codes", "authors": ["Yang Zhang", "Rupam Acharyya"], "emails": ["yangzhang@knigths.ucf.edu", "racharyy@cs.rochester.edu", "jliu@cs.rochester.edu", "bgong@crcv.ucf.edu"], "sections": [{"heading": null, "text": "ar X\niv :1"}, {"heading": "1 Introduction", "text": "We formalize a new machine learning paradigm, infinite-label learning, where the goal is to learn a labeling function to annotate a data point with all relevant labels from a candidate set, which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels. Infinite-labeling learning is motivated by a plethora of real-world applications. As below, we give a few examples to facilitate our further discussion.\nImage tagging. More than 50% of the Flickr images have no text tags and are never retrieved for text queries. As a result, automatically suggesting relevant tags/labels for the images is a pressing need and yet a daunting challenge; there are more than 53M tags on Flickr. Moreover, tons of new tags emerge over time.\nAds-query association. Search engines allow advertisers to bid on user queries to deliver ads to targeted audience. It is both commercially intriguing and scientifically interesting to automatically associate the virtually infinite number of user queries (labels) to the ads (data points).\nText categorization. The wikipedia pages are grouped by more than 200K categories. A text might be about any of science, politics, finance, and/or other topics. Infinite-label learning can save the writers from the tedious process of manually classifying the future wikipedia pages, even if new categories are added to wikipedia.\nThe above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3]. However, we contend that the conventional multi-label learning tackles the problems on the surface, and fails to consider any new labels that appear in the test stage and were omitted from the training stage. In a sharp contrast, infinite-label learning fundamentally expands the scope of multi -label learning; it is expected to explicitly handle novel labels that show up after the labeling function has been learned.\nHow can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels. We can thus learn a labeling function to take as input a data point and a label\u2019s semantic codes, and output whether or not the encoded label is relevant to the data. Both seen and unseen labels are encoded by the same mechanism and usually by the same knowledge base. As a result, the labeling function learned from a finite training set is able to extrapolate to the labels unseen during the training. To verify this intuition, we provide a PAC bound (Section 3) and some empirical studies (Section 4) on both synthetic and real data.\nThe semantic codes of the class labels are often derived from a large knowledge base in addition to the training set. Thanks to the studies in linguistics, words in speech recognition are represented by combinations of phonemes [22]. In visual object recognition, a class is described by a set of visual attributes [10, 7]. More recently, the distributed representations of English words [12, 16] have found their applications in a variety of tasks.\nWe organize the remaining of the paper as follows. Section 2 formally states the problem of infinite-label learning and contrasts it to some related work. A PAC bound and some theoretical analyses are presented in Section 3. In Section 4, we experimentally study the proposed infinite-label learning and reveal some of its empirical properties. Section 5 concludes the paper."}, {"heading": "2 Problem statement, modeling, and algorithmic solutions", "text": "In this section, we formally state the problem of infinite-label learning, followed by a modeling assumption about its corresponding data generation process. We also provide some exemplar algorithmic solutions. Finally, we contrast infinite-label learning to the closely related multi-label learning and zero-shot learning."}, {"heading": "2.1 Problem statement", "text": "Suppose we have a set of semantic codes of the labels L = {\u03bbl \u2208 Rn}Ll=1 and a training sample S = {(xm,ym) \u2208 Rd \u00d7 {\u22121,+1}L}Mm=1, where the annotation yml = +1 indicates that the l-th label, which is semantically encoded as \u03bbl, is relevant to the m-th data point xm. The semantic codes could be the distributed representations of English words, phoneme composition of words in speech, co-occurrence vector, or visual attributes of objects. For brevity, we refer to the semantic codes of the labels L = {\u03bbl}Ll=1 as labels in the rest of the paper. Denote by U = {\u03bbl}l>L another set of labels disjoint from L. The labels in U correspond to no data points in the training sample and thus often termed zero-shot labels [10, 24]. The objective of infinite-label learning is to infer a labeling function h : Rd 7\u2192 2L\u222aU from the training sample S and observed labels L, such that h(x) \u2286 L \u222a U assigns all the relevant labels to a test instance x."}, {"heading": "2.2 A modeling assumption", "text": "We consider the following distributions over the data in infinite-label learning,\nx \u223c PX , \u03bb \u223c P\u039b, y(\u03bb;x) \u223c PY |X\u039b, where PXY \u039b = PXP\u039bPY |X\u039b, (1)\nwhere PXY \u039b denotes the joint distribution of the data point x, label assignment indicators y, and the label \u03bb. In this paper, we consider a deterministic/noiseless label assignment function y(\u03bb;x) \u223c PY |X\u039b, and leave the more generic case to the future work.\nNote that we explicitly treat the labels L \u222a U as an i.i.d. sample drawn from the marginal distribution P\u039b. Moreover, we make the following observation: any data point x can be regarded as incurring a binary classification rule, denoted by y(\u00b7;x), over the label space, such that y(\u03bb;x) = +1 tells that the label \u03bb \u223c P\u039b is relevant to the data x and y(\u03bb;x) = \u22121 means the opposite. This rule is only partially observed at training in the form of the data annotations ym.\nImmediately following the above modeling assumption, our learning objective is to infer the (conditional) classification rule y(\u00b7;x) for the labels from the training sample S. We consider a fixed set of hypotheses H for the rule y(\u00b7;x), and then select a hypothesis h \u2208 H such that it gives rise to the smallest generalization risk,\nR(h) = Ex\u223cPXE\u03bb\u223cP\u039bEy\u223cPY |X\u039b [h(\u03bb;x) 6= y] (2)\nwhere [\u00b7] is the 0-1 loss. We provide a PAC bound in Section 3 for the risk along with its empirical estimate from the training set S,\nR\u0302(h) = 1\nML M\u2211 m=1 L\u2211 l=1 L(h(\u03bbl;xm), yml) + \u2126(h), (3)\nwhere L(\u00b7) is a 0-1 loss (in practice one uses a surrogate differentiable loss), and \u2126(h) is a regularization over h. At the high level, the risk of infinite-label learning can be understood as the traditional binary classification risk over the label \u03bb and assignment y|x, conditioning on the variable X representing the input data.\nIt is worth pointing out the implication of eq. (3): we employ a \u201cnon-canonical\u201d data sampling process from the underlying distribution PXY \u039b = PXP\u039bPY |X\u039b. Instead of sampling i.i.d. triplets {(xk,\u03bbk, yk)}k\u22651 from PXY \u039b, we draw from PX for M times to have {xm}Mm=1, draw from P\u039b for L times and obtain {\u03bbl}Ll=1. After that, we use the (deterministic) label assignment function y(\u03bb;x) to assign the binary (ir)relevance to all the pairwise combination of data and labels, i.e., we arrive at M\u00d7 L training data points {(xm,\u03bbl, yml)}m\u2208[M],l\u2208[L] and they are non-i.i.d."}, {"heading": "2.3 Algorithmic solutions", "text": "Given the modeling in Section 2.2, the solution to infinite-label learning boils down to how to specify the labeling function h(x) = {h(\u03bb;x)|\u03bb \u2208 L \u222a U}. We give some examples below, and will examine Example 1 both theoretically (Section 3) and empirically (Section 4).\nExample 1. Assuming that each label \u03bb \u2208 L \u222a U is represented by a vector (e.g., word vectors, attributes, or click-through statistics) and h(\u03bb;x) = sgn \u3008V x,\u03bb\u3009, we essentially try to find a hyper-plane V x for classifying either seen or unseen labels {\u03bb} by minimizing the empirical risk in eq. (3).\nExample 2. We can define a new hypothesis set H by replacing V x in Example 1 with a neural network nn(x), such that h(\u03bb;x) = sgn \u3008nn(x),\u03bb\u3009.\nExample 3. Consider h(\u03bb;x) = \u2211L\nl=1 \u03b1l(x)k(\u03bb,\u03bbl), where \u03b1l(\u00b7) takes as input a data point x and outputs a scalar and k(\u00b7, \u00b7) is a kernel function over the labels. The kernel is particularly useful when the semantic codes of the labels are in structured forms (e.g., trees, graphs)."}, {"heading": "2.4 Related work", "text": "The modeling of the data, label, and label assignment in infinite-label learning is in sharp contrast to the prevalent modeling assumptions for the traditional classification problems or the closely related multi-label learning problems, none of which induce the labels L and the label assignment y by separate distributions. We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1. Multi-label learning usually assumes that the training sample S = {(xm,ym)}Mm=1 is drawn i.i.d. from the joint distribution PXY , totally ignoring the distribution of the labels L. Consequently, the multi-label classifiers cannot deal with any unseen labels in U .\nWe can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17]. They both rely on an external knowledge base for encoding the classes/labels, in addition to the training set, in order to extrapolate the learned classifier or labeling function to handle previously unseen classes/labels. Their difference echos that between multi-label and multi-class classifications, namely, infinite-label learning assigns more than one relevant labels to a data point while under zero-shot learning a data point can belong to one and only one class.\nThe recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours. However, their label representations are inferred from the training set and are not applicable to unseen labels. Moreover, they lack the basic modeling of the labels as introduced in this paper, that the labels \u03bb are drawn i.i.d. from the distribution P\u039b. The most recent works [13, 24] can be regarded as specific instantiations of infinite-label learning, while we formally formulate the problem and provide both theoretical and empirical studies as follows. In addition, the pairwise learning [20] is a very interesting framework which encapsulates matrix completion, collaborative filtering, zero-shot and the infinite-label learning. As a result, our analyses apply to pairwise learning when yh is binary in [20]."}, {"heading": "3 Infinite-label learning under the PAC learning framework", "text": "In this section, we examine the theoretical properties of infinite-label learning under the PAC learning framework. Given the training sample S = {(xm,ym) \u2208 Rd\u00d7{\u22121,+1}L}Mm=1 and the semantic codes of the seen labels L = {\u03bbl \u2208 Rn}Ll=1, the theorem below sheds lights on the numbers of data points and labels that are necessary to achieve a particular level of error. The supplementary materials give tighter bounds under some sparse regularizations over the hypothesis set H.\nTheorem 1. For any \u03b4 > 0 and any h(x,\u03bb) \u2208 H := {sgn \u3008V x,\u03bb\u3009 | V \u2208 Rn\u00d7d}, the following holds with probability at least 1\u2212 \u03b4,\nExE\u03bbEy|x,\u03bb[h(x,\u03bb) 6= y] \u2264 1\nML M\u2211 m=1 L\u2211 l=1 [h(xm,\u03bbl) 6= ym,l]+\n2 max\n(\u221a 8 log(8/\u03b4) + 8d log(2eM/d)\nM ,\n\u221a 8 log(8M/\u03b4) + 8n log(2eL/n)\nL\n) . (4)\nThis generalization error bound is roughly O(1/M + 1/L) if all log terms are ignored. When M and L converge to infinity, the error vanishes. An immediate implication is the learnability of the infinite-label learning: to obtain a certain accuracy on all labels, one does not have to observe all of them in the training phase. Another interesting implication for practice is that M and L should be chosen in the same order, because M L (or M L) would not bring significant improvements to the generalization bound. Acute readers may notice that the classifier \u3008V x,\u03bb\u3009 can be cast into \u3008V,\u03bbx>\u3009, which is in the form of the vanilla linear classifier1. However, a fundamental difference from the linear classifier lies on that the sample triplets (xm,\u03bbl, yml) are not independent in the empirical risk (cf. eq. (3)), causing the main challenge in analysis.\n1Remark: We note that the bound is indeed close to that for the linear classifier except the term of log(M)/L. This term is likely an artifact of our proof and may be removed by finer analysis.\nProof. To prove the theorem, we essentially need to consider the following probability bound:\nP ( \u2203 h \u2208 H : ExE\u03bbEy|x,\u03bb[h(x,\u03bb) 6= y]\u2212 1\nML M\u2211 m=1 L\u2211 l=1 [h(xm,\u03bbl) 6= ym,l)] \u2265\n)\n=P ( \u2203 h \u2208 H : ExE\u03bbEy|x,\u03bb[h(x,\u03bb) 6= y]\u2212 1\nM M\u2211 m=1 E\u03bbEy|xm,\u03bb[h(xm,\u03bb) 6= y]\n+ 1\nM M\u2211 m=1 E\u03bbEy|x,\u03bb[h(xm,\u03bb) 6= y]\u2212 1 ML M\u2211 m=1 L\u2211 l=1 [h(xm,\u03bbl) 6= ym,l] \u2265\n)\n\u2264P ( \u2203 h \u2208 H : ExE\u03bbEy|x,\u03bb[h(x,\u03bb) 6= y]\u2212 1\nM M\u2211 m=1 E\u03bbEy|xm,\u03bb[h(xm,\u03bb) 6= y] \u2265 2\n) (5)\n+ P ( \u2203 h \u2208 H : 1\nM M\u2211 m=1 E\u03bbEy|xm,\u03bb[h(xm,\u03bb) 6= y]\u2212 1 ML \u2211 m,l [h(xm,\u03bbl) 6= ym,l] \u2265 2\n) (6)\nwhere the last inequality is due to the fact P(a+ b \u2265 ) \u2264 P(a \u2265 /2) +P(b \u2265 /2). Next we consider the bound for the two terms (5) and (6) above respectively.\nFor the first term (5), we have\nP ( \u2203h \u2208 H : Ex,\u03bb,y[h(x,\u03bb) 6= y]\u2212 1\nM M\u2211 m=1 E\u03bb,y|xm [h(xm,\u03bb) 6= y] \u2265 1\n)\n=P ( \u2203h \u2208 H : Ex,\u03bbEy|x,\u03bb[h(x,\u03bb) 6= y]\u2212 E\u03bb ( 1\nM M\u2211 m=1 Ey|xm,\u03bb[h(xm,\u03bb) 6= y]\n) \u2265 1 )\n\u2264P ( \u2203h \u2208 H,\u2203\u03bb \u2208 L : ExEy|x,\u03bb[h(x,\u03bb) 6= y]\u2212 ( 1\nM M\u2211 m=1 Ey|xm,\u03bb[h(xm,\u03bb) 6= y]\n) \u2265 1 ) (7)\nwhere the first equality is due to the dependence between \u03bb and xm. Denote Ey|xm,\u03bb[h(xm,\u03bb) 6= y] by a function f(xm). We have E(f(xm)) = ExEy|x,\u03bb[h(x,\u03bb) 6= y]. f(x1), \u00b7 \u00b7 \u00b7 , f(xm) are i.i.d. random variables in {0, 1} \u2014 recall that PY |X\u039b is deterministic. Define F to be a hypothesis space\nF := {f(x) = Ey|x,\u03bb[h(x,\u03bb) 6= y] | h \u2208 H, \u03bb \u2208 L}.\nUsing the new notation, we can rewrite (7) in the following\nP ( \u2203f \u2208 F : E(f(x))\u2212 1\nM M\u2211 m=1 f(xm) \u2265 1 ) \u2264 4r(F , 2M,X )\u00d7 exp(\u2212M 21/8) (8)\nwhere the inequality uses the Growth function bound, and r(F , 2M,X ) is number of maximally possible configurations (or values of {f(x1), \u00b7 \u00b7 \u00b7 , f(x2M)}) given 2M samples in X over the hypothesis space F . Since h(x,\u03bb) is in the form of sgn\u3008V x,\u03bb\u3009, r(F , 2M,X ) is bounded by r(W, 2M,X ) with W := {sgn(w>x) | w \u2208 Rd}. Hence, we have\nr(F , 2M,X ) \u2264 r(W, 2M,X ) \u2264 ( 2eM\nd\n)VC(Rd) = ( 2eM\nd\n)d (9)\nwhere VC(Rd) is the VC dimension for the space Rd. We then have\n(8) \u2264 4 ( 2eM\nd\n)d \u00d7 exp(\u2212M 21/8).\nBy letting the right hand side equal \u03b4/2, we have\n1 =\n\u221a 8 log 8/\u03b4 + 8d log(2eM/d)\nM . (10)\nNext we consider the upper bound for (6):\nP ( \u2203h \u2208 H : 1\nM M\u2211 m=1 E\u03bb,y|xm [h(xm,\u03bb) 6= y]\u2212 1 M M\u2211 m=1 1 L L\u2211 l=1 [h(xm,\u03bbl) 6= ym,l] \u2265 2\n) (11)\n=P ( \u2203h \u2208 H : 1\nM M\u2211 m=1 E\u03bbEy|xm,\u03bb[h(xm,\u03bb) 6= y]\u2212 1 M M\u2211 m=1 1 L L\u2211 l=1 [h(xm,\u03bbl) 6= ym,l] \u2265 2\n)\n\u2264P ( \u2203h \u2208 H, \u2203m \u2208 {1, \u00b7 \u00b7 \u00b7 ,M} : E\u03bbEy|xm,\u03bb[h(xm,\u03bb) 6= y]\u2212 1\nL L\u2211 l=1 [h(xm,\u03bbl) 6= ym,l] \u2265 2\n)\n\u2264M\u00d7 P ( \u2203h \u2208 H : Ey,\u03bb|x1 [h(x1,\u03bb) 6= y]\u2212 1\nL L\u2211 l=1 [h(x1,\u03bbl) 6= y1,l] \u2265 2\n) . (12)\nwhere the last inequality uses the union bound. Denote [h(x1,\u03bb) 6= y] by g(\u03bb, y) for short. Then we have E(g(\u03bb, y)) = Ey,\u03bb|x1 [h(x1,\u03bb) 6= y] and g(\u03bb1, y1,1), \u00b7 \u00b7 \u00b7 , g(\u03bbL, y1,L) are i.i.d. samples taking values from {0, 1} given x1. Define G to be a hypothesis space\nG := {g(\u03bb, y) = [h(\u03bb,x1) 6= y] | h \u2208 H}.\nThen we can cast the probabilistic factor in (12) into\nP ( \u2203g \u2208 G : E(g(\u03bb, y))\u2212 1\nL L\u2211 l=1 g(\u03bbl, yl) \u2265 2 ) \u2264 4r(G, 2L,L)\u00d7 exp(\u2212L 22/8) (13)\nwhere the last inequality uses the Growth function bound again. To bound r(G, 2L,L), we consider the structure of h(x,\u03bb) = sgn(V x,\u03bb) and define W \u2032 = {sgn(z>\u03bb) | z \u2208 Rn}. It suggests that\nr(G, 2L,L) \u2264 r(W \u2032, 2L,Rn) \u2264 ( 2eL\nn\n)VC(Rn) = ( 2eL\nn\n)n .\nHence we have (11) \u2264 4M\n( 2eL\nn\n)n \u00d7 exp(\u2212L 22/8).\nBy letting the right hand side equal \u03b4/2, we have\n2 =\n\u221a 8 log 8M/\u03b4 + 8n log(2eL/n)\nL . (14)\nPlugging (10) and (14) into (5) and (6) respectively, we prove the theorem."}, {"heading": "4 Empirical studies", "text": "In this section, we continue to investigate the properties of infinite-label learning. While the theory result in Section 3 justifies its \u201clearnability\u201d in some sort, there are many other questions of interest to explore for the practical use of infinite-label learning. We focus on the following two questions and provide some empirical insights using respectively synthetic data and real data.\n1. After we learn a labeling function from the training set, how many and what types of unseen labels can we confidently handle using this labeling function?\n2. What is the effect of varying the number of seen labels L, given a fixed union L \u222a U of seen and unseen labels? Namely, the same set of labels L \u222a U will be used at the test stage, but we learn different labeling functions by varying the seen labels L."}, {"heading": "4.1 Synthetic experiments", "text": "We generate some synthetic data to answer the first question. This offers us the flexibility to control the number of labels.\nData. We randomly sample 500 training data points {xm \u2208 R3}M=500m=1 and 1000 testing data points from a five-component Gaussian mixture model. We also sample 10 seen labels L = {\u03bbl \u2208 R2}L=10l=1 and additional 2990 unseen labels U = {\u03bb11, \u00b7 \u00b7 \u00b7 ,\u03bb3000} from a Gaussian distribution. Note that only the seen labels L are revealed during the training stage. As below specifies the distributions,\nx \u223c P (x) = 5\u2211\nk=1\n\u03c0kN ( \u00b5k,UkU T k , ) , \u03bb \u223c N ([ 2 3 ] , [ 1 1.5 1.5 3 ]) , x \u2208 R3, \u03bb \u2208 R2\nwhere the mixture weights \u03c0k are sampled from a Dirichlet distribution Dir(3, 3, 3, 3, 3), and both mean \u00b5k and Uk are sampled from a standard normal distribution (using the randn function in MATLAB). Finally, we generate a \u201cgroundtruth\u201d matrix V \u2208 R2\u00d73 from a standard normal distribution. The label assignments are thus given by yml = sgn \u3008V xm,\u03bbl\u3009 for both training and testing data and both seen and unseen labels. For the training set S = {(xm,ym)}M=500m=1 , we randomly flip the sign of each yml with probability p = 0.1.\nWe deliberately choose the low dimensions for the data and labels so that we can visualize them and have a concrete understanding about the results to be produced. Figure 1(a) and (b) show the data points and labels we have sampled. The training data and the seen labels are in red color, while all the other (test) data points and labels are unseen during training.\nGiven the training set S, we learn the model parameters by minimizing a hinge loss\nV\u0302 \u2190 arg min V\n1\nML M=500\u2211 m=1 L=10\u2211 l=1 max (1\u2212 yml \u3008V xm,\u03bbl\u3009 , 0) ,\nand then try to assign both seen and unseen labels L \u222a S to the 1000 test data points, using V\u0302 . We challenge the learned infinite-label assignment model yml = sgn \u2329 V\u0302 xm,\u03bbl \u232a by gradually increasing the difficulties. Namely, we rank all the labels according to their distances to the seen labels L, where the distance between \u03bbl and the seen labels L is calculated by min\u03bb\u2208L \u2016\u03bb\u2212 \u03bbl\u20162. We then\nevaluate the label assignment results given every 500 consecutive labels in the ranking list (as well as the 10 seen labels). Arguably, the last 500 labels, which are the furthest subset of labels from the seen labels L, impose the biggest challenge to the learned model.\nResults. Figure 1(c) shows the label assignment errors for different subsets of labels. We run 5 rounds of experiments each with different randomly sampled data, and report their results as well as the average. We borrow from multi-label classification [23] the Hamming loss as the evaluation metric. It is computed by 11000 \u22111000 m=1(|ym 6= zm|T1)/|ym|T1, where ym is the groundtruth label assignments to the data point xm and zm is the predictions. Note that this is inherently different from the classification accuracy/error used for evaluating multi-class classification.\nWe draw the following observations from Figure 1(c). First, infinite-label learning is feasible since we have obtained decent classification results for 3000 labels although only 10 of them are seen in training. Second, when the unseen labels are not far from the seen labels, the label assignment results are on par with the performance of assigning only the seen labels to the test data (cf. the Hamming losses over the first, second, and third 500-label subsets). Third, labels that are far from the seen labels may cause larger confusions to the infinite-label assignment model learned from finite seen labels. Increasing the number of seen labels and/or data points during training can improve the model\u2019s generalization capability to unseen labels."}, {"heading": "4.2 Image tagging", "text": "We experiment with image tagging to seek empirical answers to the second question raised at the beginning of this section.\nImage tagging is a real scenario that can benefit from the proposed work. It is often defined as ranking a set of labels for a query image. The main challenge in practice, however, is that the candidate set could be very large\u2014there are about 53M tags on Flickr. Under infinite-label learning, one does not need to see all the candidate tags during training; otherwise much tedious human efforts are required to tag the images. Instead, we can learn a labeling function from a small number of seen tags and then apply it to all the possible tags. In this case, how many seen tags should we use in order to achieve about the same results as using all the tags for training? This is exactly the second question we asked at the beginning of this section.\nData. We conduct our experiments using the NUS-WIDE dataset [6]. It has 269,648 images in total although we are only able to retrieve 223,821 images using the provided image URLs. Among them, 134,281 are training images and the rest are testing images, according to the split by NUS-WIDE. We further randomly choose 20% of the training images as a validation set. The image features x are l2-normalized VGGNet-19 [18] last fully-connected-layer activations.\nEach image in NUS-WIDE has been manually annotated with the relevant tags out of 81 candidate tags in total. We obtain their semantic codes by the pre-trained GloVe word vectors [16]. While all the 81 tags are considered at the test stage, we randomly choose L = 81 (100% out of the 81 tags), 73 (90% out of the 81 tags), 65 (80% out of 81), 57 (70% out of 81), 49 (60% out of 81), and 41 (50% out of 81) seen tags for training different labeling functions h(\u03bb;x) = sgn \u3008V x,\u03bb\u3009. Note that some training images would have no relevant tags under some settings; we simply drop them out from the training and validation sets.\nLearning and evaluation. Image tagging is often evaluated based on the top few tags returned by a system, assuming that users do not care about the remaining tags. We report the results measured by four popular metrics: Mean image Average Precision (MiAP) [11] and the top-3 precision, recall, and F1-score. Accordingly, in order to impose the ranking property to our labeling function, we learn it using the RankNet loss [5],\nV\u0302 \u2190 arg min V\n1\nM M\u2211 m=1\n1\n(L\u2212 Km)Km \u2211 k:ymk=+1 k\u0304:ymk\u0304=\u22121 log(1 + exp(\u3008V xm,\u03bbk\u0304\u3009 \u2212 \u3008V xm,\u03bbk\u3009)) + \u03b3 \u2016V \u20162 ,\nwhere Km is the number of relevant tags to the m-th image. The hyper-parameter \u03b3 is tuned by the MiAP results on the validation set for each experiment.\nBaselines. We compare our results to those of two zeroshot learning methods, respectively proposed by Norouzi et al. [14] and Akata et al. [1]. Both methods are developed for multi-class classification, namely, to classify an image to one and only one of the tags/classes. When we use them to solve the multi-label image tagging problem, different tags/classes will have duplicated images. In other words, they are naturally flawed in solving the infinite-label learning problem. We still include them as the references here, since there is no prior work to handle both multi-label learning and unseen labels at testing, to the best of our knowledge.\nResults. Figure 2 shows the MiAP results under different numbers of seen tags. We include the results evaluated by other metrics (the top-3 precision, recall, and F1-score) in the supplementary materials. Recall that although we train the labeling functions using different numbers of seen\ntags, the task at the testing stage remains the same for them: the labeling functions rank all the 81 tags for each test image (and return the top few as the suggested tags).\nWe can see that, without any surprise, the performance decreases as less seen tags are used for training. However, the decrease is fairly gentle\u2014the MiAP drops by 1% and 3% (absolutely) using from 100% to 90% and from 90% to 80% of the 81 tags for training, respectively. Besides, no matter under which experiment setting, the proposed infinite-label learning outperforms the zero-shot learning methods by a large margin, thanks to that it is a more appropriate solution to the image tagging problem. It is interesting to see that the gaps between infinite-label learning and zero-shot learning methods are smaller as the seen tags become less, likely because the overlapped images between different tags/classes are reduced and the tagging problem approaches multi-class classification in training."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a new machine learning paradigm which we term infinite-label learning. It fundamentally expands the scope of multi-label learning, in that at the testing stage, the learned labeling function can assign potentially an infinite number of relevant labels to a data point. Infinite-label learning is made feasible by representing the labels using semantic codes. We provide a full treatment to the infinite-label learning, including a new modeling assumption about the data generation process, a PAC bound, and empirical studies about its performance and properties.\nThere are many avenues for the future work to further explore infinite-label learning. We note that our current bound can be likely improved. Theoretical understanding about its performance under the MiAP evaluation is also necessary given that MiAP is prevalent in evaluating the multi-label results. One particularly interesting application of infinite-label learning is on the extreme multi-label classification problems [3].\nReferences Cited [1] Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output\nembeddings for fine-grained image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2927\u20132936, 2015.\n[2] Krishnakumar Balasubramanian and Guy Lebanon. The landmark selection method for multiple output prediction. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 983\u2013990, 2012.\n[3] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain. Sparse local embeddings for extreme multi-label classification. In Advances in Neural Information Processing Systems, pages 730\u2013738, 2015.\n[4] Wei Bi and James Kwok. Efficient multi-label classification with many labels. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 405\u2013413, 2013.\n[5] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pages 89\u201396. ACM, 2005.\n[6] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. Nus-wide: a real-world web image database from national university of singapore. In Proceedings of the ACM international conference on image and video retrieval, page 48. ACM, 2009.\n[7] Alireza Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing objects by their attributes. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1778\u20131785. IEEE, 2009.\n[8] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u20132129, 2013.\n[9] Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang. Multi-label prediction via compressed sensing. In NIPS, volume 22, pages 772\u2013780, 2009.\n[10] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zeroshot visual object categorization. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 36(3):453\u2013465, 2014.\n[11] Xirong Li, Tiberio Uricchio, Lamberto Ballan, Marco Bertini, Cees GM Snoek, and Alberto Del Bimbo. Socializing the semantic gap: A comparative survey on image tag assignment, refinement and retrieval. arXiv preprint arXiv:1503.08248, 2015.\n[12] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.\n[13] Jinseok Nam, Eneldo Loza Menc\u00eda, Hyunwoo J Kim, and Johannes F\u00fcrnkranz. Predicting unseen labels using label hierarchies in large-scale multi-label learning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 102\u2013118. Springer, 2015.\n[14] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic embeddings. arXiv preprint arXiv:1312.5650, 2013.\n[15] Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell. Zero-shot learning with semantic output codes. In Advances in neural information processing systems, pages 1410\u20131418, 2009.\n[16] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.\n[17] Bernardino Romera-Paredes and PHS Torr. An embarrassingly simple approach to zero-shot learning. In Proceedings of The 32nd International Conference on Machine Learning, pages 2152\u20132161, 2015.\n[18] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n[19] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning through cross-modal transfer. In Advances in neural information processing systems, pages 935\u2013943, 2013.\n[20] Michiel Stock, Tapio Pahikkala, Antti Airola, Bernard De Baets, and WillemWaegeman. Efficient pairwise learning using kernel ridge regression: an exact two-step method. arXiv preprint arXiv:1606.04275, 2016.\n[21] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667\u2013685. Springer, 2009.\n[22] Alex Waibel. Modular construction of time-delay neural networks for speech recognition. Neural computation, 1(1):39\u201346, 1989.\n[23] Min-Ling Zhang and Zhi-Hua Zhou. A review on multi-label learning algorithms. Knowledge and Data Engineering, IEEE Transactions on, 26(8):1819\u20131837, 2014.\n[24] Yang Zhang, Boqing Gong, and Mubarak Shah. Fast zero-shot image tagging. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.\n[25] Yi Zhang and Jeff G Schneider. Multi-label output codes using canonical correlation analysis. In International Conference on Artificial Intelligence and Statistics, pages 873\u2013882, 2011."}], "references": [{"title": "The landmark selection method for multiple output prediction", "author": ["Krishnakumar Balasubramanian", "Guy Lebanon"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Manik Varma", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Efficient multi-label classification with many labels", "author": ["Wei Bi", "James Kwok"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning to rank using gradient descent", "author": ["Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Nus-wide: a real-world web image database from national university of singapore", "author": ["Tat-Seng Chua", "Jinhui Tang", "Richang Hong", "Haojie Li", "Zhiping Luo", "Yantao Zheng"], "venue": "In Proceedings of the ACM international conference on image and video retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Describing objects by their attributes", "author": ["Alireza Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Multi-label prediction via compressed sensing", "author": ["Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Attribute-based classification for zeroshot visual object categorization", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Socializing the semantic gap: A comparative survey on image tag assignment, refinement and retrieval", "author": ["Xirong Li", "Tiberio Uricchio", "Lamberto Ballan", "Marco Bertini", "Cees GM Snoek", "Alberto Del Bimbo"], "venue": "arXiv preprint arXiv:1503.08248,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Predicting unseen labels using label hierarchies in large-scale multi-label learning", "author": ["Jinseok Nam", "Eneldo Loza Menc\u00eda", "Hyunwoo J Kim", "Johannes F\u00fcrnkranz"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Zero-shot learning with semantic output codes", "author": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey E Hinton", "Tom M Mitchell"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["Bernardino Romera-Paredes", "PHS Torr"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Efficient pairwise learning using kernel ridge regression: an exact two-step method", "author": ["Michiel Stock", "Tapio Pahikkala", "Antti Airola", "Bernard De Baets", "WillemWaegeman"], "venue": "arXiv preprint arXiv:1606.04275,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667\u2013685", "author": ["Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis Vlahavas"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Modular construction of time-delay neural networks for speech recognition", "author": ["Alex Waibel"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}, {"title": "A review on multi-label learning algorithms. Knowledge and Data Engineering", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Fast zero-shot image tagging", "author": ["Yang Zhang", "Boqing Gong", "Mubarak Shah"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Multi-label output codes using canonical correlation analysis", "author": ["Yi Zhang", "Jeff G Schneider"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 19, "context": "The above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3].", "startOffset": 108, "endOffset": 119}, {"referenceID": 21, "context": "The above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3].", "startOffset": 108, "endOffset": 119}, {"referenceID": 1, "context": "The above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3].", "startOffset": 108, "endOffset": 119}, {"referenceID": 13, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 17, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 6, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 15, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 13, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 295, "endOffset": 299}, {"referenceID": 20, "context": "Thanks to the studies in linguistics, words in speech recognition are represented by combinations of phonemes [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "In visual object recognition, a class is described by a set of visual attributes [10, 7].", "startOffset": 81, "endOffset": 88}, {"referenceID": 5, "context": "In visual object recognition, a class is described by a set of visual attributes [10, 7].", "startOffset": 81, "endOffset": 88}, {"referenceID": 10, "context": "More recently, the distributed representations of English words [12, 16] have found their applications in a variety of tasks.", "startOffset": 64, "endOffset": 72}, {"referenceID": 14, "context": "More recently, the distributed representations of English words [12, 16] have found their applications in a variety of tasks.", "startOffset": 64, "endOffset": 72}, {"referenceID": 8, "context": "The labels in U correspond to no data points in the training sample and thus often termed zero-shot labels [10, 24].", "startOffset": 107, "endOffset": 115}, {"referenceID": 22, "context": "The labels in U correspond to no data points in the training sample and thus often termed zero-shot labels [10, 24].", "startOffset": 107, "endOffset": 115}, {"referenceID": 19, "context": "We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 21, "context": "We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 1, "context": "We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 13, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 17, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 6, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 15, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 7, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 36, "endOffset": 49}, {"referenceID": 23, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 36, "endOffset": 49}, {"referenceID": 1, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 36, "endOffset": 49}, {"referenceID": 0, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 78, "endOffset": 84}, {"referenceID": 2, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 78, "endOffset": 84}, {"referenceID": 11, "context": "The most recent works [13, 24] can be regarded as specific instantiations of infinite-label learning, while we formally formulate the problem and provide both theoretical and empirical studies as follows.", "startOffset": 22, "endOffset": 30}, {"referenceID": 22, "context": "The most recent works [13, 24] can be regarded as specific instantiations of infinite-label learning, while we formally formulate the problem and provide both theoretical and empirical studies as follows.", "startOffset": 22, "endOffset": 30}, {"referenceID": 18, "context": "In addition, the pairwise learning [20] is a very interesting framework which encapsulates matrix completion, collaborative filtering, zero-shot and the infinite-label learning.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "As a result, our analyses apply to pairwise learning when yh is binary in [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "k=1 \u03c0kN ( \u03bck,UkU T k , ) , \u03bb \u223c N ([ 2 3 ] , [ 1 1.", "startOffset": 34, "endOffset": 41}, {"referenceID": 1, "context": "k=1 \u03c0kN ( \u03bck,UkU T k , ) , \u03bb \u223c N ([ 2 3 ] , [ 1 1.", "startOffset": 34, "endOffset": 41}, {"referenceID": 21, "context": "We borrow from multi-label classification [23] the Hamming loss as the evaluation metric.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "We conduct our experiments using the NUS-WIDE dataset [6].", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "The image features x are l2-normalized VGGNet-19 [18] last fully-connected-layer activations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "We obtain their semantic codes by the pre-trained GloVe word vectors [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "We report the results measured by four popular metrics: Mean image Average Precision (MiAP) [11] and the top-3 precision, recall, and F1-score.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "Accordingly, in order to impose the ranking property to our labeling function, we learn it using the RankNet loss [5],", "startOffset": 114, "endOffset": 117}, {"referenceID": 12, "context": "[14] and Akata et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "One particularly interesting application of infinite-label learning is on the extreme multi-label classification problems [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "[2] Krishnakumar Balasubramanian and Guy Lebanon.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Wei Bi and James Kwok.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Alireza Farhadi, Ian Endres, Derek Hoiem, and David Forsyth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Xirong Li, Tiberio Uricchio, Lamberto Ballan, Marco Bertini, Cees GM Snoek, and Alberto Del Bimbo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Jinseok Nam, Eneldo Loza Menc\u00eda, Hyunwoo J Kim, and Johannes F\u00fcrnkranz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, and Jeffrey Dean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Jeffrey Pennington, Richard Socher, and Christopher D Manning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Bernardino Romera-Paredes and PHS Torr.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Karen Simonyan and Andrew Zisserman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Michiel Stock, Tapio Pahikkala, Antti Airola, Bernard De Baets, and WillemWaegeman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Alex Waibel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Min-Ling Zhang and Zhi-Hua Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Yang Zhang, Boqing Gong, and Mubarak Shah.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Yi Zhang and Jeff G Schneider.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We develop a new statistical machine learning paradigm, named infinite-label learning, to annotate a data point with more than one relevant labels from a candidate set, which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels. The infinite-label learning fundamentally expands the scope of conventional multi-label learning, and better models the practical requirements in various real-world applications, such as image tagging, ads-query association, and article categorization. However, how can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning, where the key is to represent a class/label by a vector of semantic codes, as opposed to treating them as atomic labels. We validate the infinite-label learning by a PAC bound in theory and some empirical studies on both synthetic and real data. 1 ar X iv :1 60 8. 06 60 8v 1 [ cs .L G ] 2 3 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}