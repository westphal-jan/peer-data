{"id": "1608.07793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2016", "title": "Partially Observable Markov Decision Process for Recommender Systems", "abstract": "we report the \" recurrent deterioration \" ( initial rd ) phenomenon observed in online recommender systems. the rd phenomenon is reflected by the sharp trend of performance degradation when the recommendation model is always trained based on users'past feedbacks of the poor previous recommend recommendations. there are several obvious reasons for the recommender systems to negatively encounter the rd phenomenon, including the lack of negative training data and the evolution distortion of users'interests, etc. motivated solely to tackle the problems causing the rd phenomenon, we propose the pomdp - rec framework, which is a neural - optimized partially observable markov decision process algorithm for recommender systems. we show that the pomdp - rec framework effectively uses the naturally accumulated historical data from real - world recommender systems and automatically achieves comparable rb results often with those models using fine - value tuned exhaustively by domain exports reliance on public financial datasets.", "histories": [["v1", "Sun, 28 Aug 2016 09:42:52 GMT  (44kb,D)", "http://arxiv.org/abs/1608.07793v1", null], ["v2", "Thu, 1 Sep 2016 15:41:02 GMT  (44kb,D)", "http://arxiv.org/abs/1608.07793v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["zhongqi lu", "qiang yang"], "accepted": false, "id": "1608.07793"}, "pdf": {"name": "1608.07793.pdf", "metadata": {"source": "CRF", "title": "Partially Observable Markov Decision Process for Recommender Systems", "authors": ["Zhongqi Lu", "Qiang Yang"], "emails": ["zluab@cse.ust.hk", "qyang@cse.ust.hk"], "sections": [{"heading": "1 Introduction", "text": "Recommender systems are widely applicable in real-world scenarios such as e-learning, online shopping, mobile app stores, etc. In recommender systems, the training data usually come from users\u2019 positive feedbacks (such as \u201cclicks\u201d) towards its previous recommendations. This process goes on recurrently in the recommender system. Under this recurrent \u201ctrain with feedbacks, make recommendations, collect feedbacks, re-train, . . . \u201d mechanism, the outputs of the previous recommendation models would affect the input of the future training and the pool of recommendation candidates would become smaller when the recurrences go on. Therefore, this specific characteristic of online recommender systems could lead to a significant performance degradation as the system runs. This phenomenon is called \u2018Recurrent Deterioration\u2019 (RD).\nIn our analysis, the recommender systems encounters the RD phenomenon mainly because of two reasons: (a) potential system biases caused by a lack of negative training samples (For example, the advertising recommender systems only see the click as a positive feedback), and the biases accumulate as the recurrent recommendation mechanism runs, and (b) frequent changes of data distributions caused by the changes of users\u2019 interests and the changes of recommendation context. Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe. But the two problems are not solved naturally. Besides, these solutions need lots of human efforts.\nIn the previous researches, modeling the sequential recommendation process based on the Markov Decision Process (MDP) was proposed [24]. However, because the optimization of the MDP assumes full observations of its states and thus requires dense training data, the MDP approach is not very practical for current online recommender systems, in which an user usually views less than 1% items. The sparse nature of the training data in recommender systems indicates the observations of users\u2019 behaviours shall be \u201cpartial\u201d. Therefore, we propose to use a Partially Observable Markov Decision Process (POMDP) to model the sequential recommendations and tackle the RD phenomenon. We call\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 8.\n07 79\n3v 1\n[ cs\n.A I]\n2 8\nA ug\n2 01\nour POMDP based framework for recommender systems \u201cPOMDP-Rec\u201d. Intuitively, we consider a recommender system as an agent exploring an environment. The environment could be observed through the users\u2019 positive feedbacks. Based on limited observations, the recommender system estimates the interests of its users. In other words, the recommender system (as an agent) has some hidden belief about what state it is in, and what would be the next state to be expected. Whenever taking some actions (i.e. recommendations), the recommender system gets its rewards based on short-term user feedbacks (such as clicks) and/or long-term outcomes (such as users\u2019 payments). The objective is to maximize the cumulative rewards.\nOur proposed POMDP-Rec framework is capable to tackle the two problems causing RD phenomenon, i.e., a lack of negative samples in recurrently training a recommender system, and frequent change of the data distribution. Different from classical supervised learning methods for recommender systems, this training process of the POMDP-Rec avoids the needs of negative sample. Indeed, the classical supervised learning methods try to learn a ranking of all the candidates before making the recommendations, and thus need to compare with positive and negative samples. But for this POMDP-Rec framework, the recommender system will only need to train on recommendations with positive feedbacks. Another obvious benefit from the POMDP-Rec modeling is the ability to capture the shifts of data distributions caused by users\u2019 changing interests and the changing recommendation context, because it is the transitions between states that are considered in the POMDP-Rec modeling.\nOther than solving the two problems causing RD phenomenon, the POMDP-Rec framework is suitable for modeling the recommender systems because it can use the historical training data in the recommender systems efficiently. Due to the Markovian property, we could train the POMDP-Rec without the need to maintain the whole sequential order of user\u2019s behaviours. Besides, because our POMDP-Rec model does not have any assumption on user\u2019s future interests and the training could be done off-line, we could use as many historical samples as possible without worrying about the changing of users\u2019 interests during the testing. So far as we are concerned, this is the first literature to introduce the POMDP-based modeling for recommender systems."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 The POMDP", "text": "The recommendation problem considered in this paper could be formulated by a POMDP [25]. A POMDP models a decision process in which it is assumed that the system dynamics are determined by an MDP, but the underlying state cannot be directly observed. It maintains a probability distribution over the set of possible states, based on a set of observations and observation probabilities, as well as the underlying MDP.\nThe POMDP can be a proper modeling of the recommender systems because of two reasons. On one hand, the Markovian assumption has been proven to be valid for the sequential recommendations [24]. On the other hand, we shall not expect to fully observe an user\u2019s actions on all items in a recommender system. Therefore, we shall avoid to define the users\u2019 actions as states. Instead, we argue that the observed user\u2019s actions would depend on some hidden states with a probability. Under such a modeling, the POMDP would be suitable for the sequential recommendations."}, {"heading": "2.2 Neural fitted Q-learning", "text": "Reinforcement learning methods, such as Q-learning, are often adopted to optimize the MDP-based models. In classical Q-learning [26], the goal is to learn a Q-value function, which is a mapping from the state and action space to an evaluation space. This Q-value function follows an important identity known as Bellman equation. This is based on an intuition: if the optimal Q?(s\u2032, a\u2032) of the sequence s\u2032 at the next time-step was known for all possible actions a\u2032, then the optimal strategy is to select the action a\u2032 maximizing the expected value of r(s, a) + \u03b3maxa\u2032 Q?(s\u2032, a\u2032). The update rule is given by\nQ?(s, a) = r(s, a) + \u03b3max a\u2032 Q?(s\u2032, a\u2032), (1)\nwhere s \u2208 S denotes the state where the transition starts, a \u2208 A is the current action, and s\u2032 \u2208 S is the resulting state. r(s, a) is an immediate reward function r : S\u00d7A\u2192 R. \u03b3 is a discounting factor. Recent researches have demonstrated the success of using neural fitted function to approximate the Q-values in real-world applications, such as playing Atari [18]. As mentioned in [22], the above\nprocess of generating Q-values can be parameterized via an instance of the Fitted Q Iteration family of algorithms [6]. For instance, a neural network function approximator with parameters \u03b8 could be used to estimate the Q-value. To learn \u03b8, the common way is to minimize a sequence of loss functions Li(\u03b8i) iteratively,\nLi(\u03b8i) = (yi \u2212Q(s, a; \u03b8i))2 (2)\nwhere yi = r(s, a) + \u03b3maxa\u2032 Q(s\u2032, a\u2032; \u03b8i\u22121) is the target for iteration i. We would like to fix the parameters \u03b8i\u22121 when optimizing the loss function Li.\nIn this work, we would like to adopt the idea of Fitted Q Iteration procedures to estimate the Q-values and come up with a neural fitted Q-learning solution for recommender systems."}, {"heading": "3 POMDP-Rec for Recommender Systems", "text": ""}, {"heading": "3.1 The POMDP-Rec framework", "text": "Given the Markovian properties of recommender systems [24], we will find the POMDP modeling to be suitable to explain the nature of recommender systems. The recommender system would not be able to fully observe the users\u2019 actions towards the entire item set, because the users usually provide limited feedbacks [1].\nFormally, the POMDP-Rec framework in this paper is represented by an 8-tuple (S,A,T,R,\u2126,O,P, \u03b3), where\n\u2022 S is a set of hidden states. \u2022 A is a set of actions. For example, the recommendations. \u2022 T(s\u2032 | s, a) is a set of conditional transition probabilities when taking action a \u2208 A, which\ncauses the environment transit from state s to s\u2032. \u2022 R : S\u00d7A \u2212\u2192 R is the reward function. \u2022 O is a set of observations of users\u2019 behaviours. \u2022 \u2126(o | s, a) is a set of conditional observation probabilities of receiving an observation o \u2208 O after the agent takes action a at hidden state s.\n\u2022 P is a probability distribution, from which the underlying state s \u2208 S is generated. \u2022 \u03b3 \u2208 [0, 1] is the discount factor.\nUnder the framework of POMDP-Rec, at each time point, the recommender system (agent) interacts with the environment and gets an observation o \u2208 O from users\u2019 (positive) feedbacks. Based on the observation o, the underlying state s \u2208 S is then generated from the probability distribution P. At state s, an action a \u2208 A is chosen to determine both the reward r = R(s, a) and the next state s\u2032 \u223c T(s, a)."}, {"heading": "3.2 Belief Approximation of POMDP-Rec", "text": "Although the POMDP approach provides a natural way to model the recommender systems, solving it is non-trivial [10]. To solve for the proposed POMDP-Rec framework under the recommender system settings, we developed an approximation by introducing the beliefs of the hidden states in POMDP, known as belief states. To estimate the belief states, we propose to refer to the low-dimensional factor model [17]. The approach could handle the sparseness of the observations, because the belief states are learned in a collaborative manner.\nIn the low-dimensional factor model, a user\u2019s actions towards an item is modeled by linearly combining the estimations of items\u2019 latent features and the estimations of user\u2019s latent interests. Following [17], the estimations of items\u2019 latent features V and the estimations of user\u2019s latent interests U are assumed to be from some particular normal distributions with zero-mean and corresponding variances. With a partially observed user-item matrix O, V and U are inferred from:\np(O | U,V, \u03c32) = \u220f i=1 \u220f j=1 ( N (Oij | Ui>Vj , \u03c32) )\u03c6ij (3)\nwhere N (x | \u00b5, \u03c32) is the probability density function of the Gaussian distribution with mean \u00b5 and variance \u03c32, and \u03c6ij \u2208 {0, 1} is a indicator of observing (i, j), and\np ( U | \u03c32u ) = \u220f i=1 N ( Ui | 0, \u03c32uIk ) , p ( V | \u03c32v ) = \u220f j=1 N ( Vj | 0, \u03c32vIk ) , (4)\nwhere Ui is the estimation of user i\u2019s latent interests, and Vj is the estimation of item j\u2019s latent features. The belief state bij (corresponding to the interaction between user i and item j) is defined as the concatenation of the estimation of user\u2019s latent interests and the estimation of item\u2019s latent features, and is denoted as bij = \u3008Ui,Vj\u3009, where Ui and Vj are as in Equation 4. To infer user i\u2019s interaction on item j, we split bij half by half so that the first half is for user\u2019s latent interests Ui and the rest is for item\u2019s latent features Vj . The prediction is give by h(bij) = UTi Vj .\nWith the definition of belief states, solving the original POMDP-Rec is equivalent to solving an MDP, which is defined as a tuple (B,A, \u03c4, r, \u03b3), where\n\u2022 B is the set of belief states. B = {\u3008Ui,Vj\u3009}i=1,2,...,|U|;j=1,2,...,|V|, where U and V come from Equation (4),\n\u2022 A is the same set of actions as for the original POMDP, \u2022 \u03c4 is the belief state transition function, \u2022 R : B\u00d7A \u2212\u2192 R is the reward function on belief states, \u2022 \u03b3 is the discount factor, same to the one in the original POMDP.\nThe above transformation introduces a Markovian belief state that allows the POMDP-Rec to be re-formulated as a solvable MDP where every belief is defined as a state. An important consequence is that the belief states form a fully observable MDP, and thus it is a sufficient statistic for choosing optimal actions [3].\nQ-learning has been adopted in solving the POMDP problems in some recent works [9, 29]. Because the future recommendations depend on how the user\u2019s behaviours change, we would like to use the transitions of the belief states as the inputs of the Q function. Details are given in the next section."}, {"heading": "3.3 Solving POMDP-Rec for Recommendations", "text": "We would like to solve the POMDP-Rec by Q-learning [27]. As the input of the Q function, we define a transition of belief states \u3008b, b\u2032\u3009, where b\u2032 is the consecutive belief state after b. The Q-value function is the solution to the Bellman optimality equation:\nQ(\u3008b, b\u2032\u3009) = r + \u03b3 \u2211 b\u2032\u2032\u2208B \u03c4(b\u2032\u2032 | b\u2032)Q(\u3008b\u2032, b\u2032\u2032\u3009) (5)\nTo handle the complex real-world scenarios, we introduce a neural network to approximate the value function Q. Our approach is an instance of the Fitted Q Iteration family [6], which has successful applications in playing Atari games [18].\nThe POMDP-Rec framework consists of two major steps: (1) Generating a training pattern set PS, and (2) training these patterns within a neural network. Besides, we found in experiments that randomly shuffling the training pattern set PS would lead to better results. The algorithm is shown in Algorithm 1. We provide the key implementation details in Section 3.5.\nThe learned Q function is used to evaluate future recommendations. In order to make future recommendations, we generate the candidates by varying the parameters of a matrix factorization model [30], and the candidate with the highest Q value is adopted for the future recommendation."}, {"heading": "3.4 Properties of POMDP-Rec", "text": "The POMDP-Rec framework is naturally fitted for the training data from recommender system. The training data collected in recommender systems have three main characteristics.\nFirst, the users\u2019 feedbacks are usually unevenly distributed. For instance, an online advertising recommender system only see positive feedbacks. But no feedback may indicate either the negative\nAlgorithm 1 POMDP solver for Recommender Systems Input: set of transitive observations D = {(oij ; a; o\u2032ij), i \u2208 {1, . . . , | U |}, j \u2208 {1, . . . , | V |}}; number of iterations N ; Output: a neural network to estimate the Q-value function QN ;\niter = 0; %Init iteration counter Init_MLP()\u2192 Q0; %Rand init the neural network Estimate a transition function \u03c4(.) by sampling from D and calculating belief states; while iter < N do\nGenerate pattern set PS = {(inputij , targetij)} by iterating over the transitive observations {(oij ; a; o\u2032ij)}, where the sub-steps are:\nGenerate bij = \u3008Ui,Vj\u3009 from Eq. 4 with O; Generate b\u2032ij = \u3008U\u2032i,V\u2032j\u3009 from Eq. 4 with O\u2032; Calculate reward r(b\u2032ij , a) by Eq. 6; inputij = \u3008bij , b\u2032ij\u3009; targetij = r(b\u2032ij , a) + \u03b3 \u2211 b\u2032\u2032\u2208B \u03c4(b\n\u2032\u2032 | b\u2032ij)Q(\u3008b\u2032ij , b\u2032\u2032\u3009); Randomly shuffle pattern set PS; Train_NN(PS)\u2192 Qiter+1 by optimizing Eq. 2 using SGD; iter := iter + 1;\nend while\nfeedback or the users miss the recommendations [20]. The lack of negative training samples may cause a big problem for the supervised learning methods, whose learning objective is essentially classifying the positive and negative feedbacks. But it would not affect our proposed POMDPRec, because the objective of the POMDP-Rec is to make the recommendations that maximize the prediction accuracy for positive feedbacks.\nSecondly, training data in recommender systems could have been accumulated for a long time span. The popular Collaborative Filtering (CF) approaches for recommender systems aim to capture the users\u2019 up-to-date interests, thus the historical data are usually used with a time decay factor [12] and lots of data could be \u201cexpired\u201d. For our POMDP-Rec framework, because we are focusing on the transitions of belief states, instead of the users\u2019 interests, thus all historical feedbacks could be used, which leads to better data efficiency.\nThirdly, the data are sparse at each time interval. Handling the sparseness in data from recommender systems has been a popular research direction. Our partially observable assumption comes right from this sparseness nature of data in recommender systems. And we handle the sparse observation by collaboratively learning the belief states."}, {"heading": "3.5 Implementation details", "text": "Preprocess data for training The raw data accumulated in a recommender system can be collected in triples of the form (o, a, o\u2032), where o is the original observation, a is the recommendation to be made, and o\u2032 is the next observation. The consecutive observations o and o\u2032 shall be related to the same (user, item) pair. The set of these triples is denoted as the sample set D. Definition of rewards The definition of rewards is important to the Q function. By inheriting the notations in the previous sections, we introduce a definition of the reward:\nr(b\u2032, a) = 1 1 + exp(C \u2217 ( \u221a\u2211m\ni=1 \u2211n j=1 Iij(a(i, j)\u2212 h(b\u2032ij))2/ | I |))\n(6)\nwhere h(b\u2032ij) is the future interaction between user i and item j inferred from the belief b \u2032 ij and it serves as the ground truth, Iij is the 0/1 indicator function for the availability of a(i, j), and C is a scaling constant to be set according to the scales of the input data, e.g., C = 0.1 for a 0\u2212 100 rating prediction problem.\nThere could be various choices of the reward functions, depending on the applications. For example, if we would like to optimize the Click Through Rate (CTR) for the recommender system, the reward\ncould be defined as the real CTR. Directly optimizing the future reward is another advantage of our POMDP-Rec framework.\nTransition function The transition function \u03c4(.) is essential to estimate the expected value function as in Equation (5). In our experiments, we learn a logistic regression function y = f(b, b\u2032), where y = 1 if there exists a transition from belief state b to b\u2032. After learning the regression function f(\u00b7, \u00b7), we use its output value to estimate the transition probability.\nImplementation of NN The approximation algorithm for the value function is not the main concern of this work. Thus, we use the regular implementation of a neural network to estimate the value function. The neural network is composed of an input layer, one hidden layer, and an output layer."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset and setup", "text": "We would like to test our proposed POMDP-Rec framework on the public datasets that are retrieved from real world recommender systems. From real world recommender systems, the data are likely to (a) be sparse, (b) contain uneven distributions of numerical ratings, and (c) demonstrate users\u2019 changing interests if the system runs for a relatively long time. We found the MovieLens dataset and the Yahoo Music dataset to be ideal as an off-line dataset to test the POMDP-Rec framework and the state-of-the-art methods, although the anonymilization of these public datasets prevent us from further analyzing the belief states, and Q function etc in the POMDP-Rec model.\nThe 1M MovieLens dataset [7] contains about 1 million ratings of 3,952 movies by 6,040 users with Unix timestamps. Each user has at least 20 ratings, and the average sparsity of the whole dataset is about 4.2%. The MovieLens dataset was collected from the MovieLens web site,1 where the users rate on a 5-star system.\nThe Yahoo Music dataset [5] comprises 262, 810, 175 ratings of 624, 961 music items by 1, 000, 990 users collected during the year 1999\u2212 2010. The ratings include one-minute resolution timestamps, allowing refined temporal analysis. The average sparsity of the whole dataset is about 0.42%. This Yahoo Music dataset was collected from the Yahoo Music service.2 The ratings range from 1 to 100."}, {"heading": "4.2 Performance comparison", "text": "In order to simulate the real world recommendation scenario, we would like to split the training data and the testing data by time. We use about 80% data from earlier time as the training data, and the remaining data for testing. As discussed in the previous section, the data are split and scattered into time windows, and the predictions of the POMDP-Rec are evaluated for each time windows. We report the average performance of the POMDP-Rec on the MovieLens dataset in Table 1, and the Yahoo Music dataset in Table 2.\n1http://movielens.org 2https://www.yahoo.com/music/\nIn order to compare with the previous results, we adopt KDDCUP\u203211 official evaluation metric, root-mean-square error (RMSE). That is, RMSE = \u221a\u2211m\ni=1 \u2211n j=1 Iij(Rij \u2212 R\u0302ij)2/ | I |, where\ni, j are indices for users and items respectively, Iij is the 0/1 indicator function for availability of rating Rij .\nThe baselines include\n\u2022 timeSVD++ [12]. This method is reported to achieve excellent performance on the Netflix dataset [2] by considering the time changing behaviors throughout the life span of the data.\n\u2022 Factorization Machine (FM) [21]. As a general predictor working with any real valued feature vector, FM combines the advantages of Support Vector Machine with factorization models. It achieves good results in several Kaggle (http://www.kaggle.com) recommendation competitions.\n\u2022 Matrix Factorization (MF). MF is a classical collaborative filtering approach for recommendations. We use the libMF implementation [30].\nThe parameters are all fine-tuned. In order to perform fair comparisons, we fixed the dimension of latent factors in U and V to be 32 in all baselines and the POMDP-Rec.\nAs a reference, for the Yahoo Music dataset, the best ensemble of models achieves RMSE \u2248 21.0, and the best single model achieves RMSE \u2248 22.1 [4], by the official KDDCUP\u203211 competition report [5]. However, the parameters of these winning models are fine-tuned exhaustively by domain experts and the performance depends much on the feature engineering, while our model does not take much human efforts in fitting any specific recommendation mechanisms."}, {"heading": "4.3 Analysis of POMDP-Rec", "text": "In this section, we analyze the POMDP-Rec framework empirically on the Yahoo Music dataset. Previous literatures [8] showed that the estimation of the value function could be rather biased, especially when there are large over-estimations of future Q-values. It appears that iterating over the randomly shuffled sample set for multiple times could improve the performance. In the experiments, we evaluate the POMDP-Rec framework after each iteration, and show the trend of performance improvements in Figure 1.\nFrom Figure 1, we notice that stating from iteration 6, the model\u2019s performance has a significant improvement. We would like to exam its stability starting from iteration 6. In the previous researches on Q-learning [18], the average reward and the average maxQ are often adopted as indicators of the model stability. Therefore, we plot the average reward and the average maxQ with respect to the number of training samples in Figure 2. The POMDP-Rec framework is stable in modeling over the Yahoo Music recommendation dataset, because we see in Figure 2(a) the average rewards smoothly increases as the performance of the model improves, and in Figure 2(b) the average maxQ appears to be stable (i.e., slope becomes small in the figure) after a large amount of training samples have been provided."}, {"heading": "5 Related Works", "text": "The sequential nature of the recommendation process was noticed in the past [24]. The common practice is to model the sequential nature via the Markove Decision Process (MDP) [24, 23]. Taking the Markovian idea to model the sequential reccommendations one step further, we suggest the sequential recommendation shall be considered as a Partially Observable MDP. Because in the settings of recommender systems, the user\u2019s actions may not be fully observed, but every actions we observed can be used to collaboratively estimate the distributions of the user\u2019s interests. As an intuitive sketch of a natural modeling of a recommender system, at each time interval, the recommender system observes users\u2019 actions, infers its belief states, and makes recommendations. These processes are seamlessly modeled through our proposed POMDP-Rec framework.\nFrom the perspective of handling the RD phenomenon, our work is also related to solving the one-class problem. As one of the causes of the RD phenomenon in recommender systems, one class problem refers to the recommendation scenario where only the positive feedbacks is received. To solve this problem, there are mainly two approaches. On one hand, the researchers proposed to augment the training data by generating negative samples [15, 11]. On the other hand, the reinforcement learning approaches have been proposed, such as the contextual bandit models [13, 28].\nIn terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9]. A deep reinforcement learning model employs a deep network to estimate the value function of each discrete action, and when acting, select the maximally valued output for a given state input. We adopt the parameterized optimization for the value function in our work."}, {"heading": "6 Conclusion", "text": "We propose the POMDP-Rec framework designed to model the recommendation process. The POMDP-Rec framework handles the changing distribution of the testing samples through transiting between its states. The learning of the POMDP-Rec does not require a balance of the positive and the negative samples. And the Markov property of the POMDP-Rec eliminates the impacts of recurrent training. With these nice properties, the POMDP-Rec framework achieves good results in the off-line experiments on both the MovieLens 1M dataset and the Yahoo Music dataset. In the future, we would like to deploy the POMDP-Rec framework to a real-world online advertising recommender system, train and evaluate it based on the practical metrics (e.g., CTR or revenue), and try to analyze the learned belief states."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "The netflix prize", "author": ["J. Bennett", "S. Lanning"], "venue": "KDD cup and workshop", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Dynamic Programming: Deterministic and Stochastic Models", "author": ["D.P. Bertsekas"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1987}, {"title": "et al", "author": ["P.-L. Chen", "C.-T. Tsai", "Y.-N. Chen", "K.-C. Chou", "C.-L. Li", "C.-H. Tsai", "K.-W. Wu", "Y.-C. Chou", "C.-Y. Li", "W.-S. Lin"], "venue": "A linear ensemble of individual and blended models for music rating prediction. In KDD Cup, pages 21\u201360", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The yahoo! music dataset and kdd-cup\u201911", "author": ["G. Dror", "N. Koenigstein", "Y. Koren", "M. Weimer"], "venue": "KDD Cup, pages 8\u201318", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, pages 503\u2013556", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Double q-learning", "author": ["H.V. Hasselt"], "venue": "Advances in Neural Information Processing Systems, pages 2613\u20132621", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Value-function approximations for partially observable markov decision processes", "author": ["M. Hauskrecht"], "venue": "Journal of Artificial Intelligence Research, pages 33\u201394", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic matrix factorization with nonrandom missing data", "author": ["J.M. Hernandez-lobato", "N. Houlsby", "Z. Ghahramani"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1512\u20131520", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Y. Koren"], "venue": "KDD, pages 447\u2013456. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "WWW, pages 661\u2013670. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative prediction and ranking with non-random missing data", "author": ["B.M. Marlin", "R.S. Zemel"], "venue": "Proceedings of the third ACM conference on Recommender systems, pages 5\u201312. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Content-boosted collaborative filtering for improved recommendations", "author": ["P. Melville", "R.J. Mooney", "R. Nagarajan"], "venue": "AAAI/IAAI, pages 187\u2013192", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, pages 1257\u20131264", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "One-class collaborative filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "Eighth IEEE International Conference on Data Mining,ICDM\u201908., pages 502\u2013511. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 995\u20131000. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Machine Learning: ECML 2005, pages 317\u2013328. Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "A hidden markov model for collaborative filtering", "author": ["N. Sahoo", "P.V. Singh", "T. Mukhopadhyay"], "venue": "MIS Quarterly, 36(4):1329\u20131356", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "An mdp-based recommender system", "author": ["G. Shani", "D. Heckerman", "R.I. Brafman"], "venue": "Journal of Machine Learning Research, 6:1265\u20131295", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "The optimal control of partially observable markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research, 21(5):1071\u20131088", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1973}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 1. MIT press Cambridge", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3-4):279\u2013292", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "Hierarchical exploration for accelerating contextual bandits", "author": ["Y. Yue", "S.A. Hong", "C. Guestrin"], "venue": "The 29th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploration vs", "author": ["X. Zhao", "P.I. Frazier"], "venue": "exploitation in the information filtering problem. arXiv preprint arXiv:1407.8186", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast parallel sgd for matrix factorization in shared memory systems", "author": ["Y. Zhuang", "W.-S. Chin", "Y.-C. Juan", "C.-J. Lin"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 249\u2013256. ACM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe.", "startOffset": 51, "endOffset": 59}, {"referenceID": 10, "context": "Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe.", "startOffset": 51, "endOffset": 59}, {"referenceID": 15, "context": "Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe.", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "In the previous researches, modeling the sequential recommendation process based on the Markov Decision Process (MDP) was proposed [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "The recommendation problem considered in this paper could be formulated by a POMDP [25].", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "On one hand, the Markovian assumption has been proven to be valid for the sequential recommendations [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "In classical Q-learning [26], the goal is to learn a Q-value function, which is a mapping from the state and action space to an evaluation space.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "Recent researches have demonstrated the success of using neural fitted function to approximate the Q-values in real-world applications, such as playing Atari [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "As mentioned in [22], the above", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "process of generating Q-values can be parameterized via an instance of the Fitted Q Iteration family of algorithms [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 23, "context": "Given the Markovian properties of recommender systems [24], we will find the POMDP modeling to be suitable to explain the nature of recommender systems.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The recommender system would not be able to fully observe the users\u2019 actions towards the entire item set, because the users usually provide limited feedbacks [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "\u2022 \u03b3 \u2208 [0, 1] is the discount factor.", "startOffset": 6, "endOffset": 12}, {"referenceID": 9, "context": "Although the POMDP approach provides a natural way to model the recommender systems, solving it is non-trivial [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "To estimate the belief states, we propose to refer to the low-dimensional factor model [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "Following [17], the estimations of items\u2019 latent features V and the estimations of user\u2019s latent interests U are assumed to be from some particular normal distributions with zero-mean and corresponding variances.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "An important consequence is that the belief states form a fully observable MDP, and thus it is a sufficient statistic for choosing optimal actions [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Q-learning has been adopted in solving the POMDP problems in some recent works [9, 29].", "startOffset": 79, "endOffset": 86}, {"referenceID": 28, "context": "Q-learning has been adopted in solving the POMDP problems in some recent works [9, 29].", "startOffset": 79, "endOffset": 86}, {"referenceID": 26, "context": "We would like to solve the POMDP-Rec by Q-learning [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "Our approach is an instance of the Fitted Q Iteration family [6], which has successful applications in playing Atari games [18].", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "Our approach is an instance of the Fitted Q Iteration family [6], which has successful applications in playing Atari games [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "In order to make future recommendations, we generate the candidates by varying the parameters of a matrix factorization model [30], and the candidate with the highest Q value is adopted for the future recommendation.", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "feedback or the users miss the recommendations [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "The popular Collaborative Filtering (CF) approaches for recommender systems aim to capture the users\u2019 up-to-date interests, thus the historical data are usually used with a time decay factor [12] and lots of data could be \u201cexpired\u201d.", "startOffset": 191, "endOffset": 195}, {"referenceID": 6, "context": "The 1M MovieLens dataset [7] contains about 1 million ratings of 3,952 movies by 6,040 users with Unix timestamps.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "The Yahoo Music dataset [5] comprises 262, 810, 175 ratings of 624, 961 music items by 1, 000, 990 users collected during the year 1999\u2212 2010.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "\u2022 timeSVD++ [12].", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "This method is reported to achieve excellent performance on the Netflix dataset [2] by considering the time changing behaviors throughout the life span of the data.", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "\u2022 Factorization Machine (FM) [21].", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "We use the libMF implementation [30].", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "1 [4], by the official KDDCUP\u203211 competition report [5].", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "1 [4], by the official KDDCUP\u203211 competition report [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Previous literatures [8] showed that the estimation of the value function could be rather biased, especially when there are large over-estimations of future Q-values.", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "In the previous researches on Q-learning [18], the average reward and the average maxQ are often adopted as indicators of the model stability.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "The sequential nature of the recommendation process was noticed in the past [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "The common practice is to model the sequential nature via the Markove Decision Process (MDP) [24, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 22, "context": "The common practice is to model the sequential nature via the Markove Decision Process (MDP) [24, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 14, "context": "On one hand, the researchers proposed to augment the training data by generating negative samples [15, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "On one hand, the researchers proposed to augment the training data by generating negative samples [15, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 12, "context": "On the other hand, the reinforcement learning approaches have been proposed, such as the contextual bandit models [13, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 27, "context": "On the other hand, the reinforcement learning approaches have been proposed, such as the contextual bandit models [13, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 17, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}, {"referenceID": 18, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}, {"referenceID": 13, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}, {"referenceID": 8, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}], "year": 2017, "abstractText": "We report the \u2018Recurrent Deterioration\u2019 (RD) phenomenon observed in online recommender systems. The RD phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained based on users\u2019 feedbacks of the previous recommendations. There are several reasons for the recommender systems to encounter the RD phenomenon, including the lack of negative training data and the evolution of users\u2019 interests, etc. Motivated to tackle the problems causing the RD phenomenon, we propose the POMDP-Rec framework, which is a neural-optimized Partially Observable Markov Decision Process algorithm for recommender systems. We show that the POMDP-Rec framework effectively uses the accumulated historical data from real-world recommender systems and automatically achieves comparable results with those models fine-tuned exhaustively by domain exports on public datasets.", "creator": "LaTeX with hyperref package"}}}