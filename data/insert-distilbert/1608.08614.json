{"id": "1608.08614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "What makes ImageNet good for transfer learning?", "abstract": "the tremendous success of features learnt using the imagenet classification acquisition task on a wide range of transfer tasks begs the question : : what are the intrinsic properties necessary of the imagenet dataset that are critical for learning good, general - purpose features? this detailed work provides an empirical investigation of suggesting various facets of this question : is handling more pre - training data always better? how does feature quality depend upon on the number numbers of training examples per class? does adding ten more object classes improve performance? for such the same data budget, how should gradually the data be split into classes? is fine - grained segment recognition capabilities necessary for learning good features? given the same number of required training classes, is it better to have coarse classes or fine - grained classes? which is better : more classes or more examples acceptable per class?", "histories": [["v1", "Tue, 30 Aug 2016 19:45:09 GMT  (4593kb,D)", "http://arxiv.org/abs/1608.08614v1", null], ["v2", "Sat, 10 Dec 2016 13:37:06 GMT  (2051kb,D)", "http://arxiv.org/abs/1608.08614v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["minyoung huh", "pulkit agrawal", "alexei a efros"], "accepted": false, "id": "1608.08614"}, "pdf": {"name": "1608.08614.pdf", "metadata": {"source": "CRF", "title": "What makes ImageNet good for transfer learning?", "authors": ["Minyoung Huh", "Pulkit Agrawal", "Alexei A. Efros"], "emails": ["minyoung@berkeley.edu", "pulkitag@berkeley.edu", "aaefros@berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "It has become increasingly common within the computer vision community to treat image classification on ImageNet [37] not as an end in itself, but rather as a \u201cpretext task\u201d for training deep convolution neural networks (CNNs [27, 25]) to learn good general-purpose features. This practice of first training a CNN to perform image classification on ImageNet (i.e. pre-training) and then adapting these features for a new target task (i.e. fine-tuning) has become the de facto standard for solving a wide variety of computer vision problems. Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].\nOur work can be found on our website http://minyounghuh.com/papers/analysis\nGiven the success of ImageNet pre-trained CNN features, it is only natural to ask: what is it about the ImageNet dataset that makes the learnt features as good as they are? One school of thought believes that it is the sheer size of the dataset (1.2 million labelled images) that forces the representation to be general. Others argue that it is the large number (1000) of distinct object classes, which forces the network to learn a hierarchy of generalizable features. Yet others believe that the secret sauce is not just the large number of classes, but the fact that many of these classes are visually similar (e.g. many different breeds of dogs), turning this into a fine-grained recognition task, and therefore pushing the representation to work harder. But, while almost everyone in computer vision seems to have their own opinion on this hot topic, very little hard empirical evidence has so far been produced.\nIn this work, we systematically investigate which aspects of the ImageNet task are most critical for learning good general-purpose features. We evaluated the features by finetuning for object detection on PASCAL-VOC 2007 dataset (PASCAL-DET), action classification on PASCALVOC 2012 dataset (PASCAL-ACT-CLS) and scene classification on the SUN dataset (SUN-CLS); see Section 3 for more details. The following is a summary of our main findings:\n1. How many pre-training ImageNet examples are sufficient for transfer learning? Pre-training with only half the ImageNet data (500 images per class instead of 1000) results in only a small drop in transfer learning performance (1.5 mAP drop on PASCAL-DET). This drop is much smaller than the drop on the ImageNet classification task itself. See Section 4 and Figure 1 for details.\n2. How many pre-training ImageNet classes are sufficient for transfer learning? Pre-training with an order of magnitude fewer classes (127 classes instead of 1000) results in only a small drop in transfer learning performance (drop of 2.8 mAP on PASCAL-DET). Quite interestingly, we also found that for some transfer tasks, pre-training with\nar X\niv :1\n60 8.\n08 61\n4v 1\n[ cs\n.C V\n] 3\n0 A\nug 2\nfewer number of classes leads to better performance. See Section 5.1 and Figure 2 for details.\n3. How important is fine-grained recognition for learning good features for transfer learning? The above experiment also suggests that transferable features are learnt even when a CNN is pre-trained with a set of classes that do not require fine-grained discrimination. See Section 5.2 and Figure 2 for details.\n4. Given the same budget of pre-training images, should we have more classes or more images per class? Training with fewer classes but more images per class performs slightly better than training with more classes but fewer images per class. See Section 5.5 and Table 2 for details.\n5. Is more data always helpful? We found that training using 771 ImageNet classes that excludes all PASCAL VOC classes, achieves nearly the same performance on PASCALDET as training on complete ImageNet. Further experiments confirm that blindly adding more training data does not always lead to better performance and can sometimes hurt performance. See Section 6, and Table 9 for more details."}, {"heading": "2. Related Work", "text": "Understanding internal representations of CNN: For understanding what information is encoded by different CNN features, several works have developed feature visualization methods using either deconvolution [50], backpropagation[40, 52] or by training a decoder network [14]. Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44]. An interesting observation that CNNs have \u201cblindspots\u201d was made by [43]. Recently, a mathematical framework for analyzing CNN features was provided by [28]. In contrast to analyzing features learned by training on the full ImageNet dataset, our paper analyzes what aspects of data are important for learning transferable features.\nFactors that affect fine-tuning: The question of whether pre-training should be terminated early to prevent overfitting and what layers should be used for transfer learning was studied by [2, 49]. A thorough investigation of good architectural choices for transfer learning was conducted by [4]. In contrast to these works, we use a fixed finetuning procedure and investigate the factors of variation in pretraining data that effect transfer performance.\nOther pre-training methods: A common assumption in supervised pre-training is that large quantity of expen-\nsive manually-supervised training data is required. The possibility of using large amounts of unlabelled data for feature learning has therefore been very attractive. Numerous methods for learning features by optimizing some auxiliary criterion of the data itself have been proposed. The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].\nUnfortunately, these unsupervised methods turned out not to be competitive with those obtained from supervised ImageNet pre-training. To try and force better feature generalization, more recent \u201cself-supervised\u201d methods use more difficult data prediction auxiliary tasks in an effort to make the CNNs \u201cwork harder\u201d. Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32]. While features learned using these methods often come close to ImageNet performance, to date, none has been able to beat it. Therefore, in this work, we are trying to understand what is the secret to ImageNet\u2019s continuing success."}, {"heading": "3. Experimental Setup", "text": "The process of using supervised learning to initialize CNN parameters using the task of ImageNet classification is referred to as pre-training. The pre-trained CNN is adapted by continued training on a target dataset, this process is referred to as finetuning. All of our experiments use the Caffe [21] implementation of the a single network architecture proposed by Krizhevsky et al. [25]. We refer to this architecture as AlexNet.\nWe closely follow the experimental setup of Agrawal et al. [2] for evaluating the generalization of pre-trained features on three transfer tasks: PASCAL VOC 2007 object detection (PASCAL-DET), PASCAL VOC 2012 action recognition (PASCAL-ACT-CLS) and scene classification on SUN dataset (SUN-CLS). For PASCAL-DET, we used the PASCAL VOC 2007 train/val for finetuning using the experimental setup and code provided by FasterRCNN [36] and report performance on the test set. For PASCAL-ACT-CLS, we used PASCAL VOC 2012 train/val for finetuning and testing using the experimental setup and code provided by R*CNN [17]. For SUN-CLS we used the same train/val/test splits as used by [2]. Finetuning on SUN was performed by first replacing the FC-8 layer in the AlexNet model with a randomly initialized, and fully connected layer with 397 output units. Finetuning was performed for 50K iterations using stochastic gradient descent (SGD) with an initial learning rate of 0.001 which was reduced by a factor of 10 (1/10-th) every 20K iterations.\nIn order to account for difference in performance that may result from different runs of Faster-RCNN and R*CNN, we ran each finetuning experiment 3 times to com-\npute the variance in performance. We report the performance as mean\u00b1 standard deviation. Due to computational constraints we only report SUN-CLS results using a single run. As reported by [2], error bars on SUN-CLS are small, therefore having a single run should not affect the conclusions that we draw.\nIn some experiments we pre-train on ImageNet using a different number of images per class. The model with 1000 images/class uses the original ImageNet ILSVRC 2012 training set. Models with N images/class for N < 1000 are trained by drawing a random sample of N images from all images of that class made available as part of the ImageNet training set."}, {"heading": "4. How does the amount of pre-training data", "text": "affect transfer performance?\nFor answering this question, we trained 5 different AlexNet models from scratch using 50, 125, 250, 500 and 1000 images per each of the 1000 ImageNet classes using the procedure described in Section 3. The variation in performance with amount of pre-training data when these models are finetuned for PASCAL-DET, PASCAL-ACT-CLS and SUN-CLS is shown in Figure 1. For PASCAL-DET, the mean average precision (mAP) for CNNs with 1000, 500 and 250 images/class is found to be 58.3, 57.0 and 54.6. A similar trend is observed for PASCAL-ACT-CLS and SUNCLS. These results indicate that using half the amount of pre-training data leads to only a marginal reduction in per-\nformance on transfer tasks. It\u2019s important to note that the performance on the ImageNet classification task (the pretraining task) steadily increases with the amount of training data, whereas on transfer tasks, the performance increase with respect to additional pre-training data is significantly slower."}, {"heading": "5. How does the taxonomy of the pre-training", "text": "task affect transfer performance?\nIn the previous section we investigated how varying number of pre-training images per class effects the performance in transfer tasks. Here we investigate the flip side: keeping the amount of data constant while changing the nomenclature of training labels. Through manipulating different aspects of the ImageNet dataset, we hope to narrow in on what makes ImageNet good for transfer tasks."}, {"heading": "5.1. The effect of number of pre-training classes on", "text": "transfer performance\nThe 1000 classes of the ImageNet challenge [37] are derived from leaves of the WordNet tree [15]. Using this tree, it is possible to generate different class taxonomies while keeping the total number of images constant. One can generate taxonomies in two ways: (1) bottom up clustering, wherein the leaf nodes belonging to a common parent are iteratively clustered together (see Figure 3), or (2) by fixing the distance of the nodes from the root node (i.e. top down clustering). Using bottom up clustering, 18 possible\ntaxonomies can be generated. Among these, we chose 5 sets of labels constituting 918, 753, 486, 79 and 9 classes respectively. Using top-down clustering only 3 label sets of 127, 10 and 2 can be generated, and we used the one with 127 classes. For studying the effect of number of pretraining classes on transfer performance, we trained separate AlexNet CNNs from scratch using these label sets.\nFigure 2 shows the effect of number of pre-training classes obtained using bottom up clustering of WordNet tree on transfer performance. Table 1 shows the same results for 127 classes obtained from top down clustering. Using only 486 classes results in a performance drop of 1.7 mAP for PASCAL-DET, 0.8% accuracy for SUN-CLS and a boost\nof 0.6 mAP for PASCAL-ACT-CLS. Moreover, only diminishing returns in transfer performance are observed when more than 127 classes are used.\nIt can be argued that the PASCAL task requires discrimination between only 20 classes and therefore pre-training with only 127 classes should not lead to substantial reduction in performance. However, the trend also holds true for SUN-CLS that requires discrimination between 397 classes. These two results taken together suggest that although training with a large number of classes is beneficial, diminishing returns are observed beyond using 127 distinct classes for pre-training.\nFurthermore, for PASCAL-ACT-CLS and SUN-CLS, finetuning on CNNs pre-trained with class set sizes of 918, and 753 actually results in better performance than using all 1000 classes. This can indicate a few things: either having too many classes for pre-training works against learning good generalizable features, or that the original labeling set of ImageNet is suboptimal. If latter is the case, then there exists a better labeling set for ImageNet that can improve the performance on transfer tasks. Though nontrivial, it may be interesting to investigate discovering such an optimal label set."}, {"heading": "5.2. Is fine-grain recognition necessary for learning", "text": "transferable features?\nImageNet challenge requires a classifier to distinguish between 1000 classes, some of which are very fine-grained, such as different breeds of dogs and cats. Indeed, most humans don\u2019t perform well on ImageNet unless specifically trained [37], and yet are easily able to perform most everyday visual tasks. This raises the question: is fine-grained\nrecognition necessary for CNN models to learn good feature representations, or is coarse-grained object recognition (e.g. just distinguishing cats from dogs) is sufficient?\nNote that the label set of 127 classes from the previous experiment contains 65 classes that are present in the original set of 1000 classes and the remainder are inner nodes of the WordNet tree. However, all these 127 classes (see supplementary materials) represent coarse semantic concepts. As discussed earlier, pre-training with these classes results in only a small drop in transfer performance (see Table 1). This suggests that performing fine-grained recognition is only marginally helpful and does not appear to be critical for learning good transferable features."}, {"heading": "5.3. Does training with coarse classes induce features relevant for fine-grained recognition?", "text": "Earlier, we have shown that the features learned on the 127 coarse classes perform almost as well on our transfer tasks as the full set of 1000 ImageNet classes. Here we will probe this further by asking a different question: is the feature embedding induced by the coarse class classification task capable of separating the fine labels of ImageNet (which it never saw at training)? To investigate this, we used top-1 and top-5 nearest neighbors in the FC7 feature space to measure the accuracy of identifying finegrained ImageNet classes after training only on a set of coarse classes. We call this measure, \u201cinduction accuracy\u201d. As a qualitative example, Figure 5 shows nearest neighbors for a macaque (left) and a schnauzer (right) for feature embeddings trained on ImageNet but with different number of classes. All green-border images below the dotted line indicate instances of correct fine-grain nearest neighbor retrieval for features that were never trained on that class.\nQuantitative results are shown in Figure 4. The results show that when 127 classes are used, fine-grained recognition k-NN performance is only about 15% lower compared to training directly for these fine-grained classes (i.e. baseline accuracy). This is rather surprising and suggests that\nCNNs implicitly discover features capable of distinguishing between finer classes while attempting to distinguish between relatively coarse classes."}, {"heading": "5.4. Does training with fine-grained classes induce features relevant for coarse recognition?", "text": "Investigating whether the network learns features relevant for fine-grained recognition by training on coarse classes begs the reverse question: does training with finegrained classes induce features relevant for coarse recognition? If this is indeed the case, then we would expect\nthat when a CNN makes an error, it is more likely to confuse a sub-class (i.e. error in fine-grained recognition) with other sub-classes of the same coarse class. This effect can be measured by computing the difference between the accuracy of classifying the coarse class and the average accuracy of individually classifying all the sub-classes of this coarse class (please see supplementary materials for details).\nFigure 6 shows the results. We find that coarse semantic classes such as dogs, reptiles, clothing etc that contain visually similar sub-classes show the hypothesized effect, whereas classes such as instruments and buildings that contain visually dissimilar subclasses do not exhibit this effect. These results indicate that subclasses that share a common visual structure allow the CNN to learn features that are more generalizable. This might suggest a way to improve feature generalization by making class labels respect visual commonality rather than simply WordNet semantics."}, {"heading": "5.5. More Classes or More Examples Per Class?", "text": "Results in previous sections show that it is possible to achieve good performance on transfer tasks using significantly less pre-training data and fewer pre-training classes. However it is unclear what is more important \u2013 the number of classes or the number or examples per class. One extreme is to only have 1 class and all 1.2M images from this class and the other extreme is to have 1.2M classes and 1 image per class. It is clear that both ways of splitting the data will result in poor generalization, so the answer must lie somewhere in-between.\nTo investigate this, we split the same amount of pretraining data in two ways: (1) more classes with fewer images per class, and (2) fewer classes with more images per class. We use datasets of size 500K, 250K and 125K images for this experiment. For 500K images, we considered two ways of constructing the training set \u2013 (1) 1000 classes with 500 images/class, and (2) 500 classes with 1000 images/class. Similar splits were made for data budgets of 250K and 125K images. The 500, 250 and 125 classes for these experiments were drawn from a uniform distribution among the 1000 ImageNet classes. Similarly, the image subsets containing 500, 250 and 125 images were drawn from a uniform distribution among the images that belong to the class.\nThe results presented in Table 2 show that having more\nimages per class with fewer number of classes results in features that perform very slightly better on PASCALDET, whereas for SUN-CLS, the performance is comparable across the two settings."}, {"heading": "5.6. How important is to pre-train on classes that", "text": "are also present in a target task?\nIt is natural to expect that higher correlation between pretraining and transfer tasks leads to better performance on a transfer task. This indeed has been shown to be true in [49]. One possible source of correlation between pre-training and transfer tasks are classes common to both tasks. In order to investigate how strong is the influence of these common classes, we ran an experiment where we removed all the classes from ImageNet that are contained in the PASCAL challenge. PASCAL has 20 classes, some of which map to more than one ImageNet class and thus, after applying this exclusion criterion we are only left with 771 ImageNet classes.\nTable 3 compares the results on PASCAL-DET when the PASCAL-removed-ImageNet is used for pre-training against the original ImageNet and a baseline of pretraining on the Places [53] dataset. The PASCAL-removedImageNet achieves mAP of 57.8 (compared to 58.3 with the full ImageNet) indicating that training on ImageNet classes that are not present in PASCAL is sufficient to learn features that are also good for PASCAL classes."}, {"heading": "6. Does data augmentation from non-target", "text": "classes always improve performance?\nThe analysis using PASCAL-removed ImageNet indicates that pre-training on non-PASCAL classes aids performance on PASCAL. This raises the question: is it always better to add pre-training data from additional classes that are not part of the target task? We investigated this question by first splitting ImageNet classes into two sets randomly: ran-split A and ran-split B (Figure 7). In order to determine if additional data helps performance for classes in split A, we pre-trained two CNNs \u2013 one for classifying all classes in split A and the other for classifying all classes in both split A and B (i.e. full dataset). We then finetuned the last layer of the network trained on the full dataset on split A only. If it is the case that additional data from split B helps performance on split A, then the CNN pre-trained with the full dataset should perform better than CNN pre-trained only on split A. Figure 9 shows the results of this experiment for both splits and confirms the intuition that additional data is indeed useful.\nHowever, under a random class split within ImageNet, we are almost certain to have extremely similar classes (e.g. two different breeds of dogs) ending up on the different sides of the split. So, what we have shown so far is that we can improve performance on, say, husky classification by also training on poodles. But we can also ask: does adding arbitrary, unrelated classes, such as fire trucks, help dog classification?\nTo test this, we constructed the \u201cminimal\u201d ImageNet split. The classes in minimal split A do not share any com-\nmon ancestor with minimal split B up until the nodes at depth 4 of the WordNet hierarchy (Figure 7). This ensures that any class in split A is sufficiently disjoint from split B. Split A has 522 classes and split B has 478 classes (N.B.: for consistency, random splits A and B also had the same number of classes). In order to intuitively understand the difference between min splits A and B, we have visualized a random sample of images in these splits in Figure 8. Min split A consists of mostly static images and min split B consists of living objects.\nContrary to the earlier observation, Figure 9 shows that both min split A and B performs better than the full dataset irrespective of whether we finetune only the last layer or all the layers. This result is quite surprising because it shows that, even when all layers are finetuned from a network pretrained on the full dataset, it is not possible to match the performance of a network trained on just one split.\nWhile it might be possible to recover performance with very clever adjustments of learning rates, current results suggest that training with data from unrelated classes may push the network into a local minimum from which it might be hard to find a better optima that can be obtained by training the network from scratch."}, {"heading": "7. Discussion", "text": "In this work we analyzed factors that affect the quality of ImageNet pre-trained features for transfer learning. The results were quite surprising. For example, we have found that a significant reduction in the number classes or the number of images used in pre-training has only a modest effect\non transfer task performance. Indeed, we observed that the pre-trained features were surprisingly resilient to the various changes in the training data. While we do not have an explanation as to the cause of this resilience, we list some speculative possibilities that should inform further study of this topic:\n\u2022 In our experiments, we investigated only one CNN architecture \u2013 AlexNet. While ImageNet-trained AlexNet features are currently the most popular starting point for fine-tuning on transfer tasks, there exist deeper architectures such as VGG [42] and ResNet [19]. It would be interesting to see if any of our findings are consistent when applied to these deeper networks. If so, it might suggest that AlexNet capacity is less than previously though.\n\u2022 Our results might indicate that researchers have been overestimating the amount of data required for learning good general CNN features. If that is the case, it might suggest that CNN training is not as data-hungry as previously thought. It would also dash the hopes of beating ImageNet-trained features with unsupervised feature learning on a much bigger data corpus.\n\u2022 Finally, it might be that the currently popular target tasks, such as PASCAL and SUN, are all too similar to the original ImageNet task to really test the generalisation ability of the learned features. Or that generalization should be tested with much less fine-tuning (e.g. one-shot-learning) or no fine-tuning at all (e.g. nearest neighbour in the learned feature space).\nIn conclusion: the answer to the titular question \u201cwhat makes ImageNet good for transfer learning?\u201d remains elusive for the time being. But the results of the experiments performed in the present study give rise to even more tantalizing questions. We hope that this work will pique our colleagues\u2019 curiosity and facilitate further research on this fascinating topic."}, {"heading": "8. Acknowledgements", "text": "This work was supported in part by ONR MURI N00014-14-1-0671. We gratefully acknowledge NVIDIA corporation for the donation of K40 GPUs and access to the NVIDIA PSG cluster for this research. We would like to acknowledge the support from the Berkeley Vision and Learning Center (BVLC) and Berkeley DeepDrive (BDD). Minyoung Huh was partially supported by the Rose Hill Foundation."}, {"heading": "1. Does additional pre-training data always improve performance? - Finetuning all layers", "text": "In Section 4 of the main paper, we investigated if additional pre-training data always improves performance. We presented results under the experimental paradigm where only the last layer of the network was finetuned. Another common practice while using pre-trained network is to finetune all the layers. We also finetuned in this way and the results are reported in Table 1. The obtained results follow the same trend as in the main paper. The most interesting bit is that even when all layers of a network pre-trained on minimal split A+B are finetuned on a single split, it performs worse than directly training on that split. This suggests that it might be the case, pre-training with A+B both, pushed the network parameters in a local minima from which it is not possible to recover a solution that is as good as starting from scratch."}, {"heading": "2. Visualizing how training evolves over time", "text": "Does a CNN learn to distinguish coarse semantic classes prior to learning fine grain classes? We investigated this by visualizing FC7 nearest neighbors of randomly chosen images at different time steps during the training (see Figure 3). The visualization shows that even a randomly initialized network preserves some semantic structure. During training, the network learns to first perform matching based on color and texture and slowly hones into features that are required for more fine-grained recognition that is required to solve the ImageNet challenge. Figure 5 conveys similar information through t-SNE visualization. One thing worth noticing is the shift in the t-SNE embedding space from a almost connected sphere to disjoint connected components in the early phases of training."}, {"heading": "3. Visualizing the effect of number of pretraining classes", "text": "Additional visualizations for results presented in Figure 5 and Section 5.3 of the main paper are shown in Figure 4."}, {"heading": "4. Visualizing ImageNet", "text": "When we train for the leaf nodes of WordNet, how well do the learnt features perform on classifying the intermediate nodes? Ideally, we would like to visualize this by color coding all nodes of the WordNet tree by their accuracy. However, as the WordNet tree is quite imbalanced, visualizing the entire tree is tough. However, there are several\nOur work can be found on our website http://minyounghuh.com/papers/analysis\ngraph visualization algorithms that have been developed to solve this problem. We used a linear time variant of the Reingold-Tilford algorithm [8] to visualize the accuracy of all WordNet nodes in Figure 2. A higher resolution figure is available on the project website."}, {"heading": "5. Sparsity", "text": "Around 80% sparsity in FC7 features of AlexNet trained on ImageNet provides an attractive compression attribute for data storage and fast image querying. Figure 1 shows the average FC7 feature sparsity when different number of pre-training classes are used. The results indicate that with the need of distinguising between finer classes, the features become more sparse."}, {"heading": "6. Computing coarse class classification accuracy", "text": "In Section 5.3 of the main paper, we discussed, \u201cDoes training with fine-grained classes induce features relevant for coarse recognition\u201d. In this section, we computed the difference between the accuracy of the coarse class and the average of sub-class accuracies. In order to account for the different number of examples in each coarse class, we normalized the accuracy in the following way: Let \u03a6(A) be the normalized accuracy for class A. Let x denote the ground truth class of an image and x\u0302 denote the predicted class. Then \u03a6(A) is defined as below:\n\u03a6(A) = P (x\u0302 \u2208 A|x \u2208 A) + P (x\u0302 /\u2208 A|x /\u2208 A)\n2"}, {"heading": "7. List of classes 127 classes", "text": "In Section 5.1, we generated different label sets based on the hierarchy of the WordNet tree. The list below shows all the classes used in the 127 label set.\nn02856463 board n02638596 ganoid n04235291 sled n01428580 soft-finned fish n10401829 participant n04500060 turner n02512938 food fish n03664943 ligament n03446832 golf equipment n03764276 military vehicle n03122748 covering n06874019 light n03619396 kit n04128837 sailing vessel n03528263 home appliance n04100174 rod n03880531 pan n02924116 bus n01693783 chameleon n04015204 protective garment n01482330 shark n06793231 sign n01629276 salamander n13134947 fruit n02954340 cap n09820263 athlete n09214060 bar n01687665 agamid n03476083 hairpiece n01698434 alligator n03241093 drill rig n03151500 cushion n01703569 ceratopsian n01861778 mammal n03450516 gown n01495701 ray n03678362 litter n04571292 weight n03597469 jewelry n04077734 rescue equipment n03294833 eraser n01909422 coelenterate n07929519 coffee n04285622 sports implement n03497657 hat n07930554 punch n01692864 lacertid lizard n07683786 loaf of bread n03039947 cleaning implement n04447443 toiletry n02642644 scorpaenid n10019552 diver n01726692 snake n02942699 camera n04317420 stick n01767661 arthropod n01674990 gecko n03309808 fabric n03257586 duplicator n07829412 sauce n01922303 worm n01940736 mollusk n07882497 concoction n02605316 butterfly fish\nn07891726 wine n06595351 magazine n09287968 geological formation n07681926 cracker n03540267 hosiery n01691951 venomous lizard n02606384 damselfish n03513137 helmet n03510583 heavier-than-air craft n07557434 dish n07707451 vegetable n02858304 boat n01689411 anguid lizard n03964744 plaything n01503061 bird n15074962 tissue n12992868 fungus n04099429 rocket n07582609 dip n03472232 gymnastic apparatus n03414162 game equipment n03035510 cistern n03236735 dress n01662784 turtle n03419014 garment n03613592 key n04509592 uniform n07612996 pudding n01697178 crocodile n03405725 furniture n11669921 flower n02316707 echinoderm n07680932 bun n03278248 electronic equipment n03896233 passenger train n04125853 safety belt n03906997 pen n04341686 structure n07579575 entree n03825080 nightwear n01685439 teiid lizard n03990474 pot n07560652 fare n04264914 spacecraft n01676755 iguanid n03441112 glove n07800740 fodder n02807260 bath linen n07611358 frozen dessert n04377057 system n03101156 cooker n03666917 lighter-than-air craft n03094503 container n03183080 device n04451818 tool n09289709 globule n03837422 oar n04185071 sharpener n04194289 ship n03961939 platform n01694709 monitor n02652668 plectognath n01639765 frog\nA complete list of classes used in the experiment studying the effect of the number of pre-training classes can be found on our website."}], "references": [{"title": "Learning to see by moving", "author": ["P. Agrawal", "J. Carreira", "J. Malik"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 37\u201345,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the performance of multilayer neural networks for object recognition", "author": ["P. Agrawal", "R. Girshick", "J. Malik"], "venue": "Computer Vision\u2013ECCV 2014, pages 329\u2013344. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding deep features with computer-generated imagery", "author": ["M. Aubry", "B.C. Russell"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2875\u20132883,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "From generic to specific deep representations for visual recognition", "author": ["H. Azizpour", "A. Razavian", "J. Sullivan", "A. Maki", "S. Carlsson"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 36\u201345,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Digging deep into the layers of cnns: In search of how cnns achieve view invariance", "author": ["A. Bakry", "M. Elhoseiny", "T. El-Gaaly", "A. Elgammal"], "venue": "arXiv preprint arXiv:1508.01983,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A.C. Courville", "P. Vincent"], "venue": "CoRR, abs/1206.5538, 1,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological cybernetics, 59(4-5):291\u2013294,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Improving walker\u2019s algorithm to run in linear time", "author": ["C. Buchheim", "M. J\u00fcnger", "S. Leipert"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Human pose estimation with iterative error feedback", "author": ["J. Carreira", "P. Agrawal", "K. Fragkiadaki", "J. Malik"], "venue": "arXiv preprint arXiv:1507.06550,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "arXiv preprint arXiv:1512.04412,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised visual representation learning by context prediction", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1422\u20131430,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2625\u20132634,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Inverting convolutional networks with convolutional networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "arXiv preprint arXiv:1506.02753,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Wordnet: An electronic lexical database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 580\u2013587. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual action recognition with rcnn", "author": ["G. Gkioxari", "R. Girshick", "J. Malik"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Unsupervised feature learning from temporal data", "author": ["R. Goroshin", "J. Bruna", "J. Tompson", "D. Eigen", "Y. LeCun"], "venue": "arXiv preprint arXiv:1504.02518,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning image representations tied to ego-motion", "author": ["D. Jayaraman", "K. Grauman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1413\u20131421,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe. berkeleyvision.org/,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Datadependent initializations of convolutional neural networks", "author": ["P. Kr\u00e4henb\u00fchl", "C. Doersch", "J. Donahue", "T. Darrell"], "venue": "ICLR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, 1(4):541\u2013551,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "Understanding deep convolutional networks", "author": ["S. Mallat"], "venue": "arXiv preprint arXiv:1601.04920,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 737\u2013744. ACM,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised learning of visual representations by solving jigsaw puzzles", "author": ["M. Noroozi", "F. Paolo"], "venue": "arXiv preprint arXiv:1603.09246v2,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1996}, {"title": "Visually indicated sounds", "author": ["A. Owens", "P. Isola", "J. McDermott", "A. Torralba", "E. Adelson", "F. William"], "venue": "CVPR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Context encoders: Feature learning by inpainting", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "J. Donahue", "T. Darrell", "A. Efros"], "venue": "CVPR,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["M. Ranzato", "F.J. Huang", "Y.-L. Boureau", "Y. LeCun"], "venue": "Computer Vision and  Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on, pages 1\u20138. IEEE,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806\u2013813,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in Neural Information Processing Systems, pages 91\u201399,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 448\u2013455,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems, pages 568\u2013576,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Discovering internal representations from object-cnns using population encoding", "author": ["J. Wang", "Z. Zhang", "V. Premachandran", "A. Yuille"], "venue": "arXiv preprint arXiv:1511.06855,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised learning of visual representations using videos", "author": ["X. Wang", "A. Gupta"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2794\u20132802,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding intra-class knowledge inside cnn", "author": ["D. Wei", "B. Zhou", "A. Torrabla", "W. Freeman"], "venue": "arXiv preprint arXiv:1507.02379,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1385\u20131392,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2013}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["L. Wiskott", "T.J. Sejnowski"], "venue": "Neural computation, 14(4):715\u2013770,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2002}, {"title": "How transferable are features in deep neural networks", "author": ["J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Computer vision\u2013ECCV 2014, pages 818\u2013833. Springer,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2014}, {"title": "Colorful image colorization", "author": ["R. Zhang", "P. Isola", "A. Efros"], "venue": "ECCV,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "arXiv preprint arXiv:1412.6856,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 36, "context": "It has become increasingly common within the computer vision community to treat image classification on ImageNet [37] not as an end in itself, but rather as a \u201cpretext task\u201d for training deep convolution neural networks (CNNs [27, 25]) to learn good general-purpose features.", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "It has become increasingly common within the computer vision community to treat image classification on ImageNet [37] not as an end in itself, but rather as a \u201cpretext task\u201d for training deep convolution neural networks (CNNs [27, 25]) to learn good general-purpose features.", "startOffset": 226, "endOffset": 234}, {"referenceID": 24, "context": "It has become increasingly common within the computer vision community to treat image classification on ImageNet [37] not as an end in itself, but rather as a \u201cpretext task\u201d for training deep convolution neural networks (CNNs [27, 25]) to learn good general-purpose features.", "startOffset": 226, "endOffset": 234}, {"referenceID": 12, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 120, "endOffset": 128}, {"referenceID": 34, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 120, "endOffset": 128}, {"referenceID": 15, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 147, "endOffset": 155}, {"referenceID": 38, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 147, "endOffset": 155}, {"referenceID": 40, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 176, "endOffset": 180}, {"referenceID": 8, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 204, "endOffset": 207}, {"referenceID": 9, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 228, "endOffset": 232}, {"referenceID": 46, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 247, "endOffset": 251}, {"referenceID": 11, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 270, "endOffset": 278}, {"referenceID": 21, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 270, "endOffset": 278}, {"referenceID": 25, "context": "Using ImageNet pre-trained CNN features, impressive results have been obtained on several image classification datasets [13, 35], object detection [16, 39], action recognition [41], human pose estimation [9], image segmentation [10], optical flow [47], image captioning [12, 22] and others [26].", "startOffset": 290, "endOffset": 294}, {"referenceID": 49, "context": "Understanding internal representations of CNN: For understanding what information is encoded by different CNN features, several works have developed feature visualization methods using either deconvolution [50], backpropagation[40, 52] or by training a decoder network [14].", "startOffset": 206, "endOffset": 210}, {"referenceID": 39, "context": "Understanding internal representations of CNN: For understanding what information is encoded by different CNN features, several works have developed feature visualization methods using either deconvolution [50], backpropagation[40, 52] or by training a decoder network [14].", "startOffset": 227, "endOffset": 235}, {"referenceID": 51, "context": "Understanding internal representations of CNN: For understanding what information is encoded by different CNN features, several works have developed feature visualization methods using either deconvolution [50], backpropagation[40, 52] or by training a decoder network [14].", "startOffset": 227, "endOffset": 235}, {"referenceID": 13, "context": "Understanding internal representations of CNN: For understanding what information is encoded by different CNN features, several works have developed feature visualization methods using either deconvolution [50], backpropagation[40, 52] or by training a decoder network [14].", "startOffset": 269, "endOffset": 273}, {"referenceID": 1, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 98, "endOffset": 105}, {"referenceID": 42, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 98, "endOffset": 105}, {"referenceID": 1, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 178, "endOffset": 185}, {"referenceID": 2, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 258, "endOffset": 261}, {"referenceID": 4, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 314, "endOffset": 317}, {"referenceID": 45, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 389, "endOffset": 397}, {"referenceID": 43, "context": "Another line of prior work investigated questions such as how distributed are CNN representations [2, 43], what is the the effect of feature transformations such as binarization [2, 14], how do CNN features change with systematic variations in scene factors [3], how invariant are features to different viewpoints [5] and how is class specific information organized in groups of CNN units [46, 44].", "startOffset": 389, "endOffset": 397}, {"referenceID": 42, "context": "An interesting observation that CNNs have \u201cblindspots\u201d was made by [43].", "startOffset": 67, "endOffset": 71}, {"referenceID": 27, "context": "Recently, a mathematical framework for analyzing CNN features was provided by [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Factors that affect fine-tuning: The question of whether pre-training should be terminated early to prevent overfitting and what layers should be used for transfer learning was studied by [2, 49].", "startOffset": 188, "endOffset": 195}, {"referenceID": 48, "context": "Factors that affect fine-tuning: The question of whether pre-training should be terminated early to prevent overfitting and what layers should be used for transfer learning was studied by [2, 49].", "startOffset": 188, "endOffset": 195}, {"referenceID": 3, "context": "A thorough investigation of good architectural choices for transfer learning was conducted by [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 59, "endOffset": 82}, {"referenceID": 37, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 59, "endOffset": 82}, {"referenceID": 30, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 59, "endOffset": 82}, {"referenceID": 28, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 59, "endOffset": 82}, {"referenceID": 33, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 59, "endOffset": 82}, {"referenceID": 22, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 59, "endOffset": 82}, {"referenceID": 5, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 88, "endOffset": 91}, {"referenceID": 47, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 143, "endOffset": 151}, {"referenceID": 17, "context": "The most well-known such criteria are image reconstruction [7, 38, 31, 29, 34, 23] (see [6] for a comprehensive overview) and feature slowness [48, 18].", "startOffset": 143, "endOffset": 151}, {"referenceID": 0, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 66, "endOffset": 73}, {"referenceID": 19, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 66, "endOffset": 73}, {"referenceID": 10, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 91, "endOffset": 103}, {"referenceID": 32, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 91, "endOffset": 103}, {"referenceID": 29, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 91, "endOffset": 103}, {"referenceID": 44, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 122, "endOffset": 126}, {"referenceID": 50, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "Attempted self-supervised tasks include predictions of ego-motion [1, 20], spatial context [11, 33, 30], temporal context [45], and even color [51] and sound [32].", "startOffset": 158, "endOffset": 162}, {"referenceID": 20, "context": "All of our experiments use the Caffe [21] implementation of the a single network architecture proposed by Krizhevsky et al.", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "[25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] for evaluating the generalization of pre-trained features on three transfer tasks: PASCAL VOC 2007 object detection (PASCAL-DET), PASCAL VOC 2012 action recognition (PASCAL-ACT-CLS) and scene classification on SUN dataset (SUN-CLS).", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "For PASCAL-DET, we used the PASCAL VOC 2007 train/val for finetuning using the experimental setup and code provided by FasterRCNN [36] and report performance on the test set.", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "For PASCAL-ACT-CLS, we used PASCAL VOC 2012 train/val for finetuning and testing using the experimental setup and code provided by R*CNN [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 1, "context": "For SUN-CLS we used the same train/val/test splits as used by [2].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "As reported by [2], error bars on SUN-CLS are small, therefore having a single run should not affect the conclusions that we draw.", "startOffset": 15, "endOffset": 18}, {"referenceID": 36, "context": "The 1000 classes of the ImageNet challenge [37] are derived from leaves of the WordNet tree [15].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "The 1000 classes of the ImageNet challenge [37] are derived from leaves of the WordNet tree [15].", "startOffset": 92, "endOffset": 96}, {"referenceID": 36, "context": "Indeed, most humans don\u2019t perform well on ImageNet unless specifically trained [37], and yet are easily able to perform most everyday visual tasks.", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "3 [24] 30.", "startOffset": 2, "endOffset": 6}, {"referenceID": 48, "context": "This indeed has been shown to be true in [49].", "startOffset": 41, "endOffset": 45}, {"referenceID": 52, "context": "Table 3 compares the results on PASCAL-DET when the PASCAL-removed-ImageNet is used for pre-training against the original ImageNet and a baseline of pretraining on the Places [53] dataset.", "startOffset": 175, "endOffset": 179}, {"referenceID": 41, "context": "While ImageNet-trained AlexNet features are currently the most popular starting point for fine-tuning on transfer tasks, there exist deeper architectures such as VGG [42] and ResNet [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "While ImageNet-trained AlexNet features are currently the most popular starting point for fine-tuning on transfer tasks, there exist deeper architectures such as VGG [42] and ResNet [19].", "startOffset": 182, "endOffset": 186}], "year": 2016, "abstractText": "The tremendous success of features learnt using the ImageNet classification task on a wide range of transfer tasks begs the question: what are the intrinsic properties of the ImageNet dataset that are critical for learning good, general-purpose features? This work provides an empirical investigation of various facets of this question: Is more pre-training data always better? How does feature quality depend on the number of training examples per class? Does adding more object classes improve performance? For the same data budget, how should the data be split into classes? Is fine-grained recognition necessary for learning good features? Given the same number of training classes, is it better to have coarse classes or fine-grained classes? Which is better: more classes or more examples per class?", "creator": "LaTeX with hyperref package"}}}