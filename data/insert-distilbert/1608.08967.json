{"id": "1608.08967", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Robustness of classifiers: from adversarial to random noise", "abstract": "several recent works have shown that state - of - the - art classifiers are vulnerable to worst - case ( versus i. e., adversarial ) perturbations of the datapoints. on the other hand, it has been empirically simultaneously observed that these same classifiers conversely are relatively robust to random noise. in this paper, we propose to study a \\ textit { semi - random } noise regime that generalizes both the random and worst - case noise regimes. we propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. we establish precise theoretical bounds on the general robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. moreover, we quantify the functional robustness of classifiers in terms of the subspace dimension in the semi - random noise regime, and show that our bounds remarkably interpolate between the worst - case and convex random noise regimes. we perform experiments and show that the derived bounds provide very accurate estimates when applied to various state - public of - the - art deep neural networks and datasets. this result typically suggests bounds on the curvature of establishing the classifiers'decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification classification problems.", "histories": [["v1", "Wed, 31 Aug 2016 17:54:34 GMT  (6040kb,D)", "http://arxiv.org/abs/1608.08967v1", "Accepted to NIPS 2016"]], "COMMENTS": "Accepted to NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["alhussein fawzi", "seyed-mohsen moosavi-dezfooli", "pascal frossard"], "accepted": true, "id": "1608.08967"}, "pdf": {"name": "1608.08967.pdf", "metadata": {"source": "CRF", "title": "Robustness of classifiers: from adversarial to random noise", "authors": ["Alhussein Fawzi"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [10] and speech processing [8]. An equally important property of a classifier that is often overlooked is its robustness in noisy regimes, when data samples are perturbed by noise. The robustness of a classifier is especially fundamental when it is deployed in real-world, uncontrolled, and possibly hostile environments. In these cases, it is crucial that classifiers exhibit good robustness properties. In other words, a sufficiently small perturbation of a datapoint should ideally not result in altering the estimated label of a classifier. State-of-the-art deep neural networks have recently been shown to be very unstable to worst-case perturbations of the data (or equivalently, adversarial perturbations) [18]. In particular, despite the excellent classification performances of these classifiers, well-sought perturbations of the data can easily cause misclassification, since data points often lie very close to the decision boundary \u2217The first two authors contributed equally to this work.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 8.\n08 96\n7v 1\n[ cs\n.L G\n] 3\nof the classifier. Despite the importance of this result, the worst-case noise regime that is studied in [18] only represents a very specific type of noise. It furthermore requires the full knowledge of the classification model, which may be a hard assumption in practice.\nIn this paper, we precisely quantify the robustness of nonlinear classifiers in two practical noise regimes, namely random and semi-random noise regimes. In the random noise regime, datapoints are perturbed by noise with random direction in the input space. The semi-random regime generalizes this model to random subspaces of arbitrary dimension, where a worst-case perturbation is sought within the subspace. In both cases, we derive bounds that precisely describe the robustness of classifiers in function of the curvature of the decision boundary. We summarize our contributions as follows:\n\u2022 In the random regime, we show that the robustness of classifiers behaves as \u221a d times the\ndistance from the datapoint to the classification boundary (where d denotes the dimension of the data) provided the curvature of the decision boundary is sufficiently small. This result highlights the blessing of dimensionality for classification tasks, as it implies that robustness to random noise in high dimensional classification problems can be achieved, even at datapoints that are very close to the decision boundary.\n\u2022 This quantification notably extends to the general semi-random regime, where we show that the robustness precisely behaves as \u221a d/m times the distance to boundary, with m the\ndimension of the subspace. This result shows in particular that, even when m is chosen as a small fraction of the dimension d, it is still possible to find small perturbations that cause data misclassification.\n\u2022 We empirically show that our theoretical estimates are very accurately satisfied by stateof-the-art deep neural networks on various sets of data. This in turn suggests quantitative insights on the curvature of the decision boundary that we support experimentally through the visualization and estimation on two-dimensional sections of the boundary.\nThe robustness of classifiers to noise has been the subject of intense research. The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11]. More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13]. In [19], the authors provide an interesting empirical analysis of the adversarial instability, and show that adversarial examples are not isolated points, but rather occupy dense regions of the pixel space. In [5], state-of-the-art classifiers are shown to be vulnerable to geometrically constrained adversarial examples. Our work differs from these works, as we provide a theoretical study of the robustness of classifiers to random and semi-random noise in terms of the robustness to adversarial noise. In [4], a formal relation between the robustness to random noise, and the worst-case robustness is established in the case of linear classifiers. Our result therefore generalizes [4] in many aspects, as we study general nonlinear classifiers, and robustness to semi-random noise. Finally, it should be noted that the authors in [6] conjecture that the \u201chigh linearity\u201d of classification models explains their instability to adversarial perturbations. The objective and approach we follow here is however different, as we study theoretical relations between the robustness to random, semi-random and adversarial noise."}, {"heading": "2 Definitions and notations", "text": "Let f : Rd \u2192 RL be an L-class classifier. Given a datapoint x0 \u2208 Rd, the estimated label is obtained by k\u0302(x0) = argmaxk fk(x0), where fk(x) is the k th component of f(x) that corresponds to the kth class. Let S be an arbitrary subspace of Rd of dimensionm. Here, we are interested in quantifying the robustness of f with respect to different noise regimes. To do so, we define r\u2217S to be the perturbation in S of minimal norm that is required to change the estimated label of f at x0.2\nr\u2217S(x0) = argmin r\u2208S \u2016r\u20162 s.t. k\u0302(x0 + r) 6= k\u0302(x0). (1)\n2Perturbation vectors sending a datapoint exactly to the boundary are assumed to change the estimated label of the classifier.\nNote that r\u2217S(x0) can be equivalently written\nr\u2217S(x0) = argmin r\u2208S \u2016r\u20162 s.t. \u2203k 6= k\u0302(x0) : fk(x0 + r) \u2265 fk\u0302(x0)(x0 + r). (2)\nWhen S = Rd, r\u2217(x0) := r\u2217Rd(x0) is the adversarial (or worst-case) perturbation defined in [18], which corresponds to the (unconstrained) perturbation of minimal norm that changes the label of the datapoint x0. In other words, \u2016r\u2217(x0)\u20162 corresponds to the minimal distance from x0 to the classifier boundary. In the case where S \u2282 Rd, only perturbations along S are allowed. The robustness of f at x0 along S is naturally measured by the norm \u2016r\u2217S(x0)\u20162. Different choices for S permit to study the robustness of f in two different regimes:\n\u2022 Random noise regime: This corresponds to the case where S is a one-dimensional subspace (m = 1) with direction v, where v is a random vector sampled uniformly from the unit sphere Sd\u22121. Writing it explicitly, we study in this regime the robustness quantity defined by mint |t| s.t. \u2203k 6= k\u0302(x0), fk(x0 + tv) \u2265 fk\u0302(x0)(x0 + tv), where v is a vector sampled uniformly at random from the unit sphere Sd\u22121.\n\u2022 Semi-random noise regime: In this case, the subspace S is chosen randomly, but can be of arbitrary dimension m.3 We use the semi-random terminology as the subspace is chosen randomly, and the smallest vector that causes misclassification is then sought in the subspace. It should be noted that the random noise regime is a special case of the semi-random regime with a subspace of dimension m = 1. We differentiate nevertheless between these two regimes for clarity.\nIn the remainder of the paper, the goal is to establish relations between the robustness in the random and semi-random regimes on the one hand, and the robustness to adversarial perturbations \u2016r\u2217(x0)\u20162 on the other hand. We recall that the latter quantity captures the distance from x0 to the classifier boundary, and is therefore a key quantity in the analysis of robustness.\nIn the following analysis, we fix x0 to be a datapoint classified as k\u0302(x0). To simplify the notation, we remove the explicit dependence on x0 in our notations (e.g., we use r\u2217S instead of r \u2217 S(x0) and k\u0302 instead of k\u0302(x0)), and it should be implicitly understood that all our quantities pertain to the fixed datapoint x0."}, {"heading": "3 Robustness of affine classifiers", "text": "We first assume that f is an affine classifier, i.e., f(x) = W>x + b for a given W = [w1 . . .wL] and b \u2208 RL. The following result shows a precise relation between the robustness to semi-random noise, \u2016r\u2217S\u20162 and the robustness to adversarial perturbations, \u2016r\u2217\u20162. Theorem 1. Let \u03b4 > 0 and S be a random m-dimensional subspace of Rd, and f be a L-class affine classifier. Let\n\u03b61(m, \u03b4) =\n( 1 + 2 \u221a ln(1/\u03b4)\nm +\n2 ln(1/\u03b4)\nm\n)\u22121 , (3)\n\u03b62(m, \u03b4) =\n( max ( (1/e)\u03b42/m, 1\u2212 \u221a 2(1\u2212 \u03b42/m) ))\u22121 . (4)\nThe following inequalities hold between the robustness to semi-random noise \u2016r\u2217S\u20162, and the robustness to adversarial perturbations \u2016r\u2217\u20162:\u221a\n\u03b61(m, \u03b4)\n\u221a d\nm \u2016r\u2217\u20162 \u2264 \u2016r\u2217S\u20162 \u2264\n\u221a \u03b62(m, \u03b4)\n\u221a d\nm \u2016r\u2217\u20162, (5)\nwith probability exceeding 1\u2212 2(L+ 1)\u03b4.\n3A random subspace is defined as the span of m independent vectors drawn uniformly at random from Sd\u22121.\nThe proof can be found in the appendix. Our upper and lower bounds depend on the functions \u03b61(m, \u03b4) and \u03b62(m, \u03b4) that control the inequality constants (for m, \u03b4 fixed). It should be noted that \u03b61(m, \u03b4) and \u03b62(m, \u03b4) are independent of the data dimension d. Fig. 1 shows the plots of \u03b61(m, \u03b4) and \u03b62(m, \u03b4) as functions of m, for a fixed \u03b4. It should be noted that for sufficiently large m, \u03b61(m, \u03b4) and \u03b62(m, \u03b4) are very close to 1 (e.g., \u03b61(m, \u03b4) and \u03b62(m, \u03b4) belong to the interval [0.8, 1.3] for m \u2265 250 in the settings of Fig. 1). The interval [\u03b61(m, \u03b4), \u03b62(m, \u03b4)] is however (unavoidably) larger when m = 1.\nThe result in Theorem 1 shows that in the random and semi-random noise regimes, the robustness to noise is precisely related to \u2016r\u2217\u20162 by a factor of \u221a d/m. Specifically, in the random noise regime\n(m = 1), the magnitude of the noise required to misclassify the datapoint behaves as \u0398( \u221a d\u2016r\u2217\u20162) with high probability, with constants in the interval [\u03b61(1, \u03b4), \u03b62(1, \u03b4)]. Our results therefore show that, in high dimensional classification settings, affine classifiers can be robust to random noise, even if the datapoint lies very closely to the decision boundary (i.e., \u2016r\u2217\u20162 is small). In the semi-random noise regime with m sufficiently large (e.g., m \u2265 250), we have \u2016r\u2217S\u20162 \u2248 \u221a d/m\u2016r\u2217\u20162 with high probability, as the constants \u03b61(m, \u03b4) \u2248 \u03b62(m, \u03b4) \u2248 1 for sufficiently large m. Our bounds therefore \u201cinterpolate\u201d between the random noise regime, which behaves as \u221a d\u2016r\u2217\u20162, and the worst-case noise \u2016r\u2217\u20162. More importantly, the square root dependence is also notable here, as it shows that the semirandom robustness can remain small even in regimes where m is chosen to be a very small fraction of d. For example, choosing a small subspace of dimension m = 0.01d results in semi-random robustness of 10\u2016r\u2217\u20162 with high probability, which might still not be perceptible in complex visual tasks. Hence, for semi-random noise that is mostly random and only mildly adversarial (i.e., the subspace dimension is small), affine classifiers remain vulnerable to such noise."}, {"heading": "4 Robustness of general classifiers", "text": ""}, {"heading": "4.1 Decision boundary curvature", "text": "We now consider the general case where f is a nonlinear classifier. We derive relations between the random and semi-random robustness \u2016r\u2217S\u20162 and worst-case robustness \u2016r\u2217\u20162 using properties of the classifier\u2019s boundary. Let i and j be two arbitrary classes; we define the pairwise boundary Bi,j as the boundary of the binary classifier where only classes i and j are considered. Formally, the decision boundary Bi,j reads as follows:\nBi,j = {x \u2208 Rd : fi(x)\u2212 fj(x) = 0}. The boundary Bi,j separates between two regions of Rd, namelyRi andRj , where the estimated label of the binary classifier is respectively i and j. Specifically, we have\nRi = {x \u2208 Rd : fi(x) > fj(x)}, Rj = {x \u2208 Rd : fj(x) > fi(x)}.\nWe assume for the purpose of this analysis that the boundary Bi,j is smooth. We are now interested in the geometric properties of the boundary, namely its curvature. There are many notions of curvature that one can define on hypersurfaces [12]. In the simple case of a curve in a two-dimensional space, the curvature is defined as the inverse of the radius of the so-called oscullating circle. One way to define curvature for high-dimensional hypersurfaces is by taking normal sections of the hypersurface, and looking at the curvature of the resulting planar curve (see Fig. 4). We however introduce a notion of curvature that is specifically suited to the analysis of the decision boundary of a classifier. Informally, our curvature captures the global bending of the decision boundary by inscribing balls in the regions separated by the decision boundary.\nWe now formally define this notion of curvature. For a given p \u2208 Bi,j , we define qi \u2016 j(p) to be the radius of the largest open ball included in the regionRi that intersects with Bi,j at p; i.e.,\nqi \u2016 j(p) = sup z\u2208Rd\n{\u2016z \u2212 p\u20162 : B(z, \u2016z \u2212 p\u20162) \u2286 Ri} , (6)\nwhere B(z, \u2016z \u2212 p\u20162) is the open ball in Rd of center z and radius \u2016z \u2212 p\u20162. An illustration of this quantity in two dimensions is provided in Fig. 2. It is not hard to see that any ball B(z\u2217, \u2016z\u2217 \u2212 p\u20162) centered in z\u2217 and included inRi will have its tangent space at p coincide with the tangent of the decision boundary at the same point.\nIt should further be noted that the definition in Eq. (6) is not symmetric in i and j; i.e., qi \u2016 j(p) 6= qj \u2016 i(p) as the radius of the largest ball one can inscribe in both regions need not be equal. We therefore define the following symmetric quantity qi,j(p), where the worst-case ball inscribed in any of the two regions is considered:\nqi,j(p) = min ( qi \u2016 j(p), qj \u2016 i(p) ) .\nThis definition describes the curvature of the decision boundary locally at p by fitting the largest ball included in one of the regions. To measure the global curvature, the worst-case radius is taken over all points on the decision boundary, i.e.,\nq(Bi,j) = inf p\u2208Bi,j qi,j(p), (7)\n\u03ba(Bi,j) = 1\nq(Bi,j) . (8)\nThe curvature \u03ba(Bi,j) is simply defined as the inverse of the worst-case radius over all points p on the decision boundary.\nIn the case of affine classifiers, we have \u03ba(Bi,j) = 0, as it is possible to inscribe balls of infinite radius inside each region of the space. When the classification boundary is a union of (sufficiently distant) spheres with equal radius R (see Fig. 3), the curvature \u03ba(Bi,j) = 1/R. In general, the quantity \u03ba(Bi,j) provides an intuitive way of describing the nonlinearity of the decision boundary by fitting balls inside the classification regions.\nIn the following section, we show a precise characterization of the robustness to semi-random and random noise of nonlinear classifiers in terms of the curvature of the decision boundaries \u03ba(Bi,j)."}, {"heading": "4.2 Robustness to random and semi-random noise", "text": "We now establish bounds on the robustness to random and semi-random noise in the binary classification case. Let x0 be a datapoint classified as k\u0302 = k\u0302(x0). We first study the binary classification problem, where only classes k\u0302 and k \u2208 {1, . . . , L}\\{k\u0302} are considered. To simplify the notation, we let Bk := Bk,k\u0302 be the decision boundary between classes k and k\u0302. In the case of the binary classifi-\ncation problem where classes k and k\u0302 are considered, the semi-random robustness and adversarial (or worst-case) robustness defined in Eq. (2) can be re-written as follows:\nrkS = argmin r\u2208S \u2016r\u20162 s.t. fk(x0 + r) \u2265 fk\u0302(x0 + r),\nrk = argmin r \u2016r\u20162 s.t. fk(x0 + r) \u2265 fk\u0302(x0 + r).\n(9)\nFor a randomly chosen subspace, \u2016rkS\u20162 is the random or semi-random robustness of the classifier, in the setting where only the two classes k and k\u0302 are considered. Likewise, \u2016rk\u20162 denotes the worst-case robustness in this setting. It should be noted that the global quantities r\u2217S and r\n\u2217 are obtained from rkS and r k by taking the vectors with minimum norm over all classes k.\nThe following result gives upper and lower bounds on the ratio \u2016r k S\u20162\n\u2016rk\u20162 in function of the curvature of\nthe boundary separating class k and k\u0302. Theorem 2. Let S be a random m-dimensional subspace of Rd. Let \u03ba := \u03ba(Bk). Assuming that the curvature satisfies\n\u03ba \u2264 C \u03b62(m, \u03b4)\u2016rk\u20162 m d ,\nthe following inequality holds between the semi-random robustness \u2016rkS\u20162 and the adversarial robustness \u2016rk\u20162:(\n1\u2212 C1\u2016rk\u20162\u03ba\u03b62(m, \u03b4) d\nm\n)\u221a \u03b61(m, \u03b4) \u221a d\nm \u2264 \u2016r k S\u20162 \u2016rk\u20162 \u2264 ( 1 + C2\u2016rk\u20162\u03ba\u03b62(m, \u03b4) d m )\u221a \u03b62(m, \u03b4) \u221a d\nm (10)\nwith probability larger than 1\u2212 4\u03b4. We recall that \u03b61(m, \u03b4) and \u03b62(m, \u03b4) are defined in Eq. (3, 4). The constants are C = 0.2, C1 = 0.625, C2 = 2.25.\nThe proof can be found in the appendix. This result shows that the bounds relating the robustness to random and semi-random noise to the worst-case robustness can be extended to nonlinear classifiers,\nprovided the curvature of the boundary \u03ba(Bk) is sufficiently small. In the case of linear classifiers, we have \u03ba(Bk) = 0, and we recover the result for affine classifiers from Theorem 1.\nTo extend this result to multi-class classification, special care has to be taken. In particular, if k denotes a class that has no boundary with class k\u0302, we have \u2016rk\u20162 =\u221e, and the previous curvature condition cannot be satisfied. It is therefore crucial to exclude such classes that have no boundary in common with class k\u0302, or more generally, boundaries that are far from class k\u0302. We define the set A of excluded classes k where \u2016rk\u20162 is large\nA = {k : \u2016rk\u20162 \u2265 1.45 \u221a \u03b62(m, \u03b4)\n\u221a d\nm \u2016r\u2217\u20162}. (11)\nNote that A is independent of S, and depends only on d, m and \u03b4. Moreover, the constants in (11) were chosen for simplicity of exposition.\nAssuming a curvature constraint only on the close enough classes, the following result establishes a simplified relation between \u2016r\u2217S\u20162 and \u2016r\u2217\u20162. Corollary 1. Let S be a random m-dimensional subspace of Rd. Assume that, for all k /\u2208 A, we have\n\u03ba(Bk)\u2016rk\u20162 \u2264 0.2\n\u03b62(m, \u03b4)\nm d (12)\nThen, we have 0.875 \u221a \u03b61(m, \u03b4)\n\u221a d\nm \u2016r\u2217\u20162 \u2264 \u2016r\u2217S\u20162 \u2264 1.45\n\u221a \u03b62(m, \u03b4)\n\u221a d\nm \u2016r\u2217\u20162 (13)\nwith probability larger than 1\u2212 4(L+ 2)\u03b4.\nUnder the curvature condition in (12) on the boundaries between k\u0302 and classes in Ac, our result shows that the robustness to random and semi-random noise exhibits the same behavior that has been observed earlier for linear classifiers in Theorem 1. In particular, \u2016r\u2217S\u20162 is precisely related to the adversarial robustness \u2016r\u2217\u20162 by a factor of \u221a d/m. In the random regime (m = 1), this factor\nbecomes \u221a d, and shows that in high dimensional classification problems, classifiers with sufficiently flat boundaries are much more robust to random noise than to adversarial noise. More precisely, the addition of a sufficiently small random noise does not change the label of the image, even if the image lies very closely to the decision boundary (i.e., \u2016r\u2217\u20162 is small). However, in the semi-random regime where an adversarial perturbation is found on a randomly chosen subspace of dimension m, the \u221a d/m factor that relates \u2016r\u2217S\u20162 to \u2016r\u2217\u20162 shows that robustness to semi-random noise might not be achieved even if m is chosen to be a tiny fraction of d (e.g., m = 0.01d). In other words, if a classifier is highly vulnerable to adversarial perturbations, then it is also vulnerable to noise that is overwhelmingly random and only mildly adversarial (i.e. worst-case noise sought in a random subspace of low dimensionality m).\nIt is important to note that the curvature condition in (12) is not an assumption on the curvature of the global decision boundary, but rather an assumption on the decision boundaries between pairs of classes. The distinction here is significant, as junction points where two decision boundaries meet might actually have a very large (or infinite) curvature (even in linear classification settings), and the curvature condition in (12) typically does not hold for this global curvature definition. We refer to our experimental section for a visualization of this phenomenon.\nWe finally stress that our results in Theorem 2 and Corollary 1 are applicable to any classifier, provided the decision boundaries are smooth. If we assume prior knowledge on the considered family of classifiers and their decision boundaries (e.g., the decision boundary is a union of spheres in Rd), similar bounds can further be derived under less restrictive curvature conditions (compared to Eq. (12))."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental results", "text": "We now evaluate the robustness of different image classifiers to random and semi-random perturbations, and assess the accuracy of our bounds on various datasets and state-of-the-art classifiers.\nSpecifically, our theoretical results show that the robustness \u2016r\u2217S(x)\u20162 of classifiers satisfying the curvature property precisely behaves as \u221a d/m\u2016r\u2217(x)\u20162. We first check the accuracy of these results in different classification settings. For a given classifier f and subspace dimension m, we define\n\u03b2(f ;m) = \u221a m/d 1 |D | \u2211 x\u2208D \u2016r\u2217S(x)\u20162 \u2016r\u2217(x)\u20162 ,\nwhere S is chosen randomly for each sample x and D denotes the test set. This quantity provides indication to the accuracy of our \u221a d/m\u2016r\u2217(x)\u20162 estimate of the robustness, and should ideally be equal to 1 (for sufficiently large m). Since \u03b2 is a random quantity (because of S), we report both its mean and standard deviation for different networks in Table 1. It should be noted that finding \u2016r\u2217S\u20162 and \u2016r\u2217\u20162 involves solving the optimization problem in (1). We have used a similar approach to [14] to find subspace minimal perturbations. For each network, we estimate the expectation by averaging \u03b2(f ;m) on 1000 random samples, with S also chosen randomly for each sample. Observe that \u03b2 is suprisingly close to 1, even when m is a small fraction of d. This shows that our quantitative analysis provide very accurate estimates of the robustness to semi-random noise. We visualize the robustness to random noise, semi-random noise (with m = 10) and worst-case perturbations on a sample image in Fig. 5. While random noise is clearly perceptible due to the\u221a d \u2248 400 factor, semi-random noise becomes much less perceptible even with a relatively small value of m = 10, thanks to the 1/\u221am factor that attenuates the required noise to misclassify the datapoint. It should be noted that the robustness of neural networks to adversarial perturbations has previously been observed empirically in [18], but we provide here a quantitative and generic explanation for this phenomenon.\nThe high accuracy of our bounds for different state-of-the-art classifiers, and different datasets suggest that the decision boundaries of these classifiers have limited curvature \u03ba(Bk), as this is a key assumption of our theoretical findings. To support the validity of this curvature hypothesis in practice, we visualize two-dimensional sections of the classifiers\u2019 boundary in Fig. 6 in three different settings. Note that we have opted here for a visualization strategy rather than the numerical estimation of \u03ba(B), as the latter quantity is difficult to approximate in practice in high dimensional problems. In Fig. 6, x0 is chosen randomly from the test set for each data set, and the decision boundaries are shown in the plane spanned by r\u2217 and r\u2217S , where S is a random direction (i.e., m = 1). Different colors on the boundary correspond to boundaries with different classes. It can be observed that\nthe curvature of the boundary is very small except at \u201cjunction\u201d points where the boundary of two different classes intersect. Our curvature assumption in Eq. (12), which only assumes a bound on the curvature of the decision boundary between pairs of classes k\u0302(x0) and k (but not on the global decision boundary that contains junctions with high curvature) is therefore adequate to the decision boundaries of state-of-the-art classifiers according to Fig. 6. Interestingly, the assumption in Corollary 1 is satisfied by taking \u03ba to be an empirical estimate of the curvature of the planar curves in Fig. 6 (a) for the dimension of the subspace being a very small fraction of d; e.g., m = 10\u22123d. While not reflecting the curvature \u03ba(Bk) that drives the assumption of our theoretical analysis, this result still seems to suggest that the curvature assumption holds in practice, and that the curvature of such classifiers is therefore very small. It should be noted that a related empirical observation was made in [6]; our work however provides a precise quantitative analysis on the relation between the curvature and the robustness in the semi-random noise regime.\nWe now show a simple demonstration of the vulnerability of classifiers to semi-random noise in Fig. 7, where a structured message is hidden in the image and causes data misclassification. Specifically, we consider S to be the span of random translated and scaled versions of words \u201cNIPS\u201d, \u201cSPAIN\u201d and \u201c2016\u201d in an image, such that bd/mc = 228. The resulting perturbations in the subspace are therefore linear combinations of these words with different intensities.4 The perturbed image x0 +r\u2217S shown in Fig. 7 (c) is clearly indistinguishable from Fig. 7 (a). This shows that imperceptibly small structured messages can be added to an image causing data misclassification."}, {"heading": "6 Conclusion", "text": "In this work, we precisely characterized the robustness of classifiers in a novel semi-random noise regime that generalizes the random noise regime. Specifically, our bounds relate the robustness in this regime to the robustness to adversarial perturbations. Our bounds depend on the curvature of the decision boundary, the data dimension, and the dimension of the subspace to which the perturbation belongs. Our results show, in particular, that when the decision boundary has a small curvature, classifiers are robust to random noise in high dimensional classification problems (even if the robustness to adversarial perturbations is relatively small). Moreover, for semi-random noise that is mostly random and only mildly adversarial (i.e., the subspace dimension is small), our results show that state-of-the-art classifiers remain vulnerable to such perturbations. To improve the robustness to semi-random noise, our analysis encourages to impose geometric constraints on the curvature of the decision boundary, as we have shown the existence of an intimate relation between the robustness of classifiers and the curvature of the decision boundary.\n4This example departs somehow from the theoretical framework of this paper, where random subspaces were considered. However, this empirical example suggests that the theoretical findings in this paper seem to approximately hold when the subspace S have statistics that are close to a random subspace."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their helpful comments. We thank Omar Fawzi and Louis Merlin for the fruitful discussions. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. This work has been partly supported by the Hasler Foundation, Switzerland, in the framework of the CORA project."}], "references": [{"title": "Robust optimization in machine learning", "author": ["C. Caramanis", "S. Mannor", "H. Xu"], "venue": "Optimization for machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "An elementary proof of a theorem of johnson and lindenstrauss", "author": ["S. Dasgupta", "A. Gupta"], "venue": "Random Structures & Algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations. CoRR, abs/1502.02590", "author": ["A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Manitest: Are classifiers really invariant", "author": ["A. Fawzi", "P. Frossard"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068", "author": ["S. Gu", "L. Rigazio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Learning with a strong adversary. CoRR, abs/1511.03034", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (NIPS), pages 1097\u20131105", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A robust minimax approach to classification", "author": ["G. Lanckriet", "L. Ghaoui", "C. Bhattacharyya", "M. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Manifolds and differential geometry, volume 107", "author": ["J.M. Lee"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Foveation-based mechanisms alleviate adversarial examples. arXiv preprint arXiv:1511.06292", "author": ["Y. Luo", "X. Boix", "G. Roig", "T. Poggio", "Q. Zhao"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deepfool: a simple and accurate method to fool deep neural networks", "author": ["Moosavi-Dezfooli", "S.-M", "A. Fawzi", "P. Frossard"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Adversarial manipulation of deep representations", "author": ["S. Sabour", "Y. Cao", "F. Faghri", "D.J. Fleet"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Understanding adversarial training: Increasing local stability of neural nets through robust optimization. arXiv preprint arXiv:1511.05432", "author": ["U. Shaham", "Y. Yamada", "S. Negahban"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations (ICLR)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Exploring the space of adversarial images", "author": ["P. Tabacof", "E. Valle"], "venue": "IEEE International Joint Conference on Neural Networks", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Robustness and regularization of support vector machines", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Suppressing the unusual: towards robust cnns using symmetric activation functions", "author": ["Q. Zhao", "L.D. Griffin"], "venue": "arXiv preprint arXiv:1603.05145", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [10] and speech processing [8].", "startOffset": 152, "endOffset": 156}, {"referenceID": 7, "context": "State-of-the-art classifiers, especially deep networks, have shown impressive classification performance on many challenging benchmarks in visual tasks [10] and speech processing [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 17, "context": "State-of-the-art deep neural networks have recently been shown to be very unstable to worst-case perturbations of the data (or equivalently, adversarial perturbations) [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "Despite the importance of this result, the worst-case noise regime that is studied in [18] only represents a very specific type of noise.", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11].", "startOffset": 245, "endOffset": 252}, {"referenceID": 10, "context": "The robustness properties of SVM classifiers have been studied in [20] for example, and robust optimization approaches for constructing robust classifiers have been proposed to minimize the worst possible empirical error under noise disturbance [1, 11].", "startOffset": 245, "endOffset": 252}, {"referenceID": 17, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 5, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 14, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 18, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 180, "endOffset": 194}, {"referenceID": 6, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 8, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 20, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 13, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 15, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 12, "context": "More recently, following the recent results on the instability of deep neural networks to worst-case perturbations [18], several works have provided explanations of the phenomenon [4, 6, 15, 19], and designed more robust networks [7, 9, 21, 14, 16, 13].", "startOffset": 230, "endOffset": 252}, {"referenceID": 18, "context": "In [19], the authors provide an interesting empirical analysis of the adversarial instability, and show that adversarial examples are not isolated points, but rather occupy dense regions of the pixel space.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "In [5], state-of-the-art classifiers are shown to be vulnerable to geometrically constrained adversarial examples.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "In [4], a formal relation between the robustness to random noise, and the worst-case robustness is established in the case of linear classifiers.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "Our result therefore generalizes [4] in many aspects, as we study general nonlinear classifiers, and robustness to semi-random noise.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Finally, it should be noted that the authors in [6] conjecture that the \u201chigh linearity\u201d of classification models explains their instability to adversarial perturbations.", "startOffset": 48, "endOffset": 51}, {"referenceID": 17, "context": "When S = R, r\u2217(x0) := rRd(x0) is the adversarial (or worst-case) perturbation defined in [18], which corresponds to the (unconstrained) perturbation of minimal norm that changes the label of the datapoint x0.", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "There are many notions of curvature that one can define on hypersurfaces [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "The VGG-F and VGG-19 are respectively introduced in [2, 17].", "startOffset": 52, "endOffset": 59}, {"referenceID": 16, "context": "The VGG-F and VGG-19 are respectively introduced in [2, 17].", "startOffset": 52, "endOffset": 59}, {"referenceID": 13, "context": "We have used a similar approach to [14] to find subspace minimal perturbations.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "It should be noted that the robustness of neural networks to adversarial perturbations has previously been observed empirically in [18], but we provide here a quantitative and generic explanation for this phenomenon.", "startOffset": 131, "endOffset": 135}, {"referenceID": 5, "context": "It should be noted that a related empirical observation was made in [6]; our work however provides a precise quantitative analysis on the relation between the curvature and the robustness in the semi-random noise regime.", "startOffset": 68, "endOffset": 71}], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}