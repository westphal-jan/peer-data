{"id": "1609.00081", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "All Fingers are not Equal: Intensity of References in Scientific Articles", "abstract": "research accomplishment is usually measured uniquely by considering all citations with typically equal importance, thus ignoring the wide variety of reporting purposes an article is being cited for. here, we posit that measuring the intensity dimension of a reference is made crucial not somehow only to perceive better understanding of research endeavor, but also to improve the quality of citation - based applications. to this end, we collect a rich annotated dataset with particular references labeled by the intensity, and propose a novel graph - based semi - supervised model, gralap to label the intensity of references. experiments with aan datasets show a significant improvement compared to the baselines to achieve the true labels of the references ( 46 % better quantitative correlation ). finally, we provide four applications examples to demonstrate beyond how the knowledge of reference intensity leads to design better real - world applications.", "histories": [["v1", "Thu, 1 Sep 2016 01:34:56 GMT  (263kb)", "http://arxiv.org/abs/1609.00081v1", "11 pages, 4 figures, 4 tables, Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)"]], "COMMENTS": "11 pages, 4 figures, 4 tables, Conference on Empirical Methods in Natural Language Processing (EMNLP 2016)", "reviews": [], "SUBJECTS": "cs.CL cs.DL", "authors": ["tanmoy chakraborty 0002", "ramasuri narayanam"], "accepted": true, "id": "1609.00081"}, "pdf": {"name": "1609.00081.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tanchak@umiacs.umd.edu", "ramasurn@in.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n00 08\n1v 1\n[ cs\n.C L\n] 1\nS ep"}, {"heading": "1 Introduction", "text": "With more than one hundred thousand new scholarly articles being published each year, there is a rapid growth in the number of citations for the relevant scientific articles. In this context, we highlight the following interesting facts about the process of citing scientific articles: (i) the most commonly cited paper by Gerard Salton, titled \u201cA Vector Space Model for Information Retrieval\u201d (alleged to have been published in 1975) does not actually exist in reality (Dubin, 2004), (ii) the scientific authors read only 20% of the works they cite (Simkin and Roychowdhury, 2003), (iii) one third of\nthe references in a paper are redundant and 40% are perfunctory (Moravcsik and Murugesan, 1975), (iv) 62.7% of the references could not be attributed a specific function (definition, tool etc.) (Teufel et al., 2006). Despite these facts, the existing bibliographic metrics consider that all citations are equally significant.\nIn this paper, we would emphasize the fact that all the references of a paper are not equally influential. For instance, we believe that for our current paper, (Wan and Liu, 2014) is more influential reference than (Garfield, 2006), although the former has received lower citations (9) than the latter (1650) so far1. Therefore the influence of a cited paper completely depends upon the context of the citing paper, not the overall citation count of the cited paper. We further took the opinion of the original authors of few selective papers and realized that around 16% of the references in a paper are highly influential, and the rest are trivial (Section 4). This motivates us to design a prediction model, GraLap to automatically label the influence of a cited paper with respect to a citing paper. Here, we label paper-reference pairs rather than references alone, because a reference that is influential for one citing paper may not be influential with equal extent for another citing paper.\nWe experiment with ACL Anthology Network (AAN) dataset and show that GraLap along with the novel feature set, quite efficiently, predicts the intensity of references of papers, which achieves (Pearson) correlation of 0.90 with the human anno-\n1The statistics are taken from Google Scholar on June 2, 2016.\ntations. Finally, we present four interesting applications to show the efficacy of considering unequal intensity of references, compared to the uniform intensity.\nThe contributions of the paper are four-fold: (i) we acquire a rich annotated dataset where paperreference pairs are labeled based on the influence scores (Section 4), which is perhaps the first goldstandard for this kind of task; (ii) we propose a graph-based label propagation model GraLap for semi-supervised learning which has tremendous potential for any task where the training set is less in number and labels are non-uniformly distributed (Section 3); (iii) we propose a diverse set of features (Section 3.3); most of them turn out to be quite effective to fit into the prediction model and yield improved results (Section 5); (iv) we present four applications to show how incorporating the reference intensity enhances the performance of several stateof-the-art systems (Section 6)."}, {"heading": "2 Defining Intensity of References", "text": "All the references of a paper usually do not carry equal intensity/strength with respect to the citing paper because some papers have influenced the research more than others. To pin down this intuition, here we discretize the reference intensity by numerical values within the range of 1 to 5, (5: most influential, 1: least influential). The appropriate definitions of different labels of reference intensity are presented in Figure 1, which are also the basis of building the annotated dataset (see Section 4):\nNote that \u201creference intensity\u201d and \u201creference similarity\u201d are two different aspects. It might happen that two similar reference are used with different intensity levels in a citing paper \u2013 while one is just mentioned somewhere in the paper and other is used as a baseline. Here, we address the former problem as a semi-supervised learning problem with clues taken from content of the citing and cited papers."}, {"heading": "3 Reference Intensity Prediction Model", "text": "In this section, we formally define the problem and introduce our prediction model."}, {"heading": "3.1 Problem Definition", "text": "We are given a set of papers P = {P1, P2, ..., PM} and a sets of references R = {R1, R2, ..., RM}, where Ri corresponds to the set of references (or cited papers) of Pi. There is a set of papers PL \u2208 P whose references RL \u2208 R are already labeled by \u2113 \u2208 L = {1, ..., 5} (each reference is labeled with exactly one value). Our objective is to define a predictive function f that labels the references RU \u2208 {R \\ RL} of the papers PU \u2208 {P \\ PL} whose reference intensities are unknown, i.e., f : (P,R, PL, RL, PU , RL) \u2212\u2192 L.\nSince the size of the annotated (labeled) data is much smaller than unlabeled data (|PL| \u226a |PU |), we consider it as a semi-supervised learning problem.\nDefinition 1. (Semi-supervised Learning) Given a set of entries X and a set of possible labels YL, let us assume that (x1, y1), (x2, y2),..., (xl, yl) be the set of labeled data where xi is a data point and yi \u2208 YL is its corresponding label. We assume that at least one instance of each class label\nis present in the labeled dataset. Let (xl+1, yl+1), (xl+2, yl+2),..., (xl+n, yl+u) be the unlabeled data points where YU = {yl+1, yl+2, ...yl+u} are unknown. Each entry x \u2208 X is represented by a set of features {f1, f2, ..., fD}. The problem is to determine the unknown labels using X and YL.\n3.2 GraLap: A Prediction Model\nWe propose GraLap, a variant of label propagation (LP) model proposed by (Zhu et al., 2003) where a node in the graph propagates its associated label to its neighbors based on the proximity. We intend to assign same label to the vertices which are closely connected. However unlike the traditional LP model where the original values of the labels continue to fade as the algorithm progresses, we systematically handle this problem in GraLap. Additionally, we follow a post-processing in order to handle \u201cclassimbalance problem\u201d. Graph Creation. The algorithm starts with the creation of a fully connected weighted graph G = (X,E) where nodes are data points and the weight wij of each edge eij \u2208 E is determined by the radial basis function as follows:\nwij = exp\n(\n\u2212\n\u2211D\nd=1(x d i \u2212 x d j ) 2\n\u03c32\n)\n(1)\nThe weight is controlled by a parameter \u03c3. Later in this section, we shall discuss how \u03c3 is selected. Each node is allowed to propagate its label to its neighbors through edges (the more the edge weight, the easy to propagate). Transition Matrix. We create a probabilistic transition matrix T|X|\u00d7|X|, where each entry Tij indicates the probability of jumping from j to i based on the following: Tij = P (j \u2192 i) =\nwij\u2211|X| k=1 wkj .\nLabel Matrix. Here, we allow a soft label (interpreted as a distribution of labels) to be associated with each node. We then define a label matrix Y|X|\u00d7|L|, where ith row indicates the label distribution for node xi. Initially, Y contains only the values of the labeled data; others are zero. Label Propagation Algorithm. This algorithm works as follows:\nAfter initializing Y and T , the algorithm starts by disseminating the label from one node to its neighbors (including self-loop) in one step (Step 3). Then we normalize each entry of Y by the sum of its cor-\n1: Initialize T and Y 2: while (Y does not converge) do 3: Y \u2190 TY 4: Normalize rows of Y , yij =\nyij\u2211 k yik\n5: Reassign original labels to XL\nresponding row in order to maintain the interpretation of label probability (Step 4). Step 5 is crucial; here we want the labeled sources XL to be persistent. During the iterations, the initial labeled nodes XL may fade away with other labels. Therefore we forcefully restore their actual label by setting yil = 1 (if xi \u2208 XL is originally labeled as l), and other entries (\u2200j 6=lyij) by zero. We keep on \u201cpushing\u201d the labels from the labeled data points which in turn pushes the class boundary through high density data points and settles in low density space. In this way, our approach intelligently uses the unlabeled data in the intermediate steps of the learning. Assigning Final Labels. Once YU is computed, one may take the most likely label from the label distribution for each unlabeled data. However, this approach does not guarantee the label proportion observed in the annotated data (which in this case is not well-separated as shown in Section 4). Therefore, we adopt a label-based normalization technique. Assume that the label proportions in the labeled data are c1, ..., c|L| (s.t. \u2211|L| i=1 ci = 1). In case of YU , we try to balance the label proportion observed in the ground-truth. The label mass is the column sum of YU , denoted by YU.1 , ..., YU.|L| , each of which is scaled in such a way that YU.1 : ... : YU.|L| = c1 : ... : c|L|. The label of an unlabeled data point is finalized as the label with maximum value in the row of Y . Convergence. Here we briefly show that our algorithm is guaranteed to converge. Let us combine Steps 3 and 4 as Y \u2190 T\u0302 Y , where T\u0302 = Tij/ \u2211\nk Tik. Y is composed of YLl\u00d7|L| and YUu\u00d7|L| , where YU never changes because of the reassignment. We can split T\u0302 at the boundary of labeled and unlabeled data as follows:\nF\u0302 =\n[\nT\u0302ll T\u0302lu T\u0302ul T\u0302uu\n]\nTherefore, YU \u2190 T\u0302uuYU+ T\u0302ulYL, which can lead to YU = limn\u2192\u221e T\u0302 nuuY 0 + [ \u2211n i=1 T\u0302 (i\u22121) uu ]T\u0302ulYL, where Y 0 is the shape of Y at iteration 0. We need\nto show T\u0302 nuuijY 0 \u2190 0. By construction, T\u0302ij \u2265 0, and since T\u0302 is row-normalized, and T\u0302uu is a part of T\u0302 , it leads to the following condition: \u2203\u03b3 < 1,\n\u2211u j=1 T\u0302uuij \u2264 \u03b3, \u2200i = 1, ..., u. So,\n\u2211\nj\nT\u0302nuuij = \u2211\nj\n\u2211\nk\nT\u0302 (n\u22121) uuik T\u0302uukj\n= \u2211\nk\nT\u0302 (n\u22121) uuik\n\u2211\nj\nT\u0302uuik\n\u2264 \u2211\nk\nT\u0302 (n\u22121) uuik \u03b3\n\u2264 \u03b3n\nTherefore, the sum of each row in T\u0302 nuuij converges\nto zero, which indicates T\u0302 nuuijY 0 \u2190 0. Selection of \u03c3. Assuming a spatial representation of data points, we construct a minimum spanning tree using Kruskal\u2019s algorithm (Kruskal, 1956) with distance between two nodes measured by Euclidean distance. Initially, no nodes are connected. We keep on adding edges in increasing order of distance. We choose the distance (say, df ) of the first edge which connects two components with different labeled points in them. We consider df as a heuristic to the minimum distance between two classes, and arbitrarily set \u03c3 = d0/3, following 3\u03c3 rule of normal distribution (Pukelsheim, 1994)."}, {"heading": "3.3 Features for Learning Model", "text": "We use a wide range of features that suitably represent a paper-reference pair (Pi, Rij), indicating Pi refers to Pj through reference Rij . These features can be grouped into six general classes. 3.3.1 Context-based Features (CF)\nThe \u201creference context\u201d of Rij in Pi is defined by three-sentence window (sentence where Rij occurs and its immediate previous and next sentences). For multiple occurrences, we calculate its average score. We refer to \u201creference sentence\u201d to indicate the sentence where Rij appears. (i) CF:Alone. It indicates whether Rij is mentioned alone in the reference context or together with other references. (ii) CF:First. When Rij is grouped with others, this feature indicates whether it is mentioned first (e.g., \u201c[2]\u201d is first in \u201c[2,4,6]\u201d).\nNext four features are based on the occurrence of words in the corresponding lists created manually (see Table 1) to understand different aspects.\n(iii) CF:Relevant. It indicates whether Rij is explicitly mentioned as relevant in the reference context (Rel in Table 1). (iv) CF:Recent. It tells whether the reference context indicates that Rij is new (Rec in Table 1). (v) CF:Extreme. It implies that Rij is extreme in some way (Ext in Table 1). (vi) CF:Comp. It indicates whether the reference context makes some kind of comparison with Rij (Comp in Table 1).\nNote we do not consider any sentiment-based features as suggested by (Zhu et al., 2015)."}, {"heading": "3.3.2 Similarity-based Features (SF)", "text": "It is natural that the high degree of semantic similarity between the contents of Pi and Pj indicates the influence of Pj in Pi. We assume that although the full text of Pi is given, we do not have access to the full text of Pj (may be due to the subscription charge or the unavailability of the older papers). Therefore, we consider only the title of Pj as a proxy of its full text. Then we calculate the cosine-similarity2 between the title (T) of Pj and (i) SF:TTitle. the title, (ii) SF:TAbs. the abstract, SF:TIntro. the introduction, (iv) SF:TConcl. the conclusion, and (v) SF:TRest. the rest of the sections (sections other than abstract, introduction and conclusion) of Pi.\nWe further assume that the \u201creference context\u201d (RC) of Pj in Pi might provide an alternate way of summarizing the usage of the reference. Therefore, we take the same similarity based approach mentioned above, but replace the title of Pj with its RC and obtain five more features: (vi) SF:RCTitle, (vii) SF:RCAbs, (viii) SF:RCIntro, (ix) SF:RCConcl and (x) SF:RCRest. If a reference appears multiple times in a citing paper, we consider the aggregation of all RCs together."}, {"heading": "3.3.3 Frequency-based Feature (FF)", "text": "The underlying assumption of these features is that a reference which occurs more frequently in a citing paper is more influential than a single occurrence (Singh et al., 2015). We count the frequency of Rij in (i) FF:Whole. the entire content, (ii) FF:Intro. the introduction, (iii) FF:Rel. the related\n2We use the vector space based model (Turney and Pantel, 2010) after stemming the words using Porter stammer (Porter, 1997).\nwork, (iv) FF:Rest. the rest of the sections (as mentioned in Section 3.3.2) of Pi. We also introduce (v) FF:Sec. to measure the fraction of different sections of Pi where Rij occurs (assuming that appearance of Rij in different sections is more influential). These features are further normalized using the number of sentences in Pi in order to avoid unnecessary bias on the size of the paper."}, {"heading": "3.3.4 Position-based Features (PF)", "text": "Position of a reference in a paper might be a predictive clue to measure the influence (Zhu et al., 2015). Intuitively, the earlier the reference appears in the paper, the more important it seems to us. For the first two features, we divide the entire paper into two parts equally based on the sentence count and then see whether Rij appears (i) PF:Begin. in the beginning or (ii) PF:End. in the end of Pi. Importantly, if Rij appears multiple times in Pi, we consider the fraction of times it occurs in each part.\nFor the other two features, we take the entire paper, consider sentences as atomic units, and measure position of the sentences where Rij appears, including (iii) PF:Mean. mean position of appearance, (iv) PF:Std. standard deviation of different appearances. These features are normalized by the total length (number of sentences) of Pi. , thus ranging from 0 (indicating beginning of Pi) to 1 (indicating the end of Pi)."}, {"heading": "3.3.5 Linguistic Features (LF)", "text": "The linguistic evidences around the context of Rij sometimes provide clues to understand the intrinsic influence of Pj on Pi. Here we consider word level and structural features. (i) LF:NGram. Different levels of n-grams (1- grams, 2-grams and 3-grams) are extracted from the\nreference context to see the effect of different word combination (Athar and Teufel, 2012). (ii) LF:POS. Part-of-speech (POS) tags of the words in the reference sentence are used as features (Jochim and Schu\u0308tze, 2012). (iii) LF:Tense. The main verb of the reference sentence is used as a feature (Teufel et al., 2006). (iv) LF:Modal. The presence of modal verbs (e.g., \u201ccan\u201d, \u201cmay\u201d) often indicates the strength of the claims. Hence, we check the presence of the modal verbs in the reference sentence. (v) LF:MainV. We use the main-verb of the reference sentence as a direct feature in the model. (vi) LF:hasBut. We check the presence of conjunction \u201cbut\u201d, which is another clue to show less confidence on the cited paper. (vii) LF:DepRel. Following (Athar and Teufel, 2012) we use all the dependencies present in the reference context, as given by the dependency parser (Marneffe et al., 2006). (viii) LF:POSP. (Dong and Schfer, 2011) use seven regular expression patterns of POS tags to capture syntactic information; then seven boolean features mark the presence of these patterns. We also utilize the same regular expressions as shown below 3 with the examples (the empty parenthesis in each example indicates the presence of a reference token Rij in the corresponding sentence; while few examples are complete sentences, few are not):\n\u2022 \u201c.*\\\\(\\\\) VV[DPZN].*\u201d: Chen () showed that cohesion is held in the vast majority of cases for English-French.\n\u2022 \u201c.*(VHP|VHZ) VV.*\u201d: while Cherry and Lin () have shown it to be a strong feature for word alignment...\n\u2022 \u201c.*VH(D|G|N|P|Z) (RB )*VBN.*\u201d: Inducing features for taggers by clustering has been tried by several researchers ().\n3The meaning of each POS tag can be found in http://nlp.stanford.edu/software/tagger.shtml(Toutanova and Manning,\n\u2022 \u201c.*MD (RB )*VB(RB )* VVN.*\u201d: For example, the likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair ().\n\u2022 \u201c[ IW.]*VB(D|P|Z) (RB )*VV[ND].*\u201d: Our experimental set-up is modeled after the human evaluation presented in ().\n\u2022 \u201c(RB )*PP (RB )*V.*\u201d: We use CRF () to perform this tagging.\n\u2022 \u201c.*VVG (NP )*(CC )*(NP ).*\u201d: Following (), we provide the annotators with only short sentences: those with source sentences between 10 and 25 tokens long.\nThese are all considered as Boolean features. For each feature, we take all the possible evidences from all paper-reference pairs and prepare a vector. Then for each pair, we check the presence (absence) of tokens for the corresponding feature and mark the vector accordingly (which in turn produces a set of Boolean features)."}, {"heading": "3.3.6 Miscellaneous Features (MS)", "text": "This group provides other factors to explain why is a paper being cited. (i) MS:GCount. To answer whether a highly-cited paper has more academic influence on the citing paper than the one which is less cited, we measure the number of other papers (except Pi) citing Pj . (ii) MS:SelfC. To see the effect of self-citation, we check whether at least one author is common in both Pi and Pj . (iii) MG:Time. The fact that older papers are rarely cited, may not stipulate that these are less influential. Therefore, we measure the difference of the publication years of Pi and Pj . (iv) MG:CoCite. It measures the co-citation counts of Pi and Pj defined by\n|Ri\u2229Rj | |Ri\u222aRj |\n, which in turn answers the significance of reference-based similarity driving the academic influence (Small, 1973).\nFollowing (Witten and Frank, 2005), we further make one step normalization and divide each feature by its maximum value in all the entires."}, {"heading": "4 Dataset and Annotation", "text": "We use the AAN dataset (Radev et al., 2009) which is an assemblage of papers included in ACL related venues. The texts are preprocessed where sentences, paragraphs and sections are properly separated using different markers. The filtered dataset contains 12,843 papers (on average 6.21 references per paper) and 11,092 unique authors.\nNext we use Parscit (Councill et al., 2008) to identify the reference contexts from the dataset and then extract the section headings from all the papers. Then each section heading is mapped into one of the following broad categories using the method proposed by (Liakata et al., 2012): Abstract, Introduction, Related Work, Conclusion and Rest.\nDataset Labeling. The hardest challenge in this task is that there is no publicly available dataset where references are annotated with the intensity value. Therefore, we constructed our own annotated dataset in two different ways. (i) Expert Annotation: we requested members of our research group4 to participate in this survey. To facilitate the labeling process, we designed a portal where all the papers present in our dataset are enlisted in a drop-down menu. Upon selecting a paper, its corresponding references were shown with five possible intensity values. The citing and cited papers are also linked to the original texts so that the annotators can read the original papers. A total of 20 researchers participated and they were asked to label as many paperreference pairs as they could based on the definitions of the intensity provided in Section 2. The annotation process went on for one month. Out of total 1640 pairs annotated, 1270 pairs were taken such that each pair was annotated by at least two annotators, and the final intensity value of the pair was considered to be the average of the scores. The Pearson correlation and Kendell\u2019s \u03c4 among the annotators are 0.787 and 0.712 respectively. (ii) Author Annotation: we believe that the authors of a paper are the best experts to judge the intensity of references present in the paper. With this intension, we launched a survey where we requested the authors whose papers are present in our dataset with significant numbers. We designed a web portal in similar fashion mentioned earlier; but each author was only shown her own papers in the drop-down menu. Out of 35 requests, 22 authors responded and total 196 pairs are annotated. This time we made sure that each paper-reference pair was annotated by only one author. The percentages of labels in the overall annotated dataset are as follows: 1: 9%, 2: 74%, 3: 9%, 4: 3%, 5: 4%.\n4All were researchers with the age between 25-45 working on document summarization, sentiment analysis, and text mining in NLP."}, {"heading": "5 Experimental Results", "text": "In this section, we start with analyzing the importance of the feature sets in predicting the reference intensity, followed by the detailed results. Feature Analysis. In order to determine which features highly determine the gold-standard labeling, we measure the Pearson correlation between various features and the ground-truth labels. Figure 2(a) shows the average correlation for each feature group, and in each group the rank of features based on the correlation is shown in Figure 2(b). Frequencybased features (FF) turn out to be the best, among which FF:Rest is mostly correlated. This set of features is convenient and can be easily computed. Both CF and LF seem to be equally important. However, PF tends to be less important in this task.\nResults of Predictive Models. For the purpose of evaluation, we report the average results after 10-fold cross-validation. Here we consider five baselines to compare with GraLap: (i) Uniform: assign 3 to all the references assuming equal intensity, (ii) SVR+W: recently proposed Support Vector Regression (SVR) with the feature set mentioned in (Wan and Liu, 2014), (iii) SVR+O: SVR model with our feature set, (iv) C4.5SSL: C4.5 semi-supervised algorithm with our feature set (Quinlan, 1993), and (v) GLM: the traditional graph-based LP model with our feature set\n(Zhu et al., 2003). Three metrics are used to compare the results of the competing models with the annotated labels: Root Mean Square Error (RMSE), Pearson\u2019s correlation coefficient (\u03c1), and coefficient of determination (R2)5.\nTable 2 shows the performance of the competing models. We incrementally include each feature set into GraLap greedily on the basis of ranking shown in Figure 2(a). We observe that GraLap with only FF outperforms SVR+O with 41% improvement of \u03c1. As expected, the inclusion of PF into the model improves the model marginally. However, the overall performance of GraLap is significantly higher than any of the baselines (p < 0.01)."}, {"heading": "6 Applications of Reference Intensity", "text": "In this section, we provide four different applications to show the use of measuring the intensity of references. To this end, we consider all the labeled entries for training and run GraLap to predict the intensity of rest of the paper-reference pairs."}, {"heading": "6.1 Discovering Influential Articles", "text": "Influential papers in a particular area are often discovered by considering equal weights to all the citations of a paper. We anticipate that considering the reference intensity would perhaps return more meaningful results. To show this, Here we use the following measures individually to compute the influence of a paper: (i) RawCite: total number of citations per paper, (ii) RawPR: we construct a citation network (nodes: papers, links: citations), and measure PageRank (Page et al., 1998) of each node n: PR(n) = 1\u2212q\nN + q\n\u2211 m\u2208M(n) PR(m) |L(m)| ; where,\nq, the damping factor, is set to 0.85, N is the total number of nodes, M(n) is the set of nodes that have edges to n, and L(m) is the set of nodes that m has an edge to, (iii) InfCite: the weighted version of RawCite, measured by the sum of intensities of all citations of a paper, (iv) InfPR: the weighted version of RawPR: PR(n) = 1\u2212q\nN +\nq \u2211 m\u2208M(n) Inf(m\u2192n)PR(m)\u2211\na\u2208L(m)Inf(m\u2192a) , where Inf indicates\nthe influence of a reference. We rank all the articles based on these four measures separately. Table 3(a) shows the Spearman\u2019s rank correlation be-\n5The less (resp. more) the value of RMSE and R2 (resp. \u03c1), the better the performance of the models.\ntween pair-wise measures. As expected, (i) and (ii) have high correlation (same for (iii) and (iv)), whereas across two types of measures the correlation is less. Further, in order to know which measure is more relevant, we conduct a subjective study where we select top ten papers from each measure and invite the experts (not authors) who annotated the dataset, to make a binary decision whether a recommended paper is relevant. 6. The average pairwise inter-annotator\u2019s agreement (based on Cohen\u2019s kappa (Cohen, 1960)) is 0.71. Table 3(b) presents that out of 10 recommendations of InfPR, 7 (5) papers are marked as influential by majority (all) of the annotators, which is followed by InfCite. These results indeed show the utility of measuring reference intensity for discovering influential papers. Top three papers based on InfPR from the entire dataset are shown in Table 4."}, {"heading": "6.2 Identifying Influential Authors", "text": "H-index, a measure of impact/influence of an author, considers each citation with equal weight (Hirsch, 2005). Here we incorporate the notion of reference intensity into it and define hif-index.\nDefinition 2. An author A with a set of papers P (A) has an hif-index equals to h, if h is the largest value such that |{p \u2208 P (A)|Inf(p) \u2265 h}| \u2265 h; where Inf(p) is the sum of intensities of all citations of p.\nWe consider 37 ACL fellows as the list of goldstandard influential authors. For comparative evaluation, we consider the total number of papers (TotP), total number of citations (TotC) and average citations per paper (AvgC) as three competing measures along with h-index and hif-index. We arrange all the authors in our dataset in decreasing order of each measure. Figure 3(a) shows the Spearman\u2019s rank correlation among the common elements across pair-wise rankings. Figure 3(b)\n6We choose papers from the area of \u201csentiment analysis\u201d on which experts agree on evaluating the papers.\nshows the Precision@k for five competing measures at identifying ACL fellows. We observe that hif-index performs significantly well with an overall precision of 0.54, followed by AvgC (0.37), h-index (0.35), TotC (0.32) and TotP (0.34). This result is an encouraging evidence that the referenceintensity could improve the identification of the influential authors. Top three authors based on hif-index are shown in Table 4."}, {"heading": "6.3 Effect on Recommendation System", "text": "Here we show the effectiveness of referenceintensity by applying it to a real paper recommendation system. To this end, we consider FeRoSA7 (Chakraborty et al., 2016), a new (probably the first) framework of faceted recommendation for scientific articles, where given a query it provides facet-wise recommendations with each facet representing the purpose of recommendation (Chakraborty et al., 2016). The methodology is based on random walk with restarts (RWR) initiated from a query paper. The model is built on AAN dataset and considers both the citation links and the content information to produce the most relevant results. Instead of using the unweighted citation network, here we use the weighted network with each edge labeled by the intensity score. The final recommendation of FeRoSA is obtained by performing RWR with the transition probability proportional to the edge-weight (we call it Inf-FeRoSA). We observe that Inf-FeRoSA achieves an average precision of 0.81 at top 10 recommendations, which is 14% higher then FeRoSA while considering the flat version and 12.34% higher than FeRoSAwhile considering the faceted version."}, {"heading": "6.4 Detecting Citation Stacking", "text": "Recently, Thomson Reuters began screening for journals that exchange large number of anoma-\n7www.ferosa.org\nTable 4: Top three papers and authors based on InfPR and Hif-index respectively.\nNo Paper Author 1. Lexical semantic techniques for corpus analysis (Pustejovsky et al., 1993) Mark Johnson 2. An unsupervised method for detecting grammatical errors (Chodorow and Leacock, 2000) Christopher D. Manning 3. A maximum entropy approach to natural language processing (Berger et al., 1996) Dan Klein\n0 500 1000 1500 2000 0\n500\n1000\n1500\nTotal citations\nT ot\nal c\nita tio\nns\n(e xc\nlu di\nng s\nel f\u2212\nci ta\ntio ns\n)\n0 1 2 3 4 5 0\n1\n2\n3\n4\n5\nIF\nIF if\n(a) (b)\nFigure 4: Correlation between (a) IF and IFif and (b) number of citations before and after removing self-journal citations.\nlous citations with other journals in a cartel-like arrangement, often known as \u201ccitation stacking\u201d (Jump, 2013; Hardcastle, 2015). This sort of citation stacking is much more pernicious and difficult to detect. We anticipate that this behavior can be detected by the reference intensity. Since the AAN dataset does not have journal information, we use DBLP dataset (Singh et al., 2015) where the complete metadata information (along with reference contexts and abstract) is available, except the full content of the paper (559,338 papers and 681 journals; more details in (Chakraborty et al., 2014)). From this dataset, we extract all the features mentioned in Section 3.3 except the ones that require full text, and run our model using the existing annotated dataset as training instances. We measure the traditional impact factor (IF ) of the journals and impact factor after considering the reference intensity (IFif ). Figure 4(a) shows that there are few journals whose IFif significantly deviates (3\u03c3 from the mean) from IF ; out of the suspected journals 70% suffer from the effect of self-journal citations as well (shown in Figure 4(b)), example including Expert Systems with Applications (current IF of 2.53). One of the future work directions would be to predict such journals as early as possible after their first appearance."}, {"heading": "7 Related Work", "text": "Although the citation count based metrics are widely accepted (Garfield, 2006; Hirsch, 2010), the belief that mere counting of citations is dubious has also been a subject of study (Chubin and Moitra, 1975).\n(Garfield, 1964) was the first who explained the reasons of citing a paper. (Pham and Hoffmann, 2003) introduced a method for the rapid development of complex rule bases for classifying text segments. (Dong and Schfer, 2011) focused on a less manual approach by learning domaininsensitive features from textual, physical, and syntactic aspects To address concerns about hindex, different alternative measures are proposed (Waltman and van Eck, 2012). However they too could benefit from filtering or weighting references with a model of influence. Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al., 2011). Recently, (Wan and Liu, 2014) proposed a SVR based approach to measure the intensity of citations. Our methodology differs from this approach in at lease four significant ways: (i) they used six very shallow level features; whereas we consider features from different dimensions, (ii) they labeled the dataset by the help of independent annotators; here we additionally ask the authors of the citing papers to identify the influential references which is very realistic (Gilbert, 1977); (iii) they adopted SVR for labeling, which does not perform well for small training instances; here we propose GraLap , designed specifically for small training instances; (iv) four applications of reference intensity mentioned here are completely new and can trigger further to reassessing the existing bibliometrics."}, {"heading": "8 Conclusion", "text": "We argued that the equal weight of all references might not be a good idea not only to gauge success of a research, but also to track follow-up work or recommending research papers. The annotated dataset would have tremendous potential to be utilized for other research. Moreover, GraLap can be used for any semi-supervised learning problem. Each application mentioned here needs separate attention. In\nfuture, we shall look into more linguistic evidences to improve our model."}], "references": [{"title": "Context-enhanced citation sentiment detection", "author": ["Athar", "Teufel2012] Awais Athar", "Simone Teufel"], "venue": "In NAACL,", "citeRegEx": "Athar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Athar et al\\.", "year": 2012}, {"title": "Positive and negative aspects of citation indices and journal impact factors", "author": ["Alexandru T. Balaban"], "venue": null, "citeRegEx": "Balaban.,? \\Q2012\\E", "shortCiteRegEx": "Balaban.", "year": 2012}, {"title": "A maximum entropy approach to natural language processing", "author": ["Vincent J. Della Pietra", "Stephen A. Della Pietra"], "venue": "Comput. Linguist.,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Towards a stratified learning approach to predict future citation counts", "author": ["Chakraborty", "Suhansanu Kumar", "Pawan Goyal", "Niloy Ganguly", "Animesh Mukherjee."], "venue": "Proceedings of the 14th ACM/IEEE-CS", "citeRegEx": "Chakraborty et al\\.,? 2014", "shortCiteRegEx": "Chakraborty et al\\.", "year": 2014}, {"title": "Advances in Knowledge Discovery and Data Mining: 20th Pacific-Asia Conference, PAKDD 2016", "author": ["Amrith Krishna", "Mayank Singh", "Niloy Ganguly", "Pawan Goyal", "Animesh Mukherjee"], "venue": null, "citeRegEx": "Chakraborty et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chakraborty et al\\.", "year": 2016}, {"title": "An unsupervised method for detecting grammatical errors", "author": ["Chodorow", "Leacock2000] Martin Chodorow", "Claudia Leacock"], "venue": "In NAACL,", "citeRegEx": "Chodorow et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chodorow et al\\.", "year": 2000}, {"title": "Content-Analysis of References Adjunct or Alternative to Citation Counting", "author": ["Chubin", "Moitra1975] D.E. Chubin", "S.D. Moitra"], "venue": "Social studies of science,", "citeRegEx": "Chubin et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Chubin et al\\.", "year": 1975}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["J. Cohen"], "venue": "Educational and Psychological Measurement,", "citeRegEx": "Cohen.,? \\Q1960\\E", "shortCiteRegEx": "Cohen.", "year": 1960}, {"title": "Parscit: an open-source crf reference string parsing package", "author": ["C Lee Giles", "Min-Yen Kan"], "venue": "In LREC,", "citeRegEx": "Councill et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Councill et al\\.", "year": 2008}, {"title": "Applying weighted pagerank to author citation", "author": ["Ying Ding"], "venue": "networks. JASIST,", "citeRegEx": "Ding.,? \\Q2011\\E", "shortCiteRegEx": "Ding.", "year": 2011}, {"title": "The most influential paper gerard salton never wrote", "author": ["David Dubin"], "venue": "Library Trends,", "citeRegEx": "Dubin.,? \\Q2004\\E", "shortCiteRegEx": "Dubin.", "year": 2004}, {"title": "Can citation indexing be automated? Statistical association methods for mechanized documentation", "author": ["Eugene Garfield"], "venue": "Symposium proceedings,", "citeRegEx": "Garfield.,? \\Q1964\\E", "shortCiteRegEx": "Garfield.", "year": 1964}, {"title": "Referencing as persuasion", "author": ["G.N. Gilbert"], "venue": "Social Studies of Science,", "citeRegEx": "Gilbert.,? \\Q1977\\E", "shortCiteRegEx": "Gilbert.", "year": 1977}, {"title": "Citations, self-citations, and citation stacking, http://editorresources.taylorandfrancisgroup.com/ci", "author": ["James Hardcastle"], "venue": null, "citeRegEx": "Hardcastle.,? \\Q2015\\E", "shortCiteRegEx": "Hardcastle.", "year": 2015}, {"title": "An index to quantify an individual\u2019s scientific research", "author": ["J.E. Hirsch"], "venue": null, "citeRegEx": "Hirsch.,? \\Q2005\\E", "shortCiteRegEx": "Hirsch.", "year": 2005}, {"title": "An index to quantify an individual\u2019s scientific research output that takes into account the effect of multiple coauthorship", "author": ["J.E. Hirsch"], "venue": null, "citeRegEx": "Hirsch.,? \\Q2010\\E", "shortCiteRegEx": "Hirsch.", "year": 2010}, {"title": "Counting citations in texts rather than reference lists to improve the accuracy of assessing scientific contribution", "author": ["Hou et al.2011] Wen-Ru Hou", "Ming Li", "Deng-Ke Niu"], "venue": "BioEssays,", "citeRegEx": "Hou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2011}, {"title": "Towards a generic and flexible citation classifier based on a faceted classification scheme", "author": ["Jochim", "Sch\u00fctze2012] Charles Jochim", "Hinrich Sch\u00fctze"], "venue": "In COLING,", "citeRegEx": "Jochim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jochim et al\\.", "year": 2012}, {"title": "On the Shortest Spanning Subtree of a Graph and the Traveling Salesman Problem", "author": ["J.B. Kruskal"], "venue": "In Proceedings of the American Mathematical Society,", "citeRegEx": "Kruskal.,? \\Q1956\\E", "shortCiteRegEx": "Kruskal.", "year": 1956}, {"title": "Automatic recognition of conceptualization zones in scientific articles and two life science applications", "author": ["Shyamasree Saha", "Simon Dobnik", "Colin R. Batchelor", "Dietrich Rebholz-Schuhmann"], "venue": null, "citeRegEx": "Liakata et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liakata et al\\.", "year": 2012}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["Marneffe et al.2006] M. Marneffe", "B. Maccartney", "C. Manning"], "venue": "In LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2006}, {"title": "Some results on the function and quality of citations", "author": ["Moravcsik", "Murugesan1975] M.J. Moravcsik", "P. Murugesan"], "venue": "Social studies of science,", "citeRegEx": "Moravcsik et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Moravcsik et al\\.", "year": 1975}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["Page et al.1998] L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "In WWW,", "citeRegEx": "Page et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Page et al\\.", "year": 1998}, {"title": "A new approach for scientific citation classification using cue phrases", "author": ["Pham", "Hoffmann2003] Son Bao Pham", "Achim Hoffmann"], "venue": null, "citeRegEx": "Pham et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2003}, {"title": "Readings in information retrieval. chapter An Algorithm for Suffix Stripping, pages 313\u2013316", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "Porter.,? \\Q1997\\E", "shortCiteRegEx": "Porter.", "year": 1997}, {"title": "Lexical semantic techniques for corpus analysis", "author": ["Peter Anick", "Sabine Bergler"], "venue": "Comput. Linguist.,", "citeRegEx": "Pustejovsky et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 1993}, {"title": "C4.5: Programs for Machine Learning", "author": ["J. Ross Quinlan"], "venue": null, "citeRegEx": "Quinlan.,? \\Q1993\\E", "shortCiteRegEx": "Quinlan.", "year": 1993}, {"title": "The acl anthology network corpus", "author": ["Pradeep Muthukrishnan", "Vahed Qazvinian"], "venue": "In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,", "citeRegEx": "Radev et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2009}, {"title": "The role of citation context in predicting long-term citation profiles: An experimental study", "author": ["Singh et al.2015] Mayank Singh", "Vikas Patidar", "Suhansanu Kumar", "Tanmoy Chakraborty", "Animesh Mukherjee", "Pawan Goyal"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2015}, {"title": "Co-citation in the scientific literature: A new measure of the relationship between two documents", "author": ["Henry Small"], "venue": null, "citeRegEx": "Small.,? \\Q1973\\E", "shortCiteRegEx": "Small.", "year": 1973}, {"title": "Automatic classification of citation function", "author": ["Teufel et al.2006] Simone Teufel", "Advaith Siddharthan", "Dan Tidhar"], "venue": "In EMNLP,", "citeRegEx": "Teufel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teufel et al\\.", "year": 2006}, {"title": "Enriching the knowledge sources used in a maximum entropy part-ofspeech tagger", "author": ["Toutanova", "Manning2000] Kristina Toutanova", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Toutanova et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2000}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Peter D. Turney", "Patrick Pantel"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "The inconsistency of the h-index", "author": ["Waltman", "van Eck2012] Ludo Waltman", "Nees Jan van Eck"], "venue": "JASIST,", "citeRegEx": "Waltman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Waltman et al\\.", "year": 2012}, {"title": "Are all literature citations equally important? automatic citation strength estimation and its applications", "author": ["Wan", "Liu2014] Xiaojun Wan", "Fang Liu"], "venue": "JASIST,", "citeRegEx": "Wan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2014}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques, Second Edition (Morgan Kaufmann Series in Data Management Systems)", "author": ["Witten", "Frank2005] Ian H. Witten", "Eibe Frank"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2005}, {"title": "Weighted citation: An indicator of an article\u2019s prestige", "author": ["Yan", "Ding2010] Erjia Yan", "Ying Ding"], "venue": null, "citeRegEx": "Yan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2010}, {"title": "Semi-supervised learning using gaussian fields and harmonic functions", "author": ["Zhu et al.2003] Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty"], "venue": "In ICML,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}, {"title": "Measuring academic influence: Not all citations are equal", "author": ["Zhu et al.2015] Xiaodan Zhu", "Peter Turney", "Daniel Lemire", "Andr Vellino"], "venue": null, "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "cess of citing scientific articles: (i) the most commonly cited paper by Gerard Salton, titled \u201cA Vector Space Model for Information Retrieval\u201d (alleged to have been published in 1975) does not actually exist in reality (Dubin, 2004), (ii) the scientific authors read only 20% of the works they cite (Simkin and Roychowdhury, 2003), (iii) one third of the references in a paper are redundant and 40% are perfunctory (Moravcsik and Murugesan, 1975), (iv) 62.", "startOffset": 220, "endOffset": 233}, {"referenceID": 30, "context": ") (Teufel et al., 2006).", "startOffset": 2, "endOffset": 23}, {"referenceID": 11, "context": "For instance, we believe that for our current paper, (Wan and Liu, 2014) is more influential reference than (Garfield, 2006), although the former has received lower citations (9) than the latter (1650) so far1.", "startOffset": 109, "endOffset": 202}, {"referenceID": 38, "context": "(Zhu et al., 2015) for this paper).", "startOffset": 0, "endOffset": 18}, {"referenceID": 24, "context": ", (Porter, 1997) for this paper).", "startOffset": 2, "endOffset": 16}, {"referenceID": 28, "context": ", (Singh et al., 2015) for this paper).", "startOffset": 2, "endOffset": 22}, {"referenceID": 37, "context": "We propose GraLap, a variant of label propagation (LP) model proposed by (Zhu et al., 2003) where a node in the graph propagates its associated label to its neighbors based on the proximity.", "startOffset": 73, "endOffset": 91}, {"referenceID": 18, "context": "Assuming a spatial representation of data points, we construct a minimum spanning tree using Kruskal\u2019s algorithm (Kruskal, 1956) with distance between two nodes measured by Euclidean distance.", "startOffset": 113, "endOffset": 128}, {"referenceID": 38, "context": "Note we do not consider any sentiment-based features as suggested by (Zhu et al., 2015).", "startOffset": 69, "endOffset": 87}, {"referenceID": 28, "context": "The underlying assumption of these features is that a reference which occurs more frequently in a citing paper is more influential than a single occurrence (Singh et al., 2015).", "startOffset": 156, "endOffset": 176}, {"referenceID": 24, "context": "We use the vector space based model (Turney and Pantel, 2010) after stemming the words using Porter stammer (Porter, 1997).", "startOffset": 108, "endOffset": 122}, {"referenceID": 38, "context": "Position of a reference in a paper might be a predictive clue to measure the influence (Zhu et al., 2015).", "startOffset": 87, "endOffset": 105}, {"referenceID": 30, "context": "The main verb of the reference sentence is used as a feature (Teufel et al., 2006).", "startOffset": 61, "endOffset": 82}, {"referenceID": 20, "context": "Following (Athar and Teufel, 2012) we use all the dependencies present in the reference context, as given by the dependency parser (Marneffe et al., 2006).", "startOffset": 131, "endOffset": 154}, {"referenceID": 29, "context": "driving the academic influence (Small, 1973).", "startOffset": 31, "endOffset": 44}, {"referenceID": 27, "context": "We use the AAN dataset (Radev et al., 2009) which is an assemblage of papers included in ACL related venues.", "startOffset": 23, "endOffset": 43}, {"referenceID": 8, "context": "Next we use Parscit (Councill et al., 2008) to identify the reference contexts from the dataset and then extract the section headings from all the papers.", "startOffset": 20, "endOffset": 43}, {"referenceID": 19, "context": "Then each section heading is mapped into one of the following broad categories using the method proposed by (Liakata et al., 2012): Abstract, Introduction, Related Work, Conclusion and Rest.", "startOffset": 108, "endOffset": 130}, {"referenceID": 26, "context": "5 semi-supervised algorithm with our feature set (Quinlan, 1993), and (v) GLM: the traditional graph-based LP model with our feature set (Zhu et al.", "startOffset": 49, "endOffset": 64}, {"referenceID": 37, "context": "5 semi-supervised algorithm with our feature set (Quinlan, 1993), and (v) GLM: the traditional graph-based LP model with our feature set (Zhu et al., 2003).", "startOffset": 137, "endOffset": 155}, {"referenceID": 22, "context": "To show this, Here we use the following measures individually to compute the influence of a paper: (i) RawCite: total number of citations per paper, (ii) RawPR: we construct a citation network (nodes: papers, links: citations), and measure PageRank (Page et al., 1998) of each node n: PR(n) = 1\u2212q N + q \u2211", "startOffset": 249, "endOffset": 268}, {"referenceID": 7, "context": "The average pairwise inter-annotator\u2019s agreement (based on Cohen\u2019s kappa (Cohen, 1960)) is 0.", "startOffset": 73, "endOffset": 86}, {"referenceID": 14, "context": "H-index, a measure of impact/influence of an author, considers each citation with equal weight (Hirsch, 2005).", "startOffset": 95, "endOffset": 109}, {"referenceID": 4, "context": "To this end, we consider FeRoSA7 (Chakraborty et al., 2016), a new (probably the first) framework of faceted recommendation for scientific articles, where given a query it provides facet-wise recommendations with each facet representing the purpose of recommendation (Chakraborty et al.", "startOffset": 33, "endOffset": 59}, {"referenceID": 4, "context": ", 2016), a new (probably the first) framework of faceted recommendation for scientific articles, where given a query it provides facet-wise recommendations with each facet representing the purpose of recommendation (Chakraborty et al., 2016).", "startOffset": 215, "endOffset": 241}, {"referenceID": 25, "context": "Lexical semantic techniques for corpus analysis (Pustejovsky et al., 1993) Mark Johnson 2.", "startOffset": 48, "endOffset": 74}, {"referenceID": 2, "context": "A maximum entropy approach to natural language processing (Berger et al., 1996) Dan Klein", "startOffset": 58, "endOffset": 79}, {"referenceID": 13, "context": "lous citations with other journals in a cartel-like arrangement, often known as \u201ccitation stacking\u201d (Jump, 2013; Hardcastle, 2015).", "startOffset": 100, "endOffset": 130}, {"referenceID": 28, "context": "Since the AAN dataset does not have journal information, we use DBLP dataset (Singh et al., 2015) where the complete metadata information (along with reference contexts and abstract) is available, except the full content of the paper (559,338 papers and 681 journals; more details in (Chakraborty et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 3, "context": ", 2015) where the complete metadata information (along with reference contexts and abstract) is available, except the full content of the paper (559,338 papers and 681 journals; more details in (Chakraborty et al., 2014)).", "startOffset": 194, "endOffset": 220}, {"referenceID": 15, "context": "accepted (Garfield, 2006; Hirsch, 2010), the belief that mere counting of citations is dubious has also been a subject of study (Chubin and Moitra, 1975).", "startOffset": 9, "endOffset": 39}, {"referenceID": 11, "context": "(Garfield, 1964) was the first who explained the reasons of citing a paper.", "startOffset": 0, "endOffset": 16}, {"referenceID": 9, "context": "Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al.", "startOffset": 116, "endOffset": 148}, {"referenceID": 1, "context": "Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al.", "startOffset": 172, "endOffset": 187}, {"referenceID": 16, "context": "Several research have been proposed to weight citations based on factors such as the prestige of the citing journal (Ding, 2011; Yan and Ding, 2010), prestige of an author (Balaban, 2012), frequency of citations in citing papers (Hou et al., 2011).", "startOffset": 229, "endOffset": 247}, {"referenceID": 12, "context": "Our methodology differs from this approach in at lease four significant ways: (i) they used six very shallow level features; whereas we consider features from different dimensions, (ii) they labeled the dataset by the help of independent annotators; here we additionally ask the authors of the citing papers to identify the influential references which is very realistic (Gilbert, 1977); (iii) they adopted SVR for labeling, which does not perform well for small training instances; here we propose GraLap , designed specifically for small training instances; (iv) four applications of reference intensity mentioned here are completely new and can trigger further to reassessing the", "startOffset": 371, "endOffset": 386}], "year": 2016, "abstractText": "Research accomplishment is usually measured by considering all citations with equal importance, thus ignoring the wide variety of purposes an article is being cited for. Here, we posit that measuring the intensity of a reference is crucial not only to perceive better understanding of research endeavor, but also to improve the quality of citation-based applications. To this end, we collect a rich annotated dataset with references labeled by the intensity, and propose a novel graph-based semisupervised model, GraLap to label the intensity of references. Experiments with AAN datasets show a significant improvement compared to the baselines to achieve the true labels of the references (46% better correlation). Finally, we provide four applications to demonstrate how the knowledge of reference intensity leads to design better real-world applications.", "creator": "LaTeX with hyperref package"}}}