{"id": "1609.00559", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors", "abstract": "vector space methods that adequately measure semantic similarity spectra and their relatedness often rely on distributional information such as actual co - - occurrence frequencies or statistical measures of association to weight the importance of particular co - occurrences. in this paper we extend these methods by embedding a measure of semantic similarity based on a human curated taxonomy into a second - - sequence order vector representation. this results in a measure of semantic relatedness that combines both the contextual information technology available in allowing a specialized corpus - - based vector space representation with the semantic knowledge found in a biomedical ontology. our results show that embedding semantic semantic similarity into a second order co - - occurrence matrix systematically improves correlation with fewer human judgments for both similarity and relatedness.", "histories": [["v1", "Fri, 2 Sep 2016 11:44:17 GMT  (25kb)", "http://arxiv.org/abs/1609.00559v1", null], ["v2", "Sat, 27 May 2017 00:23:06 GMT  (29kb)", "http://arxiv.org/abs/1609.00559v2", "10 pages, Appears in the Proceedings of the 16th Workshop on Biomedical Natural Language Processing (BioNLP-2017), August 2017, Vancouver, BC"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bridget t mcinnes", "ted pedersen"], "accepted": false, "id": "1609.00559"}, "pdf": {"name": "1609.00559.pdf", "metadata": {"source": "CRF", "title": "Improving Correlation with Human Judgments by Embedding Second\u2013Order Vectors with Semantic Similarity", "authors": ["Bridget T. McInnes"], "emails": ["btmcinnes@vcu.edu", "tpederse@d.umn.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n00 55\n9v 1\n[ cs\n.C L\n] 2\nS ep\n2 01\n6"}, {"heading": "1 Introduction", "text": "Measures of semantic similarity and relatedness quantify the degree to which two concepts are similar (e.g., lung-heart) or related (e.g., lungbronchitis). Semantic similarity can be viewed as a special case of semantic relatedness \u2013 to be similar is one of many ways that a pair of concepts may be related. The automated discovery of groups of semantically similar or related terms is critical to improving the retrieval (Rada et al., 1989) and clustering (Lin et al., 2007) of biomedical and clinical documents, and the development of biomedical terminologies and ontologies (Bodenreider and Burgun, 2004).\nThere is a long history of success in using distributional methods to discover semantic similarity and relatedness (e.g., (Lin and Pantel, 2002;\nReisinger and Mooney, 2010; Radinsky et al., 2011; Yih and Qazvinian, 2012)). These methods are all based on the distributional hypothesis, which holds that two terms that are distributionally similar (i.e., used in the same context) will also be semantically similar (Harris, 1954; Weeds et al., 2004).\nHowever, despite these successes distributional methods do not perform well when data is very sparse (which is common). One possible solution is to use second\u2013order co\u2013occurrence vectors (Schu\u0308tze, 1992; Schu\u0308tze, 1998). In this approach the similarity between two words is not strictly based on their co\u2013occurrence frequencies, but rather on the frequencies of the other words which occur with both of them (i.e., second order co\u2013occurrences). This approach has been shown to be successful in quantifying semantic relatedness (Islam and Inkpen, 2006; Pedersen et al., 2007). However, while more robust in the face of sparsity, second\u2013order methods can result in significant amounts of noise, where contextual information that is overly general is included and does not contribute to quantifying the semantic relatedness between the two concepts.\nOur goal then is to discover methods that automatically reduce the amount of noise in a second\u2013 order co\u2013occurrence vector. We achieve this by embedding pairwise semantic similarity scores derived from a taxonomy into our second\u2013order vectors, and then using these scores to select only the most semantically similar co\u2013occurrences (thereby reducing noise).\nWe evaluate our method on two datasets that have been annotated in multiple ways. One has been annotated for both similarity and relatedness, and the other has been annotated for relatedness by two different types of experts (medical doctors and medical coders). Our results show that embedding second order co\u2013occurrences with measures\nof semantic similarity increases correlation with our human reference standards. This suggests that these methods have the potential to improve performance over purely distributional methods."}, {"heading": "2 Similarity and Relatedness Measures", "text": "This section describes the similarity and relatedness measures we embed in our second\u2013order co\u2013 occurrence vectors. We use two taxonomies in this study, SNOMED-CT and MeSH. SNOMED-CT (Systematized Nomenclature of Medicine Clinical Terms) is a comprehensive clinical terminology created for the electronic representation of clinical health information. MeSH (Medical Subject Headings) is a taxonomy of biomedical terms developed for indexing biomedical journal articles.\nWe obtain SNOMED-CT and MeSH via the Unified Medical Language System (UMLS) Metathesaurus (version 2014AA). The Metathesaurus contains approximately 2 million biomedical and clinical concepts from over 150 different terminologies that have been semi-automatically integrated into a single source. Concepts in the Metathesaurus are connected largely by two types of hierarchical relations: parent/child (PAR/CHD) and broader/narrower (RB/RN)."}, {"heading": "2.1 Similarity Measures", "text": "Measures of semantic similarity can be classified into three broad categories : path-based, featurebased and information content (IC). Path-based similarity measures use the structure of a taxonomy to measure similarity - concepts positioned close to each other are more similar than those further apart. Feature-based methods rely on set theoretic measures of overlap between features (union and intersection). The information content measures quantify the amount of information that a concept provides \u2013 more specific concepts have a higher amount of information content."}, {"heading": "2.1.1 Path-based", "text": "(Rada et al., 1989) introduce the Conceptual Distance measure. This measure is simply the length of the shortest path between two concepts (c1 and c2) in the MeSH hierarchy. Paths are based on broader than (RB) and narrower than (RN) relations. (Caviedes and Cimino, 2004) extends this measure to use parent (PAR) and child (CHD) relations. Our path measure is simply the reciprocal of this shortest path value (Equation 1), so that\nlarger values (approaching 1) indicate a high degree of similarity.\npath = 1\nspath(c1, c2) (1)\nWhile the simplicity of path is appealing, it can be misleading when concepts are at different levels of specificity. Two very general concepts may have the same path length as two very specific concepts. (Wu and Palmer, 1994) introduce a correction to path that incorporates the depth of the concepts, and the depth of their Least Common Subsumer (LCS). This is the most specific ancestor two concepts share. In this measure, similarity is twice the depth of the two concept\u2019s LCS divided by the product of the depths of the individual concepts (Equation 2). Note that if there are multiple LCSs for a pair of concepts, the deepest of them is used in this measure.\nwup = 2 \u2217 depth(lcs(c1, c2))\ndepth(c1) + depth(c2) (2)\n(Zhong et al., 2002) take a very similar approach and again scale the depth of the LCS by the sum of the depths of the two concepts (Equation 3), where m(c) = k\u2212depth(c). The value of k was set to 2 based on their recommendations.\nzhong = 2 \u2217m(lcs(c1, c2))\nm(c1) +m(c2) (3)\n(Pekar and Staab, 2002) offer another variation on path, where the shortest path of the two concepts to the LCS is used, in addition to the shortest bath between the LCS and the root of the taxonomy (Equation 4).\npks = \u2212 log spath(lcs(c1, c2), root)\u2211\nx=c1,c2,root spath(lcs(c1, c2), x)\n(4)"}, {"heading": "2.1.2 Feature-based", "text": "Feature-based methods represent each concept as a set of features and then measure the overlap or sharing of features to measure similarity. In particular, each concept is represented as the set of their ancestors, and similarity is a ratio of the intersection and union of these features.\n(Maedche and Staab, 2001) quantify the similarity between two concepts as the ratio of the intersection over their union as shown in Equation 5.\ncmatch = |A(c1)\n\u22c2 A(c2)|\n|A(c1) \u22c3 A(c2)| (5)\n(Batet and Valls, 2011) extend this by excluding any shared features (in the numerator) as shown in Equation 6.\nbatet = \u2212log2( |A(c1)\n\u22c3 A(c2)| \u2212 |A(c1) \u22c2 A(c2)|\n|A(c1) \u22c3 A(c2)| )\n(6)"}, {"heading": "2.1.3 Information Content", "text": "Information content is formally defined as the negative log of the probability of a concept. The effect of this is to assign rare (low probability) concepts a high measure of information content, since the underlying assumption is that more specific concepts are less frequently used than more common ones.\n(Resnik, 1995) modified this notion of information content in order to use it as a similarity measure. He defines the similarity of two concepts to be the information content of their LCS (Equation 7).\nres = IC(lcs(c1, c2) = \u2212 log(P (lcs(c1, c2))) (7)\n(Jiang and Conrath, 1997), (Lin, 1998), and (Pirro\u0301 and Euzenat, 2010) extend res by incorporating the information content of the individual concepts in various different ways. Lin defines the similarity between two concepts as the ratio of information content of the LCS with the sum of the individual concept\u2019s information content (Equation 8). Note that lin has the same form as wup and zhong, and is in effect using information content as a measure of specificity (rather than depth). If there is more than one possible LCS, the LCS with the greatest IC is chosen.\nlin = 2 \u2217 IC(lcs(c1, c2))\nIC(c1) + IC(c2) (8)\nJiang and Conrath define the distance between two concepts to be the sum of the information content of the two concepts minus twice the information content of the concepts\u2019 LCS. We modify this from a distance to a similarity measure by taking the reciprocal of the distance (Equation 9). Note that the denominator of jcn is very similar to the numerator of batet.\njcn = 1\nIC(c1) + IC(c2)\u2212 2 \u2217 IC(lcs(c1, c2)) (9)\n(Pirro\u0301 and Euzenat, 2010) define the similarity between two concepts as the information content of the two concept\u2019s LCS divided by the sum of their individual information content values minus the information content of their LCS (Equation 10). Note that batet can be viewed as a settheoretic version of faith.\nfaith = IC(lcs(c1, c2))\nIC(c1) + IC(c2)\u2212 IC(lcs(c1, c2)) (10)"}, {"heading": "2.2 Information Content", "text": "The information content of a concept may be derived from a corpus (corpus-based) or directly from a taxonomy (intrinsic-based). In this work we focus on corpus\u2013based techniques.\nFor corpus\u2013based information content, we estimate the probability of a concept c by taking the sum of the probability of the concept P (c) and the probability its descendants P (d) (Equation 11).\nP (c\u2217) = P (c) + \u2211\nd\u2208descendant(c)\nP (d) (11)\nThe initial probabilities of a concept (P (c)) and its descendants (P (d)) are obtained by dividing the number of times each concept and descendant occurs in the corpus, and dividing that by the total numbers of concepts (N ).\nIdeally the corpus from which we are estimating the probabilities of concepts will be sense tagged. However, sense-tagging is a challenging problem in its own right, and it is not always possible to carry out reliably on larger amounts of text. In fact in this paper we did not use any sense tagging of the corpus we derived information content from.\nInstead, we estimated the probability of a concept by using the UMLSonMedline dataset. This was created by the National Library of Medicine and consists of concepts from the 2009AB UMLS and the counts of the number of times they occurred in a snapshot of Medline taken on 12 January, 2009. These counts were obtained by using the Essie Search Engine (Ide et al., 2007) which queried Medline with normalized strings from the\n2009AB MRCONSO table in the UMLS. The frequency of a CUI was obtained by aggregating the frequency counts of the terms associated with the CUI to provide a rough estimate of its frequency. The information content measures then use this information to calculate the probability of a concept.\nAnother alternative is the use of Intrinsic Information Content. It assess the informativeness of concept based on its placement within a taxonomy by considering the number of incoming (ancestors) relative to outgoing (descendant) links (Sa\u0301nchez et al., 2011) (Equation 12).\nIC(c) = \u2212log(\n|leaves(c)| |subsumers(c)| + 1 max leaves + 1 ) (12)\nwhere leaves are the number of descendants of concept c that are leaf nodes, subsumers are the number of concept c\u2019s ancestors and max leaves are the total number of leaf nodes in the taxonomy."}, {"heading": "2.3 Relatedness Measures", "text": "(Lesk, 1986) observed that concepts that are related should share more words in their respective definitions than concepts that are less connected. He was able to perform word sense disambiguation by identifying the senses of words in a sentence with the largest number of overlaps between their definitions. An overlap is the longest sequence of one or more consecutive words that occur in both definitions. (Banerjee and Pedersen, 2003) extended this idea to WordNet, but observed that WordNet glosses are often very short, and did not contain enough information to distinguish between multiple concepts. Therefore, they created a super gloss for each concept by adding the glosses of related concepts to the gloss of the concept itself (and then finding overlaps).\n(Patwardhan and Pedersen, 2006) adapted this measure to second-order co-occurrence vectors. In this approach, a vector is created for each word in a concept\u2019s definition that shows which words co\u2013occur with it in a corpus. These word vectors are averaged to create a single co-occurrence vector for the concept. The similarity between the concepts is calculated by taking the cosine between the concepts second-order vectors. (Liu et al., 2012) modified and extended this measure to be used to quantify the relatedness between biomedical and clinical terms in the UMLS.\nThe work in this paper can be seen as a further extension of (Patwardhan and Pedersen, 2006; Liu et al., 2012)."}, {"heading": "3 Method", "text": "In this section, we describe our embedded second\u2013 order similarity vector measure. This incorporates both contextual information using the term pair\u2019s definition and their pairwise semantic similarity scores derived from a taxonomy. There are two stages to our approach. First, a co\u2013occurrence matrix must be constructed. Second, this matrix is used to construct a second\u2013order co\u2013occurrence vector for each concept in a pair of concepts to be measured for relatedness."}, {"heading": "3.1 Co-occurrence Matrix Construction", "text": "We build an m x n similarity matrix using an external corpus where the rows and columns represent words within the corpus and the element contains the similarity score between the row word and column word using the similarity measures discussed above. If a word maps to more than one possible sense, we use the sense that returns the highest similarity score.\nFor this paper our external corpus was the NLM Medline Bigram data1. This consists of bigram counts obtained from the 2014 Medline baseline. The bigrams are collected from the title and abstract fields in the Medline citation resulting in 44,450,245 bigrams. We then calculate the similarity for each bigram in this dataset and include those that have a similarity score greater than a specified threshold on these experiments."}, {"heading": "3.2 Measure Term Pairs for Relatedness", "text": "We obtain definitions for each of the two terms we wish to measure. Due to the sparsity and inconsistencies of the definitions in the UMLS, we not only use the definition of the term (CUI) but also include the definition of its related concepts. This follows the method proposed by (Patwardhan and Pedersen, 2006) for general English and WordNet, and which was adapted for the UMLS and the medical domain by (Liu et al., 2012). In particular we add the definitions of any concepts connected via a parent (PAR), child (CHD), RB (broader than), RN (narrower than) or TERM (terms associated with CUI)\n1http://mbr.nlm.nih.gov/Download/index.shtml\nrelation. All of the definitions for a term are combined into a single super\u2013gloss. At the end of this process we should have two super\u2013glosses, one for each term to be measured for relatedness.\nNext, we process each super\u2013gloss as follows.\n1. We extract a first\u2013order co\u2013occurrence vector for each term in the super\u2013gloss from the co\u2013occurrence matrix created in the previous step.\n2. We take the average of the first order co\u2013 occurrence vectors associated with the terms in a super-gloss and use that to represent the meaning of the term. This is a second\u2013order co\u2013occurrence vector.\n3. After a second\u2013order co\u2013occurrence vector has been constructed for each term, then we calculate the cosine between these two vectors to measure the relatedness of the terms."}, {"heading": "3.3 Reference Standards", "text": "We use two reference standards to evaluate the semantic similarity and relatedness measures 2. UMNSRS was annotated for both similarity and relatedness by medical residents. MiniMayoSRS was annotated for relatedness by medical doctors (MD) and medical coders (coder). In this section, we describe these data sets and describe a few of their differences."}, {"heading": "3.3.1 MiniMayoSRS", "text": "MiniMayoSRS is a subset of the MayoSRS data set (Pakhomov et al., 2011), and consists of 30 term pairs for which a high level of inter-annotator agreement was attained. The average correlation between physicians is 0.68 and between medical coders is 0.78. We evaluate our method on the mean of the physician scores and the mean of the coders scores in this subset in the same manner as reported by (Pedersen et al., 2007)."}, {"heading": "3.3.2 UMNSRS", "text": "UMNSRS (University of Minnesota Semantic Relatedness Set) was developed by (Pakhomov et al., 2010), and consists of 725 clinical term pairs whose semantic similarity and relatedness was determined independently by four medical residents from the University of Minnesota Medical School. The similarity and relatedness of each term pair was annotated based\n2http://www.people.vcu.edu/ btmcinnes/downloads.html\non a continuous scale by having the resident touch a bar on a touch sensitive computer screen to indicate the degree of similarity or relatedness. The Intraclass Correlation Coefficient (ICC) for the reference standard tagged for similarity was 0.47, and 0.50 for relatedness. Therefore, as suggested by Pakhomov and colleagues,we use a subset of the ratings consisting of 401 pairs for the similarity set and 430 pairs for the relatedness set which each have an ICC of 0.73."}, {"heading": "4 Experimental Framework", "text": "We conducted our experiments using the freely available open source software package UMLS::Similarity (McInnes et al., 2009) version 1.413. This package takes as input two terms (or UMLS concepts) and returns their similarity or relatedness using the measures discussed in Section 2.\nCorrelation between the similarity measures and human judgments were estimated using Spearman\u2019s Rank Correlation (\u03c1). Spearman\u2019s measures the statistical dependence between two variables to assess how well the relationship between the rankings of the variables can be described using a monotonic function. We used Fisher\u2019s r-to-z transformation (Fisher, 1915) to calculate the significance between the correlation results."}, {"heading": "5 Results and Discussion", "text": "Table 1 shows the Spearman\u2019s Rank Correlation between the human scores from the four reference standards and the scores from the various measures of similarity introduced in Section 2. Each class of measure is followed by the scores obtained when embedding our second order vector approach with these measures of semantic similarity."}, {"heading": "5.1 Results Comparison", "text": "For the UMNSRS tagged for similarity (sim) and the MiniMayoSRS tagged by coders, the results show that all of the second-order similarity vector measures (Embedded) except for vector-jcn obtain a higher correlation than the original measures with vector-res and vector-faith obtaining the highest correlation with human judgments. For the UMNSRS dataset tagged for relatedness and the MiniMayoSRS tagged by physicians (MD),\n3http://search.cpan.org/edist/UMLS-Similarity/\nthe original vector measure obtains a higher correlation than our measure (Embedded) although he differences between the above results is not statistically significant (p \u2264 0.2).\nAnalyzing the above results, we filtered the bigram pairs used to create the initial similarity matrix based on the strength of their similarity using the faith and the res measures. Note that the faith measure holds to a 0 to 1 scale, while res ranges from 0 to an unspecified upper bound that is dependent on the size of the corpus from which information content is estimated."}, {"heading": "5.2 Thresholding Experiments", "text": "Table 2 shows the results of applying the threshold parameter on each of the reference standards using the res measure. For example, a threshold of zero indicates that all of the bigrams were included in the similarity matrix; and a threshold of one indicates that only the bigram pairs with a similarity score greater than one were included.\nThese results show that using a threshold cutoff of two obtains the highest correlation for the UMNSRS dataset; and lastly a threshold cutoff of four obtains the highest correlation for the MiniMayoSRS dataset. All of the results show an increase in correlation with human judgments when incorporating a threshold cutoff over all of the original measures. The increase in the results for the UMNSRS tagged for similarity is statistically significant (p \u2264 0.05). This is not the case for the UMNSRS tagged for relatedness nor the MiniMayoSRS.\nSimilarly, Table 3 shows the results of applying the threshold parameter (T) on each of the reference standards using the faith measure. Although, unlike res whose scores are greater than or equal to zero, the faith measure returns scores between zero and one. Therefore, here a threshold of zero indicates that all of the bigrams were\nincluded in the similarity matrix; and a threshold of 0.1 indicates that only the bigram pairs with a similarity score greater than 0.1 were included. The results show an increase in accuracy for all of the datasets except for the MiniMayoSRS tagged for physicians. The increase in the results for the UMNSRS tagged for similarity and the MayoSRS is statistically significant (p \u2264 0.05). This is not the case for the UMNSRS tagged for relatedness nor the MiniMayoSRS.\nOverall, these results indicate that including only those bigrams that have a sufficiently high similarity score increases the correlation results with human judgments, but what quantifies as sufficiently high varies depending on the dataset and measure."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we present a method for quantifying the similarity and relatedness between two terms that incorporates pair-wise similarity scores into 2nd order vectors in order to restrict the context used by the vector measure to words that exist in the biomedical domain and weight those word pairs that are more similar higher. Our hypothesis was that this would reduce the amount of noise in the vectors and increase the correlation results. We evaluated our method on datasets that have been annotated for relatedness and similarity. The results show that using a combination of contextual information using the term pair\u2019s definition and their pairwise semantic similarity scores derived from a taxonomy obtain a high correlation with human judgments for both similarity and relatedness. In the future, we plan to further explore combining this type of information to quantify the\nrelatedness and similarity between term pairs. In this work, we also explored using a threshold cutoff to include only those term pairs that obtained pre-specified similarity score. In the future, we plan to explore metrics to automatically determine the threshold cutoff appropriate given a dataset. We also plan to look at additional features that can be used with in the second-order vector measure that will reduce the noise but still provide adequate information to quantify relatedness."}], "references": [{"title": "The design, implementation, and use of the Ngram Statistics Package", "author": ["Banerjee", "Pedersen2003] Satanjeev Banerjee", "Ted Pedersen"], "venue": "In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Lin-", "citeRegEx": "Banerjee et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2003}, {"title": "An ontology-based measure to compute semantic similarity in biomedicine", "author": ["Batet", "Valls2011] D. Batet", "M. S\u00e1nchez", "A. Valls"], "venue": "J. of Biomedical Informatics,", "citeRegEx": "Batet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Batet et al\\.", "year": 2011}, {"title": "Aligning knowledge sources in the UMLS: methods, quantitative results, and applications", "author": ["Bodenreider", "Burgun2004] O. Bodenreider", "A. Burgun"], "venue": "In Proceedings of the 11th World Congress on Medical Informatics (MEDINFO),", "citeRegEx": "Bodenreider et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bodenreider et al\\.", "year": 2004}, {"title": "Towards the development of a conceptual distance metric for the UMLS", "author": ["Caviedes", "Cimino2004] J. Caviedes", "J. Cimino"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Caviedes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Caviedes et al\\.", "year": 2004}, {"title": "Frequency distribution of the values of the correlation coefficient in samples from an indefinitely large population", "author": ["Ronald A Fisher"], "venue": null, "citeRegEx": "Fisher.,? \\Q1915\\E", "shortCiteRegEx": "Fisher.", "year": 1915}, {"title": "Essie: a concept-based search engine for structured biomedical text", "author": ["Ide et al.2007] N.C. Ide", "R.F. Loane", "D. DemnerFushman"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Ide et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ide et al\\.", "year": 2007}, {"title": "Second order co-occurrence pmi for determining the semantic similarity of words", "author": ["Islam", "Inkpen2006] Aminul Islam", "Diana Inkpen"], "venue": "In Proceedings of the International Conference on Language Resources and Evaluation,", "citeRegEx": "Islam et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Islam et al\\.", "year": 2006}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["Jiang", "Conrath1997] J. Jiang", "D. Conrath"], "venue": "In Proceedings on International Conference on Research in Computational Linguistics,", "citeRegEx": "Jiang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 1997}, {"title": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone", "author": ["M.E. Lesk"], "venue": "In Proceedings of the 5th annual international conference on Systems documentation,", "citeRegEx": "Lesk.,? \\Q1986\\E", "shortCiteRegEx": "Lesk.", "year": 1986}, {"title": "Concept discovery from text", "author": ["Lin", "Pantel2002] D. Lin", "P. Pantel"], "venue": "In Proceedings of the 19th International Conference on Computational Linguistics", "citeRegEx": "Lin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2002}, {"title": "A document clustering and ranking system for exploring medline citations", "author": ["Y. Lin", "W. Li", "K. Chen", "Y. Liu"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Lin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2007}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Lin.,? \\Q1998\\E", "shortCiteRegEx": "Lin.", "year": 1998}, {"title": "Semantic relatedness study using second\u2013order co\u2013 occurrene vectors computed from biomedical corpora, umls, and wordnet", "author": ["Y. Liu", "B. McInnes", "T. Pedersen", "G. Melton-Meaux", "S. Pakhomov"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Comparing ontologiessimilarity measures and a comparison study", "author": ["Maedche", "Staab2001] Alexander Maedche", "Steffen Staab"], "venue": null, "citeRegEx": "Maedche et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Maedche et al\\.", "year": 2001}, {"title": "UMLS-Interface and UMLSSimilarity : Open source software for measuring paths and semantic similarity", "author": ["McInnes et al.2009] B. McInnes", "T. Pedersen", "S. Pakhomov"], "venue": "In Proceedings of the Annual Symposium of the American Medical Infor-", "citeRegEx": "McInnes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "McInnes et al\\.", "year": 2009}, {"title": "Semantic similarity and relatedness between clinical terms : An experimental study", "author": ["Pakhomov et al.2010] S. Pakhomov", "B. McInnes", "T. Adam", "Y. Liu", "T. Pedersen", "G. Melton"], "venue": "In Proceedings of the Annual Symposium of the American Medical", "citeRegEx": "Pakhomov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pakhomov et al\\.", "year": 2010}, {"title": "Towards a framework for developing semantic relatedness reference standards", "author": ["Pakhomov et al.2011] S. Pakhomov", "T. Pedersen", "B. McInnes", "G. Melton", "A. Ruggieri", "C. Chute"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Pakhomov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pakhomov et al\\.", "year": 2011}, {"title": "Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts", "author": ["Patwardhan", "Pedersen2006] S. Patwardhan", "T. Pedersen"], "venue": "In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing", "citeRegEx": "Patwardhan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2006}, {"title": "Measures of semantic similarity and relatedness in the biomedical domain", "author": ["Pedersen et al.2007] T. Pedersen", "S. Pakhomov", "S. Patwardhan", "C. Chute"], "venue": "Journal of Biomedical Informatics,", "citeRegEx": "Pedersen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pedersen et al\\.", "year": 2007}, {"title": "Taxonomy learning: Factoring the structure of a taxonomy into a semantic classification decision", "author": ["Pekar", "Staab2002] Viktor Pekar", "Steffen Staab"], "venue": "In Proceedings of the 19th International Conference on Computational Linguistics - Volume", "citeRegEx": "Pekar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pekar et al\\.", "year": 2002}, {"title": "A feature and information theoretic framework for semantic similarity and relatedness", "author": ["Pirr\u00f3", "Euzenat2010] Giuseppe Pirr\u00f3", "J\u00e9r\u00f4me Euzenat"], "venue": "In The Semantic Web\u2013ISWC", "citeRegEx": "Pirr\u00f3 et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pirr\u00f3 et al\\.", "year": 2010}, {"title": "Development and application of a metric on semantic nets", "author": ["Rada et al.1989] R. Rada", "H. Mili", "E. Bicknell", "M. Blettner"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Rada et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Rada et al\\.", "year": 1989}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "In Proceedings of the 20th international conference", "citeRegEx": "Radinsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Using information content to evaluate semantic similarity in a taxonomy", "author": ["P. Resnik"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Resnik.,? \\Q1995\\E", "shortCiteRegEx": "Resnik.", "year": 1995}, {"title": "Ontology-based information content computation", "author": ["Montserrat Batet", "David Isern"], "venue": "Knowledge-Based Systems,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2011}, {"title": "Dimensions of meaning", "author": ["H. Sch\u00fctze"], "venue": "In Proceedings of Supercomputing", "citeRegEx": "Sch\u00fctze.,? \\Q1992\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1992}, {"title": "Automatic word sense discrimination", "author": ["Hinrich Sch\u00fctze"], "venue": "Computational Linguistics,", "citeRegEx": "Sch\u00fctze.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds et al.2004] Julie Weeds", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Verb semantics and lexical selection", "author": ["Wu", "Palmer1994] Z. Wu", "M. Palmer"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wu et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wu et al\\.", "year": 1994}, {"title": "Measuring word relatedness using heterogeneous vector space models", "author": ["Yih", "Qazvinian2012] Wen-tau Yih", "Vahed Qazvinian"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association", "citeRegEx": "Yih et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2012}, {"title": "Conceptual graph matching for semantic search", "author": ["Zhong et al.2002] J. Zhong", "H. Zhu", "J. Li", "Y. Yu"], "venue": "Proceedings of the 10th International Conference on Conceptual Structures,", "citeRegEx": "Zhong et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 21, "context": "The automated discovery of groups of semantically similar or related terms is critical to improving the retrieval (Rada et al., 1989) and clustering (Lin et al.", "startOffset": 114, "endOffset": 133}, {"referenceID": 10, "context": ", 1989) and clustering (Lin et al., 2007) of biomedical and clinical documents, and the development of biomedical terminologies and ontologies (Bodenreider and Burgun, 2004).", "startOffset": 23, "endOffset": 41}, {"referenceID": 22, "context": ", (Lin and Pantel, 2002; Reisinger and Mooney, 2010; Radinsky et al., 2011; Yih and Qazvinian, 2012)).", "startOffset": 2, "endOffset": 100}, {"referenceID": 28, "context": ", used in the same context) will also be semantically similar (Harris, 1954; Weeds et al., 2004).", "startOffset": 62, "endOffset": 96}, {"referenceID": 26, "context": "One possible solution is to use second\u2013order co\u2013occurrence vectors (Sch\u00fctze, 1992; Sch\u00fctze, 1998).", "startOffset": 67, "endOffset": 97}, {"referenceID": 27, "context": "One possible solution is to use second\u2013order co\u2013occurrence vectors (Sch\u00fctze, 1992; Sch\u00fctze, 1998).", "startOffset": 67, "endOffset": 97}, {"referenceID": 18, "context": "This approach has been shown to be successful in quantifying semantic relatedness (Islam and Inkpen, 2006; Pedersen et al., 2007).", "startOffset": 82, "endOffset": 129}, {"referenceID": 21, "context": "(Rada et al., 1989) introduce the Conceptual Distance measure.", "startOffset": 0, "endOffset": 19}, {"referenceID": 31, "context": "(Zhong et al., 2002) take a very similar approach and again scale the depth of the LCS by the sum of the depths of the two concepts (Equation 3), where m(c) = k\u2212depth(c).", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "(Resnik, 1995) modified this notion of information content in order to use it as a similarity measure.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "(Jiang and Conrath, 1997), (Lin, 1998), and (Pirr\u00f3 and Euzenat, 2010) extend res by incorporating the information content of the individual concepts in various different ways.", "startOffset": 27, "endOffset": 38}, {"referenceID": 5, "context": "These counts were obtained by using the Essie Search Engine (Ide et al., 2007) which queried Medline with normalized strings from the", "startOffset": 60, "endOffset": 78}, {"referenceID": 25, "context": "It assess the informativeness of concept based on its placement within a taxonomy by considering the number of incoming (ancestors) relative to outgoing (descendant) links (S\u00e1nchez et al., 2011) (Equation 12).", "startOffset": 172, "endOffset": 194}, {"referenceID": 8, "context": "(Lesk, 1986) observed that concepts that are related should share more words in their respective definitions than concepts that are less connected.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "(Liu et al., 2012) modified and extended this measure to be used to quantify the relatedness between biomedical and clinical terms in the UMLS.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "The work in this paper can be seen as a further extension of (Patwardhan and Pedersen, 2006; Liu et al., 2012).", "startOffset": 61, "endOffset": 110}, {"referenceID": 12, "context": "adapted for the UMLS and the medical domain by (Liu et al., 2012).", "startOffset": 47, "endOffset": 65}, {"referenceID": 16, "context": "MiniMayoSRS is a subset of the MayoSRS data set (Pakhomov et al., 2011), and consists of 30", "startOffset": 48, "endOffset": 71}, {"referenceID": 18, "context": "mean of the physician scores and the mean of the coders scores in this subset in the same manner as reported by (Pedersen et al., 2007).", "startOffset": 112, "endOffset": 135}, {"referenceID": 15, "context": "UMNSRS (University of Minnesota Semantic Relatedness Set) was developed by (Pakhomov et al., 2010), and consists of 725", "startOffset": 75, "endOffset": 98}, {"referenceID": 14, "context": "We conducted our experiments using the freely available open source software package UMLS::Similarity (McInnes et al., 2009) version 1.", "startOffset": 102, "endOffset": 124}, {"referenceID": 4, "context": "We used Fisher\u2019s r-to-z transformation (Fisher, 1915) to calculate the significance between the correlation results.", "startOffset": 39, "endOffset": 53}], "year": 2016, "abstractText": "Vector space methods that measure semantic similarity and relatedness often rely on distributional information such as co\u2013occurrence frequencies or statistical measures of association to weight the importance of particular co-occurrences. In this paper we extend these methods by embedding a measure of semantic similarity based on a human curated taxonomy into a second\u2013order vector representation. This results in a measure of semantic relatedness that combines both the contextual information available in a corpus\u2013based vector space representation with the semantic knowledge found in a biomedical ontology. Our results show that embedding semantic semantic similarity into a second order co\u2013occurrence matrix improves correlation with human judgments for both similarity and relatedness.", "creator": "LaTeX with hyperref package"}}}