{"id": "1609.00978", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2016", "title": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences", "abstract": "we provide two fundamental results on the population ( infinite - sample ) likelihood function composing of gaussian mixture models with $ m \\ geq 3 $ components. our first main result shows that the population likelihood function has bad local maxima even in the special case composed of equally - weighted mixtures of well - separated and spherical gaussians. we prove that the log - likelihood survival value of these bad local maxima can be arbitrarily modeled worse than that of any global optimum, thereby resolving an open question of srebro ( reed 2007 ). our second main result shows that the em algorithm ( or a first - order variant of it ) with random initialization will converge to bad critical points with probability at least $ 1 - e ^ { - \\ omega ( m ) } $. we have further establish that a first - order variant of em will indeed not converge to strict saddle points almost surely, indicating that the poor performance of the first - order method can be attributed to the existence of bad local maxima rather than bad saddle points. overall, our results continually highlight about the necessity of careful numerical initialization when using the em algorithm in practice, even when applied in highly favorable settings.", "histories": [["v1", "Sun, 4 Sep 2016 19:34:56 GMT  (211kb)", "http://arxiv.org/abs/1609.00978v1", "Neural Information Processing Systems (NIPS) 2016"]], "COMMENTS": "Neural Information Processing Systems (NIPS) 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["chi jin", "yuchen zhang", "sivaraman balakrishnan", "martin j wainwright", "michael i jordan"], "accepted": true, "id": "1609.00978"}, "pdf": {"name": "1609.00978.pdf", "metadata": {"source": "CRF", "title": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences", "authors": ["Chi Jin", "Yuchen Zhang", "Sivaraman Balakrishnan", "Martin J. Wainwright", "Michael Jordan"], "emails": ["chijin@cs.berkeley.edu,", "yuczhang@berkeley.edu,", "siva@stat.cmu.edu", "wainwrig@berkeley.edu,", "jordan@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n00 97\n8v 1\n[ st"}, {"heading": "1 Introduction", "text": "Finite mixture models are widely used in variety of statistical settings, as models for heterogeneous populations, as flexible models for multivariate density estimation and as models for clustering. Their ability to model data as arising from underlying subpopulations provides essential flexibility in a wide range of applications Titterington [23]. This combinatorial structure also creates challenges for statistical and computational theory, and there are many problems associated with estimation of finite mixtures that are still open. These problems are often studied in the setting of Gaussian mixture models (GMMs), reflecting the wide use of GMMs in applications, particular in the multivariate setting, and this setting will also be our focus in the current paper.\nEarly work [22] studied the identifiability of finite mixture models, and this problem has continued to attract significant interest (see the recent paper of Allman et al. [1] for a recent\noverview). More recent theoretical work has focused on issues related to the use of GMMs for the density estimation problem [12, 13]. Focusing on rates of convergence for parameter estimation in GMMs, Chen [7] established the surprising result that when the number of mixture components is unknown, then the standard \u221a n-rate for regular parametric models is not achievable. Recent investigations [14] into exact-fitted, under-fitted and over-fitted GMMs have characterized the achievable rates of convergence in these settings.\nFrom an algorithmic perspective, the dominant practical method for estimating GMMs is the Expectation-Maximization (EM) algorithm [9]. The EM algorithm is an ascent method for maximizing the likelihood, but is only guaranteed to converge to a stationary point of the likelihood function. As such, there are no general guarantees for the quality of the estimate produced via the EM algorithm for Gaussian mixture models.1 This has led researchers to explore various alternative algorithms which are computationally efficient, and for which rigorous statistical guarantees can be given. Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].\nAlthough general guarantees have not yet emerged, there has nonetheless been substantial progress on the theoretical analysis of EM and its variations. Dasgupta and Schulman [8] analyzed a two-round variant of EM, which involved over-fitting the mixture and then pruning extra centers. They showed that this algorithm can be used to estimate Gaussian mixture components whose means are separated by at least \u2126(d1/4). Balakrishnan et al. [4] studied the local convergence of the EM algorithm for a mixture of two Gaussians with \u2126(1)-separation. Their results show that global optima have relatively large regions of attraction, but still require that the EM algorithm be provided with a reasonable initialization in order to ensure convergence to a near globally optimal solution.\nTo date, computationally efficient algorithms for estimating a GMM provide guarantees under the strong assumption that the samples come from a mixture of Gaussians\u2014i.e., that the model is well-specified. In practice however, we never expect the data to exactly follow the generative model, and it is important to understand the robustness of our algorithms to this assumption. In fact, maximum likelihood has favorable properties in this regard\u2014maximumlikelihood estimates are well known to be robust to perturbations in the Kullback-Leibler metric of the generative model [10]. This mathematical result motivates further study of EM and other likelihood-based methods from the computational point of view. It would be useful to characterize when efficient algorithms can be used to compute a maximum likelihood estimate, or a solution that is nearly as accurate, and which retains the robustness properties of the maximum likelihood estimate.\nIn this paper, we focus our attention on uniformly weighted mixtures of M isotropic Gaussians. For this favorable setting, Srebro [21] conjectured that any local maximum of the likelihood function is a global maximum in the limit of infinite samples\u2014in other words, that there are no bad local maxima for the population GMM likelihood function. This conjecture, if true, would provide strong theoretical justification for EM, at least for large sample sizes. For suitably small sample sizes, it is known [2] that configurations of the samples can be constructed which lead to the likelihood function having an unbounded number of local maxima. The conjecture of Srebro [21] avoids this by requiring that the samples come from the specified GMM, as well as by considering the (infinite-sample-size) population setting. In the context of high-dimensional regression, it has been observed that in some cases despite\n1In addition to issues of convergence to non-maximal stationary points, solutions of infinite likelihood exist for GMMs where both the location and scale parameters are estimated. In practice, several methods exist to avoid such solutions. In this paper, we avoid this issue by focusing on GMMs in which the scale parameters are fixed.\nhaving a non-convex objective function, every local optimum of the objective is within a small, vanishing distance of a global optimum [see, e.g., 17, 25]. In these settings, it is indeed the case that for sufficiently large sample sizes there are no bad local optima.\nA mixture of two spherical Gaussians: A Gaussian mixture model with a single component is simply a Gaussian, so the conjecture of Srebro [21] holds trivially in this case. The first interesting case is a Gaussian mixture with two components, for which empirical evidence supports the conjecture that there are no bad local optima. It is possible to visualize the setting when there are only two components and to develop a more detailed understanding of the population likelihood surface.\nConsider for instance a one-dimensional equally weighted unit variance GMM with true centers \u00b5\u22171 = \u22124 and \u00b5\u22172 = 4, and consider the log-likelihood as a function of the vector \u00b5 : = (\u00b51, \u00b52). Figure 1 shows both the population log-likelihood, \u00b5 7\u2192 L(\u00b5), and the negative 2-norm of its gradient, \u00b5 7\u2192 \u2212\u2016\u2207L(\u00b5)\u20162. Observe that the only local maxima are the vectors (\u22124, 4) and (4,\u22124), which are both also global maxima. The only remaining critical point is (0, 0), which is a saddle point. Although points of the form (0, R), (R, 0) have small gradient when |R| is large, the gradient is not exactly zero for any finite R. Rigorously resolving the question of existence or non-existence of local maxima for the setting when M = 2 remains an open problem.\nIn the remainder of our paper, we focus our attention on the setting where there are more than two mixture components and attempt to develop a broader understanding of likelihood surfaces for these models, as well as the consequences for algorithms.\nOur first contribution is a negative answer to the open question of Srebro [21]. We construct a GMM which is a uniform mixture of three spherical unit-variance, well-separated, Gaussians whose population log-likelihood function contains local maxima. We further show that the loglikelihood of these local maxima can be arbitrarily worse than that of the global maxima. This result immediately implies that any local search algorithm cannot exhibit global convergence (meaning convergence to a global optimum from all possible starting points), even on wellseparated mixtures of Gaussians.\nThe mere existence of bad local maxima is not a practical concern unless it turns out that natural algorithms are frequently trapped in these bad local maxima. Our second main result shows that the EM algorithm, as well as a variant thereof known as the first-order EM algorithm, with random initialization, converges to a bad critical point with an exponentially high probability. In more detail, we consider the following practical scheme for parameter estimation in an M -component Gaussian mixture:\n(a) Draw M i.i.d. points \u00b51, . . . , \u00b5M uniformly at random from the sample set.\n(b) Run the EM or first-order EM algorithm to estimate the model parameters, using \u00b51, . . . , \u00b5M as the initial centers.\nWe note that in the limit of infinite samples, the initialization scheme we consider is equivalent to selecting M initial centers i.i.d from the underlying mixture distribution. We show that for a universal constant c > 0, with probability at least 1\u2212 e\u2212cM , the EM and first-order EM algorithms converge to a suboptimal critical point, whose log-likelihood could be arbitrarily worse than that of the global maximum. Conversely, in order to find a solution with satisfactory log-likelihood via this initialization scheme, one needs repeat the above scheme exponentially many (in M) times, and then select the solution with highest log-likelihood. This result strongly indicates that repeated random initialization followed by local search (via either EM or its first order variant) can fail to produce useful estimates under reasonable constraints on computational complexity.\nWe further prove that under the same random initialization scheme, the first-order EM algorithm with a suitable stepsize does not converge to a strict saddle point with probability one. This fact strongly suggests that the failure of local search methods for the GMM model is due mainly to the existence of bad local optima, and not due to the presence of (strict) saddle points.\nOur proofs introduce new techniques to reason about the structure of the population loglikelihood, and in particular to show the existence of bad local optima. We expect that these general ideas will aid in developing a better understanding of the behavior of algorithms for non-convex optimization. From a practical standpoint, our results strongly suggest that careful initialization is required for local search methods, even in large-sample settings, and even for extremely well-behaved mixture models.\nThe remainder of this paper is organized as follows. In Section 2, we introduce GMMs, the EM algorithm, its first-order variant and we formally set up the problem we consider. In Section 3, we state our main theoretical results and develop some of their implications. Section 4 is devoted to the proofs of our results, with some of the more technical aspects deferred to the appendices."}, {"heading": "2 Background and Preliminaries", "text": "In this section, we formally define the Gaussian mixture model that we study in the paper. We then describe the EM algorithm, the first-order EM algorithm, as well as the form of random initialization that we analyze. Throughout the paper, we use [M ] to denote the set {1, 2, \u00b7 \u00b7 \u00b7 ,M}, and N (\u00b5,\u03a3) to denote the d-dimensional Gaussian distribution with mean vector \u00b5 and covariance matrix \u03a3. We use \u03c6(\u00b7 | \u00b5,\u03a3) to denote the probability density function of the Gaussian distribution with mean vector \u00b5 and covariance matrix \u03a3:\n\u03c6(x | \u00b5,\u03a3) := 1\u221a (2\u03c0)ddet(\u03a3) e\u2212 1 2 (x\u2212\u00b5)\u22a4\u03a3\u22121(x\u2212\u00b5). (1)"}, {"heading": "2.1 Gaussian Mixture Models", "text": "A d-dimensional Gaussian mixture model (GMM) with M components can be specified by a collection \u00b5\u2217 = {\u00b5\u2217i , . . . , \u00b5\u2217M} of d-dimensional mean vectors, a vector \u03bb\u2217 = (\u03bb\u22171, . . . , \u03bb\u2217M ) of non-negative mixture weights that sum to one, and a collection \u03a3\u2217 = {\u03a3\u22171, . . . ,\u03a3\u2217M} of covariance matrices. Given these parameters, the density function of a Gaussian mixture model takes the form\np(x | \u03bb\u2217,\u00b5\u2217,\u03a3\u2217) = M\u2211\ni=1\n\u03bb\u2217i\u03c6(x | \u00b5\u2217i ,\u03a3\u2217i ),\nwhere the Gaussian density function \u03c6 was previously defined in equation (1). In this paper, we focus on the idealized situation in which every mixture component is equally weighted, and the covariance of each mixture component is the identity. This leads to a mixture model of the form\np(x | \u00b5\u2217) := 1 M\nM\u2211\ni=1\n\u03c6(x | \u00b5\u2217i , I), (2)\nwhich we denote by GMM(\u00b5\u2217). In this case, the only parameters to be estimated are the mean vectors \u00b5\u2217 = {\u00b5\u2217i }Mi=1 of the M components.\nThe difficulty of estimating a Gaussian mixture distribution depends on the amount of separation between the mean vectors. More precisely, for a given parameter \u03be > 0, we say that the GMM(\u00b5\u2217)-model is \u03be-separated if\n\u2016\u00b5\u2217i \u2212 \u00b5\u2217j\u20162 \u2265 \u03be, for all distinct pairs i, j \u2208 [M ]. (3)\nWe say that the mixture is well-separated if condition (3) holds for some \u03be = \u2126( \u221a d).\nSuppose that we observe an i.i.d. sequence {x\u2113}n\u2113=1 drawn according to the distribution GMM(\u00b5\u2217), and our goal is to estimate the unknown collection of mean vectors \u00b5\u2217. The sample-based log-likelihood function Ln is given by\nLn(\u00b5) := 1\nn\nn\u2211\n\u2113=1\nlog ( 1 M M\u2211\ni=1\n\u03c6(x\u2113 | \u00b5i, I) ) . (4a)\nAs the sample size n tends to infinity, this sample likelihood converges to the population log-likelihood function L given by\nL(\u00b5) = E\u00b5\u2217 log ( 1\nM\nM\u2211\ni=1\n\u03c6(X | \u00b5i, I) ) . (4b)\nHere E\u00b5\u2217 denotes expectation taken over the random vector X drawn according to the model GMM(\u00b5\u2217).\nA straightforward implication of the positivity of the KL divergence is that the population likelihood function is in fact maximized at \u00b5\u2217 (along with permutations thereof, depending on how we index the mixture components). On the basis of empirical evidence, Srebro [21] conjectured that this population log-likelihood is in fact well-behaved, in the sense of having no spurious local optima. In Theorem 1, we show that this intuition is false, and provide a simple example of a mixture of M = 3 well-separated Gaussians in dimension d = 1, whose population log-likelihood function has arbitrarily bad local optima."}, {"heading": "2.2 Expectation-Maximization Algorithm", "text": "A natural way to estimate the mean vectors \u00b5\u2217 is by attempting to maximize the sample loglikelihood defined by the samples {x\u2113}n\u2113=1. For a non-degenerate Gaussian mixture model, the log-likelihood is non-concave. Rather than attempting to maximize the log-likelihood directly, the EM algorithm proceeds by iteratively maximizing a lower bound on the log-likelihood. It does so by alternating between two steps:\n1. E-step: For each i \u2208 [M ] and \u2113 \u2208 [n], compute the membership weight\nwi(x\u2113) = \u03c6(x\u2113 | \u00b5i, I)\u2211M j=1 \u03c6(x\u2113 | \u00b5j, I) .\n2. M-step: For each i \u2208 [M ], update the mean \u00b5i vector via\n\u00b5newi = \u2211n i=1 wi(x\u2113)x\u2113\u2211n \u2113=1wi(x\u2113) .\nIn the population setting, the M-step becomes:\n\u00b5newi = E\u00b5\u2217 [wi(X)X]\nE\u00b5\u2217 [wi(X)] . (5)\nIntuitively, the M-step updates the mean vector of each Gaussian component to be a weighted centroid of the samples for appropriately chosen weights.\nFirst-order EM updates: For a general latent variable model with observed variables X = x, latent variables Z and model parameters \u03b8, by Jensen\u2019s inequality, the log-likelihood function can be lower bounded as\nlogP(x | \u03b8\u2032) \u2265 EZ\u223cP(\u00b7|x;\u03b8) log P(x,Z | \u03b8\u2032)\ufe38 \ufe37\ufe37 \ufe38 :=Q(\u03b8\u2032|\u03b8) \u2212EZ\u223cP(\u00b7|x;\u03b8) log P(Z | x; \u03b8\u2032).\nEach step of the EM algorithm can also be viewed as optimizing over this lower bound, which gives:\n\u03b8new := argmax \u03b8\u2032 Q(\u03b8\u2032 | \u03b8)\nThere are many variants of the EM algorithm which rely instead on partial updates at each iteration instead of finding the exact optimum of Q(\u03b8\u2032 | \u03b8). One important example, analyzed in the work of Balakrishnan et al. [4], is the first-order EM algorithm. The first-order EM algorithm takes a step along the gradient of the function Q(\u03b8\u2032 | \u03b8) (with respect to its first argument) in each iteration. Concretely, given a step size s > 0, the first-order EM updates can be written as:\n\u03b8new = \u03b8 + s\u2207\u03b8\u2032Q(\u03b8\u2032 | \u03b8) |\u03b8\u2032=\u03b8 .\nIn the case of the model GMM(\u00b5\u2217), the gradient EM updates on the population objective take the form\n\u00b5newi = \u00b5i + s E\u00b5\u2217 [ wi(X)(X \u2212 \u00b5i) ] . (6)\nThis update turns out to be equivalent to gradient ascent on the population likelihood L with step size s > 0 (see the paper [4] for details)."}, {"heading": "2.3 Random Initialization", "text": "Since the log-likelihood function is non-concave, the point to which the EM algorithm converges depends on the initial value of \u00b5. In practice, it is standard to choose these values by some form of random initialization. For instance, one method is to to initialize the mean vectors by sampling uniformly at random from the data set {x\u2113}n\u2113=1. This scheme is intuitively reasonable, because it automatically adapts to the locations of the true centers. If the true centers have large mutual distances, then the initialized centers will also be scattered. Conversely, if the true centers concentrate in a small region of the space, then the initialized centers will also be close to each other. In practice, initializing \u00b5 by uniformly drawing from the data is often more reasonable than drawing \u00b5 from a fixed distribution.\nIn this paper, we analyze the EM algorithm and its variants at the population level. We focus on the above practical initialization scheme of selecting \u00b5 uniformly at random from the sample set. In the idealized population setting, this is equivalent to sampling the initial values of \u00b5 i.i.d from the distribution GMM(\u00b5\u2217). Throughout this paper, we refer to this particular initialization strategy as random initialization."}, {"heading": "3 Main results", "text": "We now turn to the statements of our main results, along with a discussion of some of their consequences."}, {"heading": "3.1 Structural properties", "text": "In our first main result (Theorem 1), for any M \u2265 3, we exhibit an M -component mixture of Gaussians in dimension d = 1 for which the population log-likelihood has a bad local maximum whose log-likelihood is arbitrarily worse than that attained by the true parameters \u00b5\u2217. This result provides a negative answer to the conjecture of Srebro [21].\nTheorem 1. For any M \u2265 3 and any constant Cgap > 0, there is a well-separated uniform mixture of M unit-variance spherical Gaussians GMM(\u00b5\u2217) and a local maximum \u00b5\u2032 such that\nL(\u00b5\u2032) \u2264 L(\u00b5\u2217)\u2212 Cgap.\nIn order to illustrate the intuition underlying Theorem 1, we give a geometrical description of our construction for M = 3. Suppose that the true centers \u00b5\u22171, \u00b5 \u2217 2 and \u00b5 \u2217 3, are such that the distance between \u00b5\u22171 and \u00b5 \u2217 2 is much smaller than the respective distances from \u00b5 \u2217 1 to \u00b5 \u2217 3, and from \u00b5\u22172 to \u00b5 \u2217 3. Now, consider the point \u00b5 := (\u00b51, \u00b52, \u00b53) where \u00b51 = (\u00b5 \u2217 1 + \u00b5 \u2217 2)/2; the points \u00b52 and \u00b53 are both placed at the true center \u00b5 \u2217 3. This assignment does not maximize the population log-likelihood, because only one center is assigned to the two Gaussian components centered at \u00b5\u22171 and \u00b5 \u2217 2, while two centers are assigned to the Gaussian component centered at \u00b5\u22173. However, when the components are well-separated we are able to show that there is a local maximum in the neighborhood of this configuration. In order to establish the existence of a local maximum, we first define a neighborhood of this configuration ensuring that it does not contain any global maximum, and then prove that the log-likelihood on the boundary of this neighborhood is strictly smaller than that of the sub-optimal configuration \u00b5. Since the log-likelihood is bounded from above, this neighborhood must contain at least one maximum of the log-likelihood. Since the global maxima are not in this neighborhood by construction, any maximum in this neighborhood must be a local maximum. See Section 4 for a detailed proof."}, {"heading": "3.2 Algorithmic consequences", "text": "An important implication of Theorem 1 is that any iterative algorithm, such as EM or gradient ascent, that attempts to maximize the likelihood based on local updates cannot be globally convergent\u2014that is, cannot converge to (near) globally optimal solutions from an arbitrary initialization. Indeed, if any such algorithm is initialized at the local maximum, then they will remain trapped. However, one might argue that this conclusion is overly pessimistic, in that we have only shown that these algorithms fail when initialized at a certain (adversarially chosen) point. Indeed, the mere existence of bad local minima need not be a practical concern unless it can be shown that a typical optimization algorithm will frequently converge to one of them. The following result shows that the EM algorithm, when applied to the population likelihood and initialized according to the random scheme described in Section 2.2, converges to a bad critical point with high probability.\nTheorem 2. Let \u00b5t be the tth iterate of the EM algorithm initialized by the random initialization scheme described previously. There exists a universal constant c, for any M \u2265 3 and any constant Cgap > 0, such that there is a well-separated uniform mixture of M unit-variance spherical Gaussians GMM(\u00b5\u2217) with\nP [ \u2200t \u2265 0, L(\u00b5t) \u2264 L(\u00b5\u2217)\u2212 Cgap ] \u2265 1\u2212 e\u2212cM .\nTheorem 2 shows that, for the specified configuration \u00b5\u2217, the probability of success for the EM algorithm is exponentially small as a function of M . As a consequence, in order to guarantee recovering a global maximum with at least constant probability, the EM algorithm with random initialization must be executed at least e\u2126(M) times. This result strongly suggests that that effective initialization schemes, such as those based on pilot estimators utilizing the method of moments [15, 18], are critical to finding good maxima in general GMMs.\nThe key idea in the proof of Theorem 2 is the following: suppose that all the true centers are grouped into two clusters that are extremely far apart, and suppose further that we initialize all the centers in the neighborhood of these two clusters, while ensuring that at least one center lies within each cluster. In this situation, all centers will remain trapped within the cluster in which they were first initialized, irrespective of how many steps we take in the EM algorithm. Intuitively, this suggests that the only favorable initialization schemes (from which convergence to a global maximum is possible) are those in which (1) all initialized centers fall in the neighborhood of exactly one cluster of true centers, (2) the number of centers initialized within each cluster of true centers exactly matches the number of true centers in that cluster. However, this observation alone only suffices to guarantee that the success probability is polynomially small in M .\nIn order to demonstrate that the success probability is exponentially small inM , we need to further refine this construction. In more detail, we construct a Gaussian mixture distribution with a recursive structure: on top level, its true centers can be grouped into two clusters far apart, and then inside each cluster, the true centers can be further grouped into two miniclusters which are well-separated, and so on. We can repeat this structure for \u2126(logM) levels. For this GMM instance, even in the case where the number of true centers exactly matches the number of initialized centers in each cluster at the top level, we still need to consider the configuration of the initial centers within the mini-clusters, which further reduces the probability of success for a random initialization. A straightforward calculation then shows that the probability of a favorable random initialization is on the order of e\u2212\u2126(M). The full proof is given in Section 4.2.\nWe devote the remainder of this section to a treatment of the first-order EM algorithm. Our first result in this direction shows that the problem of convergence to sub-optimal fixed points remains a problem for the first-order EM algorithm, provided the step-size is not chosen too aggressively.\nTheorem 3. Let \u00b5t be the tth iterate of the first-order EM algorithm with stepsize s \u2208 (0, 1), initialized by the random initialization scheme described previously. There exists a universal constant c, for any M \u2265 3 and any constant Cgap > 0, such that there is a well-separated uniform mixture of M unit-variance spherical Gaussians GMM(\u00b5\u2217) with\nP ( \u2200t \u2265 0, L(\u00b5t) \u2264 L(\u00b5\u2217)\u2212 Cgap ) \u2265 1\u2212 e\u2212cM . (7)\nWe note that the restriction on the step-size is weak, and is satisfied by the theoretically optimal choice for a mixture of two Gaussians in the setting studied by Balakrishnan et al. [4]. Recall that the first-order EM updates are identical to gradient ascent updates on the log-likelihood function. As a consequence, we can conclude that the most natural local search heuristics for maximizing the log-likelihood (EM and gradient ascent), fail to provide statistically meaningful estimates when initialized randomly, unless we repeat this procedure exponentially many (in M) times.\nOur final result concerns the type of fixed points reached by the first-order EM algorithm in our setting. Pascanu et al. [20] argue that for high-dimensional optimization problems, the principal difficulty is the proliferation of saddle points, not the existence of poor local maxima. In our setting, however, we can leverage recent results on gradient methods [16, 19] to show that the first-order EM algorithm cannot converge to strict saddle points. More precisely:\nDefinition 1 (Strict saddle point [11]). For a maximization problem, we say that a critical point xss of function f is a strict saddle point if the Hessian \u22072f(xss) has at least one strictly positive eigenvalue.\nWith this definition, we have the following:\nTheorem 4. Let \u00b5t be the tth iterate of the first-order EM algorithm with constant stepsize s \u2208 (0, 1), and initialized by the random initialization scheme described previously. Then for any M -component mixture of spherical Gaussians:\n(a) The iterates \u00b5t converge to a critical point of the log-likelihood. (b) For any strict saddle point \u00b5ss, we have P ( limt\u2192\u221e \u00b5 t = \u00b5ss ) = 0.\nTheorems 3 and 4 provide strong support for the claim that the sub-optimal points to which the first-order EM algorithm frequently converges are bad local maxima. The algorithmic failure of the first-order EM algorithm is most likely due to the presence of bad local maxima, as opposed to (strict) saddle-points.\nThe proof of Theorem 4 is based on recent work [16, 19] on the asymptotic performance of gradient methods. That work reposes on the stable manifold theorem from dynamical systems theory, and, applied directly to our setting, would require establishing that the population likelihood L is smooth. Our proof technique avoids such a smoothness argument; see Section 4.4 for the details. The proof technique makes use of specific properties of the first-order EM algorithm that do not hold for the EM algorithm. We conjecture that a similar result is true for the EM algorithm; however, we suspect that a generalized version of the stable manifold theorem will be needed to establish such a result."}, {"heading": "4 Proofs", "text": "This section is devoted to the proofs of Theorems 1 through 4. Certain technical aspects of the proofs are deferred to the appendix."}, {"heading": "4.1 Proof of Theorem 1", "text": "In this section, we prove Theorem 1. The proof consists of three parts: starting with the case M = 3, the first part shows the existence of a local maximum for certain GMMs, whereas the second part shows that this local maximum has a log-likelihood that is much worse than that of the global maximum. The third part provides the extension to the general case of M > 3 mixture components."}, {"heading": "4.1.1 Existence of a local maximum", "text": "In this section, we prove the existence of a local maximum by first constructing a family of GMMs parametrized by a scalar \u03b3, and then proving the existence of local maxima in the limiting case when \u03b3 \u2192 +\u221e. By continuity of the log-likelihood function, we can then conclude that there exists some finite \u03b3 whose corresponding log-likelihood has local maxima.\nWe begin by considering the special case of M = 3 components in dimension d = 1. For parameters R > 0 and \u03b3 \u226b 1, suppose that the true centers \u00b5\u2217 are given by\n\u00b5\u22171 = \u2212R, \u00b5\u22172 = R, \u00b5\u22173 = \u03b3R.\nBy construction, the two centers \u00b5\u22171 and \u00b5 \u2217 2 are relatively close together near the origin, while the third center \u00b5\u22173 is located far away from both of the first two centers. We first claim that when \u03b3 is sufficiently large, there is a local maximum in the closed set:\nD = { (\u00b51, \u00b52, \u00b53) \u2208 R3 | \u00b51 \u2264 \u03b3R\n3 , \u00b52 \u2265\n2\u03b3R\n3 and \u00b53 \u2265\n2\u03b3R\n3\n} .\nTo establish this claim, we consider the value of population log-likelihood function L(\u00b5\u0303) at an interior point \u00b5\u0303 = (0, \u03b3R, \u03b3R) of D, and compare it to the log-likelihood on the boundary of the set D. We show that for a sufficiently large \u03b3, the log-likelihood at the interior point is strictly larger than the log-likelihood on the boundary, and use this to argue that there must be a local maxima in the set D. Concretely, define v0 : = L(\u00b5\u0303), and the maximum value of L(\u00b5) on the three two-dimensional faces of D, i.e.,\nv1 : = sup \u00b51=\u03b3R/3 \u00b52\u22652\u03b3R/3 \u00b53\u22652\u03b3R/3 L(\u00b5), v2 : = sup \u00b51\u2264\u03b3R/3 \u00b52=2\u03b3R/3 \u00b53\u22652\u03b3R/3 L(\u00b5), and v3 : = sup \u00b51\u2264\u03b3R/3 \u00b52\u22652\u03b3R/3 \u00b53=2\u03b3R/3 L(\u00b5).\nThe population log-likelihood function is given by the expression\nL(\u00b5) = E\u00b5\u2217 log ( 3\u2211\ni=1\ne\u2212 1 2 (X\u2212\u00b5i)2 ) \u2212 log(3 \u221a 2\u03c0).\nAs \u03b3 \u2192 \u221e, it is easy to verify that\nv0 = L(\u00b5\u0303) \u2192 \u2212 2R2 + 3\u2212 2 log(2)\n6 \u2212 log(3\n\u221a 2\u03c0).\nSimilarly, we can calculate the value of v1, v2 and v3 as \u03b3 \u2192 \u221e; i.e., a straightforward calculation shows that\nlim \u03b3\u2192+\u221e\nv1 = \u2212\u221e,\nlim \u03b3\u2192+\u221e\nv2 = \u2212 2R2 + 3\n6 \u2212 log(3\n\u221a 2\u03c0) (the maximum is attained at \u00b51 \u2192 0 and \u00b53 \u2192 \u03b3R),\nlim \u03b3\u2192+\u221e\nv3 = \u2212 2R2 + 3\n6 \u2212 log(3\n\u221a 2\u03c0) (the maximum is attained at \u00b51 \u2192 0 and \u00b52 \u2192 \u03b3R).\nThis gives the relation v0 > max{v1, v2, v3} when \u03b3 \u2192 \u221e. Since L is a continuous function of \u03b3, we know that v0, v1, v2, v3 are also continuous functions of \u03b3. Therefore, there exists a finite A such that, as long as \u03b3 > A, we will still have v0 > max{v1, v2, v3}. This in turn implies that the function value at an interior point is strictly greater than the function value on the boundary of D, which implies the existence of at least one local maximum inside D.\nOn the other hand, the global maxima of the population likelihood function are (\u2212R,R, \u03b3R) and its permutations, which are not in D. This shows the existence of at least one local maximum which is not a global maximum."}, {"heading": "4.1.2 Log-likelihood at a local maximum", "text": "In order to prove that the log-likelihood of a local maximum can be arbitrarily worse than the log-likelihood of the global maximum, we consider the limit when R \u2192 \u221e. In this case, the limiting value of the global maximum will be\nlim R\u2192\u221e L(\u00b5\u2217) = \u22121 2 \u2212 log(3\n\u221a 2\u03c0).\nLet \u00b5\u2032 = (\u00b5\u20321, \u00b5 \u2032 2, \u00b5 \u2032 3) be one of the local maxima in the closed set D. We have previously established the existence of such a local maximum. Since \u00b5\u22171 \u2212 \u00b5\u22172 = 2R, we know that either | \u00b5\u22172 \u2212 \u00b5\u20321 |> R or | \u00b5\u22171 \u2212 \u00b5\u20321 |> R has to be true. Without loss of generality, we may assume that | \u00b5\u22172 \u2212 \u00b5\u20321 |> R. From the definition of the set D, we can also see that | \u00b5\u22172 \u2212 \u00b5\u20322 |> R and | \u00b5\u22172 \u2212 \u00b5\u20323 |> R. Putting together the pieces yields\nlim R\u2192+\u221e L(\u00b5\u2032) \u2264 lim R\u2192+\u221e\n1 3 EX\u223cN (\u00b5\u2217 2 ,1) log\n( 3\u2211\ni=1\ne\u2212 1 2 (X\u2212\u00b5\u2032i) 2 ) \u2212 1\n3 log(3\n\u221a 2\u03c0) = \u2212\u221e.\nAgain, by the continuity of the function L with respect to R, we know for any Cgap > 0, there always exists a large constant A\u2032, so that if R > A\u2032, we will have L(\u00b5\u2217)\u2212L(\u00b5\u2032) > Cgap. This completes the proof for case M = 3."}, {"heading": "4.1.3 Extension to the case M > 3", "text": "We now provide an outline of how this argument can be extended to the general setting of M > 3. Consider a GMM with true centers\n\u00b5\u2217i = (2i\u2212 k)R k \u2212 2 , for i = 1, \u00b7 \u00b7 \u00b7 ,M \u2212 1 and \u00b5 \u2217 M = \u03b3R,\nfor some parameter \u03b3 > 0 to be chosen. We claim that when \u03b3 is sufficiently large, there is at least one local maximum in the closed set\nDM = { (\u00b51, \u00b7 \u00b7 \u00b7 , \u00b5M ) | \u00b51 \u2264 \u03b3R\n3 , \u00b52 \u2265\n2\u03b3R\n3 , \u00b7 \u00b7 \u00b7 , \u00b5M \u2265\n2\u03b3R\n3\n} .\nThe proof follows from an identical argument as in the M = 3 case."}, {"heading": "4.2 Proof of Theorem 2", "text": "In this section, we prove Theorem 2. We first present an important technical lemma that addresses the behavior of the EM algorithm for a particular configuration of true and initial centers. We then prove the theorem by constructing a bad example and recursively applying this lemma. The proof of this lemma is given in Appendix A.\nWe focus on the one-dimensional setting throughout this proof. We use Bx(\u03b4) to denote an interval centered at x with radius \u03b4, that is, Bx(\u03b4) = [x \u2212 \u03b4, x + \u03b4]. We also use Bx(\u03b4) to represent the complement of the interval Bx(\u03b4), i.e. Bx(\u03b4) = (\u2212\u221e, x\u2212 \u03b4) \u222a (x+ \u03b4,\u221e).\nAs a preliminary, let us define a class of GMMs, which we refer to as diffuse GMMs. We say that a mixture model GMM(\u00b5\u2217) consisting of M\u0303 components is (c, \u03b4)-diffuse if:\n(a) For some M \u2264 M\u0303 , there are M centers contained in Bc\u03b4(\u03b4) \u222a B\u2212c\u03b4(\u03b4);\n(b) Each of the sets Bc\u03b4(\u03b4) and B\u2212c\u03b4(\u03b4) contain at least one center;\n(c) The remaining M\u0303 \u2212M centers are all in B0(20c\u03b4).\nConsider the EM algorithm, and denote by M (t) 1 ,M (t) 2 and M (t) 3 the number of centers the EM algorithm has in the tth iteration in the sets B\u2212c\u03b4(2\u03b4),Bc\u03b4(2\u03b4) and B0(20c\u03b4) respectively, where c and \u03b4 are those specified in the definition of the diffuse GMM. To be clear, M (0) 1 ,M (0) 2 and M (0) 3 denote the number of centers in these sets in the initial configuration specified to the EM algorithm. With these definitions in place, we can state our lemma.\nLemma 1. Suppose that the true underlying distribution is a (c, \u03b4)-diffuse GMM with c > 20 and \u03b4 > logM + 3, and that the EM algorithm is initialized so that M (0) 1 ,M (0) 2 \u2265 1.\n(a) If M = M\u0303 , then\nM (t) 1 = M (0) 1 and M (t) 2 = M (0) 2 for every t \u2265 0. (8)\n(b) If M < M\u0303 , suppose further that for each center in \u00b5\u2217j \u2208 B0(20c\u03b4), there is an initial center \u00b5\n(0) j\u2032 such that |\u00b5 (0) j\u2032 \u2212 \u00b5\u2217j | \u2264 |\u00b5\u2217j |/10. Then the same conditions (8) hold.\nIntuitively, these results show that if the true centers are clustered together into two clusters that are well separated, and the EM algorithm is initialized so that each cluster is accounted for by at least one initial center then the EM algorithm remains trapped in the initial configuration of centers. A concrete implication of part (a) is that if the true distribution is a (c, \u03b4)-diffuse GMM with M\u0303 = M and M\u22171 ,M \u2217 2 true clusters lie in B\u2212c\u03b4(\u03b4) and Bc\u03b4(\u03b4) respectively, then there are only three possible ways to initialize the EM algorithm that might possibly converge to a global maximum of the log-likelihood function; i.e., the pair (M (0) 1 ,M (0) 2 ) must be one of {(M, 0), (M\u22171 ,M\u22172 ), (0,M)}, where M = M\u22171 +M\u22172 . We are now equipped to prove Theorem 2. We will first focus on the case M = 2m for some positive integer m; the case of arbitrary M will be addressed later. At a high level, we will first construct the distribution GMM(\u00b5\u2217) that establishes the theorem, and then use the above technical lemma in order to reason about the behavior of the EM algorithm on this distribution.\nCase M = 2m: First, define the collection of 2m binary vectors of the form \u01eb = (\u01eb1, \u01eb2, \u00b7 \u00b7 \u00b7 , \u01ebm) where each \u01ebi \u2208 {\u22121, 1}. Consider the distribution, GMM(\u00b5\u2217), with the locations of the true centers indexed by these 2m vectors; i.e., each center is located at\n\u00b5(\u01eb) =\nm\u2211\ni=1\n\u01ebi ( 1 100 )i\u22121 R, (9)\nwhere we choose R \u2265 100m+1(M + 1). This in turn implies that the distance between the closest pair of true centers is at least 104 \u00d7 (M + 1).\nOur random initialization strategy samples the initial centers \u00b51, \u00b52, \u00b7 \u00b7 \u00b7 , \u00b5M i.i.d. from the distribution GMM(\u00b5\u2217). We can view this sampling process as two separate steps:\n(i) Sample an integer Zi uniformly from [M ].\n(ii) Sample value \u00b5i from the Gaussian distribution N (\u00b5\u2217Zi , I). Concentration properties of the Gaussian distribution will ensure that \u00b5i will not be too far from its expectation \u00b5\u2217Zi . Formally, we define the following event:\nEM := { all M initial points \u00b5i are contained in B\u00b5\u2217Zi (M) } . (10)\nBy standard Gaussian tail bounds, we have\nP(EM ) = (1\u2212 PX\u223cN (0,1)(|X| > M))M \u2265 (1\u2212 2Me\u2212M 2/2) \u2265 1\u2212 e\u2212\u2126(M)\nThis implies that the event EM will hold with high probability (when M is large). Conditioned on the event EM , we are guaranteed that all initialized points are relatively close to some true center.\nA key observation regarding the configuration of centers in the model GMM(\u00b5\u2217) specified by equation (9) is that the true centers can be partitioned into two well separated regions. More precisely, it is easy to verify that there are M/2 true centers in the interval B\u2212R(R/99) while the remaining M/2 true centers are contained in the interval BR(R/99). In what follows, we refer to B\u2212R(2R/99) as the left urn and to BR(2R/99) as the right urn.\nConditioned on EM , each initial point lands in either the left urn or the right urn with equal probability. Suppose we initialize EM with (M1,M2) centers in the left and right urn respectively. By Lemma 1(a), the only three possible values of the pair (M1,M2) for which the EM algorithm might converge to a global optimum are (0,M), (M, 0), (M/2,M/2). A simple calculation will show that the first and second possibilities occur with exponentially small probability. However, the third possibility occurs with only polynomially small probability, and so we need to further investigate this possibility.\nConsider, for example, the left urn: the true centers in the left urn can further be partitioned into two intervals B\u22121.01R(R/9900) and B\u22120.99R(R/9900) with M/4 true centers in each. Thus, each urn can be further partitioned into a left urn and a right urn. Following the same analysis as above and now using part (b) of Lemma 1 instead of part (a), we see that in order to ensure that the EM algorithm converges to a global optimum, the number of initial centers in B\u22121.01R(2R/9900) and B\u22120.99R(2R/9900) must be one of the following pairs {(0,M/2), (M/2, 0), (M/4,M/4)}.\nOur configuration of centers in equation (9) guarantees that this argument can be recursively applied until we reach an interval which contains only two true centers. For a configuration of M initial centers, we call these initial centers a good initialization for a collection of true centers if one of the following holds:\n(a) M = 1,\n(b) the number of initial centers assigned to the left urn and the right urn of the collection of true centers are either (0,M) or (M, 0),\n(c) the number of initial centers assigned to the left urn and the right urn of the collection of true centers are (M/2,M/2); and further recursively the initialization in both the left and the right urns are good initializations.\nLemma 1 implies that the EM algorithm converges to a global maximum only if a good initialization is realized. We will now show that the probability of a good initialization is exponentially small.\nLet FM represent the event that a good initialization is generated on a mixture with M components, for our configuration of true centers. Let M1 and M2 represent the number of initial centers in the left urn and the right urn, respectively. Conditioning on the event EM from equation (10), we have\nP(FM | EM ) \u2264 P(M1 = 0) + P(M1 = M) + P(M1 = M/2) \u00b7 ( P(FM/2 | EM ) )2\n\u2264 2\u00d7 ( M\n0\n) 1\n2M +\n( M\nM/2\n) 1\n2M \u00b7 ( P(FM/2 | EM ) )2\n\u2264 1 2M\u22121 + 1 2 \u00b7 ( P(FM/2 | EM ) )2 .\nSince P(F1 | EM ) = 1, solving this recursive inequality implies that P(FM | EM ) \u2264 e\u2212cM for some universal constant c. Thus, the probability that the EM algorithm converges to a global maximum is upper bounded by:\nP(FM ) \u2264 P(EM )P(FM | EM ) + P(EM ) \u2264 P(FM | EM ) + P(EM ) \u2264 e\u2212\u2126(M).\nTo complete the proof for the case when M = 2m for a positive integer m, we need to argue that on the event P(FM ), the log-likelihood of the solution reached by the EM algorithm can be arbitrarily worse than that of the global maximum. We claim that when the event FM occurs, the EM algorithm returns a solution \u00b5 for which there is at least one urn containing two true centers which is assigned a single center by the EM algorithm at every iteration t \u2265 0. As a consequence, there is at least one true center \u00b5\u2217j for which we have that |\u00b5\u2217j \u2212 \u00b5\u2032i| \u2265 R100m for all i = 1, . . . ,M . Now, we claim that we can choose R to be large enough to ensure an arbitrarily large gap in the likelihood of the EM solution and the global maximum. Concretely, as R \u2192 \u221e, we have:\nlim R\u2192+\u221e L(\u00b5) \u2264 lim R\u2192+\u221e\n1\nM EX\u223cN (\u00b5\u2217j ,1)\nlog\n( M\u2211\ni=1\ne\u2212 1 2 (X\u2212\u00b5i)2 ) \u2212 1\nM log(M\n\u221a 2\u03c0) = \u2212\u221e.\nHowever, the global maximum \u00b5\u2217 has log-likelihood\nlim R\u2192+\u221e L(\u00b5\u2217) = \u22121 2 \u2212 log(M\n\u221a 2\u03c0).\nOnce again we can use the continuity of the log-likelihood as a function of R to conclude that there is a finite sufficiently large R > 0 such that the conclusion of Theorem 2 holds.\nCase 2m\u22121 < M \u2264 2m: At a high level, we deal with this case by constructing a configuration with 2m centers and pruning this down to have M centers, while ensuring that the resulting urns are still approximately balanced which in turn ensures that our previous calculations continue to hold.\nOur configuration of true centers in equation (9) can be viewed as the 2m leaves of a binary tree with depth M , where the vectors \u01eb indexing the true centers represent the unique path from the root and to the leaf: the value of \u01ebi indicates whether to go down to the left child or to the right child at the i-th level of the tree. We choose M true centers from the 2m leaves by the following procedure. Starting from the root, we assign \u2308M/2\u2309 true centers to the left sub-tree, and assign \u230aM/2\u230b true centers to the right sub-tree. For any sub-tree, suppose that it was assigned l true centers, then we assign \u2308l/2\u2309 true centers to its left subtree and \u230al/2\u230b true centers to its right subtree. This procedure is recursively continued until all the true centers are assigned to leaves. Each leaf corresponds to a point on the real line and we choose this point as the location of the corresponding center.\nThe above construction has the following two properties: first, the locations of the true centers satisfy the separation requirements we used in dealing with the case when M = 2m, and further the assignment of the centers to the left and right urns in each case is roughly balanced. By leveraging these two properties we can follow essentially the same steps as we did in the case with M = 2m, and we omit these remaining proof details here."}, {"heading": "4.3 Proof of Theorem 3", "text": "We now embark on the proof of Theorem 3. The proof follows from a similar outline to the proof of Theorem 2 and we only develop the main ideas here. Concretely, it is easy to verify that in order to prove the result we only need to establish the analogue of Lemma 1 for the first-order EM algorithm.\nIntuitively, we first argue that the first-order EM updates can be viewed as less aggressive versions of the corresponding EM updates, and we use this fact to argue that Lemma 1 continues to hold for the first-order EM algorithm. Concretely, we can compare the update of EM algorithm:\n\u00b5new, EMi = E\u00b5\u2217wi(X) \u00b7X E\u00b5\u2217wi(X)\nwith the update of the first-order EM algorithm:\n\u00b5new, first-order EMi = \u00b5i + sE\u00b5\u2217wi(X)(X \u2212 \u00b5i).\nIf for any parameter \u00b5i, we choose the stepsize s = 1\nE\u00b5\u2217wi(X) , for the first-order EM algorithm,\nthen the two updates will match for that parameter. For the first-order EM algorithm, we always use a step size s \u2208 (0, 1), while 1\nE\u00b5\u2217wi(X) \u2265 1. Consequently, there must exist some\n\u03b8i \u2208 [0, 1] such that\n\u00b5new, first-order EMi = \u03b8i\u00b5i + (1\u2212 \u03b8i)\u00b5 new, EM i .\nThus, we see that the first-order EM update is a less aggressive version of the EM update. An examination of the proof of Lemma 1 reveals that this property suffices to ensure that its guarantees continue to hold for the first-order EM algorithm, which completes the proof of Theorem 3."}, {"heading": "4.4 Proof of Theorem 4", "text": "In this section, we prove Theorem 4. Throughout this proof, we use the fact that the first-order EM updates with step size s \u2208 (0, 1) take the form\n\u00b5new = \u00b5+ s\u2207L(\u00b5). (11)\nIn order to reason about the behavior of the first-order EM algorithm, we first provide a result that concerns the Hessian of the log-likelihood.\nLemma 2. For any scalar s \u2208 (0, 1) and for any \u00b5, we have s\u22072L(\u00b5) \u227b \u2212I.\nWe prove this claim at the end of the section. Taking this lemma as given, we can now prove the theorem\u2019s claims. We first show that the first-order EM algorithm with stepsize s \u2208 (0, 1) converges to a critical point. By a Taylor expansion of the log-likelihood function, we have\nL(\u00b5new) =L(\u00b5) + \u3008\u2207L(\u00b5),\u00b5new \u2212 \u00b5\u3009+ 1 2 (\u00b5new \u2212 \u00b5)T\u22072L(\u00b5\u0303)(\u00b5new \u2212 \u00b5),\nfor some \u00b5\u0303 on the line joining \u00b5 and \u00b5new. Applying Lemma 2 guarantees that\nL(\u00b5new) \u2265L(\u00b5) + \u3008\u2207L(\u00b5),\u00b5new \u2212 \u00b5\u3009 \u2212 1 2s \u2016\u00b5new \u2212 \u00b5\u201622.\nFrom the form (11) of the gradient EM updates, we then have\nL(\u00b5new) \u2265L(\u00b5) + ( s\u2212 s\n2\n) \u2016\u2207L(\u00b5)\u201622.\nConsequently, for any choice of step size s \u2208 (0, 1) and any point \u00b5 for which \u2207L(\u00b5) 6= 0, applying the gradient EM update leads to a strict increase in the value of the population likelihood L. Since L is upper bounded by a constant for a mixture of M spherical Gaussians, we can conclude that first-order EM must converge to some point. It is easy to further verify that it must converge to a point for which \u2207L(\u00b5) = 0 which concludes the first part of our proof.\nNext we show that the first-order EM algorithm will not converge to strict saddle points almost surely. We do this via a technique that has been used in recent papers [16, 19], exploiting the stable manifold theorem from dynamical systems theory. For this portion of the proof, it will be convenient to view the first-order EM updates as a map from the parameter space to the parameter space; i.e., we define the first-order EM map by:\ng(\u00b5) := \u00b5+ s\u2207L(\u00b5). (12)\nRecalling Definition 1 of strict saddle points, we denote by Dss the set of initial points from which the first-order EM algorithm converges to a strict saddle point. With these definitions in place, we can state an intermediate result:\nLemma 3 ([16, 19]). If the map \u00b5 7\u2192 g(\u00b5) defined by equation (12) is a local diffeomorphism for each \u00b5, then Dss has zero Lebesgue measure.\nDenote the Jacobian matrix of map g at point \u00b5 as \u2207g(\u00b5) where [\u2207g(\u00b5)]ij = \u2202\u2202\u00b5j gi(\u00b5). By Lemma 2, the Jacobian \u2207g(\u00b5) = I+s\u22072L(\u00b5) is strictly positive definite, and hence invertible for all \u00b5, which implies that the map g is a local diffeomorphism everywhere. Furthermore, our random initialization strategy specifies the distribution of the initial point \u00b5(0) which is\nabsolutely continuous with respect to Lebesgue measure. Combined these facts with lemma 3, we have proved Theorem 4.\nFinally, the only remaining detail is to prove Lemma 2. By definition, we have\nI + s\u22072L(\u00b5) =\n  (1\u2212 sEw1(X))Id . . . 0\n. . . . . . 0 . . . (1\u2212 sEwM(X))Id\n \n\ufe38 \ufe37\ufe37 \ufe38 :=D\n+sQ,\nwhere the matrix Q has d-dimensional blocks of the form\nQij = { E(wi(X) \u2212 w2i (X))(X \u2212 \u00b5i)(X \u2212 \u00b5i)\u22a4 if i = j \u2212Ewi(X)wj(X)(X \u2212 \u00b5i)(X \u2212 \u00b5j)\u22a4 otherwise.\nSince wi(X) \u2264 1 for all i \u2208 [M ] and s < 1, it follows that the diagonal matrix D is strictly positive definite. Consequently, in order to prove Lemma 2, it suffices to show that Q is positive semidefinite. Letting v = (v\u22a41 , . . . ,v \u22a4 M )\n\u22a4, where vi \u2208 Rd, be arbitrary vectors, we have:\nv\u22a4Qv = M\u2211\ni=1\nEwi(X)[v \u22a4 i (X \u2212 \u00b5i)]2 \u2212\nM\u2211\ni=1\nM\u2211\nj=1\nEwi(X)wj(X)[v \u22a4 i (X \u2212 \u00b5i)][v\u22a4j (X \u2212 \u00b5j)]\n(i) \u2265 M\u2211\ni=1\nEwi(X)[v \u22a4 i (X \u2212 \u00b5i)]2 \u2212\nM\u2211\ni=1\nM\u2211\nj=1\n1\n2\n[ Ewi(X)wj(X)[v \u22a4 i (X \u2212 \u00b5i)]2 + Ewi(X)wj(X)[v\u22a4j (X \u2212 \u00b5j)]2 ]\n(ii) =\nM\u2211\ni=1\nEwi(X)[v \u22a4 i (X \u2212 \u00b5i)]2 \u2212\nM\u2211\ni=1\nEwi(X)[v \u22a4 i (X \u2212 \u00b5i)]2 = 0,\nwhere step (i) uses the elementary inequality |ab| \u2264 12(a2+ b2); and step (ii) uses the fact that\u2211M i=1 wi(X) = 1 for any X. This completes the proof."}, {"heading": "Acknowledgements", "text": "This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, the Mathematical Data Science program of the Office of Naval Research under grant number N00014-15-1-2670, and National Science Foundation Grant CIF-31712-23800."}, {"heading": "A Proofs of Technical Lemmas", "text": "The bulk of this section is devoted to the proof of Lemma 1, which is based on a number of technical lemmas.\nA.1 Proof of Lemma 1\nUnderlying our proof is the following auxiliary result:\nLemma 4. Suppose that:\n(a) The true distribution is a GMM(\u00b5\u2217) with M components and that all true centers are located in (\u2212\u221e,\u221210a)\u222a (a,+\u221e) with at least one center in (a, 3a), with a > logM +3.\n(b) The current configuration of centers has the property that for any true center \u00b5\u2217j in (\u2212\u221e,\u221210a), there exists a current center \u00b5j\u2032 such that |\u00b5j\u2032 \u2212 \u00b5\u2217j | \u2264 |\u00b5\u2217j |/6.\nThen, for any i \u2208 [M ] for which the current parameter \u00b5i \u2208 [0, 4a], we have Ewi(X)X \u2265 0.\nSee Section A.2 for the proof of this claim.\nUsing Lemma 4, let us now prove Lemma 1. Without loss of generality, we may assume that \u00b5i \u2208 Bc\u03b4(2\u03b4), for some i \u2208 [M ]. Thus, in order to establish the claim, it suffices to show that after one step of the EM algorithm, the new iterate \u00b5newi belongs to Bc\u03b4(2\u03b4) as well.\nIn order to show that \u00b5newi \u2208 Bc\u03b4(2\u03b4), note that by the update equation (5), we have \u00b5newi = Ewi(X)X Ewi(X) . Thus, it is equivalent to prove that\nEwi(X)(X \u2212 (c\u2212 2)\u03b4) \u2265 0, and Ewi(X)(X \u2212 (c+ 2)\u03b4) \u2264 0.\nThe first inequality can be proved by substituting Z = X \u2212 (c \u2212 2)\u03b4 and applying Lemma 4 to Z. Similarly, the second inequality can be proved by defining Y : = (c+2)\u03b4 \u2212X, and then applying Lemma 4 to Y .\nA.2 Proof of Lemma 4\nOur proof of this claim hinges on two auxiliary lemmas, which we begin by stating. Intuitively, our first lemma shows that if the data are generated by a single Gaussian, whose mean is at least \u2126(logM) to the right of the origin, then it will affect any \u00b5i \u2265 0, by forcing it to the right no matter where the other {\u00b5j}j 6=i are.\nLemma 5. Suppose that the true distribution is a unit variance Gaussian with mean \u00b5\u2217 \u2265 a for some a > logM +3, and that the current configuration of centers, \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5M , has the ith center \u00b5i \u2265 0. Then we have\nEwi(X)X \u2265 0. (13a)\nFurthermore, if \u00b5\u2217 \u2264 3a, and 0 \u2264 \u00b5i \u2264 4a, then:\nEwi(X)X \u2265 a\n5M e\u22129a 2/2. (13b)\nSee Section A.3 for the proof of this claim. In a similar vein, if the data is generated by a single Gaussian far to the left of the origin, and some current center \u00b5j is sufficiently close to it then this Gaussian will force \u00b5i towards the negative direction, but will only have a small effect on \u00b5i. More formally, we have the following result: Lemma 6. Suppose that the true distribution is a unit variance Gaussian with mean \u00b5\u2217 = \u2212r, and that the current configuration of centers, \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5M , has the ith center \u00b5i \u2265 0 and further has at least one \u00b5j such that |\u00b5j \u2212 \u00b5\u2217| \u2264 r6 . Then we have that:\nEwi(X)X \u2265 \u22123re\u2212r 2/18. (14)\nSee Section A.4 for the proof of this claim.\nEquipped with these two auxiliary results, we can now prove Lemma 4. Without loss of generality, suppose that the centers are sorted in ascending order, and that the \u2113th true center is the smallest true center in (0,+\u221e). From the assumptions of Lemma 4, we know \u00b5\u2217\u2113 belongs to the interval (a, 3a). Thus, when X is drawn from a Gaussian mixture, we have\nEwi(X)X = 1\nM\nM\u2211\nj=1\nEX\u223cN (\u00b5\u2217j ,1) wi(X)X\n= 1\nM\n\u2113\u22121\u2211\nj=1\nEX\u223cN (\u00b5\u2217j ,1) wi(X)X +\n1\nM EX\u223cN (\u00b5\u2217\u2113 ,1)\nwi(X)X + 1\nM\nM\u2211\nj=\u2113+1\nEX\u223cN (\u00b5\u2217j ,1) wi(X)X.\nWe now use Lemma 6 to bound the first term. Since f(y) = \u22123y \u00b7 e\u2212y2/18 is monotonically increasing in [3,+\u221e), and from the assumptions of Lemma 4, we have |\u00b5\u2217j | > \u221210a > \u2212(9a+2) for all j < \u2113. Then:\n1\nM\n\u2113\u22121\u2211\nj=1\nEX\u223cN (\u00b5\u2217j , 1 2 )wi(X)X \u2265 \u2212\n1\nM\n\u2113\u22121\u2211\nj=1\n3 | \u00b5i | e\u2212\u00b5 2 i /18 \u2265 \u22123(9a+ 2)e\u2212(9a+2)2/18.\nBy Lemma 5, we know that the third term is non-negative and that the second term can be lower bounded by a sufficiently large quantity. Putting together the pieces, we find that\nEXwi(X)X \u2265 \u22123(9a+ 2)e\u22129a 2/2\u22122a\u2212 2 9 + a\n5M2 e\u22129a 2/2\n\u2265 e\u22129a2/2 [ a 5M2 \u2212 3(9a+ 2)e\u22122 logM\u22126 ] \u2265 e \u22129a2/2\u22126\nM2 [80a\u2212 3(9a + 2)]\n\u2265 0, which completes the proof.\nA.3 Proof of Lemma 5\nIntroducing the shorthand w\u2217 : = minx\u2208[1,2]wi(x), we have\nEwi(X)X \u2265 1\u221a 2\u03c0\n\u222b 0\n\u2212\u221e wi(x)xe \u2212(x\u2212\u00b5\u2217)2/2dx+ 1\u221a 2\u03c0\n\u222b 2\n1 wi(x)xe\n\u2212(x\u2212\u00b5\u2217)2/2dx\n+ 1\u221a 2\u03c0\n\u222b 3a\na wi(x)xe\n\u2212(x\u2212\u00b5\u2217)2/2dx.\nWe calculate the first two terms: for this purpose, the following lemma is useful:\nLemma 7. For any \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5M where \u00b5i \u2265 0, we have following:\nmin x\u2208[1,2]\nwi(x) \u2265 1\nMe2 max x\u2208(\u2212\u221e,0] wi(x). (15)\nSee Section A.3.1 for the proof of this claim. From Lemma 7, we have that:\n1\u221a 2\u03c0\n\u222b 0\n\u2212\u221e wi(x)xe \u2212(x\u2212\u00b5\u2217)2/2dx+ 1\u221a 2\u03c0\n\u222b 2\n1 wi(x)xe\n\u2212(x\u2212\u00b5\u2217)2/2\n\u2265 1\u221a 2\u03c0\n[\u222b 0\n\u2212\u221e Me2w\u2217xe\u2212(x\u2212\u00b5 \u2217)2/2dx+\n\u222b 2\n1 w\u2217xe\u2212(x\u2212\u00b5 \u2217)2/2dx\n]\n\u2265 w \u2217\n\u221a 2\u03c0\n[\u222b 0\n\u2212\u221e Me2(x\u2212 \u00b5\u2217)e\u2212(x\u2212\u00b5\u2217)2/2dx+\n\u222b 2\n1 xe\u2212(x\u2212\u00b5 \u2217)2/2dx\n]\n\u2265 w \u2217\n\u221a 2\u03c0\n[ \u2212Me2\u2212(\u00b5\u2217)2/2 + e\u2212(\u00b5\u2217\u22121)2/2 ] = w\u2217e\u2212(\u00b5 \u2217)2\n\u221a 2\u03c0\n[ e\u00b5 \u2217\u22121/2 \u2212Me2 ] \u2265 0.\nThe last inequality holds since \u00b5\u2217 > a > logM + 3. The third term is always positive, and this finishes the proof of first claim.\nFor second claim: if we further know that \u00b5\u2217 \u2264 3a, and \u00b5i \u2264 4a, then for any x \u2208 [a, 3a], wi(x) \u2265 e \u22129a2/2\nM , we have:\n1\u221a 2\u03c0\n\u222b 3a\na wi(x)xe \u2212(x\u2212\u00b5\u2217)2/2dx \u2265 1 M \u221a 2\u03c0 e\u22129a 2/2a\n\u222b 3a\na e\u2212(x\u2212\u00b5 \u2217)2/2dx\n\u2265 a M \u221a 2e\u03c0 e\u22129a 2/2 \u2265 a 5M e\u22129a 2/2.\nThe last inequality is true by integrating over an interval of length 1 around \u00b5\u2217 contained in (a, 3a).\nA.3.1 Proof of Lemma 7\nWe split the proof into two cases.\nCase \u00b5i \u2208 [0, 2]: In this case, we are guaranteed that maxx\u2208(\u2212\u221e,0]wi(x) \u2264 1. Also, for any x \u2208 [1, 2], we have:\nwi(x) = e\u2212(x\u2212\u00b5i) 2/2\n\u2211 j e\n\u2212(x\u2212\u00b5j)2/2 \u2265 1 Me2 , (16)\nwhich proves the required result.\nCase \u00b5i > 2: In this case, we have\nwi(x) = e\u2212(x\u2212\u00b5i) 2/2\n\u2211 j e \u2212(x\u2212\u00b5j )2/2 = 1\u2211 j 6=i 1 M\u22121 + e [(x\u2212\u00b5i)2\u2212(x\u2212\u00b5j)2]/2\n= 1\u2211\nj 6=i 1 M\u22121 + e (\u00b5i\u2212\u00b5j)(\u00b5i+\u00b5j\u22122x)/2\n= 1\u2211\nj 6=iAij(x) ,\nwhere Aij(x) := 1 M\u22121 + e (\u00b5i\u2212\u00b5j)(\u00b5i+\u00b5j\u22122x)/2. It suffices to show that\nAij(x) \u2264 MAij(x\u2032) for any x \u2208 [1, 2], x\u2032 \u2208 (\u2212\u221e, 0] and j \u2208 [M ]. (17)\nUsing this, we know:\nwi(x) = 1\u2211 j 6=iAij(x) \u2265 1\u2211 j 6=iMAij(x \u2032) = 1 M wi(x \u2032), (18)\nand the claim of Lemma 7 easily follows. In order to establish the claim of equation (17), we note that if \u00b5j \u2264 \u00b5i, then since x\u2032 < x, we have\n(\u00b5i \u2212 \u00b5j)(\u00b5i + \u00b5j \u2212 2x) \u2264 (\u00b5i \u2212 \u00b5j)(\u00b5i + \u00b5j \u2212 2x\u2032),\nwhich implies that Aij(x) \u2264 Aij(x\u2032). If \u00b5i < \u00b5j, then we know:\n(\u00b5i \u2212 \u00b5j)(\u00b5i + \u00b5j \u2212 2x) < 0. (19)\nThis implies Aij(x) \u2264 1M\u22121 + 1 = MM\u22121 . On the other hand, we always have Aij(x\u2032) \u2265 1M\u22121 , this gives Aij(x) \u2264 MAij(x\u2032), which finishes the proof.\nA.4 Proof of Lemma 6\nWe have\nEwi(X)X \u2265 1\u221a 2\u03c0\n\u222b \u22122r/3\n\u2212\u221e wi(x)xe \u2212(x\u2212\u00b5\u2217)2/2dx+ 1\u221a 2\u03c0\n\u222b 0\n\u22122r/3 wi(x)xe\n\u2212(x\u2212\u00b5\u2217)2/2dx.\nFor the first term, we know for any x \u2208 (\u2212\u221e,\u22122r/3], we have:\nwi(x) \u2264 e\u2212x 2/2\ne\u2212(x\u2212\u00b5j) 2/2\n= e\u2212x\u00b5j+\u00b5 2 j/2 \u2264 e\u2212 2r3 \u00b5j+\u00b52j/2 \u2264 e\u2212 772 r2 \u2264 e\u2212r2/18.\nThe second last inequality is true since \u00b5j \u2265 \u22127r6 . Thus, we know:\n1\u221a 2\u03c0\n\u222b \u22122r/3\n\u2212\u221e wi(x)xe\n\u2212(x\u2212\u00b5\u2217)2/2dx \u2265 e \u2212r2/18\n\u221a 2\u03c0\n\u222b \u22122r/3\n\u2212\u221e xe\u2212(x\u2212\u00b5 \u2217)2/2dx\n\u2265e \u2212r2/18\n\u221a 2\u03c0\n[\u222b \u22122r/3\n\u2212\u221e (x\u2212 \u00b5\u2217)e\u2212(x\u2212\u00b5\u2217)2/2dx+ \u00b5\u2217\n\u221a 2\u03c0\n]\n\u2265e \u2212r2/18\n\u221a 2\u03c0 [ \u22121 2 e\u2212r 2/18 \u2212 r \u221a 2\u03c0 ] \u2265 \u22122re\u2212r2/18.\nFor the second term, we have:\n1\u221a 2\u03c0\n\u222b 0\n\u22122r/3 wi(x)xe \u2212(x\u2212\u00b5\u2217)2/2dx \u2265\u2212 2r 3 \u221a 2\u03c0\n\u222b 0\n\u22122r/3 e\u2212(x\u2212\u00b5 \u2217)2/2dx\n\u2265\u2212 2r 3 \u221a 2\u03c0\n\u222b +\u221e\n\u22122r/3 e\u2212(x\u2212\u00b5 \u2217)2/2dx \u2265 \u22122r 3 e\u2212r 2/18.\nPutting the pieces together we obtain,\nEwi(X)X \u2265 \u2212( 2\n3 + 2)e\u2212r 2/18 \u2265 \u22123re\u2212r2/18,\nas desired."}], "references": [{"title": "Identifiability of parameters in latent structure models with many observed variables", "author": ["Elizabeth S Allman", "Catherine Matias", "John A Rhodes"], "venue": "Annals of Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Maximum likelihood estimates for Gaussian mixtures are transcendental", "author": ["Carlos Am\u00e9ndola", "Mathias Drton", "Bernd Sturmfels"], "venue": "In International Conference on Mathematical Aspects of Computer and Information Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning mixtures of separated nonspherical Gaussians", "author": ["Sanjeev Arora", "Ravi Kannan"], "venue": "The Annals of Applied Probability,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin J Wainwright", "Bin Yu"], "venue": "Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In 21st Annual Conference on Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Optimal rate of convergence for finite mixture models", "author": ["Jiahua Chen"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "The \u201cautomatic\u201d robustness of minimum distance functionals", "author": ["David L Donoho", "Richard C Liu"], "venue": "Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}, {"title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition", "author": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In 28th Annual Conference on Learning Theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Rates of convergence for the Gaussian mixture sieve", "author": ["Christopher R Genovese", "Larry Wasserman"], "venue": "Annals of Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Entropies and rates of convergence for maximum likelihood and Bayes estimation for mixtures of normal densities", "author": ["Subhashis Ghosal", "Aad W Van Der Vaart"], "venue": "Annals of Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Identifiability and optimal rates of convergence for parameters of multiple types in finite mixtures", "author": ["Nhat Ho", "XuanLong Nguyen"], "venue": "arXiv preprint arXiv:1501.02497,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions", "author": ["Daniel Hsu", "Sham M Kakade"], "venue": "In Proceedings of the 4th Conference on Innovations in Theoretical Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Gradient descent converges to minimizers", "author": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "author": ["Po-Ling Loh", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Settling the polynomial learnability of mixtures of Gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Gradient descent converges to minimizers: The case of non-isolated critical points", "author": ["Ioannis Panageas", "Georgios Piliouras"], "venue": "arXiv preprint arXiv:1605.00405,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "On the saddle point problem for non-convex optimization", "author": ["Razvan Pascanu", "Yann N Dauphin", "Surya Ganguli", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1405.4604,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Are there local maxima in the infinite-sample likelihood of Gaussian mixture estimation", "author": ["Nathan Srebro"], "venue": "In 20th Annual Conference on Learning Theory,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Identifiability of finite mixtures", "author": ["Henry Teicher"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1963}, {"title": "Statistical Analysis of Finite Mixture Distributions", "author": ["D Michael Titterington"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1985}, {"title": "A spectral algorithm for learning mixtures of distributions", "author": ["Santosh Vempala", "Grant Wang"], "venue": "In The 43rd Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}], "referenceMentions": [{"referenceID": 20, "context": "We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [21].", "startOffset": 168, "endOffset": 172}, {"referenceID": 22, "context": "Their ability to model data as arising from underlying subpopulations provides essential flexibility in a wide range of applications Titterington [23].", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "Early work [22] studied the identifiability of finite mixture models, and this problem has continued to attract significant interest (see the recent paper of Allman et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 0, "context": "[1] for a recent", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "More recent theoretical work has focused on issues related to the use of GMMs for the density estimation problem [12, 13].", "startOffset": 113, "endOffset": 121}, {"referenceID": 12, "context": "More recent theoretical work has focused on issues related to the use of GMMs for the density estimation problem [12, 13].", "startOffset": 113, "endOffset": 121}, {"referenceID": 6, "context": "Focusing on rates of convergence for parameter estimation in GMMs, Chen [7] established the surprising result that when the number of mixture components is unknown, then the standard \u221a n-rate for regular parametric models is not achievable.", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "Recent investigations [14] into exact-fitted, under-fitted and over-fitted GMMs have characterized the achievable rates of convergence in these settings.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": "From an algorithmic perspective, the dominant practical method for estimating GMMs is the Expectation-Maximization (EM) algorithm [9].", "startOffset": 130, "endOffset": 133}, {"referenceID": 2, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 5, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 7, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 23, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 57, "endOffset": 70}, {"referenceID": 4, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 99, "endOffset": 110}, {"referenceID": 14, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 99, "endOffset": 110}, {"referenceID": 17, "context": "Broadly, these algorithms are based either on clustering [3, 6, 8, 24] or on the method of moments [5, 15, 18].", "startOffset": 99, "endOffset": 110}, {"referenceID": 7, "context": "Dasgupta and Schulman [8] analyzed a two-round variant of EM, which involved over-fitting the mixture and then pruning extra centers.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "[4] studied the local convergence of the EM algorithm for a mixture of two Gaussians with \u03a9(1)-separation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In fact, maximum likelihood has favorable properties in this regard\u2014maximumlikelihood estimates are well known to be robust to perturbations in the Kullback-Leibler metric of the generative model [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 20, "context": "For this favorable setting, Srebro [21] conjectured that any local maximum of the likelihood function is a global maximum in the limit of infinite samples\u2014in other words, that there are no bad local maxima for the population GMM likelihood function.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "For suitably small sample sizes, it is known [2] that configurations of the samples can be constructed which lead to the likelihood function having an unbounded number of local maxima.", "startOffset": 45, "endOffset": 48}, {"referenceID": 20, "context": "The conjecture of Srebro [21] avoids this by requiring that the samples come from the specified GMM, as well as by considering the (infinite-sample-size) population setting.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "A mixture of two spherical Gaussians: A Gaussian mixture model with a single component is simply a Gaussian, so the conjecture of Srebro [21] holds trivially in this case.", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "Our first contribution is a negative answer to the open question of Srebro [21].", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "On the basis of empirical evidence, Srebro [21] conjectured that this population log-likelihood is in fact well-behaved, in the sense of having no spurious local optima.", "startOffset": 43, "endOffset": 47}, {"referenceID": 3, "context": "[4], is the first-order EM algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "(6) This update turns out to be equivalent to gradient ascent on the population likelihood L with step size s > 0 (see the paper [4] for details).", "startOffset": 129, "endOffset": 132}, {"referenceID": 20, "context": "This result provides a negative answer to the conjecture of Srebro [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "This result strongly suggests that that effective initialization schemes, such as those based on pilot estimators utilizing the method of moments [15, 18], are critical to finding good maxima in general GMMs.", "startOffset": 146, "endOffset": 154}, {"referenceID": 17, "context": "This result strongly suggests that that effective initialization schemes, such as those based on pilot estimators utilizing the method of moments [15, 18], are critical to finding good maxima in general GMMs.", "startOffset": 146, "endOffset": 154}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] argue that for high-dimensional optimization problems, the principal difficulty is the proliferation of saddle points, not the existence of poor local maxima.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In our setting, however, we can leverage recent results on gradient methods [16, 19] to show that the first-order EM algorithm cannot converge to strict saddle points.", "startOffset": 76, "endOffset": 84}, {"referenceID": 18, "context": "In our setting, however, we can leverage recent results on gradient methods [16, 19] to show that the first-order EM algorithm cannot converge to strict saddle points.", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "Definition 1 (Strict saddle point [11]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "The proof of Theorem 4 is based on recent work [16, 19] on the asymptotic performance of gradient methods.", "startOffset": 47, "endOffset": 55}, {"referenceID": 18, "context": "The proof of Theorem 4 is based on recent work [16, 19] on the asymptotic performance of gradient methods.", "startOffset": 47, "endOffset": 55}, {"referenceID": 0, "context": "Consequently, there must exist some \u03b8i \u2208 [0, 1] such that \u03bc first-order EM i = \u03b8i\u03bci + (1\u2212 \u03b8i)\u03bc new, EM i .", "startOffset": 41, "endOffset": 47}, {"referenceID": 15, "context": "We do this via a technique that has been used in recent papers [16, 19], exploiting the stable manifold theorem from dynamical systems theory.", "startOffset": 63, "endOffset": 71}, {"referenceID": 18, "context": "We do this via a technique that has been used in recent papers [16, 19], exploiting the stable manifold theorem from dynamical systems theory.", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "With these definitions in place, we can state an intermediate result: Lemma 3 ([16, 19]).", "startOffset": 79, "endOffset": 87}, {"referenceID": 18, "context": "With these definitions in place, we can state an intermediate result: Lemma 3 ([16, 19]).", "startOffset": 79, "endOffset": 87}], "year": 2016, "abstractText": "We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M \u2265 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [21]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 \u2212 e. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.", "creator": "LaTeX with hyperref package"}}}