{"id": "1609.01203", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2016", "title": "Live Orchestral Piano, a system for real-time orchestral music generation", "abstract": "this seminal paper introduces the first system for performing automatic orchestration based on a real - time piano input. we believe that it is possible to learn the underlying regularities once existing between piano soloist scores and their orchestrations by notorious composers, in order to automatically perform this task on novel instrumental piano inputs. to that end, we investigate a class of statistical inference models called conditional restricted boltzmann machine ( crbm ). we introduce a specific evaluation framework for orchestral generation based on a prediction task in order to assess the quality of different models. as prediction geometry and creation are two widely interpreted different endeavours, so we discuss the potential biases in evaluating temporal generative models through prediction tasks and their impact on discovering a creative system. finally, we introduce an important implementation of the proposed model called live orchestral piano ( lop ), which allows to precisely perform real - time projective orchestration of a midi keyboard input.", "histories": [["v1", "Mon, 5 Sep 2016 15:58:11 GMT  (1296kb,D)", "http://arxiv.org/abs/1609.01203v1", "Unpublished"], ["v2", "Thu, 18 May 2017 14:15:30 GMT  (1259kb,D)", "http://arxiv.org/abs/1609.01203v2", null]], "COMMENTS": "Unpublished", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["l\\'eopold crestel", "philippe esling"], "accepted": false, "id": "1609.01203"}, "pdf": {"name": "1609.01203.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "orchestration based on a real-time piano input. We believe that it is possible to learn the underlying regularities existing between piano scores and their orchestrations by notorious composers, in order to automatically perform this task on novel piano inputs. To that end, we investigate a class of statistical inference models called conditional Restricted Boltzmann Machine (cRBM ). We introduce a specific evaluation framework for orchestral generation based on a prediction task in order to assess the quality of different models. As prediction and creation are two widely different endeavours, we discuss the potential biases in evaluating temporal generative models through prediction tasks and their impact on a creative system. Finally, we introduce an implementation of the proposed model called Live Orchestral Piano (LOP), which allows to perform real-time projective orchestration of a MIDI keyboard input."}, {"heading": "1. Introduction", "text": "Orchestration is the subtle art of writing musical pieces for the orchestra, by combining the properties of various instruments in order to achieve a particular sonic rendering [12, 18]. Because it extensively relies on spectral characteristics, orchestration is often referred to as the art of manipulating instrumental timbres [15]. Timbre is defined as the property which allows listeners to distinguish two sounds produced at the same pitch and intensity. Hence, the sonic palette offered by the pitch range and intensities of each instrument is augmented by the wide range of expressive timbres produced through the use of the different playing styles. Furthermore, it has been shown that some instrumental mixtures can not be characterized by a simple summation of their spectral components, but can lead to a unique emerging timbre, with phenomenon such as orchestral blend [19]. Given the number of different instruments in a symphonic orchestra, their respective range of expressiveness (timbre, pitch and intensity), and the phenomenon of emerging timbre, one can foresee the extensive combinatorial complexity embedded in the process of orchestral composition.\nThose difficulties have been a major obstacle towards the construction of a scientific basis for the study of orchestration. From a mathematical point of view, it seems that no set of descriptors can exhaustively fit with the perceptual complexity of timbre [16]. From a musical point of view, there is a lack of specific symbolic notation for timbre, and orchestration remains an empirical discipline taught through the observation of existing orchestration examples [17]. Among the different writing\nDate: September 6, 2016. Key words and phrases. Automatic orchestration, Conditional Restricted Boltzmann Machine,\nDeep learning.\n1\nar X\niv :1\n60 9.\n01 20\n3v 1\n[ cs\n.L G\n] 5\nS ep\n2 01\n6\ntechniques for orchestral works, one of them consists in first writing an harmonic and rhythmic structure in the form of a piano score and then adding the timbre dimension by spreading the different voices over the various instruments [17]. We refer to this operation of extending a piano draft to an orchestral score as projective orchestration [6].\nThe orchestral repertoire contains a large number of examples of projective orchestration (such as the piano reductions by Liszt of Beethoven symphonies or the pictures at an exhibition, a piano piece by Moussorgsky orchestrated by several notorious composers). By observing a case of projective orchestration (see Figure 1 on page 2), we can clearly see that this process involves more than the mere allocation of notes written on the piano score across the different instruments. It rather implies harmonic enhancements and timbre manipulations to underline the already existing harmonic and rhythmic structure [15]. However, the visible correlations between a piano score and its orchestrations appear as a fertile framework for laying the foundations of a computational exploration of orchestration. Even though systemic rules remain difficult to identify because of the high dimensionality of the problem, preventing us from building a general theory of orchestration, it does not mean that underlying local regularities could not be extracted.\nStatistical inference covers a range of methods aimed at automatically extracting a structure from observations. These approaches hypothesize that a particular type of data might be structured by an underlying probability distribution. The objective is to deduce properties of this distribution, by observing a set of those data. If the structure of the data is efficiently extracted and organized, it should be possible in turn to generate novel examples. We believe that learning the underlying distribution of a corpus of piano scores and their orchestrations by famous composers through statistical inference is a promising lead toward the automatic generation of orchestrations with a sensible timbre structure. In the context of projective orchestration, the data are defined as the scores, formed by a series of pitches and intensities for each instrument. The set of observations is the set of\nprojective orchestrations performed by famous composers, and the probability distribution would model the set of notes played by each instrument conditionally on the corresponding piano score.\nIt might be surprising at first to rely solely on symbolic information (scores) whereas we insisted on the fact that orchestration is the art of timbre, typically not represented in the musical notation but rather conveyed in the signal information (audio recording). However, we make the assumption that spectrally consistent orchestrations could be generated from a purely symbolic learning by uncovering the composers\u2019 knowledge about timbre embedded in the score. As we rely on the works of composers that effectively took into account the subtleties of timbral effects, these symbolic relationships embed the spectral information.\nA wide range of statistical inference models have been devised, among which deep learning recently appeared as a promising field in artificial intelligence and representation learning [2, 14]. The building blocks of many models in this field is the Restricted Boltzmann Machine (RBM ) [9]. We focus on a class of models called Conditional RBM [21], which implements a notion of context particularly adapted to represent both the temporal dependencies in the orchestral score and the influence of the piano score. Besides these structural advantages, those models are also generative, which means that once the underlying distribution of the observed data is correctly modelled, it is possible to generate orchestration from unseen piano scores.\nIn order to rank the different models, it is important to devise a quantitative evaluation framework. Designing a criterion assessing the performance of a generative model is a major difficulty for creative systems. In the polyphonic music generation field, a predictive task is commonly used [22, 5, 13]. Relying on these works, we introduce a specific framework for projective orchestration and discuss the results of the cRBM and FGcRBM models.\nFinally, an interesting property of both the cRBM and FGcRBM models is their ability to generate orchestration from an unseen piano score sufficiently fast for a real-time implementation. Using the defined performance criterion we selected the most efficient model and implemented it in a system called Live Orchestral Piano (LOP), an interface for real-time orchestration from a piano input.\nThe remainder of this paper is organized as follows. The first section introduces the state of the art in conditional models, in which RBM, cRBM and FGcRBM models are detailed. The projective orchestration task is presented in the next section along with an evaluation framework based on a frame-level accuracy measure. The introduced models are evaluated within this framework and compared to existing models. Then, we introduce LOP, the real-time projective orchestration system. Finaly, we provide our conclusions and directions of future work."}, {"heading": "2. State of the art", "text": "In this section, three statistical inference models are detailed. The RBM, cRBM and FGcRBM are presented by increasing level of complexity.\n2.1. Restricted-Boltzmann Machine. The RBM [9] is a graphical probabilistic model (Figure 2 on page 6) composed by a set of I visible units v = (v1, ..., vI), each representing a binary random variable. Those visible units usually model the observed data, in which case I is equal to the dimension of the observed vectors. Note that this model can easily be extended to continuous variables [8]. A set of\nJ binary random variables (h = (h1, ..., hJ)) are called hidden (or latent) units. Hidden variables are not directly observed and model explanatory factors for the visible units distribution. They are connected to visible units through weights Wij which model conditional dependencies between the visible and hidden units activation\np(hj = 1|v) = \u03c3 ( bj +\n\u2211 i Wijvi\n) (1)\np(vi = 1|h) = \u03c3 ai +\u2211 j Wijhj (2) where \u03c3(x) = 11+e\u2212x is the sigmoid function . The biases ai and bj act as a permanent offset to the activation of a unit. Along with weights, they form the set of parameters of the model \u03b8 = {W ,a, b} that we seek to learn.\nThe joint probability of the visible and hidden random variables is given by\npmodel(v,h) = exp\u2212E(v,h)\nZ where\n(3) E(v,h) = \u2212 m\u2211 i=1 aivi \u2212 m\u2211 i=1 n\u2211 j=1 viWijhj \u2212 n\u2211 j=1 bjhj\nis the energy function associated to the model. Z = \u2211\nv,h exp \u2212E(v,h) is a normaliz-\ning factor called partition function which ensures that the sum of the probabilities over the possible configuration of visible and hidden variables is equal to one.\nTraining a model on a set of data means modifying its parameter \u0398 in order to approximate the hypothetical real distribution of the data with the distribution represented by the model (denoted p). A commonly used criterion for training a model is to maximize the likelihood of the training set, which can be interpreted as the probability that the observed data have been generated under the distribution p of the model. Instead of maximizing the likelihood, minimizing the negative log-likelihood is often preferred as it simplifies the mathematical expressions. Be denoting v(l) the vectors from the training set D, this quantity is given by\n(4) L(\u03b8|D) = 1 ND \u2211 v(l)\u2208D \u2212 ln ( p(v(l)|\u03b8) ) where ND is the size of the dataset.\nThe search for the minimum of this non-linear function can be tackled by using gradient descent [4]. The gradient of the negative log-likelihood of any vector from the training database v(l) is given by\n\u2212 \u2202 ln\n[ p(v(l)|\u03b8) ] \u2202\u03b8 = Ep(h|v(l)) [ \u2202E(v(l),h) \u2202\u03b8 ] \u2212\nEp(v,h) [ \u2202E(v,h)\n\u2202\u03b8 ](5) It is interesting to note that this represents the difference between two expectations of the same quantity. The first left-side expectation is referred to as the datadriven term since it is an expectation over the distribution of the hidden units conditionally on a specific sample from the data distribution. The expectation on the right is referred to as the model-driven term since it is an expectation over\nthe joint distribution of the model. Unfortunately the model-driven quantity is intractable because it involves a sum over all the possible configurations of the hidden (alternatively visible) units in order to compute the partition function [7].\nIn order to alleviate this problem, a training algorithm called Contrastive Divergence (CD) [10] proposes to approximate the model driven term by\nEp(v,h) [ \u2202E(v,h)\n\u2202\u03b8\n] \u2248 Ep(h|v(l,k)) [ \u2202E(v(l,k),h)\n\u2202\u03b8\n] (6)\nwhere v(l,k) is obtained by running a k-step Gibbs chain. A Gibbs chain consists in alternatively sampling the hidden units knowing the visible units (1) and the visible knowing the hidden (2).\nNote that sampling from the marginal distribution is easy since visible units (respectively hidden units) are independent from each others. Hence, knowing the hidden units, all the visible units can be sampled in one step. This allows for a fast implementation of the sampling step through matrix calculus, known as block sampling. It has been proved [1] that the samples obtained after an infinite number of iterations will be drawn from the joint distribution of the visible and hidden units of the model. However, to speed up the convergence of the chain, a first approximation consists to limit the number of sampling steps to a fixed number K. A second approximations can be made by starting the chain from the same sample v(l) used in the data-driven term [8]. After evaluating the statistics of the distribution h \u223c p(h|v(l)) and v(l,k) and h \u223c p(h|v(l,k)) from the Gibbs sampling chain, the parameters can be updated. By introducing the notation <>data (respectively <>model) to express an expectation over the data (respectively model) distribution the update rules in a RBM are given by\n\u2206Wij =< vihj >data \u2212 < vihj >model(7) \u2206ai =< vi >data \u2212 < vi >model(8) \u2206bj =< hj >data \u2212 < hj >model(9)\n2.2. Conditional RBM. The conditional RBM (cRBM ) model [20] is an extension of the RBM in which dynamic biases are added to the biases of the visible (a) and hidden (b) units (see Figure 3 on page 7). These dynamic biases linearly depend on a set of K context units denoted x = (x1, ..., xK), and the sum of the static and dynamic biases is equal to\na\u0302i(t) = ai + \u2211 k Akixk(t)\nb\u0302j(t) = bj + \u2211 k Bkjxk(t)\nIn the case of time series, if the visible units represent a frame at time t, these context units can be used to model the influence of the recent past frames [t\u2212N, t\u2212 1] on the current frame (N defining the temporal order). Thus, the energy function\nHidden units\nof the cRBM is given by\nE(v(t),h(t)|x(t)) = \u2212 \u2211 i\na\u0302i(t)vi(t)\u2212\u2211 ij Wijvi(t)hj(t)\u2212 \u2211 j b\u0302j(t)hj(t) (10)\nThis model can be trained using CD, since the marginal probabilities of visible and hidden units are the same as the RBM (simply replacing the static biases by dynamics ones). Hence, the update rules are unchanged for W , a and b, and\n\u2206Aik =< vixk >data \u2212 < vixk >model(11) \u2206Bjk =< hjxk >data \u2212 < hjxk >model(12)\n2.3. Factored Gated cRBM. The Factored Gated cRBM (FGcRBM ) model [20] proposes to extend the cRBM model by adding a layer of feature units z which modulates the weights of the conditional architecture in a multiplicative way (see Figure 4 on page 8). Hence, the parameters of the model become \u03b8 = {W ,A,B,a, b}, where W = (W )ijl, A = (A)ikl and B = (B)jkl are three-dimensional tensors.\nThis multiplicative influence can be interpreted as a modification of the energy function of the model depending on a for of style features. For a fixed configuration of feature units, a new energy function is defined by the cRBM (v, h, and x). The number of parameters grows cubically with the number of units. To reduce the computation load, the three dimensional tensors can be factorized into a product of three matrices by including factor units indexed by f such that\nWijl = Wif .Wjf .Wlf . The energy function of the FGcRBM is then given by E(v(t),h(t)|x(t), z(t)) = \u2212 \u2211 i\na\u0302i(t)vi(t)\u2212\u2211 j b\u0302j(t)hj(t)\u2212 \u2211 f \u2211 ijl W vifW h jfW z lfvi(t)hj(t)zl(t) (13)\nwhere the dynamic biases of the visible and hidden units are defined by (14) a\u0302i(t) = ai + \u2211 m \u2211 kl AvimA x kmA z lmxk(t)zl(t) (15) b\u0302j(t) = bj + \u2211 n \u2211 kl BhjnB x knB z lnxk(t)zl(t)\n2.4. Sampling from the models. Using gradient descent during the learning phase brings no guarantee over the quality of the approximation between the model distribution and the data distribution. However, CD guarantees that the KullbackLeibler divergence between the model and data distribution is reduced after each iteration[10], and an acceptable approximation should be found after large number of CD steps. Therefore, by sampling from the model distribution, we are able to generate novel data similar to the one observed in the training dataset but yet unseen. In practice, sampling from the conditional models is performed by computing the marginal distribution of the visible units knowing the context p(v|x) = \u2211 h p(v, h|x), and then sampling from a Bernoulli distribution of parameter p(v|x). Unfortunately, this marginal distribution remains computationally intractable in the introduced models because of the partition function. However, samples from an approximate distribution can be reached through alternate Gibbs sampling. After randomly setting the visible units (for each index i, vi \u223c U(0, 1)), K Gibbs sampling steps are performed to obtain a visible sample. The objective of these K steps is to reach the equilibrium distribution of the model. Even though a\ntheoretically infinite number of steps is necessary, 20 to 100 steps are typically sufficient. Note that, in practice, a threshold is applied on the activation of the visible units before the last sampling step in the Gibbs chain so that unlikely activations are set to zero."}, {"heading": "3. Projective orchestration", "text": "In this section, we introduce and formalize the automatic projective orchestration task presented in Figure 1 on page 2. In particular, we detail the database and\ndata representation used, evaluation framework, and discuss the results obtained by different models.\n3.1. Database. We use a database of piano scores and their orchestration by famous composers. The database consists of 76 excerpts of orchestral pieces, and fourteen different instruments were present in the database. This database has been collected by orchestration teachers and are the transcription of famous composers orchestration in the MusicXML format.\n3.2. Data representation. In order to process the scores, we import them as matrices called piano-roll, a data representation traditionally used to model polyphonic music of a single instrument (see Figure 6 on page 10). Its extension to represent an orchestra is straightforwardly obtained by the concatenation of the piano-rolls of each instrument along the pitch dimension.\nThe rhythmic quantization is defined as the number of time frame in the pianoroll per quarter note. When constructing the piano-rolls, we used a rhythmic quantization of 4.\nIn order to reduce the number of units, we systematically remove, for each instrument, any pitch which is never played in the training database. Hence, the dimension of the orchestral vector decreased from 2432 to 456 and the piano vector dimension from 128 to 88. Also, we follow the classic orchestral simplifications used when writing orchestral scores by grouping together all the instruments of the same section. For instance, the violin section, which might be composed by several instrumentalists, is written as a single part. Finally, the velocity information is discarded, since we use binary units which solely indicate if a note is on or off.\n3.3. Model definition. For each orchestral piece, we define Orch(t) and Piano(t) as the sequence of column vectors from the piano-roll of the orchestra and piano part respectively, with t \u2208 [1, NT ] where NT is the length of the piece.\nAt each time index t, the visible units of the cRBM will model the current orchestral vector Orch(t) that we want to generate. Conditional units are used to add the influence of the past orchestral vectors Orch(t \u2212 1), ..., Orch(t \u2212 N) and the influence of the current piano frame Piano(t) over the visible units. Hence, in the cRBM model, the context units at time t are defined by the concatenation of the past orchestral frames and current piano frame\n(16) x(t) = [Piano(t),Orch(t\u2212 1), ...,Orch(t\u2212N)] The FGcRBM model allows to separate the influence of the current piano frame and the past orchestral frames. Hence, the current piano frame defines the feature units\n(17) z(t) = Piano(t)\nwhile the concatenation of the past orchestral frames defines the context units (18) x(t) = [ Orch(t\u2212 1)T , ...,Orch(t\u2212N)T ] 3.4. Evaluation framework. We introduce an event-level orchestral inference task to evaluate our models. To our best knowledge, this is the first attempt to define a quantitative evaluation framework for automatic projective orchestration.\nIn order to evaluate the models, the data used to train and those used to assess the performances must be different [3]. Hence, the dataset is usually split between a train and test set. Given the complexity of the distribution we want to model\nand the extremely narrow size of the database, we rely on a Leave-One-Out (LOO) evaluation where 75 scores are used to train the model and one is kept for testing. This choice is motivated by the will to obtain the best generative model possible.\nPredictive symbolic music tasks are usually evaluated through a frame-level accuracy measure where each time frame of the piano-roll is considered as a separate data example [22, 5, 13]. We propose to work in an event-level framework, where data examples are only those for which an event occurs. An event is defined as a time te where Orch(te) 6= Orch(te\u2212 1). The reason why we do not rely on a framelevel evaluation is that a model which simply repeats the previous frame gradually becomes the best model as the quantization gets finer. Moreover, in the projective orchestration framework, rhythmic patterns are already established by the known piano score and, thus, does not have to be learned by the model.\nFinally, to speed-up the convergence of the gradient descent, mini-batches are used to train the model [3]. Following these procedures, 89 mini-batches of 100 event-level points were obtained for training.\n3.5. Measure. An objective criterion of the performances of the models is necessary to provide an evaluation. The best way for evaluating generative models is to compute the likelihood of the test set (see equation (4)). However, we have seen that this quantity is intractable in all RBM, cRBM and FGcRBM models. An alternative criterion commonly used in the music generation field is the accuracy measure [22, 5, 13]. For each event te (see previous section), the visible units of the model are sampled with its context (and features for the FGcRBM) units set to Orch(te\u22121), ...Orch(te\u2212N) and Piano(te) from the test dataset. This sampled prediction Pred(te) is then compared to the ground-truth vector Orch(te) from the test dataset via the accuracy measure\n(19) Accuracy = TP (t)\nTP (t) + FP (t) + FN(t)\nwhere TP (t) (true positives) is the number of notes correctly predicted (note played in the prediction and ground-truth). FP (t) (false positive) is the number of notes predicted which are not in the original sequence (note played in the prediction, but not in ground-truth). FN(t) (false negative) is the number on unreported notes (note absent in prediction, but played in ground-truth). Instead of binary values, activation probabilities are used for the predicted samples in order to avoid sampling noise.\n3.6. Results. Four models are evaluated on the orchestral inference task. The first model is a random generation of the orchestral frames from a Bernoulli distribution of parameter 0.5. The second model predicts an orchestral frame at time t by repeating the frame at time t\u22121. Those two naive models constitute a first baseline against which we compare the cRBM and FGcRBM models.\nThe results are summed up in Table 1 on page 12. As expected, the random model has poor results. The repeat model performs significantly better. Indeed, it is relatively common that a note is sustained between two successive events. The cRBM largely outperforms those two models. However, the FGcRBM model has a lower score than the repeat model. We observed that the activation function of the visible units during the generative step was extremely noisy, which suggests that the training procedure mostly failed. We believe that this is due to the number of parameter, and especially the dimension of the feature units. Indeed, in [20], only ten different configuration of the feature units were possible. When training on an orchestral database, each different piano frame define a different configuration.\n3.7. Discussion : evaluating a generative model ? It is important to state that we do not consider this predictive measure as as reliable measure of the creative performance of a model. Indeed, predicting and creating are two fairly different things. Hence, the predictive evaluation framework we have built does not assess the generative ability of a model, but is rather used as a selection criterion among different models."}, {"heading": "4. Live Orchestral Piano (LOP)", "text": "We introduce in this section the Live Orchestral Piano (LOP) application, which is the first software able to provide a way to compose music with a full classical orchestra in real-time by simply playing on a MIDI piano. The goal of this framework is to rely on the knowledge learned by the model introduced in the previous sections in order to perform the projection from a piano melody to the orchestra.\n4.1. Workflow. The software is implemented on a client/server paradigm. This choice allows to separate the orchestral computation part from the interface and sound rendering engine. That way, multiple interfaces can easily be implemented. It should also be noted that separating the computing and rendering on different computers, can allow to use high-quality and CPU-intensive orchestral rendering plugins. This can allow a more realistic orchestral rendering with heavy amounts of computation performed while ensuring the real-time constraint on the overall system (preventing degradation of the computing part). The complete implementation workflow is presented in Figure 7.\nAs we can see, the user can input a melody (single notes or chords) through a MIDI keyboard, which is retrieved inside the Max/Msp interface. The interface transmits this symbolic information (as a variable-length vector of active notes) via OSC to the MATLAB server. The interface performs a real-time transcription of the piano score to the screen in parallel. The server uses this vector of events to produce an 88 vector of binary input note activations (as defined in the sub-section Data representation). This vector is then processed by using the orchestration algorithms presented in sub-section Model definition in order to obtain a projection of a specific symbolic piano melody to the full orchestra (an operation defined as projective orchestration). The resulting orchestration is then sent back to the client interface which performs both the real-time audio rendering and score transcription.\n4.2. Interface. The interface has been developed in Max/Msp, to facilitate both the score and audio rendering aspects in a real-time environment. The score rendering is handled by the Bach library environment. This interface provides a way to easily switch between different orchestration models, while controling other metaparameters of the sampling. For instance the cutoff probability gives a direct access to the density of the generated orchestration (in terms of number of played instruments). Indeed, a low cutoff probability implies that most activation of notes will\nbe taken into account in the playback, while a high cutoff will produce more sparse orchestration."}, {"heading": "5. Conclusion and future works", "text": "We have introduced a system for real-time projective orchestration of a midi piano input. In order to select the most adapted model, we have proposed an evaluation framework called orchestral inference which rely on an orchestral inference task. We have assessed the performance of the cRBM and FGcRBM, and observed the better performances of the cRBM model.\nThe general objective of building a generative model for time series is one of the most prominent topic for the machine learning field. Orchestral inference sets a slightly more specific framework where the generated time series is conditioned by an other observed time series (the piano score). Besides, being able to grasp the long range dependencies structuring music appears as a challenging and worthwhile task.\nThe high dimensionality of the data and their sparsity are a major obstacle for learning algorithms. A first remark is that a larger database would be required to train any model sufficiently complex to properly represent the underlying distribution of a projective orchestration. It is important to build a reference database of piano scores and their orchestration by acknowledge composers, with all instrument name indicated and velocity for the notes. Indeed, we believe that taking the notes\u2019 velocity into consideration is crucial, since many orchestral effects are justified by intensity variations in the original piano scores. Besides, the sparse representation of the data suggests that a more compact distributed representation might be found. Lowering the dimensionality of the data would greatly improve the efficiency of the learning procedure. For instance, methods close to the word-embedding techniques used in natural language processing might be useful [11].\nFinally, a better performance measure should be developed for the orchestral inference task. A solution could be to derive estimators for the likelihood of sequences under the proposed models. Recent work on methods such as Annealed Importance Sampling are promising."}, {"heading": "6. Acknowledgements", "text": "This work has been supported by the NVIDIA GPU Grant program. The authors would like to thank Denys Bouliane for kindly providing the orchestral database."}], "references": [{"title": "Learning deep architectures for ai, Foundations and trends in Machine Learning", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Representation learning: A review and new perspectives, Pattern Analysis and Machine Intelligence", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE Transactions on", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Pattern recognition and machine learning, springer", "author": ["Christopher M Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Multiobjective time series matching and classification", "author": ["Philippe Esling"], "venue": "Ph.D. thesis, IRCAM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Progress in pattern recognition, image analysis, computer vision, and applications: 17th iberoamerican congress, ciarp 2012, buenos aires, argentina, september", "author": ["Asja Fischer", "Christian Igel"], "venue": "proceedings, ch. An Introduction to Restricted Boltzmann Machines,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A practical guide to training restricted boltzmann machines, Momentum", "author": ["Geoffrey Hinton"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E Hinton"], "venue": "Neural computation", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Trait\u00e9 de l\u2019orchestration", "author": ["Charles Koechlin"], "venue": "E\u0301ditions Max Eschig,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1941}, {"title": "Polyphonic music modeling with random fields", "author": ["Victor Lavrenko", "Jeremy Pickens"], "venue": "Proceedings of the eleventh ACM international conference on Multimedia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Timbre as a structuring force in music", "author": ["Stephen McAdams"], "venue": "Proceedings of Meetings on Acoustics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "The timbre toolbox: Extracting audio descriptors from musical signals", "author": ["Geoffroy Peeters", "Bruno L Giordano", "Patrick Susini", "Nicolas Misdariis", "Stephen McAdams"], "venue": "The Journal of the Acoustical Society of America", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Perception of dyads of impulsive and sustained instrument sounds", "author": ["Damien Tardieu", "Stephen McAdams"], "venue": "Music Perception", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["Graham W Taylor", "Geoffrey E Hinton"], "venue": "Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Modeling human motion using binary latent variables, Advances in neural information processing", "author": ["Graham W Taylor", "Geoffrey E Hinton", "Sam T Roweis"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}], "referenceMentions": [{"referenceID": 10, "context": "Orchestration is the subtle art of writing musical pieces for the orchestra, by combining the properties of various instruments in order to achieve a particular sonic rendering [12, 18].", "startOffset": 177, "endOffset": 185}, {"referenceID": 12, "context": "Because it extensively relies on spectral characteristics, orchestration is often referred to as the art of manipulating instrumental timbres [15].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "Furthermore, it has been shown that some instrumental mixtures can not be characterized by a simple summation of their spectral components, but can lead to a unique emerging timbre, with phenomenon such as orchestral blend [19].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "From a mathematical point of view, it seems that no set of descriptors can exhaustively fit with the perceptual complexity of timbre [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "We refer to this operation of extending a piano draft to an orchestral score as projective orchestration [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "It rather implies harmonic enhancements and timbre manipulations to underline the already existing harmonic and rhythmic structure [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "A wide range of statistical inference models have been devised, among which deep learning recently appeared as a promising field in artificial intelligence and representation learning [2, 14].", "startOffset": 184, "endOffset": 191}, {"referenceID": 8, "context": "The building blocks of many models in this field is the Restricted Boltzmann Machine (RBM ) [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "We focus on a class of models called Conditional RBM [21], which implements a notion of context particularly adapted to represent both the temporal dependencies in the orchestral score and the influence of the piano score.", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "In the polyphonic music generation field, a predictive task is commonly used [22, 5, 13].", "startOffset": 77, "endOffset": 88}, {"referenceID": 11, "context": "In the polyphonic music generation field, a predictive task is commonly used [22, 5, 13].", "startOffset": 77, "endOffset": 88}, {"referenceID": 8, "context": "The RBM [9] is a graphical probabilistic model (Figure 2 on page 6) composed by a set of I visible units v = (v1, .", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Note that this model can easily be extended to continuous variables [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "The search for the minimum of this non-linear function can be tackled by using gradient descent [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": "Unfortunately the model-driven quantity is intractable because it involves a sum over all the possible configurations of the hidden (alternatively visible) units in order to compute the partition function [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 9, "context": "In order to alleviate this problem, a training algorithm called Contrastive Divergence (CD) [10] proposes to approximate the model driven term by", "startOffset": 92, "endOffset": 96}, {"referenceID": 0, "context": "It has been proved [1] that the samples obtained after an infinite number of iterations will be drawn from the joint distribution of the visible and hidden units of the model.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "A second approximations can be made by starting the chain from the same sample v used in the data-driven term [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 15, "context": "The conditional RBM (cRBM ) model [20] is an extension of the RBM in which dynamic biases are added to the biases of the visible (a) and hidden (b) units (see Figure 3 on page 7).", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "The Factored Gated cRBM (FGcRBM ) model [20] proposes to extend the cRBM model by adding a layer of feature units z which modulates the weights of the conditional architecture in a multiplicative way (see Figure 4 on page 8).", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "However, CD guarantees that the KullbackLeibler divergence between the model and data distribution is reduced after each iteration[10], and an acceptable approximation should be found after large number of CD steps.", "startOffset": 130, "endOffset": 134}, {"referenceID": 2, "context": "In order to evaluate the models, the data used to train and those used to assess the performances must be different [3].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Predictive symbolic music tasks are usually evaluated through a frame-level accuracy measure where each time frame of the piano-roll is considered as a separate data example [22, 5, 13].", "startOffset": 174, "endOffset": 185}, {"referenceID": 11, "context": "Predictive symbolic music tasks are usually evaluated through a frame-level accuracy measure where each time frame of the piano-roll is considered as a separate data example [22, 5, 13].", "startOffset": 174, "endOffset": 185}, {"referenceID": 2, "context": "Finally, to speed-up the convergence of the gradient descent, mini-batches are used to train the model [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "An alternative criterion commonly used in the music generation field is the accuracy measure [22, 5, 13].", "startOffset": 93, "endOffset": 104}, {"referenceID": 11, "context": "An alternative criterion commonly used in the music generation field is the accuracy measure [22, 5, 13].", "startOffset": 93, "endOffset": 104}, {"referenceID": 15, "context": "Indeed, in [20], only ten different configuration of the feature units were possible.", "startOffset": 11, "endOffset": 15}], "year": 2016, "abstractText": "This paper introduces the first system for performing automatic orchestration based on a real-time piano input. We believe that it is possible to learn the underlying regularities existing between piano scores and their orchestrations by notorious composers, in order to automatically perform this task on novel piano inputs. To that end, we investigate a class of statistical inference models called conditional Restricted Boltzmann Machine (cRBM ). We introduce a specific evaluation framework for orchestral generation based on a prediction task in order to assess the quality of different models. As prediction and creation are two widely different endeavours, we discuss the potential biases in evaluating temporal generative models through prediction tasks and their impact on a creative system. Finally, we introduce an implementation of the proposed model called Live Orchestral Piano (LOP), which allows to perform real-time projective orchestration of a MIDI keyboard input.", "creator": "LaTeX with hyperref package"}}}