{"id": "1609.01977", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Doubly Stochastic Neighbor Embedding on Spheres", "abstract": "recently, stochastic neighbor embedding ( sne ) modelling methods have widely been applied jointly in data visualization. these methods minimize the divergence relation between the pairwise similarities of high - and low - dimensional data. despite their popularity, the current sne methods experience the \" crowding problem \" when the data include highly highly imbalanced similarities. this implies that the data points with higher total similarity tend to get crowded around the display - center. to solve this problem, we normalize the similarity matrix bound to be doubly stochastic such that all the data points have equal total similarities. a fast normalization method is proposed. furthermore, we show empirically and theoretically that the doubly stochasticity constraint itself often leads to approximately approximately spherical embeddings. this suggests replacing a flat space with spheres as the embedding space. the spherical embedding eliminates the discrepancy between the center and the periphery in visualization and thus resolves the \" crowding problem \". we compared the proposed method with the state - physics of - the - art sne method on three objective real - world datasets. the results indicate that our method is more favorable in terms of visualization quality.", "histories": [["v1", "Wed, 7 Sep 2016 13:42:26 GMT  (2325kb,D)", "http://arxiv.org/abs/1609.01977v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yao lu", "zhirong yang", "jukka corander"], "accepted": false, "id": "1609.01977"}, "pdf": {"name": "1609.01977.pdf", "metadata": {"source": "CRF", "title": "Doubly Stochastic Neighbor Embedding on Spheres", "authors": ["Yao Lu", "Zhirong Yang", "Jukka Corander"], "emails": ["(yaolubrain@gmail.com),", "(zhirong.yang@helsinki.fi),"], "sections": [{"heading": null, "text": "sualization. These methods minimize the divergence between the pairwise similarities of high- and low-dimensional data. Despite their popularity, the current SNE methods experience the \u201ccrowding problem\u201d when the data include highly imbalanced similarities. This implies that the data points with higher total similarity tend to get crowded around the display center. To solve this problem, we normalize the similarity matrix to be doubly stochastic such that all the data points have equal total similarities. A fast normalization method is proposed. Furthermore, we show empirically and theoretically that the doubly stochasticity constraint often leads to approximately spherical embeddings. This suggests replacing a flat space with spheres as the embedding space. The spherical embedding eliminates the discrepancy between the center and the periphery in visualization and thus resolves the \u201ccrowding problem\u201d. We compared the proposed method with the state-of-the-art SNE method on three real-world datasets. The results indicate that our method is more favorable in terms of visualization quality."}, {"heading": "1 Introduction", "text": "Information visualization by dimensionality reduction facilitates a viewer to quickly digest information in massive data. It is therefore increasingly applied as a critical component in scientific research, digital libraries, data mining, financial data analysis, market studies, manufacturing production control and drug discovery, etc. Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8]. See [20] for a survey.\nRecently, a Stochastic Neighbor Embedding (SNE) method, t-SNE [19], and its variants (e.g. [25, 14, 15]) have achieved remarkable progress in data visualization, especially for displaying clusters in data. The t-SNE algorithm takes the pairwise similarities between data points in the high dimensional space and tries to preserve the similarities in the low-dimensional space by minimizing the Kullback-Leibler divergence between the input and output similarity matrices.\nThe input similarity to t-SNE is a symmetric and nonnegative matrix, i.e., the affinity of an undirected weighted graph. If the node degrees are different, t-SNE tends to place the high-degree nodes in the center and the low-degree ones in the periphery, regardless of the intrinsic similarities between the nodes. If the degrees vary vastly, t-SNE will suffer \u201ccrowding-in-the-center\u201d problem for large graphs.\nWe propose two techniques to overcome the above-mentioned drawback. First, we impose a doubly stochasticity constraint on the input similarity matrix. Two-way normalization has been shown to improve spectral clustering [26] and here we verify that it is also beneficial for data visualization. Moreover, if the neighborhood graph is asymmetric, for example, k-Nearest-Neighbors (kNN) or entropy affinities [19, 21], we provide an efficient method for converting it to a doubly stochastic matrix.\n*Equal contribution. Emails: Yao Lu (yaolubrain@gmail.com), Zhirong Yang (zhirong.yang@helsinki.fi), Jukka Corander\n(jukka.corander@helsinki.fi)\nar X\niv :1\n60 9.\n01 97\n7v 1\n[ cs\n.L G\n] 7\nS ep\n2 01\nSecond, we observe that the data points are often distributed approximately around a sphere if the input similarity matrix is doubly stochastic. We provide theoretical analysis of the observation. The observation and its analysis suggest we replace the two-dimensional Euclidean embedding space with spheres in the three-dimensional space. Since there is no global center or periphery on the sphere geometry, the visualization is naturally free of \u201ccrowding-in-the-center\u201d problem. Moreover, we present an efficient projection step for adapting a SNE method with the spherical constraint.\nWe tested the proposed method on three real-world datasets and compared it with 2D and 3D t-SNE. The new method is superior to t-SNE in resolving the crowding problem and in preserving intrinsic similarities.\nIn the next section we briefly review SNE methods. We then discuss doubly stochastic similarity matrix and spherical embedding in Sections 3 and 4, respectively. The related work is reviewed in Section 5. We present the experimental results in Section 6 and conclude the paper in Section 7."}, {"heading": "2 Stochastic Neighbor Embedding", "text": "Stochastic Neighbor Embedding (SNE) [6] is a family of nonlinear dimensionality reduction methods. Given a set of multivariate data points {x1, x2, . . . , xn}, where xi \u2208 RD, their neighborhood is encoded in a square nonnegative matrix P , where Pij is the probability that xj is a neighbor of xi. SNE finds a mapping xi 7\u2192 yi \u2208 Rd for i = 1, . . . , n such that the neighborhoods are approximately preserved in the mapped space. Usually d = 2 or 3, and d < D. If the neighborhood in the mapped space is encoded in Q \u2208 Rn\u00d7n such that Qij is the probability that yj is a neighbor of yi, the SNE task is to minimize the Kullback-Leibler divergence DKL(P ||Q) over Y = [y1, y2, . . . , yn]T .\nSymmetric Stochastic Neighbor Embedding (s-SNE) [19] is a specialized SNE method. Given input similarity pij \u2265 0, s-SNE minimizes Kullback-Leibler divergence between the matrix-wise normalized similarities Pij = pij/ \u2211 ab pab and Qij = qij/ \u2211 ab qab. The output similarity qij is typically chosen to be proportional\nto a Gaussian distribution so that qij = exp ( \u2212\u2016yi \u2212 yj\u20162 ) , or proportional to a Cauchy distribution so that qij = (1 + \u2016yi \u2212 yj\u20162)\u22121. The Cauchy s-SNE method is also called t-Distributed Stochastic Neighbor Embedding (t-SNE) [19]. The optimization of s-SNE can be implemented with the gradients for Gaussian case: \u2202J /\u2202yi = 4 \u2211 j(Pij \u2212Qij)(yi \u2212 yj) and for Cauchy case \u2202J /\u2202yi = 4 \u2211 j(Pij \u2212Qij)(yi \u2212 yj)qij . Here\n4 \u2211 j Pij(yi\u2212yj) or 4 \u2211 j Pij(yi\u2212yj)qij can be interpreted as the attractive force for yi, while \u22124 \u2211\nj Qij(yi\u2212yj) or \u22124 \u2211 j Qij(yi \u2212 yj)qij as the repulsive force."}, {"heading": "3 Doubly Stochastic Similarity Matrix", "text": "The input to s-SNE, P , is a nonnegative and symmetric matrix and it can be treated as the affinity matrix of an undirected weighted graph. If the degree (i.e. row sum or column sum of P ) distribution of nodes is highly non-uniform, then the high-degree nodes will usually receive and emit more attractive force than the average nodes during the iterative learning. As a result, these nodes often glue together and form the center of display. On the other hand, the low-degree nodes tend to be placed in the periphery due to less attraction. This behavior is often undesired in visualization because it only reveals the data centrality but hinders the discovery of other useful patterns.\nTo overcome the above drawback, we can normalize the graph affinity such that the nodes have the same degree. For undirected graphs, this can be implemented by replacing the unitary matrix-wise sum constraint\u2211\nij Pij = 1 in s-SNE with the doubly stochasticity constraint, i.e. \u2211 i Pij = 1 and \u2211\nj Pij = 1. Given a non-normalized matrix, we can apply Sinkhorn-Knopp [13] or Zass-Shashua method [26] to project it to the closest doubly stochastic matrix P . In this work we use the former because it can maintain the sparsity of in the similarity matrix, which is often needed for large-scale tasks. Given an unnormalized similarity matrix S, the Sinkhorn-Knopp method initializes P = S and iterates the following update rules until P is converged:\nFor all i, j\nPij \u2190 Pij/ \u2211\nu\nPiu, (1)\nPij \u2190 Pij/ \u2211\nv\nPvj . (2)\nAlternatively, the neighborhood information in high-dimensional space can be encoded in an asymmetric matrix B \u2265 0 with n rows, for example, the kNN graph or the entropy affinities [19, 21]. B can also be a non-square dyadic data such as document-term or author-paper co-occurrence matrix. In these cases, we can apply the following steps to construct a doubly stochastic matrix: suppose \u2211 k Bik > 0 for all i, we calculate\nAik \u2190 Bik/ \u2211\nu\nBiu, for all i, k, (3)\nPij \u2190 \u2211\nk\nAikAjk\u2211 v Avk , for all i, j. (4)\nIt is easy to verify that by this construction P is symmetric and doubly stochastic. Eqs. 3 and 4 are performed only once and thus much more efficient than Sinkhorn-Knopp method which needs iterative steps. Here the matrix Aik can be treated as the random walk probability from the ith row index to the kth column index and Pij is interpreted as the two-step random walk probability between two row indices i and j via any column index k (with uniform prior over row indices). Besides computational considerations, the choice of which projection to use is also data-dependent."}, {"heading": "4 Spherical Embedding of Doubly Stochastic Similarity Matrices", "text": "When the input similarity matrix is doubly stochastic, we find that s-SNE often embeds the data points around a sphere in the low-dimensional space. The phenomenon is illustrated in Figure 1, where we generated a 2000\u00d72000 similarity matrix with uniform distribution and visualize it by t-SNE. We can see from the left subfigure that the embedding is close to a ball. In contrast, if the matrix is doubly stochastically normalized (by Sinkhorn-Knopp method), the resulting embedded points approximately lie around a sphere. The same phenomenon also holds for 3D visualizations.\nWe provide a theoretical analysis of this phenomenon. If P is doubly stochastic, then Q is approximately doubly stochastic because P \u2248 Q. That is, \u2211 j qij is approximately the same for all i. In this case,\u2211\nj \u2016yi \u2212 yj\u20162 also becomes approximately the same for all i (bounded by the same constants). When\u2211 j \u2016yi \u2212 yj\u20162 is exactly the same for all i, the embedded points must be on a sphere. The above analysis is formalized in the following propositions. The proofs are left in the Appendix.\nProposition 4.1. If \u2211 j qij = c for i = 1, . . . , n and c > 0, then L \u2264 \u2211\nj \u2016yi \u2212 yj\u20162 \u2264 U , where 1) for qij = exp(\u2212\u2016yi \u2212 yj\u20162), L = n ln n\nc and U = n ln n c\u2212 nb , with b = a + (1 \u2212 a)m \u2212 ma, m =\nminj exp(\u2212\u2016yi \u2212 yj\u20162) and a = ln[ln(1/m)/(1\u2212m)]\nln(1/m) ;\n2) for qij = (1 + \u2016yi \u2212 yj\u20162)\u22121, L = n2\nc \u2212 n and U = n\n2\nc \u2212 n + n(b1/2 \u2212 1)2 with b = 1 + maxj \u2016yi \u2212 yj\u20162.\nProposition 4.2. If \u2211 j \u2016yi \u2212 yj\u20162 = c for i = 1, . . . , n, c > 0 and \u2211\ni yi = 0, then \u2016y1\u20162 = \u2016y2\u20162 = \u00b7 \u00b7 \u00b7 = \u2016yn\u20162.\nSince the embedding is often nearly spherical for doubly stochastic similarity matrices, it is more suitable to replace the 2D Euclidean embedding space with spheres in 3D space. The resulting layout can be encoded with n\u00d7 2 + 1 numbers (two angles for each data point plus the common radius). Therefore the embedding is still intrinsically two-dimensional.\nThe spherical geometry itself brings other benefits for visualization. First, the embedding in the Euclidean space has a global center in the middle, while on spheres there is no such global center. Therefore a spherical visualization is free of the \u201ccrowding-in-the-center\u201d problem. Every point on the sphere can be a local center, which provides fish-eye views for navigation and for examining patterns beyond centrality. Second, the attractive and repulsive forces can be transmitted in a cyclic manner, which helps in discovering macro patterns such inter-cluster similarities.\nWe thus formulate our learning objective as follows:\nmin Y \u2208S J (Y ) = D(P ||Q), (5)\nwhere J (Y ) is a SNE objective function with P doubly stochastic and\nS = { Y \u2223\u2223\u2223 Y \u2208 Rn\u00d73; \u2016y1\u2016 = \u2016y2\u2016 = \u00b7 \u00b7 \u00b7 = \u2016yn\u2016; \u2211\ni\nyi = 0 } . (6)\nWe call the new method DOubly Stochastic Neighbor Embedding on Spheres (DOSNES). In this work we mainly use t-SNE on S, while it is straightforward to combine the sphere constraint with other SNE objectives. Note that S include all centered spheres in three-dimensional space, not only the unit sphere.\nWe employ a projection step after each SNE update step to enforce the sphere constraint. The DOSNES algorithm steps are summarized as follows:\n1. Normalize P to be doubly stochastic.\n2. Repeat until convergence\n(a) Y\u0303 \u2190OneStepUpdateSNE(P , Y )\n(b) Y \u2190 arg minZ\u2208S \u2016Z \u2212 Y\u0303 \u2016\nThe projection step is performed by implicitly switching Y\u0303 = [y\u03031, . . . , y\u0303n]T to the spherical coordinate system and taking the mean radius, which is implemented as:\nFor i = 1, . . . , n\ny\u0303i \u2190 y\u0303i \u2212 1 n \u2211 j y\u0303j , (7)\nyi \u2190 y\u0303i \u2016y\u0303i\u2016 \u00b7  1 n \u2211 j \u2016y\u0303j\u2016  . (8)"}, {"heading": "5 Related Work", "text": "Normalizing a matrix to be doubly stochastic has been used to improve cluster analysis. Zass and Shashua proposed to improve spectral clustering by replacing the original similarity matrix by its closest doubly stochastic similarities under L1 or Frobenius norm [26]. Wang et al. generalized the projection to the family of Bregman divergences [22]. To our knowledge, DOSNES is the first method that applies doubly stochastic matrices to improve data visualization.\nSpherical visualization has appeared earlier in the visualization literature. For example, Spherical Multidimensional Scaling (MDS) replaces Euclidean embedding space in the classical MDS with the unit sphere [3]. Similar replacement was used in [24, 5, 9]. [9] also changes the output similarities with the Exit distribution, although this makes the objective function non-smooth.\nOur method has a critical difference from the above approaches: the DOSNES embedding space is not restricted to the unit sphere. This is advantageous in two aspects: 1) our objective function is smooth and thus easy to optimize with gradients; 2) DOSNES does not require an explicit scale variable for Y or kernel bandwidth variable for Q, which is difficult to optimize. The sphere radius in DOSNES is implicitly adapted during optimization.\nDOSNES does not require that the input high-dimensional data must be on spheres. The sphere constraint in DOSNES is on the low-dimensional output space. It roots in the use of doubly stochastic similarity matrix and aims at solving the crowding problem in SNE, which is quite different from the methods that embed high-dimensional spherical data to low-dimensional spherical one [9, 23].\nCompared with hyperbolic visualizations (see e.g. [7, 11]), the DOSNES display and navigation are more natural for most viewers without comprehensive knowledge of the transformation models such as Klein or Poincar\u00e9."}, {"heading": "6 Experiments", "text": "We developed a web-based software for displaying and navigating the DOSNES results. In the paper we present the 2D projected views of the spheres. Code and demo can be found in\nhttps://github.com/yaolubrain/DOSNES\nWe compare our proposed method DOSNES with two and three-dimensional t-SNE in Euclidean embedding space [19] to verify the effectiveness of using doubly stochastic similarities and the sphere constraint. We used a popular t-SNE implementation1 and its default settings.\nThe compared methods were tested on three real-world datasets from different domains:\n\u2022 NIPS2: the proceedings of NIPS (1987-2015) which contains 5,993 papers and their associated 6,621 authors. We used the largest connected component in the co-author graph with 5,300 papers and 5,422 authors. The (unnormalized) similarity matrix is from the co-author graph, i.e. BBT where B is the author-paper co-occurrence matrix.\n\u2022 WorldTrade3: trade miscellaneous manufactures of metal among 80 countries in 1994. Each edge represents the total trade amount (imports and exports) between two countries.\n\u2022 MIREX4: the dataset is from the the Third Music Information Retrieval Evaluation eXchange (MIREX 2007). We used the version from the collection [2]. It is a similarity graph of 3090 songs. The songs are evenly divided among 10 classes that roughly correspond to different music genres. The weighted edges are human judgment on how similar two songs are.\nThe visualization results of the above datasets are in Figure 2, 3 and 4.\n1https://lvdmaaten.github.io/tsne/ 2https://papers.nips.cc/ 3http://vlado.fmf.uni-lj.si/pub/networks/data/esna/metalWT.htm 4http://www.music-ir.org/mirex/wiki/2007\nIn the NIPS co-author graph, the node degrees of the graph are highly uneven. Many authors have only one paper while the the most productive author has 93 papers. In Figure 2 (a) and (b), we can see both 2D and 3D t-SNE caused the most productive NIPS authors crowded in the center. This is undesirable because these authors actually do not often co-author NIPS papers. For example, Hinton_G has no co-authored paper with Sch\u00f6lkopf_B but they are very close in the t-SNE layout. In Figure 2 (c) and (d), we can see DOSNES resolves neatly the crowding problem, by normalizing the similarity matrix with the random walk method in Section 3 and visualizing the authors with spherical layout. The productive NIPS authors are now more evenly distributed. For example, Hinton_G becomes more distant to Sch\u00f6lkopf_B. Meanwhile, retrieval around the most established authors reveals accurate co-authorship. For example, Revow_M, Nair_V and Brown_A are close to Hinton_G because all their NIPS papers are co-authored with Hinton_G.\nIn the WorldTrade graph, some countries such as United States and Germany have more much total trade amount than many others. In Figure 3 (a) and (b), we can see both 2D and 3D t-SNE caused these countries crowded in the center. In contrast, DOSNES places the countries more evenly. In Figure 3 (c) and (d), we can see on the sphere many meaningful clusters (such as Europe and Asia) which well match the geography information.\nIn Figure 4 (a) and (b), t-SNE caused over 90 percent of songs crowded in the center. In contrast, DOSNES performs much better in terms of separating the song genres and their subgroups, as in Figure 4 (c) and (d)."}, {"heading": "7 Conclusion", "text": "We have presented a new visualization method for high-dimensional and graph data. The proposed DOSNES method is based on the Stochastic Neighbor Embedding principle but with two key improvements: we normalize the input similarity matrix to be doubly stochastic and replace the 2D Euclidean embedding space with spheres in 3D space. Empirical results show that our method significantly outperforms the state-of-the-art approach t-SNE in terms of resolving the crowding problem and preserving intrinsic similarities.\nIn the future, we aim at performing a more thorough theoretical study on the connection between doubly stochastic similarity matrix and spherical embedding, especially when they are coupled with various SNE objectives. There could be many possibilities to improve the user interface of spherical visualization. For example, mouse dragging could be replaced by trackball rolling for sphere rotation. Spherical screens would further facilitate the data navigation and analysis. It would be also straightforward to develop visualizations viewed from inside the sphere."}, {"heading": "8 Appendix", "text": "Proposition 8.1. If \u2211\nj\nexp(\u2212\u2016yi \u2212 yj\u20162) = c, (9)\nfor i = 1...n and some c > 0, where yi \u2208 Rd, then\nn ln n c \u2264 \u2211\nj\n\u2016yi \u2212 yj\u20162 \u2264 n ln n\nc\u2212 nb , (10)\nwhere b = a + (1\u2212 a)m\u2212ma, m = minj exp(\u2212\u2016yi \u2212 yj\u20162) and a = ln[ln(1/m)/(1\u2212m)]ln(1/m) .\nProof. Let\nA = 1 n \u2211 j exp(\u2212\u2016yi \u2212 yj\u20162) = c n (arithmetic mean), (11)\nG = [exp(\u2212 \u2211\nj\n\u2016yi \u2212 yj\u20162)]1/n (geometric mean). (12)\n(13)\nFor lower bound, we have\nG \u2264 A, (14) [exp(\u2212 \u2211\nj\n\u2016yi \u2212 yj\u20162)]1/n \u2264 c\nn , (15)\nexp(\u2212 \u2211\nj\n\u2016yi \u2212 yj\u20162) \u2264 ( c\nn )n, (16)\n\u2212 \u2211\nj\n\u2016yi \u2212 yj\u20162 \u2264 n ln c\nn , (17)\nn ln n c \u2264 \u2211\nj\n\u2016yi \u2212 yj\u20162. (18)\n(19)\nFor upper bound, by Tung Theorem [18], we have\nA\u2212G \u2264 am + (1\u2212 a)M \u2212maM1\u2212a, (20)\nwhere m = minj exp(\u2212\u2016yi \u2212 yj\u20162), M = maxj exp(\u2212\u2016yi \u2212 yj\u20162) and\na = ln[M/(M \u2212m) ln(M/m)]ln(M/m) . (21)\nSince maxj exp(\u2212\u2016yi \u2212 yj\u20162) = 1, we have\nA\u2212G \u2264 am + (1\u2212 a)\u2212ma (22)\nand\na = ln[1/(1\u2212m) ln(1/m)]ln(1/m) . (23)\nLet b = am + (1\u2212 a)\u2212ma, we have\nG \u2265 A\u2212 b, (24) [exp(\u2212 \u2211\nj\n\u2016yi \u2212 yj\u20162)]1/n \u2265 c\nn \u2212 b, (25)\nn ln n c\u2212 nb\n\u2265 \u2211\nj\n\u2016yi \u2212 yj\u20162. (26)\nProposition 8.2. If \u2211 j (1 + \u2016yi \u2212 yj\u20162)\u22121 = c, (27)\nfor i = 1...n and some c > 0, where yi \u2208 Rd, then\nn2\nc \u2212 n \u2264 \u2211 j \u2016yi \u2212 yj\u20162 \u2264 n2 c \u2212 n + n(b1/2 \u2212 1)2, (28)\nwhere b = 1 + maxj \u2016yi \u2212 yj\u20162.\nProof. Let\nA = 1 n \u2211 j (1 + \u2016yi \u2212 yj\u20162) (arithmetic mean), (29) H = n\u2211 j(1 + \u2016yi \u2212 yj\u20162)\u22121 (harmonic mean). (30)\n(31)\nFor upper bound, we have\nA \u2265 H = n c\n, (32) 1 n \u2211 j \u2016yi \u2212 yj\u20162 + 1 \u2265 n c , (33)\n\u2211 j \u2016yi \u2212 yj\u20162 \u2265 n2 c \u2212 n. (34)\n(35)\nFor lower bound, due to Meyer Theorem [10], let b = 1 + maxj \u2016yi \u2212 yj\u20162, then we have\nA\u2212H \u2264 (b1/2 \u2212 1)2, (36) 1 n \u2211 j (1 + \u2016yi \u2212 yj\u20162) \u2264 n c + (b1/2 \u2212 1)2, (37)\n\u2211 j \u2016yi \u2212 yj\u20162 \u2264 n2 c \u2212 n + n(b1/2 \u2212 1)2. (38)\n(39)\nProposition 8.3. If \u2211 j \u2016yi \u2212 yj\u20162 = c, (40)\nfor i = 1...n and some c > 0, where yi \u2208 Rd and \u2211 i yi = 0, then\n\u2016y1\u20162 = \u2016y2\u20162 = ... = \u2016yn\u20162. (41)\nProof. \u2211 j\n\u2016yi \u2212 yj\u20162 = c, (42)\u2211 j (\u2016yi\u20162 + \u2016yj\u20162 \u2212 2yTi yj) = c, (43)\nn\u2016yi\u20162 + \u2211\nj\n\u2016yj\u20162 \u2212 2yTi \u2211\nj\nyj = c, (44)\nn\u2016yi\u20162 + \u2211\nj\n\u2016yj\u20162 = c, (45) \u2016yi\u20162 = (c\u2212 \u2211\nj\n\u2016yj\u20162)/n. (46)\nSince \u2211\nj \u2016yj\u20162 is independent of i, we have\n\u2016y1\u20162 = \u2016y2\u20162 = ... = \u2016yn\u20162. (47)"}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["M. Belkin", "P. Niyogi"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Similarity-based classification: Concepts and algorithms", "author": ["Y. Chen", "E. Garcia", "M. Gupta", "A. Rahimi", "L. Cazzanti"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Multidimensional scaling on a sphere", "author": ["T. Cox", "M. Cox"], "venue": "Communications in Statistics-Theory and Methods,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Curvilinear component analysis: a self-organizing neural network for nonlinear mapping of data sets", "author": ["P. Demartines", "J. H\u00e9rault"], "venue": "IEEE TNN,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "sLLE: Spherical locally linear embedding with applications to tomography", "author": ["Y. Fang", "M. Sun", "S. Vishwanathan", "K. Ramani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Stochastic neighbor embedding", "author": ["G. Hinton", "S. Roweis"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "A focus+context technique based on hyperbolic geometry for visualizing large hierarchies", "author": ["J. Lamping", "R. Rao", "P. Pirolli"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N. Lawrence"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Spherical stochastic neighbor embedding of hyperspectral data", "author": ["D. Lunga", "O. Ersoy"], "venue": "IEEE TGRS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Some inequalities for elementary mean values", "author": ["B. Meyer"], "venue": "Mathematics of Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1984}, {"title": "Visualizing the structure of the world wide web in 3d hyperbolic space", "author": ["T. Munzner", "P. Burchard"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Concerning nonnegative matrices and doubly stochastic matrices", "author": ["R. Sinkhorn", "P. Knopp"], "venue": "Pacific Journal of Mathematics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1967}, {"title": "Space-time local embeddings", "author": ["K. Sun", "J. Wang", "A. Kalousis", "S. Marchand-Maillet"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Visualizing large-scale and high-dimensional data", "author": ["J. Tang", "J. Liu", "M. Zhang", "Q. Mei"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. de Silva", "J. Langford"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Multidimensional scaling: I. theory and method", "author": ["W. Torgerson"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1952}, {"title": "On lower and upper bounds of the difference between the arithmetic and the geometric mean", "author": ["S. Tung"], "venue": "Mathematics of Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1975}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Dimensionality reduction: A comparative review", "author": ["L. van der Maaten", "E. Postma", "J. van den Herik"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Entropic affinities: Properties and efficient numerical computation", "author": ["M. Vladymyrov", "M. Carreira-Perpi\u00f1\u00e1n"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Improving clustering by learning a bi-stochastic data similarity matrix", "author": ["F. Wang", "P. Li", "A. K\u00f6nig", "M. Wan"], "venue": "Knowledge and Information Kystems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "VMF-SNE: Embedding for spherical data", "author": ["M. Wang", "D. Wang"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Spherical embeddings for non-euclidean dissimilarities", "author": ["R. Wilson", "E. Hancock", "E. Pekalska", "R. Duin"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Optimization equivalence of divergences improves neighbor embedding", "author": ["Z. Yang", "J. Peltonen", "S. Kaski"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Doubly stochastic normalization for spectral clustering", "author": ["R. Zass", "A. Shashua"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8].", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8].", "startOffset": 218, "endOffset": 222}, {"referenceID": 11, "context": "Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8].", "startOffset": 249, "endOffset": 253}, {"referenceID": 3, "context": "Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8].", "startOffset": 286, "endOffset": 289}, {"referenceID": 0, "context": "Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8].", "startOffset": 311, "endOffset": 314}, {"referenceID": 7, "context": "Numerous dimensionality reduction methods have been introduced, from linear methods based on eigendecomposition, such as Principal Component Analysis, to nonlinear methods such as Multidimensional Scaling [17], Isomap [16], Locally Linear Embedding [12], Curvilinear Component Analysis [4], Laplacian Eigenmaps [1] and Gaussian Process Latent Variable Models [8].", "startOffset": 359, "endOffset": 362}, {"referenceID": 19, "context": "See [20] for a survey.", "startOffset": 4, "endOffset": 8}, {"referenceID": 18, "context": "Recently, a Stochastic Neighbor Embedding (SNE) method, t-SNE [19], and its variants (e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 24, "context": "[25, 14, 15]) have achieved remarkable progress in data visualization, especially for displaying clusters in data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 13, "context": "[25, 14, 15]) have achieved remarkable progress in data visualization, especially for displaying clusters in data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 14, "context": "[25, 14, 15]) have achieved remarkable progress in data visualization, especially for displaying clusters in data.", "startOffset": 0, "endOffset": 12}, {"referenceID": 25, "context": "Two-way normalization has been shown to improve spectral clustering [26] and here we verify that it is also beneficial for data visualization.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "Moreover, if the neighborhood graph is asymmetric, for example, k-Nearest-Neighbors (kNN) or entropy affinities [19, 21], we provide an efficient method for converting it to a doubly stochastic matrix.", "startOffset": 112, "endOffset": 120}, {"referenceID": 20, "context": "Moreover, if the neighborhood graph is asymmetric, for example, k-Nearest-Neighbors (kNN) or entropy affinities [19, 21], we provide an efficient method for converting it to a doubly stochastic matrix.", "startOffset": 112, "endOffset": 120}, {"referenceID": 5, "context": "2 Stochastic Neighbor Embedding Stochastic Neighbor Embedding (SNE) [6] is a family of nonlinear dimensionality reduction methods.", "startOffset": 68, "endOffset": 71}, {"referenceID": 18, "context": "Symmetric Stochastic Neighbor Embedding (s-SNE) [19] is a specialized SNE method.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "The Cauchy s-SNE method is also called t-Distributed Stochastic Neighbor Embedding (t-SNE) [19].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Given a non-normalized matrix, we can apply Sinkhorn-Knopp [13] or Zass-Shashua method [26] to project it to the closest doubly stochastic matrix P .", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "Given a non-normalized matrix, we can apply Sinkhorn-Knopp [13] or Zass-Shashua method [26] to project it to the closest doubly stochastic matrix P .", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "Alternatively, the neighborhood information in high-dimensional space can be encoded in an asymmetric matrix B \u2265 0 with n rows, for example, the kNN graph or the entropy affinities [19, 21].", "startOffset": 181, "endOffset": 189}, {"referenceID": 20, "context": "Alternatively, the neighborhood information in high-dimensional space can be encoded in an asymmetric matrix B \u2265 0 with n rows, for example, the kNN graph or the entropy affinities [19, 21].", "startOffset": 181, "endOffset": 189}, {"referenceID": 25, "context": "Zass and Shashua proposed to improve spectral clustering by replacing the original similarity matrix by its closest doubly stochastic similarities under L1 or Frobenius norm [26].", "startOffset": 174, "endOffset": 178}, {"referenceID": 21, "context": "generalized the projection to the family of Bregman divergences [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "For example, Spherical Multidimensional Scaling (MDS) replaces Euclidean embedding space in the classical MDS with the unit sphere [3].", "startOffset": 131, "endOffset": 134}, {"referenceID": 23, "context": "Similar replacement was used in [24, 5, 9].", "startOffset": 32, "endOffset": 42}, {"referenceID": 4, "context": "Similar replacement was used in [24, 5, 9].", "startOffset": 32, "endOffset": 42}, {"referenceID": 8, "context": "Similar replacement was used in [24, 5, 9].", "startOffset": 32, "endOffset": 42}, {"referenceID": 8, "context": "[9] also changes the output similarities with the Exit distribution, although this makes the objective function non-smooth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "It roots in the use of doubly stochastic similarity matrix and aims at solving the crowding problem in SNE, which is quite different from the methods that embed high-dimensional spherical data to low-dimensional spherical one [9, 23].", "startOffset": 226, "endOffset": 233}, {"referenceID": 22, "context": "It roots in the use of doubly stochastic similarity matrix and aims at solving the crowding problem in SNE, which is quite different from the methods that embed high-dimensional spherical data to low-dimensional spherical one [9, 23].", "startOffset": 226, "endOffset": 233}, {"referenceID": 6, "context": "[7, 11]), the DOSNES display and navigation are more natural for most viewers without comprehensive knowledge of the transformation models such as Klein or Poincar\u00e9.", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[7, 11]), the DOSNES display and navigation are more natural for most viewers without comprehensive knowledge of the transformation models such as Klein or Poincar\u00e9.", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "com/yaolubrain/DOSNES We compare our proposed method DOSNES with two and three-dimensional t-SNE in Euclidean embedding space [19] to verify the effectiveness of using doubly stochastic similarities and the sphere constraint.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "We used the version from the collection [2].", "startOffset": 40, "endOffset": 43}, {"referenceID": 17, "context": "(19) For upper bound, by Tung Theorem [18], we have A\u2212G \u2264 am + (1\u2212 a)M \u2212maM1\u2212a, (20)", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "For lower bound, due to Meyer Theorem [10], let b = 1 + maxj \u2016yi \u2212 yj\u2016, then we have A\u2212H \u2264 (b1/2 \u2212 1)2, (36) 1 n \u2211", "startOffset": 38, "endOffset": 42}], "year": 2016, "abstractText": "Recently, Stochastic Neighbor Embedding (SNE) methods have widely been applied in data visualization. These methods minimize the divergence between the pairwise similarities of highand low-dimensional data. Despite their popularity, the current SNE methods experience the \u201ccrowding problem\u201d when the data include highly imbalanced similarities. This implies that the data points with higher total similarity tend to get crowded around the display center. To solve this problem, we normalize the similarity matrix to be doubly stochastic such that all the data points have equal total similarities. A fast normalization method is proposed. Furthermore, we show empirically and theoretically that the doubly stochasticity constraint often leads to approximately spherical embeddings. This suggests replacing a flat space with spheres as the embedding space. The spherical embedding eliminates the discrepancy between the center and the periphery in visualization and thus resolves the \u201ccrowding problem\u201d. We compared the proposed method with the state-of-the-art SNE method on three real-world datasets. The results indicate that our method is more favorable in terms of visualization quality.", "creator": "LaTeX with hyperref package"}}}