{"id": "1609.01984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Human Body Orientation Estimation using Convolutional Neural Network", "abstract": "personal robots are expected to interact with the user by recognizing the user's face. however, in most of the service robot applications, the user needs to move normally himself / herself to allow the robot to see him / her face to face. to overcome such limitations, a method for estimating autonomous human body orientation is required. previous studies used various components such as feature extractors and classification models required to classify the orientation which resulted in low performance. for a more robust and accurate approach, we propose the light weight driven convolutional neural networks, an end to end system, for estimating human, body orientation. our body orientation estimation model achieved 81. 78 58 % and 94 % accuracy with the google benchmark dataset and our own dataset respectively. the proposed method can be confidently used in a wide range of service robot applications which depend on the ability to estimate human physical body orientation. to show its usefulness in service robot terrain applications, we designed a simple robot application which allows the robot to move towards the user's frontal plane. with this, we demonstrated an improved face detection rate.", "histories": [["v1", "Wed, 7 Sep 2016 13:53:26 GMT  (3496kb)", "http://arxiv.org/abs/1609.01984v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["jinyoung choi", "beom-jin lee", "byoung-tak zhang"], "accepted": false, "id": "1609.01984"}, "pdf": {"name": "1609.01984.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": " Abstract\u2014 Personal robots are expected to interact with the user by recognizing the user\u2019s face. However, in most of the service robot applications, the user needs to move himself/herself to allow the robot to see him/her face to face. To overcome such limitations, a method for estimating human body orientation is required. Previous studies used various components such as feature extractors and classification models to classify the orientation which resulted in low performance. For a more robust and accurate approach, we propose the light weight convolutional neural networks, an end to end system, for estimating human body orientation. Our body orientation estimation model achieved 81.58% and 94% accuracy with the benchmark dataset and our own dataset respectively. The proposed method can be used in a wide range of service robot applications which depend on the ability to estimate human body orientation. To show its usefulness in service robot applications, we designed a simple robot application which allows the robot to move towards the user\u2019s frontal plane. With this, we demonstrated an improved face detection rate. I. INTRODUCTION A large part of developing assistance and service robotics is the study of robot vision techniques to observe the surrounding environment, for example, object detection, activity, emotion, and face recognition. However, most of these techniques require the robot to have correctly orientated itself to the subjects or objects of interest. Likewise, many studies for face recognition generally used images orientated correctly for observation by robots. However in the real world environment, robots are required to move and orientate itself to observe the user for more natural interaction. Here, we propose a method for estimating human body orientation through the use of convolutional neural networks (CNN), which is more suitable for high accuracy and invariance to the conditions like illumination, cluttered environment. The rest of the paper is structured as follows. In Section II, we overview the related works on human orientation estimation. Section III discusses our approach in detail. Experiments are presented in Section IV. Lastly, Section V concludes our work. Appendix will cover some details about our experiments.\n*This work was partly supported by the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (R0126-15-1072-SW.StarLab, 10044009-HRI.MESSI). Also, partially supported by a grant to Bio-Mimetic Robot Research Center funded by Defense Acquisition Program Administration, and by Agency for Defense Development (UD130070ID).\nThe authors are with the Seoul National University, Seoul, Korea (phone: +82-2-880-1847; fax: +82-2-875-2240; e-mail: {jychoi, bjlee, btzhang}@ bi.snu.ac.kr)."}, {"heading": "II. RELATED WORK", "text": "Previous studies on human body orientation estimation use external sensors attached on the user\u2019s body. [1] proposed a method for estimating human body orientation by using the skeleton gained from motion capture devices. [2] used magnetic sensors to estimate human body orientation. However, these classical approaches required the motion capture markers or magnetic sensors to be attached to the user\u2019s body, which is not ideal for service robot applications. Other studies on human body orientation estimation include analyzing surveillance videos. In [3], they combined the head and body cues to estimate both their orientations. This study introduced the concurrent use of two different information. However, [3] showed low performance when the illumination or brightness changed. In contrast to this study, we focused on estimating body orientation using images from the mobile robot, instead of a surveillance camera and we used a deep learning technique to achieve robustness against illumination changes. Recent works used a color-depth camera such as the Microsoft Kinect. Using the 3D-Point-Cloud, [4] proposed a 3D feature to estimate the human pose. Also, [5] adopted the depth information to extract the superpixel-based viewpoint feature histogram (SVFH) and estimated the human body orientation. Although these studies showed positive results, they needed fixed cameras and complicated feature extraction processes. The most similar work to ours is [6], which used convolutional neural networks and reported very successful human head orientation estimation. In contrast to these previous studies, our method can estimate human body orientation using only the RGB image from the mobile robot and learn in an end-to-end fashion without any manual feature extraction."}, {"heading": "III. METHOD", "text": "In this section, we propose a novel approach for human body orientation estimation. First, we give a brief overview of the convolutional neural networks then present details of our model.\nA. Convolutional Neural Networks Convolutional neural networks [7] are neural networks with convolutional filter layers. Convolutional layers produce the output feature map from the input image using Eq 1.\nHuman Body Orientation Estimation using Convolutional Neural Network\nJinyoung Choi, Beom-Jin Lee and Byoung-Tak Zhang\nWhere Hi,j,k is the value of the output feature image in location (i,j) in channel k, f is the activation function, is the value of z\u2019th weight filter in location (x,y) in channel k and Xi-x,j-y,z is the value of the input image in location (i-x,j-y) in channel z. Weight filters are trained using stochastic gradient descent. Recently, CNNs with multiple convolutional layers and fully connected layers [8,9] are being widely used due to their ability to discover powerful and robust features using only input images."}, {"heading": "B. Human Orientation Estimation", "text": "Our CNN model is described in Figure 1. The structure of the proposed model has two convolutional layers, followed by two fully connected layers. Both convolutional layers consist of 5x5x64 filters with stride 2. Local-response normalization [8] is applied to both layer\u2019s outputs. Two fully connected layers are built with 384 and 192 hidden units respectively. We used the ReLU [8] activation function for all layers. The output softmax layer has 8 output units. As described in Figure 2, each output unit represents specific range of body orientation. We used a classification model instead of a regression model because labels of the dataset were not accurate enough to train an accurate regression model. Unlike conventional deep learning methods such as GoogLeNet [9], we did not use a pooling layer. This is because human orientation estimation requires more spatial information (such as locations of the head and hands) than recognizing objects. We also used resized cropped human images of size 32x32 as the input. This size reduction approach is inspired by the work in [6]. In [6], they used cropped head images with size 32x32 for head orientation estimation. This allowed the usage of smaller networks for real-time processing. This reduction did not affect the recognition accuracy, which means that key features for orientation estimation were still preserved after the reduction. For cropping the human region from the camera image, we used the YOLO algorithm [10]."}, {"heading": "IV. EXPERIMENTS", "text": "We first trained the proposed network using the benchmark dataset and compared its performance to previous methods. Secondly, we collected our own dataset in the home environment and evaluated our model using our dataset. Lastly, we experimented our trained model in the wild environment using a simple personal robot application. Experiments were performed using a turtlebot2 platform with Asus Xtion camera and high-end laptop having core-i7 processor, 8GB memory, GTX960M GPU. We used ROS with Ubuntu 14.04 to implement our robot application."}, {"heading": "A. Experiment on benchmarking dataset and our dataset", "text": "To verify the usefulness of our proposed model, we used the Human 3.6M [11] as the benchmark dataset. This dataset consists of videos of 11 people (6 male and 5 female) who performed certain scenarios filmed by multiple cameras. The dataset provides accurate 3D positions of the subject\u2019s joints, however, it did not provide any ground truth label of the subjects\u2019 body orientation. Therefore we extracted the ground\ntruth about the body orientation from the provided joint positions as described in Figure 3. First, we applied cross-product to the vector from the neck to right upper leg (red vector in Figure 3) and vector from the neck to left upper leg (green vector in Figure 3). Then we projected the calculated vector (blue vector in Figure 3) to an XY plane and calculated the body orientation angle from the projected vector (orange vector in Figure 3). These values are used as the ground truth for classification and the angles were categorized into the 8 labels as shown in Figure 2. From the video, we cropped the human body with the YOLO algorithm every 25 frames and calculated the orientation. The total data size we collected was 74,862 from six subjects and we used 64,862 samples as the training set and randomly separated 10,000 samples as the validation set. With the collected data, the learning process of our model is shown in Figure 4. Each step comprised of 100 minibatches and each minibatch contained 100 samples. Our model converged within approximately 30 steps of training.\nThe performance of our model in comparison to previous methods are shown in Table I. As shown in the table, we outperformed traditional methods using the HOG feature and multi-class SVM classifier [12]. Furthermore, we did not find any previous works which use deep learning methods for classifying human orientation which, to the best of our knowledge, gives novelty to our study which uses deep learning for human orientation. Our model presented a high performance rate of 81.58%. We are planning to further improve the performance by adding other modalities such as depth information to our model and compare it with other state-of-art models using 3D cameras like in [5] for future works.\nFor more analysis, we calculated the mean orientation error using the angle differences of misclassified labels and true labels. This error was approximately 10.6 degrees. We achieved a reasonable mean error since most of the mislabeled instances were assigned to the nearest labels from the true labels, which differed only by 45 degrees. The confusion matrix is presented in Table II.\nWith the trained model using the Human 3.6M dataset, we fine-tuned our model with our own dataset collected in a real home environment, illustrated in Figure 5. To collect data, we captured images of three human subjects using the mobile robot in various locations of home environment. To obtain the orientation label, the robot was fixed while it collected data from the subjects who stood, sat or performed random actions in front of the robot facing designated angles. We collected 50,000 samples and fine-tuned our pre-trained model with randomly selected 45,000 samples as the training set and the remaining as the validation set. Our fine-tuned model scored a 94% accuracy on the validation set.\nTABLE III. PERFORMANCE ON HOME ENVIRONMENT DATASET\nModel Dataset Accuracy CNN (proposed) Training set\n(Our training data) 97.0% CNN (proposed) Validation set\n(Our test data) 94.0%"}, {"heading": "B. Application", "text": "To test our fine-tuned model in the real environment, we implemented a simple robot application. The robot is designed to follow the user until the user stays in a fixed location for 2.5 seconds. If the user does not move for 2.5 seconds, the robot estimates the user\u2019s body orientation using our module and\nFigure 6. Application experiment. Blue dots at 0 indicate failed detection of face by the robot and 1 indicates successful detection of the face. RGB images from the robot are also presented in graphs.\nmoves to the user\u2019s frontal plane. We used \u2018Chance-constrained target tracking for mobile robots\u2019 [13] algorithm for the following module. For the repositioning, we used a very simple algorithm since our goal was to show the performance of our orientation estimation model in a real environment. Details regarding the repositioning algorithm are described in the Appendix. Since we could not get the user\u2019s orientation during this experiment, we measured the usefulness of our model by performing face detection before and after the robot repositioned itself according to body orientation predicted by our model. We used Microsoft Oxford API [14] as our face detector and performed two experiments in the home environment. The results are presented in Figure 6. Our robot detected the user\u2019s face successfully after repositioning in both of the experiments, which demonstrates that our model could be used for various service robots which require the observation of the user\u2019s face."}, {"heading": "V. CONCLUSION", "text": "We built a robust and accurate human body orientation estimation model using the deep convolutional neural network. Our model presented higher accuracy and robustness compared to other classical shallow learning models in a complex benchmark dataset. Our model also showed high accuracy when examined in a real home environment setting. Furthermore, we showed usefulness of our model in real environments using simple robot application. Future research topics would be to build a highly accurate regression model that estimates body orientation in precise degrees and not in classes."}, {"heading": "ACKNOWLEDGMENT", "text": "Thanks to Christina Baek for proofreading and revision."}, {"heading": "APPENDIX \u2013 REPOSITIONING ALGORITHM", "text": "We used algorithm similar to [15]. We defined the utility function U(p) to calculate scores of pre-defined candidate positions around the target using the target\u2019s orientation and positions of target and robot. We formulated the utility function as\nwhere p is the candidate position, t is the target, and r is the robot\u2019s current position. Orientation(t) is the multiplier corresponding to the orientation class that will be observed after the robot moves to p. Values for orientation multiplier is presented in Table IV.\nDistance(p, r) is\nand Radius(p) is the multiplier corresponding to the distance between p and target\u2019s position. Values for radius multiplier is presented in Table V.\nOccupancy(p) is\nwhere the accessibility is determined by occupancy grid\u2019s pixel value. Obstacle(p, t) is\nwhere the existence of obstacles is determined by checking the occupancy grid\u2019s pixel values in the line between p and t. Once we calculate U(p) for all candidates, we choose the position which has the maximum U(p) and the robot navigates to this position accordingly. We used the tinySLAM [16] algorithm for SLAM and navigation, available in an open-source ROS package [17]."}], "references": [{"title": "Binocular Dance Pose Recognition and Body Orientation Estimation via Multilinear Analysis", "author": ["Bo Peng", "Gang Qian"], "venue": "Conf. 2008 Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Estimating Three-Dimensional Orientation of Human Body Parts by Inertial/Magnetic Sensing", "author": ["Angelo Maria Sabatini"], "venue": "Sensors 2011, 11(2), 1489-1525", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A Joint Estimation of Head and Body Orientation Cues in Surveillance Video", "author": ["Cheng Chen", "Alexandre Heili", "Jean-Marc Odobez"], "venue": "Conf. 2011 IEEE international conference on Computer Vision(ICCV) Workshops.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "A 3D-Point-Cloud Feature for Human-Pose Estimation", "author": ["Kai-Chi Chan", "Cheng-Kok Koh", "C.S. George Lee"], "venue": "Conf. 2013 IEEE International Conference on Robotics and Automation (ICRA).", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Accurate Estimation of Human Body Orientation From RGB-D Sensors", "author": ["Wu Liu", "Yongdong Zhang", "Sheng Tang", "Jinhui Tang", "Richang Hong", "Jintao Li"], "venue": "Conf. 2013 IEEE transactions on cybernetics.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Real-time Head Orientation from a Monocular Camera using Deep Neural Network", "author": ["Byungtae Ahn", "Jaesik Park", "In So Kweon"], "venue": "Conf. 2014 Asian Conference on Computer Vision (ACCV)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324, 1998.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G Hinton"], "venue": "NIPS, 2012", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Going Deeper with Convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "2014 Imagenet Large Scale Visual Recognition Challenge(ILSVRC)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "YouOnlyLookOnce: Unified,Real-TimeObjectDetection, in Conf. 2016 Computer Vision and Pattern Recognition (CVPR)", "author": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments", "author": ["Catalin Ionescu", "Dragos Papava", "Vlad Olaru", "Cristian Sminchisescu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, No. 7, July 2014", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimation of Human Upper Body Orientation for Mobile Robotics using an SVM Decision Tree on Monocular Images", "author": ["Christoph Weinrich", "Christian Vollmer", "Horst-Michael Gross"], "venue": "Conf. 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Chance-Constrained Target Tracking for Mobile Robots", "author": ["Yoonseon Oh", "Sungjoon Choi", "Songhwai Oh"], "venue": "Proc. of the IEEE International Conference on Robotics and Automation (ICRA), May 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Mobile robotic active view planning for physiotherapy and physical exercise guidance", "author": ["Kalana Ishara", "Ivan Lee", "Russell Brinkworth"], "venue": "Conf. IEEE International Conferences On Cybernetics And Intelligent Systems and Robotics, Automation And Mechatronics (CIS-RAM)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 0}, {"title": "tinySLAM: A SLAM algorithm in less than 200 lines C-language program", "author": ["Bruno Steux", "Oussama El Hamzaoui"], "venue": "Conf. 2010 11th International Conference on Control Automation Robotics & Vision (ICARCV).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1] proposed a method for estimating human body orientation by using the skeleton gained from motion capture devices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] used magnetic sensors to estimate human body orientation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In [3], they combined the head and body cues to estimate both their orientations.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "However, [3] showed low performance when the illumination or brightness changed.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "Using the 3D-Point-Cloud, [4] proposed a 3D feature to estimate the human pose.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "Also, [5] adopted the depth information to extract the superpixel-based viewpoint feature histogram (SVFH) and estimated the human body orientation.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "The most similar work to ours is [6], which used convolutional neural networks and reported very successful human head orientation estimation.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "Convolutional Neural Networks Convolutional neural networks [7] are neural networks with convolutional filter layers.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "Recently, CNNs with multiple convolutional layers and fully connected layers [8,9] are being widely used due to their ability to discover powerful and robust features using only input images.", "startOffset": 77, "endOffset": 82}, {"referenceID": 8, "context": "Recently, CNNs with multiple convolutional layers and fully connected layers [8,9] are being widely used due to their ability to discover powerful and robust features using only input images.", "startOffset": 77, "endOffset": 82}, {"referenceID": 7, "context": "Local-response normalization [8] is applied to both layer\u2019s outputs.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "We used the ReLU [8] activation function for all layers.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "Unlike conventional deep learning methods such as GoogLeNet [9], we did not use a pooling layer.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "This size reduction approach is inspired by the work in [6].", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "In [6], they used cropped head images with size 32x32 for head orientation estimation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "For cropping the human region from the camera image, we used the YOLO algorithm [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "6M [11] as the benchmark dataset.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "As shown in the table, we outperformed traditional methods using the HOG feature and multi-class SVM classifier [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 4, "context": "We are planning to further improve the performance by adding other modalities such as depth information to our model and compare it with other state-of-art models using 3D cameras like in [5] for future works.", "startOffset": 188, "endOffset": 191}, {"referenceID": 12, "context": "We used \u2018Chance-constrained target tracking for mobile robots\u2019 [13] algorithm for the following module.", "startOffset": 63, "endOffset": 67}], "year": 2016, "abstractText": "Abstract\u2014 Personal robots are expected to interact with the user by recognizing the user\u2019s face. However, in most of the service robot applications, the user needs to move himself/herself to allow the robot to see him/her face to face. To overcome such limitations, a method for estimating human body orientation is required. Previous studies used various components such as feature extractors and classification models to classify the orientation which resulted in low performance. For a more robust and accurate approach, we propose the light weight convolutional neural networks, an end to end system, for estimating human body orientation. Our body orientation estimation model achieved 81.58% and 94% accuracy with the benchmark dataset and our own dataset respectively. The proposed method can be used in a wide range of service robot applications which depend on the ability to estimate human body orientation. To show its usefulness in service robot applications, we designed a simple robot application which allows the robot to move towards the user\u2019s frontal plane. With this, we demonstrated an improved face detection rate.", "creator": null}}}