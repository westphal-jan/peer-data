{"id": "1609.03145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2016", "title": "Relational Models", "abstract": "we help provide a survey base on relational models. relational models describe complete networked { domains by taking into account global dependencies in analyzing the data }. relational models can only lead to more accurate predictions if compared to non - relational machine learning approaches. relational models choices typically are based on probabilistic \u2013 graphical information models, e. g., bayesian networks, markov networks, or latent variable models. relational models have proven applications in social networks analysis, the modeling of knowledge graphs, bioinformatics, recommendation inference systems, business natural language processing processing, medical decision support, and linked data.", "histories": [["v1", "Sun, 11 Sep 2016 10:14:18 GMT  (71kb,D)", "http://arxiv.org/abs/1609.03145v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["volker tresp", "maximilian nickel"], "accepted": false, "id": "1609.03145"}, "pdf": {"name": "1609.03145.pdf", "metadata": {"source": "CRF", "title": "Relational Models", "authors": ["Volker Tresp", "Maximilian Nickel"], "emails": [], "sections": [{"heading": "1 Synonyms", "text": "Relational learning, statistical relational models, statistical relational learning, relational data mining"}, {"heading": "2 Glossary", "text": "Entities are (abstract) objects. We denote an entity by a lowercase e. An actor in a social network can be modelled as an entity. There can be multiple types of entities in a domain (e.g., individuals, cities, companies), entity attributes (e.g., income, gender) and relationships between entities (e.g., knows, likes, brother, sister). Entities, relationships and attributes are defined in the entity-relationship model, which is used in the design of a formal relational model\nRelation A relation or relation instance I(R) is a set of tuples. A tuple t is an ordered list of elements (e1, e2, . . . , earity), which, in the context of this discussion, represent entities. The arity of a relation is the number\nar X\niv :1\n60 9.\n03 14\n5v 1\n[ cs\n.A I]\nof elements in each of its tuples, e.g., a relation might be unary, binary or higher order. R is the name or type of the relation. For example, (Jack, Mary) might be a tuple of the relation instance knows, indicating that Jack knows Mary. A database instance (or world) is a set of relation instances. For example, a database instance might contain instances of the unary relations student, teacher, male, female, and instances of the binary relations knows, likes, brother, sister (see Figure 1)\nPredicate A predicate R is a mapping of tuples to true or false. R(t) is a ground predicate and is true when t \u2208 I(R), otherwise it is false. Note that we do not distinguish between the relation name R and the predicate name R. Example: knows is a predicate and knows(Jack, Mary) returns True if it is true that Jack knows Mary, i.e., that (Jack, Mary) \u2208 I(knows). The convention is that relations and predicates are written in lowercase and entities in uppercase\nProbabilistic Database A (possible) world corresponds to a database instance. In a probabilistic database, a probability distribution is defined over all possible worlds under consideration. Probabilistic databases with potentially complex dependencies can be described by probabilistic graphical models. In a canonical representation, one assigns a binary random variable XR,t to each possible tuple t in each relation R. Then\nt \u2208 I(R) \u21d4 R(t) = True \u21d4 XR,t = 1\nand t /\u2208 I(R) \u21d4 R(t) = False \u21d4 XR,t = 0.\nThe probability for a world x is written as P (X = x) where X = {XR,t}R,t is the set of random variables and x denotes their values in the world (see Figure 1)\nTriple Database A triple database consists of binary relations represented as subject-predicate-object triples. An example of a triple is: (Jack, knows, Mary). A triple database can be represented as a knowledge graph with entities as nodes and predicates as directed links, pointing from the subject node to the object node. The Resource Description Framework (RDF) is triple based and is the basic data model of the Semantic Web\u2019s Linked Open Data. In social network analysis, nodes would be individuals or actors and links would correspond to ties\nLinked Data Linked Open Data describes a method for publishing structured data so that it can be interlinked and can be exploited by machines. Linked Open Data uses the RDF data model\nCollective learning refers to the effect that an entity\u2019s relationships, attributes or class membership can be predicted not only from its attributes but also from its (social) network environment\nCollective classification A special case of collective learning: The class membership of entities can be predicted from the class memberships of entities in their (social) network environment. Example: Individuals\u2019 income classes can be predicted from those of their friends\nRelationship prediction The prediction of the existence of a relationship between entities, for example friendship between individuals. A relationship is typically modelled as a binary relation\nEntity resolution The task of predicting if two constants refer to the same entity\nHomophily The tendency of an individual to associate with similar others\nGraphical models A graphical description of a probabilistic domain where nodes represent random variables and edges represent direct probabilistic dependencies\nLatent Variables Latent variables are quantities which are not measured directly and whose states are inferred from data"}, {"heading": "3 Definition", "text": "Relational models are machine-learning models that are able to truthfully represent some or all distinguishing features of a relational domain such as longrange dependencies over multiple relationships. Typical examples for relational domains include social networks and knowledge bases. Relational models concern nontrivial relational domains with at least one relation with an arity of two or larger that describes the relationship between entities, e.g., knows, likes, dislikes. In the following we will focus on nontrivial relational domains."}, {"heading": "4 Introduction", "text": "Social networks can be modelled as graphs, where actors correspond to nodes and where relationships between actors such as friendship, kinship, organizational position, or sexual relationships are represented by directed labelled links (or ties) between the respective nodes. Typical machine learning tasks would concern the prediction of unknown relationship instances between actors, as well as the prediction of actors\u2019s attributes and class labels. In addition, one might be interested in a clustering of actors. To obtain best results, machine learning should take an actors\u2019s network environment into account. Thus two individuals might appear in the same cluster because they have common friends.\nRelational learning is a branch of machine learning that is concerned with these tasks, i.e. to learn efficiently from data where information is represented in form of relationships between entities.\nRelational models are machine learning models that truthfully model some or all distinguishing features of relational data such as long-range dependencies\npropagated via relational chains and homophily, i.e. the fact that entities with similar attributes are neighbors in the relationship structure. In addition to social network analysis, relational models are used to model knowledge graphs, preference networks, citation networks, and biomedical networks such as genedisease networks or protein-protein interaction networks. Relational models can be used to solve the aforementioned machine learning tasks, i.e., classification, attribute prediction, clustering. Moreover, relational models can be used to solve additional relational learning tasks such as relationship prediction and entity resolution. Relational models are derived from directed and undirected graphical models or latent variable models and typically define a probability distribution over a relational domain."}, {"heading": "5 Key Points", "text": "Statistical relational learning is a subfield of machine learning. Relational models learn a probabilistic model of a complete networked domain by taking into account global dependencies in the data. Relational models can lead to more accurate predictions if compared to non-relational machine learning approaches. Relational models typically are based on probabilistic graphical models, e.g., Bayesian networks, Markov networks, or latent variable models."}, {"heading": "6 Historical Background", "text": "Inductive logic programming (ILP) was maybe the first machine learning effort that seriously focussed on a relational representation. It gained attention in the early 1990s and focusses on learning deterministic or close-to-deterministic dependencies, with representations derived from first order logic. As a field, ILP was introduced in a seminal paper by Muggleton [21]. A very early and still very influential algorithm is Quinlan\u2019s FOIL [29]. ILP will not be a focus in the following, since social networks exhibit primarily statistical dependencies. Statistical relational learning started around the beginning of the millennium with the work by Koller, Pfeffer, Getoor and Friedman [17, 9]. Since then many combinations of ILP and relational learning have been explored. The Semantic Web, Linked Open Data are producing vast quantities of relational data and [39, 27] describe the application of statistical relational learning to these emerging fields. Relational learning has been applied to the learning of knowledge graphs, which model large domains as triple databases. [24] is a recent review on the application of relational learning to knowledge graphs. An interesting application is the semi-automatic completion of knowledge graphs by analysing information from the Web and other sources, in combination with relational learning, which exploits the information already present on the knowledge graph [5]."}, {"heading": "7 Machine Learning in Relational Domains", "text": ""}, {"heading": "7.1 Relational Domains", "text": "Relational domains are domains that can truthfully be represented by relational databases. The glossary defines the key terms such as a relation, a predicate, a tuple and a database. Nontrivial relational domains contain at least one relation with an arity of two or larger that describes the relationship between entities, e.g., knows, likes, dislikes. The main focus here is on nontrivial relational domains.\nSocial networks are typical relational domains, where information is represented by multiple types of relationships (e.g., knows, likes, dislikes) between entities (here: actors), as well as through the attributes of entities."}, {"heading": "7.2 Generative Models for a Relational Database", "text": "Typically, relational models can exploit long-range or even global dependencies and have principled ways of dealing with missing data. Relational models are often displayed as probabilistic graphical models and can be thought of as relational versions of regular graphical models, e.g., Bayesian networks, Markov networks, and latent variable models. The approaches often have a \u201cBayesian flavor\u201d but a fully Bayesian statistical treatment is not always performed.\nThe following section describes common relational graphical models."}, {"heading": "7.3 Non-relational Learning", "text": "Although we are mostly concerned with relational learning, it is instructive to analyse the special case of non-relational learning. Consider a database with a key entity class actor with elements ei and with only unary relations; thus we are considering a trivial relational domain. Then one can partition the random variables into independent disjoint sets according to the entities, and the joint distribution factorizes as \u220f\ni\nP ({XR,ei}R)\nwhere the binary random variable XR,ei is assigned to tuple ei in unary relation R (see glossary).\nThus the set of random variables can be reduced to non-overlapping independent sets of random variables. This is the common non-relational learning setting with i.i.d. instances, corresponding to the different actors."}, {"heading": "7.4 Non-relational Learning in a Relational Domain", "text": "An common approximation to a relational model is to model unary relations of key entities in a similar way as in a non-relational model as\u220f\ni\nP ({XR,ei}R | fi)\nwhere fi is a vector of relational features that are derived from the relational network environment of the actor i. Relational features provide additional information to support learning and prediction tasks. For instance, the average income of an individual\u2019s friends might be a good covariate to predict an individual\u2019s income in a social network. The underlying mechanism that forms these patterns might be homophily, the tendency of individuals to associate with similar others. The goal of this approach is to be able to use i.i.d. machine learning by exploiting some of the relational information. This approach is commonly used in applications where probabilistic models are computationally too expensive. The application of non-relational machine learning to relational domains is sometimes referred to as propositionalization.\nRelational features are often high-dimensional and sparse (e.g., there are many people, but only a small number of them are an individual\u2019s friends; there are many items but an individual has only bought a small number of them) and in some domains it can be easier to define useful kernels than to define useful features. Relational kernels often reflect the similarity of entities with regard to the network topology. For example a kernel can be defined based on counting the substructures of interest in the intersection of two graphs defined by neighborhoods of the two entities [20] (see also the discussion on RDF graphs further down)."}, {"heading": "7.5 Learning Rule-Premises in Inductive Logic Programming", "text": "Some researchers apply a systematic search for good features and consider this as an essential distinction between relational learning and non-relational learning: in non-relational learning features are essentially defined prior to the training phase whereas relational learning includes a systematic and automatic search for features in the relational context of the involved entities. Inductive logic programming (ILP) is a form of relational learning with the goal of finding deterministic or close-to-deterministic dependencies, which are described in logical form such as Horn clauses. Traditionally, ILP involves a systematic search for sensible relational features that form the rule premises [6]."}, {"heading": "8 Relational Models", "text": "In this section we describe the most important relational models in some detail. These are based on probabilistic graphical models, which efficiently model high-dimensional probability distributions by exploiting independencies between random variables. In particular, we consider Bayesian networks, Markov networks and latent variable models. We start with a more detailed discussion on possible world models for relational domains and with a discussion on the dual structures of the triple graph and the probabilistic graph."}, {"heading": "8.1 Random Variables for Relational Models", "text": "As mentioned before, a probabilistic database defines a probability distribution over the possible worlds under consideration. The goal of relational learning is to derive a model of this probability distribution.\nIn a canonical representation, we assign a binary random variable XR,t to each possible tuple in each relation. Then\nt \u2208 I(R) \u21d4 R(t) = True \u21d4 XR,t = 1\nand t /\u2208 I(R) \u21d4 R(t) = False \u21d4 XR,t = 0.\nThe probability for a world x is written as P (X = x) where X = {XR,t}R,t is the set of random variables and x denotes their values in the world (see Figure 1). What we have just described corresponds to a closed-world assumption where all tuples, which are not part of the database instance, map to R(t) = False and thus XR,t = 0. In contrast in an open world assumption, we would consider the corresponding truth values and states as being unknown and the database instance as being only partially observed. Often in machine learning some form of a local closed-world assumption is applied with a mixture of true, false and unknown ground predicates [5, 18]. For example one might assume that, if at least one child of an individual is specified, it implies that all children are specified (closed-world), whereas if no child is specified, children are considered unknown (open-world). Another aspect is that type constraint imply that certain ground predicates are false. For example, only individuals can get married, but neither cities or buildings. Other types of background knowledge might materialize tuples that are not explicitly specified. For example, if individuals live in Munich, by simple reasoning one can conclude that they also live in Bavaria and Germany. The corresponding tuples can be added to the database.\nBased on background knowledge, one might want to modify the canonical representation, which uses only binary random variables. For example, discrete random variables with N states are often used to implement the constraint that exactly one out off N ground predicates is true, e.g. that an individual belongs exactly to one out of N income classes or age classes. It is also possible to extend the model towards continuous variables.\nSo far we have considered an underlying probabilistic model and an observed world. In probabilistic databases one often assumes a noise process between the actual database instance and the observed database instance by specifying a conditional probability\nP (YR,t|XR,t).\nThus only YR,t is observed whereas the real interest is on XR,t: One observes a t \u2208 Iy(R) \u21d4 YR,t = 1 from which one can infer for the database instance P (t \u2208 I(R)) \u21d4 P (XR,t = 1). With an observed YR,t = 1, there is a certain probability that XR,t = 0 (error in the database) and with an observed YR,t = 0 there is a certain probability that XR,t = 1 (missing tuples).\nThe theory of probabilistic databases focussed on the issues of complex query answering under a probabilistic model. In probabilistic databases [36] the canonical representation is used in tuple-independent databases, while multi-state random variables are used in block-independent-disjoint (BID) databases.\nMost relational models assume that all entities (or constants) and all predicates are known and fixed (domain closure assumption). In general these constraints can be relaxed, for example if one needs to include new individuals in the model. Also, latent variables derived from a cluster or a factor analysis can be interpreted as new \u201cinvented\u201d predicates."}, {"heading": "8.2 Triple Graphs and Probabilistic Graphical Networks", "text": "A triple database consists of binary relations represented as subject-predicateobject triples. An example of a triple is: (Jack, knows, Mary). A triple database can be represented as a knowledge graph with entities as nodes and predicates as directed links, pointing from the subject node to the object node. Triple databases are able to represent web-scale knowledge bases and sociograms that allow multiple types of directed links. Relations of higher order can be reduced to binary relations by introducing auxiliary entities (\u201cblank nodes\u201d). Figure 2 shows an example of a triple graph. The Resource Description Framework (RDF) is triple based and is the basic data model of the Semantic Web\u2019s Linked Open Data. In social network analysis, nodes would be individuals or actors and links would correspond to ties.\nFor each triple a random variable is introduced. In Figure 2 these random variables are represented as elliptical red nodes. The binary random variable associated with the tripe (s = i, p = k, o = j) will be denoted as Xk(i,j)."}, {"heading": "8.3 Directed Relational Models", "text": "The probability distribution of a directed relational model, i.e. a relational Bayesian model, can be written as\nP ({XR,t}R,t) = \u220f R,t P (XR,t|par(XR,t)). (1)\nHere {XR,t}R,t refers to the set of random variables in the directed relational model, while XR,t denotes a particular random variable. In a graphical representation, directed arcs are pointing from all parent nodes par(XR,t) to the node XR,t (Figure 2). As Equation 1 indicates the model requires the specification of the parents of a node and the specification of the probabilistic dependency of a node, given the states of its parent nodes. In specifying the former, one often follows a causal ordering of the nodes, i.e., one assumes that the parent nodes causally influence child nodes and their descendents. An important constraint is that the resulting directed graph is not permitted to have directed loops, i.e. that it is a directed acyclic graph. A major challenge is to specify P (XR,t|par(XR,t)), which might require the calculation of complex aggregational features as intermediate steps."}, {"heading": "8.3.1 Probabilistic Relational Models", "text": "Probabilistic relational models (PRMs) were one of the first published directed relational models and found great interest in the statistical machine learning community [17, 10]. An example of a PRM is shown in Figure 3. PRMs combine a frame-based (i.e., object-oriented) logical representation with probabilistic semantics based on directed graphical models. The PRM provides a template for specifying the graphical probabilistic structure and the quantification of the probabilistic dependencies for any ground PRM. In the basic PRM models only the entities\u2019 attributes are uncertain whereas the relationships between entities are assumed to be known. Naturally, this assumption greatly simplifies the model. Subsequently, PRMs have been extended to also consider the case that relationships between entities are unknown, which is called structural uncertainty in the PRM framework [10].\nIn PRMs one can distinguish parameter learning and structural learning. In the simplest case the dependency structure is known and the truth values of all ground predicates are known as well in the training data. In this case, parameter learning consists of estimating parameters in the conditional probabilities. If the dependency structure is unknown, structural learning is applied, which optimizes an appropriate cost function and typically uses a greedy search strategy to find the optimal dependency structure. In structural learning, one needs to guarantee that the ground Bayesian network does not contain directed loops.\nIn general the data will contain missing information, i.e., not all truth values of all ground predicates are known in the available data. For some PRMs, regularities in the PRM structure can be exploited (encapsulation) and even exact inference to estimate the missing information is possible. Large PRMs require approximate inference; commonly, loopy belief propagation is being used."}, {"heading": "8.3.2 More Directed Relational Graphical Models", "text": "A Bayesian logic program is defined as a set of Bayesian clauses [16]. A Bayesian clause specifies the conditional probability distribution of a random variable given its parents. A special feature is that, for a given random variable, several such conditional probability distributions might be given and combined based on various combination rules (e.g., noisy-or). In a Bayesian logic program, for each clause there is one conditional probability distribution and for each random variable there is one combination rule. Relational Bayesian networks [14] are related to Bayesian logic programs and use probability formulae for specifying conditional probabilities. The probabilistic entity-relationship (PER) models [12] are related to the PRM framework and use the entity-relationship model as a basis, which is often used in the design of a relational database. Relational dependency networks [22] also belong to the family of directed relational models and learn the dependency of a node given its Markov blanket (the smallest node set that make the node of interest independent of the remaining network). Relational dependency networks are generalizations of dependency networks as introduced\nby [11, 13]. A relational dependency networks typically contains directed loops and thus is not a proper Bayesian network."}, {"heading": "8.4 Undirected Relational Graphical Models", "text": "The probability distribution of an undirected graphical model, i.e. a Markov network, is written as a log-linear model in the form\nP (X = x) = 1\nZ exp \u2211 i wifi(xi)\nwhere the feature functions fi can be any real-valued function on the set xi \u2286 x and where wi \u2208 R. In a probabilistic graphical representation one forms undirected edges between all nodes that jointly appear in a feature function. Consequently, all nodes that appear jointly in a function will form a clique in the graphical representation. Z is the partition function normalizing the distribution.\nA major advantage is that undirected graphical models can elegantly model symmetrical dependencies, which are common in social networks."}, {"heading": "8.4.1 Markov Logic Network (MLN)", "text": "A Markov logic network (MLN) is a probabilistic logic which combines Markov networks with first-order logic. In MLNs the random variables, representing ground predicates, are part of a Markov network, whose dependency structure is derived from a set of first-order logic formulae (Figure 4).\nFormally, a MLN L is defined as follows: Let Fi be a first-order formula, (i.e., a logical expression containing constants, variables, functions and predicates) and let wi \u2208 R be a weight attached to each formula. Then L is defined as a set of pairs (Fi, wi) [32, 4].\nFrom L the ground Markov network ML,C is generated as follows. First, one generates nodes (random variables) by introducing a binary node for each possible grounding of each predicate appearing in L given a set of constants c1, . . . , c|C| (see the discussion on the canonical probabilistic representation). The state of a node is equal to one if the ground predicate is true, and zero otherwise. The feature functions fi, which define the probabilistic dependencies in the Markov network, are derived from the formulae by grounding them in a domain. For formulae that are universally quantified, grounding is an assignment of constants to the variables in the formula. If a formula contains N variables, then there are |C|N such assignments. The feature function fi is equal to one if the ground formula is true, and zero otherwise. The probability distribution of the ML,C can then be written as\nP (X = x) = 1\nZ exp (\u2211 i wini(x) ) ,\nwhere ni(x) is the number of formula groundings that are true for Fi and where the weight wi is associated with formula Fi in L.\nThe joint distribution P (X = x) will be maximized when large weights are assigned to formulae that are frequently true. In fact, the larger the weight, the higher is the confidence that a formula is true for many groundings. Learning in MLNs consists of estimating the weights wi from data. In learning, MLN makes a closed-world assumption and employs a pseudo-likelihood cost function, which is the product of the probabilities of each node given its Markov blanket. Optimization is performed using a limited memory BFGS algorithm.\nThe simplest form of inference in a MLN concerns the prediction of the truth value of a ground predicate given the truth values of other ground predicates. For this task an efficient algorithm can be derived: In the first phase of the algorithm, the minimal subset of the ground Markov network is computed that is required to calculate the conditional probability of the queried ground predicate. It is essential that this subset is small since in the worst case, inference could involve all nodes. In the second phase, the conditional probability is then computed by applying Gibbs sampling to the reduced network.\nFinally, there is the issue of structural learning, which, in this context, means the learning of first order formulae. Formulae can be learned by directly optimizing the pseudo-likelihood cost function or by using ILP algorithms. For the latter, the authors use CLAUDIAN [30], which can learn arbitrary first-order clauses (not just Horn clauses, as in many other ILP approaches).\nAn advantage of MLNs is that the features and thus the dependency structure is defined using a well-established logical representation. On the other hand, many people are unfamiliar with logical formulae and might consider the PRM framework to be more intuitive."}, {"heading": "8.4.2 Relational Markov Networks (RMNs)", "text": "RMNs generalize many concepts of PRMs to undirected relational models [37]. RMNs use conjunctive database queries as clique templates, where a clique in an undirected graph is a subset of its nodes such that every two nodes in the subset are connected by an edge. RMNs are mostly trained discriminately. In contrast to MLNs and similarly to PRMs, RMNs do not make a closed-world assumption during learning."}, {"heading": "8.5 Relational Latent Variable Models", "text": "In the approaches described so far, the structures in the graphical models were either defined using expert knowledge or were learned directly from data using some form of structural learning. Both can be problematic since appropriate expert domain knowledge might not be available, while structural learning can be very time consuming and possibly results in local optima which are difficult to interpret. In this context, the advantage of relational latent variable models is that the structure in the associated graphical models is purely defined by the entities and relations in the domain.\nThe additional complexity of working with a latent representation is counterbalanced by the great simplification by avoiding structural learning. In the following discussion, we assume that data is in triple format; generalizations to relational databases haven been described [41, 19]."}, {"heading": "8.5.1 The IHRM: A Latent Class Model", "text": "The infinite hidden relational model (IHRM) [41] (a.k.a infinite relational model [15]) is a generalization to a probabilistic mixture model where a latent variable with states 1, . . . , R is assigned to each entity ei. If the latent variable for subject s = i is in state r and the latent variable for object o = j is in state q, then the triple (s = i, p = k, o = j) exists with probability P (Xk(i,j)|r, q). Since the latent states are unobserved, we obtain\nP (Xk(i,j)|i, j) = \u2211 r,q P (r|i)P (q|j)P (Xk(i,j)|r, q)\nwhich can be implemented as the sum-product network of Figure 5. In the IHRM the number of states (latent classes) in each latent variable is allowed to be infinite and fully Bayesian learning is performed based on a Dirichlet process mixture model. For inference Gibbs sampling is employed where only a small number of the infinite states are occupied in sampling, leading to a clustering solution where the number of states in the latent variables is automatically determined. Models with a finite number of states have been studied as stochastic block models [28].\nSince the dependency structure in the ground Bayesian network is local, one might get the impression that only local information influences prediction. This is not true, since latent representations are shared and in the ground Bayesian network the latter are parents to the random network variables Xk,(i,j). Thus common children with evidence lead to interactions between the parent latent variables. Thus information can propagate in the network of latent variables.\nThe IHRM has a number of key advantages. First, no structural learning is required, since the directed arcs in the ground Bayesian network are directly given by the structure of the triple graph. Second, the IHRM model can be thought of as an infinite relational mixture model, realizing hierarchical Bayesian modeling. Third, the mixture model can be used for a cluster analysis providing insight into the relational domain.\nThe IHRM has been applied to social networks, recommender systems, for gene function prediction and to develop medical recommender systems. The IHRM was the first relational model applied to trust learning [31].\nIn [1] the IHRM is generalized to a mixed-membership stochastic block model, where entities can belong to several classes."}, {"heading": "8.5.2 RESCAL: A Latent Factor Model", "text": "The RESCAL model was introduced in [26] and follows a similar dependency structure as the IHRM as shown in Figure 5. The main differences are that, first,\nthe latent variables do not describe entity classes but are latent entity factors and that, second, there are no nonnegativity or normalization constraints on the factors. The probability of a triple is calculated with\nf(i, j, k) = \u2211 r,q a(i, r)a(j, q)g(k, r, q) (2)\nas P (Xk(i,j)|i, j) = sig (f(i, j, k))\nwhere sig(x) = 1/(1 + exp\u2212x). As in the IHRM, factors are unique to entities which leads to interactions between the factors in the ground Bayesian network, enabling the propagation of information in the network of latent factors. The relation-specific matrix GR = g(k, :, :) encodes the factor interactions for a specific relation and its asymmetry permits the representation of directed relationships.\nThe calculation of the latent factors is based on the factorization of a multirelational adjacency tensor where two modes represent the entities in the domain and the third mode represents the relation type (Figure 6). With a closed-world assumption and a squared-error cost function, efficient alternating least squares (ALS) algorithm can be used; for local closed world assumptions and open world assumptions, stochastic gradient descent is being used.\nThe relational learning capabilities of the RESCAL model have been demonstrated on classification tasks and entity resolution tasks, i.e., the mapping of entities between knowledge bases. One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.\nRESCAL is part of a tradition on relation prediction using factorization of matrices and tensors. [42] describes a Gaussian process-based approach for predicting a single relation type, which has been generalized to a mutli-relational setting in [40].\nA number of variations and extensions exist. The SUNS approach [39] is based on a Tucker1 decomposition of the adjacency tensor, which can be computed by a singular value decomposition (SVD). The Neural Tensor Network [34] combines several tensor decompositions. Approaches with a smaller memory footprint are TransE [3] and HolE [25]. The multiway neural network in the Knowledge Vault project [5] combines the strengths of latent factor models and neural networks and was successfully used in semi-automatic completion of knowledge graphs. [24] is a recent review on the application of relational learning to knowledge graphs."}, {"heading": "9 Key Applications", "text": "Typical applications of relational models are in social networks analysis, knowledge graphs, bioinformatics, recommendation systems, natural language pro-\nj-th entity\ni-th entity\nrelation R\nAT\nGRA\nFigure 6: In RESCAL, Equation 2 describes a tensor decomposition of the tensor F = f(:, :, :) into the factor matrix A = a(:, :) and core tensor G = g(:, :, :). In the multi-relational adjacency tensor on the left, two modes represent the entities in the domain and the third mode represents the relation type. The i-th row of the matrix A contains the factors of the i-th entity. GR is a slice in the G-tensor and encodes the relation-type specific factor interactions. The factorization can be interpreted as a constrained Tucker2 decomposition.\ncessing, medical decision support, and Linked Open Data."}, {"heading": "10 Future Directions", "text": "As a number of publications have shown, best results can be achieved by committee solutions integrating factorization approaches with user defined or learned rule patterns [23, 5]. The most interesting application in recent years was in projects involving large knowledge graphs, where performance and scalability could clearly be demonstrated [5, 24]. The application of relational learning to sequential data and time series opens up new application areas, for example in clinical decision support and sensor networks [7, 8]. [38] studies the relevance of relational learning to cognitive brain functions."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M. Airoldi", "David M. Blei", "Stephen E. Fienberg", "Eric P. Xing"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary G. Ives"], "venue": "In ISWC/ASWC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Translating embeddings for modeling multirelational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Markov logic: A unifying framework for statistical relational learning", "author": ["Pedro Domingos", "Matthew Richardson"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Inductive logic programming in a nutshell", "author": ["Saso Dzeroski"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["Crist\u00f3bal Esteban", "Danilo Schmidt", "Denis Krompa\u00df", "Volker Tresp"], "venue": "In Healthcare Informatics (ICHI), 2015 International Conference on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Predicting the co-evolution of event and knowledge graphs", "author": ["Crist\u00f3bal Esteban", "Volker Tresp", "Yinchong Yang", "Denis Baier", "Stephan Krompa\u00df"], "venue": "In International Conference on Information Fusion,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning probabilistic relational models", "author": ["Nir Friedman", "Lise Getoor", "Daphne Koller", "Avi Pfeffer"], "venue": "In IJCAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Probabilistic relational models", "author": ["Lise Getoor", "Nir Friedman", "Daphne Koller", "Avi Pferrer", "Benjamin Taskar"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["David Heckerman", "David Maxwell Chickering", "Christopher Meek", "Robert Rounthwaite", "Carl Myers Kadie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Probabilistic entity-relationship models, prms, and plate models", "author": ["David Heckerman", "Christopher Meek", "Daphne Koller"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Nonlinear markov networks for continuous variables", "author": ["Reimar Hofmann", "Volker Tresp"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Relational bayesian networks", "author": ["Manfred Jaeger"], "venue": "In UAI, pages 266\u2013273,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Bayesian logic programs", "author": ["Kristian Kersting", "Luc De Raedt"], "venue": "CoRR, cs.AI/0111058,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Probabilistic frame-based systems", "author": ["Daphne Koller", "Avi Pfeffer"], "venue": "In AAAI/IAAI, pages 580\u2013587,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Type-constrained representation learning in knowledge graphs", "author": ["Denis Krompa\u00df", "Stephan Baier", "Volker Tresp"], "venue": "In International Semantic Web Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Probabilistic latent-factor database models. Linked Data for Knowledge Discovery, page", "author": ["Denis Krompa\u00df", "Xueyian Jiang", "Maximilian Nickel", "Volker Tresp"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Graph kernels for rdf data", "author": ["Uta L\u00f6sch", "Stephan Bloehdorn", "Achim Rettinger"], "venue": "In ESWC,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Inductive logic programming", "author": ["Stephen Muggleton"], "venue": "New Generation Comput.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Dependency networks for relational data", "author": ["Jennifer Neville", "David Jensen"], "venue": "In ICDM, pages 170\u2013177,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Reducing the rank in relational factorization models by including observable patterns", "author": ["Maximilian Nickel", "Xueyan Jiang", "Volker Tresp"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1510.04935,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In WWW,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["Krzysztof Nowicki", "Tom A B Snijders"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Learning logical definitions from relations", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1990}, {"title": "A statistical relational model for trust learning", "author": ["Achim Rettinger", "Matthias Nickles", "Volker Tresp"], "venue": "In AAMAS", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Introducing the knowledge graph: things, not strings", "author": ["Amit Singhal"], "venue": "Technical report, Ofcial Google Blog,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In WWW,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Probabilistic Databases", "author": ["Dan Suciu", "Dan Olteanu", "Christopher R\u00e9", "Christoph Koch"], "venue": "Synthesis Lectures on Data Management. Morgan & Claypool Publishers,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Discriminative probabilistic models for relational data", "author": ["Benjamin Taskar", "Pieter Abbeel", "Daphne Koller"], "venue": "In UAI,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "Learning with memory embeddings", "author": ["Volker Tresp", "Crist\u00f3bal Esteban", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "venue": "arXiv preprint arXiv:1511.07972,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Materializing and querying learned knowledge", "author": ["Volker Tresp", "Yi Huang", "Markus Bundschus", "Achim Rettinger"], "venue": "In First ESWC Workshop on Inductive Reasoning and Machine Learning on the Semantic Web (IRMLeS", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Multi-relational learning with gaussian processes", "author": ["Zhao Xu", "Kristian Kersting", "Volker Tresp"], "venue": "In IJCAI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Infinite hidden relational models", "author": ["Zhao Xu", "Volker Tresp", "Kai Yu", "Hans-Peter Kriegel"], "venue": "In UAI,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Stochastic relational models for discriminative link prediction", "author": ["Kai Yu", "Wei Chu", "Shipeng Yu", "Volker Tresp", "Zhao Xu"], "venue": "In NIPS,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "As a field, ILP was introduced in a seminal paper by Muggleton [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "A very early and still very influential algorithm is Quinlan\u2019s FOIL [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Statistical relational learning started around the beginning of the millennium with the work by Koller, Pfeffer, Getoor and Friedman [17, 9].", "startOffset": 133, "endOffset": 140}, {"referenceID": 8, "context": "Statistical relational learning started around the beginning of the millennium with the work by Koller, Pfeffer, Getoor and Friedman [17, 9].", "startOffset": 133, "endOffset": 140}, {"referenceID": 36, "context": "The Semantic Web, Linked Open Data are producing vast quantities of relational data and [39, 27] describe the application of statistical relational learning to these emerging fields.", "startOffset": 88, "endOffset": 96}, {"referenceID": 26, "context": "The Semantic Web, Linked Open Data are producing vast quantities of relational data and [39, 27] describe the application of statistical relational learning to these emerging fields.", "startOffset": 88, "endOffset": 96}, {"referenceID": 23, "context": "[24] is a recent review on the application of relational learning to knowledge graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "An interesting application is the semi-automatic completion of knowledge graphs by analysing information from the Web and other sources, in combination with relational learning, which exploits the information already present on the knowledge graph [5].", "startOffset": 248, "endOffset": 251}, {"referenceID": 19, "context": "For example a kernel can be defined based on counting the substructures of interest in the intersection of two graphs defined by neighborhoods of the two entities [20] (see also the discussion on RDF graphs further down).", "startOffset": 163, "endOffset": 167}, {"referenceID": 5, "context": "Traditionally, ILP involves a systematic search for sensible relational features that form the rule premises [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Often in machine learning some form of a local closed-world assumption is applied with a mixture of true, false and unknown ground predicates [5, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 17, "context": "Often in machine learning some form of a local closed-world assumption is applied with a mixture of true, false and unknown ground predicates [5, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 33, "context": "In probabilistic databases [36] the canonical representation is used in tuple-independent databases, while multi-state random variables are used in block-independent-disjoint (BID) databases.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "Probabilistic relational models (PRMs) were one of the first published directed relational models and found great interest in the statistical machine learning community [17, 10].", "startOffset": 169, "endOffset": 177}, {"referenceID": 9, "context": "Probabilistic relational models (PRMs) were one of the first published directed relational models and found great interest in the statistical machine learning community [17, 10].", "startOffset": 169, "endOffset": 177}, {"referenceID": 9, "context": "Subsequently, PRMs have been extended to also consider the case that relationships between entities are unknown, which is called structural uncertainty in the PRM framework [10].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "A Bayesian logic program is defined as a set of Bayesian clauses [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Relational Bayesian networks [14] are related to Bayesian logic programs and use probability formulae for specifying conditional probabilities.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "The probabilistic entity-relationship (PER) models [12] are related to the PRM framework and use the entity-relationship model as a basis, which is often used in the design of a relational database.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "Relational dependency networks [22] also belong to the family of directed relational models and learn the dependency of a node given its Markov blanket (the smallest node set that make the node of interest independent of the remaining network).", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "by [11, 13].", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "by [11, 13].", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "Then L is defined as a set of pairs (Fi, wi) [32, 4].", "startOffset": 45, "endOffset": 52}, {"referenceID": 34, "context": "RMNs generalize many concepts of PRMs to undirected relational models [37].", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "Redrawn from [4].", "startOffset": 13, "endOffset": 16}, {"referenceID": 38, "context": "In the following discussion, we assume that data is in triple format; generalizations to relational databases haven been described [41, 19].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "In the following discussion, we assume that data is in triple format; generalizations to relational databases haven been described [41, 19].", "startOffset": 131, "endOffset": 139}, {"referenceID": 38, "context": "The infinite hidden relational model (IHRM) [41] (a.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "a infinite relational model [15]) is a generalization to a probabilistic mixture model where a latent variable with states 1, .", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "Models with a finite number of states have been studied as stochastic block models [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "The IHRM was the first relational model applied to trust learning [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "In [1] the IHRM is generalized to a mixed-membership stochastic block model, where entities can belong to several classes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "2 RESCAL: A Latent Factor Model The RESCAL model was introduced in [26] and follows a similar dependency structure as the IHRM as shown in Figure 5.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 222, "endOffset": 225}, {"referenceID": 30, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 257, "endOffset": 261}, {"referenceID": 39, "context": "[42] describes a Gaussian process-based approach for predicting a single relation type, which has been generalized to a mutli-relational setting in [40].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[42] describes a Gaussian process-based approach for predicting a single relation type, which has been generalized to a mutli-relational setting in [40].", "startOffset": 148, "endOffset": 152}, {"referenceID": 36, "context": "The SUNS approach [39] is based on a Tucker1 decomposition of the adjacency tensor, which can be computed by a singular value decomposition (SVD).", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "The Neural Tensor Network [34] combines several tensor decompositions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "Approaches with a smaller memory footprint are TransE [3] and HolE [25].", "startOffset": 54, "endOffset": 57}, {"referenceID": 24, "context": "Approaches with a smaller memory footprint are TransE [3] and HolE [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "The multiway neural network in the Knowledge Vault project [5] combines the strengths of latent factor models and neural networks and was successfully used in semi-automatic completion of knowledge graphs.", "startOffset": 59, "endOffset": 62}, {"referenceID": 23, "context": "[24] is a recent review on the application of relational learning to knowledge graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "As a number of publications have shown, best results can be achieved by committee solutions integrating factorization approaches with user defined or learned rule patterns [23, 5].", "startOffset": 172, "endOffset": 179}, {"referenceID": 4, "context": "As a number of publications have shown, best results can be achieved by committee solutions integrating factorization approaches with user defined or learned rule patterns [23, 5].", "startOffset": 172, "endOffset": 179}, {"referenceID": 4, "context": "The most interesting application in recent years was in projects involving large knowledge graphs, where performance and scalability could clearly be demonstrated [5, 24].", "startOffset": 163, "endOffset": 170}, {"referenceID": 23, "context": "The most interesting application in recent years was in projects involving large knowledge graphs, where performance and scalability could clearly be demonstrated [5, 24].", "startOffset": 163, "endOffset": 170}, {"referenceID": 6, "context": "The application of relational learning to sequential data and time series opens up new application areas, for example in clinical decision support and sensor networks [7, 8].", "startOffset": 167, "endOffset": 173}, {"referenceID": 7, "context": "The application of relational learning to sequential data and time series opens up new application areas, for example in clinical decision support and sensor networks [7, 8].", "startOffset": 167, "endOffset": 173}, {"referenceID": 35, "context": "[38] studies the relevance of relational learning to cognitive brain functions.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We provide a survey on relational models. Relational models describe complete networked domains by taking into account global dependencies in the data. Relational models can lead to more accurate predictions if compared to non-relational machine learning approaches. Relational models typically are based on probabilistic graphical models, e.g., Bayesian networks, Markov networks, or latent variable models. Relational models have applications in social networks analysis, the modeling of knowledge graphs, bioinformatics, recommendation systems, natural language processing, medical decision support, and linked data.", "creator": "LaTeX with hyperref package"}}}