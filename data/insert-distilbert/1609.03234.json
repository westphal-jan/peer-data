{"id": "1609.03234", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Reduced Space and Faster Convergence in Imperfect-Information Games via Regret-Based Pruning", "abstract": "counterfactual regret minimization ( cfr ) is the most popular iterative algorithm for solving zero - sum imperfect - information game games. regret - based pruning ( rbp ) is an improvement strategies that allows poorly - performing actions to be temporarily less pruned, thus speeding up cfr. we introduce total rbp, a new form of rbp that reduces the space requirements effectiveness of cfr as actions are pruned. we prove that in zero - sum games it asymptotically prunes any action that is essentially not part of a best response to losing some nash equilibrium. this decrease leads to provably faster convergence and lower space requirements. experiments together show that total rbp results in avoiding an order of magnitude reduction in space, and the reduction factor increases with game piece size.", "histories": [["v1", "Mon, 12 Sep 2016 00:30:54 GMT  (293kb,D)", "http://arxiv.org/abs/1609.03234v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["noam brown", "tuomas sandholm"], "accepted": true, "id": "1609.03234"}, "pdf": {"name": "1609.03234.pdf", "metadata": {"source": "CRF", "title": "Reduced Space and Faster Convergence in Imperfect- Information Games via Regret-Based Pruning", "authors": ["Noam Brown"], "emails": ["noamb@cmu.edu", "sandholm@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Imperfect-information extensive-form games model strategic multi-step scenarios between agents with hidden information, such as auctions, security interactions (both physical and virtual), negotiations, and military situations. Typically in imperfect-information games, one wishes to find a Nash equilibrium, which is a profile of strategies in which no player can improve her outcome by unilaterally changing her strategy. A linear program can find an exact Nash equilibrium in two-player zero-sum games containing fewer than about 108 nodes [5]. For larger games, iterative algorithms are used to converge to a Nash equilibrium. There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16]. CFR minimizes regret independently at each decision point in the game. CFR+, a variant of CFR, was used to essentially solve Limit Texas Hold\u2019em, the largest imperfect-information game ever to be essentially solved [1].\nBoth computation time and storage space are difficult challenges when solving large imperfectinformation games. For example, solving Limit Texas Hold\u2019em required nearly 8 million core hours and a complex, domain-specific streaming compression algorithm to store the 262 TiB of uncompressed data in only 10.9 TiB. This data had to be repeatedly decompressed from disk into memory and then compressed back to disk in order to run CFR+ [14].\nRegret Based Pruning (RBP) is an improvement to CFR that greatly reduces the computation time needed to solve large games by temporarily pruning suboptimal actions [2]. Specifically, if an action has negative regret, then RBP skips that action for the minimum number of iterations it would take for its regret to become positive in CFR. The skipped iterations are then \u201cmade up\u201d in a single iteration once pruning ends.\nIn this paper we introduce a new form of RBP which we coin Total RBP. It alters the starting and ending conditions for pruning, and changes the way regrets are updated once pruning ends. We refer to the prior form of RBP as Interval RBP to differentiate it from our new method.\nA primary advantage of Total RBP is that in addition to faster convergence, it also reduces the space requirements of CFR over time. Specifically, once pruning begins on a branch, Total RBP ensures\nar X\niv :1\n60 9.\n03 23\n4v 1\n[ cs\n.G T\n] 1\n2 Se\np 20\nthe regrets currently stored on that branch will never be needed again. In other words, all the data stored for a branch that is pruned can be discarded, and the space allocated to that branch can be freed. Space need not be reallocated for that branch until pruning ends and the branch cannot immediately be pruned again. In Section 3.1, we prove that after enough iterations are completed, space for certain pruned branches will never need to be allocated again. Specifically, we prove that Total RBP need only asymptotically store actions that have positive probability in a best response to a Nash equilibrium. This is extremely advantageous when solving large imperfect-information games, which are often constrained by space and in which the set of best response actions may be orders of magnitude smaller than the size of the game.\nWhile Total RBP still requires enough memory to store the entire game in the early iterations, recent work has shown that these early iterations can be skipped by first solving a low-memory abstraction of the game and then using its solution to warm starting CFR in the full game [4]. Total RBP\u2019s reduction in space is also helpful to the Simultaneous Abstraction and Equilibrium Finding (SAEF) algorithm [3], which starts CFR with a small abstraction of the game and progressively expands the abstraction while also solving the game. SAEF\u2019s space requirements increase the longer the algorithm runs, and may eventually exceed the constraints of a system. Total RBP can counter this increase in space by eliminating the need to store suboptimal paths of the game tree.\nWhile prior work on Interval RBP has shown empirical evidence of better performance, this paper proves that CFR converges faster when using Total RBP, because certain suboptimal actions will only need to be traversed O ( ln(T ) ) times over T iterations.\nThe magnitude of these gains in speed and space varies depending on the game. It is possible to construct games where Total RBP provides no benefit. However, if there are many suboptimal actions in the game\u2014as is frequently the case in large games\u2014Total RBP can speed up CFR by multiple orders of magnitude and require orders of magnitude less space. Our experiments show an order of magnitude space reduction already in medium-sized games, and a reduction factor increase with game size."}, {"heading": "2 Background", "text": "This section presents the notation used in the rest of the paper. An imperfect-information extensiveform game has a finite set of players, P . H is the set of all possible histories (nodes) in the game tree, represented as a sequence of actions, and includes the empty history. A(h) is the actions available in a history and P (h) \u2208 P \u222a c is the player who acts at that history, where c denotes chance. Chance plays an action a \u2208 A(h) with a fixed probability \u03c3c(h, a) that is known to all players. The history h\u2032 reached after action a in h is a child of h, represented by h \u00b7 a = h\u2032, while h is the parent of h\u2032. More generally, h\u2032 is an ancestor of h (and h is a descendant of h\u2032), represented by h\u2032 @ h, if there exists a sequence of actions from h\u2032 to h. Z \u2286 H are terminal histories for which no actions are available. For each player i \u2208 P , there is a payoff function ui : Z \u2192 <. If P = {1, 2} and u1 = \u2212u2, the game is two-player zero-sum. Define \u2206i = maxz\u2208Z ui(z)\u2212minz\u2208Z ui(z) and \u2206 = maxi \u2206i. Imperfect information is represented by information sets for each player i \u2208 P by a partition Ii of h \u2208 H : P (h) = i. For any information set I \u2208 Ii, all histories h, h\u2032 \u2208 I are indistinguishable to player i, so A(h) = A(h\u2032). I(h) is the information set I where h \u2208 I . P (I) is the player i such that I \u2208 Ii. A(I) is the set of actions such that for all h \u2208 I , A(I) = A(h). |Ai| = maxI\u2208Ii |A(I)| and |A| = maxi |Ai|. Define U(I) to be the maximum payoff reachable from a history in I , and L(I) to be the minimum. That is, U(I) = maxz\u2208Z,h\u2208I:hvz uP (I)(z) and L(I) = minz\u2208Z,h\u2208I:hvz uP (I)(z). Define \u2206(I) = U(I) \u2212 L(I) to be the range of payoffs reachable from a history in I . Similarly U(I, a), L(I, a), and \u2206(I, a) are the maximum, minimum, and range of payoffs (respectively) reachable from a history in I after taking action a. Define D(I, a) to be the set of information sets reachable by player P (I) after taking action a. Formally, I \u2032 \u2208 D(I, a) if for some history h \u2208 I and h\u2032 \u2208 I \u2032, h \u00b7 a v h\u2032 and P (I) = P (I \u2032). A strategy \u03c3i(I) is a probability vector over A(I) for player i in information set I . The probability of a particular action a is denoted by \u03c3i(I, a). Since all histories in an information set belonging to player i are indistinguishable, the strategies in each of them must be identical. That is, for all h \u2208 I , \u03c3i(h) = \u03c3i(I) and \u03c3i(h, a) = \u03c3i(I, a). Define \u03c3i to be a probability vector for player i over all available strategies \u03a3i in the game. A strategy profile \u03c3 is a tuple of strategies, one for each player.\nui(\u03c3i, \u03c3\u2212i) is the expected payoff for player i if all players play according to the strategy profile \u3008\u03c3i, \u03c3\u2212i\u3009. If a series of strategies are played over T iterations, then \u03c3\u0304Ti = \u2211 t\u2208T \u03c3 t i T .\n\u03c0\u03c3(h) = \u03a0h\u2032\u2192avh\u03c3P (h)(h, a) is the joint probability of reaching h if all players play according to \u03c3. \u03c0\u03c3i (h) is the contribution of player i to this probability (that is, the probability of reaching h if all players other than i, and chance, always chose actions leading to h). \u03c0\u03c3\u2212i(h) is the contribution of all players other than i, and chance. \u03c0\u03c3(h, h\u2032) is the probability of reaching h\u2032 given that h has been reached, and 0 if h 6@ h\u2032. In a perfect-recall game, \u2200h, h\u2032 \u2208 I \u2208 Ii, \u03c0i(h) = \u03c0i(h\u2032). In this paper we focus on perfect-recall games. Therefore, for i = P (I) define \u03c0i(I) = \u03c0i(h) for h \u2208 I . Moreover, I \u2032 @ I if for some h\u2032 \u2208 I \u2032 and some h \u2208 I , h\u2032 @ h. Similarly, I \u2032 \u00b7 a @ I if h\u2032 \u00b7 a @ h. The average\nstrategy \u03c3\u0304Ti (I) for an information set I is defined as \u03c3\u0304 T i (I) =\n\u2211 t\u2208T \u03c0 \u03c3ti i (I)\u03c3\nt i(I)\u2211\nt\u2208T \u03c0 \u03c3t i (I)\n.\nA best response to \u03c3\u2212i is a strategy \u03c3\u2217i such that ui(\u03c3 \u2217 i , \u03c3\u2212i) = max\u03c3\u2032i\u2208\u03a3i ui(\u03c3 \u2032 i, \u03c3\u2212i). A Nash equilibrium \u03c3\u2217 is a strategy profile where every player plays a best response: \u2200i, ui(\u03c3\u2217i , \u03c3\u2217\u2212i) = max\u03c3\u2032i\u2208\u03a3i ui(\u03c3 \u2032 i, \u03c3 \u2217 \u2212i). A Nash equilibrium strategy for player i as a strategy \u03c3i that is part of any Nash equilibrium. In two-player zero-sum games, if \u03c3i and \u03c3\u2212i are both Nash equilibrium strategies, then \u3008\u03c3i, \u03c3\u2212i\u3009 is a Nash equilibrium. An -equilibrium as a strategy profile \u03c3\u2217 such that \u2200i, ui(\u03c3\u2217i , \u03c3\u2217\u2212i) + \u2265 max\u03c3\u2032i\u2208\u03a3i ui(\u03c3 \u2032 i, \u03c3 \u2217 \u2212i)."}, {"heading": "2.1 Counterfactual Regret Minimization", "text": "Counterfactual Regret Minimization (CFR) is a popular algorithm for extensive-form games in which the strategy vector for each information set is determined according to a regret-minimization algorithm [16]. We use regret matching (RM) [6] as the regret-minimization algorithm.\nThe analysis of CFR makes frequent use of counterfactual value. Informally, this is the expected utility of an information set given that player i tries to reach it. For player i at information set I given a strategy profile \u03c3, this is defined as\nv\u03c3(I) = \u2211 h\u2208I ( \u03c0\u03c3\u2212i(h) \u2211 z\u2208Z ( \u03c0\u03c3(h, z)ui(z) )) (1)\nThe counterfactual value of an action a is v\u03c3(I, a) = \u2211 h\u2208I ( \u03c0\u03c3\u2212i(h) \u2211 z\u2208Z ( \u03c0\u03c3(h \u00b7 a, z)ui(z) )) (2)\nA counterfactual best response [10] (CBR) is a strategy similar to a best response, except that it maximizes counterfactual value even at information sets that it does not reach due to its earlier actions. Specifically, a counterfactual best response to \u03c3\u2212i is a strategy CBR(\u03c3\u2212i) such that if CBR(\u03c3\u2212i)(I, a) > 0 then v\u3008CBR(\u03c3\u2212i),\u03c3\u2212i\u3009(I, a) = maxa\u2032 v\u3008CBR(\u03c3\u2212i),\u03c3\u2212i\u3009(I, a\u2032). The counterfactual best response value CBV \u03c3\u2212i(I) is similar to counterfactual value, except that player i = P (I) plays according to a CBR to \u03c3\u2212i. Formally, CBV \u03c3\u2212i(I) = v\u3008CBRi(\u03c3\u2212i),\u03c3\u2212i\u3009(I).\nLet \u03c3t be the strategy profile used on iteration t. The instantaneous regret on iteration t for action a in information set I is rt(I, a) = v\u03c3 t\n(I, a) \u2212 v\u03c3t(I) and the regret for action a in I on iteration T is RT (I, a) = \u2211 t\u2208T r\nt(I, a). Additionally, RT+(I, a) = max{RT (I, a), 0} and RT (I) = maxa{RT+(I, a)}. Our analysis of Total RBP will occasionally reference the potential function of R(I), defined as \u03a6(RT (I)) = \u2211 a\u2208A(I) ( RT+(I, a) )2 . Regret for player i in the entire\ngame is RTi = max\u03c3\u2032i\u2208\u03a3i \u2211 t\u2208T ( ui(\u03c3 \u2032 i, \u03c3 t \u2212i)\u2212 ui(\u03c3ti , \u03c3t\u2212i) ) .\nIn regret matching, a player picks a distribution over actions in an information set in proportion to the positive regret on those actions. Formally, on each iteration T + 1, player i selects actions a \u2208 A(I) according to probabilities\n\u03c3T+1(I, a) =  RT+(I,a)\u2211 a\u2032\u2208A(I) R T +(I,a \u2032) , if \u2211 a\u2032 R T +(I, a \u2032) > 0\n1 |A(I)| , otherwise\n(3)\nIf a player plays according to RM on every iteration then on iteration T , RT (I) \u2264 \u2206(I) \u221a |A(I)| \u221a T .\nIf a player plays according to CFR in every iteration then RTi \u2264 \u2211 I\u2208Ii R\nT (I). So, as T \u2192 \u221e, RTi T \u2192 0. In two-player zero-sum games, if both players\u2019 average regret RTi T \u2264 , their average strategies \u3008\u03c3\u0304T1 , \u03c3\u0304T2 \u3009 form a 2 -equilibrium [15]. Thus, CFR constitutes an anytime algorithm for finding an -Nash equilibrium in zero-sum games."}, {"heading": "2.2 Partial Pruning and Interval Regret-Based Pruning", "text": "This section reviews forms of pruning that allow parts of the game tree to be skipped in CFR. Typically, regret is updated by traversing each node in the game tree separately for each player, and calculating the contribution of a history h \u2208 I to rt(I, a) for each action a \u2208 A(I). If a history h is reached in which \u03c0\u03c3 t\n\u2212i(h) = 0 (that is, an opponent\u2019s reach is zero), then from (1) and (2) the strategy at h contributes nothing on iteration t to the regret of I(h) (or to the information sets above it). Moreover, any history that would be reached beyond h would also contribute nothing to its information set\u2019s regret because \u03c0\u03c3 t\n\u2212i(h \u2032) = 0 for every history h\u2032 where h @ h\u2032 and P (h\u2032) = P (h).\nThus, when traversing the game tree for player i, there is no need to traverse beyond any history h when \u03c0\u03c3 t\n\u2212i(h) = 0. The benefit of this form of pruning, which we refer to as partial pruning, varies depending on the game, but empirical results show a factor of 30 improvement in some games [9].\nWhile partial pruning allows one to prune paths that an opponent reaches with zero probability, interval regret-based pruning (Interval RBP) allows one to also prune paths that the traverser reaches with zero probability [2]. However, this pruning is necessarily temporary. Consider an action a \u2208 A(I) such that \u03c3t(I, a) = 0, and assume that it is known action a will not be played with positive probability until some far-future iteration t\u2032 (in RM, this would be the case if Rt(I, a) 0). Since action a is played with zero probability on iteration t, so from (1) the strategy played and reward received following action a (that is, in D(I, a)) will not contribute to the regret for any information set preceding action a on iteration t. In fact, what happens in D(I, a) has no bearing on the rest of the game tree until iteration t\u2032 is reached. So one could, in theory, \u201cprocrastinate\u201d in deciding what happened beyond action a on iteration t, t+ 1, ..., t\u2032 \u2212 1 until iteration t\u2032. However, upon reaching iteration t\u2032, rather than individually making up the t\u2032 \u2212 t iterations over D(I, a), one can instead do a single iteration, playing against the average of the opponents\u2019 strategies in the t\u2032\u2212 t iterations that were missed, and declare that strategy was played on all the t\u2032\u2212 t iterations. This accomplishes the work of the t\u2032\u2212 t iterations in a single traversal. Moreover, since player i never plays action a with positive probability between iterations t and t\u2032, that means every other player can apply partial pruning on that part of the game tree for iterations t\u2032 \u2212 t, and skip it completely. This, in turn, means that player i has free rein to play whatever they want in D(I, a) without affecting the regrets of the other players. In light of that, and of the fact that player i gets to decide what is played in D(I, a) after knowing what the other players have played, player i might as well play a strategy that ensures zero regret for all information sets I \u2032 \u2208 D(I, a) in the iterations t to t\u2032. A CBR to the average of the opponent strategies on the t\u2032 \u2212 t iterations would qualify as such a zero-regret strategy. Interval regret-based pruning only allows a player to skip traversing D(I, a) for as long as \u03c3t(I, a) = 0. Thus, in RM, if Rt0(I, a) < 0, we can prune the game tree beyond action a from iteration t0 until iteration t1 so long as \u2211t0 t=1 v \u03c3t(I, a) + \u2211t1 t=t0+1 \u03c0\u03c3 t \u2212i(I)U(I, a) \u2264 \u2211t1 t=1 v \u03c3t(I).\n3 Total RBP: A New Form of Regret-Based Pruning This section introduces a new form of RBP which we coin Total RBP. When pruning ends and regret must be updated in the pruned branch, Interval RBP calculates a CBR to the average opponent strategy over the skipped iterations, and updates regret in the pruned branch as if that CBR strategy were played on each of the skipped iterations. By contrast, when pruning ends in Total RBP, it calculates a CBR in the pruned branch against the opponent\u2019s average strategy over all iterations played so far, and sets regret as if that CBR strategy were played on every iteration played in the game so far\u2014even those that were played before pruning began.\nWhile using a CBR works correctly in Total RBP, it is also sound to choose a strategy that is almost a CBR (formalized later in this section), as long as that strategy does not result in a violation of the CFR bound on the potential function \u03a6(RT (I)) of any information set I . In practice, this means that the strategy is close to a CBR, and approaches a CBR as T \u2192 \u221e. We now present the theory to show that such a near-CBR can be used. However, in practice CFR converges much faster than the\ntheoretical bound, so the potential function is typically far lower than the theoretical bound. Thus, while choosing a near-CBR rather than an exact CBR may allow for slightly longer pruning according to the theory, it may actually result in worse performance. All of the theoretical results presented in this paper, including the improved convergence bound as well as the lower space requirements, still hold if only a CBR is used, and our experiments use a CBR. Nevertheless, clever heuristics for deciding on a near-CBR may lead to even better performance in practice.\nWe define a strategy NBR(\u03c3\u2212i, T ) as a T -near counterfactual best response (T -near CBR) to \u03c3\u2212i if for all I belonging to player i\u2211\na\u2208A(I)\n( v\u3008NBR(\u03c3\u2212i,T ),\u03c3\u2212i\u3009(I, a)\u2212 v\u3008NBR(\u03c3\u2212i,T ),\u03c3\u2212i\u3009(I) )2 + \u2264 x T I\nT 2 (4)\nwhere xTI can be any value in the range 0 \u2264 xTI \u2264 yTI and yTI is the CFR bound on \u03a6(RT (I)). If xTI = 0, then a T -near CBR is always a CBR. The set of strategies that are T -near CBRs to \u03c3\u2212i is represented as \u03a3NBR(\u03c3\u2212i, T ). We also define the T -near counterfactual best response value as NBV \u03c3\u2212i,T (I, a) = min\u03c3\u2032i\u2208\u03a3NBR(\u03c3\u2212i,T ) v\n\u3008\u03c3\u2032i,\u03c3\u2212i\u3009(I, a) and NBV \u03c3\u2212i,T (I) = min\u03c3\u2032i\u2208\u03a3NBR(\u03c3\u2212i,T ) v \u3008\u03c3\u2032i,\u03c3\u2212i\u3009(I).\nIn Total RBP, an action is pruned only if it would still have negative regret had a T -near CBR against the opponent\u2019s average strategy been played on every iteration. Specifically, on iteration T of CFR with RM, if\nT ( NBV \u03c3\u0304 T \u2212i,T (I, a) ) \u2264 T\u2211 t=1 v\u03c3 t (I) (5)\nthen D(I, a) can be pruned for\nT \u2032 =\n\u2211T t=1 v \u03c3t(I)\u2212NBV \u03c3\u0304 T \u2212i,T (I, a)\nU(I, a)\u2212 L(I) (6)\niterations. After those T \u2032 iterations are over, we calculate a T + T \u2032-near CBR in D(I, a) to the opponent\u2019s average strategy and set regret as if that T + T \u2032-near CBR had been played on every iteration. Specifically, for each t \u2264 T + T \u2032 we set1 v\u03c3t(I, a) = NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) so that\nRT+T \u2032 (I, a) = ( T + T \u2032 )( NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) ) \u2212 T+T \u2032\u2211 t=1 v\u03c3 t (I) (7)\nand for every information set I \u2032 \u2208 D(I, a) we set v\u03c3t(I \u2032, a\u2032) = NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032, a\u2032) and\nv\u03c3 t (I \u2032) = NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032) so that\nRT+T \u2032 (I \u2032, a\u2032) = ( T + T \u2032 )( NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032, a\u2032)\u2212NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032) )\n(8)\nTheorem 1 proves that if (5) holds for some action, then the action can be pruned for T \u2032 iterations, where T \u2032 is defined in (6). The same theorem holds if one replaces the T -near counterfactual best response values with counterfactual best response values. The proof for Theorem 1 draws from recent work on warm starting CFR using only an average strategy profile [4]. Essentially, we warm start regrets in the pruned branch using only the average strategy of the opponent and knowledge of T . Theorem 1. Assume T iterations of CFR with RM have been played in a two-player zerosum game and assume T ( NBV \u03c3\u0304 T \u2212i,T (I, a) ) \u2264 \u2211T t=1 v \u03c3t(I) where P (I) = i. Let T \u2032 =\nb \u2211T t=1 v\n\u03c3t (I)\u2212T ( NBV \u03c3\u0304T\u2212i,T (I,a) )\nU(I,a)\u2212L(I) c. If both players play according to CFR with RM for the next T \u2032 iterations in all information sets I \u2032\u2032 6\u2208 D(I, a) except that \u03c3(I, a) is set to zero and \u03c3(I) is renormalized, then (T + T \u2032) ( NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) ) \u2264 \u2211T+T \u2032 t=1 v \u03c3t(I). Moreover, if one then sets v\u03c3 t (I, a) = NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) for each t \u2264 T+T \u2032 and v\u03c3t(I \u2032, a\u2032) = NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I \u2032, a\u2032) for each I \u2032 \u2208 D(I, a), then after T \u2032\u2032 additional iterations of CFR with RM, the bound on exploitability of \u03c3\u0304T+T \u2032+T \u2032\u2032 is no worse than having played T + T \u2032 + T \u2032\u2032 iterations of CFR with RM without RBP.\n1In practice, only the sums \u2211T t=1 v \u03c3t(I) and either \u2211T t=1 v \u03c3t(I, a) or RT (I, a) are stored.\nIn practice, rather than check whether (5) is met for an action on every iteration, one could only check actions that have very negative regret, and do a check only once every several iterations. This would still be safe and would save some computational cost of the checks, but would lead to less pruning.\nAs is the case with Interval RBP, the duration of pruning can be increased by giving up knowledge beforehand of exactly how many iterations can be skipped. From (2) and (1) we see that rT (I, a) \u2264 \u03c0\u03c3 t \u2212i(I) ( U(I, a) \u2212 L(I) ) . Thus, if \u03c0\u03c3 t\n\u2212i(I) is very low, then (5) would continue to hold for more iterations than (6) guarantees. Specifically, we can prune D(I, a) from iteration t0 until iteration t1 as long as\nt0 ( NBV \u03c3\u0304 t0 \u2212i,t0(I, a) ) + t1\u2211 t=t0+1 \u03c0\u03c3 t \u2212i(I)U(I, a) \u2264 t1\u2211 t=1 v\u03c3 t (I) (9)"}, {"heading": "3.1 Total RBP Requires Less Space", "text": "A key advantage of Total RBP is that setting the new regrets according to (7) and (8) requires no knowledge of what the regrets were before pruning began. Thus, once pruning begins, all the regrets in D(I, a) can be discarded and the space that was allocated to storing the regret can be freed. That space need only be re-allocated once (9) ceases to hold and we cannot immediately begin pruning again (that is, (5) does not hold). Theorem 2 proves that for any information set I and action a \u2208 A(I) that is not part of a best response to a Nash equilibrium, there is an iteration TI,a such that for all T \u2265 TI,a, action a in information set I (and its descendants) can be pruned.2 Thus, once this TI,a is reached, it will never be necessary to allocate space for regret in D(I, a) again. Theorem 2. In a two-player zero-sum game, if for every opponent Nash equilibrium strategy \u03c3\u2217\u2212P (I), CBV \u03c3 \u2217 \u2212P (I)(I, a) < CBV \u03c3 \u2217 \u2212P (I)(I), then there exists a TI,a and \u03b4I,a > 0 such that after T \u2265 TI,a\niterations of CFR, CBV \u03c3\u0304 T \u2212i(I, a)\u2212\n\u2211T t=1 v \u03c3t (I)\nT \u2264 \u2212\u03b4I,a.\nWhile such a constant TI,a exists for any suboptimal action, Total RBP cannot determine whether or when TI,a is reached. So, it is still necessary to check whether (5) is satisfied whenever (9) no longer holds, and to recalculate how much longer D(I, a) can safely be pruned. This requires the algorithm to periodically calculate a best response (or near-best response) in D(I, a). However, this (near-)best response calculation does not require knowledge of regret in D(I, a), so it is still never necessary to store regret after iteration TI,a is reached.\nWhile it is possible to discard regrets in D(I, a) without penalty once pruning begins, regret is only half the space requirement of CFR. Every information set I also stores a sum of the strategies played\u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I) ) which is normalized once CFR ends in order to calculate \u03c3\u0304T (I). Fortunately, if action a in information set I is pruned for long enough, then the stored cumulative strategy in D(I, a) can also be discarded at the cost of a small increase in the distance of the final average strategy from a Nash equilibrium. Specifically, if \u03c0\u03c3\u0304 T\ni (I, a) \u2264 C\u221aT , where C is some constant, then setting \u03c3\u0304T (I, a) = 0 and renormalizing \u03c3\u0304T (I), and setting \u03c3\u0304T (I \u2032, a\u2032) = 0 for I \u2032 \u2208 D(I, a), can result in at most C|I|\u2206\u221a\nT higher exploitability for the whole strategy \u03c3\u0304T . Since CFR only guarantees that \u03c3\u0304T\nis a 2|I|\u2206 \u221a |A|\u221a\nT -Nash equilibrium anyway, C|I|\u2206\u221a T is only a constant factor of the bound. If an action is pruned from T \u2032 to T , then \u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I, a) ) \u2264 T \u2032\nT . Thus, if an action is pruned for long enough, then eventually \u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I, a) ) \u2264 C\u221a T for any C, so \u2211T t=1 ( \u03c0\u03c3 t i (I)\u03c3 t(I, a) ) could be set to zero (as well as all descendants of I \u00b7 a), while suffering at most a constant factor increase in exploitability. As more iterations are played, this penalty will continue to decrease and eventually be negligible. The constant C can be set by the user: a higher C allows the average strategy to be discarded sooner, while a lower C reduces the potential penalty in exploitability.\nWe define IS as the set of information sets that are not guaranteed to be asymptotically pruned by Theorem 2. Specifically, I \u2208 IS if I 6\u2208 D(I \u2032, a\u2032) for some I \u2032 and a\u2032 \u2208 A(I \u2032) such that for every opponent Nash equilibrium strategy \u03c3\u2217\u2212P (I\u2032), CBV \u03c3\u2217\u2212P (I\u2032)(I \u2032, a\u2032) < CBV \u03c3 \u2217 \u2212P (I\u2032)(I \u2032). Theorem 2 implies the following. 2If CFR converges to a particular Nash equilibrium, then this condition could be broadened to any information set I and action a \u2208 A(I) that is not a best response to that particular Nash equilibrium. While empirically CFR does appear to always converge to a particular Nash equilibrium, there is no known proof that it always does so.\nCorollary 1. In a two-player zero-sum game with some threshold on the average strategy C\u221a T for C > 0, after a finite number of iterations, CFR with Total RBP requires only O ( |IS ||A| ) space."}, {"heading": "3.2 Total RBP Has a Better Convergence Bound", "text": "We now prove that Total RBP in CFR speeds up convergence to an -Nash equilibrium. Section 3 proved that CFR with Total RBP converges in the same number of iterations as CFR alone. In this section, we prove that Total RBP allows each iteration to be traversed more quickly. Specifically, if an action a \u2208 A(I) is not a CBR to a Nash equilibrium, then D(I, a) need only be traversed O(ln(T )) times over T iterations. Intuitively, as both players converge to a Nash equilibrium, actions that are not a counterfactual best response will eventually do worse than actions that are, so those suboptimal actions will accumulate increasing amounts of negative regret. This negative regret allows the action to be safely pruned for increasingly longer periods of time.\nSpecifically, let S \u2286 H be the set of histories where h \u00b7 a \u2208 S if h \u2208 S and action a is part of some CBR to some Nash equilibrium. Formally, S contains \u2205 and every history h \u00b7 a such that h \u2208 S and CBV \u03c3 \u2217 \u2212P (I)(I, a) = CBV \u03c3 \u2217 \u2212P (I)(I) for some Nash equilibrium \u03c3\u2217.\nTheorem 3. In a two-player zero-sum game, if both players choose strategies according to CFR with Total RBP, then conducting T iterations requires only O ( |S|T + |H| ln(T ) ) nodes to be traversed.\nThe definition of S uses properties of the Nash equilibria of the game, and an action a \u2208 A(I) not in S is only guaranteed to be pruned by RBP after some TI,a is reached, which also depends on the Nash equilibria of the game. Since CFR converges to only an -Nash equilibrium, CFR cannot determine with certainty which nodes are in S or when TI,a is reached. Nevertheless, both S and TI,a are fixed properties of the game."}, {"heading": "4 Experiments", "text": "We compare Total RBP to Interval RBP, to only partial pruning, and to no pruning at all. We also show the amount of space used by Total RBP over the course of the run. The experiments are conducted on Leduc Hold\u2019em [13] and Leduc-5 [2]. Leduc Hold\u2019em is a common benchmark in imperfect-information game solving because it is small enough to be solved but still strategically complex. In Leduc Hold\u2019em, there is a deck consisting of six cards: two each of Jack, Queen, and King. There are two rounds. In the first round, each player places an ante of 1 chip in the pot and receives a single private card. A round of betting then takes place with a two-bet maximum, with Player 1 going first. A public shared card is then dealt face up and another round of betting takes place. Again, Player 1 goes first, and there is a two-bet maximum. If one of the players has a pair with the public card, that players wins. Otherwise, the player with the higher card wins. The bet size in the first round is 2 chips, and 4 chips in the second round. Leduc-5 is like Leduc Hold\u2019em but larger: there are 5 bet sizes to choose from. In the first round a player may bet 0.5, 1, 2, 4, or 8 chips, while in the second round a player may bet 1, 2, 4, 8, or 16 chips.\nResults are presented for both CFR and CFR+. Nodes touched is a hardware and implementationindependent proxy for time. Overhead costs are counted in nodes touched. CFR+ is a variant of CFR in which a floor on regret is set at zero and each iteration is weighted linearly in the average strategy (that is, iteration t is weighted by t) rather than each iteration being weighted equally. Since Interval RBP can only prune negative-regret actions, Interval RBP modifies the definition of CFR+ so that regret can be negative, but immediately jumps up to zero as soon as regret increases. Total RBP does not require this modification. Both forms of RBP modify the behavior of CFR+ because without pruning, CFR+ would put positive probability on an action as soon as its regret increases, while RBP waits until pruning is over. This is not, by itself, a problem. However, CFR+\u2019s linear weighting of the average strategy is only guaranteed to converge to a Nash equilibrium if pruning does not occur. While pruning does well empirically with CFR+, the convergence is noisy. This noise can be reduced by using the lowest-exploitability average strategy profile found so far. We do this in the experiments.\nFigure 1 shows the reduction in space needed to store the average strategy and regrets for Total RBP\u2014for various values of the constant threshold C, where an action\u2019s probability is set to zero if it is reached with probability less than C\u221a\nT in the average strategy, as we explained in Section 3.1. In\nboth games, a threshold between 0.01 and 0.1 performed well in both space and number of iterations,\nwith the lower thresholds converging somewhat faster and the higher thresholds reducing space faster. We also tested thresholds below 0.01, but the speed of convergence was essentially the same as when using 0.01. In Leduc, all variants resulted in a quick drop-off in space to about half the initial amount. In Leduc-5, a threshold of 0.1 resulted in a factor of 10 reduction in space for CFR+, and a factor of 7 reduction for CFR. In the case of CFR, this space reduction factor appears to continue to increase.\nFigure 2 compares the convergence rates of Total RBP, Interval RBP, and only partial pruning for both CFR and CFR+. In Leduc, Total RBP and Interval RBP perform comparably when added to CFR. When added to CFR+, Interval RBP does significantly better. In Leduc-5, which is a far larger game, Total RBP outperforms Interval RBP by a factor of 2 when added to CFR. When added to CFR+, Total RBP initially does far better but its performance is eventually surpassed by Interval RBP. This may be due to the noisy performance of CFR+ with RBP."}, {"heading": "5 Conclusions", "text": "We introduced Total RBP, a new form of regret-based pruning that provably reduces both the space needed to solve an imperfect-information game and the time needed to reach an -Nash equilibrium. This addresses both of the major bottlenecks in solving large imperfect-information games. Experimentally, Total RBP reduced the space needed to solve a game by an order of magnitude, with the reduction factor increasing with game size."}, {"heading": "A Lemma 1", "text": "Lemma 1 proves that if (5) is satisfied for some action a \u2208 A(I) on iteration T , then the value of action a and all its descendants on every iteration played so far can be set to the T -near counterfactual best response value. The same lemma holds if one replaces the T -near counterfactual best response values with exact counterfactual best response values. The proof for Lemma 1 draws from recent work on warm starting CFR using only an average strategy profile [4]. Lemma 1. Assume T iterations of CFR with RM have been played in a two-player zero-sum game. If T ( NBV \u03c3\u0304 T \u2212i,T (I, a) ) \u2264 \u2211T t=1 v \u03c3t(I) and one sets v\u03c3 t (I, a) = NBV \u03c3\u0304 T \u2212i,T (I, a) for each t \u2264 T and for each I \u2032 \u2208 D(I, a) sets v\u03c3t(I \u2032, a\u2032) = NBV \u03c3\u0304 T \u2212i,T (I \u2032, a\u2032) and v\u03c3 t (I \u2032) = NBV \u03c3\u0304 T \u2212i,T (I \u2032) then after T \u2032 additional iterations of CFR with RM, the bound on exploitability of \u03c3\u0304T+T \u2032\nis no worse than having played T + T \u2032 iterations of CFR with RM unaltered. Proof. The proof builds upon Theorem 2 in [4]. Assume T ( NBV \u03c3\u0304 T \u2212i,T (I, a) ) \u2264 \u2211T t=1 v\n\u03c3t(I). We wish to warm start to T iterations. For each I \u2032 \u2208 D(I, a) set v\u03c3t(I \u2032, a\u2032) = NBV \u03c3\u0304 T \u2212i,T (I \u2032, a\u2032) and v\u03c3 t (I \u2032) = NBV \u03c3\u0304 T \u2212i,T (I \u2032) and set v\u03c3 t (I, a) = NBV \u03c3\u0304 T \u2212i,T (I, a) for all t \u2264 T . For every other action, leave regret unchanged. For each I \u2032 \u2208 D(I, a) we know by construction that \u03a6(RT (I \u2032)) is within the CFR bound yTI\u2032 after changing regret. By assumption T ( NBV \u03c3\u0304 T \u2212i,T (I, a) ) \u2264\u2211T\nt=1 v \u03c3t(I), so RT (I, a) \u2264 0 and therefore \u03a6(RT (I)) is unchanged. Finally, since the T iterations were played according to CFR with RM and regret is unchanged for every other information set I \u2032\u2032, so the conditions for Theorem 2 in [4] hold for every information set, and therefore we can warm start to T iterations of CFR with RM with no penalty to the convergence bound."}, {"heading": "B Proof of Theorem 1", "text": "Proof. From Lemma 1 we can immediately set regret for a \u2208 A(I) to v\u03c3t(I, a) = NBV \u03c3\u0304 T \u2212i,T (I, a). By construction of T \u2032, Rt(I, a) is guaranteed to be nonpositive for T \u2264 t \u2264 T + T \u2032 and therefore \u03c3t(I, a) = 0. Thus, \u03c3\u0304T+T \u2032\ni (I \u2032) for I \u2032 \u2208 D(I, a) is identical regardless of what is played in D(I, a)\nduring T \u2264 t \u2264 T + T \u2032.\nSince (T + T \u2032) ( NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) ) \u2264 T ( NBV \u03c3\u0304 T \u2212i,T (I, a) ) + T \u2032 ( U(I, a) ) and \u2211T+T \u2032 t=1 v \u03c3t(I) \u2265 \u2211T t=1 v \u03c3t(I) + T \u2032 ( L(I) ) , so by the definition of T \u2032,\n(T + T \u2032) ( NBV \u03c3\u0304 T+T \u2032 \u2212i ,T+T \u2032 (I, a) ) \u2264 \u2211T+T \u2032 t=1 v \u03c3t(I). So if regrets in D(I, a) and RT+T \u2032 (I, a) are set according to Lemma 1, then after T \u2032\u2032 additional iterations of CFR with RM, the bound on exploitability of \u03c3\u0304T+T\n\u2032+T \u2032\u2032 is no worse than having played T + T \u2032 + T \u2032\u2032 iterations of CFR with RM from scratch."}, {"heading": "C Proof of Theorem 2", "text": "Proof. Consider an information set I and action a \u2208 A(I) where for every opponent Nash equilibrium strategy \u03c3\u2217\u2212P (I), CBV \u03c3\u2217\u2212P (I)(I, a) < CBV \u03c3 \u2217 \u2212P (I)(I). Let i = P (I). Let \u03b4 =\nmin\u03c3\u2212i\u2208\u03a3\u2217 ( CBV \u03c3\u2212i(I) \u2212 CBV \u03c3\u2212i(I, a) ) where \u03a3\u2217 is the set of Nash equilibria. Let \u03c3\u2032\u2212i = arg max\u03c3\u2212i\u2208\u03a3\u2212i|CBV \u03c3\u2212i (I)\u2212CBV \u03c3\u2212i (I,a)\u2264 3\u03b44 u\u2212i(\u03c3\u2212i, BR(\u03c3\u2212i)) Since \u03c3\u2032\u2212i is not a Nash equilibrium strategy and CFR converges to a Nash equilibrium strategy for both players, so there exists a T\u03b4 such that for all T \u2265 T\u03b4 , CBV \u03c3\u0304 T \u2212i(I)\u2212CBV \u03c3\u0304 T \u2212i(I, a) > 3\u03b44 . Let T \u2032 I,a = 4|I|2\u22062|A| \u03b42 . For T \u2265 T \u2032 I,a\nsince RTi \u2264 \u2211 I\u2208Ii R T (I), so CBV \u03c3\u0304 T \u2212i(I) \u2212 \u2211T t=1 v \u03c3t(I) \u2264 \u03b42 . Let TI,a = max(T \u2032 I,a, T\u03b4) and\n\u03b4I,a = \u03b4 4 . Then for T \u2265 TI,a, CBV\n\u03c3\u0304T\u2212i(I, a)\u2212 \u2211T t=1 v \u03c3t (I)\nT \u2264 \u2212\u03b4I,a."}, {"heading": "D Proof of Corollary 1", "text": "Proof. Let I 6\u2208 IS . Then I \u2208 D(I \u2032, a\u2032) for some I \u2032 and a\u2032 \u2208 A(I \u2032) such that for every opponent Nash equilibrium strategy \u03c3\u2217\u2212P (I\u2032), CBV \u03c3\u2217\u2212P (I\u2032)(I \u2032, a\u2032) < CBV \u03c3 \u2217 \u2212P (I\u2032)(I \u2032). Applying Theorem 2, this\nmeans there exists a TI\u2032,a\u2032 and \u03b4I\u2032,a\u2032 > 0 such that for T \u2265 TI\u2032,a\u2032 , CBV \u03c3\u0304 T \u2212i(I \u2032, a\u2032)\u2212\n\u2211T t=1 v \u03c3t (I\u2032)\nT \u2264 \u2212\u03b4I\u2032,a\u2032 . So (5) always applies for T \u2265 TI\u2032,a\u2032 for I \u2032 and a\u2032 and I will always be pruned. Since (8) does not require knowledge of regret, it need not be stored for I .\nSince D(I \u2032, a\u2032) will always be pruned for T \u2265 TI\u2032,a\u2032 , so for any T \u2265 (TI\u2032,a\u2032 )\n2\nC2 iterations for some constant C > 0, \u03c0\u03c3\u0304 T\ni (I) \u2264 C\u221aT , which satisfies the threshold of the average strategy. Thus, the average strategy in D(I, a) can be discarded."}, {"heading": "E Lemma 2", "text": "Lemma 2. If for all T \u2265 T \u2032 iterations of CFR with RBP, T ( CBV \u03c3\u0304 T (I, a) ) \u2212 \u2211T t=1 v\n\u03c3t(I) \u2264 \u2212xT for some x > 0, then any history h\u2032 such that h \u00b7 a v h\u2032 for some h \u2208 I need only be traversed at most O ( ln(T ) ) times. Proof. Let a \u2208 A(I) be an action such that for all T \u2265 T \u2032, T ( CBV \u03c3\u0304 T (I, a) ) \u2212 \u2211T t=1 v\n\u03c3t(I) \u2264 \u2212xT for some x > 0. NBV \u03c3\u0304 T \u2212i,T (I, a) \u2264 CBV \u03c3\u0304 T \u2212i , so from Theorem 1, D(I, a) can be pruned for m \u2265 b xTU(I,a)\u2212L(I)c iterations on iteration T . Thus, over iterations T \u2264 t \u2264 T +m, only a constant number of traversals must be done. So each iteration requires only Cm work when amortized, where C is a constant. Since x, U(I, a), and L(I) are constants, so on each iteration t \u2265 T \u2032, only an average of Ct traversals of D(I, a) is required. Summing over all t \u2264 T for T \u2265 T \u2032, and recognizing that T \u2032\nis a constant, we get that action a is only taken O ( ln(T ) ) over T iterations. Thus, any history h\u2032\nsuch that h \u00b7 a v h\u2032 for some h \u2208 I need only be traversed at most O ( ln(T ) ) times."}, {"heading": "F Proof of Theorem 3", "text": "Proof. Consider an h\u2217 6\u2208 S. Then there exists some h \u00b7 a v h\u2217 such that h \u2208 S but h \u00b7 a 6\u2208 S. Let I = I(h) and i = P (I). Since h \u00b7 a 6\u2208 S but h \u2208 S, so for every Nash equilibrium \u03c3\u2217, CBV \u03c3 \u2217 (I, a) < CBV \u03c3 \u2217 (I). From Theorem 2, there exists a TI,a and \u03b4I,a > 0 such that after\nT \u2265 TI,a iterations of CFR, CBV \u03c3\u0304 T \u2212i(I, a)\u2212\n\u2211T t=1 v \u03c3t(I)\nT \u2264 \u2212\u03b4I,a. Thus from Lemma 2, h \u2217 need only be traversed at most O ( ln(T ) ) times."}], "references": [{"title": "Heads-up limit hold\u2019em poker is solved", "author": ["Michael Bowling", "Neil Burch", "Michael Johanson", "Oskari Tammelin"], "venue": "Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Regret-based pruning in extensive-form games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Simultaneous abstraction and equilibrium finding in games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Strategy-based warm starting for regret minimization in games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": "In AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Lossless abstraction of imperfect information games", "author": ["Andrew Gilpin", "Tuomas Sandholm"], "venue": "Journal of the ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A simple adaptive procedure leading to correlated", "author": ["Sergiu Hart", "Andreu Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Smoothing techniques for computing Nash equilibria of sequential games", "author": ["Samid Hoda", "Andrew Gilpin", "Javier Pe\u00f1a", "Tuomas Sandholm"], "venue": "Mathematics of Operations Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Faster first-order methods for extensive-form game solving", "author": ["Christian Kroer", "Kevin Waugh", "Fatma K\u0131l\u0131n\u00e7-Karzan", "Tuomas Sandholm"], "venue": "In Proceedings of the ACM Conference on Economics and Computation (EC),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Monte Carlo sampling for regret minimization in extensive games", "author": ["Marc Lanctot", "Kevin Waugh", "Martin Zinkevich", "Michael Bowling"], "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Refining subgames in large imperfect information games", "author": ["Matej Moravcik", "Martin Schmid", "Karel Ha", "Milan Hladik", "Stephen J Gaukrodger"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Excessive gap technique in nonsmooth convex minimization", "author": ["Yurii Nesterov"], "venue": "SIAM Journal of Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "An interior point approach to large games of incomplete information", "author": ["Fran\u00e7ois Pays"], "venue": "In AAAI Computer Poker Workshop,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "bluff: Opponent modelling in poker", "author": ["Finnegan Southey", "Michael Bowling", "Bryce Larson", "Carmelo Piccione", "Neil Burch", "Darse Billings", "Chris Rayner. Bayes"], "venue": "In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Solving heads-up limit texas hold\u2019em", "author": ["Oskari Tammelin", "Neil Burch", "Michael Johanson", "Michael Bowling"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Abstraction pathologies in extensive games", "author": ["Kevin Waugh", "David Schnizlein", "Michael Bowling", "Duane Szafron"], "venue": "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 4, "context": "A linear program can find an exact Nash equilibrium in two-player zero-sum games containing fewer than about 10 nodes [5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 6, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 11, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 7, "context": "There are a number of such iterative algorithms [11, 7, 12, 8], the most popular of which is Counterfactual Regret Minimization (CFR) [16].", "startOffset": 48, "endOffset": 62}, {"referenceID": 0, "context": "CFR+, a variant of CFR, was used to essentially solve Limit Texas Hold\u2019em, the largest imperfect-information game ever to be essentially solved [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "This data had to be repeatedly decompressed from disk into memory and then compressed back to disk in order to run CFR+ [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Regret Based Pruning (RBP) is an improvement to CFR that greatly reduces the computation time needed to solve large games by temporarily pruning suboptimal actions [2].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "While Total RBP still requires enough memory to store the entire game in the early iterations, recent work has shown that these early iterations can be skipped by first solving a low-memory abstraction of the game and then using its solution to warm starting CFR in the full game [4].", "startOffset": 280, "endOffset": 283}, {"referenceID": 2, "context": "Total RBP\u2019s reduction in space is also helpful to the Simultaneous Abstraction and Equilibrium Finding (SAEF) algorithm [3], which starts CFR with a small abstraction of the game and progressively expands the abstraction while also solving the game.", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "We use regret matching (RM) [6] as the regret-minimization algorithm.", "startOffset": 28, "endOffset": 31}, {"referenceID": 9, "context": "A counterfactual best response [10] (CBR) is a strategy similar to a best response, except that it maximizes counterfactual value even at information sets that it does not reach due to its earlier actions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "In two-player zero-sum games, if both players\u2019 average regret R i T \u2264 , their average strategies \u3008\u03c3\u0304 1 , \u03c3\u0304 2 \u3009 form a 2 -equilibrium [15].", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "The benefit of this form of pruning, which we refer to as partial pruning, varies depending on the game, but empirical results show a factor of 30 improvement in some games [9].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "While partial pruning allows one to prune paths that an opponent reaches with zero probability, interval regret-based pruning (Interval RBP) allows one to also prune paths that the traverser reaches with zero probability [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 3, "context": "The proof for Theorem 1 draws from recent work on warm starting CFR using only an average strategy profile [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 12, "context": "The experiments are conducted on Leduc Hold\u2019em [13] and Leduc-5 [2].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "The experiments are conducted on Leduc Hold\u2019em [13] and Leduc-5 [2].", "startOffset": 64, "endOffset": 67}], "year": 2016, "abstractText": "Counterfactual Regret Minimization (CFR) is the most popular iterative algorithm for solving zero-sum imperfect-information games. Regret-Based Pruning (RBP) is an improvement that allows poorly-performing actions to be temporarily pruned, thus speeding up CFR. We introduce Total RBP, a new form of RBP that reduces the space requirements of CFR as actions are pruned. We prove that in zero-sum games it asymptotically prunes any action that is not part of a best response to some Nash equilibrium. This leads to provably faster convergence and lower space requirements. Experiments show that Total RBP results in an order of magnitude reduction in space, and the reduction factor increases with game size.", "creator": "LaTeX with hyperref package"}}}