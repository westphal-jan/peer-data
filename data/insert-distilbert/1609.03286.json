{"id": "1609.03286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks", "abstract": "natural finite language understanding ( nlu ) is a core component of a spoken dialogue system. recently recurrent neural networks ( rnn ) obtained strong results on nlu due to their superior ability of preserving sequential information over time. traditionally, the nlu module tags semantic slots for utterances considering their flat structures, as the underlying rnn structure is a linear chain. however, natural language exhibits linguistic properties that simultaneously provide rich, structured information for better understanding. this paper introduces a novel model, knowledge - guided structural attention networks ( k - san ), initiating a generalization of rnn to additionally incorporate non - flat network topologies guided by prior knowledge. there are two recognized characteristics : 1 ) important substructures can continually be captured from small training data, allowing the model to generalize to previously unseen test data ; about 2 ) the concept model automatically figures out the salient substructures that are essential to predict the good semantic tags of viewing the given sentences, so that the understanding performance can be improved. the experiments on the benchmark air travel information system ( atis ) data show that the proposed k - san architecture can effectively extract simple salient knowledge from substructures with an attention mechanism, and outperform the performance of the state - of - the - art neural network based frameworks.", "histories": [["v1", "Mon, 12 Sep 2016 07:29:59 GMT  (1620kb,D)", "http://arxiv.org/abs/1609.03286v1", "11 pages, 5 figures"]], "COMMENTS": "11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["yun-nung chen", "dilek hakkani-tur", "gokhan tur", "asli celikyilmaz", "jianfeng gao", "li deng"], "accepted": false, "id": "1609.03286"}, "pdf": {"name": "1609.03286.pdf", "metadata": {"source": "CRF", "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks", "authors": ["Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng"], "emails": ["gokhan}@ieee.org", "deng}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In the past decade, goal-oriented spoken dialogue systems (SDS), such as the virtual personal assis-\ntants Microsoft\u2019s Cortana and Apple\u2019s Siri, are being incorporated in various devices and allow users to speak to systems freely in order to finish tasks more efficiently. A key component of these conversational systems is the natural language understanding (NLU) module-it refers to the targeted understanding of human speech directed at machines (Tur and De Mori, 2011). The goal of such \u201ctargeted\u201d understanding is to convert the recognized user speech into a task-specific semantic representation of the user\u2019s intention, at each turn, that aligns with the back-end knowledge and action sources for task completion. The dialogue manager then interprets the semantics of the user\u2019s request and associated back-end results, and decides the most appropriate system action, by exploiting semantic context and user specific meta-information, such as geo-location and personal preferences (McTear, 2004; Rudnicky and Xu, 1999).\nA typical pipeline of NLU includes: domain classification, intent determination, and slot filling (Tur and De Mori, 2011). NLU first decides the domain of user\u2019s request given the input utterance, and based on the domain, predicts the intent and fills associated slots corresponding to a domain-specific semantic template. For example, Figure 1 shows a user utterance, \u201cshow me the flights from seattle to san francisco\u201d and its semantic frame, find flight(origin=\u201cseattle\u201d, dest=\u201csan francisco\u201d). It is easy to see the relationship between the origin city and the destination city in this example, although these do not appear next to each other. Traditionally, domain detection and intent prediction are framed as utterance classification ar X\niv :1\n60 9.\n03 28\n6v 1\n[ cs\n.A I]\n1 2\nSe p"}, {"heading": "O O O O B-origin O B-dest I-destO", "text": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014). Then slot filling is framed as a word sequence tagging task, where the IOB (in-outbegin) format is applied for representing slot tags as illustrated in Figure 1, and hidden Markov models (HMM) or conditional random fields (CRF) have been employed for slot tagging (Pieraccini et al., 1992; Wang et al., 2005).\nWith the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014). Recently, Ravuri and Stolcke (2015) proposed an RNN architecture for intent determination. For slot filling, deep learning has been viewed as a feature generator and the neural architecture can be merged with CRFs (Xu and Sarikaya, 2013). Yao et al. (2013) and Mesnil et al. (2015) later employed RNNs for sequence labeling in order to perform slot filling. However, the above studies benefit from large training data without leveraging any existing knowledge. When tagging sequences RNNs consider them as flat structures, with their underlying linear chain structures, potentially ignoring the structured information typical of natural language sequences.\nHierarchical structures and semantic relationships contain linguistic characteristics of input word sequences forming sentences, and such information may help interpret their meaning. Furthermore, prior knowledge would help in the tagging of sequences, especially when dealing with previously unseen sequences (Tur et al., 2010; Deoras and Sarikaya, 2013). Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al. (2013) and Chen et al. (2015) proposed approaches that leverage linguistic knowledge encoded in parse trees for language understanding, where the ex-\ntracted syntactic structural features and semantic dependency features enhance inference model learning, and the model achieves better language understanding performance in various domains.\nEven with the emerging paradigm of integrating deep learning and linguistic knowledge for different NLP tasks (Socher et al., 2014), most of the previous work utilized such linguistic knowledge and knowledge bases as additional features as input to neural networks, and then learned the models for tagging sequences. These feature enrichment based approaches have some possible limitations: 1) poor generalization and 2) error propagation. Poor generalization comes from the mismatch between knowledge bases and the input data, and then the incorrectly extracted features due to errors in previous processing propagate errors to the neural models. In order to address the issues and better learn the sequence tagging models, this paper proposes knowledge-guided structural attention networks, K-SAN, a generalization of RNNs that automatically learn the attention guided by external or prior knowledge and generate sentence-based representations specifically for modeling sequence tagging. The main difference between K-SAN and previous approaches is that knowledge plays the role of a teacher to guide networks where and how much to focus attention considering the whole linguistic structure simultaneously. Our main contributions are three-fold:\n\u2022 End-to-end learning To our knowledge, this is the first neural network approach that utilizes general knowledge as guidance in an end-to-end fashion, where the model automatically learns important substructures with an attention mechanism.\n\u2022 Generalization for different knowledge There is no required schema of knowledge, and different types of parsing results, such as dependency relations, knowledge graph-specific relations, and parsing output of hand-crafted grammars, can serve as the knowledge guidance in this model.\n\u2022 Efficiency and parallelizability Because the substructures from the input utterance are modeled separately, modeling time\nmay not increase linearly with respect to the number of words in the input sentence.\nIn the following sections, we empirically show the benefit of K-SAN on the targeted NLU task."}, {"heading": "2 Related Work", "text": "Knowledge-Based Representations There is an emerging trend of learning representations at different levels, such as word embeddings (Mikolov et al., 2013), character embeddings (Ling et al., 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al., 2013). In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding.\nRecently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al. (2015) both proposed dependency-based approaches to combine deep learning and linguistic structures, where the model used tree-based n-grams instead of surface ones to capture knowledge-guided relations for sentence modeling and classification. Roth and Lapata (2016) utilized lexicalized dependency paths to learn embedding representations for semantic role labeling. However, the performance of these approaches highly depends on the quality of \u201cwhole\u201d sentence parsing, and there is no control of degree of attentions on different substructures. Learning robust representations incorporating whole structures still remains unsolved. In this paper, we address the limitation by proposing K-SAN to learn robust representations of whole sentences, where the whole representation is composed of the salient substructures in order to avoid error propagation.\nNeural Attention and Memory Model One of the earliest work with a memory component applied to language processing is memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA). Following their success, Xiong et al. (2016) proposed dynamic memory networks (DMN) to additionally capture position and\ntemporality of transitive reasoning steps for different QA tasks. The idea is to encode important knowledge and store it into memory for future usage with attention mechanisms. Attention mechanisms allow neural network models to selectively pay attention to specific parts. There are also various tasks showing the effectiveness of attention mechanisms.\nHowever, most previous work focused on the classification or prediction tasks (predicting a single word given a question), and there are few studies for NLU tasks (slot tagging). Based on the fact that the linguistic or knowledge-based substructures can be treated as prior knowledge to benefit language understanding, this work borrows the idea from memory models to improve NLU. Unlike the prior NLU work that utilized representations learned from knowledge bases to enrich features of the current sentence, this paper directly learns a sentence representation incorporating memorized substructures with an automatically decided attention mechanism in an end-to-end manner."}, {"heading": "3 Knowledge-Guided Structural Attention Networks (K-SAN)", "text": "For the NLU task, given an utterance with a sequence of words/tokens ~s = w1, ..., wT , our model is to predict corresponding semantic tags ~y = y1, ..., yT for each word/token by incorporating knowledge-guided structures. The proposed model is illustrated in Figure 2. The knowledge encoding module first leverages external knowledge to generate a linguistic structure for the utterance, where a discrete set of knowledge-guided substructures {xi} is encoded into a set of vector representations (\u00a7 3.1). The model learns the representation for the whole sentence by paying different attention on the substructures (\u00a7 3.2). Then the learned vector encoding the knowledge-guided structure is used for improving the semantic tagger (\u00a7 4)."}, {"heading": "3.1 Knowledge Encoding Module", "text": "The prior knowledge obtained from external resources, such as dependency relations, knowledge bases, etc., provides richer information to help decide the semantic tags given an input utterance. This paper takes dependency relations as an example for knowledge encoding, and other structured relations\ncan be applied in the same way. The input utterance is parsed by a dependency parser, and the substructures are built according to the paths from the root to all leaves (Chen and Manning, 2014). For example, the dependency parsing of the utterance \u201cshow me the flights from seattle to san francisco\u201d is shown in Figure 3, where the associated substructures are obtained from the parsing tree for knowledge encoding. Here we do not utilize the dependency relation labels in the experiments for better generalization, because the labels may not be always available for different knowledge resources. Note that the number of substructures may be less than the number of words in the utterance, because non-leaf nodes do not have corresponding substructure in order to reduce the duplicated information in the model. The top-left component of Figure 2 illustrates the module for modeling knowledge-guided substructures."}, {"heading": "3.2 Model Architecture", "text": "The model embeds all knowledge-guided substructures into a continuous space and stores embeddings of all x\u2019s in the knowledge memory. The representation of the input utterance is then compared with encoded knowledge representations to integrate the carried structure guided by knowledge via an attention mechanism. Then the knowledge-guided representation of the sentence is taken together with the word sequence for estimating the semantic tags. Four main procedures are described below.\nEncoded Knowledge Representation To store the knowledge-guided structure, we convert each substructure (e.g. path starting from the root to the leaf in the dependency tree), xi, into a structure vectormi with dimension d by embedding the substructure in a continuous space through the knowledge encoding model Mkg. The input utterance s is also embedded to a vector u with the same dimension through the model Min.\nmi = Mkg(xi), (1)\nu = Min(s). (2)\nWe apply the three types for knowledge encoding models, Mkg and Min, in order to model multiple words from a substructure xi or an input sentence s into a vector representation: 1) fully-connected neural networks (NN) with linear activation, 2) recur-\nrent neural networks (RNN), and 3) convolutional neural networks (CNN) with a window size 3 and a max-pooling operation. For example, one of substructures shown in Figure 3, \u201cshow flights seattle from\u201d, is encoded into a vector embedding. In the experiments, the weights ofMkg andMin are tied together based on their consistent ability of sequence encoding.\nKnowledge Attention Distribution In the embedding space, we compute the match between the current utterance vector u and its substructure vector mi by taking their inner product followed by a softmax.\npi = softmax(uTmi), (3) where softmax(zi) = ezi/ \u2211 j e zj and pi can be viewed as attention distribution for modeling important substructures from external knowledge in order to understand the current utterance.\nSentence Representation In order to encode the knowledge-guided structure, a vector h is a sum over the encoded knowledge embeddings weighted by the attention distribution.\nh = \u2211 i pimi, (4)\nwhich indicates that the sentence pays different attention to different substructures guided from external knowledge. Because the function from input to output is smooth, we can easily compute gradients and back propagate through it. Then the sum of the substructure vector h and the current input embedding u are then passed through a neural network modelMout to generate an output knowledge-guided representation o.\no =Mout(h+ u), (5)\nwhere we employ a fully-connected dense network for Mout.\nSequence Tagging To estimate the tag sequence ~y corresponding to an input word sequence ~s, we use an RNN module for training a slot tagger, where the knowledge-guided representation o is fed into the input of the model in order to incorporate the structure information.\n~y = RNN(o,~s) (6)"}, {"heading": "4 Recurrent Neural Network Tagger", "text": ""}, {"heading": "4.1 Chain-Based RNN Tagger", "text": "Given ~s = w1, ..., wT , the model is to predict ~y = y1, ..., yT where the tag yi is aligned with the word wi. We use the Elman RNN architecture, consisting of an input layer, a hidden layer, and an output layer (Elman, 1990). The input, hidden and output layers consist of a set of neurons representing the input, hidden, and output at each time step t, wt, ht, and yt, respectively.\nht = \u03c6(Wwt + Uht\u22121), (7)\ny\u0302t = softmax(V ht), (8)\nwhere \u03c6 is a smooth bounded function such as tanh, and y\u0302t is the probability distribution over of semantic tags given the current hidden state ht. The sequence probability can be formulated as\np(~y | ~s) = p(~y | w1, ..., wT ) = \u220f i p(yi | w1, ..., wi). (9) The model can be trained using backpropagation to maximize the conditional likelihood of the training set labels.\nTo overcome the frequent vanishing gradients issue when modeling long-term dependencies, gated RNN was designed to use a more sophisticated activation function than a usual activation function, consisting of affine transformation followed by a simple element-wise nonlinearity by using gating units (Chung et al., 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014). RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014). In this paper, we use RNN with GRU cells to allow each recurrent unit to adaptively capture dependencies of different time scales (Cho et al., 2014; Chung et al., 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al., 2014)\nA GRU has two gates, a reset gate r, and an update gate z (Cho et al., 2014; Chung et al., 2014). The reset gate determines the combination between\nthe new input and the previous memory, and the update gate decides how much the unit updates its activation, or content.\nr = \u03c3(W rwt + U rht\u22121), (10) z = \u03c3(W zwt + U zht\u22121), (11)\nwhere \u03c3 is a logistic sigmoid function. Then the final activation of the GRU at time t, ht, is a linear interpolation between the previous activation ht\u22121 and the candidate activation h\u0303t:\nht = (1\u2212 z) h\u0303t + z ht\u22121, (12) h\u0303t = \u03c6(W hwt + U h(ht\u22121 r))), (13)\nwhere is an element-wise multiplication. When the reset gate is off, it effectively makes the unit act as if it is reading the first symbol of an input sequence, allowing it to forget the previously computed state. Then y\u0302t can be computed by (8)."}, {"heading": "4.2 Knowledge-Guided RNN Tagger", "text": "In order to model the encoded knowledge from previous turns, for each time step t, the knowledgeguided sentence representation o in (5) is fed into the RNN model together with the word wt. For the plain RNN, the hidden layer can be formulated as\nht = \u03c6(Mo+Wwt + Uht\u22121) (14)\nto replace (7) as illustrated in the right block of Figure 2. RNN-GRU can incorporate the encoded knowledge in the similar way, where Mo can be added into gating mechanisms for modeling contextual knowledge similarly."}, {"heading": "4.3 Joint RNN Tagger", "text": "Because the chain-based tagger and the knowledgeguided tagger carry different information, the joint RNN tagger is proposed to balance the information between two model architectures. Figure 4 presents the architecture of the joint RNN tagger.\nh1t = \u03c6(W 1wt + U 1ht\u22121), (15) h2t = \u03c6(Mo+W 2wt + U 2ht\u22121), (16)\ny\u0302t = softmax(V (\u03b1h1t + (1\u2212 \u03b1)h2t )), (17)\nwhere \u03b1 is the weight for balancing chain-based and knowledge-guided information. By jointly considering chain-based information (h1t ) and knowledgeguided information (h2t ), the joint RNN tagger is expected to achieve better generalization, and the performance may be less sensitive to poor structures from external knowledge. In the experiments, \u03b1 is set to 0.5 for balancing two sides. The objective of the proposed model is to maximize the sequence probability p(~y | ~s) in (9), and the model can be trained in an end-to-end manner, where the error would be back-propagated through the whole architecture."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "The dataset for experiments is the benchmark ATIS corpus, which is extensively used by the NLU community (Mesnil et al., 2015). There are 4978 training utterances selected from Class A (context independent) in the ATIS-2 and ATIS-3, while there are 893 utterances selected from the ATIS-3 Nov93 and Dec94. In the experiments, we only use lexical features. In order to show the robustness to data scarcity, we conduct the experiments with 3 different sizes of training data (Small, Medium, and Large), where Small is 1/40 of the original set, Medium is 1/10 of the original set, and Large is the full set. The evaluation metrics for NLU is F-measure on the predicted slots1.\nFor experiments with K-SAN, we parse all data with the Stanford dependency parser (Chen and Manning, 2014) and represent words as their embeddings trained on the in-domain data, where the parser is pre-trained on PTB. The loss function is\n1The used evaluation script is conlleval.\ncross-entropy, and the optimizer we use is adam with the default setting (Kingma and Ba, 2014), where the learning rate \u03bb = 0.001, \u03b21 = 0.9, \u03b22 = 0.999, and = 1e\u221208. The maximum iteration for training our K-SAN models is set as 300. The dimensionality of input word embeddings is 100, and the hidden layer sizes are in {50, 100, 150}. The dropout rates are set as {0.25, 0.50}. All reported results are from the joint RNN tagger, and the hyperparameters are tuned in the dev set for all experiments."}, {"heading": "5.2 Baseline", "text": "To validate the effectiveness of the proposed model, we compare the performance with the following baselines.\n\u2022 Baseline: \u2013 CRF Tagger (Tur et al., 2010): predicts a\nsemantic slot for each word with a context window (size = 5). \u2013 RNN Tagger (Mesnil et al., 2015): predicts a semantic slot for each word. \u2013 CNN Encoder-Tagger (Kim, 2014): tag semantic slots with consideration of sentence embeddings learned by a convolutional model.\n\u2022 Structural: The NLU models utilize linguistic information when tagging slots, where DCNN and Tree-RNN are the state-of-the-art approaches for embedding sentences with linguistic structures.\n\u2013 CRF Tagger (Tur et al., 2010): predicts slots based on the lexical (5-word win-\ndow) and syntactic (dependent head in the parsing tree) features. \u2013 DCNN (Ma et al., 2015a): predicts slots by incorporating sentence embeddings learned by a convolutional model with consideration of dependency tree structures. \u2013 Tree-RNN (Tai et al., 2015): predicts slots with sentence embeddings learned by an RNN model based on the tree structures of sentences."}, {"heading": "5.3 Slot Filling Results", "text": "Table 1 shows the performance of slot filling on different size of training data, where there are three datasets (Small, Medium, and Large use 1/40, 1/10, and whole training data). For baselines (models without knowledge features), CNN Encoder-Tagger achieves the best performance on all datasets.\nAmong structural models (models with knowledge encoding), Tree-RNN Encoder-Tagger performs better for Small data but slightly worse than the DCNN Encoder-Tagger.\nCNN (Kim, 2014) performs better compared to DCNN (Ma et al., 2015a) and Tree-RNN (Tai et al., 2015), even though CNN does not leverage external knowledge when encoding sentences. When comparing the NLU performance between baselines and other state-of-the-art structural models, there is no significant difference. This suggests that encoding sentence information without distinguishing substructure may not capture salient semantics in order to improve understanding performance.\nAmong the proposed K-SAN models, CNN for encoding performs best on Small (75% on F1) and Medium (88% on F1), and RNN for encoding performs best on the Large set (95% on F1). Also, most of the proposed models outperform all baselines, where the improvement for the small dataset is more significant. This suggests that the proposed models carry better generalization and are less sensitive to unseen data. For example, given an utterance \u201cwhich flights leave on monday from montreal and arrive in chicago in the morning\u201d, \u201cmorning\u201d can be correctly tagged with a semantic tag B-arrive time.period of day by K-SAN, but it is incorrectly tagged with Bdepart time.period of day by baselines, because knowledge guides the model to pay correct attention to salient substructures. The proposed model presents the state-of-the-art performance on the large dataset (RNN-BLSTM in baselines), showing the effectiveness of leveraging knowledge-guided structures for learning embeddings that can be used for specific tasks and the robustness to data scarcity and mismatch."}, {"heading": "5.4 Attention Analysis", "text": "In order to show the effectiveness of boosting performance by learning correct attention from much smaller training data through the proposed model, we present the visualization of the attention for both words and relations decoded by K-SAN with CNN in the Figure 5. The darker color of blocks and lines indicates the higher attention for words and relations respectively. From the figure, the words and the relations with higher attention are the most crucial parts\nfor predicting correct slots, e.g. origin, destination, and time. Furthermore, the difference of attention distribution between three datasets is not significant; this suggests that our proposed model is able to pay correct attention to important substructures guided by the external knowledge even the training data is scarce."}, {"heading": "5.5 Knowledge Generalization", "text": "In order to show the capacity of generalization to different knowledge resources, we perform the KSAN model for different knowledge bases. Below we compare two types of knowledge formats: dependency tree and Abstract Meaning Representation (AMR). AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph (Banarescu et al., 2013), where nodes represent concepts, and labeled directed edges represent the relations between two concepts. The formalism is based on propositional logic and neoDavidsonian event representations (Parsons, 1990; Davidson, 1967). The semantic concepts in AMR were leveraged to benefit multiple NLP tasks (Liu et al., 2015). Unlike syntactic information from dependency trees, the AMR graph contains semantic information, which may offer more specific conceptual relations. Figure 6 shows the comparison of a dependency tree and an AMR graph associated with the same example utterance and how the knowledgeguided substructures are constructed.\nTable 2 presents the performance of CRF and K-SAN with CNN taggers that utilize dependency relations and AMR edges as knowledge guidance on the same datasets, where CRF takes the\nSentence s\nshow me the flights from seattle to san francisco\nSentence s\nshow me the flights from seattle to san francisco\nhead words from either dependency trees or AMR graphs as additional features and K-SAN incorporates knowledge-guided substructures as illustrated in Figure 6. The dependency trees are obtained from the Stanford dependency parser or the SyntaxNet parser2, and AMR graphs are generated by a rulebased AMR parser or JAMR3.\nAmong four knowledge resources (different types and obtained from different parsers), all results show the similar performance for three sizes of datasets. The maximum number of substructures for the dependency tree is larger than the number in the AMR graph (53 and 25 v.s. 19 and 8), because syntax is more general and may provide richer cues for guiding more attention while semantics is more specific and may offer stronger guidance. In sum, the models applying four different resources achieve similar performance, and all significantly outperform the\n2https://github.com/tensorflow/models/ tree/master/syntaxnet\n3https://github.com/jflanigan/jamr\nstate-of-the-art NLU tagger, showing the effectiveness, generalization, and robustness of the proposed K-SAN model."}, {"heading": "6 Conclusion", "text": "This paper proposes a novel model, knowledgeguided structural attention networks (K-SAN), that leverages prior knowledge as guidance to incorporate non-flat topologies and learn suitable attention for different substructures that are salient for specific tasks. The structured information can be captured from small training data, so the model has better generalization and robustness. The experiments show benefits and effectiveness of the proposed model on the language understanding task, where all knowledge-guided substructures captured by different resources help tagging performance, and the state-of-the-art performance is achieved on the ATIS benchmark dataset."}], "references": [{"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the Linguistic Annota-", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Convolutional neural network based semantic tagging with entity embeddings", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur."], "venue": "NIPS Workshop on Machine Learning for SLU and Interaction.", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? 2015", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2015}, {"title": "Speech utterance classification", "author": ["Ciprian Chelba", "Monika Mahajan", "Alex Acero."], "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP), volume 1, pages I\u2013280. IEEE.", "citeRegEx": "Chelba et al\\.,? 2003", "shortCiteRegEx": "Chelba et al\\.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding", "author": ["Yun-Nung Chen", "Dilek Hakkani-Tur", "Gokan Tur."], "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), pages", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding", "author": ["Yun-Nung Chen", "William Yang Wang", "Anatole Gershman", "Alexander I Rudnicky."], "venue": "Proceedings of ACL-IJCNLP.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "The logical form of action sentences", "author": ["Donald Davidson"], "venue": null, "citeRegEx": "Davidson.,? \\Q1967\\E", "shortCiteRegEx": "Davidson.", "year": 1967}, {"title": "Deep belief network based semantic taggers for spoken language understanding", "author": ["Anoop Deoras", "Ruhi Sarikaya."], "venue": "INTERSPEECH, pages 2713\u20132717.", "citeRegEx": "Deoras and Sarikaya.,? 2013", "shortCiteRegEx": "Deoras and Sarikaya.", "year": 2013}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Optimizing svms for complex call classification", "author": ["Patrick Haffner", "Gokhan Tur", "Jerry H Wright."], "venue": "2003 IEEE International Conference on", "citeRegEx": "Haffner et al\\.,? 2003", "shortCiteRegEx": "Haffner et al\\.", "year": 2003}, {"title": "Leveraging knowledge graphs for web-scale unsupervised semantic parsing", "author": ["Larry P Heck", "Dilek Hakkani-T\u00fcr", "Gokhan Tur."], "venue": "INTERSPEECH, pages 1594\u20131598.", "citeRegEx": "Heck et al\\.,? 2013", "shortCiteRegEx": "Heck et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of the 22nd ACM international conference on Conference on information &", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Query understanding enhanced by hierarchical parsing structures", "author": ["Jingjing Liu", "Panupong Pasupat", "Yining Wang", "Scott Cyphers", "James Glass."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 72\u201377. IEEE.", "citeRegEx": "Liu et al\\.,? 2013", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Toward abstractive summarization using semantic representations", "author": ["Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A Smith."], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Ma et al\\.,? 2015a", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Knowledge graph inference for spoken dialog systems", "author": ["Yi Ma", "Paul A Crook", "Ruhi Sarikaya", "Eric FoslerLussier."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5346\u20135350. IEEE.", "citeRegEx": "Ma et al\\.,? 2015b", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Spoken dialogue technology: toward the conversational user interface", "author": ["Michael F McTear."], "venue": "Springer Science & Business Media.", "citeRegEx": "McTear.,? 2004", "shortCiteRegEx": "McTear.", "year": 2004}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Events in the semantics of english: A study in subatomic semantics", "author": ["Terence Parsons"], "venue": null, "citeRegEx": "Parsons.,? \\Q1990\\E", "shortCiteRegEx": "Parsons.", "year": 1990}, {"title": "A speech understanding system based on statistical representation of semantics", "author": ["Roberto Pieraccini", "Evelyne Tzoukermann", "Zakhar Gorelov", "Jean-Luc Gauvain", "Esther Levin", "Chin-Hui Lee", "Jay G Wilpon."], "venue": "1992 IEEE International Conference on", "citeRegEx": "Pieraccini et al\\.,? 1992", "shortCiteRegEx": "Pieraccini et al\\.", "year": 1992}, {"title": "Recurrent neural network and lstm models for lexical utterance classification", "author": ["Suman Ravuri", "Andreas Stolcke."], "venue": "Sixteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Ravuri and Stolcke.,? 2015", "shortCiteRegEx": "Ravuri and Stolcke.", "year": 2015}, {"title": "Neural semantic role labeling with dependency path embeddings", "author": ["Michael Roth", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1605.07515.", "citeRegEx": "Roth and Lapata.,? 2016", "shortCiteRegEx": "Roth and Lapata.", "year": 2016}, {"title": "An agendabased dialog management architecture for spoken language systems", "author": ["Alexander Rudnicky", "Wei Xu."], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, volume 13, page 17.", "citeRegEx": "Rudnicky and Xu.,? 1999", "shortCiteRegEx": "Rudnicky and Xu.", "year": 1999}, {"title": "Deep belief nets for natural language call-routing", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Bhuvana Ramabhadran."], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5680\u20135683. IEEE.", "citeRegEx": "Sarikaya et al\\.,? 2011", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2011}, {"title": "Application of deep belief networks for natural language understanding", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):778\u2013 784.", "citeRegEx": "Sarikaya et al\\.,? 2014", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng."], "venue": "Transactions of the Association for Computational Linguistics, 2:207\u2013218.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Spoken language understanding: Systems for extracting semantic information from speech", "author": ["Gokhan Tur", "Renato De Mori."], "venue": "John Wiley & Sons.", "citeRegEx": "Tur and Mori.,? 2011", "shortCiteRegEx": "Tur and Mori.", "year": 2011}, {"title": "What is left to be understood in atis? In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 19\u201324", "author": ["Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Larry Heck."], "venue": "IEEE.", "citeRegEx": "Tur et al\\.,? 2010", "shortCiteRegEx": "Tur et al\\.", "year": 2010}, {"title": "Towards deeper understanding: Deep convex networks for semantic utterance classification", "author": ["Gokhan Tur", "Li Deng", "Dilek Hakkani-T\u00fcr", "Xiaodong He."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5045\u2013", "citeRegEx": "Tur et al\\.,? 2012", "shortCiteRegEx": "Tur et al\\.", "year": 2012}, {"title": "Spoken language understanding", "author": ["Ye-Yi Wang", "Li Deng", "Alex Acero."], "venue": "IEEE Signal Processing Magazine, 22(5):16\u201331.", "citeRegEx": "Wang et al\\.,? 2005", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordesa."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher."], "venue": "arXiv preprint arXiv:1603.01417.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["Puyang Xu", "Ruhi Sarikaya."], "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 78\u201383. IEEE.", "citeRegEx": "Xu and Sarikaya.,? 2013", "shortCiteRegEx": "Xu and Sarikaya.", "year": 2013}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "arXiv preprint arXiv:1412.6575.", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu."], "venue": "INTERSPEECH, pages 2524\u20132528.", "citeRegEx": "Yao et al\\.,? 2013", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi."], "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), pages 189\u2013194. IEEE.", "citeRegEx": "Yao et al\\.,? 2014", "shortCiteRegEx": "Yao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "The dialogue manager then interprets the semantics of the user\u2019s request and associated back-end results, and decides the most appropriate system action, by exploiting semantic context and user specific meta-information, such as geo-location and personal preferences (McTear, 2004; Rudnicky and Xu, 1999).", "startOffset": 267, "endOffset": 304}, {"referenceID": 31, "context": "The dialogue manager then interprets the semantics of the user\u2019s request and associated back-end results, and decides the most appropriate system action, by exploiting semantic context and user specific meta-information, such as geo-location and personal preferences (McTear, 2004; Rudnicky and Xu, 1999).", "startOffset": 267, "endOffset": 304}, {"referenceID": 12, "context": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014).", "startOffset": 107, "endOffset": 169}, {"referenceID": 2, "context": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014).", "startOffset": 107, "endOffset": 169}, {"referenceID": 4, "context": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014).", "startOffset": 107, "endOffset": 169}, {"referenceID": 28, "context": "Then slot filling is framed as a word sequence tagging task, where the IOB (in-outbegin) format is applied for representing slot tags as illustrated in Figure 1, and hidden Markov models (HMM) or conditional random fields (CRF) have been employed for slot tagging (Pieraccini et al., 1992; Wang et al., 2005).", "startOffset": 264, "endOffset": 308}, {"referenceID": 41, "context": "Then slot filling is framed as a word sequence tagging task, where the IOB (in-outbegin) format is applied for representing slot tags as illustrated in Figure 1, and hidden Markov models (HMM) or conditional random fields (CRF) have been employed for slot tagging (Pieraccini et al., 1992; Wang et al., 2005).", "startOffset": 264, "endOffset": 308}, {"referenceID": 32, "context": "With the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014).", "startOffset": 157, "endOffset": 221}, {"referenceID": 40, "context": "With the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014).", "startOffset": 157, "endOffset": 221}, {"referenceID": 33, "context": "With the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014).", "startOffset": 157, "endOffset": 221}, {"referenceID": 44, "context": "For slot filling, deep learning has been viewed as a feature generator and the neural architecture can be merged with CRFs (Xu and Sarikaya, 2013).", "startOffset": 123, "endOffset": 146}, {"referenceID": 28, "context": "Recently, Ravuri and Stolcke (2015) proposed an RNN architecture for intent determination.", "startOffset": 10, "endOffset": 36}, {"referenceID": 28, "context": "Recently, Ravuri and Stolcke (2015) proposed an RNN architecture for intent determination. For slot filling, deep learning has been viewed as a feature generator and the neural architecture can be merged with CRFs (Xu and Sarikaya, 2013). Yao et al. (2013) and Mesnil et al.", "startOffset": 10, "endOffset": 257}, {"referenceID": 25, "context": "(2013) and Mesnil et al. (2015) later employed RNNs for sequence labeling in order to perform slot filling.", "startOffset": 11, "endOffset": 32}, {"referenceID": 39, "context": "Furthermore, prior knowledge would help in the tagging of sequences, especially when dealing with previously unseen sequences (Tur et al., 2010; Deoras and Sarikaya, 2013).", "startOffset": 126, "endOffset": 171}, {"referenceID": 9, "context": "Furthermore, prior knowledge would help in the tagging of sequences, especially when dealing with previously unseen sequences (Tur et al., 2010; Deoras and Sarikaya, 2013).", "startOffset": 126, "endOffset": 171}, {"referenceID": 13, "context": "Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al.", "startOffset": 106, "endOffset": 162}, {"referenceID": 23, "context": "Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al.", "startOffset": 106, "endOffset": 162}, {"referenceID": 4, "context": "Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al.", "startOffset": 106, "endOffset": 162}, {"referenceID": 4, "context": ", 2015b; Chen et al., 2014) Liu et al. (2013) and Chen et al.", "startOffset": 9, "endOffset": 46}, {"referenceID": 4, "context": ", 2015b; Chen et al., 2014) Liu et al. (2013) and Chen et al. (2015) proposed approaches that leverage linguistic knowledge encoded in parse trees for language understanding, where the extracted syntactic structural features and semantic dependency features enhance inference model learning, and the model achieves better language understanding performance in various domains.", "startOffset": 9, "endOffset": 69}, {"referenceID": 34, "context": "Even with the emerging paradigm of integrating deep learning and linguistic knowledge for different NLP tasks (Socher et al., 2014), most of the previous work utilized such linguistic knowledge and knowledge bases as additional features as input to neural networks, and then learned the models for tagging sequences.", "startOffset": 110, "endOffset": 131}, {"referenceID": 26, "context": "Knowledge-Based Representations There is an emerging trend of learning representations at different levels, such as word embeddings (Mikolov et al., 2013), character embeddings (Ling et al.", "startOffset": 132, "endOffset": 154}, {"referenceID": 19, "context": ", 2013), character embeddings (Ling et al., 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 18, "context": ", 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al., 2013).", "startOffset": 33, "endOffset": 75}, {"referenceID": 15, "context": ", 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al., 2013).", "startOffset": 33, "endOffset": 75}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014).", "startOffset": 160, "endOffset": 214}, {"referenceID": 45, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014).", "startOffset": 160, "endOffset": 214}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al.", "startOffset": 161, "endOffset": 453}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al. (2015) both proposed dependency-based approaches to combine deep learning and linguistic structures, where the model used tree-based n-grams instead of surface ones to capture knowledge-guided relations for sentence modeling and classification.", "startOffset": 161, "endOffset": 475}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al. (2015) both proposed dependency-based approaches to combine deep learning and linguistic structures, where the model used tree-based n-grams instead of surface ones to capture knowledge-guided relations for sentence modeling and classification. Roth and Lapata (2016) utilized lexicalized dependency paths to learn embedding representations for semantic role labeling.", "startOffset": 161, "endOffset": 736}, {"referenceID": 42, "context": "Neural Attention and Memory Model One of the earliest work with a memory component applied to language processing is memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA).", "startOffset": 133, "endOffset": 179}, {"referenceID": 35, "context": "Neural Attention and Memory Model One of the earliest work with a memory component applied to language processing is memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA).", "startOffset": 133, "endOffset": 179}, {"referenceID": 35, "context": ", 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA). Following their success, Xiong et al. (2016) proposed dynamic memory networks (DMN) to additionally capture position and temporality of transitive reasoning steps for different QA tasks.", "startOffset": 8, "endOffset": 169}, {"referenceID": 3, "context": "The input utterance is parsed by a dependency parser, and the substructures are built according to the paths from the root to all leaves (Chen and Manning, 2014).", "startOffset": 137, "endOffset": 161}, {"referenceID": 10, "context": "We use the Elman RNN architecture, consisting of an input layer, a hidden layer, and an output layer (Elman, 1990).", "startOffset": 101, "endOffset": 114}, {"referenceID": 7, "context": "To overcome the frequent vanishing gradients issue when modeling long-term dependencies, gated RNN was designed to use a more sophisticated activation function than a usual activation function, consisting of affine transformation followed by a simple element-wise nonlinearity by using gating units (Chung et al., 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al.", "startOffset": 299, "endOffset": 319}, {"referenceID": 14, "context": ", 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 77, "endOffset": 129}, {"referenceID": 6, "context": ", 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 77, "endOffset": 129}, {"referenceID": 25, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 47, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 11, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 36, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 6, "context": "In this paper, we use RNN with GRU cells to allow each recurrent unit to adaptively capture dependencies of different time scales (Cho et al., 2014; Chung et al., 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al.", "startOffset": 130, "endOffset": 168}, {"referenceID": 7, "context": "In this paper, we use RNN with GRU cells to allow each recurrent unit to adaptively capture dependencies of different time scales (Cho et al., 2014; Chung et al., 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al.", "startOffset": 130, "endOffset": 168}, {"referenceID": 7, "context": ", 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al., 2014) A GRU has two gates, a reset gate r, and an update gate z (Cho et al.", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": ", 2014) A GRU has two gates, a reset gate r, and an update gate z (Cho et al., 2014; Chung et al., 2014).", "startOffset": 66, "endOffset": 104}, {"referenceID": 7, "context": ", 2014) A GRU has two gates, a reset gate r, and an update gate z (Cho et al., 2014; Chung et al., 2014).", "startOffset": 66, "endOffset": 104}, {"referenceID": 25, "context": "The dataset for experiments is the benchmark ATIS corpus, which is extensively used by the NLU community (Mesnil et al., 2015).", "startOffset": 105, "endOffset": 126}, {"referenceID": 3, "context": "For experiments with K-SAN, we parse all data with the Stanford dependency parser (Chen and Manning, 2014) and represent words as their embeddings trained on the in-domain data, where the parser is pre-trained on PTB.", "startOffset": 82, "endOffset": 106}, {"referenceID": 17, "context": "cross-entropy, and the optimizer we use is adam with the default setting (Kingma and Ba, 2014), where the learning rate \u03bb = 0.", "startOffset": 73, "endOffset": 94}, {"referenceID": 39, "context": "\u2013 CRF Tagger (Tur et al., 2010): predicts a semantic slot for each word with a context window (size = 5).", "startOffset": 13, "endOffset": 31}, {"referenceID": 25, "context": "\u2013 RNN Tagger (Mesnil et al., 2015): predicts a semantic slot for each word.", "startOffset": 13, "endOffset": 34}, {"referenceID": 16, "context": "\u2013 CNN Encoder-Tagger (Kim, 2014): tag semantic slots with consideration of sentence embeddings learned by a convolutional model.", "startOffset": 21, "endOffset": 32}, {"referenceID": 39, "context": "\u2013 CRF Tagger (Tur et al., 2010): predicts slots based on the lexical (5-word window) and syntactic (dependent head in the parsing tree) features.", "startOffset": 13, "endOffset": 31}, {"referenceID": 22, "context": "\u2013 DCNN (Ma et al., 2015a): predicts slots by incorporating sentence embeddings learned by a convolutional model with consideration of dependency tree structures.", "startOffset": 7, "endOffset": 25}, {"referenceID": 37, "context": "\u2013 Tree-RNN (Tai et al., 2015): predicts slots with sentence embeddings learned by an RNN model based on the tree structures of sentences.", "startOffset": 11, "endOffset": 29}, {"referenceID": 16, "context": "CNN (Kim, 2014) performs better compared to DCNN (Ma et al.", "startOffset": 4, "endOffset": 15}, {"referenceID": 22, "context": "CNN (Kim, 2014) performs better compared to DCNN (Ma et al., 2015a) and Tree-RNN (Tai et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 37, "context": ", 2015a) and Tree-RNN (Tai et al., 2015), even though CNN does not leverage external knowledge when encoding sentences.", "startOffset": 22, "endOffset": 40}, {"referenceID": 0, "context": "AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph (Banarescu et al., 2013), where nodes represent concepts, and labeled directed edges represent the relations between two concepts.", "startOffset": 111, "endOffset": 135}, {"referenceID": 27, "context": "The formalism is based on propositional logic and neoDavidsonian event representations (Parsons, 1990; Davidson, 1967).", "startOffset": 87, "endOffset": 118}, {"referenceID": 8, "context": "The formalism is based on propositional logic and neoDavidsonian event representations (Parsons, 1990; Davidson, 1967).", "startOffset": 87, "endOffset": 118}, {"referenceID": 21, "context": "The semantic concepts in AMR were leveraged to benefit multiple NLP tasks (Liu et al., 2015).", "startOffset": 74, "endOffset": 92}], "year": 2016, "abstractText": "Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks.", "creator": "LaTeX with hyperref package"}}}