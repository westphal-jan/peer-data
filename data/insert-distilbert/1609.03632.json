{"id": "1609.03632", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Joint Extraction of Events and Entities within a Document Context", "abstract": "events and entities are closely related ; entities are neither often actors or participants in events and events without entities are uncommon. the interpretation of events and implied entities is highly contextually dependent. existing work in information extraction typically vastly models events separately from entities, and performs inference at quite the sentence level, ostensibly ignoring the detailed rest of the document. in this paper, we propose a novel approach that models the dependencies among variables consisted of events, entities, and their statistical relations, designs and performs joint inference of these variables across a document. the goal is to enable access to fully document - level contextual information relationships and facilitate context - aware predictions. we demonstrate that our research approach considerably substantially outperforms the state - of - performing the - art methods for event extraction as well as a strong baseline for entity extraction.", "histories": [["v1", "Mon, 12 Sep 2016 23:27:37 GMT  (503kb,D)", "http://arxiv.org/abs/1609.03632v1", "11 pages, 2 figures, published at NAACL 2016"]], "COMMENTS": "11 pages, 2 figures, published at NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["bishan yang", "tom m mitchell"], "accepted": true, "id": "1609.03632"}, "pdf": {"name": "1609.03632.pdf", "metadata": {"source": "CRF", "title": "Joint Extraction of Events and Entities within a Document Context", "authors": ["Bishan Yang", "Tom Mitchell"], "emails": ["bishan@cs.cmu.edu", "tom.mitchell@cs.cmu.edu"], "sections": [{"heading": null, "text": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate contextaware predictions. We demonstrate that our approach substantially outperforms the stateof-the-art methods for event extraction as well as a strong baseline for entity extraction."}, {"heading": "1 Introduction", "text": "Events are things that happen or occur; they involve entities (people, objects, etc.) who perform or are affected by the events and spatio-temporal aspects of the world. Understanding events and their descriptions in text is necessary for any generallyapplicable machine reading systems. It is also essential in facilitating practical applications such as news summarization, information retrieval, and knowledge base construction.\nThe interpretation of event descriptions is highly contextually dependent. To make correct predictions, a model needs to account for mentions of\nevents and entities together with the discourse context. Consider, for example, the following excerpt from a news report:\n\u201cOn Thursday, there was a massive U.S. aerial bombardment in which more than 300 Tomahawk cruise missiles rained down on Baghdad. Earlier Saturday, Baghdad was again targeted. ...\u201d\nThe excerpt describes two U.S. attacks on Baghdad. The two event anchors (triggers) are boldfaced and the mentions of entities and spatio-temporal information are italicized. The first event anchor \u201caerial bombardment\u201d along with its surrounding entity mentions \u2014 \u201cU.S.\u201d, \u201cTomahawk cruise missiles\u201d, and \u201cBaghdad\u201d, describe an attack from the U.S. on Baghdad with Tomahawk cruise missiles being the weapon. The second sentence on its own contains little event-related information, but together with the context of the previous sentence, it indicates another U.S. attack on Baghdad.\nState-of-the-art event extraction systems have difficulties inferring such information due to two main reasons. First, they extract events and entities in separate stages: entities such as people, organization, and locations are first extracted by a named entity tagger, and then these extracted entities are used as inputs for extracting events and their arguments (Li et al., 2013). This often causes errors to propagate. In the above example, if the entity tagger mistakenly identifies \u201cBaghdad\u201d as a person, then the event extractor will fail to extract \u201cBaghdad\u201d as the place where the attack happened. In fact, previous work (Li et al., 2013) observes that using previously extracted entities in event extraction results in ar X\niv :1\n60 9.\n03 63\n2v 1\n[ cs\n.C L\n] 1\n2 Se\np 20\na substantial decrease in performance compared to using gold-standard entity information.\nSecond, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers\u2019 outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Grishman (2010) further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information. Both approaches only propagate the highly confident predictions from the first stage to the second stage. To the best of our knowledge, there is no unified model that jointly extracts events from sentences across the whole document.\nIn this paper, we propose a novel approach that simultaneously extracts events and entities within a document context.1 We first decompose the learning problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the cooccurrence relations between events across the document, and (3) learning for entity extraction. Then we combine the learned models for these subproblems into a joint optimization framework that simultaneously extracts events, semantic roles, and entities in a document. In summary, our main contributions are:\n1. We propose a structured model for learning within-event structures that can effectively capture the dependencies between an event and its arguments, and between the semantic roles and entity types for the arguments.\n2. We introduce a joint inference framework that combines probabilistic models of within-event structures, event-event relations, and entity ex-\n1The code for our system is available at https://github.com/bishanyang/ EventEntityExtractor.\ntraction for joint extraction of the set of entities and events over the whole document.\n3. We conduct extensive experiments on the Automatic Content Extraction (ACE) corpus, and show that our approach significantly outperforms the state-of-the-art methods for event extraction and a strong baseline for entity extraction."}, {"heading": "2 Task Definition", "text": "We adopt the ACE definition for entities ((LDC), 2005a) and events ((LDC), 2005b):\n\u2022 Entity mention: An entity is an object or set of objects in the world. An entity mention is a reference to an entity in the form of a noun phrase or a pronoun.\n\u2022 Event trigger: the word or phrase that clearly expresses its occurrence. Event triggers can be verbs, nouns, and occasionally adjectives like \u201cdead\u201d or \u201cbankrupt\u201d.\n\u2022 Event argument: event arguments are entities that fill specific roles in the event. They mainly include participants (i.e., the entities that are involved in the event) and general event attributes such as place and time, and some event-typespecific attributes that have certain values (e.g., JOB-TITLE, CRIME).\nWe are interested in extracting entity mentions, event triggers, and event arguments. We consider ACE entity types PER, ORG, GPE, LOC, FAC, VEH, WEA and ACE VALUE and TIME expressions2, and focus on 33 ACE event subtypes, each of which has its own set of semantic roles for the potential arguments. There are 35 such roles in total, but we collapse 8 of them that are time-related (e.g., TIME-HOLDS, TIME-AT-END) into one, because most of these roles have very few training examples. Figure 2 shows an example of ACE annotations for events and entities in a sentence. Note that not every entity mention in the sentence is involved in events and a single entity mention can be associated with multiple events.\n2To simplify notation, we include values and times when referring to entities in the rest of the paper."}, {"heading": "3 Approach", "text": "In this section, we describe our approach for joint extraction of events and entities within a document context. We first decompose the learning problem into three tractable subproblems: learning withinevent structures, learning event-event relations, and learning for entity extraction. We will describe the probabilistic models for learning these subproblems. Then we present a joint inference framework that integrates these learned models into a single model to jointly extract events and entities across a document."}, {"heading": "3.1 Learning Within-event Structures", "text": "As described in Section 2, a mention of an event consists of an event trigger and a set of event arguments. Each event argument is also an entity mention with an entity type. In the following, we develop a probabilistic model to learn such dependency structure for each individual event mention.\nGiven a document x, we first generate a set of event trigger candidates T and a set of entity candidates N .3 For each trigger candidate i \u2208 T , we associate it with a discrete variable ti that takes values from the 33 ACE event types and a NONE class indicating other events or no events. Denote the set of entity candidates that are potential arguments for trigger candidate i as Ni.4 For each j \u2208 Ni, we associate it with a discrete variable rij which models the event-argument relation between trigger candidate i and entity candidate j. It takes values from 28 semantic roles and a NONE class indicating invalid\n3We describe how to extract these candidates in Section 4. 4In this paper we only consider entity mentions that are in the same sentence as the trigger to be potential event arguments due to the ACE annotations. However, our model is general and can handle event-argument relations across sentences with appropriate features.\nroles. Each argument candidate j is also associated with an entity type variable aj , which takes values from 9 entity types and a NONE class indicating invalid entity types.\nWe define the joint distribution of variables ti, ri\u00b7 = {rij}j\u2208Ni , and a\u00b7 = {aj}j\u2208Ni conditioned on the observations, which can be factorized according to the factor graph shown in Figure 2:\np\u03b8(ti, ri\u00b7,a\u00b7|i,Ni, x) \u221d exp ( \u03b8T1 f1(ti, i, x)+\u2211\nj\u2208Ni \u03b8T2 f2(rij , i, j, x) + \u2211 j\u2208Ni \u03b8T3 f3(ti, rij , i, j, x)+\n\u2211 j\u2208Ni \u03b8T4 f4(aj , j, x) + \u2211 j\u2208Ni \u03b8T5 f5(rij , aj , j, x) ) (1)\nwhere \u03b81, ...,\u03b85 are vectors of parameters that need to be estimated, and f1, ..., f5 are different forms of feature functions which we will describe later.\nNote that not all configurations of the variables are valid in our model. Based on the definitions in Section 2, each event type takes arguments with certain semantic roles. For example, the arguments of the event MARRY can only play the roles of\nPERSON, TIME, and PLACE. In addition, a NONE event type should not take any arguments. Similarly, each semantic role should be filled with entities with compatible types. For example, the PERSON role type can only be filled with an entity of type PER. However, a NONE role type can be filled with an entity of any type. To account for these compatibility constraints, we enforce the probabilities of all invalid configurations to be zero.\nFeatures. f1, f2, and f4 are unary feature functions that depend on trigger variable ti, argument variable rij , and entity variable aj respectively. We construct a set of features for each feature function (see Table 1). Many of these features overlap with those used in previous work (Li et al., 2013; Li et al., 2014), except for the word embedding features for triggers and the features for entities which are derived from multiple entity resources. f3 and f5 are pairwise feature functions that depend on trigger-argument pair (ti, rij) and argument-entity pair (rij , aj) respectively. We consider simple indicator functions 1t,r and 1r,a as features (1y(x) equals 1 when x = y and 0 otherwise).\nTraining. For model training, we find the optimal parameters \u03b8 using the maximum-likelihood estimates with an L2 regularization:\n\u03b8\u2217 = argmax \u03b8 L(\u03b8)\u2212 \u03bb||\u03b8||22\nL(\u03b8) = \u2211 i log p(ti, ri\u00b7,a\u00b7|i,Ni, x)\nWe use L-BFGS to optimize the training objective. To calculate the gradient, we use the sum-product algorithm to compute the exact marginals for the unary cliques ti, rij , aj and the pairwise cliques (ti, rij), (rij , aj). Typically the training complexity for graphical models with unary and pairwise cliques is quadratic in the size of the label set. However, the complexity of our model is much lower than that since we only need to compute the joint distributions over valid variable configurations. Denote the number of event subtypes as T , the number of event argument roles as N , the average number of argument roles for each event subtype as k1, the average number of entity types for each event argument as k2, and the average number of argument candidates for each trigger candidate as M . The complexity of computing the joint distribution\nisO(M\u00d7(k1T+k2N)), and k1 and k2 are expected to be small in practice (k1 = 6, k2 = 3 in ACE)."}, {"heading": "3.2 Learning Event-Event Relations", "text": "So far we have described a model for learning structures for a single event. However, the inference of the event types for individual events may depend on other events that are mentioned in the document. For example, an ATTACK event is more likely to occur with INJURE and DIE events than with life events like MARRY and BORN. In order to capture this intuition, we develop a pairwise model of event-event relations in a document.\nOur training data consists of all pairs of trigger candidates that co-occur in the same sentence or are connected by a coreferent subject/object if they are in different sentences.5 We want to propagate information between these trigger pairs since they are more likely to be related.\nFormally, given a trigger candidate pair (i, i\u2032), we estimate the probabilities for their event types (ti, ti\u2032) as\np\u03c6(ti, ti\u2032 |x, i, i\u2032) \u221d exp ( \u03c6T g(ti, ti\u2032 , x, i, i \u2032) ) (2)\nwhere \u03c6 is a vector of parameters and g is a feature function that depends on the trigger candidate pair and their context. We consider both trigger-specific features and relational features. For trigger-specific features, we use the same trigger features listed in Table 1. For relational features, we consider for each pair of trigger candidates: (1) whether they are connected by a conjunction dependency relation (based on dependency parsing); (2) whether they share a subject or an object (based on dependency parsing and coreference resolution); (3) whether they have the same head word lemma; (4) whether they share a semantic frame based on FrameNet. During training, we use L-BFGS to compute the maximumlikelihood estimates of \u03c6."}, {"heading": "3.3 Entity Extraction", "text": "For entity extraction, we trained a standard linearchain Conditional Random Field (CRF) (Lafferty et al., 2001) using the BIO scheme (i.e., identifying the Beginning, the Inside and the Outside of the\n5We use the Stanford coreference system (Lee et al., 2013) for within-document entity coreference.\ntext segments). We use features that are similar to those from previous work (Ratinov and Roth, 2009): (1) current words and part-of-speech tags; (2) context words in a window of size 2; (3) word type such as all-capitalized, is-capitalized, and all-digits; (4) Gazetteer-based entity type if the current word matches an entry in the gazetteers collected from Wikipedia (Ratinov and Roth, 2009). In addition, we consider pre-trained word embeddings (Mikolov et al., 2013) as dense features for each word in order to improve the generalizability of the model."}, {"heading": "3.4 Joint Inference", "text": "Our end goal is to extract coherent event mentions and entity mentions across a document. To achieve this, we propose a joint inference approach that allows information flow among the three local models and finds globally-optimal assignments of all variables, including the trigger variables t, the argument role variables r, and the entity variables a. Specifically, we define the following objective:\nmax t,r,a \u2211 i\u2208T E(ti, ri\u00b7,a\u00b7)+ \u2211 i,i\u2032\u2208T R(ti, ti\u2032)+ \u2211 j\u2208N D(aj)\n(3)\nThe first term is the sum of confidence scores for individual event mentions based on the parameter estimates from the within-event model. E(ti, ri\u00b7,a\u00b7) can be further decomposed into three parts.\nE(ti, ri\u00b7,a\u00b7) = log p\u03b8(ti|i,Ni, x) + \u2211 j\u2208Ni log p\u03b8(ti, rij |i,Ni, x)\n+ \u2211 j\u2208Ni log p\u03b8(rij , aj |i,Ni, x)\nThe second term is the sum of confidence scores for event relations based on the parameter estimates from the pairwise event model, where R(ti, ti\u2032) = log p\u03c6(ti, ti\u2032 |i, i\u2032, x). The third term is the sum of confidence scores for entity mentions, where D(aj) = log p\u03c8(aj |j, x) and p\u03c8(aj |j, x) is the marginal probability derived from the linear-chain CRF described in Section 3.3. The optimization is subjected to agreement constraints that enforce the overlapping variables among the three components to agree on their values.\nThe joint inference problem can be formulated as an integer linear program (ILP). To solve it efficiently, we find solutions for the relaxation of\nthe problem using a dual decomposition algorithm AD3 (Martins et al., 2011). AD3 has been shown to be orders of magnitude faster than a general purpose ILP solver in practice (Das et al., 2012). It is also particularly suitable for our problem since it involves decompositions that have many overlapping simple factors. We observed that AD3 recovers the exact solutions for all the test documents in our experiments and the runtime for labeling each document is only three seconds in average in a 64-bit machine with two 2GHz CPUs and 8GB of RAM."}, {"heading": "4 Experiments", "text": "We conduct experiments on the ACE2005 corpus.6 It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al. (2013). Table 2 shows the data statistics.\nWe adopt the evaluation metrics for events as defined in Li et al. (2013). An event trigger is correctly identified if its offsets match those of a goldstandard trigger; and it is correctly classified if its event subtype (33 in total) also match the subtype of the gold-standard trigger. An event argument is correctly identified if its offsets and event subtype match those of any of the reference argument mentions in the document; and it is correctly classified if its semantic role (28 in total) is also correct. For entities, a predicted mention is correctly extracted if its head offsets and entity type (9 in total) match those of the reference entity mention.\nNote that our approach requires entity mention candidates and event trigger candidates as input. Instead of enumerating all possible text spans, we generate high-quality entity mentions from the kbest predictions of our CRF entity extractor (in Section 3.3).7 Similarly, we train a CRF for event trigger extraction using the same features except for the gazetteers, and generate trigger candidates based on the k-best predictions. We set k = 50 for entities and k = 10 for event triggers based on performance on the development set. They cover 92.3% of the gold-standard entity mentions and 96.3% of the gold-standard event triggers in the test set.\n6http://www.itl.nist.gov/iad/mig/tests/ ace/2005/\n7During training, we randomly split the training data into 10"}, {"heading": "4.1 Results", "text": "Event Extraction. We compare the proposed models WITHINEVENT (in Section 3.1) and JOINTEVENTENTITY (in Section 3.4) with two strong baselines. One is JOINTBEAM (Li et al., 2013), a state-of-the-art event extractor that uses a structured perceptron with beam search for sentence-level joint extraction of event triggers and arguments. The other is STAGEDMAXENT, a typical two-stage approach that detects event triggers first and then event arguments. We use the same event trigger candidates and entity mention candidates as input to all the comparing models except for JOINTBEAM, because JOINTBEAM only extracts event mentions and assumes entity mentions are given. We consider a realistic experimental setting where no gold-standard annotations are available for entities during testing. To obtain results from JOINTBEAM, we ran the actual system8 used in Li et al. (2013) using the entity mentions output by our CRF-based entity extractor.\nTable 3 shows the average9 precision, recall, and F1 score for event triggers and event arguments. We can see that our WITHINEVENT model, which explicitly models the trigger-argument dependencies and argument-role-entity-type dependencies, outperforms the MaxEnt pipeline, especially in event argument extraction. This shows that modeling the trigger-argument dependencies is effective in reducing error propagation.\nComparing to the state-of-the-art event extractor JOINTBEAM, the improvements introduced by WITHINEVENT are substantial in both event triggers and event arguments. We believe there are two main reasons: (1) WITHINEVENT considers all possible joint trigger/argument label assignments, whereas\nparts and consider the k-best predictions for each part. 8https://github.com/oferbr/ BIU-RPI-Event-Extraction-Project 9We report the micro-average scores as in previous work (Li et al., 2013).\nJOINTBEAM considers only a subset of the possible assignments based on a heuristic beam search. More specifically, when predicting labels for token i, JointBeam considers only the K-best (K = 4 in their paper) partial trigger/argument label configurations for the previous i \u2212 1 tokens. As the length of the sentence increases, a large amount of information will be thrown away. (2) WITHINEVENT models argument-role-entity-type dependencies, whereas JOINTBEAM assumes the entity types are given. This can cause error propagation.\nJOINTEVENTENTITY provides the best performance among all the models on all evaluation categories. It boosts both precision and recall compared to WITHINEVENT.10 This demonstrates the advantages of JOINTEVENTENTITY in allowing information propagation across event mentions and entity mentions and making more context-aware and semantically coherent predictions.\nWe also compare the results of JOINTEVENTENTITY with the best known results on the ACE event\n10All significance tests reported in this paper were computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012) with 10,000 samples of the test documents.\nextraction task in Table 4. CROSS-DOC (Ji and Grishman, 2008) performs cross-document inference of events using document clustering information, and CNN (Nguyen and Grishman, 2015) is a convolutional neural network for extracting event triggers at the sentence level. We see that JOINTEVENTENTITY outperforms both models and achieves new state-of-the-art results for event trigger and argument extraction in an end-to-end evaluation setting.\nEntity Extraction. In addition to extracting event mentions, JOINTEVENTENTITY also extracts entity mentions. We compare its output with the output of a strong entity extraction baseline CRFENTITY (described in Section 3.3). Table 5 shows the (micro)average precision, recall, and F1 score. We see that JOINTEVENTENTITY introduces a significant improvement in recall and F1. Table 6 further shows the F1 score for four major entity types PER, GPE, ORG, and TIME in ACE. The promising improvements indicate that joint modeling of events and entities allows for more accurate predictions about not only events but also entities."}, {"heading": "4.2 Error Analysis", "text": "Table 7 divides the errors made by JOINTEVENTENTITY based on different subtasks and the classification error types in each task. For event triggers, the majority of the errors relates to missing triggers and only 3.7% involves misclassified event types (e.g., a DEMONSTRATION event is mistaken for a TRANSPORT event). Among the missing triggers, we examine the cases where the event types are correctly identified in a sentence but with in-\ncorrect triggers and find that there are only 5% of such cases. For event arguments, the majority of the errors relates to missing arguments and only 4.1% is about misclassified argument roles. Among the missing event arguments, 10% of them has correctly identified entity types.\nIn general, the errors for event extraction are commonly due to three reasons: (1) Lexical sparsity. For example, in the sentence \u201cAt least three members of a family ... were hacked to death ...\u201d, our model fails to detect that \u201chacked\u201d triggers an ATTACK event, because it has never seen \u201chacked\u201d with this sense during training. Using WordNet and pretrained word vectors may alleviate the sparsity issue. It is also important to disambiguate word senses in context. (2) Shallow understanding of context, especially long-range context. For example, given the sentence \u201cShe is being held on 50,000 dollars bail on a charge of first-degree reckless homicide ...\u201d, the model detects that \u201chomicide\u201d triggers an event, but fails to detect that \u201cShe\u201d refers to the AGENT who committed the homicide. This is mainly due to the complex long-distance dependency between the trigger and the argument. (3) Use of complex language such as metaphor, idioms, and sarcasm. Addressing these phenomena is in general difficult since it requires richer background knowledge and more sophisticated inference.\nFor entity extraction, we find that integrating event information into entity extraction successfully improves recall and F1. However, since the ACE dataset is restricted to a limited set of events, a large portion of the sentences does not contain any event triggers and event arguments that are of interest. For these sentences, there is little or no benefit of joint modeling. We also find that some entity misclassification errors can be avoided if entity coreference information is available. We plan to investigate coreference resolution as an additional component to our joint model in future work."}, {"heading": "5 Related Work", "text": "Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedical data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bjo\u0308rne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropriate features for events triggers, argument roles, and entities. We consider this as future work.\nThere has been work on improving event extraction by exploiting document-level context. Berant et al. (2014) exploits event-event relations, e.g., causality, inhibition, which frequently occur in biological texts. For general texts most work focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to propagation event classification decisions (Ji and Grishman, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a DIE event tends to co-occur with ATTACK events and TRANSPORT events). It can be extended to handle other types of event relations (e.g., causal and temporal) by designing appropriate features. Chambers and\nJurafsky (2009; 2011) learn narrative schemas by linking event verbs that have coreferring syntactic arguments. Our model also adopts this intuition to relate event triggers across sentences. In addition, each event argument is grounded by its entity type (e.g., an entity mention of type PER can only fill roles that can be played by a person)."}, {"heading": "6 Conclusion", "text": "In this paper, we introduce a new approach for automatic extraction of events and entities across a document. We first decompose the learning problem into three tractable subproblems: learning within-event structures, learning event-event relations, and learning for entity extraction. We then integrate these learned models into a single model that performs joint inference of all event triggers, semantic roles for events, and entities across the whole document. Experimental results demonstrate that our approach outperforms the state-of-the-art event extractors by a large margin and substantially improves a strong entity extraction baseline. For future work, we plan to integrate entity and event coreference as additional components into the joint inference framework. We are also interested in investigating the integration of more sophisticated event-event relation models of causality and temporal ordering."}, {"heading": "Acknowledgments", "text": "This work was supported in part by NSF grant IIS1250956, and in part by the DARPA DEFT program under contract FA87501320005. We would like to thank members of the CMU NELL group for helpful comments. We also thank the anonymous reviewers for insightful suggestions."}], "references": [{"title": "The stages of event extraction", "author": ["David Ahn"], "venue": "In Proceedings of the Workshop on Annotating and Reasoning about Time and Events,", "citeRegEx": "Ahn.,? \\Q2006\\E", "shortCiteRegEx": "Ahn.", "year": 2006}, {"title": "Modeling biological processes for reading comprehension", "author": ["Vivek Srikumar", "PeiChun Chen", "Brad Huang", "Christopher D Manning", "Abby Vander Linden", "Brittany Harding", "Peter Clark"], "venue": "In Proceedings of the 2014 Con-", "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "An empirical investigation of statistical significance in nlp", "author": ["David Burkett", "Dan Klein"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Berg.Kirkpatrick et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2012}, {"title": "Extracting complex biological events with rich graph-based feature sets", "author": ["Bj\u00f6rne et al.2009] Jari Bj\u00f6rne", "Juho Heimonen", "Filip Ginter", "Antti Airola", "Tapio Pahikkala", "Tapio Salakoski"], "venue": "In Proceedings of the Workshop on Current Trends in Biomedi-", "citeRegEx": "Bj\u00f6rne et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bj\u00f6rne et al\\.", "year": 2009}, {"title": "Seed-based event trigger labeling: How far can event descriptions get us? In ACL Volume 2: Short Papers, pages 372\u2013376", "author": ["Ido Dagan", "Qi Li", "Heng Ji", "Anette Frank"], "venue": null, "citeRegEx": "Bronstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bronstein et al\\.", "year": 2015}, {"title": "Jointly combining implicit constraints improves temporal ordering", "author": ["Chambers", "Jurafsky2008] Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chambers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["Chambers", "Jurafsky2009] Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Chambers et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2009}, {"title": "Template-based information extraction without the templates", "author": ["Chambers", "Jurafsky2011] Nathanael Chambers", "Dan Jurafsky"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-", "citeRegEx": "Chambers et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chambers et al\\.", "year": 2011}, {"title": "Event extraction via dynamic multi-pooling convolutional neural networks", "author": ["Chen et al.2015] Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "An exact dual decomposition algorithm for shallow semantic parsing with constraints", "author": ["Das et al.2012] Dipanjan Das", "Andr\u00e9 FT Martins", "Noah A Smith"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Das et al\\.", "year": 2012}, {"title": "Joint inference for event timeline construction", "author": ["Do et al.2012] Quang Xuan Do", "Wei Lu", "Dan Roth"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-", "citeRegEx": "Do et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Do et al\\.", "year": 2012}, {"title": "The automatic content extraction (ace) program-tasks, data, and evaluation", "author": ["Alexis Mitchell", "Mark A Przybocki", "Lance A Ramshaw", "Stephanie Strassel", "Ralph M Weischedel"], "venue": "In Proceedings of the Fourth In-", "citeRegEx": "Doddington et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Doddington et al\\.", "year": 2004}, {"title": "Refining event extraction through crossdocument inference", "author": ["Ji", "Grishman2008] Heng Ji", "Ralph Grishman"], "venue": "In Proceedings of ACL-08: HLT,", "citeRegEx": "Ji et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2008}, {"title": "Event extraction as frame-semantic parsing", "author": ["Judea", "Strube2015] Alex Judea", "Michael Strube"], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM", "citeRegEx": "Judea et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Judea et al\\.", "year": 2015}, {"title": "Overview of bionlp\u201909 shared task on event extraction", "author": ["Kim et al.2009] Jin-Dong Kim", "Tomoko Ohta", "Sampo Pyysalo", "Yoshinobu Kano", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Workshop on Current Trends", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": "In Proc. 18th International Conf. on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Deterministic coreference resolution based on entity-centric, precision-ranked rules", "author": ["Lee et al.2013] Heeyoung Lee", "Angel Chang", "Yves Peirsman", "Nathanael Chambers", "Mihai Surdeanu", "Dan Jurafsky"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Joint event extraction via structured prediction with global features", "author": ["Li et al.2013] Qi Li", "Heng Ji", "Liang Huang"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Constructing information networks using one single model", "author": ["Li et al.2014] Qi Li", "Heng Ji", "Yu Hong", "Sujian Li"], "venue": "In Proceedings of the Conference on Empirical Methods on Natural Language Processing (EMNLP),", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Using document level cross-event inference to improve event extraction", "author": ["Liao", "Grishman2010] Shasha Liao", "Ralph Grishman"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Liao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2010}, {"title": "Nomlex: A lexicon of nominalizations", "author": ["Ralph Grishman", "Adam Meyers", "Leslie Barrett", "Ruth Reeves"], "venue": "In Proceedings of EURALEX,", "citeRegEx": "Macleod et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Macleod et al\\.", "year": 1998}, {"title": "An augmented lagrangian approach to constrained map inference", "author": ["Mario AT Figeuiredo", "Pedro MQ Aguiar", "Noah A Smith", "Eric P Xing"], "venue": "In Proceedings of the International Conference on Machine Learning", "citeRegEx": "Martins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2011}, {"title": "Learning constraints for consistent timeline extraction", "author": ["McClosky", "Manning2012] David McClosky", "Christopher D Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "McClosky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2012}, {"title": "Event extraction as dependency parsing", "author": ["Mihai Surdeanu", "Christopher D Manning"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "McClosky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Event detection and domain adaptation with convolutional neural networks", "author": ["Nguyen", "Grishman2015] Thien Huu Nguyen", "Ralph Grishman"], "venue": "In Proceedings of ACL-IJCNLP 2015 Volume 2: Short Papers,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Joint inference for knowledge extraction from biomedical literature", "author": ["Poon", "Vanderwende2010] Hoifung Poon", "Lucy Vanderwende"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Poon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Fast and robust joint models for biomedical event extraction", "author": ["Riedel", "McCallum2011] Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Riedel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "First, they extract events and entities in separate stages: entities such as people, organization, and locations are first extracted by a named entity tagger, and then these extracted entities are used as inputs for extracting events and their arguments (Li et al., 2013).", "startOffset": 254, "endOffset": 271}, {"referenceID": 17, "context": "In fact, previous work (Li et al., 2013) observes that using previously extracted entities in event extraction results in ar X iv :1 60 9.", "startOffset": 23, "endOffset": 40}, {"referenceID": 17, "context": "Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015).", "startOffset": 122, "endOffset": 190}, {"referenceID": 17, "context": "Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers\u2019 outputs to satisfy document-wide (or document-cluster-wide) consistency.", "startOffset": 123, "endOffset": 302}, {"referenceID": 17, "context": "Second, most existing work extracts events independently from each individual sentence, ignoring the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Grishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers\u2019 outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Grishman (2010) further improved the rule-based inference by training additional classifiers for event triggers and arguments using document-level information.", "startOffset": 123, "endOffset": 600}, {"referenceID": 17, "context": "Many of these features overlap with those used in previous work (Li et al., 2013; Li et al., 2014), except for the word embedding features for triggers and the features for entities which are derived from multiple entity resources.", "startOffset": 64, "endOffset": 98}, {"referenceID": 18, "context": "Many of these features overlap with those used in previous work (Li et al., 2013; Li et al., 2014), except for the word embedding features for triggers and the features for entities which are derived from multiple entity resources.", "startOffset": 64, "endOffset": 98}, {"referenceID": 15, "context": "For entity extraction, we trained a standard linearchain Conditional Random Field (CRF) (Lafferty et al., 2001) using the BIO scheme (i.", "startOffset": 88, "endOffset": 111}, {"referenceID": 16, "context": "We use the Stanford coreference system (Lee et al., 2013) for within-document entity coreference.", "startOffset": 39, "endOffset": 57}, {"referenceID": 20, "context": "nominalization of the words based on Nomlex (Macleod et al., 1998)", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": "similarity features between the head word and a list of trigger seeds based on WordNet (Bronstein et al., 2015) 5.", "startOffset": 87, "endOffset": 111}, {"referenceID": 18, "context": "semantic frames that associate with the head word and its p-o-s tag based on FrameNet (Li et al., 2014) 6.", "startOffset": 86, "endOffset": 103}, {"referenceID": 24, "context": "pre-trained vector for the head word (Mikolov et al., 2013)", "startOffset": 37, "endOffset": 59}, {"referenceID": 24, "context": "In addition, we consider pre-trained word embeddings (Mikolov et al., 2013) as dense features for each word in order to improve the generalizability of the model.", "startOffset": 53, "endOffset": 75}, {"referenceID": 21, "context": "the problem using a dual decomposition algorithm AD3 (Martins et al., 2011).", "startOffset": 53, "endOffset": 75}, {"referenceID": 9, "context": "AD3 has been shown to be orders of magnitude faster than a general purpose ILP solver in practice (Das et al., 2012).", "startOffset": 98, "endOffset": 116}, {"referenceID": 17, "context": "We use the same data split as in Li et al. (2013). Table 2 shows the data statistics.", "startOffset": 33, "endOffset": 50}, {"referenceID": 17, "context": "We adopt the evaluation metrics for events as defined in Li et al. (2013). An event trigger is correctly identified if its offsets match those of a goldstandard trigger; and it is correctly classified if its event subtype (33 in total) also match the subtype of the gold-standard trigger.", "startOffset": 57, "endOffset": 74}, {"referenceID": 17, "context": "One is JOINTBEAM (Li et al., 2013), a state-of-the-art event extractor that uses a structured perceptron with beam search for sentence-level joint extraction of event triggers and arguments.", "startOffset": 17, "endOffset": 34}, {"referenceID": 17, "context": "One is JOINTBEAM (Li et al., 2013), a state-of-the-art event extractor that uses a structured perceptron with beam search for sentence-level joint extraction of event triggers and arguments. The other is STAGEDMAXENT, a typical two-stage approach that detects event triggers first and then event arguments. We use the same event trigger candidates and entity mention candidates as input to all the comparing models except for JOINTBEAM, because JOINTBEAM only extracts event mentions and assumes entity mentions are given. We consider a realistic experimental setting where no gold-standard annotations are available for entities during testing. To obtain results from JOINTBEAM, we ran the actual system8 used in Li et al. (2013) using the entity mentions output by our CRF-based entity extractor.", "startOffset": 18, "endOffset": 731}, {"referenceID": 17, "context": "BIU-RPI-Event-Extraction-Project We report the micro-average scores as in previous work (Li et al., 2013).", "startOffset": 88, "endOffset": 105}, {"referenceID": 17, "context": "JOINTBEAM (Li et al., 2013) 76.", "startOffset": 10, "endOffset": 27}, {"referenceID": 2, "context": "All significance tests reported in this paper were computed using the paired bootstrap procedure (Berg-Kirkpatrick et al., 2012) with 10,000 samples of the test documents.", "startOffset": 97, "endOffset": 128}, {"referenceID": 11, "context": "Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedical data for the BioNLP shared tasks (Kim et al.", "startOffset": 60, "endOffset": 85}, {"referenceID": 14, "context": ", 2004) and biomedical data for the BioNLP shared tasks (Kim et al., 2009).", "startOffset": 56, "endOffset": 74}, {"referenceID": 0, "context": "To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj\u00f6rne et al., 2009).", "startOffset": 144, "endOffset": 176}, {"referenceID": 3, "context": "To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj\u00f6rne et al., 2009).", "startOffset": 144, "endOffset": 176}, {"referenceID": 8, "context": "Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015).", "startOffset": 91, "endOffset": 137}, {"referenceID": 17, "context": "As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al.", "startOffset": 177, "endOffset": 194}, {"referenceID": 23, "context": ", 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011).", "startOffset": 85, "endOffset": 108}, {"referenceID": 0, "context": "To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj\u00f6rne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees.", "startOffset": 145, "endOffset": 771}, {"referenceID": 0, "context": "To reduce task complexity, early work employs a pipeline of classifiers that extracts event triggers first, and then determines their arguments (Ahn, 2006; Bj\u00f6rne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suffer from error propagation, researchers have proposed methods for joint extraction of event triggers and arguments, using either structured perceptron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algorithms (McClosky et al., 2011). However, existing joint models largely rely on heuristic search to aggressively shrink the search space. One exception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint inference with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries.", "startOffset": 145, "endOffset": 902}, {"referenceID": 10, "context": "For general texts most work focuses on exploiting temporal event relations (Chambers and Jurafsky, 2008; Do et al., 2012; McClosky and Manning, 2012).", "startOffset": 75, "endOffset": 149}, {"referenceID": 1, "context": "Berant et al. (2014) exploits event-event relations, e.", "startOffset": 0, "endOffset": 21}], "year": 2016, "abstractText": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate contextaware predictions. We demonstrate that our approach substantially outperforms the stateof-the-art methods for event extraction as well as a strong baseline for entity extraction.", "creator": "LaTeX with hyperref package"}}}