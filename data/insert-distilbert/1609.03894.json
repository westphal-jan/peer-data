{"id": "1609.03894", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Crafting a multi-task CNN for viewpoint estimation", "abstract": "implicit convolutional neural networks ( cnns ) were recently shown to provide state - of - the - art results for object evaluation category product viewpoint estimation. however different ways of formulating this problem have been proposed and the competing computational approaches have been explored with very different design choices. this further paper further presents a comparison of these approaches, in a unified setting instead as well as a detailed analysis of identifying the key factors occurring that impact performance. followingly, we present a new joint training method with the detection task and demonstrate regarding its benefit. we are also highlight the superiority of classification approaches over regression approaches, quantify the benefits accumulated of deeper architectures and extended training diagnostic data, and demonstrate that synthetic data is beneficial even when calculated using imagenet training validation data. by combining all these elements, we demonstrate an improvement of approximately 5 % mavp over previous state - much of - the - form art results on the pascal3d + dataset. in particular for their most challenging 24 view classification task we improve the results from between 31. 43 1 % to 36. 1 % mavp.", "histories": [["v1", "Tue, 13 Sep 2016 15:19:38 GMT  (52kb,D)", "http://arxiv.org/abs/1609.03894v1", "To appear in BMVC 2016"]], "COMMENTS": "To appear in BMVC 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["francisco massa", "renaud marlet", "mathieu aubry"], "accepted": false, "id": "1609.03894"}, "pdf": {"name": "1609.03894.pdf", "metadata": {"source": "CRF", "title": "Crafting a multi-task CNN for viewpoint estimation", "authors": ["Francisco Massa", "Renaud Marlet", "Mathieu Aubry"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Joint object detection and viewpoint estimation is a long-standing problem in computer vision. While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories. The interest in this problem has recently increased both by the availability of the Pascal3D+ dataset [29], which provides a standard way to compare algorithms on diverse classes, and by the improved performance of object detection, which encouraged researchers to focus on extracting more complex information from the images than the position of objects.\nConvolutional Neural Networks were recently applied successfully to this task of object category pose estimation [27, 28], leading to large improvements of state-of-the-art results on the Pascal3D+ benchmark. However many elements play an important role in the quality of these results, which have not yet been fully analyzed. In particular, several approaches have been proposed, such as a regression approach with joint training for detection [20, 21], a direct viewpoint classification [28], and a geometric structure aware fine-grained viewpoint classification [27], where the authors modify the classification objective to take into account\nc\u00a9 2016. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\nar X\niv :1\n60 9.\n03 89\n4v 1\n[ cs\nthe uncertainty of the annotations and encode implicitly the topology of the pose space. These papers however differ in a number of other ways, such as the training data or the network architecture they use, making it difficult to compare performances. We explore systematically the essential design choices for a CNN-based approach to pose estimation and we demonstrate that a number of elements influence the performance of the final algorithm in an important way.\nContributions\nIn this paper, we study several factors that affect performance for the task of joint object detection and pose estimation with CNNs. Using the best design options, we rationally define an effective method to integrate detection and viewpoint estimation, quantify its benefits, as well as the boost given by deeper networks and more training data, including data from ImageNet and synthetic data. We demonstrate that the combination of all these elements leads to an important improvement over state-of-the-art results on Pascal3D+, from 31.1% to 36.1% AVP in the case of the most challenging 24 viewpoints classification. While several of the elements that we employ have been used in previous work [21, 27, 28], we know of no systematic study of their respective and combined effect, resulting in an absence of clear good practices for viewpoint estimation and sub-optimal performances. Our code is available at http://imagine.enpc.fr/~suzano-f/bmvc2016-pose/.\nRelated work\nConvolutional Neural Networks. While convolutional neural network have a long history in computer vision (e.g. [15]), their use has been generalized only in 2012 after the demonstration of their benefits by Krizhevsky et al. [14] on the ImageNet large-scale visual recognition challenge [4]. Since then, they have been used to increase performances on many vision tasks.\nThis has been true in particular for object detection, where the R-CNN technique of Girshick et al. [8] provided an important improvement over previous methods on the Pascal VOC dataset [5]. Relying on an independent method to provide bounding box proposals for the objects in the image, R-CNN fine-tunes a network pre-trained on ImageNet to classify these proposal as objects or background. This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].\nViewpoint estimation. Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25]. These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].\nWith the advent of Pascal3D+ dataset [29], which extends Pascal VOC dataset [5] by aligning a set of 3D CAD models for 12 rigid object classes, learning-based approaches using only on example images became possible and proved their superior performance. For example, Xiang et al. [29] extended the method of [22], which uses an adaptation of DPM with 3D constraints to estimate the pose. CNN-based approaches, which were until the availability of the Pascal3D+ data limited to special cases such as faces [20] and small datasets\n[21], also began to be applied to this problem at a larger scale. In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data. [28] used a simple classification approach with the VGG16 network [26] and annotations for ImageNet objects and established the current state-of-the-art on Pascal3D+. [27] introduced a discrete but fine-grained formulation of the pose estimation which takes into account the geometry of the pose space, and demonstrate using AlexNet that adding rendered CAD models could improve the results over using Pascal VOC data alone."}, {"heading": "2 Overview", "text": "We focus on the problem of detecting and estimating the pose of objects in images, as defined by the Pascal3D+ challenge Average Viewpoint Precision (AVP) metric. In particular, we focus on the estimation of the azimuthal angle. For object detection, we use the standard Fast R-CNN framework [7], which relies on region proposal but is significantly faster than the original R-CNN [8]. In addition, we associate a viewpoint to each bounding box and for each object class. Indeed, since viewpoint conventions may not be coherent for the different classes, we learn a different estimator for each class. However, to avoid having to learn one network per class, we share all but the last layer of the network between the different classes.\nIn Section 3, we first discuss different approaches to viewpoint prediction with CNNs and in particular the differences between regression and classification approaches. Then in Section 4, we introduce different ways to integrate the viewpoint estimation and the detection problem. Finally, in Section 5 we present the results of the different methods as well as a detailed analysis of different factors that impact performance.\nNotations. We call Ns be the number of training samples and Nc the number of object classes. For i \u2208 {1, ...,Ns} we associate to the i-th training sample xi its azimuthal angle \u03b8 i \u2208 [0,2\u03c0[, its class ci \u2208 {1, ...,Nc} and the output of the network with parameters w, f w(xi). The viewpoints are often discretized and we call Nv the number of bins, and \u03b8\u0303 i \u2208 {1, ...,Nv} the bin that includes \u03b8 i. We use subscripts to denote the elements of a tensor; for example, f w(xi)k,l is the element (k, l) from tensor f w(xi)."}, {"heading": "3 Approaches for viewpoint estimation", "text": "In this section, we assume the bounding box and the class of the objects are known and we focus on the different approaches to estimate their pose. Section 3.1 first discusses the design of regression approaches. Section 3.2 then presents two variants of classification approaches. The intuition behind these different approaches are visualized on Figure 1."}, {"heading": "3.1 Viewpoint estimation as regression", "text": "The azimuth angle of a viewpoint being a continuous quantity, it is natural to tackle pose estimation as a regression problem. The choice of the pose representation F(\u03b8) of an azimuthal angle \u03b8 is of course crucial for the effectiveness of this regression. Indeed, if we simply consider F(\u03b8) = \u03b8 , the periodicity of the pose is not taken into account. Thus, as highlighted in [20], a good pose representation F(\u03b8) satisfies the following properties: (a) it is invariant to the periodicity of the angle \u03b8 , and (b) it is analytically invertible.\nWe explore two representations which satisfy both properties:\n(i) F(\u03b8) = [ cos(\u03b8), sin(\u03b8) ] , probably the simplest way to represent orientations, used\nfor example in [21]; (ii) F(\u03b8) = [ cos ( \u03b8 \u2212 \u03c03 ) , cos(\u03b8) , cos ( \u03b8 + \u03c03 )] , a formulation which was presented in\n[20], and that has a higher dimensionality than the previous one, allowing more flexibility for the network to better capture the pose information.\nThese representations have different output dimensionality Nd , respectively 2 and 3, and we designate the associated regressions by regression 2D and regression 3D respectively. Since we treat the regression independently for each class, the outputs f w(x) of the network that we train for pose estimation have values in RNc\u00d7Nd and we designate by f w(x)c,k the angular element k of the output for class c.\nFor training the regression with these representations, we used the Huber loss (also known as Smooth L1) on each component of the pose representation F(\u03b8). It is known to be more robust to outliers than the Euclidean loss and provides much better results in our experiments. Our regression loss can then be written:\nLreg(w) = Ns\n\u2211 i=1\nNd\n\u2211 k=1 H( f w(xi)ci,k\u2212F(\u03b8 i)k) (1)\nwith H the Huber loss. Given the output f w(x)c,\u2022 of the network for a sample x of class c, we can estimate its pose simply by computing the pose of the closest point on the curve described by F (cf. Figure 1). Other regression approaches and loss are discussed in [19] but lead to lower performances."}, {"heading": "3.2 Viewpoint estimation as classification", "text": "As pointed out by [27], the main limitation of a regression approach to viewpoint estimation is that it cannot represent well the ambiguities that may exist between different viewpoints. Indeed, objects such as a table have symmetries or near symmetries that make the viewpoint estimation problem intrinsically ambiguous, and this ambiguity is not well handled by the representations discussed in the previous paragraph. One solution to this problem is to discretize the pose space and predict a probability for each orientation bin, thus formulating the problem as one of classification. Note that a similar difficulty is found in the problem of keypoint prediction, for which the similar solution of predicting a heat map for each keypoint instead of predicting directly its position has proven successful [28].\nIn the case of a classification approach, the output of the network belongs to RNc\u00d7Nv and each value can be interpreted as a log-probability. We write f w(x)c,v the value corresponding to the orientation bin v for an input x of class c."}, {"heading": "3.2.1 Direct classification", "text": "The approach successfully applied in [28] is to simply predict, for each class independently, the bin in which the orientation of the object falls. This classification problem can be addressed for each object class with the standard cross-entropy loss:\nLclassif(w) =\u2212 Ns\n\u2211 i=1 log\n( exp( f w(xi)ci,\u03b8\u0303 i)\n\u2211Nvv=1 exp( f w(xi)ci,v)\n) (2)\nAt test time, the predicted angular bin \u03b8\u0302(x,c) for an input x of class c is given by\n\u03b8\u0302(x,c) = argmax v\u2208{1,...,Nv}\nf w(x)c,v (3)"}, {"heading": "3.2.2 Geometric structure aware classification", "text": "The drawback of the previous classification approach is that it learns to predict the poses without using explicitly the continuity between close viewpoints. Two neighboring bins have indeed a lot in common. This geometrical information may be especially important for fine-grained orientation prediction, where only few examples per bin are available.\nA solution to this problem was proposed in [27]. The authors finely discretize the orientations in Nv = 360 bins and consider the angle estimation as a classification problem, but adapt the loss to include a structured relation between neighboring bins and penalize less angle errors that are smaller:\nLgeom(w) =\u2212 Ns\n\u2211 i=1\nNv\n\u2211 v=1\nexp ( \u2212d(v, \u03b8\u0303 i)\n\u03c3\n) log ( exp( f w(xi)ci,v)\n\u2211Nvv=1 exp( f w(xi)ci,v)\n) (4)\nwhere d(v, \u03b8\u0303 i) is the distance between the centers of the two bins v and \u03b8\u0303 i, and \u03c3 is a parameter controlling how much similarity is enforced between neighboring bins. Following [27], we use \u03c3 = 3 for Nv = 360. The inference is done as in Equation (3)."}, {"heading": "4 Joint detection and pose estimation", "text": "The methods presented in the previous section assume that the object detector is already trained and kept independent from the pose estimator. Since object detection and pose estimation relies on related information, we expect a benefit from training them jointly. We thus present extensions of the methods from Section 3 to perform this joint training."}, {"heading": "4.1 Joint model with regression", "text": "Two main approaches can be considered to extend the regression approach of Section 3.1 to jointly perform detection. The first one, described in [20] is to encode respectively the presence or absence of an object by a point close or far from the regression line described by F in the space where the regression is performed. An alternative approach, discussed\nin [21], is to add an output to the regression network specifically dedicated to detection. The loss used to train the network can then be decomposed into two terms: a classification loss Ldet(w), which is independent on the pose, and a regression loss Lreg(w) which takes into account only the pose estimation. Since state-of-the-art performance for detection are obtained using a classification loss, we selected the second option in the following.\nOur network thus has two outputs: f w,det(x) \u2208 RNc+1 for the detection part (predicting probabilities for each of the Nc classes and the background class), and f w,pose(x) \u2208 RNc\u00d7Nd for the pose estimation part. The multi-task loss for joint classification and regression-based pose estimation writes as follows:\nLj-reg(w) = Ldet(w)+\u03bbLreg(w) (5)\nWe define Lreg exactly as in Equation (1), using the pose estimation output of the network f w,pose(x). The detection loss Ldet is the standard cross-entropy loss for detection, using the detection part of the network output f w,det(x). We set the balancing parameter \u03bb = 1 in our experiments.\nAlso, we share the weights of the detection and pose estimation network only up to the pool5 layer. This is essential to obtain a good performance, as the regression and classification losses are different enough that sharing more weights leads to much worse results."}, {"heading": "4.2 Joint model with classification", "text": "A similar approach, separating two branches of the network, can be applied for classification. However, we introduce a new simpler and parameter-free way to perform jointly detection and pose estimation in a classification setup. Indeed, one can simply add a component, associated to the background patches, to the output vector of the pose estimation setup of Section 3.2 and normalize it globally, rather than for each class independently as in Equation (2). Each value is then interpreted as a log probability of the object being of one class and in a given orientation bin, rather than the conditional probability of the object being in a given orientation bin knowing its class. To obtain the probability of the object to belong to one class, one can simply sum the probabilities corresponding to all the bins for this class.\nSimilar to Section 3.2, we write f w,obj(x)c,v \u2208 RNc\u00d7Nv the value of the network output corresponding to the orientation bin v for an input x of class c. We additionally write f w,bg(x) \u2208 R its value corresponding to the background and associate a class ci = 0 to the elements xi in the background. The loss, which derives from the cross-entropy, writes:\nLj-classif(w) =\u2212 Ns\n\u2211 i=1 1ci=0 log\n( exp( f w,bg(xi))\nexp( f w,bg(xi))+\u2211Ncc=1 \u2211 Nv v=1 exp( f w,obj(xi)c,v)\n)\n\u2212 Ns\n\u2211 i=1 1ci 6=0 log\n( exp( f w,obj(xi)ci,\u03b8\u0303 i)\nexp( f w,bg(xi))+\u2211Ncc=1 \u2211 Nv v=1 exp( f w,obj(xi)c,v)\n) (6)\nAt inference, the score associated to the detection of an object x for class c is\nS(x,c) = \u2211Nvv=1 exp( f w,obj(x)c,v)\nexp( f w,bg(x))+\u2211Ncc\u2032=1 \u2211 Nv v=1 exp( f w,obj(x)c\u2032,v)\n(7)"}, {"heading": "5 Experiments", "text": "We now present experiments comparing the different approaches for pose estimation which were presented in the previous sections. Our experiments are based on the Fast R-CNN object detection framework [7], with Deep Mask [24] bounding boxes proposals.\nWe trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4]. We also extended the training data by adding the synthetic images from [27]. The evaluation metric we used is the Average Viewpoint Precision (AVP) associated to Pascal3D, which is very similar to the standard Average Precision (AP) metric used in detection tasks, but which considers as positive only the detections for which the viewpoint estimate is correct. More precisely, the viewpoints are discretized into K bins and the viewpoint estimate is considered correct if it falls in the same bin as the ground-truth annotation. We focus on the AVP24 metric, which discretizes the orientation into K = 24 bins and is the most fine-grained of the Pascal3D+ challenge [29]. We also consider the mean AP (mAP) and mean AVP (mAVP) over all classes."}, {"heading": "5.1 Training details", "text": "We fine-tuned our networks, starting from a network trained for ImageNet classification, using Stochastic Gradient Descent with a momentum of 0.9 and a weight decay of 0.0005. We augment all datasets with the horizontally-flipped versions of each image, flipping the target orientations accordingly. During the training of the joint detection and pose estimation models, 25% of the mini-batches consist of positive examples. Our mini-batches are of size 128 except when using synthetic images. When using synthetic images, we randomly create montages with the rendered views from [27], each montage containing 9 objects, for a total mini-batch size of 137 (96 backgrounds and 32 positive patches from real images and 9 positive synthetic objects). This allows for an efficient training in the setup of Fast R-CNN.\nWe initialized the learning rate at 0.001, and divided it by 10 after convergence of the training error. The number of iterations depends of the amount of training data: when using only Pascal VOC data, we decrease the learning rate after 30K iterations and continue to train until 40K; when adding ImageNet data we decrease the learning rate after 45K iterations and continue to train until 100K; and finally, when adding synthetic data, we decrease the learning rate after 100K iterations and continue to train until 300K.\nAll experiments were conducted using the Torch7 framework [3] and we will release our full code upon publication."}, {"heading": "5.2 Results", "text": ""}, {"heading": "5.2.1 Comparison of the different approaches for pose estimation", "text": "We first compare the different approaches for pose estimation from Section 3. We use a fixed object detector based on the AlexNet architecture, trained for detection on Pascal VOC 2012 training set and we report the results in Table 1. We can first observe that for regression, a pose representation with a higher dimensionality (3D) performs better than when using a smaller dimensionality (2D). We believe the redundancy in the representation helps to better handle ambiguities in the estimation. The classification approach however significantly outperforms both regressions (19.3% AVP compared to 13.9% and 15.7%). Interestingly, the simplest classification approach from Section 3.2 performs slightly better than the geometryaware method. We think the main reason for this difference is that the simple classification optimize exactly for the objective evaluated by the AVP, and thus this result can be seen as an artefact of the evaluation. Note that the results could be different for even more finegrained estimation where less examples per class are available. Nevertheless, since the more complex geometric structure aware approach performed worse than the direct classification baseline, we focus in the rest of this paper on the simplest direct classification approach."}, {"heading": "5.2.2 Benefits of joint training for detection and pose estimation", "text": "We evaluate the benefits of jointly training a model to detect the objects and predict their orientation. These benefits can be of two kinds. First, the order of the detections candidates given by the new detector may favor the confident orientations and thus increase the AVP. Second, the pose estimates can be better for a given object. To evaluate both effects independently, we report in Table 2 the results using both the order given by the detector used in the previous section and the order given by the new joint classifier. All experiments were performed as above, with the AlexNet architecture and the Pascal VOC training data.\nComparing Table 2 to Table 1 shows two main effects. First, the mAVP is improved even when using the same classifier, demonstrating improved viewpoint estimation with joint training. Second, the mAP is decreased, showing that the detection performs worse when trained jointly. However, one can also notice that the best mAVP is still obtained with the joint classifier. This shows that the pose estimation is better in the joint model, and also that for the case of classification the order learned when training jointly the detector favours confident poses. This is not the case for the regression approaches for which the best results are obtained using the independent detector and the jointly-learned pose estimation."}, {"heading": "5.2.3 Influence of network architectures and training data", "text": "In this section, we consider our joint classification approach, which performs best in the evaluations of the previous section, and study how its performance varies when using different architectures and more training data.\nThe comparison of the left and right columns of Table 3 shows that unsurprisingly the use of the VGG16 network instead of AlexNet consistently improves performances. This improvement is slightly less for the mAVP than for the mAP, hinting that the mAVP boost is mainly due to improved detection performances.\nFor the training data, we first progressively add training images from ImageNet to the training images from Pascal VOC. The full subset of the ImageNet dataset annotated in Pascal3D+ contains in average approximately 1900 more images per class, but is strongly unbalanced between the different classes. The analysis of these results shows consistent improvements when the training set includes more data. Interestingly, the mAVP is improved more than the mAP, showing that the additional data is more useful for pose estimation than for detection. The addition of synthetic data (2.4M positive examples) improves the results even more, demonstrating that the amount of training data is still a limiting factor even if one uses an AlexNet architecture and includes the ImageNet images, a fact that was not demonstrated in [27]. Note that our joint approach significantly outperforms the state-of-theart results [28] (currently 31.1% mAVP, based on VGG16 and ImageNet annotations) both without using synthetic data with VGG16, and with synthetic data and AlexNet architecture."}, {"heading": "5.2.4 Comparison to the state of the art", "text": "Table 4 provides the details of the AVP24 performance improvements over all classes as well as a comparison with three baselines: DPM-VOC+VP [22], which uses a modified version of DPM to also predict poses, Render for CNN [27] which uses real images from Pascal VOC as well as CAD renders for training a CNN based on AlexNet, and [28] which uses a VGG16 architecture and ImageNet data to classify orientations for each object category. It can be seen that we improve consistently on all baselines except for the chair class. A more detailed analysis shows that this exception is related to the difference between the ImageNet and Pascal chairs. Indeed, when adding the ImageNet data to the Pascal data, the detection performance for chairs drops from 34.5% AP to 19.23% AP. Similarly, the difference between the very different appearance of the rendered 3D models and real images is responsible for the fact that synthetic training data decreases performance on boats, motorbikes and trains. In average, we still found that synthetic images boost the results by 1.7% mAVP.\nFinaly, Table 5 provides the comparison between our full pipeline and the baselines for the 4, 8 and 16 viewpoint classification tasks, showing that our improvement of the state of the art is consistently high.\nTable 4: Summary of results and comparison with baselines using AVP24\nMethod aero bike boat bus car chair table mbike sofa train tv mAVP24\nDPM-VOC+VP [22] 9.7 16.7 2.2 42.1 24.6 4.2 2.1 10.5 4.1 20.7 12.9 13.6 Render For CNN [27] 21.5 22.0 4.1 38.6 25.5 7.4 11.0 24.4 15.0 28.0 19.8 19.8 Viewpoints & Keypoints [28] 37.0 33.4 10.0 54.1 40.0 17.5 19.9 34.3 28.9 43.9 22.7 31.1 Classif. approach & AlexNet 21.6 15.4 5.6 41.2 26.4 7.3 9.3 15.3 13.5 32.9 24.3 19.3\n+ our joint training 24.4 16.2 4.7 49.2 25.1 7.7 10.3 17.7 14.8 36.6 25.6 21.1 + VGG16 instead of AlexNet 26.3 29.0 8.2 56.4 36.3 13.9 14.9 27.7 20.2 41.5 26.2 27.3\n+ ImageNet data 42.4 37.0 18.0 59.6 43.3 7.6 25.1 39.3 29.4 48.1 28.4 34.4 + synthetic data 43.2 39.4 16.8 61.0 44.2 13.5 29.4 37.5 33.5 46.6 32.5 36.1"}, {"heading": "6 Conclusion", "text": "Combining our joint classification approach to the improvements provided by a deep architecture and additional training data, we increase state-of-the-art performance of pose estimation by 5% mAVP. We think that highlighting the different factors of this improvement and setting a new baseline will help and stimulate further work on viewpoint estimation."}], "references": [{"title": "Smooth object retrieval using a bag of boundaries", "author": ["R. Arandjelovi\u0107", "A. Zisserman"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models", "author": ["M. Aubry", "D. Maturana", "A.A. Efros", "B.C. Russell", "J. Sivic"], "venue": "International Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A largescale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "International Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The Pascal visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Object detection with discriminatively trained part based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 32(9)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "International Conference on Computer Vision (ICCV), pages 1440\u20131448", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Viewpoint-aware object detection and pose estimation", "author": ["Daniel Glasner", "Meirav Galun", "Sharon Alpert", "Ronen Basri", "Gregory Shakhnarovich"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "European Conf. on Comp. Vision (ECCV), pages 346\u2013361", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Analyzing 3D objects in cluttered images", "author": ["M. Hejrati", "D. Ramanan"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Object recognition using alignment", "author": ["D.P. Huttenlocher", "S. Ullman"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural comp., 1 (4):541\u2013551", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Worldwide pose estimation using 3D point clouds", "author": ["Y. Li", "N. Snavely", "D. Huttenlocher", "P. Fua"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Parsing ikea objects: Fine pose estimation", "author": ["J.J. Lim", "H. Pirsiavash", "A. Torralba"], "venue": "International Conference on Computer Vision (ICCV), pages 2992\u20132999. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "The viewpoint consistency constraint", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision (IJCV), 1(1):57\u201372", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1987}, {"title": "Convolutional neural networks for joint object detection and pose estimation: A comparative study", "author": ["Francisco Massa", "Mathieu Aubry", "Renaud Marlet"], "venue": "arXiv preprint arXiv:1412.7190,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Synergistic face detection and pose estimation with energy-based models", "author": ["M. Osadchy", "Y. LeCun", "M.L. Miller"], "venue": "The Journal of Machine Learning Research, 8:1197\u20131215", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving object classification using pose information", "author": ["H. Penedones", "R. Collobert", "F. Fleuret", "D. Grangier"], "venue": "Technical report, Idiap Research Institute", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Teaching 3D geometry to deformable part models", "author": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "venue": "In International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning to segment object candidates", "author": ["Pedro O Pinheiro", "Ronan Collobert", "Piotr Dollar"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Machine perception of 3-D solids", "author": ["L. Roberts"], "venue": "PhD. Thesis", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1965}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Beyond Pascal: A benchmark for 3D object detection in the wild", "author": ["Y. Xiang", "R. Mottaghi", "S. Savarese"], "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating the aspect layout of object categories", "author": ["Yu Xiang", "Silvio Savarese"], "venue": "In International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Detailed 3D representations for object recognition and modeling", "author": ["M. Zia", "M. Stark", "B. Schiele", "K. Schindler"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [29].", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories.", "startOffset": 71, "endOffset": 83}, {"referenceID": 17, "context": "While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories.", "startOffset": 71, "endOffset": 83}, {"referenceID": 23, "context": "While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories.", "startOffset": 71, "endOffset": 83}, {"referenceID": 25, "context": "The interest in this problem has recently increased both by the availability of the Pascal3D+ dataset [29], which provides a standard way to compare algorithms on diverse classes, and by the improved performance of object detection, which encouraged researchers to focus on extracting more complex information from the images than the position of objects.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "In particular, several approaches have been proposed, such as a regression approach with joint training for detection [20, 21], a direct viewpoint classification [28], and a geometric structure aware fine-grained viewpoint classification [27], where the authors modify the classification objective to take into account", "startOffset": 118, "endOffset": 126}, {"referenceID": 20, "context": "In particular, several approaches have been proposed, such as a regression approach with joint training for detection [20, 21], a direct viewpoint classification [28], and a geometric structure aware fine-grained viewpoint classification [27], where the authors modify the classification objective to take into account", "startOffset": 118, "endOffset": 126}, {"referenceID": 20, "context": "While several of the elements that we employ have been used in previous work [21, 27, 28], we know of no systematic study of their respective and combined effect, resulting in an absence of clear good practices for viewpoint estimation and sub-optimal performances.", "startOffset": 77, "endOffset": 89}, {"referenceID": 14, "context": "[15]), their use has been generalized only in 2012 after the demonstration of their benefits by Krizhevsky et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] on the ImageNet large-scale visual recognition challenge [4].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[14] on the ImageNet large-scale visual recognition challenge [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "[8] provided an important improvement over previous methods on the Pascal VOC dataset [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[8] provided an important improvement over previous methods on the Pascal VOC dataset [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 6, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 199, "endOffset": 206}, {"referenceID": 9, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 199, "endOffset": 206}, {"referenceID": 0, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 12, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 15, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 16, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 17, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 23, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 5, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 8, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 11, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 21, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 26, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 152, "endOffset": 160}, {"referenceID": 27, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 152, "endOffset": 160}, {"referenceID": 1, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 195, "endOffset": 202}, {"referenceID": 25, "context": "With the advent of Pascal3D+ dataset [29], which extends Pascal VOC dataset [5] by aligning a set of 3D CAD models for 12 rigid object classes, learning-based approaches using only on example images became possible and proved their superior performance.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "With the advent of Pascal3D+ dataset [29], which extends Pascal VOC dataset [5] by aligning a set of 3D CAD models for 12 rigid object classes, learning-based approaches using only on example images became possible and proved their superior performance.", "startOffset": 76, "endOffset": 79}, {"referenceID": 25, "context": "[29] extended the method of [22], which uses an adaptation of DPM with 3D constraints to estimate the pose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "CNN-based approaches, which were until the availability of the Pascal3D+ data limited to special cases such as faces [20] and small datasets", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "[21], also began to be applied to this problem at a larger scale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data.", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data.", "startOffset": 128, "endOffset": 131}, {"referenceID": 24, "context": "[28] used a simple classification approach with the VGG16 network [26] and annotations for ImageNet objects and established the current state-of-the-art on Pascal3D+.", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "For object detection, we use the standard Fast R-CNN framework [7], which relies on region proposal but is significantly faster than the original R-CNN [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "For object detection, we use the standard Fast R-CNN framework [7], which relies on region proposal but is significantly faster than the original R-CNN [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 19, "context": "Thus, as highlighted in [20], a good pose representation F(\u03b8) satisfies the following properties: (a) it is invariant to the periodicity of the angle \u03b8 , and (b) it is analytically invertible.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "(i) F(\u03b8) = [ cos(\u03b8), sin(\u03b8) ] , probably the simplest way to represent orientations, used for example in [21];", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "(ii) F(\u03b8) = [ cos ( \u03b8 \u2212 3 ) , cos(\u03b8) , cos ( \u03b8 + 3 )] , a formulation which was presented in [20], and that has a higher dimensionality than the previous one, allowing more flexibility for the network to better capture the pose information.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Other regression approaches and loss are discussed in [19] but lead to lower performances.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "The first one, described in [20] is to encode respectively the presence or absence of an object by a point close or far from the regression line described by F in the space where the regression is performed.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "in [21], is to add an output to the regression network specifically dedicated to detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Our experiments are based on the Fast R-CNN object detection framework [7], with Deep Mask [24] bounding boxes proposals.", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "Our experiments are based on the Fast R-CNN object detection framework [7], with Deep Mask [24] bounding boxes proposals.", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "We trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "We trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "We trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4].", "startOffset": 224, "endOffset": 227}, {"referenceID": 25, "context": "We focus on the AVP24 metric, which discretizes the orientation into K = 24 bins and is the most fine-grained of the Pascal3D+ challenge [29].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": "All experiments were conducted using the Torch7 framework [3] and we will release our full code upon publication.", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "Convolutional Neural Networks (CNNs) were recently shown to provide state-of-theart results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [29]. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP.", "creator": "LaTeX with hyperref package"}}}