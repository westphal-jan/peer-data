{"id": "1609.03976", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Multimodal Attention for Neural Machine Translation", "abstract": "the linguistic attention mechanism is an important part of the neural machine 3d translation ( mammalian nmt ) where it was reported to produce richer random source representation compared to fixed - length encoding sequence - to - input sequence models. speaking recently, the effectiveness of our attention has previously also been explored in building the context of image captioning. in this work, we assess the feasibility of framing a multimodal attention mechanism that ultimately simultaneously reflects focus over an image and its natural language description for generating a description in another language. we train several variants of our proposed attention mechanism on the multi30k multilingual image captioning dataset. broadly we show annually that a dedicated attention for each modality achieves up to 1. 6 points in bleu and meteor compared to a textual nmt baseline.", "histories": [["v1", "Tue, 13 Sep 2016 18:46:03 GMT  (782kb,D)", "http://arxiv.org/abs/1609.03976v1", "10 pages, under review COLING 2016"]], "COMMENTS": "10 pages, under review COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["ozan caglayan", "lo\\\"ic barrault", "fethi bougares"], "accepted": false, "id": "1609.03976"}, "pdf": {"name": "1609.03976.pdf", "metadata": {"source": "CRF", "title": "Multimodal Attention for Neural Machine Translation", "authors": ["Ozan Caglayan", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "emails": ["FirstName.LastName@univ-lemans.fr", "ocaglayan@gsu.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "While dealing with multimodal stimuli in order to perceive the surrounding environment and to understand the world is natural for human beings, it is not the case for artificial intelligence systems where an efficient integration of multimodal and/or multilingual information still remains a challenging task. Once tractable, this grounding of multiple modalities against each other may enable a more natural language interaction with computers.\nRecently, deep neural networks (DNN) achieved state-of-the-art results in numerous tasks of computer vision (CV), natural language processing (NLP) and speech processing (LeCun et al., 2015) where the input signals are monomodal i.e. image/video, characters/words/phrases, audio signal. With these successes in mind, researchers now attempt to design systems that will benefit from the fusion of several modalities in order to reach a wider generalization ability.\nMachine translation (MT) is another field where purely neural approaches (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies.\nAnother straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al. (2015), You et al. (2016) have achieved state-of-the-art results on popular image captioning datasets Flickr30k (Young et al., 2014) and MSCOCO (Lin et al., 2014).\nIn this paper, we propose a multimodal architecture that is able to attend to multiple input modalities and we realize an extrinsic analysis of this specific attention mechanism. Furthermore, we attempt to determine the optimal way of attending to multiple input modalities in the context of image captioning. Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.\nThe architecture that we introduce for achieving multimodal learning can either be seen as an NMT enriched with convolutional image features as auxiliary source representation or an image captioning\nar X\niv :1\n60 9.\n03 97\n6v 1\n[ cs\n.C L\n] 1\n3 Se\np 20\n16\nsystem producing image descriptions in a language T, supported with source descriptions in another language S. The modality specific pathways of the proposed architecture are inspired from two previously published approaches (Bahdanau et al., 2014; Xu et al., 2015) where an attention mechanism is learned to focus on different parts of the input sentence. Such integration implies a careful design of the underlying blocks: many strategies are considered in order to correctly attend to multiple input modalities. Along this line, different possibilities are also assessed for taking into account multimodal information during the decoding process of target words.\nWe compare the proposed architecture against single modality baseline systems using the recently published Multi30k multilingual image captioning dataset (Elliott et al., 2016). The empirical results show that we obtain better captioning performance in terms of different evaluation metrics when compared to baselines. We further provide an analysis of the descriptions and the multimodal attention mechanism to examine the impact of the multimodality on the quality of generated descriptions and the attention."}, {"heading": "2 Related Work", "text": "Since the presented work is at the intersection of several research topics, this part is divided into further sections for clarity."}, {"heading": "2.1 Neural Machine Translation (NMT)", "text": "End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al. (2014), Sutskever et al. (2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al. (2014) used the last hidden state of the encoder as a fixed size source sentence representation while Bahdanau et al. (2014) introduced an attention mechanism where a set of attentional weights are assigned to the recurrent hidden states of the encoder to obtain a weighted sum of the states instead of just taking the last. The idea of attention still continues to be an active area of research in the NMT community (Luong et al., 2015b)."}, {"heading": "2.2 Neural Image Captioning", "text": "End-to-end neural image description models are basically recurrent language models conditioned in different ways on image features. These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term. Finally Vinyals et al. (2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word.\nDifferent from the previously cited works, Xu et al. (2015) applied the attention mechanism over convolutional image features of size 14x14x512 extracted from VGG, which makes the image context a collection of feature maps instead of a single vector. During training, an LSTM network jointly learns to generate an image description while selectively attending to the presented image features. This model is similar to the attentional NMT introduced by Bahdanau et al. (2014) except that the source word annotations produced by the encoder are replaced by the convolutional features.\nAnother attention based model proposed by You et al. (2016) introduced two separate attention mechanisms, input and output attention models, that are applied over a set of visual attributes detected using different methods like k-NN and neural networks. The image representation extracted from GoogLeNet\n(Szegedy et al., 2015) is only used as the initial input to the RNN. The authors use an ensemble of 5 differently initialized models as their final system.\nIn the context of multimodality, Elliott et al. (2015) explore the effectiveness of conditioning a target language model on the image features from the last fully-connected layer of VGG and on the features from a source language model using the IAPR-TC12 multilingual image captioning dataset."}, {"heading": "2.3 Multi-Task Learning", "text": "Several recent studies in the literature explored multi-task learning for different NLP and MT tasks. Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages. Specifically, they made use of a single encoder and multiple language-specific decoders each embedded with its own attention mechanism. Firat et al. (2016) extended this idea into a multi-way multilingual NMT which can translate between multiple languages using a single attention mechanism shared across all the languages. Luong et al. (2015a) experimented with one-to-many, many-to-one and many-to-many schemes in order to quantify the mutual benefit of several NLP, MT and CV tasks to each other."}, {"heading": "3 Architecture", "text": "The general architecture (Figure 1) is composed of four building blocks namely the textual encoder, the visual encoder, the attention mechanism and the decoder, which will all be described in what follows."}, {"heading": "3.1 Textual Encoder", "text": "We define by X and Y , a source sentence of length N and a target sentence of length M respectively where each word is represented in a source/target embedding space with dimension E:\nX = (x1, x2, ..., xN ), xi \u2208 RE (1) Y = (y1, y2, ..., yM ), yj \u2208 RE (2)\nA bi-directional GRU (Cho et al., 2014) encoder with D hidden units reads the input X sequentially in forwards and backwards to produce two sets of hidden states based on the current source word embedding and the previous hidden state of the encoder. We call atxti the textual annotation vector obtained at time step i by concatenating the forward and backward hidden states of the encoder:\natxti = [ ~hi ~hi ] , atxti \u2208 R2D (3)\nAt the end of this encoding stage, we obtain a set of N textual annotation vectors Atxt = {atxt1 , atxt2 , . . . , atxtN } that represents the source sentence."}, {"heading": "3.2 Visual Encoder", "text": "We use convolutional feature maps of size 14x14x1024 extracted from the so-called res4f relu layer (end of Block-4, after ReLU) of a ResNet-50 trained on ImageNet (See Section 5.1 for details). In order to make the dimensionality compatible with the textual annotation vectors atxti , a linear transformationWim is applied to the image features leading to the visual annotation vectors Aim = {aim1 , aim2 , . . . , aim196}."}, {"heading": "3.3 Decoder", "text": "A conditional GRU1 (CGRU) decoder has been extended for multimodal context and equipped with a multimodal attention mechanism in order to generate the description in the target language. CGRU consists of two stacked GRU activations that we will name g1 and g2 respectively. We experimented with several ways of initializing the hidden state of the decoder and found out that the model performs better when h(1)0 is learned from the mean textual annotation using a feed-forward layer with tanh nonlinearity. The hidden state h(2) of g2 is initialized with h (1) 1 :\nh (1) 0 = tanh ( W Tinit ( 1\nN N\u2211 i=1 atxti\n) + binit ) , h (2) 0 = h (1) 1 (4)\nAt each time step, a multimodal attention mechanism (detailed in Section 4) computes two modality specific context vectors {ctxtt , cimt } given the hidden state of the first GRU h (1) t and the textual/visual annotations {Atxt, Aim}. The multimodal context vector ct is then obtained by applying a tanh nonlinearity over a fusion of these modality specific context vectors. We experimented with two different fusion techniques that we will refer to as SUM(FS) and CONCAT(FC):\nct = FS(c txt t , c im t ) = tanh(c txt t + c im t ) (5)\nct = FC(c txt t , c im t ) = tanh ( W Tfus [ ctxtt\ncimt\n] + bfus ) (6)\nThe probability distribution over the target vocabulary is conditioned on the hidden state h(2)t of the second GRU (which receives as input ct), the multimodal context vector ct and the embedding of the (true) previous target word Eyt\u22121 :\nh (2) t = g2(h (2) t\u22121, ct) (7) P (yt = k|y<t, Atxt, Aim) = softmax ( Lo tanh(Lsh (2) t + Lcct + Eyt\u22121) ) (8)"}, {"heading": "4 Attention Mechanism", "text": "In the context of multi-way multilingual NMT, Firat et al. (2016) benefit from a single attention mechanism shared across different language pairs. Although this may be reasonable within a multilingual task where all inputs and outputs are solely based on textual representations, this may not be the optimal approach when the modalities are completely different.\nIn order to verify the above statement, we consider different schemes for integrating the attention mechanism into multimodal learning. First, a shared feed-forward network is used to produce modalityspecific attention weights {\u03b1txtt , \u03b1imt }:\n\u03b1txtt = softmax ( UA tanh(WD h (1) t +WC A txt) )\n(9) \u03b1imt = softmax ( UA tanh(WD h (1) t +WC A im) ) (10)\n1github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf\nWe call this basic approach where linear transformations {WC , UA} from contexts to attentional weights are all shared, the encoder-independent attention (Figure 2A). An encoder-dependent attention in contrast, has distinct {WC , UA} for each modality which means that they are projected differently (Figure 2B).\nIt should be noted that both mechanisms depicted above still have a common projection layer WD applied to the hidden state h(1)t of the first GRU. In order to objectively assess the impact of modalitydependency on the decoder side, we also need to consider the two cases where a shared WD or distinct {W txtD ,W imD } transformations are used in the projection layer (Figure 2C and 2D). These two variants will be respectively referred as independent and dependent decoder state projection in the rest of the paper.\nTo sum up, we propose four variants of the multimodal attention mechanism in terms of modality dependency with respect to encoder and decoder. While encoder-independent attention with dependent decoder state projection (Figure 2C) might seem unnatural for this single target task, it may be interesting as a contrastive system.\nOnce the attentional weights {\u03b1txtt , \u03b1imt } are computed, the modality-specific context vectors are defined as the weighted sum of each annotation set with its relevant \u03b1:\nctxtt = N\u2211 i=1 \u03b1txtti a txt i , c im t = 196\u2211 j=1 \u03b1imtj a im j (11)\nThese vectors are then merged into a multimodal context vector that will be used in the target word generation (see equations 6, 8 in Section 3)."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Dataset", "text": "We used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities dataset (Young et al., 2014). Multi30K extends the original Flickr30K that contains 31K images with 5 English descriptions with 5 more independently crowdsourced descriptions in German. It should be emphasized that the provided bilingual descriptions are not direct translations but could be considered somewhat comparable.\nThe training, validation and test set consist of 29000, 1014 and 1000 images respectively each having 5 English and 5 German annotations. A total of 25 sentence pairs can be obtained by taking the cross\nproduct of these annotations leading to a training set of 725K samples. Throughout this work, we reduced 725K to 145K by considering only 5 sentence pairs for each image.\nWe tokenized the sentences with tokenizer.perl from Moses, removed punctuation and lowercased the sentences. We only kept sentence pairs with sentence lengths \u2208 [3, 50] having a length ratio of at most 3. This results in a final training dataset of 131K sentences (Table 1). We picked the most frequent 10K German words and replaced the rest with an UNK token for the target side.\nFor the image part, convolutional image features of size 14x14x2014 are extracted from the res4f relu layer of ResNet-50 CNN (He et al., 2016) trained on ImageNet. The images were resized to 224x224 without any cropping prior to extraction. The final features are used as 196x1024 dimension matrices per each image during training and testing."}, {"heading": "5.2 Training", "text": "We trained two monomodal baselines namely NMT, IMGTXT and different attentional variants of our proposed MNMT. The baselines have exactly the same architecture presented throughout this work but with only a single source modality. All models have word embeddings and recurrent layers of dimensionality 620 and 1000 respectively. We used Adam (Kingma and Ba, 2014) as the stochastic gradient descent variant with a minibatch size of 32. The weights of the networks are initialized using Xavier scheme (Glorot and Bengio, 2010) while the biases are initially set to 0. L2 regularization with \u03bb = 0.00001 is applied to the training cost to avoid overfitting.\nThe performance of the networks is evaluated on the first validation split using BLEU (Papineni et al., 2002) at the end of each epoch and the training is stopped if BLEU does not improve for 20 evaluation periods.\nA classical left to right beam-search with a beam size of 12 is used for sentence generation during test time. Besides evaluating performance on the first validation split, we also experimented with a best source selection strategy where for each image we obtain 5 German hypotheses that correspond to 5 English descriptions and pick the one with the highest log-likelihood and the least number of UNK tokens."}, {"heading": "6.1 Quantitative Analysis", "text": "The description generation performance of the models is presented in Table 2 using BLEU, METEOR (Lavie and Agarwal, 2007) and CIDEr-D (Vedantam et al., 2015) as automatic evaluation metrics.\nIt is clear from Table 2 that MNMT with CONCAT as the fusion operator improves over both the NMT and the IMGTXT baselines when combined with modality-dependent attention mechanism (MNMT 6 to 8). The results are slightly worse than the NMT baseline regardless of the multimodal attention type when the fusion is realized with the SUM operator. This difference can be attributed to the fact that concatenation makes use of a linear layer that learns how to integrate the modality-specific activations into the multimodal context vector.\nThe improvement in terms of the automatic metrics is more significant with the best source selection strategy compared to the first validation split. The reason for this is that the first split contains source sentences which are much longer and detailed than the target ones: 18.35 words/sentence in average on the source side compared to 8.03 words/sentence on the target side. Once we use the best source selection method, the models compensate for this discrepancy by choosing source sentences of different lengths (in average \u223c13 words/sentence).\nWe observe that a completely independent (shared) attention mechanism (MNMT5) has the worst performance among all CONCAT variants. This empirical evidence is on par with our initial statement that a single shared attention may not be the optimal approach in the case of different input modalities. We also notice that once a dependency be it on the encoder or the decoder side is established in the topology, the performance improves compared to the NMT baseline and MNMT5. The results obtained by MNMT6 that we previously conjectured as unnatural (see Section 4) also confirm this."}, {"heading": "6.2 Qualitative Analysis", "text": "In Table 3 we present German descriptions generated by the NMT baseline and the best performing MNMT model using the same source description from the first validation split. In the first image, MNMT clearly produces a richer description where it provides additional visual information like the color and the type of clothing and also the positioning of the woman. In the second case, MNMT again generates a coherent and rich description but ignores the pink laptop bag that is mentioned in the NMT output. Lastly, third image shows a situation where NMT wrongly describes the color of an object while MNMT does its job correctly although METEOR penalizes it, a phenomenon observed when the references contain\nless detail than the hypotheses. One advantage of the attention mechanism is the ability to visualize where exactly the network pays attention. Although the visualization over the textual context is trivial to apply, inferring the spatial region in the original image that is attended by the model is not. Here we adopt the approach proposed by Xu et al. (2015) which upsamples the attention weights with a factor of 16 in order to lay them over the original image as white smoothed regions. We compare the multimodal attention between completely independent MNMT5 and encoder-dependent MNMT7 in Figure 3.\nAlthough both models achieve a comparable attention pattern over the image regions, textual alignment is completely disturbed (for all examples) in MNMT5. The failure of the shared attention which contrasts with Firat et al. (2016) can be attributed to a representational discrepancy between convolutional image features and the source side textual annotations. Moreover, the number of visual annotations for a single image (196) is \u223c15 times higher than the number of textual annotations (with 13 words/sentence in average) which may bias the joint attention towards the visual side during the learning stage."}, {"heading": "7 Conclusion", "text": "In this paper, we have proposed an architecture that performs multimodal machine translation with multimodal attention. By quantitatively and qualitatively analyzing different attention schemes, we demonstrated that modality-dependent MNMT outperforms the textual NMT baseline by up to 1 BLEU/METEOR and 2.5 CIDEr-D points. The difference is even more significant with the best source selection strategy, where we reach a gain of almost 1.6 BLEU/METEOR and 4.2 CIDEr-D points. We further visualize multimodal attention and show that modality-dependent attention mechanism is able to learn an alignment over both the source words and the image features but this is not the case for shared attention where textual alignment is completely disturbed.\nAs future work, fine-tuning the extracted image features with a learnable convolutional layer inside MNMT and/or extracting image features based on source language description are interesting paths to explore in order to gain more insight about multimodality."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei."], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR, abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30k: Multilingual english-german image descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "arXiv preprint arXiv:1605.00459", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1601.01073.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and Statistics.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3128\u20133137. IEEE.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48\u201354. Association for Computational Linguistics.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments", "author": ["Alon Lavie", "Abhaya Agarwal."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT \u201907, pages 228\u2013231, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lavie and Agarwal.,? 2007", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."], "venue": "Nature, 521(7553):436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."], "venue": "ICLR.", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Stroudsburg, PA, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR, abs/1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u2013 4575.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3156\u20133164. IEEE.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 2048\u20132057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo."], "venue": "arXiv preprint arXiv:1603.03925.", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Recently, deep neural networks (DNN) achieved state-of-the-art results in numerous tasks of computer vision (CV), natural language processing (NLP) and speech processing (LeCun et al., 2015) where the input signals are monomodal i.", "startOffset": 170, "endOffset": 190}, {"referenceID": 23, "context": "Machine translation (MT) is another field where purely neural approaches (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 0, "context": "Machine translation (MT) is another field where purely neural approaches (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 13, "context": ", 2014) challenge the classical phrase-based approach (Koehn et al., 2003).", "startOffset": 54, "endOffset": 74}, {"referenceID": 29, "context": "(2016) have achieved state-of-the-art results on popular image captioning datasets Flickr30k (Young et al., 2014) and MSCOCO (Lin et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 17, "context": ", 2014) and MSCOCO (Lin et al., 2014).", "startOffset": 19, "endOffset": 37}, {"referenceID": 4, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 3, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 18, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 6, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al.", "startOffset": 8, "endOffset": 587}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al.", "startOffset": 8, "endOffset": 610}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al. (2015), You et al.", "startOffset": 8, "endOffset": 628}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al. (2015), You et al. (2016) have achieved state-of-the-art results on popular image captioning datasets Flickr30k (Young et al.", "startOffset": 8, "endOffset": 647}, {"referenceID": 0, "context": "The modality specific pathways of the proposed architecture are inspired from two previously published approaches (Bahdanau et al., 2014; Xu et al., 2015) where an attention mechanism is learned to focus on different parts of the input sentence.", "startOffset": 114, "endOffset": 154}, {"referenceID": 27, "context": "The modality specific pathways of the proposed architecture are inspired from two previously published approaches (Bahdanau et al., 2014; Xu et al., 2015) where an attention mechanism is learned to focus on different parts of the input sentence.", "startOffset": 114, "endOffset": 154}, {"referenceID": 5, "context": "We compare the proposed architecture against single modality baseline systems using the recently published Multi30k multilingual image captioning dataset (Elliott et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 13, "context": "(2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003).", "startOffset": 87, "endOffset": 107}, {"referenceID": 19, "context": "The idea of attention still continues to be an active area of research in the NMT community (Luong et al., 2015b).", "startOffset": 92, "endOffset": 113}, {"referenceID": 8, "context": "1 Neural Machine Translation (NMT) End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al.", "startOffset": 113, "endOffset": 145}, {"referenceID": 0, "context": "1 Neural Machine Translation (NMT) End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al. (2014), Sutskever et al.", "startOffset": 169, "endOffset": 187}, {"referenceID": 0, "context": "1 Neural Machine Translation (NMT) End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al. (2014), Sutskever et al. (2014) and Bahdanau et al.", "startOffset": 169, "endOffset": 212}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al.", "startOffset": 11, "endOffset": 246}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al. (2014) used the last hidden state of the encoder as a fixed size source sentence representation while Bahdanau et al.", "startOffset": 11, "endOffset": 268}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al. (2014) used the last hidden state of the encoder as a fixed size source sentence representation while Bahdanau et al. (2014) introduced an attention mechanism where a set of attentional weights are assigned to the recurrent hidden states of the encoder to obtain a weighted sum of the states instead of just taking the last.", "startOffset": 11, "endOffset": 386}, {"referenceID": 2, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009).", "startOffset": 157, "endOffset": 176}, {"referenceID": 14, "context": "The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014).", "startOffset": 100, "endOffset": 125}, {"referenceID": 22, "context": ", 2012) and VGG (Simonyan and Zisserman, 2014).", "startOffset": 16, "endOffset": 46}, {"referenceID": 9, "context": "(2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word.", "startOffset": 107, "endOffset": 132}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 158, "endOffset": 196}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 158, "endOffset": 225}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al.", "startOffset": 158, "endOffset": 248}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space.", "startOffset": 158, "endOffset": 358}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term.", "startOffset": 158, "endOffset": 702}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term. Finally Vinyals et al. (2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word.", "startOffset": 158, "endOffset": 861}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term. Finally Vinyals et al. (2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word. Different from the previously cited works, Xu et al. (2015) applied the attention mechanism over convolutional image features of size 14x14x512 extracted from VGG, which makes the image context a collection of feature maps instead of a single vector.", "startOffset": 158, "endOffset": 1143}, {"referenceID": 0, "context": "This model is similar to the attentional NMT introduced by Bahdanau et al. (2014) except that the source word annotations produced by the encoder are replaced by the convolutional features.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": "This model is similar to the attentional NMT introduced by Bahdanau et al. (2014) except that the source word annotations produced by the encoder are replaced by the convolutional features. Another attention based model proposed by You et al. (2016) introduced two separate attention mechanisms, input and output attention models, that are applied over a set of visual attributes detected using different methods like k-NN and neural networks.", "startOffset": 59, "endOffset": 250}, {"referenceID": 24, "context": "(Szegedy et al., 2015) is only used as the initial input to the RNN.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "In the context of multimodality, Elliott et al. (2015) explore the effectiveness of conditioning a target language model on the image features from the last fully-connected layer of VGG and on the features from a source language model using the IAPR-TC12 multilingual image captioning dataset.", "startOffset": 33, "endOffset": 55}, {"referenceID": 3, "context": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages. Specifically, they made use of a single encoder and multiple language-specific decoders each embedded with its own attention mechanism. Firat et al. (2016) extended this idea into a multi-way multilingual NMT which can translate between multiple languages using a single attention mechanism shared across all the languages.", "startOffset": 0, "endOffset": 275}, {"referenceID": 3, "context": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages. Specifically, they made use of a single encoder and multiple language-specific decoders each embedded with its own attention mechanism. Firat et al. (2016) extended this idea into a multi-way multilingual NMT which can translate between multiple languages using a single attention mechanism shared across all the languages. Luong et al. (2015a) experimented with one-to-many, many-to-one and many-to-many schemes in order to quantify the mutual benefit of several NLP, MT and CV tasks to each other.", "startOffset": 0, "endOffset": 464}, {"referenceID": 1, "context": ", yM ), yj \u2208 R (2) A bi-directional GRU (Cho et al., 2014) encoder with D hidden units reads the input X sequentially in forwards and backwards to produce two sets of hidden states based on the current source word embedding and the previous hidden state of the encoder.", "startOffset": 40, "endOffset": 58}, {"referenceID": 6, "context": "In the context of multi-way multilingual NMT, Firat et al. (2016) benefit from a single attention mechanism shared across different language pairs.", "startOffset": 46, "endOffset": 66}, {"referenceID": 5, "context": "1 Dataset We used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities dataset (Young et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 29, "context": ", 2016) which is an extended version of the Flickr30K Entities dataset (Young et al., 2014).", "startOffset": 71, "endOffset": 91}, {"referenceID": 8, "context": "For the image part, convolutional image features of size 14x14x2014 are extracted from the res4f relu layer of ResNet-50 CNN (He et al., 2016) trained on ImageNet.", "startOffset": 125, "endOffset": 142}, {"referenceID": 12, "context": "We used Adam (Kingma and Ba, 2014) as the stochastic gradient descent variant with a minibatch size of 32.", "startOffset": 13, "endOffset": 34}, {"referenceID": 7, "context": "The weights of the networks are initialized using Xavier scheme (Glorot and Bengio, 2010) while the biases are initially set to 0.", "startOffset": 64, "endOffset": 89}, {"referenceID": 21, "context": "The performance of the networks is evaluated on the first validation split using BLEU (Papineni et al., 2002) at the end of each epoch and the training is stopped if BLEU does not improve for 20 evaluation periods.", "startOffset": 86, "endOffset": 109}, {"referenceID": 15, "context": "1 Quantitative Analysis The description generation performance of the models is presented in Table 2 using BLEU, METEOR (Lavie and Agarwal, 2007) and CIDEr-D (Vedantam et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 25, "context": "1 Quantitative Analysis The description generation performance of the models is presented in Table 2 using BLEU, METEOR (Lavie and Agarwal, 2007) and CIDEr-D (Vedantam et al., 2015) as automatic evaluation metrics.", "startOffset": 158, "endOffset": 181}, {"referenceID": 27, "context": "Here we adopt the approach proposed by Xu et al. (2015) which upsamples the attention weights with a factor of 16 in order to lay them over the original image as white smoothed regions.", "startOffset": 39, "endOffset": 56}, {"referenceID": 6, "context": "The failure of the shared attention which contrasts with Firat et al. (2016) can be attributed to a representational discrepancy between convolutional image features and the source side textual annotations.", "startOffset": 57, "endOffset": 77}], "year": 2016, "abstractText": "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.", "creator": "TeX"}}}