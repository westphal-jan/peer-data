{"id": "1609.04325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Transliteration in Any Language with Surrogate Languages", "abstract": "we introduce a method for transliteration generation that can produce transliterations in every language. where previous results are only as multilingual as wikipedia, we show how to use training data from japanese wikipedia as surrogate training for any language. thus, computing the problem becomes one of ranking wikipedia languages in order of suitability with respect to ranking a larger target particular language. we introduce like several task - bound specific methods for ranking languages, clarify and show that our expert approach is comparable to the oracle ceiling, tends and even outperforms it in some cases.", "histories": [["v1", "Wed, 14 Sep 2016 15:58:55 GMT  (49kb,D)", "http://arxiv.org/abs/1609.04325v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stephen mayhew", "christos christodoulopoulos", "dan roth"], "accepted": false, "id": "1609.04325"}, "pdf": {"name": "1609.04325.pdf", "metadata": {"source": "CRF", "title": "Transliteration in Any Language with Surrogate Languages", "authors": ["Stephen Mayhew", "Christos Christodoulopoulos"], "emails": ["danr}@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "Transliteration is the task of converting a word or phrase, typically a named entity, from one script into another while maintaining the pronunciation. The problem is largely of interest for downstream tasks such as cross lingual named entity recognition (Darwish, 2013; Kim et al., 2012). Work in transliteration can be divided into two areas: discovery, where a query name is provided along with a list of candidates, and the task is to select the best candidate, and generation, where the task is to generate a transliteration for a given test name. We focus on the more challenging task, generation.\nTo date, all of the work in generation has required some level of supervision in the target language T in the form of name pairs. Multilingual-minded approaches typically gather this from Wikipedia (Pasternack and Roth, 2009; Irvine et al., 2010). However, languages with little or no presence in Wikipedia are still out of reach.\nWe describe a simple method to use the languages present in Wikipedia to extend transliteration capability to all languages. The intuition is that there are enough languages in Wikipedia such that for any language in the world, we can select a Wikipedia language that is close enough to the target language to build a transliteration model. Our method produces results that are consistently comparable or superior to an oracle that selects the best language every time. Further, for some small Wikipedia languages, we can produce results better than training on the target language."}, {"heading": "2 Method", "text": "Central to our approach is the fact that we have name pair data from 281 languages in Wikipedia. Of these, many have a trivial number of name pairs (< 20), so we keep only those 131 languages with 200 name pairs or more. We refer to this set as W .\nWe used uroman1 to romanize all of our data. This means that the text in each language is represented\n1http://www.isi.edu/natural-language/ software/romanizer/uroman-v0.4.tar.gz\nar X\niv :1\n60 9.\n04 32\n5v 1\n[ cs\n.C L\n] 1\n4 Se\np 20\nusing Latin characters, and that we can train on any language, and use that model to test on any other language, unconstrained by script. Note that romanization is a deterministic process that is reversible with little loss.2\nOur goal is to generate transliterations from English into some (potentially low-resource) target language T . We use English because of the availability of resources, although in principle our method works for any source language with enough resources. One application is to use English to search for named entities in a large corpus in language T . We also note that transliteration is symmetric. Given name pairs, one can learn a model in either direction.\nNow, given T , there are two situations: (1) T is in W , or (2) T has little or no data in Wikipedia. In the first case, if T has a sufficient number of training pairs, then we can simply use that data to train. However, if it has a small number of training pairs, or if it is not in Wikipedia, we need to resort to more interesting methods.\nThe method we propose is shown in Algorithm 1. The interesting part is in selecting a surrogate language S from W . We call it a surrogate because it acts exactly as though it were T . This selection is built around the notion of language similarity, as seen in the sim() function.\nFor all of our experiments, we use the transliteration model described in Pasternack and Roth (2009). This is a probabilistic model that allows the mapping of variable length segments between languages. In all following experiments we used 5 EM iterations, with a maximum segment length of 15 for source and target words, and SegmentFactor of 0.5."}, {"heading": "3 Transliteration-Driven Language Similarity", "text": "We start by presenting the supervised oracle ranking, and then describe our proposed similarity metrics."}, {"heading": "3.1 Oracle Ranking", "text": "In order to evaluate our proposed similarity metrics, we need to have an oracle ranking. We gather this oracle ranking with respect to language T as follows. For each language L in Wikipedia (including T , if it is there), we learn a model, and test this model on\n2uroman does not currently support this.\nAlgorithm 1 Our method Input\nN : set of names in English T : target language\nReturns N \u2032: names transliterated into T\n1: for \u2200L \u2208 W do . Surrogate selection 2: Get sim(L, T ) 3: end for 4: Order W according to scores 5: S = top language in W 6: Train model using (English, S) 7: Use model to transliterate N into N \u2032\nname pairs in language T . Now we have a score for each L, which we use to induce a ranking over W . This is the oracle ranking.\nWe would expect that T is ranked at the top, and languages much different from T are near the bottom. For example, a model trained on Russian should perform well on Ukrainian data, but a model trained on Korean should not. Note that in order to get the ranking, you need name pairs from T ."}, {"heading": "3.2 Phonetic Similarity", "text": "Our phonetic similarity metric is based on phonetic inventories, under the intuition that languages with similar inventories will have similar transliterations. This addresses the second challenge from Karimi et al. (2011), that of missing sounds.\nWe use PHOIBLE (Moran et al., 2014), a database of phonetic inventories of over 1600 languages, each composed of a common set of symbols from the International Phonetic Alphabet. We compute similarity between two languages X and Y by measuring the F1 between phoneme inventory sets LX and LY . We refer to this as simphon(X,Y )."}, {"heading": "3.3 Script Distribution Similarity", "text": "We calculate the script similarity of languages X and Y as the cosine similarity of character histogram vectors HX and HY . We romanize and lowercase all text first, so these vectors usually have length about 26 (some languages include spurious characters, and some do not use all 26 characters). Any small body of target text will suffice to estimate HX and HY . We refer to this as simscript(X,Y )."}, {"heading": "3.4 Genealogical Similarity", "text": "WALS (Dryer and Haspelmath, 2013) provides some basic features of nearly every language, including the language family and genus (Comrie et al., 2013), where family is a broader distinction than genus. We calculate the genealogical similarity of languages X and Y as follows: 1, if genus and family match, 0.5 if just family matches, and 0 otherwise. We call this simgen(X,Y )"}, {"heading": "3.5 Learned Similarity", "text": "We combined the component similarities by using them as features in SVMrank (Joachims, 2006), and using the oracle rankings as supervision. In addition to the component similarities, we also used all 6 distance features from URIEL (Littel et al., 2016), a collection of language resources. These distances are calculated from features in WALS and PHOIBLE, and are distributed with the URIEL package. The distance features are: genetic, geographic, inventory, phonological, syntactic, and featural. When predicting rankings on any target language, we trained on all the other experimental languages (see \u00a7 4). We refer to this as simlearned(X,Y )."}, {"heading": "4 Experiments and Results", "text": "We chose 9 languages to act as low-resource target languages (although some of them are actually highresource): Bengali, Chuvash, Armenian (hye), Kannada, Mazandarani, Newar, Thai, Uzbek, and Mingrelian (xmf). We included a diverse set of scripts, and we tended towards languages with a small number of training pairs.\nTo test our hypotheses, we did three experiments. The first, in \u00a7 4.1, validates the similarity metric. The second, in \u00a7 4.2, shows that using surrogate languages can be effective. The third, in \u00a7 4.3, shows that combining multiple surrogate languages can be even more effective."}, {"heading": "4.1 Similarity Metric Validation", "text": "In the first experiment, we compare our similarity rankings against the oracle ranking, using normalized discounted cumulative gain (NDCG), a metric commonly used in Information Retrieval (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002). In this setting, an IR system ranks a set of documents, each of which has a\npre-defined relevance score. This captures the notion that highly relevant documents should be placed at the top of a retrieved list. The normalized version (NDCG) divides each DCG score by the maximum possible score. In our situation, documents correspond to languages, and the relevance score of each language S is the score achieved when we train on S and test on T (i.e. scores from the oracle ranking). We set k = 5, which means that we expect relevant languages to appear in the first 5 results.\nTable 1 shows results. The first row shows scores from a random permutation of the relevance scores. Each row following that shows results from the similarity metrics described in \u00a7 3.\nThe clear winner is the script distance, in the third row. This is a surprising result because it is the least knowledge-intensive metric. We had expected that the phonetic similarity would be more successful. Although it performs well for Kannada, it produces dismal scores for others. simlearned performs well on average, but never attains the best score for a language. We found that script similarity is by far the strongest feature, with a weight of about 30 times that of other weights."}, {"heading": "4.2 Effectiveness of Surrogate Languages", "text": "In the second experiment, we select the top language (as predicted by script similarity), and show that it can produce transliterations comparable in quality to the best language.\nThe results are shown in Table 2, where every value is an MRR score. The \u2018Oracle top\u2019 row shows scores from training on the top oracle language, and the \u2018Predicted top\u2019 shows the score from the top predicted language. The oracle score is, by definition, always greater than or equal to the predicted ranking. Note that we removed T from all rankings in order to simulate a low-resource setting.\nThe top predicted language is the same as the oracle in 4 out of 9 cases. In each other case, the score of the top predicted language is not far from the best.\nThe exception is Newar (new), which is 8 points below the best. We hypothesize that this is because the script similarity score doesn\u2019t take into account the size of the training data. The top predicted language, Nepali, has a small number of training pairs."}, {"heading": "4.3 Combining Surrogate Languages", "text": "The third experiment combines the top k predicted languages into a single prediction result via weighted voting. At test time, each predicted language produces an n-best list with scores for each word in the test set. We combine n-best lists by using the score to vote. The results are seen in Table 2, in the row titled \u2018Predicted top k\u2019, where k = 5.\nIn all cases, if the predicted top is not the best choice (for example, as in Uzbek), then the combination score helps. In two cases, Newar and Uzbek, the top 5 combination gives a score that is better than the oracle (indicated by a *)."}, {"heading": "5 Related Work", "text": "Transliteration is typically studied in the context of a single language or a group of languages. To name a few, there is work on Arabic (Sherif and Kondrak, 2007) and Japanese (Knight and Graehl, 1998). For an excellent survey on the state of the art in transliteration, see Karimi et al. (2011).\nPasternack and Roth (2009) and Irvine et al. (2012) both do transliteration using data harvested from Wikipedia, but cannot address those languages with little or no presence in Wikipedia.\nThere is work on transliteration using only monolingual phonetic mappings (Jagarlamudi and Daume\u0301 III, 2012; Yoon et al., 2007). While these\nmappings are less expensive than name pairs, they still require expert knowledge to create.\nWe contrast our similarity metric with a transliterability measure called Weighted AVerage Entropy (WAVE) (Kumaran et al., 2010). WAVE measures the frequency of alphabet ngrams weighted by the entropy of the ngram mapping. However, to estimate WAVE, one needs to have a set of name pairs in T .\nRosa and Zabokrtsky\u0301 (2015) introduce a language similarity metric for selecting a source treebank in cross lingual parsing. The metric is based on KLdivergence of POS trigrams, and is analogous to WAVE in that annotations in T are required.\nIn the multilingual tradition, this paper is closely related to direct transfer techniques (Ta\u0308ckstro\u0308m et al., 2012; McDonald et al., 2011)."}, {"heading": "6 Discussion and Future Work", "text": "We have shown a way to extend transliteration capabilities into all languages using Wikipedia as a data source. We showed a method for using task-driven language similarity metrics to induce a ranking over languages that is very close to an oracle ranking. Finally, our combination algorithm is capable of producing scores that outperform the oracle scores.\nIn the future, we will explore more sophisticated techniques for combining languages, with the hope\nof significantly beating the oracle every time."}], "references": [{"title": "Named entity recognition using cross-lingual resources: Arabic as an example", "author": ["Kareem Darwish"], "venue": null, "citeRegEx": "Darwish.,? \\Q2013\\E", "shortCiteRegEx": "Darwish.", "year": 2013}, {"title": "Transliterating from all languages. In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA)", "author": ["Irvine et al.2010] Ann Irvine", "Chris Callison-Burch", "Alexandre Klementiev"], "venue": null, "citeRegEx": "Irvine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2010}, {"title": "Processing informal, romanized pakistani text messages", "author": ["Irvine et al.2012] Ann Irvine", "Jonathan Weese", "Chris Callison-Burch"], "venue": "In Proceedings of the Second Workshop on Language in Social Media,", "citeRegEx": "Irvine et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2012}, {"title": "Regularized interlingual projections: evaluation on multilingual transliteration", "author": ["Jagarlamudi", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Jagarlamudi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jagarlamudi et al\\.", "year": 2012}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Machine transliteration survey", "author": ["Falk Scholer", "Andrew Turpin"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Karimi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2011}, {"title": "Multilingual named entity recognition using parallel data and metadata from wikipedia", "author": ["Kim et al.2012] Sungchul Kim", "Kristina Toutanova", "Hwanjo Yu"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Compositional machine transliteration", "author": ["Kumaran et al.2010] A Kumaran", "Mitesh M Khapra", "Pushpak Bhattacharyya"], "venue": "ACM Transactions on Asian Language Information Processing (TALIP),", "citeRegEx": "Kumaran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2010}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Learning better transliterations", "author": ["Pasternack", "Roth2009] J. Pasternack", "D. Roth"], "venue": "In Proc. of the ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Pasternack et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pasternack et al\\.", "year": 2009}, {"title": "Klcpos3 - a language similarity measure for delexicalized parser transfer", "author": ["Rosa", "Zabokrtsk\u00fd2015] Rudolf Rosa", "Zdenek Zabokrtsk\u00fd"], "venue": null, "citeRegEx": "Rosa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosa et al\\.", "year": 2015}, {"title": "Substring-based transliteration", "author": ["Sherif", "Kondrak2007] Tarek Sherif", "Grzegorz Kondrak"], "venue": "In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Sherif et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sherif et al\\.", "year": 2007}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 conference of the North American chapter of the association", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Multilingual transliteration using feature based phonetic method", "author": ["Yoon et al.2007] Su-Youn Yoon", "Kyoung-Young Kim", "Richard Sproat"], "venue": "In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Yoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "The problem is largely of interest for downstream tasks such as cross lingual named entity recognition (Darwish, 2013; Kim et al., 2012).", "startOffset": 103, "endOffset": 136}, {"referenceID": 7, "context": "The problem is largely of interest for downstream tasks such as cross lingual named entity recognition (Darwish, 2013; Kim et al., 2012).", "startOffset": 103, "endOffset": 136}, {"referenceID": 1, "context": "Multilingual-minded approaches typically gather this from Wikipedia (Pasternack and Roth, 2009; Irvine et al., 2010).", "startOffset": 68, "endOffset": 116}, {"referenceID": 6, "context": "This addresses the second challenge from Karimi et al. (2011), that of missing sounds.", "startOffset": 41, "endOffset": 62}, {"referenceID": 5, "context": "We combined the component similarities by using them as features in SVMrank (Joachims, 2006), and using the oracle rankings as supervision.", "startOffset": 76, "endOffset": 92}, {"referenceID": 6, "context": "For an excellent survey on the state of the art in transliteration, see Karimi et al. (2011).", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "Pasternack and Roth (2009) and Irvine et al. (2012) both do transliteration using data harvested from Wikipedia, but cannot address those languages with little or no presence in Wikipedia.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "There is work on transliteration using only monolingual phonetic mappings (Jagarlamudi and Daum\u00e9 III, 2012; Yoon et al., 2007).", "startOffset": 74, "endOffset": 126}, {"referenceID": 8, "context": "We contrast our similarity metric with a transliterability measure called Weighted AVerage Entropy (WAVE) (Kumaran et al., 2010).", "startOffset": 106, "endOffset": 128}, {"referenceID": 13, "context": "In the multilingual tradition, this paper is closely related to direct transfer techniques (T\u00e4ckstr\u00f6m et al., 2012; McDonald et al., 2011).", "startOffset": 91, "endOffset": 138}, {"referenceID": 9, "context": "In the multilingual tradition, this paper is closely related to direct transfer techniques (T\u00e4ckstr\u00f6m et al., 2012; McDonald et al., 2011).", "startOffset": 91, "endOffset": 138}], "year": 2016, "abstractText": "We introduce a method for transliteration generation that can produce transliterations in every language. Where previous results are only as multilingual as Wikipedia, we show how to use training data from Wikipedia as surrogate training for any language. Thus, the problem becomes one of ranking Wikipedia languages in order of suitability with respect to a target language. We introduce several task-specific methods for ranking languages, and show that our approach is comparable to the oracle ceiling, and even outperforms it in some cases.", "creator": "LaTeX with hyperref package"}}}