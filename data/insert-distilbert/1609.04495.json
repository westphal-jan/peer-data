{"id": "1609.04495", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Tsallis Regularized Optimal Transport and Ecological Inference", "abstract": "optimal selective transport is a financially powerful framework for computing meaningful distances between probability distributions. we unify the two main approaches to optimal transport, namely using monge - kantorovitch and sinkhorn - cuturi, into with what we define as tsallis regularized symmetric optimal adaptive transport ( \\ trot ). \\ trot ~ interpolates a rich family of distortions from wasserstein to max kullback - leibler, encompassing as fairly well pearson, neyman and hellinger divergences, to name a few. we now show that metric properties known for sinkhorn - cuturi generalize roughly to \\ trot, find and provide globally efficient algorithms for finding the optimal transportation plan with formal convergence proofs. we also present the first explicitly application logic of optimal transport to the problem of ecological inference, that is, applying the reconstruction of joint distributions from their marginals, a problem seemingly of large interest in the social sciences. \\ trot ~ provides a convenient framework for ecological inference by allowing to compute the geographic joint distribution - - - that is, the optimal transportation plan itself - - - when side information is available, which is \\ textit { e. g. } ` typically not what census represents in all political science. experiments on data from the 2012 utah us presidential elections display the potential of \\ math trot ~ in delivering a faithful reconstruction of how the joint distribution of ethnic groups and meaningful voter outcome preferences.", "histories": [["v1", "Thu, 15 Sep 2016 02:30:10 GMT  (269kb,D)", "http://arxiv.org/abs/1609.04495v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["boris muzellec", "richard nock", "giorgio patrini", "frank nielsen"], "accepted": true, "id": "1609.04495"}, "pdf": {"name": "1609.04495.pdf", "metadata": {"source": "CRF", "title": "Tsallis Regularized Optimal Transport and Ecological Inference", "authors": ["Boris Muzellec", "Richard Nock", "Giorgio Patrini"], "emails": ["boris.muzellec@polytechnique.edu", "richard.nock@data61.csiro.au", "giorgio.patrini@anu.edu.au", "Frank.Nielsen@acm.org"], "sections": [{"heading": "1 Introduction", "text": "Optimal transport (ot) allows to compare probability distributions by exploiting the underlying metric space on their supports [22, 26]. A number of prominent applications allow for a natural definition of this underlying metric space, from image processing [32] to natural language processing [25], music processing [13] and computer graphics [36].\nOne key problem of ot is its processing complexity \u2014 cubic in the support size, ignoring low order terms (on state of the art LP solvers [8]). Moreover, the optimal transportation plan has often many zeroes, which is not desirable in some applications. An important workaround was found and\nar X\niv :1\n60 9.\n04 49\n5v 1\n[ cs\nconsists in penalizing the transport cost with a Shannon entropic regularizer [8]. At the price of changing the transport distance, for a distortion with metric related properties, comes an algorithm with geometric convergence rates [8, 16]. As a result, we can picture two separate approches to ot: one essentially relies on the initial Monge-Kantorovitch formulation optimizing the transportation cost itself [39], but is computationally expensive; the other is based on tweaking the transportation cost by Shannon regularizer [8]. The corresponding optimization algorithm, grounded in a variety of different works [7, 34, 37], is fast and can be very efficiently parallelized [8].\nOur paper brings three contributions. (i) We interpolate these two worlds using a family of entropies celebrated in nonextensive statistical mechanics, Tsallis entropies [38], and hence we define the Tsallis regularized optimal transport (trot). We show that the metric properties for Shannon entropy still hold in this more general case, and prove new properties that are key to our application. (ii) We provide efficient optimization algorithms to compute trot and the optimal transportation plan. (iii) Last but not least, we provide a new application of trot to a field in which this optimal transportation plan is the key unknown: the problem of ecological inference.\nEcological inference deals with recovering information from aggregate data. It arises in a diversity of applied fields such as econometrics [6, 4], sociology and political science [23, 24] and epidemiology [40], with a long history [31]; interestingly, the empirical software engineering com-\nmunity has also explored the idea [28]. Its iconic application is inferring electorate behaviour: given turnout results for several parties and proportions of some population strata, e.g. percentages of ethnic groups, for many geographical regions such as counties, the aim is to recover contingency tables for parties \u00d7 groups for all those counties. In the language of probability the problem is isomorphic to the following: given two random variables and their respective marginal distributions \u2014 conditioned to another variable, the geography \u2014, compute their conditional joint distribution (See Figure 1).\nThe problem is fundamentally under-determined and any solution can only either provide loose deterministic bounds [12, 6, 4] or needs to enforce additional assumptions and prior knowledge on the data domain [23]. More recently, the problem has witnessed a period of renaissance along with the publication of a diversity of methods from the second family, mostly inspired by distributional assumptions as summarised in [24]. Closer to our approach, [21] follows the road of a minimal subset of assumptions and frame the inference as an optimization problem. The method favors one solution according to some information-theoretic solution, e.g. the Cressie-Read power divergence, intended as an entropic measure of the joint distribution.\nThere is an intriguing link between optimal transport and ecological inference: if we can figure out the computation of the ground metric, then the optimal transportation plan provides a solution to the ecological inference problem. This is appealing because it ties the computation of the joint distribution to a ground individual distance between people. Figure 1 gives an example. As recently advocated in ecological inference [14], it turns out that we have access to more and more side information that helps to solve ecological inference \u2014 in our case, the computation of this ground metric. Polls, census, social networks are as many sources of public or private data that can be of help. It is not our objective to show how to best compute the ground metric, but we show an example on real world data for which a simple approach gives very convincing results.\nTo our knowledge, there is no former application of optimal transport (regularized or not) to ecological inference. The closest works either assume that the joint distribution follows a random distribution constrained to structural or marginal constraints [15] (and references therein) or modify the constraints to the marginals and / or add constraints to the problem [11]. In all cases, there is no ground metric (or anything that looks like a cost) among supports that ties the computation of the joint distribution. More importantly, as noted in [14], traditional ecological inference would not use side information of the kind that would be useful to estimate our ground metric.\nThis paper is organized as follows. In Section \u00a7 2, we present the main definitions for ot. \u00a7 3 presents trot and its geometric properties. \u00a7 4 presents the algorithms to compute trot and the optimal transportation plan, and their properties. \u00a7 5 details experiments. A last Section concludes with open problems. All proofs, related comments, and some experiments are deferred to a Supplementary Material (sm)."}, {"heading": "2 Basic definitions and concepts", "text": "In the following, we let 4n . = {x \u2208 Rn+ : x>1 = 1} denote the probability simplex (bold faces like x denote vectors). \u3008P,Q\u3009 .= vec(P )>vec(Q) denotes Frobenius product (vec(.) is the vectorization of a matrix). For any two r, c \u2208 4n, we define their transportation polytope U(r, c) . = {P \u2208 Rn\u00d7n+ : P1 = r, P>1 = c}. For any cost matrix M \u2208 Rn\u00d7n, the transportation distance between r and c\nas the solution of the following minimization problem:\ndM (r, c) .\n= min P\u2208U(r,c)\n\u3008P,M\u3009 . (1)\nIts argument, P ? .\n= arg minP\u2208U(r,c)\u3008P,M\u3009 is the (optimal) transportation plan between r and c. Assuming M 6= 0, P ? is unique. Furthermore, if M is a metric matrix, then dM is also a metric [39, \u00a76.1].\nIn current applications of optimal transport, the key unknown is usually the distance dM [8, 9, 19, 29, 36] (etc). In the context of ecological inference [21], it is rather P ?: P ? describes a joint distribution between two discrete random variables R and C with respective marginals r and c, p?ij = Pr(R = ri \u2227 C = cj), for example the support of R being the votes for year Y US presidential election, and C being the ethnic breakdown in the US population in year Y , see Figure 1. In this case, p?ij denotes an \u201dideal\u201d joint distribution of votes within ethnicities, ideal in the sense that it minimizes a distance based on the belief that votes correlate positively with a similarity between an ethnic profile and a party\u2019s profile. While we will carry out most of our theory on formal transportation grounds, requiring in particular that M be a distance matrix, it should be understood that requiring just \u201dcorrelation\u201d alleviates the need for M to formally be a distance for ecological inference."}, {"heading": "3 Tsallis Regularized Optimal Transport", "text": "For any p \u2208 Rn+, q \u2208 R, the Tsallis entropy of p, Hq(p) is:\nHq(p) . = 1 1\u2212 q \u00b7 \u2211 i (pqi \u2212 pi) , (2)\nand for any P \u2208 Rn\u00d7n+ , we letHq(P ) . = Hq(vec(P )). Notably, we have limq\u21921Hq(p) = \u2212 \u2211 i pi ln pi .\n= H1(p), which is just Shannon\u2019s entropy. For any \u03bb > 0, we define the Tsallis Regularized Optimal Transport (trot) distance.\nDefinition 1 The trot(q, \u03bb,M) distance (or trot distance for short) between r and c is:\nd\u03bb,qM (r, c) .\n= min P\u2208U(r,c) \u3008P,M\u3009 \u2212 1 \u03bb \u00b7Hq(P ) . (3)\nA simple yet important property is that trot distance unifies both usual modalities of optimal transport. It generalizes optimal transport (ot) when q \u2192 0, since Hq converges to a constant and so the ot-distance is obtained up to a constant additive term [22, 26]. It also generalizes the regularized optimal transport approach of [8] since limq\u21921 d \u03bb,q M (r, c) = d \u03bb M (r, c), the Sinkhorn distance between r and c [8]. There are several important structural properties of d\u03bb,qM that motivate the unification of both approaches. To state them, we respectively define the q-logarithm,\nlogq(x) . = (1\u2212 q)\u22121 \u00b7 (x1\u2212q \u2212 1) , (4)\nthe q-exponential, expq(x) . = (1 + (1 \u2212 q) \u00b7 x)1/(1\u2212q) and Tsallis relative q-entropy between P,R \u2208 Rn\u00d7n+ as:\nKq(P,R) . = 1 1\u2212 q \u00b7 \u2211 i,j ( qpij + (1\u2212 q)rij \u2212 pqijr 1\u2212q ij ) . (5)\nTaking joint distribution matrices P,R and q \u2192 1 allows to recover the natural logarithm, the exponential and Kullback-Leibler (kl) divergence, respectively [1]. Other notable examples include (i) Pearson\u2019s \u03c72 statistic (q = 2), (ii) Neyman\u2019s statistic (q = \u22121), (iii) square Hellinger distance (q = 1/2) and the reverse kl divergence if scaled appropriately by q [21], which also allows to span Amari\u2019s \u03b1 divergences for \u03b1 = 1\u2212 2q [1]. For any function f : R\u2192 R, denoting f(P ) for matrix P as the matrix whose general term is f(pij).\nLemma 2 Let U\u0303 . = expq(\u22121) exp\u22121q (\u03bbM). Then:\nd\u03bb,qM (r, c) = 1\n\u03bb \u00b7 min P\u2208U(r,c) K1/q(P q, U\u0303 q) + g(M) , (6)\nwhere g(M) .\n= (1/\u03bb) \u00b7 \u3008U\u0303 q, 1\u3009 does not play any role in the minimization of K1/q(.\u2016.).\nLemma 2 shows that the trot distance is a divergence involving escort distributions [1, \u00a7 4], a particularity that disappears in Sinkhorn distances since it becomes an ordinary kl divergence between distributions. Predictably, the generalization is useful to create new solutions to the regularized optimal transport problem that are not captured by Sinkhorn distances (solution refers to (optimal) transportation plans, i.e. the argument of the min in eq. (3)).\nTheorem 3 Let S\u03bb,q(r, c) denote the set of solutions of eq. (3) when M ranges over all distance matrices. Then \u2200q, q\u2032 such that q 6= q\u2032, \u2200\u03bb, \u03bb\u2032, S\u03bb,q(r, c) 6= S\u03bb\u2032,q\u2032(r, c).\nFigure 2 provides examples of solutions. Adding the free parameter q is not just interesting for the reason that we bring new solutions to the table: (1/q) \u00b7Kq(p, r) turns out to be Cressie-Read Power Divergence (for q = \u03bb + 1, [21]), and so trot has an applicability in ecological inference that Sinkhorn distances alone do not have. In addition, we also generalize two key facts already known for Sinkhorn distances [8]. First, the solution to trot is unique (for q 6= 0) and satisfies a simple analytical expression amenable to convenient optimization.\nTheorem 4 There exists exactly one matrix P \u2208 U(r, c) solution to trot(q, \u03bb,M). It satisfies:\npij = expq(\u22121) exp\u22121q (\u03b1i + \u03bbmij + \u03b2j) ,\u2200i, j . (7)\n(\u03b1,\u03b2 \u2208 Rn are unique up to an additive constant).\nSecond, we can tweak trot to meet distance axioms. Let\ndM,\u03b1,q(r, c) .\n= min P\u2208U(r,c)\nHq(P )\u2212Hq(r)\u2212Hq(c)\u2265\u03b1\n\u3008P,M\u3009 , (8)\nwhere \u03b1 \u2265 0. For any M, r, c, \u03bb \u2265 0, \u2203\u03b1 \u2265 0 such that dM,\u03b1,q(r, c) = d\u03bb,qM (r, c). Also, the following holds.\nTheorem 5 For q \u2265 1, \u03b1 \u2265 0 and if M is a metric matrix, function (r, c)\u2192 1{r=c}dM,\u03b1,q(r, c) is a distance.\nTheorem 5 is a generalization of [8, Theorem 1] (for q = 1). As we explain more precisely in sm (Section 10), there is a downside to using dM,\u03b1,q as proof of the good properties of d \u03bb,q M : the triangle inequality, key to Euclidean geometry, transfers to d\u03bb,qM with varying and uncontrolled parameters \u2014 in the inequality, the three values of \u03bb may all be different! This does not break down the good properties of d\u03bb,qM , it just calls for workarounds. We now give one, which replaces dM,\u03b1,q by the quantity (\u03b2 \u2208 R is a constant):\nd\u03bb,q,\u03b2M (r, c) . = d\u03bb,qM (r, c) + \u03b2\n\u03bb \u00b7 (Hq(r) +Hq(c)) . (9)\nThis has another trivial advantage that dM,\u03b1,q does not have: the solutions (optimal transportation plans) are always the same on both sides. Also, the right-hand side is lowerbounded for any r, c and the trick that ensures the identity of the indiscernibles still works on d\u03bb,q,\u03b2M . The good news is that if q = 1, d\u03bb,q,\u03b2M , as is, can satisfy the triangle inequality.\nTheorem 6 d\u03bb,1,\u03b2M satisfies the triangle inequality, \u2200\u03b2 \u2265 1.\nHence, the solutions to d\u03bb,1M are optimal transport plans for distortions that meet the triangle inequality. This is new compared to [8]. For a general q \u2265 1, the proof, in Supplementary Material (Section 10), shows more, namely that d\n\u03bb,q,1/2 M satisfies a weak form of the identity of the\nindiscernibles. Finally, there always exist a value \u03b2 \u2265 0 such that d\u03bb,q,\u03b2M is non negative (d \u03bb,q,\u03b2 M is lowerbounded \u2200\u03b2 \u2265 0)."}, {"heading": "4 Efficient trot optimizers", "text": "The key idea behind Sinkhorn-Cuturi\u2019s solution is that the KKT conditions ensure that the optimal transportation plan P ? satisfies P ? = diag(u) exp(\u2212\u03bbM)diag(v). Sinkhorn\u2019s balancing normalization can then directly be used for a fast approximation of P ? [34, 33]. This trick does not fit at first sight for Tsallis regularization because the q-exponential is not multiplicative for general q and KKT conditions do not seem to be as favorable. We give however workarounds for the optimization, that work for any q \u2208 R+.\nFirst, we assume wlog that q 6= 0, 1 since in those cases, any efficient LP solver (q = 0) or Sinkhorn balancing normalization (q = 1) can be used. The task is non trivial because for q \u2208 (0, 1), the function minimized in d\u03bb,qM is not Lipschitz, which impedes the convergence of gradient methods. In this case, our workaround is Algorithm 1 (so\u2013trot), which relies on a Second Order approximation of a fundamental quantity used in its convergence proof, auxiliary functions [10].\nAlgorithm 1 Second Order Row\u2013trot (so\u2013trot) Input: marginal r, matrix M , params \u03bb \u2208 R+\u2217, q \u2208 (0, 1) 1: A\u2190 \u03bbM 2: P \u2190 expq(\u22121) exp\u22121q (A) 3: repeat 4: P1 \u2190 P A,P2 \u2190 P1 A // = Kronecker divide 5: d\u2190 r \u2212 P1, b\u2190 P11,a\u2190 (2\u2212 q)P21 6: for i = 1, 2, ..., n 7: if di \u2265 0 then 8: yi \u2190 \u2212bi+ \u221a b2i+4aidi\n2ai 9: else\n10: yi \u2190 di/bi 11: end if 12: if |yi| > q\n(6\u22124q)\u00b7maxj p1\u2212qij then\nyi \u2190 q \u00b7 sign(ri \u2212\n\u2211 j pij)\n(6\u2212 4q) \u00b7maxj p1\u2212qij . (10)\n13: A\u2190 A\u2212 y1> 14: P \u2190 expq(\u22121) exp\u22121q (A) 15: until convergence\nOutput: P\nTheorem 7 (Convergence of so\u2013trot) For any fixed q \u2208 (0, 1), matrix P output by so\u2013trot converges to P ? with:\nP ? = arg min P\u2208Rn\u00d7n+ :P1=r K1/q(P q, U\u0303 q) .\nThe proof (in Supplementary Material, Section 11) is involved but interesting in itself because it represents one of the first use of the theory of auxiliary functions outside the realm of Bregman\nAlgorithm 2 KL Projected Gradient \u2013trot (kl\u2013trot)\nInput: Marginals r, c, Matrix U\u0303 , Gradient steps {tk} 1: P (0) \u2190 U\u0303 2: repeat 3: P (k+1) \u2190 SK(P (k) \u2297 exp(\u2212tk\u2207fq(P (k))), r, c) 4: until convergence\ndivergences in machine learning [5, 10]. Some important remarks should be made. First, since so\u2013trot uses only one of the two marginal constraints, it would need to be iterated (\u201dwrapped\u201d), swapping the row and column constraints like in Sinkhorn balancing. In practice, this is not efficient. Furthermore, iterating so\u2013trot over constraint swapping does not necessarily converge. For these reasons, we swap constraints in the algorithm, making one iteration of Steps 4-14 over rows, and then one iteration of Steps 4-14 over columns (this boils down to transposing matrices in so\u2013trot), and so on. This converges, but still is not the most efficient. To improve efficiency we perform two modifications, that do not impede convergence experimentally. First, we remove Step 12. In doing so, we not only save O(n2) computations for each outer loop, we essentially make so\u2013trot as parallelizable as Sinkhorn balancing [8]. Second, we remarked experimentally that convergence is faster when multiplying yi by 2 in Step 10, and dividing a by 2 in Step 5.\nFor simplicity, we still refer to this algorithm (balancing constraints in the algorithm, with the modifications for Steps 5, 10, 12) as so\u2013trot in the experiments.\nLast, when q \u2265 1, the function minimized in d\u03bb,qM becomes Lipschitz. In this case, we take the particular geometry of U(r, c) into account by using mirror gradient methods, which are equivalent to gradient methods projected according to some suitable divergence [2]. In our case, we consider Kullback-Leibler divergence, which can save a factor O(n/ \u221a log n) iterations [2]. Furthermore, the Kullback-Leibler projection can be written in terms of Sinkhorn-Knopp\u2019s (SK) algorithm with marginals constraints r, c [35], as is shown in Algorithm 2, named kl\u2013trot (\u2297 is Kronecker product). Theorem 8 If q > 1 and the gradient steps {tk} are s.t. \u2211 k tk \u2192 \u221e and \u2211 k t 2 k \u221e, matrix P output by kl\u2013trot converges to P ? with:\nP ? = arg min P\u2208U(r,c) K1/q(P q, U\u0303 q) .\n(proof omitted, follows [2, 35])"}, {"heading": "5 Experiments", "text": "We evaluate empirically the trot framework with its application to ecological inference. The dataset we use describes about 10 millions individual voters from Florida for the 2012 US presidential elections, as obtained from [20]. The data is much richer than is required for ecological inference: surely we could estimate the joint distribution of every voters\u2019 available attributes by counting. This is itself a particularly rare case of data quality in political science, where any analysis is often carried out on aggregate measurements. In fact, since ground truth distributions are effectively available, the Florida dataset has been used to test methodological advances in the field\n[14, 20]. As a demonstrative example, we focus on inferring the distributions of ethnicity and party for all Florida counties.\nDataset description and preprocessing. The data contains the following attributes for each voter : location (district, county), gender, age, party (Democrat, Republican, Other), ethnicity (White, African-american, Hispanic, Asian, Native, Other), 2008 vote (yes, no). About 800K voters with missing attributes are excluded from the study. Thanks to the richness of the data, marginal probabilities of ethnic groups and parties can be obtained by counting: for each county we obtain marginals r, c for the optimal transport problems.\nEvaluation assumptions. Two assumptions are made in terms of information available for inference. First, the ground truth joint distributions for one district are known; we chose district number 3 which groups 9 out of 68 counties of about 285K voters in total. This information will be used to tune hyper-parameters. Second, a cost matrix M rbf is computed based on mean voter\u2019s attributes at state level. For the sake of simplicity, we retain only age (normalized in [0, 1]), gender and the 2008 vote; notice that in practice geographical attributes may encode relevant information for computing distances between voter behaviours [14]. We do not use this. For distance matrix M rbf, we aggregate those features over all Florida for each party to obtain the vectors \u00b5p of the party\u2019s expected profile and for each ethnic group to obtain the vectors \u00b5e of the ethnicity\u2019s expected profile. The dissimilarity measure relies on a Gaussian kernel between average county profiles:\nmrbfij .\n= \u221a\n2\u2212 2 exp(\u2212\u03b3 \u00b7 \u2016\u00b5pi \u2212 \u00b5ej\u20162) , (11)\nwith \u03b3 = 10. The given function is actually the Hilbert metric in the RBF space. Table 1 shows the resulting cost matrix. Notice how it does encode some common-sense knowledge: White and Republican is the best match, while Hispanic and Asians are the worst match with Republican profiles. It is rather surprising that only 3 features such as age, gender and whether people voted at the last election can reflect so well those relative political traits; these results are indeed much in line with survey-based statistics [18]. We also try another cost matrix M , M sur, derived from the ID proportions of parties composition given in [18]; msurij is computed as 1 \u2212 pij , where pij is the proportion of people registered to party j belonging to ethnic group i. Finally, we consider a \u201dno prior\u201d matrix Mno, in which mnoij = 1,\u2200i, j.\nCross-validation of q. We study the solution of trot for a grid of \u03bb \u2208 [0.01, 1000], q \u2208 [0.5, 4], inferring the joint distributions of all counties of district number 3. We measure average KLdivergence between inferred and ground truth joint distributions. Notice that each county defines a different optimal transport problem; inferring the joint distributions for multiple counties at a time is therefore trivial to parallelize. This is somewhat counter-intuitive since we may believe that geographically wider spread data should improve inference at a local level, that is, more data better inference. Indeed, the implicit coupling of the problem is represented by cost matrix, which expresses some prior knowledge of the problem by means of all data from Florida.\nBaselines and comparisons with other methods. To evaluate quantitatively the solution of trot is useful to define a set of baseline methods: i) Florida-average, which the same state-level joint distribution (assumed prior knowledge) for each of the 67 county; ii) Simplex, that is the solution of optimal transport with no regularization as given by the Simplex algorithm; iii) Sinkhorn(Cuturi)\u2019s algorithm, which is trot with q = 1; iv) trot. ii-iv are tested with M \u2208 {M rbf,M sur}, and we provide in addition the results for trot with M = Mno. Hyper-parameters are crossvalidated independently for each algorithm.\nTable 2 reports a quantitative comparison. From the most general to the most specific, there are three remarks to make. First, optimal transport can be (but is not always) better than the default distribution (Florida average). Second, regularizing optimal transport consistently improves upon these baselines. Third, trot successfully matches Sinkhorn\u2019s approach when q = 1 is be the best solution in trot\u2019s range of q (M = M rbf), and manages to tune q to significantly beat Sinkhorn\u2019s when better alternatives exist: with M = M sur, trot divides the expected KL divergence by more than seven (7) compared to Sinkhorn. This is a strong advocacy to allow for the tuning of q. Notice that in this case, \u03bb is larger compared to M = M rbf, which makes sense since M = M sur is more accurate for the optimal transport problem (see the Simplex results) and so the weight of the regularizer predictably decreases in the regularized optimal transport distance. We conjecture that M = M sur beats M = M rbf in part because it is somehow finer grained: M rbf is computed from sufficient statistics for the marginals alone, while M sur exploits information computed from the cartesian product of the supports. Figure 3 compares all 1 836 inferred probabilities (3\u00d7 6 per county) with respect to the ground truth for Sinkhorn vs trot using M = M sur. Remark that the figures in Table 2 translate to per-county ecological inference results that are significantly more in favor of trot, which basically has no \u201dhard-to-guess\u201d counties compared to Sinkhorn for which the absolute difference between inference and ground truth can exceed 10%.\nTo finish up, additional experiments, displayed in sm (Sections 12 and 13) also show that trot with M = M sur manages to have a distribution of per county errors extremely peaked around zero error, compared to the simplest baselines (Florida average and trot with M = Mno). These are good news, but there are some local discrepancies. For example, there exists one county on which trot with M = M sur is beaten by trot with M = Mno."}, {"heading": "6 Discussion and conclusion", "text": "In this paper, we have bridged Shannon regularized optimal transport and unregularized optimal transport, via Tsallis entropic regularization. There are three main motivations to the generalization, the two first have already been discussed: trot allows to keep the properties of Sinkhorn distances, and fields like ecological inference bring natural applications for the general trot family. The application to ecological inference is also interesting because the main unknown is the optimal transportation plan and not necessarily the transportation distance obtained. The third and last motivation is important for applications at large and ecological inference in particular. trot spans a subset of f -divergences, and f -divergences satisfy the information monotonicity property that coarse graining does not increase the divergence [1, \u00a7 3.2]. Furthermore, f -divergences are invariant under diffeomorphic transformations [30, Theorem 1]. This is a powerful statement: if the ground metric is affected by such a transformation h (for example, we change the underlying manifold coordinate system, e.g. for privacy reasons), then, from the optimal trot transportation plan P ?, the transportation plan corresponding to the initial coordinate system can be recovered from the sole knowledge of h\u22121.\nThe algorithms we provide allow for the efficient optimization of the regularized optimal transport for all values of q \u2265 0, and include notable cases for which conventional gradient-based ap-\nproaches would probably not be the best approaches due to the fact that the function to optimize is not Lipschitz for the q chosen. In fact, the main notable downside of the generalization is that we could not prove the same (geometric) convergence rates as the ones that are known for Sinkhorn\u2019s approach [16].\nOur results display that there can be significant discrepancies in the regularized optimal transport results depending on how cost matrix M is crafted, yet the information we used for our best experiments is readily available from public statistics (matrices M rbf,M sur). Even the instantiation without prior knowledge (M = 11>) does not strictly fail in returning useful solutions (compared e.g. to Florida average and unregularized optimal transport). This may be a strong advocacy to use trot even on domains for which little prior knowledge is available."}, {"heading": "Acknowledgments", "text": "The authors wish to thank Seth Flaxman and Wendy K. Tam Cho for numerous stimulating discussions. Work done while Boris Muzellec was visiting Nicta / Data61. Nicta was funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."}, {"heading": "7 Supplementary Material \u2014 Table of contents", "text": "Supplementary material on proofs Pg 16 Proof of Theorem 3 Pg 16 Proof of Theorem 4 Pg 17 Proof of Theorems 5 and 6 Pg 18 Proof of Theorem 7 Pg 23\nSupplementary material on experiments Pg 29 Per county error distribution, trot survey vs Florida average Pg 29 Per county errors, trot survey vs trot 11> Pg 29\nSupplementary Material: proofs"}, {"heading": "8 Proof of Theorem 3", "text": "Let M \u2208 Rn\u00d7n+ be a distance matrix, and q, q\u2032 \u2208 R\u2212{1}, q 6= q\u2032 (the case when q = 1 xor q\u2032 = 1 can be treated in a similar fashion). We suppose wlog that the support does not reduce to a singleton (otherwise the solution to optimal transport is trivial). Rescaling M and a constant row vector and a constant column vector, the solution of trot(q, \u03bb,M) can be written wlog as\npij = expq(\u22121) exp\u22121q (mij) . (12)\nAssume there exists a \u03bb\u2032 \u2208 R such that the solution of trot(q\u2032, \u03bb\u2032,M) is equal to that of trot(q, \u03bb,M). This is equivalent to saying that there exists \u03b1,\u03b2 \u2208 Rn such that\nexpq(mij) = expq\u2032(\u03b1i + \u03bb \u2032mij + \u03b2j) ,\u2200i, j . (13)\nComposing with logq\u2032 and rearranging, this implies that\nf\u03bb \u2032\nq\u2032,q(mij) = \u03b1i + \u03b2j , \u2200i, j , (14)\nwhere\nf\u03bb \u2032 q\u2032,q(x) . = logq\u2032 \u25e6 expq \u2212\u03bb\u2032Id . (15)\nNow, remark that, since M is a distance, mii = 0, \u2200i because of the identity of the indiscernibles, and so \u03b1i + \u03b2i = f \u03bb\u2032 q\u2032,q(0) = 0, implying \u03b1 = \u2212\u03b2. f\u03bb \u2032 q\u2032,q is differentiable. Let:\ng\u03bb \u2032 q\u2032,q(x) . =\nd\ndx f\u03bb \u2032 q\u2032,q(x)\n= expq\u2212q \u2032 q (x)\u2212 \u03bb\u2032 ; (16)\nh\u03bb \u2032 q\u2032,q(x) . =\nd\ndx g\u03bb \u2032 q\u2032,q(x)\n= (q \u2212 q\u2032) \u00b7 exp2q\u2212q\u2032\u22121q (x) . (17)\nIf we assume wlog that q > q\u2032, then g\u03bb \u2032 q\u2032,q is increasing and zeroes at most once over R, eventually on some m\u2217 that we define as m\u2217 = logq ( \u03bb\u2032 1 q\u2212q\u2032 ) if (\u03bb\u2032 > 1)\u2227(0 \u2208 Img\u03bb\u2032q\u2032,q) (and +\u221e otherwise). Notice that m\u2217 > 0 and f\u03bb \u2032\nq\u2032,q is bijective over (0,m \u2217). Suppose wlog that mij \u2264 m\u2217, \u2200i, j. Otherwise, all\ndistances are scaled by the same real so that mij \u2264 m\u2217,\u2200i, j: this does not alter the property of M being a distance. A distance being symmetric, we also have mij = mji and since f \u03bb\u2032 q\u2032,q is strictly increasing in the range of distances, then we get from eq. (14) that \u03b1i + \u03b2j = \u03b1j + \u03b2i, \u2200i, j and so \u03b1i \u2212 \u03b1j = \u03b2i \u2212 \u03b2j = \u2212(\u03b1i \u2212 \u03b1j) (since \u03b1 = \u2212\u03b2). Hence, there exists a real \u03b1 such that \u03b1 = \u03b1 \u00b7 1. We get, in matrix form\nf\u03bb \u2032 q\u2032,q(M) = \u03b11 > + 1\u03b2> (18)\n= \u03b1 \u00b7 11> \u2212 \u03b1 \u00b7 11> = 0 . (19)\nHence, mij = mii,\u2200i, j and the support reduces to a singleton (because of the identity of the indiscernibles), which is impossible.\nRemark that the proof also works when M is not a distance anymore, but for example contains all arbitrary non negative matrices. To see this, we remark that the right hand side of eq. (18) is a matrix of rank no larger than 2. Since f\u03bb\n\u2032 q\u2032,q is continuous, we have\nIm(f\u03bb \u2032 q\u2032,q) . = I \u2286 R\nwhere I is not reduced to a singleton and so the left hand side of eq. (18) spans matrices of arbitrary rank. Hence, eq. (18) cannot always hold."}, {"heading": "9 Proof of Theorem 4", "text": "Denote\nfij : pij \u2192 pijmij \u2212 1\n\u03bb(1\u2212 q) (pqij \u2212 pij) .\nfij is twice differentiable on R+\u2217, and\nd2\ndx2 fij(x) =\nq \u03bb xq\u22122 > 0\nfor any fixed q > 0, and so fij is strictly convex on R+\u2217. We also remark that U(r, c) is a nonempty compact subset of Rn\u00d7n. Indeed, rc> \u2208 U(r, c), \u2200P \u2208 U(r, c), \u2016P\u20161 = 1 (which proves boundedness) and U(r, c) is a closed subset of U(r, c) (being the intersection of the pre-images of singletons by continuous functions). Hence, since \u3008P,M\u3009 \u2212 1\u03bbHq(P ) = \u2211 i,j fij(pij), there exists a unique minimum of this function in U(r, c).\nTo prove the analytic shape of the solution, we remark that trot(q, \u03bb,M) consists in minimizing a convex function given a set of affine constraints, and so the KKT conditions are necessary and sufficient. The KKT conditions give\npij = expq(\u22121) exp\u22121q (\u03b1i + \u03bbmij + \u03b2j) ,\nwhere \u03b1,\u03b2 \u2208 Rn are Lagrange multipliers. Finally, let us show that Lagrange multipliers \u03b1,\u03b2 \u2208 Rn are unique up to an additive constant. Assume that \u03b1,\u03b1\u2032,\u03b2,\u03b2\u2032 \u2208 Rn are such that\n\u2200i, j, pij = expq(\u22121) exp\u22121q (\u03bbmij + \u03b1i + \u03b2j) = expq(\u22121) exp\u22121q (\u03bbmij + \u03b1\u2032i + \u03b2\u2032j) ,\nwhere P is the unique solution of trot(q, \u03bb,M). This implies\n\u03b1i + \u03b2j = \u03b1 \u2032 i + \u03b2 \u2032 j , \u2200i, j ,\ni.e.\n\u03b1i \u2212 \u03b1\u2032i = \u03b2\u2032j \u2212 \u03b2j , \u2200i, j .\nIn particular, if there exists i0 and C 6= 0 such that \u03b1i0 \u2212 \u03b1\u2032i0 = C, then \u2200j, \u03b2 \u2032 j = \u03b2j + C and in turn \u2200i, \u03b1i = \u03b1\u2032i + C, which proves our claim."}, {"heading": "10 Proof of Theorems 5 and 6", "text": "For reasons that we explain now, we will in fact prove Theorem 6 before we prove Theorem 5. Had we chosen to follow [8], we would have replaced trot(q, \u03bb,M) by:\ndM,\u03b1,q(r, c) .\n= min P\u2208U(r,c)\nHq(P )\u2212Hq(r)\u2212Hq(c)\u2265\u03b1\n\u3008P,M\u3009 , (20)\nfor some \u03b1 > 0. Both problems are equivalent since \u03bb in trot(q, \u03bb,M) plays the role of the Lagrange multiplier for the entropy constraint in eq. (20) [8, Section 3], and so there exists an equivalent value of \u03b1\u2217 for which both problems coincide:\ndM,\u03b1\u2217,q(r, c) = d \u03bb,q M (r, c) , (21)\nso eq. (20) indeed matches trot(q, \u03bb,M). It is clear from eq. (21) that \u03b1 does not depend solely on \u03bb, but also (eventually) on all other parameters, including r, c.\nThis would not be a problem to state the triangle inequality for dM,\u03b1,q, as in [8] (\u2200x,y, z \u2208 4n):\ndM,\u03b1,q(x, z) \u2264 dM,\u03b1,q(x,y) + dM,\u03b1,q(y, z) . (22)\nHowever, \u03b1 is fixed and in particular different from the \u03b1\u2217 that guarantee eq. (21) \u2014 and there might be three different sets of parameters for d\u03bb,qM as it would equivalently appear from eq. (22). Under the simplifying assumption that only \u03bb changes, we might just get from eq. (22):\nd\u03bb \u2217,q M (x, z) \u2264 d \u03bb\u2032\u2217,q M (x,y) + d \u03bb\u2032\u2032\u2217,q M (y, z) , (23)\nwith \u03bb\u2217 6= \u03bb\u2032\u2217 6= \u03bb\u2032\u2032\u2217. Worse, the transportation plans may change with \u03bb: for example, we may have\narg min P\u2208U(x,z) d\u03bb1,qM (x, z) 6= arg min P\u2208U(x,z) d\u03bb2,qM (x, z) ,\nwith \u03bb1 6= \u03bb2 and \u03bb1, \u03bb2 \u2208 {\u03bb\u2217, \u03bb\u2032\u2217, \u03bb\u2032\u2032\u2217}. So, the triangle inequality for d\u03bb,qM that follows from ineq. (22) does not allow to control the parameters of trot(q, \u03bb,M) nor the optimal transportation plans that follows. It does not show a problem in regularizing the optimal transport distance, but rather that the distance dM,\u03b1,q chosen from eq. (21) does not completely fulfill its objective in showing that regularization in d\u03bb,qM still keeps some of the attractive properties that unregularized optimal transport meets.\nTo bypass this problem and establish a statement involving a distance in which all parameters are in the clear and optimal transportation plans still coincide with d\u03bb,qM , we chose to rely on measure:\nd\u03bb,q,\u03b2M (r, c) .\n= min P\u2208U(r,c)\n\u3008P,M\u3009\n\u2212 1 \u03bb \u00b7 (Hq(P )\u2212 \u03b2 \u00b7 (Hq(r) +Hq(c))) ,\nwhere \u03b2 is some constant. There is one trivial but crucial fact about d\u03bb,q,\u03b2M (r, c): regardless of the choice of \u03b2, its optimal transportation plan is the same as for trot(q, \u03bb,M).\nLemma 9 For any r, c \u2208 4n and constant \u03b2 \u2208 R, let\nP1 .\n= arg min P\u2208U(r,c)\n\u3008P,M\u3009\n\u2212 1 \u03bb \u00b7 (Hq(P )\u2212 \u03b2 \u00b7 (Hq(r) +Hq(c))) . (24)\nP2 .\n= arg min P\u2208U(r,c)\n\u3008P,M\u3009\n\u2212 1 \u03bb \u00b7 (Hq(P )) . (25)\nThen P1 = P2.\nTheorem 10 The following holds for any fixed q \u2265 1 (unless otherwise stated):\n\u2022 for any \u03b2 \u2265 1, d\u03bb,1,\u03b2M satisfies the triangle inequality;\n\u2022 for the choice \u03b2 = 1/2, d\u03bb,q,1/2M satisfies the following weak version of the identity of the indiscernibles: if r = c, then d\n\u03bb,q,1/2 M (r, c) \u2264 0.\n\u2022 for the choice \u03b2 = 1/2, \u2200r \u2208 4n, choosing the (no) transportation plan P = Diag(r) brings \u3008P,M\u3009 \u2212 1 \u03bb \u00b7 ( Hq(P )\u2212 1 2 \u00b7 (Hq(r) +Hq(r)) ) = 0 .\nRemark: the last property is trivial but worth stating since the (no) transportation plan P = Diag(r) also satisfies P = arg minQ\u2208U(r,r)\u3008Q,M\u3009, which zeroes the (no) transportation distance dM (r, r). Proof To prove the Theorem, we need another version of the Gluing Lemma with entropic constraints [8, Lemma 1], generalized to handle Tsallis entropy.\nLemma 11 (Refined gluing Lemma) Let x,y, z \u2208 4n. Let P \u2208 U(x,y) and Q \u2208 U(y, z). Let S \u2208 Rn\u00d7n defined by general term\nsik . = \u2211 j pijqjk yj . (26)\nThe following holds about S:\n1. S \u2208 U(x, z);\n2. if q \u2265 1, then:\nHq(S)\u2212Hq(x)\u2212Hq(z) \u2265 Hq(P )\u2212Hq(x)\u2212Hq(y) . (27)\nProof The proof essentially builds upon [8, Lemma 1]. We remark that S can be built by sik = \u2211 j tijk , (28)\nwhere \u2200i, j, k \u2208 {1, 2, ..., n}, we have\ntijk . = pijqjk yj\n(29)\nif yj 6= 0 (and tijk = 0 otherwise) S is a transportation matrix between x and z. Indeed,\u2211\ni \u2211 j sijk = \u2211 j \u2211 i pijqjk yj\n= \u2211 j qjk yj \u2211 i pij\n= \u2211 j qjk yj yj = \u2211 j\nqjk = zk ;\u2211 k \u2211 j sijk = \u2211 j \u2211 k pijqjk yj\n= \u2211 j pij yj \u2211 k qjk\n= \u2211 j pij yj yj = \u2211 j pij = xi .\nSo, S \u2208 U(x, z). To prove ineq. (27), we need the following definition from [17].\nDefinition 12 [17] Let X and Y denote random variables. The Tsallis conditional entropy of X given Y, and Tsallis joint entropy of X and Y, are respectively given by:\nHq(X|Y) . = \u2212 \u2211 x,y p(x, y)q logq p(x|y) , Hq(X,Y) .\n= \u2212 \u2211 x,y p(x, y)q logq p(x, y) .\nThe Tsallis mutual entropy of X and Y is defined by\nIq(X;Y) .\n= Hq(X)\u2212Hq(X|Y) = Hq(X) +Hq(Y)\u2212Hq(X,Y) .\nWe have made use of the simplifying notation that removes variables names when unambiguous, like p(x) . = p(X = x). Let X,Y,Z be random variables jointly distributed as T , that is, for any x, y, z,\np(x, y, z) = p(x, y)p(y, z)\np(y) (30)\nIt follows from that and Bayes rule that:\np(x|y) = p(x, y) p(y)\n= p(x, y, z)\np(y, z) , \u2200z\n= p(x|y, z) , \u2200z , (31)\nand so\nIq(X;Z|Y) .\n= Hq(X|Y)\u2212Hq(X|Y,Z) = 0 . (32)\nIt comes from [17, Theorem 4.3],\nIq(X;Y,Z) = Iq(X;Z) + Iq(X;Y|Z) (33) = Iq(X;Y) + Iq(X;Z|Y) , (34)\nbut since Iq(X;Z|Y) = 0, we obtain\nIq(X;Y) = Iq(X;Z) + Iq(X;Y|Z) . (35)\nIt also follows from [17, Theorem 3.4] that Iq(X;Y|Z) \u2265 0 whenever q \u2265 1, and so\nIq(X;Y) \u2265 Iq(X;Z) , \u2200q \u2265 1 . (36)\nNow, it comes from Definition 12 and the definition of X,Y and Z from eq. (30),\n\u2212Iq(X;Y) = Hq(X,Y)\u2212Hq(X)\u2212Hq(Y) = Hq(P )\u2212Hq(x)\u2212Hq(y) , (37) \u2212Iq(X;Z) = Hq(X,Z)\u2212Hq(X)\u2212Hq(Z) = Hq(S)\u2212Hq(x)\u2212Hq(z) . (38)\nSince P \u2208 U\u03bb(x,y), by assumption, we obtain from ineq. (36) that whenever q \u2265 1,\nHq(S)\u2212Hq(x)\u2212Hq(z) \u2265 Hq(P )\u2212Hq(x)\u2212Hq(y) ,\nas claimed.\nWe can now prove Theorem 10. Shannon\u2019s entropy is denoted H1 for short. Define for short\n\u2206 . = H1(P ) +H1(Q)\u2212H1(S)\u2212 2\u03b2 \u00b7H1(y) , (39)\nwhere P,Q, S are defined in Lemma 11. It follows from the definition of S and [8, Proof of Theorem 1] that\nd\u03bb,q,\u03b2M (x, z)\n. = min R\u2208U(x,z) \u3008R,M\u3009 \u2212 1 \u03bb \u00b7 (H1(R)\u2212 \u03b2 \u00b7 (H1(x) +H1(z)))\n\u2264 \u3008S,M\u3009 \u2212 1 \u03bb \u00b7 (H1(S)\u2212 \u03b2 \u00b7 (H1(x) +H1(z))) \u2264 \u3008P,M\u3009+ \u3008Q,M\u3009 \u2212 1 \u03bb \u00b7 (H1(S)\u2212 \u03b2 \u00b7 (H1(x) +H1(z)))\n= \u3008P,M\u3009 \u2212 1 \u03bb \u00b7 (H1(P )\u2212 \u03b2 \u00b7 (H1(x) +H1(y))) +\u3008Q,M\u3009 \u2212 1 \u03bb \u00b7 (H1(Q)\u2212 \u03b2 \u00b7 (H1(y) +H1(z))) + 1\n\u03bb \u00b7 (H1(P ) +H1(Q)\u2212H1(S)\u2212 2\u03b2 \u00b7H1(y))\n. = d\u03bb,q,\u03b2M (x,y) + d \u03bb,q,\u03b2 M (y, z) +\n1 \u03bb \u00b7\u2206 . (40)\nWe now show that \u2206 \u2264 0. For this, observe that ineq. (27) yields:\n\u2206\n\u2264 (H1(S) +H1(y)\u2212H1(z)) +H1(Q)\u2212H1(S)\u2212 2\u03b2 \u00b7H1(y) = H1(Q)\u2212H1(y)\u2212H1(z) + 2(1\u2212 \u03b2)H1(y) , (41)\nand, by definition of Q,y, z,\nH1(Q)\u2212H1(y)\u2212H1(z) .\n= H1(Y,Z)\u2212H1(Y)\u2212H1(Z) . (42)\nShannon\u2019s entropy of a joint distribution is maximal with independence: H1(Y,Z) \u2264 H1(Y \u00d7 Z) = H1(Y) +H1(Z), so we get from eq. (41) after simplifying\n\u2206 \u2264 2(1\u2212 \u03b2)H1(y) . (43)\nHence if \u03b2 \u2265 1, then \u2206 \u2264 0. We get that for any \u03b2 \u2265 1,\nd\u03bb,1,\u03b2M (x, z) \u2264 d \u03bb,1,\u03b2 M (x,y) + d \u03bb,1,\u03b2 M (y, z) , (44)\nand d\u03bb,1,\u03b2M satisfies the triangle inequality. For \u03b2 = 1/2, it is trivial to check that for any x \u2208 4n, the (no) transportation plan P = Diag(x) is in U(x,x) and satisfies\n\u3008P,M\u3009 \u2212 1 \u03bb \u00b7 ( Hq(P )\u2212 1 2 \u00b7 (Hq(x) +Hq(x)) ) = 0\u2212 1\n\u03bb \u00b7 (Hq(x)\u2212Hq(x)) = 0 . (45)\nThis ends the proof of Theorem 10.\nNotice that Theorem 6 is in fact a direct consequence of Theorem 10. To finish up, we now prove Theorem 5. To simplify notations, let\nU\u03b1(r, c) . = {P \u2208 U(r, c) : Hq(P )\u2212Hq(r)\u2212Hq(c) \u2265 \u03b1(\u03bb)} . (46)\nSuppose P,Q in Lemma 11 are such that P,Q \u2208 U\u03bb(x,y). In this case,\nHq(P )\u2212Hq(x)\u2212Hq(y) \u2265 \u03b1 (47)\nand so point 2. in Lemma 11 brings\nHq(S)\u2212Hq(x)\u2212Hq(z) \u2265 \u03b1 , (48)\nso S \u2208 U\u03bb(x, z). The proof of [8, Theorem 1] can then be used to show that \u2200x,y, z \u2208 4n,\ndM,\u03b1,q(x, z) \u2264 dM,\u03b1,q(x,y) + dM,\u03b1,q(y, z) . (49)\nIt is easy to check that dM,\u03b1,q is non negative and that 1{r=c}dM,\u03b1,q(r, c) meets, in addition, the identity of the indiscernibles. This achieves the proof of Theorem 5."}, {"heading": "11 Proof of Theorem 7", "text": "Basic facts and definitions \u2014 In this proof, we make two simplifying assumptions: (i) we consider matrices either as matrices or as vectorized matrices without ambiguity, and (ii) we let \u03c6(P ) . = \u2212Hq(P ), noting that the domain of \u03c6 is 4n2 (nonnegative matrices with row- and columnsums in the simplex) when P \u2208 U(r, c). Since \u03c6 is convex, we can define a Bregman divergence with generator D\u03c6 [3] as:\nD\u03c6(P\u2016R) . = \u03c6(P )\u2212 \u03c6(R)\u2212 \u3008\u2207\u03c6(R), P \u2212R\u3009 .\nWe define\naij . = \u03b1i + \u03bbmij + \u03b2j , (50)\nso that\npij = expq(\u22121) exp\u22121q (aij) (51)\nin eq. (7). Finally, let us denote for short\nDq(P\u2016R) . = K1/q(P q, Rq) , (52)\nso that we can, reformulate eq. (6) as:\nd\u03bb,qM (r, c) = 1\n\u03bb \u00b7 min P\u2208U(r,c) Dq(P\u2016U\u0303) + g(M) , (53)\nand our objective \u201dreduces\u201d to the minimization of Dq(P\u2016U\u0303) over U(r, c). In so\u2013trot (Algorithm 1), we just care for a single constraint out of the two possible in U(r, c), so we will focus without loss of generality on the row constraint and therefore to the solution of:\nP ? .\n= arg min P\u2208Rn\u00d7n+ :P1=r\nDq(P\u2016U\u0303) . (54)\nThe same result would apply to the column constraint. Convergence proof \u2014 We reuse the theory of auxiliary functions developed for the iterative constrained minimization of Bregman divergences [3, 10]. We reuse notation \u201d \u201d following [5, 27] and define for any y \u2208 Rn, P \u2208 Rn\u00d7n matrix y q P \u2208 Rn\u00d7n such that\n(y q P )ij .\n= exp\u22121q (yi)pij\nexpq [ (1\u2212 q)yi exp1\u2212qq (yi) logq(pij) ] . (55) We also define key matrix P\u0303 \u2208 Rn\u00d7n with:\nP\u0303 . = rc> . (56)\nLet us denote\nQ . = { Q \u2208 Rn\u00d7n : Q = expq(\u22121) exp\u22121q (\u03b1>1 + \u03bbM + 1>\u03b2) } . P .\n= {P \u2208 4n2 : P1 = P\u03031 = r} .\nOne function will be key.\nDefinition 13 We define A(P,y) . = \u2211\niAi(P,y), with:\nAi(P,y) . = yiri + \u2211 j (pqij \u2212 exp q q(\u22121) exp\u2212qq (aij \u2212 yi)) . (57)\nHere aij is defined in eq. (50), ri is the i-th coordinate in r (the row marginal constraint), and y \u2208 Rn.\nLemma 14 For any y,\nA(P,y) = D\u03c6(P\u0303\u2016P )\u2212D\u03c6(P\u0303\u2016y q P ) . (58)\nFurthermore, A(P,0) = 0.\nProof We have\nD\u03c6(P\u0303\u2016P )\u2212D\u03c6(P\u0303\u2016y q P ) = \u2212D\u03c6(P\u2016y q P )\n+\u3008P\u0303 \u2212 P,\u2207\u03c6(y q P )\u2212\u2207\u03c6(P )\u3009 .\nBecause a Bregman divergence is non-negative and A(P,0) = 0, if, as long as there exists some y for which A(P,y) > 0 we keep on updating P by replacing it by y\u2217 q P such that A(P,y\u2217) > 0, then the sequence\nP0 = U\u0303 \u2192 P1 . = y\u22170 q P0 \u2192 P2 . = y\u22171 q P1 \u00b7 \u00b7 \u00b7 (59)\nwill converge to a limit matrix in the sequence,\nlim j Pj\n. = y\u2217j\u22121 q Pj\u22121 . (60)\nThis matrix turns out to be the one we seek.\nTheorem 15 Let Pj+1 . = yj q Pj (with P0 .\n= U\u0303) be such that A(Pj ,yj) > 0, \u2200j \u2265 0, and the sequence ends when no such yj exists. Then S . = {Pj}j\u22650 \u2282 Q\u0304. If furthermore S lies in a compact of Q\u0304, then it satisfies\nP ? .\n= lim j Pj = arg min P\u2208P Dq(P\u2016U\u0303) . (61)\nProof sketch: The proof relies on two steps, first that\nP ? .\n= lim j Pj = arg min P\u2208P D\u03c6(P\u2016U\u0303) , (62)\nand then the fact that (61) holds as well, which \u201damounts\u201d to replacing D\u03c6, which is Bregman, by Dq, which is not. Because it is standard in Bregman divergences, we sketch the first step. The fundamental result we use is adapted from [10] (see also [5, Theorem 1]).\nTheorem 16 Suppose that D\u03c6(P\u0303 , U\u0303) <\u221e. Then there exists a unique P ? satisfying the following four properties:\n1. P ? \u2208 P \u2229 Q\u0304\n2. \u2200P \u2208 P, \u2200R \u2208 Q\u0304, D\u03c6(P\u2016R) = D\u03c6(P\u2016P ?) +D\u03c6(P ?\u2016R)\n3. P ? = arg min P\u2208P D\u03c6(P\u2016U\u0303)\n4. P ? = arg min R\u2208Q\u0304 D\u03c6(P\u0303\u2016R)\nMoreover, any of these four properties determines P ? uniquely.\nIt is not hard to check that U\u0303 \u2208 Q\u0304 and whenever Pj \u2208 Q\u0304, then y q Pj \u2208 Q\u0304, \u2200y, so we indeed have S \u2282 Q\u0304. With the constraint that A(Pj ,yj) > 0,\u2200j \u2265 0, it follows from Lemma 14 that A(P,y) is an auxiliary function for S [5] if we can show in addition that if y = 0 is a maximum of A(P,y), then P \u2208 P. To remark that this is true, we have\n\u2207A(P,y)y = r \u2212 P1 , (63)\nso whenever A(P,y) reaches a maximum in y, we indeed have P1 = r and so P \u2208 P, and if y = 0 then because a Bregman divergence satisfies the identity of the indiscernibles, if y = 0 is the maximum, then S has converged to some P ?. From 4. above, we get\nP ? = arg min R\u2208Q\u0304 D\u03c6(P\u0303\u2016R) , (64)\nand so from 3. above, we also get\nP ? = arg min P\u2208P D\u03c6(P\u2016U\u0303) . (65)\nTo \u201dtransfer\u201d this result to Dq, we just need to remark that there is one remarkable trivial equality: D\u03c6(P\u2016R) = Dq(P\u2016R)\u2212 \u2211 i,j (pqij \u2212 r q ij) , (66)\nso that even when K1/q is not a Bregman divergence for a general q, it still meets the Bregman triangle equality [1].\nLemma 17 We have;\nDq(P\u2016R) +Dq(R\u2016S)\u2212Dq(P\u2016S) = D\u03c6(P\u2016R) +D\u03c6(R\u2016S)\u2212D\u03c6(P\u2016S) = \u3008P \u2212R,\u2207\u03c6(S)\u2212\u2207\u03c6(R)\u3009 . (67)\nHence, point 2. implies as well\nDq(P\u2016R) = Dq(P\u2016P ?) +Dq(P ?\u2016R) , (68)\n\u2200P \u2208 P, \u2200R \u2208 Q\u0304, and so Dq(P\u2016U\u0303) = Dq(P\u2016P ?) + Dq(P ?\u2016U\u0303), \u2200P \u2208 P, so that we also have (since Dq is non negative and satisfies Dq(P\u2016P ) = 0)\nP ? = arg min P\u2208P Dq(P\u2016U\u0303) ,\nas claimed (end of the proof of Theorem 15). Figure 4 summarizes Theorem 15. We are left with the problem of finding an auxiliary function for the sequence S, which we recall boils down to finding, whenever it exists, some y such that A(P,y) > 0.\nTheorem 18 A(P,y) is an auxiliary function for S for the sequence of updates y given as in steps 6-11 of so\u2013trot (Algorithm 1).\nProof We shall need the complete Taylor expansion of A(P,y).\nLemma 19 Let us denote for short \u03b3 .\n= 1\u2212 q. The Taylor series expansion of Ai(P,y) (as defined in Definition 13) is:\nAi(P,y) = yi(ri \u2212 \u2211 j pij)\n\u2212 \u2211 j pij \u221e\u2211 k=2\n[ 1\nk k\u22121\u220f l=1 (\u03b3 + q/l) ] yki ( p\u03b3ij q )k\u22121 . (69)\nProof Let us denote f(x) = exp\u2212qq (x). We have:\nd\ndx f(x) = q exp1\u2212qq (x)\nd\ndx exp\u22121q (x)\n= \u2212q exp\u22121q (x) . (70)\nA simple recursion also shows (\u2200k \u2265 2):\ndk\ndxk exp\u22121q (x) = (\u22121)k [ k\u220f i=1 (i\u2212 (i\u2212 1)q) ] expkq\u2212(k+1)q (x) ,\nwhich yields \u2200k \u2265 1,\ndk\ndxk f(x) = \u2212q d\nk\u22121\ndxk\u22121 exp\u22121q (x)\n= (\u22121)kq [ k\u22121\u220f i=1 (i\u03b3 + q) ] exp\u2212(k\u22121)\u03b3\u22121q (x) .\nSince expqq(\u22121) = expq(\u22121)/q and \u2200i, j, pij = expq(\u22121) exp\u22121q (aij), writing the Taylor development of f at point aij evaluated at yi, and adding the yiri + \u2211 j p q ij term, we obtain the desired result.\nWe have two special reals to define, ti and zi. If ri \u2264 \u2211\nj pij , we let ti denote the maximum of the second order approximation of Ai(P,y),\nT (2) i (yi) . = yi(ri \u2212 \u2211 j pij)\u2212 y2i 2 \u2211 j p1+\u03b3ij q , (71)\ni.e. the root of\nd\ndy T (2)(yi) = (ri \u2212 \u2211 j pij)\u2212 yi \u2211 j p1+\u03b3ij q .\nIf \u2211\nj pij \u2264 ri, we let zi be the the largest root of\nRi . = (ri \u2212 \u2211 j pij)\n\u2212yi \u2211 j p1+\u03b3ij q \u2212 y2i (2\u2212 q) \u2211 j p1+2\u03b3ij q2 . (72)\nWe shall see that zi is positive. Let y \u2217 i . = ti if ri \u2264 \u2211 j pij , and y \u2217 i . = zi otherwise. We first make the assumption that \u2223\u2223\u2223\u2223\u2223y\u2217i p \u03b3 ij q \u00b7 ( \u03b3 + q 3 )\u2223\u2223\u2223\u2223\u2223 \u2264 12 , \u2200i, j . (73)\nUnder this assumption, we have two cases. (?) Case ri \u2264 \u2211 j pij . By definition, we have in this case that yi = ti \u2264 0 in so\u2013trot (Step 10). We also have\nAi(P,y)\n= T (2)(yi)\n\u2212 \u2211 j pij \u221e\u2211 k=3\n[ 1\nk k\u22121\u220f l=1 (\u03b3 + q/l) ] yki ( p\u03b3ij q )k\u22121 \ufe38 \ufe37\ufe37 \ufe38\n. =S3\n. (74)\nSince yi = ti \u2264 0, S3 is an alternating series, that is a series whose general term is alternatively positive and negative. Under assumption (73), the module of its general term is decreasing. A classic result on series allows us to deduce from this fact that (a) S3 \u221e and (b) the sign of S3 is that of its first term, i.e., it is negative. Since Ai(P,y) = T (2)(yi)\u2212 S3, we have that\nAi(P,y) \u2265 T (2)(yi) = 0 . (75)\nNote also that Ai(P,y) = 0 iff \u2211 j pij = ri as T (2)(yi) is decreasing on [ti, 0] and T\n(2)(0) = 0. Hence, for the choice in Step 10, Ai(P,y) is an auxiliary function for variable i. (?) Case \u2211\nj pij \u2264 ri: we still have Ai(P,y) = T (2)(yi) \u2212 S3, but this time yi will be positive, ensuring yi(ri \u2212 \u2211 j pij) \u2265 0. We first show that S3 is upperbounded by a geometric series under assumption (73):\nS3\n= \u2211 j pijy 3 i ( p\u03b3ij q )2 \u221e\u2211 k=0 yki k + 3 [ k+2\u220f l=1 (\u03b3 + q/l) ]( p\u03b3ij q )k \u2264 \u2211 j pij(1\u2212 q/2) y3i 3 ( p\u03b3ij q )2 \u221e\u2211 k=0 ( yip \u03b3 ij q (\u03b3 + q/3) )k = \u2211 j pij(1\u2212 q/2) y3i 3 ( p\u03b3ij q )2 \u00d7 1 1\u2212 yip \u03b3 ij\nq (\u03b3 + q/3)\n\u2264 (2\u2212 q) \u2211 j pij y3i 3 ( p\u03b3ij q )2 ,\nwhich conveniently yields\nAi(P,y) \u2265 T (2)(yi)\u2212 (2\u2212 q) \u2211 j pij y3i 3 ( p\u03b3ij q )2 . (76)\nThe derivative of the right-hand term of (76) is Ri defined in eq. (72) above. Let us define:\na . = (2\u2212 q) \u2211 j p1+2\u03b3ij q2 , (77)\nb . = \u2211 j p1+\u03b3ij q , (78) c .\n= \u2212(ri \u2212 \u2211 j pij) . (79)\nWe have ac < 0 and consequently the discriminant \u2206 .\n= b2 \u2212 4ac > b2, implying Ri has a positive root zi . = (\u2212b+ \u221a \u2206)/(2a) which maximises the right-hand term of 76, and is such that this right-\nhand term is positive. Further, we again have that zi = 0 iff \u2211\nj pij = ri. It is easy to check that zi = yi in Step 8 of so\u2013trot, for which we check that Ai(P,y) \u2265 0, wich equality iff \u2211 j pij = ri. Hence, for the choice in Step 8, Ai(P,y) is an auxiliary function for variable i.\nWe can now conclude that under assumption (73), A(P,y) is an auxiliary function.\nIf assumption (73) does not hold, then notice that this cannot not hold at convergence for coordinate i. For this reason, ri 6= \u2211 j pij and the sign sign(ri\u2212 \u2211 j pij) is also well defined. Therefore, we just need to pick a value for yi 6= 0 which guarantees Ai(P,y) > 0. To do so, we pick\nyi = q \u00b7 sign(ri \u2212\n\u2211 j pij)\n(6\u2212 4q) \u00b7maxj p1\u2212qij , (80)\nremarking that this yi indeed violates (73) (recalling \u03b3 . = 1\u2212 q). We also have |yi| \u2208 (0, n2(1\u2212q)/2]. Notice that this choice guarantees Ai(P,y) > 0. (end of the proof of Theorem 18)\nTheorems 15 and 18 altogether prove Theorem 7.\nSupplementary Material: experiments"}, {"heading": "12 Per county error distribution, trot survey vs Florida average", "text": "Figure 5 displays the empirical distribution of the errors for trot vs Florida average. While not being a true distribution of the solution error of trot \u2014 in a Bayesian sense \u2014, the graph should convey the intuition that algorithms with a distribution that shrinks around zero provide better inference.\n13 Per county errors, trot survey vs trot 11>\nFigure 6 confronts the prediction errors by county of trot when we use M = M sur (survey) and M = Mno(= 11>) as cost matrix: while the overall performance of the two algorithms is very close, the graph demonstrates that trot optimized with M sur achieves very often smaller error, although the average error is worsen by few particularly bad counties."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>Optimal transport is a powerful framework for computing distances between probability dis-<lb>tributions. We unify the two main approaches to optimal transport, namely Monge-Kantorovitch<lb>and Sinkhorn-Cuturi, into what we define as Tsallis regularized optimal transport (trot).<lb>trot interpolates a rich family of distortions from Wasserstein to Kullback-Leibler, encompass-<lb>ing as well Pearson, Neyman and Hellinger divergences, to name a few. We show that metric<lb>properties known for Sinkhorn-Cuturi generalize to trot, and provide efficient algorithms for<lb>finding the optimal transportation plan with formal convergence proofs. We also present the<lb>first application of optimal transport to the problem of ecological inference, that is, the recon-<lb>struction of joint distributions from their marginals, a problem of large interest in the social<lb>sciences. trot provides a convenient framework for ecological inference by allowing to compute<lb>the joint distribution \u2014 that is, the optimal transportation plan itself \u2014 when side information<lb>is available, which is e.g. typically what census represents in political science. Experiments<lb>on data from the 2012 US presidential elections display the potential of trot in delivering a<lb>faithful reconstruction of the joint distribution of ethnic groups and voter preferences.", "creator": "TeX"}}}