{"id": "1609.05234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning", "abstract": "user - machine interaction is important for spoken sung content retrieval. for text content retrieval, the user can easily scan through and select on a list of his retrieved item. this is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. one way to counter such difficulties is through user - machine interaction. the machine can take different actions prior to interact with the user to obtain better retrieval results before then showing to the user. the suitable impulse actions depend on the retrieval status, for example requesting for extra information from the individual user, returning a list of topics for user to select, etc. in our previous work, some hand - crafted states estimated behavior from the present retrieval results are used to determine the proper actions. in this paper, we propose solutions to use deep - q - learning techniques instead to determine the machine actions for your interactive spoken content retrieval. so deep - q - learning bypasses the need for estimation of the hand - crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. it is shown to achieve significantly better performance compared with the previous hand - crafted states.", "histories": [["v1", "Fri, 16 Sep 2016 20:56:22 GMT  (205kb,D)", "http://arxiv.org/abs/1609.05234v1", "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\""]], "COMMENTS": "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\"", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yen-chen wu", "tzu-hsiang lin", "yang-de chen", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1609.05234"}, "pdf": {"name": "1609.05234.pdf", "metadata": {"source": "CRF", "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning", "authors": ["Yen-Chen Wu", "Tzu-Hsiang Lin", "Yang-De Chen", "Hung-Yi Lee", "Lin-Shan Lee"], "emails": ["tlkagkb93901106}@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "Interactive Information Retrieval (IIR)[1, 2] enhances a retrieval system by incorporating the user-system interaction into the retrieval process. The eventual goal of IIR is to guide the users to smoothly find out the desired information through the interactive process[3, 4]. Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search. Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.\nThere are several reasons which make user-system interaction important for spoken content retrieval. Firstly, speech recognition is still far from being perfect, and it inevitably produces errors which make the retrieval results uncontrollable. Moreover, the subword-based technologies are widely used in spoken content retrieval to deal with the OOV issue. These techniques result in higher recall rates, but also lead to lower precision rates. So very often many retrieved results are completely wrong. Also, it\u2019s difficult to show the retrieved multimedia or spoken information items on the screen, and it\u2019s hard for the users to scan through the retrieved results on the screen[12]. IIR is effective in updating the user instructions to boost the retrieval performance.\nIn several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR. An earlier attempt is to let the user select among a list of retrieved key terms. However, this approach turned out to be inefficient for users. A different approach was then proposed, in which the system has more actions to choose from and decides the most suitable action based on the present status including the present retrieved results, the number of interaction turns, and so on. Some successful results have been achieved by first estimating some human-defined indicators from a set of features as the states and then selecting the actions based on the estimated states [14, 15]. However, such hand-crafted states may be inadequate for the purpose. And in those approaches, the state estimation and action selection are modeled as two cascading blocks and trained independently. Without jointly estimating the whole process, both of them can be sub-optimal.\nIn this paper, we propose to use deep reinforcement learning in IIR for spoken content. Deep reinforcement learning has recently achieved great renown and success, and deep-QNetwork (DQN) serves as a capable solution to learn from very raw inputs [16, 17]. In IIR with this approach, DQN can take the features originally used for state estimation as the input, and decide the actions directly. Two major contributions were made in this paper. First, we show that the hand-crafted states used previously, like average precision, cannot represent the retrieval status very well, and cascading the estimation of such states with action selection based on such states is definitely not optimal. This is why the DQN proposed here, which is an end-to-end approach, achieved remarkable improvements. Secondly, through utilizing DQN, we show that even using the raw feature\u2013the retrieval scores from the search engine\u2013as the DQN input outperformed the previous approach with hand-crafted states."}, {"heading": "2. Proposed Approach", "text": "The framework for the proposed approach is depicted in Fig. 1. At the left hand side, the user first enters a query q into the system. With the user query q or plus other feedback information from the user during the interaction, the retrieval module (in the upper middle) described in Section 2.1 will generate a list of retrieved result.\nThe system takes actions based on the retrieved results to interact with the user as discussed in Section 2.2. A set of features extracted from the retrieved results (at the upper right corner), and the dialogue manager (in the lower middle) determining the action based on the extracted feature is introduced in Section 2.3 and 2.4 respectively. In the dialogue manager, there are paths (A) and (B) for determine the actions. Path (A) (in Section 2.4.1) is the previous approach with estimated states, while path (B) (in Section 2.4.2) uses the deep reinforcement learning proposed in this paper.\nar X\niv :1\n60 9.\n05 23\n4v 1\n[ cs\n.C L\n] 1\n6 Se\np 20\n16"}, {"heading": "2.1. Retrieval Module", "text": "2.1.1. Language Modeling Retrieval Module\nThe basic idea of a language model based retrieval framework is to represent the query q and a document d both as language models \u03b8q and \u03b8d. More details about estimating \u03b8d for spoken documents are left out here [18, 19]. The relevance score S(q, d) for the given query q and a document d, which is used to rank the documents d during the retrieval process, is evaluated based on the KL divergence between \u03b8q and \u03b8d, or S(q, d) = \u2212KL(\u03b8q\u2016\u03b8d). Furthermore, the user may designate a set of terms to be irrelevant to what he/she is seeking for, which can be modeled as a negative information model \u03b8N . Thus, the complete relevance score S(q, d) considers both the query model \u03b8q and the negative information model \u03b8N as below.\nS(q, d) = \u2212[KL(\u03b8q\u2016\u03b8d)\u2212 \u03b2KL(\u03b8N\u2016\u03b8d)], (1) where \u03b2 as an adjustable parameter [19, 20].\n2.1.2. Query-Regularized Mixture Model for Query Expansion\nAfter receiving the feedback from the user during the iteration, the system generates a new query model \u03b8\u2032q . We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032q . However, in this query expansion process, the new query model may be tainted by unrelated information in the pseudo-relevant documents. We therefore regularize the query models using a key term set, which is initially composed of the original query and can be expanded throughout the interactive session."}, {"heading": "2.2. Actions to be taken by the system", "text": "In order to help the user offer useful information, with which the system can retrieve documents better matched to the user\u2019s goal, five actions are defined for user-system interaction as presented below.\n(a) Return Documents: The dialogue manager returns the current list of retrieved results ranked by Sk(q, d) at the present time k in decreasing order and asks the user to select a relevant document.\n(b) Return Key Term: the dialogue manager asks the user whether a key term t\u2217 is relevant.\n(c) Return Request: the dialogue manager asks the user to provide an additional query term t\u0302.\n(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.\n(e) Show list: The dialogue manager shows the retrieved results ranked by Sk(q, d) to the user and ends the interactive session.\nWith actions (a),(b),(c),(d) the system receives extra information from the user and generates a new query q\u2032 accordingly for the next step retrieval. Action (e) ends the interactive session and shows the retrieved results to the user."}, {"heading": "2.3. Feature Extraction", "text": "Sets of features describing the characteristics or present status of the retrieved results from the search engine in Section 2.1 is extracted, based on which the proper actions are selected in Section 2.4. Two sets of features are tested here:\n\u2022 Human Knowledge Feature: A set of features handcrafted based on human knowledge is extracted. These features were used in the previous works [15]. Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].\n\u2022 Raw Relevance Scores: Considering the power of deep learning, the dialogue manager may be able to make decisions simply based on the raw relevance scores of retrieved items without any human knowledge. Here the relevance scores of the top-N documents in the retrieved results are taken as the features with N -dimensions."}, {"heading": "2.4. Dialogue Manager", "text": "The dialogue manager is based on Markov Decision Process (MDP). MDP [31] is defined as a tuple {S,A, T ,R, \u03b3}, where S is the set of states, A the set of actions, T (s\u2032|s, a) is the transition probability of ending up in state s\u2032 when executing action a in state s,R is the reward function, and \u03b3 the discount factor. A mapping from a state s \u2208 S to an action a \u2208 A, or action selection at each state, is a policy \u03c0. Given a policy \u03c0, the\nvalue of the Q-function (Q\u03c0 : S \u00d7A\u2192 R) is the estimation of the expected discounted sum of all rewards that can be received over an infinite state transition path starting from state s taking action \u03c0(s): Q(s, a) = E[ \u2211\u221e k=0 \u03b3\nkrk|s0 = s, a0 = a], where rk is the reward received from the action ak taken at state sk, and k is the sequence index for states and actions. The optimal policy maximizes the value of each state-action pair: \u03c0\u2217(s) = argmaxa\u2208AQ\n\u2217(s, a), so finding an optimal policy is equivalent to finding the optimal Q-function.\nThe reward of ak,the action taken at state sk, is defined as\nrk = \u2212Ck + \u03c4 [E(sk)\u2212 E(sk\u22121)]. (2) E(s) is some retrieval quantity metric at the state s and Ck is the estimated effort by the user to perform the action ak. \u03c4 is a trade-off parameter between user effort and the retrieval quality improvement.\n2.4.1. Previous Approach: Estimating the hand-crafted state\nThe previous approach [14, 15] for the dialogue management is path (A) in Figure 1. The underlying assumption behind this approach is that the proper choice of the action can be made by considering some evaluation metric for the retrieved results, which is the average precision (AP) here. This assumption leads to a two-stage process, shown as blocks A-1 and A-2, in the dialogue manager of Fig.1. Block A-1 is the state estimation. It takes the feature set extracted from the retrieved results in Section 2.3 as the input, and estimates the AP for them, taken as the state s in the Q-function for action selection. Block A-2 is for action decision. It uses fitted value iteration (FVI) [32] to train a Gaussian mixture model (GMM) to approximate the Qvalue function Q(s, a) for each action a, and the system takes the action a with the maximum Q(s, a).\nThis approach have several weaknesses. First, the evaluation metric AP is not necessary a good representation for states. AP simply indicates how well the retrieved results are. Empirical results have shown that even with the same AP, the optimal actions can be different. However, it\u2019s not easy to come up with better state definition with human knowledge. Next, it\u2019s not able to fix the error margin for relatively weak state estimation, because the state estimation (block A-1) and action selection (block A-2) are separately trained rather than considered jointly.\n2.4.2. Proposed Approach: Deep Reinforcement Learning\nThe proposed approach used Deep-Q-Network (DQN) to do deep reinforcement learning of Q-function. DQN is able to overcome the problems of the previous approach mentioned above at least to some degree. As can be seem in path (B) of Fig 1, the DQN directly generates the proper action from the input features through the hidden layers. In this way the error propagation for the two cascaded stages (block A-1 and block A-2 in path (A)) is eliminated, and the machine automatically learns from the features extracted from the retrieve module including the human knowledge features and the raw relevance scores of the retrieved results.\nThe DQN is a deep neural network (DNN) [33, 34] with parameters \u03b8 to estimate the state-action value function Q(s, a; \u03b8)1. The input of the DQN is the features extracted in Section 2.3, while its output dimension is the same as the number of possible actions a, and the output is the state-action value Q(s, a; \u03b8) for each action a in the action set A. The DQN is\n1Because the state-action value function Q(s, a) here depends on the deep neural network parameters \u03b8, the function should be written as Q(s, a; \u03b8).\ntrained by iteratively updating the parameters \u03b8. With parameters obtained at the i-th iteration, denoted as \u03b8i, \u03b8 can be learned by minimizing the following the loss function Li(\u03b8i) in (3) by gradient descent.\nLi(\u03b8 i) = E s,a,r,s\u2032\u223cU(D) [(y\u0302i \u2212Q(s, a; \u03b8i))2]. (3)\nD = {e1, e2, ..., et, ...eL} includes experiences et = (st, at, rt, st+1) (taking action at at state st obtaining reward rt and reaching state st+1 at the next time step) is a dataset collected from many retrieval episodes to be used for training. The expression s, a, r, s\u2032 \u223c U(D) in (3) means, instead of using the current experience as prescribed by the standard temporal-difference learning, the network is trained by sampling mini-batches of experiences from D uniformly at random. This method is referred to as experience replay, which is a key ingredient behind the success of DQN. In this way the efficiency in using the training data can be improve through reuse of the experience samples in multiple updates, and the correlation among the samples used in the update can be reduced through the uniform sampling from the replay buffer [35, 17].\ny\u0302i in (3) is defined as below:\ny\u0302i = r + \u03b3 \u00b7max a\u2032\u2208A\nQ(s\u2032, a\u2032; \u03b8\u2212) (4)\nwhere \u03b8\u2212 represents the parameters of a fixed and separate target network, which is taken here as the parameters obtained several iterations before. Freezing the parameters of the target networkQ(s\u2032, a\u2032; \u03b8\u2212) for a fixed number of iterations while updating the online network Q(s, a; \u03b8i) is another key innovation for the success of DQN [35, 17], which improved the stability of the algorithm."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Experiment Setting", "text": "We used a broadcast news corpus in Mandarin Chinese recorded from radio or TV stations in Taipei from 2001 to 2003 as the target document archive to be retrieved. There was a total of 5047 news documents with a total length of 198 hours. We used onebest transcriptions and lattices for the spoken archive. We used a tri-gram language model trained on 39M words of Yahoo news. 163 text queries and their relevant spoken documents (not necessarily including the query terms) were provided by 22 graduate students. We used DQN with two and four hidden layers of 1024 nodes and relu as the activation function [36]. The DQN framework is modified from an open-source code 2. Mean Average Precision (MAP) was selected as our retrieval evaluation metric. The costs of actions were set empirically considering the extra burden caused when the user provides feedback. And 10-fold cross validation was performed in all experiments.\nWe generated simulated users with the following behavior for training the dialogue manager.\n\u2022 (a) Return Documents: The simulated user viewed the list from the top and chooses the first relevant document.\n\u2022 (b) Return Key Term: Replied \u201dYES\u201d if the key term appeared in more than 50% of the relevant documents and \u201dNO\u201d otherwise.\n\u2022 (c) Return Request: Entered a key term based on Tf-idf (term frequency-inverse document frequency).\n\u2022 (d) Return Topic: Randomly returned one of the relevant topics manually labeled by graduate students.\n2https://github.com/spragunr/deep q rl\nFigure 2: Learning curves with either raw relevance scores alone (for top-100 retrieved items) (dotted curves) or with human knowledge features in addition (solid curves) with different DQN depths over lattice transcriptions.\nFigure 3: Learning curves of DQN with 2 layers using different sizes of relevance scores. N is the number of the top-N retrieved items which raw relevance scores were used."}, {"heading": "3.2. Result and Discussion", "text": "Table 1 shows the results in MAP and Return R = \u2211T k=0 rk achieved on spoken content retrieval module based on either one-best transcriptions (left half) or lattices (right half). \u2022 (a) Baselines: (a-1) are the first-pass results without any interaction, and their returns can be regarded as 0. (a-2) show the results for taking random actions, repeating 1000 times to estimate the MAP and expected return, and (a-3) are the results of the previous work with state estimation. \u2022 (b) DQN with different features: (b-1) are the results when only the raw relevance scores were used. And (b-2) combine the human knowledge features and the raw relevance scores in (b-1). \u2022 (c) Oracle: Obtained by brute-force search over every action sequence whose length was less than 5, and picked the action sequence with the highest return. It is considered as the upper bound for the finite interactive scenario.\nFrom (a-2), we notice that taking actions randomly is not a good way to improve the retrieval process: although the system gains extra information after every interaction, inefficient requests from the system may also burden the user and yield poor returns. The previous hand-crafted state approach in (a-3) obtained much better performance than random actions in (a-2) in terms of both MAP and return. Surprisingly, the proposed approach in (b-1) using DQN end-to-end reinforcement learning only with raw relevance scores without any human knowledge easily outperformed the results using plenty of human knowl-\nedge by estimating hand-crafted states in (a-3). This shows that the end-to-end neural network can properly estimate useful action selection indicators implicitly in its hidden layers directly from raw retrieval scores. Finally, using both the raw relevance scores and human knowledge features together in (b-2) achieved the best results.\nTo further explore DQN\u2019s ability on selecting actions, Figure 2 shows the learning curves of the final return for the proposed approach with different neural network depth using different sets of features: raw relevance scores for the top 100 retrieved items (dotted curves) and plus the human knowledge features (solid curves). Colors green, red, blue represents the curves using linear model (neural network without hidden layers), 2 and 4 layers, respectively. We observe that the linear model can be easily trained when the two sets of features are jointly used. But when solely using raw relevance scores, it diverged and didn\u2019t work (dotted-green). This shows that the mapping from raw relevance scores to proper actions is complicated and cannot be modeled without any hidden layers. The red and blue curves for 2 and 4 hidden layers showed that deeper model usually yielded better performance in most cases, especially when using raw relevance scores exclusively. But with a deeper model, it took more epochs to converge. This figure shows the results experimented on the lattice transcription, whereas the results on one-best transcription were similar.\nSince the results using raw relevance scores alone is comparable to those using both sets of features jointly with DQN, we conducted an extra experiment using raw relevance scores alone but with different sizes on the one-best transcription. Over DQN of 2 layers, Figure 3 shows the learning curves with the raw relevance scores alone, where N is the number of the retrieved items which relevance scores were used. We observe that DQN could learn from only 1 relevance score, though it gave a relatively poor performance, while all others (5,10,50,100) converged to some good return."}, {"heading": "4. Conclusion", "text": "Due to the high degree of uncertainty in speech and the difficulty of showing on screen to users, user-system interaction is highly desired for spoken content retrieval. In this paper, we utilize Deep-Q-network(DQN) to learn better state-action values without estimating the hand-crafted states which were used previously. This end-to-end learning is able offer overall optimization for user-system interaction and produce significant improvements on return. We further found that even with raw relevance scores alone without any human knowledge, we achieved very good performance. We hope the results seen here can light up the future directions of DQN in interactive spoken language systems."}, {"heading": "5. References", "text": "[1] D. Robins, \u201cInteractive information retrieval: Context and basic\nnotions,\u201d Informing Science, vol. 3, no. 2, pp. 57\u201362, 2000.\n[2] I. Ruthven, \u201cInteractive information retrieval,\u201d Annual review of information science and technology, vol. 42, no. 1, pp. 43\u201391, 2008.\n[3] J. Luo, S. Zhang, and H. Yang, \u201cWin-win search: Dual-agent stochastic game in session search,\u201d in Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 2014, pp. 587\u2013596.\n[4] X. Jin, M. Sloan, and J. Wang, \u201cInteractive exploratory search for multi page search results,\u201d in Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 655\u2013666.\n[5] T. Misu and T. Kawahara, \u201cBayes risk-based dialogue management for document retrieval system with speech interface,\u201d Speech Communication, vol. 52, no. 1, pp. 61\u201371, 2010.\n[6] \u2014\u2014, \u201cSpeech-based interactive information guidance system using question-answering technique,\u201d in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013145.\n[7] I. McGraw, S. Cyphers, P. Pasupat, J. Liu, and J. R. Glass, \u201cAutomating crowd-supervised learning for spoken language systems.\u201d in INTERSPEECH, 2012, pp. 2474\u20132477.\n[8] J. Liu, S. Cyphers, P. Pasupat, I. McGraw, and J. R. Glass, \u201cA conversational movie search system based on conditional random fields.\u201d in INTERSPEECH, 2012, pp. 2454\u20132457.\n[9] D. L. Patton, P. R. Ashe, and J. A. Manico, \u201cInteractive image storage, indexing and retrieval system,\u201d Jun. 18 2002, uS Patent 6,408,301.\n[10] S.-B. Cho, \u201cEmotional image and musical information retrieval with interactive genetic algorithm,\u201d Proceedings of the IEEE, vol. 92, no. 4, pp. 702\u2013711, 2004.\n[11] J. S. Garofolo, C. G. Auzanne, and E. M. Voorhees, \u201cThe trec spoken document retrieval track: A success story,\u201d in Content-Based Multimedia Information Access-Volume 1. LE CENTRE DE HAUTES ETUDES INTERNATIONALES D\u2019INFORMATIQUE DOCUMENTAIRE, 2000, pp. 1\u201320.\n[12] Y. Zhang and C. Zhai, \u201cInformation retrieval as card playing: A formal model for optimizing interactive retrieval interface,\u201d in Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 685\u2013694.\n[13] Y.-C. Pan, H.-Y. Lee, and L.-S. Lee, \u201cInteractive spoken document retrieval with suggested key terms ranked by a markov decision process,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 2, pp. 632\u2013645, 2012.\n[14] T.-H. Wen, H.-Y. Lee, and L.-S. Lee, \u201cInteractive spoken content retrieval with different types of actions optimized by a markov decision process.\u201d in INTERSPEECH, 2012, pp. 2458\u20132461.\n[15] T.-H. Wen, H.-y. Lee, P.-h. Su, and L.-S. Lee, \u201cInteractive spoken content retrieval by extended query model and continuous state space markov decision process,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8510\u20138514.\n[16] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., \u201cMastering the game of go with deep neural networks and tree search,\u201d Nature, vol. 529, no. 7587, pp. 484\u2013 489, 2016.\n[17] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., \u201cHuman-level control through deep reinforcement learning,\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.\n[18] J. Lafferty and C. Zhai, \u201cDocument language models, query models, and risk minimization for information retrieval,\u201d in Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2001, pp. 111\u2013119.\n[19] T. K. Chia, K. C. Sim, H. Li, and H. T. Ng, \u201cStatistical latticebased spoken document retrieval,\u201d ACM Transactions on Information Systems (TOIS), vol. 28, no. 1, p. 2, 2010.\n[20] H.-Y. Lee, T.-H. Wen, and L.-S. Lee, \u201cImproved semantic retrieval of spoken content by language models enhanced with acoustic similarity graph,\u201d in Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 182\u2013187.\n[21] C. Zhai and J. Lafferty, \u201cModel-based feedback in the language modeling approach to information retrieval,\u201d in Proceedings of the tenth international conference on Information and knowledge management. ACM, 2001, pp. 403\u2013410.\n[22] T. Tao and C. Zhai, \u201cRegularized estimation of mixture models for robust pseudo-relevance feedback,\u201d in Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2006, pp. 162\u2013169.\n[23] T. Hofmann, \u201cProbabilistic latent semantic indexing,\u201d in Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999, pp. 50\u201357.\n[24] D. M. Blei, A. Y. Ng, and M. I. Jordan, \u201cLatent dirichlet allocation,\u201d the Journal of machine Learning research, vol. 3, pp. 993\u2013 1022, 2003.\n[25] D. M. Blei and J. D. Lafferty, \u201cDynamic topic models,\u201d in Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 113\u2013120.\n[26] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth, \u201cThe author-topic model for authors and documents,\u201d in Proceedings of the 20th conference on Uncertainty in artificial intelligence. AUAI Press, 2004, pp. 487\u2013494.\n[27] B. He and I. Ounis, \u201cQuery performance prediction,\u201d Information Systems, vol. 31, no. 7, pp. 585\u2013594, 2006.\n[28] S. Cronen-Townsend, Y. Zhou, and W. B. Croft, \u201cPredicting query performance,\u201d in Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2002, pp. 299\u2013306.\n[29] Y. Zhao, F. Scholer, and Y. Tsegay, \u201cEffective pre-retrieval query performance prediction using similarity and variability evidence,\u201d in Advances in Information Retrieval. Springer, 2008, pp. 52\u201364.\n[30] Y. Zhou and W. B. Croft, \u201cQuery performance prediction in web search environments,\u201d in Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 543\u2013550.\n[31] R. Bellman, \u201cA markovian decision process,\u201d DTIC Document, Tech. Rep., 1957.\n[32] S. Chandramohan, M. Geist, and O. Pietquin, \u201cOptimizing spoken dialogue management from data corpora with fitted value iteration.\u201d\n[33] G. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[34] Y. Bengio, \u201cLearning deep architectures for ai,\u201d Foundations and trends\u00ae in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.\n[35] L.-J. Lin, \u201cReinforcement learning for robots using neural networks,\u201d DTIC Document, Tech. Rep., 1993.\n[36] A. L. Maas, A. Y. Hannun, and A. Y. Ng, \u201cRectifier nonlinearities improve neural network acoustic models,\u201d in Proc. ICML, vol. 30, 2013, p. 1."}], "references": [{"title": "Interactive information retrieval: Context and basic notions", "author": ["D. Robins"], "venue": "Informing Science, vol. 3, no. 2, pp. 57\u201362, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Interactive information retrieval", "author": ["I. Ruthven"], "venue": "Annual review of information science and technology, vol. 42, no. 1, pp. 43\u201391, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Win-win search: Dual-agent stochastic game in session search", "author": ["J. Luo", "S. Zhang", "H. Yang"], "venue": "Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 2014, pp. 587\u2013596.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Interactive exploratory search for multi page search results", "author": ["X. Jin", "M. Sloan", "J. Wang"], "venue": "Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 655\u2013666.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayes risk-based dialogue management for document retrieval system with speech interface", "author": ["T. Misu", "T. Kawahara"], "venue": "Speech Communication, vol. 52, no. 1, pp. 61\u201371, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech-based interactive information guidance system using question-answering technique", "author": ["\u2014\u2014"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013145.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Automating crowd-supervised learning for spoken language systems.", "author": ["I. McGraw", "S. Cyphers", "P. Pasupat", "J. Liu", "J.R. Glass"], "venue": "INTERSPEECH,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A conversational movie search system based on conditional random fields.", "author": ["J. Liu", "S. Cyphers", "P. Pasupat", "I. McGraw", "J.R. Glass"], "venue": "INTERSPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Interactive image storage, indexing and retrieval system", "author": ["D.L. Patton", "P.R. Ashe", "J.A. Manico"], "venue": "Jun. 18 2002, uS Patent 6,408,301.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Emotional image and musical information retrieval with interactive genetic algorithm", "author": ["S.-B. Cho"], "venue": "Proceedings of the IEEE, vol. 92, no. 4, pp. 702\u2013711, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "The trec spoken document retrieval track: A success story", "author": ["J.S. Garofolo", "C.G. Auzanne", "E.M. Voorhees"], "venue": "Content-Based Multimedia Information Access-Volume 1. LE CENTRE DE HAUTES ETUDES INTERNATIONALES D\u2019INFORMATIQUE DOCUMENTAIRE, 2000, pp. 1\u201320.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Information retrieval as card playing: A formal model for optimizing interactive retrieval interface", "author": ["Y. Zhang", "C. Zhai"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 685\u2013694.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive spoken document retrieval with suggested key terms ranked by a markov decision process", "author": ["Y.-C. Pan", "H.-Y. Lee", "L.-S. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 2, pp. 632\u2013645, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Interactive spoken content retrieval with different types of actions optimized by a markov decision process.", "author": ["T.-H. Wen", "H.-Y. Lee", "L.-S. Lee"], "venue": "INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Interactive spoken content retrieval by extended query model and continuous state space markov decision process", "author": ["T.-H. Wen", "H.-y. Lee", "P.-h. Su", "L.-S. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8510\u20138514.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013 489, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Document language models, query models, and risk minimization for information retrieval", "author": ["J. Lafferty", "C. Zhai"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2001, pp. 111\u2013119.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical latticebased spoken document retrieval", "author": ["T.K. Chia", "K.C. Sim", "H. Li", "H.T. Ng"], "venue": "ACM Transactions on Information Systems (TOIS), vol. 28, no. 1, p. 2, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved semantic retrieval of spoken content by language models enhanced with acoustic similarity graph", "author": ["H.-Y. Lee", "T.-H. Wen", "L.-S. Lee"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 182\u2013187.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Model-based feedback in the language modeling approach to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proceedings of the tenth international conference on Information and knowledge management. ACM, 2001, pp. 403\u2013410.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Regularized estimation of mixture models for robust pseudo-relevance feedback", "author": ["T. Tao", "C. Zhai"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2006, pp. 162\u2013169.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999, pp. 50\u201357.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u2013 1022, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 113\u2013120.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence. AUAI Press, 2004, pp. 487\u2013494.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Query performance prediction", "author": ["B. He", "I. Ounis"], "venue": "Information Systems, vol. 31, no. 7, pp. 585\u2013594, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Predicting query performance", "author": ["S. Cronen-Townsend", "Y. Zhou", "W.B. Croft"], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2002, pp. 299\u2013306.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Effective pre-retrieval query performance prediction using similarity and variability evidence", "author": ["Y. Zhao", "F. Scholer", "Y. Tsegay"], "venue": "Advances in Information Retrieval. Springer, 2008, pp. 52\u201364.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Query performance prediction in web search environments", "author": ["Y. Zhou", "W.B. Croft"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 543\u2013550.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "A markovian decision process", "author": ["R. Bellman"], "venue": "DTIC Document, Tech. Rep., 1957.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1957}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends\u00ae in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.-J. Lin"], "venue": "DTIC Document, Tech. Rep., 1993.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1993}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, 2013, p. 1.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Interactive Information Retrieval (IIR)[1, 2] enhances a retrieval system by incorporating the user-system interaction into the retrieval process.", "startOffset": 39, "endOffset": 45}, {"referenceID": 1, "context": "Interactive Information Retrieval (IIR)[1, 2] enhances a retrieval system by incorporating the user-system interaction into the retrieval process.", "startOffset": 39, "endOffset": 45}, {"referenceID": 2, "context": "The eventual goal of IIR is to guide the users to smoothly find out the desired information through the interactive process[3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 3, "context": "The eventual goal of IIR is to guide the users to smoothly find out the desired information through the interactive process[3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 4, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 58, "endOffset": 64}, {"referenceID": 5, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 58, "endOffset": 64}, {"referenceID": 6, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 86, "endOffset": 92}, {"referenceID": 7, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 86, "endOffset": 92}, {"referenceID": 8, "context": "Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.", "startOffset": 52, "endOffset": 59}, {"referenceID": 9, "context": "Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.", "startOffset": 52, "endOffset": 59}, {"referenceID": 10, "context": "Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Also, it\u2019s difficult to show the retrieved multimedia or spoken information items on the screen, and it\u2019s hard for the users to scan through the retrieved results on the screen[12].", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "In several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR.", "startOffset": 26, "endOffset": 38}, {"referenceID": 13, "context": "In several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR.", "startOffset": 26, "endOffset": 38}, {"referenceID": 14, "context": "In several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR.", "startOffset": 26, "endOffset": 38}, {"referenceID": 13, "context": "Some successful results have been achieved by first estimating some human-defined indicators from a set of features as the states and then selecting the actions based on the estimated states [14, 15].", "startOffset": 191, "endOffset": 199}, {"referenceID": 14, "context": "Some successful results have been achieved by first estimating some human-defined indicators from a set of features as the states and then selecting the actions based on the estimated states [14, 15].", "startOffset": 191, "endOffset": 199}, {"referenceID": 15, "context": "Deep reinforcement learning has recently achieved great renown and success, and deep-QNetwork (DQN) serves as a capable solution to learn from very raw inputs [16, 17].", "startOffset": 159, "endOffset": 167}, {"referenceID": 16, "context": "Deep reinforcement learning has recently achieved great renown and success, and deep-QNetwork (DQN) serves as a capable solution to learn from very raw inputs [16, 17].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "More details about estimating \u03b8d for spoken documents are left out here [18, 19].", "startOffset": 72, "endOffset": 80}, {"referenceID": 18, "context": "More details about estimating \u03b8d for spoken documents are left out here [18, 19].", "startOffset": 72, "endOffset": 80}, {"referenceID": 18, "context": "where \u03b2 as an adjustable parameter [19, 20].", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "where \u03b2 as an adjustable parameter [19, 20].", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032 q .", "startOffset": 45, "endOffset": 57}, {"referenceID": 20, "context": "We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032 q .", "startOffset": 45, "endOffset": 57}, {"referenceID": 21, "context": "We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032 q .", "startOffset": 45, "endOffset": 57}, {"referenceID": 22, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 23, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 24, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 25, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 14, "context": "These features were used in the previous works [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 137, "endOffset": 141}, {"referenceID": 28, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 191, "endOffset": 195}, {"referenceID": 29, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 229, "endOffset": 233}, {"referenceID": 29, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 253, "endOffset": 257}, {"referenceID": 30, "context": "MDP [31] is defined as a tuple {S,A, T ,R, \u03b3}, where S is the set of states, A the set of actions, T (s\u2032|s, a) is the transition probability of ending up in state s\u2032 when executing action a in state s,R is the reward function, and \u03b3 the discount factor.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "The previous approach [14, 15] for the dialogue management is path (A) in Figure 1.", "startOffset": 22, "endOffset": 30}, {"referenceID": 14, "context": "The previous approach [14, 15] for the dialogue management is path (A) in Figure 1.", "startOffset": 22, "endOffset": 30}, {"referenceID": 31, "context": "The DQN is a deep neural network (DNN) [33, 34] with parameters \u03b8 to estimate the state-action value function Q(s, a; \u03b8).", "startOffset": 39, "endOffset": 47}, {"referenceID": 32, "context": "The DQN is a deep neural network (DNN) [33, 34] with parameters \u03b8 to estimate the state-action value function Q(s, a; \u03b8).", "startOffset": 39, "endOffset": 47}, {"referenceID": 33, "context": "In this way the efficiency in using the training data can be improve through reuse of the experience samples in multiple updates, and the correlation among the samples used in the update can be reduced through the uniform sampling from the replay buffer [35, 17].", "startOffset": 254, "endOffset": 262}, {"referenceID": 16, "context": "In this way the efficiency in using the training data can be improve through reuse of the experience samples in multiple updates, and the correlation among the samples used in the update can be reduced through the uniform sampling from the replay buffer [35, 17].", "startOffset": 254, "endOffset": 262}, {"referenceID": 33, "context": "Freezing the parameters of the target networkQ(s\u2032, a\u2032; \u03b8\u2212) for a fixed number of iterations while updating the online network Q(s, a; \u03b8) is another key innovation for the success of DQN [35, 17], which improved the stability of the algorithm.", "startOffset": 186, "endOffset": 194}, {"referenceID": 16, "context": "Freezing the parameters of the target networkQ(s\u2032, a\u2032; \u03b8\u2212) for a fixed number of iterations while updating the online network Q(s, a; \u03b8) is another key innovation for the success of DQN [35, 17], which improved the stability of the algorithm.", "startOffset": 186, "endOffset": 194}, {"referenceID": 34, "context": "We used DQN with two and four hidden layers of 1024 nodes and relu as the activation function [36].", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states.", "creator": "LaTeX with hyperref package"}}}