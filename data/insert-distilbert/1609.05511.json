{"id": "1609.05511", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "Multilinear Grammar: Ranks and Interpretations", "abstract": "multilinear grammar ( mlg ) is an approach suited to integrating the many different syntagmatic sentiment structures, of language into a coherent architecture, the rank - interpretation architecture. the architecture defines ranks from discourse structure through utterances, phrasal term structures, word structures to speech sounds. each rank has its own specific kind of prosodic - phonetic spatial interpretation notation and semantic - pragmatic interpretation. common to models of hierarchical all these distinctive subdomains hierarchy are models based on regular languages, and processors with partial finite working memory.", "histories": [["v1", "Sun, 18 Sep 2016 16:29:20 GMT  (1684kb)", "http://arxiv.org/abs/1609.05511v1", "41 pages"], ["v2", "Sun, 9 Oct 2016 14:07:26 GMT  (1554kb)", "http://arxiv.org/abs/1609.05511v2", "76 pages (double spaced), approx 42 standard print pages"], ["v3", "Tue, 11 Oct 2016 13:14:19 GMT  (1536kb)", "http://arxiv.org/abs/1609.05511v3", "34 pages"], ["v4", "Mon, 10 Jul 2017 23:00:53 GMT  (1975kb)", "http://arxiv.org/abs/1609.05511v4", "45 pages, 10 figures. In press, journal Open Linguistics (de Gruyter Open)"], ["v5", "Sun, 27 Aug 2017 21:29:55 GMT  (1975kb)", "http://arxiv.org/abs/1609.05511v5", "45 pages, 10 figures. In press, journal Open Linguistics (de Gruyter Open), proofread and corrected version"]], "COMMENTS": "41 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dafydd gibbon", "sascha griffiths"], "accepted": false, "id": "1609.05511"}, "pdf": {"name": "1609.05511.pdf", "metadata": {"source": "CRF", "title": "Multilinear Grammar: Ranks and Interpretations", "authors": ["Dafydd Gibbon", "Sascha Griffiths"], "emails": [], "sections": [{"heading": null, "text": "V01, 2016-09-18\nMultilinear Grammar: Ranks and Interpretations\nDafydd Gibbon and Sascha Griffiths\nContents 1 Ranks, processes and flat grammar ....................................................................................... 2 2 The Rank-Interpretation Architecture for Multilinear Grammars ......................................... 6 2.1 Outline of the Rank-Interpretation Architecture framework .............................................. 6 2.2 A preliminary note on linearity and hierarchy at the phrasal rank ..................................... 7 2.3 Characterisation of the Rank-Interpretation Architecture .................................................. 9 2.3.1 Background ..................................................................................................................... 9 2.3.2 Formal summary ........................................................................................................... 10 2.3.3 Contrast with traditional views of language architecture .............................................. 11 2.3.4 Search for sui generis properties of ranks ..................................................................... 11 2.4 Procedural perspectives on the rank hierarchy ................................................................. 12 3 The discourse rank .............................................................................................................. 15 3.1 The primacy of discourse patterning ................................................................................ 15 3.2 Intonation of an adjacency pair ........................................................................................ 16 3.3 Chanted \u2018call\u2019 intonation .................................................................................................. 18 4 The utterance or text rank ................................................................................................... 20 5 The phrase rank ................................................................................................................... 22 5.1 Characteristics of phrasal structure .................................................................................. 22 5.2 Linear sequences, iteration: regular and subregular grammars ........................................ 23 5.3 A note on long-distance and cross-serial dependencies ................................................... 26 5.4 Prosodic-phonetic interpretation at the phrasal rank ........................................................ 27 6 The word rank ..................................................................................................................... 30 6.1 Flat words ......................................................................................................................... 30 6.2 Flat derivations ................................................................................................................. 30 6.3 Flat compounds ................................................................................................................ 31 6.4 Prosodic-phonetic interpretation at the word rank ........................................................... 31 7 Summary and conclusion .................................................................................................... 33 7.1 From Duality to Multilinear Grammar and Rank Interpretation Architecture ................. 33 7.2 Generalisation to stochastic flat linear models ................................................................. 34 7.3 Future work ...................................................................................................................... 35 8 References ........................................................................................................................... 36\nGibbon & Griffiths 2016-09-18 1/41 Multilinear Grammar V01"}, {"heading": "1 Ranks, processes and flat grammar", "text": "Language models in both linguistics and the human language technologies tend to concentrate on single components, such as a syntactic language model, a morphological language model, a discourse model, or a model for argumentation in texts. The result of this specialisation is that language modelling as a whole turns out to be a hybrid system with interfaces between component subsystems. It is not impossible that this concept is an adequate model of human language abilities, or a model of language acquisition or evolution, subsystem by subsystem. But in terms of Occam\u2019s Razor, the economy of descriptive and explanatory means, it is a wasteful strategy. There are recent \u2013 or revived \u2013 developments in language modelling which promise a more integrated formally and semiotically motivated approach. It is in this context that we address the notion of combinatorial communication (Scott-Phillips and Blythe 2013), without claiming to model human language abilities directly.\nFirst, the structures of language, particularly speech or spoken language, are increasingly grounded in an empirically motivated \u2018procedural turn\u2019 (as developed in Christensen and Chater 2016 and List et al. 2016), which thereby fills the traditional linguistic perspective on the primacy of speech data (e.g. over written data) with new life. The notion of \u2018procedure\u2019 in this context means, essentially, \u2018algorithm\u2019 or \u2018computation\u2019 and is not to be confused with \u2018performance\u2019 as used in the Chomskyan sense to refer to the vagaries of actual language use. Procedures can be (but do not have to be) just as abstract and selective as generative and constraint-based competence theories.\nSecond, attention is turning beyond the traditional linguistic focus on the word, its internal structure (\u2018morphology\u2019 and \u2018phonology\u2019) and external distribution in the sentence (\u2018syntax\u2019), and its semantic-pragmatic interpretations. Different sized ranks of language structures and processing requirements for discourse and text, together with semiotic properties such as prosodic-phonetic and semantic-pragmatic interpretations, have in general been seen in linguistics and the language technologies to be somehow different from the ranks of sentence, word, morpheme and phoneme. The principled incorporation of the potentially \u2018larger\u2019 domains of language structure, which require longer time windows for prosody (Tillmann and Mansell 1980), for semantic-pragmatic and prosodic-phonetic interpretation (Gibbon 1992; Carson-Berndsen 1998), and for discourse (Christensen and Chater 2016), is coming more into focus, and figures prominently in the present study.\nWe explore this concept of structural and processing domains, expanding it into a ternary semiotically motivated multidimensional Rank-Interpretation Architecture, in which each rank has its own specific prosodic-phonetic and semantic-pragmatic interpretations. The multidimensionality of speech processing pertains first to the ranks in the rank hierarchy, second to the phonetic and semantic-pragmatic interpretation of each rank, and third to the parallel processing of simultaneous streams of speech input and output, as modelled in more linguistic terms by Firthian polysystemic (Firth 1948, 1957; Palmer 1968, 1969), autosegmental (Goldsmith 1976, 1990) and articulatory (Browman and Goldstein 1989) prosodic phonologies, as well as in many studies of conversational gesture (cf. McNeill 2000; Rossini 2012). The multidimensionality principle also applies to intonation in discourse contexts (cf. Couper-Kuhlen 2015; Selting 1995) as metalocutionary channels of information which are parallel to and partly synchronised with lexico-syntactic locutions (Gibbon 1976).\nGibbon & Griffiths 2016-09-18 2/41 Multilinear Grammar V01\nA third recent development is a move towards \u2018flat grammar\u2019, in the default case regular grammars (equivalently: finite state automata). The flat grammar paradigm follows Occam\u2019s Razor in two main ways. First, powerful formalisms with unbounded working memory requirements are being abandoned in favour of more empirically motivated devices such as regular (Type 1)1 grammars and finite state automata with finite working memory requirements and specific finite data structure types, or at least constraints on unboundedness (Frazier and Fodor 1978; Church 2007; Karlsson 2010). Second, this is happening not only for morphology, phonology and prosody, for which such models are well established, but also for phrasal grammar. Formalisms such as context-free (phrase structure, Type 2) and contextsensitive (Type 1) grammars in principle require unbounded working memory, which is an empirically implausible assumption.\nThe properties of flat linear grammars have been well investigated in terms of their algebraic and parsing and generation properties as well as their learnability (Berwick and Pilato 1987). The status of flat grammars has figured in discussions of types of recursion in phrasal syntax during recent years, for example in the context of the controversial claim (Hauser et al. 2002; Fitch et al. 2006) that recursive hierarchy formation (\u2018merge\u2019) is the defining property of human languages (extensively discussed in contributions to van der Hulst 2010 and to Roeper and Speas 2015). Many issues are still open and some of these are addressed in the present study, for example the consequences of linearity in both categorial (phonological) and physical aspects of prosodic-phonetic interpretation at discourse and utterance ranks. Evidently, there are a hierarchical and cross-referencing syntagmatic relations in speech and language which go beyond parallel flat linear patterns and appear prima facie to need more complex mechanisms. However, a formally well-defined default starting point based on linearity and the claim that nonlinear structure comes at a cost, is at least a useful and potentially falsifiable heuristic benchmark, perhaps even a necessary theoretical core (cf. also Wittenberg and Jackendoff 2014; Jackendoff and Wittenberg 2016), in discussions of the complexity, ontogeny and phylogeny of language.\nThe issues of flat linearity and recursivity in language were reviewed comprehensively by Karlsson (2010), who comprehensively catalogues types of iteration and recursion in phrasal syntax and on the basis of extensive corpus studies empirically establishes systematic small finite bounds for recursion in written language and even smaller finite bounds for speech. These bounds in turn motivate the use of flat grammars with finite bounds on working memory. Formally, in order to accommodate the patterns for which apparently more memoryhungry systems seem to be needed but which are nevertheless bounded, flat linear devices can be enhanced with operations over finite registers, thus remaining technically within the domain of regular languages. These configurations can if necessary be compiled out into regular grammars and finite state automata (in some cases at the expense of exponentially complex operations).\nMorphology, phonology and prosody have been known to be adequately modelled by flat linear devices since Johnson (1972), Pierrehumbert (1980), Gibbon (1981a, 1987), Koskenniemi (1984), Kaplan et al. (1994). Later studies have confirmed this and extended these flat linear models. In morphology (e.g. compounding) and phrasal syntax, hierarchies and cross-hierarchical patterns (for cross-serial and long-distance dependencies) appear to be\n1 We refer here to the Chomsky-Sch\u00fctzenberger (1963) implication hierarchy of formal languages and formal grammars, from Type 3 regular grammars (modelling right and left branching structures and flat sequences) through Type 2 context-free grammars (for modelling tree structures) and context-sensitive grammars (relevant for modelling tree structures with cross-references); unrestricted Type 0 transformational grammars are known to be unnecessarily powerful for natural language grammar models.\nGibbon & Griffiths 2016-09-18 3/41 Multilinear Grammar V01\nnecessary for semantic disambiguation and compositional semantic interpretation (and for some aspects of prosody such as \u2018metrical\u2019 stress hierarchies) but again, are strongly bounded in practice. Therefore we argue that if distributional syntactic properties are separated from semantic constraints then the only context where syntax necessarily requires hierarchical structuring is for centre-embedding recursion, since it is well-established that right and left branching recursions can be reduced to \u2018flat\u2019 linear sequences (Hopcroft et al. 2007); we surmise that their semantic-pragmatic interpretations and prosodic-phonetic interpretations can also be effected incrementally.\nIn addition to Occam\u2019s Razor and arguments for empirical grounding, a number of other formal and informal arguments for multi-rank parallel processing and for structural linearity have been put forward. One key argument is the traditional Saussurean postulate of the primacy of spoken language versus writing (to which the primacy of gesture versus spoken language may be added, Rossini 2012): spoken utterance events are time functions which require fast and near-deterministic processing of many parallel streams of events (Browman and Goldstein 1989; Christensen and Chater 2016). Spoken utterance events are ontologically quite different from the inscribed traces which are characteristic of normalised written languages, which determine slower production and perception processes, and which are enhanced with additional \u2018paper and screen memory\u2019. We note that written or \u2018writable\u2019 data types, often imagined, i.e. introspected rather than empirically observed. data types, inform many descriptive and theoretical linguistic studies.\nA second argument with regard to linearity concerns the well-known properties of the right-branching grammars which figure prominently in linguistic description. Any purely right-branching grammar covers a regular language Lreg, and for any purely right-branching language there is (a) a left-branching grammar and (b) a finite state automaton which also covers the language Lreg. This is also true, mutatis mutandis, for left-branching grammars. These results are well-known in computational contexts, but in linguistic discussions they tend to be sidelined by Chomsky\u2019s (1957) theorem English is not a finite-state language.\nWe take Chomsky\u2019s theorem to be true (allowing for finiteness of working memory) for written language and rehearsed or stereotypic speech, because these registers have at their disposal additional memory resources and more leisurely time constraints which may support centre-embedding constructions of limited depth. But we do not accept Chomsky\u2019s theorem for non-stereotypic spontaneous speech, which is produced and understood on the spur of the moment and is restricted to working memory with a rather small finite bound.\nA third argument for structural linearity derives from older pyscholinguistic studies of the lexicon, in which decision-tree-like lexical access patterns defined cohorts of lexical items to be disambiguated incrementally as input continues (Marslen-Wilson 1987). The cohort approach has been replaced by different formal paradigms such as quantitative artificial neural network classifiers but all these approaches still capture a key insight of \u2018flatness\u2019. Controlled studies on the use of flat linear n-gram contexts (essentially finite state automata) in phoneme, morpheme and word segmentation continue the flat linear system modelling strategy (Church 2007; Griffiths et al. 2015).\nA fourth, more circumstantial argument for linearity is based on \u2018external evidence\u2019: the success on theoretical and operational grounds of stochastic flat grammars in speech technology and of flat approximations to more complex grammars (Pereira and Wright 1997) in Natural Language Processing and in Machine Translation, often exploiting complementary properties and similarities between artificial neural networks and stochastic regular grammars\nGibbon & Griffiths 2016-09-18 4/41 Multilinear Grammar V01\nof the Hidden Markov Model type (Jurafsky et al. 2009:233ff., 266ff.; Huang et al. 2001:377ff., 457ff.).\nIn discussing combinatorial communication (Scott-Phillips and Blythe 2013), it makes sense to combine the three dimensions: (1) ranked structures of different size and type; (2) flat grammar; (3) parallel processing. This is what motivates our concept of a Multilinear Grammar (MLG) as a Rank-Interpretation Architecture, i.e. a small finite tuple of ranks, or levels of abstraction from discourse through utterance or text, to phrases and words, which are processed incrementally (cf. also Gaspers et al. 2011) and in parallel, each rank being assigned its own characteristic semantic-pragmatic interpretation and prosodic-phonetic interpretation, also processed incrementally and in parallel. We concentrate primarily on the structures of the ranks and their prosodic-phonetic interpretations.\nThe crucial point for our purposes is semiotic: whether each rank has sui generis structures and categorial (phonological) and physical (phonetic) interpretations. Secondarily, the default linearity of language at each rank is dealt with. Despite our semiotic approach, we are not so much concerned with semantic-pragmatic interpretation, since we consider semantics and pragmatics to share general cognitive patterning with other areas of cognitive activity, whether music and dance or town planning, not subject to the distinctive architectural constraints which apply to speech processing. As already noted, we also consider speech to differ crucially from written language in terms of production and perception resources, both of time (utterance vs. inscription processing) and space (short term memory vs. memory enhancements through paper or screen).\nSince our approach is empirically grounded in speech rather than writing (though we take examples eclectically as needed from both domains), an important role in the following discussion is played by speech melody in the prosodic-phonetic interpretation of signs, concentrating particularly on intonation, especially at utterance and discourse ranks. We do not address the extensive literature on the roles of rhythm and timing in the present study (cf. Wagner, P. 2008), however, but concentrate on a number of rank-specific properties of pitch contours.\nThe parallel information channels of prosody, and to some extent of conversational gesture, convey complementary information to the locutionary channel of words, sentences, utterances and discourse sequences:. The metalocutionary hypothesis (Gibbon 1976, 1981b, 1983) suggests that prosody is a parallel channel manifesting Peirce\u2019s (1905) semiotic functionality of index and icon, but in a metalinguistic or metalocutionary domain in which structural properties of utterances (rather than conventional objects) may be denoted at each rank. Ohala\u2019s sound-symbolic Frequency Code (1994) is a specific case of a model of paralinguistic aspects of prosody as a parallel information channel.\nThe metalocutionary functions of indexically and iconically marking the structure of speech hold at all ranks: phonemic (with lexical tone, pitch accent and stress); morphemic (e.g. inflectional and word compositional tones in Niger-Congo languages); phrasal (in intonation, accentuation and boundary marking), text (intonation structures such as the \u2018paratone\u2019 or paragraph intonation) and discourse intonations (marking phases in discourse). The boundary tones and pitch accents of intonation are clear metadeictic markers of positions and degrees of relevance in utterances (Gibbon 1983). Lexical tones, both phonemic and morphemic, may arguably extend indexical and iconic functionality of prosody to that of semantically arbitrary Peircean symbols.\nFor the reasons outlined above, prosody, particularly pitch patterning, plays a central role in the empirical underpinning of the MLG framework, and we discuss relevant prosodic data\nGibbon & Griffiths 2016-09-18 5/41 Multilinear Grammar V01\nin each of the sections demonstrating the sui generis character of each rank. We anticipate that our discussion of the relative independence of prosodic evidence may also help to underline work on the primacy of gesture and prosody in both ontogenetic and phylogenetic perspectives on speech.\nThe specific contributions of our approach lie crucially (1) in the semiotically motivated ternary Rank-Interpretation Architecture (Gibbon 2011) for MLG; (2) in the assignment of semantic-pragmatic and prosodic-phonetic interpretations (and the necessary lexica) to every rank of the grammar rather than to the \u2018top\u2019 and \u2018bottom\u2019 of the grammar, respectively; (3) in the claim that default structures are flat, with explicitly finite working memory resources, at every rank in the hierarchy.\nThe primary focus is on structures at the different ranks as sui generis subsystems, and on prosodic-phonetic interpretations at each rank. Semantic-pragmatic interpretation is only considered in passing. Secondarily we address issues pertaining specifically to the default linear properties of each rank. We do not argue on the basis of experimental results in the present study, but rather have the aim of providing a coherent approach to modelling the architecture and processing principles of language.\nIn the following sections, first we summarise the ternary semiotic Rank-Interpretation Architecture for our MLG model. Then we discuss selected examples of signs and structures from the ranks, each with their prosodic-phonetic interpretation, concluding with perspectives for future theoretical and practical development of MLG."}, {"heading": "2 The Rank-Interpretation Architecture for Multilinear Grammars", "text": ""}, {"heading": "2.1 Outline of the Rank-Interpretation Architecture framework", "text": "In developing our model of combinatorial communication, we expand the linearity and parallelism premises of MLG to a ternary architecture of ranks of semiotically functional signs and structures, with semantic-pragmatic interpretation and prosodic-phonetic interpretation at each rank. We note that prosodic-phonetic interpretation is actually a special case of multimodal interpretation, which includes other gestural interpretations including writing, signing and conversational gesture (Gibbon 2011; Rossini 2012). Specifically temporal properties of prosodic-phonetic interpretation (e.g. the \u2018categorial time\u2019 of phonology, the \u2018clock time\u2019 of digital phonetics, \u2018cloud time\u2019 of analog signals, Gibbon 1992; Carson-Berndsen 1998) will not be discussed further in the present context.\nThe multidimensional architecture model, with distinct semantic-pragmatic and prosodicphonetic interpretations at each rank, differs markedly from traditional architectures which place semantics and pragmatics at the \u2018top\u2019 and phonetics at the \u2018bottom\u2019 of the cascade, or have pragmatics subsuming semantics subsuming syntax. Each rank in the hierarchy has its own ternary semiotic make-up, with its own flat default grammar and also its own lexicon in the sense of an inventory of basic signs, from inventories of phonemes and morphemes to the conventional word lexicon and to phrasal idioms, fixed utterances and texts (from proverbs to literary works) and fixed discourses (like greetings and stereotypic small talk, but also poetry, religious liturgies, legal discourse).\nIn our MLG model we make the following basic claims about this semiotically motivated ternary Rank-Interpretation Architecture, and then discuss them more formally. We refer to structures and their interpretations both in declarative terms and in terms of processing properties.\nGibbon & Griffiths 2016-09-18 6/41 Multilinear Grammar V01\n1. The architecture for language is a non-recursive \u2018strictly layered\u2019 rank hierarchy of finite depth, from discourse through utterance (or text) to phrase (including clause and sentence), word (including inflected, compound and derived words), morpheme and phoneme (cf. Gibbon 1992).\n2. Syntax. Each rank has its own structural principles of organisation or \u2018syntax\u2019, but the default patterns for all ranks are linear, permitting only iterative (equivalently: right or left branching) patterns. Each rank in the hierarchy has its own conditions on flat linear processing, from discourse adjacency pairs and triplets to through utterance or text grammar and phrasal syntax to morphotactic and phonotactic constraints. Embeddings, whether nominal, prepositional, clausal or parenthetic, are handled by processors with finite working memories which lead to failure when their bounds are reached. We note that hierarchical structures in grammatical description are in general used for parsimonious representation of generalisations which are needed for semantic-pragmatic interpretation and are not required in the specification of \u2018plain distributional syntax\u2019, i.e. the default distributions of lexical items at each rank. The apparent exception of centre-embedding at the phrasal rank is discussed separately.\n1. Semantics. Each rank has its own semantic-pragmatic interpretation which in general operates incrementally over flat patterns in parallel with structural and prosodicphonetic processing. Semantic-pragmatic interpretation at the discourse rank concerns discourse framing, dialogue acts and turn-taking. Semantic-pragmatic interpretation at the utterance or text rank concerns argument, narration and and other genres, while at phrase and word ranks it concerns time/tense, aspect and modality, predicates and arguments. Semantic-pragmatic interpretation at phoneme level may seem to be a n odd concept, but it concerns contrastive encoding, and also restricted submorphemic phonaesthetic interpretations (Zelinski-Wibbelt 1983) such as the gl phonaestheme (glow-glitter-glimmer-gleam-glimpse-glint-glisten-glance-gloom-gloaming-glareglass-glaze-gloss, perhaps also glade-glamour, all of which are connected with aspects of sight or light) or the sl phonaestheme (sludge-slime-slug-slip-slop-slide-slithersledge-slurp-slow-slink-slither-slick-slough-slaver-slobber-sleet-slake-slush-sloopsluice, all of which are concerned with wet, sometimes slimy stuff).\n2. Phonetics. Each rank has its own prosodic-phonetic interpretation as part of a prosodic hierarchy. Intonation patterns of different kinds are assigned at discourse, utterance or text and phrase ranks, and tones and accents are applied at the word rank (simplex, derived and compound). Prosodic-phonetic interpretation requires the parallel processing, on the one hand of locutionary syllable, word, and sentence signs as prosody-bearing units (PBUs), and on the other hand of prosodic signs (tone, accent, intonation) on the other. The rank hierarchy and prosodic-phonetic interpretations of each rank are formal properties which are related to the Prosodic Hierarchy (Selkirk 1984), but include prosodic-phonetic interpretation at the utterance and discourse ranks."}, {"heading": "2.2 A preliminary note on linearity and hierarchy at the phrasal rank", "text": "Linearity versus hierarchy at the phrasal rank is the hard case for any grammar. There has been so much work on the relations between flat linear structure, hierarchical \u2018merging\u2019 and non-linear cross-referencing \u2018movement\u2019 in phrasal syntax since the 1950s that a review here is completely out of the question. But the issues are clear, as noted in the introductory section. First, we take the view that the main distinguishing design feature of human languages is the\nGibbon & Griffiths 2016-09-18 7/41 Multilinear Grammar V01\nRank-Interpretation hierarchy and its complex parallel processing requirements, from discourse patterning to phonemic patterning.\nSecond, we take the default structural principle behind spontaneous speech to be linearity, not hierarchical nesting. Third, we distinguish communication registers with different processing properties: from memory-enhanced communication (writing, rehearsed and stereotypic speech) to spontaneous, unrehearsed speech and gesture. And, independently of much current discussion, we consequently reject the view that recursion is the distinguishing design feature of human languages, a discussion triggered by Hauser et al. (2002), on the grounds that it fails to account both for the overall architecture of language and for processing constraints, as discussed in the present contribution. After all, real-life architects also have to account for complete buildings and their processing requirements.\nIn support of these views we argue for the following positions on linearity and recursion, bearing in mind the numerous insightful contributions to the topic in relatively recent collections (e.g. van der Hulst 2010; Roeper and Speas 2015) and in discussions surrounding extensive detailed empirical and formal analysis by Everett (cf. Futrell et al. 2016):\n1. Unilateral head (initial, left-branching) recursion and tail (right-branching) recursion are both formally equivalent to flat iterative repetition, and have the same finite memory processing requirements. In this they differ from centre-embedding recursion, a well-known result in the theory of formal languages (Hopcroft et al. 2007) and often noted in computational linguistics, while key differences between (1) left and right recursion, (2) centre-embedding and (3) general tree hierarchy formation are often ignored in linguistic debates.\n2. Apparent exceptions to flat linear structure, such as centre embedded recursion of shallow finite depth, are handled by processors, and indeed may fail in spontaneous speech, producing disfluencies. The actual finite and very small depth of recursion depends on both the user and on the medium: the additional storage facility and greater processing time provided by writing enable greater depth. Karlsson (2010) found on the basis of extensive corpus analyses that there are severe limits to the depth of centre embedding, e.g. 2 or 3, with somewhat greater depth in written corpora than in spoken corpora. Further, nominal and verbal constituents may be assigned separate grammars, each for example with right-branching, and then concatenated. In this way some cases of apparent centre-embedding may even be handled as left-branching or right-branching patterns.\n3. Hierarchical (tree) structures are convenient conceptual tools for representing generalisations about similar routes along the flat linear paths defined by a finite machine, even a finite machine for non-recursive constructions such as kernel sentences. Similarities of path segments are relevant for semantic and pragmatic interpretation, and may also be relevant for information structure and prosodic grouping.\nFor \u2018plain distributional syntax\u2019, hierarchies are sufficient but not necessary for expressing the combinatorics of the grammar itself when semantic interpretation is factored out. A thought experiment may help to clarify: attending a lecture in some advanced topic in which one has no training gives access to the basic distribution patterns of phones, morphemes (particularly affixes), words, and perhaps to information structure via prosody, but not to semantic interpretation except for the syncategorematic frame provided by closed class items and ordering.\nGibbon & Griffiths 2016-09-18 8/41 Multilinear Grammar V01\nSampson (2001) provides numerous examples of attested centre-embeddings. We claim that it is not a coincidence that the listed examples focus on writing, supporting the MLG approach, which distinguishes processing constraints in speech and writing. An instructive empirical example of the failure of centre-embedding in spontaneous speech occurs in the Christine corpus of spoken English (Sampson 1995). In unrehearsed informal spontaneous speech in English, in everyday contexts, nested self-embedded constructions are extremely rare, partly because utterances tend to be rather short. A search for one type of nested construction was made for the purpose of the present study in the CHRISTINE1 treebank of informal spoken English (Sampson 1995). This corpus of 10394 sentences is based on a random extract from the demographically sampled British National Corpus.\nThe specific search criterion was clause-initial who/whose (whom did not occur). Other types of relative clause, e.g. with initial that, which, what and zero pronoun, were not investigated. The search failed to find any successful cases of nested constructions: 145 sentences (1.4%) contained who/whose pronoun occurrences, of which 129 (1.24%) were sentence-initial interrogatives; 16 sentences (0.15%) were relative who/whose clauses, of which 9 (0.09%) were interrupted fragments (missing \u2018mandatory\u2019 constituents) and only 7 (0.07%) were complete relative clauses. The complete relative clauses were all rightbranching, i.e. linear; none was centre-embedded.\nIn the majority of cases of who/whose-initiated relative clauses, the attempt to recurse was broken off. In the only clear example of a potentially centre-embedded who/whose relative clause, the sentence petered out incohesively and without completion of the top embedded sentence:\nwe found out that the neighbours on the left hand side\nwho were in fact an elderly couple and his was erm and he had his own business working at home\n[ end; expected predicate phrase missing here ] A plausible explanation for the processing failure is (1) that spontaneous speech is linear in the default case, and (2) that the processing of nested sentences requires more memory than is spontaneously accessible. As already noted, rehearsed speech and writing are media with different constraints on code and channel. More complex processes are made possible by memory enhancement provided by writing and by rehearsal and experience with formal or read speech. This is taken as a given in the language technologies. Unaided by enhanced memory capacity, speakers in spontaneous scenarios appear to be highly averse to centreembedded recursion and tend to fail when they do attempt nested constructions. Failure in spontaneous speech also calls for theoretically principled explanation, not just successful formulation in rehearsed speech and writing."}, {"heading": "2.3 Characterisation of the Rank-Interpretation Architecture", "text": ""}, {"heading": "2.3.1 Background", "text": "The Rank-Interpretation Architecture, with its Peircean semiotic motivation and equally Peircean ternary dimensionality (Peirce 1905), has been influenced by a number of different directions in semiotics and linguistic theory, each of which incorporates some notion of \u2018level of abstraction\u2019, \u2018level of representation\u2019, \u2018stratum\u2019 or \u2018rank\u2019. Precursors are found most clearly in the rank scale of Halliday\u2019s scale and category grammar (1961), though not capped at the sentence rank, or in the levels of tagmemes in Pike\u2019s tagmemics (1967), or the concept of\nGibbon & Griffiths 2016-09-18 9/41 Multilinear Grammar V01\nstratum in Lamb\u2019s stratificational grammar (1966), as well as in the word-level rank concepts of \u2018duality\u2019 (Hockett 1958) and \u2018double articulation\u2019 (Martinet 1960). The more traditional clause-level functional rank concept of Jespersen (1924) is more related to structural linearity and hierarchy than to the present rank concept.\nThe concepts represented in these approaches are very different, deriving as they do from different structuralist and functionalist approaches, and covering the linguistic domain to very differing extents. Nevertheless the approaches share with the Rank-Interpretation Architecture model (1) the semiotic premise that sounds and meanings are related by signs and structures of language and (2) the premise that the architecture of languages covers signs of different types and sizes, from discourse events through phrase and word to phoneme signs, the syntax or \u2018tactics\u2019 of each type being organised in different ways into larger patterns."}, {"heading": "2.3.2 Formal summary", "text": "The ranks in the Rank-Interpretation Architecture \u03b1 (Greek \u03b1 for \u03ac\u03c1\u03c7\u03b9\u03c4\u03ad\u03b1\u03c4\u03c9\u03bd \u2018architect\u2019) are characterised compactly as an emergent rank sextuple:\n\u03b1 = < discourse, utterance, phrase, word, morpheme, phoneme >, or \u03b1 = <\u03b1disc, \u03b1utt, \u03b1phrase, \u03b1word, \u03b1morph, \u03b1phon>, or \u03b1 = <\u03b11, \u03b12, \u03b13, \u03b14, \u03b15, \u03b16>\nThe ranks are emergent in that they are the results of different communicative functionalities which arose in the course of evolution (polygenetic emergence) and arise in language acquisition (ontogenetic emergence). Details of structure and of functional worksharing between the ranks \u03b1i of \u03b1 may vary typologically from register to register, style to style, dialect to dialect, language to language, for instance between tones at phonemic and morphemic ranks, between inflection at the morpheme rank and word order at the phrasal rank, incorporation of words at the phrasal rank. There are fuzzy transitions between utterances and phrases (e.g. in expressing speech acts), phrases and words (e.g. morphosyntax, morphological incorporation), words and morphemes (e.g. nontransparent lexicalisation), morphemes and phonemes (morphophonemics). But we take the upper and lower ends of the rank scale to be well-defined: discourse as a recognisable kind of event, and the phoneme or speech sound as a recognisable kind of event. Indeed, a minimal utterance may consist of a speech sound alone: \u201cOh!\u201d, \u201cMm.\u201d, \u201cAh!\u201d, \u201cSh!\u201d.\nEach individual rank \u03b1i from the discourse rank through to the phonemic rank, is characterised as an emergent semiotic triple of units and structures of the specific rank, with their rank-specific semantic-pragmatic interpretations and prosodic-phonetic interpretations (Greek \u03c4 \u2018tau\u2019 for \u03c4\u03ac\u03be\u03b9\u03c2 \u2018taxis\u2019 structure or arrangement, \u03c3 \u2018sigma\u2019 for \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03b1\u03cc\u03c2 \u2018semantikos\u2019 significant, \u03c6 \u2018phi\u2019 for \u03c6\u03c9\u03bd\u03ae \u2018phone\u2019 sound or voice.):\n\u03b1i, = < \u03c4i, \u03c3i, \u03c6i > The units and their interpretations are emergent in the sense that they are functions of the semiotic relation between sign events and the contexts in which they occur over time. Spelled out, the Rank-Interpretation Architecture thus has the following structure as a rank sextuple of signs and their semantic-pragmatic and prosodic-phonetic interpretations:\n< < \u03c4disc, \u03c3disc, \u03c6disc >, < \u03c4utt, \u03c3utt, \u03c6utt >, < \u03c4phrase, \u03c3phrase, \u03c6phrase >, < \u03c4word, \u03c3word, \u03c6word >, < \u03c4phon, \u03c3phon, \u03c6phon > >\nGibbon & Griffiths 2016-09-18 10/41 Multilinear Grammar V01\nFor processing purposes in production and perception, the three dimensions < \u03c4i, \u03c3i, \u03c6i > at each rank may be re-ordered semasiologically (\u03c6i first) or onomasiologically (\u03c3i first). More plausibly still, the ranks and their dimensions are processed in parallel, incrementally and synchronously."}, {"heading": "2.3.3 Contrast with traditional views of language architecture", "text": "The Rank-Interpretation Architecture model has a fundamentally different semiotic basis from previous architectures. In particular, there is no assignment of phonetics to the \u2018bottom\u2019 of a hierarchy of units and semantics or pragmatics to the \u2018top\u2019 with syntax in between, or the Morrisian abstraction scale (or Carnapian inclusion scale) of syntax \u2192 semantics \u2192 pragmatics. Distinct semantic interpretations \u03c3i and prosodic-phonetic interpretations \u03c6i are assigned to the grammar \u03c4i at each rank \u03b1i.\nSemantic interpretation \u03c3 is a hierarchy which in the default (unmarked) case reflects the structure \u03c4 of \u03b1, and in the default case of a flat linear syntax at a given rank, is interpreted incrementally along the time axis. Parts of the \u03c3 rank hierarchy have received attention in predicate, propositional and modal logic, as well as in speech act theory and discourse theory, but an overall account of the \u03c3 rank hierarchy has yet to be created.\nLikewise, prosodic-phonetic interpretation \u03c6 is a hierarchy which again in the default case reflects the rank hierarchy of \u03b1. The Tillmann and Mansell (1980) and Christensen and Chater (2016) concepts of different sized time domains elaborate on the rank structure of prosodicphonetic interpretation. The architecture of the \u03c6 hierarchy has received most attention in phonology, in the form of the well-researched \u2018strictly layered\u2019 Prosodic Hierarchy (Selkirk 1984).\nAs already noted, the crucial claim is that the distinguishing \u2018design feature\u2019 of human language does not lie in a single specific structural property such as recursion or morphemicphonemic dual articulation, but in the structural and processing resources which apply to a complex rank hierarchy of structures from discourse to speech sound.\n2.3.4 Search for sui generis properties of ranks The consequence of this claim is that it must be shown that there are properties, either of structure or of prosodic-phonetic interpretation, which are unique to specific ranks. (The same applies to semantics, which is not at issue here, however.) Processing and structural properties differ markedly from rank to rank, though the common feature is clear: default processing is serial, linear, incremental and efficient, and the default formal model at each rank is a regular language defined by regular grammars or finite state automata, enhanced by different elaborations at each rank. The characteristics of the rank architecture are outlined in a little more detail as follows.\nThe rank architecture \u03b1 has a structure \u03c4, which differs from rank to rank but is basically linear at each level, and has a vocabulary at each rank \u03b1i, composed of sequences of elements of \u03b1i+1: discourse of utterance or text elements, utterance or text of phrasal elements, phrases of word elements, words of morpheme elements and morphemes of phoneme elements. That is, each rank is typed. The default architecture of the rank hierarchy is determined by a Strict Layering Hypothesis2, meaning that there is no inter-rank recursion between the ranks themselves, and compositional rules at each rank \u03b1i refer only to the types relevant for this\n2 Strict Layering Hypothesis, as in the Prosodic Hierarchy (Selkirk 1984, with a number of variants since then).\nGibbon & Griffiths 2016-09-18 11/41 Multilinear Grammar V01\nrank. There is intra-rank recursion, in the default case iteration. It was noted already that there may be stereotypic \u2018fuzzy transitions\u2019 between levels, as shown in the downranking of phrasal signs in the King of Spain\u2019s daughter, where the genitive inflection s applies to the phrase King of Spain which is \u2018rank-shifted\u2019 (in the sense of Jespersen 1924) to function as an inflected word.\nThe rank architecture has a semantic interpretation \u03c3. Each rank \u03b1i has its own semantic interpretation \u03c3i: \u03c3phon is the contrastive encoding of morphemes in the \u2018dual articulation\u2019 (Martinet 1960) or \u2018duality\u2019 (Hockett 1958) of language; \u03c3morph is the assignment of basic contextually dependent meanings (we are agnostic with regards to \u2018mentalese\u2019 or other notions of atomic meaning); \u03c3word is the assignment of lexical names, predicates and compositional operators; \u03c3phrase is the assignment of propositional meaning and modalities; \u03c3utt is the assignment of narrative, argument and information structures; \u03c3disc is the assignment of illocutions, perlocutions and adjacency pair determining dialogue acts and turn-taking. The semantic interpretation is situation dependent and indexical, incorporating a pragmatic interpretation \u03c0 with connotative meaning at each rank: for instance speaker and style or dialect variants of phonemes; speaker and dialect or language specific variants of morphemes, words, phrasal structures, utterance or text conventions and dialogue interaction.\nThe rank architecture \u03b1 has a prosodic-phonetic interpretation \u03c6, each rank \u03b1i having its own prosodic-phonetic interpretation \u03c6i: \u03c6phon is prosodic-phonetic interpretation in terms of distinctive features (including tone, pitch accent and stress accent), \u03c6morph is the prosodicphonetic interpretation of morphemes in terms of phoneme sequences (including sequences of tones and tonal sandhi in Niger-Congo languages, pitch accent in Japanese and stress accent in English or German); \u03c6word is the prosodic-phonetic interpretation of words, depending on language typology variants (also including the linking tones of Niger-Congo languages and the stress accent patterns of languages like English and German); \u03c6phrase is the interpretation of \u03b1phrase in terms of intonation (boundary tones, global contours and local pitch contours and durations for stress and pitch accents) ; \u03c6utt is assignment of rhetorical intonation patterns including additional prominence for rhematic or topical focus, contrast or emphasis, and destressing for thematic, comment, old information; \u03c6disc is the prosodic-phonetic interpretation with intonation, rhythm and vocalisations of discourse patterns such as question-answer, instruction-reaction, back channel feedback, or \u2018call contour chanting\u2019.\nOur term Multilinear Grammar , MLG, represents a claim that in the default case each of the ranks of \u03c4 and of \u03c6 is linear in the sense that it has structures which are also modelled as regular languages which can be captured by a regular grammar or finite state automaton. This is not necessarily true of the ranks of the semantic-pragmatic interpretation \u03c3, which in the default case is incremental and linear, but necessarily reflects the whole range of cognitive capabilities of humans, from handling complex linear railway networks to the recursions and networks of of logic and mathematics."}, {"heading": "2.4 Procedural perspectives on the rank hierarchy", "text": "One important key to understanding speech (and language) is to understand the processing constraints on speech and language (Christensen and Cater 2016; List et al. 2016), as often noted in psycholinguistic studies of disfluencies (Lickley 2015). In further discussion we focus initially on the prosodic hierarchy (e.g. Pierrehumbert 1980; Selkirk 1984), which is a form of rank hierarchy, with levels from phoneme through foot to word, phrase and intonation. But there are few approaches to rank structures in prosodic-phonetic interpretation which actually go above the sentence level in detail. An exception is found in the prosody\nGibbon & Griffiths 2016-09-18 12/41 Multilinear Grammar V01\ndomains of Tillmann & Mansell (1980), who distinguish between three timescales for language processing, which we interpret as prosodic-phonetic interpretations for word (B and C) and sentence, text and discourse ranks (A):\n1. A-Prosodie: intonation, timing (including pauses), intensity variation, for instance for discourse purposes and information structure; we distinguish further between the ranks of phrase, clause or sentence, utterance or text (e.g. \u2018paratone\u2019 or \u2018paragraph intonation\u2019) and discourse intonation.\n2. B-Prosodie: The syllabic rhythm of the language, determined by its phonotactics, in particular the alternation of sonorant (e.g. vowels) and non-sonorant phones (e.g. obstruents); we also include morphotactics because of the influence of affixal patterns on rhythm hierarchies with stronger and weaker syllables.\n3. C-Prosodie: The allophonic assimilatory and dissimilatory effects on transitions between phones.\nAnother exception is the recent proposal by Christensen and Chater (2016), who propose three timescales for language development (phylogeny, ontogeny and utterance), and take language structure at all ranks to be an outcome of discovery procedures at the different timescales. We distinguish the traditional four timescales, rather than the three of Christensen and Chater\u2019s: (1) the speech timescale Sspeech (or Sutterance) with tracking, analysis and understanding of speech productions; (2) the ontogenetic timescale Sont, with complexification of production, tracking, analysis and understanding as individual communicators mature; (3) the historical timescale Shist, with modification of details of the production, tracking, analysis and understanding as behaviours are passed from adult generations to the child generations with cross-links due to inter-language contacts; (4) the phylogenetic timescale Sphyl, as the species evolves. We are concerned with the first and, very briefly, with the third, with occasional remarks on the second and the fourth.\nIn terms of combinatorial communication, the procedural timescales are each empirically grounded in a diachronic implicational series:\nSphyl \u2192 Shist \u2192 Sont \u2192 Sspeech meaning that the if there is phylogenetic development, then there is historical development, if there is historical development then there is ontogenetic development, and if there is ontogenetic development there is spontaneous development in speech processing. The two scales Sphyl and Shist are distinguished because there is a clear difference between (1) historical changes such as Grimm\u2019s Law (\u2018weakening\u2019 consonants)3 or the syntactic change of SOV \u2192 SVO word order in English, coupled with loss of much inflectional morphology after the 11 th century, and (2) phylogenetic changes in the communication of the species on the other hand, such as the massive complexification of human communication in terms of the rank hierarchy in contrast with other primate species.\nIn the context of MLG it is relevant that many, perhaps all, sound laws in language change can be modelled by finite state transducers, like the phonotactic systems themselves. This has been shown for Slavic languages by Kilbury & Bontcheva (2004) and Kilbury et al. (2011). Grimm\u2019s Law, Verner\u2019s Law and the High German Soundshift can also easily be modelled by finite state transducers. In fact, transducers for Grimm\u2019s Law and Verner\u2019s Law can be unified into a single transducer, showing complementary environments for the two laws, which indicate that the laws are unordered, not ordered4 as the traditional controversy would have it.\n3 Grimm\u2019s Law (First Germanic Consonant Shift): 4 Note that the simultaneous application of apparently contradictory soundshifts is not a problem if the\nsoundshifts are assumed to operate gradually rather than in a sudden step.\nGibbon & Griffiths 2016-09-18 13/41 Multilinear Grammar V01\nThis is reflected in the transducer-related Stellenzuordnung and Korrelationsbeziehung relations of Kindt and Wirrer (1978) in their formal reconstruction of the complementarity of Grimm\u2019s Law and Verner\u2019s Law.\nIt makes sense to assume that the diachronic implicational series is gesture and prosody driven (bearing in mind that speech is also gesture-based). The beginnings are with rhythmic phase modulations and melodic frequency modulations at the discourse rank, with communicative functionality. Ever increasing granularity of rhythm and melody develops (cf. Speer and Kiwako 2009) as rank after rank emerges, from unstructured utterances through simple combinatory sequences (cf. Jackendoff and Wittenberg 2014, 2016) to words and ultimately the amplitude modulation of phoneme and syllable structures, with superimposed phase modulation of syllable and word rhythms.\nThis approach makes more sense, functionally, than a traditional prosody-free \u2018bottom-up\u2019 approach from phonemes through morphemes to ever more complex patterns in language acquisition or evolution. The complexification of the headed structures of \u03c4, reflected in the rhythm patterns of \u03c6 and the incremental compositional patterns of \u03c3, may be described in terms of a scale from simple to complex structures. We do not address the Sphyl, Shist and Sont timescales and further, however, but concentrate on the Supeech timescale, and at this timescale we address the rank levels in terms of their structure and prosodic-phonetic interpretation.\nThe most well-known benchmark for syntagmatic complexity is the ChomskySch\u00fctzenberger hierarchy of formal languages and their grammars (Chomsky and Sch\u00fctzenberger 1963; Hopcroft et al. 2007) The hierarchy ranges from from simple flat linear (highly restricted subregular and regular, Type 3) languages through more complex (contextfree, Type 2) and even more complex (context-sensitive, Type 1) to the most complex (unrestricted) languages, with subregular \u2018downward\u2019 extensions which include acyclic grammars for finite languages.\nType 3 languages are appropriate for modelling \u2018flat\u2019 structures (including unilaterally left or right branching tree structures), Type 2 for tree structures in general (including centreembedding), Type 1 for tree structures with cross-connections between branches, and Type 0 for arbitrary structures. We take the default model and the default processor for all ranks of language organisation, from discourse to phoneme, to be the regular grammar and the finite machine (finite state automaton, finite state transducer, finite transition network, etc.) respectively, with finite working memory requirements.\nNon-adjacent dependencies at any given rank (e.g. subject-verb agreement) can either be handled by a small finite bank of registers or compiled out into a single finite automaton. A corollary of these claims is that under the rank assumption, the core processors for all ranks can theoretically be composed into a single (albeit exponentially large) finite machine (Kaplan and Kay 1994).5 Different models for the parallel processing of flat linear streams, as well as for the composition of transducers into a single transducer are well-known in computational morphology: in Koskenniemi\u2019s (1984) approach to phonological finite state transducers, and Kay\u2019s (1986) multi-tape transducers for Arabic intercalatory morphology; cf. also Kaplan and Kay (1994) and Beesley and Karttunen (2003).\nUnlike grammars for other formal language types, Type 3 grammars have the remarkable property of requiring only finite working memory. Type 2, Type 1 and Type 0 languages on the other hand require, in principle, unbounded working memory of different structures. If\n5 The human brain is pretty big in terms of the number of neurons it contains, and children take many years to develop their language system, so we do not worry about the physical plausibility of this claim. And we make no claim about whether specific types of FM correspond to specific types of neuron.\nGibbon & Griffiths 2016-09-18 14/41 Multilinear Grammar V01\nmore complex structures than unilaterally branching trees are needed, procedures must be limited to a finite depth of recursion or cross-reference. In sentences like the zebra whose skin a man from Orlando bought illegally got lost was the last of its species the three clause recursions are tolerated only because of the different semantic interpretations of the recursion levels; on the other hand, a sentence like the man whose car the man the other man saw saw saw the man is puzzling, to say the least.\nWe take finite working memory to be an essential defining property of an empirically plausible model of human languages and their processing at all ranks, not necessarily as a one-dimensional tape, but possibly enhanced by other storage register structures. A corollary of this claim is that grammars which in principle require unbounded memory must be constrained with appropriately bounded data structures.\nThe following sections fill in details of the default flat linear processing constraints for \u03c4i at each rank \u03b1i and the associated default prosodic-phonetic interpretation \u03c6i. We note that the hierarchical patterns in traditional syntaxes are not strictly necessary from the point of view of plain distributional compositionality, which is flat linear in the default case. The hierarchies are a heuristic based ultimately on the rhythms of each \u03c6i in order to bootstrap the semantic interpretation \u03c3i of \u03c4i at each rank \u03b1i. In order to show this, the task is to establish whether there are structures \u03c4i and prosodic-phonetic interpretations \u03c6i at each rank \u03b1i which are sui generis in comparison with the structures and prosodic-phonetic interpretations at other ranks."}, {"heading": "3 The discourse rank", "text": ""}, {"heading": "3.1 The primacy of discourse patterning", "text": "We have suggested that the discourse rank and particularly discourse prosody is primary in the development of language, and that the rank architecture became more differentiated with time. Starting with the discourse rank \u03b1disc will show in this section that, is has properties of structure \u03c4disc and discourse prosodic-phonetic interpretation \u03c6disc which are distinct from properties at other ranks. Secondarily, the default flat linear structure of discourse patterns will be pointed out.\nModels of discourse patterning \u03c4disc vary between the basic dyadic \u2018Q&A\u2019 types of stimulus-response patterns of behaviourist linguistics through the adjacency pairs and their extensions in ethnomethodological conversation analysis (Sacks et al. 1974) to patterns based on dialogue act sequences (Bunt 2011). Formal models of dialogue patterning are rare, however. Finite state dialogue models in the human language technologies for humanmachine communication have been frequently and successfully used in the human language technologies (reviewed and applied in Bachan 2011) and are so widespread there as to need practically no further comment.\nFinite state models of dialogue do not stop there, however. Adjacency pairs and their extensions are straightforward to model as regular languages. An early finite state approach (Gibbon 1985) included a number of types of loops which model question-answerconfirmation triads, where the confirmation element is optional, or embedded back-channel loops with question-explanation-confirmation6:\n(question answer (confirmation | \u03b5)) (question (bc-question explanation (confirmation | \u03b5) )* ( confirmation | \u03b5 ) )\n6 The convention (c | \u03b5) is used to express optionality of the symbol c.\nGibbon & Griffiths 2016-09-18 15/41 Multilinear Grammar V01\nIt is not impossible for dialogue sequences to be interpreted semantically as having two or three levels of \u2018discourse centre-embedding\u2019, as Christensen and Chater (2016)7 point out. They provide the following example:\nC1: I would like to purchase some cheese, please. S2: What kind of cheese would you like? C3: Do you have any French cheeses? S3: Yes, we have Camembert, Port Salut, and Rocquefort. C2: I\u2019ll take some Camembert, then. S1: (wraps up some Camembert for the customer)\nA closer look shows that centre-embedding is not the only possible analysis of such a sequence, and the driving principle may actually be semantic rather than syntactic. There is in fact a linear interpretation, clarification by hyponymy proceeding through two steps: some cheese \u2192 French cheeses \u2192 Camembert, Port Salut and Roquefort, followed by a decision based on this clarification after the customer\u2019s intention and the merchant\u2019s offer have both been made more precise. Whether or not the discourse structure \u03c4disc is linear or not, it is indeed sui generis in having its own characteristic structures of interactive discussion and argumentation with adjacency pairs, triplets or extensions of these."}, {"heading": "3.2 Intonation of an adjacency pair", "text": "In the domain of discourse phonetics \u03c6disc there are numerous prosodic patterns of pitch and speech rate (tempo) which are used to mark \u03c4disc components of adjacency pairs and triplets, such as questions, answers, stereotypic and short back-channel responses and vocalisations such as I see, mhm, aha, yes (Couper-Kuhlen 2015) and which are not characteristic of other ranks. The main task here is again to show that the prosodic-phonetic interpretation \u03c6disc is indeed sui generis.\nFigure 1 (top) visualises one type of pitch assignment which \u03c6disc needs to provide, displaying the waveform and pitch trace of a question-answer adjacency pair extracted from a formal BBC radio interview in the Aix-MARSEC corpus (Auran et al. 2004, J0104G): Martin, do you think that the best side won it in the end? - Yes, I think there\u2019s no question that Argentina deserved to win...\u201d . Figure 1 (middle and bottom) show the pitch traces for the question and the answer, respectively, with a superimposed linear regression8 slope line to demonstrate a number of properties of the \u03c6disc mapping.\nThe pitch contours show characteristic discourse rank properties: 1. The global pitch modulation of the two utterances is approximately indicated the\nlinear regression slope over the pitch trace. Also the average pitch of the question is higher than that of the answer. This is a general tendency, not an accidental feature of this illustration.\n1. The regression line is level (or minimally positive, inclination: 0.041) for the question, and falling (negative, declination: -0.201) for the answer. Both of these pitch modulation features illustrate general tendencies, though in specific cases they may be indexical properties of individual speakers. The trend variable will be referred to as slope, covering the inclination of the question and the declination or downtrend of the answer.\n7 Indentation has been added to the original citation to indicate levels of centre-embedding. No page number is given since the reference is to the e-book version. 8 There are many models of the global F0 time function, mainly logarithmic, for present purposes the linear function is sufficient as with the relatively flat trajectory linear and log functions are practically indistinguishable.\nGibbon & Griffiths 2016-09-18 16/41 Multilinear Grammar V01\n2. Both question and answer terminate with a rising pitch modulation. The rising modulation of the answer termination is much narrower than that of the question, and indicates a \u2018continuation\u2019 (cf. also \u2018uptalk\u2019 intonation) with further statements.\n3. The pitch accents shown in the question (Figure 1 middle) are all Low, on MARtin, do you THINK that the BEST side WON it in the END?; these pitch accent modulations are below the regression line. In contrast, in the answer (Figure 1 bottom) the pitch accents are all High, with excursions of the pitch modulation above the regression line. This is another pair of tendencies in English: (1) a pitch accent shape is selected for the first accent position, (2) the selected shape tends to apply to the whole following sequence, with the possible exception of the final nuclear phrase accent.\n4. A further pairing tendency in English can be observed: (1) low pitch accents tend to co-occur with a flat or rising slope, while (2) high pitch accents tend to occur with falling slope, another general tendency.\nThe tendencies described here are indeed tendencies; much depends on incremental pragmatic decisions by the speaker to make one or other segment of the dialogue more prominent with pitch movement, rhythm changes or breaks. The description shows that the\nGibbon & Griffiths 2016-09-18 17/41 Multilinear Grammar V01\n\u03c6disc mapping is more complex than the traditional abstract characterisation of questionanswer pairs as being marked by rising and falling terminal tones, respectively."}, {"heading": "3.3 Chanted \u2018call\u2019 intonation", "text": "A prosodic feature which is exclusive to the discourse level and has received a little attention from phonetic, phonological and discourse functional perspectives, is the chanted stereotypic \u2018chroma\u2019 feature of the so-called \u2018call contour\u2019. The functional, non-phonetic term \u2018call contour\u2019 is a misnomer, because it is not only used for calls but has a more general function as a prosodic boundary marker at the discourse rank, introducing or terminating the uptakesecuring phase of discourse (Searle 1969), or with disagreement markers:\n-John-ny! -Yoo-hoo! -By-ee! -mm-mm!\nThere are two main functions of the chanted contour in English (ignoring related patterns like the \u2018ritual insult\u2019 chants of children, and liturgical chanting): (1) teleglossic (used at a distance in order to attract someone\u2019s attention or to find out if someone is present), in general with raised volume; (2) to start a conversation, simply at conversational volume. The farewell function may be used at conversational volume to terminate a discourse on the phone, for example. The chanted contour, its structure, function and phonetics has been discussed extensively by Pike (1945), Gibbon (1976), Liberman (1976), Ladd (1980), Niebuhr (2013) and many others.\nExamples of three chanted contours are given in Figure 2 for Hell-o-o!, Goodb-y-ye!, and -John-ny, where -are-you?, the latter with a sequence of two contours. The pitch traces are slightly irregular: this is a male voice with the normal variations of vibrating vocal fold tissue, not a trained voice. The first case, on Hello, shows a normal high pitch accent on the initial part, while the other cases have approximately flat pitch on each part of the chanted contour.\nThe characteristics of the chant are shown clearly in the figure: high pitch accents are associated with hel, good, John and are. In the first contour the pitch accent has a peak, as expected for pitch accents in general, and the tail contour is flat and lower, at mid pitch. In the next contour the pitch accent is flat, and the lower tail contour is also flat, at a mid pitch.\nGibbon & Griffiths 2016-09-18 18/41 Multilinear Grammar V01\nIt has been noted many times in the literature that the relation between the higher pitch accent and the lower tail approximates a minor third, though not with the precision of bel canto singing. This is the case with the examples shown in Figure 2, with syllable-averaged pitch values and ratios from this illustration shown in Table 1.\nIt is important for the present argument to note that the grammatical distribution of the chanted contour is distinctly different from other types of intonation pattern which apply to other ranks. The chant contour relates to discourse level \u03c6disc only; it does not co-occur with intonation patterns within sequences at utterance rank \u03c6utt and phrase rank \u03c6phrase. *I saw -John-ny yesterday! Embedding the chant contour at utterance and phrase ranks is distinctly odd, except perhaps jocularly when it is simultaneously intended to call the attention of someone at the other side of the room. The intonation also occurs with farewells. The semantic-pragmatic interpretation \u03c3disc for the utterance with the chanted contour is metalocutionary denotation of the a position in discourse, namely the introduction or termination of a discourse encounter.\nAlthough the basic chanted contour could be a substantive universal of speech, there are apparently language-specific differences (Gibbon 1976). In English, a rising version with an interval of approximately a sixth may also occur. In German the functionality is broader: the intonation may also signal an interruption of the discourse when securing of uptake for a successful speech act fails, either directly or in cited direct speech:\n-Lau-ter! Louder! \u201c-End-lich!\u201d hab ich gesagt. \u201cFinally!\u201d I said (chant continues to end of utterance).\nGibbon & Griffiths 2016-09-18 19/41 Multilinear Grammar V01\nThe differences between the discourse distributions of the stylised contour in English and in German at the \u03c6disc rank of prosodic-phonetic interpretation are modelled diagrammatically in Figure 3.\nThere are too many studies in discourse phonology and phonetics to be reviewed here. Many of them which point in the direction not only of the distinct character of discourse intonation, but also towards flat linear structuring and incremental linear interpretation (Cutler and Ladd 1983; Gibbon and Richter 1984; Selting 1995; Couper-Kuhlen 2015). But the preceding discussion is sufficient to demonstrate that the discourse level, understood as dialogue, indeed has its own sui generis structural and phonetic properties."}, {"heading": "4 The utterance or text rank", "text": "The task for the utterance rank \u03b1utt is analogous to the task for the discourse rank \u03b1disc: primarily to show that utterance level structure \u03c4utt and prosodic-phonetic interpretation are sui generis, and secondarily, if possible, whether they are linear in the default case. It may seem rather artificial to separate discourse (understood as dialogue) and utterance (understood as monologue), and on the other hand to associate utterances in dialogue with text of any size. The sui generis properties of this rank result from the sequencing of sentences and sentencelike components, and from production as a very short to very long monologues, rather than as a discourse dialogue.\nThe semantic structure of monologue utterances in spoken language can be simple, or very complex, with non-iconic temporal ordering of sentences, complex anaphoric and cataphoric cross-references, citations, justifications, asides, quoting of entire narratives and discourses. The complexity is very genre-specific. On the one hand, in unpractised speech and in much writing, the flat linear \u2018Hemingway style\u2019 of short sentences and linear structures is the default pattern, with more complex structures such as parenthetic or other embeddings tending to be highly stereotypic, for example: by the way, I meant to say, if you will permit. On the other hand, the extended working memory resources of rehearsed utterances and read-aloud texts permit much more flexibility in recursion than speech does (Karlsson 2010): the discourse context for stereotypic and rehearsed utterances and written texts offers the luxury of additional time and memory resources in both production and perception, provided by familiarity of repetition and lexicalisation, by rehearsal, and by the use of paper and screen. Detailed justification of the linearity of narrative, argumentative, instructional spoken utterance genres would require a separate study.\nThe important issue in the present context is what happens in prosodic-phonetic interpretation \u03c6utt. Utterance rank prosodic-phonetic interpretation \u03c6utt is highly varied, and the most prominent features are pitch patterning and rhythm (the latter is not discussed here).. One particular genre is selected here, a news item read by a female BBC newsreader (Auran et al. 2004, A0101B). The pitch trace for a large segment of the news item is visualised in Figure 4, as an illustration of relevant concepts, not a proof of the validity of a model of \u03c6utt.\nThe most obvious structuring parameter in the utterance is the pause. For convenience of reference the pauses pause1, \u2026 , pause9 which are longer than 200ms are numbered in Figure 4. The point of using an unusually compact rendering of the pitch contour of a long utterance (just under 60 seconds) is to show the structural characteristics of the utterance rank. The monologue is not transcribed as the prosodic patterning is more important for present purposes, and a general overview of the semantics of the interpausal units is sufficient.\nGibbon & Griffiths 2016-09-18 20/41 Multilinear Grammar V01\nAn initial greeting, Good morning, precedes pause1 and has its own falling pitch contour. Between pause1 and pause2 the main topic of the news item is introduced and between pause2 and pause3 the relevant event is described, while between pause3 and pause4 additional background information is given. A longer pause, pause4, introduces a different topic, for which pause4 and pause5 enclose the relevant information, further information being added by the next interpausal intervals (the short final section is a fragment of the next topic). In this highly formal variety of speech the function of the pauses is highly regular, in distinction from spontaneous speech, in which additional factors, such as disfluencies, hesitations and rhetorical pauses frequently occur.\nA conspicuous property of the pitch trace for this female speaker is the extreme prominence of the very high startup pitch accent modulation after pauses, which appear in Figure 4 as upward spikes to about 400 Hz (see also Figure 7 for the first sentence in the sequence). Very high pitch on intial pitch accents is not uncommon with female speakers of educated southern British English (a variety of Received Pronunciation). The height of the pitch spikes in the sequence tend to fall in the course of the news item, with occasional minor resets which do not reach the startup pitch height, but which start new episodes in the news item.\nA restart of the peak spike sequence may be predicted for a new news item, and indeed this the case: after pause 4, the height of the pitch accent even exceeds the height of the initial pitch accent, providing an instance of paratone or paragraph intonation structure (Wichmann 2000) in which a larger prosodic window than the single intonation unit is opened. The data in the illustration suggest that the structure may be more complex than two paratone levels, with pitch resets and pauses of different sizes, according to the exigencies of the information flow, as understood by the speaker (Bolinger 1972). This structure is partially reflected in the pitch trace in Figure 4: each unit starts with an initial boundary effect of rising anacrusis or pre-head (the initial unstressed syllables), and a very high topic-introducing pitch accent spike, followed by a sequence of less prominent pitch accents terminated by a falling pitch accent and final post-focus compression (Yi 2011) in the tail of de-stressed or unstressed syllables.\nThe properties of regular pause distribution and structured sequences of interpausal pitch patterning with paratone grouping are sufficient to show the sui generis properties of utterance rank prosodic-phonetic interpretation, \u03c6utt, versus both discourse and phrasal intonation, within MLG. It may perhaps be argued that the structure and phonetics of the utterance level are simply an iteration of structures and the phonetics of the phrasal level, but evidence concerning paratones refutes this.\nGibbon & Griffiths 2016-09-18 21/41 Multilinear Grammar V01"}, {"heading": "5 The phrase rank", "text": ""}, {"heading": "5.1 Characteristics of phrasal structure", "text": "It is not difficult to demonstrate that phrasal structure \u03c4phrase at rank \u03b1phrase is sui generis: this rank has arguably been investigated more rigorously than any other in linguistics. Phrasal structures are different from utterance or text structures and also from word structures (to be shown in the present section). The status of the linearity claim is more controversial than for the other ranks and figures in the discussion of hierarchy in language as combinatorial communication (Scott-Phillips and Blythe 2013). The controversy centres on centreembedding recursive structures, which in principle require non-finite working memory and processing in polynomial time (or worse), possibly a convenient assumption for some purposes but empirically implausible and based on false assumptions about processing.\nWe are primarily concerned with speech and its constraints, and argue in the same vein as Karlsson (2010) for strong constraints on recursion, limiting it to finite depth and thus, formally speaking, to tractability with finite memory systems. We advocate strict separation of cognitively complex semantic-pragmatic interpretation from a \u2018plain syntax\u2019 based on distributional constraints, not on semantic-pragmatic issues such as disambiguation. Phonetic \u2013 i.e. prosodic \u2013 interpretation \u03c6phrase, on the other hand, we see as determined only partly by structure, and partly by semantic-pragmatic structures such as contrast between hyponyms or by information structure and attention focussing.\nPhrasal patterns and their properties have mainly been discussed in terms of (1) tree graphs and context-free grammars, and (2) constraints which cut across tree graph structure and thus context-sensitive grammars, for example for cross-serial dependencies. But some these discussions have been more relevant to semantic interpretation rather than to \u2018plain syntax\u2019 (i.e. distributional constraints on the flat linear order of lexical items). An example of an essentially semantic condition is c-command (between a tree constituent and constituents of the subtree rooted in a sibling constituent) in determining the scope of some kinds of phrasal anaphora. In languages such as English the condition overwhelmingly corresponds to leftright linear order.\nRecursion is one of the key concepts at rank \u03c4phrase. Five main types, dependent on formal grammar types, are widespread in the linguistic literature on phrasal structures, though they are not always clearly distinguished:\nR1.Recursion in the general definition of any tree hierarchy which can be represented by a rooted tree graph. This concept underlies the general \u2018merge\u2019 functions of recent generative linguistics as the design feature of language (van der Hulst 2010; Roeper and Speas 2015). The epistemological point to make is, in addition to other counterarguments, that pretty much everything in the universe is organised hierarchically, at least according to our perception and theories of it.\nR2.Recursion in strictly layered finite depth tree hierarchies: if the hierarchy is strictly layered (i.e. all hierarchies described by the relevant grammars have a finite maximum depth), then a finite memory system is sufficient, and recursion is spurious, as with the SAAD (simple active affirmative declarative) \u2018kernel sentences\u2019 of Chomsky 1957 or the more general RSCs (root small clauses) of Progovac (2010), i.e. simple clauses with no embedding of relative or complement clauses or phrases.\nR3.Purely left-branching (head-recursive) or purely right-branching (tail-recursive) tree hierarchies. Tail-recursive hierarchies are very common in linguistic descriptions, and indeed ometimes a condition is stipulated that branching must be strictly right-\nGibbon & Griffiths 2016-09-18 22/41 Multilinear Grammar V01\nbranching, at least in the default case, as in this is the dog that chased the cat that worried the rat .... It is well-known, as already noted, that such grammars require only finite memory, that for any left-branching grammar there is a right-branching grammar and a finite state automaton which covers the same set of strings, that, likewise, for any right-branching grammar there is a left-branching grammar and a finite state automaton which covers the same set of strings. Recursion of this type can be substituted without loss by simple iteration of the type shown as loops in a finite state transition diagram.\nR4.Recursion over non-layered tree hierarchies, as permitted by general context-free grammars, at the expense of unbounded memory. This kind of recursion is by far the most popular in \u2018mainstream\u2019 linguistics, with \u2018phrase structure grammars\u2019, but clearly over-generates the language: in a sense, \u2018high recall and low precision\u2019. If the premise that unbounded memory is implausible is accepted, then additional constrained memory structures are needed in order to tame this type of recursion.\nR5.Tree hierarchies with cross-connections between the branches, e.g. for unbounded and cross-serial dependencies, require indexed grammars, a subset of context-sensitive grammars, also at the expense of unbounded memory. In this case, too, in order to satisfy the premise of plausibility, the unbounded memory requirement must be tamed by specific finite memory structures in order to handle the \u2018high recall and low precision\u2019 problem.\nThe first type, R1, is really a metatheoretical type. The others, R2 \u2026 R5, are types which are relevant for descriptive modelling, and types R2 and R3 may well represent the start of a scale of complexity in the polygenetic and ontogenetic development of languages. The scale may be extensible to types R4 and R5 provided that appropriate constraints on overgeneration are defined to prevent unwanted structures. Several such constraints do exist (e.g. Island Constraints on \u2018extraction\u2019 of pronouns from embedded clauses), but none addressing the overgeneration issue. The collections of van der Hulst (2010) and Roeper and Speas (2015) provide discussions and overviews of a variety of positions on recursion. It therefore remains to be seen how far \u03c4phrase modelling can get with recursion of types R2 and R3."}, {"heading": "5.2 Linear sequences, iteration: regular and subregular grammars", "text": "Regular grammars, with finite memory requirements, are really a family of more specific types, subregular grammars, each of which has the potential for modelling some aspect of language structure. Subregular grammars are a subset of regular grammars, in particular (but not only) the acyclic grammars which define finite languages. Where \u03a3 is a finite vocabulary of symbols, acyclic grammars define sets of strings from the set \u03a3n, where n is some finite integer. Kernel sentences or root small clauses are of this type. Juarros-Daussa (2010) claims that such constructions have a length of maximally 4 obligatory items, i.e. Subject-VerbObject1-Object2, as in Jack gave Mary cheese, according to a \u2018Two Argument Restriction\u2019. The limit can be flexibly, but still finitely, extended in English by optional adverbial constructions. Discussion of the increasing value of n has figured prominently for many decades in studies of first language acquisition, and more recently of evolution, from n=0 (silence is no doubt an option), through n=1 for single word utterances, n=2 for binary, and n > 3 for the general case. Jackendoff and Wittenberg (2016) suggest that the move from sequences of 2 to sequences of 3 was a qualitative leap in evolution.\nThere are several constructions in the subregular clausal domain which are often modelled by context-sensitive grammar rules (see below), perhaps enhanced with features, for example\nGibbon & Griffiths 2016-09-18 23/41 Multilinear Grammar V01\nmorphosyntactic agreement (congruence, concord), or the English auxiliary verb transformation (flipflop rule, affix-hopping). However, these rules can easily be \u2018compiled down\u2019 into context-free rules by assigning a rule to each agreement condition, and even into regular rules (Figure 5, Figure 6); cf. the discussion of affix-hopping by Pullum (2011).\nSubregular grammars are also relevant in the precedence tendencies noted by Karlsson (2010), at least in Germanic languages such as Swedish, German and English: grammatical subject main verb; main verb grammatical object; main verb subcategorised locative;\u227a \u227a \u227a determiner nominal head; determiner adjectival modifier; adjectival modifier nominal\u227a \u227a \u227a head; case-marked genitival NP nominal head.\u227a\nRegular grammars are a subset of context-free grammars, and can define either leftbranching or or right-branching trees over sequences (but not both in the same grammar). For any given grammar, the same set of sequences can be defined by finite state automata with\nGibbon & Griffiths 2016-09-18 24/41 Multilinear Grammar V01\niterative loops, i.e. \u2018flat recursion\u2019. There are many indications in the discussion of tree structures in linguistics that right-branching trees are the default constructions, both in the subregular simple clause domain and with recursive serial phrases and subordinate clauses, such as the slug on the plant in the pot in front of the window, or this is the horse that kicked the dog that chased the cat that worried the rat that ate the cheese.\nSo why is the branching needed, actually? The tree structures express syntagmatic grouping generalisations over the sequence in order to permit compositional semantic interpretation, and may be reflected to a limited extent in prosodic-phonetic interpretation by intonation boundary indicators and pitch accents (Wagner, A. 2008). However, the tree structures are not needed for the \u2018plain syntax\u2019 of these constructions, which can be modelled with iteration flat alone (cf. Figure 5), and incremental linear semantic interpretation applies in such cases. In cases usually analysed as left-branching, such as the subject in John\u2019s brother\u2019s mate\u2019s bike was a Raleigh, the same applies: a finite state automaton is adequate, and semantic interpretation is incremental.\nThe point made by van der Hulst (2010), that there is a difference between left and right branching, is in principle correct. The real problem actually lies in processing: left-branching is hard for top-down parsers with left-right processing, for example, and, vice versa, rightbranching is hard for bottom-up left-right processing. However, if branching is abandoned (or if the left-branching grammar is trivially translated into an equivalent right-branching grammar), or into an iterative finite state automaton, the issue does not arise.\nKarlsson (2010) describes several constraints on branching constructions, such as: \u2018an ifclause prefers initial embedding position\u2019, \u2018a when-clause may be embedded initially or finally\u2019, retaining the regular grammar property. In addition, Karlsson notes six distinct types of iteration in English i.e. constructions which are adequately modelled by regular grammars and are essentially flat (examples are mainly ours):\n1. structural iteration (e.g. conjunction): Bonnie and Clyde (syndetic); veni, vidi, vici \u2013 I came, I saw, I conquered (asyndetic);\n2. apposition: Ludwig Mies van der Rohe, the Bauhaus architect, once remarked that it was easier to design a skyscraper than a good chair. - Lohr, Steven. 1992. In the hot seat. Chicago Tribune 1992-09-13).\n3. reduplication: It\u2019s a long long way to Tipperary but my heart\u2019s right there \u2013 pre-WWI music hall song;\n4. repetition (disfluent iteration): Yeah, it, it, it is it\u2019s it\u2019s it\u2019s it\u2019s good. (Biber et al. 1999: 1055, cited by Karlsson 2010:49);\n5. listing (in restricted lexical taxonomies, e.g. proper names): Jake, Jock, Jack; (asyndetic); Fitzgerals, Dietrich and Lady Gaga (syndetic).\n6. succession: Monday, Tuesday, Wednesday, Thursday, Friday, Sat'day night / On the bedpost overnight (Lonnie Donegan. 1959. Does your chewing gum lose its flavour. Skiffle adaptation); one, two, three \u2026 GO! (trigger for synchronising simultaneous competition start).\nThe domain of agreement (congruence, concord) relations is subregular in the default case, not context-sensitive, as many linguistic descriptions. Agreement is expressed by a small finite set of categories in acyclic simple clauses. Intra-nominal agreement in German, for instance, requires the set {CASE, GENDER, NUMBER}, and nominal-verbal agreement requires the set {PERSON, NUMBER and NUMBER}. However, agreement may also occur across recursive contexts, as with relative pronouns in German. To express these cooccurrences context sensitive rules are sufficient, but not necessary: they may be compiled out\nGibbon & Griffiths 2016-09-18 25/41 Multilinear Grammar V01\ninto context-free rules ore even to subregular grammars with registers (cf. ATN storage registers, Wanner and Maratsos 1978), comparable with the features of a unification grammar such as LFG (Bresnan 2001). Simpler still, these systems may be compiled further into a finite state automaton, illustrated in Figure 5.\nAnother powerful mechanism which is not strictly necessary for \u2018plain syntax\u2019, but which generalises over flat linear structures for semantic purposes, is Chomsky\u2019s auxiliary transformation9 (jargon names: \u2018flipflop rule\u2019, \u2018affix-hopping\u2019) for describing the dependency of auxiliary verb suffixes on the previous auxiliary verb in English. Evidently a subregular acyclic grammar is sufficient to describe the facts since the set of English auxiliary verb sequences is finite: 24 = 16 combinations of TENSE, PERFECT, PROGRESSIVE and PASSIVE, multiplied by the number of modal verbs, with a maximal overt length of 4 items, as in it might have been being repaired. The stem-suffix dependencies can be expressed by means of a finite system in several ways: with 1-place lookahead (LL(k) grammar for k=1, or minimally >1 allowing for intervening adverbs of frequency etc.), with a register for each auxiliary, or simply with a finite state automaton, as shown in Figure 6 (cf. Berwick and Pilato 1987 for a slightly different finite state analysis).\nSummarising: in an analysis of \u2018plain syntax\u2019 it turns out that a flat grammar is quite adequate even for cases which are traditionally described with more complex mechanisms."}, {"heading": "5.3 A note on long-distance and cross-serial dependencies", "text": "Further cases of prima facie more complex grammars are worth investigating, in particular context-free and indexed grammars.\nContext-free grammars (Type 2, phrase structure grammars) are a null-context subset of the context-sensitive grammars, and have formal and processing properties which are very different from regular grammars. As already noted, unlike regular grammars, in principle they require unbounded working memory, an empirically implausible assumption. The unilaterally left or right branching tree structures frequently proposed in linguistics analyses of sentence syntax correspond to the more restricted case of regular grammars, as already noted. Gazdar et al. (1985) demonstrated with Generalised Phrase Structure Grammar that long-distance dependencies with verba dicendi et cognoscendi, previously thought to need transformational power: Who did you say that John thought Joe had told Mary he had seen __? One straightforward way to compute this dependency is to use a finite register or set of registers for the pronominal reference, analogous to the use of registers for concord, together with a right-branching or flat grammar. The technique is a fortiori transferable to left-branching structures.\nA type of grammar widely claimed to be necessary for modelling cross-serial constructions, among other cases, is the indexed grammar are a subset of Type 1 contextsensitive grammars, which are in turn a subset of unrestricted (Type 0) grammars. Other Type 1 and Type 0 grammars are now generally considered unnecessarily powerful for natural language modelling. The term \u2018mildly context-sensitive\u2019 was coined by Joshi (1985) for crossserial dependencies such as Jake, Jock and Jack married June, Joan and Jane, respectively, where elements of lists of equal length are paired and semantically interpreted in the given order (cf. also Shieber 1985). different cross-serial relations are found in some other languages. In Tree Adjoining Grammars (TAGs, Joshi 1985; cf. also Michaelis 1998) the\n9 Let Af stand for any of the affixes past, \u00d8, en, ing. Let v stand for any M or V, or have or be (i.e. for any non-affix in the phrase Verb). Then: Af + v \u2192 v + Af # , where # is interpreted as a word boundary (Chomsky 1957:39).\nGibbon & Griffiths 2016-09-18 26/41 Multilinear Grammar V01\nindexing is factored out by tree copying and adjoining rather than string embedding. A more complex case of cross-serial dependency is Jake, Jock and Jack gave June, Joan and Jane chocolates, flowers and diamonds at Easter, Whitsuntide and Christmas, respectively, subject to the same conditions but generalised to triples rather than pairs.\nCross-serial dependency is not constrained by embedding: Jake and Jock, who are married to June and Joan, respectively, will be visiting us tomorrow, but multiple \u2018respectively embedding\u2019 does not work: Jake and Jock, who are married to June and Joan, respectively, are British and Australian, respectively. There is no problem if the embedded respectively is omitted: Jake and Jock, who are married to June and Joan, are British and Australian, respectively. The constraint seems to be that respectively does not appear twice: *Jake and Jock, who are married to June and Joan, respectively, are British and Australian, respectively.\nIt may be objected that linear indexed languages (Gazdar 1987) and counting or copy languages (Kallmeyer 2010a, 2010b) {wn | w {a, b}*}, \u2208 n>1. These appear at first glance to be needed for modelling cross-serial dependencies, and are more powerful than context-free languages and a fortiori than regular languages.\nThe counter-argument is that the copies required are not for lists of arbitrary length requiring Type 1 indexed languages with unbounded memory, but for lists with a small finite bound of two or three, as Karlsson has shown (2010). The copying or counting operation in itself is linear in the length of the list; consequently if there is no arbitrarily deep embedding, the influence of the operation on overall complexity is minimal. Therefore the contribution to the overall complexity of the grammar is minimal and unlikely to be empirically measurable.\nA similar argument for linear modelling of cross-serial dependencies was advanced by Kindt (1998), who points out that separating ID (hierarchical immediate dominance) and LP (linear precedence) relations as parallel constraints enables a regular grammar to be used. Kindt had already noted that cross-serial dependencies are empirically bounded and the failure of regular grammars to generate unbounded cross-serial dependencies is thus not a counter-argument.\nFinally, the bounds on the cross-serial constraint are unclear. There are constructions with potential multiple ambiguities in such lists, as in Jake and Jock married June, Joan and Jane. Indeed, even with respectively the constraint does not need to be heeded, as in Jake, Jock and Jack married June and Joan, respectively; in informal tests with linguistically na\u00efve native speakers various interpretations were offered but the acceptability of the formulation was rarely called into question. It seems that respectively may have a more general meaning of \u2018ordered sequence\u2019 rather than the strict meaning of \u2018paired in exactly that order\u2019. This goes against received linguistic wisdom, but that does not render the observation invalid."}, {"heading": "5.4 Prosodic-phonetic interpretation at the phrasal rank", "text": "There are many different ways of looking at phrasal interpretation \u03c6phrase. In generative parlance, \u03c6phrase is, on the one hand, \u2018postlexical\u2019 and deals with morphophonemic alternations and stress position assignment beyond the domain of the word, within a prosodic hierarchy. On the other hand, phrasal prosodic-phonetic interpretation is associated with intonation.\nTraditional studies of the prosodic-phonetic interpretation of phrases, intonation and rhythm, have relied on imagined data, i.e. intuited intonation patterns associated with prior starting points of semantically interpreted phrasal patterns, as in the mainly pedagogical models reviewed in Gibbon (1976). This does not make them necessarily less valid than \u2018measurable data\u2019, but they are far from the intonation of spontaneous speech. A common example of intuited data which is in principle valid concerns the disambiguation of the scope\nGibbon & Griffiths 2016-09-18 27/41 Multilinear Grammar V01\nof structural and semantic operators, as in I did not go because I was tired, disambiguated with punctuation and potentially also by prosodic boundaries, as I did not go, because I was tired (i.e. I did not go), contrasting with I did not go because I was tired but because I was bored (i.e. I went).\nHalliday (1967) introduced a convenient trio of terms covering the main properties of intonation which is still popular in applied linguistics, and which may be rephrased in terms of later terminologies: tonality (phrasing, i.e. the assignment of boundaries and global contours to utterances); tonicity (the placement of stress realised by a nuclear or phrase accent, which can also be generalised to the placement of pitch accents, whether nuclear or not), and tone (the shape of the local pitch contour on the nuclear accented syllable, e.g. rising, falling, or a combination of these).\nIn generative phonologies, tonality and tonicity, i.e. phrasing and accent placement, were modelled cyclically, in a compositional interpretation of recursive syntactic structures, by means of the Nuclear Stress Rule (Chomsky and Halle 1968; but cf. Bierwisch 1966, van der Hulst 2010). The algorithm can be summarised in top-down fashion as follows: (1) Assign stress value 1 to the rightmost constituent and stress value 2 to all others. (2) For each child constituent, recursively assign the parent value to the rightmost constituent, and the parent value plus one to the others. Thus a structure such as ((big John) (saw (small Joan))) is assigned the stress values 3 2 3 4 1. Little was said about the prosodic-phonetic interpretation of stress placement or tonicity in terms of tone, and the boundaries were predetermined by the phrasal syntax. This was changed by the appearance of Pierrehumbert (1980) and her associates.\nFigure 7 visualises the first sentence from the example already given in Figure 4, from the Aix-MARSEC corpus (Auran et al. 2004). The syntactic structure is straightforwardly linear: the \u2018sentence\u2019 lacks a main verb, and consists of a single extensive nominal phrase, which includes two recursions, separated by a pause, each of depth 1: an appositional parenthesis, and a right-branching relative clause. The example illustrates one kind of \u2018sentence intonation\u2019 and how the iterative structure of the sentence, with smaller and larger loops, is reflected to some extent by pauses (with a shorter pause before the apposition than before the relative clause). The structure is not closely reflected by the pitch contour itself, which is very specific to language, genre and gender..\nThe example visualises a number of characteristic features of sentence intonation for a reading aloud genre (BBC newsreading, female voice, Educated Southern British English). First a very high onset pitch accent (443Hz) occurs on the first lexical item \u2018more\u2019, which is characteristic of this type of speaker, as already noted in the context of \u03c6utt. The body of the\nGibbon & Griffiths 2016-09-18 28/41 Multilinear Grammar V01\nfirst phrase has the expected declination and a final pitch accent on the name \u2018Moon\u2019 with final irregular pitch due to creaky voice. After a short pause, a parenthetic recursion occurs on \u2018founder of the Unification Church\u2019, and the pitch contour restarts slightly higher, with overall declination until the syllable \u2018ca\u2019 of Unification, followed by post-focus compression with low pitch on the following syllable. It is noteworthy that the pitch does not reflect the subordinated apposition. The pitch accent is in fact considerably higher than the preceding accent on the commanding noun \u2018Moon\u2019. The sentence continues with a right-branching recursion on the relative clause \u2018who\u2019s currently in jail for tax evasion\u2019. A similar pattern to the preceding main clause occurs: an extremely high onset pitch accent modulation, though not as high as the modulation of the first onset pitch accent. Then there is pitch declination until the final nuclear pitch accent on \u2018tax\u2019.\nThe pitch pattern does not show a branching structure, but rather relatively ad hoc iterations of linear patterns with attention directed to the successive parts by means of pitch accents determined by the speaker. The iteration types are marked prosodically by repetition of a global pitch pattern (e.g. declination, flat or inclination) or a local pitch pattern (accent), or similarity of timing (rhythm), and by intonational boundaries (Pierrehumbert 1980; Wagner, P. 2008). The exact phonetic form of the pitch accent is quite variable in a stressbased language like English: it can be high, low, rising, falling, rising-falling, falling rising (Silverman et al. 1992). In fact there is an extensive line of literature which supports the flat grammar hypothesis for intonation structure, the most well-known of which is the finite state automaton model for intonation of Pierrehumbert (1980).\nIn Figure 8, two interesting patterns are shown. The low pitch accents and rising slope (inclination) reflect the two points made earlier in connection with the \u03c6utt:\n1. All pitch accents are have the same shape as the first in the sequence, up to but not necessarily including the nuclear accent.\n2. If pitch accents are low, then slope is rising, else slope is falling. In order to discover how general the constraint is, more empirical investigation is required. However, it should be noted that such constraints have often been listed in the pedagogical textbooks on intonation, under names such as \u2018stepping head\u2019, \u2018rising head\u2019, but have been less prominent, or ignored, in linguistic studies..\nGibbon & Griffiths 2016-09-18 29/41 Multilinear Grammar V01\nIn summary: it is presumably uncontroversial that the structures \u03c4phrase are sui generis., and there is strong evidence that the default grammar is linear in the senses outlined in this section. The linearity of prosodic-phonetic interpretation in \u03c6phrase has been extensively documented in the literature and illustrated here, and the features of pitch accent iteration noted here underline the sui generis character of the \u03b1phrase rank."}, {"heading": "6 The word rank", "text": ""}, {"heading": "6.1 Flat words", "text": "Morphology is the \u2018upper half\u2019 of the dual articulation (Martinet 1960) or duality (Hockett 1958) semiotic structure of the lexicon; phonology is the \u2018lower half\u2019. The argument for possible words is of the same type as in conventional arguments for \u2018possible syllables\u2019: there are syllables such as blade and flunk, so there are possible syllables blunk and flade, based on free concatenation of syllable onsets and nuclei. Such syllables (or words) may currently have no meaning but are available for semanticisation and lexicalisation in the creation of brand name neologisms, such as BiC, or Twix, or slang neologisms such as twerk. For modelling syllable phonotactics an acyclic subregular grammar is sufficient (Gibbon 2007); for modelling alternations at syllable boundaries a different kind of cyclical subregular grammar was introduced (Koskenniemi 1984). It is hardly necessary to point out that \u03c4word and \u03c6word are adequately described at rank \u03b1word by flat linear grammars. The linearity of morphology is well established in computational morphology, morphophonology, phonology and tonology (Johnson 1972; Koskenniemi 1984; Gibbon 1987a; Kay 1987; Kaplan and Kay 1994; Beesley and Karttunen 2003). Hierarchical superstructures are needed for the cognitive complexity of semantic-pragmatic interpretation, as already noted for the other ranks, but this is a different issue.\nFor inflections the situation is clear, and has been partly dealt with in the discussion of \u03b1phrase: there is a finite set of inflections, and a finite maximal length for combinations of inflections, both for agglutinating languages (Finnish, Turkish) and fusional languages (most Indo-European languages); cf. especially Beesley and Karttunen (2003). Consequently a subregular acyclic grammar is sufficient to model inflection. For word formation (Marchand 1960) the situation is more complicated."}, {"heading": "6.2 Flat derivations", "text": "In principle the set of derivational suffixes, for example, is finite and the maximal length of a suffix sequence is finite, with sequences like regularisational, but there is a cyclic oddity in English: a sequence like regional can be extended to regionalisation and regionalisational. So in principle a further extension can be made, by induction on flat iteration\nregion alisation alisation alisation... etc. The result is still flat, though not acyclic. In linguistic descriptions, suffixal derivation is often described with left branching, illustrating parts of speech and semantic interpretation:\n(A (N (V (V (A (N region) al) is) at) ion) al) The \u2018plain syntax\u2019 combinatorics do not require branching, however. Additionally there is the constraint, most clearly formulated in Lexical Phonology (Rubach 2008) that the essentially Latinate suffixes occur closer to the root than the essentially Germanic suffixes, as in rationalisationless (as in a rationalisationless approach) or regionally.\nGibbon & Griffiths 2016-09-18 30/41 Multilinear Grammar V01\nFor derivational suffixes the situation is a little simpler: in general they can occur in any order. That is, given a vocabulary such as Vpref = {over, post, pre, pro, trans, un} the number of possible combinations is V*, covered by the simplest possible regular grammar. Whether they are currently meaningful is a different issue. For example: transprepostpreoverkill is a possible derived word."}, {"heading": "6.3 Flat compounds", "text": "If semantic-pragmatic interpretation is again kept separate from \u2018plain syntax\u2019 combinatorics, compound nouns are easily described by the simplest possible recursive (iterative) regular grammar. Given a basic vocabulary of simplex words V = {twin, cylinder, over, head, cam, shaft, motor, cycle} can occur in any number and any order, i.e. as members of the full set V*.\ntwin cylinder overhead camshaft motorbike over twin shaft cam motor head bike cylinder cam bike head cylinder over shaft twin motor ...\nThe combination which makes most sense to a biker, but perhaps not to many others, is twin cylinder overhead camshaft motorcycle; other items are still grammatically possible, and indeed potentially meaningful: over twin shaft cam motor head bike cylinder or cam bike head cylinder over shaft motor twin. Nonsensical compounds can be used in principle for new brand names, such as Kit-Kat or created as nonce words in particular situations.\nOrthographic spacing conventions are not relevant here: in the written language, the spaces (or hyphens) play a somewhat unsystematic role for hierarchical structuring in \u03c3word.\nThe motorbike example given here is semantically an endocentric compound (traditionally with the Sanskrit term: tatpurusa), like screwdriver (cf. also the lexically rare or unattested driverscrew), in which the meaning of the whole is a hyponym of the meaning of the head or determinatum, i.e. is subsumed by the meaning of the determinatum (in English the rightmost element), dependent on the meaning of the determinans (Marchand 1960). In other words the semantic relation between the head and the whole is paradigmatic, i.e. classificatory. The bicentric (Sanskrit: dvandva) compounds such as fighter-bomber (cf. the lexically rare or unattested bomber-fighter) or whisky-soda (soda-whisky is also a possible compound) are similar.\nThe exocentric (Sanskrit: bahuvrihi) compounds are different, with a syntagmatic relation between the head and the whole: the constituents have different parts of speech, and the structural head is not associated with the implicit semantic head: redhead (a person with a \u2018red head\u2019) scarecrow (an object which \u2018scares a crow\u2019) or pickpocket (someone who picks pockets). Note that the inverses headred and crowscare are also possible bahuvrihi compounds. These items require a more restricted subregular grammar than simply V*, which must take into account the parts of speech of the constituents."}, {"heading": "6.4 Prosodic-phonetic interpretation at the word rank", "text": "The prosodic-phonetic interpretation of words \u03c6word comprises the whole of phonology, and morphophonology, plus lexical prosody. There is not really much point in treating this broad domain here, but the main aspects will be referred to in passing, and a number of unconventional points concerning prosody will be addressed.\nThe phonetic interpretation of phonemic variants as allophones has long been known to be accessible to the finite state modelling of phonotactics (Johnson 1972; Kaplan and Kay 1994 and many others).\nGibbon & Griffiths 2016-09-18 31/41 Multilinear Grammar V01\nIn the context of prosodic typology, lexical stress patterns of simplex, derived and inflected words are standardly described in terms of linear position: initial (as in Hungarian), final (as in isolated words in French), penultimate (as in Polish), or with rules stating dependencies on flat linear structures, including rhythmic patterns, as in English.\nAn apparent counterexample to the linear combinatorics of morphology and in particular \u03c6word, is found in the Compound Stress Rule (CSR, Chomsky and Halle 1968) of compounding in English and related languages such as German, which assigns stress positions in compounds, and in compounds with more than 2 components additionally assigns degrees of stress which are dependent on position and depth of embedding, for example ((desk1 top3) (pen2 stand3)). The CSR is essentially a reversal of the Nuclear Stress Rule (NSR) mentioned in connection with \u03c6phrase. A number of algorithms based on either bracketings or on trees are available for calculating this indexing10.\nBut, on reflection, this apparent recursion, often centre-embedding, is actually a function of semantic-pragmatic interpretation in terms of hyponymy and \u2018new information\u2019 and not of plain combinatorics. The semantically motivated stress positioning by the Compound Stress Rule can in fact be overridden by flat linear patterns, as in lexicalised place-name compounds in German: Stein1 ha2 gen is the expected stress pronunciation, with primary stress on the first syllable, and is used in national news broadcasts. However, the local pronunciation in the village of Steinhagen is Stein2 ha1 gen, following a common linearly assigned penultimate stress pattern found in German (cf. also the inflection Dok1 tor , singular, vs. Dok tor1 en, plural). Similarly with Pa1 der born2 and Pa2 der born1 (cf. also Bleiching 1992).\nIn Figure 9 the pitch trace over a compound noun consisting of a derived noun and a simplex noun, the Unification Church, is displayed. Two regularities are involved: (1) As predicted by standard English Stress rules, including the Compound Stress Rule, the stress position is semantically predicted to be on the first word and is phonetically interpreted as a pitch accent, since the word is in nuclear stress position; (2) the first word in the compound is a derivation with penultimate stress, following standard rules for stress position assignment in English, cf. \u00fanify, unific\u00e1tion. The rules together account for the overall pattern of the compound.\n10 For example Liberman\u2019s algorithm: In a rooted binary tree, label the root R, each left child node s and each right child node w. For each leaf, traverse the path to the root, and starting at the first non-s node, count the intervening node.\nGibbon & Griffiths 2016-09-18 32/41 Multilinear Grammar V01\nAs already noted, the exact form of the pitch accent is quite variable in a stress-based language like English: it can be high, low, rising, falling, rising-falling, falling rising. These contours are often annotated not as explicit contours but as relations between pitch levels (Silverman et al. 1992).\nThe evidence shown in the discussion of \u03c4word and \u03c6word confirms the existing body of analysis for word prosody. The novel aspect is the reinterpretation of the entire domain of word formation as flat, and thus amenable to finite state automaton modelling, and the attribution of semantic motivation rather than purely distributional motivation to the Compound Stress Rule."}, {"heading": "7 Summary and conclusion", "text": ""}, {"heading": "7.1 From Duality to Multilinear Grammar and Rank Interpretation Architecture", "text": "The main goal of the present study is to show that one of the main design features of human languages, perhaps the main design feature, is a hierarchy of finite depth consisting of ranks of signs, each sign with its own distinct structures and semantic-pragmatic and prosodicphonetic interpretation, and by default with flat structure. A semiotically motivated ternary Rank-Interpretation Architecture was proposed, , generalising traditional concepts of \u2018duality\u2019 and \u2018dual articulation\u2019. As an overall framework, Multilinear Grammar, MLG, was described as a default principle operating over the entire Rank-Interpretation Architecture, contrasting with hierarchical grammars.\nThe overall structure of the Rank-Interpretation Architecture is shown informally in Figure 10, including a generalisation of prosodic-phonetic interpretation to multimodal interpretation. Relations between structure, form and functionality of locutionary signs, prosodic signs and conversational gesture (and, indirectly, of signing) are thereby captured in a non-ad hoc coherent fashion, and suggesting a multidimensional parallel processing system to handle default linear processing and interaction between components.\nThe crucial task has been to demonstrate that the default properties of the ranks are clearly delineated and sui generis: this was accomplished by showing specific types of prosodicphonetic interpretation associated with each rank, mainly in terms of prosody, and specific types of flat linear structure at each rank as the default structuring principle. We provided evidence for flat linear processing at each rank to demonstrate the feasibility of the model, and for flat linear incremental prosodic-phonetic interpretation at each rank. We are aware, at the same time, of the limitations of such a presentation: the approach is a well-motivated heuristic hypothesis rather than a proven theory.\nA major difference between the Rank-Interpretation Architecture of MLG on the one hand, and traditional architectures, on the other, is that each rank has not only its own specific kind of structure, but also its own specific lexicon, semantic interpretation and prosodic-phonetic interpretation principles. This contrasts starkly with traditional architectures which put semantics at the \u2018top\u2019, syntax in the \u2018middle\u2019 and phonetics at the \u2018bottom\u2019, a conceptualisation which is often reflected in the \u2018back end\u2019 - \u2018front end\u2019 operational models of the language technologies.\nGibbon & Griffiths 2016-09-18 33/41 Multilinear Grammar V01\nIn discussing prosody, only pitch patterning was discussed. Prominence was only mentioned in passing in connection with pitch accents, and timing and rhythm, core factors, were not treated. Likewise, the gestural character of all phonetic interpretation, and its relation to conversational gesture of the face and limbs, was not considered. Note that the Prosodic Hierarchy is not claimed to be isomorphic with the Rank hierarchy (though in a given instance the hierarchies align well), and that similar prosodic patterns, for example rising terminal intonation, may be associated with structurally similar meanings at different ranks. For instance the meaning \u2018non-terminal\u2019 for a rising intonation contour is interpreted differently as a list element or subject constituent at the phrasal rank, but as a question at the discourse rank."}, {"heading": "7.2 Generalisation to stochastic flat linear models", "text": "Work on implementing the MLG model with Rank-Interpretation Architecture, as outlined above, in an actual computational model is in progress. A first step is the design of a toy application for a restricted register of English or German on the lines of the IDyOM model (Pearce 2005, Pearce, M\u00fcllensiefen and Wiggins 2010a), which has also been used for the processing (Pearce, M\u00fcllensiefen and Wiggins 2010b) and production (Pearce and Wiggins, 2007) of music, implemented in LISP. IDyOM relies on NLP techniques together with computations of information-theoretic measures, and it has been demonstrated that it can also handle language data (Griffiths, Purver and Wiggins 2015).\nThe algorithm builds a variable order n-gram model of symbolic data such as musical notes or textual representations, for example of phonemes. This allows a linear model of the data to be built incrementally as it comes in, generating a sequence of probability values for each symbol encountered. Like speech prosody, music is clearly not just a sequence of pitches, but a configuration of notes11 in sequence and in parallel, with durations, tempos, rhythms, etc. Likewise, as the Rank-Interpretation model shows, speech is more than just a sequence of\n11https://code.soundsoftware.ac.uk/projects/idyom-project/wiki/Database\nGibbon & Griffiths 2016-09-18 34/41 Multilinear Grammar V01\neither pitches, spectral shapes or phonemes. Representations of several ranks of events, and tiers at each rank, are required. Stochastic models therefore also need to model simultaneous occurrence of perceivable events. For language, it has already been shown that segmentation performance increases when language models which include stress patterns are considered (e.g. Wiggins 2012, Griffiths et al. 2015).\nAn MLG model could also be implemented in a viewpoint system (Pearce, Conklin and Wiggins 2005, Hedges and Wiggins 2016), which takes account of the multilinear nature of the input. Probabilities are calculated as symbols are consumed on one layer. However, for the prediction of the next symbol the probabilities of several layers are taken into account and combined to calculate the probability of the next event."}, {"heading": "7.3 Future work", "text": "What, then is the benefit of the Multilinear Grammar, MLG, model and its associated RankInterpretation Architecture, other than as an exercise in neatness of linguistic description and in the application of Occam\u2019s Razor to the architecture of language, as an alternative to hybrid models with ad hoc interfaces? There are several significant areas where the MLG model has advantages over earlier models.\nWe suggest that the most significant benefit is that the traditional model of semantics at the \u2018top\u2019 and \u2018phonetics as a \u2018front end\u2019 at the \u2018bottom\u2019 of a cascade of units from phoneme through morpheme to sentence, leaves many areas of language and speech in isolated positions and mainly amenable to ad hoc analyses, but the Rank-Interpretation Architecture of MLG does not, and provides a framework for integrating components of the combinatorial communication system, rather than interfacing a set of hybrid components.\nThere are three relatively isolated areas (from the perspective of mainstream linguistics) which stand out but which the MLG approach handles in an integrative fashion. One relatively isolated area is prosody, which is often treated as something quite different from conventional phonetic interpretation which in turn tends to be restricted to words rather than being seen as a general principle of prosodic-phonetic interpretation, as in the MLG approach. A second relatively isolated area concerns multi-word signs such as fixed expressions, idiomatic metaphors, proverbs, and learned texts such as prayers and poems. A third relatively isolated area (at least from the perspective of mainstream linguistics) is the area of gestural comunication, both signing and conversational gesture, which slots very well into the RankInterpretation Architecture of MLG concept, yet tends to be absent from previous architectures, with rare exceptions: a potential principled link between speech and gesture is provided by the gestural base of Articulatory Phonology (Brownman and Goldstein 1989).\nFinally: MLG owes much to a very large range and number of previous studies, not least to earlier studies of linear systems. In a relatively early original account of the ability of multidimensional finite systems to account for far more in language than they are generally given credit for, in particular cross-serial dependencies, Kindt (1998) predicted:\nUnd speziell wird deutlich, dass eine Neubewertung der Beschreibungskapazit\u00e4t von Finite State-Systemen und naheliegenden Erweiterungen erforderlich ist. Sie wird \u00fcber kurz oder lang zu einem Paradigmenwechsel in der Grammatiktheorie f\u00fchren und die von Chomsky gepr\u00e4gte Epoche der Untersuchung eindimensionaler Systeme beenden.12\n12 Translation: \u201cAnd in particular it is becoming clear that a reappraisal of the descriptive capacity of finite state systems and related extensions is required. In the long run this will lead to a paradigm change in the theory of grammar and will end the Chomskyan epoch of the investigation of one-dimensional systems.\u201d\nGibbon & Griffiths 2016-09-18 35/41 Multilinear Grammar V01"}, {"heading": "8 References", "text": "Auran, Cyril, Caroline Bouzon and Daniel Hirst. 2004. The Aix-MARSEC Project: An\nEvolutive Database of Spoken British English. In Proceedings of Speech Prosody 2. Bachan, Jolanta. 2011. Communicative Alignment of Synthetic Speech. Dissertation. Pozna\u0144:\nAdam Mickiewicz University. Beesley, Kenneth R. and Laurie Karttunen. 2003. Finite State Morphology. Palo Alto: CSLI\nPublications. Berwick, Robert C. and Pilato. 1987. Learning Syntax by Automata Induction. Machine\nLearning, 2(l):9-38. Biber, Douglas, Stig Johansson, Georey Leech, Susan Conrad and Edward Finegan. 1999.\nLongman Grammar of Spoken and Written English. London: Longman. Bierwisch, Manfred. 1966. Regeln f\u00fcr die Intonation deutscher S\u00e4tze. In: Studia Grammatica\n7, 99-201. Berlin: Akademie-Verlag. Bleiching, Doris. 1992. Prosodisches Wissen im Lexikon. Proceedings of KONVENS 1992,\npp. 59-68. Bolinger, D. 1972. Accent is predictable (if you are a mind reader). Language 48:633\u2013644. Browman, Catherine P. and Louis Goldstein. 1989. Articulatory gestures as phonological\nunits. Phonology 6:201-251. Bresnan, Joan. 2001. Lexical Functional Syntax. Oxford: Blackwell. Bunt, Harry C. (2011). The Semantics of Dialogue Acts. Proceedings of the 9th International\nConference on Computational Semantics (IWCS 2011), pp. 1-14 Carson-Berndsen, Julie. 1998. Time Map Phonology: Finite State Models and Event Logics in\nSpeech Recognition. Dordrecht, Holland: Kluwer Academic Publishers. Chomsky, Noam. 1957. Syntactic Structures. The Hague: Mouton. Chomsky, Noam and Morris Halle. 1968. The Sound Pattern of English. New York: Harper\nand Row. Chomsky, Noam and Sch\u00fctzenberger, Marcel-Paul. 1963. The Algebraic Theory of Context-\nFree Languages\". In Braffort, P. and D. Hirschberg, eds. Computer Programming and Formal Systems, pp. 118\u2013161. Amsterdam: North-Holland.\nChristensen, Morten and Nick Chater. 2016. Creating language: integrating evolution, acquisition and processing. Cambridge MA: The MIT Press.\nChurch, Kenneth. 2007. A pendulum swung too far. Linguistic Issues in Language Technology. LiLT 2 (4). CSLI Publications.\nCouper-Kuhlen, Elizabeth. 2015. Intonation and discourse. In Schiffrin, Deborah, Deborah Tannen, D. & Heidi E. Hamilton, eds. Handbook of Discourse Analysis: Second edition. Oxford: Blackwell.\nCutler, Ann and D. Robert Ladd, eds. 1983. Prosody: models and measurements. Berlin: Springer-Verlag, 1983.\nFirth, John Rupert 1948. Sounds and prosodies. Transactions of the Philological Society 1948, pp. 127- 152.\nFirth, John Rupert. 1957. A synopsis of linguistic theory, 1930-1955. In Firth, J. R. et al. Studies in Linguistic Analysis. Special volume of the Philological Society. Oxford: Blackwell.\nFitch, W. Tecumseh, Marc D. Hauser and Noam Chomsky. 2006. The evolution of the language faculty: Clarifications and implications. Cognition 97, pp. 179\u2013210.\nGibbon & Griffiths 2016-09-18 36/41 Multilinear Grammar V01\nFrazier, Lyn and Janet Dean Fodor. 1978. The sausage machine: a new two-stage parsing model. Cognition 6 (4), 1978:291\u2013325.\nFutrell, Richard Laura Stearns, Daniel L. Everett, Steven T. Piantadosi, Edward Gibson. 2016. A Corpus Investigation of Syntactic Embedding in Pirah\u00e3. PLOSone. http://dx.doi.org/10.1371/journal.pone.0145289\nGaspers, Judith, Philipp Cimiano, Sascha S. Griffiths and Britta Wrede. 2011. An unsupervised algorithm for the induction of constructions. IEEE International Conference on Development and Learning (ICDL), Volume 2:1-6.\nGazdar, Gerald. 1987. Applicability of Indexed Grammars to Natural Language. In Uwe Reyle and Christian Rohrer, editors,Natural Language Parsing and Linguistic Theories, pages 69\u201394. D. Reidel.\nGazdar, Gerald, Ewan H. Klein, Geoffrey K. Pullum and Ivan A. Sag. 1985. Generalized Phrase Structure Grammar. Oxford: Blackwell, and Cambridge, MA: Harvard University Press.\nGibbon, Dafydd. 1976. Perspectives of Intonation Analysis. Bern: Lang. Gibbon, Dafydd. 1981a. A new look at intonation syntax and semantics. In: Alan James and\nPaul Westney, eds. New Linguistic Impulses in Foreign Language Teaching. T\u00fcbingen: Narr, 71-98.\nGibbon, Dafydd. 1981b. Metalocutions, structural types and functional variation in English and German Papers and studies in contrastive linguistics, 13, p. 17-39.\nGibbon, Dafydd. 1983. Intonation in context. An essay on metalocutionary deixis. In: Gisa Rauh, ed. Essays on Deixis. T\u00fcbingen: Narr, pp. 195-218.\nGibbon, Dafydd 1985. Context and variation in two-way radio discourse. In: Charles A. Ferguson, ed. Discourse Processes 8,4:391-420.\nGibbon, Dafydd. 1987. Finite state processing of tone languages. Proceedings of the European Association for Computational Linguistics, Copenhagen.\nGibbon, Dafydd. 1992. Prosody, time types and linguistic design factors in spoken language system architectures. In: G. G\u00f6rz, ed. (1992), Proceedings of KONVENS '92. Berlin, Springer, 90-99.\nGibbon, Dafydd. 2007. Formal is natural: toward an ecological phonology. Proceedings of ICPhS 16. pp. 83-88.\nGibbon, Dafydd. 2011. Modelling gesture as speech: A linguistic approach. Pozna\u0144 Studies in Contemporary Linguistics (PSiCL) 47:470ff.\nGibbon, Dafydd and Helmut Richter, eds. 1984. Intonation, Accent and Rhythm. Studies in Discourse Phonology. Berlin: Mouton de Gruyter.\nGoldsmith, John A. 1972. Autosegmental Phonology. PhD thesis, MIT. Goldsmith, John. 1990. Autosegmental and metrical phonology. Cambridge MA: Basil\nBlackwell. G\u00f3mez-Rodr\u00edguez, Carlos, Marco Kuhlmann, and Giorgio Satta. 2010. Efficient Parsing of\nWell-Nested Linear Context-Free Rewriting Systems. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), Los Angeles, USA, pp. 276-284.\nGriffiths, Sascha S, Mariano Mora, McGinity, Jamie Forth, Matthew Purver and Geraint A. Wiggins. 2015. Information-Theoretic Segmentation of Natural Language. In A. Lieto, C. Battaglino, D. P. Radicioni, & M. Sanguinetti, eds. AIC 2015 \u2013 International Workshop on Artificial Intelligence and Cognition. Turin, Italy, pp. 54-67.\nGibbon & Griffiths 2016-09-18 37/41 Multilinear Grammar V01\nGriffiths, Sascha, Matthew Purver and Geraint A. Wiggins. 2015. From Phoneme to Morpheme: A Computational Model. In H. Baayen, G. J\u00e4ger, M. K\u00f6llner, J. Wahle, & A. Baayen-Oudshoorn (Eds.), 6th Quantitative Investigations in Theoretical Linguistics Conference (QITL). T\u00fcbingen, Germany. http://doi.org/10.15496/publikation-8639\nGrosz, B.J., and Sidner, C.L. 1986. \"Attention, Intentions, and the Structure of Discourse\", Computational Linguistics, 12:3.\nHalliday, M.A.K. 1961. Categories of the theory of grammar. Word, 17 (3), pp. 241\u201392. Halliday, M.A.K. 1967. Intonation and Grammar in British English. The Hague: Mouton,\n1967. Hauser, Marc D., Noam Chomsky and W. Tecumseh Fitch. 2002. The Faculty of Language:\nWhat Is It, Who Has It, and How Did It Evolve? Science Vol 298, 22-11-2002, pp. 1569- 1579.\nHedges, T., & Wiggins, G. A. (2016). The prediction of merged attributes with multiple viewpoint systems. Journal of New Music Research, 1-19.\nHockett, Charles F. 1958. A Course in Modern Linguistics. The Macmillan Company: New York.\nHopcroft, J. E., R. Motwani and J. D. Ullman. 2007. Introduction to Automata Theory, Languages, and Computation. New York: Addison-Wesley.\nHotz, G\u00fcnter and Gisela Pitsch. On Parsing Coupled-Context-Free Languages. Theoretical Computer Science, 161(1\u20132):205\u2013233, 1996.\nHuybregts, Riny. 1984. The Weak Inadequacy of Context-Free Phrase Structure Grammars. In Ger de Haan, Mieke Trommelen, and Wim Zonneveld, editors, Van periferie naar kern, pages 81\u201399. Foris, Dordrecht, The Netherlands.\nJackendoff, Ray and Eva Wittenberg. 2016. Linear grammar as a possible stepping-stone in the evolution of language. In: Psychonomic Bulletin and Review. (DOI 10.3758/s13423016-1073-y). Berlin: Springer.\nJespersen, Otto. 1924. The Philosophy of Grammar. London: George Allen & Unwin. Johnson, C. Douglas. Formal Aspects of Phonological Description. Mouton. The Hague.\n1972. Joshi, Aravind K. 1985. Tree Adjoining Grammars: How Much Context-Sensitivity Is\nRequired to Provide Reasonable Structural Descriptions?. In David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, eds. Natural Language Parsing. Cambridge University Press, pp. 206\u2013250.\nJuarros-Dauss\u00e0, Eva. 2010. Lack of recursion in the leicon: the two-argument restriction. In: van der Hulst, H. ed., Recursion and Human Language. Berlin/New York: Mouton de Gruyter. pp.. 247-261.\nJurafsky, Daniel, and James H. Martin. 2009. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. 2nd edition. Prentice-Hall.\nKallmeyer, Laura. 2010. On Mildly Context-Sensitive Non-Linear Rewriting. Research on Language and Computation, 8(4):341\u2013363.\nKallmeyer, Laura. 2010. Parsing Beyond Context-Free Grammars. Berlin: Springer. Kallmeyer, Laura. 2010a. On Mildly Context-Sensitive Non-Linear Rewriting. Research on\nLanguage and Computation, 8(4):341\u2013363. Kallmeyer, Laura. 2010b. Parsing Beyond Context-Free Grammars. Springer.\nGibbon & Griffiths 2016-09-18 38/41 Multilinear Grammar V01\nKaplan, Ronald M. and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331-378.\nKarlsson, F. 2010. Syntactic recursion and iteration. In: van der Hulst, H. ed., Recursion and Human Language. Berlin/New York: Mouton de Gruyter, 2010: 43-67.\nKilbury, James and Katina Bontcheva. 2004. Historical-Comparative Reconstruction and Multilingual Lexica. Proceedings of Papillon 2004, 5th Workshop on Multilingual Lexical Databases, 30.08 - 01.09.2004, Grenoble, France.\nKilbury, James, Katina Bontcheva, Natalia Mamerow and Younes Samih. 2011. HistoricalComparative Reconstruction in Finite-State Technology. 9th International Tbilisi Symposium on Language, Logic and Computation, September 26-30, 2011, Kutaisi, Georgia.\nKilbury, James. 1997. Automaton theory and the formalization of historical-comparative reconstruction. 13th International Conference on Historical Linguistics.\nKindt, Walther and Jan Wirrer. 1978. Argumentation und Theoriebildung in der historischen Linguistik. Eine Untersuchung am Beispiel des Vernerschen Gesetzes. Indogermanische Forschungen 83:1-39.\nKindt, Walther. 1998. Die Beschreibungskapazit\u00e4t von Finite State-Grammatiken. Linguistische Berichte 174: 264-266.\nLamb, Sydney. 1966. Outline of Stratificational Grammar. Washington DC: Georgetown University Press.\nLickley, Robin J. 2015. Fluency and disfluency. In: Redford, Mellissa A., ed., The Handbook of Speech Production. Hoboken: Wiley-Blackwell.\nList, Johann-Mattis, Jananan Sylvestre Pathmanathan, Philippe Lopez and Eric Bapteste. 2016. Unity and disunity in evolutionary sciences: process-based analogies open common research avenues for biology and linguistics. Biology Direct 2016.\nMarchand, Hans 1960. The Categories and Types of English Word Formation. Wiesbaden: Harrassowitz.\nMarslen-Wilson, William. 1987. Functional parallelism in spoken word recognition. Cognition, 25:71-102.\nMartinet, Andr\u00e9. 1960. \u00c9l\u00e9ments de linguistique g\u00e9n\u00e9rale. Paris: Armand Colin. McNeill, David, ed. 2000). Language and Gesture: Window into Thought and Action.\nCambridge: Cambridge University Press. Michaelis, Jens. 1998. Derivational minimalism is mildly context-sensitive. In Logical\nAspects of Computational Linguistics, Third International Conference, LACL 1998, Grenoble, France, December 14\u201316, 1998, Selected Papers, volume 2014 of Lecture Notes in Computer Science, pages 179\u2013198. Springer, 1998.\nNiebuhr, O. (2013). Resistance is futile - The intonation between continuation rise and calling contour in German. Proceedings of the 14th Interspeech Conference, Lyon, France, 225- 229.\nOhala, John J. 1994. The frequency codes underlies the sound symbolic use of voice pitch. In L. Hinton, J. Nichols, and J. J. Ohala, eds. Sound Symbolism. Cambridge University Press, pp. 325-347.\nPalmer, F. R. 1969. Prosodic Analysis. Oxford: Oxford University Press. Palmer, F. R., ed. 1968. Selected Papers of J. R. Firth 1952\u201359. London: Longman.\nGibbon & Griffiths 2016-09-18 39/41 Multilinear Grammar V01\nPearce, M. T. (2005). The Construction and Evaluation of Statistical Models of Melodic Structure in Music Perception and Composition. Doctoral Dissertation, Department of Computing, City University, London, UK.\nPearce, M. T. and Wiggins, G. A. (2007). Evaluating cognitive models of musical composition. In A. Cardoso and G. A. Wiggins (Eds.), Proceedings of the 4th International Joint Workshop on Computational Creativity, (pp. 73-80). London: Goldsmiths, University of London.\nPearce, M. T., Conklin, D. and Wiggins, G. A. (2005). Methods for combining statistical models of music. In U. K. Wiil (Ed.), Computer Music Modelling and Retrieval (pp. 295- 312). Heidelberg: Springer.\nPearce, M. T., M\u00fcllensiefen, D., and Wiggins, G. A. (2010a). The role of expectation and probabilistic learning in auditory boundary perception: A model comparison. Perception, 39, 1367-1391.\nPearce, M. T., M\u00fcllensiefen, D., and Wiggins, G. A. (2010b). Melodic grouping in music information retrieval: New methods and applications . In Z. W. Ras and A. Wieczorkowska (Eds.), Advances in Music Information Retrieval (pp. 364-388). Berlin: Springer.\nPeirce, C. S. (1905). Prolegomena to an apology for pragmatism. The Monist, cited in Ogden, C. K. and I. A. Richards. 1923. The Meaning of Meaning. A Study of the Influence of Language uoon Thought and of the Science of Symbolism. p. 280.\nPereira, Fernando and Rebecca Wright. 1997. Finite-State Approximation of Phrase Structure Grammars. In E. Roche and Y. Schabes, eds. Finite-State Language Processing, pp. 149\u2013 173. Cambridge, MA: MIT Press.\nPike, Kenneth L. 1945. The Intonation of American English. University of Michigan Publications. Linguistics 1. Ann Arbor: University of Michigan.\nPike, Kenneth L. 1967. Language in Relation to a Unified Theory of the Structure of Human Behaviour. The Hague: Mouton.\nPierrehumbert, J. 1980. The phonology and phonetics of English intonation. PhD thesis, MIT. Progovac, Liliana. 2010. When clauses refuse to be recursive: An evolutionary perspective.\nIn: van der Hulst, H. ed., Recursion and Human Language. Berlin/New York: Mouton de Gruyter, 193-211.\nPullum, Geoffrey. 2011. On the mathematical foundations of Syntactic Structures. Journal of Logic, Language and Information, Vol. 20 (3), pp 277\u2013296.\nRoeper, Tom and Margaret Speas, eds. 2015. Recursion: Complexity in Cognition. Berlin: Springer.\nRossini, Nicla. 2012. Reinterpreting Gesture as Language. Language \u201cin Action\u201d. In series: Emerging Communication: Studies in New Technologies and Practices in Communication, Vol. 11. Amsterdam: IOS Press.\nRubach, Jerzy. 2008. An overview of Lexical Phonology. Language and Linguistics Compass 2/3, pp. 456\u2013477.\nSacks, Harvey, Emmanuel A. Schegloff and Gail Jefferson. 1974. A simplest systematics for the organization of turn-taking for conversation. Language, 50, 696-735.\nSampson, Geoffrey. 1996. The CHRISTINE Corpus stage 1. Project Report. Sampson, Geoffrey. 2001. Empirical Linguistics. London: Continuum International. Scott-Phillips, Thomas C. and Richard A. Blythe 2013. Why is combinatorial communication\nrare in the natural world, and why is language an exception to this trend? Journal of the Royal Society Interface 10 (rsif.royalsocietypublishing.org).\nGibbon & Griffiths 2016-09-18 40/41 Multilinear Grammar V01\nSearle, John. 1969. Speech Acts. Cambridge: Cambridge University Press. Selkirk, Elizabeth O. 1984. Phonology and syntax. the relation between sound and structure.\nCambridge, MA.: MIT Press. Selting, Margret. 1995. Prosodie im Gespr\u00e4ch. Aspekte einer interaktionalen Phonologie der\nKonversation. T\u00fcbingen: Niemeyer. Shieber, Stuart M. 1985. Evidence Against the Context-Freeness of Natural Language.\nLinguistics and Philosophy, 8(3):333\u2013343. Silverman, K., M. Beckman, J. Pitrelli, M. Ostendorf, C. Wightman, P. Price, J. Pierrehumbert\nand J. Hirschberg (1992) ToBI: A Standard for Labeling English Prosody. In J. J. Ohala, T. M. Nearey, B. L. Derwing, M. M. Hodge and G. E. Wiebe, eds. Proceedings of International Conference on Spoken Language Processing (ICSLP), Volume 2. Department of Linguistics, University of Alberta. 867-870.\nSpeer, Shari R. and Kiwako Ito. 2009. Prosody in First Language Acquisition \u2013 Acquiring Intonation as a Tool to Organize Information in Conversation. In: Language and Linguistics Compass 3/1: 90\u2013110.\nTillmann, Hans G\u00fcnther and Phil Mansell. 1980. Phonetik. Lautsprachliche Zeichen, Sprachsignale und lautsprachlicher Kommunikationsproze\u00df. Klett-Cotta, Stuttgart.\nvan der Hulst, Harry, ed., 2010. Recursion and Human Language. Berlin: Mouton de Gruyter. Wagner, Agnieszka. 2008. A comprehensive model of intonation for application in speech\nsynthesis. Dissertation. Pozna\u0144: Adam Mickiewicz University. Wagner, Petra. 2008. The rhythm of language and speech: Constraints, models, metrics and\napplications. Online publication, U Bielefeld: https://pub.uni-bielefeld.de/publication/1916845\nWanner, Eric and Michael Maratsos. 1978. An ATN approach to comprehension. In Halle, Morris, Joan Bresnan and George A. Miller, eds. Linguistic Theory and Psychological Reality. Cambridge: MIT Press.\nWichmann, Anne. 2000. Intonation and Discourse: Beginnings, Middles and Ends. Harlow and New York: Longman.\nWiggins, G. A. (2011). \u201cI let the music speak\u201d: Cross-domain application of a cognitive model of musical learning. Statistical learning and language acquisition. Amsterdam, The Netherlands: Mouton De Gruyter, 463-94.\nWittenberg, Eva and Ray Jackendoff. 2014. What You Can Say Without Syntax: A Hierarchy of Grammatical Complexity. In: Newmeyer, Fritz and Laurel Preston, eds. Measuring Grammatical Complexity. Oxford: Oxford University Press, 65-82.\nYi. Xu. 2011. Post-focus compression: cross-linguistic distribution and historical origin. Proceedings of the International Congress of Phonetic Sciences 2011, pp. 152-155.\nZelinsky-Wibbelt, Cornelia. 1983. Die semantische Belastung von submorphematischen Einheiten im Englischen. Frankfurt etc.: Verlag Peter Lang.\nGibbon & Griffiths 2016-09-18 41/41 Multilinear Grammar V01"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "1 Ranks, processes and flat grammar ....................................................................................... 2 2 The Rank-Interpretation Architecture for Multilinear Grammars ......................................... 6 2.1 Outline of the Rank-Interpretation Architecture framework .............................................. 6 2.2 A preliminary note on linearity and hierarchy at the phrasal rank ..................................... 7 2.3 Characterisation of the Rank-Interpretation Architecture .................................................. 9 2.3.1 Background ..................................................................................................................... 9 2.3.2 Formal summary ........................................................................................................... 10 2.3.3 Contrast with traditional views of language architecture .............................................. 11 2.3.4 Search for sui generis properties of ranks ..................................................................... 11 2.4 Procedural perspectives on the rank hierarchy ................................................................. 12 3 The discourse rank .............................................................................................................. 15 3.1 The primacy of discourse patterning ................................................................................ 15 3.2 Intonation of an adjacency pair ........................................................................................ 16 3.3 Chanted \u2018call\u2019 intonation .................................................................................................. 18 4 The utterance or text rank ................................................................................................... 20 5 The phrase rank ................................................................................................................... 22 5.1 Characteristics of phrasal structure .................................................................................. 22 5.2 Linear sequences, iteration: regular and subregular grammars ........................................ 23 5.3 A note on long-distance and cross-serial dependencies ................................................... 26 5.4 Prosodic-phonetic interpretation at the phrasal rank ........................................................ 27 6 The word rank ..................................................................................................................... 30 6.1 Flat words ......................................................................................................................... 30 6.2 Flat derivations ................................................................................................................. 30 6.3 Flat compounds ................................................................................................................ 31 6.4 Prosodic-phonetic interpretation at the word rank ........................................................... 31 7 Summary and conclusion .................................................................................................... 33 7.1 From Duality to Multilinear Grammar and Rank Interpretation Architecture ................. 33 7.2 Generalisation to stochastic flat linear models ................................................................. 34 7.3 Future work ...................................................................................................................... 35 8 References ........................................................................................................................... 36", "creator": "Writer"}}}