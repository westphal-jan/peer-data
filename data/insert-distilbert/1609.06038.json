{"id": "1609.06038", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Enhanced LSTM for Natural Language Inference", "abstract": "reasoning and inference are ultimately central roles to human and artificial intelligence. modeling inference protocol in pure human language is notoriously challenging but is fundamental to natural language understanding and across many applications. with the availability of large annotated data, neural induced network alignment models have recently explicitly advanced the field significantly. in this paper, we present a new state - of - the - art result, achieving the accuracy performance of 88. 3 % on the standard model benchmark, the stanford natural language inference protocol dataset. this result is therefore achieved first through our enhanced sequential encoding model, which outperforms the previous best model that employs more complicated network architectures, suggesting that the potential of sequential lstm - based models have not been fully explored yet in previous work. we cannot further show that by explicitly considering recursive architectures, we achieve additional efficiency improvement. that particularly, incorporating syntactic parse information contributes to our best result ; it systematically improves the performance even when gradually the parse information is added to an easily already very strong buffer system.", "histories": [["v1", "Tue, 20 Sep 2016 06:59:31 GMT  (287kb,D)", "http://arxiv.org/abs/1609.06038v1", "10 pages, 2 figures"], ["v2", "Tue, 7 Mar 2017 03:34:41 GMT  (507kb,D)", "http://arxiv.org/abs/1609.06038v2", "Update results, add case analysis"], ["v3", "Wed, 26 Apr 2017 17:37:13 GMT  (604kb,D)", "http://arxiv.org/abs/1609.06038v3", "ACL 2017"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qian chen", "xiaodan zhu", "zhen-hua ling", "si wei", "hui jiang 0001", "diana inkpen"], "accepted": true, "id": "1609.06038"}, "pdf": {"name": "1609.06038.pdf", "metadata": {"source": "CRF", "title": "Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference", "authors": ["Qian Chen", "Xiaodan Zhu"], "emails": ["cq1231@mail.ustc.edu.cn", "xiaodan.zhu@nrc-cnrc.gc.ca", "zhling@ustc.edu.cn", "siwei@iflytek.com", "hj@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "Reasoning and inference are central to both human and artificial intelligence. Modeling inference in human language is notoriously challenging but is fundamental to natural language understanding and many applications. As pointed out in [MacCartney and Manning, 2008], \u201ca necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.\u201d\nSpecifically, natural language inference (NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a natural-language premise p, as depicted in the following example from [MacCartney, 2009], in which the hypothesis is regarded to be entailed from the premise.\np: Several airlines polled saw costs grow more than expected, even after adjusting for inflation.\nh: Some of the companies in the poll reported cost increases.\nar X\niv :1\n60 9.\n06 03\n8v 1\n[ cs\n.C L\nThe most recent years have seen advance in modeling natural language inference (NLI). An important contribution is the availability of the large annotated dataset, the Stanford Natural Language Inference (SNLI) dataset, made available by [Bowman et al., 2015]. The corpus has 570k human-written English sentence pairs manually labeled by multiple human subjects, which can be use to train computers to learn inference knowledge with rather complicated models. In this benchmark dataset, the computer needs to decide if a premise p entails a hypothesis h, if they are contradicting to each other, or have no inference relation.\nNeural network models, which often need relatively large-scale annotated data to estimate parameters, have showed to keep improving the state of the art performance [Rockt\u00e4schel et al., 2015, Wang and Jiang, 2015, Munkhdalai and Yu, 2016b, Parikh et al., 2016, Cheng et al., 2016, Liu et al., 2016].\nWhile the previous best performing model [Munkhdalai and Yu, 2016b] wires rather complicated network architectures to achieve its state-of-the-art performance, it is not clear if the potential of the more basic sequential models, e.g., Long Short-Term Memory(LSTM) based architectures, have been fully explored. In this paper, we revisit this problem and further explore the potential of the basic LSTM-based sequential encoder. We show that such models can achieve an accuracy of 87.7% on the SNLI benchmark, outperforming the previous best model [Munkhdalai and Yu, 2016b] that employs more complicated network architectures.\nBased on this, we further show that by explicitly considering recursive architectures, we achieve additional improvement, increasing the performance to the accuracy of 88.3%. Particularly, incorporating syntactic parse information contributes to our best result; it improves the performance even when the parse information is added onto an already very strong system."}, {"heading": "2 Related Work", "text": "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to [MacCartney, 2009] for a good literature survey). More recently, [Bowman et al., 2015] made available the SNLI dataset with about 570K human annotated sentence pair. They also experimented with simple classification models as well as simple neural networks that encode the premise and hypothesis independently. [Rockt\u00e4schel et al., 2015] proposed neural attention-based model on NLI, which captured the attention information. In general, attention based models have shown to be effective in a wide range of tasks, including machine translation [Bahdanau et al., 2014], speech recognition [Chorowski et al., 2015, Chan et al., 2015], image caption [Xu et al., 2015], and text summarization [Rush et al., 2015, Chen et al., 2016], among others. For NLI, the idea allows neural models to pay attention to specific area of the input sentence.\nA variety of more advanced networks have been developed since then [Bowman et al., 2016, Vendrov et al., 2015, Mou et al., 2016, Liu et al., 2016, Munkhdalai and Yu, 2016a], and inter-sentence attention-based models [Rockt\u00e4schel et al., 2015, Wang and Jiang, 2015, Cheng et al., 2016, Parikh et al., 2016, Munkhdalai and Yu, 2016b]. Among them, more relevant to our work here are the approaches proposed in [Parikh et al., 2016] and [Munkhdalai and Yu, 2016b], which achieved the best performance.\nSpecifically, [Parikh et al., 2016] propose a relatively simple but very effective decomposable model. More specifically, the model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, the work of [Munkhdalai and Yu, 2016b] wire much more complicated networks that consider sequential LSTM-based encoding, recursive networks, and complicated combination of attention models, which provide about 0.5% gain over the results reported in [Parikh et al., 2016].\nIt is however not very clear if the potentials of the basic sequential networks have been fully explored. In this paper, we revisit this problem and further explore the the more basic sequential encoding model based with attention. Since the model of [Parikh et al., 2016] achieve one of the best results, we take it as our baseline. We show that enhancing the model from soft alignment, subcomponent inference, and inference composition improves performance by an absolute 1% accuracy, without using more complicated neural net architectures as in [Munkhdalai and Yu, 2016b] but already outperform them. This suggests the potentials of such very basic models had not been fully explored yet and we hope our work shed some light on the future work along this line.\nOn the other hand, we also employ rich information from syntactic parse and from recursive networks, i.e., the tree-LSTM [Zhu et al., 2015]. In general, exploring syntax together with semantics for NLI is very attractive to us. As pointed out in [Barker and Jacobson, 2007] \u201cthe syntax and the semantics work together in tandem\", and natural language inference is very likely to involve both. We show that incorporating syntactic parse information is useful even when the parse information is added to the already very strong system. Used with recursive LSTM, it improves the performance to a new state of the art."}, {"heading": "3 Hybrid Neural Inference Models", "text": "In this section, we present our hybrid inference networks which are composed of the following components: soft alignment, subcomponent inference collection, inference composition, and extension to recursive structures. Figure 1 depicts a very high level view about the components of the models.\nWe first revisit NLI by exploiting the basic models based on sequential chain-structured LSTM, which, as we will show in our results, can actually surpass the previous best results [Munkhdalai and Yu, 2016b] that instead leverage more complicated architectures. To this end, we take the basic framework introduced in [Parikh et al., 2016], which has achieved a performance comparable to the best [Munkhdalai and Yu, 2016b], but using a rather simple yet very effective architecture.\nIn our notation, we have two sentences a = (a1, . . . ,a`a) and b = (b1, . . . ,b`b), where a is the premise and b the hypothesis. Each ai or bj \u2208 Rl is an embedding of l-dimensional vector, which can be initialized with some pre-trained word embedding. The goal is to predict a label y that indicates the logic relationship between a and b.\nSoft Alignment Most NLI models explore some forms of alignment to associate the relevant subcomponent (e.g., words or phrases) between a premise and hypothesis. This includes early methods motivated from the alignment in conventional automatic machine translation [MacCartney,\n2009]. In neural network based models, this is often achieved with soft attention. The work of [Parikh et al., 2016], however, decomposes this process: the word sequence of the premise (or hypothesis) is regarded as a bag-of-word embedding vectors and inter-sentence \u201calignment\" (or attention) is computed individually to softly align each word to the content of hypothesis (or premise, respectively).\nWhile their basic framework is very effective, using pre-trained word embedding by itself does not automatically consider the context round a word in NLI. Parikh et al. did take into account the word order and context information through an optional distance-sensitive intra-sentence attention; in this paper we leverage instead the bidirectional LSTM (BiLSTM) to encoder the context, which shows to play an important role in achieving our best results. The intra-sentence attention used in [Parikh et al., 2016] actually does not further improve over our model.\na\u0304i = BiLSTM1(a),\u2200i \u2208 [1, . . . , `a], (1) b\u0304j = BiLSTM1(b),\u2200j \u2208 [1, . . . , `b], (2)\nThe BiLSTM hidden vectors above, i.e., {a\u0304i}`ai=1 and {b\u0304j} `b j=1, encode a word token and context around it. For completeness, the following equations define a regular chain LSTM.\nit = \u03c3(Wixt + Uiht\u22121), (3)\nft = \u03c3(Wfxt + Ufht\u22121), (4)\not = \u03c3(Woxt + Uoht\u22121), (5)\nct = ft ct\u22121 + it tanh(Wcxt + Ucht\u22121), (6)\nht = ot tanh(ct), (7) where at each word position t, LSTM employs a set of internal vectors: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct, to generate a hidden state ht (refer to [Hochreiter and Schmidhuber, 1997] for details).\nIn this work, we found BiLSTM is particularly effective to help build our final NLI models, when applied to both the soft alignment and inference composition. BiLSTM simply runs a forward and a backward LSTM along each input sequence and concatenate two resulting hidden-state at each time (word position), resulting in representation that naturally considers both the past and the future context.\nOnce BiLSTM hidden state is computed for each word, we simply insert it into the attention framework of [Parikh et al., 2016]. The model in Parikh et al. uses a function F (a\u0304i), i.e., a feedforward neural network, to map original word representation to calculate attention weight eij between ai in a premise and bj in the hypothesis:\neij = a\u0304 T i b\u0304j ,\u2200i \u2208 [1, . . . , `a],\u2200j \u2208 [1, . . . , `b]. (8)\nWe instead just simply replace the vector resulted from F (.) with the corresponding BiLSTM vector. We tried the function F (.) but did not find it helps our final best models.\nThe attention weights eij are then normalized and used to obtain new vectors as follows:\na\u0303i = `b\u2211 j=1 exp(eij)\u2211`b k=1 exp(eik) b\u0304j ,\u2200i \u2208 [1, . . . , `a], (9)\nb\u0303j = `a\u2211 i=1 exp(eij)\u2211`a k=1 exp(ekj) a\u0304i,\u2200j \u2208 [1, . . . , `b]. (10)\nHere a\u0303i, called as mimic vector, which is a weighted summation of {bj}`bj=1, is softly aligned to context dependent representation vector a\u0304i, and vice versa for b\u0303j . Intuitively, when mimic vector a\u0303i is semantically related to the vector ai in the premise p, the two vectors will be collected to determine the final inference relation between p and h.\nSubcomponent Inference Collection After alignment is performed, one can collect the inference information from these softly aligned subcomponents of sentences. These subcomponents could correspond to words and their context in a sequential model, phrases and their context in a recursive tree model, or even skip n-grams due to the nature of soft attention. More specifically, the subcomponent pairs between a premise and hypothesis that we use here is the hidden vector resulted from BiLSTM that encodes the current word and its context (e.g., in premise), as well as the inter-sentence soft-aligned content from the other sentence (e.g., the hypothesis). Later, when we incorporated tree-LSTM, the subcomponent could correspond to a node in a tree.\nThe inference relationship between the subcomponents of the sentence pairs is critical to help determine the overall inference between these two sentences. We therefore carefully model them here. First, we leverage heuristic matching [Mou et al., 2016] between original vectors and mimic vectors. The work of [Mou et al., 2016] used it to compare the premise and hypothesis sentence embeddings, while we extend it to subcomponents of sentences. Complicated models could be used here to model high-order interaction between subcomponent pairs, but that could dramatically increase number of parameters. Here we explore three simple matching heuristics and showed they improve the performance significantly. Particularly we use the concatenation of the two vectors, their difference, and element-wise product. Our experiment shows that these simple heuristics is every effective for NLI, compared with concatenation used in [Parikh et al., 2016]. The minus operation, for example, could help capture contradiction information.\nma = [a\u0304, a\u0303, a\u0304\u2212 a\u0303, a\u0304 a\u0303] (11) mb = [b\u0304, b\u0303, b\u0304\u2212 b\u0303, b\u0304 b\u0303], (12)\nInference Composition To determine the inference relationship between the entire premise sentence and its hypothesis, we explore a composition layer to mix subcomponent inference information.\nWe first discuss several simple strategies. Then, we introduce tree-LSTM composition. The work presented in [Parikh et al., 2016] separately compare pairs of the representation vector and mimic vector using a feed-forward neural network. Here we use another layer of BiLSTM to model the interaction between subcomponent inference collected above.\nv1,i = BiLSTM2(ma),\u2200i \u2208 [1, . . . , `a], (13) v2,j = BiLSTM2(mb),\u2200j \u2208 [1, . . . , `b]. (14)\nThen we convert the resulting vectors obtained above to a fixed-length vector and feed it to the final classifier to determine overall inference relationship. We use average pooling and max pooling technology and concatenate these vectors to get fixed length vector c, instead of using summation [Parikh et al., 2016]. We consider that summation is sensitive to the sequence length and is less robust. Our experiments verify average pooling and max pooling have better results than summation. The final fixed length vector v is calculated as follows:\nv1,ave = `a\u2211 i=1 v1,i/`a, (15)\nv2,ave = `b\u2211 j=1 v1,j/`b, (16)\nv1,max = `a\nmax i=1 v1,i, (17)\nv2,max = `b\nmax j=1 v1,j , (18)\nv = [v1,ave,v2,ave,v1,max,v2,max]. (19) We put v into a final multilayer perceptron (MLP) classifier. Specially, the MLP has a hidden layer with tanh activation and softmax output layer in our experiments. For training, we use multi-class\ncross-entropy loss. Again, the composition from subcomponent inference can be performed with recursive composition to incorporate syntactic information, as discussed below.\nFor simplicity, we call the model we obtain so far as Enhanced BiLSTM Inference Model (EBIM) in the remainder of this paper.\nExtension to Recursive Structures As discussed earlier in this paper, we are very interested in exploring syntax together with semantics for NLI. As pointed out in [Barker and Jacobson, 2007] \u201cthe syntax and the semantics work together in tandem\", and natural language inference is very likely to involve both of them. We show in this paper that incorporating syntactic parse information is useful even when the parse information is added to the already very strong system. Incorporated in recursive tree-LSTM, syntactic parse information contributes to achieving our best results.\nTo explore this, we replace the two BiLSTM (one used in performing soft alignment and one in inference composition) with tree-LSTM. In general, tree-LSTM has recently been proposed to explicitly model tree structures [Zhu et al., 2015]. Specifically, the forward propagation of tree-LSTM is computed as follows in Equation 20\u2013 26, and a node (a memory block) of the network is wired as in Figure 2.\nFigure 2 shows the memory block at each node of a recursive tree structure. In general, at each node, an input vector xt and the hidden vectors of two children (the left child hLt\u22121 and the right h R t\u22121) are taken in as the input to calculate their parent node hidden vector ht.1 These sources of information are used to configure the four gates as well, i.e., the input gate it, output gate ot, as well as the two forget gates fLt and f R t . The memory cell ct considers each child\u2019s cell vector, c L t\u22121 and c R t\u22121, which are gated by the left forget gate and right forget gate, respectively.\nit = \u03c3(Wixt + U L i h L t\u22121 + U R i h R t\u22121), (20)\nfLt = \u03c3(Wfxt + U LL f h L t\u22121 + U LR f h R t\u22121), (21)\nfRt = \u03c3(Wfxt + U RL f h L t\u22121 + U RR f h R t\u22121), (22)\not = \u03c3(Woxt + U L o h L t\u22121 + U R o h R t\u22121), (23)\nut = tanh(Wcxt + U L c h L t\u22121 + U R c h R t\u22121), (24)\nct = f L t cLt\u22121 + fRt cRt\u22121 + it ut, (25)\nht = ot tanh(ct), (26)\nwhere \u03c3 is the sigmoid function, is the element-wise multiplication of two vectors, and all W \u2208 Rd\u00d7l, U \u2208 Rd\u00d7d are weight matrices to be learned. Note that for brevity, all bias vectors are also omitted.\nWe use binary tree-LSTM in this paper. The tree structure for each sentence (the premise or hypothesis) is producer by a constituency parser. And as noted in [Zhu et al., 2015], one can always choose to binarize a non-binary tree, and the syntactic information will largely be kept. Binarization can help avoid the need of designing different types of memory blocks for tree nodes with different topology. In addition to using syntactic parse trees, we also borrow the idea from [Munkhdalai and Yu, 2016b] to use full binary trees without encoding syntactic information.\nNote that, after replacing BiLSTM with tree-LSTM in our experiment, we feed the root hidden states of tree-LSTM to the classifier, rather than using average pooling or max pooling as in BiLSTM. We found root hidden states have better performance in our experiments. Meanwhile, we ensemble the EBIM and tree-LSTM based model, and it tends to yield a further improvement when there is a significant diversity between the two models. We use a simple strategy that averaging predicted probabilities of two models as a final predicted probabilities.\n1The non-leaf nodes have no corresponding word, and the initial embedding vector is set to be 0."}, {"heading": "4 Experiment Set-up", "text": "Data The Stanford Natural Language Inference (SNLI) corpus [Bowman et al., 2015] focuses on three basic relationship between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The original SNLI corpus contains also \u201cthe other\u201d category, which includes the sentence pairs lacking consensus among multiple human annotators. Same as in previous work, we remove this category. We used the same split as in [Bowman et al., 2015] and as in other previous work.\nThe parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 [Klein and Manning, 2003] and they are delivered as a part of the SNLI corpus. We use classification accuracy as the evaluation metric, same as in previous work.\nTraining Details To help duplicate our results, we publish our code at http://www.place-holder.com. Below, we list our training details. We use the Adam method [Kingma and Ba, 2014] for optimization. The first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. All hidden states of LSTMs, tree-LSTMs, and word embeddings are 300 dimensions.\nWe use dropout with a dropout rate of 0.5, which is applied to the neural network classifier and word embedding layer. We use pre-trained 300-D Glove 840B vectors [Pennington et al., 2014] to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. All vectors including word embedding are updated during training."}, {"heading": "5 Results", "text": "Overall Performance Table 1 shows the results of different models. The first row is a baseline classifier presented in [Bowman et al., 2015] that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc.\nThe next group of models (2)-(7) are based on sentence encoding. The model of [Bowman et al., 2016] encodes the premise and hypothesis with two different LSTMs. The model in [Vendrov et al., 2015] uses unsupervised \u2019skip-thoughts\u2019 pre-training in GRU encoders. The approach proposed in [Mou et al., 2016] consider also tree-based CNN to capture sentence-level semantics, while the model of [Bowman et al., 2016] introduces a Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model. The work of [Liu et al., 2016] use BiLSTM to generate sentence representation, and then replace average pooling with intra-attention. The approach proposed in [Munkhdalai and Yu, 2016a] presents a memory augmented neural network, Neural Semantic Encoders (NSE), to encode sentences.\nThe next group of methods in the table, model (8)-(13), are inter-sentence attention-based model. The model marked with [Rockt\u00e4schel et al., 2015] is LSTMs enforcing so called word-by-word attention. The model in [Wang and Jiang, 2015] extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion [Cheng et al., 2016] link the current word to previous words stored in memory. The work of [Parikh et al., 2016] proposed a decomposable attention model without relying on any wordorder information. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. The model of [Munkhdalai and Yu, 2016b] extends the framework in [Wang and Jiang, 2015] to a full n-ary tree model and achieved further improvement.\nWe first show that our EBIM model achieves an accuracy of 87.7%, which has already outperformed previous best model reported in [Munkhdalai and Yu, 2016b] that use more complicated network architectures, including recursive models. It also significantly better than the model of [Parikh et al., 2016] (attaining a 86.8% accuracy). We showed that the potentials of very basic models such as chain LSTM-based models had not been fully explored in the previous work. It could deserve a further exploration on the power of such models.\nWe ensemble our EBIM model with syntactic tree-LSTM [Zhu et al., 2015] based on binary parse trees, and achieve significant improvement over our best sequential encoding model EBIM, obtaining an accuracy of 88.3%. The syntactic tree-LSTM complement very well with EBIM in achieving the result. Also, according to the results shown in the table, the information brought in from syntactic tree-LSTM generalize well from training to testing: making the performance on train and test data closer.\nAblation Results To investigate the effectiveness the major components of our models, Table 2 provides additional ablation analysis. From the best model, we first replace the syntactic tree-LSTM with the full tree-LSTM without encoding parsing information, similarly to in [Munkhdalai and Yu, 2016b]. More specifically, two adjacent words in a sentence are merged to form a parent node, and\nthis process continues and results in a full binary tree, where padding words are inserted to the nodes when there are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block [Zhu et al., 2015]. Table 2 depicts that with this replacement, the performance drops to 88.1%, showing that incorporating syntactic parse information is useful even when the parse information is added to the already very strong system. Again, adding syntactic parsing information seems to make the model generalize well from training to testing: in model (15) the performance for testing is closer to that in training, compared with those in model (16).\nFrom the EBIM model (14), we first remove the final average and max pooling and replace it with summation as used in Parikh et al.. The performance drops to 87.1%. If we further remove the difference and element-wise product from subcomponent inference collection, the performance drops to 86.7%. Further removing BiLSTM from inference composition and simply using a feed-forward neural network reduce the performance to 86.3%. Finally, we remove BiLSTM from soft alignment; the performance significantly dropped to 83.6%."}, {"heading": "6 Conclusions", "text": "We present several neural network models towards better solving the natural language inference (NLI) problem, which achieve the best results seen so far on the SNLI benchmark. The result is first achieved through our enhanced sequential inference model, which has already outperformed the previous best model that employs more complicated network architectures, suggesting that the potentials of sequential LSTM-based models have not been fully explored yet in previous work. We further show that by explicitly considering recursive architectures, we achieve additional improvement. Particularly, incorporating syntactic parse information contributes to our best result; it improves the performance even when the parse information is added to an already very strong system."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Direct Compositionality", "author": ["Chris Barker", "Pauline Jacobson"], "venue": null, "citeRegEx": "Barker and Jacobson.,? \\Q2007\\E", "shortCiteRegEx": "Barker and Jacobson.", "year": 2007}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "A fast unified model for parsing and sentence understanding", "author": ["Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc Viet Le", "Oriol Vinyals"], "venue": "CoRR, abs/1508.01211,", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Distraction-based neural networks for modeling document", "author": ["Qian Chen", "Xiao-Dan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang"], "venue": "In IJCAI,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Long short-term memory-networks for machine reading", "author": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "venue": "arXiv preprint arXiv:1601.06733,", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "KyungHyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Lei Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning"], "venue": "In ACL,", "citeRegEx": "Klein and Manning.,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Learning natural language inference using bidirectional lstm model and inner-attention", "author": ["Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang"], "venue": "arXiv preprint arXiv:1605.09090,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Natural Language Inference", "author": ["Bill MacCartney"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "MacCartney.,? \\Q2009\\E", "shortCiteRegEx": "MacCartney.", "year": 2009}, {"title": "Modeling semantic containment and exclusion in natural language inference", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1,", "citeRegEx": "MacCartney and Manning.,? \\Q2008\\E", "shortCiteRegEx": "MacCartney and Manning.", "year": 2008}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin"], "venue": "In The 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04315,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Neural tree indexers for text understanding", "author": ["Tsendsuren Munkhdalai", "Hong Yu"], "venue": "arXiv preprint arXiv:1607.04492,", "citeRegEx": "Munkhdalai and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "A decomposable attention model for natural language inference", "author": ["Ankur P Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "venue": null, "citeRegEx": "Parikh et al\\.,? \\Q1933\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 1933}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": "CoRR, abs/1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In EMNLP,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Order-embeddings of images and language", "author": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "venue": "arXiv preprint arXiv:1511.06361,", "citeRegEx": "Vendrov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vendrov et al\\.", "year": 2015}, {"title": "Learning natural language inference with lstm", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "CoRR, abs/1512.08849,", "citeRegEx": "Wang and Jiang.,? \\Q2015\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Lei Jimmy Ba", "Ryan Kiros", "KyungHyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Long Short-Term Memory over Recursive Structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In Proceedings of International Conference on Machine Learning,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "As pointed out in [MacCartney and Manning, 2008], \u201ca necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.", "startOffset": 18, "endOffset": 48}, {"referenceID": 12, "context": "\u201d Specifically, natural language inference (NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a natural-language premise p, as depicted in the following example from [MacCartney, 2009], in which the hypothesis is regarded to be entailed from the premise.", "startOffset": 211, "endOffset": 229}, {"referenceID": 2, "context": "An important contribution is the availability of the large annotated dataset, the Stanford Natural Language Inference (SNLI) dataset, made available by [Bowman et al., 2015].", "startOffset": 152, "endOffset": 173}, {"referenceID": 12, "context": "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to [MacCartney, 2009] for a good literature survey).", "startOffset": 126, "endOffset": 144}, {"referenceID": 2, "context": "More recently, [Bowman et al., 2015] made available the SNLI dataset with about 570K human annotated sentence pair.", "startOffset": 15, "endOffset": 36}, {"referenceID": 19, "context": "[Rockt\u00e4schel et al., 2015] proposed neural attention-based model on NLI, which captured the attention information.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "In general, attention based models have shown to be effective in a wide range of tasks, including machine translation [Bahdanau et al., 2014], speech recognition [Chorowski et al.", "startOffset": 118, "endOffset": 141}, {"referenceID": 23, "context": ", 2015], image caption [Xu et al., 2015], and text summarization [Rush et al.", "startOffset": 23, "endOffset": 40}, {"referenceID": 24, "context": ", the tree-LSTM [Zhu et al., 2015].", "startOffset": 16, "endOffset": 34}, {"referenceID": 1, "context": "As pointed out in [Barker and Jacobson, 2007] \u201cthe syntax and the semantics work together in tandem\", and natural language inference is very likely to involve both.", "startOffset": 18, "endOffset": 45}, {"referenceID": 8, "context": "ht = ot tanh(ct), (7) where at each word position t, LSTM employs a set of internal vectors: an input gate it, a forget gate ft, an output gate ot, and a memory cell ct, to generate a hidden state ht (refer to [Hochreiter and Schmidhuber, 1997] for details).", "startOffset": 210, "endOffset": 244}, {"referenceID": 14, "context": "First, we leverage heuristic matching [Mou et al., 2016] between original vectors and mimic vectors.", "startOffset": 38, "endOffset": 56}, {"referenceID": 14, "context": "The work of [Mou et al., 2016] used it to compare the premise and hypothesis sentence embeddings, while we extend it to subcomponents of sentences.", "startOffset": 12, "endOffset": 30}, {"referenceID": 1, "context": "As pointed out in [Barker and Jacobson, 2007] \u201cthe syntax and the semantics work together in tandem\", and natural language inference is very likely to involve both of them.", "startOffset": 18, "endOffset": 45}, {"referenceID": 24, "context": "In general, tree-LSTM has recently been proposed to explicitly model tree structures [Zhu et al., 2015].", "startOffset": 85, "endOffset": 103}, {"referenceID": 24, "context": "And as noted in [Zhu et al., 2015], one can always choose to binarize a non-binary tree, and the syntactic information will largely be kept.", "startOffset": 16, "endOffset": 34}, {"referenceID": 2, "context": "Data The Stanford Natural Language Inference (SNLI) corpus [Bowman et al., 2015] focuses on three basic relationship between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral).", "startOffset": 59, "endOffset": 80}, {"referenceID": 2, "context": "We used the same split as in [Bowman et al., 2015] and as in other previous work.", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "3 [Klein and Manning, 2003] and they are delivered as a part of the SNLI corpus.", "startOffset": 2, "endOffset": 27}, {"referenceID": 9, "context": "We use the Adam method [Kingma and Ba, 2014] for optimization.", "startOffset": 23, "endOffset": 44}, {"referenceID": 18, "context": "We use pre-trained 300-D Glove 840B vectors [Pennington et al., 2014] to initialize our word embeddings.", "startOffset": 44, "endOffset": 69}, {"referenceID": 2, "context": "The first row is a baseline classifier presented in [Bowman et al., 2015] that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc.", "startOffset": 52, "endOffset": 73}, {"referenceID": 3, "context": "The model of [Bowman et al., 2016] encodes the premise and hypothesis with two different LSTMs.", "startOffset": 13, "endOffset": 34}, {"referenceID": 21, "context": "The model in [Vendrov et al., 2015] uses unsupervised \u2019skip-thoughts\u2019 pre-training in GRU encoders.", "startOffset": 13, "endOffset": 35}, {"referenceID": 14, "context": "The approach proposed in [Mou et al., 2016] consider also tree-based CNN to capture sentence-level semantics, while the model of [Bowman et al.", "startOffset": 25, "endOffset": 43}, {"referenceID": 3, "context": ", 2016] consider also tree-based CNN to capture sentence-level semantics, while the model of [Bowman et al., 2016] introduces a Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model.", "startOffset": 93, "endOffset": 114}, {"referenceID": 11, "context": "The work of [Liu et al., 2016] use BiLSTM to generate sentence representation, and then replace average pooling with intra-attention.", "startOffset": 12, "endOffset": 30}, {"referenceID": 2, "context": "(1) Handcrafted features [Bowman et al., 2015] - 99.", "startOffset": 25, "endOffset": 46}, {"referenceID": 3, "context": "(2) 300D LSTM encoders [Bowman et al., 2016] 3.", "startOffset": 23, "endOffset": 44}, {"referenceID": 21, "context": "6 (3) 1024D pretrained GRU encoders [Vendrov et al., 2015] 15M 98.", "startOffset": 36, "endOffset": 58}, {"referenceID": 14, "context": "4 (4) 300D tree-based CNN encoders [Mou et al., 2016] 3.", "startOffset": 35, "endOffset": 53}, {"referenceID": 3, "context": "1 (5) 300D SPINN-PI encoders [Bowman et al., 2016] 3.", "startOffset": 29, "endOffset": 50}, {"referenceID": 11, "context": "2 (6) 600D BiLSTM intra-attention encoders [Liu et al., 2016] 2.", "startOffset": 43, "endOffset": 61}, {"referenceID": 19, "context": "(8) 100D LSTM with attention [Rockt\u00e4schel et al., 2015] 250k 85.", "startOffset": 29, "endOffset": 55}, {"referenceID": 22, "context": "5 (9) 300D mLSTM [Wang and Jiang, 2015] 1.", "startOffset": 17, "endOffset": 39}, {"referenceID": 6, "context": "1 (10) 450D LSTMN with deep attention fusion [Cheng et al., 2016] 3.", "startOffset": 45, "endOffset": 65}, {"referenceID": 19, "context": "The model marked with [Rockt\u00e4schel et al., 2015] is LSTMs enforcing so called word-by-word attention.", "startOffset": 22, "endOffset": 48}, {"referenceID": 22, "context": "The model in [Wang and Jiang, 2015] extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise.", "startOffset": 13, "endOffset": 35}, {"referenceID": 6, "context": "Long short-term memory-networks (LSTMN) with deep attention fusion [Cheng et al., 2016] link the current word to previous words stored in memory.", "startOffset": 67, "endOffset": 87}, {"referenceID": 22, "context": "The model of [Munkhdalai and Yu, 2016b] extends the framework in [Wang and Jiang, 2015] to a full n-ary tree model and achieved further improvement.", "startOffset": 65, "endOffset": 87}, {"referenceID": 24, "context": "We ensemble our EBIM model with syntactic tree-LSTM [Zhu et al., 2015] based on binary parse trees, and achieve significant improvement over our best sequential encoding model EBIM, obtaining an accuracy of 88.", "startOffset": 52, "endOffset": 70}, {"referenceID": 24, "context": "Each tree node is implemented with a tree-LSTM block [Zhu et al., 2015].", "startOffset": 53, "endOffset": 71}], "year": 2016, "abstractText": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is notoriously challenging but is fundamental to natural language understanding and many applications. With the availability of large annotated data, neural network models have recently advanced the field significantly. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.3% on the standard benchmark, the Stanford Natural Language Inference dataset. This result is achieved first through our enhanced sequential encoding model, which outperforms the previous best model that employs more complicated network architectures, suggesting that the potential of sequential LSTM-based models have not been fully explored yet in previous work. We further show that by explicitly considering recursive architectures, we achieve additional improvement. Particularly, incorporating syntactic parse information contributes to our best result; it improves the performance even when the parse information is added to an already very strong system.", "creator": "LaTeX with hyperref package"}}}