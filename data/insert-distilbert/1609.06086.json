{"id": "1609.06086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Modelling Stock-market Investors as Reinforcement Learning Agents [Correction]", "abstract": "decision making in uncertain and risky environments is still a greatly prominent area of research. standard economic theories fail broadly to fully explain human society behaviour, while a potentially promising alternative may lie in the direction of reinforcement learning ( rl ) theory. we analyse data for 46 players extracted from a financial market online versus game and test whether reinforcement learning ( alternatively q - learning ) could capture these players behaviour using a descriptive risk measure based on financial modeling. moreover we test an earlier hypothesis that fewer players are \" na \\ \" et ive \" ( short - sighted ). our results indicate that a simple reinforcement learning model which considers selling only the selling component of the cognitive task captures the initial decision - making process for such a subset of players but this evaluation is not sufficient to draw any conclusion exclusively on the actual population. we also find that there is not a significant improvement of fitting of the supporting players when using a simple full rl model against a myopic version, where only immediate reward is valued by the players. this indicates that weak players, if using a reinforcement learning approach, do so na \\ \" n ively", "histories": [["v1", "Tue, 20 Sep 2016 10:36:01 GMT  (260kb,D)", "http://arxiv.org/abs/1609.06086v1", "8 pages (including bibliography and appendix), 5 figures (2 in main body, 3 in appendix). IEEE EAIS 2015 Conference paper erratum"]], "COMMENTS": "8 pages (including bibliography and appendix), 5 figures (2 in main body, 3 in appendix). IEEE EAIS 2015 Conference paper erratum", "reviews": [], "SUBJECTS": "cs.CE cs.LG", "authors": ["alvin pastore", "umberto esposito", "eleni vasilaki"], "accepted": false, "id": "1609.06086"}, "pdf": {"name": "1609.06086.pdf", "metadata": {"source": "CRF", "title": "Modelling Stock-market Investors as Reinforcement Learning Agents [Correction]", "authors": ["Alvin Pastore", "Umberto Esposito", "Eleni Vasilaki"], "emails": ["apastore1@sheffield.ac.uk", "acp12ue@sheffield.ac.uk", "e.vasilaki@sheffield.ac.uk"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nOne of the most challenging fields of research is human decision-making. Understanding the processes involved and trying to predict or replicate behaviours has been, historically, the ultimate goal of many disciplines. Economics for example, has a long tradition of trying to formalise human behaviour into descriptive or normative models. These models have been employed for several years (e.g. Expected Utility model [1]) but have been proven to be inadequate [2]\u2013[5], giving rise to new research areas like behavioural and experimental economics. Psychology as well, is natively concerned with decision-making. Sequential decision problems have been used to evaluate people\u2019s risk attitude, in order to predict actual risk proneness in real life scenarios [6]\u2013[8]. While economics and psychology are focused on the high-level manifestations and implications of decision-making, neuroscience aims at understanding the biological machinery and the neural processes behind human (or animal) behaviour [9]\u2013[15].\nRecently these fields of research have started to collaborate, contributing to the rise of an emerging multi-disciplinary field called neuroeconomics [16]\u2013[20]. This discipline approaches the problem from several perspectives and on different levels of abstraction. RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27]. RL is based on the concept of reward/punishment for the actions taken. The agents act in an environment of which they possess only partial knowledge.\nTo be able to achieve the best behaviour, i.e. maximise their reward, the agents have to learn through experience and update their beliefs. Learning happens as a result of the agent\u2019s interpretation of the interactions with its surroundings and the consequences of a \u201creward\u201d feedback signal. The ability of this framework to model and therefore understand behavioural data and its underlying neural implications, is of pivotal importance in decision making [28].\nRL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31]. One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40]. These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt. Humans and animals are very advanced signal detectors whose behaviour is susceptible to changes in the rewards resulting from their choices [51], [52]. Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56]. It is crucial for the individuals to maximise their reward using the information at their disposal but to do so advantageously they need to learn which actions lead to better rewards. Decision making in uncertain environments is a challenging problem because of the competing importance of the two strategies: exploitation is, of course, the best course of action, but only when enough knowledge about the quality of the actions is available, while exploration increases the knowledge about the environment.\nA complicated task that encompasses all these features is stocks selection in financial markets, where investors have to choose among hundreds of possible securities to create their portfolio. Stock trends are non-monotonic because they are not guaranteed to achieve a global maximum and the future distribution of reward is intrinsically stochastic. After purchasing a stock, investors are faced with the decisions on when to sell it (Market timing problem [56]). To be able to achieve the best return from their investments, people need to be careful in considering how to maximise their profit in the long term and not only in a single episode. We speculate that RL is part of the decision making process for investors.\nar X\niv :1\n60 9.\n06 08\n6v 1\n[ cs\n.C E\n] 2\n0 Se\np 20\n16\nThis speculation is supported by Choi et al. [57], who studied individual investors decisions on 401(k) savings plans. Over the years, investors could decide to increase or decrease the percentage of their salary to commit to this retirement plan. Their results suggest that investors\u2019 decisions are influenced by personal experiences: they show that those investors who have experienced a positive reward from previous investment in their 401(k) fund, tend to increase the investment in that particular product, compared to those who experienced a lower reward outcome. This kind of behaviour follows a \u201cna\u0131\u0308ve reinforcement learning\u201d and is in contrast with the disposition effect [59], [65](the unwillingness of investors to sell \u201closing\u201d investments). Huang et al. investigated how personal experience in investments affects future decisions about the selection of stocks [58]. They used data that spans from 1991 to 1996, from a large discount broker. Again, the pattern of repurchasing financial product which yielded positive return was found. As Huang suggests, by understanding the way past experience affects investors\u2019 decisions, it might be possible to make predictions about the financial markets involved. RL has also been used, with promising results, to develop Stock\nMarket Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64]. While these works use RL to predict future prices, they do not try to describe human behaviour. With these notions as background we decided to investigate and try to model human choices in a stochastic, non-stationary environment. We hypothesise that RL is a component of decision making and to test this we compare two RL models against a purely random one. Our modelling attempts are based on two assumptions. First, we assume that risk is a proxy of the internal representation of the actions for some players. To test this we use a measure of systematic risk widely used in finance and economics to categorise the different choices into three discrete classes. We also assume that the reward signal is based on the cash income arising from the sales an investor makes. This assumption follows a widely researched behaviour referred to as \u201cdisposition effect\u201d in literature [59], [65], the tendency of individual investors to sell stocks which increased in value since when they were purchased, while holding onto the stocks which lost value. This phenomenon is stronger for individual investors but it also exhibited by institutional investors, such as mutual funds\nand corporations [66]\u2013[72]. Following these indications we mapped the sell transactions to a reward signal to fit our models. Finally, we hypothesise that not all players are shortsighted, to test this we compare a full RL model (3 free parameters) against a myopic model (2 free parameters, no gamma). The difference is that the latter can be considered a na\u0131\u0308ve RL as it does not take into account future rewards, it only seeks to maximise immediate rewards.\nII. METHOD"}, {"heading": "A. Dataset", "text": "The dataset has been extracted from the publicly accessible online trading simulation game VirtualTrader1, which is managed by IEX Media Group BV in the Netherlands. Players can subscribe for free and start playing the game with an assigned virtual cash budget of 100k GBP. The players will then pick the stocks they prefer from the FTSE100 stock index pool (107 stocks at the time of data collection) and create their own portfolio. These competitors are ranked according to the return of their investment. This is composed of \u201choldings\u201d and \u201ccash\u201d. The former represent the shares possessed by a player while the latter is the amount of money not invested (i.e. deriving from sold stocks or never invested). The simulation follows real world data evolution, for example price fluctuations and price splits. The delay is usually in the order of 10-15 minutes and the player can access a visual representation of the stocks time series. All the transactions are stored for each player. For this study we considered transactions that span from the 1st of January 2014 to the 31st of May 2014. This time period has been chosen because at that time the player ranking was determining the winner of the monthly prize giveaway.\n1http://www.virtualtrader.co.uk - Copyright IEX Media Group BV\nTwo possible rewards can be identified: a psychological one, consisting of the ranking position and a tangible one being the prize for the highest achiever.\nThe transactions have been stored in a database in order to be manipulated and used to fit models with different combination of free parameters. The rows are structured in 6 fields: Date, Type, Stock, Volume, Price and Total. The dataset initially contained about 100k transactions that were reduced to about 1.4k. This was due to preprocessing, which removed the many instances of inactive players who played only at the beginning and/or at the end of the time frame considered. In the final version of the dataset there are 46 players. The average amount of transactions per player is 30. The player who played the most during the six months performed 107 transactions. We considered the full amount of transactions each player operated in the game."}, {"heading": "B. Reinforcement Learning Setup", "text": "We adopted a widely used off-policy RL framework called Q-learning [21]. The learning rule of this model is:\n\u2206Q(st, at) = \u03b1 [ rt+1 + \u03b3max\na Q(st+1, a)\u2212Q(st, at)\n] (1)\nwhere Q(st, at) represents the value of action a while in state s, at time t. \u03b1 \u2208 [0, 2] is the step-size parameter and controls the rate of learning. \u03b3 \u2208 [0, 1] is the discount factor and represents how far-sighted the model is, It encodes how much a future reward is worth at time t. When \u03b3 = 0 only immediate rewards are taken into account by the player.\nTo test this framework the task has been mapped as follows. There are two states (win, loss) calculated according to the profit of the player (details in equations 6 and 7). These two states reflect the dichotomy rooted in the Prospect Theory\u2019s value function gain/loss spectrum [73].\nSince all players begin with the same initial budget our calculation of the profit uses the returns accumulated by selling stocks. This choice reduces the scope of the model, focusing on the cash component of the players assets. This will be referred to as the \u201cSell\u201d model. The actions are mapped to the stocks available for trading.\nIn order to avoid dimensionality issues, 107 stocks for 2 states give rise to 214 potential actions, we decided to classify the stocks in 3 classes of risk using a widely used financial modelling measure, CAPM Beta. The acronym stands for Capital Asset Pricing Model, a model developed by Sharpe[74] used to explain the relationship between the expected return of a security and its risk . In this report we will refer to financial volatility measure CAPM Beta as \u03b2F :\n\u03b2F = Cov(ra, rb)\nV ar(rb) (2)\nThis financial modelling measure quantifies the volatility of a security in comparison to the market or a reference benchmark [75]. Relatively safe investments like utilities stocks (e.g. gas and power) have a low \u03b2F , while high-tech stocks (e.g. Nasdaq or MSH Morgan Stanley High-Tech) have a high \u03b2F .\nAs an example, the \u03b2F of the index of reference (that represents the portion of the market considered) is exactly 1. A \u03b2F \u2208 (0, 1) indicates that the asset has a lower volatility compared to the market or low correlation of the asset price movements compared to the market. While if \u03b2F > 1 it signifies an investment with higher volatility compared to the benchmark. Following the previous example, high-tech securities with a \u03b2F > 1 could yield better returns compared to their benchmark index, when the market is going up. This also poses more risk because in case the market loses value, the security would lose value at a higher rate than the index.\n\u03b2F is considered a measure of the systematic risk and can be estimated by regression. Considering the returns of an asset a and the returns of the corresponding benchmark b:\nra = \u03b1+ \u03b2rb (3)\n\u03b2F has been calculated for each stock in the FTSE100 at the time of the game by considering daily returns in the year between 1st June 2013 and 31st May 2014. The measure associated with each stock is used to rank them and subdivide them in three classes, containing respectively 36, 36 and 35 stocks each.\nReward r at time step t+ 1 is defined by the gain (or loss) made in a sell transaction. Buying transactions are kept into account to track players portfolios and to calculate the price difference. They were not used as actions, but we might extend our modelling scenario by integrating a \u201cBuy\u201d model in the future and consider purchase actions by changing the reward scheme. The reward is calculated as:\nrt+1 = vt+1 ( pt+1 \u2212 1\nt t\u2211 i=1 vipi\n) (4)\nwhere vi and pi are the volume and the price of the stock traded at the i-th time step. The second term of the difference is a weighted average of the stock prices at previous times.\nTo avoid numerical instabilities, reward has been flattened with a sigmoid function into the range [\u22121,+1]. Specifically a hyperbolic tangent has been used, with \u03c1 = 500 to capture most of the variability of the rewards, only flattening the extreme values. This choice is in line with prospect theory value function which is concave for gains and convex for losses [73].\ntanh(r) = 1\u2212 e\u2212r/\u03c1\n1 + e\u2212r/\u03c1 (5)\nAs in this study we focused on the sell subset of the players interactions, the states are based solely on profit, which in turn is based on the reward of the sell transactions. The profit and states are defined as:\nProfit = \u2211 t tanh(rt) (6)\nState = { 0, if Profit < 0. 1, otherwise.\n(7)\nThe RL framework is composed of a learning model (eq. 1) and an action-selection model that is responsible for picking the best action. In our setup the former is Q-learning and the latter is Soft-Max. An action a is picked at time t with probability:\nP (at) = eQt(a)\u03b2\u2211n b=1 e Qt(b)\u03b2 (8)\nwhere n is the number of available actions (i.e. 3 in this study) and \u03b2 is the inverse temperature parameter and represents the greediness of the action-selection model. In the limit \u03b2 \u2192 0 the actions become equiprobable and the model reverts to random. Higher values of \u03b2 approximate a greedy model which picks the best known action (fully exploitative). The full model has 3 free parameters: \u03b1 (step-size parameter or learning rate), \u03b2 (exploration-exploitation trade-off) and \u03b3 (discount factor). For this study we used a bounded gradient descent search with 27 combinations of initial guess points. These are the combination of values of the free parameters from where the search starts. By having different entry points we hope to reduce the chance of the search getting stuck in a local minimum solution. The search has been performed with the following boundaries:\n\u2022 \u03b1 \u2208 (0.0001, 2)\n\u2022 \u03b2 \u2208 (0, 50)\n\u2022 \u03b3 \u2208 (0, 0.9999) (for the myopic model \u03b3 = 0)\nThe entry points are the combinations rising from the following values:\n\u2022 \u03b1 \u2208 {0, 0.5, 2}\n\u2022 \u03b2 \u2208 {0, 25, 50}\n\u2022 \u03b3 \u2208 {0, 0.5, 0.9999}\nThe search results have been obtained on python 2.7.9 and scipy.optimize.minimize with scipy 0.17.1."}, {"heading": "C. Model Testing Routine", "text": "Maximum Likelihood Estimate has been used as a measure of the model fitness, following Daw\u2019s comprehensive analysis of methodology [76]. MLE is the appropriate method to assess model performance because it evaluates which set of model parameters are more likely to generate the data using a probabilistic approach. Data likelihood is a powerful method because it keeps into account the presence of noise in the choices. It does so by using probability estimates for the potential actions.\nGiven a model M and its corresponding set of parameters \u03b8M the likelihood function is defined as P (D|M, \u03b8M ), where D is the dataset (the list of choices and the associated rewards). Applying Bayes\u2019 rule:\nP (\u03b8M |D,M) \u221d P (D|M, \u03b8M ) \u00b7 P (\u03b8M |M) (9)\nThe left hand side of the proportionality is the posterior probability distribution over the free parameters, given the data.\nThis quantity is proportional to the product of the likelihood of the data, given the parameters and the prior probability of the parameters. Treating the latter as flat we obtain that the most probable value for \u03b8M (the best set of free parameters) is the Maximum Likelihood Estimate (MLE), that is the set of parameter which maximises the likelihood function, P (D|M, \u03b8M ) and it is commonly denoted \u03b8\u0302M . The likelihood function is maximised through the following process. At each timestep, for every action, the observation model (Soft-Max) estimates a probability. These probabilities are then multiplied together. To avoid numerical problems that could arise when multiplying probabilities, the sum of their logarithm is calculated instead. The negative of this value, also known as Negative Log-Likelihood, is then used. The aim is then to minimise this quantity, which is the equivalent of maximising the likelihood function, P (D|M, \u03b8M ). In the future we will refer to the Negative Log-Likelihood as MLE for simplicity, keeping in mind that lower values represent better fit.\nThe values of MLE generated represent the goodness of fit of the model with its associated set of parameters. To compare the selected model with a random model and for statistical significance we adopted the Likelihood Ratio Test [77]. This statistical test uses the likelihood ratio to compare two nested models and takes into account the different number of free parameters of the two. It encapsulates this information, when testing for significance, using the difference of the two amounts as degrees of freedom for the chi-square (\u03c72) test. Since the test statistic is distributed \u03c72 it is straightforward to estimate the p-value associated with the \u03c72 value.\nThe baseline for comparison is a random model which has 0 parameters as there is no learning involved and the action-selection policy is random ( 13 chance of picking any of the three stock bins). The first comparison is between this random model and the simpler of the proposed models, which has only two free parameters (\u03b3 = 0). This setup represent the na\u0131\u0308ve learning procedure that could explain investors\u2019 behaviour showed in literature[57]. The full model, with all the three free parameters, has also been tested against the myopic model to assess whether some players are better fitted by a more complex version of the framework. Finally the model goodness of fit has been evaluated with the adopted action classification (based on risk) against 500 randomly generated stock classifications. This has been done to test the assumption that players internally classify the stock range into discrete degrees of risk."}, {"heading": "III. RESULTS", "text": "Results for the test of the hypothesis that RL is a component of decision making are shown in Fig. 1. The best set of parameters was found according to MLE through gradient descent search. The best model MLE has been compared to the random model MLE using the Likelihood Ratio Test [77]. The random model MLE is easily estimated as:\nP (D|Mrand) = log Nt\u220f 1\n3 =\nNt\u2211 log 1\n3 (10)\nwhere Nt is the number of transactions for each player in the dataset. As shown in Fig. 1 (a) and (b) 15% of the\nplayers in our dataset is better fitted by a myopic RL model as opposed to a random model. In Fig. 1 (c) and (d) we report an improvement in the fitting for some players using a full RL model against the myopic (nested) version of the model. This improvement is not reflected in the comparison of the full RL model against the random model, as shown in Fig. 1 (e) and (f). Most of the players that can be fitted with our models are well represented by a myopic model. These results follow what found by Choi et al.[57] and Huang et al. [58]. We made the assumption that players, when faced with the choice to trade many stocks (107 for this task), internally model these in discrete groups of risk using readily available information such as stock historical prices and returns, which in turn are used to estimate their volatility (\u03b2F ). To test whether this assumption holds true for the players in our dataset, we ran the simpler version of our model on the risk-ranked discretisation and on 500 independent and randomly scrambled discretisations. The results are shown in Fig. 2 and are generated using Bayesian Information Criterion (BIC) as a measure for comparison of fitness and Binomial Proportion Confidence Interval calculated with Clopper-Pearson method using Matlab 8.4.0.150421 (R2014b) function binofit. The BIC has been used as the Likelihood Ratio Test can only be used to compare nested models, while in this case the comparison is between models with the same number of parameters that are tested on different data arrangements. This procedure estimates the probability that the ranked discretisation is better than the 500 scrambled discretisations (BICranked < BICs, where BICs is the BIC for the s-th scrambled). The results shown in Fig. 2 are for 99% confidence interval. The results shown are only for those 7 players who are fitted significantly by the myopic model. As shown in Fig. 2, all the players are well above the chance threshold. This indicates that risk based on historical data could be considered a proxy for action selection for the players who are well fitted by our RL myopic model."}, {"heading": "IV. CONCLUSION", "text": "We investigated a publicly available dataset consisting of trading transactions operated by players of an investment game. We based the discretisation of the actions on the assumption that risk can capture the internal modelling that players operate when facing this task. This assumption was shown to hold true and be statistically significant for a subset of the players, 31 out of 46 and specifically for the 7 players who are best fitted by a RL model. This could signify that the remaining players might use other types of discretisation techniques based on different measures (or a combination of them) or they do not use technical analysis but fundamental analysis (e.g. using financial statements and reports). In this work, we investigated a model which combines two versions of a Reinforcement Learning framework using Q-learning as an update rule and Soft-Max as action-selection policy on a discretised action space according to the risk measure \u03b2F . It is possible that different model combinations, which use different learning rules or different measures of risk, fit the players population in our dataset better. It is also likely that, by restricting our focus on the sell model, we missed some features of what constitutes the reward signal that players receive. In the full version of the game, in fact, players might try to maximise both holdings and cash simultaneously, in order to compete in the ranking.\nThe myopic model is a nested version with only two free\nparameters, representing the learning rate (\u03b1) and the degree of greediness (\u03b2). The full version extends the simpler model with a discount factor (\u03b3) which regulates how much of the future rewards is taken into account when updating the values of present state-action pairs. 15% of the players are well fitted by a RL model with \u03b3 = 0 and there is no significant improvement of fitting by extending this model including gamma as a free parameter. Previous literature pointed in the direction of investors being na\u0131\u0308ve (short-sighted) [57], [58] and these results, albeit for a subset of the dataset, confirm this indication. The hypothesis that RL is a component of the decision making process for some investors is not confirmed as either version of the tested model (short or far-sighted) is statistically better than chance only for a subset of the players. This subset, within this population, is not large enough to draw a statistically meaningful positive result. By means of a Binomial Proportion Confidence Interval calculated with Clopper-Pearson method we get a negative result for the entire population within a 99% confidence interval (Fig. 3 in the Appendix). While this exploratory study gives some perspectives on how Reinforcement Learning can be used to model learning and action-selection for investing problems, future work will focus on different models and risk classification techniques as well as on a deeper investigation of the typical parameters of the best performing players and the correlation of different strategies and performance of stock trading together with a study of different RL models."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank their colleagues of the Sheffield Neuroeconomics interdisciplinary research group for insightful discussion."}], "references": [{"title": "Developments in Non-Expected Utility Theory: The Hunt for a Descriptive Theory of Choice under Risk.", "author": ["C. Stramer"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "and T", "author": ["S. Frederick", "G. Loewenstein"], "venue": "O\u2019Donoghue, \u201cTime Discounting and time preference: a critical review.\u201d Journal of Economic Literature, vol. XL, pp. 351\u2013401", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Anomalies: The Endowment Effect", "author": ["D. Kahneman", "J.L. Knetsch", "R.H. Thaler"], "venue": "Loss Aversion, and Status Quo Bias.\u201d Journal of Economic Perspectives, vol. 5, no. 1, pp. 193\u2013206", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "and V", "author": ["U. Hoffrage", "A. Weber", "R. Hertwig"], "venue": "M. Chase, \u201cHow to Keep Children Safe in Traffic: Find the Daredevils Early.\u201d Journal of Experimental Psychology: Applied, vol. 9, no. 4, pp. 249\u2013260", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Pleskac, \u201cDecision making and learning while taking sequential risks.", "author": ["J. T"], "venue": "Journal of Experimental Psychology: Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "and C", "author": ["T.S. Wallsten", "T.J. Pleskac"], "venue": "W. Lejuez, \u201cModeling Behavior in a Clinically Diagnostic Sequential Risk-Taking Task.\u201d Psychological Review,, vol. 112, no. 4, pp. 862\u2013880", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "and J", "author": ["K.H. Britten", "W.T. Newsome", "M.N. Shadlen", "S. Celebrini"], "venue": "a. Movshon, \u201cA relationship between behavioral choice and the visual responses of neurons in macaque MT.\u201d Visual neuroscience, vol. 13, no. 1, pp. 87\u2013100", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "and J", "author": ["K.H. Britten", "M.N. Shadlen", "W.T. Newsome"], "venue": "a. Movshon, \u201cThe analysis of visual motion: a comparison of neuronal and psychophysical performance.\u201d The Journal of neuroscience: the official journal of the Society for Neuroscience, vol. 12, no. 12, pp. 4745\u20134765", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Neural computations that underlie decisions about sensory stimuli.", "author": ["J.I. Gold", "M.N. Shadlen"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "and J", "author": ["M.N. Shadlen", "K.H. Britten", "W.T. Newsome"], "venue": "a. Movshon, \u201cA computational analysis of the relationship between neuronal and behavioral responses to visual motion.\u201d J Neurosci, vol. 16, no. 4, pp. 1486\u20131510", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Motion perception: seeing and deciding.", "author": ["M.N. Shadlen", "W.T. Newsome"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Neuroeconomics: cross-currents in research on decision-making.", "author": ["A.G. Sanfey", "G. Loewenstein", "S.M. McClure", "J.D. Cohen"], "venue": "Trends in cognitive sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Neuroeconomics: the consilience of brain and decision.", "author": ["P.W. Glimcher", "A. Rustichini"], "venue": "Science (New York, N.Y.),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "and D", "author": ["D.J. Barraclough", "M.L. Conroy"], "venue": "Lee, \u201cPrefrontal cortex and decision making in a mixed-strategy game.\u201d Nature neuroscience, vol. 7, no. 4, pp. 404\u2013410", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "and X", "author": ["A. Soltani", "D. Lee"], "venue": "J. Wang, \u201cNeural mechanism for stochastic behaviour during a competitive game.\u201d Neural Networks, vol. 19, no. 8, pp. 1075\u20131090", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement Learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "A Bradford Book, Ed. MIT Press, Cambridge, MA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Reinforcement learning: Computational theory and biological mechanisms.", "author": ["K. Doya"], "venue": "HFSP Journal,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "TD-Gammon", "author": ["G. Tesauro"], "venue": "a Self-Teaching Backgammon Program, Achieves Master-Level Play.\u201d Neural Computation, vol. 6, no. 2, pp. 215\u2013219", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight.", "author": ["P A. Abbeel", "A Coates", "Q Morgan", "Ng"], "venue": "Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Neural reinforcement learning controllers for a real robot application.", "author": ["R. Hafner", "M. Riedmiller"], "venue": "Proceedings - IEEE International Conference on Robotics and Automation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email.", "author": ["M. a. Walker"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "a. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, \u201cHuman-level control through deep reinforcement learning.\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Decision theory, reinforcement learning, and the brain.", "author": ["P. Dayan", "N.D. Daw"], "venue": "Cognitive, affective & behavioral neuroscience,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "A neural substrate of prediction and reward.", "author": ["W. Schultz", "P. Dayan", "P.R. Montague"], "venue": "Science (New York, N.Y.),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Complementary roles of basal ganglia and cerebellum in learning and motor control.", "author": ["K. Doya"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "The computational neurobiology of learning and reward.", "author": ["N.D. Daw", "K. Doya"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Learning from delayed rewards.", "author": ["C.J.C.H. Watkins"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1989}, {"title": "A menu of designs for reinforcement learning over time", "author": ["P. Werbos"], "venue": "Neural Networks for Control .\u201d Neural Networks for Control, MIT Press, Cambridge, Massachusetts, pp. 67\u201395", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1990}, {"title": "and H", "author": ["O. Hikosaka", "K. Nakamura"], "venue": "Nakahara, \u201cBasal ganglia orient eyes to reward.\u201d Journal of neurophysiology, vol. 95, no. 2, pp. 567\u2013 584", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "W", "author": ["A. Schultz"], "venue": "Romo, R, Ljungberg, T, Mirenowicz, J, Hollerman, JR, and Dickson, \u201cReward-related signals carried by dopamine neurons.\u201d in Models of Information Processing in the Basal Ganglia, M. Cambridge, Ed.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning of sequential movements by neural network model with dopamine-like reinforcement signal.", "author": ["R.E. Suri", "W. Schultz"], "venue": "Experimental Brain Research,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1998}, {"title": "a", "author": ["P. Waelti"], "venue": "Dickinson, and W. Schultz, \u201cDopamine responses comply with basic assumptions of formal learning theory.\u201d Nature, vol. 412, no. 6842, pp. 43\u201348", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "and M", "author": ["T. Satoh", "S. Nakai", "T. Sato"], "venue": "Kimura, \u201cCorrelated coding of motivation and outcome of decision by dopamine neurons.\u201d The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 23, no. 30, pp. 9913\u20139923", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "and O", "author": ["H. Nakahara", "H. Itoh", "R. Kawagoe", "Y. Takikawa"], "venue": "Hikosaka, \u201cDopamine Neurons Can Represent Context-Dependent Prediction Error.\u201d Neuron, vol. 41, no. 2, pp. 269\u2013280", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "and H", "author": ["G. Morris", "A. Nevet", "D. Arkadir", "E. Vaadia"], "venue": "Bergman, \u201cMidbrain dopamine neurons encode decisions for future action.\u201d Nature neuroscience, vol. 9, no. 8, pp. 1057\u20131063", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive critics and the basal ganglia.\u201d in Models of Information Processing in the Basal Ganglia, M", "author": ["A. Barto"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1995}, {"title": "and T", "author": ["P.R. Montague", "P. Dayan"], "venue": "J. Sejnowski, \u201cA framework for mesencephalic dopamine systems based on predictive Hebbian learning.\u201d The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 16, no. 5, pp. 1936\u20131947", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1996}, {"title": "and M", "author": ["K. Samejima", "Y. Ueda", "K. Doya"], "venue": "Kimura, \u201cRepresentation of action-specific reward values in the striatum.\u201d Science (New York, N.Y.), vol. 310, no. 5752, pp. 1337\u20131340", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "and O", "author": ["R. Kawagoe", "Y. Takikawa"], "venue": "Hikosaka, \u201cExpectation of reward modulates cognitive signals in the basal ganglia.\u201d Nature neuroscience, vol. 1, no. 5, pp. 411\u2013416", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Getting formal with dopamine and reward.", "author": ["W. Schultz"], "venue": "Neuron, vol. 36,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "and R", "author": ["J.J. Day", "M.F. Roitman", "R.M. Wightman"], "venue": "M. Carelli, \u201cAssociative learning mediates dynamic shifts in dopamine signaling in the nucleus accumbens.\u201d Nature neuroscience, vol. 10, no. 8, pp. 1020\u20131028", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "and E", "author": ["S.E. Hyman", "R.C. Malenka"], "venue": "J. Nestler, \u201cNeural mechanisms of addiction: the role of reward-related learning and memory.\u201d Annual review of neuroscience, vol. 29, pp. 565\u2013598", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "and E", "author": ["D. Joel", "Y. Niv"], "venue": "Ruppin, \u201cActor-critic models of the basal ganglia: new anatomical and computational perspectives.\u201d Neural Networks, vol. 15, no. 4-6, pp. 535\u2013547", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2002}, {"title": "and S", "author": ["J.R. Wickens", "J.C. Horvitz", "R.M. Costa"], "venue": "Killcross, \u201cDopaminergic mechanisms in actions and habits.\u201d The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 27, no. 31, pp. 8181\u20138183", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Noise characteristics and prior expectations in human visual speed perception.", "author": ["A. a. Stocker", "E.P. Simoncelli"], "venue": "Nature neuroscience,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2006}, {"title": "Decision theory: what \u201dshould\u201d the nervous system do?\u201d Science (New York", "author": ["K. K\u00f6rding"], "venue": "N.Y.), vol. 318, no. 5850, pp. 606\u2013610", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "J", "author": ["N.D. Daw"], "venue": "P. O\u2019Doherty, P. Dayan, B. Seymour, and R. J. Dolan, \u201cCortical substrates for exploratory decisions in humans.\u201d Nature, vol. 441, no. 7095, pp. 876\u2013879", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Stress, genotype and norepinephrine in the prediction of mouse behavior using reinforcement learning.", "author": ["G. Luksys", "W. Gerstner", "C. Sandi"], "venue": "Nature neuroscience,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2009}, {"title": "Sell in May and Go Away ? Learning and Risk Taking in Nonmonotonic Decision Problems.", "author": ["R. Frey", "R. Hertwig"], "venue": "Journal of Experimental Psychology,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Heuristics and Biases in Retirement Savings Behavior.", "author": ["S. Benartzi", "R.H. Thaler"], "venue": "Journal of Economic Perspectives,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}, {"title": "Reinforcement learning and savings behavior.", "author": ["J. Choi", "D. Laibson"], "venue": "The Journal of Finance,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2009}, {"title": "Industry Investment Experience and Stock Selection.", "author": ["X. Huang"], "venue": "Available at SSRN 1786271,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Are Investors Reluctant to Realize Their Losses ?\u201d vol", "author": ["T. Odean"], "venue": "LIII, no. 5, pp. 1775\u20131798", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "and J", "author": ["Y. Chen", "S. Mabu", "K. Hirasawa"], "venue": "Hu, \u201cTrading rules on stock markets using genetic network programming with sarsa learning.\u201d Proceedings of the 9th annual conference on Genetic and evolutionary computation GECCO 07, vol. 12, p. 1503", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}, {"title": "Stock price prediction using reinforcement learning.", "author": ["J. Lee"], "venue": "Industrial Electronics. Proceedings. ISIE", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2001}, {"title": "Saffell, \u201cLearning to trade via direct reinforcement.", "author": ["M.J. Moody"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2001}, {"title": "Building an artificial stock market populated by reinforcementlearning agents.", "author": ["A.V. Rutkauskas", "T. Ramanauskas"], "venue": "Journal of Business Economics and Management,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2009}, {"title": "The Behavior of Individual Investors.", "author": ["B.M. Barber", "T. Odean"], "venue": "Handbook of the Economics of Finance,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "and T", "author": ["B.M. Barber", "Y.T. Lee", "Y.J. Liu"], "venue": "Odean, \u201cIs the aggregate investor reluctant to realise losses? Evidence from Taiwan.\u201d European Financial Management, vol. 13, no. 3, pp. 423\u2013447", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2007}, {"title": "R", "author": ["P. Brown", "N. Chappel"], "venue": "Da Silva Rosa, and T. Walter, \u201cThe Reach of the Disposition Effect: Large Sample Evidence Across Investor Classes.\u201d International Review of Finance, vol. 6, no. 1-2, p. 43", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2006}, {"title": "The disposition effect and underreaction to news.", "author": ["A. Frazzini"], "venue": "Journal of Finance,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "What Makes Investors Trade?\u201d The Journal Of Finance", "author": ["M. Grinblatt", "M. Keloharju"], "venue": "vol. 56 (2), no. 2, pp. 549\u2013578", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2001}, {"title": "Psychological Factors and Stock Option Exercise.", "author": ["C. Heath", "M. Lang"], "venue": "Quarterly Journal of Economics vol. 114,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1999}, {"title": "Do Investors Trade Too Much?\u201d American Economic Review", "author": ["T. Odean"], "venue": "vol. 89 (5), pp. 1279-1298", "citeRegEx": "71", "shortCiteRegEx": null, "year": 1998}, {"title": "Patterns of behavior of professionally managed and independent investors", "author": ["Z. Shapira", "I. Venezia"], "venue": "Journal of Banking and Finance, vol. 25, no. 8, pp. 1573\u20131587", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2001}, {"title": "Prospect Theory: An Analysis of Decision under Risk.", "author": ["D. Kahneman", "A. Tversky"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1979}, {"title": "Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk,", "author": ["W. Sharpe"], "venue": "The Journal of Finance, vol. XIX,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1964}, {"title": "Trial-by-trial data analysis using computational models,\u201d in Decision Making, Affect, and Learning: Attention and Performance XXIII", "author": ["N.D. Daw"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2011}, {"title": "Phylogeny Estimation and Hypothesis Testing Using Maximum Likelihood,", "author": ["J.P. Huelsenbeck", "K. a. Crandall"], "venue": "Annual Review of Ecology and Systematics,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Expected Utility model [1]) but have been proven to be inadequate [2]\u2013[5], giving rise to new research areas like behavioural and experimental economics.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Expected Utility model [1]) but have been proven to be inadequate [2]\u2013[5], giving rise to new research areas like behavioural and experimental economics.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "Sequential decision problems have been used to evaluate people\u2019s risk attitude, in order to predict actual risk proneness in real life scenarios [6]\u2013[8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "Sequential decision problems have been used to evaluate people\u2019s risk attitude, in order to predict actual risk proneness in real life scenarios [6]\u2013[8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "While economics and psychology are focused on the high-level manifestations and implications of decision-making, neuroscience aims at understanding the biological machinery and the neural processes behind human (or animal) behaviour [9]\u2013[15].", "startOffset": 233, "endOffset": 236}, {"referenceID": 10, "context": "While economics and psychology are focused on the high-level manifestations and implications of decision-making, neuroscience aims at understanding the biological machinery and the neural processes behind human (or animal) behaviour [9]\u2013[15].", "startOffset": 237, "endOffset": 241}, {"referenceID": 11, "context": "Recently these fields of research have started to collaborate, contributing to the rise of an emerging multi-disciplinary field called neuroeconomics [16]\u2013[20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 14, "context": "Recently these fields of research have started to collaborate, contributing to the rise of an emerging multi-disciplinary field called neuroeconomics [16]\u2013[20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27].", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 21, "context": "RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27].", "startOffset": 183, "endOffset": 187}, {"referenceID": 22, "context": "The ability of this framework to model and therefore understand behavioural data and its underlying neural implications, is of pivotal importance in decision making [28].", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31].", "startOffset": 246, "endOffset": 250}, {"referenceID": 15, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 133, "endOffset": 137}, {"referenceID": 26, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 139, "endOffset": 143}, {"referenceID": 28, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 199, "endOffset": 203}, {"referenceID": 34, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 205, "endOffset": 209}, {"referenceID": 23, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 123, "endOffset": 127}, {"referenceID": 28, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 129, "endOffset": 133}, {"referenceID": 35, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 135, "endOffset": 139}, {"referenceID": 43, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 140, "endOffset": 144}, {"referenceID": 44, "context": "Humans and animals are very advanced signal detectors whose behaviour is susceptible to changes in the rewards resulting from their choices [51], [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 45, "context": "Humans and animals are very advanced signal detectors whose behaviour is susceptible to changes in the rewards resulting from their choices [51], [52].", "startOffset": 146, "endOffset": 150}, {"referenceID": 14, "context": "Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56].", "startOffset": 138, "endOffset": 142}, {"referenceID": 46, "context": "Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56].", "startOffset": 144, "endOffset": 148}, {"referenceID": 49, "context": "Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56].", "startOffset": 149, "endOffset": 153}, {"referenceID": 49, "context": "After purchasing a stock, investors are faced with the decisions on when to sell it (Market timing problem [56]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 50, "context": "[57], who studied individual investors decisions on 401(k) savings plans.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "This kind of behaviour follows a \u201cna\u0131\u0308ve reinforcement learning\u201d and is in contrast with the disposition effect [59], [65](the unwillingness of investors to sell \u201closing\u201d investments).", "startOffset": 112, "endOffset": 116}, {"referenceID": 57, "context": "This kind of behaviour follows a \u201cna\u0131\u0308ve reinforcement learning\u201d and is in contrast with the disposition effect [59], [65](the unwillingness of investors to sell \u201closing\u201d investments).", "startOffset": 118, "endOffset": 122}, {"referenceID": 51, "context": "investigated how personal experience in investments affects future decisions about the selection of stocks [58].", "startOffset": 107, "endOffset": 111}, {"referenceID": 53, "context": "RL has also been used, with promising results, to develop Stock Market Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64].", "startOffset": 87, "endOffset": 91}, {"referenceID": 55, "context": "RL has also been used, with promising results, to develop Stock Market Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64].", "startOffset": 92, "endOffset": 96}, {"referenceID": 56, "context": "RL has also been used, with promising results, to develop Stock Market Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64].", "startOffset": 147, "endOffset": 151}, {"referenceID": 52, "context": "This assumption follows a widely researched behaviour referred to as \u201cdisposition effect\u201d in literature [59], [65], the tendency of individual investors to sell stocks which increased in value since when they were purchased, while holding onto the stocks which lost value.", "startOffset": 104, "endOffset": 108}, {"referenceID": 57, "context": "This assumption follows a widely researched behaviour referred to as \u201cdisposition effect\u201d in literature [59], [65], the tendency of individual investors to sell stocks which increased in value since when they were purchased, while holding onto the stocks which lost value.", "startOffset": 110, "endOffset": 114}, {"referenceID": 58, "context": "and corporations [66]\u2013[72].", "startOffset": 17, "endOffset": 21}, {"referenceID": 64, "context": "and corporations [66]\u2013[72].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "We adopted a widely used off-policy RL framework called Q-learning [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "\u03b1 \u2208 [0, 2] is the step-size parameter and controls the rate of learning.", "startOffset": 4, "endOffset": 10}, {"referenceID": 65, "context": "These two states reflect the dichotomy rooted in the Prospect Theory\u2019s value function gain/loss spectrum [73].", "startOffset": 105, "endOffset": 109}, {"referenceID": 66, "context": "The acronym stands for Capital Asset Pricing Model, a model developed by Sharpe[74] used to explain the relationship between the expected return of a security and its risk .", "startOffset": 79, "endOffset": 83}, {"referenceID": 65, "context": "This choice is in line with prospect theory value function which is concave for gains and convex for losses [73].", "startOffset": 108, "endOffset": 112}, {"referenceID": 67, "context": "Maximum Likelihood Estimate has been used as a measure of the model fitness, following Daw\u2019s comprehensive analysis of methodology [76].", "startOffset": 131, "endOffset": 135}, {"referenceID": 68, "context": "To compare the selected model with a random model and for statistical significance we adopted the Likelihood Ratio Test [77].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "This setup represent the na\u0131\u0308ve learning procedure that could explain investors\u2019 behaviour showed in literature[57].", "startOffset": 111, "endOffset": 115}, {"referenceID": 68, "context": "The best model MLE has been compared to the random model MLE using the Likelihood Ratio Test [77].", "startOffset": 93, "endOffset": 97}, {"referenceID": 50, "context": "[57] and Huang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[58].", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Previous literature pointed in the direction of investors being na\u0131\u0308ve (short-sighted) [57], [58] and these results, albeit for a subset of the dataset, confirm this indication.", "startOffset": 87, "endOffset": 91}, {"referenceID": 51, "context": "Previous literature pointed in the direction of investors being na\u0131\u0308ve (short-sighted) [57], [58] and these results, albeit for a subset of the dataset, confirm this indication.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "Decision making in uncertain and risky environments is a prominent area of research. Standard economic theories fail to fully explain human behaviour, while a potentially promising alternative may lie in the direction of Reinforcement Learning (RL) theory. We analyse data for 46 players extracted from a financial market online game and test whether Reinforcement Learning (Q-Learning) could capture these players behaviour using a risk measure based on financial modeling. Moreover we test an earlier hypothesis that players are \u201cna\u0131\u0308ve\u201d (short-sighted). Our results indicate that a simple Reinforcement Learning model which considers only the selling component of the task captures the decision-making process for a subset of players but this is not sufficient to draw any conclusion on the population. We also find that there is not a significant improvement of fitting of the players when using a full RL model against a myopic version, where only immediate reward is valued by the players. This indicates that players, if using a Reinforcement Learning approach, do so na\u0131\u0308vely.", "creator": "LaTeX with hyperref package"}}}