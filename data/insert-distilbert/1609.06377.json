{"id": "1609.06377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Geometry-Based Next Frame Prediction from Monocular Video", "abstract": "we propose a method for next state frame prediction from video input. a symmetric convolutional recurrent neural network is trained to gradually predict depth from monocular video animation input, which, along with the current video image and the camera trajectory, can however then be similarly used to compute the next frame. unlike prior next - frame prediction approaches, basically we take advantage of changing the scene geometry and use the predicted depth for generating next frame prediction. a useful side effect of our optimal technique work is that it produces depth from video, which can be used in other applications.", "histories": [["v1", "Tue, 20 Sep 2016 22:49:34 GMT  (4218kb,D)", "http://arxiv.org/abs/1609.06377v1", null], ["v2", "Mon, 12 Jun 2017 21:52:06 GMT  (6581kb,D)", "http://arxiv.org/abs/1609.06377v2", "To appear in 2017 IEEE Intelligent Vehicles Symposium"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["reza mahjourian", "martin wicke", "anelia angelova"], "accepted": false, "id": "1609.06377"}, "pdf": {"name": "1609.06377.pdf", "metadata": {"source": "CRF", "title": "Geometry-Based Next Frame Prediction from Monocular Video", "authors": ["Reza Mahjourian", "Martin Wicke", "Anelia Angelova"], "emails": ["reza@cs.utexas.edu", "wicke@google.com,", "anelia@google.com"], "sections": [{"heading": null, "text": "We evaluate the proposed approach on the KITTI raw dataset, which is collected from a vehicle moving through urban environments. The results are compared with the state-of-theart models for next frame prediction. We show that our method produces visually and numerically superior results to existing methods that directly predict the next frame.\nI. INTRODUCTION\nScene understanding, i. e., attaching meaning to images or video, is a problem with many potential applications in computer vision, computer graphics, and robotics. We are interested in a particular test for such approaches: whether they are able to predict what happens next, i. e., given a video stream, predict the next frame in the video sequence.\nTraditionally, many such approaches have been modelbased, with strong assumptions about what kind of scenes are permissible [18], [10] e. g., a bouncing ball or a rigid object. Such assumptions lead to a parametric model of the world, which can be fitted to the observations. For example, assuming that a camera observes only one object, one can conceivably fit the degrees of freedom of the object and their rates of change to best match the observations. Then, one can use generative computer graphics to predict the next frame to be observed. While model-based methods [18], [10] perform well in restricted environments, they are not suitable for unconstrained environments.\nOn the other hand, model-free approaches do not rely on any assumption about the world and predict future frames simply based on the video stream. The simplest such techniques use a 2D optical flow field computed from the video to warp the last frame [24]. The resulting next frame prediction is not optimized for visual quality, but work well in applications such as video compression. Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.\n*This work was done at Google Brain. 1Department of Computer Science, University of Texas at Austin, Austin,\nTX 78712, USA reza@cs.utexas.edu 2Google Brain, Mountain View, CA 94043, USA wicke@google.com, anelia@google.com\nOur approach is inherently model-based, however, it produces one of the most general models possible: a depth map for the scene. This type of model has the advantage that is does not impose any assumptions on the scene and therefore does not limit its generality. To achieve that we have a model-free component by training an RNN consisting of convolutional LSTM units. These units have the ability to take into account not only the current frame, but a history of video frames of theoretically unbounded length.\nContrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame. Similar to classic modelbased approaches, we then use generative computer graphics to render the next video frame using our predicted depth map, the current video frame, and the camera trajectory (see Figures 1 and 2). We show that this yields better outcomes in terms of visual quality, as well as, quantitative metrics, namely Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) [14]. We are not aware of other approaches that use depth for next frame predictions.\nWhile the depth data is needed for training the model, it is not required at serving time. The camera trajectory can be obtained from inertial measurements and gyroscopes (best), from GPS (good for large-scale motions like boats or planes), from optical flow (probably lowest quality but always available), or from analyzing a low-resolution depth camera (which does not have to be calibrated to the camera itself, just affixed to it). It is also possible to train a model to predict the near-future trajectory of the camera given its trajectory up to the current point in time, or directly from the video stream. The most suitable source for the camera trajectory depends on the hardware and situation. But its quality would impact the quality of our results.\nWe evaluate our approach on the KITTI raw dataset [11]. The dataset includes stereo video, 3D point clouds for each frame, and the vehicle trajectory. We only use monocular video, and show that we can extract a depth stream from the monocular video and predict the next video frame with high accuracy.\nWhile we compare our approach with state-of-the-art models for next frame prediction, our approach has the side effect of producing depth from video. This is useful in applications such as robot navigation and planning, actuator control, and object recognition and tracking.\nThe main contributions of this work are: \u2022 We propose a recurrent neural network architecture\nto predict depth from a sequence of monocular video frames. The network uses convolutional LSTM units to capture the motion of objects and the background. Based\nar X\niv :1\n60 9.\n06 37\n7v 1\n[ cs\n.L G\n] 2\n0 Se\np 20\n16\non the motion patterns for different regions in the source image, the network can produce a better estimate on the scene depth. Our experimental results show that seeing more input frames improves depth predictions. \u2022 Our method creates an internal 3D model of the scene from the raw monocular video input. This 3D representation is more suitable for predicting the motion of the objects and the changes in view as a result of the viewer\u2019s own motion (ego-motion). \u2022 We propose a model for generating next frame predictions using depth predictions and the camera\u2019s trajectory. Next frame predictions are constructed using geometric projections, translations, and rotations, which are implemented as additional network layers on top of the depth prediction output. This allows the model to directly output next frame predictions. \u2022 The proposed method can be used to generate hypothetical next frame predictions as a result of exploratory or hypothetical actions. This capability allows a decisionmaking algorithm to use the next frame predictions to evaluate the potential outcomes of a set of available actions."}, {"heading": "II. RELATED WORK", "text": "Depth Prediction from Single Images. A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural networks. The pioneering work in [3] uses a multi-scale setup to predict depth at multiple resolutions. Their model uses the lower-resolution predictions as coarse starting points for finer predictions. It also uses fully-connected layers. The state-ofthe-art work in [16] uses a ResNet [12] model to improve the quality of depth predictions. We are not aware of any prior work on learning depth from a sequence of video frames, as is done in our method.\nNext Frame Prediction. Unsupervised learning from large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17]. Next frame prediction using recurrent neural networks has also been proposed in [20], [17],\n[19], [25], [8]. These methods typically use a loss function based on the RGB values of the pixels in the predicted image. This results in conservative and blurry predictions where the pixel values are close to the target values, but rarely identical to them. These models usually predict a weighted average of potential outcomes. In contrast, our proposed method produces images whose RGB distribution is very close to the target next frame. Such an output is more suitable for detecting anomalies or surprising outcomes where the predicted next frame does not match the future state.\nVisual State Prediction for Reinforcement Learning. Reinforcement Learning (RL) has been experiencing renewed interest with recent successes in game environments [4]. Being able to generate hypothetical next frames as a result of the agent\u2019s actions is useful in RL, especially in methods that use a state value approximator as part of their\n[44 , 1\n44 , 3\n2]\n5x5 s=2\n[44 , 1\n44 , 3\n2]\n[22 , 7\n2, 64\n]\n[22 , 7\n2, 64\n]\n[11 , 3\n6, 12\n8]\n[11 , 3\n6, 12\n8]\n3x3 s=2\n3x3 s=2\n[88 , 2\n88 , 3\n] 5x5 Conv-LSTM5x5 Conv-LSTM5x5 Conv-LSTM [22, 72, 64] [22, 72, 64]5x5 Conv-LSTM 3x3b=2 [44, 144, 32][44, 144, 32]3x3b=25x5 Conv-LSTM [88, 288, 1]5x5b=2LSTM states from previous frameLSTM states to next frame Fig. 3. The depth prediction RNN using convolutional LSTM cells. Themodel receives a sequence of RGB images, each with size 88\u00d7 288\u00d7 3.It produces depth predictions of size 88\u00d7 288\u00d7 1. The encoder usesconvolutions with stride two to downsize the feature maps. The decoderuses depth-to-space layers with block size two followed by convolutions with stride one to upsize the feature maps.\npolicy. Most RL applications are developed for simulated environments [19] and transferring the methods to realworld environments has been difficult. Recent work [21] uses an adversarial learning approach for next frame generation in real-world environments, generating future frames with minor modifications. Our approach produces much more realistic next frames resulting from many possible hypothetical movements by the agent."}, {"heading": "III. NEXT FRAME PREDICTION METHOD", "text": ""}, {"heading": "A. Problem Formulation", "text": "The problem that our proposed method addresses can be defined as follows. Given a sequence of RGB frames {X1,X2, . . . ,Xk\u22121}, and a sequence of camera poses {P1,P2, . . .Pk}, predict the next RGB frame Xk."}, {"heading": "B. Approach", "text": "Our proposed method predicts two depth maps Dk\u22121 and Dk corresponding to frames k\u22121 and k. The depth map Dk\u22121 is predicted directly from the sequence of images X1 . . .Xk\u22121. The depth map Dk is constructed from Dk\u22121 and the camera\u2019s ego-motion from Pk\u22121 to Pk.\nThe next frame prediction Xk is constructed from the RGB frame Xk\u22121 and the two depth maps Dk\u22121,Dk using geometric projections and transformations."}, {"heading": "C. Depth Prediction from Monocular Video", "text": "Figure 3 shows the recurrent neural network that is used for predicting depth from monocular video. The model uses convolutions with stride two without any max-pooling layers for downsizing the feature maps in the encoder. Unlike tasks like object classification, the features in this domain are not invariant to translation. So, we avoid using max-pooling layers to preserve the spatial structure of the feature maps. In the decoder, the feature maps are gradually upsized to reach the input resolution. Upsizing is done using depth-to-space layers, which spatially rearrange the activations, followed by convolutions. The model uses convolutional LSTM cells at various spatial resolutions. Convolutional LSTM cells are\nsimilar to regular LSTM cells [13], however, their gates are implemented by convolutions instead of fully-connected layers [7].\nFigure 4 shows the depth prediction model unrolled through time. At each timestep, the network receives one video frame and produces one depth prediction. Since the LSTM states are retained between subsequent frames, they enable the model to capture motion between two or more frames. The output of the LSTM cells are passed to the next layer, while their states are passed through time to the next frame. Therefore, the block processing frame i receives the input frame Xi and the LSTM states Si\u22121 as inputs, where Si is the set of LSTM states from all layers after processing frame i, and S0 = 0. Unrolling the model simplifies training. Although multiple copies of the RNN are instantiated, there is a single set of model parameters shared across the instances.\nOur model applies layer normalization [1] after each convolution or LSTM cell. In recurrent networks layer normalization performs better than batch normalization. Table I lists the architectural details on the depth prediction model.\nWe also experimented with more elaborate models, whose performance was not better than our model: 1) Adding skip connections from the encoder to the decoder. A skip connection concatenates the output of a layer in the decoder to the inputs of its corresponding similarly-sized layer in the decoder. 2) Producing and consuming intermediate low-resolution predictions as done in FlowNet [9]. The intermediate predictions were used in the loss function as well. 3) Adding a fully-connected layer plus dropout in the model bottleneck. Using a fully-connected layer resulted in overfitting.\n...Frame 0OutputInputRNN State OutputInputRNN State OutputInputRNN State OutputInputRNN State OutputInputRNN State Frame 1Frame 2Frame 3Frame 4 Fig. 4. Depth prediction model unrolled through time. At each timestep, the RNN receives one video frame and produces one depth prediction. The statesfor the LSTM cells are updated at each timestep and the updated values are used for depth prediction on subsequent frames. The output of the LSTM cellsare passed to the next layer, while their states are passed through time to the next frame. Fig. 5. Example depth and next frame predictions by our model from a four-frame sequence. The four columns show the ground truth and predictions for frames 1-4. From top to bottom in each column: 1) Ground truth frame. 2) Ground truth depth 3) Predicted depth. 4) Next frame prediction constructed using ground truth depth 5) Next frame prediction constructed using predicted depth. For frames 1-3 the ground truth next frame is visible at the top of the corresponding next column. It can be seen that the quality of depth and next frame predictions slightly improves as the model receives more video frames. After seeing only the first frame, the model believes that the ground is closer than what it actually is (visualized by a stronger red hue.) After seeing more frames, the depth estimate is improved."}, {"heading": "D. Depth Prediction Loss Function", "text": "We experimented with the L2 and reverse Huber losses. The L2 loss minimizes the squared euclidean norm between predicted depth Di and ground truth depth label Yi for frame i: L2(Di\u2212Yi) = \u2016Di\u2212Yi\u201622.\nThe reverse Huber loss is defined in [16] as:\nB(\u03b4 ) = { |\u03b4 | |\u03b4 | \u2264 c, \u03b4 2+c2\n2c |\u03b4 |> c (1)\nwhere \u03b4 = Di\u2212Yi and c = 15 maxi(D j i \u2212Y j i ) where j iterates over all pixels in the depth map. The reverse Huber loss computes the L1 norm when |\u03b4 | \u2264 c and the L2 norm otherwise.\nAdditionally, the loss equation can include an optional term to minimize the depth Gradient Difference Loss (GDL) [3], which is defined as:\nGDL(Di,Yi) = \u2211 x,y \u2223\u2223(Dx,yi \u2212Dx\u22121,yi )\u2212 (Y x,yi \u2212Y x\u22121,yi )\u2223\u22232+\u2223\u2223(Dx,yi \u2212Dx,y\u22121i )\u2212 (Y x,yi \u2212Y x,y\u22121i )\u2223\u22232 (2) where x,y iterate over pixel rows and columns in the depth map. The purpose of the GDL term is to encourage local structural similarity between predicted and ground truth depth.\nThe final loss function is formed by computing the average loss over all frames in a sequence:\nL(\u03b8) = 1 k\nk\n\u2211 i=1 \u03b1iL\u03b8 (Di,Yi) (3)\nwhere \u03b8 represents all model parameters, k is the number of frames in sequence, \u03b1i is the scaling coefficient for frame i, and L\u03b8 (Di,Yi) is equal to either L2(Di \u2212Yi) + \u03bbgdlGDL(Di,Yi) or B(Di\u2212Yi)+\u03bbgdlGDL(Di,Yi). In experiments we set \u03b1i = 1 for all i, and set \u03bbgdl to either zero or one. In all loss terms, we mask out pixels where there is no ground truth depth."}, {"heading": "E. Next Frame Prediction", "text": "The next frame prediction is generated by additional transformation layers that are added after the depth output layer (not shown in figure 4). For each frame i, the next frame prediction X \u2032i is generated using: \u2022 Video frame Xi\u22121 from the last timestep. \u2022 Depth map prediction Di\u22121 from the last timestep. \u2022 Camera poses Pi\u22121,Pi. First, the points in depth map Di\u22121 are projected into a three-dimensional point cloud C. The x,y,z coordinates of the projected points in C depend on their two-dimensional coordinates on the depth map Di\u22121 as well as their depth values. In addition to the three-dimensional coordinates, each point in C is also assigned an RGB value. The RGB value for each point is determined by its corresponding image pixel in Xi\u22121 located at the same image coordinates as the point\u2019s origin on depth map Di\u22121.\nNext, the camera\u2019s ego-motion between frames i\u22121 and i are computed from pose vectors Pi\u22121 and Pi. The computed ego-motion is six-dimensional and contains three translation components tx, ty, tz and three rotation components rx,ry,rz. Given the camera\u2019s new coordinates and principal axis, the point cloud C is projected back onto a plane at a fixed distance from the camera and orthogonal to its principal axis. Each projected point receives an updated depth value based on its newly-calculated distance to the projection plane. The result of this projection is a depth map prediction Di for frame i. Painting the projected points with their affixed RGB values creates the next frame prediction X \u2032i . Embedding the matrix multiplications that represent the necessary projections and transformations in the model allows it to directly produce next frame predictions.\nThe net effect of the two projections and the intermediate translation and rotation is to move pixels in Xi\u22121 to updated coordinates in the predicted image X \u2032i . The magnitude and direction of the movement for each pixel is a function of 1) the depth of the pixel\u2019s corresponding point in the depth maps, and 2) the magnitude and direction of ego-motion components.\nSince different pixels may move by different amounts, this process can produce overlapping pixels as well as gaps where no pixel moves to a given coordinate in the next frame prediction. The overlaps are resolved by picking the point whose depth value is smaller (closer to the camera). In our implementation the gaps are partly filled using a simple\nsplatting technique which writes each point over all four image pixels that it touches."}, {"heading": "IV. EXPERIMENTAL EVALUATION", "text": "We test our approach on the KITTI dataset [11] which is collected from a vehicle moving through urban environments. The vehicle is equipped with cameras, lidar, GPS, and inertial sensors. The dataset is organized as a series of videos with frame counts ranging from about 100 to a few thousands. For each frame, the dataset contains RGB images, 3D point clouds, and the vehicle\u2019s pose as latitude, longitude, elevation, and yaw, pitch, roll angles.\nWe split the videos into training and evaluation sets and generate 10-frame sequences from each video. In total, our dataset contains about 38000 training sequences and 4200 validation sequences."}, {"heading": "A. Generating Ground Truth Depth Maps", "text": "We generate ground truth depth maps by first transforming the point clouds using the calibration matrices in the KITTI dataset and then projecting the points onto a plane at a fixed distance from the camera. The points that are included in the depth map have depth values ranging from 3m (approximate cutoff for points visible by camera) to 80m (sensor\u2019s maximum range.) Instead of requiring the model to predict such large values, we use (3.0 / depth) as labels. We also experimented with using log(depth) as labels. Additionally, the labels are linearly scaled to values in the interval [0.25,0.75]. This normalization helps reduce the\nimbalance between the importance of accurate predictions for points that are far and near. Without normalization, loss terms like L2 can give disproportionate weights to near and far points.\nThe generated depth maps contain areas with missing depth due to a number of causes: 1) Since the camera and the lidar are at different locations on the car, there are often overlaps and shadows when the point cloud is viewed from the camera. 2) Objects that are farther than the sensor\u2019s range (80m) and objects that do not reflect the light back to the sensor (shiny objects) are not detected by the sensor. 3) Since the point clouds are sparse, there are image coordinates that usually do not line up with any point."}, {"heading": "B. Quality Metrics", "text": "We employ two image quality metrics [14] to evaluate next frame predictions produced by the model: \u2022 Peak signal-to-noise ratio (PSNR) \u2022 Structural similarity (SSIM) Both metrics are standard in measuring the quality of image predictions [19], [17], [8]. Using these metrics allows us to measure the quality of predictions independently of\nthe loss functions and depth transform functions used. The metrics are computed for pixels where the model makes a prediction."}, {"heading": "C. Training", "text": "Our model is implemented in TensorFlow [5]. We use the Adam optimizer [15] with a learning rate of 0.0001. The model weights are initialized from a Gaussian distribution with a standard deviation of 0.01. The LSTM states are initialized to 0.0 and the forget gates are initialized to 1.0. Each training timestep processed a mini-batch of eight 10- frame sequences. We use the L2 loss for training."}, {"heading": "D. Results", "text": "Figure 5 shows the outputs generated by our trained model. The figure shows depth predictions and next frame predictions from four frames of a sequence. Each column corresponds to one frame. The first row shows the ground truth last frame and the last row shows next frame predictions generated using predicted depth.\nBy comparing the quality of depth predictions for each frame it can be observed that the model\u2019s predictions are\n0 2000 4000 6000 8000 10000 12000 14000 16000 10\n15\n20\n25\n30 PSNR compared to the state\u2212of\u2212the art methods STP, DNA, CDNA\nTimestep\nP S\nN R\nSSIM compared to the state\u2212of\u2212the art methods STP, DNA, CDNA\nimproving with seeing more frames. After seeing only the first frame, the model believes that the ground is closer than what it actually is. This is reflected in the predicted depth map as the strong red hue. By receiving more input images, the model\u2019s depth prediction improves and turns more similar to ground truth.\nThis observation is also supported quantitatively in Figure 6, which shows how the quality metrics improve as a function of the number of prior frames seen by the model. The plot shows per-frame PSNR and SSIM averages over 100 mini-batches for different loss functions. As seen, for all loss functions and both metrics, the model performs better as it receives more video frames. The biggest jump in quality occurs between frames one and two, when the model gains access to the motion information in the sequence. However, the metrics continue to improve with more frames.\nWe further compare our next frame predictions using predicted depth, to predictions generated when using ground truth depth. The last two rows in Figure 5 show a comparison in visual quality. Figure 11 (shown later) plots the quality difference between these two sets of predictions using SSIM and PSNR metrics. These plots show that our model\u2019s predictions track closely the best predictions possible that are based on known depth."}, {"heading": "E. Comparison to Prior Work", "text": "To compare our model with state-of-the-art video prediction models, we trained three model variants DNA, CDNA, and STP [8] on our dataset. All the above-mentioned models are action-conditioned i. e., they have placeholders to receive\nstate and action inputs. In addition to the video images, we pass the current camera pose as the state and the computed ego-motion as the action to all three models.\nFigure 7 qualitatively compares the next frame predictions generated by our model and by the prior method of Finn et al [8]. As we can see, [8] is usually able to place the objects at the right location in the next frame. However, it produces fuzzy predictions, especially when the scene background is moving between frames. Our method, on the other hand, produces sharp and accurate predictions.\nFigure 8 quantitatively compares the predictions generated by our model with DNA, CDNA, and STP. The predictions by our method outperform prior models on both PSNR and SSIM metrics. In terms of PSNR, our model performs much better by producing results in the order of 24-25, whereas the three prior methods from [8] produce values in range 15-17. Similarly for SSIM, with a maximum possible value of 1.0, our model achieves 0.92-0.93, whereas for prior methods the value is around 0.7-0.8.\nFig. 9. Next frame simulations using ground truth depth and hypothetical ego-motions. Middle row: current frame. Other rows: Simulated next frames for moving forward/backward."}, {"heading": "F. Failure cases", "text": "We have observed cases where the depth of thin elongated objects e. g., poles are not estimated correctly. The left-most frame in Figure 5 shows an example. Since our approach is based on depth estimation, this affects the quality of our next frame predictions. The primary reason behind these errors is probably the low impact of these objects in the loss equation. Another contributing factor is the imperfect alignment of depth maps and video frames in the training data, which affects thin objects more. These misalignments are primarily due to varying time delays between the rotating lidar and the camera for different regions of the image."}, {"heading": "G. Simulating Hypothetical Next Frames", "text": "Our approach can be used to generate potential next frames based on hypothetical motions of a moving agent e.g. a vehicle or a pedestrian. Such next frame simulations can be useful for exploring counterfactual world representations and for exploring the outcome of available actions in planning. Figures 9, 10 show example next frame simulations based on a range of hypothetical ego-motions corresponding to moving forward/backward and sideways. The frames shown are generated using ground truth depth. These results are best viewed as an animation. Please see the accompanying video."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "We have presented a method for predicting the next frame from monocular video. Our method uses an RNN that is trained to predict depth from a sequence of images. Our\nexperiments show that the RNN can capture the motion between subsequent frames and improve its depth predictions.\nWe would like to improve the visual quality of the predictions by 1) upsampling and inpainting ground truth depth maps 2) inpainting next frame predictions where possible. Furthermore, predicting multiple frames into the future is a useful extension to this work. Applying our approach to anomaly detection will be an important next step. For example, we can superimpose our next frame prediction with the actually observed frame and analyze the mismatches in the scene topology (depth) or appearance (RGB frame). Large mismatches may be an indication of an object moving with an unexpected velocity, and can be used as informing signals for safer navigation."}], "references": [{"title": "Single-image depth perception in the wild", "author": ["W. Chen", "Z. Fu", "D. Yang", "J. Deng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "In ICCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. Abadi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Towards unified depth and semantic prediction from a single image", "author": ["P. Wang"], "venue": "In CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning predictive visual models of physics for playing billiards", "author": ["K. Fragkiadaki", "P. Agrawal", "S. Levine", "J. Malik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Vision meets robotics: The kitti dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "IJRR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Long short-temp memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Image quality metrics: Psnr vs. ssim", "author": ["A. Hore", "D. Ziou"], "venue": "In Int. Conf. on Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deeper depth prediction with fully convolutional residual networks", "author": ["I. Laina", "C. Rupprecht", "V. Belagiannis", "F. Tombari", "N. Navab"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Modeling deep temporal dependencies with recurrent grammar cells", "author": ["V. Michalski", "R. Memisevic", "K. Konda"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R. L Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning a driving simulator", "author": ["E. Santana", "G. Hotz"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Anticipating visual representations from unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Dense optical flow prediction from a static image", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "In ICCV,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Probabilistic modeling of future frames from a single image", "author": ["T. Xue", "J. Wu", "K. Bouman", "Freeman W"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Traditionally, many such approaches have been modelbased, with strong assumptions about what kind of scenes are permissible [18], [10] e.", "startOffset": 124, "endOffset": 128}, {"referenceID": 8, "context": "Traditionally, many such approaches have been modelbased, with strong assumptions about what kind of scenes are permissible [18], [10] e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "While model-based methods [18], [10] perform well in restricted environments, they are not suitable for unconstrained environments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "While model-based methods [18], [10] perform well in restricted environments, they are not suitable for unconstrained environments.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "The simplest such techniques use a 2D optical flow field computed from the video to warp the last frame [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "We show that this yields better outcomes in terms of visual quality, as well as, quantitative metrics, namely Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) [14].", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "We evaluate our approach on the KITTI raw dataset [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 24, "endOffset": 27}, {"referenceID": 14, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "The pioneering work in [3] uses a multi-scale setup to predict depth at multiple resolutions.", "startOffset": 23, "endOffset": 26}, {"referenceID": 14, "context": "The state-ofthe-art work in [16] uses a ResNet [12] model to improve the quality of depth predictions.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "The state-ofthe-art work in [16] uses a ResNet [12] model to improve the quality of depth predictions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Next frame prediction using recurrent neural networks has also been proposed in [20], [17], Fig.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Next frame prediction using recurrent neural networks has also been proposed in [20], [17], Fig.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "[19], [25], [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[19], [25], [8].", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "[19], [25], [8].", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "Reinforcement Learning (RL) has been experiencing renewed interest with recent successes in game environments [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "[44 , 1 44 , 3 2] 5x5 s=2", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "[44 , 1 44 , 3 2] 5x5 s=2", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 18, "endOffset": 33}, {"referenceID": 5, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 18, "endOffset": 33}, {"referenceID": 0, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 18, "endOffset": 33}, {"referenceID": 20, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 0, "endOffset": 15}, {"referenceID": 9, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 1, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 4, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 10, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 6, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 9, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 1, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 10, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 6, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 0, "context": "[88 , 2 88 , 3 ] 5x5 Conv-LSTM5x5 Conv-LSTM5x5 Conv-LSTM [22 72 64 [22 72 64 5x5 Conv-LSTM 3x3 b=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 1, "context": "[88 , 2 88 , 3 ] 5x5 Conv-LSTM5x5 Conv-LSTM5x5 Conv-LSTM [22 72 64 [22 72 64 5x5 Conv-LSTM 3x3 b=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 17, "context": "Most RL applications are developed for simulated environments [19] and transferring the methods to realworld environments has been difficult.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "Recent work [21] uses an adversarial learning approach for next frame generation in real-world environments, generating future frames with minor modifications.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "similar to regular LSTM cells [13], however, their gates are implemented by convolutions instead of fully-connected layers [7].", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "similar to regular LSTM cells [13], however, their gates are implemented by convolutions instead of fully-connected layers [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "2) Producing and consuming intermediate low-resolution predictions as done in FlowNet [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 14, "context": "The reverse Huber loss is defined in [16] as:", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "Additionally, the loss equation can include an optional term to minimize the depth Gradient Difference Loss (GDL) [3], which is defined as:", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "We test our approach on the KITTI dataset [11] which is collected from a vehicle moving through urban environments.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "Comparison of next frame prediction by our method with state-of-the-art video prediction model STP [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 12, "context": "We employ two image quality metrics [14] to evaluate next frame predictions produced by the model:", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "Both metrics are standard in measuring the quality of image predictions [19], [17], [8].", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "Both metrics are standard in measuring the quality of image predictions [19], [17], [8].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "Both metrics are standard in measuring the quality of image predictions [19], [17], [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Our model is implemented in TensorFlow [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "We use the Adam optimizer [15] with a learning rate of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": "Comparison of quality metrics on next frame predictions by our model against STP, DNA, and CDNA methods [8] over the validation dataset (higher values are better for both metrics).", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "tion models, we trained three model variants DNA, CDNA, and STP [8] on our dataset.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Figure 7 qualitatively compares the next frame predictions generated by our model and by the prior method of Finn et al [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "As we can see, [8] is usually able to place the objects at the right location in the next frame.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "In terms of PSNR, our model performs much better by producing results in the order of 24-25, whereas the three prior methods from [8] produce values in range 15-17.", "startOffset": 130, "endOffset": 133}], "year": 2016, "abstractText": "We propose a method for next frame prediction from video input. A convolutional recurrent neural network is trained to predict depth from monocular video input, which, along with the current video image and the camera trajectory, can then be used to compute the next frame. Unlike prior next-frame prediction approaches, we take advantage of the scene geometry and use the predicted depth for generating next frame prediction. A useful side effect of our technique is that it produces depth from video, which can be used in other applications. We evaluate the proposed approach on the KITTI raw dataset, which is collected from a vehicle moving through urban environments. The results are compared with the state-of-theart models for next frame prediction. We show that our method produces visually and numerically superior results to existing methods that directly predict the next frame.", "creator": "LaTeX with hyperref package"}}}