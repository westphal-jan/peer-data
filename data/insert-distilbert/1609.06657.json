{"id": "1609.06657", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)", "abstract": "visual question answering ( vqa ) task has showcased a new stage of interaction between language and vision, two of the most pivotal primary components of artificial intelligence. however, it has evolved mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. we should introduce additional full - sentence visual question and answering ( fsvqa ) dataset, consisting combined of nearly 1 million pairs of questions and full - sentence answers for images, built by applying a number of rule - based natural language output processing techniques to original vqa dataset and captions in the ms coco dataset. this design poses many additional complexities to conventional vqa task, and we provide a baseline for approaching and evaluating the task, on top of actions which we only invite the research community to build further improvements.", "histories": [["v1", "Wed, 21 Sep 2016 18:12:04 GMT  (6458kb,D)", "http://arxiv.org/abs/1609.06657v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["andrew shin", "yoshitaka ushiku", "tatsuya harada"], "accepted": false, "id": "1609.06657"}, "pdf": {"name": "1609.06657.pdf", "metadata": {"source": "CRF", "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)", "authors": ["Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "emails": [], "sections": [{"heading": "Introduction", "text": "The research community in artificial intelligence (AI) has witnessed a series of dramatic advances in the AI tasks concerning language and vision in recent years, thanks to the successful applications of deep learning techniques, particularly convolutional neural networks (CNN) and recurrent neural networks (RNN). AI has moved on from naming the entities in the image (Mei et al. 2008; Wang et al. 2009), to describing the image with a natural sentence (Vinyals et al. 2015; Xu et al. 2015; Karpathy and Li 2015) and then to answering specific questions about the image with the advent of visual question answering (VQA) task (Antol et al. 2015).\nHowever, current VQA task is focused on generating a short answer, mostly single words, which does not fully take advantage of the wide range of expressibility inherent in human natural language. Just as we moved from merely naming entities in the image to description of the images with natural sentence, it naturally follows that VQA will also move towards full-sentence answers. One way to tackle this issue would be to apply appropriate linguistic rules after a single-word answer is generated. However, previous works in natural language processing field have demonstrated that data-driven response generation achieves better performance than rule-based generation (Ritter, Cherry, and Dolan 2011). In other words, it is more efficient to provide data only once\nQ: What is the color of the cat? Q: Does the man have a mustache? VQA: gray VQA: no \u2192FSVQA: The color of the cat is gray. \u2192FSVQA: No, the man does not have a mustache.\nfor pre-training than to parse and tag the text every time to apply universal rules. In addition, training with full-sentence answers provides an opportunity for the learning of complex morphological transformations along with visual and semantic understanding, which cannot be done with manual application of rules.\nLearning and generating full-sentence answers will inevitably increase the number of distinct answers at an exponential scale; for example, thousands of samples with simple identical answer \u201cyes\u201d will be further divided into \u201cyes, the color of the car is red,\u201d\u201cyes, the boy is holding a bat,\u201d etc. Indeed, our FSVQA dataset contains almost 40 times more\nar X\niv :1\n60 9.\n06 65\n7v 1\n[ cs\n.C V\n] 2\n1 Se\np 20\n16\nanswers that are unique than the original VQA dataset. This poses additional challenges on top of original VQA task, since now it not only has to come up with the correct answer, but also has to form a full sentence considering how the words are conjugated, inflected, and ordered.\nWe introduce Full-Sentence Visual Question Answering (FSVQA) dataset, built by applying linguistic rules to original VQA dataset at zero financial cost. We also provide an augmented version of FSVQA by converting image captions to question and answers. We examine baseline approaches, and utilize complementary metrics for evaluation, providing a guideline upon which we invite the research community to build further improvements.\nOur primary contributions can be summarized as following: 1) introducing a novel task of full-sentence visual question answering, 2) building a large, publicly available dataset consisting of up to 1 million full-sentence Q&A pairs, and 3) examining baseline approaches along with a novel combination of evaluation metrics."}, {"heading": "Related Work", "text": "A number of datasets on visual question answering have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros, and Zemel 2015), among which (Antol et al. 2015) in particular has gained the most attention and helped popularize the task. However, these datasets mostly consist of a small set of answers covering most of the questions, and most of the answers being single word. Our FSVQA dataset, derived from (Antol et al. 2015), minimizes such limitation by converting the answers to full-sentences, thus widely expanding the set of answers.\n(Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representations. This approach won the 1st place in 2016 VQA Challenge in real images category. (Saito et al. 2016) proposed DualNet, in which both addition and multiplication of the input features are performed, in order to fully take advantage of the discriminative features in the data. This method won the 1st place in 2016 VQA Challenge in abstract scenes category.\n(Yang et al. 2016) was one of the first to propose attention model for VQA. They proposed stacked attention networks (SANs) that utilize question representations to search for most relevant regions in the image. (Noh and Han 2016) also built an attention-based model, which optimizes the network by minimizing the joint loss from all answering units. They further-proposed an early stopping strategy, in which overfitting units are disregarded in training.\n(Lu et al. 2016) argued that not only visual attention is important, but also question attention is important. Coattention model was thus proposed to jointly decide where to attend visually and linguistically. (Kim et al. 2016) introduced multimodal residual network (MRN), which uses element-wise multiplication for joint residual learning of attention models.\nMost of the works above limited the number of possible answers, which was possible due to a small number of answers covering the majority of the dataset. Our\nFSVQA dataset imposes additional complexity to existing approaches by having a much larger set of possible answers, in which no small set of labels can cover the majority of the dataset."}, {"heading": "Dataset", "text": "Collecting full-sentence annotations from crowd-sourcing tools can be highly costly. We circumvent this financial cost by converting the answers in the original VQA dataset (Antol et al. 2015) to full-sentence answers by applying a number of linguistic rules using natural language processing techniques. Furthermore, we also provide an augmented version of dataset by converting the human-written captions provided in the MS COCO (Lin et al. 2014). We generated questions with a set of rules, for which the caption itself becomes the answer. Both versions of FSVQA dataset along with the features used in our experiment, as will be described in the Experiment Section, are publicly available for download. Note that, for both versions, only train and validation splits are provided, since test splits are not publicly available. Also, we only provide open-ended version, and do not provide multiple choice version.\nConverting VQA VQA dataset comes with 10 annotations per question, and we chose one annotation per question that has the highest frequency as the single answer for corresponding question. If par, one annotation was randomly selected.\nVQA dataset mainly consists of three categories of questions; yes/no, number, and others. Table 1 summarizes the general conversion rule for generating full-sentence answers for each category, along with examples. Part-of-speech tag notation follows that of PennTree I Tags (Marcus et al. 1994), except NP and VP refer to parse tree instead of than part-of-speech. tense:V \u2192 T returns the tense of the input verb, conjug:V \u00d7 T \u2192 V conjugates the input verb to the input tense, where V is a space of verbs of all forms, and T is a set of tenses such that T={past, present, future, past perfect, present perfect, future perfect}, except it returns the input as is if the input is of JJ tag. negate:V \u2192 V negates the input verb of given tense, and replace(A,B) substitutes A by B. Parentheses indicate an optional addition, while A/B indicates an insertion of one of the two sides depending on the question. To briefly illustrate the process, these general conversion rules substitute the question phrase with the answer, and reorder the sentence with appropriate conjugation and negation.\nWhile these general rules cover the majority of the questions, some types of questions require additional processing. For example, conditional statements with \u201cif,\u201d or selective statements such as \u201cwho is in the picture, A or B?,\u201d can be handled by disregarding the sub-clauses and applying the rules to the main clause. Also, since VQA dataset consists of human-written natural language, it inevitably contains variations encompassing colloquialism, typos, grammar violations, and abridgements, which make it difficult to apply any type of general conversion rule. We either manually modify them, or leave them as they are."}, {"heading": "Converting Captions", "text": "We also provide an augmented version of the dataset by converting the human-written captions for images into questions and answers. Apart from yes/no questions, the answers to the generated questions are the captions themselves, eliminating the burden for generating reliable answers. Most images in MS COCO come with 5 captions, and we generated distinct question for each caption, whose conversion rule is shown in Table 2.\nWe assigned at least two yes/no questions with one \u201cyes\u201d and one \u201cno\u201d to all images, roughly balancing the number of answers with \u201cyes,\u201d and \u201cno.\u201d Questions with affirmative answers involving \u201cyes\u201d were generated by simply rephrasing the caption such that the question asks to confirm the contents in the caption, for which the answer is an affirmative statement of the question (which is the caption itself),\naccompanied by \u201cYes,\u201d in the beginning.\nQuestions with non-affirmative answers involving \u201cno\u201d were generated by substituting parts of captions with random actions or agents. For agents, we randomly choose one class from 1,000 object classes of ILSVRC 2014 object detection task, and substitute it for given agent in the caption. For actions, we randomly choose one class from 101 action classes of UCF-101 (Soomro, Zamir, and Shah 2012) plus 20 classes of activities of daily living (ADL) from (Ohnishi et al. 2016), and substitute it for given verb phrase. Resulting questions are frequently of interesting non-sensical type, such as \u201care the birds doing push-ups on the tree?,\u201d for which the answer is simply a negation of the question, accompanied by \u201cNo,\u201d in the beginning. We expect that such set of questions and answers can also potentially help the learning of distinction between common sense and nonsense.\nWe also generated questions that are category-specific. When an object or a property of a specific category is referred to, we replace it with the category name, preceded by wh-determiner or wh-pronoun, to ask which object or property it belongs to. Our manually pre-defined categories included color, animal, room, food, transportation, and sport. Other rules are mostly reversed process of the conversion rules in Table 1, in which we deliberately mask certain content of the caption, replace it with appropriate question forms, and reorder the sentence."}, {"heading": "Statistics", "text": "Table 3 shows statistics for original VQA dataset and two versions of FSVQA dataset. Both versions of FSVQA contain much longer answers on average, and the number of unique answers is more than ten times larger in the regular version and about 38 times larger in the augmented version compared to VQA dataset. Augmented version is longer since captions in MS COCO tend to be longer than questions in VQA. The size of vocabularies is also many times larger in both versions. Note that, consistently with the conversion process, only the most frequent answer from 10 answers per question was taken into consideration for VQA dataset\u2019s statistics.\nComparing the coverage of 1,000 most frequent answers in the dataset shows even more striking contrast. In VQA dataset, 1,000 most frequent answers covered about 86.5% of the entire dataset, so that learning a small set of frequent answers could perform reasonably well. In FSVQA, the coverage by 1,000 most frequent answers is merely 12.7% and 4.8% for regular and augmented version respectively, which is clearly much less than VQA dataset. Thus, it adds a critical amount of computational complexity, in which less frequent answers cannot easily be disregarded.\nTable 4 shows the number of unique answers for each category. While FSVQA has much more answers in all cate-\ngories, most striking example is in the yes/no category. VQA dataset essentially contains only two answers \u201cyes\u201d and \u201cno\u201d for yes/no questions (infrequent answers such as \u201cnot sure\u201d were filtered out in the conversion process). In fact, answering with \u201cyes\u201d alone for all questions achieved 70.97% accuracy for yes/no category in the original VQA dataset. On the contrary, FSVQA datasets contain approximately 49,000 times more answer and 240,000 times more answers for yes/no category in each version. It becomes clear again that FSVQA cannot be taken advantage of by manipulating a small set of frequent answers.\nFigure 2 shows the percentage distribution of answers in the respective datasets for number of words in the answers. While over 90% of the answers in VQA dataset are single words, both versions of FSVQA dataset show much smoother distribution over a wide range of number of words.\nExperiment"}, {"heading": "Setting", "text": "We used 4096-dimensional features from the second fullyconnected layer (fc7) of VGG (Simonyan and Zisserman 2014) with 19 layers, trained on ImageNet (Deng et al. 2009), as our image features. Words in the question were input to LSTM (Hochreiter and Schmidhuber 1997) one at a time as one-hot vector, where the dictionary contains only the words appearing more than once. Image features and question features are then mapped to common embedding space as a 1,024-dimensional vector. Batch size was 500 and training was performed for 300 epochs.\nWe trained only with the answers that appear twice or more in train split, as using all unique answers in the dataset fails to run, with required memory far beyond the capacity of most of the contemporary GPUs, NVIDIA Tesla 40m in our case. 20,130 answers appear more than once in regular version, covering 95,340 questions from 62,292 images, and 23,400 answers appear more than once in the augmented version, which cover 105,563 questions from 64,060 images. This is only about 25% and 15.5% of the entire train split in respective version, which again shows a striking contrast with the original VQA dataset, in which only 1,000 answers covered up to 86.5% of the dataset.\nFollowing (Antol et al. 2015), we examined the effect of\nremoving images, since questions alone may frequently provide sufficient amount of clue to correct answers. Training procedure is identical as above, except only question features are mapped to the common embedding space since there are no image features.\nConversely, we also examined an approach where only image features are concerned. This requires a slightly different training procedure, as it does not involve a series of onehot vector inputs. We followed the conventional approach used in image captioning task (Vinyals et al. 2015), where the image features are fixed, and a stack of LSTM units learns to generate the ground truth captions. Each LSTM unit generates one word at a time, which in turn enters the next LSTM unit. The only difference in our case is that the ground truth captions are replaced by full-sentence answers."}, {"heading": "Evaluation", "text": "Metrics Evaluating the results from the experiments also poses a challenge. Since original VQA dataset consisted mostly of short answers, evaluation was as simple as matching the results with ground truths, and yielding the percentage. Yet, since we now have full-sentence answers, simply matching the results with ground truths will not be compatible. For example, \u201cyes, the color of the cat is red\u201d for the question in which the original ground truth is \u201cyes,\u201d will be classified as incorrect using the current evaluation tool for original VQA.\nWe thus come up with a set of mutually complementary ways of evaluating full-sentence answers. First, we employ the frequently used evaluation metrics for image captioning task, namely BLEU (Papineni et al. 2002), METEOR (Denkowski and Lavie 2014), and CIDEr (Vedantam, Zitnick, and Parikh 2015). The goal is to quantify the overall resemblance of the results to the ground truth answers.\nHowever, higher performance on these evaluation metrics does not necessarily imply that it is a more accurate answer. For instance, \u201cthe color of the car is red\u201d will have higher scores than \u201cit is blue\u201d for a ground-truth answer \u201cthe color of the car is blue,\u201d due to more tokens being identical. Yet, the former is clearly an incorrect answer, whereas the latter should be considered correct. In order to overcome this drawback, we also employ a simple complementary evaluation metric, whose procedure is as follows: we examine whether the short answer from the original dataset is present in the generated result, and extract the short answer if present. If not, we leave the answer blank. Extracted terms in this way are tested with the evaluation tool for original VQA. Using the previous example, the rationale is that as long as the original short answer \u201cblue\u201d is present in the generated result, it can be assumed that the answer is correct. We refer to this metric as VQA accuracy. Note that, for augmented version, generated answers for only the original subset are extracted to measure VQA accuracy, since there are no ground truth VQA answers for the augmented segment.\nHowever, there also exist cases in which VQA accuracy can be misleading, since the rest of the context may not be compatible with the question. For example, \u201cyes, the color is blue.\u201d will be considered correct if the original answer is \u201cyes,\u201d but it should not be considered correct if the question was \u201cis it raining?\u201d In fact, this was one of the underlying concerns in the original VQA dataset, since we cannot be sure whether \u201cyes\u201d or \u201cno\u201d was generated in the right sense or purely by chance. In order to alleviate this issue, we also report FSVQA accuracy, which is the percentage in which the ground truth answer in FSVQA dataset contains the generated answer. Since the answers have to be matched at the sentence level, it assures us with high confidence that the answer was correct in the intended context.\nResults & Discussion Results for all metrics are shown in Table 5. Note that evaluation is performed on the results for validation split, since ground truths for test split are not publicly available. While each metric is concerned with slightly different aspect of the answers, results shows that they gen-\nerally tend to agree with each other. Figure 3 shows examples of generated full-sentence answers for each model, along with the ground truth answer from the original VQA dataset.\nAs expected, answers generated from using both question and image features turn out to be most reliable. Answers from question features alone result in answers that match the questions but are frequently out of visual context given by the image. Likewise, answers generated from image features alone fit the images but are frequently out of textual context given by the question. It is notable that using image features alone performs very poorly, whereas using question features alone results in performances comparable to using both features. One plausible explanation is that, since using image features alone always generates the same answer for the same image regardless of the question, it can only get 1 out of k questions correctly at best, where k is the number of questions per image. On the contrary, using question features alone essentially reduces the problem to a semantic Q&A task, which can be handled one at a time. This tendency is consistent with the results reported in (Antol et al. 2015). It must nevertheless be reminded that the best performances in both (Antol et al. 2015) and our experiment were achieved with the presence of both visual and textual clues."}, {"heading": "Conclusion", "text": "We introduced FSVQA, a publicly available dataset consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying linguistic rules to existing datasets. While pushing forward the VQA task to a more human-like stage, it poses many extra complexities. We examined baseline approaches for tackling this novel task. Applying some of the successful approaches from the original VQA task, such as attention mechanism, will be an intriguing and important future work. Whether generative approach can play more role in the future, as the number of answers grows larger and classification approach becomes less efficient, is also of interest. We invite the research community to come up with an innovative and efficient way to improve the performance on FSVQA."}, {"heading": "Acknowledgement", "text": "This work was funded by ImPACT Program of Council for Science, Technology and Innovation (Cabinet Office, Government of Japan)."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Antol"], "venue": null, "citeRegEx": "Antol,? \\Q2015\\E", "shortCiteRegEx": "Antol", "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["Deng"], "venue": null, "citeRegEx": "Deng,? \\Q2009\\E", "shortCiteRegEx": "Deng", "year": 2009}, {"title": "and Lavie", "author": ["M. Denkowski"], "venue": "A.", "citeRegEx": "Denkowski and Lavie 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["Fukui"], "venue": null, "citeRegEx": "Fukui,? \\Q2016\\E", "shortCiteRegEx": "Fukui", "year": 2016}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Li", "author": ["A. Karpathy"], "venue": "F.", "citeRegEx": "Karpathy and Li 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Residual Learning for Visual QA", "author": ["Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2016\\E", "shortCiteRegEx": "Kim", "year": 2016}, {"title": "C", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollr", "Zitnick"], "venue": "L.", "citeRegEx": "Lin et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["Lu"], "venue": null, "citeRegEx": "Lu,? \\Q2016\\E", "shortCiteRegEx": "Lu", "year": 2016}, {"title": "and Fritz", "author": ["M. Malinowski"], "venue": "M.", "citeRegEx": "Malinowski and Fritz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Penn Treebank: annotating predicate argument structure", "author": ["Marcus"], "venue": "In HLT", "citeRegEx": "Marcus,? \\Q1994\\E", "shortCiteRegEx": "Marcus", "year": 1994}, {"title": "Coherent Image Annotation by Learning Semantic Distance", "author": ["Mei"], "venue": null, "citeRegEx": "Mei,? \\Q2008\\E", "shortCiteRegEx": "Mei", "year": 2008}, {"title": "and Han", "author": ["H. Noh"], "venue": "B.", "citeRegEx": "Noh and Han 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Recognizing activities of daily living with a wrist-mounted camera", "author": ["Ohnishi"], "venue": null, "citeRegEx": "Ohnishi,? \\Q2016\\E", "shortCiteRegEx": "Ohnishi", "year": 2016}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Papineni"], "venue": null, "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Exploring Models and Data for Image Question Answering", "author": ["Kiros Ren", "M. Zemel 2015] Ren", "R. Kiros", "R. Zemel"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Data-Driven Response Generation in Social Media", "author": ["Cherry Ritter", "A. Dolan 2011] Ritter", "C. Cherry", "B. Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Saito"], "venue": null, "citeRegEx": "Saito,? \\Q2016\\E", "shortCiteRegEx": "Saito", "year": 2016}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv:1212.0402", "author": ["Zamir Soomro", "K. Shah 2012] Soomro", "A. Zamir", "M. Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "C", "author": ["Vedantam, R.", "Zitnick"], "venue": "L.; and Parikh, D.", "citeRegEx": "Vedantam. Zitnick. and Parikh 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals"], "venue": null, "citeRegEx": "Vinyals,? \\Q2015\\E", "shortCiteRegEx": "Vinyals", "year": 2015}, {"title": "Multi-Label Sparse Coding for Automatic Image Annotation", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2009\\E", "shortCiteRegEx": "Wang", "year": 2009}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Xu"], "venue": "In ICML", "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2016\\E", "shortCiteRegEx": "Yang", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Visual Question Answering (VQA) task has showcased a new stage of interaction between language and vision, two of the most pivotal components of artificial intelligence. However, it has mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. We introduce Full-Sentence Visual Question Answering (FSVQA) dataset (www.mi.t.u-tokyo.ac. jp/static/projects/fsvqa), consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset. This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements.", "creator": "LaTeX with hyperref package"}}}