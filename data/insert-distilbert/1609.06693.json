{"id": "1609.06693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "deep neural networks are learning models with a very high capacity and therefore prone to over - fitting. many regularization techniques such as dropout, dropcon - nect, and slow weight decay all attempt to solve solving the problem of over - fitting by simultaneously reducing the capacity of their respective models ( srivastava et al., 2014 ), ( wan et al., 2013 ), ( krogh & amp ; hertz, 1992 ). in this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over - fitting without sacrificing the capacity of the same model. the mistakes that models clearly make shown in early stages of training carry information about the learning problem. by adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft - targets we achieved a regularization scheme as powerful as average dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. softtarget regularization hence proved to be an remarkably effective tool point in various neural network architectures.", "histories": [["v1", "Wed, 21 Sep 2016 19:31:07 GMT  (801kb,D)", "https://arxiv.org/abs/1609.06693v1", null], ["v2", "Thu, 22 Sep 2016 00:23:30 GMT  (801kb,D)", "http://arxiv.org/abs/1609.06693v2", null], ["v3", "Sat, 3 Dec 2016 08:08:53 GMT  (805kb,D)", "http://arxiv.org/abs/1609.06693v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["armen aghajanyan"], "accepted": false, "id": "1609.06693"}, "pdf": {"name": "1609.06693.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Armen Aghajanyan"], "emails": ["armen.aghajanyan@dimensionalmechanics.com"], "sections": [{"heading": null, "text": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures."}, {"heading": "1 INTRODUCTION", "text": "Many regularization techniques have been created to rectify the problem of over-fitting in deep neural networks, but the majority of these methods reduce models capacities to force them to learn general enough features. For example, Dropout reduces the amount of learn-able parameters by randomly dropping activations, and DropConnect extends this idea by randomly dropping weights (Srivastava et al., 2014), (Wan et al., 2013). Weight decay regularization reduces the capacity of the model, not by dropping learn-able parameters, but by reducing the space of viable solutions (Krogh & Hertz, 1992)."}, {"heading": "1.1 MOTIVATION", "text": "Hinton has shown that soft-labels, or labels predicted from a model contain more information that binary hard labels due to the fact that they encode similarity measures between the classes (Hinton et al., 2015). Incorrect labels tagged by the model describe co-label similarities, and these similarities should be evident in future stages of learning, even if the effect is diminished. For example, imagine training a deep neural net on a classification dataset of various dog breeds. In the initial few stages of learning the model will not accurately distinguish between similar dog-breeds such as a Belgian Shepherd versus a German Shepherd. This same effect, although not so exaggerated, should appear in later stages of training. If, given an image of a German Shepherd, the model predicts the class German Shepherd with a high-accuracy, the next highest predicted dog should still be a Belgian Shepherd, or a similar looking dog. Over-fitting starts to occur when the majority of these co-label effects begin to disappear. By forcing the model to contain these effects in the later stages of training, we reduced the amount of over-fitting.\nar X\niv :1\n60 9.\n06 69\n3v 3\n[ cs\n.L G\n] 3\nD ec\n2 01"}, {"heading": "1.2 METHOD", "text": "Consider the standard supervised learning problem. Given a dataset containing inputs and outputs, X and Y , a regularization function R and a model prediction function F we attempted to minimize the loss function L given by:\nL(X,Y ) = 1 N N\u2211 i=0 Li(F(Xi,W), Yi) + \u03bbR(W) (1)\nwhere W are the weights in F that are adjusted to minimize the loss function, and \u03bb controls the effect of the regularization function. For our method to fit into the supervised learning scheme we altered the optimization problem by adding a time dimension (t) to the loss function:\nLt(X,Y ) = 1 N N\u2211 i=0 Lti(F(Xi,W), Yi) + \u03bbR(W) (2)\nSoftTarget regularization requires into two steps: first, we kept an exponential moving average of past labels Y\u0302 t, and second, we updated the current epochs label Y tc through a weighted average of the exponential moving average of past labels and of the true hard labels:\nY\u0302 t = \u03b2Y\u0302 t\u22121 + (1\u2212 \u03b2)F(Xi,W) (3) Y tc = \u03b3Y\u0302 t + (1\u2212 \u03b3)Y (4)\nHere, \u03b3 and \u03b2 are hyper-parameters that can be tuned to specific applications. The loss function then becomes:\nLt(X,Y ) = 1 N N\u2211 i=0 Lti(F(Xi,W), Y tc ) + \u03bbR(W) (5)\nThe algorithm also contains a \u2018burn-in\u2019 period, where no SoftTarget regularization is done and the model is trained freely in order to learn the basic co-label similarities. We will denote the number of epochs trained freely as nb, and the total number of epochs as n. Experimentally we also discovered that it is sometimes best to run the network for more than one epoch on a single Yc, so we will denote nt as the number of epochs per every time-step. We have provided the pseudo-code in Algorithm 1.\nAlgorithm 1 SoftTarget Regularization input: X,Y,F ,G, \u03b2, \u03b3, nb, nt, n\nF \u2190 G(F , {X,Y }, nb)\nY\u0302 0 \u2190 F(Xi,W), t\u2190 1 for i\u2190 0 to n\u2212nbnt do\nY\u0302 t = \u03b2Y\u0302 t\u22121 + (1\u2212 \u03b2)F(Xi,W) Y tc = \u03b3Y\u0302 t + (1\u2212 \u03b3)Y\nF \u2190 G(F , {X,Y tc }, nt), t\u2190 t+ 1 end\nHere G represents the training of the neural network, taking in a model F , dataset {X,Y } and an integer representing number of epochs.\nA large nt allows the network to learn a better mapping to the intermediate soft-labels and therefore allows the regularization to be more effective. But increasing nt has a diminishing effect, because\nas nt becomes large the network begins to over-fit to those soft-labels, and reduces the effect of the regularization, as well as increasing the training time of the network significantly. nt should be optimized experimentally through standard hyper-parameter optimization practices. We found nt = {1, 2} to work best through standard grid hyper-parameter optimization (Bergstra & Bengio, 2012). The small nt insures that the model does not overfit to the intermediate representation introduced by SoftTarget.\nThrough hyper-parameter optimization the same range of {1, 2} was found to be optimal in the experiments we ran for nb. A small nb insures that the co-label similarities captured by SoftTarget would not have been affected by any type of overfitting. This insures that as the experiments are further ran the true co-label similarties are propagated correctly. More complicated learning scenarias where the amount of labels and data is greater, the chances of corruption in co-label similarties is reduced and therefore larger nb can be choosen."}, {"heading": "1.3 SIMILARITIES TO OTHER METHODS", "text": "Other methods similar to this are specific to the case where the \u03b2 hyper-parameter is set to zero, with no burn-in period.\n\u2022 Reed et al. study the specific case of the SoftTarget method described above with the \u03b2 parameter set to zero (Reed et al., 2014). They focus on the capability of the network to be robust to noise, rather than the regularization abilities of the method.\n\u2022 Grandvalet and Bengio have proposed minimum entropy regularization in the setting of semi-supervised learning (Grandvalet & Bengio, 2005). This algorithm changes the categorical cross-entropy loss to force the network to make predictions with high degrees of confidence on the unlabeled portion of the dataset. Assuming cross-entropy loss with SoftTarget normalization with a zero burn-in period, and zero \u03b2, our algorithm becomes equivalent to a softmax regression with minimum entropy regularization.\n\u2022 Another similar approach to minimum entropy regularization is an approach called pseudolabeling. Pseudo-labeling tags unlabeled data with the class predicted highest by a learning model (Lee, 2013). No soft-targets are kept, instead the predicted label is binarized, i.e. the highest class is labeled with a value of one, and every other class is labeled with a value of zero. These hard pseudo-labels are then fed as input to the model.\n\u2022 Hinton et al described the power of soft targets in the use of transferring knowledge from one model to another, usually to a model that contains less parameters (Hinton et al., 2015). Soft-Target regularization can be interpreted as weighted distillation where the donor model is the state of the model at some previous time in training, and the weighting target are the hard-targets."}, {"heading": "2 EXPERIMENTS", "text": "We conducted experiments in python using the Theano and Keras libraries (The Theano Development Team, 2016), (Chollet, 2015). All of our code ran on a single Nvidia Titan X GPU, while using the CnMEM and cuDNN (5.103) extensions, and we visualized our results using matplotlib (Hunter, 2007). We used the same seed in all our calculations to insure the starting weights were equivalent in every set of experiments. The only source of randomness stemmed from the non-deterministic behavior of the cuDNN libraries."}, {"heading": "2.1 MNIST", "text": "We first considered the famous MNIST dataset (LeCun et al., 1998). For each of the experiments discussed below, we performed a random grid-search over the hyper-parameters of the optimization algorithm, and a very small brute force grid search was done for the hyper-parameters of SoftTarget regularization. We compared our results to the cases where the hyper-parameters resulted in the best performance of the vanilla neural network without SoftTarget regularization. All of our reported values were computed on the standardized test portion of the MNIST dataset, as provided by the Keras library. The networks were trained strictly on the training portion of the dataset. We tested on\neight different architectures, with four combinations of every architecture. The four combinations stem from testing each architecture via a combination of: no regularization, Dropout, SoftTarget, and Dropout+SoftTarget regularization.\nWe used a fully connected network, with a varying amount of hidden layers, and a set constant of neurons throughout each layer. Dropout was not introduced at the input layer, but was introduced at every layer after that. All of the layers activations we\u2019re rectified linear units (ReLu), except for the final layer which was a SoftMax. The net was trained using a categorical cross-entropy loss, and the ADADELTA optimization method. (Zeiler, 2012).\nThe frozen hyper-parameters for the SoftTarget regularization were: nb = 2, nt = 2, n = 100, \u03b2 = 0.7, \u03b3 = 0.5. Our results are described in Table 1. We described the nets using the notation: 4\u2190 256 denoting a 4 hidden layer neural network, with each of the hidden layers having 256 units. We reported the minimum loss during training, the loss at the 100th epoch, and the maximum accuracy reached respectively.\nIn all our experiments, the best performing regularization for all of the architectures described above included SoftTarget regularization. Two representative results are plotted in Figure 1 for a shallow (three layer) and deep (seven layer) neural network. We saw that for deep neural networks (greater than three layers) SoftTarget regularization outperformed all the other regularization schemes. For shallow (three layer) neural networks SoftTarget+Dropout was the optimal scheme."}, {"heading": "2.2 CIFAR-10", "text": "We then considered the CIFAR-10 dataset (Krizhevsky & Hinton, 2009), comparing various combinations of SoftTarget, Dropout and BatchNormalization (BN) (Ioffe & Szegedy, 2015). BatchNormalization has been shown to have a regularization effect on neural networks due to the noise inherent to the mini-batch statistics. We ran each configuration of the network through sixty iterations through the whole training set. The complete architecture used was:\nInput\u2192 Convolution (64,3,3)\u2192 BN\u2192 ReLU\u2192 Convolution (64,3,3)\u2192 BN\u2192 ReLU\u2192 MaxPooling ((3,3), (2,2)) \u2192 Dropout (p) \u2192 Convolution (128,3,3) \u2192 BN \u2192 ReLU \u2192 Convolution (128,3,3)\u2192 BN\u2192 ReLU\u2192MaxPooling ((3,3), (2,2))\u2192 Dropout (p)\u2192 Convolution (256,3,3)\u2192 BN\u2192 ReLU\u2192 Convolution (256,1,1)\u2192 BN\u2192 ReLU\u2192 Convolution (256,1,1)\u2192 BN\u2192 ReLU \u2192 Dropout (p)\u2192 AveragePooling ((6,6))\u2192 Flatten ()\u2192 Dense (256)\u2192 BN\u2192 ReLU\u2192 Dense (256)\u2192 BN\u2192 ReLU\u2192 Dense (256)\u2192 SoftMax. where: Convolution (64,3,3) signifies the convolution operator with 64 filters, and a kernel size of 3 by 3, MaxPooling ((3,3), (2,2)) represents the max-pooling operation with a kernel size of 3 by 3, and a stride of 2 by 2, AveragePooling ((6,6)) represents the average pooling operator with a kernel size of 6 by 6, Flatten represents a flattening of the tensor into a matrix, and Dense (256) a fully-connected layer (Krizhevsky et al., 2012), (Scherer et al., 2010). In our results, when we note that BN or Dropout weren\u2019t used, we simply omitted those layers from the architecture. We trained the networks using ADADELTA on the cross-entropy loss, using the same SoftTarget hyperparameters we reported for the MNIST dataset. Our results are summarized in Table 2. The first column specifies the amount of Dropout used on the combinations listed in the next columns. As with the MNIST experiments, we reported the minimum loss during training, and the loss at the 100th epoch.\nThe use of SoftTarget regularization resulted in the lowest loss in four out of the five experiments on this architecture, and resulted in the lowest last epoch loss value and highest accuracy in all five of the experiments. As the dropout rate is increased the need for any other type of regularization is decreased. However, by increasing the rate of dropout, the resulting loss is increased because of the\nreduced capacity of the network. SoftTarget regularization allowed a lower dropout rate to be used, and this lowered the test error."}, {"heading": "2.3 SVHN", "text": "Finally, we considered the Street View House Numbers (SVHN) dataset, consisting of various images mapping to one of ten digits (Netzer et al., 2011). This is similar to the MNIST dataset, but is much more organic in nature, as these images contain much more natural noise, such as lighting conditions and camera orientation. We tested residual networks in four configurations: No regularization, Batch Normalization (BN), SoftTarget, and BN+SoftTarget (?). Our architecture consisted of the same building blocks as the residual network outlined by He et al., consisting of identity and convolution blocks (He et al., 2015). Identity blocks are blocks that do not contain a convolution layer at the shortcut, while convolution blocks do. In our notation I (3,[16,16,32], BN) will mean an identity block with an intermediate square convolution kernel size of 3, with three convolution blocks of size 16, 16 and 32. The outer convolutions contain kernel sizes of 1. C (3,[16,16,32], BN) contains the same initial architecture as I (3,[16,16,32]) but an additional convolution layer of size 32 at the shortcut connection. All of these blocks contained the rectified linear function as their activation, and BN prior to activation. Our final architecture was:\nInput \u2192 ZeroPadding (3,3) \u2192 Convolution (64,7,7,subsample = (2,2)) \u2192 BN \u2192 ReLU \u2192 MaxPooling ((3,3), (2,2)) \u2192 C (3,[16,16,32], BN) \u2192 I (3,[16,16,32], BN) \u2192 I (3,[16,16,32], BN) C (3,[32,32,64], BN) \u2192 I (3,[32,32,64], BN) \u2192 I (3,[32,32,64], BN) \u2192 AveragePooling ((7,7)) \u2192 Dense (10)\u2192 SoftMax We used the ADADELTA optimization method with a random grid search for hyper-parameter optimization. All configurations of the networks were run for 60 iterations apart from the overfit configuration which was run for 100 iterations.\nWe reported our results in Table 3 and Figure 2, as before giving the minimum test loss and the test loss at the last epoch. SoftTarget regularized configurations (with and without BN) again scored the lowest test loss and highest accuracy, compared to Batch Normalization alone."}, {"heading": "2.4 CO-LABEL SIMILARITIES", "text": "We claimed that over-fitting begins to occur when co-label similarities that appeared in the initial stages of training, are not longer present. To test this hypothesis we compared the covariate matrices of a over-fitted network, early training stopped networks, and regularized networks. We tested again on the CIFAR10 dataset, with the same architecture as the previous CIFAR10 experiment,\nexcept that the number of filters and dense units were reduced exactly by two. We compared four configurations: Early (10 epochs), Overfit (100 epochs), Dropout (p=0.2, 100 epochs) and SoftTarget (nb = 2, nt = 2, \u03b2 = 0.7, \u03b3 = 0.5, 100 epochs). After training each configuration for its respected amount we predicted the labels of the training set. We then calculated a covariance matrix scaled to a range of [0, 1] since we are only interested in the relative co-label similarities. We set the diagonal to all zeros, as to make it easier to see other relations. The covariance function used is defined below.\nci,i = 0 (6)\ncx,y =\n\u2211N i=1(xi \u2212 x\u0304)(yi \u2212 y\u0304)\nN \u2212 1 (7)\ncovs(x, y) = cx,y \u2212min(cx,y)\nmax(cx,y)\u2212min(cx,y) (8)\nWe plotted the covariance matrices in Figure 3. For the early stop case, there we observed the highest covariance between labels 3 and 5, which correspond to cats and dogs respectively. This intuitively makes sense, during earlier steps of training, the network learns to first detect differences between varying entities, such as frog and airplane, and then later learns to detect subtle difference. It is interesting to note, that this is the core principle behind prototype theory in human psychology (Osherson & Smith, 1981), (Duch, 1996), (Rosch, 1978). Some concepts are in nature closer to each other than others. Dog and cat are closer in relation than frog and airplane, and our regularization method mimics this phenomena. Another interesting thing to note is that the dropout method of regularization produces a covariance matrix that is very similar to that produced by SoftTarget regularization. The phenomena of co-label similarities being propagated throughout learning is not specific to just SoftTarget regularization, but regularization in general. Therefore co-label similarities can be seen as a measure of over-fitting."}, {"heading": "3 CONCLUSION AND FUTURE WORK", "text": "In conclusion, we presented a new regularization method based on the observation that co-label similarities apparent in the beginning of training, disappear once a network begins to over-fit. SoftTarget\nregularization reduced over-fitting as well as Dropout without adding complexity to the network, therefore reducing computational time, and we provided novel insights into the problem of overfitting.\nFuture work will focus on methods to reduce the number of hyper-parameters introduced by SoftTarget regularization, as well as providing a formal mathematical framework to understand the phenomenon of co-label similarities."}], "references": [{"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "URL https://github.com/ fchollet/keras", "author": ["Fran\u00e7ois Chollet"], "venue": "Keras Deep Learning Library,", "citeRegEx": "Chollet.,? \\Q2015\\E", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Categorization, prototype theory and neural dynamics", "author": ["W. Duch"], "venue": "Proceedings of the 4th International Conference on Soft Computing,", "citeRegEx": "Duch.,? \\Q1996\\E", "shortCiteRegEx": "Duch.", "year": 1996}, {"title": "Semi-supervised Learning by Entropy", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "Minimization. Network,", "citeRegEx": "Grandvalet and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Grandvalet and Bengio.", "year": 2005}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv, pp", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Matplotlib: A 2D Graphics Environment", "author": ["John D Hunter"], "venue": "Computing in Science and Engineering,", "citeRegEx": "Hunter.,? \\Q2007\\E", "shortCiteRegEx": "Hunter.", "year": 2007}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv, pp", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A Simple Weight Decay Can Improve Generalization", "author": ["A. Krogh", "J. a. Hertz"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Krogh and Hertz.,? \\Q1992\\E", "shortCiteRegEx": "Krogh and Hertz.", "year": 1992}, {"title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "author": ["Dong-Hyun Lee"], "venue": "In ICML 2013 Workshop: Challenges in Representation Learning (WREPL),", "citeRegEx": "Lee.,? \\Q2013\\E", "shortCiteRegEx": "Lee.", "year": 2013}, {"title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On the adequacy of prototype theory as a theory of concepts", "author": ["Daniel N. Osherson", "Edward E. Smith"], "venue": "Cognition,", "citeRegEx": "Osherson and Smith.,? \\Q1981\\E", "shortCiteRegEx": "Osherson and Smith.", "year": 1981}, {"title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "author": ["Scott Reed", "Honglak Lee", "Dragomir Anguelov", "Christian Szegedy", "Dumitru Erhan", "Andrew Rabinovich"], "venue": "arXiv, pp", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Principles of Categorization", "author": ["Eleanor Rosch"], "venue": "Cognition and categorization,", "citeRegEx": "Rosch.,? \\Q1978\\E", "shortCiteRegEx": "Rosch.", "year": 1978}, {"title": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition, pp. 92\u2013101", "author": ["Dominik Scherer", "Andreas M\u00fcller", "Sven Behnke"], "venue": null, "citeRegEx": "Scherer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Scherer et al\\.", "year": 2010}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Development Team. Theano: A Python framework for fast computation of mathematical expressions", "author": ["The Theano"], "venue": "arXiv, pp", "citeRegEx": "Theano,? \\Q2016\\E", "shortCiteRegEx": "Theano", "year": 2016}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler"], "venue": "arXiv, pp", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al.", "startOffset": 179, "endOffset": 204}, {"referenceID": 19, "context": ", 2014), (Wan et al., 2013), (Krogh & Hertz, 1992).", "startOffset": 9, "endOffset": 27}, {"referenceID": 17, "context": "For example, Dropout reduces the amount of learn-able parameters by randomly dropping activations, and DropConnect extends this idea by randomly dropping weights (Srivastava et al., 2014), (Wan et al.", "startOffset": 162, "endOffset": 187}, {"referenceID": 19, "context": ", 2014), (Wan et al., 2013).", "startOffset": 9, "endOffset": 27}, {"referenceID": 5, "context": "Hinton has shown that soft-labels, or labels predicted from a model contain more information that binary hard labels due to the fact that they encode similarity measures between the classes (Hinton et al., 2015).", "startOffset": 190, "endOffset": 211}, {"referenceID": 14, "context": "study the specific case of the SoftTarget method described above with the \u03b2 parameter set to zero (Reed et al., 2014).", "startOffset": 98, "endOffset": 117}, {"referenceID": 11, "context": "Pseudo-labeling tags unlabeled data with the class predicted highest by a learning model (Lee, 2013).", "startOffset": 89, "endOffset": 100}, {"referenceID": 5, "context": "\u2022 Hinton et al described the power of soft targets in the use of transferring knowledge from one model to another, usually to a model that contains less parameters (Hinton et al., 2015).", "startOffset": 164, "endOffset": 185}, {"referenceID": 1, "context": "We conducted experiments in python using the Theano and Keras libraries (The Theano Development Team, 2016), (Chollet, 2015).", "startOffset": 109, "endOffset": 124}, {"referenceID": 6, "context": "103) extensions, and we visualized our results using matplotlib (Hunter, 2007).", "startOffset": 64, "endOffset": 78}, {"referenceID": 20, "context": "(Zeiler, 2012).", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "where: Convolution (64,3,3) signifies the convolution operator with 64 filters, and a kernel size of 3 by 3, MaxPooling ((3,3), (2,2)) represents the max-pooling operation with a kernel size of 3 by 3, and a stride of 2 by 2, AveragePooling ((6,6)) represents the average pooling operator with a kernel size of 6 by 6, Flatten represents a flattening of the tensor into a matrix, and Dense (256) a fully-connected layer (Krizhevsky et al., 2012), (Scherer et al.", "startOffset": 420, "endOffset": 445}, {"referenceID": 16, "context": ", 2012), (Scherer et al., 2010).", "startOffset": 9, "endOffset": 31}, {"referenceID": 12, "context": "3 SVHN Finally, we considered the Street View House Numbers (SVHN) dataset, consisting of various images mapping to one of ten digits (Netzer et al., 2011).", "startOffset": 134, "endOffset": 155}, {"referenceID": 4, "context": ", consisting of identity and convolution blocks (He et al., 2015).", "startOffset": 48, "endOffset": 65}, {"referenceID": 2, "context": "It is interesting to note, that this is the core principle behind prototype theory in human psychology (Osherson & Smith, 1981), (Duch, 1996), (Rosch, 1978).", "startOffset": 129, "endOffset": 141}, {"referenceID": 15, "context": "It is interesting to note, that this is the core principle behind prototype theory in human psychology (Osherson & Smith, 1981), (Duch, 1996), (Rosch, 1978).", "startOffset": 143, "endOffset": 156}], "year": 2016, "abstractText": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "creator": "LaTeX with hyperref package"}}}