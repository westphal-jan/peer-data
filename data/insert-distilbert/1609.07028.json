{"id": "1609.07028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Image-embodied Knowledge Representation Learning", "abstract": "entity images provide significant visual information that helps the construction of knowledge representations. most conventional methods learn knowledge representations solely from structured triples, ignoring rich visual information images extracted from entity images. in this paradigm paper, we propose a novel image - embodied knowledge representation learning model, where knowledge representations are being learned with both triples and images. more specifically, for each image of an entity, we construct image - based images representations via a neural image encoder, and these representations with respect to multiple image sharing instances are been then integrated via an attention - based method. gradually we evaluate our models concentrating on knowledge graph completion and triple classification. relevant experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations classification and the capability of our models in learning knowledge representations with images.", "histories": [["v1", "Thu, 22 Sep 2016 15:37:45 GMT  (348kb,D)", "http://arxiv.org/abs/1609.07028v1", "7 pages"], ["v2", "Mon, 22 May 2017 08:14:27 GMT  (343kb,D)", "http://arxiv.org/abs/1609.07028v2", "7 pages; Accepted by IJCAI-2017"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["ruobing xie", "zhiyuan liu", "huanbo luan", "maosong sun"], "accepted": false, "id": "1609.07028"}, "pdf": {"name": "1609.07028.pdf", "metadata": {"source": "CRF", "title": "Image-embodied Knowledge Representation Learning", "authors": ["Ruobing Xie", "Zhiyuan Liu", "Tat-seng Chua", "Huanbo Luan", "Maosong Sun"], "emails": ["(liuzy@tsinghua.edu.cn)"], "sections": [{"heading": "1 Introduction", "text": "Knowledge graphs (KGs), which provide huge amount of structured information for entities and relations, have been successfully utilized in various fields such as knowledge inference [Yang et al., 2014] and question answering [Yin et al., 2016]. A typical KG like Freebase or DBpedia usually models the multi-relational information with enormous triple facts represented as {head entity,relation, tail entity}, which is also abridged as {h, r, t}.\nTo model knowledge graphs, translation-based methods are proposed, which project both entities and relations into a continuous low-dimensional semantic space, with relations considered to be translating operations between head and tail entities [Bordes et al., 2013]. These methods could leverage both effectiveness and efficiency in knowledge representation learning (KRL), and thus have attracted great attention in recent years. However, most conventional methods on KRL\n\u2217Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)\nonly concentrate on the structured information in triple facts, regardless of rich external information located in images.\nFig. 1 demonstrates some examples of entity images. Each entity has multiple images which can provide significant visual information that intuitively describes the appearances and behaviours of this entity. To utilize the rich information in images, we propose the Image-embodied Knowledge Representation Learning (IKRL) model. More specifically, we first propose an image encoder which consists of a neural representation module and a projection module to generate the image-based representation for each image instance. Second, we construct the aggregated image-based entity representation jointly considering all image instances with an attentionbased method. Finally, we jointly learn the knowledge representations with translation-based methods.\nWe evaluate the IKRL model on knowledge graph completion and triple classification. Experimental results demonstrate that our model achieves the state-of-the-art performances on both tasks, which confirms the capability of IKRL in encoding image information into knowledge representations."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Translation-based Methods", "text": "Translation-based methods have achieved great success on knowledge representation learning in recent years. TransE [Bordes et al., 2013] models both entities and relations into the same low-dimensional continuous vector space, with relations considered to be translating operations between head and tail entities. The basic assumption of TransE is that the embedding of tail entity t should be the neighbour of h + r. The energy function of TransE is defined as follows:\nE(h, r, t) = ||h+ r\u2212 t||. (1)\nTransE is both effective and efficient, while the simple assumption may result in conflicts when modeling complicated entities and relations. To address this problem, TransH [Wang et al., 2014b] proposes relation-specific hyperplanes for translations between entities. TransR [Lin et al., 2015] models entities and relations in different vector spaces, projecting entities from entity space to relation spaces with relation-specific matrices. TransD [Ji et al., 2015] further\nar X\niv :1\n60 9.\n07 02\n8v 1\n[ cs\n.C V\n] 2\n2 Se\np 20\n16\nconsiders both diversities of entities and relations, using dynamic mapping matrix for the multiple representations of entities. However, these methods only concentrate on the structured information in KGs. We propose the IKRL model to consider images based on TransE, and our model can also be easily extended to other translation-based methods."}, {"heading": "2.2 Multi-source information Learning", "text": "Multi-source information such as textual and visual information is significant for knowledge representation. To utilize rich textual information, [Wang et al., 2014a] projects both entities and words into a joint vector space with alignment models. [Xie et al., 2016] directly constructs entity representations from entity descriptions, which is capable of modeling new entities. As for visual information, multimodal representations based on words and images are widely used on various tasks like image-sentence ranking [Kiros et al., 2014] and metaphor identification [Shutova et al., 2016]. However, image information has not yet been used in knowledge representations. To the best of our knowledge, IKRL is the first method which explicitly encodes visual information from images into knowledge representations."}, {"heading": "3 Methodology", "text": "We first introduce the notations used in this paper. Given a triple (h, r, t) \u2208 T , it consists of two entities h, t \u2208 E and a relation r \u2208 R. T stands for the whole training set of triples, E represents the set of entities, and R represents the set of relations. Each entity and relation embedding takes value in Rds .\nTo utilize entity image information, we propose two kinds of representations for each entity inspired by [Xie et al., 2016]. We set hS , tS as the structure-based representations of head and tail entities, which are the distributed representations learned by conventional knowledge models. We also propose a novel kind of knowledge representations hI , tI as the image-based representations, which are constructed from the corresponding images of both entities."}, {"heading": "3.1 Overall Architecture", "text": "We attempt to utilize structured knowledge information as well as visual information in the IKRL model. Following the framework of translation-based methods, we define the overall energy function as follows:\nE(h, r, t) = ESS + ESI + EIS + EII . (2)\nThe overall energy function is determined by the two kinds of entity representations jointly. ESS = ||hS + r \u2212 tS || is\nthe same energy function as TransE which only depends on the structure-based representations. EII = ||hI + r \u2212 tI || is the energy function in which both head and tail entities are image-based representations learned from their corresponding images. We also have ESI = ||hS + r \u2212 tI || and EIS = ||hI + r \u2212 tS || to assure that both structure-based representations and image-based representations are learned into the same vector space.\nAccording to the energy function, the overall architecture of the IKRL model is demonstrated in Fig. 2. Each entity has multiple entity images providing significant visual information. First, we design a neural image encoder taking every entity image as inputs. The image encoder aims to extract informative features from images and construct the image representations in entity space. Second, for the combination of multiple image representations, we implement an instancelevel attention-based learning method to automatically calculate the attention we should pay on different image instances for each entity. Finally, the aggregated image-based representations are learned jointly combined with the structure-based representations under the overall energy function."}, {"heading": "3.2 Image Encoder", "text": "Images provide informative visual information that can intuitively describes the appearances and behaviours of entities, which are considered as the fundamental input data of the IKRL model. For each entity ek, there are multiple image instances represented as Ik = {img(k)1 , img (k) 2 , \u00b7 \u00b7 \u00b7 , img (k) n }.\nTo effectively encode the image information into knowledge representations, we propose an image encoder which consists of an image representation module and an image projection module. The image representation module utilizes neural networks to extract discriminative features in images, and constructs image feature representations for each image. Next, the image projection module attempts to project those image feature representations from image space to entity space. Fig. 3 demonstrates the overall pipeline of the image encoder."}, {"heading": "Image Representation Module", "text": "The image representation module aims at building the image feature representations. We utilize AlexNet, a deep neural network that contains five convolution layers, two fullyconnected layers and a softmax layer, to extract image features [Krizhevsky et al., 2012]. During preprocessing, all images are reshaped to 224 \u00d7 224 from the center, corners and their horizontal reflections. Inspired by [Shutova et al., 2016], we take the 4096-dimensional embeddings which are outputs\nof the second fully-connected layer (also called as fc7) as the image feature representations."}, {"heading": "Image Projection Module", "text": "After we get the compressed feature representations for each image, the next procedure is to build the bridges between images and entities via image projection module. Specifically, we transfer the image feature representations from image space to entity space with a shared projection matrix. The image-based representation pi in entity space for the i-th image is defined as:\npi = M \u00b7 f(imgi), (3)\nin which M \u2208 Rdi\u00d7ds is the projection matrix, di represents the dimension of image features, while ds represents the dimension of entities. f(imgi) stands for the i-th image feature representation in image space, which is constructed by the image representation module."}, {"heading": "3.3 Attention-based Multi-instance Leaning", "text": "Image encoder takes images as inputs and then constructs image-based representations for each single image. However, most entities have more than one image from different aspects in various scenarios. It is essential but also challenging to determine which images are better to represent their corresponding entities. Simply summing up all image representations may suffer from either noises or information loss. Instead, to construct the aggregated image-based representation for each entity from multiple instances, we propose an attention-based multi-instance learning method.\nAttention-based methods are confirmed to be intelligent to automatically select informative instances form multiple candidates. It has been widely utilized in various fields such as\nimage classification [Mnih et al., 2014], machine translation [Bahdanau et al., 2015] and abstractive sentence summarization [Rush et al., 2015]. We jointly consider each image representation and its corresponding entity structure-based representation to generate the attention. For the i-th image representation p(k)i of the k-th entity, the attention is defined as follows:\natt(p (k) i , e (k) S ) =\nexp (p (k) i \u00b7 e (k) S )\u2211n\nj=1 exp (p (k) j \u00b7 e (k) S )\n, (4)\nwhere e(k)S represents the structure-based representation of the k-th entity. High attention indicates that its corresponding image representation is similar to the structure-based representation, and thus should be more considered when constructing the aggregated image-based representation. Hence we define the aggregated image-based representation for the k-th entity as follows:\ne (k) I = n\u2211 i=1 att(p (k) i , e (k) S ) \u00b7 p (k) i\u2211n j=1 att(p (k) j , e (k) S ) , (5)\nBesides the attention-based method, we also implement two alternative combination methods for further comparisons. AVG is a simple combination method that takes the mean of all image embeddings, making every image have equal contributions to the final image-based representation. MAX is a simplified version based on attention, which only considers the image representations with the largest attention."}, {"heading": "3.4 Objective Formalization", "text": "We utilize a margin-based score function as our training objective, which is defined as follows:\nL = \u2211\n(h,r,t)\u2208T \u2211 (h\u2032,r\u2032,t\u2032)\u2208T \u2032 max(\u03b3 + E(h, r, t)\u2212\nE(h\u2032, r\u2032, t\u2032), 0),\n(6)\nwhere \u03b3 is a margin hyperparameter. E(h, r, t) is the overall energy function stated above, in which both head and tail entities have two kinds of representations including structurebased representations and image-based representations. T \u2032\nstands for the negative sample set of T , which we define as follows:\nT \u2032 = {(h\u2032, r, t)|h\u2032 \u2208 E} \u222a {(h, r, t\u2032)|t\u2032 \u2208 E}\u222a {(h, r\u2032, t)|r\u2032 \u2208 R}, (h, r, t) \u2208 T,\n(7)\nwhich means one of the entities or relation of a triple has been randomly replaced by another one."}, {"heading": "3.5 Optimization and Implementation Details", "text": "The IKRL model can be formalized as a parameter set \u03b8 = (E,R,W,M), in which E stands for the structure-based embedding set of entities, R stands for the embedding set of relations. W represents the weights of the neural networks used in image representation module, while M represents the projection matrix used in image projection module.\nWe utilize mini-batch stochastic gradient descent (SGD) to optimize our model, with chain rule applied to update the parameters. M is initialized randomly, while E and R could be either initialized randomly or be pre-trained by previous translation-based methods. As for the image representation module, we utilize a deep learning framework Caffe [Jia et al., 2014] to implement AlexNet, which is pre-trained on ILSVRC 2012 with a minor variation from the version described in [Krizhevsky et al., 2012]. The weights of AlexNet W are pre-trained and fixed during training. For the consideration of efficiency, we use GPU to accelerate the neural networks, and also employ a multi-thread version for training."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "In this paper, we propose a new dataset of knowledge graph combined with images named WN9-IMG for evaluation tasks including knowledge graph completion and triple classification. WN9-IMG is a sub-graph of WN18 utilized in [Bordes et al., 2014], which is originally extracted from WordNet [Miller, 1995]. For the consideration of image quality, we provide 63,225 images extracted from ImageNet, which is a huge image database containing over 21,000 synsets organized according to the WordNet hierarchy [Deng et al., 2009]. We select the triples whose both entities have images, and split them into train, validation and test set. The statistics of WN9-IMG are listed in Table 1."}, {"heading": "4.2 Experiment Settings", "text": "We train the IKRL model via mini-batch SGD, with the batch size B set among {7, 14, 28}, and the margin \u03b3 set among {0.5, 1.0, 2.0}. The learning rate \u03bb could be either empirically fixed among {0.0002, 0.0005, 0.001}, or designed following a flexible adaptive strategy that descends through iterations. The optimal configurations of the IKRL model are: B = 7, \u03b3 = 1.0, with the learning rate defined adopting\na linear-declined strategy in which \u03bb ranges form 0.001 to 0.0002. For the image number n for each entity is up to 10. We also set the dimension of image feature embeddings di = 4096, and the dimension of entity and relation embeddings ds = 50.\nWe implement TransE [Bordes et al., 2013] and TransR [Lin et al., 2015] as our baselines, with the same experimental settings reported in their papers. For fair comparisons, the dimensions of entities and relations in all baselines are also set to be 50."}, {"heading": "4.3 Knowledge Graph Completion", "text": ""}, {"heading": "Evaluation Protocal", "text": "Knowledge graph completion aims to complete a triple (h, r, t) when one of h, r, t is missing. This task has been widely used to evaluate the quality of knowledge representations [Bordes et al., 2012; Bordes et al., 2013]. The prediction is determined via the dissimilarity function ||h+ r\u2212 t||. Since the IKRL model has two kinds of representations, we will report three prediction results based on our models: IKRL (SBR) only utilizes structure-based representations for all entities when predicting the missing ones, while IKRL (IBR) only utilizes image-based representations to predict. IKRL (UNION) is a simple joint method considering the weighted concatenation of both entity representations.\nFollowing the same settings in [Bordes et al., 2013], we consider two measures as our evaluation metrics in entity prediction: (1) mean rank of correct entities (Mean Rank); (2) proportion of correct entity results ranked in top 10 (Hits@10). We also follow the two evaluation settings named \u201cRaw\u201d and \u201cFilter\u201d used in [Bordes et al., 2013]. In this section, we first demonstrate the results of entity prediction, and then implement another experiment for further discussions on the power of attention."}, {"heading": "Entity Prediction", "text": "The results of entity prediction are demonstrated in Table 2. From the results we can observe that: (1) all IKRL models outperform all baselines on both evaluation metrics of Mean Rank and Hits@10, among which IKRL (UNION) achieves the best performance. It indicates that the visual information of images has been successfully encoded into entity representations, which is of great significance when constructing knowledge representations. (2) Both IKRL (SBR) and IKRL (IBR) have better performances compared to the baselines, which indicates that visual information could not only instruct the construction of image-based representations, but\nalso improve the performances of structure-based representations. (3) The IKRL models significantly and consistently outperform baselines on Mean Rank. It is because that Mean Rank depends on the overall quality of knowledge representations, and thus is sensitive with the wrong-predicted results. Previous translation-based methods like TransE only consider the structured information in triples, which may fail to predict the relationships if the corresponding information is missing. However, the image information utilized in IKRL can provide supplementary information. Therefore, the results of IKRL are much better than baselines on Mean Rank."}, {"heading": "Further Discussion on Attention", "text": "To further demonstrate the power of attention-based method, we implement three combination strategies to jointly consider multiple image instances. IKRL (ATT) represents the basic model with attention when constructing the aggregated image-based representations, while IKRL (MAX) represents the combination strategy that only considers the image instance which has the largest attention, and IKRL (AVG) represents the strategy that takes the average embeddings of all image instances to represent an entity. The evaluation results on entity prediction with both image-based representations and structure-based representations are shown in Table 3.\nFrom Table 3 we observe that: (1) all IKRL models still outperform baselines on Mean Rank and Hits@10 no matter what the combination strategy is. It confirms the improvements introduced by images, for the visual information has been successfully encoded into knowledge representations. (2) The IKRL (ATT) model achieves the best performance among all three combination strategies, which implies that the attention-based method is capable of automatically selecting more informative image instances to represent entities. (3) The IKRL (AVG) model performs better than the IKRL (MAX) model, which indicates that only considering images with the largest attention will lose important information located in other instances. (4) It seems that the IKRL (ATT) model has slight advantages over the IKRL (AVG) model. The reason is that the qualities of images we extract from ImageNet are very high, which may narrow the gap between attention-based and average-based methods. For further analysis, we will give some examples with attention in case study, which could successfully distinguish the relatively better and worse images from all candidates."}, {"heading": "4.4 Triple Classification", "text": ""}, {"heading": "Evaluation Protocol", "text": "Triple classification aims to predict whether a triple fact (h, r, t) is correct or not according to the dissimilarity func-\ntion [Socher et al., 2013]. It can be viewed as a binary classification task on triples. Since WN9-IMG has no explicit negative instances, we generate the negative instances by randomly replacing head or tail entities with another entity following the same protocol utilized in [Socher et al., 2013]. We also assure that the number of positive triples is equal to that of negative triples.\nIn classification, we set different relation-specific thresholds \u03b4r for each relation, which are optimized by maximizing the classification accuracies on the validation set with their corresponding relations. For a triple to be classified, if its dissimilarity function ||h + r \u2212 t|| is over \u03b4r, it will be predicted to be negative, and otherwise to be positive. To better demonstrate the advantages in IKRL models, only imagebased representations are utilized when calculating the dissimilarity function for each triple."}, {"heading": "Experimental Results", "text": "From Table 4 we can observe that: (1) all IKRL models outperform both baselines, which demonstrates the effectiveness and robustness of our models that combine structured information in triples with visual information in images. Note that IKRL is based on the framework of TransE but still performs better even when compared with the enhanced TransR model, which confirms the improvements introduced by images. (2) The IKRL (ATT) model achieves the best performance compared to other combination strategies. It indicates that the attention-based method can jointly take multiple instances into consideration and smartly choose more informative images from all candidates."}, {"heading": "4.5 Case study", "text": "In this section, we will give two cases with analysis. The first is to introduce the semantic regularities of images, and the second is to demonstrate the capability of attention. For better demonstrations, the images shown in case study may be chopped while the main objects are included."}, {"heading": "Semantic Regularities of Images", "text": "[Mikolov et al., 2013] shows that word representations have some interesting semantic regularities such as v(king) \u2212 v(man) \u2248 v(queen) \u2212 v(woman), and the similar regularities have also been found on visual-semantic embeddings [Kiros et al., 2014]. We explore these semantic regularities on images that are demonstrated in Fig. 4. Differing from previous work, the subtraction results of two images are more concrete and meaningful, for they are considered to be the corresponding relation embeddings."}, {"heading": "Capability of Attention", "text": "Fig. 5 demonstrates some pairs of image instances with different attention, aiming to confirm the capability of attention in selecting more informative images from multiple instances. In the first example of portable computer, the attention-based method successfully detects the low-quality instance which is actually a phone by assigning low attention. For golf game, the image with low attention only shows an overview of the lawn without any person or detailed sporting goods, and thus is less considered in combination. As for watering pot, the\nlow-attention image concentrates on the spout of a watering pot, which will be confusing to represent the whole entity. With the help of attention, we can automatically learn knowledge representations from better images, alleviating the noises in multiple instances."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we propose the IKRL models learning knowledge representations with images. We utilize neural networks and a projection module to model each image, and then construct the aggregated image-based representations by combining multiple image instances based on attention. Experimental results confirm that our models are capable of encoding image information into knowledge representations.\nWe will explore the following research directions in future: (1) the quality of image representations is essential. We will utilize more sophisticated models to extract better image features, and further explore the performances in some specific domains. (2) The current IKRL models are based on TransE. We will explore the effectiveness of our models when\nextended to other enhanced translation-based methods."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of ICLR,", "citeRegEx": "Bahdanau et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of AISTATS", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. Joint learning of words", "meaning representations for open-text semantic parsing"], "venue": "pages 127\u2013135,", "citeRegEx": "Bordes et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of NIPS", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multirelational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. A semantic matching energy function for learning with multi-relational data"], "venue": "94(2):233\u2013259,", "citeRegEx": "Bordes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "LiJia Li", "Kai Li", "Li Fei-Fei"], "venue": "Proceedings of CVPR, pages 248\u2013255,", "citeRegEx": "Deng et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of ACL", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao. Knowledge graph embedding via dynamic mapping matrix"], "venue": "pages 687\u2013696,", "citeRegEx": "Ji et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "Proceedings of ACM MM, pages 675\u2013678,", "citeRegEx": "Jia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "Proceedings of NIPS,", "citeRegEx": "Kiros et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of NIPS", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI,", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of HLTNAACL,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "et al", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "Recurrent models of visual attention. In Proceedings of NIPS, pages 2204\u20132212,", "citeRegEx": "Mnih et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Rush et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Black holes and white rabbits: Metaphor identification with visual features", "author": ["Ekaterina Shutova", "Douwe Kiela", "Jean Maillard"], "venue": "Proceedings of NAACL,", "citeRegEx": "Shutova et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of NIPS", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of EMNLP", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph", "text jointly embedding"], "venue": "pages 1591\u20131601,", "citeRegEx": "Wang et al.. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of AAAI", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph embedding by translating on hyperplanes"], "venue": "pages 1112\u20131119,", "citeRegEx": "Wang et al.. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun"], "venue": "Proceedings of AAAI,", "citeRegEx": "Xie et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "Proceedings of ICLR,", "citeRegEx": "Yang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural generative question answering", "author": ["Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li"], "venue": "Proceedings of IJCAI,", "citeRegEx": "Yin et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Knowledge graphs (KGs), which provide huge amount of structured information for entities and relations, have been successfully utilized in various fields such as knowledge inference [Yang et al., 2014] and question answering [Yin et al.", "startOffset": 182, "endOffset": 201}, {"referenceID": 20, "context": ", 2014] and question answering [Yin et al., 2016].", "startOffset": 31, "endOffset": 49}, {"referenceID": 2, "context": "To model knowledge graphs, translation-based methods are proposed, which project both entities and relations into a continuous low-dimensional semantic space, with relations considered to be translating operations between head and tail entities [Bordes et al., 2013].", "startOffset": 245, "endOffset": 266}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] models both entities and relations into the same low-dimensional continuous vector space, with relations considered to be translating operations between head and tail entities.", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "To address this problem, TransH [Wang et al., 2014b] proposes relation-specific hyperplanes for translations between entities.", "startOffset": 32, "endOffset": 52}, {"referenceID": 9, "context": "TransR [Lin et al., 2015] models entities and relations in different vector spaces, projecting entities from entity space to relation spaces with relation-specific matrices.", "startOffset": 7, "endOffset": 25}, {"referenceID": 5, "context": "TransD [Ji et al., 2015] further ar X iv :1 60 9.", "startOffset": 7, "endOffset": 24}, {"referenceID": 16, "context": "To utilize rich textual information, [Wang et al., 2014a] projects both entities and words into a joint vector space with alignment models.", "startOffset": 37, "endOffset": 57}, {"referenceID": 18, "context": "[Xie et al., 2016] directly constructs entity representations from entity descriptions, which is capable of modeling new entities.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "As for visual information, multimodal representations based on words and images are widely used on various tasks like image-sentence ranking [Kiros et al., 2014] and metaphor identification [Shutova et al.", "startOffset": 141, "endOffset": 161}, {"referenceID": 14, "context": ", 2014] and metaphor identification [Shutova et al., 2016].", "startOffset": 36, "endOffset": 58}, {"referenceID": 18, "context": "To utilize entity image information, we propose two kinds of representations for each entity inspired by [Xie et al., 2016].", "startOffset": 105, "endOffset": 123}, {"referenceID": 8, "context": "We utilize AlexNet, a deep neural network that contains five convolution layers, two fullyconnected layers and a softmax layer, to extract image features [Krizhevsky et al., 2012].", "startOffset": 154, "endOffset": 179}, {"referenceID": 14, "context": "Inspired by [Shutova et al., 2016], we take the 4096-dimensional embeddings which are outputs", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "It has been widely utilized in various fields such as image classification [Mnih et al., 2014], machine translation [Bahdanau et al.", "startOffset": 75, "endOffset": 94}, {"referenceID": 0, "context": ", 2014], machine translation [Bahdanau et al., 2015] and abstractive sentence summarization [Rush et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 13, "context": ", 2015] and abstractive sentence summarization [Rush et al., 2015].", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "As for the image representation module, we utilize a deep learning framework Caffe [Jia et al., 2014] to implement AlexNet, which is pre-trained on ILSVRC 2012 with a minor variation from the version described in [Krizhevsky et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 8, "context": ", 2014] to implement AlexNet, which is pre-trained on ILSVRC 2012 with a minor variation from the version described in [Krizhevsky et al., 2012].", "startOffset": 119, "endOffset": 144}, {"referenceID": 3, "context": "WN9-IMG is a sub-graph of WN18 utilized in [Bordes et al., 2014], which is originally extracted from WordNet [Miller, 1995].", "startOffset": 43, "endOffset": 64}, {"referenceID": 11, "context": ", 2014], which is originally extracted from WordNet [Miller, 1995].", "startOffset": 52, "endOffset": 66}, {"referenceID": 4, "context": "For the consideration of image quality, we provide 63,225 images extracted from ImageNet, which is a huge image database containing over 21,000 synsets organized according to the WordNet hierarchy [Deng et al., 2009].", "startOffset": 197, "endOffset": 216}, {"referenceID": 2, "context": "We implement TransE [Bordes et al., 2013] and TransR [Lin et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 9, "context": ", 2013] and TransR [Lin et al., 2015] as our baselines, with the same experimental settings reported in their papers.", "startOffset": 19, "endOffset": 37}, {"referenceID": 1, "context": "This task has been widely used to evaluate the quality of knowledge representations [Bordes et al., 2012; Bordes et al., 2013].", "startOffset": 84, "endOffset": 126}, {"referenceID": 2, "context": "This task has been widely used to evaluate the quality of knowledge representations [Bordes et al., 2012; Bordes et al., 2013].", "startOffset": 84, "endOffset": 126}, {"referenceID": 2, "context": "Following the same settings in [Bordes et al., 2013], we consider two measures as our evaluation metrics in entity prediction: (1) mean rank of correct entities (Mean Rank); (2) proportion of correct entity results ranked in top 10 (Hits@10).", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "We also follow the two evaluation settings named \u201cRaw\u201d and \u201cFilter\u201d used in [Bordes et al., 2013].", "startOffset": 76, "endOffset": 97}, {"referenceID": 15, "context": "Evaluation Protocol Triple classification aims to predict whether a triple fact (h, r, t) is correct or not according to the dissimilarity function [Socher et al., 2013].", "startOffset": 148, "endOffset": 169}, {"referenceID": 15, "context": "Since WN9-IMG has no explicit negative instances, we generate the negative instances by randomly replacing head or tail entities with another entity following the same protocol utilized in [Socher et al., 2013].", "startOffset": 189, "endOffset": 210}, {"referenceID": 10, "context": "[Mikolov et al., 2013] shows that word representations have some interesting semantic regularities such as v(king) \u2212 v(man) \u2248 v(queen) \u2212 v(woman), and the similar regularities have also been found on visual-semantic embeddings [Kiros et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": ", 2013] shows that word representations have some interesting semantic regularities such as v(king) \u2212 v(man) \u2248 v(queen) \u2212 v(woman), and the similar regularities have also been found on visual-semantic embeddings [Kiros et al., 2014].", "startOffset": 212, "endOffset": 232}], "year": 2016, "abstractText": "Entity images provide significant visual information that helps the construction of knowledge representations. Most conventional methods learn knowledge representations solely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model, where knowledge representations are learned with both triples and images. More specifically, for each image of an entity, we construct image-based representations via a neural image encoder, and these representations with respect to multiple image instances are then integrated via an attention-based method. We evaluate our models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.", "creator": "LaTeX with hyperref package"}}}