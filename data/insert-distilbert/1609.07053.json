{"id": "1609.07053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Semantic Tagging with Deep Residual Networks", "abstract": "we propose a novel semantic tagging security task, semtagging, tailored independently for the exclusive purpose of multilingual semantic parsing, and present the first tagger using deep residual networks ( resnets ). our tagger uses both word and character derivation representations and includes a novel residual bypass architecture. we evaluate the tagset depend both intrinsically on executing the new processing task of semantic tagging, as well as on part - of - cheek speech ( pos ) tagging. our system, consisting of a resnet and an auxiliary loss function predicting our semantic matching tags, significantly outperforms prior results on english universal dependencies pos tagging ( 95. 71 % accuracy on ud v1. 2 and 95. 67 % accuracy achieved on ud v1. 3 ).", "histories": [["v1", "Thu, 22 Sep 2016 16:34:00 GMT  (184kb,D)", "http://arxiv.org/abs/1609.07053v1", "10 pages, to appear at COLING 2016"], ["v2", "Mon, 31 Oct 2016 18:33:13 GMT  (118kb,D)", "http://arxiv.org/abs/1609.07053v2", "COLING 2016, camera ready version"]], "COMMENTS": "10 pages, to appear at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["johannes bjerva", "barbara plank", "johan bos"], "accepted": false, "id": "1609.07053"}, "pdf": {"name": "1609.07053.pdf", "metadata": {"source": "CRF", "title": "Semantic Tagging with Deep Residual Networks", "authors": ["Johannes Bjerva", "Barbara Plank", "Johan Bos"], "emails": ["j.bjerva@rug.nl", "b.plank@rug.nl", "johan.bos@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "A key issue in computational semantics is the transferability of semantic information across languages. Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014). However, these tags are often customised for the language at hand (Marcus et al., 1993) or massively abstracted, such as the Universal Dependencies tagset (Nivre et al., 2016). Furthermore, POS tags are syntactically oriented, and therefore often contain both irrelevant and insufficient information for semantic analysis and deeper semantic processing. This means that, although POS tags are highly useful for many downstream tasks, they are unsuitable both for semantic parsing in general, and for tasks such as recognising textual entailment.\nWe present a novel set of semantic labels tailored for the purpose of multilingual semantic parsing. This tagset (i) abstracts over POS and named entity types; (ii) fills gaps in semantic modelling by adding new categories (for instance for phenomena like negation, modality, and quantification); and (iii) generalises over specific languages (see Section 2). We introduce and motivate this new task in this paper, and refer to it as semantic tagging. Our experiments aim to answer the following two research questions:\n1. Given an annotated corpus of semantic tags, it is straightforward to apply off-the-shelf sequence taggers. Can we significantly outperform these with recent neural network architectures?\n2. Semantic tagging is essential for deep semantic parsing. Can we find evidence that semtags are effective also for other NLP tasks?\nTo address the first question, we will look at convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which are both highly prominent approaches in recent natural language processing (NLP) literature. A recent development is the emergence of deep residual networks (ResNets), a building block for CNNs. ResNets consist of several stacked residual units, which can be thought of as a collection of convolutional layers coupled with a \u2018shortcut\u2019 which aids the propagation of the signal in a neural network. This allows for the construction of much deeper networks, since keeping a \u2018clean\u2019 information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/\nar X\niv :1\n60 9.\n07 05\n3v 1\n[ cs\n.C L\n] 2\n2 Se\np 20\n16\nalso seen some recent use in NLP (O\u0308stling, 2016; Conneau et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks.\nTo answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging."}, {"heading": "2 Semantic Tagging", "text": ""}, {"heading": "2.1 Background", "text": "We refer to semantic tagging, or semtagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper these units can be morphemes, words, or multi-word expressions. The linguistic information traditionally obtained for deep processing is insufficient for fine-grained lexical semantic analysis. The widely used Penn Treebank (PTB) Partof-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples.\nThere are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single language, but it falls short when considering a multilingual setting. Furthermore, determiners like any can have several interpretations and need to be disambiguated in context.\nSemantic tagging does not only apply to determiners, but reaches all parts of speech. Other examples where semantic classes disambiguate are reflexive versus emphasising pronouns (both POS tagged as PRP, personal pronoun); the comma, that could be a conjunction, disjunction, or apposition; intersective vs. subsective and privative adjectives (all POS tagged as JJ, adjective); proximal vs. medial and distal demonstratives (see Example 1); subordinate vs. coordinate discourse relations; agent nouns vs. entity nouns. The set of semantic tags that we use in this paper is established in a data-driven manner, considering four languages in a parallel corpus (English, German, Dutch and Italian). This first inventory of classes comprises 13 coarse-grained tags and 66 fine-grained tags (see Table 1). As can be seen from this table and the examples given below, the tagset also includes named entity classes (see also Example 2).\n(1) These PRX cats CON live ENS in REL that DST house CON . NIL\n(2) Ukraine GPE \u2019s HAS glory CON has ENT not NOT yet IST perished EXT , NIL neither NOT her HAS freedom CON . NIL\nIn Example 1, both these and that would be tagged as DT. However, with our semantic tagset, they are disambiguated as PRX (proximal) and DST (distal). In Example 2, Ukraine is tagged as GPE rather than NNP."}, {"heading": "2.2 Annotated Data", "text": "We use two semtag datasets. The Groningen Meaning Bank (GMB) corpus of English texts (1.4 million words) containing silver standard semantic tags obtained by running a simple rule-based semantic tagger (Bos et al., Forthcoming). This tagger uses POS and named entity tags available in the GMB (automatically obtained with the C&C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually crafted rules to output semantic tags. Some tags related to specific phenomena were hand-corrected in a second stage.\nOur second dataset is smaller but equipped with gold standard semantic tags and used for testing (PMB, the Parallel Meaning Bank). It comprises a selection of 400 sentences of the English part of a parallel corpus. It has no overlap with the GMB corpus. For this dataset, we used the Elephant tokeniser, which performs word, multi-word and sentence segmentation (Evang et al., 2013). We then used the simple rule-based semantic tagger described above to get an initial set of tags. These tags were then corrected by a human annotator (one of the authors of this paper).\nFor the extrinsic evaluation, we use the English portion of the Universal Dependencies dataset, version 1.2 and 1.3 (Nivre et al., 2016). An overview of the data used is shown in Table 2."}, {"heading": "3 Method", "text": "Our tagger is a hierarchical deep neural network consisting of a bidirectional Gated Recurrent Unit (GRU) network a the upper level, and a Convolutional Neural Network (CNN) and/or Deep Residual Network (ResNet) at the lower level, including an optional novel residual bypass function (cf. Figure 1)."}, {"heading": "3.1 Gated Recurrent Unit networks", "text": "GRUs (Cho et al., 2014) are a recently introduced variant of RNNs, and are designed to prevent vanishing gradients, thus being able to cope with longer input sequences than vanilla RNNs. GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014). A bi-directional GRU is a GRU which makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015). Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016). We build on previous approaches by combining bi-GRUs with character representations from a basic CNN and ResNets."}, {"heading": "3.2 Deep Residual Networks", "text": "Deep Residual Networks (ResNets) are built up by stacking residual units. A residual unit can be expressed as:\nyl = h(xl) + F(xl,Wl), xl+1 = f(yl),\n(3)\nwhere xl and xl+1 are the input and output of the l-th layer,Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.g., the identity function (He et al., 2015), which we also use in our experiments. ResNets can be intuitively understood by thinking of residual functions as paths through which information can propagate easily. This means that, in every layer, a ResNet learns more complex feature combinations, which it combines with the shallower representation from the previous layer. This architecture allows for the construction of much deeper networks. ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers. In this paper we use the assymetric variant of ResNets as described in Equation 9 in He et al. (2016):\nxl+1 = xl + F(f\u0302(xl),Wl). (4)\nResNets have been very recently applied in NLP to morphological reinflection (O\u0308stling, 2016) and tasks such as sentiment analysis and text categorisation (Conneau et al., 2016). Our work is the first to apply ResNets to NLP sequence tagging tasks. We further contribute to the literature on ResNets by introducing a residual bypass function. The intuition is to combine both deep and shallow processing, which opens a path of easy signal propagation between lower and higher layers in the network."}, {"heading": "3.3 Modelling character information and residual bypass", "text": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly. Dos Santos and Zadrozny (2014) construct character-based word-level representations by running a convolutional network over the character representations of each word. All of these approaches have in common that the character-based representation is passed through the entire remainder of the network. Our work is the\nfirst to combine the use of character-level representations with both deep processing (i.e., passing this representation through the network) and shallow processing (i.e., bypassing the network in our residual bypass function). We achieve this by applying our novel residual bypass function to our character representations, inspired by the success of ResNets. In particular, we first apply the bypass to a CNN-based model achieving large gains over a plain CNN, and later evaluate its effectiveness in a ResNet.\nA core intuition behind CNNs is the processing of an input signal in a hierarchical manner (LeCun et al., 1998; Goodfellow et al., 2016). Taking, e.g., a 3-dimensional image (width\u00d7 height\u00d7 depth), the approach is typically to reduce spatial dimensions of the image while increasing depth. This hierarchical processing allows a CNN to learn high-level features of an input, essential to image recognition tasks. A drawback of this method, however, is that lower-level features are potentially lost in the abstraction to higher-level features. This issue is partially countered by ResNets, as information is allowed to flow more easily between residual blocks. However, this approach does not allow for simple and direct use of information in the network input in final layers. To alleviate this issue, we present a residual bypass function, which can be seen as a global residual function (depicted in Figure 1). This function allows both lower-level and higher-level features to be taken directly into account in the final layers of the network. The intuition behind using such a global residual function in NLP is that character information primarily ought to be of importance for the prediction of the current word. Hence, allowing these representations to bypass our bi-GRU might be beneficial. This residual bypass function is not dependent on the usage of ResNets, and can be combined with other NN architectures as in our experiments. We define the penultimate layer of a network with n layers, using a residual bypass, as follows:\nyn\u22121 = h(xn\u22121) + F(xi,Wi), (5)\nwhere xi and Wi are the input and weights of the ith layer, F is a residual function (in our case the identity function), and h(xn\u22121) is the output of the penultimate layer. In our experiments, we apply a residual bypass function to our convolutional character representations."}, {"heading": "3.4 System description", "text": "The core of our architecture consists of a bi-GRU taking an input based on words and/or characters, with an optional residual bypass as defined in subsection 3.3. We experiment with a basic CNN, ResNets and our novel residual bypass function. We also implemented a variant of the Inception model (Szegedy et al., 2015), but found this to be outperformed by ResNets. Our system is implemented in Keras using the Tensorflow backend (Chollet, 2015; Abadi et al., 2016), and the code is available at https://github.com/bjerva/semtagger.\nWe represent each sentence using both a character-based representation (Sc) and a word-based representation (Sw). The character-based representation is a 3-dimensional matrix Sc \u2208 Rs\u00d7w\u00d7dc , where s is the zero-padded sentence length, w is the zero-padded word length, and dc is the dimensionality of the character embeddings. The word-based representation is a 2-dimensional matrix Sw \u2208 Rs\u00d7dw , where s is the zero-padded sentence length and dw is the dimensionality of the word embeddings. We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings.\nWord embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature.\nWe use CNNs for character-level modelling. Our basic CNN is inspired by dos Santos and Zadrozny (2014), who use character-representations to produce local features around each character of a word, and combine these with a maximum pooling operation in order to create fixed-size character-level word embeddings. The convolutions used in this manner cover a few neighbouring letters at a time, as well as the entire character vector dimension (dc). In contrast to dos Santos and Zadrozny (2014), we treat a word analogously to an image. That is to say, we see a word of n characters embedded in a space\nwith dimensionality dc as an image of dimensionality n \u00d7 dc. This view gives us additional freedom in terms of sizes of convolutional patches used, which offers more computational flexibility than using only, e.g., 4\u00d7 dc convolutions. This view is applied to all CNN variations explored in this work.\nA neural network is trained with respect to some loss function, such as the cross-entropy between the predicted tag probability distribution and the gold probability distribution. Recent work has shown that the addition of an auxiliary loss function can be beneficial to several tasks. Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition. Plank et al. (2016) use the log frequency of the current token as an auxiliary loss function, and find this to improve POS tagging accuracy. Since our semantic tagging task is based on predicting fine semtags, which can be mapped to coarse semtags, we add the prediction of these coarse semtags as an auxiliary loss for the semtagging experiments. Similarly, we also experiment with POS tagging, where we use the fine semtags as an auxiliary information."}, {"heading": "3.4.1 Hyperparameters", "text": "All hyperparameters are tuned with respect to loss on the semtag validation set. We use rectified linear units (ReLUs) for all activation functions (Nair and Hinton, 2010), and apply dropout with p = 0.1 to both input weights and recurrent weights in the bi-GRU (Srivastava et al., 2014). In the CNNs, we apply batch normalisation (Ioffe and Szegedy, 2015) followed by dropout with p = 0.5 after each layer. In our basic CNN, we apply a 4\u00d7 8 convolution, followed by 2\u00d7 2 maximum pooling, followed by 4\u00d7 4 convolution and another 2 \u00d7 2 maximum pooling. Our ResNet has the same setup, with the addition of a residual connection. We also experimented with using average pooling instead of maximum pooling, but this yielded lower validation data performance on the semantic tagging task. We set both dc and dw to 64. All GRU layers have 100 hidden units. All experiments were run with early stopping monitoring validation set loss, using a maximum of 50 epochs. We use a batch size of 500. Optimisation is done using the ADAM algorithm (Kingma and Ba, 2014), with the categorical cross-entropy loss function as training objective. The main and auxiliary loss functions have a weighting parameter, \u03bb. In our experiments, we weight the auxiliary loss with \u03bb = 0.1, as set on the semtag auxiliary task.\nMulti-word expressions (MWEs) are especially prominent in the semtag data, where they are annotated as single tokens. Pre-trained word embeddings are unlikely to include entries such as \u2018International Organization for Migration\u2019, so we apply a simple heuristic in order to avoid treating most MWEs as unknown words. In particular, the representation of a MWE is set to the sum of the individual embeddings of each constituent word."}, {"heading": "4 Evaluation", "text": "We evaluate our tagger on two tasks: semantic tagging and POS tagging. Note that the tagger is developed solely on the semantic tagging task, using the GMB silver training and validation data. Hence, no further fine-tuning of hyperparameters for the POS tagging task is performed. We calculate significance using bootstrap resampling (Efron and Tibshirani, 1994). We manipulate the following independent variables in our experiments:\n1. character and word representations (~w,~c);\n2. residual bypass for character representations (~cbp);\n3. convolutional representations (Basic CNN and ResNets);\n4. auxiliary loss (using coarse semtags on ST and fine semtags on UD).\nWe compare our results to four baselines:\n1. the most frequent baseline per word (MFC), where we assign the most frequent tag for a word in the training data to that word in the test data, and unseen words get the global majority tag;\n2. the trigram statistic based TNT tagger offers a slightly tougher baseline (Brants, 2000);\n3. the BI-LSTM baseline, running the off-the-shelf state-of-the-art POS tagger for the UD dataset (Plank et al., 2016) (using default parameters with pre-trained Polyglot embeddings);\n4. we use a baseline consisting of running our own system with only a BI-GRU using word representations (~w), with pre-trained Polyglot embeddings."}, {"heading": "4.1 Experiments on semantic tagging", "text": "We evaluate our system on two semantic tagging (ST) datasets: our silver semtag dataset and our gold semtag dataset. For the +AUX condition we use coarse semtags as an auxiliary loss. Results from these experiments are shown in Table 3."}, {"heading": "4.2 Experiments on Part-of-Speech tagging", "text": "We evaluate our system on v1.2 and v1.3 of the English part of the Universal Dependencies (UD) data. We report results for POS tagging alone, comparing to commonly used baselines and prior work using LSTMs, as well as using the fine-grained semantic tags as auxiliary information. For the +AUX condition, we train a single joint model using a multi-task objective, with POS and ST as our two tasks. This model is trained on the concatenation of the ST silver data with the UD data, updating the loss of the respective task of an instance in each iteration. Hence, the weights leading to the UD softmax layer are not updated on the ST silver portion of the data, and vice-versa for the ST softmax layer on the UD portion of the data. Results from these experiments are shown in Table 4."}, {"heading": "5 Discussion", "text": ""}, {"heading": "5.1 Performance on semantic tagging", "text": "The overall best system is the ResNet combining both word and character representations ~c \u2227 ~w. It outperforms all baselines, including the recently proposed RNN-based bi-LSTM. On the ST silver data, a significant difference (p < 0.01) is found when comparing our best system to the strongest baseline (BI-LSTM). On the ST gold data, we observe significant differences at the alpha values recommended by S\u00f8gaard et al. (2014), with p < 0.0025. The residual bypass effectively helps improve the performance of the basic CNN. However, the tagging accuracy of the CNN falls below baselines. In addition, the large gap between gold and silver data for the CNN shows that the CNN model is more prone to overfitting, thus favouring the use of the ResNet. Adding the coarse-grained semtags as auxiliary task only helps for\nthe weaker CNN model. The ResNet does not benefit from this additional information, which is already captured in the fine-grained labels.\nIt is especially noteworthy that the ResNet character-only system performs remarkably well, as it outperforms the BI-GRU and TNT baselines, and is considerably better than the basic CNN. Since performance increases further when adding in ~w, it is clear that the character and word representations are complimentary in nature. The high results for characters only are particularly promising for multilingual language processing, a direction we want to explore next."}, {"heading": "5.2 Performance on POS tagging", "text": "Our system was tuned solely on semtag data. This is reflected in, e.g., the fact that even though our ~c\u2227 ~w ResNet system outperforms the Plank et al. (2016) system on semtags, we are substantially outperformed on UD 1.2 and 1.3 in this setup. However, adding an auxiliary loss based on our semtags markedly increases performance on POS tagging. In this setting, our tagger outperforms the BI-LSTM system, and results in new state-of-the-art results on both UD 1.2 (95.71% accuracy) and 1.3 (95.67% accuracy). The difference between the BI-LSTM system and our best system is significant at p < 0.0025.\nThe fact that the semantic tags improve POS tagging performance reflects two properties of semantic tags. Firstly, it indicates that the semantic tags carry important information for the prediction of POS tags. This should come as no surprise, considering the fact that the semtags abstract over and carry more information than POS tags. Secondly, it indicates that the new semantic tagset and released dataset are useful for downstream NLP tasks. In this paper we show this by using semtags as an auxiliary loss. In future work we aim to investigate the effect of introducing the semtags directly as features into the embedded input representation."}, {"heading": "5.3 ResNets for sequence tagging", "text": "This work is the first to apply ResNets to NLP tagging tasks. Our experiments show that ResNets significantly outperform standard convolutional networks on both POS tagging and semtagging. ResNets allow better signal propagation and carry lower risk of overfitting, allowing for the model to capture more elaborate feature representations than in a standard CNN."}, {"heading": "5.4 Pre-trained embeddings", "text": "In our main experiments, we initialised the word embedding layer with pre-trained polyglot embeddings. We also compared this with initialising this layer from a uniform distribution over the interval [\u22120.05, 0.05). For semantic tagging, the difference with random initialisation is negligible, with pretrained embeddings yielding an increase in about 0.04% accuracy. For POS tagging, however, using pre-trained embeddings increased accuracy by almost 3 percentage points for the ResNet."}, {"heading": "6 Conclusions", "text": "We introduce a semantic tagset tailored for multilingual semantic parsing. We evaluate tagging performance using standard CNNs and the recently emerged ResNets. ResNets are more robust and result in our best model. Combining word and ResNet-based character representations helps to outperform stateof-the art taggers on semantic tagging. Coupling this with an auxiliary loss from our semantic tagset yields state-of-the art performance on the UD 1.2 and 1.3 POS datasets."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Robert O\u0308stling for tips on ResNets, and Calle Bo\u0308rstell and Johan Sjons for feedback on earlier versions of this manuscript. We would like to thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine high performance computing cluster. This work was partially funded by the NWO\u2013VICI grant \u201cLost in Translation \u2013 Found in Meaning\u201d (288-89-003)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "author": ["nanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vi\u00e9gas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vi\u00e9gas et al\\.", "year": 2016}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "CoNLL-2013.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "ACL, pages 1415\u20131425.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Wide-Coverage Semantic Analysis with Boxer", "author": ["Johan Bos."], "venue": "J. Bos and R. Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277\u2013286. College Publications.", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Tnt: a statistical part-of-speech tagger", "author": ["Thorsten Brants."], "venue": "Proceedings of the sixth conference on Applied natural language processing, pages 224\u2013231. Association for Computational Linguistics.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "The Semantics of Grammatical Dependencies, volume 23", "author": ["Alastair Butler."], "venue": "Emerald Group Publishing Limited.", "citeRegEx": "Butler.,? 2010", "shortCiteRegEx": "Butler.", "year": 2010}, {"title": "Open-domain name error detection using a multi-task rnn", "author": ["Hao Cheng", "Hao Fang", "Mari Ostendorf."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github.com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Text segmentation with character-level text embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Workshop on Deep Learning for Audio, Speech and Language Processing, ICML.", "citeRegEx": "Chrupa\u0142a.,? 2013", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2013}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Procedings of ACL 2016, arXiv preprint arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for natural language processing", "author": ["Alexis Conneau", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Yann Lecun"], "venue": "arXiv preprint arXiv:1606.01781", "citeRegEx": "Conneau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Conneau et al\\.", "year": 2016}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Ivan Sag", "Carl Pollard."], "venue": "Journal of Research on Language and Computation, 3(2\u20133):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Linguistically Motivated Large-Scale NLP with C&C and Boxer", "author": ["James Curran", "Stephen Clark", "Johan Bos."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33\u201336, Prague, Czech Republic.", "citeRegEx": "Curran et al\\.,? 2007", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In ICML,", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Elephant: Sequence labeling for word and sentence segmentation", "author": ["Kilian Evang", "Valerio Basile", "Grzegorz Chrupa\u0142a", "Johan Bos."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422\u20131426.", "citeRegEx": "Evang et al\\.,? 2013", "shortCiteRegEx": "Evang et al\\.", "year": 2013}, {"title": "Multilingual language processing", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1603.05027.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Universal dependencies v1: A multilingual treebank collection", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira"], "venue": "In Proceedings of the 10th International Conference on Language Resources and Evaluation", "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert \u00d6stling."], "venue": "Proceedings of the 2016 Meeting of SIGMORPHON, Berlin, Germany. Association for Computational Linguistics.", "citeRegEx": "\u00d6stling.,? 2016", "shortCiteRegEx": "\u00d6stling.", "year": 2016}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of ACL 2016, arXiv preprint arXiv:1604.05529.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Shallow semantic parsing using support vector machines", "author": ["Sameer S Pradhan", "Wayne Ward", "Kadri Hacioglu", "James H Martin", "Daniel Jurafsky."], "venue": "HLT-NAACL, pages 233\u2013240.", "citeRegEx": "Pradhan et al\\.,? 2004", "shortCiteRegEx": "Pradhan et al\\.", "year": 2004}, {"title": "Whats in a p-value in nlp? In CoNLL-2014", "author": ["Anders S\u00f8gaard", "Anders Johannsen", "Barbara Plank", "Dirk Hovy", "Hector Martinez"], "venue": null, "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u2013 1958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A unified tagging solution: Bidirectional lstm recurrent neural network with word embedding", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao."], "venue": "arXiv preprint arXiv:1511.00215.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen."], "venue": "arXiv preprint arXiv:1603.06270.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 13, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 3, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 5, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 2, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 25, "context": "However, these tags are often customised for the language at hand (Marcus et al., 1993) or massively abstracted, such as the Universal Dependencies tagset (Nivre et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 27, "context": ", 1993) or massively abstracted, such as the Universal Dependencies tagset (Nivre et al., 2016).", "startOffset": 75, "endOffset": 95}, {"referenceID": 21, "context": "This allows for the construction of much deeper networks, since keeping a \u2018clean\u2019 information path in the network facilitates optimisation (He et al., 2016).", "startOffset": 139, "endOffset": 156}, {"referenceID": 20, "context": "ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have", "startOffset": 88, "endOffset": 122}, {"referenceID": 21, "context": "ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have", "startOffset": 88, "endOffset": 122}, {"referenceID": 28, "context": "also seen some recent use in NLP (\u00d6stling, 2016; Conneau et al., 2016).", "startOffset": 33, "endOffset": 70}, {"referenceID": 12, "context": "also seen some recent use in NLP (\u00d6stling, 2016; Conneau et al., 2016).", "startOffset": 33, "endOffset": 70}, {"referenceID": 25, "context": "The widely used Penn Treebank (PTB) Partof-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing.", "startOffset": 57, "endOffset": 78}, {"referenceID": 3, "context": "Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010).", "startOffset": 76, "endOffset": 101}, {"referenceID": 5, "context": "Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010).", "startOffset": 76, "endOffset": 101}, {"referenceID": 14, "context": "This tagger uses POS and named entity tags available in the GMB (automatically obtained with the C&C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually crafted rules to output semantic tags.", "startOffset": 107, "endOffset": 128}, {"referenceID": 17, "context": "For this dataset, we used the Elephant tokeniser, which performs word, multi-word and sentence segmentation (Evang et al., 2013).", "startOffset": 108, "endOffset": 128}, {"referenceID": 27, "context": "3 (Nivre et al., 2016).", "startOffset": 2, "endOffset": 22}, {"referenceID": 7, "context": "GRUs (Cho et al., 2014) are a recently introduced variant of RNNs, and are designed to prevent vanishing gradients, thus being able to cope with longer input sequences than vanilla RNNs.", "startOffset": 5, "endOffset": 23}, {"referenceID": 10, "context": "GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014).", "startOffset": 119, "endOffset": 139}, {"referenceID": 19, "context": "A bi-directional GRU is a GRU which makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015).", "startOffset": 169, "endOffset": 215}, {"referenceID": 35, "context": "Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016).", "startOffset": 150, "endOffset": 208}, {"referenceID": 36, "context": "Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016).", "startOffset": 150, "endOffset": 208}, {"referenceID": 29, "context": "Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016).", "startOffset": 150, "endOffset": 208}, {"referenceID": 21, "context": "where xl and xl+1 are the input and output of the l-th layer,Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.", "startOffset": 128, "endOffset": 145}, {"referenceID": 20, "context": ", the identity function (He et al., 2015), which we also use in our experiments.", "startOffset": 24, "endOffset": 41}, {"referenceID": 20, "context": "ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers.", "startOffset": 130, "endOffset": 164}, {"referenceID": 21, "context": "ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers.", "startOffset": 130, "endOffset": 164}, {"referenceID": 20, "context": "where xl and xl+1 are the input and output of the l-th layer,Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.g., the identity function (He et al., 2015), which we also use in our experiments. ResNets can be intuitively understood by thinking of residual functions as paths through which information can propagate easily. This means that, in every layer, a ResNet learns more complex feature combinations, which it combines with the shallower representation from the previous layer. This architecture allows for the construction of much deeper networks. ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers. In this paper we use the assymetric variant of ResNets as described in Equation 9 in He et al. (2016):", "startOffset": 129, "endOffset": 942}, {"referenceID": 28, "context": "ResNets have been very recently applied in NLP to morphological reinflection (\u00d6stling, 2016) and tasks such as sentiment analysis and text categorisation (Conneau et al.", "startOffset": 77, "endOffset": 92}, {"referenceID": 12, "context": "ResNets have been very recently applied in NLP to morphological reinflection (\u00d6stling, 2016) and tasks such as sentiment analysis and text categorisation (Conneau et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 33, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 9, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 37, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 11, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 18, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al.", "startOffset": 8, "endOffset": 179}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations.", "startOffset": 8, "endOffset": 202}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly.", "startOffset": 8, "endOffset": 339}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly. Dos Santos and Zadrozny (2014) construct character-based word-level representations by running a convolutional network over the character representations of each word.", "startOffset": 8, "endOffset": 445}, {"referenceID": 24, "context": "A core intuition behind CNNs is the processing of an input signal in a hierarchical manner (LeCun et al., 1998; Goodfellow et al., 2016).", "startOffset": 91, "endOffset": 136}, {"referenceID": 34, "context": "We also implemented a variant of the Inception model (Szegedy et al., 2015), but found this to be outperformed by ResNets.", "startOffset": 53, "endOffset": 75}, {"referenceID": 8, "context": "Our system is implemented in Keras using the Tensorflow backend (Chollet, 2015; Abadi et al., 2016), and the code is available at https://github.", "startOffset": 64, "endOffset": 99}, {"referenceID": 1, "context": "We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings.", "startOffset": 39, "endOffset": 61}, {"referenceID": 10, "context": "Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014).", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": "We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. We use CNNs for character-level modelling. Our basic CNN is inspired by dos Santos and Zadrozny (2014), who use character-representations to produce local features around each character of a word, and combine these with a maximum pooling operation in order to create fixed-size character-level word embeddings.", "startOffset": 40, "endOffset": 731}, {"referenceID": 1, "context": "We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. We use CNNs for character-level modelling. Our basic CNN is inspired by dos Santos and Zadrozny (2014), who use character-representations to produce local features around each character of a word, and combine these with a maximum pooling operation in order to create fixed-size character-level word embeddings. The convolutions used in this manner cover a few neighbouring letters at a time, as well as the entire character vector dimension (dc). In contrast to dos Santos and Zadrozny (2014), we treat a word analogously to an image.", "startOffset": 40, "endOffset": 1121}, {"referenceID": 6, "context": "Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition. Plank et al. (2016) use the log frequency of the current token as an auxiliary loss function, and find this to improve POS tagging accuracy.", "startOffset": 0, "endOffset": 177}, {"referenceID": 26, "context": "We use rectified linear units (ReLUs) for all activation functions (Nair and Hinton, 2010), and apply dropout with p = 0.", "startOffset": 67, "endOffset": 90}, {"referenceID": 32, "context": "1 to both input weights and recurrent weights in the bi-GRU (Srivastava et al., 2014).", "startOffset": 60, "endOffset": 85}, {"referenceID": 22, "context": "In the CNNs, we apply batch normalisation (Ioffe and Szegedy, 2015) followed by dropout with p = 0.", "startOffset": 42, "endOffset": 67}, {"referenceID": 23, "context": "Optimisation is done using the ADAM algorithm (Kingma and Ba, 2014), with the categorical cross-entropy loss function as training objective.", "startOffset": 46, "endOffset": 67}, {"referenceID": 16, "context": "We calculate significance using bootstrap resampling (Efron and Tibshirani, 1994).", "startOffset": 53, "endOffset": 81}, {"referenceID": 4, "context": "the trigram statistic based TNT tagger offers a slightly tougher baseline (Brants, 2000);", "startOffset": 74, "endOffset": 88}, {"referenceID": 29, "context": "the BI-LSTM baseline, running the off-the-shelf state-of-the-art POS tagger for the UD dataset (Plank et al., 2016) (using default parameters with pre-trained Polyglot embeddings);", "startOffset": 95, "endOffset": 115}, {"referenceID": 29, "context": "MFC indicates the per-word most frequent class baseline, TNT indicates the TNT tagger, and BI-LSTM indicates the system by Plank et al. (2016). BI-GRU indicates the ~ w only baseline.", "startOffset": 123, "endOffset": 143}, {"referenceID": 31, "context": "On the ST gold data, we observe significant differences at the alpha values recommended by S\u00f8gaard et al. (2014), with p < 0.", "startOffset": 91, "endOffset": 113}, {"referenceID": 29, "context": ", the fact that even though our ~c\u2227 ~ w ResNet system outperforms the Plank et al. (2016) system on semtags, we are substantially outperformed on UD 1.", "startOffset": 70, "endOffset": 90}], "year": 2016, "abstractText": "We propose a novel semantic tagging task, semtagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks (ResNets). Our tagger uses both word and character representations and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging (95.71% accuracy on UD v1.2 and 95.67% accuracy on UD v1.3).", "creator": "TeX"}}}