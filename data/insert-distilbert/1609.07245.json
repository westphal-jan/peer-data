{"id": "1609.07245", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "A New Statistic Feature of the Short-Time Amplitude Spectrum Values for Human's Unvoiced Pronunciation", "abstract": "stochastic property of speech signal is a fundamental research topic in nonlinear speech analysis and processing. in this field paper, some multiple levels of randomness in speech signal are discussed, and the stochastic properties of unvoiced pronunciation are studied in good detail, which has not received sufficient research attention since before. the study is based on the signals of sustained unvoiced pronunciation captured in exactly the experiments, for which the amplitude and phase values both in the short - - time spectrum are studied as random variables. the statistics of amplitude for each frequency component is studied individually, based precisely on which a new property of \" consistent standard deviation coefficient \" is sufficiently revealed for the amplitude correction spectrum of unvoiced pronunciation. the specific relationship between the amplitude probability auditory distributions of drastically different frequency perception components clearly is further studied, which indicates that all the frequency components have a common prototype of transient amplitude probability distribution. as an adaptive and flexible probability distribution, the weibull distribution is adopted to fit the expectation - normalized amplitude spectrum data. the phase distribution for the short - time spectrum is also weakly studied, and the results show again a uniform distribution. similarly a synthesis method for unvoiced pronunciation is proposed as based on defining the weibull acoustic distribution of amplitude and uniform distribution independently of phase, which is implemented properly by stft with artificially generated short - time spectrum with random linear amplitude and phase. the synthesis results have identical quality of auditory perception as the original pronunciation, and have similar autocorrelation value as that of identifying the received original signal, theoretically which proves the expressive effectiveness of the proposed stochastic model of short - time spectrum for unvoiced pronunciation.", "histories": [["v1", "Fri, 23 Sep 2016 07:03:32 GMT  (809kb)", "http://arxiv.org/abs/1609.07245v1", "24 pages, 16 figures, original work"], ["v2", "Wed, 21 Dec 2016 08:57:51 GMT  (1282kb)", "http://arxiv.org/abs/1609.07245v2", "5 pages, 4 figures, original work"]], "COMMENTS": "24 pages, 16 figures, original work", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["xiaodong zhuang"], "accepted": false, "id": "1609.07245"}, "pdf": {"name": "1609.07245.pdf", "metadata": {"source": "CRF", "title": "Novel stochastic properties of the short-time spectrum for unvoiced pronunciation modeling and synthesis", "authors": ["Xiaodong Zhuang", "Nikos E. Mastorakis"], "emails": [], "sections": [{"heading": null, "text": "synthesis, consistent standard deviation coefficient"}, {"heading": "1. Introduction", "text": "Speech signal can be mathematically modeled by stochastic process. There have been researches on stochastic properties of continuous speech signals (i.e. signals of daily conversations). Such researches are based on the large amount of speech data in corpora like TIMIT (Garofolo, 1993), AURORA (Hirsch and Pearce, 2000) or other database of daily speech signal from the internet (Gazor and Zhang, 2003). These studies have investigated the probability distribution for time-domain speech signal, and also for the data in transformed \u2217 The corresponding author\ndomain as well, such as DCT, KLT, DFT, etc. In these studies, several probability distributions have been tested to propose a stochastic model for comprehensible speech with meaningful language contents (i.e. sentences or paragraphs). Such distributions include the Gaussian distribution (GD), Laplacian distribution (LD), Gamma distribution (\u0393D), Generalized Gaussian distribution (GGD), and Generalized Gamma distribution (G\u0393D). The probability distribution function (pdf) of time-domain speech signal was studied in 1950s and 1960s, in which the Gamma and Laplacian distributions were tested (Davenport, 1952; Richards, 1964). The two-side Gamma distribution was also found to be a good approximation of the underlying pdf for time-domain speech samples (Paez and Glisson, 1972; Jayant and Noll, 1984; Shin et al., 2005). For different speech lengths and different speech classes, the most proper pdf type was studied by Chi-square goodness-of-fit test on time-domain speech signal, with Laplacian and Gaussian distribution as two options (Jensen et al., 2005). Besides the time-domain, the pdf for transformed domain was also studied for Karhunen-Loeve (K-L) transform and DCT, where the Laplacian distribution showed prevailing performance by Chi-square test and moment test (Gazor and Zhang, 2003). In frequency domain, the pdf of the spectrum\u2019s real and imaginary parts was studied and modeled by Generalized Gaussian distribution based on Kullback-Leibler divergence as the fitting criterion (Tashev, 2010). And the amplitude spectrum of speech signal was modeled as Rayleigh distribution (Erkelens, 2007). These stochastic models facilitates the application of speech enhancement (Martin, 2005; Loizou, 2007; Boubakir and Berkani, 2010; Borgstrom and Alwan, 2011) and voice activity detection (Sohn et al., 1999), whose object is the daily-life speech interfered by some kind of noise. Suchmodels also facilitate speech coding (Paez and Glisson, 1972) and speech recognition (Rabiner and Juang, 1993; Huang and Zhao, 2000).\nThe stochastic property of speech signal can be analyzed on multiple levels. For one level, the speech signal in daily communication varies with the continuously changing of the language content (i.e. different words in sentences). This can be regarded as randomness at the language content level in speech. On the other hand, for the same language content (i.e. the same word or sentence), the speech signal will also vary randomly due to the speaker characteristics such as gender, age, emotion, etc.\nMoreover, even for a sustained pronunciation (such as a single phoneme) by a specific speaker, randomness still exists in the signal, which is caused by the physical mechanism of pronunciation. Such randomness has been observed and proved in previous research. For voiced pronunciation there are phenomena of jitter and shimmer observed (Teixeira et al., 2013; Ghosh and Narayanan, 2011; Farrus and Hernando, 2009), and for unvoiced pronunciation the sound source is random by itself which is caused by turbulence of air flow in the vocal tract (Sinder et al.,1998). Such randomness can be regarded as another different level of speech randomness.\nAlthough dozens of probability models have been proposed for speech signal, in existing researches the different levels of speech randomness discussed above have not been differentiated and studied separately. Since these studies are based on the large amount of speech data as daily-life sentences in corpora like TIMIT, AURORA, etc., the current stochastic models eventually represent the overall stochastic property, which is the combination of the several randomness levels in speech mentioned above. However, the\noverall statistic property of speech can not represent the specific properties of different pronunciation types, which is important for deeper understanding of pronunciation mechanism and improvement of algorithms in practical applications. As a fundamental aspect for understanding the nature of speech signal, detailed stochastic properties of different specific types of pronunciation need to be studied. Nevertheless, so far as the authors know, there is little study on the stochastic distribution of short-time spectrum of specific unvoiced pronunciations yet, which may reveal intrinsic property of such speech pronunciation.\nThe unvoiced pronunciation is closely related to the aerodynamic process in vocal tract, which is an ongoing research topic (Mittal et al., 2013; Lu et al., 2011, Sinder et al., 1996: McGowan, 1987). The physical process during unvoiced pronunciation is complicated, while the stochastic study of the signal produced may reveal some underlying properties of this process. The randomness in time-domain signal corresponds to the random fluctuation of its short-time spectrum. The focus of this paper is the random fluctuation of amplitude and phase for specific frequency component in the short-time spectrum of a sustained unvoiced pronunciation (or unvoiced phoneme). Moreover, the relationship between two different frequency components in amplitude probability distribution is also investigated, which is important but captured little research attention before. In Section 2, a novel property of \u201cconsistent standard deviation coefficient\u201d is revealed. Such property inspires the study of the relationship between the amplitude distributions of different frequency components in Section 3, where the Weibull distribution is adopted to model the prototype of amplitude distribution. In Section 4, combining the estimation of phase spectrum distribution, a stochastic model of short-time spectrum is proposed for unvoiced pronunciation. Based on this model, a synthesis method of unvoiced pronunciation is presented in Section 5, which proves the effectiveness of the proposed model, and can also serve as an efficient synthesis method in practice."}, {"heading": "2. The property of \u201cconsistent standard deviation coefficient\u201d for amplitude spectrum of unvoiced pronunciation", "text": "For unvoiced pronunciation, in the discrete spectrum obtained by STFT, the spectrum value of each discrete frequency component is considered as random variable due to the randomness of the signal. For each discrete frequency component, the statistics of short-time amplitude spectrum is studied by estimating the expectation and standard deviation as two basic statistics. Moreover, a novel stochastic property about the relationship between these two basic statistics is revealed for unvoiced pronunciation. Because large amount of data is needed for this statistic study, the signals captured and used in this paper are sustained unvoiced pronunciations, not the words or sentences in daily communication. In another word, the study concentrates on the signal randomness at the level of single unvoiced phoneme, which is different from most previous research.\nAlthough well-organized corpuses like TIMIT have well labeled the detailed words, syllables or even phonemes on the time axis, the single pronunciations in such data are too short for statistic study. Since currently there is little corpus of sustained phoneme pronunciation, signals have been captured by the authors using microphones connected to the sound card on computers. To guarantee the generality of experimental results and conclusion, signals have been captured for a group of unvoiced pronunciation in English spoken by different speakers, and on different recording platforms (different microphones and sound\ncards on different computers). In the collection of pronunciation signal, the speakers were informed with the requirements of stable pronunciation during sufficient time length, based on which reliable statistic study can be achieved. Since the stability of pronunciation in recording largely determines the effectiveness of further analysis, for each speaker the signals were captured repeatedly for several times, so that the most stable signal suitable for further study can be selected. Because the unvoiced pronunciation is produced by the aeroacoustic process in the vocal tract without vocal cord vibration, it is much less affected by individual difference such as age and gender. The signals were recorded at sample frequency of 16 kHz, with 16 bit per sample.\nFor a recorded signal of a sustained unvoiced pronunciation, STFT is performed on that signal, and large amount of short-time spectrum data can then be obtained, based on which the expectation and variance can be estimated for each frequency component. The programming is implemented in Matlab. In the experiments, the frame length is 512, which corresponds to a time interval of 32ms with a 16 kHz sampling frequency. A Hamming window is used on each frame for STFT. For all frequency components, the estimation results of amplitude expectation and variance can be represented by two functions \u03bc(\u03c9k) and \u03c32(\u03c9k), which are the estimated expectation and variance of \u03c9k\u2019s amplitude respectively:\n1\n1( ) ( ) N\ni k k i a N \u03bc \u03c9 \u03c9 = =  (1)\n( )22 1\n1( ) ( ) ( ) N\ni k k k i a N \u03c3 \u03c9 \u03c9 \u03bc \u03c9 = = \u2212 (2)\nwhere N is the frame number, \u03c9k is the k-th frequency component in STFT, and ai(\u03c9k) is the amplitude spectrum value of \u03c9k for the i-th frame. Moreover, the standard deviation \u03c3(\u03c9k) is also estimated as the square root of \u03c32(\u03c9k):\n( )2 1\n1( ) ( ) ( ) N\ni k k k i a N \u03c3 \u03c9 \u03c9 \u03bc \u03c9 = = \u2212 (3)\nSome of the results are shown from Fig. 1 to Fig. 4 (for the pronunciation of [h], [s], unvoiced [a], unvoiced [e] by a male speaker), where the curves of \u03bc(\u03c9k), \u03c32(\u03c9k) and \u03c3(\u03c9k) are plotted for comparison. By comparing the curves of \u03bc(\u03c9k) and \u03c3(\u03c9k) in each figure, their evident similarity can be observed, which inspires further study of the relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k).\n(a) amplitude expectation \u03bc(\u03c9k) (b) amplitude standard deviation \u03c3(\u03c9k)\nIn order to investigate the relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k) more directly, for an unvoiced pronunciation, the two-dimensional points of (\u03bc(\u03c9k), \u03c3(\u03c9k)) are plotted in Matlab for all \u03c9k. For a frequency component \u03c9k, (\u03bc(\u03c9k), \u03c3(\u03c9k)) is a point with the amplitude expectation as the x-coordinate and the amplitude standard deviation as the y-coordinate. The results of such plotting indicate a linear proportional relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k). Some results are shown in Fig. 5, which demonstrate the linear relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k) in a more direct way, and inspire further quantitative verification of this relationship using the correlation coefficient.\nBesides the above experimental results, the relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k) is quantitatively verified by calculating the correlation coefficient between the two curves of \u03bc(\u03c9k) and \u03c3(\u03c9k). Since the spectrum obtained by STFT is discrete in frequency domain, the correlation coefficient is calculated in a discrete form:\n1\n2 2\n1 1\n( ) ( )\n( ) ( )\nN\nk k k\nN N\nk k k k\n\u03c3\u03bc\n\u03c3 \u03c9 \u03bc \u03c9 \u03c1\n\u03c3 \u03c9 \u03bc \u03c9\n=\n= =\n\u22c5 =\n\u22c5\n\n  (4)\nwhere N is the number of discrete frequencies in the discrete spectrum. Experimental results are shown in Table 1. The first part of the results are based on the pronunciation signals recorded for one male speaker. The correlation coefficients between \u03bc(\u03c9k) and \u03c3(\u03c9k) are calculated for different unvoiced phonemes, together with those between \u03bc(\u03c9k) and \u03c32(\u03c9k) for comparison. The correlation coefficients between \u03bc(\u03c9k) and \u03c3(\u03c9k) are much close to 1.0. Consider the unavoidable error caused by the instability of sustained natural pronunciation, and also the noise introduced in the signal capture process, the strong correlation between \u03bc(\u03c9k) and \u03c32(\u03c9k) observed in the experiments did not happen merely by chance. Such results indicate that \u03bc(\u03c9k) and \u03c3(\u03c9k) are related by a linear proportional relationship.\nExperiments have also been done on the unvoiced pronunciation recorded for other speakers, and by other recording devices, which yield consistent results indicating the linear proportional relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k). Some of these results are also shown in Table 1.\nSince the standard deviation coefficient represents the \u03c3 to \u03bc ratio, such property revealed in the experiments is named as \u201cconsistent standard deviation coefficient\u201d. It means that the proportional coefficient between the standard deviation and the expectation is consistent for all the frequency components in the short-time amplitude spectrum of unvoiced pronunciation. From the viewpoint of probability, for any frequency component of an unvoiced pronunciation, the larger the amplitude expectation, the larger the random fluctuation of amplitude in the short-time spectrum. This stochastic property revealed by the above experiments also indicates there should be certain connection between the amplitude probability distributions of different frequency components, which is studied in detail in Section 3."}, {"heading": "3. The probability distribution model of the short-time amplitude spectrum for unvoiced pronunciation", "text": "Besides the basic statistics mentioned in Section 2, the histogram of amplitude value for each frequency component is also computed respectively using a voting method, which virtually corresponds to the estimation of amplitude probability distribution. Then a model is proposed describing the connection between amplitude distributions of different frequency components for unvoiced pronunciation, which accords well with the property of \u201cconsistent standard deviation coefficient\u201d. And the pdf of expectation-normalized amplitude is quantitatively fitted with Weibull distribution."}, {"heading": "3.1 Amplitude histogram computation as the estimation of probability distribution", "text": "For amplitude histogram computation, a voting method is presented here. For each frequency component, the following steps are carried out: Step 1: Determine a reasonable range of amplitude value for this frequency component. This range should contain all the amplitude spectrum values obtained from experimental data. Step 2: Uniformly divide the above range into reasonable amount of intervals (or bins) with the same length. Step 3: In the spectrum of each frame, find the amplitude value of the current frequency component, then find the bin into which this amplitude value falls, and increase the count of that bin by one. In another word, let the amplitude values of a frequency component vote for their corresponding bins into which they fall respectively.\nAfter the above voting process, the amplitude histogram can be computed by dividing the voting results by the total number of data, which obtains the estimated probability of falling into each interval. Moreover, since the amplitude is a continuous random variable, in order to estimate its probability density distribution, the amplitude histogram values are further divided by the interval length.\nIn the above Step 2, the length of the interval or bin for the amplitude range can be determined by experiment. In order to conveniently compare the amplitude distribution for any two frequency components, a common value range [0, Amax] is used for all the frequency components, where Amax is the maximum of all the amplitude values for all the frequency components.\nIn Section 2, the linear proportional relationship between the amplitude expectation and standard deviation indicates that there should be some connection between the amplitude probability distribution of any two frequency components. Therefore, besides the amplitude distribution of each frequency component alone, the connection between the amplitude probability distribution of any two frequency component is also important. In order to find clue of the connection between amplitude distributions of different frequency components, the estimated distribution curves of all the frequency components are plotted together and shown as family of curves. Some of the results are shown in Fig. 6(a) to Fig. 11(a) (i.e. only the figures labeled with (a) in Fig. 6 to Fig. 11), which are for the pronunciation of [h], [s], [\u03b8], unvoiced [a], unvoiced [e] and unvoiced [i]. In Fig. 6(a) to Fig. 11(a), the curves are mixed and there is no obvious regularity between the curves. In Section 2, the experimental results indicate that the expectation and variance of the amplitude spectrum value for different frequency components are usually different. Because the expectation and variance can determine the location and sharpness of the distribution curve, it is natural to find little regularity in the mixed plotting of the estimated distribution curves in Fig. 6(a) to Fig. 11(a). Nevertheless, the regularity of distribution curves will become clear after normalization of expectation for the amplitude spectrum data, as presented in the following.\nThe expectation and variance (or the standard deviation as an alternative of the variance) are two basic statistics of random variables. For some probability distribution types, these basic statistics can even determine the probability distribution function, such as the normal distribution, the exponential distribution, Rayleigh distribution, etc. The standard deviation reflects the degree of random fluctuation, and largely determines the flatness of the probability distribution curve. For unvoiced pronunciation, the expectation and standard\ndeviation of amplitude spectrum value are found to be proportionally related as the property of \u201cconsistent standard deviation coefficient\u201d. Therefore, the shape of estimated amplitude distribution curve for each frequency component is affected by the corresponding expectation, which may make it inconvenient for study the connection between two amplitude probability distribution curves estimated.\nTherefore, for each frequency component, a preprocessing step is added to normalize the expectation of amplitude values, so that the connection between the amplitude probability distribution of different frequencies may be revealed more clearly. Considering the linear proportional relationship between \u03bc(\u03c9k) and \u03c3(\u03c9k), the preprocessing is proposed as dividing each amplitude spectrum data by the average amplitude value of its corresponding frequency component. This preprocessing is called \u201cexpectation-normalization\u201d hereafter, because after such processing the data will have an average of 1. In another word, for each frequency component, the amplitude expectation after normalization will be 1. Some of the results of distribution estimation for amplitude after the preprocessing of expectation-normalization are shown in Fig. 6(b) to Fig. 11(b) (i.e. only the figures labeled with (b) in Fig. 6 to Fig. 11).\nIn Fig. 6 to Fig. 11, it is for the purpose of clear comparison to put together the distribution curves estimated before and after the expectation-normalization in one figure. The results show that, after the preprocessing of expectation-normalization, the estimated distribution curves obviously converge to one central curve (shown in black color in Fig. 6(b) to Fig. 11(b)), especially compared to the part (a) in these figures. Because the distribution curves converge so closely, the mixed plotting results in a belt around a central curve, and no individual distribution curve can be clearly seen. From part (b) of each figure, the strong connection between the amplitude distributions of different frequency components can be visibly observed, which is based on the amplitude data after expectation-normalization. Based on such results, a model of amplitude distribution for unvoiced pronunciation is proposed in the following Section 3.2, where theoretical analysis accords well with the experimental results obtained. And more quantitative estimations of the probability distribution of the normalized amplitude data are given in Section 3.3. 3.2 A model of the relationship connection between the amplitude distributions of different frequency components In Fig. 6(b) to Fig. 11(b), the estimated distribution curves after expectation-normalization converge closely to one central curve. Considering the inevitable error caused by pronunciation instability and noise in signal capture and computation, it is reasonable to propose a common pdf prototype of amplitude for all frequency components. In another word, the amplitude distributions of different frequency components are of the same pdf type, but with different expectation values. In such model, there is a prototype distribution function p0(a0), from which the amplitude distribution of any frequency component can be derived by varying the expectation (i.e. altering the expectation with a scaling factor). The prototype p0(a0) corresponds to the central curve to which the estimated curves converge in Fig. 6(b) to Fig. 11(b). This model of amplitude spectrum for unvoiced pronunciation is described mathematically as follows. As a random variable, the amplitude of a frequency component a is modeled as some scaling of a prototype variable a0, whose expectation is 1:\n0a k a= \u22c5 (5)\nwhere k is the scaling parameter.\nEquation (5) is a mathematical description of the model proposed, in which the random variable a representing the amplitude of each frequency component is the result of scaling a basic variable a0. Moreover, a0 is the same for each frequency component, which corresponds to the prototype of amplitude distribution.\nBased on the above mathematical description, it can be theoretically proved that such model of \u201ccommon prototype of amplitude distribution\u201d accords well with the \u201cconsistent standard deviation coefficient\u201d property for unvoiced pronunciation revealed by experiments in Section 2. In another word, it can be proved that the random variable a in Equation (5) has a consistent standard deviation coefficient regardless of the value k.\nFirst, consider the probability distribution of a in Equation (5), given the probability distribution of a0 is the prototype distribution p0(a0). According to Equation (5), the expectation of a is:\n0 0 0[ ] [ ] [ ]a E a E k a k E a k\u03bc \u03bc= = \u22c5 = \u22c5 = \u22c5 (6)\nwhere \u03bc0 is the expectation of a0. Without loss of generality, \u03bc0 can be set to 1 considering the preprocessing of expectation-normalization in Section 3.1. Based on the pdf of a variable\u2019s function in probability theory, according to the relationship between a and a0, the probability distribution of a can be deduced as:\n0 1( ) ap a p k k  = \u22c5    \n(7)\nSecond, consider the standard deviation coefficient of a:\n( )2 ( )( ) aa a a a a p a daVar a \u03bc\u03c3 \u03bc \u03bc \u03bc\n+\u221e \u2212\u221e \u2212\n= =  (8) Consider Equation (6) and (7), Equation (8) can be rewritten as:\n( )20 0\n0\n1 a\na\naa k p da k k\nk \u03bc \u03c3 \u03bc \u03bc\n+\u221e\n\u2212\u221e  \u2212 \u22c5 \u22c5    =\n\u22c5\n (9)\nThen do the variable substitution a=ka0 to the integral on the right side of Equation (9):\n( )20 0\n0\n1 a\na\naa k p da k k\nk \u03bc \u03c3 \u03bc \u03bc\n+\u221e\n\u2212\u221e  \u2212 \u22c5 \u22c5    =\n\u22c5\n\n( ) ( ) ( )20 0 0 0 0 0 1ka k p a d ka k k \u03bc \u03bc +\u221e \u2212\u221e \u2212 \u22c5 = \n( ) ( )22 0 0 0 0 0 0 1k a p a kda k k \u03bc \u03bc +\u221e \u2212\u221e \u2212 \u22c5 \u22c5 =  ( ) ( )22 0 0 0 0 0 0 k a p a da k \u03bc \u03bc +\u221e \u2212\u221e \u22c5 \u2212 \u22c5 =  (10)\nRemember that the variables a and a0 represent the amplitude value, which is non-negative. Therefore, k is also non-negative. Then Equation (10) can be rewritten as:\n( ) ( )20 0 0 0 0 0 a a a p a da\u03bc\u03c3 \u03bc \u03bc\n+\u221e \u2212\u221e \u2212 \u22c5\n=  (11)\nNotice that the numerator of the right side of Equation (11) is just the standard deviation of a0. Therefore,\n0 0 a a \u03c3 \u03c3 \u03bc \u03bc = (12)\nNotice that the right side of Equation (12) is constant given the prototype distribution p0(a0). Therefore, the standard deviation coefficient of a is consistent whatever the scaling factor k is. It is consistent with that of the prototype variable a0. For any positive value k, the variable a=ka0 has the same (or consistent) standard deviation coefficient, which accords well with the experimental results shown in Section 2. Therefore, the property of \u201cconsistent standard deviation coefficient\u201d supports the model proposed here: the random variables representing amplitude of each frequency component belong to the same pdf type, but have different expectations. If the prototype pdf p0(a0) is determined, the pdf of any frequency\u2019s amplitude a can then be derived by a=\u03bca0, where \u03bc is a\u2019s expectation.\n3.3 Estimation of the prototype pdf for expectation-normalized amplitude spectrum values In order to quantitatively study the prototype pdf of the amplitude spectrum value, further investigation has been done. As shown in Fig. 6(b) to Fig. 11(b), for each frequency component, a curve of amplitude distribution after expectation-normalization can be obtained. And these curves are so close that no individual curve can be shown separately in the figure. To clearly show the pdf of the expectation-normalized amplitude, the curves in Fig. 6(b) to Fig. 11(b) are averaged in order to eliminate the fluctuation of the estimated curves, which is primarily due to the randomness of amplitude in frequency domain. The averaged curve is shown in Fig. 6(b) to Fig. 11(b) as a black one (the central curve mentioned in Section 3.1), surrounding by the belt area formed by those curves estimated for each frequency component. Based on the above experimental results, a proper probability distribution is studied for the expectation-normalized amplitude data in the short-time spectrum for unvoiced pronunciation. In order to find a proper distribution as possible, in this paper the Weibull distribution is used as a general distribution type, which is adaptive to represent multiple distributions commonly used (including the exponential distribution, Rayleigh distribution, Gaussian distribution, etc.) by varying the shape parameter of the distribution function (Weibull W., 1951; Lindquist, 1994; Khaledi and Kochar, 2006; Szymkowiak and Iwinska, 2016). Such generalization ability is quite suitable for the study here, which is mainly based on experimental data. There are two other reasons to use Weibull distribution. First, the amplitude spectrum data is non-negative, which suits the requirement of the Weibull distribution. Second, in the experiment all the estimated distribution curves of expectation-normalized amplitude data have single-peak shape (as shown in Fig. 6(b) to Fig. 11(b)), which also suits the characteristic of Weibull distribution function. The Weibull distribution can be expressed as a two-parameter function (Lindquist, 1994; Khaledi and Kochar, 2006):\n( )( 1)( ) bx b b ap x b a x e \u2212\u2212 \u2212= \u22c5 \u22c5 \u22c5 (13)\nwhere a is the scale parameter and b is the shape parameter. The shape parameter b makes the\ndistribution adaptive to represent different distribution types (Weibull W., 1951; Lindquist, 1994; Khaledi and Kochar, 2006; Szymkowiak and Iwinska, 2016). For example, if b=1, Equation (13) reduces to the exponential distribution. If b=2, it turns to the Rayleigh distribution. If b=3, it well approximates the Gaussian distribution. Therefore, it is highly flexible in fitting experimental data.\nFor unvoiced pronunciation, in order to estimate Weibull parameters a and b for the prototype pdf of the short-time amplitude spectrum data, all the expectation-normalized amplitude data of every frequency component are used as a whole data set, since all the frequency components share a common prototype of amplitude distribution. The statistic toolbox in Matlab is used to estimate this pdf. The Matlab function wblfit is used to get the maximum likelihood estimate of Weibull parameters a and b, which can obtain a reliable estimation result due to the sufficiently large amount of experimental data (i.e. the large amount of frames). The estimation results are shown in Table 2.\nThe experimental results in Table 2 indicate that different pronunciations have obviously different shape parameter values of b, but their scale parameters a are similar due to the preprocessing of expectation-normalization on the amplitude spectrum values. By varying the shape parameter, the Weibull distribution can well approximates different distribution types such as the exponential distribution, the Rayleigh distribution, the Gaussian distribution, etc. In the experiments, the shape parameter values approximately range in (1.5, 2.1), which indicate the probability distribution falls in between the exponential distribution (with b=1) to the Rayleigh distribution (with b=2), and with an obvious tendency to the Rayleigh distribution. Using the Weibull distribution here has obvious advantage over those distributions of fixed shape, because the study is mainly based on experimental data, and there is little prior knowledge which can determine the distribution of the spectrum data.\nThere is another interesting phenomenon in the results that the pronunciations of the same phoneme by different speakers or different recording platform result in same parameters (such as the unvoiced [a] or [\u0259] in Table 2), which deserves further study in future.\nIn Section 5, the Weibull distribution is used as the prototype of amplitude distribution in unvoiced voice synthesis, which yields satisfying results as the support to the above model of amplitude distribution."}, {"heading": "4. The stochastic model for the short-time spectrum of unvoiced pronunciation", "text": ""}, {"heading": "4.1 The estimation of phase distribution for unvoiced pronunciation", "text": "The spectrum value is usually complex number, whose modulus represents the amplitude and the argument represents the phase. Besides the amplitude, in this paper the phase distribution is also studied for sustained unvoiced pronunciation. The similar method is used to estimate the probability distribution of the phase as in Section 3.1, except that the range of phase value is defined as [-\u03c0, \u03c0]. Such value range is uniformly divided into intervals of identical length. The phase distribution is estimated for each frequency component respectively. For each interval of phase, the number of phase data falling in it is counted. Such estimation can be performed to obtain a phase distribution curve. Some of the results are shown in Fig. 12 for the pronunciation of [h], [s], unvoiced [a] and unvoiced [e]. The experimental results indicate a uniform distribution for the phase spectrum value, just as the phase models proved and applied widely in many communication applications."}, {"heading": "4.2 The model of short-time amplitude and phase spectrum of unvoiced pronunciation", "text": "By combining the model of amplitude pdf proposed in Section 3 and the phase distribution in Section 4.1, a stochastic model can be proposed for the short-time spectrum of unvoiced pronunciation: (1) The short-time spectrum of unvoiced pronunciation is random; for each frequency component, its amplitude can be modeled as Weibull distribution; its phase can be modeled as uniform distribution in the range of [-\u03c0, \u03c0]. (2) Moreover, the standard deviation coefficient of amplitude is consistent for all frequency components; in another word, for any frequency component of a specific unvoiced pronunciation, the standard deviation of its amplitude is proportional to the expectation. (3) For a specific unvoiced pronunciation, all the frequency components have a common prototype of amplitude distribution a0, where a0 is the prototype random variable of Weibull distribution; for any frequency component \u03c9k, the probability distribution of its amplitude is modeled as \u03bck a0, where \u03bck is the expectation of \u03c9k\u2019s amplitude.\nAlthough the above model is based on the experiments for sustained unvoiced pronunciation, due to the short-time stable property of speech, the above model can also be valid for a short period (such as 16ms) during which the signal is considered as stable. The proposed model also has potential application in the synthesis of continuous speech in daily communication."}, {"heading": "5. Synthesis of unvoiced pronunciation based on the proposed stochastic model", "text": "In this section, the signal of sustained unvoiced pronunciation is synthesized based on the proposed stochastic model of short-time spectrum. The purpose of the synthesis here has two aspects. The first one is to generate synthesized signal with the same perception quality as the original pronunciation as possible. The second one is to experimentally verify the proposed stochastic model. Since the short-time spectrum data obtained in the experiments is obtained by STFT, based on the stochastic model proposed above, it is possible to artificially generate random short-time spectrum data, based on which the synthesis of unvoiced pronunciation can be implemented by the reverse STFT.\nThere are three steps for the proposed synthesis: Step 1: The first step is model parameter estimation from the original signal of unvoiced pronunciation. The key parameters of the proposed model in Section 4.2 are the average amplitude value \u03bc(\u03c9k) for each frequency \u03c9k, and the two parameters a and b of the Weibull distribution p0(x) for the expectation-normalized amplitude data. Due to the \u201cconsistent standard deviation coefficient\u201d property, the pdf of each frequency\u2019s amplitude a can be derived from p0(x) based on \u03bc(\u03c9k), which is a distinguishing feature of the proposed method compared to other models.\nIn order to obtain the model parameters, STFT is performed on the original signal to get the group of short-time spectrums, and the average value of amplitude is estimated for each frequency. Then for each frequency \u03c9k, its amplitude value for each signal frame is normalized by dividing the corresponding amplitude average \u03bc(\u03c9k), and the two parameters of Weibull distribution are estimated based on the normalized amplitude data for all frequencies as a whole data set. Because in the model the phase has a uniform distribution, there is no parameter needed for phase here.\nStep 2: The second step is generating random amplitude and phase values of each \u03c9k for each synthesized frame according to the stochastic model. For each frequency component, the amplitude value is generated according to the Weibull distribution, and the phase value is generated according to uniform distribution.\nThe Weibull distribution for the expectation-normalized amplitude data (i.e. the prototype distribution of amplitude for all frequency components) can be determined by the two parameter s a and b (which are estimated in Step 1):\n( )( 1) 0 ( ) bx b b ap x b a x e \u2212\u2212 \u2212= \u22c5 \u22c5 \u22c5 (14)\nwhere p0(x) is the prototype pdf of amplitude, a and b are the scale parameter and shape parameter respectively. However, p0(x) is only the distribution for the normalized amplitude data. The actual amplitude pdf of the k-th frequency component should be deduced from p0(x). Because the actual amplitude data for the k-th frequency component has the expectation value \u03bc(\u03c9k), the corresponding actual distribution can be deduced as:\n0 1( )\n( ) ( )k k k\nxp x p \u03bc \u03c9 \u03bc \u03c9   = \u22c5     (15)\nwhere pk(x) is the actual amplitude pdf for the k-th frequency component. Then the amplitude data for \u03c9k can be produced artificially according to the pdf as Equation (15). The phase value can be generated according to the uniform distribution in the range of [-\u03c0, \u03c0]. Step 3: The third step is short-time spectrum construction and reverse STFT. The reverse STFT transforms a group of successive short-time spectrum to the time domain signal, which includes the IDFT (inverse discrete Fourier transform) on each short-time spectrum:\n21\n0\n1( ) ( ) N j kn\nN i i k x n X k e N\n\u03c0\u2212\n=\n= \u22c5 (16)\nwhere xi(n) is the i-th synthesized frame, and Xi(k) is its corresponding discrete spectrum. In Step 2, for each synthesized frame, the amplitude and phase values of all \u03c9k are randomly generated. Since the DFT results of real signal has the conjugate symmetric property, the short-time spectrum Xi(k) can be constructed with the artificially generated amplitude and phase values in a conjugate symmetric manner.\nIn such a way, the spectrum Xi(k) for the i-th synthesized frame can be constructed, and transformed to time domain by IDFT. Then the successive frames are combined to produce the synthesized signal by an overlap and adding process: two adjacent frames are overlapped by half of the frame length, and then added. The final result is the synthesized signal.\nIn the listening test, for several listeners (three male and three female listeners of the age 22-24 with normal audition), the synthesized signals have identical quality of auditory perception compared to the corresponding original pronunciations. Moreover, the average amplitude \u03bc(\u03c9k) and the signal autocorrelation of the original and synthesized pronunciation are computed for comparison. Some of the results are shown in Fig. 13 to Fig. 16. It is indicated in Fig. 13 to Fig. 16 that the time-domain autocorrelation of the synthesized signals are much similar to those of the original pronunciations. Because the power spectrum of a signal can be determined by its time-domain autocorrelation according to the Wiener-Khinchin theorem, similar autocorrelation functions correspond to similar power\nspectrum, which is the main reason of the identical perception quality of the synthesized pronunciation compared to the original one. This is also indicated by the similar average amplitude curve \u03bc(\u03c9k) of the original and the synthesized signal, which is shown in Fig. 13 to Fig. 16. The high quality of synthesis for unvoiced pronunciation also verifies the effectiveness of the stochastic model proposed in Section 4.2.\n(a) original autocorrelation (b) autocorrelation of the synthesized signal"}, {"heading": "6. Conclusion", "text": "In this paper, the stochastic property of the short-time spectrum of unvoiced pronunciation is studied. Based on the signals of sustained unvoiced pronunciation, in the short-time spectrum obtained by STFT, the relationship between the amplitude\u2019s expectation and standard deviation is investigated for different frequency components. In the experiments, for an unvoiced pronunciation, the ratio of standard deviation to expectation (also called the standard deviation coefficient) is found to be consistent for all the frequency components. Such novel property is related to the physical aero-acoustic process of unvoiced pronunciation, and further study is worthwhile.\nBased on such phenomenon of \u201cconsistent standard deviation coefficient\u201d, a stochastic model of amplitude spectrum value is proposed, in which all the frequency components share a common prototype of pdf for amplitude spectrum value. Moreover, the probability distribution of amplitude after expectation-normalization is estimated. And the probability distribution of phase is also studied.\nBy combining the models of amplitude and phase distribution, a stochastic model of short-time spectrum is proposed for unvoiced pronunciation, with a Weibull distribution for the expectation-normalized amplitude, and a uniform distribution for the phase. Based on such stochastic model, an efficient synthesis method is presented and implemented, which yields equivalent quality of auditory perception as the original unvoiced pronunciation, and\nalso similar autocorrelation compared to that of the original signal. The effectiveness of the synthesis results proves the validity of the proposed stochastic model of unvoiced pronunciation in frequency domain.\nThe work in this paper indicates that, besides the general statistic properties of speech signals in daily communication, it is worthwhile to study the stochastic properties of specific pronunciation types, which is on another refined level of randomness for speech signal. Just as the unvoiced pronunciation studied in this paper, specific type of pronunciation signal has more detailed and specific properties compared to general speech signals in daily communication. Although the voiced and unvoiced pronunciations are of different type, randomness exists in the signals of both types, which will also be investigated in future study. Since the unvoiced pronunciation is physically based on an aerodynamic process, the signal properties revealed in this paper is also inspiring for the research on acoustic signal produced by physical aerodynamic process."}], "references": [{"title": "Log-spectral amplitude estimation with Generalized Gamma distributions for speech enhancement", "author": ["J. Borgstrom B", "A. Alwan"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "B. and Alwan,? \\Q2011\\E", "shortCiteRegEx": "B. and Alwan", "year": 2011}, {"title": "Speech enhancement using minimum mean-square error amplitude estimators under normal and generalized gamma distribution", "author": ["C. Boubakir", "D. Berkani"], "venue": "Journal of Computer Science,", "citeRegEx": "Boubakir and Berkani,? \\Q2010\\E", "shortCiteRegEx": "Boubakir and Berkani", "year": 2010}, {"title": "An experimental study of speech wave probability distributions", "author": ["B. Davenport W"], "venue": "J. Acoust. Soc. Amer.,", "citeRegEx": "W.,? \\Q1952\\E", "shortCiteRegEx": "W.", "year": 1952}, {"title": "Speech enhancement using a minimum mean-square error-log-spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Ephraim and Malah,? \\Q1985\\E", "shortCiteRegEx": "Ephraim and Malah", "year": 1985}, {"title": "Speech enhancement based on Rayleigh mixture modeling of speech spectral amplitude distributions", "author": ["S. Erkelens J", "J. Jensen", "R. Heusdens"], "venue": "European Signal Processing Conference (EUSIPCO", "citeRegEx": "J. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "J. et al\\.", "year": 2007}, {"title": "A DCT-based fast signal subspace technique for robust speech recognition", "author": ["J. Huang", "Y. Zhao"], "venue": "IEEE Trans. Speech Audio Processing,", "citeRegEx": "Huang and Zhao,? \\Q2000\\E", "shortCiteRegEx": "Huang and Zhao", "year": 2000}, {"title": "Using jitter and shimmer in speaker verification", "author": ["M. Farrus", "J. Hernando"], "venue": "IET Signal Processing,", "citeRegEx": "Farrus and Hernando,? \\Q2009\\E", "shortCiteRegEx": "Farrus and Hernando", "year": 2009}, {"title": "TIMIT Acoustic-phonetic continuous speech corpus, Linguistic Data Consortium, Philadelphia", "author": ["J. Garofolo", "L. Lamel", "W. Fisher", "J. Fiscus", "D. Pallett", "N. Dahlgren", "V. Zue"], "venue": null, "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Speech probability distribution", "author": ["S. Gazor", "W. Zhang"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Gazor and Zhang,? \\Q2003\\E", "shortCiteRegEx": "Gazor and Zhang", "year": 2003}, {"title": "Joint source-filter optimization for robust glottal source estimation in the presence of shimmer and jitter", "author": ["K. Ghosh P", "S. Narayanan S"], "venue": "Speech Communication,", "citeRegEx": "P. and S.,? \\Q2011\\E", "shortCiteRegEx": "P. and S.", "year": 2011}, {"title": "The Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions. Automatic Speech Recognition: Challenges for the Next Millennium", "author": ["Hirsch", "H.-G", "D. Pearce"], "venue": "(ISCA ITRW ASR2000),", "citeRegEx": "Hirsch et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hirsch et al\\.", "year": 2000}, {"title": "Some fluid dynamic aspects of speech", "author": ["A. Hirschberg"], "venue": "Bulletin de la Communication Parlee,", "citeRegEx": "Hirschberg,? \\Q1992\\E", "shortCiteRegEx": "Hirschberg", "year": 1992}, {"title": "Digital coding of waveforms", "author": ["S. Jayant N", "P. Noll"], "venue": null, "citeRegEx": "N. and Noll,? \\Q1984\\E", "shortCiteRegEx": "N. and Noll", "year": 1984}, {"title": "A study of the distribution of time-domain speech samples and discrete Fourier coefficients", "author": ["J. Jensen", "I. Batina", "C. Hendriks R", "R. Heusdens"], "venue": "Proceedings of SPS-DARTS (The first annual IEEE BENELUX/DSP Valley Signal Processing Symposium),", "citeRegEx": "Jensen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2005}, {"title": "Weibull distribution: Some stochastic comparisons results", "author": ["Khaledi B.-E", "Kochar S"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "B..E. and S.,? \\Q2006\\E", "shortCiteRegEx": "B..E. and S.", "year": 2006}, {"title": "Strength of materials and the Weibull distribution", "author": ["S. Lindquist E"], "venue": "Probabilistic Engineering Mechanics,", "citeRegEx": "E.,? \\Q1994\\E", "shortCiteRegEx": "E.", "year": 1994}, {"title": "Speech enhancement, theory and practice", "author": ["C. Loizou P"], "venue": null, "citeRegEx": "P.,? \\Q2007\\E", "shortCiteRegEx": "P.", "year": 2007}, {"title": "Aeroacoustic modeling of frictives /s/ and /sh", "author": ["B. Lu X", "W. Thorpe C", "E. Cater J", "J. Hunter P"], "venue": "Proceedings of the 18th international congress on sound & vibration,", "citeRegEx": "X. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "X. et al\\.", "year": 2011}, {"title": "Speech enhancement based on minimum mean-square error estimation and supergaussian priors", "author": ["R. Martin"], "venue": "IEEE Transactions on Speech and Audio Processing", "citeRegEx": "Martin,? \\Q2005\\E", "shortCiteRegEx": "Martin", "year": 2005}, {"title": "An aeroacoustics approach to phonation: some experimental and theoretical observations", "author": ["S. McGowan R"], "venue": "Haskins Laboratories: Status Report on Speech Research", "citeRegEx": "R.,? \\Q1987\\E", "shortCiteRegEx": "R.", "year": 1987}, {"title": "Fluid dynamics of human phonation and speech", "author": ["R. Mittal", "D. Erath B", "W. Plesniak M"], "venue": "Annual Review of Fluid Mechanics,", "citeRegEx": "Mittal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mittal et al\\.", "year": 2013}, {"title": "Minimum mean-square error quantization in speech", "author": ["D. Paez M", "H. Glisson T"], "venue": "IEEE Trans. Comm.,", "citeRegEx": "M. and T.,? \\Q1972\\E", "shortCiteRegEx": "M. and T.", "year": 1972}, {"title": "Fundamentals of speech recognition", "author": ["Rabiner L", "Juang B.-H"], "venue": "Prentice-Hall International,", "citeRegEx": "L. and B..H.,? \\Q1993\\E", "shortCiteRegEx": "L. and B..H.", "year": 1993}, {"title": "Statistical properties of speech signals", "author": ["L. Richards D"], "venue": "Proc. Inst. Elect. Eng.,", "citeRegEx": "D.,? \\Q1964\\E", "shortCiteRegEx": "D.", "year": 1964}, {"title": "Statistical modeling of speech signals based on generalized gamma distribution", "author": ["W. Shin J", "J.-H. Chang", "S. Kim N"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "J. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "J. et al\\.", "year": 2005}, {"title": "Synthesis of fricative sounds using an aeroacoustic noise generation model", "author": ["J. Sinder D", "H. Krane M", "L. Flanagan J"], "venue": "Proceedings of 16th International Congress Acoustics,", "citeRegEx": "D. et al\\.,? \\Q1998\\E", "shortCiteRegEx": "D. et al\\.", "year": 1998}, {"title": "A fluid flow approach to speech generation", "author": ["D. Sinder", "G. Richard", "H. Duncan", "Q. Lin", "J. Flanagan"], "venue": "First ETRW on Speech Production Modelling,", "citeRegEx": "Sinder et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Sinder et al\\.", "year": 1996}, {"title": "A statistical model-based voice activity detection", "author": ["J. Sohn", "Kim N. S", "W. Sung"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "Sohn et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sohn et al\\.", "year": 1999}, {"title": "Characterizations of Discrete Weibull related distributions", "author": ["M. Szymkowiak", "M. Iwinska"], "venue": "Statistics & Probability Letters,", "citeRegEx": "Szymkowiak and Iwinska,? \\Q2016\\E", "shortCiteRegEx": "Szymkowiak and Iwinska", "year": 2016}, {"title": "Statistical modeling of the speech signal, International Workshop on Acoustic, Echo, and Noise Control (IWAENC), Tel Aviv, Israel", "author": ["I. Tashev", "A. Acero"], "venue": null, "citeRegEx": "Tashev and Acero,? \\Q2010\\E", "shortCiteRegEx": "Tashev and Acero", "year": 2010}, {"title": "Vocal acoustic analysis - jitter, shimmer and HNR parameters", "author": ["P. Teixeira J", "C. Oliveira", "C. Lopes"], "venue": "Procedia Technology,", "citeRegEx": "J. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "J. et al\\.", "year": 2013}, {"title": "A statistical distribution function of wide applicability", "author": ["W. Weibull"], "venue": "Journal of Applied Mechanics,", "citeRegEx": "Weibull,? \\Q1951\\E", "shortCiteRegEx": "Weibull", "year": 1951}], "referenceMentions": [{"referenceID": 8, "context": "Such researches are based on the large amount of speech data in corpora like TIMIT (Garofolo, 1993), AURORA (Hirsch and Pearce, 2000) or other database of daily speech signal from the internet (Gazor and Zhang, 2003).", "startOffset": 193, "endOffset": 216}, {"referenceID": 13, "context": "For different speech lengths and different speech classes, the most proper pdf type was studied by Chi-square goodness-of-fit test on time-domain speech signal, with Laplacian and Gaussian distribution as two options (Jensen et al., 2005).", "startOffset": 217, "endOffset": 238}, {"referenceID": 8, "context": "Besides the time-domain, the pdf for transformed domain was also studied for Karhunen-Loeve (K-L) transform and DCT, where the Laplacian distribution showed prevailing performance by Chi-square test and moment test (Gazor and Zhang, 2003).", "startOffset": 215, "endOffset": 238}, {"referenceID": 18, "context": "These stochastic models facilitates the application of speech enhancement (Martin, 2005; Loizou, 2007; Boubakir and Berkani, 2010; Borgstrom and Alwan, 2011) and voice activity detection (Sohn et al.", "startOffset": 74, "endOffset": 157}, {"referenceID": 1, "context": "These stochastic models facilitates the application of speech enhancement (Martin, 2005; Loizou, 2007; Boubakir and Berkani, 2010; Borgstrom and Alwan, 2011) and voice activity detection (Sohn et al.", "startOffset": 74, "endOffset": 157}, {"referenceID": 27, "context": "These stochastic models facilitates the application of speech enhancement (Martin, 2005; Loizou, 2007; Boubakir and Berkani, 2010; Borgstrom and Alwan, 2011) and voice activity detection (Sohn et al., 1999), whose object is the daily-life speech interfered by some kind of noise.", "startOffset": 187, "endOffset": 206}, {"referenceID": 5, "context": "Suchmodels also facilitate speech coding (Paez and Glisson, 1972) and speech recognition (Rabiner and Juang, 1993; Huang and Zhao, 2000).", "startOffset": 89, "endOffset": 136}, {"referenceID": 6, "context": "For voiced pronunciation there are phenomena of jitter and shimmer observed (Teixeira et al., 2013; Ghosh and Narayanan, 2011; Farrus and Hernando, 2009), and for unvoiced pronunciation the sound source is random by itself which is caused by turbulence of air flow in the vocal tract (Sinder et al.", "startOffset": 76, "endOffset": 153}, {"referenceID": 20, "context": "The unvoiced pronunciation is closely related to the aerodynamic process in vocal tract, which is an ongoing research topic (Mittal et al., 2013; Lu et al., 2011, Sinder et al., 1996: McGowan, 1987).", "startOffset": 124, "endOffset": 198}, {"referenceID": 28, "context": ") by varying the shape parameter of the distribution function (Weibull W., 1951; Lindquist, 1994; Khaledi and Kochar, 2006; Szymkowiak and Iwinska, 2016).", "startOffset": 62, "endOffset": 153}, {"referenceID": 28, "context": "distribution adaptive to represent different distribution types (Weibull W., 1951; Lindquist, 1994; Khaledi and Kochar, 2006; Szymkowiak and Iwinska, 2016).", "startOffset": 64, "endOffset": 155}], "year": 2016, "abstractText": "Stochastic property of speech signal is a fundamental research topic in speech analysis and processing. In this paper, multiple levels of randomness in speech signal are discussed, and the stochastic properties of unvoiced pronunciation are studied in detail, which has not received sufficient research attention before. The study is based on the signals of sustained unvoiced pronunciation captured in the experiments, for which the amplitude and phase values in the short-time spectrum are studied as random variables. The statistics of amplitude for each frequency component is studied individually, based on which a new property of \u201cconsistent standard deviation coefficient\u201d is revealed for the amplitude spectrum of unvoiced pronunciation. The relationship between the amplitude probability distributions of different frequency components is further studied, which indicates that all the frequency components have a common prototype of amplitude probability distribution. As an adaptive and flexible probability distribution, the Weibull distribution is adopted to fit the expectation-normalized amplitude spectrum data. The phase distribution for the short-time spectrum is also studied, and the results show a uniform distribution. A synthesis method for unvoiced pronunciation is proposed based on the Weibull distribution of amplitude and uniform distribution of phase, which is implemented by STFT with artificially generated short-time spectrum with random amplitude and phase. The synthesis results have identical quality of auditory perception as the original pronunciation, and have similar autocorrelation as that of the original signal, which proves the effectiveness of the proposed stochastic model of short-time spectrum for unvoiced pronunciation.", "creator": "PScript5.dll Version 5.2.2"}}}