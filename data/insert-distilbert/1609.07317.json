{"id": "1609.07317", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression", "abstract": "in supporting this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. instead we formulate a partial variational auto - encoder bound for inference in this model frame and apply it to minimize the task prediction of compressing constructed sentences. in this application the generative model first draws forwards a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. in our empirical systematic evaluation we show that generative formulations of software both abstractive and extractive compression yield state - of - by the - art results when trained on a large amount of supervised simulation data. further, we explore semi - supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed partly supervised flow models while training on a generous fraction of the supervised data.", "histories": [["v1", "Fri, 23 Sep 2016 11:25:41 GMT  (1299kb,D)", "http://arxiv.org/abs/1609.07317v1", "EMNLP 2016"], ["v2", "Fri, 14 Oct 2016 00:21:00 GMT  (1299kb,D)", "http://arxiv.org/abs/1609.07317v2", "EMNLP 2016"]], "COMMENTS": "EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yishu miao", "phil blunsom"], "accepted": true, "id": "1609.07317"}, "pdf": {"name": "1609.07317.pdf", "metadata": {"source": "CRF", "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression", "authors": ["Yishu Miao", "Phil Blunsom"], "emails": ["phil.blunsom}@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "The recurrent sequence-to-sequence paradigm for natural language generation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) has achieved remarkable recent success and is now the approach of choice for applications such as machine translation (Bahdanau et al., 2015), caption generation (Xu et al., 2015) and speech recognition (Chorowski et al., 2015). While these models have developed sophisticated conditioning mechanisms, e.g. attention, fundamentally they are discriminative models trained only to approximate the conditional output distribution of strings. In this paper we explore modelling the\njoint distribution of string pairs using a deep generative model and employing a discrete variational autoencoder (VAE) for inference (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014). We evaluate our generative approach on the task of sentence compression. This approach provides both alternative supervised objective functions and the opportunity to perform semi-supervised learning by exploiting the VAEs ability to marginalise the latent compressed text for unlabelled data.\nAuto-encoders (Rumelhart et al., 1985) are a typical neural network architecture for learning compact data representations, with the general aim of performing dimensionality reduction on embeddings (Hinton and Salakhutdinov, 2006). In this paper, rather than seeking to embed inputs as points in a vector space, we describe them with explicit natural language sentences. This approach is a natural fit for summarisation tasks such as sentence compression. According to this, we propose a generative auto-encoding sentence compression (ASC) model, where we introduce a latent language model to provide the variablelength compact summary. The objective is to perform Bayesian inference for the posterior distribution of summaries conditioned on the observed utterances. Hence, in the framework of VAE, we construct an inference network as the variational approximation of the posterior, which generates compression samples to optimise the variational lower bound.\nThe most common family of variational autoencoders relies on the reparameterisation trick, which is not applicable for our discrete latent language model. Instead, we employ the REINFORCE algorithm (Mnih et al., 2014; Mnih and Gregor, 2014)\nar X\niv :1\n60 9.\n07 31\n7v 1\n[ cs\n.C L\n] 2\n3 Se\np 20\nto mitigate the problem of high variance during sampling-based variational inference. Nevertheless, when directly applying the RNN encoder-decoder to model the variational distribution it is very difficult to generate reasonable compression samples in the early stages of training, since each hidden state of the sequence would have |V | possible words to be sampled from. To combat this we employ pointer networks (Vinyals et al., 2015) to construct the variational distribution. This biases the latent space to sequences composed of words only appearing in the source sentence (i.e. the size of softmax output for each state becomes the length of current source sentence), which amounts to applying an extractive compression model for the variational approximation.\nIn order to further boost the performance on sentence compression, we employ a supervised forcedattention sentence compression model (FSC) trained on labelled data to teach the ASC model to generate compression sentences. The FSC model shares the pointer network of the ASC model and combines a softmax output layer over the whole vocabulary. Therefore, while training on the sentencecompression pairs, it is able to balance copying a word from the source sentence with generating it from the background distribution. More importantly, by jointly training on the labelled and unlabelled datasets, this shared pointer network enables the model to work in a semi-supervised scenario. In\nthis case, the FSC teaches the ASC to generate reasonable samples, while the pointer network trained on a large unlabelled data set helps the FSC model to perform better abstractive summarisation.\nIn Section 6, we evaluate the proposed model by jointly training the generative (ASC) and discriminative (FSC) models on the standard Gigaword sentence compression task with varying amounts of labelled and unlabelled data. The results demonstrate that by introducing a latent language variable we are able to match the previous benchmakers with small amount of the supervised data. When we employ our mixed discriminative and generative objective with all of the supervised data the model significantly outperforms all previously published results."}, {"heading": "2 Auto-Encoding Sentence Compression", "text": "In this section, we introduce the auto-encoding sentence compression model (Figure 1)1 in the framework of variational auto-encoders. The ASC model consists of four recurrent neural networks \u2013 an encoder, a compressor, a decoder and a language model.\nLet s be the source sentence, and c be the compression sentence. The compression model (encodercompressor) is the inference network q\u03c6(c|s) that takes source sentences s as inputs and generates extractive compressions c. The reconstruction\n1The language model, layer connections and decoder soft attentions are omitted in Figure 1 for clarity.\nmodel (compressor-decoder) is the generative network p\u03b8(s|c) that reconstructs source sentences s based on the latent compressions c. Hence, the forward pass starts from the encoder to the compressor and ends at the decoder. As the prior distribution, a language model p(c) is pre-trained to regularise the latent compressions so that the samples drawn from the compression model are likely to be reasonable natural language sentences."}, {"heading": "2.1 Compression", "text": "For the compression model (encoder-compressor), q\u03c6(c|s), we employ a pointer network consisting of a bidirectional LSTM encoder that processes the source sentences, and an LSTM compressor that generates compressed sentences by attending to the encoded source words.\nLet si be the words in the source sentences, hei be the corresponding state outputs of the encoder. hei are the concatenated hidden states from each direction:\nhei = f\u2212\u2192enc( ~hei\u22121, si)||f\u2190\u2212enc( ~hei+1, si) (1)\nFurther, let cj be the words in the compressed sentences, hcj be the state outputs of the compressor. We construct the predictive distribution by attending to the words in the source sentences:\nhcj =fcom(h c j\u22121, cj\u22121) (2)\nuj(i) =w T 3 tanh(W1h c j+W2h e i ) (3)\nq\u03c6(cj |c1:j\u22121, s)= softmax(uj) (4)\nwhere c0 is the start symbol for each compressed sentence and hc0 is initialised by the source sentence vector of he|s|. In this case, all the words cj sampled from q\u03c6(cj |c1:j\u22121, s) are the subset of the words appeared in the source sentence (i.e. cj \u2208 s)."}, {"heading": "2.2 Reconstruction", "text": "For the reconstruction model (compressor-decoder) p\u03b8(s|c), we apply a soft attention sequence-tosequence model to generate the source sentence s based on the compression samples c \u223c q\u03c6(c|s).\nLet sk be the words in the reconstructed sentences and hdk be the corresponding state outputs of the decoder:\nhdk = fdec(h d k\u22121, sk\u22121) (5)\nIn this model, we directly use the recurrent cell of the compressor to encode the compression samples2:\nh\u0302 c j =fcom(h\u0302 c j\u22121, cj) (6)\nwhere the state outputs h\u0302 c j corresponding to the word inputs cj are different from the outputs hcj in the compression model, since we block the information from the source sentences. We also introduce a start symbol s0 for the reconstructed sentence and hd0 is initialised by the last state output h\u0302 c |c|. The soft attention model is defined as:\nvk(j) =w T 6 tanh(W 4h d k +W 5h\u0302 c j) (7)\n\u03b3k(j) = softmax(vk(j)) (8) dk = \u2211|c|\nj \u03b3k(j)h\u0302\nc j(vk(j)) (9)\nWe then construct the predictive probability distribution over reconstructed words using a softmax:\np\u03b8(sk|s1:k\u22121, c) = softmax(W 7dk) (10)"}, {"heading": "2.3 Inference", "text": "In the ASC model there are two sets of parameters, \u03c6 and \u03b8, that need to be updated during inference. Due to the non-differentiability of the model, the reparameterisation trick of the VAE is not applicable in this case. Thus, we use the REINFORCE algorithm (Mnih et al., 2014; Mnih and Gregor, 2014) to reduce the variance of the gradient estimator.\nThe variational lower bound of the ASC model is:\nL =Eq\u03c6(c|s)[log p\u03b8(s|c)]\u2212DKL[q\u03c6(c|s)||p(c)] 6 log \u222b q\u03c6(c|s) q\u03c6(c|s) p\u03b8(s|c)p(c)dc = log p(s) (11)\nTherefore, by optimising the lower bound (Eq. 11), the model balances the selection of keywords for the summaries and the efficacy of the composed compressions, corresponding to the reconstruction error and KL divergence respectively.\nIn practise, the pre-trained language model prior p(c) prefers short sentences for compressions. As one of the drawbacks of VAEs, the KL divergence term in the lower bound pushes every sample drawn from the variational distribution towards the prior.\n2The recurrent parameters of the compressor are not updated by the gradients from the reconstruction model.\nThus acting to regularise the posterior, but also to restrict the learning of the encoder. If the estimator keeps sampling short compressions during inference, the LSTM decoder would gradually rely on the contexts from the decoded words instead of the information provided by the compressions, which does not yield the best performance on sentence compression.\nHere, we introduce a co-efficient \u03bb to scale the learning signal of the KL divergence: L=Eq\u03c6(c|s)[log p\u03b8(s|c)]\u2212\u03bbDKL[q\u03c6(c|s)||p(c)] (12)\nAlthough we are not optimising the exact variational lower bound, the ultimate goal of learning an effective compression model is mostly up to the reconstruction error. In Section 6, we empirically apply \u03bb = 0.1 for all the experiments on ASC model. Interestingly, \u03bb controls the compression rate of the sentences which can be a good point to be explored in future work.\nDuring the inference, we have different strategies for updating the parameters of \u03c6 and \u03b8. For the parameters \u03b8 in the reconstruction model, we directly update them by the gradients:\n\u2202L \u2202\u03b8 = Eq\u03c6(c|s)[ \u2202 log p\u03b8(s|c) \u2202\u03b8 ]\n\u2248 1 M \u2211 m \u2202 log p\u03b8(s|c(m)) \u2202\u03b8\n(13)\nwhere we draw M samples c(m) \u223c q\u03c6(c|s) independently for computing the stochastic gradients.\nFor the parameters \u03c6 in the compression model, we firstly define the learning signal,\nl(s, c) = log p\u03b8(s|c)\u2212 \u03bb(log q\u03c6(c|s)\u2212 log p(c)).\nThen, we update the parameters \u03c6 by:\n\u2202L \u2202\u03c6 = Eq\u03c6(c|s)[l(s, c) \u2202 log q\u03c6(c|s) \u2202\u03c6 ]\n\u2248 1 M \u2211 m [l(s, c(m)) \u2202 log q\u03c6(c (m)|s) \u2202\u03c6 ] (14)\nHowever, this gradient estimator has a big variance because the learning signal l(s, c(m)) relies on the samples from q\u03c6(c|s). Therefore, following the REINFORCE algorithm, we introduce two baselines b and b(s), the centred learning signal and inputdependent baseline respectively, to help reduce the variance.\nHere, we build an MLP to implement the inputdependent baseline b(s). During training, we learn the two baselines by minimising the expectation:\nEq\u03c6(c|s)[(l(s, c)\u2212 b\u2212 b(s)) 2]. (15)\nHence, the gradients w.r.t. \u03c6 are derived as,\n\u2202L \u2202\u03c6 \u2248 1 M \u2211 m (l(s, c(m))\u2212b\u2212b(s)) \u2202 log q\u03c6(c (m)|s) \u2202\u03c6\n(16) which is basically a likelihood-ratio estimator."}, {"heading": "3 Forced-attention Sentence Compression", "text": "In neural variational inference, the effectiveness of training largely depends on the quality of the inference network gradient estimator. Although we introduce a biased estimator by using pointer networks, it is still very difficult for the compression model to generate reasonable natural language sentences at the early stage of learning, which results in\nhigh-variance for the gradient estimator. Here, we introduce our supervised forced-attention sentence compression (FSC) model to teach the compression model to generate coherent compressed sentences.\nNeither directly replicating the pointer network of ASC model, nor using a typical sequence-tosequence model, the FSC model employs a forceattention strategy (Figure 2) that encourages the compressor to select words appearing in the source sentence but keeps the original full output vocabulary V . The force-attention strategy is basically a combined pointer network that chooses whether to select a word from the source sentence s or to predict a word from V at each recurrent state. Hence, the combined pointer network learns to copy the source words while predicting the word sequences of compressions. By sharing the pointer networks between the ASC and FSC model, the biased estimator obtains further positive biases by training on a small set of labelled source-compression pairs.\nHere, the FSC model makes use of the compression model (Eq. 1 to 4) in the ASC model,\n\u03b1j =softmax(uj), (17)\nwhere \u03b1j(i), i \u2208 (1, . . . , |s|) denotes the probability of selecting si as the prediction for cj .\nOn the basis of the pointer network, we further introduce the probability of predicting cj that is selected from the full vocabulary,\n\u03b2j = softmax(Wh c j), (18)\nwhere \u03b2j(w), w \u2208 (1, . . . , |V |) denotes the probability of selecting the wth from V as the prediction for cj . To combine these two probabilities in the RNN, we define a selection factor t for each state output, which computes the semantic similarities between the current state and the attention vector,\n\u03b7j = \u2211|s|\ni \u03b1j(i)h\ne i (19)\ntj = \u03c3(\u03b7 T jMh c j). (20)\nHence, the probability distribution over compressed words is defined as,\np(cj |c1:j\u22121, s)= { tj\u03b1j(i) + (1\u2212 tj)\u03b2j(cj), cj=si (1\u2212 tj)\u03b2j(cj), cj 6\u2208s\n(21)\nEssentially, the FSC model is the extended compression model of ASC by incorporating the pointer network with a softmax output layer over the full vocabulary. So we employ \u03c6 to denote the parameters of the FSC model p\u03c6(c|s), which covers the parameters of the variational distribution q\u03c6(c|s)."}, {"heading": "4 Semi-supervised Training", "text": "As the auto-encoding sentence compression (ASC) model grants the ability to make use of an unlabelled dataset, we explore a semi-supervised training framework for the ASC and FSC models. In this scenario we have a labelled dataset that contains source-compression parallel sentences, (s, c) \u2208 L, and an unlabelled dataset that contains only source sentences s \u2208 U. The FSC model is trained on L so that we are able to learn the compression model by maximising the log-probability,\nF = \u2211\n(c,s)\u2208L\nlog p\u03c6(c|s). (22)\nWhile the ASC model is trained on U, where we maximise the modified variational lower bound,\nL= \u2211 s\u2208U (Eq\u03c6(c|s)[log p\u03b8(s|c)]\u2212\u03bbDKL[q\u03c6(c|s)||p(c)]).\n(23)\nThe joint objective function of the semi-supervised learning is,\nJ= \u2211 s\u2208U (Eq\u03c6(c|s)[log p\u03b8(s|c)]\u2212\u03bbDKL[q\u03c6(c|s)||p(c)])\n+ \u2211\n(c,s)\u2208L\nlog p\u03c6(c|s). (24)\nHence, the pointer network is trained on both unlabelled data, U, and labelled data, L, by a mixed criterion of REINFORCE and cross-entropy."}, {"heading": "5 Related Work", "text": "As one of the typical sequence-to-sequence tasks, sentence-level summarisation has been explored by a series of discriminative encoder-decoder neural models. Filippova et al. (2015) carries out extractive summarisation via deletion with LSTMs, while Rush et al. (2015) applies a convolutional encoder and an\nattentional feed-forward decoder to generate abstractive summarises, which provides the benchmark for the Gigaword dataset. Nallapati et al. (2016) further improves the performance by exploring multiple variants of RNN encoder-decoder models. The recent works Gulcehre et al. (2016), Nallapati et al. (2016) and Gu et al. (2016) also apply the similar idea of combining pointer networks and softmax output. However, different from all these discriminative models above, we explore generative models for sentence compression. Instead of training the discriminative model on a big labelled dataset, our original intuition of introducing a combined pointer networks is to bridge the unsupervised generative model (ASC) and supervised model (FSC) so that we could utilise a large additional dataset, either labelled or unlabelled, to boost the compression performance. Dai and Le (2015) also explored semi-supervised sequence learning, but in a pure deterministic model focused on learning better vector representations.\nRecently variational auto-encoders have been applied in a variety of fields as deep generative models. In computer vision Kingma and Welling (2014), Rezende et al. (2014), and Gregor et al. (2015) have demonstrated strong performance on the task of image generation and Eslami et al. (2016) proposed variable-sized variational auto-encoders to identify multiple objects in images. While in natural language processing, there are variants of VAEs on modelling documents (Miao et al., 2016), sentences (Bowman et al., 2015) and discovery of relations (Marcheggiani and Titov, 2016). Apart from the typical initiations of VAEs, there are also a series of works that employs generative models for supervised learning tasks. For instance, Ba et al. (2015) learns visual attention for multiple objects by optimising a variational lower bound, Kingma et al. (2014) implements a semi-supervised framework for image classification and Miao et al. (2016) applies a conditional variational approximation in the task of factoid question answering. Dyer et al. (2016) proposes a generative model that explicitly extracts syntactic relationships among words and phrases which further supports the argument that generative models can be a statistically efficient method for learning neural networks from small data."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Dataset & Setup", "text": "We evaluate the proposed models on the standard Gigaword3 sentence compression dataset. This dataset was generated by pairing the headline of each article with its first sentence to create a source-compression pair. Rush et al. (2015) provided scripts4 to filter out outliers, resulting in roughly 3.8M training pairs, a 400K validation set, and a 400K test set. In the following experiments all models are trained on the training set with different data sizes5 and tested on a 2K subset, which is identical to the test set used by Rush et al. (2015) and Nallapati et al. (2016). We decode the sentences by k = 5 Beam search and test with full-length Rouge score.\nFor the ASC and FSC models, we use 256 for the dimension of both hidden units and lookup tables. In the ASC model, we apply a 3-layer bidirectional RNN with skip connections as the encoder, a 3-layer RNN pointer network with skip connections as the compressor, and a 1-layer vanilla RNN with soft attention as the decoder. The language model prior is trained on the article sentences of the full training set using a 3-layer vanilla RNN with 0.5 dropout. To lower the computational cost, we apply different vocabulary sizes for encoder and compressor (119,506 and 68,897) which corresponds to the settings of Rush et al. (2015). Specifically, the vocabulary of the decoder is filtered by taking the most frequent 10,000 words from the vocabulary of the encoder, where the rest of the words are tagged as \u2018<unk>\u2019. In further consideration of efficiency, we use only one sample for the gradient estimator. We optimise the model by Adam (Kingma and Ba, 2015) with a 0.0002 learning rate and 64 sentences per batch. The model converges in 5 epochs. Except for the pretrained language model, we do not use dropout or embedding initialisation for ASC and FSC models."}, {"heading": "6.2 Extractive Summarisation", "text": "The first set of experiments evaluate the models on extractive summarisation. Here, we denote the joint\n3https://catalog.ldc.upenn.edu/LDC2012T21 4https://github.com/facebook/NAMAS 5The hyperparameters where tuned on the validation set to maximise the perplexity of the summaries rather than the reconstructed source sentences.\nmodels by ASC+FSC1 and ASC+FSC2 where ASC is trained on unlabelled data and FSC is trained on labelled data. The ASC+FSC1 model employs equivalent sized labelled and unlabelled datasets, where the article sentences of the unlabelled data are the same article sentences in the labelled data, so there is no additional unlabelled data applied in this case. The ASC+FSC2 model employs the full unlabelled dataset in addition to the existing labelled dataset, which is the true semi-supervised setting.\nTable 1 presents the test Rouge score on extractive compression. We can see that the ASC+FSC1 model achieves significant improvements on F-1 scores when compared to the supervised FSC model only trained on labelled data. Moreover, fixing the labelled data size, the ASC+FSC2 model achieves better performance by using additional unlabelled data than the ASC+FSC1 model, which means the semi-supervised learning works in this scenario. Interestingly, learning on the unlabelled data largely increases the precisions (though the recalls do not benefit from it) which leads to significant improvements on the F-1 Rouge scores. And surprisingly, the extractive ASC+FSC1 model trained on full labelled data outperforms the abstractive NABS (Rush et al., 2015) baseline model (in Table 4)."}, {"heading": "6.3 Abstractive Summarisation", "text": "The second set of experiments evaluate performance on abstractive summarisation (Table 2). Consistently, we see that adding the generative objective to the discriminative model (ASC+FSC1) results in a significant boost on all the Rouge scores, while employing extra unlabelled data increase performance\nfurther (ASC+FSC2). This validates the effectiveness of transferring the knowledge learned on unlabelled data to the supervised abstractive summarisation.\nIn Figure 3, we present the validation perplexity to compare the abilities of the three models to learn the compression languages. The ASC+FSC1(red) employs the same dataset for unlabelled and labelled training, while the ASC+FSC2(black) employs the full unlabelled dataset. Here, the joint ASC+FSC1 model obtains better perplexities than the single discriminative FSC model, but there is not much difference between ASC+FSC1 and ASC+FSC2 when the size of the labelled dataset grows. From the perspective of language modelling, the generative ASC model indeed helps the discriminative model learn to generate good summary sentences. Table 3 displays the validation perplexities of the benchmark models, where the joint ASC+FSC1 model trained on the full labelled and unlabelled datasets performs the best on modelling compression languages.\nTable 4 compares the test Rouge score on abstractive summarisation. Encouragingly, the semisupervised model ASC+FSC2 outperforms the baseline model NABS when trained on 500K supervised pairs, which is only about an eighth of the supervised data. In Nallapati et al. (2016), the authors exploit the full limits of discriminative RNN encoderdecoder models by incorporating a sampled softmax, expanded vocabulary, additional lexical features, and combined pointer networks6, which yields the best performance listed in Table 4. However, when all the data is employed with the mixed ob-\n6The idea of the combined pointer networks is similar to the FSC model, but the implementations are slightly different.\njective ASC+FSC1 model, the result is significantly better than this previous state-of-the-art. As the semisupervised ASC+FSC2 model can be trained on unlimited unlabelled data, there is still significant space left for further performance improvements.\nTable 5 presents the examples of the compression sentences decoded by the joint model ASC+FSC1 and the FSC model trained on the full dataset."}, {"heading": "7 Discussion", "text": "From the perspective of generative models, a significant contribution of our work is a process for reducing variance for discrete sampling-based variational inference. The first step is to introduce two baselines in the control variates method due to the fact that the reparameterisation trick is not applica-\nble for discrete latent variables. However it is the second step of using a pointer network as the biased estimator that makes the key contribution. This results in a much smaller state space, bounded by the length of the source sentence (mostly between 20 and 50 tokens), compared to the full vocabulary. The final step is to apply the FSC model to transfer the knowledge learned from the supervised data to the pointer network. This further reduces the sampling variance by acting as a sort of bootstrap or constraint on the unsupervised latent space which could encode almost anything but which thus becomes biased towards matching the supervised distribution. By using these variance reduction methods, the ASC model is able to carry out effective variational inference for the latent language model so that it learns to summarise the sentences from the large unlabelled training data.\nIn a different vein, according to the reinforcement learning interpretation of sequence level training (Ranzato et al., 2016), the compression model of the ASC model acts as an agent which iteratively generates words (takes actions) to compose the com-\npression sentence and the reconstruction model acts as the reward function evaluating the quality of the compressed sentence which is provided as a reward signal. Ranzato et al. (2016) presents a thorough empirical evaluation on three different NLP tasks by using additional sequence-level reward (BLEU and Rouge-2) to train the models. In the context of this paper, we apply a variational lower bound (mixed reconstruction error and KL divergence regularisation) instead of the explicit Rouge score. Thus the ASC model is granted the ability to explore unlimited unlabelled data resources. In addition we introduce a supervised FSC model to teach the compression model to generate stable sequences instead of starting with a random policy. In this case, the pointer network that bridges the supervised and unsupervised model is trained by a mixed criterion of REINFORCE and cross-entropy in an incremental learning framework. Eventually, according to the experimental results, the joint ASC and FSC model is able to learn a robust compression model by exploring both labelled and unlabelled data, which outperforms the other single discriminative compression models that are only trained by cross-entropy reward signal."}, {"heading": "8 Conclusion", "text": "In this paper we have introduced a generative model for jointly modelling pairs of sequences and evaluated its efficacy on the task of sentence compression. The variational auto-encoding framework provided an effective inference algorithm for this approach and also allowed us to explore combinations of discriminative (FSC) and generative (ASC) compression models. The evaluation results show that supervised training of the combination of these models improves upon the state-of-the-art performance for the Gigaword compression dataset. When we train the supervised FSC model on a small amount of labelled data and the unsupervised ASC model on a large set of unlabelled data the combined model is able to outperform previously reported benchmarks trained on a great deal more supervised data. These results demonstrate that we are able to model language as a discrete latent variable in a variational auto-encoding framework and that the resultant generative model is able to effectively exploit both supervised and unsupervised data in sequence-to-sequence tasks."}], "references": [{"title": "Volodymyr Mnih", "author": ["Jimmy Ba"], "venue": "and Koray Kavukcuoglu.", "citeRegEx": "Ba et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kyunghyun Cho", "author": ["Dzmitry Bahdanau"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bahdanau et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Rafal Jozefowicz", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai"], "venue": "and Samy Bengio.", "citeRegEx": "Bowman et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kyunghyun Cho", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chorowski et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Semi-supervised sequence learning", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Miguel Ballesteros", "author": ["Chris Dyer", "Adhiguna Kuncoro"], "venue": "and Noah A Smith.", "citeRegEx": "Dyer et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Koray Kavukcuoglu", "author": ["SM Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa"], "venue": "and Geoffrey E Hinton.", "citeRegEx": "Eslami et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Lukasz Kaiser", "author": ["Katja Filippova", "Enrique Alfonseca", "Carlos A Colmenares"], "venue": "and Oriol Vinyals.", "citeRegEx": "Filippova et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Alex Graves", "author": ["Karol Gregor", "Ivo Danihelka"], "venue": "and Daan Wierstra.", "citeRegEx": "Gregor et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Hang Li", "author": ["Jiatao Gu", "Zhengdong Lu"], "venue": "and Victor OK Li.", "citeRegEx": "Gu et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Bowen Zhou", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati"], "venue": "and Yoshua Bengio.", "citeRegEx": "Gulcehre et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Hinton", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Welling2014] Diederik P Kingma", "Max Welling"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Danilo Jimenez Rezende", "author": ["Diederik P Kingma", "Shakir Mohamed"], "venue": "and Max Welling.", "citeRegEx": "Kingma et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Discrete-state variational autoencoders for joint discovery and factorization of relations", "author": ["Marcheggiani", "Titov2016] Diego Marcheggiani", "Ivan Titov"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Marcheggiani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Marcheggiani et al\\.", "year": 2016}, {"title": "Lei Yu", "author": ["Yishu Miao"], "venue": "and Phil Blunsom.", "citeRegEx": "Miao et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Gregor2014] Andriy Mnih", "Karol Gregor"], "venue": "In Proceedings of ICML", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Nicolas Heess", "author": ["Volodymyr Mnih"], "venue": "and Alex Graves.", "citeRegEx": "Mnih et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "\u00c7a glar Gul\u00e7ehre", "author": ["Ramesh Nallapati", "Bowen Zhou"], "venue": "and Bing Xiang.", "citeRegEx": "Nallapati et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Shakir Mohamed", "author": ["Danilo J Rezende"], "venue": "and Daan Wierstra.", "citeRegEx": "Rezende et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Geoffrey E Hinton", "author": ["David E Rumelhart"], "venue": "and Ronald J Williams.", "citeRegEx": "Rumelhart et al.1985", "shortCiteRegEx": null, "year": 1985}, {"title": "Sumit Chopra", "author": ["Alexander M Rush"], "venue": "and Jason Weston.", "citeRegEx": "Rush et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Oriol Vinyals", "author": ["Ilya Sutskever"], "venue": "and Quoc V Le.", "citeRegEx": "Sutskever et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Meire Fortunato", "author": ["Oriol Vinyals"], "venue": "and Navdeep Jaitly.", "citeRegEx": "Vinyals et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Richard Zemel", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov"], "venue": "and Yoshua Bengio.", "citeRegEx": "Xu et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.", "creator": "LaTeX with hyperref package"}}}