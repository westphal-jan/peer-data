{"id": "1609.07568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects", "abstract": "discriminating between very closely - related language varieties is considered a challenging numerical and important task. this paper describes throughout our submission to the dsl 2016 shared - task, essentially which included two sub - tasks : ; one on discriminating similar languages and one on identifying arabic dialects. we developed a character - level neural network for this task. initially given a sequence of characters, our model embeds each character in vector message space, runs the sequence through multiple convolutions with different character filter widths, expands and pools the stored convolutional representations to obtain a hidden vector representation of the text that is used usually for predicting the language or dialect. we primarily focused on the adaptive arabic dialect identification task and generally obtained an f1 score of with 0. 4834, ranking 6th out of 18 participants. we also analyze errors made by our system on the innate arabic data typing in some detail, and point to challenges such an approach is confidently faced with.", "histories": [["v1", "Sat, 24 Sep 2016 04:02:13 GMT  (49kb,D)", "http://arxiv.org/abs/1609.07568v1", "DSL 2016"]], "COMMENTS": "DSL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yonatan belinkov", "james glass"], "accepted": false, "id": "1609.07568"}, "pdf": {"name": "1609.07568.pdf", "metadata": {"source": "CRF", "title": "A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects", "authors": ["Yonatan Belinkov"], "emails": ["glass}@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Automatic language identification is an important first step in many applications. For many languages and texts, this is a fairly easy step that can be solved with familiar methods like n-gram language models. However, distinguishing between similar languages is not so easy. The shared-task on discriminating between similar languages (DSL) has offered a test bed for evaluating models on this task since 2014 (Zampieri et al., 2014; Zampieri et al., 2015). The 2016 shared-task included two sub-tasks: (1) discriminating between similar languages from several groups; and (2) discriminating between Arabic dialects (Malmasi et al., 2016). The following language varieties were considered in sub-task 1: Bosnian, Croatian, and Serbian; Malay and Indonesian; Portuguese of Brazil and Portugal; Spanish of Argentina, Mexico, and Spain; and French of France and Canada. In sub-task 2, the following Arabic varieties were considered: Levantine, Gulf, Egyptian, North African, and Modern Standard Arabic (MSA).\nThe training datasets released in the two sub-tasks were very different. Sub-task 1 was comprised of journalistic texts and included training and development sets. Sub-task 2 had automatic transcriptions of spoken recordings and included only a training set. This was the first time DSL has offered a task on Arabic dialects. The shared-task also included an open track that allows additional resources, but we have only participated in the closed track.\nPrevious DSL competitions attracted a variety of methods, achieving very high results with accuracies of over 95%. Most teams used character and word n-grams with various classifiers. One team in 2015 used vector embeddings of words and sentences (Franco-Salvador et al., 2015), achieving very good, though not state-of-the-art results. They trained unsupervised vectors and fed them as input to a classifier. Here we are interested in character-level neural network models. Such models showed recent success in other tasks (Kim et al., 2016, among many others). The basic question we ask is this: how well can a character-level neural network perform on this task without the notion of a word? To answer this, our model takes as input a sequence of characters, embeds them in vector space, and generates a high-level representation of the sequence through multiple convolutional layers. At the top of the network, we output a probability distribution over labels and backpropagate errors, such that the entire network can be learned end-to-end.\n1The code for this work is available at https://github.com/boknilev/dsl-char-cnn.\nar X\niv :1\n60 9.\n07 56\n8v 1\n[ cs\n.C L\n] 2\n4 Se\np 20\n16\nWe experimented with several configurations of convolutional layers, focusing on Arabic dialect identification (sub-task 2). We also participated in sub-task 1, but have not tuned our system to this scenario. Our best system obtained an F1 score of 0.4834 on the Arabic sub-task, ranking 6th out of 18 participants. The same system did not perform well on sub-task 1 (ranked 2nd to last), although we have not spent much time on adapting it to this task. In the next section, we discuss related work on identifying similar languages and dialects. We then present our methodology and results, and conclude with a discussion and a short error analysis for the Arabic system that sheds light on potential sources of errors."}, {"heading": "2 Related Work", "text": "Discriminating similar languages and dialects has been the topic of two previous iterations of the DSL task (Zampieri et al., 2014; Zampieri et al., 2015). The previous competitions proved that despite very good performance (over 95% accuracy), it is still a non-trivial task that is considerably more difficult than identifying unrelated languages. Admittedly, even humans have a hard time identifying the correct language in certain cases, as observed by Goutte et al. (2016). The previous shared-task reports contain detailed information on the task, related work, and participating systems. Here we only highlight a few relevant trends.\nIn terms of features, the majority of the groups in previous years used sequences of characters and words. A notable exception is the use of word white-lists by Porta and Sancho (2014). Different learning algorithms were used for this task, most commonly linear SVMs or maximum entropy models. Some groups formulated a two-step classification model: first predicting the language group and then predicting an individual language. For simplicity, we only trained a single multi-class classification model, although we speculate that a two-step process could improve our results. For Arabic dialect identification (sub-task 2), one could first distinguish MSA from all the other dialects, and then identify the specific dialect.\nLast year, Franco-Salvador et al. (2015) used vector embeddings of words and sentences, achieving very good results, though not state-of-the-art. They trained unsupervised word vectors and fed them as input to a classifier. In contrast, we build an end-to-end neural network over character sequences, and train character embeddings along with other parameters of the network. Using character embeddings is particularly appealing for this task given the importance of character n-gram features in previous work. In light of the recent success of character-level neural networks in various language processing and understanding tasks (Santos and Zadrozny, 2014; Zhang et al., 2015; Luong and Manning, 2016; Kim et al., 2016), we were interested to see how far one can go on this task without any word-level information.\nFinally, this year\u2019s task offered a sub-task on Arabic dialect identification. It is unique in that the texts are automatic transcriptions generated by a speech recognizer. Previous work on Arabic dialect identification mostly used written texts (Zaidan and Callison-Burch, 2014; Malmasi et al., 2015) or speech recordings, with access to the acoustic signal (Biadsy et al., 2009; Ali et al., 2016). For example, Ali et al. (2016) exploited both acoustic and ASR-based features, finding that their combination works best. Working with automatic transcriptions obscures many dialectal differences (e.g. in phonology) and leads to inevitable errors. Still, we were interested to see how well a character-level neural network can perform on this task, without access to acoustic features."}, {"heading": "3 Methodology", "text": "We formulate the task as a multi-class classification problem, where each language (or dialect) is a separate class. We do not consider two-step classification, although this was found useful in previous work (Zampieri et al., 2015). Formally, given a collection of texts and associated labels, {t(i), l(i)}, we need to find a predictor f : t \u2192 l. Our predictor is a neural network over character sequences. Let t := c = c1, \u00b7 \u00b7 \u00b7 , cL denote a sequence of characters, where L is a maximum length that we set empirically. Longer texts are truncated and shorter ones are padded with a special PAD symbol. Each character c in the alphabet is represented as a real-valued vector xc. This character embedding is learned during training.\nOur neural network has the following structure:\n\u2022 Input layer: mapping the character sequence c to a vector sequence x. The embedding layer is followed by dropout.\n\u2022 Convolutional layers: multiple parallel convolutional layers, mapping the vector sequence x to a hidden sequence h. Each convolution is followed by a Rectified Linear Unit (ReLU) nonlinearity (Glorot et al., 2011). The outputs of all the convolutional layers are concatenated.\n\u2022 Pooling layer: a max-over-time pooling layer, mapping the vector sequence h to a single hidden vector h representing the entire sequence.\n\u2022 Fully-connected layer: one hidden layer with a ReLU non-linearity and dropout, mapping h to the final vector representation of the text, h\u2032.\n\u2022 Output layer: a softmax layer, mapping h\u2032 to a probability distribution over labels l. During training, each sequence is fed into this network to create label predictions. As errors are backpropagated down the network, the weights at each layer are updated, including the embedding layer. During testing, the learned weights are used in a forward step to compute a prediction over the labels. We always take the best predicted label for evaluation."}, {"heading": "3.1 Training details and submitted runs", "text": "We train the entire network jointly, including the embedding layer. We use Adam (Kingma and Ba, 2014) with the default original parameters to minimize the cross-entropy loss. Training is run with shuffled mini-batches of size 16 and stopped once the loss on the dev set stops improving; we allow a patience of 10 epochs. Our implementation is based on Keras (Chollet, 2015) with the TensorFlow backend (Abadi et al., 2015).\nWe mostly experimented with the sub-task 2 dataset of Arabic dialects. Since the official shared-task did not include a dedicated dev set, we randomly allocated 90% of the training set for development. We tuned the following hyperparameters using this split (chosen parameters are in bold): embedding layer dropout (0.2, 0.5), fully-connected layer dropout (0.2, 0.5), maximum text length (200, 400, 800), character embedding size (25, 50, 100), fully-connected layer output size (100, 250). Removing the fully-connected layer led to a small drop in performance.\nFor the convolutional layers, we experimented with different combinations of filter widths and number of filters. We started with a single filter width and noticed that a width of 5 characters performs fairly well with enough filters (250). We then added multiple widths, similarly to a recent character-CNN used in language modeling (Kim et al., 2016). Using multiple widths led to a small improvement. Our best configuration was: {1 \u2217 50, 2 \u2217 50, 3 \u2217 100, 4 \u2217 100, 5 \u2217 100, 6 \u2217 100, 7 \u2217 100}, where w \u2217 n indicates n filters of width w.\nSince the shared-task did not provide a dev set for sub-task 2, we explored several settings in our three submitted runs. Run 1 used 90% of the training set for training and 10% for development. Run 2 used 100% of the training for training, but stopped at the same epoch as Run 1. Run 3 used 10 different models, each trained on a different random 90%/10% train/dev split, with a plurality vote among the 10 models to determine the final prediction.\nFor the (larger) dataset of sub-task 1, we did not perform any tuning of hyperparameters and used the same setting as in sub-task 2, except for a larger mini-batch size (64) to speed up training. This was our Run 1, whereas in Run 2 we used more filter maps, following the setting in (Kim et al., 2016): {1 \u2217 50, 2 \u2217 100, 3 \u2217 150, 4 \u2217 200, 5 \u2217 200, 6 \u2217 200, 7 \u2217 200}. Run 3 was the same as Run 2, but with more hidden units (500) and a higher dropout rate (0.7) in the fully-connected layer."}, {"heading": "4 Results", "text": "Table 1 shows the results of our submitted runs, along with two baselines: a random baseline for sub-task 1, test set A (a balanced test set), and a majority baseline for sub-task 2, test set C (a slightly unbalanced test set). We also report results of the best performing systems in the shared-task.\nIn sub-task 2, test set C, we notice a fairly large difference between our runs. Our best result, with Run 3, used plurality voting among 10 different models trained on 90% of the training data. We chose this strategy in an effort to avoid overfitting. However, during development we obtained accuracy results of around 57-60% on a separate train/dev split, so we suspect there was still overfitting of the training set. With this run our team ranked 6th out of 18 teams according to the official evaluation.\nFor sub-task 1, test set A, larger models perform somewhat better, due to the larger training set. In this case we only have a small drop of about 2% in comparison to our performance on the dev set (not shown). Our system did not perform very well compared to other teams (we ranked 2nd to last), but this may be expected as we did not tune our system for this dataset.\nFigure 1 shows confusion matrices for our best runs. Clearly, the vast majority of the confusion in subtask 1 (test set A) comes from languages in the same group, whereas languages from different groups are rarely confused. The Spanish varieties are the most difficult to distinguish, with F1 scores between 0.57 (Mexican Spanish) and 0.75 (Argentinian Spanish). The South Slavic languages are less confused, with F1 scores of 0.75-0.83. French and Portuguese languages are easier to distinguish (F1 around 0.90), and Malay and Indonesian are the least confused (F1 of 0.96-0.97).\nTurning to sub-task 2 (test set C), we see much more confusion, as also reflected in the final results (Table 1). Gulf is the most confusing dialect: true Gulf examples are often predicted as MSA, and true Levantine and North African examples are wrongly predicted as Gulf relatively frequently. This is also reflected in a low F1 score of 0.34 for Gulf. The other dialects have higher F1 scores ranging between 0.47 and 0.50, with MSA having an F1 of 0.60, making it the easiest variety to detect."}, {"heading": "5 Discussion", "text": "In this section we focus on the Arabic dataset (sub-task 2, test set C) and consider some of our findings. The first issue that should be stressed is the nature of the data. As the texts are automatically generated speech transcriptions, they contain mistakes that depend on the training data used for the speech recognizer. This might have a negative effect on the ability to correctly recognize the dialect just from the transcriptions. For comparison, previous work found that using acoustic features improves dialect recognition in a similar setting (Ali et al., 2016). Secondly, the transcribed texts use a transliteration system2 that was designed for written MSA and obscures many dialectal features that are important for distinguishing Arabic dialects, especially phonological features.\nThe fact that MSA is confused with dialectal varieties at all may sound surprising at first. However, due to the writing scheme many of the known differences between MSA and the dialects are not accessible. In addition, MSA is often mixed with dialectal Arabic in many settings (e.g. news broadcasts), so it is reasonable to find MSA features in dialectal speech.\nLevantine and Gulf dialects are relatively close geographically and linguistically, so confusing one with the other might be expected. The confusion between Gulf and North African dialects is more surprising, although there are always parallel innovations in dialects that stem from a mutual ancestor, as all Arabic dialects do.\nOther factors that can play into similarities and differences are local foreign languages, religious terminology, and genre and domain. However, these are aspects that are difficult to tease apart without a more careful analysis, and they also depend on the data sources and the ASR system.\nError analysis We conclude this discussion with an analysis of several example errors made by our systems, as shown in Figure 2. These are examples from a dev set that we kept held out during the development of our system. In each case, we give the true and predicted labels, the text, and a rough translation. Note that the translations are often questionable due to ASR mistakes and the lack of context.\nIn the first example, an MSA text is predicted as Levantine, possibly due to the word AllbnAnyp \u201cLebanese\", whose character sequence could be typical of Levantine texts. There are clear MSA features like the dual forms hmA and -An, but these are ambigious with dialectal forms that are found in the training data. Note also the Verb-Subject word order, typical of MSA \u2013 such a pattern requires syntactic knowledge and cannot be easily detected with simple character (or even word) features.\nThe second error shows an example of a mixed text, containing both dialectal (>h HDrp \u201cyes the honorable\", bdy \u201cI want\") and MSA features (hl syHdv \u201cwill it happen\"). It is an example of mixing that is common in spoken data and can confuse the model.\nIn the third example, the form Alm$kwk fyh \u201cdoubtful\" has a morphological construction that is typical of MSA. Such a feature is difficult for a character-model to capture and might require morphological knowledge.\nIn the fourth example, the words AlmAlky and AlhA$my are actually seen most commonly in MSA in the training data, but they do not contain typical character-sequences. The phrase Al Al, which indicates some stuttering, is more common in some of the dialects than in MSA in the training data, but is about as common in NOR and MSA.\nIn the fifth example, the word byt>vr \u201cinfluences\" is likely misrecognized, as in Egyptian it would probably be byt>tr, but an Arabic language model with much MSA in it might push the ASR system towards the sequence >vr, which in turn is more common in Gulf.\nIn the seventh example, the phrase <HnA wyAhm \u201cwe\u2019re with them\" might be more indicative of Gulf, but in any case is rare in the training data in both North African and Gulf. bqyt \u201cremained\u201d should have been a good clue for North African, and indeed it appears 5 times in training as North African and not at all as Gulf.\n2http://www.qamus.org/transliteration.htm."}, {"heading": "6 Conclusion", "text": "In this work, we explored character-level convolutional neural networks for discriminating between similar languages and dialects. We demonstrated that such a model can perform quite well on Arabic dialect identification, although it does not achieve state-of-the-art results. We also conducted a short analysis of errors made by our system on the Arabic dataset, pointing to challenges that such a model is faced with.\nA natural extension of this work is to combine word-level features in the neural network. White-lists of words typical of certain dialects might also help, although in preliminary experiments we were not able to obtain performance gains by adding such features. We are also curious to see how our model would perform with access to the speech recordings, for example by running it on recognized phone sequences or directly incorporating acoustic features. This, however, would require a different preparation of the dataset, which we hope would be made available in future DSL tasks."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Sergiu Nisioi and Noam Ordan for helpful discussions. This work was supported by the Qatar Computing Research Institute (QCRI). Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "author": ["gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "gas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "gas et al\\.", "year": 2015}, {"title": "Peter Bell", "author": ["Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass"], "venue": "and Steve Renals.", "citeRegEx": "Ali et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Julia Hirschberg", "author": ["Fadi Biadsy"], "venue": "and Nizar Habash.", "citeRegEx": "Biadsy et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Paolo Rosso", "author": ["Marc Franco-Salvador"], "venue": "and Francisco Rangel.", "citeRegEx": "Franco.Salvador et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Antoine Bordes", "author": ["Xavier Glorot"], "venue": "and Yoshua Bengio.", "citeRegEx": "Glorot et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Serge L\u00c3l\u2019ger", "author": ["Cyril Goutte"], "venue": "Shervin Malmasi, and Marcos Zampieri.", "citeRegEx": "Goutte et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "David Sontag", "author": ["Yoon Kim", "Yacine Jernite"], "venue": "and Alexander Rush.", "citeRegEx": "Kim et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. arXiv preprint arXiv:1604.00788", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Eshrag Refaee", "author": ["Shervin Malmasi"], "venue": "and Mark Dras.", "citeRegEx": "Malmasi et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ahmed Ali", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov"], "venue": "and J\u00f6rg Tiedemann.", "citeRegEx": "Malmasi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Using Maximum Entropy Models to Discriminate between Similar Languages and Varieties", "author": ["Porta", "Sancho2014] Jordi Porta", "Jos\u00e9-Luis Sancho"], "venue": "In Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),", "citeRegEx": "Porta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Porta et al\\.", "year": 2014}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "2014", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann"], "venue": "A Report on the DSL Shared Task", "citeRegEx": "Zampieri et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2015", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": "Overview of the DSL Shared Task", "citeRegEx": "Zampieri et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Junbo Zhao", "author": ["Xiang Zhang"], "venue": "and Yann LeCun.", "citeRegEx": "Zhang et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.1", "creator": "LaTeX with hyperref package"}}}