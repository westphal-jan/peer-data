{"id": "1609.08326", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation", "abstract": "with the fast development of deep learning, people have started to train very big neural networks using massive data. asynchronous stochastic gradient descent ( asgd ) is widely used to fulfill this task, which, traditionally however, is known to suffer from the particular problem of delayed gradient. that is, when a local factory worker adds the gradient it calculates to the global model, the global model may have been updated by other workers and this gradient becomes \" fundamentally delayed \". we propose producing a novel technology to compensate this delay, so as to make the optimization behavior of asgd closer accurate to that of sequential sgd. this is done by leveraging taylor constant expansion of the gradient function and efficient approximators to the hessian matrix of the loss function. we call the corresponding new algorithm delay compensated asgd ( dc - asgd ). we evaluated the proposed algorithm on cifar - 10 and imagenet datasets, and experimental results strongly demonstrate that dc - asgd can outperform both synchronous sgd and asgd, and nearly approaches the performance gains of regular sequential sgd.", "histories": [["v1", "Tue, 27 Sep 2016 09:22:03 GMT  (327kb,D)", "http://arxiv.org/abs/1609.08326v1", "7 pages, 5 figures"], ["v2", "Mon, 12 Jun 2017 17:53:10 GMT  (383kb,D)", "http://arxiv.org/abs/1609.08326v2", "20 pages, 5 figures"], ["v3", "Tue, 13 Jun 2017 09:02:32 GMT  (384kb,D)", "http://arxiv.org/abs/1609.08326v3", "20 pages, 5 figures"], ["v4", "Wed, 14 Jun 2017 12:45:50 GMT  (384kb,D)", "http://arxiv.org/abs/1609.08326v4", "20 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["shuxin zheng", "qi meng", "taifeng wang", "wei chen", "nenghai yu", "zhiming ma", "tie-yan liu"], "accepted": true, "id": "1609.08326"}, "pdf": {"name": "1609.08326.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning", "authors": ["Shuxin Zheng", "Qi Meng", "Taifeng Wang", "Wei Chen", "Nenghai Yu", "Zhi-Ming Ma", "Tie-Yan Liu"], "emails": ["zhengsx@mail.ustc.edu.cn,", "ynh@ustc.edu.cn", "qimeng13@pku.edu.cn", "tie-yan.liu}@microsoft.com", "mazm@amt.ac.cn"], "sections": [{"heading": "1 Introduction", "text": "Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition Sak, Senior, and Beaufays (2014); Sercu et al. (2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al. (2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016).\nStochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al. (2012). As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way. In synchronous SGD (SSGD) Chen et al. (2016), local workers compute the gradients over a mini-batch of their own data, and add these gradients to the \u2217This work was done when the author was visiting Microsoft Research Asia. Copyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nglobal model. By using a barrier, these workers wait for each other during this process, and will not continue their local training until the gradients from all the M workers have been added to the global model. Due to the barrier, the training speed is dragged by the slowest worker.1 To improve the training efficiency, many people choose to use asynchronous SGD (ASGD) Dean et al. (2012), with which no barrier is imposed, and each local work continues its training process right after its gradient is added to the global model. It is clear that ASGD could be faster than SSGD due to no waiting overhead, however, ASGD suffers from another critical problem which we call delayed gradient. That is, because there is no barrier, before a worker wants to add its gradient g(wt) to the global model, several other workers may have already added their own gradients and the global model has been updated to wt+\u03c4 (here \u03c4 is called the delay factor). Adding the gradient g(wt) (which is calculated with respect to wt) to another model wt+\u03c4 is not mathematically sound, and the training trajectory may suffer from unexpected turbulence. This problem has been well known, and some researchers have analyzed its negative effect to the convergence speed Lian et al. (2015); Avron, Druinsky, and Gupta (2015).\nIn this paper, we want to tackle the challenges of delayed gradient so as to make ASGD both efficient and more mathematically sound. The key idea is as follows. Clearly, we should better adopt the gradient with respect to the current global model wt+\u03c4 (denoted as g(wt+\u03c4 )) but not a delayed version g(wt). In order to bridge the gap between g(wt+\u03c4 ) and g(wt) (i.e., compensate the delay), we investigate the Taylor expansion of g(wt+\u03c4 ) and found that the delayed gradient g(wt) is just its zero-order approximator. We then propose further leveraging the first-order item in the Taylor expansion to achieve more accurate approximation of g(wt+\u03c4 ). However, this simple idea is actually highly non-trivial, because the first-order derivatives of the gradient g(wt+\u03c4 ) correspond to the second-order derivatives of the original loss function. That is, we may need to calculate the Hessian matrix of the loss function in order to compensate the delay. This is expensive because of the high computation and space complexity\n1Recently, people proposed to partially resolve this problem by using additional backup workers Chen et al. (2016). However, this technique requires additional computation resource, and works on the assumption that the majority of workers train almost equally fast.\nar X\niv :1\n60 9.\n08 32\n6v 1\n[ cs\n.L G\n] 2\n7 Se\np 20\n16\nto obtain the Hessian matrix. To overcome this new challenge, we first construct an easy-to-compute approximator to the Hessian matrix, which can be computed based on the previously known gradients only. We prove that this approximator is equal to the Hessian matrix in expectation, when the loss function takes the form of cross entropy. After that, we further improve the approximator by considering better trade-off between bias and variance.\nBy using the Taylor expansion and the proposed approximators to the Hessian matrix, we have designed a new algorithm, which we call Delay Compensated ASGD (or DCASGD for short). DC-ASGD is very similar to ASGD in the sense that no worker needs to wait for others. It differs from ASGD in that it does not simply add the local gradient to the global model, but compensates the delay in the local gradient by using the approximate Taylor expansion before adding it to the global model. By doing so, it maintains almost the same efficiency as ASGD and achieves much higher accuracy. We conducted experiments on CIFAR-10 and ImageNet datasets. The results have demonstrated the effectiveness of our proposal. As compared to SSGD and ASGD, DC-ASGD accelerated the convergence of the training process, and the accuracy of the obtained model within the same time period was better (and actually very close to the accuracy obtained by the sequential SGD algorithm)."}, {"heading": "2 Problem Setting", "text": "In this section, we introduce DNN and its parallel training through the ASGD method.\nGiven a multi-class classification problem, we denote X = Rd as the input space, Y = {1, ...,K} as the output space, and P as the joint underlying distribution over X \u00d7 Y . Here d denotes the dimension of the input space, and K denotes the number of categories in the output space.\nWe have a training set {(x1, y1), ..., (xS , yS)}, whose elements are i.i.d. sampled from X \u00d7 Y according to the distribution P. Our goal is to learn a neural network model O \u2208 F : X \u00d7 Y \u2192 R parameterized by w \u2208 Rn based on the training set. Specifically, the neural network models have hierarchical structures, in which each node conducts linear combination and non-linear activation over its connected nodes in the lower layer. The parameters are the weights for the edges between two layers. The neural network model produces an output vector, i.e., (O(x, k; w); k \u2208 Y) for each input x \u2208 X , indicating its likelihoods of belonging to different categories.\nA widely used loss function for deep neural networks is the cross entropy loss, which is defined as follows,\nf(x, y; w) = \u2212 K\u2211 k=1 (I[y=k] log \u03c3k(x; w)). (1)\nHere \u03c3k(x; w) = e O(x,k;w)\u2211K\nk\u2032=1 e O(x,k\u2032;w) is the Softmax operation.\nBecause the underlying distribution P is unknown, the common way of learning the model is to minimize the following empirical loss function,\nF (w) = 1 S S\u2211 i=1 fi(w) = 1 S S\u2211 i=1 f(xi, yi; w). (2)\nAlthough the cross entropy loss is convex w.r.t. the output vector, it is well-known that, with the multi-layer activation, the objective is non-convex w.r.t. the parameter w.\nAs mentioned in the introduction, ASGD is a widely used approach to perform parallel training of neural networks on multiple machines. Although ASGD is highly efficient, it is well known to suffer from the problem of delayed gradient. To better illustrate this problem, let us have a close look at the training process of ASGD as shown in Figure 1. According to the figure, a particular worker starts from the global model wt at time t, then it calculates the local gradient g(wt) and add this gradient back to the global model.2 However, before this happens, some other \u03c4 workers may have already added their local gradients to the global model, the global model has been updated \u03c4 times and becomes wt+\u03c4 . The ASGD algorithm ignores this problem, and simply adds the gradient g(wt) to the global model wt+\u03c4 , as follows.\nwt+\u03c4+1 = wt+\u03c4 \u2212 \u03b7g(wt), (3)\nwhere \u03b7 is the learning rate. It is clear that the above update rule of ASGD is inequivalent to that of SGD: one actually adds a \u201cdelayed\u201d gradient g(wt) to the current global model wt+\u03c4 . In contrast, with SGD, one should update the global model wt+\u03c4 based on the gradient w.r.t. wt+\u03c4 . Actually, this problem of delayed gradient has been well known Lian et al. (2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al. (2013), however, to the best of our knowledge, there is no sound solution to compensate the delayed gradient while keeping the high efficiency of ASGD yet. This is exactly the motivation of our paper.\n2Actually, the local gradient is also related to the randomly sampled data (x, y). For simplicity, when there is no confusion, we will omit x, y in the notation."}, {"heading": "3 Delay Compensation using Taylor Expansion and Hessian Approximation", "text": "ASGD is a double-edged sword: it has high efficiency but suffers from the problem of delayed gradient. Ideally, the optimization algorithm should add g(wt+\u03c4 ) to the global model wt+\u03c4 (just as SGD does), however, ASGD adds a delayed version g(wt). Then the question is whether we can find a way to bridge this gap. In this section, we show that this gap can be illustrated using Taylor Expansion, and based on this insight, we can design effective algorithms to compensate the delay.\nGradient decomposition using Taylor expansion The Taylor expansion of g(wt+\u03c4 ) at wt can be written as follows Folland (2005),\ng(wt+\u03c4 ) = g(wt) +\u2207g(wt)(wt+\u03c4 \u2212wt) +O((wt+\u03c4 \u2212wt)2)In,\nwhere (wt+\u03c4 \u2212 wt)2 = (wt+\u03c4,1 \u2212 wt,1)\u03b11 \u00b7 \u00b7 \u00b7 (wt+\u03c4,n \u2212 wt,n) \u03b1n with \u2211n i=1 \u03b1i = 2 and \u03b1i \u2208 N and In is a ndimension vector with all the elements equal to 1. By comparing the above formula with Eqn. (3), we can find that ASGD actually uses the zero-order item in the Taylor expansion as its approximation to g(wt+\u03c4 ), and totally ignores all the other terms \u2207g(wt)(wt+\u03c4 \u2212 wt) +O((wt+\u03c4 \u2212 wt)2)In. This is exactly how the problem of delayed gradient comes into being.\nWith this insight, we can effectively compensate the delay by keeping higher-order items in the Taylor expansion. As a very first step, we investigate the simplest delay compensation, i.e., only keeping the zero-order and first-order items in the Taylor expansion, which is shown below.\ng(wt+\u03c4 ) \u2248 g(wt) +\u2207g(wt)(wt+\u03c4 \u2212 wt). (4)\nFor ease of reference, we call the corresponding algorithm delay-compensated ASGD, or DC-ASGD for short. Next we will show the advantage of DC-ASGD over traditional ASGD in terms of convergence condition for non-convex problems.\nAt first, we introduce some notations. Let L1 be upper bound of the Lipschitz coefficient of fi, L2 be the upper bound of smooth coefficient of fi, L3 is the upper bound of smooth coefficient of \u2207fi and \u03b3 = L32 supt \u2016wt+\u03c4 \u2212 wt\u20162. 3. Assume that both Var[g(wt)] and Var[g(wt) + \u2207g(wt+\u03c4 )(wt+\u03c4\u2212wt)] are less than V 2. Following the proof of ASGD Lian et al. (2015) and under the same assumptions, we can get the following theorem, which describe the convergence rate of DC-ASGD. Theorem 3.1 Set the learning rate\n\u03b7 = min { 1, \u03b32\nL22\n}\u221a (F (w1)\u2212 F (w\u2217))\nbTL2V 2 , (5)\nwhere b is the mini-batch size. If the delay \u03c4 is upper-bounded as belows,\nT \u2265 4b\u03b3 4(f(w1)\u2212 f(w\u2217)\nL32V 2\n(\u03c4 + 1)2, (6)\n3It is clear that L1, L2, L3 are the upper bounds of the first, second, and third order derivatives of each fi, i \u2208 [S].\nthen the output of DC-ASGD satisfies the following ergodic convergence rate\nmin t={1,\u00b7\u00b7\u00b7 ,T}\nE(\u2016\u2207F (wt)\u20162) \u2264 4V \u221a\n(F (w1)\u2212 F (w\u2217)L2 bT .\n(7) From the above theorem, we can find that DC-ASGD has similar convergence rate to ASGD, but under different conditions for convergence. In particular, the convergence condition for DC-ASGD is T \u2265 4b\u03b3\n4(f(w1)\u2212f(w\u2217) L32V 2 (\u03c4 + 1) 2.\nRecall that the convergence condition for ASGD is T \u2265 4bL2(f(w1)\u2212f(w0))\nV 2 (\u03c4 + 1) 2. If \u03b3 < L2, for fixed number of\niterations T , the upper bound of \u03c4 for DC-ASGD is larger than that for ASGD by a factor of O(L 2 2\n\u03b32 ), which means DCASGD is more tolerant to large delay, i.e., it can still converge even if a much larger delay \u03c4 exists (as compared to ASGD).\nThe condition \u03b3 < L2 is equivalent to supt \u2016wt+\u03c4\u2212wt\u2016 < 2L2 L3\n. Since \u2016wt+\u03c4 \u2212 wt\u2016 is a sum of stochastic delay compensation gradients, we can bound supt \u2016wt+\u03c4 \u2212 wt\u2016 by the upper bound of the delay compensation gradients which is related to L1 and L2. Finally, we can get a relationship of L1, L2 and L3 to gauranree that \u03b3 < L2. Based on the above intuitive explanation, we have the following corollary.\nCorollary 3.2 If L1 \u2264 L 2 2\nL3 , we have \u03b3 < L2. DC-ASGD improves the upper bound of delay \u03c4 by a factor O ( L22 \u03b32 ) as compared with ASGD. Due to the space limitation, we put the proof of Theorem 3.1 and Corollary 3.2 in the supplementary materials.\nThe above theorem and corollary shows the theoretical advantage of DC-ASGD, however, we still needs to face the practical challenges of it. This is DC-ASGD requires the computation of the first-order derivative of the gradient, which corresponds to the Hessian matrix of the original loss function (i.e., cross entropy for neural networks), which is defined as Hf(w) = [hij ]i,j=1,\u00b7\u00b7\u00b7 ,n where hij = \u2202\n2f \u2202wi\u2202wj\n(w). For a neural network model with millions of parameters (which is very common and may only be regarded as a medium-size network today), the corresponding Hessian matrix will contain trillions of elements. It is clearly very computationally and spatially expensive to obtain such a large matrix. Fortunately, as shown in the next subsection, we demonstrate that it is possible to find easy-to-compute/store approximators to the Hessian matrix, which make our proposal of delay compensation technically feasible."}, {"heading": "Approximation of Hessian matrix", "text": "Computing the exact Hessian matrix is too computationally and spatially expensive, especially for large models. Alternatively, we want to find some approximators that are theoretically very close to the Hessian matrix, but can be computed without introducing additional complexity (i.e., just using what we already have during the previous training process). To this end, in Section 3, we give a theorem which reveals the relationship between the first-order gradients and the second-order derivatives (which are the elements in Hessian\nmatrix) for the neural networks. Based on the theorem, we can construct an unbiased approximator to Hessian matrix purely based on the existing gradients. In Section 3, we further propose two more effective approximators in terms of the trade-off between bias and variance, which can once again be computed purely based on the existing gradients.\nApproximation of Hessian matrix in expectation The following theorem illustrates the relationship between the gradients and second-order derivatives of the loss function of the neural networks (i.e., cross entropy). Theorem 3.3 Assume that Y is a discrete random variable with P(Y = k|X = x,w) = \u03c3k(x; w) for k \u2208 {1, 2, ...,K} and f(x, y,w) = \u2212 \u2211K k=1(I[y=k] log \u03c3k(x; w)). Then we have\nE(Y |x,w) \u22022\n\u2202w2 f(x, Y,w) = E(Y |x,w)\n( \u2202\n\u2202w f(x, Y,w)\n) \u2297 ( \u2202\n\u2202w f(x, Y,w)\n) ,\n(8) where \"\u2297\" denotes the outer product for a vector.\nIn order to prove the above theorem, one needs to leverage the two equivalent methods for calculating Fisher Information matrix Friedman, Hastie, and Tibshirani (2001), and consider that f(x, y,w) takes the format of negative log-likelihood. Due to space restrictions, we leave the full proof to the supplementary materials.\nConsidering that g(x, y,w) = \u2202\u2202wf(x, y,w), we can immediately get that for fixed input x and parameter w, by taking conditional expectation for output Y , E(Y |x,w)Hf(x, Y,w) = E(Y |x,w)Gf(x, Y,w), where Gf(w) is the outer product matrix of the gradient vector g(w), i.e., Gf(w) = [gij ]i,j=1,\u00b7\u00b7\u00b7 ,n where gij = g(wi)g(wj).\nApproximators with lower mean square error In the previous subsection, we show that Gf(x, y,w) is an unbiased estimator of Hf(x, Y,w), however, in practice it may induce high approximation error due to potentially large variance. A practically better approximator should have better controlled trade-off between bias and variance. To this end, we use mean square error (MSE) to measure the quality of an approximator. In order to reduce variance, we consider the following two new approximators \u03bb1 \u221a |Gf(w)| \u2206= [ \u03bb1 \u221a |gij |\n] and \u03bb2Gf(w) \u2206 = [\u03bb2gij ], where \u03bb1 and \u03bb2 can be regarded as variance controlling parameters. Theorem 3.4 shows that with appropriately set \u03bb1 and \u03bb2,\n\u03bb1 \u221a |gij | and \u03bb2gij can lead to smaller MSE than gij , when approximating any element hij in the Hessian matrix. To simplify the notations, we denote the range of |\u2202\u03c3k\u2202wi | for all\n\u03c3k as [li, ui],i.e., \u2223\u2223\u2223 \u2202\u03c3k\u2202wi \u2223\u2223\u2223 \u2208 [li, ui], \u2200k \u2208 [K]. Let Cij = (uiujlilj )2 and C \u2032 ij =\nCij 1+\u03bb2 . Theorem 3.4 If sign(\u03bb1) = sign(gij), 0 \u2264 |\u03bb1| \u2264 2K \u221a lilj\nand \u2211K k=1\n1 \u03c33 k (x,w) \u2265 2Cij\n(\u2211K k=1 1 \u03c3k(x,w) )2 , the MSE of\n\u03bb1 \u221a |gij | is smaller than the MSE of gij . If \u03bb2 \u2208 [0, 1] and \u03c3k\nsatisfies \u2211K k=1\n1 \u03c33 k (x,w) \u2265 2C\n\u2032 ij (\u2211K k=1 1 \u03c3k(x,w) )2 , the MSE of\n\u03bb2gij is smaller than the MSE of gij . According to Theorem 3.4, we have the following discussions. First, the condition for \u03c3k(x,w) is more likely to be satisfied\nwhen \u03c3k(x,w),\u2203k \u2208 [K] is near 1. In this case, the magnitude of both sides of the inequality will be dominated by those \u03c3k(x,w) that are close to 0 (and therefore 1/\u03c3k(x,w) will be a very large quantity). Considering that the left side of the inequality is in the order of 1/\u03c33k(x,w) while the right side is in the order of 1/\u03c32k(x,w), we can see that the left side will be larger than the right side in this case. Please note that this is not a very strong condition, since if no \u03c3k(x,w) is large enough, the classification power of the corresponding neural network will be very weak and not useful in practice. We give Corollary 3.5 which provides simpler sufficient conditions for Theorem 3.4. Due to space limitation, we put it in the supplementary materials.\n4 DC-ASGD: Algorithm Description In Section 3, we have shown that \u03bb1 \u221a |Gf(w)| and \u03bb2Gf(w) are two approximators that are easy to compute from existing gradients and can achieve good trade-off between bias and variance of estimation. With them, we can approximate the Hessian matrix in a much more economical way.4 To further save computations, we adopt an additional trick widely used in the literature, i.e., only using the diagonal elements in the (approximate) Hessian matrix, which have shown promising results Becker and Le Cun (1988). Accordingly, the update rule for the global model becomes\nwt+\u03c4+1 = wt+\u03c4 \u2212 \u03b7 ( g(wt) + \u03bb1 |g(wt)| (wt+\u03c4 \u2212 wt) ) , (9)\nwt+\u03c4+1 = wt+\u03c4 \u2212 \u03b7 ( g(wt) + \u03bb2g(wt) g(wt) (wt+\u03c4 \u2212 wt) ) , (10)\nwhere denotes element-wise multiplication, g(wt) denotes stochastic gradient of f(wt) calculated by a worker, wt and wt+\u03c4 denote the global model at time t and time t+ \u03c4 when worker m starts to calculate its gradient and starts to add its gradient to the global model respectively.\nAlgorithm 1: DC-ASGD: worker m"}, {"heading": "1 repeat", "text": "2 Pull wt from the parameter server. 3 Compute gradient gm = \u2207fm(wt). 4 Push gm to the parameter server. 5 until forever;\nCorresponding to the deduction above, our DC-ASGD is shown in Algorithms 1 and 2 (in which we use Option-I and Option-II to refer to Eqn. (9) and (10) respectively). Here we assume that DC-ASGD is implemented by the parameter server framework (although it can also be implemented in other frameworks). According to Algorithm 1, local worker m pulls the latest global model wt from the parameter server, computes its gradient gm and sends it back to the server. In the meanwhile, according to Algorithm 2, the parameter server will store a backup model wbak(m) when worker m pulls wt. When the delayed gradient gm calculated by worker m is received at time t, parameter server applies the update according to Option-I or Option-II. DC-ASGD has no extra\n4Although the analysis in Section 3 is element-wise, it is not difficult to choose a common \u03bb1 (or \u03bb2) for all the elements in the approximator matrix.\nAlgorithm 2: DC-ASGD: parameter server Input : learning rate \u03b7, variance controlling\nparameters \u03bb1 or \u03bb2. Initialize : t = 0, w0 is initialized randomly,\nwbak(m) = w0, m \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,M} 1 repeat 2 if receive \u201cgm\" then\nOption-I :wt+1 \u2190 wt \u2212 \u03b7 \u00b7 ( gm + \u03bb1|gm|\n(wt \u2212 wbak(m)) )\nOption-II :wt+1 \u2190 wt \u2212 \u03b7 \u00b7 ( gm + \u03bb2gm\ngm (wt \u2212 wbak(m)) )\n3 t\u2190 t+ 1 4 5 else if receive \u201cpull request\u201d then 6 wbak(m)\u2190 wt 7 Send wt back to worker m. 8\n9 until forever;\ncomputational requirement for the worker and very little for the parameter server as compared to ASGD. As for the space requirement, for each worker m \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,M}, the parameter server only needs to store a backup model wbak(m). This is not a critical issue due to the following reason. First, the parameter server is usually implemented in a distributed manner, so its usable memory is much beyond the capacity of a single machine. Second, we can also take an alternative approach and ask each worker to push both wt and its gradient gm to the parameter server. In this way, the communication cost is doubled but we do not need M times larger memory at the server side."}, {"heading": "5 Experiments", "text": "In this section, we evaluate our proposed algorithms. We used two datasets: CIFAR-10 Krizhevsky and Hinton (2009), and ImageNet ILSVRC 2013 Russakovsky et al. (2015). The experiments were conducted on a GPU cluster interconnected with InfiniBand. Each node has four K40 Tesla GPU processors. We treat each GPU as a separate local worker. For the DNN algorithm running on each worker, we chose ResNet He et al. (2016) since it produces the state-of-the-art accuracy in many image related tasks and its implementation is available through open-source projects.5 For the parallelization of ResNet across machines, we leveraged an open-source parameter server.6 We implemented DC-ASGD (with both approximators) on this experimental platform. In addition, we also implemented ASGD and SSGD, which have been used in many previous works as baselines Chen et al. (2016); Dean et al. (2012). Furthermore, for the experiments on CIFAR-10, we used the sequential SGD algorithm as a reference model to examine the accuracy of parallel algorithms. However, for\n5https://github.com/KaimingHe/ deep-residual-networks\n6Microsoft Distributed Machine Learning Toolkit: http:// www.dmtk.io.\nthe experiments on ImageNet, we were not able to show this reference because it simply took too long time for a single machine to finish the training.7 For sake of fairness, all experiments started from the same random initialized model, and used the same strategy for learning rate scheduling."}, {"heading": "Experimental Results on CIFAR-10", "text": "The CIFAR-10 dataset consists of a training set of 50k images and a test set of 10k images in 10 classes. We trained a 20-layer ResNet model on this dataset (without data augmentation). For all the algorithms under investigation, we performed training for 160 iterations, with a mini-batch size of 128, and an initial learning rate of \u03b7 = 0.5 which were reduced by ten times after 80 and 120 iterations following the practice in He et al. (2016). For DC-ASGD, we need to set parameters \u03bb1 and \u03bb2. Actually, Theorem 3.4 provide feasible ranges for these two parameters. By some simple calculations and empirical observations, we found the ranges are relatively wide and we set \u03bb1 = 2 and \u03bb2 = 0.04 initially in our experiments, and increased by ten times when learning rate changes.\nWe tried different number of local works in our experiments: M = {1, 4, 8}. First we explore convergence curves with fixed number of effective passes and the corresponding results are given in Figure 2. From the figure, we have the following observations: (1) Sequential SGD achieves the best training accuracy, and its final test error is 8.75%. (2) test errors of ASGD and SSGD will increase with respect to the number of local works. In particular, when M = 4, ASGD and SSGD achieved test errors of 9.39% and 9.35% respectively; and when M = 8, their test errors become 10.4% and 10.1% respectively. These results are reasonable: ASGD suffers from the problem of delayed gradients which becomes more serious for larger number of workers; SSGD will increase the effective mini-batch size byM times, and enlarged mini-batch size usually affect the training performances of DNN. (3) For DC-ASGD, no matter which approximator is used (more accurately, \u03bb1|g| seems to be slightly better than\n7We also implemented the momentum variants of these algorithms. The corresponding comparisons are very similar to those without momentum.\n\u03bb2g g), its performance is significantly better than ASGD and SSGD, and almost catches up with sequential SGD. Especially for M = 4, the test error of DC-ASGD reached 8.69%, which is indistinguishable from that achieved by sequential SGD. Even for M = 8, DC-ASGD can reduce the test error to 9.27%, which is almost 1% better than ASGD and SSGD.\nWe further compared the convergence speed of different algorithms, the corresponding results are shown in Figure 3. From this figure, we have the following observations: (1) Although the convergent point is not very good, ASGD was indeed very fast, and achieved almost linear speed-up as compared to sequential SGD. (2) SSGD was also faster than sequential SGD. However, due to the cost of synchronization, it was significantly slower than ASGD. (3) DC-ASGD achieved very good balance between accuracy and speed. On one hand, its converge speed is very similar to that of ASGD (although it involves a little more computational cost and some memory cost when compensating the delay). On the other hand, its convergent point is as good as that of sequential SGD."}, {"heading": "Experimental Results on ImageNet", "text": "The ImageNet dataset is much larger, which contains 1.28 million training images and 50k validation images in 1000 categories. We trained a 50-layer ResNet model He et al. (2016) on this dataset. According to the previous subsection, for DC-ASGD, \u03bb1|g| seems to be a better approximator than \u03bb2g g. Therefore in this large-scale experiment, we only implemented \u03bb1|g| in DC-ASGD. For all algorithms in this experiment, we performed training for 120 iterations, with a mini-batch size of 32, and an initial learning rate of \u03b7 = 0.1 (reduced by ten times after every 30 iterations). We chose \u03bb1 = 5 initially (according to our theorem, a larger \u03bb1 is preferred for classification tasks with more categories), and increased it by ten times after every 30 iterations. Since the training on the ImageNet dataset is very time consuming, we employed M = 16 GPU nodes in our experiments. The top-1 accuracies based on 1-crop testing of different algorithms are given in Figure 4.\nAccording to the figure, we have the following observations: (1) After processing the same amount of training data, DC-ASGD always outperformed SSGD and ASGD. While\nthe eventual test error achieved by ASGD and SSGD were 25.7% and 25.3% respectively, DC-ASGD achieved an error rate of 25.2%. Please note this time the accuracy of SSGD is quite good (which is consistent with a separate observation in Chen et al. (2016)). An explanation is that the training on ImageNet is less sensitive to the mini-batch size than that on CIFAR-10. (2) If we look at the learning curve with respect to wallclock time, SSGD was slowed down due the synchronization barrier; ASGD and DC-ASGD have similar efficiency, once again indicating that the extra overhead for delay compensation introduced by DC-ASGD can almost be neglected in practice. Based on all our experiments, we can clearly see that DC-ASGD algorithm have outstanding performance in terms of both classification accuracy and convergence speed, which in return verifies the soundness of our proposed delay compensation technologies."}, {"heading": "Experimental Results on the influence of \u03bb", "text": "In this section, we show how the parameter \u03bb affect our DC-ASGD algorithm. In Theorem 3.4 we provide feasible reasonable range for \u03bb, beyond this range, too large value of this parameter will lead to a wrong gradient direction and too small will make the compensation influence nearly disappear. We compare the performance of respectively sequential SGD, ASGD and DC-ASGD with different value of \u03bb18. The results are given in Figure 5. A proper \u03bb1 will lead to significant better accuracy. As \u03bb1 decreasing, DC-ASGD will gradually degrade to ASGD.\n6 Conclusion In this paper, we have given a theoretical analysis on the problem of delayed gradient in the asynchronous parallelization of stochastic gradient descent (SGD) algorithms, and proposed a novel algorithm called Delay Compensated asynchronous SGD (DC-ASGD) to tackle the problem. We have evaluated DC-ASGD on CIFAR-10 and ImageNet datasets, and the results demonstrate that it can achieve better accuracy than both synchronous SGD and asynchronous SGD, and\n8We also compare different \u03bb2 and the results are very similar to \u03bb1.\nnearly approaches the performance of sequential SGD. As for the future work, we will test DC-ASGD algorithm on larger computer clusters to see its performance in more challenging settings. Actually when the number of local workers increases, the delay will be more severe. We will investigate the economical approximation of higher-order items in the Taylor expansion to further help compensate the delay."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["A. Agarwal", "J.C. Duchi"], "venue": "NIPS.", "citeRegEx": "Agarwal and Duchi,? 2011", "shortCiteRegEx": "Agarwal and Duchi", "year": 2011}, {"title": "Revisiting asynchronous linear solvers: Provable convergence rate through randomization", "author": ["H. Avron", "A. Druinsky", "A. Gupta"], "venue": "JACM.", "citeRegEx": "Avron et al\\.,? 2015", "shortCiteRegEx": "Avron et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D Bahdanau"], "venue": "In ICLR", "citeRegEx": "Bahdanau,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "NIPS.", "citeRegEx": "Bahdanau et al\\.,? 2013", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2013}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["S. Becker", "Y. Le Cun"], "venue": "Tech.Rep.", "citeRegEx": "Becker and Cun,? 1988", "shortCiteRegEx": "Becker and Cun", "year": 1988}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade. Springer.", "citeRegEx": "Bottou,? 2012", "shortCiteRegEx": "Bottou", "year": 2012}, {"title": "Scalable training of deep learning machines by incremental block training with intrablock parallel optimization and blockwise model-update filtering", "author": ["K. Chen", "Q. Huo"], "venue": "ICASSP.", "citeRegEx": "Chen and Huo,? 2016", "shortCiteRegEx": "Chen and Huo", "year": 2016}, {"title": "Revisiting distributed synchronous sgd", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "ICLR.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Higher-order derivatives and taylor\u2019s formula in several variables", "author": ["G. Folland"], "venue": null, "citeRegEx": "Folland,? \\Q2005\\E", "shortCiteRegEx": "Folland", "year": 2005}, {"title": "The elements of statistical learning", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Springer series in statistics Springer, Berlin.", "citeRegEx": "Friedman et al\\.,? 2001", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR.", "citeRegEx": "Kingma and Ba,? 2015", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["X. Lian", "Y. Huang", "Y. Li", "J. Liu"], "venue": "NIPS.", "citeRegEx": "Lian et al\\.,? 2015", "shortCiteRegEx": "Lian et al\\.", "year": 2015}, {"title": "Delay-tolerant algorithms for asynchronous distributed online learning", "author": ["B. McMahan", "M. Streeter"], "venue": "NIPS.", "citeRegEx": "McMahan and Streeter,? 2014", "shortCiteRegEx": "McMahan and Streeter", "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "NIPS.", "citeRegEx": "Recht et al\\.,? 2011", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Long shortterm memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "INTERSPEECH.", "citeRegEx": "Sak et al\\.,? 2014", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "ICASSP.", "citeRegEx": "Sercu et al\\.,? 2016", "shortCiteRegEx": "Sercu et al\\.", "year": 2016}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "arXiv preprint arXiv:1602.07261.", "citeRegEx": "Szegedy et al\\.,? 2016", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Deep learning with elastic averaging sgd", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "NIPS.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "1 Introduction Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition Sak, Senior, and Beaufays (2014); Sercu et al. (2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al.", "startOffset": 151, "endOffset": 171}, {"referenceID": 12, "context": "1 Introduction Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition Sak, Senior, and Beaufays (2014); Sercu et al. (2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al.", "startOffset": 151, "endOffset": 229}, {"referenceID": 6, "context": "(2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al. (2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013).", "startOffset": 66, "endOffset": 83}, {"referenceID": 6, "context": "(2016), computer vision Krizhevsky, Sutskever, and Hinton (2012); He et al. (2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013).", "startOffset": 66, "endOffset": 121}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013).", "startOffset": 78, "endOffset": 105}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al.", "startOffset": 78, "endOffset": 139}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al.", "startOffset": 78, "endOffset": 347}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al.", "startOffset": 78, "endOffset": 385}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al.", "startOffset": 78, "endOffset": 405}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al.", "startOffset": 78, "endOffset": 515}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al.", "startOffset": 78, "endOffset": 537}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al. (2012). As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way.", "startOffset": 78, "endOffset": 557}, {"referenceID": 2, "context": "(2016); Szegedy, Ioffe, and Vanhoucke (2016), and natural language processing Bahdanau and others (2015); Bahdanau, Cho, and Bengio (2013). Part of the success of DNN should be attributed to the availability of big data and powerful computational resources, which allows people to train very deep and big DNN models in parallelChen and Huo (2016); Zhang, Choromanska, and LeCun (2015); Chen et al. (2016). Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks Bottou (2012); Kingma and Ba (2015); Dean et al. (2012). As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way. In synchronous SGD (SSGD) Chen et al. (2016), local workers compute the gradients over a mini-batch of their own data, and add these gradients to the", "startOffset": 78, "endOffset": 770}, {"referenceID": 14, "context": "This problem has been well known, and some researchers have analyzed its negative effect to the convergence speed Lian et al. (2015); Avron, Druinsky, and Gupta (2015).", "startOffset": 114, "endOffset": 133}, {"referenceID": 14, "context": "This problem has been well known, and some researchers have analyzed its negative effect to the convergence speed Lian et al. (2015); Avron, Druinsky, and Gupta (2015). In this paper, we want to tackle the challenges of delayed gradient so as to make ASGD both efficient and more mathematically sound.", "startOffset": 114, "endOffset": 168}, {"referenceID": 7, "context": "Recently, people proposed to partially resolve this problem by using additional backup workers Chen et al. (2016). However, this technique requires additional computation resource, and works on the assumption that the majority of workers train almost equally fast.", "startOffset": 95, "endOffset": 114}, {"referenceID": 13, "context": "Actually, this problem of delayed gradient has been well known Lian et al. (2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al.", "startOffset": 63, "endOffset": 82}, {"referenceID": 13, "context": "Actually, this problem of delayed gradient has been well known Lian et al. (2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al.", "startOffset": 63, "endOffset": 117}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al.", "startOffset": 43, "endOffset": 68}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large.", "startOffset": 43, "endOffset": 89}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al.", "startOffset": 43, "endOffset": 479}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al.", "startOffset": 43, "endOffset": 508}, {"referenceID": 0, "context": "(2015); Avron, Druinsky, and Gupta (2015); Agarwal and Duchi (2011); Recht et al. (2011), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach the same accuracy as that obtained by sequential SGD, especially when the number of workers becomes large. Researchers have also tried to improve ASGD from different perspectives Zhang, Choromanska, and LeCun (2015); McMahan and Streeter (2014); Ho et al. (2013), however, to the best of our knowledge, there is no sound solution to compensate the delayed gradient while keeping the high efficiency of ASGD yet.", "startOffset": 43, "endOffset": 526}, {"referenceID": 8, "context": "Gradient decomposition using Taylor expansion The Taylor expansion of g(wt+\u03c4 ) at wt can be written as follows Folland (2005),", "startOffset": 111, "endOffset": 126}, {"referenceID": 14, "context": "Following the proof of ASGD Lian et al. (2015) and under the same assumptions, we can get the following theorem, which describe the convergence rate of DC-ASGD.", "startOffset": 28, "endOffset": 47}, {"referenceID": 10, "context": "We used two datasets: CIFAR-10 Krizhevsky and Hinton (2009), and ImageNet ILSVRC 2013 Russakovsky et al.", "startOffset": 31, "endOffset": 60}, {"referenceID": 10, "context": "We used two datasets: CIFAR-10 Krizhevsky and Hinton (2009), and ImageNet ILSVRC 2013 Russakovsky et al. (2015). The experiments were conducted on a GPU cluster interconnected with InfiniBand.", "startOffset": 31, "endOffset": 112}, {"referenceID": 9, "context": "For the DNN algorithm running on each worker, we chose ResNet He et al. (2016) since it produces the state-of-the-art accuracy in many image related tasks and its implementation is available through open-source projects.", "startOffset": 62, "endOffset": 79}, {"referenceID": 7, "context": "In addition, we also implemented ASGD and SSGD, which have been used in many previous works as baselines Chen et al. (2016); Dean et al.", "startOffset": 105, "endOffset": 124}, {"referenceID": 7, "context": "In addition, we also implemented ASGD and SSGD, which have been used in many previous works as baselines Chen et al. (2016); Dean et al. (2012). Furthermore, for the experiments on CIFAR-10, we used the sequential SGD algorithm as a reference model to examine the accuracy of parallel algorithms.", "startOffset": 105, "endOffset": 144}, {"referenceID": 10, "context": "5 which were reduced by ten times after 80 and 120 iterations following the practice in He et al. (2016). For DC-ASGD, we need to set parameters \u03bb1 and \u03bb2.", "startOffset": 88, "endOffset": 105}, {"referenceID": 10, "context": "We trained a 50-layer ResNet model He et al. (2016) on this dataset.", "startOffset": 35, "endOffset": 52}, {"referenceID": 7, "context": "Please note this time the accuracy of SSGD is quite good (which is consistent with a separate observation in Chen et al. (2016)).", "startOffset": 109, "endOffset": 128}], "year": 2016, "abstractText": "With the fast development of deep learning, people have started to train very big neural networks using massive data. Asynchronous Stochastic Gradient Descent (ASGD) is widely used to fulfill this task, which, however, is known to suffer from the problem of delayed gradient. That is, when a local worker adds the gradient it calculates to the global model, the global model may have been updated by other workers and this gradient becomes \u201cdelayed\u201d. We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is done by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the corresponding new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and experimental results demonstrate that DC-ASGD can outperform both synchronous SGD and ASGD, and nearly approaches the performance of sequential SGD.", "creator": "LaTeX with hyperref package"}}}