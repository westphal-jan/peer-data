{"id": "1609.08359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "emoji2vec: Learning Emoji Representations from their Description", "abstract": "many current natural language processing applications for social media rely considerably on tensor representation learning and utilize pre - trained word embeddings. there currently exist several publicly - available, pre - trained sets of word embeddings, but they occasionally contain few or no emoji representations even as emoji usage in social media has increased. in this paper framework we release emoji2vec, pre - trained embeddings for all unicode - emoji which are learned from their description in the unicode emoji standard.. the resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. we to demonstrate, for the downstream task description of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip - gram model trained on a large database collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to firmly estimate a representation.", "histories": [["v1", "Tue, 27 Sep 2016 11:32:25 GMT  (2655kb,D)", "http://arxiv.org/abs/1609.08359v1", "7 pages, 4 figures, 1 table, In Proceedings of the 4th International Workshop on Natural Language Processing for Social Media at EMNLP 2016 (SocialNLP at EMNLP 2016)"], ["v2", "Sun, 20 Nov 2016 22:43:46 GMT  (2655kb,D)", "http://arxiv.org/abs/1609.08359v2", "7 pages, 4 figures, 1 table, In Proceedings of the 4th International Workshop on Natural Language Processing for Social Media at EMNLP 2016 (SocialNLP at EMNLP 2016)"]], "COMMENTS": "7 pages, 4 figures, 1 table, In Proceedings of the 4th International Workshop on Natural Language Processing for Social Media at EMNLP 2016 (SocialNLP at EMNLP 2016)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ben eisner", "tim rockt\\\"aschel", "isabelle augenstein", "matko bo\\v{s}njak", "sebastian riedel"], "accepted": false, "id": "1609.08359"}, "pdf": {"name": "1609.08359.pdf", "metadata": {"source": "CRF", "title": "emoji2vec: Learning Emoji Representations from their Description", "authors": ["Ben Eisner", "Tim Rockt\u00e4schel"], "emails": ["beisner@princeton.edu", "t.rocktaschel@ucl.ac.uk", "i.augenstein@ucl.ac.uk", "m.bosnjak@ucl.ac.uk", "s.riedel@ucl.ac.uk"], "sections": [{"heading": null, "text": "Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard.1 The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation."}, {"heading": "1 Introduction", "text": "First introduced in 1997, emoji, a standardized set of small pictorial glyphs depicting everything from smiling faces to international flags, have seen a drastic increase in usage in social media over the last decade. The Oxford Dictionary named 2015 the\n1http://www.unicode.org/emoji/charts/ full-emoji-list.html\nyear of the emoji, citing an increase in usage of over 800% during the course of the year, and elected the \u2018Face with Tears of Joy\u2019 emoji ( ) as the Word of the Year. As of this writing, over 10% of Twitter posts and over 50% of text on Instagram contain one or more emoji (Cruse, 2015).2 Due to their popularity and broad usage, they have been the subject of much formal and informal research in language and social communication, as well as in natural language processing (NLP).\nIn the context of social sciences, research has focused on emoji usage as a means of expressing emotions on mobile platforms. Interestingly, Kelly and Watts (2015) found that although essentially thought of as means of expressing emotions, emoji have been adopted as tools to express relationally useful roles in conversation. (Lebduska, 2014) showed that emoji are culturally and contextually bound, and are open to reinterpretation and misinterpretation, a result confirmed by (Miller et al., 2016). These findings have paved the way for many formal analyses of semantic characteristics of emoji.\nConcurrently we observe an increased interest in natural language processing on social media data (Ritter et al., 2011; Gattani et al., 2013; Rosenthal et al., 2015). Many current NLP systems applied to social media rely on representation learning and word embeddings (Tang et al., 2014; Dong et al., 2014; Dhingra et al., 2016; Augenstein et al.,\n2See https://twitter.com/Kyle_MacLachlan/ status/765390472604971009 for an extreme example.\nar X\niv :1\n60 9.\n08 35\n9v 1\n[ cs\n.C L\n] 2\n7 Se\n2016). Such systems often rely on pre-trained word embeddings that can for instance be obtained from word2vec (Mikolov et al., 2013a) or GloVe (Pennington et al., 2014). Yet, neither resource contain a complete set of Unicode emoji representations, which suggests that many social NLP applications could be improved by the addition of robust emoji representations.\nIn this paper we release emoji2vec, embeddings for emoji Unicode symbols learned from their description in the Unicode emoji standard. We demonstrate the usefulness of emoji representations trained in this way by evaluating on a Twitter sentiment analysis task. Furthermore, we provide a qualitative analysis by investigating emoji analogy examples and visualizing the emoji embedding space."}, {"heading": "2 Related Work", "text": "There has been little work in distributional embeddings of emoji. The first research done in this direction was an informal blog post by the Instagram Data Team in 2015 (Dimson, 2015). They generated vector embeddings for emoji similar to skip-gram-based vectors by training on the entire corpus of Instagram posts. Their research gave valuable insight into the usage of emoji on Instagram, and showed that distributed representations can help understanding emoji semantics in everyday usage. The second contribution, closest to ours, was introduced by (Barbieri et al., 2016). They trained emoji embeddings from a large Twitter dataset of over 100 million English tweets using the skip-gram method (Mikolov et al., 2013a). These pre-trained emoji representations led to increased accuracy on a similarity task, and a meaningful clustering of the emoji embedding space. While this method is able to learn robust representations for frequently-used emoji, representations of less frequent emoji are estimated rather poorly or not available at all. In fact, only around 700 emoji can be found in Barbieri et al. (2016)\u2019s corpus, while there is support of over 1600 emoji in the Unicode standard.\nOur approach differs in two important aspects. First, since we are estimating the representation of emoji directly from their description, we obtain robust representations for all supported emoji symbols \u2014 even the long tail of infrequently used ones. Sec-\nondly, our method works with much less data. Instead of training on millions of tweets, our representations are trained on only a few thousand descriptions. Still, we obtain higher accuracy results on a Twitter sentiment analysis task.\nIn addition, our work relates to the work of Hill et al. (2016) who built word representations for words and concepts based on their description in a dictionary. Similarly to their approach, we build representations for emoji based on their descriptions and keyword phrases.\nSome of the limitations of our work are evident in the work of Park et al. (2013) who showed that different cultural phenomena and languages may coopt conventional emoji sentiment. Since we train only on English-language definitions and ignore temporal definitions of emoji, our training method might not capture the full semantic characteristics of an emoji."}, {"heading": "3 Method", "text": "Our method maps emoji symbols into the same space as the 300-dimensional Google News word2vec embeddings. Thus, the resulting emoji2vec embeddings can be used in addition to 300-dimensional word2vec embeddings in any application. To this end we crawl emoji, their name and their keyword phrases from the Unicode emoji list, resulting in 6088 descriptions of 1661 emoji symbols. Figure 1 shows an example for an uncommon emoji."}, {"heading": "3.1 Model", "text": "We train emoji embeddings using a simple method. For every training example consisting of an emoji and a sequence of wordsw1, . . . , wN describing that emoji, we take the sum of the individual word vectors in the descriptive phrase as found in the Google News word2vec embeddings\nvj = N\u2211 k=1 wk\nwhere wk is the word2vec vector for word wk if that vector exists (otherwise we drop the summand) and vj is the vector representation of the description. We define a trainable vector xi for every emoji in our training set, and model the probability of a match between the emoji representation xi and its description representation vj using the sigmoid of the dot product of the two representations \u03c3(xTi vj). For training we use the logistic loss\nL(i, j, yij) = \u2212 log(\u03c3(yijxTi vj \u2212 (1\u2212 yij)xTi vj))\nwhere yij is 1 if description j is valid for emoji i and 0 otherwise."}, {"heading": "3.2 Optimization", "text": "Our model is implemented in TensorFlow (Abadi et al., 2015) and optimized using stochastic gradient descent with Adam (Kingma and Ba, 2015) as optimizer. As we do not observe any negative training examples (invalid descriptions of emoji do not appear in the original training set), to increase generalization performance we randomly sample descriptions for emoji as negative instances (i.e. induce a mismatched description). One of the parameters of our model is the ratio of negative samples to positive samples; we found that having one positive example per negative example produced the best results. We perform early-stopping on a held-out development set and found 80 epochs of training to give the best results. As we are only training on emoji descriptions and our method is simple and cheap, training takes less than 3 minutes on a 2013 MacBook Pro."}, {"heading": "4 Evaluation", "text": "We quantitatively evaluate our approach on an intrinsic (emoji-description classification) and extrinsic (Twitter sentiment analysis) task. Furthermore, we give a qualitative analysis by visualizing the learned emoji embedding space and investigating emoji analogy examples."}, {"heading": "4.1 Emoji-Description Classification", "text": "To analyze how well our method models the distribution of correct emoji descriptions, we created a manually-labeled test set containing pairs of emoji and phrases, as well as a correspondence label. For instance, our test set includes the example: { ,\n\u201dcrying\u201d, True}, as well as the example { , \u201dfish\u201d, False}. We calculate \u03c3(xTi vi) for each example in the test set, measuring the similarity between the emoji vector and the sum of word vectors in the phrase.\nWhen a classifier thresholds the above prediction at 0.5 to determine a positive or negative correlation, we obtain an accuracy of 85.5% for classifying whether an emoji-description pair is valid or not. By varying the threshold used for this classifier, we obtain a receiver operating characteristic curve (Figure 4.1) with an area-under-the-curve of 0.933, which demonstrates that high quality of the learned emoji representations."}, {"heading": "4.2 Sentiment Analysis on Tweets", "text": "As downstream task we compare the accuracy of sentiment classification of tweets for various classifiers with three different sets of pre-trained word embeddings: (1) the original Google News word2vec embeddings, (2) word2vec augmented with emoji embeddings trained by Barbieri et al. (2016), and (3) word2vec augmented with emoji2vec trained from Unicode descriptions. We use the recent dataset by Kralj Novak et al. (2015), which consists of over 67k English tweets labelled manually for positive, neutral, or negative sentiment. In both the training set and the test set, 46% of tweets are labeled neutral, 29% are labeled positive, and 25% are labeled negative. To compute the feature vectors for training, we summed the vectors corresponding to each word or emoji in the text of the Tweet. The goal\nof this simple sentiment analysis model is not to produce state-of-the-art results in sentiment analysis; it is simply to show that including emoji adds discriminating information to a model, which could potentially be exploited in more advanced social NLP systems.\nBecause the labels are rather evenly distributed, accuracy is an effective metric in determining performance on this classification task. Results are reported in Table 1. We find that augmenting word2vec with emoji embeddings improves overall classification accuracy on the full corpus, and substantially improves classification performance for tweets that contain emoji. It suggests that emoji embeddings could improve performance for other social NLP tasks as well. Furthermore, we find that emoji2vec generally outperforms the emoji embeddings trained by Barbieri et al. (2016), despite being trained on much less data using a simple model."}, {"heading": "4.3 t-SNE Visualization", "text": "To gain further insights, we project the learned emoji embeddings into two-dimensional space using t-SNE (Maaten and Hinton, 2008). This method projects high-dimensional embeddings into a lowerdimensional space while attempting to preserve relative distances. We perform this projection of emoji representation into two-dimensional space.\nFrom Figure 4.3 we see a number of notable semantic clusters, indicating that the vectors we trained have accurately captured some of the semantic properties of the emoji. For instance, all flag symbols are clustered in the bottom, and many smiley faces in the center. Other prominent emoji clusters include fruits, astrological signs, animals, vehicles, or families. On the other hand, symbolic representations of numbers are not properly disentangled in the embedding space, indicating limitations of our simple model. A two-dimensional projection is convenient from a visualization perspective, and certainly shows that some intuitively similar emoji are close to each other in vector space."}, {"heading": "4.4 Analogy Task", "text": "A well-known property of word2vec is that embeddings trained with this method to some extent capture meaningful linear relationships between\nwords directly in the vector space. For instance, it holds that the vector representation of \u2019king\u2019 minus \u2019man\u2019 plus \u2019woman\u2019 is closest to \u2019queen\u2019 (Mikolov et al., 2013b). Word embeddings have commonly been evaluated on such word analogy tasks (Levy and Goldberg, 2014). Unfortunately, it is difficult to build such an analogy task for emoji due to the small number and semantically distinct categories of emoji. Nevertheless, we collected a few intuitive examples in Figure 4. For every query we have retrieved the closest five emoji. Though the correct answer is sometimes not the top one, it is often contained in the top three."}, {"heading": "5 Conclusion", "text": "Since existing pre-trained word embeddings such as Google News word2vec embeddings or GloVe fail to provide emoji embeddings, we have released emoji2vec \u2014 embeddings of 1661 emoji symbols. Instead of running word2vec\u2019s skip-gram model on a large collection of emoji and their contexts appearing in tweets, emoji2vec is directly trained on Unicode descriptions of emoji. The resulting emoji embeddings can be used to augment any downstream task that currently uses word2vec embeddings, and might prove especially useful in social NLP tasks where emoji are used frequently (e.g. Twitter, Instagram, etc.). Despite the fact that our model is simpler and trained on much less data, we outperform (Barbieri et al., 2016) on the task of Twitter sentiment analysis.\nAs our approach directly works on Unicode descriptions, it is not restricted to emoji symbols. In the future we want to investigate the usefulness of our method for other Unicode symbol embeddings. Furthermore, we plan to improve emoji2vec in the future by also reading full text emoji description\nfrom Emojipedia3 and using a recurrent neural network instead of a bag-of-word-vectors approach for enocoding descriptions. In addition, since our approach does not capture the context-dependent definitions of emoji (such as sarcasm, or appropriation via other cultural phenomena), we would like to explore mechanisms of efficiently capturing these nuanced meanings.\nData Release and Reproducibility\nPre-trained emoji2vec embeddings as well as the training data and code are released at https: //github.com/uclmr/emoji2vec. Note that the emoji2vec format is compatible with word2vec and can be loaded into gensim4 or similar libraries."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2015}, {"title": "Stance Detection with Bidirectional Conditional Encoding", "author": ["Isabelle Augenstein", "Tim Rockt\u00e4schel", "Andreas Vlachos", "Kalina Bontcheva."], "venue": "Proceedings of EMLNP.", "citeRegEx": "Augenstein et al\\.,? 2016", "shortCiteRegEx": "Augenstein et al\\.", "year": 2016}, {"title": "What does this Emoji Mean? A Vector Space Skip-Gram Model for Twitter Emojis", "author": ["Francesco Barbieri", "Francesco Ronzano", "Horacio Saggion."], "venue": "Proceedings of LREC, May.", "citeRegEx": "Barbieri et al\\.,? 2016", "shortCiteRegEx": "Barbieri et al\\.", "year": 2016}, {"title": "Emoji usage in TV conversation", "author": ["Joe Cruse"], "venue": null, "citeRegEx": "Cruse.,? \\Q2015\\E", "shortCiteRegEx": "Cruse.", "year": 2015}, {"title": "Tweet2Vec: Character-Based Distributed Representations for Social Media", "author": ["Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William Cohen."], "venue": "Proceedings of ACL, pages 269\u2013274.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Machine Learning for Emoji Trends", "author": ["Thomas Dimson."], "venue": "http://instagram-", "citeRegEx": "Dimson.,? 2015", "shortCiteRegEx": "Dimson.", "year": 2015}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."], "venue": "Journal of machine learning research, 9(Aug):1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Entity Extraction, Linking, Classification, and Tagging for Social Media: A", "author": ["Abhishek Gattani", "Digvijay S Lamba", "Nikesh Garera", "Mitul Tiwari", "Xiaoyong Chai", "Sanjib Das", "Sri Subramaniam", "Anand Rajaraman", "Venky Harinarayan", "AnHai Doan"], "venue": null, "citeRegEx": "Gattani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gattani et al\\.", "year": 2013}, {"title": "Learning to Understand Phrases by Embedding the Dictionary", "author": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen", "Yoshua Bengio."], "venue": "TACL.", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Random decision forests", "author": ["Tin Kam Ho."], "venue": "Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on, volume 1, pages 278\u2013282. IEEE.", "citeRegEx": "Ho.,? 1995", "shortCiteRegEx": "Ho.", "year": 1995}, {"title": "Characterising the inventive appropriation of emoji as relationally meaningful in mediated close personal relationships", "author": ["Ryan Kelly", "Leon Watts."], "venue": "Experiences of Technology Appropriation: Unanticipated Users, Usage, Circumstances, and Design.", "citeRegEx": "Kelly and Watts.,? 2015", "shortCiteRegEx": "Kelly and Watts.", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "Proceedings of ICLR.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Sentiment of Emojis", "author": ["Petra Kralj Novak", "Jasmina Smailovi\u0107", "Borut Sluban", "Igor Mozeti\u010d."], "venue": "PLoS ONE, 10(12):1\u201322, 12.", "citeRegEx": "Novak et al\\.,? 2015", "shortCiteRegEx": "Novak et al\\.", "year": 2015}, {"title": "Emoji, Emoji, What for Art Thou? Harlot: A Revealing Look at the Arts of Persuasion", "author": ["Lisa Lebduska"], "venue": null, "citeRegEx": "Lebduska.,? \\Q2014\\E", "shortCiteRegEx": "Lebduska.", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of ConLL, pages 171\u2013180.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9(Nov):2579\u20132605.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Proceedings of NIPS, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of NAACLHLT, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Blissfully happy\u201d or \u201cready to fight\u201d: Varying Interpretations of Emoji", "author": ["Hannah Miller", "Jacob Thebault-Spieker", "Shuo Chang", "Isaac Johnson", "Loren Terveen", "Brent Hecht."], "venue": "Proceedings of ICWSM.", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Emoticon style: Interpreting differences in emoticons across cultures", "author": ["Jaram Park", "Vladimir Barash", "Clay Fink", "Meeyoung Cha."], "venue": "ICWSM.", "citeRegEx": "Park et al\\.,? 2013", "shortCiteRegEx": "Park et al\\.", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP, pages 1532\u2013 1543, October.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Named Entity Recognition in Tweets: An Experimental Study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni."], "venue": "Proceedings of EMNLP, pages 1524\u20131534.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "author": ["Sara Rosenthal", "Preslav Nakov", "Svetlana Kiritchenko", "Saif Mohammad", "Alan Ritter", "Veselin Stoyanov."], "venue": "Proceedings of the SemEval, pages 451\u2013 463.", "citeRegEx": "Rosenthal et al\\.,? 2015", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification", "author": ["Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin."], "venue": "Proceedings of ACL, pages 1555\u20131565.", "citeRegEx": "Tang et al\\.,? 2014", "shortCiteRegEx": "Tang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "As of this writing, over 10% of Twitter posts and over 50% of text on Instagram contain one or more emoji (Cruse, 2015).", "startOffset": 106, "endOffset": 119}, {"referenceID": 13, "context": "(Lebduska, 2014) showed that emoji are culturally and contextually bound, and are open to reinterpretation and misinterpretation, a result confirmed by (Miller et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 18, "context": "(Lebduska, 2014) showed that emoji are culturally and contextually bound, and are open to reinterpretation and misinterpretation, a result confirmed by (Miller et al., 2016).", "startOffset": 152, "endOffset": 173}, {"referenceID": 9, "context": "Interestingly, Kelly and Watts (2015) found that although essentially thought of as means of expressing emotions, emoji have been adopted as tools to express relationally useful roles in conversation.", "startOffset": 15, "endOffset": 38}, {"referenceID": 21, "context": "Concurrently we observe an increased interest in natural language processing on social media data (Ritter et al., 2011; Gattani et al., 2013; Rosenthal et al., 2015).", "startOffset": 98, "endOffset": 165}, {"referenceID": 7, "context": "Concurrently we observe an increased interest in natural language processing on social media data (Ritter et al., 2011; Gattani et al., 2013; Rosenthal et al., 2015).", "startOffset": 98, "endOffset": 165}, {"referenceID": 22, "context": "Concurrently we observe an increased interest in natural language processing on social media data (Ritter et al., 2011; Gattani et al., 2013; Rosenthal et al., 2015).", "startOffset": 98, "endOffset": 165}, {"referenceID": 16, "context": "Such systems often rely on pre-trained word embeddings that can for instance be obtained from word2vec (Mikolov et al., 2013a) or GloVe (Pennington et al.", "startOffset": 103, "endOffset": 126}, {"referenceID": 20, "context": ", 2013a) or GloVe (Pennington et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 5, "context": "The first research done in this direction was an informal blog post by the Instagram Data Team in 2015 (Dimson, 2015).", "startOffset": 103, "endOffset": 117}, {"referenceID": 2, "context": "The second contribution, closest to ours, was introduced by (Barbieri et al., 2016).", "startOffset": 60, "endOffset": 83}, {"referenceID": 16, "context": "They trained emoji embeddings from a large Twitter dataset of over 100 million English tweets using the skip-gram method (Mikolov et al., 2013a).", "startOffset": 121, "endOffset": 144}, {"referenceID": 2, "context": "The second contribution, closest to ours, was introduced by (Barbieri et al., 2016). They trained emoji embeddings from a large Twitter dataset of over 100 million English tweets using the skip-gram method (Mikolov et al., 2013a). These pre-trained emoji representations led to increased accuracy on a similarity task, and a meaningful clustering of the emoji embedding space. While this method is able to learn robust representations for frequently-used emoji, representations of less frequent emoji are estimated rather poorly or not available at all. In fact, only around 700 emoji can be found in Barbieri et al. (2016)\u2019s corpus, while there is support of over 1600 emoji in the Unicode standard.", "startOffset": 61, "endOffset": 624}, {"referenceID": 8, "context": "In addition, our work relates to the work of Hill et al. (2016) who built word representations for words and concepts based on their description in a dictionary.", "startOffset": 45, "endOffset": 64}, {"referenceID": 8, "context": "In addition, our work relates to the work of Hill et al. (2016) who built word representations for words and concepts based on their description in a dictionary. Similarly to their approach, we build representations for emoji based on their descriptions and keyword phrases. Some of the limitations of our work are evident in the work of Park et al. (2013) who showed that different cultural phenomena and languages may coopt conventional emoji sentiment.", "startOffset": 45, "endOffset": 357}, {"referenceID": 0, "context": "Our model is implemented in TensorFlow (Abadi et al., 2015) and optimized using stochastic gradient descent with Adam (Kingma and Ba, 2015) as optimizer.", "startOffset": 39, "endOffset": 59}, {"referenceID": 11, "context": ", 2015) and optimized using stochastic gradient descent with Adam (Kingma and Ba, 2015) as optimizer.", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": "As downstream task we compare the accuracy of sentiment classification of tweets for various classifiers with three different sets of pre-trained word embeddings: (1) the original Google News word2vec embeddings, (2) word2vec augmented with emoji embeddings trained by Barbieri et al. (2016), and (3) word2vec augmented with emoji2vec trained from Unicode descriptions.", "startOffset": 269, "endOffset": 292}, {"referenceID": 2, "context": "As downstream task we compare the accuracy of sentiment classification of tweets for various classifiers with three different sets of pre-trained word embeddings: (1) the original Google News word2vec embeddings, (2) word2vec augmented with emoji embeddings trained by Barbieri et al. (2016), and (3) word2vec augmented with emoji2vec trained from Unicode descriptions. We use the recent dataset by Kralj Novak et al. (2015), which consists of over 67k English tweets labelled manually for positive, neutral, or negative sentiment.", "startOffset": 269, "endOffset": 425}, {"referenceID": 2, "context": "Furthermore, we find that emoji2vec generally outperforms the emoji embeddings trained by Barbieri et al. (2016), despite being trained on much less data using a simple model.", "startOffset": 90, "endOffset": 113}, {"referenceID": 15, "context": "To gain further insights, we project the learned emoji embeddings into two-dimensional space using t-SNE (Maaten and Hinton, 2008).", "startOffset": 105, "endOffset": 130}, {"referenceID": 17, "context": "For instance, it holds that the vector representation of \u2019king\u2019 minus \u2019man\u2019 plus \u2019woman\u2019 is closest to \u2019queen\u2019 (Mikolov et al., 2013b).", "startOffset": 111, "endOffset": 134}, {"referenceID": 14, "context": "Word embeddings have commonly been evaluated on such word analogy tasks (Levy and Goldberg, 2014).", "startOffset": 72, "endOffset": 97}, {"referenceID": 2, "context": "Despite the fact that our model is simpler and trained on much less data, we outperform (Barbieri et al., 2016) on the task of Twitter sentiment analysis.", "startOffset": 88, "endOffset": 111}, {"referenceID": 2, "context": "5 Google News + (Barbieri et al., 2016) 58.", "startOffset": 16, "endOffset": 39}, {"referenceID": 2, "context": "1 Google News + (Barbieri et al., 2016) 52.", "startOffset": 16, "endOffset": 39}, {"referenceID": 2, "context": "1 Google News + (Barbieri et al., 2016) 52.", "startOffset": 16, "endOffset": 39}, {"referenceID": 2, "context": "2 Google News + (Barbieri et al., 2016) 53.", "startOffset": 16, "endOffset": 39}, {"referenceID": 9, "context": "Table 1: Three-way classification accuracy on the Twitter sentiment analysis corpus using Random Forrests (Ho, 1995) and Linear", "startOffset": 106, "endOffset": 116}, {"referenceID": 6, "context": "SVM (Fan et al., 2008) classifier with different word embeddings.", "startOffset": 4, "endOffset": 22}], "year": 2016, "abstractText": "Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, pre-trained sets of word embeddings, but they contain few or no emoji representations even as emoji usage in social media has increased. In this paper we release emoji2vec, pre-trained embeddings for all Unicode emoji which are learned from their description in the Unicode emoji standard.1 The resulting emoji embeddings can be readily used in downstream social natural language processing applications alongside word2vec. We demonstrate, for the downstream task of sentiment analysis, that emoji embeddings learned from short descriptions outperforms a skip-gram model trained on a large collection of tweets, while avoiding the need for contexts in which emoji need to appear frequently in order to estimate a representation.", "creator": "TeX"}}}