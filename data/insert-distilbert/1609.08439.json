{"id": "1609.08439", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Model-based Test Generation for Robotic Software: Automata versus Belief-Desire-Intention Agents", "abstract": "robotic code needs to be verified to ensure its safety and functional correctness, especially requirements when managing the robot is interacting with people. testing the real code in simulation is a viable option. it reduces the costs available of experiments and provides detail that is lost when using formal methods. however however, generating tests that cover interesting scenarios, while executing most of the code, is a challenge amplified simultaneously by the complexity of the interactions between the environment and involving the software. human model - based test generation methods evolved can automate otherwise manual processes and facilitate reaching rare scenarios during testing. in explaining this paper, we compare the use of linear belief - desire - intention ( bdi ) agents as models for scenario test generation, with more economical conventional, model - correlation based test generation, that either exploits automata and model checking demonstration techniques, and random test generation comparison methods, rendering in terms model of practicality, performance, behavior scalability, speed and exploration ( ` coverage'). simulators and automated testbenches were implemented in robot operating integration system ( ros ) and gazebo, for testing the code of two interaction robots, bert2 in a cooperative manufacture ( data table assembly ) task, and tiago as a home day care assistant. the results highlight the clear learning advantages of using bdi agents for test generation, compared to random and conventional automata - outcome based approaches. bdi agents naturally emulate the type agency present in human - robot interaction ( hri ). they are nowadays thus more expressive and scale well in hri applications.", "histories": [["v1", "Fri, 16 Sep 2016 14:07:28 GMT  (741kb,D)", "https://arxiv.org/abs/1609.08439v1", "arXiv admin note: text overlap witharXiv:1603.00656"], ["v2", "Mon, 12 Dec 2016 11:23:48 GMT  (371kb,D)", "http://arxiv.org/abs/1609.08439v2", "arXiv admin note: text overlap witharXiv:1603.00656"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1603.00656", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["dejanira araiza-illan", "anthony g pipe", "kerstin eder"], "accepted": false, "id": "1609.08439"}, "pdf": {"name": "1609.08439.pdf", "metadata": {"source": "CRF", "title": "Model-based Test Generation for Robotic Software: Automata versus Belief-Desire-Intention Agents", "authors": ["Dejanira Araiza-Illan", "Anthony G. Pipe", "Kerstin Eder"], "emails": ["dejanira.araizaillan@bristol.ac.uk", "tony.pipe@brl.ac.uk", "kerstin.eder@bristol.ac.uk"], "sections": [{"heading": "1 INTRODUCTION", "text": "As robot software designers, we must demonstrate the safety and functional soundness of robots that interact closely with people, if these technologies are to become viable commercial products [14]. Beyond the elimination of runtime errors, a robot\u2019s code must be verified and validated at a functional level, with respect to hardware and other software components, and interactions with the environment, including with people. The interaction of all these elements introduces complexity and concurrency, and thus the possibility of unexpected and undesirable behaviours [19, 4].\nRobot high-level behaviours and control code have been verified via model checking, either by hand-crafting an abstract model of the code or behaviours (as in [30]), or by automated translations from code, often restricted to a limited subset of the language, into models or model checking languages (as in [7]). These models might require subsequent abstraction processes [10] that remove detail from the original code, to make verification feasible [23]. Furthermore, the equivalence between the original code and\n\u2217Department of Computer Science and Bristol Robotics Laboratory, University of Bristol, UK. Email: dejanira.araizaillan@bristol.ac.uk \u2020Faculty of Engineering Technology and the Bristol Robotics Laboratory, University of the West of England, Bristol, UK. E-mail: tony.pipe@brl.ac.uk \u2021Department of Computer Science and Bristol Robotics Laboratory, University of Bristol, UK. Email: kerstin.eder@bristol.ac.uk \u00a7This work was supported by the EPSRC grants EP/K006320/1 and EP/K006223/1, part of the project \u201cTrustworthy Robotic Assistants\u201d.\nar X\niv :1\n60 9.\n08 43\n9v 2\n[ cs\n.A I]\n1 2\nD ec\n2 01\n6\nthe model needs to be demonstrated, for the verification results to be considered truthful (as it is done in counter-example guided abstraction refinement [11]). Alternatively, robots\u2019 code can be tested directly, at the cost of verification not being exhaustive. An advantage of testing is that realistic components, such as emulated or real hardware (hardware-in-the-loop) and users (human-in-the-loop) can be added to the testing environment [24, 20, 25].\nFormal methods explore models fully automatically and exhaustively. They have been used for model-based test generation [15], reducing the need for writing tests manually. In model-based testing, a model of the system under test or the testing goals is derived first, followed by its traversal to generate tests based on witness traces or paths [29]. The new challenge in testing robotic code is finding suitable models for test generation, i.e. models that capture the interactions between the robot, human and environment in an effective and natural way.\nThis paper compares Belief-Desire-Intention (BDI) agents with a conventional form of model-based test generation, in terms of practicality, performance, transferability to different scenarios, and exploration, within the context of software for robots in HumanRobot Interactions (HRIs), investigating the following research questions:\n1. How does the use of BDI agents compare with model checking timed automata (TA), for model-based test generation in the HRI domain?\n2. Are BDI models useful to generate effective and efficient tests for different types of HRI applications?\nWe use two case studies to evaluate the practicality, performance, transferability and exploration capabilities of BDI-based test generation vs. test generation based on model checking TA: a human-robot cooperative manufacturing task, and a home care assistant scenario. The generated tests were run in a simulation of the scenarios, to gather statistics on coverage of code (executed lines), safety and functional requirements (monitored during execution), and combinations of human-robot actions (denominated cross-product, Cartesian product, or situational coverage [2, 5]). Our results demonstrate that BDI agents are effective and transferable models for test generation in the HRI domain. Compared to traditional test generation by model checking automata, BDI agents achieve better or similar code, requirement and cross-product coverage and are stronger at finding requirement violations. Also, BDI agents are easy to implement, and can be explored quickly."}, {"heading": "2 RELATED WORK", "text": "In our previous work, we presented a simulation-based method to test real, high-level robot control code in an effective and scalable manner [4, 5]. Automation of the testing process and a systematic exploration of the code under test within HRI scenarios was achieved through Coverage-Driven Verification (CDV), a method that guides the generation of tests, according to feedback from coverage metrics [26]. In [4, 5], we illustrated how a CDV testbench, comprising a test generator, driver, self-checker and coverage collector, can be integrated into a simulator running in the Robot Operating System (ROS)1 framework and Gazebo2. In this paper we focus on effective and efficient test generation.\nIn many robotics applications, test generation has been needed only for stimulating dedicated and simple components (equivalent to unit testing), such as choosing from a\n1http://www.ros.org/ 2http://gazebosim.org/\nset of inputs for a controller [16], or generating a timing sequence for activating individual controllers [20]. For these applications, random data generation or sampling [22] might suffice to explore the state space or data ranges [16], along with alternatives such as constraint solving or optimization techniques [20]. When testing a full robot system, however, the orchestration of different timed sequences in parallel (e.g. for emulated hardware components), coupled with several tasks of data instantiation (e.g. for sensor readings), is more complex. Sophisticated model-based approaches, such as those presented in this paper, offer a practical and viable solution for complex test generation problems [15, 29, 13]. A model-based approach can be used in a hierarchical manner in order to coordinate lower-level random data generation and optimization with more complex, higher-level test generation tasks. A two layered approach is proposed in [4, 5].\nMany languages and formalisms have been proposed for generic software model-based test generation [27], e.g. UML and process algebras for concurrency [18], or Lustre and MATLAB/Simulink for data flow [29]. Their suitability for the HRI domain, in terms of capturing realistic and uncertain environments with people, is yet to be determined [21]. Also, deriving models automatically from generic code (e.g. Python and C++ in ROS), or from user requirements, remains a challenge. BDI agents [8] have been used successfully to model decision making in autonomous robots [12]. Because BDI agents naturally reflect agency, they are also ideal to model the agency present in the robot\u2019s environment (e.g. people). In [3], we have shown how to use BDI agents for model-based test generation.\nIf a model is available (e.g. a functional modular description in [1]), as in modelbased software engineering, the verification of the software with respect to functional requirements captured in the model can be performed at design time. Code can then be generated (e.g. refined) from the verified model. However, mechanisms such as certified code generators are needed to ensure the code is equivalent to the model and thus meets its requirements [28]."}, {"heading": "3 CASE STUDIES", "text": "Our two case studies are a cooperative manufacturing scenario and a basic home care assistant scenario."}, {"heading": "3.1 Cooperative Manufacturing Task", "text": "We used the scenario we presented in [3], where a human and a robot collaborate to jointly assemble a table. The robot, BERT2 [17], should, when asked, hand over legs to the person, one at a time. A table is completed when four legs have been handed over successfully within a time threshold. For this paper, the code and simulator in [3] were slightly modified.\nA handover starts with a voice command from the person to the robot, requesting a table leg. The robot then picks up a leg, holds it out to the human, and signals for the human to take it. The human issues another voice command, indicating readiness to take the leg. Then, the robot makes a decision to release the leg or not, within a time threshold, based on a combination of three sensors: \u201cpressure\u201d (the human is holding the leg); \u201clocation\u201d (the person\u2019s hand is near the leg); and \u201cgaze\u201d (the person is looking at the leg).\nAll sensor combinations are captured by the Cartesian product of \u201cgaze\u201d, \u201cpressure\u201d and \u201clocation\u201d readings, (g, p, l) \u2208 G \u00d7 P \u00d7 L. Each sensor reading is classified into G = P = L = {1\u0304, 1}, with 1 indicating the human \u2018is ready\u2019, and 1\u0304 for any other value. If the human is deemed ready, indicated by GPL = (1, 1, 1), the robot should decide to release the leg. Otherwise, the robot should not release the leg. The robot will time out\nwhile waiting for either a voice command, or the sensor readings. The human can \u2018get bored\u2019 and disengages from the collaboration, aborting the handover.\nA ROS \u2018node\u2019 (code under test) with 264 lines of executable code in Python implements the robot\u2019s control (e.g. calls the kinematic planner MoveIt! and reads the sensor inputs). The code was structured as a finite-state machine (FSM) using SMACH [9]. This allows an efficient implementation of control flow. The FSM has 14 states and 22 transitions.\nRequirements\nWe considered the following selected set of safety and functional requirements from [3] and the standards ISO 13482 (personal care robots), ISO 15066 (collaborative robots) and ISO 10218 (industrial robots):\n1. The robot shall always discard or release a leg within a time threshold, whenever it reaches the sensing stage and determines the human is ready or not. (functional)\n2. If the gaze, pressure and location indicate the human is ready, then a leg shall be released. (functional)\n3. If the gaze, pressure or location indicate the human is not ready, then a leg shall not be released. (functional)\n4. The robot shall always discard or release a leg, when activated. (functional)\n5. The robot shall not close its hand when the human is too close. (safety)\n6. The robot shall start and work in restricted joint speed of less than 0.25 rad/s. (safety)"}, {"heading": "3.2 Home Care Assistant", "text": "A TIAGo robot3 operates in a flat. The robot is in charge of taking care of a person with limited mobility, by bringing food to the table (\u2018feed\u2019), clearing the table (\u2018clean\u2019), checking the fridge door (\u2018fridge\u2019), and checking the sink taps (\u2018sink\u2019). The robot\u2019s code invokes motion sequences, assembled from primitives such as \u2018go to fridge\u2019, \u2018go to table\u2019, or \u2018open the gripper\u2019, to obey commands. Whenever the person asks the robot to execute a command that is not in the list of known ones, the robot will not move. The robot moves to a default location, denominated \u2018recharge\u2019, after completing a command, and should remain there until the person asks it to do something else. We assume the person will not ask the robot to perform more than three feasible tasks within a 10 minute interval.\nA small dog cohabits the operational space. To avoid dangerous collisions with the dog, the robot checks the readings from the laser scan and stops if any object is too close (within a proximity of 20 cm). Figure 1 shows the simulated environment and the robot.\nA ROS \u2018node\u2019 (code under test) with 265 executable lines of code in Python implements the robot\u2019s control (e.g. motion towards a goal, listening for human commands, and collision avoidance). We used the ROS infrastructure provided in TIAGo\u2019s repository4. The code contains 5 FSMs within the code (to execute each one of the location to location motions, e.g. table to sink, or sink to recharge station) with 5, 6, 3, 3 and 2 states, and 4, 5, 2, 2, and 1 transitions, respectively.\n3http://tiago.pal-robotics.com/ 4http://wiki.ros.org/Robots/TIAGo\nRequirements\nWe considered the following requirements, also inspired by safety standards:\n1. If the robot gets food from the fridge, it shall eventually bring it to the table. (functional)\n2. If the robot is idle waiting for the next order, it shall go to or remain in the recharge station. (functional)\n3. The robot shall start and operate in restricted motion speed of less than 0.25 m/s. (safety)\n4. The robot shall not stay put (unless in the recharge station) for a long period of time. (functional)"}, {"heading": "3.3 Simulator in ROS and Gazebo", "text": "A 3-D model was built for each scenario in Gazebo. A simulation in Gazebo is controlled by the robot\u2019s code on one hand, and by code modelling the human, sensors, objects (legs and the dog), and others (MoveIt!) on the other hand (the environment), all of them running in ROS. The simulators for both scenarios are available online5.\nCDV testbench components (a driver, a checker, and coverage collection) were extended from the ROS infrastructure previously developed in [4, 5], for each case study. We implemented assertion monitors for each one of the requirements in the two case studies as self-checkers, reusing monitors where possible, e.g. for Req. 1 in both scenarios. The monitors are executed in parallel with the code in the simulation, and restarted every time a test is run. Coverage collection is logged automatically if a monitor is triggered. A post-processing step collects statistics on which monitors were triggered by a test.\nWe implemented code coverage via the \u2018coverage\u2019 Python module6, which automatically records the executed lines of code in log files. This provides branch coverage [26].\nWe also implemented cross- or Cartesian product coverage, capturing interactions of the robot and its environment, (Human \u00d7 Robot), deemed to be fundamental for the\n5https://github.com/robosafe/mc-vs-bdi 6http://coverage.readthedocs.org/en/coverage-4.1b2/\nscenarios. For example, combinations of requesting 1 to 4 legs, and the robot deciding to release all, some or none in the manufacturing task. This coverage is collected offline after the test runs by traversing simulation log files. In the manufacturing scenario, we collapsed the cross-product of all interactions7 into 14 subgroups as shown in Table 2. This captures that the human could ask for 1 to 4 legs, none or get bored, and the robot\u2019s sensor readings have 8 possible values, or the robot can time out, for each requested leg. In the home care scenario, we also collapsed the cross-product into 6 subgroups8, as shown in Table 2. We capture 5 possible requests from the human (including one issuing any invalid request), and outcomes for the robot where none, at least one or at least two of the specified requests were completed successfully.\nThe test generator is run before the simulation (offline test generation). We used our previously proposed two-tiered test generation process [4], where abstract tests in the form of timed sequences are generated first, and then valid data is instantiated. A test stimulates the environment the robot interacts with in the simulation. This engages the robot in interactions, thereby stimulating the code under test. Example tests can be found in [5, 3]. We instantiated and extended our previous implementations of test generation by pseudorandom sampling, model checking TA [4, 5] and BDI agents [3], for the case studies in this paper."}, {"heading": "4 MODEL-BASED TEST GENERATION", "text": "We describe two types of model-based test generation, using BDI agents [3] and model checking TA [4, 5], along with a baseline: pseudorandom test generation. In modelbased approaches, a model of the system or its requirements is assembled first and then explored to produce tests. We use model-based approaches to produce abstract tests that will indirectly stimulate the robot\u2019s code in simulation by stimulating the environment that the code interacts with, instead of stimulating the code directly."}, {"heading": "4.1 BDI Agent Models and Exploration", "text": "BDI is an intelligent or rational agent architecture for multi-agent systems. BDI agents model human practical reasoning, in terms of \u2018beliefs\u2019 (knowledge about the world and the agents), \u2018desires\u2019 (goals to fulfil), and \u2018intentions\u2019 (plans to execute in order to achieve the goals, according to the available knowledge) [8]. Recently, we have shown that BDI agents are well suited to model rational, human-like decision making in HRI, for test generation purposes [3].\nWe employed the Jason interpreter, where agents are expressed in the AgentSpeak language. An agent is defined by initial beliefs (the initial knowledge) and desires (initial goals), and a library of \u2018plans\u2019 (actions to execute to fulfil the desires, according to the beliefs). A plan has a \u2018head\u2019, formed by an expression about beliefs (a \u2018context\u2019) serving as a guard for plan selection, and a \u2018body\u2019 or set of actions to execute. New beliefs appear during the agents\u2019 execution, can be sent by other agents, or are a result of the execution of plans (self-beliefs) [8].\nWe model an HRI scenario using a set of BDI agents, representing the robot\u2019s code, sensors, actuators, and its environment. Then, we add BDI verification agents that control the execution of the HRI agents, by triggering (sending) beliefs to activate plans and create new desires in the other agents, which in turn may lead to the triggering of new plans, and so on. A set of beliefs for the verification agents to send to other agents is chosen, and then the multi-agent system is executed once. Each system execution with\n7There are 6500 coverage tasks in the cross-product. 8From more than 250 coverage tasks in the cross-product, with a maximum of 3 commands given\nper test, which the robot might or might not complete.\na different set of beliefs will activate a corresponding sequence of plans in the agents. This execution (a set of chosen and executed plans) is recorded and used to generate an abstract test (a sequence of \u2018actions\u2019 according to the recorded plans). We extract the environment components from an abstract test to stimulate the robot\u2019s code indirectly."}, {"heading": "4.1.1 Model for the Cooperative Manufacturing Task", "text": "We reused the BDI model in [3], with minor modifications. The model consists of four agents: the robot\u2019s code, the human, the sensors (as a single agent), and the verification one. The verification agent makes the human agent send activation signals to both the robot\u2019s code (voice commands) and the sensors agent. There are a total of 38 possible beliefs for the verification agent, including, e.g., requesting 1 to 4 legs, readings for the three sensors for each leg, and the human getting bored. The sensors agent transmits readings of either 1 or 1\u0304 to the robot\u2019s code agent. The robot\u2019s code agent has a similar structure to the FSM in the real code, interacting only with the human and sensors agents through beliefs."}, {"heading": "4.1.2 Model for the Home Care Assistant", "text": "Our model consists of five agents: the robot\u2019s code, the human, the dog, the sensor (for collision avoidance), and the verification one. The verification agent selects the requests that the human agent communicates to the robot\u2019s code agent, one at a time. The dog agent can opt to collide with TIAGo or not. This is then perceived by the sensor agent, which transfers this information to the robot\u2019s code agent. The robot\u2019s code agent is based on an FSM that is similar to the one used in the real code. There are 5 possible beliefs for the verification agent to control the human, comprising the four available requests and an extra one representing any other invalid request."}, {"heading": "4.2 Timed Automata Models and Model Checking", "text": "Model checking is the exhaustive exploration of a model to determine whether a logic property is satisfied or not. Traces of examples or counterexamples are provided as evidence of satisfaction or proof of violation, respectively. Model checking applied to models of the code or high-level system functionality can be exploited for model-based test generation, where these traces are used to derive tests [15, 29].\nIn [4, 5], we modelled HRI in terms of TA for the model checker UPPAAL9. Nondeterminism allows capturing uncertainty in the human actions, and sensor errors, through the selection of different transitions in the automata. As robots interacting with humans are expected to fulfil goals in a finite amount of time, the timing counters in the TA allow emulating these timing thresholds. The execution of these automata is synchronized by communication events, and \u2018guards\u2019 or conditions, to transition from one state to another, according to system variables and events.\nTo derive tests from the TA, logic properties are formalized manually in TCTL (Timed Computation Tree Logic), and automatically checked by the UPPAAL model checker. For example, we would specify that \u2018the robot reaches a specified location within a minute\u2019, for the home care scenario. Formulating suitable properties to achieve high model coverage requires a good understanding of formal logic, the HRI scenario and the TA models. The UPPAAL model checker produces an example if the property is satisfied, comprising sequences of states from all the TA combined. To indirectly stimulate the robot\u2019s code, we exclude the robot\u2019s code contribution from these sequences; what remains is the stimulus used to test the robot\u2019s code.\n9http://www.uppaal.org/\nCompleteness and correctness of the models was established empirically through step-by-step execution and simulation at development time in Jason and UPPAAL, respectively for BDI agents and TA. This effort is accounted for in the reported model development time in Section 5.5.2."}, {"heading": "4.2.1 Model for the Cooperative Manufacturing Task", "text": "Our model consists of 6 TA, the human, the robot\u2019s code, the sensors, and the gaze, pressure, and location selections by the human. While the human automaton enacts the activation signals (voice commands), the gaze, pressure and location automata select inputs for the sensors non-deterministically (via variables). The sensors automaton reads the variables as 1 or 1\u0304, which are then read by the robot\u2019s code automaton to decide whether to release a leg or not. The latter has a similar structure to the FSM in the real code."}, {"heading": "4.2.2 Model for the Home Care Assistant", "text": "Our model consists of 4 TA, the human, the robot\u2019s code, the sensor, and the dog. The sensor automaton determines if the dog is within collision distance or not, according to the choices of the dog automaton. The human automaton sets the type of requests for the robot\u2019s code automaton, one at a time. The robot\u2019s code automaton, which is similar in structure to the FSMs in the code, executes the requests from the human, whilst considering the sensor readings to avoid collisions."}, {"heading": "4.3 Baseline: Pseudorandom Test Generation", "text": "As a baseline for comparisons we employed a pseudorandom abstract test generator [4, 5]. The generator concatenates sequences of \u2018actions\u2019 sampled at random from a list of specified ones. The sequences\u2019 length is also chosen pseudorandomly. For example, in the home care scenario sequences are assembled from available requests such as \u2018request food\u2019 or \u2018request clean\u2019. In the manufacturing scenario, human actions such as \u2018activate robot\u2019 or \u2018choose gaze as OK\u2019 are available to be included into sequences."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "5.1 Experimental setup", "text": "The simulator and testbench were implemented in ROS Indigo and Gazebo 2.2.5. The tests were run on a PC with Intel i5-3230M 2.60 GHz CPU, 8 GB of RAM, running Ubuntu 14.04. We used Jason 1.4.2 for the BDI models, and UPPAAL 4.1.19 for model checking. All the simulators, code and test generation models used in the experiments, along with the related tests and results data, are available online5."}, {"heading": "5.1.1 Cooperative Manufacturing Assistant", "text": "For model checking TA, as described in Section 4.2, we manually generated 91 TCTL properties, for which example traces were produced automatically, and abstract tests were extracted. These properties covered all the gaze, pressure and location sensor reading combinations, 1 to 4 leg requests, the human getting bored, and the robot timing out while waiting for a signal.\nWith the BDI-based method, we generated 131 abstract tests (from a possible total number of 238) by specifying constraints for sets of beliefs that covered the same items as the TCTL properties and more, i.e. a variety of valid human and robot actions, and\nan orchestration of the rarest events such as completing 4 legs correctly. The generator explores the constrained sets of beliefs automatically, one at a time, over the multi-agent system, following the procedure explained in Section 4.1.\nAdditionally, we generated 160 tests pseudorandomly by sampling from a defined set of human \u2018actions\u2019 for the task, as explained in Section 4.3.\nEach abstract test was concretized at least once by sampling pseudorandomly from valid ranges (i.e. parameters were instantiated for variables such as gaze, pressure and location) using as seed the test number, which lead to a total of 160 different concrete tests for each method. This process allowed the execution of model-based tests that are equivalent in terms of expected system\u2019s abstract functionality, although with different variable instantiations, for both BDI-based and model checking TA methods. Each test, once concretized, ran for a maximum of 300 seconds."}, {"heading": "5.1.2 Home Care Assistant", "text": "By model checking TA, we generated 23 TCTL properties and the consequent abstract tests. These properties covered combinations of 1 to 3 requests for feeding, cleaning, checking the fridge, checking the sink, and any other invalid order.\nWith the BDI agents, we generated 62 abstract tests by sampling belief sets from a possible total number of 25, to cover the same request combinations as with model checking. We discarded 12 tests to get a total of 50, as some of the tests were quite similar (e.g. combinations of invalid commands).\nFinally, we generated 50 abstract tests pseudorandomly, as explained in Section 4.3. As before, each abstract test was concretized pseudorandomly (at least once in the case of model checking) from valid ranges using as seed the test number, for a total of 50 different concrete tests for each method. Each test ran for a maximum of 700 seconds."}, {"heading": "5.2 Code Coverage Results", "text": "We expected that the BDI-based method would produce a large number of high-coverage tests quickly, and that both model-based methods would outperform pseudorandom test generation in terms of coverage.\nFigures 2 and 3 show the code coverage percentage reached by each produced test, and the accumulated coverage, for both scenarios. In the manufacturing scenario (Fig. 2), tests produced with BDI agents reached high levels of coverage fast (at 92% of accumulated coverage), and a large number reached the highest coverage possible (92%), consequently outperforming tests generated pseudorandomly and by model checking TA, in terms of coverage efficiency and effectiveness.\nIn the home care scenario (Fig. 3), tests produced with BDI agents reached the highest coverage results (86%), outperforming tests generated by model checking TA and pseudorandomly. Also, Fig. 3 shows that pseudorandomly generated tests start with high accumulated coverage results compared to tests from the other methods, but then this high coverage flattens and tests from the two model-based methods catch up quickly. BDI-based tests outperform the two methods later (at 86% of accumulated coverage)."}, {"heading": "5.3 Assertion Coverage Results", "text": "One of our motivations for comparing the test generation methods was to establish whether model-based methods would produce tests that achieve higher assertion coverage than pseudorandom test generation. Because the models reflect the functional requirements, we expected them to generate tests that trigger the assertion monitors more frequently. While this section is focused on assertion coverage, the purpose of\nassertions is to flag requirement violations. We assess the effectiveness of tests in terms of their ability in finding faults from the triggering of assertion monitors.\nThe assertion coverage results are shown in Table 1. We recorded the number of tests for which the requirement was satisfied (P), not satisfied (F) or not checked (NC).\nIn the manufacturing scenario, Reqs. 1 to 3 are violated as the robot occasionally fails to decide whether to release a leg or not within the given time threshold. These failures were found mostly with the model-based tests, as expected, and in the case of Req. 3, only through tests generated based on BDI agents. Req. 5 is also violated occasionally, as the person\u2019s hand is allowed to be near the robot gripper when it closes. To improve this issue, the code could be augmented to stop the robot gripper when the hand is close and a handover is then not happening. Reqs. 4 and 6 are satisfied in all tests. Seeing that violations of Req. 3 were only found through tests from BDI agents,"}, {"heading": "6 160 0 0 160 0 0 160 0 0", "text": "these tests outperformed the ones generated pseudorandomly and by model checking TA, i.e. they were the most effective at finding requirement violations.\nFor the home care scenario, requirement violations were found with all test generation methods. If the robot collides with the dog, the collision causes the robot to fall over without recovery, which will prevent the robot from completing the current request and any subsequent ones. This is reflected in the results of Reqs. 1, 2 and 4. Req. 3 is not satisfied as a velocity limit is not enforced in the motion control of the robot\u2019s base. As a consequence of failure in mission completion, depending on collisions with the dog, the overall assertion coverage results are low and quite similar for all the test generation methods."}, {"heading": "5.4 Cross-Product Functional Coverage Results", "text": "We expected that model-based methods would reach more cross-product items than pseudorandom test generation, i.e. that they would be more effective at cross-product coverage, especially for the manufacturing scenario, as 4 successful leg handovers are hard to achieve. Table 2 shows the coverage results for reachable combinations of Human\u00d7Robot behaviours as described in Section 3.3.\nThe results for the manufacturing scenario show that it is difficult to reach some of the coverage points with tests from pseudorandom generation, as expected, due to the complexity of the interaction protocol to activate the robot. Tests generated with BDI agents covered all the items, and similarly the tests generated by model checking TA, demonstrating their cross-product coverage effectiveness.\nIn the home care scenario, the coverage results were similar for all the three methods due to two factors. Firstly, the system malfunctions when collisions occur and fails to complete its mission, as explained before, leading to a low coverage of cross-product items with multiple valid requests for TIAGo. Secondly, both the models and the list of available requests for pseudorandom test generation constrain the amount of invalid requests for the robot to sporadic occurrences, thus increasing the generation of tests that contained valid requests."}, {"heading": "5.5 Discussion", "text": ""}, {"heading": "5.5.1 Exploration", "text": "Answering our first research question, the presented results demonstrate that BDI-based tests perform as well as the ones from traditional test generation by model checking automata, and outperform the tests from pseudorandom generation, in terms of reaching high levels of code, assertion and cross-product coverage quickly, i.e. coverage effectiveness and efficiency. Also, BDI-based tests discovered requirement violations in the manufacturing task that tests from the other methods did not find, i.e. they were more effective at identifying failures."}, {"heading": "5.5.2 Performance", "text": "A comparison of effort to craft the different models, the resulting models\u2019 size, and the model exploration time to produce tests, for a roboticist with similar training using Jason and UPPAAL, is shown in Table 3. The construction of automata in UPPAAL required a longer effort than constructing the BDI agents in Jason. The syntax of BDI agents offers a more rational and intuitive structure, allowing the construction of an HRI protocol with less effort than specifying automata variables, guards and transitions. Specifying BDI belief sets is also more intuitive than specifying TCTL properties, as in the latter all the variables and states in the model need to be considered. We limited the running time of the BDI model manually, and the time could have been further reduced. However, the model checking time varies depending on the properties; it is unpredictable and cannot be controlled as part of the test generation process. Note that in some cases model checking took significantly longer than exploring the BDI agents.\nAlthough model checking is fully automatic, formalizing properties for test generation requires manual input and is often error prone. More research would be needed to automate the generation of properties to achieve high model coverage without manual effort. A higher level of automation in test generation with BDI agents can be achieved\nby using machine learning techniques for the selection of the best belief sets in terms of achieved coverage [3], at the cost of increased computational effort. In addition, BDI models can also be explored via model checking [6], instead of using verification agents as we propose here. This could complement our approach to achieve coverage closure."}, {"heading": "5.5.3 Practicality and Transferability", "text": "Our results demonstrate that BDI agents are applicable to different HRI scenarios, as exemplified by our two case studies. BDI agents model an HRI task with human-like actions and rational reasoning. They are natural to program, by specifying plans of actions. Compared to model checking, we do not need to formulate temporal logic reachability properties, which requires a good understanding of formal logics, and a greater amount of manual effort. In addition, constructing automata, such as TA, for larger case studies requires several cycles of abstraction to manage the state-space explosion problem [30]."}, {"heading": "5.5.4 Limitations", "text": "In this paper, the two case studies serve to illustrate our comparison of using BDI agents, instead of model-checking TA, for model-based test generation. Industrial-sized code, and richer HRI case studies are required to further validate our results. Other coverage metrics could be employed to add further comparison dimensions in terms of system exploration during testing, such as FSM states, or transitions, making use of the FSM structure of some of the code. Nonetheless, our approach is not prescriptive on structuring the code as FSMs, or on using SMACH. Finally, all the approaches presented in this paper implement offline test generation, i.e. the tests are computed before the simulation. This is suitable when the models of the system and the environment do not change. For robots that learn and adapt in changing environments, online techniques for test generation will be required."}, {"heading": "6 CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we compared two model-based test generation approaches in the context of HRI scenarios: BDI agents and model checking automata, in terms of exploration (coverage), performance, practicality and transferability. We also compared both methods to pseudorandom test generation as a baseline. The test generation methods were applied to two case studies, a cooperative manufacturing task, and a home care scenario, for which high-level robot control code was tested in ROS and Gazebo simulators using a coverage-driven automated testbench [4, 5].\nWe have found that BDI agents allow realistic, human-like stimulus, whilst facilitating the generation of complex interactions between the robot and its environment. Tests generated with BDI agents perform similarly to the ones generated by model checking TA in terms of reaching high coverage (code, assertions, and cross-product), and are better than the ones generated pseudorandomly. Also, BDI agents are easier to specify, computationally cheaper to execute, and more predictable in terms of performance when compared to model checking TA. In conclusion, our results clearly highlight the advantages of using BDI agents for test generation in complex HRI scenarios that require the robot code under test to be stimulated with a broad variety of realistic interaction sequences.\nIn the future, we plan to investigate how BDI agents can be used to interactively stimulate the robot code during simulation, generating new stimulus on the fly in direct response to a robot\u2019s observable behaviour within the test environment. We then intend to apply this online, BDI-based test generation to stress test complex systems with agency and change."}], "references": [{"title": "Rigorous design of robot software: A formal component-based approach", "author": ["Tesnim Abdellatif", "Saddek Bensalem", "Jacques Combaz", "Lavindra de Silva", "Felix Ingrand"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Situation Coverage \u2013 A Coverage Criterion for Testing Autonomous Robots", "author": ["Rob Alexander", "Heather Hawkins", "Drew Rae"], "venue": "Technical report,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Intelligent agent-based stimulation for testing robotic software in human-robot interactions", "author": ["D. Araiza-Illan", "A.G. Pipe", "K. Eder"], "venue": "In Proceedings of the 3rd Workshop on Model-Driven Robot Software Engineering (MORSE),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Coverage-driven verification \u2014 an approach to verify code for robots that directly interact with humans", "author": ["D. Araiza-Illan", "D. Western", "K. Eder", "A. Pipe"], "venue": "In Proc. HVC,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Systematic and realistic testing in simulation of control code for robots in collaborative human-robot interactions", "author": ["D. Araiza-Illan", "D. Western", "K. Eder", "A. Pipe"], "venue": "In Proc. TAROS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Model checking AgentSpeak", "author": ["R.H. Bordini", "M. Fisher", "C. Pardavila", "M. Wooldridge"], "venue": "In Proc. AAMAS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Verifying multi-agent programs by model checking", "author": ["Rafael H. Bordini", "Michael Fisher", "Willem Visser", "Michael Wooldridge"], "venue": "Journal of Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Programming Multi-Agent Systems in AgentSpeak using Jason", "author": ["R.H. Bordini", "J.F. H\u00fcbner", "M. Wooldridge"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "The SMACH High-Level Executive", "author": ["Jonathan Boren", "Steve Cousins"], "venue": "IEEE Robotics & Automation Magazine,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Progress on the state explosion problem in model checking", "author": ["Edmund Clarke", "Orna Grumberg", "Somesh Jha", "Yuan Lu", "Helmut Veith"], "venue": "In Informatics. 10 Years Back. 10 Years Ahead,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Counterexample-guided abstraction refinement for symbolic model checking", "author": ["Edmund Clarke", "Orna Grumberg", "Somesh Jha", "Yuan Lu", "Helmut Veith"], "venue": "Journal of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Practical verification of decision-making in agent-based autonomous", "author": ["Louise A. Dennis", "Michael Fisher", "Nicholas K. Lincoln", "Alexei Lisitsa", "Sandor M. Veres"], "venue": "systems. Automated,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "A survey on model-based testing approaches: A systematic review", "author": ["Arilo C. Dias Neto", "Rajesh Subramanyan", "Marlon Vieira", "Guilherme H. Travassos"], "venue": "In Proc. WEASELTech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Towards the safety of human-in-the-loop robotics: Challenges and opportunities for safety assurance of robotic co-workers", "author": ["K.I. Eder", "C. Harper", "U.B. Leonards"], "venue": "In Proc. IEEE ROMAN,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Testing with model checkers: a survey", "author": ["Gordon Fraser", "Franz Wotawa", "Paul E. Ammann"], "venue": "Software Testing, Verification and Reliability,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Sampling-based algorithm for testing and validating robot controllers", "author": ["J. Kim", "J.M. Esposito", "R.V. Kumar"], "venue": "International Journal of Robotics Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "The BERT2 infrastructure: An integrated system for the study of human-robot interaction", "author": ["A. Lenz", "S. Skachek", "K. Hamann", "J. Steinwender", "A.G. Pipe", "C. Melhuish"], "venue": "In Proc. IEEE-RAS Humanoids,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Model-based testing of autonomous systems based on Coloured Petri Nets", "author": ["R. Lill", "F. Saglietti"], "venue": "In Proc. ARCS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "A concept for testing robustness and safety of the context-aware behaviour of autonomous systems", "author": ["Z. Micskei", "Z. Szatm\u00e1ri", "J. Ol\u00e1h", "I. Majzik"], "venue": "In Proc. KES- AMSTA,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Testing robot controllers using constraint programming and continuous integration", "author": ["M. Mossige", "A. Gotlieb", "H. Meling"], "venue": "Information and Software Technology,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Fast abstract: Stochastic model- based testing for human-robot interaction", "author": ["Akbar Siami Namin", "Barbara Millet", "Mohan Sridharan"], "venue": "In Proc. ISSRE,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "A survey of combinatorial testing", "author": ["C. Nie", "H. Leung"], "venue": "ACM Computing Surveys,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Model-based real-time testing of drone autopilots", "author": ["Andrea Patelli", "Luca Mottola"], "venue": "In Proc. DroNet,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Multilevel testing of control software for teams of autonomous mobile robots", "author": ["S. Petters", "D. Thomas", "M. Friedmann", "O. von Stryk"], "venue": "In Proc. SIMPAR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Framework using ROS and SimTwo simulator for realistic test of mobile robot controllers", "author": ["T. Pinho", "A.P. Moreira", "J. Boaventura-Cunha"], "venue": "In Proc. CON- TROLO,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Functional Verification Coverage Measurement and Analysis", "author": ["Andrew Pizialli"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A systematic review of state-based test", "author": ["Muhammad Shafique", "Yvan Labiche"], "venue": "tools. International Journal on Software Tools for Technology Transfer,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Towards reliable code generation with an open tool: Evolutions of the Gene-Auto toolset", "author": ["A. Toom", "N. Izerrouken", "T. Naks", "M. Pantel", "O. Ssi Yan Kai"], "venue": "In Proc. ERTS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "A taxonomy of model-based testing approaches", "author": ["M. Utting", "A. Pretschner", "B. Legeard"], "venue": "Software Testing, Verification and Reliability,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Formal verification of an autonomous personal robotic assistant", "author": ["Matt Webster", "Clare Dixon", "Michael Fisher", "Maha Salem", "Joe Saunders", "Kheng Lee Koay", "Kerstin Dautenhahn"], "venue": "In Proc. AAAI FVHMS", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "As robot software designers, we must demonstrate the safety and functional soundness of robots that interact closely with people, if these technologies are to become viable commercial products [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 18, "context": "The interaction of all these elements introduces complexity and concurrency, and thus the possibility of unexpected and undesirable behaviours [19, 4].", "startOffset": 143, "endOffset": 150}, {"referenceID": 3, "context": "The interaction of all these elements introduces complexity and concurrency, and thus the possibility of unexpected and undesirable behaviours [19, 4].", "startOffset": 143, "endOffset": 150}, {"referenceID": 29, "context": "Robot high-level behaviours and control code have been verified via model checking, either by hand-crafting an abstract model of the code or behaviours (as in [30]), or by automated translations from code, often restricted to a limited subset of the language, into models or model checking languages (as in [7]).", "startOffset": 159, "endOffset": 163}, {"referenceID": 6, "context": "Robot high-level behaviours and control code have been verified via model checking, either by hand-crafting an abstract model of the code or behaviours (as in [30]), or by automated translations from code, often restricted to a limited subset of the language, into models or model checking languages (as in [7]).", "startOffset": 307, "endOffset": 310}, {"referenceID": 9, "context": "These models might require subsequent abstraction processes [10] that remove detail from the original code, to make verification feasible [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 22, "context": "These models might require subsequent abstraction processes [10] that remove detail from the original code, to make verification feasible [23].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "the model needs to be demonstrated, for the verification results to be considered truthful (as it is done in counter-example guided abstraction refinement [11]).", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "An advantage of testing is that realistic components, such as emulated or real hardware (hardware-in-the-loop) and users (human-in-the-loop) can be added to the testing environment [24, 20, 25].", "startOffset": 181, "endOffset": 193}, {"referenceID": 19, "context": "An advantage of testing is that realistic components, such as emulated or real hardware (hardware-in-the-loop) and users (human-in-the-loop) can be added to the testing environment [24, 20, 25].", "startOffset": 181, "endOffset": 193}, {"referenceID": 24, "context": "An advantage of testing is that realistic components, such as emulated or real hardware (hardware-in-the-loop) and users (human-in-the-loop) can be added to the testing environment [24, 20, 25].", "startOffset": 181, "endOffset": 193}, {"referenceID": 14, "context": "They have been used for model-based test generation [15], reducing the need for writing tests manually.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "In model-based testing, a model of the system under test or the testing goals is derived first, followed by its traversal to generate tests based on witness traces or paths [29].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "The generated tests were run in a simulation of the scenarios, to gather statistics on coverage of code (executed lines), safety and functional requirements (monitored during execution), and combinations of human-robot actions (denominated cross-product, Cartesian product, or situational coverage [2, 5]).", "startOffset": 298, "endOffset": 304}, {"referenceID": 4, "context": "The generated tests were run in a simulation of the scenarios, to gather statistics on coverage of code (executed lines), safety and functional requirements (monitored during execution), and combinations of human-robot actions (denominated cross-product, Cartesian product, or situational coverage [2, 5]).", "startOffset": 298, "endOffset": 304}, {"referenceID": 3, "context": "In our previous work, we presented a simulation-based method to test real, high-level robot control code in an effective and scalable manner [4, 5].", "startOffset": 141, "endOffset": 147}, {"referenceID": 4, "context": "In our previous work, we presented a simulation-based method to test real, high-level robot control code in an effective and scalable manner [4, 5].", "startOffset": 141, "endOffset": 147}, {"referenceID": 25, "context": "Automation of the testing process and a systematic exploration of the code under test within HRI scenarios was achieved through Coverage-Driven Verification (CDV), a method that guides the generation of tests, according to feedback from coverage metrics [26].", "startOffset": 254, "endOffset": 258}, {"referenceID": 3, "context": "In [4, 5], we illustrated how a CDV testbench, comprising a test generator, driver, self-checker and coverage collector, can be integrated into a simulator running in the Robot Operating System (ROS) framework and Gazebo.", "startOffset": 3, "endOffset": 9}, {"referenceID": 4, "context": "In [4, 5], we illustrated how a CDV testbench, comprising a test generator, driver, self-checker and coverage collector, can be integrated into a simulator running in the Robot Operating System (ROS) framework and Gazebo.", "startOffset": 3, "endOffset": 9}, {"referenceID": 15, "context": "set of inputs for a controller [16], or generating a timing sequence for activating individual controllers [20].", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "set of inputs for a controller [16], or generating a timing sequence for activating individual controllers [20].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "For these applications, random data generation or sampling [22] might suffice to explore the state space or data ranges [16], along with alternatives such as constraint solving or optimization techniques [20].", "startOffset": 59, "endOffset": 63}, {"referenceID": 15, "context": "For these applications, random data generation or sampling [22] might suffice to explore the state space or data ranges [16], along with alternatives such as constraint solving or optimization techniques [20].", "startOffset": 120, "endOffset": 124}, {"referenceID": 19, "context": "For these applications, random data generation or sampling [22] might suffice to explore the state space or data ranges [16], along with alternatives such as constraint solving or optimization techniques [20].", "startOffset": 204, "endOffset": 208}, {"referenceID": 14, "context": "Sophisticated model-based approaches, such as those presented in this paper, offer a practical and viable solution for complex test generation problems [15, 29, 13].", "startOffset": 152, "endOffset": 164}, {"referenceID": 28, "context": "Sophisticated model-based approaches, such as those presented in this paper, offer a practical and viable solution for complex test generation problems [15, 29, 13].", "startOffset": 152, "endOffset": 164}, {"referenceID": 12, "context": "Sophisticated model-based approaches, such as those presented in this paper, offer a practical and viable solution for complex test generation problems [15, 29, 13].", "startOffset": 152, "endOffset": 164}, {"referenceID": 3, "context": "A two layered approach is proposed in [4, 5].", "startOffset": 38, "endOffset": 44}, {"referenceID": 4, "context": "A two layered approach is proposed in [4, 5].", "startOffset": 38, "endOffset": 44}, {"referenceID": 26, "context": "Many languages and formalisms have been proposed for generic software model-based test generation [27], e.", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "UML and process algebras for concurrency [18], or Lustre and MATLAB/Simulink for data flow [29].", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "UML and process algebras for concurrency [18], or Lustre and MATLAB/Simulink for data flow [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "Their suitability for the HRI domain, in terms of capturing realistic and uncertain environments with people, is yet to be determined [21].", "startOffset": 134, "endOffset": 138}, {"referenceID": 7, "context": "BDI agents [8] have been used successfully to model decision making in autonomous robots [12].", "startOffset": 11, "endOffset": 14}, {"referenceID": 11, "context": "BDI agents [8] have been used successfully to model decision making in autonomous robots [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 2, "context": "In [3], we have shown how to use BDI agents for model-based test generation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "a functional modular description in [1]), as in modelbased software engineering, the verification of the software with respect to functional requirements captured in the model can be performed at design time.", "startOffset": 36, "endOffset": 39}, {"referenceID": 27, "context": "However, mechanisms such as certified code generators are needed to ensure the code is equivalent to the model and thus meets its requirements [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "We used the scenario we presented in [3], where a human and a robot collaborate to jointly assemble a table.", "startOffset": 37, "endOffset": 40}, {"referenceID": 16, "context": "The robot, BERT2 [17], should, when asked, hand over legs to the person, one at a time.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "For this paper, the code and simulator in [3] were slightly modified.", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "The code was structured as a finite-state machine (FSM) using SMACH [9].", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "We considered the following selected set of safety and functional requirements from [3] and the standards ISO 13482 (personal care robots), ISO 15066 (collaborative robots) and ISO 10218 (industrial robots):", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "CDV testbench components (a driver, a checker, and coverage collection) were extended from the ROS infrastructure previously developed in [4, 5], for each case study.", "startOffset": 138, "endOffset": 144}, {"referenceID": 4, "context": "CDV testbench components (a driver, a checker, and coverage collection) were extended from the ROS infrastructure previously developed in [4, 5], for each case study.", "startOffset": 138, "endOffset": 144}, {"referenceID": 25, "context": "This provides branch coverage [26].", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "We used our previously proposed two-tiered test generation process [4], where abstract tests in the form of timed sequences are generated first, and then valid data is instantiated.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "Example tests can be found in [5, 3].", "startOffset": 30, "endOffset": 36}, {"referenceID": 2, "context": "Example tests can be found in [5, 3].", "startOffset": 30, "endOffset": 36}, {"referenceID": 3, "context": "We instantiated and extended our previous implementations of test generation by pseudorandom sampling, model checking TA [4, 5] and BDI agents [3], for the case studies in this paper.", "startOffset": 121, "endOffset": 127}, {"referenceID": 4, "context": "We instantiated and extended our previous implementations of test generation by pseudorandom sampling, model checking TA [4, 5] and BDI agents [3], for the case studies in this paper.", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "We instantiated and extended our previous implementations of test generation by pseudorandom sampling, model checking TA [4, 5] and BDI agents [3], for the case studies in this paper.", "startOffset": 143, "endOffset": 146}, {"referenceID": 2, "context": "We describe two types of model-based test generation, using BDI agents [3] and model checking TA [4, 5], along with a baseline: pseudorandom test generation.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "We describe two types of model-based test generation, using BDI agents [3] and model checking TA [4, 5], along with a baseline: pseudorandom test generation.", "startOffset": 97, "endOffset": 103}, {"referenceID": 4, "context": "We describe two types of model-based test generation, using BDI agents [3] and model checking TA [4, 5], along with a baseline: pseudorandom test generation.", "startOffset": 97, "endOffset": 103}, {"referenceID": 7, "context": "BDI agents model human practical reasoning, in terms of \u2018beliefs\u2019 (knowledge about the world and the agents), \u2018desires\u2019 (goals to fulfil), and \u2018intentions\u2019 (plans to execute in order to achieve the goals, according to the available knowledge) [8].", "startOffset": 243, "endOffset": 246}, {"referenceID": 2, "context": "Recently, we have shown that BDI agents are well suited to model rational, human-like decision making in HRI, for test generation purposes [3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 7, "context": "New beliefs appear during the agents\u2019 execution, can be sent by other agents, or are a result of the execution of plans (self-beliefs) [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "We reused the BDI model in [3], with minor modifications.", "startOffset": 27, "endOffset": 30}, {"referenceID": 14, "context": "Model checking applied to models of the code or high-level system functionality can be exploited for model-based test generation, where these traces are used to derive tests [15, 29].", "startOffset": 174, "endOffset": 182}, {"referenceID": 28, "context": "Model checking applied to models of the code or high-level system functionality can be exploited for model-based test generation, where these traces are used to derive tests [15, 29].", "startOffset": 174, "endOffset": 182}, {"referenceID": 3, "context": "In [4, 5], we modelled HRI in terms of TA for the model checker UPPAAL.", "startOffset": 3, "endOffset": 9}, {"referenceID": 4, "context": "In [4, 5], we modelled HRI in terms of TA for the model checker UPPAAL.", "startOffset": 3, "endOffset": 9}, {"referenceID": 3, "context": "As a baseline for comparisons we employed a pseudorandom abstract test generator [4, 5].", "startOffset": 81, "endOffset": 87}, {"referenceID": 4, "context": "As a baseline for comparisons we employed a pseudorandom abstract test generator [4, 5].", "startOffset": 81, "endOffset": 87}, {"referenceID": 2, "context": "by using machine learning techniques for the selection of the best belief sets in terms of achieved coverage [3], at the cost of increased computational effort.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "In addition, BDI models can also be explored via model checking [6], instead of using verification agents as we propose here.", "startOffset": 64, "endOffset": 67}, {"referenceID": 29, "context": "In addition, constructing automata, such as TA, for larger case studies requires several cycles of abstraction to manage the state-space explosion problem [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 3, "context": "The test generation methods were applied to two case studies, a cooperative manufacturing task, and a home care scenario, for which high-level robot control code was tested in ROS and Gazebo simulators using a coverage-driven automated testbench [4, 5].", "startOffset": 246, "endOffset": 252}, {"referenceID": 4, "context": "The test generation methods were applied to two case studies, a cooperative manufacturing task, and a home care scenario, for which high-level robot control code was tested in ROS and Gazebo simulators using a coverage-driven automated testbench [4, 5].", "startOffset": 246, "endOffset": 252}], "year": 2016, "abstractText": "Robotic code needs to be verified to ensure its safety and functional correctness, especially when the robot is interacting with people. Testing real code in simulation is a viable option. However, generating tests that cover rare scenarios, as well as exercising most of the code, is a challenge amplified by the complexity of the interactions between the environment and the software. Model-based test generation methods can automate otherwise manual processes and facilitate reaching rare scenarios during testing. In this paper, we compare using BeliefDesire-Intention (BDI) agents as models for test generation with more conventional automata-based techniques that exploit model checking, in terms of practicality, performance, transferability to different scenarios, and exploration (\u2018coverage\u2019), through two case studies: a cooperative manufacturing task, and a home care scenario. The results highlight the advantages of using BDI agents for test generation. BDI agents naturally emulate the agency present in Human-Robot Interactions (HRIs), and are thus more expressive than automata. The performance of the BDI-based test generation is at least as high, and the achieved coverage is higher or equivalent, compared to test generation based on model checking automata.", "creator": "LaTeX with hyperref package"}}}