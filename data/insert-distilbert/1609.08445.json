{"id": "1609.08445", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "AP16-OL7: A Multilingual Database for Oriental Languages and A Language Recognition Baseline", "abstract": "subsequently we present the ap16 - ol7 database which data was released as the training and test data for enhancing the oriental language vocabulary recognition ( olr ) challenge on apsipa 2016. based on the database, a baseline system was constructed on the basis of satisfying the i - vector model. separately we report the baseline sample results evaluated in various metrics defined by the ap16 - olr evaluation plan and demonstrate that producing ap16 - ol7 is a reasonable cognitive data resource for multilingual research.", "histories": [["v1", "Tue, 27 Sep 2016 13:50:13 GMT  (664kb,D)", "http://arxiv.org/abs/1609.08445v1", "APSIPA ASC 2016"]], "COMMENTS": "APSIPA ASC 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["dong wang", "lantian li", "difei tang", "qing chen"], "accepted": false, "id": "1609.08445"}, "pdf": {"name": "1609.08445.pdf", "metadata": {"source": "CRF", "title": "AP16-OL7: A Multilingual Database for Oriental Languages and A Language Recognition Baseline", "authors": ["Dong Wang", "Lantian Li", "Difei Tang", "Qing Chen"], "emails": ["wangdong99@mails.tsinghua.edu.cn", "lilt13@mails.tsinghua.edu.cn", "tangdifei@speechocean.com", "chenqing@speechocean.com"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nOriental languages, including various languages spoken in east, northeast and southeast Asia, belong to several language families, including Austroasiatic languages (e.g.,Vietnamese, Cambodia ) [1], TaiKadai languages (e.g., Thai, Lao), HmongMien languages (e.g., some dialects in south China), SinoTibetan languages (e.g., Chinese Mandarin), Altaic languages (e.g., Korea, Japanese), Indo-European languages (e.g., Russian) [2], [3], [4]. These languages were generally believed to be genetically unrelated and were developed from diverse cultures. However, they do share many features due to the demographic migration and international business interaction in history. For example, many languages in the so-called Mainland Southeast Asia (MSEA) linguistic area posses a particular syllable structure that involves monosyllabic morphemes, lexical tone, a fairly large inventory of consonants [5]. Another example is the significant influence of Chinese to Korean, Japanese, Vietnamese and many languages in southeast Asia. In the modern period, English becomes the most influential language, resulting in numerous English-originated words in almost all oriental languages.\nThe complex acoustic and linguistic patterns of oriental languages have attracted much interest in a multitude of research areas, including comparative phonetics, evolutionary linguistics, second language acquisition, and social linguistics. In particular, the diverse evolution paths of these languages and their complicated interaction offers a valuable opportunity for studying mixlingual and multilingual phenomena.\nDespite the broad interest, data resources of oriental languages are far from abundant. One possible reason is that many of these languages are spoken by a relatively small population, and most of the speakers are in developing countries.\nSome effort has been devoted to building data resources for oriental languages, e.g., the annual oriental COCOSDA (OC) workshop intends to promote speech and language resource construction for oriental languages, and the transactions on Asian and Low-Resource Language Information Processing (TALLIP) journal calls for original research on oriental languages, especially languages with limited resources.1 Some projects, e.g., the Babel program2, although not particularly for oriental languages, do involve Vietnamese, Thais, Lao and some other low-resource languages in southeast Asia. In spite of these efforts, resource construction and corresponding research on oriental languages are still rather limited, except one or two rich-resource languages, such as Chinese and Japanese.\nTo promote research for oriental languages, particularly on multilingual speech and language processing, the center for speech and language technologies (CSLT) at Tsinghua University and Speechocean collaborated together and organized an oriental language recognition (OLR) challenge on APSIPA 2016. This event called for a competition on a language recognition task on seven oriental languages. To support this event, Speechocean released a multilingual speech database AP16-OL7 and made it free for the challenge participants. This paper will present the data profile of the database, the evaluation rules of the challenge, and a baseline system that the participants can refer to.\nNote that there are several databases that can be used for multilingual research. For example, polyphone [6], globalPhone [7], NTT multilingual database3, SPEECHDATCAR [8],Speechdat-E [9], Babel [10], and the multilingual databases created by the new Babel project. To our best knowledge, AP16-OL7 is the first multilingual speech database specifically designed for oriental languages."}, {"heading": "II. DATABASE PROFILE", "text": "The AP16-OL7 database was originally created by Speechocean targeting for various speech processing tasks (mainly\n1https://mc.manuscriptcentral.com/tallip 2https://www.iarpa.gov/index.php/research-programs/babel 3http://www.ntt-at.com/product/speech2002/\nar X\niv :1\n60 9.\n08 44\n5v 1\n[ cs\n.C L\n] 2\n7 Se\np 20\n16\nspeech recognition). The entire database involves seven datasets, each in a particular language. The seven languages are: Mandarin, Cantonese, Indoesian, Japanese, Russian, Korean, Vietnamese. The data volume for each language is about 10 hours of speech signals recorded by 24 speakers (12 males and 12 females), and each speaker recorded about 300 utterances in reading style. The signals were recorded by mobile phones, with a sampling rate of 16kHz and a sample size of 16 bits. Each dataset was split into a training set consisting of 18 speakers, and a test set consisting of 6 speakers. For Mandarin, Cantonese, Vietnamese and Indonesia, the recording was conducted in a quiet environment. As for Russian, Korean and Japanese, there are 2 recording sessions for each speaker: the first session was recorded in a quiet environment and the second was recorded in a noisy environment. The basic information of the AP16-OL7 database is presented in Table I.\nBesides the speech signals, the AP16-OL7 database also provides lexicons of all the seven languages, and transcriptions of all the training utterances. These resources allow training acoustic-based or phonetic-based language recognition systems. Training phone-based speech recognition systems is also possible, though large vocabulary recognition systems are not well supported, due to the lack of large-scale language models.\nThe AP16-OL7 database is freely available for the participants of the AP16-OLR challenge and the APSIPA 2016 special session on multilingual speech and language processing. It is also available for any academic and industrial users, subject to a slightly different licence from SpeechOcean.4"}, {"heading": "III. AP16-OLR CHALLENGE", "text": "Based on the AP16-OL7 database, we call an oriental language recognition (OLR) challenge.5 Following the definition of NIST LRE15 [11], the task of the challenge is defined as follows: Given a segment of speech and a language hypothesis (i.e., a target language of interest to be detected), the task is to decide whether that target language was in fact spoken in the given segment (yes or no), based on an automated an analysis of the data contained in the segment. The AP16-OLR evaluation plan also follows the principles of NIST LRE15:\n4http://speechocean.com 5http://cslt.riit.tsinghua.edu.cn/mediawiki/index.php/\nASR-events-AP16-details\nit focuses on the close-set condition, and allows no additional training materials besides AP16-OL7. The evaluation details are described as follows."}, {"heading": "A. System input/output", "text": "The input to the OLR system is a set of speech segments in unknown languages (but within the 7 languages of AP16OL7). The task of the OLR system is to determine the confidence that a language is contained in a speech segment. More specifically, for each speech segment, the OLR system outputs a score vector < `1, `2, ..., `7 >, where `i represents the confidence that language i is spoken in the speech segment. Each score `i will be interpreted as follows: if `i \u2265 0, then the decision would be that language i is contained in the segment, otherwise it is not. The scores should be comparable across languages and segments. This is consistent with the principle of LRE15, but differs from that of LRE09 [12] where an explicit decision is required for each trial.\nIn summary, the output of an OLR submission will be a text file, where each line contains a speech segment plus a score vector for this segment, e.g.,\nseg1 0.5 -0.2 -0.3 0.1 -9.2 -0.1 -5.1 seg2 -0.1 -0.3 0.5 0.3 -0.5 -0.9 -3.2\n... ..."}, {"heading": "B. Test condition", "text": "\u2022 No additional training materials are allowed to use. \u2022 All the trials should be processed. Scores of lost trials\nwill be interpreted as -inf . \u2022 Each test segment should be processed independently.\nKnowledge from other test segments is not allowed to use (e.g., score distribution of all the test segments).\n\u2022 Information of speakers is not allowed to use. \u2022 Listening to any speech segments is not allowed."}, {"heading": "C. Evaluation metrics", "text": "As in LRE15, the AP16-OLR challenge chooses Cavg as the principle evaluation metric. First define the pair-wise loss that composes the missing and false alarm probabilities for a particular target/non-target language pair:\nC(Lt, Ln) = PTargetPMiss(Lt)+ (1\u2212PTarget)PFA(Lt, Ln)\nwhere Lt and Ln are the target and non-target languages, respectively; PMiss and PFA are the missing and false alarm probabilities, respectively. Ptarget is the prior probability for the target language, which is set to 0.5 in the evaluation. Then the principle metric Cavg is defined as the average of the above pair-wise performance:\nCavg = 1\nN \u2211 Lt  PTarget \u00b7 PMiss(Lt) + \u2211 Ln PNon\u2212Target \u00b7 PFA(Lt, Ln)  where N is the number of languages, and PNon\u2212Target = (1\u2212 PTarget)/(N \u2212 1)."}, {"heading": "IV. BASELINE RESULTS", "text": "We present baseline language recognition systems based on the i-vector model, and evaluate the performance in terms of the metrics defined by the AP16-OLR challenge. The purpose of these experiments is not to present a competitive submission, instead to demonstrate that the AP16-OL7 database is a reasonable data resource to conduct language recognition research."}, {"heading": "A. Experimental setup", "text": "The baseline system was constructed based on the i-vector model [13], [14]. The static acoustic features involved 19- dimensional Mel frequency cepstral coefficients (MFCCs) and the log energy. This static features were augmented by their first and second order derivatives, resulting in 60-dimensional feature vectors. The UBM involved 2, 048 Gaussian components and the dimensionality of the i-vectors was 400. Linear discriminative analysis (LDA) was employed to promote language-related information. The dimensionality of the LDA projection space was set to 6.\nWith the i-vectors (either original or after LDA transform), the score of a trail on a particular language can be simply computed as the cosine distance between the test i-vector and the mean i-vector of the training segments that belong to that language. This is denoted to be \u2018cosine distance scoring\u2019. A more powerful scoring approach is to employ various discriminative models. In our experiment, we trained a support vector machine (SVM) for each language to determine the score that a test i-vector belongs to that language. The SVMs were trained on the i-vectors of all the training segments, following the one-verse-rest scheme. We will call this scoring approach as \u2018SVM-based scoring\u2019.\nB. Visualization with T-SNE [15]\nTo provide an intuitive understanding of the discriminative capability of i-vectors on languages, the i-vectors of all the segments in the test set are plotted in a two-dimensional space via T-SNE [15]. Fig. 1 shows the original i-vectors, and Fig. 2 shows the i-vectors after LDA transform, where each color/shap represents a particular language. It can be seen that for the original i-vectors, each language is split into several clusters basically due to different speakers. After\nLDA transformation, speaker information is suppressed and the language identify is more significant."}, {"heading": "C. Performance results", "text": "The primary evaluation metric in AP16-OLR is Cavg . Besides that, we also present the performance in terms of equal error rate (EER), minimum detection cost function (minDCF), detection error tradeoff (DET) curve, and identification rate (IDR). These metrics evaluate the system from different perspectives, offering a whole picture of the verification/identification capability of the baseline system.\n1) Cavg results: The Cavg results are shown in Table II. The rows \u2018i-vector\u2019 and \u2018L-vector\u2019 present the results with the cosine distance scoring; \u2018i-vector-SVM\u2019 and \u2018L-vecotr-SVM\u2019 present the results with the SVM-based scoring. \u2018Linear\u2019, \u2018Poly\u2019(degree=3), and \u2018RBF\u2019 represent the three commonly used kernel functions. It can be seen that LDA leads to consistent performance gains, and the SVM-based scoring tends to outperform cosine distance scoring.\n2) EER and minDCF results: EER and minDCF are also widely used in measuring performance of verification systems. Compared to Cavg, these two metrics are not related to the decision result, but the quality of the scoring, and therefore evaluate the verification system from a different angle. The results for these two metrics are presented in Table II. respectively. It can be seen that similar conclusions can be drawn from these results as from the Cavg results."}, {"heading": "D. DET curve", "text": "The DET curve is another popular way to evaluate verification systems. Compared to Cavg , EER and minDCF, the DET curve presents performance on all operation points, and therefore can evaluate a verification system in a more systematic way. Experimental results are shown in Fig 3. The black circles represent the operation location where the minDCFs are obtained. Again, similar conclusions as with the Cavg, EER and minDCF can be obtained.\n1) IDR results: Note that in the OLR challenge, the target languages are known in prior, and the confidence scores are comparable across languages. This means that OLR can be treated as a language identification task, for which the language obtaining the highest score in a trail is regarded as the identification result. For such an identification task, IDR is a widely used metric [16], which treats errors on all languages equally serious. IDR is formally defined as follows:\nIDR = Tc\nTc + Ti\nwhere Tc and Ti are the numbers of correctly and incorrectly identified utterances, respectively. Table II presents the IDR results of the baseline system. We can observe similar trends as with the verification metrics: Cavg , EER, minDCF and DET curve."}, {"heading": "V. CONCLUSIONS", "text": "We presented the data profile of the AP16-OL7 database that was released to support the AP16-OLR challenge on APSIPA 2016. The evaluation rules of the challenge was described, and a baseline system was presented. We show that the AP16-OL7 database is a suitable data resource for language recognition research."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Science Foundation of China (NSFC) under the project No. 61371136, and the MESTDC PhD Foundation Project No. 20130002120011. It was also supported by SpeechOcean."}], "references": [{"title": "The languages of China", "author": ["S.R. Ramsey"], "venue": "Princeton University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1987}, {"title": "The languages of Japan", "author": ["M. Shibatani"], "venue": "Cambridge University Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "The Russian language in the twentieth century", "author": ["B. Comrie", "G. Stone", "M. Polinsky"], "venue": "Oxford University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Areal linguistics and mainland southeast asia,", "author": ["N.J. Enfield"], "venue": "Annual Review of Anthropology,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Multilingual speech databases at ldc,", "author": ["J.J. Godfrey"], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Globalphone: a multilingual speech and text database developed at karlsruhe university.", "author": ["T. Schultz"], "venue": "in INTERSPEECH,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Speechdat-car", "author": ["A. Moreno", "B. Lindberg", "C. Draxler", "G. Richard", "K. Choukri", "S. Euler", "J. Allen"], "venue": "a large speech database for automotive environments.\u201d in LREC", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Speechdat-e: five eastern european speech databases for voice-operated teleservices completed.", "author": ["H. van den Heuvel", "J. Boudy", "Z. Bakcsi", "J. Cernock\u1ef3", "V. Galunov", "J. Kochanina", "W. Majewski", "P. Pollak", "M. Rusko", "J. Sadowski"], "venue": "INTERSPEECH,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "L", "author": ["P. Roach", "S. Arnfield", "W.J. Barry", "J. Baltova", "M. Boldea", "A. Fourcin", "W. Gonet", "R. Gubrynowicz", "E. Hallum"], "venue": "Lamel et al., \u201cBabel: an eastern european multi-language database.\u201d in ICSLP, vol. 96", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "and P", "author": ["N. Dehak", "P.G. Kenny", "R. Dehak", "P. Dumouchel"], "venue": "Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013 798", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "and R", "author": ["N. Dehak", "P.A. Torres-Carrasquillo", "D.A. Reynolds"], "venue": "Dehak, \u201cLanguage recognition via i-vectors and dimensionality reduction,\u201d in INTERSPEECH", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-sne,", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "and F", "author": ["B. Yin", "E. Ambikairajah"], "venue": "Chen, \u201cHierarchical language identification based on automatic language clustering.\u201d in INTERSPEECH", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", Russian) [2], [3], [4].", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": ", Russian) [2], [3], [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": ", Russian) [2], [3], [4].", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "For example, many languages in the so-called Mainland Southeast Asia (MSEA) linguistic area posses a particular syllable structure that involves monosyllabic morphemes, lexical tone, a fairly large inventory of consonants [5].", "startOffset": 222, "endOffset": 225}, {"referenceID": 4, "context": "For example, polyphone [6], globalPhone [7], NTT multilingual database3, SPEECHDATCAR [8],Speechdat-E [9], Babel [10], and the multilingual databases created by the new Babel project.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "For example, polyphone [6], globalPhone [7], NTT multilingual database3, SPEECHDATCAR [8],Speechdat-E [9], Babel [10], and the multilingual databases created by the new Babel project.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "For example, polyphone [6], globalPhone [7], NTT multilingual database3, SPEECHDATCAR [8],Speechdat-E [9], Babel [10], and the multilingual databases created by the new Babel project.", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "For example, polyphone [6], globalPhone [7], NTT multilingual database3, SPEECHDATCAR [8],Speechdat-E [9], Babel [10], and the multilingual databases created by the new Babel project.", "startOffset": 102, "endOffset": 105}, {"referenceID": 8, "context": "For example, polyphone [6], globalPhone [7], NTT multilingual database3, SPEECHDATCAR [8],Speechdat-E [9], Babel [10], and the multilingual databases created by the new Babel project.", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "The baseline system was constructed based on the i-vector model [13], [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "The baseline system was constructed based on the i-vector model [13], [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "Visualization with T-SNE [15]", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "To provide an intuitive understanding of the discriminative capability of i-vectors on languages, the i-vectors of all the segments in the test set are plotted in a two-dimensional space via T-SNE [15].", "startOffset": 197, "endOffset": 201}, {"referenceID": 12, "context": "For such an identification task, IDR is a widely used metric [16], which treats errors on all languages equally serious.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research.", "creator": "LaTeX with hyperref package"}}}