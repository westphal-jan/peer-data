{"id": "1609.08824", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Equation Parsing: Mapping Sentences to Grounded Equations", "abstract": "knowledge identifying mathematical relations expressed in text is essential to everyone understanding a broad range of natural language text from electronic election reports, to financial news, to sport commentaries to mathematical variable word problems. this paper focuses on identifying and understanding various mathematical relations described within a single sentence. formally we introduce the problem of equation parsing - - given a sentence, identify noun character phrases parameters which represent variables, and generate the mathematical equation expressing the relation described in the sentence. meanwhile we introduce clearly the simplest notion geometry of projective equation parsing and provide an efficient algorithm to parse similar text to projective equations. our analytic system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured interval predictors, and generates correct equations in $ 70 \\ % $ of the cases. in $ 2 60 \\ % $ of the time, it also identifies the highest correct noun phrase $ \\ rightarrow $ variables mapping, significantly outperforming baselines. we also release a new annotated mesh dataset for task evaluation.", "histories": [["v1", "Wed, 28 Sep 2016 08:54:05 GMT  (190kb)", "http://arxiv.org/abs/1609.08824v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["subhro roy", "shyam upadhyay", "dan roth"], "accepted": true, "id": "1609.08824"}, "pdf": {"name": "1609.08824.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["danr}@illinois.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n08 82\n4v 1\n[ cs\n.C L\n] 2\n8 Se\nIdentifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing \u2013 given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in 70% of the cases. In 60% of the time, it also identifies the correct noun phrase \u2192 variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation."}, {"heading": "1 Introduction", "text": "Understanding text often involves reasoning with respect to quantities mentioned in it. Understanding the news article statement in Example 1 requires identifying relevant entities and the mathematical relations expressed among them in text, and determining how to compose them. Similarly, solving a math word problem with a sentence like Example 2, requires realizing that it deals with a single number, knowing the meaning of \u201cdifference\u201d and compos-\nExample 1 Emanuel\u2019s campaign contributions total three times those of his opponents put together. Example 2 Twice a number equals 25 less than triple the same number. Example 3 Flying with the wind , a bird was able to make 150 kilometers per hour. Example 4 The sum of two numbers is 80. Example 5 There are 54 5-dollar and 10- dollar notes.\ning the right equation \u2013 \u201c25\u201d needs to be subtracted from a number only after it is multiplied by 3.\nAs a first step towards understanding such relations, we introduce the Equation Parsing task - given a sentence expressing a mathematical relation, the goal is to generate an equation representing the relation, and to map the variables in the equation to their corresponding noun phrases. To keep the problem tractable, in this paper we restrict the final output equation form to have at most two (possibly coreferent) variables, and assume that each quantity mentioned in the sentence can be used at most once in the final equation.1 In example 1, the gold output of an equation parse should be V1 = 3 \u00d7 V2, with V1 = \u201cEmanuel\u2019s campaign contributions\u201d and V2 = \u201cthose of his opponents put together\u201d.\nThe task can be seen as a form of semantic parsing (Goldwasser and Roth, 2011; Kwiatkowski et al., 2013) where instead of mapping a sentence to a logical form, we want to map\n1We empirically found that around 97% of sentences describing a relation have this property.\nit to an equation. However, there are some key differences that make this problem very challenging in ways that differ from the \u201cstandard\u201d semantic parsing. In Equation Parsing, not all the components of the sentence are mapped to the final equation. There is a need to identify noun phrases that correspond to variables in the relations and determine that some are irrelevant and can be dropped. Moreover, in difference from semantic parsing into logical forms, in Equation Parsing multiple phrases in the text could correspond to the same variable, and identical phrases in the text could correspond to multiple variables.\nWe call the problem of mapping noun phrases to variables the problem of grounding variables. Grounding is challenging for various reasons, key among them are that: (i) The text often does not mention \u201cvariables\u201d explicitly, e.g., the sentence in example 3 describes a mathematical relation between the speed of bird and the speed of wind, without mentioning \u201cspeed\u201d explicitly. (ii) Sometimes, multiple noun phrases could refer to the same variable. For instance, in example 2, both \u201ca number\u201d and \u201cthe same number\u201d refer to the same variable. On the other hand, the same noun phrase might refer to multiple variables, as in example 4, where the noun phrase \u201ctwo numbers\u201d refer to two variables.\nIn addition, the task involves deciding which of the quantities identified in the sentence are relevant to the final equation generation. In example 5, both \u201c5\u201d and \u201c10\u201d are not relevant for the final equation \u201cV1 + V2 = 54\u201d. Finally, the equation needs to be constructed from a list of relevant quantities and grounded variables. Overall, the output space becomes exponential in the number of quantities mentioned in the sentence.\nDetermining the final equation that corresponds to the text is an inference step over a very large space. To address this, we define the concept of \u201cprojectivity\u201d - a condition where the final equation can be generated by combining adjacent numbers or variables, and show that most sentences expressing mathematical relations exhibit the projectivity property. Finally, we restrict our inference procedure to only search over equations which have this property.\nOur approach builds on a pipeline of structured predictors that identify irrelevant quantities, recognize coreferent variables, and, finally, generate equa-\ntions. We also leverage a high precision lexicon of mathematical expressions and develop a greedy lexicon matching strategy to guide inference. We discuss and exemplify the advantages of this approach and, in particular, explain where the \u201cstandard\u201d NLP pipeline fails to support equation parsing, and necessitates the new approach proposed here. Another contribution of this work is the development of a new annotated data set for the task of equation parsing. We evaluate our method on this dataset and show that our method predicts the correct equation in 70% of the cases and that in 60% of the time we also ground all variables correctly.\nThe next section presents a discussion of related work. Next we formally describe the task of equation parsing. The following sections describe our equation representation and the concept of projectivity, followed by the description of our algorithm to generate the equations and variable groundings from text. We conclude with experimental results."}, {"heading": "2 Related Work", "text": "The work most related to this paper is (Madaan et al., 2016), which focuses on extracting relation triples where one of the arguments is a number. In contrast, our work deals with multiple variables and complex equations involving them. There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015). These solvers cannot handle sentences individually. They require the input to be a complete math word problem, and even then, they only focus on retrieving a set of answer values without mentioning what each answer value corresponds to. Our work is also conceptually related to work on semantic parsing \u2013 mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011). However, as mentioned earlier, there are some significant differences in the task definition that necessitate the development of a new approach."}, {"heading": "3 The Equation Parsing Task", "text": "Equation parsing takes as input a sentence x describing a single mathematical equation, comprising one or two variables and other quantities mentioned in x. Let N be the set of noun phrases in the sentence x. The output of the task is the mathematical equation described in x, along with a mapping of each variable in the equation to its corresponding noun phrase in N . We refer to this mapping as the \u201cgrounding\u201d of the variable; the noun phrase represents what the variable stands for in the equation. Table 1 gives an example of an input and output for the equation parsing of the text in example 2. Since an equation can be written in various forms, we use the form which most agrees with text, as our target output. So, for example 1, we will choose V1 = 3 \u00d7 V2 and not V2 = V1 \u00f7 3. In cases where several equation forms seem to be equally likely to be the target equation, we randomly choose one of them, and keep this choice consistent across the dataset."}, {"heading": "3.1 Equation Parse Representation", "text": "In this section, we introduce an equation parse for a sentence. An equation parse of a sentence x is a pair (T,E), where T represents a set of triggers extracted from x, and E represents an equation tree formed with the set T as leaves. We now describe these terms in detail. Trigger Given a sentence x mentioning a mathematical relation, a trigger can either be a quantity trigger expressed in x, or variable trigger which is a noun phrase in x corresponding to a variable. A quantity trigger is a tuple (q, s), where q is the numeric value of the quantity mentioned in text, and s is the span of text from the sentence x which refers to the quantity. A variable trigger is a tuple (l, s), where l represents the label of the variable, and s represents the noun phrase representing the variable. For example, for the sentence in Fig 1, the spans \u201cTwice\u201d, \u201c25\u201d, and \u201ctriple\u201d generate quantity triggers, whereas \u201ca number\u201d and \u201cthe same number\u201d generate variable\ntriggers, with label V1. Trigger List The trigger list T for a sentence x contains one trigger for each variable mention and each numeric value used in the final equation expressed by the sentence x. The trigger list might consist of multiple triggers having the same label, or extracted from the same span of text. In the example sentence in Fig 1, the trigger list comprises two triggers having the same label V1. The final trigger list for the example in Fig 1 is {(2, \u201c2\u201d), (V1, \u201ca number\u201d), (25, \u201c25\u201d), (3, \u201ctriple\u201d), (V1, \u201cthe same number\u201d)}. Note that there can be multiple valid trigger lists. In our example, we could have chosen both variable triggers to point to the same mention \u201ca number\u201d. Quantity triggers in the trigger list form the quantity trigger list, and the variable triggers in trigger list form the variable trigger list. Equation Tree An equation tree of a sentence x is a binary tree whose leaves constitute the trigger list of x, and internal nodes (except the root) are labeled with one of the following operations \u2013 addition, subtraction, multiplication, division. In addition, for nodes which are labeled with subtraction or division, we maintain a separate variable to determine order of its children. The root of the tree is always labeled with the operation equal.\nAn equation tree is a natural representation for an equation. Each node n in an equation tree repre-\nsents an expression EXPR(n), and the label of the parent node determines how the expressions of its children are to be composed to construct its own expression. Let us denote the label for a non-leaf node n to be \u2299(n), where \u2299(n) \u2208 {+,\u2212,\u00d7,\u00f7,=} and the order of a node n\u2019s children by ORDER(n) (defined only for subtraction and division nodes), which takes values lr (Left-Right) or rl (Right-Left). For a leaf node n, the expression EXPR(n) represents the variable label, if n is a variable trigger, and the numeric value of the quantity, if it is a quantity trigger. Finally, we use lc(n) and rc(n) to represent the left and right child of node n, respectively. The equation represented by the tree can be generated as follows. For all non-leaf nodes n, we have\nEXPR(n) = \n         \n         \nEXPR(lc(n))\u2299(n) EXPR(rc(n))\nif \u2299(n) \u2208 {+,\u00d7,=}\nEXPR(lc(n))\u2299(n) EXPR(rc(n))\nif \u2299(n) \u2208 {\u2212,\u00f7} \u2227 ORDER(n) = lr\nEXPR(rc(n))\u2299(n) EXPR(lc(n))\nif \u2299(n) \u2208 {\u2212,\u00f7} \u2227 ORDER(n) = rl\n(1)\nGiven an equation tree T of a sentence, the equation represented by it is the expression generated by the root of T (following Equation 1). Referring to the equation tree in Fig 1, the node marked \u201c\u2212r\u201d represents (3\u00d7 V1) \u2212 25, and the root represents the full equation 2\u00d7 V1 = (3\u00d7 V1)\u2212 25."}, {"heading": "4 Projectivity", "text": "For each leaf n of an equation tree T , we define a function Location(\u00b7), to indicate the position of the corresponding trigger in text. We also\ndefine for each node n of equation tree T , functions Span-Start(n) and Span-End(n) to denote the minimum span of text containing the leaves of the subtree rooted at n. We define them as follows:\nSpan-Start(n) = \n \n \nLocation(n) if n is a leaf\nmin(Span-Start(lc(n)),Span-Start(rc(n)))\notherwise\n(2)\nSpan-End(n) = \n \n \nLocation(n) if n is a leaf\nmax(Span-End(lc(n)),Span-End(rc(n)))\notherwise\n(3)\nAn equation tree T is called projective iff for every node n of T , either Span-End(lc(n)) \u2264 Span-Start(rc(n)) or Span-End(rc(n)) \u2264 Span-Start(lc(n)). In other words, the span of the left child and the right child cannot intersect in a projective equation tree2.\nThe key observation, as our corpus analysis indicates, is that for most sentences, there exists a trigger list, such that the equation tree representing the relation in the sentence is projective. However this might involve mapping two mentions of the same variable to different noun phrases. Figure 1 shows an example of a projective equation tree, which requires different mentions of V1 to be mapped to different noun phrases. If we had mapped both mentions of V1 to same noun phrase \u201ca number\u201d, the resulting equation tree would not have been projective. We collected 385 sentences which represent\n2This is more general than the definition of projective trees used in dependency parsing (McDonald et al., 2005).\nan equation with one or two mentions of variables, and each number in the sentence used at most once in the equation. We found that only one sentence among these could not generate a projective equation tree. (See Section 6.1 for details on dataset creation). Therefore, we develop an algorithmic approach for predicting projective equation trees, and show empirically that it compares favourably with ones which do not make the projective assumption."}, {"heading": "5 Predicting Equation Parse", "text": "Equation parsing of a sentence involves predicting three components \u2013 Quantity Trigger List, Variable Trigger List and Equation Tree. We develop three structured prediction modules to predict each of the above components.\nAll our prediction modules take a similar form: given input x and output y, we learn a scoring function fw(x, y), which scores how likely is the output y given input x. The scoring function fw(x, y) is linear, fw(y) = wT\u03c6(x, y), where \u03c6(x, y) is a feature vector extracted from x and y. The inference problem, that is, the prediction y\u2217 for an input x is then: y\u2217 = argmaxy\u2208Y fw(y), where Y is the set of all allowed values of y."}, {"heading": "5.1 Predicting Quantity Trigger List", "text": "Given input text and the quantities mentioned in it, the role of this step is to identify , for each quantity in the text, whether it should be part of the final equation. For instance, in example 5 in Section 1, both \u201c5\u201d and \u201c10\u201d are not relevant for the final equation \u201cV1 + V2 = 54\u201d. Similarly, in example 4, the number \u201ctwo\u201d is irrelevant for the equation \u201cV1 + V2 = 80\u201d.\nWe define for each quantity q in the sentence, a boolean value Relevance(q), which is set to true if q is relevant for the final equation, and to false otherwise. For the structured classification, the input x is the sentence along with a set of recognized quantities mentioned in it, and the output y is the relevance values for all quantities in the sentence. We empirically found that predicting all relevance values jointly performs better than having a binary classifier predict each one separately. The feature function \u03c6(x, y) used for the classification generates neighborhood features (from neighborhood of\nq) and quantity features (properties of the quantity mention). Details added to the appendix."}, {"heading": "5.2 Predicting Variable Trigger List", "text": "The goal of this step is to predict the variable trigger list for the equation. Our structured classifier takes as input the sentence x, and the output y is either one or two noun-phrases, representing variables in the final equation. As we pointed out earlier, multiple groundings might be valid for any given variable, hence there can be multiple valid variable trigger lists. For every sentence x, we construct a set Y of valid outputs. Each element in Y corresponds to a valid variable trigger list. Finally, we aim to output only one of the elements of Y .\nWe modified the standard structured prediction algorithm to consider \u201csuperset supervision\u201d and take into account multiple gold structures for an input x. We assume access to N training examples of the form : (x1, Y1), (x2, Y2), . . . , (xN , YN ), where each Yi is a set of valid outputs for the sentence xi. Since we want to output only one variable trigger list, we want to score at least one y from Yi higher than all other possible outputs, for each xi. We use a modified latent structured SVM to learn the weight vector w. The algorithm treats the best choice among all of Yi as a latent variable. At each iteration, for all xi, the algorithm chooses the best choice y\u2217i from the set Yi, according to the weight vector w. Then, w is updated by learning on all (xi, y\u2217i ) by a standard structured SVM algorithm. The details of the algorithm are in Algorithm 1. The distinction from standard latent structural SVM is in line 5 of Algorithm 1. In order to get the best choice y\u2217i for input xi, we search only inside Yi, instead of all of Y . A similar formulation can be found in Bjo\u0308rkelund and Kuhn (2014). The features \u03c6(x, y) used for variable trigger prediction include variable features (properties of noun phrase indicating variable) and neighborhood features (lexical features from neighborhood of variable mention). Details added to the appendix.\nIf the output of the classifier is a pair of noun phrases, we use a rule based variable coreference detector, to determine whether both noun phrases should have the same variable label or not. The rules for variable coreference are as follows :\n1. If both noun phrases are the same, and they do\nAlgorithm 1 Structural SVM with Superset Supervision Input: Training data T =\n{(x1, Y1), (x2, Y2), . . . , (xN , YN )} Output: Trained weight vector w\n1: w \u2190 w0 2: repeat 3: T \u2032 \u2190 \u2205 4: for all (xi, Yi) \u2208 T do 5: y\u2217i \u2190 argmaxy\u2208Yi w\nT\u03c6(xi, y) 6: T \u2032 \u2190 T \u2032 \u222a {(xi, y \u2217 i )} 7: end for 8: Update w by running standard Structural\nSVM algorithm on T \u2032\n9: until convergence 10: return w\nnot have the token \u201ctwo\u201d or \u201c2\u201d, they have the same label.\n2. If the noun phrases are different, and the noun phrase appearing later in the sentence contains tokens \u201citself\u201d, \u201cthe same number\u201d, they have the same label.\n3. In all other cases, they have different labels.\nFinally, each noun phrase contributes one variable trigger to the variable trigger list."}, {"heading": "5.3 Predicting Equation Tree", "text": "It is natural to assume that the syntactic parse of the sentence could be very useful in addressing all the predictions we are making in the equation parsing tasks. However, it turns out that this is not the case \u2013 large portions of the syntactic parse will not be part of the equation parse, hence we need the aforementioned modules to address this. Nevertheless, in the next task of predicting the equation tree, we attempted to constraint the output space using guidance from the syntactic tree; we found, though, that even enforcing this weak level of output expectation is not productive. This was due to the poor performance of current syntactic parsers on the equation data (eg., in 32% of sentences, the Stanford parser made a mistake which does not allow recovering the correct equation).\nThe tree prediction module receives the trigger list predicted by the previous two modules, and the goal\nis to create an equation tree using the trigger list as the leaves of that tree. The input x is the sentence and the trigger list, and the output y is the equation tree representing the relation described in the sentence. We assume that the output will be a projective equation tree. For features \u03c6(x, y), we extract for each non-leaf node n of the equation tree y, neighborhood features (from neighborhood of node spans of n\u2019s children), connecting text features (from text between the spans of n\u2019s children) and number features (properties of number in case of leaf nodes). Details are included in the appendix.\nThe projectivity assumption implies that the final equation tree can be generated by combining only adjacent nodes, once the set of leaves is sorted based on Span-Start(\u00b7) values. This allows us to use CKY algorithm for inference. A natural approach to further reduce the output space is to conform to the projective structure of the syntactic parse of the sentence. However, we found this to adversely affect performance, due to the poor performance of syntactic parser on equation data. Lexicon To bootstrap the equation parsing process, we developed a high precision lexicon to translate mathematical expressions to operations and orders, like \u201csum of A and B\u201d translates to \u201cA+B\u201d, \u201cA minus B\u201d translates to \u201cA-B\u201d, etc. (where A and B denote placeholder numbers or expressions). At each step of CKY, while constructing a node n of the equation tree, we check for a lexicon text expression corresponding to node n. If found, we allow only the corresponding operation (and order) for node n, and do not explore other operations or orders. We show empirically that reducing the space using this greedy lexicon matching help improve performance. We found that using the lexicon rules as features instead of hard constraints do not help as much. Note that our lexicon comprises only generic math concepts, and around 50% of the sentences in our dataset do not contain any pattern from the lexicon.\nFinally, given input sentence, we first predict the quantity trigger and the variable trigger lists. Given the complete trigger list, we predict the equation tree relating the components of the trigger list."}, {"heading": "5.4 Alternatives", "text": "A natural approach could be to jointly learn to predict all three components, to capture the dependen-\ncies among them. To investigate this, we developed a structured SVM which predicts all components jointly, using the union of the features of each component. We use approximate inference, first enumerating possible trigger lists, and then equation trees, and find the best scoring structure. However, this method did not outperform the pipeline method. The worse performance of joint learning is due to: (1) search space being too large for the joint model to do well given our dataset size of 385, and (2) our independent classifiers being good enough, thus supporting better joint inference. This tradeoff is strongly supported in the literature (Punyakanok et al., 2005; Sutton and McCallum, 2007).\nAnother option is to enforce constraints between trigger list predictions, such as, variable triggers should not overlap with the quantity triggers. However, we noticed that often noun phrases returned by the Stanford parser were noisy, and would include neighboring numbers within the extracted noun phrases. This prevented us from enforcing such constraints."}, {"heading": "6 Experimental Results", "text": "We now describe the data set, and the annotation procedure used. We then evaluate the system\u2019s performance on predicting trigger list, equation tree, and the complete equation parse."}, {"heading": "6.1 Dataset", "text": "We created a new dataset consisting of 385 sentences extracted from algebra word problems and financial news headlines. For algebra word problems, we used the MIT dataset (Kushman et al., 2014), and two high school mathematics textbooks, Elementary Algebra (College of Redwoods) and Beginning and Intermediate Algebra (Tyler Wallace). Financial news headlines were extracted from The Latest News feed of MarketWatch, over the month of February, 2015. All sentences with information describing a mathematical relation among at most two (possibly coreferent) variables, were chosen. Next, we pruned sentences which require multiple uses of a number to create the equation. This only removed a few time related sentences like \u201cIn 10 years, John will be twice as old as his son.\u201d. We empirically found that around 97% of sentences describing a re-\nlation fall under the scope of our dataset. The annotators were shown each sentence paired with the normalized equation representing the relation in the sentence. For each variable in the equation, the annotators were asked to mark spans of text which best describe what the variable represents. The annotation guidelines are provided in the appendix. We wanted to consider only noun phrase constituents for variable grounding. Therefore, for each annotated span, we extracted the noun phrase with maximum overlap with the span, and used it to represent the variables. Finally, a tuple with each variable being mapped to one of the noun phrases representing it, forms a valid output grounding (variable trigger list). We computed interannotator agreement on the final annotations where only noun phrases represent variables. The agreement (kappa) was 0.668, indicating good agreement. The average number of mention annotations per sentence was 1.74."}, {"heading": "6.2 Equation Parsing Modules", "text": "In this section, we evaluate the performance of the individual modules of the equation parsing process. We report Accuracy - the fraction of correct predictions. Table 3 shows the 5-fold cross validation accuracy of the various modules. In each case, we also report accuracy by removing each feature group, one at a time. In addition, for equation tree prediction, we also show the effect of lexicon, projectivity, conforming to syntactic parse constraints, and using lexicon as features instead of hard constraints. For all our experiments, we use the Stanford Parser (Socher et al., 2013), the Illinois POS tagger (Roth and Zelenko, 1998) and the Illinois-SL structured prediction package (Chang et al., 2015)."}, {"heading": "6.3 Equation Parsing Results", "text": "In this section, we evaluate the performance of our system on the overall equation parsing task. We report Equation Accuracy - the fraction of sentences for which the system got the equation correct, and Equation+Grounding Accuracy - the fraction of sentences for which the system got both the equation and the grounding of variables correct. Table 4 shows the overall performance of our system, on a 5-fold cross validation. We compare against Joint Learning - a system which jointly\nlearns to predict all relevant components of an equation parse (Section 5.4). We also compare with SPF (Artzi and Zettlemoyer, 2013), a publicly available semantic parser, which can learn from sentencelogical form pairs. We train SPF with sentenceequation pairs and a seed lexicon for mathematical terms (similar to ours), and report equation accuracy. Our structured predictors pipeline approach is shown to be superior to both Joint Learning and SPF.\nSPF gets only a few sentences correct. We attribute this to the inability of SPF to handle overlapping mentions (like in Example 4), as well as its approach of parsing the whole sentence to the final output form. The developers of SPF also confirmed 3 that it is not suitable for equation parsing and that these results are expected. Since equation parsing is a more involved process, a slight adaptation of SPF does not seem possible, necessitating a more involved process , of the type we propose. Our approach, in contrast to SPF, can handle overlapping\n3Private communication\nmentions, selects triggers from text, and parses the trigger list to form equations."}, {"heading": "6.4 Error Analysis", "text": "For variable trigger list prediction, around 25% of the errors were due to the predictor choosing a span which is contained within the correct span, e.g., when the target noun phrase is \u201cThe cost of a child\u2019s ticket\u201d, our predictor chose only \u201cchild\u2019s ticket\u201d. Although this choice might be sufficient for downstream tasks, we consider it to be incorrect in our current evaluation. Another 25% of the errors were due to selection of entities which do not participate in the relation. For example, in \u201cA rancher raises 5 times as many cows as horses.\u201d, our predictor chose \u201cA rancher\u201d and \u201ccows\u201d as variables, whereas the relation exists between \u201ccows\u201d and \u201chorses\u201d. For the prediction of the equation tree, we found that 35% of the errors were due to rare math concepts expressed in text. For example, \u201c7 dollars short of the price\u201d represents 7 dollars should be subtracted from the price. These errors can be handled by carefully augmenting the lexicon. Another 15% of the errors were due to lack of world knowledge, requiring understanding of time, speed, and distance."}, {"heading": "7 Conclusion", "text": "This paper investigates methods that identify and understand mathematical relations expressed in text. We introduce the equation parsing task, which involves generating an equation from a sentence and identifying what the variables represent. We define the notion of projectivity, and construct a high precision lexicon, and use these to reduce the equation search space. Our experimental results are quite satisfying and raise a few interesting issues. In particular, it suggests that predicting equation parses using a pipeline of structured predictors performs better than jointly trained alternatives. As discussed, it also points out the limitation of the current NLP tools in supporting these tasks. Our current formulation has one key limitation; we only deal with expressions that are described within a sentence. Our future work will focus on lifting this restriction, in order to allow relations expressed across multiple sentences and multiple relations expressed in the same sentence. Code and dataset are available at\nhttp://cogcomp.cs.illinois.edu/page/publication_view/800."}, {"heading": "Acknowledgements", "text": "This work is funded by DARPA under agreement number FA8750-13-2-0008, and a grant from the Allen Institute for Artificial Intelligence (allenai.org).\nAppendix"}, {"heading": "A Features", "text": "A.1 Quantity Trigger List Prediction\nThe feature function \u03c6(x, y) used for the classification generates the following features :\n1. Neighborhood features : For each quantity q in the input sentence, we add unigrams and bigrams generated from a window around q, part of speech tags of neighborhood tokens of q. We conjoin these features with Relevance(q).\n2. Quantity Features : For each quantity q, we add unigrams and bigrams of the phrase representing the quantity. Also, we add a feature indicating whether the number is associated with number one or two, and whether it is the only number present in the sentence. These features are also conjoined with Relevance(q).\nA.2 Variable Trigger List Prediction\nThe features \u03c6(x, y) used for variable trigger prediction are as follows:\n1. Variable features : Unigrams and bigrams generated from the noun phrase representing variables, part of speech tags of tokens in noun phrase representing variables.\n2. Neighborhood Features : Unigrams and POS tags from neighborhood of variables.\nAll the above features are conjoined with two labels, one denoting whether y has two variables or one, and the second denoting whether y has two variables represented by the same noun phrase.\nA.3 Equation Tree Prediction\nFor features \u03c6(x, y), we extract for each non-leaf node n of the equation tree y, the following:\n1. Neighborhood Features : Unigrams, bigrams and POS tags from neighborhood of Span-Start(lc(n)), Span-Start(rc(n)), Span-End(lc(n)) and Span-End(rc(n)), conjoined with \u2299(n) and ORDER(n).\n2. Connecting Text Features : Unigrams, bigrams and POS tags between min(Span-End(lc(n)),Span-End(rc(n))) and max(Span-Start(lc(n)),Span-Start(rc(n))), conjoined with \u2299(n) and ORDER(n).\n3. Number Features : In case we are combining two leaf nodes representing quantity triggers, we add a feature signifying whether one number is larger than the other."}, {"heading": "B Annotation Guidelines", "text": "The annotators were shown each sentence paired with the normalized equation representing the relation in the sentence. For each variable in the equation, the annotators were asked to mark spans of text which best describe what the variable represents. They were asked to annotate associated entities if exact variable description was not present. For instance, in example 3 (Section 1), the relation holds between the speed of bird and the speed of wind. However, \u201cspeed\u201d is not explicitly mentioned in the sentence. In such cases, the annotators were asked to annotate the associated entities \u201cthe wind\u201d and \u201ca bird\u201d as representing variables.\nThe guidelines also directed annotators to choose the longest possible mention, in case they feel the mention boundary is ambiguous. As a result, in the sentence, \u201cCity Rentals rent an intermediate-size car for 18.95 dollars plus 0.21 per mile.\u201d, the phrase \u201cCity Rentals rent an intermediate-size car\u201d was annotated as representing variable. We allow multiple mentions to be annotated for the same variable. In example 2 (Section 1), both \u201ca number\u201d and \u201cthe same number\u201d were annotated as representing the same variable."}, {"heading": "C Lexicon", "text": "We construct a high precision list of rules, to parse sentences describing mathematical concepts, for example, \u201cdifference of\u201d, \u201cgreater than\u201d, etc. For each non-leaf node n of a projective equation tree, we define the following terms :\n1. MidSpan(n) : The string from min(Span-End(lc(n)),Span-End(rc(n))) to max(Span-Start(lc(n)),Span-Start(rc(n))).\n2. LeftSpan(n) : The string ending at min(Span-Start(lc(n)),Span-Start(rc(n))) and starting from the nearest trigger position on the left.\n3. RightSpan(n) : The string starting at max(Span-End(lc(n)),Span-End(rc(n))) and ending at the nearest trigger position on the right.\n4. LeftToken(n) : Defined only for leaves, indicates the span of text for the trigger of n.\nThe rules in our lexicon are described using the above terms. They are as follows, ordered from low precedence to high precedence.\n1. If LeftSpan(n) contains \u201csum of\u201d and MidSpan(n) contains \u201cand\u201d or is the empty string, \u2299(n) should be +.\n2. If MidSpan(n) contains one of \u201cadded to\u201d, \u201cplus\u201d, \u201cmore than\u201d \u201ctaller than\u201d, \u201cgreater than\u201d, \u201clarger than\u201d, \u201cfaster than\u201d, \u201clonger than\u201d, \u201cincreased\u201d, \u2299(n) should be +.\n3. If MidSpan(n) contains one of \u201cmore than\u201d \u201ctaller than\u201d, \u201cgreater than\u201d, \u201clarger than\u201d, \u201cfaster than\u201d, \u201clonger than\u201d, and RightSpan(n) contains \u201cby\u201d, \u2299(n) should be \u2212, and ORDER(n) should be lr.\n4. If LeftSpan(n) contains \u201cdifference of\u201d and MidSpan(n) contains \u201cand\u201d or is the empty string, \u2299(n) should be \u2212 and ORDER(n) should be lr.\n5. If LeftSpan(n) contains one of \u201cexceeds\u201d, \u201cminus\u201d, \u201cdecreased\u201d, \u2299(n) should be \u2212, and ORDER(n) should be lr.\n6. If MidSpan(n) contains one of \u201csubtracted\u201d \u201cshorter than\u201d, \u201cless than\u201d, \u201cslower than\u201d, \u201csmaller than\u201d, \u2299(n) should be \u2212, and ORDER(n) should be rl.\n7. If MidSpan(n) contains \u201cmultiplied by\u201d, \u2299(n) is \u00d7.\n8. If LeftSpan(n) contains \u201cproduct of\u201d and MidSpan(n) contains \u201cand\u201d, \u2299(n) should be \u00d7.\n9. If LeftSpan(n) contains \u201cratio of\u201d, \u2299(n) should be \u00f7, and ORDER(n) should be lr.\n10. If LeftToken(n) contains one of \u201cthrice\u201d, \u201ctriple\u201d, \u201ctwice\u201d, \u201cdouble\u201d, \u201chalf\u201d, or if MidSpan(n) contains \u201ctimes\u201d, \u2299(n) is \u00d7.\n11. If LeftToken(n) contains one of \u201cthrice\u201d, \u201ctriple\u201d, \u201ctwice\u201d, \u201cdouble\u201d, \u201chalf\u201d, or if MidSpan(n) contains \u201ctimes\u201d, and MidSpan(n) contains \u201cas\u201d, and RightSpan(n) contains \u201cas\u201d, operation at \u2299(n) is \u00f7, and ORDER(n) is rl."}], "references": [{"title": "Learning structured perceptrons for coreference resolution with latent antecedents and non-local features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "Semantic Parsing Freebase: Towards Opendomain Semantic Parsing", "author": ["Cai", "Yates2013] Qingqing Cai", "Alexander Yates"], "venue": "In Proceedings of the Second Joint Conference on Lexical and Computational Semantics (*SEM)", "citeRegEx": "Cai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2013}, {"title": "Illinoissl: A JAVA library for structured prediction", "author": ["Chang et al.2015] Kai-Wei Chang", "Shyam Upadhyay", "Ming-Wei Chang", "Vivek Srikumar", "Dan Roth"], "venue": "In Arxiv Preprint,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["Clarke et al.2010] J. Clarke", "D. Goldwasser", "M. Chang", "D. Roth"], "venue": "In Proc. of the Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Clarke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Learning from natural instructions", "author": ["Goldwasser", "Roth2011] D. Goldwasser", "D. Roth"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI)", "citeRegEx": "Goldwasser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2011}, {"title": "Learning to solve arithmetic word problems with verb categorization", "author": ["Hannaneh Hajishirzi", "Oren Etzioni", "Nate Kushman"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Hosseini et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hosseini et al\\.", "year": 2014}, {"title": "Learning to automatically solve algebra word problems", "author": ["Kushman et al.2014] N. Kushman", "L. Zettlemoyer", "R. Barzilay", "Y. Artzi"], "venue": null, "citeRegEx": "Kushman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kushman et al\\.", "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kwiatkowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Numerical relation extraction with minimal supervision", "author": ["Madaan et al.2016] A. Madaan", "A. Mittal", "Mausam", "G. Ramakrishnan", "S. Sarawagi"], "venue": "In Proc. of the Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Madaan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Madaan et al\\.", "year": 2016}, {"title": "Nonprojective dependency parsing using spanning tree algorithms", "author": ["Fernando Pereira", "Kiril Ribarov", "Jan Haji\u010d"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Learning and inference over constrained output", "author": ["D. Roth", "W. Yih", "D. Zimak"], "venue": "In Proc. of the International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Punyakanok et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2005}, {"title": "Part of speech tagging using a network of linear separators", "author": ["Roth", "Zelenko1998] D. Roth", "D. Zelenko"], "venue": "In Coling-Acl, The 17th International Conference on Computational Linguistics,", "citeRegEx": "Roth et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Roth et al\\.", "year": 1998}, {"title": "Solving general arithmetic word problems", "author": ["Roy", "Roth2015] S. Roy", "D. Roth"], "venue": "In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Roy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Reasoning about quantities in natural language", "author": ["Roy et al.2015] S. Roy", "T. Vieira", "D. Roth"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Roy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2015}, {"title": "Parsing With Compositional Vector Grammars", "author": ["John Bauer", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Piecewise pseudolikelihood for efficient training of conditional random fields", "author": ["Sutton", "McCallum2007] C. Sutton", "A. McCallum"], "venue": "In Zoubin Ghahramani, editor, Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Sutton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2007}, {"title": "Learning synchronous grammars for semantic parsing with lambda calculus", "author": ["Wong", "Mooney2007] Y.-W. Wong", "R. Mooney"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Wong et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wong et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 7, "context": "The task can be seen as a form of semantic parsing (Goldwasser and Roth, 2011; Kwiatkowski et al., 2013) where instead of mapping a sentence to a logical form, we want to map", "startOffset": 51, "endOffset": 104}, {"referenceID": 8, "context": "The work most related to this paper is (Madaan et al., 2016), which focuses on extracting relation triples where one of the arguments is a number.", "startOffset": 39, "endOffset": 60}, {"referenceID": 6, "context": "There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).", "startOffset": 75, "endOffset": 158}, {"referenceID": 12, "context": "There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).", "startOffset": 75, "endOffset": 158}, {"referenceID": 5, "context": "There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).", "startOffset": 75, "endOffset": 158}, {"referenceID": 3, "context": "Our work is also conceptually related to work on semantic parsing \u2013 mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011).", "startOffset": 133, "endOffset": 251}, {"referenceID": 7, "context": "Our work is also conceptually related to work on semantic parsing \u2013 mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011).", "startOffset": 133, "endOffset": 251}, {"referenceID": 9, "context": "This is more general than the definition of projective trees used in dependency parsing (McDonald et al., 2005).", "startOffset": 88, "endOffset": 111}, {"referenceID": 10, "context": "This tradeoff is strongly supported in the literature (Punyakanok et al., 2005; Sutton and McCallum, 2007).", "startOffset": 54, "endOffset": 106}, {"referenceID": 6, "context": "For algebra word problems, we used the MIT dataset (Kushman et al., 2014), and two high school mathematics textbooks, Elementary Algebra (College of Redwoods) and Beginning and Intermediate Algebra (Tyler Wallace).", "startOffset": 51, "endOffset": 73}, {"referenceID": 14, "context": "For all our experiments, we use the Stanford Parser (Socher et al., 2013), the Illinois POS tagger (Roth and Zelenko, 1998) and the Illinois-SL structured prediction package (Chang et al.", "startOffset": 52, "endOffset": 73}, {"referenceID": 2, "context": ", 2013), the Illinois POS tagger (Roth and Zelenko, 1998) and the Illinois-SL structured prediction package (Chang et al., 2015).", "startOffset": 108, "endOffset": 128}], "year": 2016, "abstractText": "Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing \u2013 given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in 70% of the cases. In 60% of the time, it also identifies the correct noun phrase \u2192 variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.", "creator": "LaTeX with hyperref package"}}}