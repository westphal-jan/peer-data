{"id": "1609.09341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Machine Learning Techniques for Stackelberg Security Games: a Survey", "abstract": "the present survey aims at presenting the current machine learning techniques employed effectively in security games domains. specifically, we now focused on papers and works developed by the teamcore of university of southern california, which deepened different different directions within in this field. after a brief introduction on stackelberg security games ( ssgs ) and the poaching setting, the rest of giving the work presents how to model a boundedly rational attacker taking into account between her human behavior, fisher then describes how to face the problem of having attacker's payoffs not defined and how to estimate them them and, finally, presents how online learning techniques have been exploited jointly to learn a model of detecting the attacker.", "histories": [["v1", "Thu, 29 Sep 2016 13:53:26 GMT  (80kb)", "http://arxiv.org/abs/1609.09341v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["giuseppe de nittis", "francesco trov\\`o"], "accepted": false, "id": "1609.09341"}, "pdf": {"name": "1609.09341.pdf", "metadata": {"source": "META", "title": "Machine Learning Techniques for Stackelberg Security Games: a Survey", "authors": ["Giuseppe De Nittis", "Francesco Trov\u00f2"], "emails": ["}@polimi.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 9.\n09 34\n1v 1\nContents"}, {"heading": "1 Introduction 3", "text": "1.1 Stackelberg paradigm and SSG . . . . . . . . . . . . . . . . . . . 3 1.2 The poaching setting . . . . . . . . . . . . . . . . . . . . . . . . . 4"}, {"heading": "2 Using human behavior models in solving SSGs 5", "text": "2.1 SUQR: modeling a boundedly rational attacker . . . . . . . . . . 5 2.1.1 Learning SUQR parameters . . . . . . . . . . . . . . . . . 6 2.2 From MATCH to SHARP . . . . . . . . . . . . . . . . . . . . . . 7 2.2.1 Adaptive utility model . . . . . . . . . . . . . . . . . . . . 8 2.2.2 SHARP\u2019s utility Computation . . . . . . . . . . . . . . . 9 2.2.3 Generating defender\u2019s strategies against SHARP . . . . . 9 2.3 Learning adversary models from three defender\u2019s strategies . . . 10 2.3.1 Linear utility functions . . . . . . . . . . . . . . . . . . . . 11 2.3.2 Learning the optimal strategy . . . . . . . . . . . . . . . . 12"}, {"heading": "3 Determining attacker\u2019s payoffs exploiting regret\u2013based solutions 13", "text": ""}, {"heading": "4 Online learning 16", "text": "4.1 Handling exploration\u2013exploitation tradeoffs in Security Games . 16 4.1.1 Problem formulation . . . . . . . . . . . . . . . . . . . . . 17 4.1.2 Learning model from defenders\u2019 previous observations . . 18 4.1.3 Restless multi\u2013armed bandit problems . . . . . . . . . . . 18 4.1.4 Restless bandit formulation . . . . . . . . . . . . . . . . . 19 4.1.5 Sufficient condition for indexability . . . . . . . . . . . . . 20 4.1.6 Numerical evaluation of indexability . . . . . . . . . . . . 21 4.1.7 Computation of Whittle index Policy . . . . . . . . . . . . 21 4.1.8 Special POMDP formulation . . . . . . . . . . . . . . . . 22 4.1.9 Value iteration for the special POMDP . . . . . . . . . . 23 4.1.10 Planning from POMDP view . . . . . . . . . . . . . . . . 24 4.2 Online planning for optimal protector strategies in resource conservation games . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.2.1 Problem formulation . . . . . . . . . . . . . . . . . . . . . 25 4.2.2 GMOP algorithm . . . . . . . . . . . . . . . . . . . . . . . 27 4.2.3 Applying Gibbs sampling in GMOP . . . . . . . . . . . . 28\nReferences 30"}, {"heading": "1 Introduction", "text": "The present survey aims at presenting the current main machine learning techniques employed in security games domains. Specifically, we focused on papers and works developed by the Teamcore of University of Southern California, which deepened different directions in this field. Among several works on this topic, e.g., [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].\nAfter a brief introduction on Stackelberg Security Games (SSGs) and the poaching setting, the rest of the work is organized according to the different problems that has been dealt with.\n\u2022 Section 2 shows how to model a boundedly rational attacker taking into account her human behavior.\n\u2022 Section 3 faces the problem of having attacker\u2019s payoffs not defined and how to estimate them studying the regret of the defender.\n\u2022 Section 4 presents how online learning techniques have been exploited to learn a model of the attacker."}, {"heading": "1.1 Stackelberg paradigm and SSG", "text": "Usually, to represent security scenarios, a specific class of games is adopted, i.e., Stackelberg Games [23]. Here, on one side there is the Defender, which publicly commits to a mixed strategy, i.e., a probability distribution on the actions available to the player, and on the other side there is an Attacker, that observes the commitment of the Defender and acts consequently. Such games are called Stackelberg Security Games (SSG). Specifically, in SSGs, the Defender attempts to protect a set of T targets from an Attacker, by optimally allocating a set of R resources, R < T . Denote by x = {xt} the Defender\u2019s strategy where xt is the coverage probability at target t, the set of feasible strategies is: X = {x : 0 \u2264 xt \u2264 1, \u2211\nt xt \u2264 R}. If the adversary attacks t when the defender is not protecting it, the adversary receives a reward Rat , otherwise the adversary gets a penalty P at . Conversely, the Defender receives a penalty P d t in the former case and a reward Rdt in the latter case. Let (R a,P a) and (Rd,P d) be the payoff vectors. The players\u2019 expected utilities at t is computed as:\nUat (x,R a, P a) = xtP a t + (1\u2212 xt)R a t Udt (x,R d, P d) = xtR d t + (1\u2212 xt)P d t\nIn general, in such situations, the most appropriate solution concept is the Leader\u2013follower equilibrium. The problem of finding such equilibrium can be\nformulated as:\nargmax xl,x \u2217\nf\n\u2211\nal\u2208Al\n\u2211\naf\u2208Af\n[Ul(al, af)xl(al)x \u2217 f (af)]\ns.t. \u2211\nal\u2208Al\nxl(al) = 1\nxl(al) \u2265 0 \u2200al \u2208 Al x\u2217f \u2208 argmaxxl \u2211\nal\u2208Al\n\u2211\nal\u2208Al\n[Uf(af, al)xf(af)xl(al)]\ns.t. \u2211\naf\u2208Af\nxf(af) = 1\nxf(af) \u2265 0 \u2200af \u2208 Af\nwhere xl(al) (xf (af )) is the probability that the leader (follower) will play action al (af ) and x \u2217 f is the best strategy of the follower.\nIn zero\u2013sum games, the Leader\u2013follower equilibrium coincides with the Nash equilibrium [21] and the maxmin/minmax strategies."}, {"heading": "1.2 The poaching setting", "text": "Poaching and illegal over\u2013fishing are critical international problems leading to destruction of ecosystems. For example, three out of nine tiger species have gone extinct in the past 100 years and others are now endangered due to poaching [18]. Law enforcement agencies in many countries are hence challenged with applying their limited resources to protecting endangered animals and fish stocks.\nBuilding upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.g., protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25]. There are several key features in green security domains that introduce novel research challenges.\n1. The defender is faced with multiple adversaries who carry out repeated and frequent illegal activities (attacks), yielding a need to go beyond the one\u2013shot SSG model.\n2. In carrying out such frequent attacks, the attackers generally do not conduct extensive surveillance before performing an attack and spend less time and effort in each attack, and thus it becomes more important to model the attackers\u2019 bounded rationality and bounded surveillance.\n3. There is more attack data available in green security domains than in infrastructure security domains, which makes it possible to learn the attackers\u2019 decision making model from data."}, {"heading": "2 Using human behavior models in solving SSGs", "text": "In game theory, the adversary is usually represented as a fully rational player. In real\u2013world, people are not fully rational, i.e., their choices are not simply determined by mere calculations. The problem that is addressed here is the representation of a boundedly rational attacker in SSGs. In fact, the fully rationality condition is relaxed to make an important step towards modeling real\u2013world attacker, both to extract useful information about already gathered data, but also to build new models of attackers to enhance the current level of security."}, {"heading": "2.1 SUQR: modeling a boundedly rational attacker", "text": "In SSGs, attacker bounded rationality is often modeled via behavior models such as Quantal Response (QR) [7]. The QR model predicts a stochastic distribution of the adversary response: the greater the expected value of a target the more likely the adversary will attack that target. QR\u2019s key parameter \u03bb represents the level of rationality in adversary\u2019s response: as \u03bb increases, the predicted response by the QR model converges to the optimal action of the adversary. Instead of using a human behavior model, MATCH, the best algorithm up to 2013, computes a robust defender strategy by guaranteeing a bound on the defender\u2019s loss in her expected value if the adversary deviates from her optimal choice. More specifically, the defender\u2019s loss is constrained to be no more than a factor of \u03b2 times the adversary\u2019s loss in his expected value. The key parameter \u03b2 describes how much the defender is willing to sacrifice when the adversary deviates from the optimal action.\nThe key idea in Subjective Expected Utility (SEU) is that individuals have their own evaluations of each alternative during decision making. Recall that in an SSG, the information presented to the human subject for each choice includes: the marginal coverage on target t(xt); the subject\u2019s reward and penalty (R a t , P a t ); the defender\u2019s reward and penalty (Rdt , P d t ). Inspired by the idea of SEU, it has been proposed [12] a subjective utility function of the adversary for SSG as the following:\nU\u0302at\u2032(x,R a,P a) = w1xt + w2R a t + w3P a t\nIn fact, SUQR is motivated by the lens model which suggested that evaluation of adversaries over targets is based on a linear combination of multiple observable features. One key advantage of these behavioral models is that they can be used to predict attack frequency for multiple attacks by the adversary, where in the attacking probability is a normalization of attack frequency.\nThe novelty of this subjective utility function is the linear combination of the values (rewards/penalty) and coverage probabilities. (Note that the decision\u2013 making of the general population is modeled, not of each individual since there are no sufficient data for each specific subject). This model actually leads to higher prediction accuracy than the classic expected value function. Other\nalternatives to this subjective utility function are feasible, e.g., including all the information presented to the subjects:\nU\u0302at (x,R a,P a) = w1xt + w2R a t + w3P a t + w4R d t + w5P d t\nThen, the QR model is modified by replacing the classic expected value function with the SU function, leading to the SUQR model. In the SUQR model, the probability that the adversary chooses target t, qt, is given by:\nqt(x,R a,P a) =\ne\u03bbU\u0302 a t (x,R a,Pa)\n\u2211 t\u2032 e \u03bbU\u0302a t\u2032 (x,Ra,Pa)\nThe problem of finding the optimal strategy for the defender can therefore be formulated as:\nmaxx\nT \u2211\nt=1\ne\u03bbU a t (x,R a,Pa)\n\u2211 t\u2032 e \u03bbUa t\u2032 (x,Ra,Pa)\n(xtR d t + (1 \u2212 xt)P d t )\ns.t.\nT \u2211\nt=1\nxt \u2264 K, 0 \u2264 xt \u2264 1\nHere, the objective is to maximize the defender\u2019s expected value given that the adversary chooses to attack each target with a probability according to the SUQR model."}, {"heading": "2.1.1 Learning SUQR parameters", "text": "Without loss of generality, \u03bb = 1. As customarily done in traditional machine learning, Maximum Likelihood Estimation (MLE) to learn the parameters (w1, w2, w3) is employed. Given the defender strategy x and N samples of the players\u2019 choices, the log\u2013likelihood of (w1, w2, w3) is given by:\nlogL(w1, w2, w3|x) = N \u2211\nj=1\nlog[qtj(w1, w2, w3|x)]\nwhere tj is the target that is chosen in sample j and qtj (w1, w2, w3|x) is the probability that the adversary chooses the target tj. Let Nt be the number of subjects attacking target t. Then:\nlogL(w1, w2, w3|x) = T \u2211\nt=1\nNtlog[qtj(w1, w2, w3|x)]\nIt can be shown that the Hessian matrix of logL[qtj(w1, w2, w3|x)] is negative semi\u2013definite. Thus, this function has an unique local maximum point and hence a convex optimization solver can be used to compute the optimal weights (w1, w2, w3), e.g., fmincon in Matlab."}, {"heading": "2.2 From MATCH to SHARP", "text": "Starting from the above results, a new model is now introduced, SHARP [6], which:\n\u2022 reasons based on success or failure of the adversary\u2019s past actions on exposed portions of the attack surface to model adversary adaptiveness;\n\u2022 reasons about similarity between exposed and unexposed areas of the attack surface, and also incorporates a discounting parameter to mitigate adversary\u2019s lack of exposure to enough of the attack surface;\n\u2022 integrates a non\u2013linear probability weighting function to capture the adversary\u2019s true weighting of probability.\nFollowing the approach presented in the previous section, MLE has been applied to learn the weights of the SUQR model based on data collected from our human subject experiments and found that the weights on coverage probability were positive for all the experiments. That is, counter\u2013intuitively humans were modeled as being attracted to regions with high coverage probability, even though they were not attacking targets with very high coverage but they were going after targets with moderate to very low coverage probability.\nTo address this issue, a solution can be the augmentation of the Subjective Utility function with a two\u2013parameter probability weighting function, that can be either inverse S\u2013shaped (concave near probability zero and convex near probability one) or S\u2013shaped.\nf(p) = \u03b4p\u03b3\n\u03b4p\u03b3 + (1\u2212 p)\u03b3 (1)\nThe SU of an adversary denoted by a can then be computed as:\nSUai (x) = w1f(xi) + w2R a i + w3P a i ,\nwhere f(xi) for coverage probability xi is computed as per Equation (1). The two parameters \u03b4 and \u03b3 control the elevation and curvature of the function respectively. \u03b3 < 1 results in an inverse S-shaped curve while \u03b3 > 1 results in an S\u2013shaped curve. This is the PSU (Probability weighted Subjective Utility) function. The curve representing human weights for probability is S\u2013shaped in nature, and not inverse S-shaped as prospect theory suggests. The S\u2013shaped curve indicates that people would overweigh high probabilities and underweigh low to medium probabilities.\nW.r.t. MATCH, SHARP introduces a new feature \u2013 distance \u2013 that affects the reward and hence the obvious question for us was to investigate the effect of this new feature in predicting adversary behavior. Several variations of PSU with different combinations of features can be considered.\nSUai (x) = w1f(xi) + w2\u03c6i + w3P a i (2) SUai (x) = w1f(xi) + w2R a i + w3P a i + w4Di (3) SUai (x) = w1f(xi) + w2\u03c6i + w3P a i + w4Di (4)\nwhere \u03c6i and Di refer to the animal density at target i and the distance to target i from the poacher\u2019s starting location respectively."}, {"heading": "2.2.1 Adaptive utility model", "text": "A second major innovation in SHARP is the adaptive nature of the adversary and addressing the issue of attack surface exposure. The attack surface \u03b1 is defined as the n\u2013dimensional space of the features used to model adversary behavior.\nFor example, as per the third PSU model in Equation (4), this would mean the space represented by the following four features: coverage probability, animal density, adversary penalty and distance from the starting location.\nA target profile \u03b2k \u2208 \u03b1 is defined as a point on the attack surface \u03b1 and can be associated with a target. Exposing the adversary to a lot of different target profiles would therefore mean exposing the adversary to more of the attack surface and gathering valuable information about their behavior. While a particular target location, defined as a distinct region in the 2\u2013d space, can only be associated with one target profile in a particular round, more than one target may be associated with the same target profile in the same round.\nObservation 1. Adversaries who have succeeded in attacking a target associated with a particular target profile in one round, tend to attack a target with similar target profiles in next round.\nObservation 2. Adversaries who have failed in attacking a target associated with a particular target profile in one round, tend not to attack a target with similar\u2019 target profiles in the next round.\nThe vulnerability associated with a target profile \u03b2i, which was shown to the adversary in round r, denoted V r\u03b2i , is defined as a function of the total number of successes and failures on the concerned target profile in that round (denoted by successr\u03b2i and failure r \u03b2i respectively).\nV r\u03b2i = successr\u03b2i \u2212 failure r \u03b2i\nsuccessr\u03b2i + failure r \u03b2i\nTherefore, more successful attacks and few failures on a target profile indicate that it was highly vulnerable in that round. Because multiple targets can be associated with the same target profile and the pure strategy generated based on the mixed strategy x in a particular round may result in a defender\nbeing present at some of these targets while not at others, there may be both successes and failures associated with the same target profile in that round.\nThe attractiveness of a target profile \u03b2i at the end of round R, denoted A R \u03b2i ,\nis defined as a function of the vulnerabilities for \u03b2i from round 1 to round R.\nAR\u03b2i =\n\u2211R r=1 V r \u03b2i\nR\nTherefore, the attractiveness of a target profile is modeled as the average of the vulnerabilities for that target profile over all the rounds till round R. This is consistent with the notion that a target profile which has led to more successful attacks over several rounds will be perceived as more attractive by the adversary."}, {"heading": "2.2.2 SHARP\u2019s utility Computation", "text": "Existing models (such as SUQR) only consider the adversary\u2019s actions from round (r \u2212 1) to predict their actions in round r. However, it is clear that the adversary\u2019s actions in a particular round are dependent on his past successes and failures. Thus, a novel adaptive probability weighted subjective utility function that captures this adaptive nature of the adversary\u2019s behavior by capturing the shifting trends in attractiveness of various target profiles over rounds is proposed.\nASUR\u03b2i = (1\u2212 d \u2217A R \u03b2i )w1f(x\u03b2i) + (1 + d \u2217A R \u03b2i )w2\u03c6\u03b2i+\n+ (1 + d \u2217AR\u03b2i)w3P a \u03b2i + (1\u2212 d \u2217AR\u03b2i)w4D\u03b2i\nd is a discounting parameter which is based on a measure of the amount of attack surface exposed. d is low in the initial rounds when the defender does not have enough of the right kind of data, but would gradually increase as more information about the attacker\u2019s preferences about various regions of the attack surface become available.\nNow, let us look at how reasoning about unexposed portions of the attack surface based on the exposed areas. If a target profile \u03b2u was not exposed to attacker response in round r, the defender will not be able to compute the vulnerability V r\u03b2u . Therefore, it is not possible to estimate the attractiveness for \u03b2u and hence the optimal defender strategy. So, in keeping with the analysis on available data and based on the spillover effect introduced earlier, the distance\u2013 weighted k\u2013nearest neighbors algorithm is employed to obtain the vulnerability V r\u03b2u of an unexposed target profile \u03b2u in round r, based on the k most similar target profiles which were exposed to the attacker in round r."}, {"heading": "2.2.3 Generating defender\u2019s strategies against SHARP", "text": "While SHARP provides an adversarymodel, the defender strategies against such model must be generated. To that end, first the parameters of SHARP from\navailable data are learned. Then, future round strategies against the boundedly rational adversary are generated, characterized by the learned model parameters by solving the following optimization problem:\nmaxx\u2208X\n[\n\u2211\ni\u2208T\nUdi (x)q R i (w|x)\n]\nwhere qRi (w|x) is the probability that the adversary will attack target i in round R."}, {"heading": "2.3 Learning adversary models from three defender\u2019s strategies", "text": "Here, a new approach to learn the parameters of the behavioral model of a bounded rational attacker (thereby pinpointing a near optimal strategy) is developed, by observing how the attacker responds to only three defender strategies.\nNote: even though the setting is the same of the previous sections, since some assumptions changed, e.g., the adaptivity of the attacker, we slightly change the adopted notation to avoid confusion.\nLet the utility function of the attacker for target t \u2208 T be denoted by ut : [0, 1] \u2192 R. Given a coverage probability vector x \u2208 X, the utility of the attacker under strategy x is defined as ut(xt). Upon observing the defender\u2019s strategy x, the attacker computes the utility on each target t, ut(xt), and based on these utilities responds to the defender\u2019s strategy. Here, a non\u2013adaptive attacker is considered. She attacks target t with probability:\nDx(t) = eut(xi) \u2211\ni\u2208T e ui(xi)\nOur model is a generalization of bounded rationality models such as SUQR. Suppose the same mixed strategy x is played for multiple time steps. The empirical distribution of attacks on target t under x is denoted by D\u0302x(\u00b7). Furthermore, it is assumed that for the strategies, and for all t, Dx(t) \u2265 \u03c1, for some \u03c1 = 1\npoly(n) . This assumption is required to estimate the value of D x(t)\nwith polynomially many samples. Our goal is to learn the utility functions, ut(\u00b7) for all t \u2208 T , by observing attacker\u2019s responses to a choice of coverage probability vectors x \u2208 X. This allows to find an approximately optimal defender strategy \u2014 the strategy that leads to the best defender utility. u\u0302t : [0, 1] \u2192 R uniformly approximates or uniformly learns ut(\u00b7) within an error of \u01eb, if \u2200x \u2208 [0, 1], |u\u0302t(x) \u2212 ut(x)| \u2264 \u01eb. Note that the attacker\u2019s mixed strategy remains the same when the utility functions corresponding to all targets are increased by the same value. Therefore, only a normalized representation of the utility functions can be learned, such that for all t and all x, |u\u0302t(x)+c\u2212ut(x)| \u2264 \u01eb for some c."}, {"heading": "2.3.1 Linear utility functions", "text": "Assume that the utility functions are linear and denoted by ut(x) = wtx + ct. Normalizing the utilities, without loss of generality, cn = 0.\nTheorem 1. Suppose the functions u1(\u00b7), . . . , un(\u00b7) are linear. Consider any 3 strategies x,y, z \u2208 X such that for any t < n, |(xt \u2212 yt)(xn \u2212 zn) \u2212 (xn \u2212 yn)(xt \u2212 zt)| \u2265 \u03bb, and for any two different strategies p, q \u2208 {x,y, z}, it holds |pt \u2212 qt| \u2265 \u03bd. If it is possible to access m = \u2126 ( 1 \u03c1 ( 1 \u01eb\u03bd\u03bb )2 log ( n \u03b4 ) ) samples of each of these strategies, then with probability 1\u2212 \u03b4, each ut(\u00b7) can be uniformly learned within error \u01eb.\n\u03bd depends on how different the strategies are from each other \u2014 a very small value means that they are almost identical on some coordinates. The lower bound of \u03bb would not be very small unless there is a very specific relation between the strategies. As a sanity check, if the three strategies were chosen uniformly at random from the simplex, both values would be at least 1/poly(n). In the quantal best\u2013response model, for each strategy x, the ratio between the attack probabilities of two targets t and n follows the relation:\nut(xt) = ln\n(\nDx(t)\nDx(n)\n)\nTherefore, each strategy induces n \u2212 1 linear equations that can be used to solve for the coefficients of ut. However, only an estimate D\u0302\nx(t) of the probability that target t is attacked under a strategy x, based on the given samples, can be obtained. So, the inaccuracy in our estimates of ln ( D\u0302x(t)\nD\u0302x(n)\n)\nleads to inaccuracy in the estimated polynomial u\u0302t. For sufficiently accurate estimates Dx(t), the value of ut differs from the true value by at most \u01eb.\nLemma 1. Given x \u2208 X, let Dx(t) be the empirical distribution of attacks based on m = \u2126 (\n1 \u03c1\n(\n1 \u01eb\u03bd\u03bb\n)2 log (\nn \u03b4\n)\n)\nsamples. With probability 1 \u2212 \u03b4, for all\nt \u2208 T , 1 \u01eb \u2264 D\u0302 x(t) Dx(t) \u2264 1 + \u01eb.\nTheorem 2. Suppose the functions u1(\u00b7), . . . , un(\u00b7) are polynomials of degree at most d. Consider any 2d+1 strategies, y(1), . . . ,y(d),y(d+1) = x(1), . . . ,x(d)x(d+1) such that for all k, k\u2032, k 6= k\u2032, y (k) 1 = y (k\u2032) 1 , p (k) n = p (k\u2032) n , |y (k) n \u2212 y (k\u2032) n | \u2265 \u03bd and for all t < n, |x (k) t \u2212 x (k\u2032) t | \u2265 \u03bd. If it is possible to have access to m = \u2126 (\n1 \u03c1\n(\n1 \u01eb\u03bd\u03bb\n)2 log (\nn \u03b4\n)\n)\nsamples of each of these strategies, then with probability"}, {"heading": "1\u2212 \u03b4, each ut(\u00b7) can be uniformly learned within error \u01eb.", "text": "Now, any utility function that is continuous and L\u2013Lipschitz, i.e., for all t and values x and y, |ut(x) \u2212 ut(y)| \u2264 L|x\u2212 y|, should be learned. Such utility functions can be uniformly learned up to error \u01eb, using O (\nL \u01eb\n)\nstrategies. For any L\u2013Lipschitz function ut(x), there is a polynomial of degree m = 12L/\u01eb that uniformly approximates ut(x) within error of \u01eb/2.\nTheorem 3. Suppose the functions u1(\u00b7), . . . , un(\u00b7) are L-Lipschitz. For d = 12L/\u01eb, consider any 2d+1 strategies, y(1), . . . ,y(d),y(d+1) = x(1), . . . ,x(d)x(d+1) such that for all k, k\u2032, k 6= k\u2032, y (k) 1 = y (k\u2032) 1 , p (k) n = p (k\u2032) n , |y (k) n \u2212y (k\u2032) n | \u2265 \u03bd and for all t < n, |x (k) t \u2212x (k\u2032) t | \u2265 \u03bd. If it is possible to have access to m = \u2126 ( L2 \u03c1\u01eb24L/\u01eb log ( n \u03b4 ) ) samples of each of these strategies, then with probability 1\u2212 \u03b4, each ut(\u00b7) can be uniformly learned within error \u01eb."}, {"heading": "2.3.2 Learning the optimal strategy", "text": "So far, the focus has been on the problem of uniformly learning the utility function of the attacker. Now, it is shown that an accurate estimate of this utility function allows to pinpoint an almost optimal strategy for the defender. Let the utility function of the defender on target t \u2208 T be denoted by vt : [0, 1] \u2192 [\u22121, 1]. Given a coverage probability vector x \u2208 X, the utility the defender receives when target t is attacked i vt(xt). The overall expected utility of the defender is:\nV (x) = \u2211\nt\u2208T\nDx(t)vt(xt)\nLet u\u0302t be the learned attacker utility functions, and D\u0304 x(t) be the predicted attack probability on target t under strategy x, according to the utilities u\u0302t, i.e.:\nD\u0304x(t) = eu\u0302t(xi) \u2211\ni\u2208T e u\u0302i(xi)\nLet V\u0304 (x) be the predicted expected utility of the defender based on the learned attacker utilities D\u0304x(t), i.e., V\u0304 (x) = \u2211\nt\u2208T D\u0304 x(t)vt(xt). When the\nattacker utilities are uniformly learned within error \u01eb, then V\u0304 estimates V with error at most 8\u01eb. At a high level, this is established by showing that one can predict the attack distribution using the learned attacker utilities. Furthermore, optimizing the defender\u2019s strategy against the approximate attack distributions leads to an approximately optimal strategy for the defender.\nTheorem 4. Assume for all x and any t \u2208 T , |u\u0302t(xt) \u2212 ut(xt)| \u2264 \u01eb \u2264 1/4. Then, for all x, |V\u0304 (x) \u2212 V (x)| \u2264 4\u01eb. Furthermore, let x\u2032 = argmaxx V\u0304 (x) be the predicted optimal strategy, then maxx V (x)\u2212 V (x\u2032) \u2264 8\u01eb."}, {"heading": "3 Determining attacker\u2019s payoffs exploiting regret\u2013", "text": "based solutions\nThe adversary behavior models (capturing bounded rationality) can be learned from real\u2013world data on where adversaries have attacked, and game payoffs can be determined precisely from data on animal densities.\nOne key approach to modeling payoff uncertainty is to express the adversary\u2019s payoffs as lying within specific intervals: for each target t, it holds:\nRat \u2208 [R a min(t), R a max(t)], P a t \u2208 [P a min(t), P a max(t)]\nWhile a behavioral model from real\u2013world data can be learned, access to data to precisely compute animal density may be not always possible. For example, given limited numbers of rangers, they may have patrolled and collected wildlife data from only a small portion of a national park, and thus payoffs in other areas of the park may remain uncertain. Also, due to the dynamic changes (e.g., animal migration), players\u2019 payoffs may become uncertain in the next season. Hence, a new MiniMaxRegret (MMR)\u2013based robust algorithm is introduced, ARROW, to handle payoff uncertainty in Green Security Games (a.k.a. GSGs, i.e., Security Games for the protection of wildlife, forest and fisheries), taking into account adversary behavioral models. Here, the main focus is on zero\u2013sum games, as motivated by recent work in green security domains and earlier major SSG applications that use zero\u2013sum games. In addition, a model inspired by SUQR is adopted, where the subjective utility function is:\nU\u0302at (x,R a,P a) = w1xt + w2R a t + w3P a t + w4\u03a6t\nwhere xt is the coverage probability of target t and \u03a6t is another feature of target t, e.g., distance, as seen for SHARP. MMRb with uncertain payoffs is now formulated for both players in zero\u2013sum SSG with a boundedly rational attacker.\nDefinition 1. Given (Ra,P a), the defender\u2019s behavioral regret is the loss in her utility for playing a strategy x instead of the optimal strategy, which is represented as follows:\nRb(x,R a,P a) = maxx\u2032\u2208XF (x \u2032,Ra,P a)\u2212 F (x,Ra,P a)\nwhere\nF (x,Ra,P a) = \u2211\nt\nq\u0302t(x,R a,P a)Udt (x,R d,P d)\nBehavioral regret measures the distance in terms of utility loss from the defender strategy x to the optimal strategy given the attacker payoffs. Here, F (x,Ra,P a) is the defender\u2019s utility for playing x where the attacker payoffs, whose response follows SUQR, are (Ra,P a). The defender\u2019s payoffs in zerosum games are Rd = \u2212P a and P d = \u2212Ra. When the payoffs are uncertain,\nif the defender plays a strategy x, she receives different behavioral regrets w.r.t to different payoff instances within the uncertainty intervals. Thus, she could receive a behavioral max regret which is defined as follows.\nDefinition 2. Given payoff intervals I, the behavioral max regret for the defender to play a strategy x is the maximum behavioral regret over all payoff instances:\nMRb(x, I) = max(Ra,Pa)\u2208IRb(x,R a,P a)\nDefinition 3. Given payoff intervals I, the behavioral minimax regret problem attempts to find the defender optimal strategy that minimizes the MRb she receives:\nMMRb(I) = minx\u2208XMRb(x, I)\nIntuitively, behavorial minimax regret ensures that the defender\u2019s strategy minimizes the loss in the solution quality over the uncertainty of all possible payoff realizations.\nOverall, MMRb can be reformulated as minimizing the max regret r such that r is no less than the behavioral regrets over all payoff instances within the intervals:\nminx\u2208X,r\u2208R r s.t. r \u2265 F (x\u2032,Ra,P a)\u2212 F (x,Ra,P a), \u2200(Ra,P a) \u2208 I,x\u2032 \u2208 X\nThe set of constraints is infinite since X and I are continuous. One practical approach to optimization with large constraint sets is constraint sampling, coupled with constraint generation (a.k.a. row generation). Following this approach, ARROW samples a subset of constraints and gradually expands this set by adding violated constraints to the relaxed problem until convergence to the optimal MMRb solution.\nSpecifically, ARROW begins by sampling pairs (Ra,P a) of the adversary payoffs uniformly from I. The corresponding optimal strategies for the defender given these payoff samples, denoted x\u2032, are then computed using the PASAQ algorithm [26] to obtain a finite set S of sampled constraints. These sampled constraints are then used to solve the corresponding relaxed MMRb program using the R.ARROW algorithm. The optimal solution (lb,x\u2217) is thus obtained, providing a lower bound (lb) on the true MMRb. Then constraint generation is applied to determine violated constraints (if any). This uses the M.ARROW algorithm which computesMRb(x\n\u2217, I), the optimal regret of x\u2217 which is an upper bound (ub) on the true MMRb. If ub > lb, the optimal solution of M.ARROW, {x\n\u2032,\u2217,Ra,\u2217,P a,\u2217} provides the maximally violated constraint, which is added to S. Otherwise, x\u2217 is the minimax optimal strategy and lb = ub = MMRb(I).\nThe first step of ARROW is to solve the relaxed MMRb problem using R.ARROW. This relaxed MMRb problem is non\u2013convex. Thus, R.ARROW presents two key ideas for efficiency:\n1. binary search (which iteratively searches the defender\u2019s utility space to find the optimal solution) to remove the fractional terms in relaxed MMRb;\n2. it then applies piecewise\u2013linear approximation to linearize the non\u2013convex terms of the resulting decision problem at each binary search step.\nTheorem 5. R.ARROW provides an O ( \u01eb+ 1 M )\n\u2013optimal solution of relaxed MMRb where \u01eb is the tolerance of binary search and M is the number of piecewise segments.\nGiven the optimal solution x\u2217 returned by R.ARROW, the second step of ARROW is to compute MRb of x\n\u2217 using M.ARROW. A local search with multiple starting points is employed since allows to reach different local optima.\nWhile ARROW incorporates an adversary behavioral model, it may not be applicable for green security domains where there may be a further paucity of data in which not only payoffs are uncertain but also parameters of the behavioral model are difficult to learn accurately. Therefore, a novel MMR-based algorithm, ARROW\u2013Perfect, can be introduced to handle uncertainty in both players\u2019 payoffs assuming a perfectly rational attacker. In general, ARROW\u2013 Perfect follows the same constraint sampling and constraint generation methodology as ARROW. Yet, by leveraging the property that the attacker\u2019s optimal response is a pure strategy (given a perfectly rational attacker) and the game is zero\u2013sum, the exact optimal solutions for computing both relaxed MMR and max regret in polynomial time are obtained (while we cannot provide such guarantees for a boundedly rational attacker)."}, {"heading": "4 Online learning", "text": ""}, {"heading": "4.1 Handling exploration\u2013exploitation tradeoffs in Security Games", "text": "Previous research optimizes defenders\u2019 strategies by modeling this problem as a repeated Stackelberg game, capturing the special property in this domain frequent interactions between defenders and attackers. However, this research fails to handle exploration\u2013exploitation tradeoff in this domain caused by the fact that defenders only have knowledge of attack activities at targets they protect. The problem is formulated as a Restless Multi\u2013Armed Bandit (RMAB) model to address this challenge. To use Whittle index policy to plan for patrol strategies in the RMAB, two sufficient conditions for indexability and an algorithm to numerically evaluate indexability are provided. Given indexability, a binary search based algorithm to find Whittle index policy efficiently is proposed.\nIt is assumed that defenders have knowledge of all poaching activities throughout the wildlife protected area. Unfortunately, given vast geographic areas for wildlife protection, defenders do not have knowledge of poaching activities in areas they do not protect. Thus, defenders are faced with the exploration\u2013 exploitation tradeoff. The exploration\u2013exploitation tradeoff here is different from that in the non\u2013Bayesian stochastic multi\u2013armed bandit problem. In stochastic multi\u2013armed bandit problems, the rewards of every arm are random variables with a stationary unknown distribution. However, in our problem, patrol affects attack activities, i.e.m, more patrol is likely to decrease attack activities and less patrol is likely to increase attack activities. Thus, the random variable distribution is changing depending on player\u2019s choice \u2014 more selection (patrol) leads to lower reward (less attack activities) and less selection (patrol) leads to higher reward (more attack activities). On the other hand, adversarial multi\u2013armed bandit problem is also not an appropriate model for this domain. In adversarial multi\u2013armed bandit problems, the reward can arbitrarily change while the attack activities in our problem are unlikely to change rapidly in a short period.\nPoaching activity is a dynamic process affected by patrol. If patrollers patrol in a certain location frequently, it is very likely that the poachers poaching in this location will switch to other locations for poaching. On the other hand, if a location has not been patrolled for a long time, poachers may gradually notice that and switch to this location for poaching. In the wildlife protection domain, both patrollers and poachers do not have perfect observation of their opponents\u2019 actions. This observation imperfection lies in two aspects: limited observability \u2014 patrollers/poachers do not know what happens at locations they do not patrol/poach; partial observability \u2014 patrollers/poachers do not have perfect observation even at locations they patrol/poach \u2014 the location might be large (e.g., a 2km \u00d7 2km area) so that it is possible that patrollers and poachers do not see each other even if they are at the same location. These two properties make it extremely difficult for defenders to optimally plan their patrol strategies."}, {"heading": "4.1.1 Problem formulation", "text": "There are n targets that are indexed by N = {1, . . . , n}. Defenders have k patrol resources that can be deployed to these n targets. At every round, defenders choose k targets to protect. After that, defenders will have an observation of the number of attack activities for targets they protect, and no information for targets they do not protect. The objective for defenders is to decide which k targets to protect at every round to catch as many attackers as possible. Due to the partial observability on defenders\u2019 side (defenders\u2019 observation of attack activities is not perfect even for targets they protect), a hidden variable attack intensity is introduced, representing the true degree of attack intensity at a certain target. Clearly, this hidden variable attack intensity cannot directly be observed by defenders. Instead, defenders\u2019 observation is a random variable conditioned on this hidden variable attack intensity, and the larger the attack intensity is, the more likely it is for defenders to observe more attack activities during their patrol. The hidden variable attack intensity is discretized into ns levels, denoted by S = {0, 1, . . . , ns \u2212 1}. Lower i represents lower attack intensity. For a certain target, its attack intensity transitions after every round. If this target is protected, attack intensity transitions according to a ns \u00d7 ns transition matrix T 1; if this target is not protected, attack intensity transitions according to another ns\u00d7ns transition matrix T\n0. The transition matrix represents how patrol affects attack intensity \u2014 T 1 tends to reduce attack intensity and T 0 tends to increase attack intensity. Note that different targets may have different transition matrices because some targets may be more attractive to attackers (for example, some locations may have more animal resources in the wildlife protection domain) so that it is more difficult for attack intensity to go down and easier for attack intensity to go up. Also defenders\u2019 observations of attack activities are discretized into no levels, denoted by O = {0, 1, . . . , no\u22121}. Lower i represents less attack activities defenders observe. Note that defenders will only have observation for targets they protect. A ns \u00d7 no observation matrix O determines how the observation depends on the hidden variable attack intensity. Generally, the larger the attack intensity is, the more likely it is for defenders to observe more attack activities during their patrol. Similar to transition matrices, different targets may have different observation matrices. While defenders get observations of attack activities during their patrol, they also receive rewards for that \u2014 arresting poachers/fareevaders/smugglers bring benefit. Clearly, the reward defenders receive depends on their observation and thus the reward function is defined as R(o), o \u2208 O \u2014 larger i leads to higher reward R(i). Note that defenders only get rewards for targets they protect. To summarize, for the targets defenders protect, defenders get an observation depending on its current attack intensity, get the reward associated with the observation, and then the attack intensity transitions according to T 1; for the targets defenders do not protect, defenders do not have any observation, get reward 0 and the attack intensity transitions according to T 0."}, {"heading": "4.1.2 Learning model from defenders\u2019 previous observations", "text": "Given defenders\u2019 action history {ai} and observation history {oi}, our objective is to learn the transition matrices T 1 and T 0, observation matrix O and initial belief \u03c0. Due to the existence of hidden variables {si}, expectation\u2013 maximization (EM) algorithm is used for learning. The update steps are the following:\n\u03c0 (d+1) i = P (s1 = i|x; \u03b8 d)\nT 1(d+1) ij =\n\u2211T\u22121 t=1:at=1 P (st = i, st+1 = j|x; \u03b8 d)\n\u2211T\u22121 t=1:at=1 P (st = i|x; \u03b8d)\nT 0(d+1) ij =\n\u2211T\u22121 t=1:at=0 P (st = i, st+1 = j|x; \u03b8d) \u2211T\u22121\nt=1:at=0 P (st = i|x; \u03b8d)\nO (d+1) ij =\n\u2211T t=1:at=1 P (st = i|x; \u03b8d)I(ot = j) \u2211T\nt=1:at=1 P (st = i|x; \u03b8d)"}, {"heading": "4.1.3 Restless multi\u2013armed bandit problems", "text": "In RMABs, each arm represents an independent Markov machine. At every round, the player chooses k out of n arms (k < n) to activate and receives the reward determined by the state of the activated arms. After that, the states of all arms will transition to new states according to certain Markov transition probabilities. The problem is called restless because the states of passive arms will also transition like active arms. The aim of the player is to maximize his cumulative reward by choosing which arms to activate at every round. It is PSPACE\u2013hard to find the optimal strategy to general RMABs [13]. An index policy assigns an index to each state of each arm to measure how rewarding it is to activate an arm at a particular state. At every round, the index policy chooses to pick the k arms whose current states have the highest indices. Since the index of an arm only depends on the properties of this arm, index policy reduces an n\u2013dimensional problem to n 1\u2013dimensional problems so that the complexity is reduced from exponential with n to linear with n. Whittle proposed a heuristic index policy for RMABs by considering the Lagrangian relaxation of the problem [24]. It has been shown that Whittle index policy is asymptotically optimal under certain conditions as k and n tend to infinity with k/n fixed. When k and n are finite, extensive empirical studies have also demonstrated the near\u2013optimal performance of Whittle index policy. Whittle index measures how attractive it is to activate an arm based on the concept of subsidy for passivity. It gives the subsidy m to passive action (not activate) and the smallest m that would make passive action optimal for the current state is defined to be the Whittle index for this arm at this state. Whittle index policy chooses to activate the k arms with the highest Whittle indices. Intuitively, the larger the m is, the larger the gap is between active action (activate) and\npassive action, the more attractive it is for the player to activate this arm. Mathematically, denote Vm(x; a = 0) (Vm(x; a = 1)) to be the maximum cumulative reward the player can achieve until the end if he takes passive (active) action at the first round at the state x with subsidy m. Whittle index I(x) of state x is then defined to be:\nI(x) = infm{m : Vm(x; a = 0) \u2265 Vm(x; a = 1)}\nHowever, Whittle index only exists and Whittle index policy can only be used when the problem satisfies a property known as indexability. Define \u03a6(m) to be the set of states for which passive action is the optimal action given subsidy m:\n\u03a6(m) = {x : Vm(x; a = 0) \u2265 Vm(x; a = 1)}\nDefinition 4. An arm is indexable if \u03a6(m) monotonically increases from \u2205 to the whole state space as m increases from \u2212\u221e to +\u221e. An RMAB is indexable if every arm is indexable."}, {"heading": "4.1.4 Restless bandit formulation", "text": "Every target is viewed as an arm and defenders choose k arms to activate (k targets to protect) at every round. Consider a single arm (target), it is associated with ns (hidden) states, no observations, ns\u00d7ns transition matrices T\n1 and T 0, ns \u00d7 no observation matrix O and reward function R(o), o \u2208 O. For the arm defenders activate, defenders get an observation, get reward associated with the observation, and the state transitions according to T 1. Note that defenders\u2019 observation is not the state. Instead, it is a random variable conditioned on the state, and reveals some information about the state. For the arms defenders do not activate, defenders do not have any observation, get reward 0 and the state transitions according to T 0. Since defenders can not directly observe the state, defenders maintain a belief b of the states for each target, based on which defenders make decisions. The belief is updated according to the Bayesian rules. The following equation shows the belief update when defenders protect this target (a = 1) and get observation o or defenders do not protect this target (a = 0).\nb\u2032(s\u2032) =\n{\n\u03b7 \u2211 s\u2208S b(s)OsoT 1 ss\u2032 , a = 1 \u2211\ns\u2208S b(s)OsoT 0 ss\u2032 , a = 0\nwhere \u03b7 is the normalization factor. When defenders do not protect this target (a = 0), defenders do not have any observation, so their belief is updated according to the state transition rule. When defenders protect this target (a = 1), their belief is firstly updated according to their observation o (bnew(s) = \u03b7b(s)Oso according to Bayes\u2019 rule), and then the new belief is then updated according to the state transition rule:\nb\u2032(s\u2032) = \u2211\ns\u2208S\nbnew(s)T 1 ss\u2032\n= \u2211\ns\u2208S\n\u03b7b(s)OsoT 1 ss\u2032\n= \u03b7 \u2211\ns\u2208S\nb(s)OsoT 1 ss\u2032\nNow the mathematical definition of Whittle index is presented. Denote Vm(b) to be the value function for belief state b with subsidy m. Vm(b; a = 0) to be the value function for belief state b with subsidym and defenders take passive action. Vm(b; a = 1) to be the value function for belief state b with subsidy m and defenders take active action. The following equations show these value functions:\nVm(b; a = 0) = m+ \u03b2Vm(ba=0) Vm(b; a = 1) = \u2211\ns\u2208S\nb(s) \u2211\no\u2208O\nOsoR(o) + \u03b2 \u2211\no\u2208O\n\u2211\ns\u2208S\nb(s)OsoVm(b o a=1)\nVm(b) = max{Vm(b; a = 0), Vm(b; a = 1)}\nWhen defenders take passive action, they get the immediate reward m and the \u03b2\u2013discounted future reward \u2014 value function at new belief ba = 0, which is updated from b according to the case a = 0. When defenders take active action, they get the expected immediate reward \u2211\ns\u2208S b(s) \u2211\no\u2208O OsoR(o) and the \u03b2\u2013discounted future reward. The value function Vm(b) is the maximum of Vm(b; a = 0) and Vm(b; a = 1). Whittle index I(b) of belief state b is then defined to be:\nI(b) = infm{m : Vm(b; a = 0) \u2265 Vm(b; a = 1)}\nThe passive action set \u03a6(m), which is the set of belief states for which passive action is the optimal action given subsidy m is then defined to be:\n\u03a6(m) = infm{b : Vm(b; a = 0) \u2265 Vm(b; a = 1)}"}, {"heading": "4.1.5 Sufficient condition for indexability", "text": "Two sufficient conditions for indexability when no = 2 and ns = 2 are provided. Denote the transition matrices to be T 0 and T 1, observation matrix to be O. Clearly in our problem, O11 > O01, O00 > O10 (higher attack intensity leads to higher probability to see attack activities when patrolling); T 111 > T 1 01, T 1 00 > T 1 10, T 011 > T 0 01, T 0 00 > T 0 10 (positively correlated arms).\nDefine \u03b1 = max{T 011 \u2212T 1 01, T 0 11 > T 1 01}. Since it is a two\u2013state problem with S = {0, 1}, x represents the belief state: x = b(s = 1), which is the probability of being in state 1.\nDefine \u03931(x) = xT 1 11 + (1 \u2212 x)T 1 01 which is the belief for the next round if the belief for the current round is x and the active action is taken. Similarly, \u03930(x) = xT 0 11 + (1 \u2212 x)T 0 01, which is the belief for the next round if the belief for the current round is x and the passive action is taken. Below two theorems demonstrating two sufficient conditions for indexability are presented.\nTheorem 6. When \u03b1 \u2264 0.5, the process is indexable, i.e., for any belief x, if Vm(x; a = 0) \u2265 Vm(x; a = 1), then Vm(x; a = 0) \u2265 V m(x; a = 1), \u2200m \u2265 m.\nTheorem 7. When \u03b1\u03b2 \u2264 0.5 and \u03931(1) \u2264 \u03930(0), the process is indexable, i.e., for any belief x, if Vm(x; a = 0) \u2265 Vm(x; a = 1), then Vm(x; a = 0) \u2265 V m(x; a = 1), \u2200m \u2265 m."}, {"heading": "4.1.6 Numerical evaluation of indexability", "text": "Proposition 1. If m < R(0)\u03b2R(no\u22121)\u2212R(0)1\u2212\u03b2 ,\u03a6(m) = \u2205; if m > R(no\u2212 1),\u03a6(m) is the whole belief state space.\nThus, it should be determined whether the set \u03a6(m) monotonically increases\nfor m \u2286 [ R(0)\u03b2R(no\u22121)\u2212R(0)1\u2212\u03b2 , R(no \u2212 1) ] . Numerically, this limited m range is discretized and then it is evaluated if \u03a6(m) monotonically increases with the increase of discretized m. Given the subsidy m, \u03a6(m) can be determined by solving a special POMDP model whose conditional observation probability is dependent on start state and action. The algorithm returns a set D which contains ns\u2013length vectors d1, d2, . . . , d|D|. Every vector di is associated with an optimal action ei. Given the belief b, the optimal action is determined by aopt = ei, i = argmaxj b Tdj . Thus, \u03a6(m) = \u222ai:ei=0{b : b Tdi \u2265 bTdj , \u2200j}.\nGiven m0 < m1, our aim is to check whether \u03a6(m0) \u2286 \u03a6(m0). A MILP to verify whether such condition holds can be solved."}, {"heading": "4.1.7 Computation of Whittle index Policy", "text": "Given the indexability, Whittle index can be found by doing a binary search within the range m \u2286 [\nR(0)\u03b2R(no\u22121)\u2212R(0)1\u2212\u03b2 , R(no \u2212 1) ] . Given the upper bound\nub and lower bound lb, the problem with middle point lb+ub2 as passive subsidy is sent to the special POMDP solver to find the optimal action for the current belief. If the optimal action is active, then the Whittle index is greater than the middle point so lb \u2190 lb+ub2 or else ub \u2190 lb+ub 2 . This binary search algorithm can find Whittle index with arbitrary precision. Naively, the Whittle index policy can be found by computing the \u01eb\u2013precision indices of all arms and then picking the k arms with the highest indices.\nSpecifically, let A be the Whittle index policy to be returned: it is set to be \u2205 at the beginning. S is the set of arms that are not known whether belonging to A or not and is set to be the whole set of arms at the beginning. Before it finds top\u2013k arms, it tests all the arms in S about their optimal action with subsidy lb+ub2 . If the optimal action is 1, it means this arm\u2019s index is higher\nthan lb+ub2 and it is added to S1; if the optimal action is 0, it means this arm\u2019s index is lower than lb+ub2 and it is added to S0. At this moment, all arms in S1 have higher indices than all arms in S0. If there is enough space in A to include all arms in S1, S1 is added to A, remove them from S and set the upper bound to be lb+ub2 because S1 belongs to Whittle index policy set and all the rest arms have the index lower than lb+ub2 . If there is not enough space in A, S0 is removed from S and set the lower bound to be lb+ub\n2 because S0 does not belong to Whittle index policy set and all the rest arms have the index higher than lb+ub2 ."}, {"heading": "4.1.8 Special POMDP formulation", "text": "Here, the algorithm to compute the passive action set \u03a6(m) with the subsidy m is discussed. This problem can be viewed as solving a special POMDP model whose conditional observation probability is dependent on start state and action while the conditional observation probability is dependent on end state and action in standard POMDPs.\nThe original state is s, the agent takes action a, and the state transitions to s\u2032 according to P (s\u2032|s, a). However, the observation o the agent gets during this process is dependent on s and a in our special POMDPs; while it depends on s\u2032 and a in standard POMDPs.\nThe special POMDP formulation for our problem is straightforward.\n\u2022 The state space is S = {0, 1, . . . , ns \u2212 1}.\n\u2022 The action space is A = {0, 1}, where a = 0 represents passive action (do not protect) and a = 1 represents active action (protect).\n\u2022 The observation space is O = {\u22121, 0, 1, . . . , no \u2212 1}. It adds a fake observation o = \u22121 to represent no observation when taking action a = 0. It\u2019s called fake because defenders have probability 1 to observe o = 1 no matter what the state is when they take action a = 0, so this observation does not provide any information. When defenders take action a = 1, they may observe observations O\\{\u22121}.\n\u2022 The conditional transition probability P (s\u2032|s, a) is defined to be P (s\u2032 = j|s = i, a = 1) = T 1ij and P (s \u2032 = j|s = i, a = 0) = T 0ij .\n\u2022 The conditional observation probability P (o|s, a) is defined to be P (o = \u22121|s, a = 0) = 1, \u2200s \u2208 S;P (o = j|s = i, a = 1) = Oij . Note that the conditional observation probability here is dependent on the start state s and action a, while it depends on end state s and action a in standard POMDP models. Intuitively, defenders\u2019 observation of attack activities today depends on the attack intensity today, not the transitioned attack intensity tomorrow.\n\u2022 The reward function R is:\nR(s, s\u2032, a, o) =\n{\n0, a = 0\nR(o), a = 1\nWith the transition probability and observation probability, R(s, a) can be computed. Note that this formulation is also slightly different due to the different definition of observation probability.\nR(s, a) = \u2211\ns\u2032\u2208S\nP (s\u2032|s, a) \u2211\no\u2208O\nP (o|s, a)R(s, s\u2032, a, o)"}, {"heading": "4.1.9 Value iteration for the special POMDP", "text": "Different from standard POMDP formulation, the belief update in the special POMDP formulation is:\nb\u2032(s\u2032) =\n\u2211\ns\u2032\u2208S b(s)P (o|s, a)P (s \u2032|s, a)\nP (o|b, a)\nwhere\nP (o|b, a) = \u2211\ns\u2032\u2208S\n\u2211\ns\u2208S\nb(s)P (o|s, a)P (s\u2032|s, a) = \u2211\ns\u2208S\nb(s)P (o|s, a)\nSimilar to standard POMDP formulation, the value function is:\nV \u2032(b) = maxa\u2208A\n(\n\u2211\ns\u2208S\nb(s)R(s, a) + \u03b2 \u2211\no\u2208O\nP (o|b, a)V (boa)\n)\nwhich can be broken up to simpler combinations of other value functions:\nV \u2032(b) = maxa\u2208AVa(b) Va(b) = \u2211\no\u2208O\nV oa (b)\nV oa (b) =\n\u2211\ns\u2208S b(s)R(s, a)\n|O| + \u03b2P (o|b, a)V (boa)\nAll the value functions can be represented as V (b) = max\u03b1\u2208Db\u03b1 since the update process maintains this property, so the set D is updated when updating the value function."}, {"heading": "4.1.10 Planning from POMDP view", "text": "Every single target can be modeled as a special POMDP model. Given that, these POMDP models at all targets can be combined to form a special POMDP model that describe the whole problem, and solving this special POMDP model leads to defenders\u2019 exact optimal strategy. Use the superscript i to denote target i. Generally, the POMDP model for the whole problem is the cross product of the single\u2013target POMDP models at all targets with the constraint that only k targets are protected at every round.\n\u2022 The state space is S = S1 \u00d7 S2 \u00d7 . . .\u00d7 Sn. Denote s = (s1, s2, . . . , sn).\n\u2022 The action space is A = {(a1, a2, . . . , an)|aj \u2208 {0, 1}, \u2200j \u2208 N, \u2211 j\u2208N a j =\nk}, which represents that only k targets can be protected at a round. Denote a = (a1, a2, . . . , an).\n\u2022 The observation spaceO = O1\u00d7O2\u00d7. . .\u00d7On. Denote o = (o1, o2, . . . , on).\n\u2022 The conditional transition probability is P (s\u2032|s, a) = \u220f j\u2208N P j(s \u2032j |sj , aj).\n\u2022 The conditional observation probability is P (o|s, a) = \u220f j\u2208N P j(oj |sj , aj).\n\u2022 The reward function is R(s, s\u2032, a, o) = \u220f j\u2208N R(s j , s \u2032j,aj ,oj ).\nSilver and Veness [20] have proposed POMCP algorithm, which provides high quality solutions and is scalable to large POMDPs. The POMCP algorithm only requires a simulator of the problem so it also applies to our special POMDPs. At a high level, the POMCP algorithm is composed of two parts: it uses a particle filter to maintain an approximation of the belief state; it draw state samples from the particle filter and then use MCTS to simulate what will happen next to find the best action. It uses a particle filter to approximate the belief state because it is even computationally impossible in many problems to update belief state due to the extreme large size of the state space. However, in our problem, the all\u2013target POMDP model is the cross product of the single\u2013target POMDP models at all targets. The single\u2013state POMDP model is small so that it is computationally inexpensive to maintain its belief state. Thus, the state si at target i can be sampled from its belief state and then compose them together to get the state sample s = (s1, s2, . . . , sn) for the all\u2013target POMDP model."}, {"heading": "4.2 Online planning for optimal protector strategies in resource conservation games", "text": "Protectors (law enforcement agencies) try to protect natural resources, while extractors (criminals) seek to exploit them. In many domains, such as illegal fishing, the extractors know more about the distribution and richness of the resources than the protectors, making it extremely difficult for the protectors to optimally allocate their assets for patrol and interdiction. Fortunately, extractors carry out frequent illegal extractions, so protectors can learn about the richness of resources by observing the extractor\u2019s behavior.\nIn resource conservation domains, the protector often does not know the distribution of resources while the extractor may have more information about it, e.g. preventing illegal fishing. Our goal is to provide an optimal asset deployment (e.g., patrol) strategy for the protector, given her lack of knowledge about the distribution of resources."}, {"heading": "4.2.1 Problem formulation", "text": "In resource conservation games, the extractor\u2019s frequent illegal extractions provide the protector with the opportunity to learn about the distribution of resources by observing the extractor\u2019s behavior. The aim is the construction of an online policy for the protector to maximize her utility given observations of the extractor. At every round, the protector chooses one site to protect and the extractor simultaneously chooses one site to steal from. Both the extractor and the protector have full knowledge about each other\u2019s previous actions.\nIn our model, the amount of resources at each site will be fixed and the extractor will have full knowledge of this distribution. The protector will have to learn this distribution by observing the extractor\u2019s behavior.\nThere is a finite time horizon t \u2208 T . There are n sites indexed by N = {1, 2, . . . , n} that represent the locations of the natural resource in question: the extractor wants to steal resources from these sites and the protector wants to interdict the extractor. The value of the sites to the extractor is represented in terms of their utilities. Each site has a utility u(i) that is only known to the extractor. The utility space is discretized into m levels, u(i) \u2208 M = {1, 2, . . . ,m}. Human beings cannot distinguish between tiny differences in utilities in the real world, so discretizing these utilities is justified. For n sites and m utility levels, there are mn possible sets of utilities across all sites. The distribution of resources is then captured by the vector of utilities at each site, and the set of possible resource distributions is:\nU = {(u(1), u(2), . . . , u(n)) : u(i) \u2208 M, \u2200i \u2208 N}\nAssuming that the resource levels u(i), i \u2208 N are independent from each other, at the beginning of the game, the protector may have some prior knowledge about the resource levels u(i) at each site i \u2208 N . This prior knowledge is represented as a probability density function p(u(i)) over M . If the protector does not know anything about u(i), then a uniform prior for u(i) over M is adopted. At each time t \u2208 T , the protector chooses a site at \u2208 N to protect and the extractor simultaneously chooses a site ot \u2208 N from which to steal. If at = ot, the protector catches the extractor and the extractor is penalized by the amount P (ot) < 0; if at 6= ot, the extractor successfully steals resources from site ot and gets a payoff of u(ot). For clarity, the protector\u2019s interdiction is always successful whenever it visits the same site as the extractor. Additionally, the protector fully observes the moves of the extractor, likewise, the extractor fully observes the moves of the protector. Note that the penalty P (i), i \u2208 N is known to both the protector and the extractor. A zero\u2013sum game is adopted,\nso the protector is trying to minimize the extractor\u2019s payoffs. In most resource conservation domains, the extractor pays the same penalty P if he is seized independent of the site he visits. Varying penalties across sites for greater generality are allowed. A fictitious Quantal Response playing (FQR) extractor is assumed. Specifically, a fictitious extractor assumes the protector\u2019s empirical distribution will be his mixed strategy in the next round. In this behavior model, the extractor makes decisions based on the parameters u(i), P (i), i \u2208 N , as well as the protector\u2019s actions in previous rounds.\nThe extractor behaves in the following way: in every round, he computes the empirical coverage probability ci for every site i based on the history of the protector\u2019s actions, then computes the expected utility EU(i) = c(i)P (i) + (1\u2212 c(i))u(i) for every site and finally attempts to steal from the site i with the probability proportional to e\u03bbEU(i) where \u03bb \u2265 0 is the parameter representing the rationality of the player (higher \u03bb represents a more rational player).\nTo implement the above model, two technical questions must be resolved. First, at every round t, based on her current belief about u, how should the protector choose sites to protect in the next round? Second, after each round, how should the protector use the observation of the latest round to update her beliefs about u? Here, decision making and belief updating in a partially observable environment, where the payoffs u are unobservable and the extractor\u2019s actions are observable, are being studied, which is the exact setup for a POMDP. A two\u2013 player game is now setup as a POMDP {S,A,O, T,\u2126, R} where the extractor follows a quantal response model.\n\u2022 The state space of our POMDP is S = U \u00d7Zn, which is the cross product of the utility space and the count space. U is the utility space. Zn is the set of possible counts of the protector\u2019s visits to each site, where Ct \u2208 Zn\nis an integer-valued vector where Ct(i), i \u2208 N is the number of times that the protector has protected site i at the beginning of round t \u2208 T . A particular state s \u2208 S is written as s = (u,C), where u is the vector of utility levels for each site and C is the current state count. The initial beliefs are expressed by a distribution over s = (u, 0), induced by the prior distribution on u. ct(i) = Ct(i) t\u22121 denotes the frequency with which the protector visits site i at the beginning of round t \u2208 T . c1 = 0 by convention.\n\u2022 The action space A is N , representing the site the protector chooses to protect.\n\u2022 The observation space O is N , representing the site the extractor chooses to attempt to steal from.\n\u2022 Let ea \u2208 Rn denote the unit vector with a 1 in slot a \u2208 N and zeros elsewhere. The conditional transition probability T governing the evolution of the state is:\nT (s\u2032 = (u\u2032, C\u2032)|s = (u,C), a) =\n{\n1 u = u\u2032, C\u2032 = C + ea,\n0, otherwise\nSpecifically, the evolution of the state is deterministic. The underlying utilities do not change, and the count for the site visited by the protector increases by one while all others stay the same.\n\u2022 EU(u,C) \u2208 Rn defines the vector of empirical expected utilities for the extractor for all sites when the actual utility is u and the count is C,\n[EU(u,C)](i) = c(i)P (i) + (1\u2212 c(i))u(i), \u2200i \u2208 N,\nwhen t \u2265 1. [EU(u, 0)](i) = u(i) by convention. Hence, our observation probabilities are explicitly:\n\u2126(o|s\u2032 = (u,C), a) = e\u03bb[EU(u,C\u2212ea)](o) \u2211\ni\u2208N e \u03bb[EU(u,C\u2212ea)](i)\nthe probability of observing the extractor takes action o when the protector takes action a and arrives at state s. Note that both a and o are the actions the protector/extractor take at the same round.\n\u2022 The reward function R is:\nR(s = (u,C), s\u2032 = (u,C + ea), a, o) =\n{\n\u2212P (o), a = o\n\u2212u(o), a 6= o"}, {"heading": "4.2.2 GMOP algorithm", "text": "The size of the utility space U is mn, and the size of the count space is O ( Tn\nn!\n)\n. The computational cost of the latest POMDP solvers soon become unaffordable for us as the problem size grows.\nSilver and Veness [20] have proposed the POMCP algorithm, which provides high quality solutions for large POMDPs. The POMCP algorithm uses a particle filter to approximate the belief state. Then, it uses Monte Carlo tree search (MCTS) for online planning where state samples are drawn from the particle filter and the action with the highest expected utility based on Monte Carlo simulations is chosen. However, the particle filter is only an approximation of the true belief state and is likely to move further away from the actual belief state as the game goes on, especially when most particles get depleted and new particles need to be added. Adding new particles will either make the particle filter a worse approximation of the exact belief state, if the added particles do not follow the distribution of the belief state or be as difficult as drawing samples directly from the belief state, if the added particles do follow the distribution of the belief state.\nOur POMDP has specific structure that can be exploited. The count state in S is known and the utility state does not change, making it possible to draw samples directly from the exact belief state using Gibbs sampling. We propose the GMOP algorithm that draws samples directly from the exact belief state using Gibbs sampling, and then runs MCTS. The samples drawn directly from the belief state better represent the true belief state compared to samples drawn from a particle filter.\nAt a high level, in round t the protector draws samples of state s from its belief state Bt(s) using Gibbs sampling and then it runs MCTS using those samples. Finally, it executes the action with the highest expected utility. MCTS starts with a tree that only contains a root node. Since the count state Ct is already known, the protector only needs to sample the utility state u from Bt. The sampled state s is comprised of the sampled utility u and the count Ct.\nGibbs sampling [2] is a Markov chain Monte Carlo (MCMC) algorithm for sampling from multivariate probability distributions. Let X = (x1, x2, . . . , xn) be a general random vector with n components and with finite support described by the multivariate probability density p(X). Gibbs sampling only requires the conditional probabilities p(xi|xi) to simulate X , where xi = (xj)j 6=i denotes the subset of all components of X except component i. Gibbs sampling is useful when direct sampling from p(X) is difficult. Suppose k samples of X = (x1, x2, . . . , xn) should be obtained.\nGibbs sampling works in general to produce these samples using only the conditional probabilities p(xi|x\u2212i). It constructs a Markov chain whose steadystate distribution is given by p(X), so that the samples also follow the distribution p(X). The states of this Markov chain are the possible realizations of X = (x1, x2, . . . , xn), and a specific state Xi is denoted as Xi = (xi1, xi2, . . . , xin) (there are finitely many such states by our assumption). The transition probabilities of this Markov chain, Pr(Xj |Xi), follow from the conditional probabilities p(xi|x\u2212i). Specifically, Pr(Xj |Xi) = p(xl|x\u2212l) when xjv = xiv for all v not equal to l, and is equal to zero otherwise, i.e. the state transitions only change one component of the vector\u2013valued sample at a time. This Markov chain is reversible (meaning p(Xi)Pr(Xj |Xi) = P (Xj)Pr(Xi|Xj), \u2200i, j) so p(X) is its steady\u2013state distribution."}, {"heading": "4.2.3 Applying Gibbs sampling in GMOP", "text": "Let Bt be the probability distribution representing the protector\u2019s beliefs about the true utilities at the beginning of round t \u2265 1; B1 represents the protector\u2019s prior beliefs when the game starts. Let Bt(u) denote the probability of the vector of utilities u with respect to the distribution Bt. Let B be the prior belief distribution and B be the posterior belief distribution. Our Bayesian belief update rule to obtain B from B and the observation is explicitly:\nB\u2032(s\u2032 = (u,C)) = \u03b7\u2126(o|s\u2032, a) \u2211\ns\u2208S\nT (s\u2032|s, a)B(s)\n= \u03b7\u2126(o|s\u2032, a)B(s = (u,C \u2212 ea))\nIf at and ot represent the actions that the protector and the extractor choose to take at round t:\nBt(u) = \u03b7Bt\u22121(u)\u2126(ot\u22121|s = (u,Ct), at\u22121)\n= \u03b7\u2032B1(u)\nt\u22121 \u220f\ni=1\n\u2126(oi|s = (u,Ci+1), ai)\nIt follows that the posterior belief Bt is proportional to the prior belief B1 multiplied by the observation probabilities over the entire history. Since there are mn possible utilities, it is impossible to store and update Bt when m and n are large, and thus it is impossible to sample directly from Bt. Hence, Gibbs sampling is adopted.\nOnly the conditional probabilities p(ui|u\u2212i), \u2200i in Bt are needed:\np(ui|u\u2212i) = \u03b7p(ui, u\u2212i) = \u03b7Bt(ui, u\u2212i) =\n\u03b7\u2032B1(ui, u\u2212i)\nt\u22121 \u220f\nj=1\n\u2126(oj |s = (u = (ui, u\u2212i), Cj+1), aj) =\n\u03b7\u2032\u2032B1(ui) t\u22121 \u220f\nj=1\n\u2126(oj |s = (u = (ui, u\u2212i), Cj+1), aj)\nThis quantity is easy to compute where B1(ui) is the prior probability that site i has utility ui. Besides the conditional probability, also a valid u with Bt(u) > 0 should be found to initialize Gibbs sampling. Finding such a u is easy in our FQR model because any u with B1(u) > 0 satisfies Bt(u) > 0."}], "references": [{"title": "Addressing scalability and robustness in security games with multiple boundedly rational adversaries", "author": ["Matthew Brown", "William B Haskell", "Milind Tambe"], "venue": "In Decision and Game Theory for Security,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Explaining the gibbs sampler", "author": ["George Casella", "Edward I George"], "venue": "The American Statistician,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "When security games go green: Designing defender strategies to prevent poaching and illegal fishing", "author": ["Fei Fang", "Peter Stone", "Milind Tambe"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Robust protection of fisheries with compass", "author": ["William B Haskell", "Debarun Kar", "Fei Fang", "Milind Tambe", "Sam Cheung", "Elizabeth Denicola"], "venue": "In AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A game of thrones: when human behavior models compete in repeated stackelberg security games", "author": ["Debarun Kar", "Fei Fang", "Francesco Delle Fave", "Nicole Sintov", "Milind Tambe"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Quantal response equilibria for normal form games", "author": ["Richard D McKelvey", "Thomas R Palfrey"], "venue": "Games and economic behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Making the most of our regrets: Regret-based solutions to handle payoff uncertainty and elicitation in green security games", "author": ["Thanh H Nguyen", "Francesco M Delle Fave", "Debarun Kar", "Aravind S Lakshminarayanan", "Amulya Yadav", "Milind Tambe", "Noa Agmon", "Andrew J Plumptre", "Margaret Driciru", "Fred Wanyama"], "venue": "In Decision and Game Theory for Security,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Capture: A new predictive anti-poaching tool for wildlife protection", "author": ["Thanh H Nguyen", "Arunesh Sinha", "Shahrzad Gholami", "Andrew Plumptre", "Lucas Joppa", "Milind Tambe", "Margaret Driciru", "Fred Wanyama", "Aggrey Rwetsiba", "Rob Critchlow"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Conquering adversary behavioral uncertainty in security games: An efficient modeling robust based algorithm. 2016", "author": ["Thanh H Nguyen", "Arunesh Sinha", "Milind Tambe"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Analyzing the effectiveness of adversary modeling in security games", "author": ["Thanh Hong Nguyen", "Rong Yang", "Amos Azaria", "Sarit Kraus", "Milind Tambe"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The complexity of optimal queuing network control", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Deployed armor protection: the application of a game theoretic model for security at the los angeles international airport", "author": ["James Pita", "Manish Jain", "Janusz Marecki", "Fernando Ord\u00f3\u00f1ez", "Christopher Portway", "Milind Tambe", "Craig Western", "Praveen Paruchuri", "Sarit Kraus"], "venue": "In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems: industrial track,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Online planning for optimal protector strategies in resource conservation games. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 733\u2013740", "author": ["Yundi Qian", "William B Haskell", "Albert Xin Jiang", "Milind Tambe"], "venue": "International Foundation for Autonomous Agents and Multiagent Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Restless poachers: Handling exploration-exploitation tradeoffs in security domains", "author": ["Yundi Qian", "Chao Zhang", "Bhaskar Krishnamachari", "Milind Tambe"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Protect: A deployed game theoretic system to protect the ports of the united states", "author": ["Eric Shieh", "Bo An", "Rong Yang", "Milind Tambe", "Craig Baldwin", "Joseph Di- Renzo", "Ben Maule", "Garrett Meyer"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems- Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Monte-carlo planning in large pomdps", "author": ["David Silver", "Joel Veness"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "On the stackelberg strategy in nonzero-sum games", "author": ["Marwan Simaan", "Jose B Cruz Jr."], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1973}, {"title": "Learning adversary behavior in security games: A pac model perspective", "author": ["Arunesh Sinha", "Debarun Kar", "Milind Tambe"], "venue": "arXiv preprint arXiv:1511.00043,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Restless bandits: Activity allocation in a changing world", "author": ["Peter Whittle"], "venue": "Journal of applied probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Adaptive resource allocation for wildlife protection against illegal poachers", "author": ["Rong Yang", "Benjamin Ford", "Milind Tambe", "Andrew Lemieux"], "venue": "In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Computing optimal strategy against quantal response in security games", "author": ["Rong Yang", "Fernando Ordonez", "Milind Tambe"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Trusts: Scheduling randomized patrols for fare inspection in transit systems using game theory", "author": ["Zhengyu Yin", "Albert Xin Jiang", "Milind Tambe", "Christopher Kiekintveld", "Kevin Leyton-Brown", "Tuomas Sandholm", "John P Sullivan"], "venue": "AI Magazine,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 7, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 4, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 6, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 13, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 12, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 16, "context": "In zero\u2013sum games, the Leader\u2013follower equilibrium coincides with the Nash equilibrium [21] and the maxmin/minmax strategies.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "Building upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "Building upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Building upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": ", protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25].", "startOffset": 42, "endOffset": 48}, {"referenceID": 3, "context": ", protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25].", "startOffset": 42, "endOffset": 48}, {"referenceID": 19, "context": ", protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25].", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "1 SUQR: modeling a boundedly rational attacker In SSGs, attacker bounded rationality is often modeled via behavior models such as Quantal Response (QR) [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "Inspired by the idea of SEU, it has been proposed [12] a subjective utility function of the adversary for SSG as the following:", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "2 From MATCH to SHARP Starting from the above results, a new model is now introduced, SHARP [6], which:", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Let the utility function of the attacker for target t \u2208 T be denoted by ut : [0, 1] \u2192 R.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "\u00fbt : [0, 1] \u2192 R uniformly approximates or uniformly learns ut(\u00b7) within an error of \u01eb, if \u2200x \u2208 [0, 1], |\u00fbt(x) \u2212 ut(x)| \u2264 \u01eb.", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "\u00fbt : [0, 1] \u2192 R uniformly approximates or uniformly learns ut(\u00b7) within an error of \u01eb, if \u2200x \u2208 [0, 1], |\u00fbt(x) \u2212 ut(x)| \u2264 \u01eb.", "startOffset": 95, "endOffset": 101}, {"referenceID": 0, "context": "Let the utility function of the defender on target t \u2208 T be denoted by vt : [0, 1] \u2192 [\u22121, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 20, "context": "The corresponding optimal strategies for the defender given these payoff samples, denoted x, are then computed using the PASAQ algorithm [26] to obtain a finite set S of sampled constraints.", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "It is PSPACE\u2013hard to find the optimal strategy to general RMABs [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Whittle proposed a heuristic index policy for RMABs by considering the Lagrangian relaxation of the problem [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Silver and Veness [20] have proposed POMCP algorithm, which provides high quality solutions and is scalable to large POMDPs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Silver and Veness [20] have proposed the POMCP algorithm, which provides high quality solutions for large POMDPs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Gibbs sampling [2] is a Markov chain Monte Carlo (MCMC) algorithm for sampling from multivariate probability distributions.", "startOffset": 15, "endOffset": 18}], "year": 2016, "abstractText": "The present survey aims at presenting the current machine learning techniques employed in security games domains. Specifically, we focused on papers and works developed by the Teamcore of University of Southern California, which deepened different directions in this field. After a brief introduction on Stackelberg Security Games (SSGs) and the poaching setting, the rest of the work presents how to model a boundedly rational attacker taking into account her human behavior, then describes how to face the problem of having attacker\u2019s payoffs not defined and how to estimate them and, finally, presents how online learning techniques have been exploited to learn a model of the attacker.", "creator": "LaTeX with hyperref package"}}}