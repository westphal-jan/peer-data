{"id": "1609.09475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge", "abstract": "robot interactive warehouse design automation has attracted significant interest in recent years, perhaps most visibly in the amazon picking challenge ( apc ). learning a fully autonomous warehouse pick - fly and - place system requires robust vision that reliably recognizes and finely locates objects amid cluttered environments, self - occlusions, sensor noise, artifacts and a historically large variety of objects. in this paper we present an approach that truly leverages multi - view rgb - linked d data and self - supervised, data - driven learning to overcome those difficulties. the approach was part of the mit - princeton team system that took 3rd - winner and 4th - place in the stowing and picking tasks, respectively at apc 2016. in the proposed approach, suppose we segment and label multiple views of representing a scene with a fully managed convolutional neural network, and then fit subsequent pre - scanned 3d pixel object models to the specified resulting segmentation to get even the 6d object pose. training a deep neural network for segmentation typically only requires a large initial amount of training data. we propose a self - supervised method to generate a large labeled dataset accessible without tedious manual segmentation. we also demonstrate that our system can indeed reliably estimate the 6d pose of capturing objects under a variety of scenarios. most all input code, data, and benchmarks are available at", "histories": [["v1", "Thu, 29 Sep 2016 19:39:13 GMT  (2091kb,D)", "http://arxiv.org/abs/1609.09475v1", "Under review at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL"], ["v2", "Sun, 2 Oct 2016 00:24:29 GMT  (2091kb,D)", "http://arxiv.org/abs/1609.09475v2", "Under review at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL"], ["v3", "Sun, 7 May 2017 20:12:55 GMT  (2091kb,D)", "http://arxiv.org/abs/1609.09475v3", "To appear at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL"]], "COMMENTS": "Under review at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["andy zeng", "kuan-ting yu", "shuran song", "daniel suo", "ed walker jr", "alberto rodriguez", "jianxiong xiao"], "accepted": false, "id": "1609.09475"}, "pdf": {"name": "1609.09475.pdf", "metadata": {"source": "CRF", "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge", "authors": ["Andy Zeng", "Kuan-Ting Yu", "Shuran Song", "Daniel Suo", "Ed Walker Jr.", "Alberto Rodriguez", "Jianxiong Xiao"], "emails": [], "sections": [{"heading": null, "text": "In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://www.cs.princeton.edu/\u223candyz/apc2016.\nI. INTRODUCTION\nThe last two decades have seen a rapid increase in warehouse automation technologies, satisfying the growing demand of e-commerce and providing faster, cheaper delivery. Some tasks, especially those involving physical interaction, are still hard to automate. Amazon, in collaboration with the academic community, has led a recent effort to define two such tasks: 1) picking an instance of a given a product ID out of a populated shelf and place it into a tote; and 2) stowing a tote full of products into a populated shelf.\nIn this paper we describe the vision system of the MITPrinceton Team, that took 3rd place in the stowing task and 4th in the picking task at the 2016 Amazon Picking Challenge (APC), and provide experiments to validate our design decisions. Our vision algorithm estimates the 6D poses of objects robustly under challenging scenarios:\n\u00b7 Cluttered environments: shelves and totes may have multiple objects and could be arranged as to deceive vision algorithms (e.g., objects on top of one another).\nThe authors would like to thank the MIT-Princeton APC team members for their contribution to this project, and ABB Inc. for hardware and technical support. Andy Zeng and Daniel Suo are supported by the Gordon Y.S. Wu fellowship. Shuran Song is supported by the Facebook fellowship. Peter Yu is supported by award [NSF-IIS-1427050] through the National Robotics Initiative. Alberto Rodriguez is supported by the Walter Henry Gale (1929) Career Development Professorship. Jianxiong Xiao is supported by the Google Faculty Award and Intel Gift Fund.\n\u00b7 Self-occlusion: due to limited camera positions, the system only sees a partial view of an object. \u00b7 Missing data: commercial depth sensors are unreliable\nat capturing reflective, transparent, or meshed surfaces, all common in product packaging. \u00b7 Small or deformable objects: small objects provide\nfewer data points, while deformable objects difficult alignment to prior models. \u00b7 Speed: the total time dedicated to capturing and pro-\ncessing visual information is under 20 seconds.\nOur approach makes careful use of known constraints in the task\u2014the list of possible objects and the expected background. The algorithm first segments the object from a scene by feeding multiple-view images to a deep neural network and then fits a 3D model to a segmented point cloud to recover the object\u2019s 6D pose. The deep neural network provides speed, and in combination with a multiple-view approach boosts performance in challenging scenarios.\nar X\niv :1\n60 9.\n09 47\n5v 1\n[ cs\n.C V\n] 2\n9 Se\np 20\n16\nTraining a deep neural network for segmentation requires a large amount of labeled training data. We have developed a self-supervised training procedure that automatically generated 130,000 images with pixel-wise category labels of the 39 objects in the APC. For evaluation, we constructed a testing dataset of over 7,000 manually-labeled images.\nIn summary, this paper contributes with: \u00b7 A robust multi-view vision system to estimate the 6D\npose of objects; \u00b7 A self-supervised method that trains deep networks by\nautomatically labeling training data; \u00b7 A benchmark dataset for estimating object poses.\nAll code, data, and benchmarks are publicly available [3]."}, {"heading": "II. RELATED WORK", "text": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects. The choice depends primarily on manipulation needs. For example, a suction based picker might have sufficient information with a 2D bounding box or with a pixel-level segmentation of the object, while a grasper might require its 6D pose.\nObject segmentation. While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object segmentation [2]. In this work, we extend the state-of-theart deep learning architecture used for image segmentation to incorporate depth and multi-view information.\nPose estimation. There are two primary approaches for estimating the 6D pose of an object. The first aligns 3D CAD models to 3D point clouds with algorithms such as iterative closest point [9]. The second uses more elaborated local descriptors such as SIFT keypoints [10] for color data or 3DMatch [11] for 3D data. The former approach is mainly used with depth-only sensors, in scenarios where lighting changes significantly, or on textureless objects. Highly textured and rigid objects, on the other hand, benefit from local descriptors. Existing frameworks such as LINEMOD [12] or MOPED [13] work well under certain assumptions such as objects sitting on a table top with good illumination, but underperform when confronted with the limited visibility, shadows, and clutter imposed by the APC scenario [14].\nBenchmark for 6D pose estimation. To properly evaluate our vision system independent from the larger robotic system, we have produced a large benchmark dataset with scenarios from APC 2016 with manual labels for objects\u2019 segmentation and 6D poses. Previous efforts to construct benchmark datasets include Berkeley\u2019s dataset [15] with a number of objects from and beyond APC 2015 and Rutgers\u2019s dataset [16] with semi-automatically labeled data."}, {"heading": "III. AMAZON PICKING CHALLENGE 2016", "text": "The APC 2016 posed a simplified version of the general picking and stowing tasks in a warehouse. In the picking task, robots sit within a 2x2 meter area in front of a shelf populated with objects, and autonomously pick 12 desired items and place them in a tote. In the stowing task, robots pick all 12 items inside a tote and place them in a pre-populated shelf.\nBefore the competition, teams were provided with a list of 39 possible objects along with 3D CAD models of the shelf and tote. At run-time, robots were provided with the initial contents of each bin on the shelf and a work-order containing which items to pick. After picking and stowing the appropriate objects, the system had to report the final contents of both shelf and tote. Competition details are in [1]."}, {"heading": "IV. SYSTEM DESCRIPTION", "text": "Our vision system takes in RGB-D images from multiple views, and outputs 6D poses and a segmented point cloud for the robot to complete the picking and stowing tasks.\nThe camera is compactly integrated in the end-effector of a 6DOF industrial manipulator ABB IRB1600id, and points at the tip of the fingers (Figure 1). This configuration gives the robot full controllability of the camera viewpoint, and provides feedback about grasp or suction success. The camera of choice is the RealSense F200 RGB-D because its depth range (0.2m\u20131.5m) is appropriate for close manipulation, and because it is a consumer-level range sensor with a decent amount of flexibility on the data capture process.\nDue to the tight integration of the camera, the gripper fingers, even when fully open, occupy a small portion of the view frustum. We overcome this limitation by combining different viewpoints, making use of the accurate forward kinematic reported by the robot controller.\nV. 6D OBJECT POSE ESTIMATION\nThe algorithm estimates the 6D pose of all objects in a scene in two phases (Figure 2). First, it segments the RGB-D point clouds captured from multiple views into different objects using a deep convolutional neural network. Second, it aligns pre-scanned 3D models of the identified objects to the segmented point clouds to estimate the 6D pose of each object. Our approach is based on well-known methods. However, our evaluations show that when used alone, they are far from sufficient. In this section we present brief descriptions of these methods followed by in-depth discussions of how we combine them into a robust vision system."}, {"heading": "A. Object Segmentation with Fully Convolutional Networks", "text": "In recent years, ConvNets have made tremendous progress for computer vision tasks [17, 2]. We leverage these advancements to segment camera data into the different objects in the scene. More explicitly, we train a VGG architecture [18] Fully Convolutional Network (FCN) [2] to perform 2D object segmentation. The FCN takes an RGB image as input and returns a set of 40 densely labeled pixel probability maps\u2013 one for each of the 39 objects and one for the background\u2013of the same dimensions as the input image.\nObject segmentation using multiple views. Information from a single camera view and from a given object, is often limited due to clutter, self-occlusions, and bad reflections. We address the missing information during the model-fitting phase by combining information from multiple views so that the object surfaces are more distinguishable. In particular, we feed the RGB images captured from each viewpoint (18 for stowing from the tote and 15 for picking from the shelf) to the trained FCN, which returns a 40 class probability\ndistribution for each pixel in each RGB-D image. After filtering by the list of expected objects in the scene, we threshold the probability maps (three standard deviations above the mean probability across all views) and ignore any pixels whose probabilities for all classes are below these thresholds. We then project the segmented masks for each object class into 3D space and directly combine them into a single segmented point cloud with the forward kinematic feedback from the robot arm (note that segmentation for different object classes can overlap each other).\nReduce noise in point cloud. Fitting pre-scanned models to the segmented point cloud directly often gives poor results because of noise from the sensor and noise from the segmentation. We address this issue in three steps: First, to reduce sensor noise, we eliminate spatial outliers from the segmented point cloud, by removing all point farther than a threshold from its k-nearest neighbors. Second, to reduce segmentation noise, especially on the object boundaries, we remove points that lie outside the shelf bin or tote, and those that are close to a pre-scanned background model. Finally, we further filter outlier points from each segmented group of points by finding the largest contiguous set of points along each principal axis (computed via PCA) and remove any points that are disjoint from this set.\nHandle object duplicates. Warehouse shelves commonly contain multiple instances of the same object. Naively segmenting RGB-D data will treat two distinct object with the same label as the same object. Since we know the inventory list in the warehouse setting, we know the number of identical objects we expect in the scene. We make use of k-means clustering to separate the segmented and aggregated point cloud into the appropriate number of objects. Each cluster is then treated independently during the model-fitting phase of the algorithm.\nB. 3D Model-Fitting\nWe use the iterative closest point (ICP) algorithm [19] on the segmented point cloud to fit pre-scanned 3D models of\nobjects and estimate their poses. The vanilla ICP algorithm, however, gives nonsensical results in many scenarios. We describe here several such pitfalls along with our solutions.\nPoint clouds with non-uniform density. In a typical RGBD point cloud, surfaces perpendicular to the sensor\u2019s optical axis have often denser point clouds. The color of the surface changes its reflectivity on the IR spectrum, which also affects the effective point cloud density. These nonuniformities are detrimental to the ICP algorithm because it biases convergence toward denser areas. By applying a 3D uniform average grid filter to the point clouds, we are able to give them consistent distributions in 3D space.\nPose initialization. ICP is an iterative local optimizer, and as such, it is sensitive to initialization. The principal directions of the segmented point cloud, as estimated by PCA, give us a reasonable first approximation to the orientation of objects with uneven aspect ratios. We have observed experimentally that the choice of initial orientation for objects with even aspect ratios has little effect on the final result of ICP. Analogously, one would use the centroid of the point cloud as the initial guess for the geometric center of the object, however we have observed that since captured point clouds are only partial, those two centers are usually biased from each other. To address this, we push back the initial pose of the pre-scanned object back along the optical axis of the RGB-D camera by half the size of the object\u2019s bounding box, under the naive assumption that we are only seeing \u201chalf\u201d the object. This initialization has proven more successful in avoiding local optimums.\nCoarse to fine ICP. Even after reducing noise in the segmentation step, the resulting point cloud may still have noise (e.g., mislabeled points from adjacent objects). We address this with two passes of ICP, acting on different subsets of the point cloud: we define the inlier threshold of an ICP iteration as the percentile L2 distance above which we ignore. ICP with a 90% inlier ratio keeps the closest pairs of points between the two point clouds up to the 90th percentile.\nThe main assumption is that regions of the point cloud that are correctly labeled are denser than regions with incorrect label. A first pass with a high inlier threshold (90%) moves the pre-scanned complete model closer to the correct portion of the partial view than the noisy portion. Starting now from a coarse but robust initialization, the second pass uses a lower inlier threshold (45%) to ignore the noisy portion of the point cloud and converge to a more accurate pose."}, {"heading": "C. Handling Objects with Missing Depth.", "text": "Many objects in the APC, as it is typical in retail warehouses, have surfaces that challenge infrared-based depth sensors, e.g., with plastic wrapping returning noisy or multiple reflections, or transparent or meshed materials which may not register at all. For these objects the captured point cloud is noisy and sparse, and our pose estimation algorithm performs poorly.\nOur solution leverages the multi-view segmentation to estimate a convex hull of the object by carving a 3D gridded space of voxels with the segmented RGB images. This process results in a 3D mask that encapsulates the real object. We use the convex hull of that mask to estimate the geometric center of the object and approximate its orientation (assuming that the object is axis-aligned)."}, {"heading": "VI. SELF-SUPERVISED TRAINING", "text": "By bringing deep learning into the approach we gain robustness. This, however, comes at the expense of amassing quality training data, which is necessary to learn highcapacity models with many parameters. Gathering and manually labeling such large amounts of training data is expensive. The existing large-scale datasets used by deep learning (e.g. ImageNet [20]) are mostly Internet photos, which have very different object and image statistics from our warehouse setting.\nTo automatically capture and pixel-wise label images, we propose a self-supervised method, based on three observations:\n\u00b7 Batch-training on scenes with a single object can yield deep models that perform well on scenes with multiple objects [17] (i.e., simultaneous training on cat-only or dog-only images enables successful testing on cat-withdog images); \u00b7 An accurate robot arm and accurate camera calibration,\ngives us at will control over camera viewpoint; \u00b7 For single object scenes, with known background and\nknown camera viewpoint, we can automatically obtain precise segmentation labels by foreground masking.\nThe captured training dataset contains 136,575 RGB-D images of 39 objects, all automatically labeled.\nSemi-automatic data gathering. To semi-autonomously gather large quantities of training data, we place single known objects inside the shelf bins or tote in arbitrary poses, and configure the robot to move the camera and capture RGB-D images of the objects from a variety of different viewpoints. The position of the shelf/tote is known\nto the robot, as it is the camera viewpoint, which we use to transform the collected RGB-D images in shelf/or tote frame. After capturing several hundred RGB-D images, the objects are manually re-arranged to different poses, and the process is repeated several times. Human involvement sums up to re-arranging the objects and labeling which objects correspond to which bin/tote. Selecting and changing the viewpoint, capturing sensor data, and labeling each image by object is automated. We finally collect RGB-D images of the empty shelf and tote from the same exact camera viewpoints to model the background, in preparation for the automatic data labeling.\nAutomatic data labeling. To obtain pixel-wise object segmentation labels, we create an object mask that separates foreground from background. The process is composed of 2D and 3D pipelines. The 2D pipeline is robust to thin objects (objects not sufficient volume to be reliably segmented in 3D when placed too close to a walls or ground) and objects with no depth information, while the 3D pipeline is robust to large miss-alignments between the pre-scanned shelf bin and tote. Results from both pipelines are combined to automatically label an object mask for each training RGB-D image.\nThe 2D pipeline starts by fixing minor possible image misalignments by using multimodal 2D intensity-based registration to align the two RGB-D images [21]. We then convert the aligned color image from RGB to HSV, and do pixelwise comparisons of the HSV and depth channels to separate and label foreground from background.\nThe 3D pipeline uses multiple views of an empty shelf bin and tote to create their pre-scanned 3D models. We then use ICP to align all training images to the background model, and remove points too close to the background to identify the foreground mask. Finally, we project the foreground points back to 2D to retrieve the object mask.\nTraining neural network. To leverage features trained from a larger image domain, we use the sizable FCN-VGG network architecture from [18] and initialize the network weights using a model pre-trained on ImageNet for 1000-way object classification. We fine-tune the network over the 40- class output classifier (39 classes for each APC object and 1 class for background) using stochastic gradient descent with momentum. Due to illumination and object viewpoint biases, we maximize performance by training two such segmentation networks: one for shelf bins and one for the tote.\nVII. IMPLEMENTATION\nAll components of the vision system are modularized into reusable ROS packages, with CUDA GPU acceleration. Deep models are trained and tested with Marvin [22], a ROScompatible and lightweight deep learning framework. Training our models takes up to 16 hours prior to convergence.\nOur robot is controlled by a computer with an Intel E3-1241 CPU 3.5 GHz and an NVIDIA GTX 1080. The run-time speeds per component are as follows: 10ms for ROS communication overhead, 400ms per forward pass of VGG-FCN, 1200ms for denoising per scene, and 800ms\non model-fitting per object. On average, pose estimation time is 3-5 seconds per shelf bin and 8-15 seconds for the tote. Combined with multi-view robot motions, total visual perception time is 10-15 seconds per shelf bin and 15-20 seconds for the tote."}, {"heading": "VIII. EVALUATION", "text": "We evaluate variants of our method in different scenarios in the benchmark dataset to understand (1) how segmentation performs under different input modalities and training dataset sizes and (2) how the full vision system performs."}, {"heading": "A. Benchmark Dataset", "text": "Our benchmark dataset, \u2018Shelf&Tote\u2019, contains over 7,000 RGB-D images spanning 477 (Figure 6) scenes at 640 \u00d7 480 resolution. We collected the data during practice runs and competition finals for the APC and manually labeled 6D object poses and segmentations using our online annotator (Figure 7). The data reflects various challenges found in the warehouse setting: reflective materials, variation in lighting conditions, partial views, and sensor limitations (noisy and missing depth) over cluttered environments.\nTables I and II summarize our experimental results and highlight the differences in performance over different overlapping scene categories:\n\u00b7 cptn: during competition at the APC finals. \u00b7 environment: in an office (off); in the APC competition\nwarehouse (whs). \u00b7 task: picking from a shelf bin or stowing from a tote. \u00b7 clutter: with multiple objects. \u00b7 occlusion: with % of object occluded by another object,\ncomputed from ground truth. \u00b7 object properties: with objects that are deformable,\nthin, or have no depth from the RealSense F200 camera."}, {"heading": "B. Evaluating Object Segmentation", "text": "We test several variants of our FCN on object segmentation to answer two questions: (1) can we leverage both color and depth segmentation? (2) is more training data useful?\nMetrics. We compare the predicted object segmentation from our trained FCNs against the ground truth segmentation labels of the benchmark dataset using pixel-wise precision and recall. Table I displays the mean average F-scores (F = 2 \u00b7 precision\u00b7recallprecision+recall ).\nDepth for segmentation. We use HHA features [23] to encode depth information into three channels: horizontal disparity, height above ground, and angle of local surface normal with the inferred direction of gravity. We compare AlexNet trained on this encoding, VGG on RGB data, and both networks concatenated in Table I.\nWe find that adding depth does not yield any notable improvements in segmentation performance, which could be in part due to the noisiness of the depth information from our sensor. On the other hand, we observe that the FCN performs significantly better when trained on color data, with the largest disparity for deformable objects and thin objects, whose textures provide more discriminative power than their geometric structure.\nSize of training data. Deep learning models have seen significant success, especially if given large amounts of training data. However in our scenario\u2014instance-level object segmentation on few object categories\u2014it is not clear whether such a large dataset is necessary. We create two new datasets by randomly sampling 1% and 10% of the\noriginal and use them to train two VGG FCNs (Table I). We confirm marked improvements in F-score across all benchmark categories going from 1% to 10% to 100% of training data."}, {"heading": "C. Evaluating Pose Estimation", "text": "We evaluate several key components of our vision system to determine whether they increase performance in isolation.\nMetrics. We report the percentage of object pose predictions with error in orientation smaller than 15\u25e6, and the percentage with error in translation smaller than 5cm. The metric also recognizes the structural invariance of several objects, some of which are axially-symmetric (cuboids), radially-symmetric (bottles, cylinders), or deformable (see web page [3] for further details). We have observed experimentally that these bonds of 15\u25e6 and 5cm are sufficient for picking with sensorguarded motions.\nMulti-view information. With multiple views the system overcomes missing information due to self-occlusions, otherobject occlusions, or clutter. Multi-view information also alleviates problems with illumination on reflective surfaces.\nTo quantify the effect of the multiple-view system, we test the full vision system on the benchmark with three different subsets of camera views:\n\u00b7 [Full] All 15 views for shelf bins a1shelf = {0 . . . 14} and all 18 views for the tote a1tote = {0 . . . 17}.\u00b7 [5v-10v] 5 views for shelf a2shelf = {0,4,7,10,14} and 10 views for tote a2tote ={0,2,4,6,8,9,11,13,15,17}, with a sparse arrangement and a preference for wide-baseline view angles. \u00b7 [1v-2v] 1 view for shelf bins a3shelf = {7} and 2 views\nfor the tote a3tote ={7,13}. The viewpoint ids are zero-indexed in row-major order as depicted in Figure 3. Our results show that multiple views robustly address occlusion and heavy clutter in the warehouse setting (Table II [clutter] and [occlusion]). They also present a clear contrast between the performance of our algorithm using a single view of the scene, versus multiple views of the scene (Table II [Full] v.s [1v-2v]).\nDenoising. The denoising step described in Section V proves important for achieving good results. With this turned off, the accuracy in estimating the translation and rotation decreases by 6.0% and 4.4% respectively (Table II).\nICP improvements. Without the pre-processing steps to ICP, we observe a drop in prediction accuracy of 0.9% in translation and 3.1% in rotation (Table II).\nPerformance upper bound. We also evaluated how well the model-fitting part of our algorithm alone performs on the benchmark by using ground truth segmentation labels from the benchmark as the performance upper bound."}, {"heading": "D. Common Failure Modes", "text": "Here we summarize the most common failure modes of our vision system, which are illustrated in Figure 9:\n\u00b7 The FCN segmentation for objects under heavy occlusion or clutter are likely to be incomplete resulting in poor pose estimation (Fig. 8.e), or undetected (Fig. 9.m and p). This happens with more frequency at back of the bin with poor illumination. \u00b7 Objects color textures are confused with each other.\nFigure 9.r shows a Dove bar (white box) on top of a mail envelope, which combined have similar appearance than the outlet plugs. \u00b7 Model fitting for cuboid objects often confuses corner\nalignments (marker boxes in Fig. 9.o). This inaccuracy, however, is still within the range of tolerance that the robot can tolerate thanks to sensor-guarded motions.\nFiltering failure modes by confidence score. We compute a confidence score per object pose prediction that favors high precision for low recall. Specifically, the confidence score of a pose prediction equals the mean value of confidence scores over all points belonging to the segmentation of the object. We observe that erroneous poses (especially those due to partial occlusions) more often have low confidence scores. The robot system uses this value to target only predictions with high score.\nWe evaluate the usefulness of the confidence scores by recalling the output of the perception system to only consider predictions with confidence scores larger than 10% and 70% respectively (see Table II). These confidence percentages are important thresholds, because the full robot system, predictions with < 10% confidence (conf-10, at 78% recall) are ignored during planning, and prediction with > 70% confidence (conf-70, at 23% recall) trigger a pick attempt."}, {"heading": "IX. DISCUSSION", "text": "Despite tremendous advances in computer vision, many state-of-the-art well-known approaches are often insufficient\nfor relatively common scenarios. We describe here two observations that can lead to improvements in real systems:\nMake the most out of every constraint. External constraints limit what systems can do. Indirectly they also limit the set of states in which the system can be, which can lead to opportunities for simplifications and robustness in the perception system. In the picking task, each team received a list of items, their bin assignments, and a model of the shelf. All teams used the bin assignments to rule out objects from consideration and the model of the shelf to calibrate their robots. These optimizations are straightforward and useful. However, further investigation yields more opportunity. By using these same constraints, we constructed a selfsupervising mechanism to train a deep neural network with significantly more data. As our evaluations show, the volume of training data is strongly correlated with performance.\nDesigning robotic and vision systems hand-in-hand. Vision algorithms are too often designed in isolation. However, vision is one component of a larger robotic system with needs and opportunities. Typical computer vision algorithms operate on single images for segmentation and recognition. Robotic arms free us from that constraint, allowing us to precisely fuse multiple views and improve performance in cluttered environments. Computer vision systems also tend to have fixed outputs (e.g., bounding boxes or 2D segmentation maps), but robotic systems with multiple manipulation strategies can benefit from variety in output. For example, suction cups and grippers might have different perceptual requirements. While the former might work more robustly with a segmented point cloud, the latter often requires knowledge of the object pose and geometry."}, {"heading": "X. CONCLUSION", "text": "In this paper, we present the vision system of Team MITPrinceton\u2019s 3rd- and 4th-place entry in the 2016 Amazon\nPicking Challenge. To address the challenges posed by the warehouse setting, our framework leverages multi-view RGB-D data and data-driven, self-supervised deep learning to reliably estimate the 6D poses of objects under a variety of scenarios. We also provide a well-labeled benchmark dataset of APC 2016 containing over 7,000 images from 477 scenes."}], "references": [{"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, 2015, pp. 3431\u20133440.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic multi-class segmentation for the amazon picking challenge", "author": ["R. Jonschkowski", "C. Eppner", "S. H\u00f6fer", "R. Mart\u0131\u0301n-Mart\u0131\u0301n", "O. Brock"], "venue": "http://dx.doi.org/10.14279/depositonce-5051, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Lessons from the amazon picking challenge: Four aspects of building robotic systems", "author": ["C. Eppner", "S. H\u00f6fer", "R. Jonschkowski", "R. Mart\u0131n-Mart\u0131n", "A. Sieverling", "V. Wall", "O. Brock"], "venue": "RSS, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Dorapicker: An autonomous picking system for general objects", "author": ["H. Zhang", "P. Long", "D. Zhou", "Z. Qian", "Z. Wang", "W. Wan", "D. Manocha", "C. Park", "T. Hu", "C. Cao", "Y. Chen", "M. Chow", "J. Pan"], "venue": "arXiv: 1603.06317, 2016. [Online]. Available: http://arxiv.org/abs/1603.06317", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "A summary of team mit\u2019s approach to the amazon picking challenge 2015", "author": ["K.-T. Yu", "N. Fazeli", "N.C. Dafle", "O. Taylor", "E. Donlon", "G.D. Lankenau", "A. Rodriguez"], "venue": "arXiv:1604.03639, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast object localization and pose estimation in heavy clutter for robotic bin picking", "author": ["M.-Y. Liu", "O. Tuzel", "A. Veeraraghavan", "Y. Taguchi", "T.K. Marks", "R. Chellappa"], "venue": "IJRR, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A method for registration of 3-d shapes", "author": ["P.J. Besl", "N.D. McKay"], "venue": "PAMI, 1992.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Sift-based homographies for efficient multiview distributed visual sensing", "author": ["A. Dias", "C. Brites", "J. Ascenso", "F. Pereira"], "venue": "IEEE Sensors Journal, vol. 15, no. 5, pp. 2643\u20132656, May 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "3dmatch:  Learning the matching of local 3d geometry in range scans", "author": ["A. Zeng", "S. Song", "M. Nie\u00dfner", "M. Fisher", "J. Xiao"], "venue": "arXiv: 1603.08182, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes", "author": ["S. Hinterstoisser", "S. Holzer", "C. Cagniart", "S. Ilic", "K. Konolige", "N. Navab", "V. Lepetit"], "venue": "ICCV, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "The moped framework: Object recognition and pose estimation for manipulation", "author": ["A. Collet", "M. Martinez", "S.S. Srinivasa"], "venue": "IJRR, vol. 30, no. 10, pp. 1284\u20131306, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis and Observations from the First Amazon Picking Challenge", "author": ["N. Correll", "K. Bekris", "D. Berenson", "O. Brock", "A. Causo", "K. Hauser", "K. Okada", "A. Rodriguez", "J. Romano", "P. Wurman"], "venue": "IEEE T-ASE, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Bigbird: A large-scale 3d database of object instances", "author": ["A. Singh", "J. Sha", "K.S. Narayan", "T. Achim", "P. Abbeel"], "venue": "ICRA, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A dataset for improved rgbd-based object detection and pose estimation for warehouse pick-and-place", "author": ["C. Rennie", "R. Shome", "K.E. Bekris", "A.F. De Souza"], "venue": "Robotics and Automation Letters, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Geometrically stable sampling for the icp algorithm", "author": ["N. Gelfand", "L. Ikemoto", "S. Rusinkiewicz", "M. Levoy"], "venue": "3DIM, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Parametric estimate of intensity inhomogeneities applied to mri", "author": ["M. Styner", "C. Brechbuhler", "G. Szckely", "G. Gerig"], "venue": "IEEE Transactions on Medical Imaging, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning rich features from rgb-d images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbel\u00e1ez", "J. Malik"], "venue": "ECCV, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Each color image is fed into a fully convolutional network [2] for 2D object segmentation.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 104, "endOffset": 110}, {"referenceID": 2, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 104, "endOffset": 110}, {"referenceID": 3, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 124, "endOffset": 130}, {"referenceID": 4, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 124, "endOffset": 130}, {"referenceID": 5, "context": "While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object", "startOffset": 106, "endOffset": 112}, {"referenceID": 0, "context": "segmentation [2].", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The first aligns 3D CAD models to 3D point clouds with algorithms such as iterative closest point [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "The second uses more elaborated local descriptors such as SIFT keypoints [10] for color data", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "or 3DMatch [11] for 3D data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "Existing frameworks such as LINEMOD [12] or MOPED [13] work well under certain assumptions such as", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Existing frameworks such as LINEMOD [12] or MOPED [13] work well under certain assumptions such as", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "objects sitting on a table top with good illumination, but underperform when confronted with the limited visibility, shadows, and clutter imposed by the APC scenario [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "Previous efforts to construct benchmark datasets include Berkeley\u2019s dataset [15] with a number of objects from and beyond APC 2015 and Rutgers\u2019s dataset [16] with semi-automatically labeled data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "Previous efforts to construct benchmark datasets include Berkeley\u2019s dataset [15] with a number of objects from and beyond APC 2015 and Rutgers\u2019s dataset [16] with semi-automatically labeled data.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "In recent years, ConvNets have made tremendous progress for computer vision tasks [17, 2].", "startOffset": 82, "endOffset": 89}, {"referenceID": 0, "context": "In recent years, ConvNets have made tremendous progress for computer vision tasks [17, 2].", "startOffset": 82, "endOffset": 89}, {"referenceID": 15, "context": "More explicitly, we train a VGG architecture [18] Fully Convolutional Network (FCN) [2] to perform 2D object", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "More explicitly, we train a VGG architecture [18] Fully Convolutional Network (FCN) [2] to perform 2D object", "startOffset": 84, "endOffset": 87}, {"referenceID": 16, "context": "We use the iterative closest point (ICP) algorithm [19] on the segmented point cloud to fit pre-scanned 3D models of", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "ImageNet [20]) are mostly Internet photos, which have very different object and image statistics from our warehouse setting.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "To automatically capture and pixel-wise label images, we propose a self-supervised method, based on three observations: \u00b7 Batch-training on scenes with a single object can yield deep models that perform well on scenes with multiple objects [17] (i.", "startOffset": 240, "endOffset": 244}, {"referenceID": 18, "context": "The 2D pipeline starts by fixing minor possible image misalignments by using multimodal 2D intensity-based registration to align the two RGB-D images [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "To leverage features trained from a larger image domain, we use the sizable FCN-VGG network architecture from [18] and initialize the network weights using a model pre-trained on ImageNet for 1000-way object classification.", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "We use HHA features [23] to encode depth information into three channels: horizontal disparity, height above ground, and angle of local surface normal with the inferred direction of gravity.", "startOffset": 20, "endOffset": 24}], "year": 2016, "abstractText": "Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC) [1]. A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multiview RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MITPrinceton Team system that took 3rdand 4thplace in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://www.cs.princeton.edu/\u223candyz/apc2016.", "creator": "LaTeX with hyperref package"}}}