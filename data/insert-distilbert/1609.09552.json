{"id": "1609.09552", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Controlling Output Length in Neural Encoder-Decoders", "abstract": "neural encoder - decoder models results have shown great success in many sequence generation tasks. however, so previous work has not investigated situations in which we would like to control around the length of encoder - decoder outputs. this capability is crucial for applications such as text summarization, in which we have to usually generate concise summaries with a desired length. in this paper, we propose methods for uniformly controlling the output sequence encoding length for some neural encoder - decoder models : two decoding - based methods described and two learning - based methods. results show that our learning - based methods have realised the capability to control length without degrading prior summary quality in a summarization task.", "histories": [["v1", "Fri, 30 Sep 2016 00:01:27 GMT  (2968kb,D)", "http://arxiv.org/abs/1609.09552v1", "11 pages. To appear in EMNLP 2016"]], "COMMENTS": "11 pages. To appear in EMNLP 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuta kikuchi", "graham neubig", "ryohei sasano", "hiroya takamura", "manabu okumura"], "accepted": true, "id": "1609.09552"}, "pdf": {"name": "1609.09552.pdf", "metadata": {"source": "CRF", "title": "Controlling Output Length in Neural Encoder-Decoders", "authors": ["Yuta Kikuchi", "Graham Neubig", "Ryohei Sasano", "Hiroya Takamura", "Manabu Okumura"], "emails": ["kikuchi@lr.pi.titech.ac.jp", "gneubig@cs.cmu.edu", "sasano@pi.titech.ac.jp", "takamura@pi.titech.ac.jp", "oku@pi.titecjh.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Since its first use for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), the encoder-decoder approach has demonstrated great success in many other sequence generation tasks including image caption generation (Vinyals et al., 2015b; Xu et al., 2015), parsing (Vinyals et al., 2015a), dialogue response generation (Li et al., 2016a; Serban et al., 2016) and sentence summarization (Rush et al., 2015; Chopra et al., 2016). In particular, in this paper we focus on sentence summarization, which as its name suggests, consists of generating shorter versions of sentences for applications such as document\n\u2217This work was done when the author was at the Nara Institute of Science and Technology.\n1Available at https://github.com/kiyukuta/lencon.\nsummarization (Nenkova and McKeown, 2011) or headline generation (Dorr et al., 2003). Recently, Rush et al. (2015) automatically constructed large training data for sentence summarization, and this has led to the rapid development of neural sentence summarization (NSS) or neural headline generation (NHG) models. There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).\nOne of the essential properties that text summarization systems should have is the ability to generate a summary with the desired length. Desired lengths of summaries strongly depends on the scene of use, such as the granularity of information the user wants to understand, or the monitor size of the device the user has. The length also depends on the amount of information contained in the given source document. Hence, in the traditional setting of text summarization, both the source document and the desired length of the summary will be given as input to a summarization system. However, methods for controlling the output sequence length of encoderdecoder models have not been investigated yet, despite their importance in these settings.\nIn this paper, we propose and investigate four methods for controlling the output sequence length for neural encoder-decoder models. The former two methods are decoding-based; they receive the desired length during the decoding process, and the training process is the same as standard encoderdecoder models. The latter two methods are learning-based; we modify the network architecture to receive the desired length as input.\nar X\niv :1\n60 9.\n09 55\n2v 1\n[ cs\n.C L\n] 3\n0 Se\np 20\nIn experiments, we show that the learning-based methods outperform the decoding-based methods for long (such as 50 or 75 byte) summaries. We also find that despite this additional length-control capability, the proposed methods remain competitive to existing methods on standard settings of the DUC2004 shared task-1."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Related Work", "text": "Text summarization is one of the oldest fields of study in natural language processing, and many summarization methods have focused specifically on sentence compression or headline generation. Traditional approaches to this task focus on word deletion using rule-based (Dorr et al., 2003; Zajic et al., 2004) or statistical (Woodsend et al., 2010; Galanis and Androutsopoulos, 2010; Filippova and Strube, 2008; Filippova and Altun, 2013; Filippova et al., 2015) methods. There are also several studies of abstractive sentence summarization using syntactic transduction (Cohn and Lapata, 2008; Napoles et al., 2011) or taking a phrase-based statistical machine translation approach (Banko et al., 2000; Wubben et al., 2012; Cohn and Lapata, 2013).\nRecent work has adopted techniques such as encoder-decoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and attentional (Bahdanau et al., 2015; Luong et al., 2015) neural network models from the field of machine translation, and tailored them to the sentence summarization task. Rush et al. (2015) were the first to pose sentence summarization as a new target task for neural sequence-to-sequence learning. Several studies have used this task as one of the benchmarks of their neural sequence transduction methods (Ranzato et al., 2015; Lopyrev, 2015; Ayana et al., 2016). Some studies address the other important phenomena frequently occurred in humanwritten summaries, such as copying from the source document (Gu et al., 2016; Gulcehre et al., 2016). Nallapati et al. (2016) investigate a way to solve many important problems capturing keywords, or inputting multiple sentences.\nNeural encoder-decoders can also be viewed as statistical language models conditioned on the target sentence context. Rosenfeld et al. (2001) have\nproposed whole-sentence language models that can consider features such as sentence length. However, as described in the introduction, to our knowledge, explicitly controlling length of output sequences in neural language models or encoder-decoders has not been investigated.\nFinally, there are some studies to modify the output sequence according some meta information such as the dialogue act (Wen et al., 2015), user personality (Li et al., 2016b), or politeness (Sennrich et al., 2016). However, these studies have not focused on length, the topic of this paper."}, {"heading": "2.2 Importance of Controlling Output Length", "text": "As we already mentioned in Section 1, the most standard setting in text summarization is to input both the source document and the desired length of the summary to a summarization system. Summarization systems thus must be able to generate summaries of various lengths. Obviously, this property is also essential for summarization methods based on neural encoder-decoder models.\nSince an encoder-decoder model is a completely data-driven approach, the output sequence length depends on the training data that the model is trained on. For example, we use sentence-summary pairs extracted from the Annotated English Gigaword corpus as training data (Rush et al., 2015), and the average length of human-written summary is 51.38 bytes. Figure 1 shows the statistics of the corpus. When we train a standard encoder-decoder model and perform the standard beam search decoding on the corpus, the average length of its output sequence is 38.02 byte.\nHowever, there are other situations where we want summaries with other lengths. For example, DUC2004 is a shared task where the maximum length of summaries is set to 75 bytes, and summarization systems would benefit from generating sentences up to this length limit.\nWhile recent NSS models themselves cannot control their output length, Rush et al. (2015) and others following use an ad-hoc method, in which the system is inhibited from generating the end-of-sentence (EOS) tag by assigning a score of\u2212\u221e to the tag and\nh t\nc t\nc t h t\nAttender\nmt\nst\nyt\ns\u0303t\n<s> yt 1\nh t\nc t\nc t h t\nAttender\nmt\nst\nyt\ns\u0303t\n<s> yt 1\nlength\nbc\nat at\nFigure 2: The encoder-decoder architecture we used as a base model in this paper.\ngenerating a fixed number of words2, and finally the output summaries are truncated to 75 bytes. Ideally, the models should be able to change the output sequence depending on the given output length, and to output the EOS tag at the appropriate time point in a natural manner."}, {"heading": "3 Network Architecture: Encoder-Decoder with Attention", "text": "In this section, we describe the model architecture used for our experiments: an encoder-decoder consisting of bi-directional RNNs and an attention mechanism. Figure 2 shows the architecture of the model.\nSuppose that the source sentence is represented as a sequence of words x = (x1, x2, x3, ..., xN ). For\n2According to the published code (https://github.com/facebook/NAMAS), the default number of words is set to 15, which is too long for the DUC2004 setting. The average number of words of human summaries in the evaluation set is 10.43.\na given source sentence, the summarizer generates a shortened version of the input (i.e. N > M ), as summary sentence y = (y1, y2, y3, ..., yM ). The model estimates conditional probability p(y|x) using parameters trained on large training data consisting of sentence-summary pairs. Typically, this conditional probability is factorized as the product of conditional probabilities of the next word in the sequence:\np(y|x) = M\u220f t=1 p(yt|y<t,x),\nwhere y<t = (y1, y2, y3, ..., yt\u22121). In the following, we describe how to compute p(yt|y<t,x)."}, {"heading": "3.1 Encoder", "text": "We use the bi-directional RNN (BiRNN) as encoder which has been shown effective in neural machine translation (Bahdanau et al., 2015) and speech recognition (Schuster and Paliwal, 1997; Graves et al., 2013).\nA BiRNN processes the source sentence for both forward and backward directions with two separate RNNs. During the encoding process, the BiRNN computes both forward hidden states ( \u2212\u2192 h 1, \u2212\u2192 h 2, ..., \u2212\u2192 hN ) and backward hidden states ( \u2190\u2212 h 1, \u2190\u2212 h 2, ..., \u2190\u2212 hN ) as follows:\n\u2212\u2192 h t = g( \u2212\u2192 h t\u22121, xt), \u2190\u2212 h t = g( \u2190\u2212 h t+1, xt).\nWhile g can be any kind of recurrent unit, we use long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks that have memory cells for both directions (\u2212\u2192c t and\u2190\u2212c t).\nAfter encoding, we set the initial hidden states s0 and memory-cell m0 of the decoder as follows:\ns0 = \u2190\u2212 h 1, m0 = \u2190\u2212c 1."}, {"heading": "3.2 Decoder and Attender", "text": "Our decoder is based on an RNN with LSTM g:\nst = g(st\u22121, xt).\nWe also use the attention mechanism developed by Luong et al. (2015), which uses st to compute contextual information dt of time step t. We first summarize the forward and backward encoder states by taking their sum h\u0304i = \u2212\u2192 h i + \u2190\u2212 h i, and then calculate the context vector dt as the weighted sum of these summarized vectors:\ndt = \u2211 i atih\u0304i,\nwhere at is the weight at the t-th step for h\u0304i computed by a softmax operation:\nati = exp(st \u00b7 h\u0304i)\u2211 h\u0304\u2032 exp(st \u00b7 h\u0304\u2032) .\nAfter context vector dt is calculated, the model updates the distribution over the next word as follows:\ns\u0303t = tanh(Whs[st;dt] + bhs),\np(yt|y<t,x) = softmax(Wsos\u0303t + bso).\nNote that s\u0303t is also provided as input to the LSTM with yt for the next step, which is called the input feeding architecture (Luong et al., 2015)."}, {"heading": "3.3 Training and Decoding", "text": "The training objective of our models is to maximize log likelihood of the sentence-summary pairs in a given training set D:\nLt(\u03b8) = \u2211\n(x,y)\u2208D\nlog p(y|x; \u03b8),\np(y|x; \u03b8) = \u220f t p(yt|y<t,x).\nOnce models are trained, we use beam search to find the output that maximizes the conditional probability."}, {"heading": "4 Controlling Length in Encoder-decoders", "text": "In this section, we propose our four methods that can control the length of the output in the encoderdecoder framework. In the first two methods, the decoding process is used to control the output length without changing the model itself. In the other two methods, the model itself has been changed and is trained to obtain the capability of controlling the length. Following the evaluation dataset used in our experiments, we use bytes as the unit of length, although our models can use either words or bytes as necessary.\n4.1 fixLen: Beam Search without EOS Tags\nThe first method we examine is a decoding approach similar to the one taken in many recent NSS methods that is slightly less ad-hoc. In this method, we inhibit the decoder from generating the EOS tag by assigning it a score of \u2212\u221e. Since the model cannot stop the decoding process by itself, we simply stop the decoding process when the length of output sequence reaches the desired length. More specifically, during beam search, when the length of the sequence generated so far exceeds the desired length, the last word is replaced with the EOS tag and also the score of the last word is replaced with the score of the EOS tag (EOS replacement).\n4.2 fixRng: Discarding Out-of-range Sequences\nOur second decoding method is based on discarding out-of-range sequences, and is not inhibited from generating the EOS tag, allowing it to decide when to stop generation. Instead, we define the legitimate range of the sequence by setting minimum and maximum lengths. Specifically, in addition to the normal beam search procedure, we set two rules:\n\u2022 If the model generates the EOS tag when the output sequence is shorter than the minimum length, we discard the sequence from the beam.\n\u2022 If the generated sequence exceeds the maximum length, we also discard the sequence from the beam. We then replace its last word with the EOS tag and add this sequence to the beam\n(EOS replacement in Section 4.1).3\nIn other words, we keep only the sequences that contain the EOS tag and are in the defined length range. This method is a compromise that allows the model some flexibility to plan the generated sequences, but only within a certain acceptable length range.\nIt should be noted that this method needs a larger beam size if the desired length is very different from the average summary length in the training data, as it will need to preserve hypotheses that have the desired length.\n4.3 LenEmb: Length Embedding as Additional Input for the LSTM\nOur third method is a learning-based method specifically trained to control the length of the output sequence. Inspired by previous work that has demonstrated that additional inputs to decoder models can effectively control the characteristics of the output (Wen et al., 2015; Li et al., 2016b), this model provides information about the length in the form of an additional input to the net. Specifically, the model uses an embedding e2(lt) \u2208 RD for each potential desired length, which is parameterized by a length embedding matrix Wle \u2208 RD\u00d7L where L is the number of length types. In the decoding process, we input the embedding of the remaining length lt as additional input to the LSTM (Figure 3). lt is initialized after the encoding process and updated during the decoding process as follows:\nl1 = length,\nlt+1 = { 0 (lt \u2212 byte(yt) \u2264 0) lt \u2212 byte(yt) (otherwise),\nwhere byte(yt) is the length of output word yt and length is the desired length. We learn the values of the length embedding matrix Wle during training. This method provides additional information about the amount of length remaining in the output sequence, allowing the decoder to \u201cplan\u201d its output based on the remaining number of words it can generate.\n3This is a workaround to prevent the situation in which all sequences are discarded from a beam.\nh t\nc t\nc t h t\nAttender\nmt\nst\nyt\ns\u0303t\n<s> yt 1\nh t\nc t\nc t h t\nAttender\nmt\nst\nyt\ns\u0303t\n<s> yt 1\nlength\nbc\nat at\nFigure 4: LenInit: initial state of the decoder\u2019s memory cell m0 manages output length.\n4.4 LenInit: Length-based Memory Cell Initialization\nWhileLenEmb inputs the remaining length lt to the decoder at each step of the decoding process, the LenInit method inputs the desired length once at the initial state of the decoder. Figure 4 shows the architecture of LenInit. Specifically, the model uses the memory cell mt to control the output length by initializing the states of decoder (hidden state s0 and memory cell m0) as follows:\ns0 = \u2190\u2212 h 1,\nm0 = bc \u2217 length, (1)\nwhere bc \u2208 RH is a trainable parameter and length is the desired length.\nWhile the model of LenEmb is guided towards the appropriate output length by inputting the remaining length at each step, this LenInit attempts to provide the model with the ability to manage the output length on its own using its inner state. Specifically, the memory cell of LSTM networks is suitable for this endeavour, as it is possible for LSTMs\nto learn functions that, for example, subtract a fixed amount from a particular memory cell every time they output a word. Although other ways for managing the length are also possible,4 we found this approach to be both simple and effective."}, {"heading": "5 Experiment", "text": ""}, {"heading": "5.1 Dataset", "text": "We trained our models on a part of the Annotated English Gigaword corpus (Napoles et al., 2012), which Rush et al. (2015) constructed for sentence summarization. We perform preprocessing using the standard script for the dataset5. The dataset consists of approximately 3.6 million pairs of the first sentence from each source document and its headline. Figure 1 shows the length histograms of the summaries in the training set. The vocabulary size is 116,875 for the source documents and 67,564 for the target summaries including the beginning-ofsentence, end-of-sentence, and unknown word tags. For LenEmb and LenInit, we input the length of each headline during training. Note that we do not train multiple summarization models for each headline length, but a single model that is capable of controlling the length of its output.\nWe evaluate the methods on the evaluation set of DUC2004 task-1 (generating very short singledocument summaries). In this task, summarization systems are required to create a very short summary for each given document. Summaries over the length limit (75 bytes) will be truncated and there is no bonus for creating a shorter summary. The evaluation set consists of 500 source documents and 4 human-written (reference) summaries for each\n4For example, we can also add another memory cell for managing the length.\n5https://github.com/facebook/NAMAS\nsource document. Figure 5 shows the length histograms of the summaries in the evaluation set. Note that the human-written summaries are not always as long as 75 bytes. We used three variants of ROUGE (Lin, 2004) as evaluation metrics: ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence). The two-sided permutation test (Chinchor, 1992) was used for statistical significance testing (p \u2264 0.05)."}, {"heading": "5.2 Implementation", "text": "We use Adam (Kingma and Ba, 2015) (\u03b1=0.001, \u03b21=0.9, \u03b22=0.999, eps=10\u22128) to optimize parameters with a mini-batch of size 80. Before every 10,000 updates, we first sampled 800,000 training examples and made groups of 80 examples with the same source sentence length, and shuffled the 10,000 groups.\nWe set the dimension of word embeddings to 100 and that of the hidden state to 200. For LSTMs, we initialize the bias of the forget gate to 1.0 and use 0.0 for the other gate biases (Jo\u0301zefowicz et al., 2015). We use Chainer (Tokui et al., 2015) to implement our models. For LenEmb, we set L to 300, which is larger than the longest summary lengths in our dataset (see Figure 1-(b) and Figure 5-(b)).\nFor all methods except fixRng, we found a beam size of 10 to be sufficient, but for fixRng we used a beam size of 30 because it more aggressively discards candidate sequences from its beams during decoding."}, {"heading": "6 Result", "text": ""}, {"heading": "6.1 ROUGE Evaluation", "text": "Table 1 shows the ROUGE scores of each method with various length limits (30, 50 and 75 byte). Regardless of the length limit set for the summariza-\ntion methods, we use the same reference summaries. Note that, fixLen and fixRng generate the summaries with a hard constraint due to their decoding process, which allows them to follow the hard constraint on length. Hence, when we calculate the scores of LenEmb and LenInit, we impose a hard constraint on length to make the comparison fair (i.e. LenEmb(0,L) and LenInit(0,L) in the table). Specifically, we use the same beam search as that for fixRng with minimum length of 0.\nFor the purpose of showing the length control capability of LenEmb and LenInit, we show at the bottom two lines the results of the standard beam search without the hard constraints on the length6. We will use the results of LenEmb(0,\u221e) and LenInit(0,\u221e) in the discussions in Sections 6.2 and 6.3.\nThe results show that the learning-based meth-\n6 fixRng is equivalence to the standard beam search when we set the range as (0,\u221e).\nods (LenEmb and LenInit) tend to outperform decoding-based methods (fixLen and fixRng) for the longer summaries of 50 and 75 bytes. However, in the 30-byte setting, there is no significant difference between these two types of methods. We hypothesize that this is because average compression rate in the training data is 30% (Figure 1-(c)) while the 30-byte setting forces the model to generate summaries with 15.38% in average compression rate, and thus the learning-based models did not have enough training data to learn compression at such a steep rate."}, {"heading": "6.2 Examples of Generated Summaries", "text": "Tables 2 and 3 show examples from the validation set of the Annotated Gigaword Corpus. The tables show that all models, including both learningbased methods and decoding-based methods, can often generate well-formed sentences.\nWe can see various paraphrases of \u201c#### us figure\nchampionships\u201d7 and \u201cwithdrew\u201d. Some examples are generated as a single noun phrase (LenEmb(30) and LenInit(30)) which may be suitable for the short length setting."}, {"heading": "6.3 Length Control Capability of Learning-based Models", "text": "Figure 6 shows histograms of output length from the standard encoder-decoder, LenEmb, and LenInit. While the output lengths from the standard model disperse widely, the lengths from our learning-based models are concentrated to the desired length. These histograms clearly show the length controlling capability of our learning-based models.\nTable 4-(a) shows the final state of the beam when LenInit generates the sentence with a length of 30 bytes for the example with standard beam search in Table 3. We can see all the sentences in the beam are generated with length close to the desired length. This shows that our method has obtained the ability to control the output length as expected. For comparison, Table 4-(b) shows the final state of the beam if we perform standard beam search in the standard encoder-decoder model (used in fixLen and fixRng). Although each sentence is well-formed, the lengths of them are much more varied."}, {"heading": "6.4 Comparison with Existing Methods", "text": "Finally, we compare our methods to existing methods on standard settings of the DUC2004 shared\n7Note that \u201c#\u201d is a normalized number and \u201cus\u201d is \u201cUS\u201d (United States).\ntask-1. Although the objective of this paper is not to obtain state-of-the-art scores on this evaluation set, it is of interest whether our length-controllable models are competitive on this task. Table 5 shows that the scores of our methods, which are copied from Table 1, in addition to the scores of some existing methods. ABS (Rush et al., 2015) is the most standard model of neural sentence summarization and is the most similar method to our baseline setting (fixLen). This table shows that the score of fixLen is comparable to those of the existing methods. The table also shows the LenEmb and the LenInit have the capability of controlling the length without decreasing the ROUGE score."}, {"heading": "7 Conclusion", "text": "In this paper, we presented the first examination of the problem of controlling length in neural encoderdecoder models, from the point of view of summarization. We examined methods for controlling length of output sequences: two decoding-based methods (fixLen and fixRng) and two learningbased methods (LenEmb and LenInit). The results showed that learning-based methods generally outperform the decoding-based methods, and the learning-based methods obtained the capability of controlling the output length without losing ROUGE score compared to existing summarization methods."}, {"heading": "Acknowledgments", "text": "This work was supported by JSPS KAKENHI Grant Number JP26280080. We are grateful to have the\nopportunity to use the Kurisu server of Dwango Co., Ltd. for our experiments."}], "references": [{"title": "Neural Headline Generation with Minimum Risk Training", "author": ["Ayana et al.2016] Ayana", "S. Shen", "Z. Liu", "M. Sun"], "venue": null, "citeRegEx": "Ayana et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ayana et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Cho", "Yoshua Bengio."], "venue": "Proceedings of ICLR15.", "citeRegEx": "Cho and Bengio.,? 2015", "shortCiteRegEx": "Cho and Bengio.", "year": 2015}, {"title": "Headline generation based on statistical translation", "author": ["Banko et al.2000] Michele Banko", "Vibhu O. Mittal", "Michael J. Witbrock"], "venue": "In Proceedings of ACL00,", "citeRegEx": "Banko et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2000}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Chopra et al.2016] Sumit Chopra", "Michael Auli", "Alexander M. Rush"], "venue": "In Proceedings of NAACL-HLT16,", "citeRegEx": "Chopra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Sentence compression beyond word deletion", "author": ["Cohn", "Lapata2008] Trevor Cohn", "Mirella Lapata"], "venue": "In Proceedings of COLING08,", "citeRegEx": "Cohn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2008}, {"title": "Hedge trimmer: A parse-and-trim approach to headline generation", "author": ["Dorr et al.2003] Bonnie Dorr", "David Zajic", "Richard Schwartz"], "venue": "In Proceedings of the HLT-NAACL 03 Text Summarization Workshop,", "citeRegEx": "Dorr et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dorr et al\\.", "year": 2003}, {"title": "Overcoming the lack of parallel data in sentence compression", "author": ["Filippova", "Altun2013] Katja Filippova", "Yasemin Altun"], "venue": "In Proceedings of EMNLP13,", "citeRegEx": "Filippova et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2013}, {"title": "Dependency tree based sentence compression", "author": ["Filippova", "Strube2008] Katja Filippova", "Michael Strube"], "venue": "In Proceedings of INLG08,", "citeRegEx": "Filippova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2008}, {"title": "Sentence compression by deletion with lstms", "author": ["Enrique Alfonseca", "Carlos A. Colmenares", "Lukasz Kaiser", "Oriol Vinyals"], "venue": "In Proceedings of EMNLP15,", "citeRegEx": "Filippova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Filippova et al\\.", "year": 2015}, {"title": "An extractive supervised two-stage method for sentence compression", "author": ["Galanis", "Ion Androutsopoulos"], "venue": "In Proceedings of NAACL-HLT10,", "citeRegEx": "Galanis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Galanis et al\\.", "year": 2010}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Graves et al.2013] A. Graves", "N. Jaitly", "A. r. Mohamed"], "venue": "In Proceedings of IEEE Workshop on ASRU13,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Gu et al.2016] Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": "In Proceedings of ACL16,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio"], "venue": "In Proceedings of ACL16,", "citeRegEx": "Gulcehre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "In Proceedings of ICML15,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of EMNLP13,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR15", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "2016a. A diversitypromoting objective function for neural conversation", "author": ["Li et al.2016a] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "2016b. A persona-based neural conversation model", "author": ["Li et al.2016b] Jiwei Li", "Michel Galley", "Chris Brockett", "Georgios Spithourakis", "Jianfeng Gao", "Bill Dolan"], "venue": "In Proceedings of ACL16,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin"], "venue": "In Proceedings of the ACL04 Workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Generating news headlines with recurrent neural networks. CoRR, abs/1512.01712", "author": ["Konstantin Lopyrev"], "venue": null, "citeRegEx": "Lopyrev.,? \\Q2015\\E", "shortCiteRegEx": "Lopyrev.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP15,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Sequence-to-sequence rnns for text summarization. CoRR, abs/1602.06023", "author": ["Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "Paraphrastic sentence compression with a character-based metric: Tightening without deletion", "author": ["Chris CallisonBurch", "Juri Ganitkevitch", "Benjamin Van Durme"], "venue": "In Proceedings of the Workshop on Monolingual Text-", "citeRegEx": "Napoles et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2011}, {"title": "Annotated gigaword", "author": ["Matthew Gormley", "Benjamin Van Durme"], "venue": "In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,", "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Automatic summarization", "author": ["Nenkova", "McKeown2011] Ani Nenkova", "Kathleen McKeown"], "venue": "In Foundations and Trends R", "citeRegEx": "Nenkova et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2011}, {"title": "Sequence level training with recurrent neural networks. CoRR, abs/1511.06732", "author": ["Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Whole-sentence exponential language models: a vehicle for linguisticstatistical integration", "author": ["Stanley F. Chen", "Xiaojin Zhu"], "venue": "Computer Speech & Language,", "citeRegEx": "Rosenfeld et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Rosenfeld et al\\.", "year": 2001}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of EMNLP15,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] M. Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Controlling politeness in neu", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron C. Courville", "Joelle Pineau"], "venue": "In Proceedings of AAAI16,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Proceedings of NIPS14,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Chainer: a nextgeneration open source framework for deep learning", "author": ["Tokui et al.2015] Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton"], "venue": "In Proceedings of NIPS15 Workshop on LearningSys", "citeRegEx": "Tokui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokui et al\\.", "year": 2015}, {"title": "2015a. Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Proceedings of NIPS15,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "2015b. Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen et al.2015] Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "In Proceedings of EMNLP15,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Title generation with quasi-synchronous grammar", "author": ["Yansong Feng", "Mirella Lapata"], "venue": "In Proceedings of the EMNLP10,", "citeRegEx": "Woodsend et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Woodsend et al\\.", "year": 2010}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Wubben et al.2012] Sander Wubben", "Antal van den Bosch", "Emiel Krahmer"], "venue": "In Proceedings of ACL12,", "citeRegEx": "Wubben et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Bbn/umd at duc-2004: Topiary", "author": ["Zajic et al.2004] David Zajic", "Bonnie J Dorr", "R. Schwartz"], "venue": "In Proceedings of NAACL-HLT04 Document Understanding Workshop,", "citeRegEx": "Zajic et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zajic et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "Since its first use for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), the encoder-decoder approach has demonstrated great success in many other sequence generation tasks including image caption generation (Vinyals et al.", "startOffset": 44, "endOffset": 118}, {"referenceID": 33, "context": "Since its first use for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), the encoder-decoder approach has demonstrated great success in many other sequence generation tasks including image caption generation (Vinyals et al.", "startOffset": 44, "endOffset": 118}, {"referenceID": 40, "context": ", 2014), the encoder-decoder approach has demonstrated great success in many other sequence generation tasks including image caption generation (Vinyals et al., 2015b; Xu et al., 2015), parsing (Vinyals et al.", "startOffset": 144, "endOffset": 184}, {"referenceID": 32, "context": ", 2015a), dialogue response generation (Li et al., 2016a; Serban et al., 2016) and sentence summarization (Rush et al.", "startOffset": 39, "endOffset": 78}, {"referenceID": 29, "context": ", 2016) and sentence summarization (Rush et al., 2015; Chopra et al., 2016).", "startOffset": 35, "endOffset": 75}, {"referenceID": 4, "context": ", 2016) and sentence summarization (Rush et al., 2015; Chopra et al., 2016).", "startOffset": 35, "endOffset": 75}, {"referenceID": 6, "context": "summarization (Nenkova and McKeown, 2011) or headline generation (Dorr et al., 2003).", "startOffset": 65, "endOffset": 84}, {"referenceID": 23, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 0, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 27, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 21, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 13, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 12, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 4, "context": "There are already many studies that address this task (Nallapati et al., 2016; Ayana et al., 2016; Ranzato et al., 2015; Lopyrev, 2015; Gulcehre et al., 2016; Gu et al., 2016; Chopra et al., 2016).", "startOffset": 54, "endOffset": 196}, {"referenceID": 4, "context": "summarization (Nenkova and McKeown, 2011) or headline generation (Dorr et al., 2003). Recently, Rush et al. (2015) automatically constructed large training data for sentence summarization, and this has led to the rapid development of neural sentence summarization (NSS) or neural headline generation (NHG) models.", "startOffset": 66, "endOffset": 115}, {"referenceID": 6, "context": "Traditional approaches to this task focus on word deletion using rule-based (Dorr et al., 2003; Zajic et al., 2004) or statistical (Woodsend et al.", "startOffset": 76, "endOffset": 115}, {"referenceID": 41, "context": "Traditional approaches to this task focus on word deletion using rule-based (Dorr et al., 2003; Zajic et al., 2004) or statistical (Woodsend et al.", "startOffset": 76, "endOffset": 115}, {"referenceID": 38, "context": ", 2004) or statistical (Woodsend et al., 2010; Galanis and Androutsopoulos, 2010; Filippova and Strube, 2008; Filippova and Altun, 2013; Filippova et al., 2015) methods.", "startOffset": 23, "endOffset": 160}, {"referenceID": 9, "context": ", 2004) or statistical (Woodsend et al., 2010; Galanis and Androutsopoulos, 2010; Filippova and Strube, 2008; Filippova and Altun, 2013; Filippova et al., 2015) methods.", "startOffset": 23, "endOffset": 160}, {"referenceID": 24, "context": "There are also several studies of abstractive sentence summarization using syntactic transduction (Cohn and Lapata, 2008; Napoles et al., 2011) or taking a phrase-based statistical machine translation approach (Banko et al.", "startOffset": 98, "endOffset": 143}, {"referenceID": 2, "context": ", 2011) or taking a phrase-based statistical machine translation approach (Banko et al., 2000; Wubben et al., 2012; Cohn and Lapata, 2013).", "startOffset": 74, "endOffset": 138}, {"referenceID": 39, "context": ", 2011) or taking a phrase-based statistical machine translation approach (Banko et al., 2000; Wubben et al., 2012; Cohn and Lapata, 2013).", "startOffset": 74, "endOffset": 138}, {"referenceID": 33, "context": "Recent work has adopted techniques such as encoder-decoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and attentional (Bahdanau et al.", "startOffset": 59, "endOffset": 133}, {"referenceID": 3, "context": "Recent work has adopted techniques such as encoder-decoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and attentional (Bahdanau et al.", "startOffset": 59, "endOffset": 133}, {"referenceID": 22, "context": ", 2014) and attentional (Bahdanau et al., 2015; Luong et al., 2015) neural network models from the field of machine translation, and tailored them to the sentence summarization task.", "startOffset": 24, "endOffset": 67}, {"referenceID": 27, "context": "Several studies have used this task as one of the benchmarks of their neural sequence transduction methods (Ranzato et al., 2015; Lopyrev, 2015; Ayana et al., 2016).", "startOffset": 107, "endOffset": 164}, {"referenceID": 21, "context": "Several studies have used this task as one of the benchmarks of their neural sequence transduction methods (Ranzato et al., 2015; Lopyrev, 2015; Ayana et al., 2016).", "startOffset": 107, "endOffset": 164}, {"referenceID": 0, "context": "Several studies have used this task as one of the benchmarks of their neural sequence transduction methods (Ranzato et al., 2015; Lopyrev, 2015; Ayana et al., 2016).", "startOffset": 107, "endOffset": 164}, {"referenceID": 12, "context": "Some studies address the other important phenomena frequently occurred in humanwritten summaries, such as copying from the source document (Gu et al., 2016; Gulcehre et al., 2016).", "startOffset": 139, "endOffset": 179}, {"referenceID": 13, "context": "Some studies address the other important phenomena frequently occurred in humanwritten summaries, such as copying from the source document (Gu et al., 2016; Gulcehre et al., 2016).", "startOffset": 139, "endOffset": 179}, {"referenceID": 1, "context": ", 2011) or taking a phrase-based statistical machine translation approach (Banko et al., 2000; Wubben et al., 2012; Cohn and Lapata, 2013). Recent work has adopted techniques such as encoder-decoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) and attentional (Bahdanau et al., 2015; Luong et al., 2015) neural network models from the field of machine translation, and tailored them to the sentence summarization task. Rush et al. (2015) were the first to pose sentence summarization as a new target task for neural sequence-to-sequence learning.", "startOffset": 75, "endOffset": 468}, {"referenceID": 0, "context": ", 2015; Lopyrev, 2015; Ayana et al., 2016). Some studies address the other important phenomena frequently occurred in humanwritten summaries, such as copying from the source document (Gu et al., 2016; Gulcehre et al., 2016). Nallapati et al. (2016) investigate a way to solve many important problems capturing keywords, or inputting multiple sentences.", "startOffset": 23, "endOffset": 249}, {"referenceID": 0, "context": ", 2015; Lopyrev, 2015; Ayana et al., 2016). Some studies address the other important phenomena frequently occurred in humanwritten summaries, such as copying from the source document (Gu et al., 2016; Gulcehre et al., 2016). Nallapati et al. (2016) investigate a way to solve many important problems capturing keywords, or inputting multiple sentences. Neural encoder-decoders can also be viewed as statistical language models conditioned on the target sentence context. Rosenfeld et al. (2001) have proposed whole-sentence language models that can consider features such as sentence length.", "startOffset": 23, "endOffset": 495}, {"referenceID": 37, "context": "Finally, there are some studies to modify the output sequence according some meta information such as the dialogue act (Wen et al., 2015), user personality (Li et al.", "startOffset": 119, "endOffset": 137}, {"referenceID": 31, "context": ", 2016b), or politeness (Sennrich et al., 2016).", "startOffset": 24, "endOffset": 47}, {"referenceID": 29, "context": "For example, we use sentence-summary pairs extracted from the Annotated English Gigaword corpus as training data (Rush et al., 2015), and the average length of human-written summary is 51.", "startOffset": 113, "endOffset": 132}, {"referenceID": 29, "context": "While recent NSS models themselves cannot control their output length, Rush et al. (2015) and others following use an ad-hoc method, in which the system is inhibited from generating the end-of-sentence (EOS) tag by assigning a score of\u2212\u221e to the tag and", "startOffset": 71, "endOffset": 90}, {"referenceID": 11, "context": ", 2015) and speech recognition (Schuster and Paliwal, 1997; Graves et al., 2013).", "startOffset": 31, "endOffset": 80}, {"referenceID": 22, "context": "We also use the attention mechanism developed by Luong et al. (2015), which uses st to compute contextual information dt of time step t.", "startOffset": 49, "endOffset": 69}, {"referenceID": 22, "context": "Note that s\u0303t is also provided as input to the LSTM with yt for the next step, which is called the input feeding architecture (Luong et al., 2015).", "startOffset": 126, "endOffset": 146}, {"referenceID": 37, "context": "Inspired by previous work that has demonstrated that additional inputs to decoder models can effectively control the characteristics of the output (Wen et al., 2015; Li et al., 2016b), this model provides information about the length in the form of an additional input to the net.", "startOffset": 147, "endOffset": 183}, {"referenceID": 25, "context": "We trained our models on a part of the Annotated English Gigaword corpus (Napoles et al., 2012), which Rush et al.", "startOffset": 73, "endOffset": 95}, {"referenceID": 24, "context": "We trained our models on a part of the Annotated English Gigaword corpus (Napoles et al., 2012), which Rush et al. (2015) constructed for sentence summarization.", "startOffset": 74, "endOffset": 122}, {"referenceID": 20, "context": "We used three variants of ROUGE (Lin, 2004) as evaluation metrics: ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence).", "startOffset": 32, "endOffset": 43}, {"referenceID": 15, "context": "0 for the other gate biases (J\u00f3zefowicz et al., 2015).", "startOffset": 28, "endOffset": 53}, {"referenceID": 34, "context": "We use Chainer (Tokui et al., 2015) to implement our models.", "startOffset": 15, "endOffset": 35}, {"referenceID": 29, "context": "ABS (Rush et al., 2015) is the most standard model of neural sentence summarization and is the most similar method to our baseline setting (fixLen).", "startOffset": 4, "endOffset": 23}, {"referenceID": 29, "context": "25 ABS(Rush et al., 2015) 26.", "startOffset": 6, "endOffset": 25}, {"referenceID": 29, "context": "05 ABS+(Rush et al., 2015) 28.", "startOffset": 7, "endOffset": 26}, {"referenceID": 4, "context": "81 RAS-Elman(Chopra et al., 2016) 28.", "startOffset": 12, "endOffset": 33}, {"referenceID": 4, "context": "06 RAS-LSTM(Chopra et al., 2016) 27.", "startOffset": 11, "endOffset": 32}], "year": 2016, "abstractText": "Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods.1 Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.", "creator": "LaTeX with hyperref package"}}}