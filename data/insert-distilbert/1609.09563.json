{"id": "1609.09563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Asynchronous Multi-Task Learning", "abstract": "many real - world machine learning applications involve several learning comprehension tasks which are uniquely inter - related. for example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. the models for each hospital may be different because any of the inherent differences in the distributions functioning of the patient populations. however, the models are also closely related because of the nature of the learning tasks modeling the same terminal disease. by simultaneously learning all the tasks, multi - task learning ( mtl ) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. when datasets for the learning tasks models are stored at different locations, it may not always be feasible to transfer the data to provide a data - centralized computing environment due to various practical geographic issues such as high data volume and privacy. in this 2013 paper, we propose a principled mtl framework for distributed and asynchronous optimization to address the aforementioned challenges. in our framework, gradient update does not wait for collecting the gradient information from all the tasks. therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. we show that many regularized mtl formulations can benefit from this framework, including the low - rank mtl for shared subspace learning. empirical collaborative studies on both computational synthetic and real - world datasets demonstrate the efficiency and effectiveness of the proposed framework.", "histories": [["v1", "Fri, 30 Sep 2016 01:23:15 GMT  (1459kb,D)", "http://arxiv.org/abs/1609.09563v1", "IEEE International Conference on Data Mining (ICDM) 2016"]], "COMMENTS": "IEEE International Conference on Data Mining (ICDM) 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["inci m baytas", "ming yan", "anil k jain", "jiayu zhou"], "accepted": false, "id": "1609.09563"}, "pdf": {"name": "1609.09563.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Multi-Task Learning", "authors": ["Inci M. Baytas", "Ming Yan", "Anil K. Jain", "Jiayu Zhou"], "emails": ["jiayuz}@msu.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nTechnological advancements in the last decade have transformed many industries, where the vast amount of data from different sources of various types are collected and stored. The huge amount of data has provided an unprecedented opportunity to build data mining and machine learning algorithms to gain insights from the data to help researchers study complex problems and develop better solutions.\nIn many application domains, we need to perform multiple machine learning tasks, where all the tasks involve learning a model such as regression and classification. For example, in medical informatics, it is of great interest to learn highperformance predictive models to accurately predict diagnostic outcomes for different types of diseases from patients\u2019 electronic medical records. Another example is to learn predictive models for different hospitals, where hospitals are usually located in different cities, and the distributions of patient populations are different from hospital to hospital. Therefore, we will need to build one predictive model for each hospital to address the heterogeneity in the population. In many situations, such machine learning models are closely related. As in the aforementioned examples, different types of diseases may be related through similar pathways in the\nindividual pathologies, and models from different hospitals are inherently related because they are predicting the same target for different populations. To leverage the relatedness among tasks, multi-task learning (MTL) techniques have been developed to simultaneously learn all the related tasks to perform inductive knowledge transfer among the tasks and improve the generalization performance.\nThe rationale behind the MTL paradigm is that when there is not enough data to learn a high-quality model, transferring and leveraging predictive information from other related tasks can improve the generalization performance. In the big data era, even though the volume of the data is often huge, we may still not have enough data to learn high-quality models because: (i) the number of features, hence the model parameters, can be huge, which leads to the curse of dimensionality (e.g., tens of millions of single nucleotide polymorphism in human genomic data); (ii) the distribution of the data points in the training set can be highly skewed leading to bias in the learning algorithms (e.g., regional-biased patient population in each hospital); (iii) powerful models often have high model complexities or degree-of-freedom, and require much more training data for fitting (e.g., deep convolutional neural networks). Therefore, MTL is a critical machine learning technique for predictive modeling and data analytics even when high-volume data is available.\nar X\niv :1\n60 9.\n09 56\n3v 1\n[ cs\n.L G\n] 3\n0 Se\np 20\n16\nA majority of research on MTL has focused on regularized MTL, where the task relatedness is enforced through adding a regularization term to the multi-task loss function. The regularization term couples the learning tasks and induces knowledge transfer during the learning phase. Regularized MTL is considered to be powerful and versatile because of its ability to incorporate various loss functions (e.g., least squares, logistic regression, hinge loss), flexibility to model many common task relationships (e.g., shared feature subset, shared feature subspace, task clustering), and principled optimization approaches. In order to couple the learning tasks, the regularization term in regularized MTL is typically nonsmooth, and thus renders the entire problem to be a composite optimization problem, which is typically solved by proximal gradient methods.\nLarge-scale datasets are typically stored in distributed systems and can be even located at different data centers. There are many practical considerations that prevent the data from being transferred over the network. Let us revisit the MTL in the setting of a hospital network, where the hospitals are located in different regions and have their own data centers. The patient data is extremely sensitive and cannot be frequently transferred over the network even after careful encryption. As such, distributed algorithms are desired to solve the distributed MTL problems, where each task node (server) computes the summary information (e.g., gradient) locally and then the information from different nodes is aggregated at a central node to guide the gradient descent direction. However, because of the non-smoothness nature of the objective, the proximal projection in the MTL optimization requires synchronized gradient information from all task nodes before the next search point can be obtained. Such synchronized distributed optimization can be extremely slow due to high network communication delays in some task nodes.\nIn this paper, we propose a distributed MTL framework to address the aforementioned challenges. The proposed MTL framework: 1) provides a principled way to solve the original MTL formulation, as compared to some recent work on distributed MTL systems that use heuristic or approximate algorithms; 2) is equipped with a linear convergence rate for a global optimal for convex MTL formulations under mild assumptions; 3) allows asynchronized updates from multiple servers and is thus robust against network infrastructures with high communication delays between the central node and some task nodes. The framework is capable of solving most existing regularized MTL formulations and is compatible with the formulations in the MTL package MALSAR [1]. As a case study, we elaborate the low-rank MTL formulations, which transfer knowledge via learning a low-dimensional subspace from task models. We evaluate the proposed framework on both synthetic and real-world datasets and demonstrate that the proposed algorithm can significantly outperform the synchronized distributed MTL, under different network settings.\nThe rest of the paper is organized as follows: Section II revisits the major areas that are related to this research and establish the connections to the state-of-the-arts. Section III\nelaborates the proposed framework. Section IV presents the results from empirical studies. Section V provides discussions on some key issues and lays out directions for future work."}, {"heading": "II. RELATED WORK", "text": "In this section, a literature review of distributed optimization and distributed MTL is given."}, {"heading": "A. Distributed optimization", "text": "Distributed optimization techniques exploit technological improvements in hardware to solve massive optimization problems fast. One commonly used distributed optimization approach is alternating direction method of multipliers (ADMM), which was firstly proposed in the 1970s [2]. Boyd et al. defined it as a well suited distributed convex optimization method. In the distributed ADMM framework in [2], local copies are introduced for local subproblems, and the communication between the work nodes and the center node is for the purpose of consensus. Though it can fit in a large-scale distributed optimization setting, the introduction of local copies increases the number of iterations to achieve the same accuracy. Furthermore, the approaches in [2] are all synchronized.\nIn order to avoid introducing multiple local copies and introduce asynchrony, Iutzeler et al. proposed an asynchronous distributed approach using randomized ADMM [3] based on randomized Gauss-Seidel iterations of a Douglas-Rachford splitting (DRS) because ADMM is equivalent to DRS. For the asynchronous distributed setting, they assumed a set of network agents, where each agent has an individual cost function. The goal was to find a consensus on the minimizer of the overall cost function which consists of individual cost functions.\nAybat et al. introduced an asynchronous distributed proximal gradient method by using the randomized block coordinate descent method in [4] for minimizing the sum of a smooth and a non-smooth functions. The proposed approach was based on synchronous distributed first-order augmented Lagrangian (DFAL) algorithm.\nLiu et al. proposed an asynchronous stochastic proximal coordinate descent method in [5]. The proposed approach introduced a distributed stochastic optimization scheme for composite objective functions. They adopted an inconsistent read mechanism where the elements of the optimization variable may be updated by multiple cores during being read by another core. Therefore, cores read a hybrid version of the variable which had never been existed in the memory. It was shown that the proposed algorithm has a linear convergence rate for a suitable step size under the assumption that the optimal strong convexity holds.\nRecently, Peng et al. proposed a general asynchronous parallel framework for coordinate updates based on solving fixed-point problems with non-expansive operators [6], [7]. Since many famous optimization algorithms such as gradient descent, proximal gradient method, ADMM/DRS, and primaldual method can be expressed as non-expansive operators. This framework can be applied to many optimization problems and\nmonotone inclusion problems [8]. The procedure for applying this framework includes transferring the problem into a fixedpoint problem with a non-expansive operator, applying ARock on this non-expansive operator, and transferring it back to the original algorithm. Depending on the structures of the problem, there may be multiple choices of non-expansive operators and multiple choices of asynchronous algorithms. These asynchronous algorithms may have very different performance on different platforms and different datasets."}, {"heading": "B. Distributed multi-task learning", "text": "In this section, studies about distributed MTL in the literature are summarized. In real-world MTL problems, geographically distributed vast amount of data such as healthcare datasets is used. Each hospital has its own data consisting of information such as patient records including diagnostics, medication, test results, etc. In this scenario, two of the main challenges are network communication and the privacy. In traditional MTL approaches, data transfer from different sources is required. However, this task can be quite costly because of the bandwidth limitations. Another point is that hospitals may not want to share their data with others in terms of the privacy of their patients. Distributed MTL provides a solution by using distributed optimization techniques for the aforementioned challenges. In distributed MTL, data is not needed to be transferred to a central node. Since only the learned models are transferred instead of the whole raw data, the cost of network communication is reduced. In addition, distributed MTL mitigates the privacy problem, since each worker updates their models independently. For instance, Dinuzzo and Pillonetto in [9] proposed a client-server MTL from distributed datasets in 2011. They designed an architecture that was simultaneously solving multiple learning tasks. In their setting, client was an individual learning task. Server was responsible for collecting the data from clients to encode the information in a common database in real time. In this setting, each client could access the information content of all the data on the server without knowing the actual data. MTL problem was solved by regularized kernel methods in that paper.\nMateos-Nu\u0301n\u0303ez and Corte\u0301z focused on distributed optimization techniques for MTL in [10]. Authors defined a separable convex optimization problem on local decision variables where the objective function comprises a separable convex cost function and a joint regularization for low rank solutions. Local gradient calculations were divided into a network of agents. Their second solution consisted of a separable saddlepoint reformulation through Fenchel conjugation of quadratic forms. A separable min-max problem was derived, and it has iterative distributed approaches that prevent from calculating the inverses of local matrices.\nJin et al., on the other hand, proposed collaborating between local and global learning for distributed online multiple tasks in [11]. Their proposed method learned individual models with continuously arriving data. They combined MTL along with distributed learning and online learning. Their proposed scheme performed local and global learning alternately. In\nthe first step, online learning was performed locally by each client. Then, global learning was done by the server side. In their framework, clients still send a portion of the raw data to the global server and the global server coordinates the collaborative learning.\nIn 2016, Wang et al. also proposed a distributed scheme for MTL with shared representation. They defined sharedsubspace MTL in a distributed multi-task setting by proposing two subspace pursuit approaches. In their proposed setting, there were m separate machines and each machine was responsible for one task. Therefore, each machine had access only for the data of the corresponding task. Central node was broadcasting the updated models back to the machines. As in [10], a strategy for nuclear norm regularization that avoids heavy communication was investigated. In addition, a greedy approach was proposed for the subspace pursuit which was communication efficient. Optimization was done by a synchronous fashion. Therefore, workers and master nodes had to wait for each other before proceeding.\nThe prominent result is that all these methods follow a synchronize approach while updating the models. When there is a data imbalance, the computation in the workers which have larger amount of data will take longer. Therefore, other workers will have to wait, although they complete their computations. In this paper, we propose to use an asynchronous optimization approach to prevent workers from waiting for others and to reduce the training time for MTL."}, {"heading": "III. ASYNCHRONOUS MULTI-TASK LEARNING", "text": ""}, {"heading": "A. Regularized multi-task learning", "text": "In MTL, multiple related learning tasks are involved, and the goal of MTL is to obtain models with improved generalization performance for all the tasks involved. Assume that we have T supervised learning tasks, and for each task, we are given the training data Dt = {xt, yt} of nt data points, where xt \u2208 Rnt\u00d7d is the feature vectors for the training data points and yt \u2208 Rnt includes the corresponding responses. Assume that the target model is a linear model parametrized by the vector wt \u2208 Rd (in this paper we slightly abuse the term \u201cmodel\u201d to denote the vector wt), and we use `t(wt) to denote the loss function `t(xt, yt;wt), examples of which include the least squares loss and the logistic loss. In addition, we assume that the tasks can be heterogeneous [12], i.e., some tasks can be regression while the other tasks are classification. In a single learning task, we treat the task independently and minimize the corresponding loss function, while in MTL, the tasks are related, and we hope that by properly assuming task relatedness, the learning of one task (the inference process of wt) can benefit from other tasks [13]. Let W = [w1, . . . , wT ] \u2208 Rd\u00d7T collectively denote the learning parameters from all T tasks. We note that\nsimply minimizing the joint objective f(W ) = T\u2211 t=1 `t(wt) cannot achieve the desired knowledge transfer among tasks because the minimization problems are decoupled for each\nwt. Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:\nmin W {\u2211T t=1 `t(wt) + \u03bbg(W ) } \u2261 f(W ) + \u03bbg(W ), (III.1)\nwhere g(W ) encodes the assumption of task relatedness and couples T tasks, and \u03bb is the regularization parameter controlling how much knowledge to shared among tasks. In this paper, we assume that the loss function f(W ) is convex and L-Lipschitz differentiable with L > 0 and g(W ) is closed proper convex. One representative task relatedness is joint feature learning, which is achieved via the grouped sparsity induced by penalizing the `2,1-norm [16] of the model matrix W : g(W ) = \u2016W\u20162,1 = \u2211d i=1 \u2016wi\u20162 where wi is the ith row of W . The grouped sparsity would encourage many rows of W to be zero and thus remove the effects of the corresponding features on the predictions in linear models. Another commonly used MTL method is the shared subspace learning [17], which is achieved by penalizing the nuclear norm g(W ) = \u2016W\u2016\u2217 = \u2211min(d,T ) i=1 \u03c3i(W ), where \u03c3i(W ) is the i-th singular value of the matrix W . Intuitively, a low-rank W indicates that the columns of W (the models of tasks) are linearly dependent and come from a shared low-dimensional subspace. The nuclear norm is the tightest convex relaxation of the rank function [18], and the problem can be solved via proximal gradient methods [19]."}, {"heading": "B. (Synchronized) distributed optimization of MTL", "text": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22]. Below we review the two key computations involved in these algorithms: 1) Gradient Computation. The gradient of the smooth component is computed by aggregating gradient vectors from the loss function of each task:\n\u2207f(W ) = \u2207 \u2211T\nt=1 `t(wt) = [\u2207`1(w1), . . . ,\u2207`T (wT )].\n(III.2)\n2) Proximal Mapping. After the gradient update, the next search point is obtained by the proximal mapping which solves the following optimization problem:\nProx\u03b7\u03bbg(W\u0302 ) = arg min W\n1\n2\u03b7 \u2016W \u2212 W\u0302\u20162F + \u03bbg(W ), (III.3)\nwhere \u03b7 is the step size, W\u0302 is computed from the gradient descent with step size \u03b7: W\u0302 = W \u2212 \u03b7\u2207f(W ), and \u2016 \u00b7 \u2016F is the Frobenius norm.\nWhen the size of data is big, the data is typically stored in different pieces that spread across multiple computers or even multiple data centers. For MTL, the learning tasks typically involve different sets of samples, i.e., D1, . . . ,DT have different data points, and it is very common that these datasets are collected through different procedures and stored at different locations. It is not always feasible to transfer the\nrelevant data pieces to one center to provide a centralized data environment for optimization. The reason why a centralized data environment is difficult to achieve is simply because the size of the dataset is way too large for efficient network transfer, and there would be concerns such as network security and data privacy. Recall the scenario in the introduction, where the learning involves patients\u2019 medical records from multiple hospitals. Transferring patients\u2019 data outside the respective hospital data center would be a huge concern, even if the data is properly encrypted. Therefore, it is imperative to seek distributed optimization for MTL, where summarizing information from data is computed only locally and then transferred over the network.\nWithout loss of generality, we assume a general setting where the datasets involved in tasks are stored in different computer systems that are connected through a star-shaped network. Each of these computer systems, called a node, worker or an agent, has full access to the data Dt for one task, and is capable of numerical computations (e.g., computing gradient). We assume that there is a central server that collects the information from the task agents and performs the proximal mapping. We now investigate aforementioned key operations involved and see how the optimization can be distributed across agents to fit this architecture and minimize the computational cost. Apparently, the gradient computation in Eq. (III.2) can be easily parallelized and distributed because of the independence of gradients among tasks. Naturally, the proximal mapping in Eq. (III.3) can be carried out as soon as the gradient vectors are collected and W\u0302 is obtained. The projected solution after the proximal mapping is then sent back to the task agents to prepare for the next iteration. This synchronized distributed approach assembles a mapreduce procedure and can be easily implemented. The term \u201csynchronize\u201d indicates that, at each iteration, we have to wait for all the gradients to be collected before the server (and other task agents) can proceed.\nSince the server waits for all task agents to finish at every iteration, one apparent disadvantage is that when one or more task agents are suffering from high network delay or even failure, all other agents must wait. Because the first-order optimization algorithm typically requires many iterations to converge to an acceptable precision, the extended period of waiting time in a synchronized optimization will lead to prohibitive algorithm running time and a waste of computing resources."}, {"heading": "C. Asynchronized framework for distributed MTL", "text": "To address the aforementioned challenges in distributed MTL, we propose to perform asynchronous multi-task learning (AMTL), where the central server begins to update model matrix W after it receives a gradient computation from one task node, without waiting for the other task nodes to finish their computations. While the server and all task agents maintain their own copies of W in the memory, the copy at one task node may be different from the copies at other nodes. The convergence analysis of the proposed AMTL framework\nis backed up by a recent approach for asynchronous parallel coordinate update problems by using Krasnosel\u2019skii-Mann (KM) iteration [6], [7]. A task node is said to be activated when it performs computation and network communication with the central server for updates. The framework is based on the following assumption on the activation rate:\nAssumption 1. All the task nodes follow independent Poisson processes and have the same activation rate.\nRemark 1. We note that when the activation rates are different for different task nodes, we can modify the theoretical result by changing the step size: if a task node\u2019s activation rate is large, then the probability that this task node is activated is large and thus the corresponding step size should be small. In Section III-D we propose a dynamic step size strategy for the proposed AMTL.\nThe proposed AMTL uses a backward-forward operator splitting method [23], [8] to solve problem (III.1). Solving problem (III.1) is equivalent to finding the optimal solution W \u2217 such that 0 \u2208 \u2207f(W \u2217) + \u03bb\u2202g(W \u2217), where \u2202g(W \u2217) denotes the set of subgradients of non-smooth function g(\u00b7) at W \u2217 and we have the following:\n0 \u2208 \u2207f(W \u2217) + \u03bb\u2202g(W \u2217) \u21d0\u21d2 \u2212\u2207f(W \u2217) \u2208 \u03bb\u2202g(W \u2217) \u21d0\u21d2W \u2217 \u2212 \u03b7\u2207f(W \u2217) \u2208W \u2217 + \u03b7\u03bb\u2202g(W \u2217).\nTherefore the forward-backward iteration is given by:\nW+ = (I + \u03b7\u03bb\u2202g)\u22121(I \u2212 \u03b7\u2207f)(W ),\nwhich converges to the solution if \u03b7 \u2208 (0, 2/L). Since \u2207f(W ) is separable, i.e., \u2207f(W ) = [\u2207`1(w1),\u2207`2(w2), \u00b7 \u00b7 \u00b7 ,\u2207`T (wT )], the forward operator, i.e., I \u2212 \u03b7\u2207f , is also separable. However, the backward operator, i.e., (I+\u03b7\u03bb\u2202g)\u22121, is not separable. Thus, we can not apply the coordinate update directly on the forward-backward iteration. If we switch the order of forward and backward steps, we obtain the following backward-forward iteration:\nV + = (I \u2212 \u03b7\u2207f)(I + \u03b7\u03bb\u2202g)\u22121(V ),\nwhere we use an auxiliary matrix V \u2208 Rd\u00d7T instead of W during the update. This is because the update variables in the forward-backward and backward-forward are different variables. Moreover, one additional backward step is needed to obtain W \u2217 from V \u2217. We can thus follow [6] to perform task block coordinate update at the backward-forward iteration, where each task block is defined by the variables associated to a task. The update procedure is given as follows:\nv+t = (I \u2212 \u03b7\u2207`t) ( (I + \u03b7\u03bb\u2202g)\u22121(V ) ) t ,\nwhere vt \u2208 Rd is the corresponding auxiliary variable of wt for task t. Note that updating one task block vt will need one full backward step and a forward step only on the task block. The overall AMTL algorithm is given in 1. The formulation in Eq. III.4 is the update rule of KM iteration discussed in [6]. KM iteration provides a generic framework to solve fixed\npoint problems where the goal is to find the fixed point of a nonexpansive operator. In the problem setting of this study, backward-forward operator is our fixed-point operator. We refer to Section 2.4 of [6] to see how Eq. III.4 is derived.\nIn general, the choice between forward-backward or backward-forward is largely dependent on the difficulty of the sub problem. If the backward step is easier to compute compared to the forward step, e.g., data (xt, yt) is large, then we can apply coordinate update on the backward-forward iteration. Specifically in the MTL settings, the backward step is given by a proximal mapping in Eq. III.3 and usually admits an analytical solution (e.g., soft-thresholding on singular values for trace norm). On the other hand, the gradient computation in Eq. III.2 is typically the most time consuming step for large datasets. Therefore backward-forward provides a more computational efficient optimization framework for distributed MTL. In addition, we note that the backward-forward iteration is a non-expansive operator if \u03b7 \u2208 (0, 2/L) because both the forward and backward steps are non-expansive.\nWhen applying the coordinate update scheme in [6], all task nodes have access to the shared memory, and they do not communicate with each other. Further the communicate between each task node and the central server is only the vector vt , which is typically much smaller than the data Dt stored locally at each task node. In the proposed AMTL scheme, the task nodes do not share memory but are exclusively connected and communicate with the central node. The computation of the backward step is located in the central node, which performs the proximal mapping after one gradient update is received from a task node (the proximal mapping can be also applied after several gradient updates depending on the speed of gradient update). In this case, we further save the communication cost between each task node and the central node, because each task node only need the task block corresponding to the task node.\nTo illustrate the asynchronous update mechanism in AMTL, we show an example in Fig. 2. The figure shows order of the backward and the forward steps performed by the central node and the task nodes. At time t1, the task node 2 receives the model corresponding to the task 2 which was previously updated by the proximal mapping step in the central node. As soon as task node 2 receives its model, the forward (gradient) step is launched. After the task gradient descent update is done, the model of the task 2 is sent back to the central node. When the central node receives the updated model from the task node 2, it starts applying the proximal step on the whole multi-task model matrix. As we can see from the figure, while task node 2 was performing its gradient step, task node 4 had already sent its updated model to the central node and triggered a proximal mapping step during time steps t2 and t3. Therefore, the model matrix was updated upon the request of the task node 4 during the gradient computation of task node 2. Thus we know that the model received by the task node 2 at time t1 is not the same copy as in the central server any more. When the updated model from the task node 2 is received by the central node, proximal mapping computations\nAlgorithm 1 The proposed Asynchronous Multi-Task Learning framework Require: Multiple related learning tasks reside at task nodes, including the training data and the loss function for each task {x1, y1, `t}, ..., {xT , yT , `T }, maximum delay \u03c4 , step size \u03b7, multi-task regularization parameter \u03bb. Ensure: Predictive models of each task v1, ..., vT . Initialize task nodes and the central server. Choose \u03b7k \u2208 [\u03b7min, c2\u03c4/\u221aT+1 ] for any constant \u03b7min > 0 and 0 < c < 1 while every task node asynchronously and continuously do\nTask node t requests the server for the forward step computation Prox\u03b7\u03bbg ( v\u0302k ) , and\nRetrieves ( Prox\u03b7\u03bbg ( v\u0302k ))\nt\nfrom the central server and\nComputes the coordinate update on vt\nvk+1t = v k t + \u03b7k (( Prox\u03b7\u03bbg ( v\u0302k ))\nt\n\u2212 \u03b7\u2207`t (( Prox\u03b7\u03bbg(v\u0302k) ) t ) \u2212 vkt ) (III.4)\nSends updated vt to the central node. end while\nare done by using the model received from task node 2 and the updated models at the end of the proximal step triggered by the task node 4. Similarly, if we think of the model received by task node 4 at time t3, we can say that it will not be the same model as in the central server when task node 4 is ready to send its model to the central node because of the proximal step triggered by the task node 2 during time steps t4 and t5. This is because in AMTL, there is no memory lock during reads. As we can see, the asynchronous update scheme has inconsistency when it comes to read model vectors from the central server. We note that such inconsistency caused by the backward step is already taken into account in the convergence analysis.\nWe summarize the proposed AMTL algorithm in Algorithm 1. The AMTL framework enjoys the following convergence property:\nTheorem 1. Let (V k)k\u22650 be the sequence generated by the proposed AMTL with \u03b7k \u2208 [\u03b7min, c2\u03c4/\u221aT+1 ] for any \u03b7min > 0 and 0 < c < 1, where \u03c4 is the maximum delay. Then (V k)k\u22650 converges to an V \u2217-valued random variable almost surely. If the MTL problem in Eq. III.1 has a unique solution, then the sequence converges to the unique solution.\nAccording to our assumptions, all the task nodes are independent Poisson processes and each task node has the same activation rate. The probability that each task node is activated before other task nodes is 1/T [24], and therefore we can assume that each coordinate is selected with the same probability. The results in [6] can be applied to directly obtain the convergence. We note that some MTL algorithms based on sparsity-inducing norms may not have a unique solution, as commonly seen in many sparse learning formulations, but in this case we typically can add an `2 term to ensure the strict convexity and obtain linear convergence as shown in [6]. An example of such technique is the elastic net variant from the original Lasso problem [25]."}, {"heading": "D. Dynamic step size controlling in AMTL", "text": "As discussed in Remark 1, the AMTL is based on the same activation rate in Assumption 1. However because of the topology of the network in real-world settings, the same activation rate is almost impossible. In this section, we propose a dynamic step size for AMTL to overcome this challenge. We note that in order to ensure convergence, the step sizes used in asynchronous optimization algorithms are typically much smaller than those used in the synchronous optimization algorithms, which limits the algorithmic efficiency of the solvers. The dynamic step was recently used in a specific setting via asynchronous optimization to achieve better overall performance [26].\nOur dynamic step size is motivated by this design, and revises the update of AMTL in Eq. III.4 by augmenting a time related multiplier:\nvk+1t = v k t + c(t,k)\u03b7k (Prox\u03c4\u03bbg (v\u0302k)) t\n\u2212\u03b7\u2207`t (( Prox\u03c4\u03bbg(v\u0302k) ) t ) \u2212 vkt ) (III.5) where the multiplier is given by:\nc(t,k) = log ( max ( \u03bd\u0304t,k, 10 )) (III.6)\nand \u03bd\u0304t,k = 1k+1 \u2211z i=z\u2212k \u03bd (i) t is the average of the last k + 1 delays in task node t, z is the current time point, and \u03bd(i)t is the delay at time i for task t. As such, the actual step size given by c(t,k)\u03b7k will be scaled by the history of communication delay between the task nodes and the central server. The longer the delay, the larger is the step size to compensate for the loss from the activation rate. We note that though in our experiments we show the effectiveness of the propose scheme, instead of using Eq. (III.6), different kinds of function could also be used. Currently, these are no theoretical results on how a dynamic step could improve the general problem. Further what types of dynamics could better serve the purpose remains an open problem in the optimization research."}, {"heading": "IV. NUMERICAL EXPERIMENTS", "text": "The proposed asynchronous distributed MTL framework is implemented in C++. In our experiments, we simulate the distributed environment using the shared memory architecture in [6] with network delays introduced to the work nodes. In this section, we first elaborate how AMTL performs and then present experiments on synthetic and real-world datasets to compare the training times of the proposed AMTL and synchronous distributed multi-task learning (SMTL). We also investigate empirical convergence behaviors of AMTL and traditional SMTL by comparing them on synthetic datasets. Finally, we study the effect of dynamic step size on synthetic datasets with various numbers of tasks. Experiments were conducted with an Intel Core i5-5200U CPU 2.20GHz x 4 laptop. Note that the performance is limited by the hardware specifications such as the number of cores."}, {"heading": "A. Experimental setting", "text": "In order to simulate the distributed environment using a shared memory system, we use threads to simulate the task nodes, and the number of threads is equal to the number of tasks. As such, each thread is responsible for learning the model parameters of one task and communicating with the central node to achieve knowledge transfer, where the central node is simulated by the shared memory.\nThough the framework can be used to solve many regularized MTL formulations, in this paper we focus on one specific MTL formulation\u2013the low-rank MTL for shared subspace learning. In the formulation, we assume that all tasks learn a regression model with the least squares loss\u2211T t=1 \u2016xtwt \u2212 yt\u2016 2 2. Recall that xt, nt, xt,i, and yt,i denote the data matrix, sample size, i-th data sample of the task y, and the i-th label of the task t, respectively. The nuclear norm is used as the regularization function that couples the tasks by learning a shared low-dimensional subspace, which serves as the basis for knowledge transfer among tasks. The formulation is given by:\nmin W {\u2211T t=1 \u2016xtwt \u2212 yt\u201622 + \u03bb\u2016W\u2016\u2217 } . (IV.1)\nAs it was discussed in the previous sections, we used the backward-forward splitting scheme for AMTL, since the\nnuclear norm is not separable. The proximal mapping for nuclear norm regularization is given by [19], [27]:\nProx\u03b7\u03bbg ( V\u0302 k ) = min{d,T}\u2211 i=1 max (0, \u03c3i \u2212 \u03b7\u03bb)uiv>i\n= U (\u03a3\u2212 \u03b7\u03bbI)+ V >\n(IV.2)\nwhere {ui} and {vi} are columns of U and V , respectively, V\u0302 k = U\u03a3V > is the singular value decomposition (SVD) of V\u0302 k and (x)+ = max (0, x). In every stage of the AMTL framework, copies of all the model vectors are stored in the shared memory. Therefore, when the central node is performing the backward step, it retrieves the current versions of the models from the shared memory. Since the proposed framework is an asynchronous distributed system, copies of the models may be changed by task nodes while the central node is performing the proximal mapping. Whenever a task node completes its computation, it sends its model to the central node for proximal mapping without waiting for other task nodes to finish their forward steps.\nAs it is seen in Eq. (IV.2), every backward step requires a singular value decomposition (SVD) of the model matrix. When the number of tasks T and the dimensionality d are high, SVD is a computationally expensive operation. Instead of computing the full SVD at every step, online SVD can also be used [28]. Online SVD updates U, V , and \u03a3 matrices by using the previous values. Therefore, SVD is performed once at the beginning and those left, right, and singular value matrices are used to compute the SVD of the updated model matrix. Whenever a column of the model matrix is updated by a task node, central node computes the proximal mapping. Therefore, instead of performing the full SVD every time, we can update the SVD of the model matrix according to the changed column. When we need to deal with a huge number of tasks and high dimensionality, online SVD can be used to mitigate computational complexity."}, {"heading": "B. Comparison between AMTL and SMTL", "text": "1) Public and Synthetic Datasets: In this section, the difference in computation times of AMTL and SMTL is investigated with varying number of tasks, dimensionality, and sample sizes since both AMTL and SMTL have nearly identical progress per iteration (every task node updates one forward step for each iteration). Synthetic datasets were randomly generated. In Fig. 3, the computation time for a varying number of tasks, sample sizes, and dimensionalities is shown. In Fig. 3a, the dimensionality of the dataset was chosen as 50, and the sample size of each task was chosen as 100. As observed from Fig. 3a, computation time increases with increasing number of tasks because the total computational complexity increases. However, the increase is more drastic for SMTL. Since each task node has to wait for other task nodes to finish their computations in SMTL, increasing the number of tasks causes the algorithm to take much longer than AMTL. For Fig. 3a, computation time still increases after 100 tasks for AMTL because the number of backward steps increases as the number\nof tasks increases. An important point we should note is that the number of cores we ran our experiments was less than 100. There is a dependecy on hardware.\nIn the next experiment, random datasets were generated with different sample sizes, and the effect of varying sample sizes on the computation times of AMTL and SMTL is shown in Fig. 3b. The number of tasks were chosen as 5, and the dimensionality was 50. Increasing the sample size did not cause abrupt changes in computation times for both asynchronous and synchronous settings. That is because computing the gradient has a similar cost to the proximal mapping for small sample sizes and the cost for computing the gradient increases as the sample size increases while the cost for the proximal mapping keeps unchanged. However, AMTL is still faster than SMTL even for a small number of tasks.\nIn Fig. 3c, the computational times of AMTL and SMTL for different dimensionalities is shown. As expected, the time required for both schemes increases with higher dimensionalities. On the other hand, we can observe from Fig. 3c that the gap between AMTL and SMTL also increases. In SMTL, task nodes have to wait longer for the updates, since the calculations of the backward and the forward steps are prolonged because of higher d.\nIn Table I, computation times of AMTL and SMTL with different network characteristics for synthetic datasets with a varying number of tasks are summarized. Synthetic datasets were randomly generated with 100 samples for each task, and the dimensionality was set to 50. The results are shown for datasets with 5, 10, and 15 tasks. Similar to the previous experimental settings, a regression problem with the squared loss and nuclear norm regularization was taken into account. As it was shown, AMTL outperforms SMTL at every dimensionality, sample size, and number of tasks considered here. AMTL also performs better than SMTL under different communication delay patterns. In Table I, AMTL-5, AMTL10, and AMTL-30 represent the AMTL framework where the offset values of the delay were chosen as 5, 10, and 30 seconds. Simulating the different network settings for our experiments, an offset parameter was taken as an input from the user. This parameter represents an average delay related to the infrastructure of the network. Then the amount of delay was computed as the sum of the offset and a random value in each task node. AMTL is shown to be more time efficient than SMTL under the same network settings.\nThe performance of AMTL and SMTL is also shown for three public datasets. The number of tasks, sample sizes, and the dimensionality of the data sets are given in Table II. School and MNIST are commonly used public datasets. School dataset has exam records of 139 schools in 1985, 1986, and 1987 provided by the London Education Authority (ILEA) [29]. MNIST is a popular handwritten digits dataset with 60, 000 training samples and 10, 000 test samples [30]. MNIST is prepared as 5 binary classification tasks such as 0 vs 9, 1 vs 8, 2 vs 7, 3 vs 6, and 4 vs 5 . Another public dataset used in experiments was Multi-Task Facial Landmark (MTFL) dataset [31]. In this dataset, there are\n12, 995 face images with different genders, head poses, and characteristics. The features are five facial landmarks, attribute of gender, smiling/not smiling, wearing glasses/not wearing glasses, and head pose. We designed four binary classification tasks such as male/female, smiling/not smiling, wearing/not wearing glasses, and right/left head pose. The logistic loss was used for binary classification tasks, and the squared loss was used for the regression task. Training times of AMTL and SMTL are given in Table III. When the number of tasks is high, the gap between training times of AMTL and SMTL is wider. The training time of AMTL is always less than the training time of SMTL for all of the real-world datasets for a fixed amount of delay in this experiment.\nExperiments show that AMTL is more time efficient than SMTL, especially, when there are delays due to the network communication. In this situation, asynchronous algorithm becomes a necessity, because network communication increases the training time drastically. Since each task node in AMTL performs the backward-forward splitting steps and the variable updates without waiting for any other node in the network, it outperforms SMTL for both synthetic and real-world datasets with a various number of tasks and network settings. Moreover, AMTL does not need to carry the raw data samples to a central node. Therefore, it is very suitable for private datasets located at different data centers compared to many\ndistributed frameworks in the literature. In addition to time efficiency, convergence of AMTL and SMTL under same network configurations are compared. In Fig. 4, convergence curves of AMTL and SMTL are given for a fixed number of iterations and synthetic data with 5 and 10 tasks. As seen in the figure, AMTL tends to converge faster than SMTL in terms of the number of iteratios as well."}, {"heading": "C. Dynamic step size", "text": "In this section, we present the experimental result of the proposed dynamic step size. A dynamic step size is proposed by utilizing the delays in the network. We simulate the delays caused by the network communication by keeping the task nodes idle for while after it completes the forward step. The dynamic step size was computed by Eq. (III.6). In this experiment, the average delay of the last 5 iterations was used to modify the step size. The effect of the dynamic step size was shown for randomly generated synthetic datasets with 100 samples in each task with the dimensionality was set to 50. The objective values of each dataset with different numbers of tasks and with different offset values were examined. These\nobjective values were calculated at the end of the total number of iterations and the final updated versions of the model vectors were used. Because of the delays, some of the task nodes have to wait much longer than other nodes, and the convergence slows down for these nodes. If we increase the step size of the nodes which had to wait for a long time in previous iterations, we can boost the convergence. The experimental results are summarized in Tables IV, V, and VI. According to our observation, if we use dynamic step size, the objective value decreases compared with the objective value of the AMTL with constant step size. The convergence criteria was chosen as a fixed number of iterations such as 10. As we can see from the tables, the dynamic step size helps to speed up the convergence. Another observation is that objective values tend to decrease with the increasing amount of delay, when the dynamic step size is used. Although no theoretical results are available to quantify the dynamic step size in existing literature, the empirical results we obtained indicate that the dynamic step size is a very promising strategy and is especially effective when there are significant delays in the network."}, {"heading": "V. CONCLUSION", "text": "In conclusion, a distributed regularized multi-task learning approach is presented in this paper. An asynchronous distributed coordinate update method is adopted to perform full updates on model vectors. Compared to other distributed MTL approaches, AMTL is more time efficient because task nodes do not need to wait for other nodes to perform the gradient updates. A dynamic step size to boost the convergence performance is investigated by scaling the step size according to the delays in the communication. Training times are compared for several synthetic, and public datasets and the results showed that the proposed AMTL is faster than traditional synchronous MTL. We also study the convergence behavior of AMTL and SMTL by comparing the precision of the two approaches. We note that current AMTL implementation is based on the ARock [6] framework, which largely limits our capability of conducting experiments for different network structures. As our future work, we will develop a standalone AMTL implementation1 that allows us to validate AMTL in realworld network settings. Stochastic gradient approach will also be incorporated into the current distributed AMTL setting."}, {"heading": "ACKNOWLEDGMENT", "text": "This research is supported in part by the National Science Foundation (NSF) under grant numbers IIS-1565596, IIS1615597, and DMS-1621798 and the Office of Naval Research (ONR) under grant number N00014-14-1-0631."}], "references": [{"title": "Malsar: Multi-task learning via structural regularization,", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers,", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Asynchronous distributed optimization using a randomized alternating direction method of multipliers,", "author": ["F. Iutzeler", "P. Bianchi", "P. Ciblat", "W. Hachem"], "venue": "IEEE Conference on Decision and Control,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "An asynchronous distributed proximal gradient method for composite convex optimization,", "author": ["N. Aybat", "Z. Wang", "G. Iyengar"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties,", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM Journal of Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "ARock: An algorithmic framework for asynchronous parallel coordinate updates,", "author": ["Z. Peng", "Y. Xu", "M. Yan", "W. Yin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Tmac: A toolbox of modern asyncparallel, coordinate, splitting, and stochastic methods,", "author": ["B. Edmunds", "Z. Peng", "W. Yin"], "venue": "UCLA CAM Report", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Coordinate-friendly structures, algorithms and applications,", "author": ["Z. Peng", "T. Wu", "Y. Xu", "M. Yan", "W. Yin"], "venue": "Annals of Mathematical Sciences and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Clientserver multitask learning from distributed datasets,", "author": ["F. Dinuzzo", "G. Pillonetto", "G. De Nicolao"], "venue": "IEEE Transactions on Neural Networks, vol. 22,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Collaborating between local and global learning for distributed online multiple tasks,", "author": ["X. Jin", "P. Luo", "F. Zhuang", "J. He", "H. Qing"], "venue": "Proceedings of the 24th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Heterogeneous multitask learning with joint sparsity constraints,", "author": ["X. Yang", "S. Kim", "E.P. Xing"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Multitask learning,", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Regularized multi-task learning,", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Clustered multi-task learning via alternating structure optimization,", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "in NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Multi-task feature learning via efficient l2,1-norm minimization,", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Proceedings of the 21th Conference on Uncertainty in Artifical Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Convex multi-task feature learning,", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A rank minimization heuristic with application to minimum order system approximation,", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "Proceedings of the American Control Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "An accelerated gradient method for trace norm minimization,", "author": ["S. Ji", "J. Ye"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems,", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Sparse reconstruction by separable approximation,", "author": ["S.J. Wright", "R.D. Nowak", "M.A. Figueiredo"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Proximal newton-type methods for minimizing composite functions,", "author": ["J.D. Lee", "Y. Sun", "M.A. Saunders"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Signal recovery by proximal forwardbackward splitting,", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Regularization and variable selection via the elastic net,", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Amortized analysis on asynchronous gradient descent,", "author": ["Y.K. Cheung", "R. Cole"], "venue": "arXiv preprint arXiv:1412.0159,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Fast online SVD revisions for lightweight recommender systems.", "author": ["M. Brand"], "venue": "Proceedings of SIAM International Conference on Data Mining,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Rasbash, \u201cDifferential school effectiveness,", "author": ["D.L. Nuttall", "H. Goldstein", "R. Prosser"], "venue": "International Journal of Educational Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Facial landmark detection by deep multi-task learning,", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The framework is capable of solving most existing regularized MTL formulations and is compatible with the formulations in the MTL package MALSAR [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "One commonly used distributed optimization approach is alternating direction method of multipliers (ADMM), which was firstly proposed in the 1970s [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "In the distributed ADMM framework in [2], local copies are introduced for local subproblems, and the communication between the work nodes and the center node is for the purpose of consensus.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Furthermore, the approaches in [2] are all synchronized.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "proposed an asynchronous distributed approach using randomized ADMM [3] based on randomized Gauss-Seidel iterations of a Douglas-Rachford splitting (DRS) because ADMM is equivalent to DRS.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "introduced an asynchronous distributed proximal gradient method by using the randomized block coordinate descent method in [4] for minimizing the sum of a smooth and a non-smooth functions.", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "proposed an asynchronous stochastic proximal coordinate descent method in [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "proposed a general asynchronous parallel framework for coordinate updates based on solving fixed-point problems with non-expansive operators [6], [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "proposed a general asynchronous parallel framework for coordinate updates based on solving fixed-point problems with non-expansive operators [6], [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "monotone inclusion problems [8].", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "For instance, Dinuzzo and Pillonetto in [9] proposed a client-server MTL from distributed datasets in 2011.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": ", on the other hand, proposed collaborating between local and global learning for distributed online multiple tasks in [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "heterogeneous [12], i.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "In a single learning task, we treat the task independently and minimize the corresponding loss function, while in MTL, the tasks are related, and we hope that by properly assuming task relatedness, the learning of one task (the inference process of wt) can benefit from other tasks [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 12, "context": "Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "One representative task relatedness is joint feature learning, which is achieved via the grouped sparsity induced by penalizing the `2,1-norm [16] of the model matrix W : g(W ) = \u2016W\u20162,1 = \u2211d i=1 \u2016w\u20162 where w is the ith row of W .", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "Another commonly used MTL method is the shared subspace learning [17], which is achieved by penalizing the nuclear", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The nuclear norm is the tightest convex relaxation of the rank function [18], and the problem can be solved via proximal gradient methods [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "The nuclear norm is the tightest convex relaxation of the rank function [18], and the problem can be solved via proximal gradient methods [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22].", "startOffset": 180, "endOffset": 184}, {"referenceID": 19, "context": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 20, "context": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22].", "startOffset": 262, "endOffset": 266}, {"referenceID": 5, "context": "coordinate update problems by using Krasnosel\u2019skii-Mann (KM) iteration [6], [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "coordinate update problems by using Krasnosel\u2019skii-Mann (KM) iteration [6], [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "The proposed AMTL uses a backward-forward operator splitting method [23], [8] to solve problem (III.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "The proposed AMTL uses a backward-forward operator splitting method [23], [8] to solve problem (III.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "We can thus follow [6] to perform task block coordinate update at the backward-forward iteration,", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "4 is the update rule of KM iteration discussed in [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "4 of [6] to see how Eq.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "When applying the coordinate update scheme in [6], all task nodes have access to the shared memory, and they do not communicate with each other.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "The results in [6] can be applied to directly obtain the convergence.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "We note that some MTL algorithms based on sparsity-inducing norms may not have a unique solution, as commonly seen in many sparse learning formulations, but in this case we typically can add an `2 term to ensure the strict convexity and obtain linear convergence as shown in [6].", "startOffset": 275, "endOffset": 278}, {"referenceID": 22, "context": "An example of such technique is the elastic net variant from the original Lasso problem [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "The dynamic step was recently used in a specific setting via asynchronous optimization to achieve better overall performance [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "In our experiments, we simulate the distributed environment using the shared memory architecture in [6] with network delays introduced to the work nodes.", "startOffset": 100, "endOffset": 103}, {"referenceID": 17, "context": "The proximal mapping for nuclear norm regularization is given by [19], [27]:", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "The proximal mapping for nuclear norm regularization is given by [19], [27]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "Instead of computing the full SVD at every step, online SVD can also be used [28].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "School dataset has exam records of 139 schools in 1985, 1986, and 1987 provided by the London Education Authority (ILEA) [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "Another public dataset used in experiments was Multi-Task Facial Landmark (MTFL) dataset [31].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "We note that current AMTL implementation is based on the ARock [6] framework, which largely limits our capability of conducting experiments for different network structures.", "startOffset": 63, "endOffset": 66}], "year": 2016, "abstractText": "Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and realworld datasets demonstrate the efficiency and effectiveness of the proposed framework.", "creator": "LaTeX with hyperref package"}}}