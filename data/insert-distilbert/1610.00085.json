{"id": "1610.00085", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2016", "title": "Latent Tree Analysis", "abstract": "latent tree analysis seeks to properly model the correlations among a set ordering of random variables using a tree of latent variables. it was proposed as an improvement to latent class analysis - - - a method widely used in social sciences and medicine to identify homogeneous subgroups together in a population. it also provides new and fruitful perspectives on a number sizes of machine learning areas, spanning including cluster analysis, topic detection, simulation and deep probabilistic modeling. this paper gives an overview of the research on latent tree network analysis and acknowledges various ways it is used in agricultural practice.", "histories": [["v1", "Sat, 1 Oct 2016 04:26:48 GMT  (1383kb,D)", "http://arxiv.org/abs/1610.00085v1", "7 pages, 5 figures"]], "COMMENTS": "7 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nevin l zhang", "leonard k m poon"], "accepted": true, "id": "1610.00085"}, "pdf": {"name": "1610.00085.pdf", "metadata": {"source": "CRF", "title": "Latent Tree Analysis", "authors": ["Nevin L. Zhang", "Leonard K. M. Poon"], "emails": ["lzhang@cse.ust.hk", "kmpoon@eduhk.hk"], "sections": [{"heading": null, "text": "Much of machine learning is about modeling and utilizing correlations among variables. In classification, the task is to establish relationships between attributes and class variables so that unseen data can be classified accurately. In Bayesian networks, dependencies among variables are represented as directed acyclic graphs and the graphs are used to facilitate efficient probabilistic inference. In topic models, word cooccurrences are accounted for by assuming that all words are generated probabilistically from the same set of topics, and the generation process is reverted via statistical inference to determine the topics. In deep belief networks, correlations among observed units are modeled using multiple levels of hidden units, and the top-level hidden units are used as a representation of the data for further analysis.\nLatent tree analysis (LTA) seeks to model the correlations among a set of observed variables using a tree model, called latent tree model (LTM), where the leaf nodes represent observed variables and the internal nodes represent latent variables.The dependence between two observed variables is explained by the path between them.\nDespite their simplicity, LTMs subsume two classes of models widely used in academic research. The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable. They are used for categorical data clustering in social sciences and medicine. The second class is probabilistic phylogenetic trees (Durbin et al., 1998), which are a tool for determining the evolution history of a set of species. Phylogenetic trees are special LTMs where the model structures are binary (bifurcating) trees and all the\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nvariables have the same number of possible states. LTA also provides new and fruitful perspectives on a number of machine learning areas. One area is cluster analysis. Here finite mixture models such as LCMs are commonly used. A finite mixture model has one latent variable and consequently it gives one soft partition of data. An LTM typically has multiple latent variables and hence LTA yields multiple soft partitions of data simultaneously. In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013). It is interesting because complex data usually have multiple facets and can be meaningfully clustered in multiple ways.\nAnother area is topic detection. Applying LTA to text data, we can partition a collection of documents in multiple ways. The document clusters in the partitions can be interpreted as topics. Furthermore, it is possible to learn hierarchical LTMs where the latent variables are organized into multiple layers. This leads to an alternative method for hierarchical topic detection (Liu, Zhang, and Chen, 2014; Chen et al., 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al., 2015).\nThe third area is deep probabilistic modeling. Hierarchical LTM and deep belief network (DBN) (Hinton, Osindero, and Teh, 2006) are similar in that they both consist of multiple layers of variables, with an observed layer at the bottom and multiple layers of hidden units on top of it. One difference is that, in DBN, units from adjacent layers are fully connected, while HLTM is tree-structured. It would be interesting to explore the middle ground between the two extreme and develop algorithms for learning what might be called sparse DBNs. Learning structures for deep models is an interesting open problem. Extension of LTA might offer one solution (Chen et al., 2016b).\nThe concept of latent tree models was introduced in (Zhang, 2002, 2004), where they were referred to as hierarchical latent class models. The term \u201clatent tree models\u201d first appeared in (Zhang et al., 2008; Wang, Zhang, and Chen, 2008). Mourad et al. (2013) surveyed the research on latent tree models as of 2012 in details. This paper provides a concise overview of the methodology. The exposition are more conceptual and less technical than (Mourad et al., 2013). Developments after 2012 are also included.\nar X\niv :1\n61 0.\n00 08\n5v 1\n[ cs\n.L G\n] 1\nO ct\n2 01\n6"}, {"heading": "Preliminaries", "text": "A latent tree model (LTM) is a tree-structured Bayesian network (Pearl, 1988), where the leaf nodes represent observed variables and the internal nodes represent latent variables. An example is shown in Figure 1 (a). All variables are assumed to be discrete. The model parameters include a marginal distribution for the root Y1 and a conditional distribution for each of the other nodes given its parent. The product of the distributions defines a joint distribution over all the variables.\nBy changing the root from Y1 to Y2 in Figure 1 (a), we get another model shown in (b). The two models are equivalent in the sense that they represent the same set of distributions over the observed variables X1, . . . , X5 (Zhang, 2004). It is not possible to distinguish between equivalent models based on data. This implies that edge orientations in LTMs are unidentifiable. It therefore makes more sense to talk about undirected LTMs, which is what we do in this paper. One example is shown in Figure 1 (c). It represents an equivalent class of directed models, which includes the two models shown in (a) and (b) as members. In implementation, an undirected model is represented using an arbitrary directed model in the equivalence class it represents.\nIn the literature, there are variations of LTMs where some internal nodes are observed (Choi et al., 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014). In this paper, we focus on basic LTMs as defined in the previous two paragraphs.\nWe use |W | to denote the number of possible states of a variable W . An LTM is regular if, for any latent node Z, we have that |Z| \u2264 \u220fk i=1 |Zi|\nmaxk i=1\n|Zi| , where Z1, . . . , Zk are the\nneighbors of Z, and that the inequality holds strictly when k = 2. For any irregular LTM, there is a regular model that has fewer parameters and represents that same set of distributions over the observed variables (Zhang, 2004). Consequently, we focus only on regular models."}, {"heading": "Learning Latent Tree Models", "text": "To fit an LTM to a dataset, one needs to determine: (1) the number of latent variables, (2) the number of possible states for each latent variable, (3) the connections among all the variables, and (4) the probability distributions.\nThere are three commonly used algorithms for learning LTMs: EAST (Expansion, Adjustment and Simplification until Termination) (Chen et al., 2012), BI (Bridged Islands) (Liu et al., 2013), and CLRG (Chow-Liu and Recursive Grouping) (Choi et al., 2011). EAST is a search-based algorithm and it aims to find the model with the highest BIC\nscore. It is the slowest among the three and finds better models, as measured by held-out likelihood, than the other two algorithms (Liu et al., 2013). It is often used to analyze survey data from medicine and social sciences, which typically contain dozens of observed variables.\nBI first divides the observed variables into unidimensional subsets. A set of variables is unidimensional if the correlations among them can be properly modeled using a single latent variable, which is determined using a test that compares the best one-latent-variable model and the best two-latentvariable. BI then introduces a latent variable for each unidimensional subset to form a LCM. The LCMs are metaphorically called islands. The latent variables in the islands are linked up using Chow-Liu\u2019s algorithm (Chow and Liu, 1968) to form a global model.\nCLRG first constructs a tree over the observed variables using Chow-Liu\u2019s algorithm. It then recursively transforms patches of the model by adding latent variables and/or rearranging the edges. Here a patch consists of an internal node and its neighbors. Information distances, as defined in (Erdos et al., 1999), between pairs of variables in the patch are estimated from data. They are used to transform the patch into a latent tree based on a theorem which states that, if the variables are indeed from a tree model, information distances between them are additive w.r.t. the tree.\nEmpirical results reported in (Liu et al., 2013) show that BI consistently yields better models than CLRG in terms of held-out likelihood. BI scales up well if progressive EM is used to estimate the parameters of the intermediate models, and was able to handle a text dataset with 10,000 distinct words (variables) and 300,000 documents in around 11 hours on a single machine (Chen et al., 2016a). CLRGC also scales up well if parameter learning is based on the method of moments and tensor decomposition, and was able to process a medical dataset with around 1,000 diagnosis categories (variables) and 1.6 million patient records in around 4.5 hours on a single machine (Huang et al., 2015)."}, {"heading": "Improving Latent Class Analysis", "text": "As mentioned before, a latent class model (LCM) is an LTM with a single latent variable, and LCA refers to the process of fitting an LCM to a dataset. The latent variable gives a soft partition of data and its states represent clusters in the partition. LCA is hence a technique for cluster analysis and it is widely used in social, behavioral and health sciences (Collins and Lanza, 2010). In medical research, it is used to identify subtypes of diseases, for instance major depression, where good standards are not available (van Smeden et al., 2013; Li et al., 2014).\nA major issue with LCA is the assumption that all the observed variables are mutually independent given the latent variable. In other words, the observed variables are assumed to be independent in each cluster of data. The assumption is hence known as the local independence assumption. It is easily violated in practice and casts doubts on the validity of the clustering results (van Smeden et al., 2013).\nLTMs provide a natural framework where the local independence assumption can be relaxed. We illustrate the point first using an example from (Fu et al., 2016), where the\ntask is to divide a collection of patients with vascular mild cognitive impairment (VMCI) into subclasses based on 27 symptoms that are related to the concept of Qi Deficiency in traditional Chinese medicine. Figure 2 (a) shows the LCM learned for the task and (b) shows the LTM. In the LTM, intermediate latent variables are introduced between the observed variables at the bottom and the clustering variable Z at the top. We refer to such models as LTM-based unidimensional clustering models. The local independence assumption is relaxed because, given the clustering variable Z, the observed variables are no longer mutually independent.\nThe relaxation of the local independence leads to better model fit. As a matter of fact, the BIC score of the LTM is -10,022, which is higher than that of the LCM, which is -10,164. It also leads better clustering results as will be explained later in this section. In addition, the LTM is also intuitively more reasonable than the LCM. For example, the symptoms \u201cdry stool or constipation\u201d and \u201casthenia of defecation\u201d are both about difficulties with defecation. It is hence reasonable to connect them to Z via an intermediate variable (Y01), which can be interpreted as the impact of Qi Deficiency on defecation.\nLTM-based clustering models can be obtained by first fitting an LTM to data using EAST or BI. This divides the observed variables into groups, called sibling clusters, each consisting of the variables directly connected to a latent variable. All sibling clusters are unidimensional and the correlations among the members are properly modeled by corresponding latent variables. The latent variables, with one possible exception1, are then used as features for clustering\n1This is determined by search (Liu, Poon, and Zhang, 2015).\ninstead of the individual observed variables, resulting in a model similar to the one shown in Figure 2 (b).\nA standard way to evaluate a clustering algorithm is to start with a labeled dataset, remove the class labels, run the algorithm to partition the resulting unlabeled dataset, and measure the performance using mutual information between the partition obtained with the partition induced by the class labels. Following this practice, Liu, Poon, and Zhang (2015) have compared LTM-based cluster analysis with LCA on 30 datasets from UCI, and found that the LTM-based method outperforms LCA in most cases and often significantly."}, {"heading": "Multidimensional Clustering", "text": "Complex data usually have multiple facets and can be meaningfully partitioned in multiple ways. For example, a student population can be clustered in one way based on academic performances and another based on extracurricular activities. Movie reviews can be clustered based on sentiment (positive or negative) or genre (comedy, action, war, etc.). The respondents in a social survey can be clustered based on demographic information or views on social issues.\nThere are efforts on developing clustering algorithms that produce multiple partitions of data, with each partition being based, solely or primarily, on a different subset of attributes. We call them multidimensional clustering methods. (This is not to be confused with multi-view clustering, which combines information from different views of data to improve the quality of one single partition.) There are sequential methods that aim at obtaining additional partitions of data that are novel w.r.t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006). There are also methods that produce multiple partitions simulataneously (Jain, Meka, and Dhillon, 2008; Niu, Dy, and Jordan, 2010). Those methods try to optimize the quality of each individual partition while keeping different partitions as dissimilar as possible. All of the methods are limited in the number of different partitions they produce, which is typically 2.\nAn LTM typically has multiple latent variables, and each of them can be interpreted as representing one soft partition of data. As such, LTA is a natural tool for multidimensional clustering (Chen et al., 2012; Liu et al., 2013). LTA can determine the number of partitions automatically and it is not limited in the number of partitions.\nWe illustrate the use of LTA for multidimensional clustering using an example from (Chen et al., 2012), where a survey dataset from ICAC \u2014 Hong Kong\u2019s anti-corruption agency \u2014 was analyzed using the EAST algorithm. The structure of the resulting model is shown in Figure 3. There are 9 latent variables. It is clear that the latent variable Y2 represents a partition of the respondents primarily based on demographic information; Y3 represents a partition based on people\u2019s tolerance toward corruption; Y4 represents a partition based on people\u2019s view on ICAC\u2019s performance; Y6 represents a partition based on people\u2019s view on the level of\nIf a latent variable is not used as a feature, then all the observed variables in its sibling cluster are.\ncorruption; Y5 represents a partition based on people\u2019s view on the trend of corruption; and so on. It makes sense that there are direct dependencies between Y2 (demographic information) and Y3 (tolerance toward corruption); between Y4 (ICAC performance) and Y6 (corruption level); and between Y4 (ICAC performance) and Y5 (corruption trend).\nThe conditional distributions of two observed variables given Y3 are given in Figure 3. The states of the observed variables are s0 (totally intolerable), s1 (intolerable), s2 (tolerable), and s3 (totally tolerable). Based on the distributions, the three states of Y3 are interpreted as classes of people who find corruption totally intolerable (Y3 = s0), intolerable (Y3 = s1), and tolerable (Y3 = s2) respectively. The distributions also suggest that people who are tough on corruption (Y3 = s0) are equally tough toward corruption in the government and corruption in the business sector, while people who are lenient towards corruption are more lenient toward corruption in the business sector than corruption in the government.\nBased on the conditional distributions of the demographic variables given Y2, the four states of the latent variable are interpreted as: Y2 = s0 \u2013 low income youngsters; Y2 = s1 \u2013 women with no/low income; Y2 = s2 \u2013 people with good education and good income; Y2 = s3 \u2013 people with poor education and average income. The conditional distribution P (Y3|Y2) is also given in Figure 3. It suggests that people with good education and good income (Y2 = s2) are the toughest toward corruption, while people with poor education and average income ( Y2 = s3) are the most lenient. This is intuitively appealing and can be a hypothesis for social scientist to verify further.\nLiu et al. (2013) quantitatively compared LTA with the\nalternative methods for multidimensional clustering mentioned above on the WebKB dataset, which has two groundtruth partitions (class labels). The class labels were first removed and all algorithms were run to recover the groundtruth partitions. LTA significantly outperforms all the alternative methods."}, {"heading": "Hierarchical Topic Detection", "text": "Liu, Zhang, and Chen (2014) propose a method to analyze text data and obtain models such as the one shown in Figure 4 (a). There is a layer of observed variables at the bottom, and multiple layers of latent variables on top. Such models are called hierarchical latent tree models (HLTMs) and the process of learning HLTMs is called hierarchical latent tree analysis (HLTA).\nThe observed variables are binary variables that represent the absence/presence of words in documents. The level1 latent variables model patterns of probabilistic word cooccurrence, and latent variables at higher levels model cooccurrences of patterns at the level below. For example, Z14 captures the co-occurrence of the words \u201ccard\u201d, \u201cvideo\u201d and \u201cdriver\u201d; Z15 captures the co-occurrence \u201cwindows\u201d and \u201cdos\u201d; Z22 captures the co-occurrence of the patterns represented by Z14, Z15, Z16 and Z17; and so on.\nThe latent variables are also binary. Each of them partitions the documents into two clusters. Information about some of the partitions is given below.\ns0 s1 Z14 (0.88) (0.12) card 0.01 0.47 video 0.02 0.32 driver 0.03 0.20\ns0 s1 Z15 (0.85) (0.15) windows 0.01 0.67 dos 0.01 0.30\ns0 s1 Z22 (0.76) (0.24) windows 0.04 0.34 card 0.02 0.22 graphics 0.02 0.17 video 0.01 0.15 dos 0.02 0.16 computer 0.05 0.21 display 0.02 0.10 drive 0.02 0.10\nWe see that the two clusters given by Z14 consists of 88% and 12% of the documents respectively. In the second clus-\nter, the words \u201ccard\u201d, \u201cvideo\u201d and \u201cdriver\u201d occur with relatively high probabilities. It is interpreted as a topic, i.e., \u201cvideo-card-driver\u201d. The words occur with very low probabilities in the first cluster. It is regarded as a background topic. Similarly, Z15 gives us the topic \u201cwindows-dos\u201d and it consists of 15% of the documents. The topic given by Z22 involves many words related to computers and hence can be simply understood as a topic about computers.\nLatent variables at low levels of the hierarchy capture \u201cshort-range\u201d word co-occurrences and hence they give topics that are relatively more specific in meaning. Latent variables at high levels of the hierarchy capture \u201clong-range\u201d word co-occurrences and hence they give topics that are relatively more general in meaning. Hence the model gives us a hierarchy of topics, part of which is listed below. HLTA is therefore considered a tool for hierarchical topic detection.\nZ22: windows card graphics video dos Z14: card video driver Z15: windows dos Z16: graphics display image Z17: computer science\nTo build a hierarchical model, HLTA first learns a model similar to the one shown in Figure 4 (b). It is a flat LTM in the sense that every latent variable is directly connected to at least one observed variable. Next, HLTA converts the latent variables in the flat model (Z11, Z12, . . . , Z111) into observed variables via data completion, and learns a flat model for them (c). Then, the second flat model is stacked on top of the first one to get the hierarchical model. In general, the process is repeated multiple times to get multiple layers of latent variables.\nLiu, Zhang, and Chen (2014) use the BI algorithm to learn flat models, which does not scale up. Chen et al. (2016a) improves HLTA by using progressive EM (PEM) to estimate parameters of the intermediate models. The idea is to estimate the parameters in steps and, in each step, EM is run on a submodel that involves only 3 or 4 observed variables. PEM is efficient because a dataset, when projected onto 3 or 4 binary variables, consists of only 8 or 16 distinct cases no matter how large it is.\nTopic detection has been one of the most active research areas in machine learning. Nested hierarchical Dirichlet process (nHDP) (Paisley et al., 2015) is the state-of-theart method for hierarchical topic detection. Empirical results reported in (Liu, Zhang, and Chen, 2014; Chen et al., 2016a) show that HLTA significantly outperforms nHDP in terms of topic quality as measured by the topic coherence score proposed by Mimno et al. (2011), and HLTA finds more meaningful topic hierarchies. Figure 5 shows a part of the topic hierarchy that HLTA obtains from all papers published at AAAI/IJCAI from 2000 to 2015. It is clearly meaningful. Interested readers can also browse and compare the topic hierarchies obtained by HLTA and nHDP from 300,000 New York Times articles at http://home.cse.ust.hk/\u223clzhang/topic/ijcai2016/."}, {"heading": "Deep Probabilistic Modeling", "text": "Deep learning has achieved great successes in recent years. It has produced superior results in a range of applications,\nincluding image classification, speech recognition, language translation and so on. Now it might be the time to ask whether it is possible and beneficial to learn structures for deep models.\nTo learn the structure of a deep model, we need to determine the number of hidden layers and the number of hidden units at each layer. More importantly, we need to determine the connections between neighboring layers. This implies that we need to talk about sparse models where neighboring layers are not fully connected.\nSparseness is desirable and full connectivity is unnecessary. In fact, Han et al. (2015) have shown that many weak connections in the fully-connected layers of Convolutional Neural Networks (CNNs) (LeCun and Bengio, 1995) can be pruned without incurring any accuracy loss. The convolutional layers of CNNs are sparse, and the fact is considered one of the key factors that lead to the success of CNNs. Moreover, it is well known that overfitting is a serious problem in deep models. Overfitting is caused not only by excessive amount of hidden units, but also excessive amount of connections. One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training. Sparseness offers an interesting alternative. It amounts to deterministically drop out connections.\nHow can one learn sparse deep models? One method is to first learn a fully connected model and then prune weak connections (Han et al., 2015). A drawback of this method is that it is computationally wasteful. Moreover, it does not offer a way to determine the number of hidden units. We would like to develop a method that determines the number of hidden units and the connections between units automatically. The key intuition is that a hidden unit should be connected to a group of strongly correlated units at the level below. This idea is used in convolutional layers of CNNs, where a unit is connected to pixels in a small patch of an image. In image analysis, spatial proximity implies strong correlation.\nTo apply the intuition to applications other than image analysis, we need to identify groups of strongly correlated variables for which latent variables should be introduced. HLTA offers a plausible solution. As explained in the previous section, HLTA first learns a flat LTM. To do so, it partitions all the variables into groups such that the variables in each group are strongly correlated and the correlations can be properly modeled using a single latent variable (Liu, Zhang, and Chen, 2014; Chen et al., 2016a). It introduces a latent variable for each group and links up the latent variables to form a flat LTM. Then it converts the latent variables into observed variables via data completion and repeats the process to produce a hierarchy.\nThe output of HLTA is a deep tree model with a layer of observed variables at the bottom and multiple layers of latent variables on top (see Figure 4 (a)). To obtain a non-tree sparse deep model, we can use the tree model as a skeleton and introduce additional connections to model the residual correlations not captured by the tree.\nChen et al. (2016b) have developed and tested the idea in the context of RBMs, which have a single hidden layer and are building blocks of Deep Belief Networks (Hinton, Osindero, and Teh, 2006) and Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009). The target domain is unsupervised text analysis. They have worked out an algorithm for learning what are called Sparse Boltzmann Machines. The method can determine the number of hidden units and the connections among the units. The models obtained by the method are significantly better, in terms of held-out likelihood, than RBMs where the hidden and observed units are fully connected. This is true even when the number of hidden units in RBMs is optimized by held-out validation. Moreover, they have demonstrated that Sparse Boltzmann Machines are also more interpretable than RBMs."}, {"heading": "Other Applications", "text": "LTMs can be used as a tool for general probabilistic inference over discrete variables. Here one works with a joint distribution over a set of variables and makes inference to compute the posterior distribution of query variables given evidence variables. It is technically challenging because explicit representation of the joint distribution takes space exponential in the number of variables, and inference takes exponential time.\nBayesian networks (Pearl, 1988) alleviate the problem by representing the joint distribution in a factorized form. LTMs offer an alternative method. The idea is to build an LTM with the variables in question as observed variables, and make inference with the LTM. LTMs have two attractive properties. On one hand, they are computationally simple to work with because they are tree structured. On the other hand, they can represent complex relationship among the observed variables. Those two properties are exploited in (Wang, Zhang, and Chen, 2008; Kaltwang, Todorovic, and Pantic, 2015; Yu, Huang, and Dauwels, 2016) for efficient probabilistic inference in various domains.\nLTMs also have a role to play in spectral clustering (Poon et al., 2012). In spectral clustering (Von Luxburg, 2007), one defines a similarity matrix for a collection of data points,\ntransforms the matrix to get a Laplacian matrix, finds the eigenvectors of the Laplacian matrix, and obtains a partition of the data using the leading eigenvectors. The last step is sometimes referred to as rounding.\nRounding amounts to clustering the data points using the eigenvectors as features.What is unique about the problem is that one needs to determine how many leading eigenvectors to use. To solve the problem using LTMs, Poon et al. (2012) binarize the eigenvectors and build a collection of LTMs. Each LTM is built in two steps. In the first step, an LCM is constructed using the first k leading vectors and a partition of data is obtained by LCA. In the second step, subsequent vectors and latent variables are added to the model. The construction of the model is motivated by some theoretical results about the ideal case where between cluster similarity is 0. According to the results, the LTM should fit the data the best when the choice of k is optimal. The problem of choosing among different k is hence turned into the problem of choose among different LTMs.\nUnlike alternative methods, this LTM-based method does not require the number of clusters equal the number of leading eigenvectors included, and determines both the number of eigenvectors and the number of clusters automatically. Empirical results show that it outperforms the alternative methods. It works correctly in the ideal case and degrades gracefully as one moves away from the ideal case, which is a desirable behavior for spectral clustering methods."}, {"heading": "Conclusions", "text": "Latent tree analysis is a novel tool for correlation modeling. It have been shown to be useful in unidimensional clustering, multidimensional clustering, hierarchical topic detection, deep probabilistic modeling, probabilistic inference and spectral clustering. It is potentially useful also in other areas because modeling correlations among variables is a fundamental task in data analysis.\nAcknowledgments Research on this article was supported by Hong Kong Research Grants Council under grants 16202515 and 16212516, and the Education University of Hong Kong under project RG90/2014-2015R. We thank all our collaborators, particularly Tao Chen, Yi Wang, Tengfei Liu, Raphael Mourad, April H. Liu, Chen Fu, Peixian Chen, Zhourong Chen."}], "references": [{"title": "Coala: A novel approach for the extraction of an alternate clustering of high quality and high dissimilarity", "author": ["E. Bae", "J. Bailey"], "venue": "ICDM, 53\u201362.", "citeRegEx": "Bae and Bailey,? 2006", "shortCiteRegEx": "Bae and Bailey", "year": 2006}, {"title": "Model-based multidimensional clustering of categorical data", "author": ["T. Chen", "N.L. Zhang", "T. Liu", "K.M. Poon", "Y. Wang"], "venue": "Artificial Intelligence 176:2246\u20132269.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Progressive em for latent tree models and hierarchical topic detection", "author": ["P. Chen", "N. Zhang", "L. Poon", "Z. Chen"], "venue": "AAAI.", "citeRegEx": "Chen et al\\.,? 2016a", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Sparse boltzmann machines with structure learning as applied to text analysis", "author": ["Z. Chen", "N.L. Zhang", "D.-Y. Yeung", "P. Chen"], "venue": "arXiv preprint arXiv:1609.05294", "citeRegEx": "Chen et al\\.,? 2016b", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning latent tree graphical models", "author": ["M.J. Choi", "V.Y.F. Tan", "A. Anandkumar", "A.S. Willsky"], "venue": "Journal of Machine Learning Research 11:1771\u20131812.", "citeRegEx": "Choi et al\\.,? 2011", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C. Chow", "C. Liu"], "venue": "IEEE transactions on Information Theory 14(3):462\u2013467.", "citeRegEx": "Chow and Liu,? 1968", "shortCiteRegEx": "Chow and Liu", "year": 1968}, {"title": "Latent class and latent transition analysis", "author": ["L.M. Collins", "S.T. Lanza"], "venue": "Wiley.", "citeRegEx": "Collins and Lanza,? 2010", "shortCiteRegEx": "Collins and Lanza", "year": 2010}, {"title": "Non-redundant multi-view clustering via orthogonalization", "author": ["Y. Cui", "X.Z. Fern", "J.G. Dy"], "venue": "ICDM, 133\u2013142.", "citeRegEx": "Cui et al\\.,? 2007", "shortCiteRegEx": "Cui et al\\.", "year": 2007}, {"title": "Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids", "author": ["R. Durbin", "S.R. Eddy", "A. Krogh", "G. Mitchison"], "venue": "Cambridge University Press.", "citeRegEx": "Durbin et al\\.,? 1998", "shortCiteRegEx": "Durbin et al\\.", "year": 1998}, {"title": "A few logs suffice to build (almost) all trees (i)", "author": ["P.L. Erdos", "M.A. Steel", "L.A. Sz\u00e9kely", "T.J. Warnow"], "venue": "Random Structures and Algorithms 14(2):153\u2013184.", "citeRegEx": "Erdos et al\\.,? 1999", "shortCiteRegEx": "Erdos et al\\.", "year": 1999}, {"title": "Identification and classification of TCM syndrome types among patients with vascular mild cognitive impairment using latent tree analysis", "author": ["C. Fu", "N.L. Zhang", "B.X. Chen", "Z.R. Chen", "X.L. Jin", "R.J. Guo", "Z.G. Chen", "Y.L. Zhang"], "venue": "arXiv preprint arXiv:1601.06923.", "citeRegEx": "Fu et al\\.,? 2016", "shortCiteRegEx": "Fu et al\\.", "year": 2016}, {"title": "Non-redundant data clustering", "author": ["D. Gondek", "T. Hofmann"], "venue": "Knowledge and Information Systems 12(1):1\u201324.", "citeRegEx": "Gondek and Hofmann,? 2007", "shortCiteRegEx": "Gondek and Hofmann", "year": 2007}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "NIPS, volume 1135\u20131143, 3.", "citeRegEx": "Han et al\\.,? 2015", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation 18(7):1527\u2013 1554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Scalable latent tree model and its application to health analytics. NIPS 2015 Machine Learning for Healthcare workshop", "author": ["F. Huang", "I. Perros", "R. Chen", "J. Sun", "A Anandkumar"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Simultaneous unsupervised learning of disparate clusterings", "author": ["P. Jain", "R. Meka", "I.S. Dhillon"], "venue": "Statistical Analysis and Data Mining 1(3):195\u2013210.", "citeRegEx": "Jain et al\\.,? 2008", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "Latent trees for estimating intensity of facial action units", "author": ["S. Kaltwang", "S. Todorovic", "M. Pantic"], "venue": "CVPR, 296\u2013304.", "citeRegEx": "Kaltwang et al\\.,? 2015", "shortCiteRegEx": "Kaltwang et al\\.", "year": 2015}, {"title": "Latent tree copulas", "author": ["S. Kirshner"], "venue": "Sixth European Workshop on Probabilistic Graphical Models, Granada.", "citeRegEx": "Kirshner,? 2012", "shortCiteRegEx": "Kirshner", "year": 2012}, {"title": "Latent variable models and factor analysis", "author": ["M. Knott", "D.J. Bartholomew"], "venue": "Number 7. Edward Arnold.", "citeRegEx": "Knott and Bartholomew,? 1999", "shortCiteRegEx": "Knott and Bartholomew", "year": 1999}, {"title": "Latent Structure Analysis", "author": ["P.F. Lazarsfeld", "N.W. Henry"], "venue": "Boston: Houghton Mifflin.", "citeRegEx": "Lazarsfeld and Henry,? 1968", "shortCiteRegEx": "Lazarsfeld and Henry", "year": 1968}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LeCun", "Y. Bengio"], "venue": "The handbook of brain theory and neural networks 3361(10):1995.", "citeRegEx": "LeCun and Bengio,? 1995", "shortCiteRegEx": "LeCun and Bengio", "year": 1995}, {"title": "Subtypes of major depression: latent class analysis in depressed han chinese women. Psychological medicine", "author": ["Y. Li", "S. Aggen", "S. Shi", "J. Gao", "M. Tao", "K. Zhang", "X. Wang", "C. Gao", "L. Yang", "Y Liu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Greedy learning of latent tree models for multidimensional clustering", "author": ["T. Liu", "N.L. Zhang", "P. Chen", "A.H. Liu", "K.M. Poon", "Y. Wang"], "venue": "Machine Learning 98(1-2):301\u2013330.", "citeRegEx": "Liu et al\\.,? 2013", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Unidimensional clustering of discrete data using latent tree models", "author": ["A.H. Liu", "L.K. Poon", "N.L. Zhang"], "venue": "AAAI, 2771\u20132777.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Hierarchical latent tree analysis for topic detection", "author": ["T. Liu", "N.L. Zhang", "P. Chen"], "venue": "ECML/PKDD, volume 8725 of Lecture Notes in Computer Science. Springer. 256\u2013272.", "citeRegEx": "Liu et al\\.,? 2014", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Optimizing semantic coherence in topic models", "author": ["D. Mimno", "H.M. Wallach", "E. Talley", "M. Leenders", "A. McCallum"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 262\u2013272. Association for Computational Linguistics.", "citeRegEx": "Mimno et al\\.,? 2011", "shortCiteRegEx": "Mimno et al\\.", "year": 2011}, {"title": "A survey on latent tree models and applications", "author": ["R. Mourad", "C. Sinoquet", "N.L. Zhang", "T. Liu", "P Leray"], "venue": "J. Artif. Intell", "citeRegEx": "Mourad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mourad et al\\.", "year": 2013}, {"title": "Multiple non-redundant spectral clustering views", "author": ["D. Niu", "J.G. Dy", "M.I. Jordan"], "venue": "ICML-10, 831\u2013838.", "citeRegEx": "Niu et al\\.,? 2010", "shortCiteRegEx": "Niu et al\\.", "year": 2010}, {"title": "Nested hierarchical dirichlet processes", "author": ["J. Paisley", "C. Wang", "D.M. Blei", "M Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Paisley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Paisley et al\\.", "year": 2015}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Variable selection in model-based clustering: To do or to facilitate", "author": ["L. Poon", "N.L. Zhang", "T. Chen", "Y. Wang"], "venue": "ICML-10, 887\u2013894.", "citeRegEx": "Poon et al\\.,? 2010", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "A modelbased approach to rounding in spectral clustering", "author": ["L.K. Poon", "A.H. Liu", "T. Liu", "N.L. Zhang"], "venue": "UAI.", "citeRegEx": "Poon et al\\.,? 2012", "shortCiteRegEx": "Poon et al\\.", "year": 2012}, {"title": "A principled and flexible framework for finding alternative clusterings", "author": ["Z. Qi", "I. Davidson"], "venue": "KDD, 717\u2013726.", "citeRegEx": "Qi and Davidson,? 2009", "shortCiteRegEx": "Qi and Davidson", "year": 2009}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "AISTATS, volume 1, 3.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Nonparametric latent tree graphical models: Inference, estimation, and structure learning", "author": ["L. Song", "H. Liu", "A. Parikh", "E. Xing"], "venue": "arXiv preprint arXiv:1401.3940.", "citeRegEx": "Song et al\\.,? 2014", "shortCiteRegEx": "Song et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "J. Mach. Learn. Res. 15(1):1929\u2013 1958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Latent class models in diagnostic studies when there is no reference standard \u2013 a systematic review", "author": ["M. van Smeden", "C.A. Naaktgeboren", "J.B. Reitsma", "K.G. Moons", "J.A. de Groot"], "venue": "American journal of epidemiology 179(4):423\u201331", "citeRegEx": "Smeden et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Smeden et al\\.", "year": 2013}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and computing 17(4):395\u2013416.", "citeRegEx": "Luxburg,? 2007", "shortCiteRegEx": "Luxburg", "year": 2007}, {"title": "Latent tree models and approximate inference in Bayesian networks", "author": ["Y. Wang", "N.L. Zhang", "T. Chen"], "venue": "AAAI.", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Latent tree ensemble of pairwise copulas for spatial extremes analysis", "author": ["H. Yu", "J. Huang", "J. Dauwels"], "venue": "2016 IEEE International Symposium on Information Theory (ISIT), 1730\u2013 1734.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Latent tree models and diagnosis in traditional chinese medicine", "author": ["N.L. Zhang", "S. Yuan", "T. Chen", "Y. Wang"], "venue": "Artificial intelligence in medicine 42(3):229\u2013245.", "citeRegEx": "Zhang et al\\.,? 2008", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "AAAI.", "citeRegEx": "Zhang,? 2002", "shortCiteRegEx": "Zhang", "year": 2002}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research 5(6):697\u2013723.", "citeRegEx": "Zhang,? 2004", "shortCiteRegEx": "Zhang", "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable.", "startOffset": 44, "endOffset": 101}, {"referenceID": 18, "context": "The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable.", "startOffset": 44, "endOffset": 101}, {"referenceID": 8, "context": "The second class is probabilistic phylogenetic trees (Durbin et al., 1998), which are a tool for determining the evolution history of a set of species.", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": "In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 57, "endOffset": 94}, {"referenceID": 22, "context": "In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 57, "endOffset": 94}, {"referenceID": 2, "context": "This leads to an alternative method for hierarchical topic detection (Liu, Zhang, and Chen, 2014; Chen et al., 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al.", "startOffset": 69, "endOffset": 117}, {"referenceID": 28, "context": ", 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al., 2015).", "startOffset": 154, "endOffset": 176}, {"referenceID": 3, "context": "Extension of LTA might offer one solution (Chen et al., 2016b).", "startOffset": 42, "endOffset": 62}, {"referenceID": 40, "context": "The term \u201clatent tree models\u201d first appeared in (Zhang et al., 2008; Wang, Zhang, and Chen, 2008).", "startOffset": 48, "endOffset": 97}, {"referenceID": 26, "context": "The exposition are more conceptual and less technical than (Mourad et al., 2013).", "startOffset": 59, "endOffset": 80}, {"referenceID": 26, "context": "Mourad et al. (2013) surveyed the research on latent tree models as of 2012 in details.", "startOffset": 0, "endOffset": 21}, {"referenceID": 29, "context": "A latent tree model (LTM) is a tree-structured Bayesian network (Pearl, 1988), where the leaf nodes represent observed variables and the internal nodes represent latent variables.", "startOffset": 64, "endOffset": 77}, {"referenceID": 42, "context": ", X5 (Zhang, 2004).", "startOffset": 5, "endOffset": 18}, {"referenceID": 4, "context": "In the literature, there are variations of LTMs where some internal nodes are observed (Choi et al., 2011) and/or the variables are continuous (Poon et al.", "startOffset": 87, "endOffset": 106}, {"referenceID": 30, "context": ", 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014).", "startOffset": 44, "endOffset": 98}, {"referenceID": 17, "context": ", 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014).", "startOffset": 44, "endOffset": 98}, {"referenceID": 34, "context": ", 2011) and/or the variables are continuous (Poon et al., 2010; Kirshner, 2012; Song et al., 2014).", "startOffset": 44, "endOffset": 98}, {"referenceID": 42, "context": "For any irregular LTM, there is a regular model that has fewer parameters and represents that same set of distributions over the observed variables (Zhang, 2004).", "startOffset": 148, "endOffset": 161}, {"referenceID": 1, "context": "There are three commonly used algorithms for learning LTMs: EAST (Expansion, Adjustment and Simplification until Termination) (Chen et al., 2012), BI (Bridged Islands) (Liu et al.", "startOffset": 126, "endOffset": 145}, {"referenceID": 22, "context": ", 2012), BI (Bridged Islands) (Liu et al., 2013), and CLRG (Chow-Liu and Recursive Grouping) (Choi et al.", "startOffset": 30, "endOffset": 48}, {"referenceID": 4, "context": ", 2013), and CLRG (Chow-Liu and Recursive Grouping) (Choi et al., 2011).", "startOffset": 52, "endOffset": 71}, {"referenceID": 22, "context": "It is the slowest among the three and finds better models, as measured by held-out likelihood, than the other two algorithms (Liu et al., 2013).", "startOffset": 125, "endOffset": 143}, {"referenceID": 5, "context": "The latent variables in the islands are linked up using Chow-Liu\u2019s algorithm (Chow and Liu, 1968) to form a global model.", "startOffset": 77, "endOffset": 97}, {"referenceID": 9, "context": "Information distances, as defined in (Erdos et al., 1999), between pairs of variables in the patch are estimated from data.", "startOffset": 37, "endOffset": 57}, {"referenceID": 22, "context": "Empirical results reported in (Liu et al., 2013) show that BI consistently yields better models than CLRG in terms of held-out likelihood.", "startOffset": 30, "endOffset": 48}, {"referenceID": 2, "context": "BI scales up well if progressive EM is used to estimate the parameters of the intermediate models, and was able to handle a text dataset with 10,000 distinct words (variables) and 300,000 documents in around 11 hours on a single machine (Chen et al., 2016a).", "startOffset": 237, "endOffset": 257}, {"referenceID": 14, "context": "5 hours on a single machine (Huang et al., 2015).", "startOffset": 28, "endOffset": 48}, {"referenceID": 6, "context": "LCA is hence a technique for cluster analysis and it is widely used in social, behavioral and health sciences (Collins and Lanza, 2010).", "startOffset": 110, "endOffset": 135}, {"referenceID": 21, "context": "In medical research, it is used to identify subtypes of diseases, for instance major depression, where good standards are not available (van Smeden et al., 2013; Li et al., 2014).", "startOffset": 136, "endOffset": 178}, {"referenceID": 10, "context": "We illustrate the point first using an example from (Fu et al., 2016), where the", "startOffset": 52, "endOffset": 69}, {"referenceID": 41, "context": "Following this practice, Liu, Poon, and Zhang (2015) have compared LTM-based cluster analysis with LCA on 30 datasets from UCI, and found that the LTM-based method outperforms LCA in most cases and often significantly.", "startOffset": 40, "endOffset": 53}, {"referenceID": 11, "context": "t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006).", "startOffset": 75, "endOffset": 171}, {"referenceID": 32, "context": "t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006).", "startOffset": 75, "endOffset": 171}, {"referenceID": 0, "context": "t a previous partition, which can, for instance, be obtained using K-means (Cui, Fern, and Dy, 2007; Gondek and Hofmann, 2007; Qi and Davidson, 2009; Bae and Bailey, 2006).", "startOffset": 75, "endOffset": 171}, {"referenceID": 1, "context": "As such, LTA is a natural tool for multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 63, "endOffset": 100}, {"referenceID": 22, "context": "As such, LTA is a natural tool for multidimensional clustering (Chen et al., 2012; Liu et al., 2013).", "startOffset": 63, "endOffset": 100}, {"referenceID": 1, "context": "We illustrate the use of LTA for multidimensional clustering using an example from (Chen et al., 2012), where a survey dataset from ICAC \u2014 Hong Kong\u2019s anti-corruption agency \u2014 was analyzed using the EAST algorithm.", "startOffset": 83, "endOffset": 102}, {"referenceID": 41, "context": "Liu, Zhang, and Chen (2014) propose a method to analyze text data and obtain models such as the one shown in Figure 4 (a).", "startOffset": 5, "endOffset": 28}, {"referenceID": 28, "context": "Nested hierarchical Dirichlet process (nHDP) (Paisley et al., 2015) is the state-of-theart method for hierarchical topic detection.", "startOffset": 45, "endOffset": 67}, {"referenceID": 2, "context": "Empirical results reported in (Liu, Zhang, and Chen, 2014; Chen et al., 2016a) show that HLTA significantly outperforms nHDP in terms of topic quality as measured by the topic coherence score proposed by Mimno et al.", "startOffset": 30, "endOffset": 78}, {"referenceID": 36, "context": "Liu, Zhang, and Chen (2014) use the BI algorithm to learn flat models, which does not scale up.", "startOffset": 5, "endOffset": 28}, {"referenceID": 1, "context": "Chen et al. (2016a) improves HLTA by using progressive EM (PEM) to estimate parameters of the intermediate models.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Chen et al. (2016a) improves HLTA by using progressive EM (PEM) to estimate parameters of the intermediate models. The idea is to estimate the parameters in steps and, in each step, EM is run on a submodel that involves only 3 or 4 observed variables. PEM is efficient because a dataset, when projected onto 3 or 4 binary variables, consists of only 8 or 16 distinct cases no matter how large it is. Topic detection has been one of the most active research areas in machine learning. Nested hierarchical Dirichlet process (nHDP) (Paisley et al., 2015) is the state-of-theart method for hierarchical topic detection. Empirical results reported in (Liu, Zhang, and Chen, 2014; Chen et al., 2016a) show that HLTA significantly outperforms nHDP in terms of topic quality as measured by the topic coherence score proposed by Mimno et al. (2011), and HLTA finds more meaningful topic hierarchies.", "startOffset": 0, "endOffset": 840}, {"referenceID": 20, "context": "(2015) have shown that many weak connections in the fully-connected layers of Convolutional Neural Networks (CNNs) (LeCun and Bengio, 1995) can be pruned without incurring any accuracy loss.", "startOffset": 115, "endOffset": 139}, {"referenceID": 35, "context": "One method to address the problem is dropout (Srivastava et al., 2014), which randomly drops out units (while keeping full connectivity) during training.", "startOffset": 45, "endOffset": 70}, {"referenceID": 12, "context": "In fact, Han et al. (2015) have shown that many weak connections in the fully-connected layers of Convolutional Neural Networks (CNNs) (LeCun and Bengio, 1995) can be pruned without incurring any accuracy loss.", "startOffset": 9, "endOffset": 27}, {"referenceID": 12, "context": "How can one learn sparse deep models? One method is to first learn a fully connected model and then prune weak connections (Han et al., 2015).", "startOffset": 123, "endOffset": 141}, {"referenceID": 2, "context": "To do so, it partitions all the variables into groups such that the variables in each group are strongly correlated and the correlations can be properly modeled using a single latent variable (Liu, Zhang, and Chen, 2014; Chen et al., 2016a).", "startOffset": 192, "endOffset": 240}, {"referenceID": 33, "context": "(2016b) have developed and tested the idea in the context of RBMs, which have a single hidden layer and are building blocks of Deep Belief Networks (Hinton, Osindero, and Teh, 2006) and Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009).", "startOffset": 210, "endOffset": 242}, {"referenceID": 1, "context": "To do so, it partitions all the variables into groups such that the variables in each group are strongly correlated and the correlations can be properly modeled using a single latent variable (Liu, Zhang, and Chen, 2014; Chen et al., 2016a). It introduces a latent variable for each group and links up the latent variables to form a flat LTM. Then it converts the latent variables into observed variables via data completion and repeats the process to produce a hierarchy. The output of HLTA is a deep tree model with a layer of observed variables at the bottom and multiple layers of latent variables on top (see Figure 4 (a)). To obtain a non-tree sparse deep model, we can use the tree model as a skeleton and introduce additional connections to model the residual correlations not captured by the tree. Chen et al. (2016b) have developed and tested the idea in the context of RBMs, which have a single hidden layer and are building blocks of Deep Belief Networks (Hinton, Osindero, and Teh, 2006) and Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009).", "startOffset": 221, "endOffset": 827}, {"referenceID": 29, "context": "Bayesian networks (Pearl, 1988) alleviate the problem by representing the joint distribution in a factorized form.", "startOffset": 18, "endOffset": 31}, {"referenceID": 31, "context": "LTMs also have a role to play in spectral clustering (Poon et al., 2012).", "startOffset": 53, "endOffset": 72}, {"referenceID": 29, "context": "Bayesian networks (Pearl, 1988) alleviate the problem by representing the joint distribution in a factorized form. LTMs offer an alternative method. The idea is to build an LTM with the variables in question as observed variables, and make inference with the LTM. LTMs have two attractive properties. On one hand, they are computationally simple to work with because they are tree structured. On the other hand, they can represent complex relationship among the observed variables. Those two properties are exploited in (Wang, Zhang, and Chen, 2008; Kaltwang, Todorovic, and Pantic, 2015; Yu, Huang, and Dauwels, 2016) for efficient probabilistic inference in various domains. LTMs also have a role to play in spectral clustering (Poon et al., 2012). In spectral clustering (Von Luxburg, 2007), one defines a similarity matrix for a collection of data points, transforms the matrix to get a Laplacian matrix, finds the eigenvectors of the Laplacian matrix, and obtains a partition of the data using the leading eigenvectors. The last step is sometimes referred to as rounding. Rounding amounts to clustering the data points using the eigenvectors as features.What is unique about the problem is that one needs to determine how many leading eigenvectors to use. To solve the problem using LTMs, Poon et al. (2012) binarize the eigenvectors and build a collection of LTMs.", "startOffset": 19, "endOffset": 1313}], "year": 2016, "abstractText": "Latent tree analysis seeks to model the correlations among a set of random variables using a tree of latent variables. It was proposed as an improvement to latent class analysis \u2014 a method widely used in social sciences and medicine to identify homogeneous subgroups in a population. It provides new and fruitful perspectives on a number of machine learning areas, including cluster analysis, topic detection, and deep probabilistic modeling. This paper gives an overview of the research on latent tree analysis and various ways it is used in practice. Much of machine learning is about modeling and utilizing correlations among variables. In classification, the task is to establish relationships between attributes and class variables so that unseen data can be classified accurately. In Bayesian networks, dependencies among variables are represented as directed acyclic graphs and the graphs are used to facilitate efficient probabilistic inference. In topic models, word cooccurrences are accounted for by assuming that all words are generated probabilistically from the same set of topics, and the generation process is reverted via statistical inference to determine the topics. In deep belief networks, correlations among observed units are modeled using multiple levels of hidden units, and the top-level hidden units are used as a representation of the data for further analysis. Latent tree analysis (LTA) seeks to model the correlations among a set of observed variables using a tree model, called latent tree model (LTM), where the leaf nodes represent observed variables and the internal nodes represent latent variables.The dependence between two observed variables is explained by the path between them. Despite their simplicity, LTMs subsume two classes of models widely used in academic research. The first one is latent class models (LCMs) (Lazarsfeld and Henry, 1968; Knott and Bartholomew, 1999), which are LTMs with a single latent variable. They are used for categorical data clustering in social sciences and medicine. The second class is probabilistic phylogenetic trees (Durbin et al., 1998), which are a tool for determining the evolution history of a set of species. Phylogenetic trees are special LTMs where the model structures are binary (bifurcating) trees and all the Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. variables have the same number of possible states. LTA also provides new and fruitful perspectives on a number of machine learning areas. One area is cluster analysis. Here finite mixture models such as LCMs are commonly used. A finite mixture model has one latent variable and consequently it gives one soft partition of data. An LTM typically has multiple latent variables and hence LTA yields multiple soft partitions of data simultaneously. In other words, LTA performs multidimensional clustering (Chen et al., 2012; Liu et al., 2013). It is interesting because complex data usually have multiple facets and can be meaningfully clustered in multiple ways. Another area is topic detection. Applying LTA to text data, we can partition a collection of documents in multiple ways. The document clusters in the partitions can be interpreted as topics. Furthermore, it is possible to learn hierarchical LTMs where the latent variables are organized into multiple layers. This leads to an alternative method for hierarchical topic detection (Liu, Zhang, and Chen, 2014; Chen et al., 2016a), which has been shown to find more meaningful topics and topic hierarchies than the state-of-the-art method based on latent Dirichlet allocation (Paisley et al., 2015). The third area is deep probabilistic modeling. Hierarchical LTM and deep belief network (DBN) (Hinton, Osindero, and Teh, 2006) are similar in that they both consist of multiple layers of variables, with an observed layer at the bottom and multiple layers of hidden units on top of it. One difference is that, in DBN, units from adjacent layers are fully connected, while HLTM is tree-structured. It would be interesting to explore the middle ground between the two extreme and develop algorithms for learning what might be called sparse DBNs. Learning structures for deep models is an interesting open problem. Extension of LTA might offer one solution (Chen et al., 2016b). The concept of latent tree models was introduced in (Zhang, 2002, 2004), where they were referred to as hierarchical latent class models. The term \u201clatent tree models\u201d first appeared in (Zhang et al., 2008; Wang, Zhang, and Chen, 2008). Mourad et al. (2013) surveyed the research on latent tree models as of 2012 in details. This paper provides a concise overview of the methodology. The exposition are more conceptual and less technical than (Mourad et al., 2013). Developments after 2012 are also included. ar X iv :1 61 0. 00 08 5v 1 [ cs .L G ] 1 O ct 2 01 6", "creator": "LaTeX with hyperref package"}}}