{"id": "1610.00087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2016", "title": "Very Deep Convolutional Neural Networks for Raw Waveforms", "abstract": "learning acoustic models processed directly from receiving the raw waveform data with minimal processing is challenging. current waveform - region based models have generally used very few ( ~ 2 ) convolutional layers, which might be insufficient size for building high - level digital discriminative features. in this work, we carefully propose very deep convolutional neural networks ( acoustic cnns ) that directly must use time - domain waveforms specified as key inputs. our cnns, with up power to maximum 34 weight layers, are efficient to optimize over very long sequences ( typically e. g., vector of linear size 32000 ), necessary for processing acoustic waveforms. this is achieved through periodic batch normalization, residual learning, and a careful design of down - sampling parameters in the subsequent initial layers. our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. we use a large bandwidth receptive power field in the first convolutional layer to mimic bandpass filters, but see very small only receptive fields use subsequently to consistently control the model capacity. we demonstrate the performance gains with developing the deeper models. our evaluation shows that the cnn with 18 weight layers outperform the cnn with 3 weight layers by over 15 % in absolute accuracy for an environmental sound recognition measurement task and matches the performance of models using characteristic log - mel features.", "histories": [["v1", "Sat, 1 Oct 2016 05:15:15 GMT  (290kb,D)", "http://arxiv.org/abs/1610.00087v1", "5 pages, 2 figures, under submission to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017"]], "COMMENTS": "5 pages, 2 figures, under submission to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["wei dai", "chia dai", "shuhui qu", "juncheng li", "samarjit das"], "accepted": false, "id": "1610.00087"}, "pdf": {"name": "1610.00087.pdf", "metadata": {"source": "CRF", "title": "VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS", "authors": ["Wei Dai", "Chia Dai", "Shuhui Qu", "Juncheng Li", "Samarjit Das"], "emails": ["wdai@cs.cmu.edu,", "chiad@cs.cmu.edu,", "shuhuiq@stanford.edu,", "billy.li@us.bosch.com", "samarjit.das@us.bosch.com"], "sections": [{"heading": null, "text": "Index Terms\u2014 Convolutional Neural Networks, Raw Waveform, Acoustic Modeling, Neural Networks, Environmental Sound\n1. INTRODUCTION\nAcoustic modeling is traditionally divided into two parts: (1) designing a feature representation of the audio data, and (2) building a suitable predictive model based on the representation. However, it is often challenging and timeintensive to find the right representation in the so-called \u201cfeature-engineering\u201d process, and the often heuristically designed features might not be optimal for the predictive task. Deep neural networks, which have achieved state-of-the-art performances in acoustic scene recognition [1] and speech recognition [2], have increasingly blurred the line between representation learning and predictive modeling. Instead of using the hand-tuned Gaussian Mixture Model features and Mel-frequency cepstrum coefficients, neural network models can directly take as input features such as spectrograms [2] and even raw waveforms [3]. By using simpler features, deep\n*These authors contributed equally.\nneural networks can be viewed as extracting feature representation jointly with classification, rather than separately [4]. This joint optimization is highly effective in speech recognition [2] and image classification [5], among others.\nA fundamental building block of these models is the convolutional neural networks (CNNs), which can learn spatially or temporally invariant features from pixels or time-domain waveforms. CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8]. A common theme among these powerful CNN models is that they are usually very deep, with the number of layers ranging from tens to even over a hundred. Nonetheless, designing and training a deep network suitable for a new application domain remain challenging.\nRecent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10]. In one case, CNNs with time-domain waveforms can match the performance of models using conventional features like log-mel features [4]. These works, however, have mostly considered only less deep networks, such as two convolutional layers [4, 11].\nIn this work, we propose and study very deep architectures with up to 34 weight layers, directly using time-series waveforms as the input. Our deep networks are efficient to optimize over long sequences (e.g., vector of length 32000), necessary for processing raw audio waveforms. Our architectures use a very small receptive field in the convolutional layers, but a large receptive field in the first layer chosen based on the audio sampling rate to mimic bandpass filter. Our models are fully convolutional, without fully connected layers and dropout, in order to maximize the representation learning in the convolutional layers and can be applied to audio of varying lengths. By applying batch normalization [12], residual learning [6], and a careful design of down-sampling layers, we overcome the difficulties in training very deep models while keeping the computation cost low.\nOn an environmental sound recognition task [13], we show that deep networks improve the performance of networks with 2 convolutional layers by over 15% in absolute accuracy. We further demonstrate that the performance of deep models using just the raw signal is competitive with models using log-mel features [11]. To our knowledge, this\nar X\niv :1\n61 0.\n00 08\n7v 1\n[ cs\n.S D\n] 1\nO ct\n2 01\n6\nis the first report of a parity performance between log-mel features and raw time signal for environmental sound recognition."}, {"heading": "2. VERY DEEP CONVOLUTIONAL NETWORKS", "text": "Table 1 outlines the 5 architectures we consider. Our architectures take as input time-series waveforms, represented as a long 1D vector, instead of hand-tuned features or specially designed spectrograms. Key design elements are: Deep architectures. To build very deep networks, we use very small receptive field 3 for all but the first 1D convolutional layers1. This reduces the number of parameters in each layer and control the model sizes and computation cost as we go deeper. Furthermore, we aggressively reduce the temporal resolution in the first two layers by 16x with large convolutional and max pooling strides to limit the computation cost in the rest of the network [16]. After the first two layers, the reduction of resolution is complemented by a doubling in the number of feature maps2. We use rectified linear units (ReLU) for lower computation cost, following [17, 15]. Fully convolutional networks. Most deep convolutional networks for classification use 2 or more fully connected (FC) layers of high dimensions (e.g., 4096 in [15, 5]) for discriminative modeling, leading to a very high number of parameters. We hypothesize that most of the learning occurs in the convolutional layers, and with a sufficiently expressive representation from convolutional layers, no FC layer is necessary. We therefore adopt a fully convolutional design for our network construction [6, 18]. Instead of FC layers, we use a single global average pooling layer which reduces each feature map into one float by averaging the activation across the temporal dimension. By removing FC layers, the network is forced to learn good representation in the convolutional layers, potentially leading to better generalization. We support this design decision in our evaluation and demonstrate that fully convolutional networks perform comparably or better compared with their counterparts endowed with FC layers. First layer receptive field. Time-domain waveforms at a reasonable sampling rate (e.g. 8000Hz) over a few seconds could have very large number of samples along a single dimension. If we exclusively use small receptive field for all convolutional layers such as in [15], which uses 3x3 in pixel for all layers, our model would need many layers in order to abstract high level features, which could be computationally expensive. Furthermore, audio sampling rate could affect the receptive field size in the first layer, since a field size of 80 at 8kHz sampling rate is at a different length scale than at 16kHz sampling rate. We thus choose our first layer receptive field to cover a 10-millisecond duration, which is similar to the window size for many MFCC computation. In Section 3\n1Small receptive fields were first popularized by [15] for 2D images. 2In the visual domain this change in resolution and the number of features maps leads to more specialized filters at the higher layers (e.g., feature maps responding to faces) and more basic filters at the bottom (e.g., feature maps responding diagonal lines).\nwe show that a much smaller or larger receptive field gives poor performance. Batch Normalization. We adopt auxiliary layers called batch normalization (BN) [12] that alleviates the problem of exploding and vanishing gradients, a common problem in optimizing deep architectures. BN normalizes the output of the previous layer so the gradients are well-behaved. This makes possible training very deep networks (M18, M34-res) that were not studied previously [19]. Following [12], we apply BN on the output of each convolutional layer before applying ReLU non-linearity. Residual Learning. Residual learning [6] is a recently proposed learning framework to ease the training of very deep networks. Normally we train a block of neural network layers to fit a desired mapping H(x) of x (x being the the input to the layers). In the residual framework, we instead let the block of layers approximate F(x) = H(x) \u2212 x, the residual mapping. Residual learning is achieved through a skip connection in the residual block (\u201cres-block\u201d, Figure 1b). We apply residual learning in M34-res (Table 1)."}, {"heading": "3. EXPERIMENT DETAILS", "text": "We use UrbanSound8k dataset which contains 10 environmental sounds in urban areas, such as drilling, car horn, and children playing [13]. The dataset consists of 8732 audio clips of 4 seconds or less, totalling 9.7 hours. We use the official fold 10 to be our test set, and the rest for training and validation. For computational speed, the audio waveforms are down-sampled to 8kHz and standardized to 0 mean and variance 1. We shuffle the training data but do not perform data augmentation.\nWe train the CNN models using Adam [20], a variant of\nM3 (0.2M) M5 (0.5M) M11 (1.8M) M18 (3.7M) M34-res (4M) Input: 32000x1 time-domain waveform [80/4, 256] [80/4, 128] [80/4, 64] [80/4, 64] [80/4, 48] Maxpool: 4x1 (output: 2000 \u00d7 n)\n[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48\n3, 48\n] \u00d7 3\nMaxpool: 4x1 (output: 500\u00d7n) [3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96\n3, 96\n] \u00d7 4\nMaxpool: 4x1 (output: 125 \u00d7 n) [3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192\n3, 192\n] \u00d7 6\nMaxpool: 4x1 (output: 32 \u00d7 n) [3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384\n3, 384\n] \u00d7 3\nGlobal average pooling (output: 1 \u00d7 n) Softmax\nTable 1: Architectures of proposed fully convolutional network for time-domain waveform inputs. M3 (0.2M) denotes 3 weight layers and 0.2M parameters. [80/4, 256] denotes a convolutional layer with receptive field 80 and 256 filters, with stride 4. Stride is omitted for stride 1 (e.g., [3, 256] has stride 1). [...] \u00d7k denotes k stacked layers. Double layers in a bracket denotes a residual block and only occur in M34-res. Output size after each pooling is written as m \u00d7 n where m is the size in time-domain and n is the number of feature maps and can vary across architectures. All convolutional layers are followed by batch normalization layers, which are omitted to avoid clutter. Without fully connected layers, we do not use dropout [14] in these architectures.\nstochastic gradient descent that adaptively tunes the step size for each dimension. We run each model for 100-400 epochs (defined as a pass over the training set) until convergence. The weights in each model are initialized from scratch without any pretrained model. We use glorot initialization [21] to avoid exploding or vanishing gradients. All weight parameters are subjected to `2 regularization with coefficient 0.0001. Our models are implemented in Tensorflow [22] and trained on machines equipped with a Titan X GPU. Additional Models. To aid analysis, we train variants of models in Table 1. The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11]. Following these works we also use a dropout layer between each fully connected layers for regularization, with a dropout rate of 0.3. We insert a batch normalization layer after each fully connected layers to aid training. These models have substantially more parameters than the original models due to the FC layers (Table 5). Additionally, M3-big and M5big (Table 4) are variants of M3 and M5, respectively, with 50% and 100% more filters (e.g., 384/256 filters in the first convolutional layer in M3-big/M5-big)."}, {"heading": "4. RESULTS AND ANALYSES", "text": "Table 2 shows the test accuracies and training time for models in Table 2. We first note that M3 perform very poorly compared with the other models, indicating that 2-layered CNNs are insufficient to extract discriminative features from raw waveforms for sound recognition. This is in contrast with models using the spectrogram as input, which achieve good performance with just 2 convolutional layers [11], and shows that applying CNN directly on time-series data is challenging. M3-big, a variant of M3 with 50% more filters and 2.5x more parameters, does not significantly improve the performance\nModel Test Time M3 56.12% 77s M5 63.42% 63s\nM11 69.07% 71s M18 71.68% 98s\nM34-res 63.47% 124s\nTable 2: Test accuracies and training time per epoch (a sweep over the training set) for models in Table 1 on UrbanSound8k dataset using a Titan X GPU.\n(Table 4), showing that shallow models have limited capacity to capture time-series inputs even with a larger model.\nDeeper networks (M5, M11, M18, M34-res) substantially improve the performance. The test accuracy improves with increasing network depth for M5, M11, and M18. Our best model M18 reaches 71.68% accuracy that is competitive with the reported test accuracy of CNNs on spectrogram input using the same dataset [11]3. The performance increases cannot be simply attributed to the larger number of parameters in the deep models. For example, M5-big has 2.2M parameters (Table 4) but only achieves 63.30% accuracy, compared with the 69.07% by M11 (1.8M parameters). By using a very deep architecture, M18 outperforms M3 by as much as 15.56% in absolute accuracy, which shows that deeper architectures substantially improve acoustic modeling using waveforms. Furthermore, by using an aggressive down-sampling in the initial layers, very deep networks can be economical to train (Table 2 Time column). When we use stride 1 instead of 4 in the first convolutional layer for M11, we observe a 3.5x increase in training time but a lower test accuracy (67.37%) af-\n3Figure 4 in [11] reports \u223c68% accuracy using a baseline CNN model. We point out that we have a different evaluation scheme: we use the 10-th fold as test set, while [11] performs 10-fold evaluation. Also we use sound at 8kHz sampling rate while they use the original 44.1kHz.\nter 10 hours of training, compared with 68.42% test accuracy reached in 2 hours by M18.\nInterestingly, the performance improves with depth up to M18, at 71.68% test accuracy. M34-res only achieves 63.47% test accuracy. This is due to overfitting. We observe that with residual learning we have no problem optimizing deep networks like M34-res, and M34-res reaches an extremely high training accuracy of 99.21%, compared with 96.72% training accuracy by M18. We also observe overfitting in a residual variant of M11 network (not shown here) which reaches higher training accuracy but a lower test accuracy (by 0.17%). Overfitting caused by very deep networks is well documented [6]. We believe that our dataset is too small to train M34-res without further regularization. Nonetheless, M34-res still outperforms M3 and M5.\nWe compare our fully convolutional network with conventional networks that use large fully connected layers (FC) for classification. Table 5 shows that FC layers can increase number of parameters significantly and increase training time by 2\u223c95%. However, FC layers do not improve test accuracy, and in the cases of M3-fc and M11-fc the additional FC layers lead to lower test accuracy (i.e., poorer generalization). We believe that the lack of FC layers in our network design pushes learning down to convolutional layers, leading to better representation and generalization.\nTo understand the effect of the receptive field (RF) size in the first convolutional layer, we train M11-srf and M18-srf, variants of M11 and M18 with RF 8, and M11-lrf and M18lrf with RF 320. Table 3 shows that the performance degrades significantly by up to 6.6% compared with M11 and M18 with RF 80. Previous works have shown that the first convolutional layer, when trained on raw waveforms, mimics wavelet transforms [9, 4]. Our results suggest that a small RF popularized by vision models is insufficient to capture the necessary bandpass filter characteristics in the first convolutional layer, while a large RF smooths out local structures and cannot effectively detect local impulse patterns.\nWe study the effect of batch normalization (BN) in optimizing very deep networks (Table 6). Without BN, both M11-no-bn and M18-no-bn can be optimized to high training\naccuracy. Note that M18-no-bn results in lower test accuracy, indicating that BN has a regularization effect [12]. M34-nobn could not be optimized without BN and performs close to random guess (10%) after 159 epochs of training.\nFig. 2 shows the learned kernels for M18 variants with different RF sizes in the first convolution layer. All of them learn a filter bank of bandpass filter. M18 (Fig. 2 left) has well-distributed filters. In contrast, the small RF model (Fig. 2 middle) has much more dispersed bands, and thus lower frequency resolution for subsequent layers. Conversely, large RF model (Fig. 2 right) has fine-grained filters, but does not have sufficient filters in the high frequency range, showing that it cannot effectively respond to local high frequency impulses."}, {"heading": "5. CONCLUSION", "text": "In this work, we propose very deep convolutional neural networks that operate directly on acoustic waveform inputs. Our networks, up to 34 weight layers, are efficient to optimize, thanks to the combination of batch normalization, residual learning, and down-sampling. We use a broad receptive field (RF) in the first convolutional layer and narrow RFs in the rest of the network. Our results show that a deep network with 18 weight layers outperforms networks with 2 convolutional layers by 15.56% accuracy absolutely and achieves 71.8% accuracy, competitive with CNNs using log-mel spectrogram inputs [11]. Our fully convolutional networks compare favorably with those with fully connected layers. Our proposed deep architectures hold the promise to improve CNNs for speech recognition and other time-series modeling."}, {"heading": "6. ACKNOWLEDGEMENT", "text": "This work is supported by contract FA8702-15-D-0002 with Software Engineering Institute, a center sponsored by the United States Department of Defense."}, {"heading": "7. REFERENCES", "text": "[1] Hamid Eghbal-Zadeh, Bernhard Lehner, Matthias Dorfer, and Gerhard Widmer, \u201cCP-JKU submissions for DCASE-2016: a hybrid approach using binaural ivectors and deep convolutional neural networks,\u201d Tech. Rep., DCASE2016 Challenge, September 2016.\n[2] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \u201cDeep speech: Scaling up end-to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014.\n[3] Yedid Hoshen, Ron J Weiss, and Kevin W Wilson, \u201cSpeech acoustic modeling from raw multichannel waveforms,\u201d in ICASSP. IEEE, 2015, pp. 4624\u20134628.\n[4] Tara N Sainath, Ron J Weiss, Andrew Senior, Kevin W Wilson, and Oriol Vinyals, \u201cLearning the speech frontend with raw waveform cldnns,\u201d in Proc. Interspeech, 2015.\n[5] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in Neural Information Processing Systems 25, P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, Eds., pp. 1106\u20131114. 2012.\n[6] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015.\n[7] Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior Wolf, \u201cDeepface: Closing the gap to human-level performance in face verification,\u201d in IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1701\u20131708.\n[8] Florian Schroff, Dmitry Kalenichenko, and James Philbin, \u201cFacenet: A unified embedding for face recognition and clustering,\u201d in CVPR, 2015, pp. 815\u2013823.\n[9] Zolta\u0301n Tu\u0308ske, Pavel Golik, Ralf Schlu\u0308ter, and Hermann Ney, \u201cAcoustic modeling with deep neural networks using raw time signal for lvcsr.,\u201d in INTERSPEECH, 2014, pp. 890\u2013894.\n[10] Pavel Golik, Zolta\u0301n Tu\u0308ske, Ralf Schlu\u0308ter, and Hermann Ney, \u201cConvolutional neural networks for acoustic modeling of raw time signal in lvcsr,\u201d in Sixteenth Annual\nConference of the International Speech Communication Association, 2015.\n[11] Karol J Piczak, \u201cEnvironmental sound classification with convolutional neural networks,\u201d in 2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2015, pp. 1\u20136.\n[12] Sergey Ioffe and Christian Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d arXiv preprint arXiv:1502.03167, 2015.\n[13] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello, \u201cA dataset and taxonomy for urban sound research,\u201d in Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 1041\u20131044.\n[14] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, \u201cDropout: A simple way to prevent neural networks from overfitting,\u201d Journal of Machine Learning Research, 2014.\n[15] Karen Simonyan and Andrew Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.\n[16] Christian et al Szegedy, \u201cGoing deeper with convolutions,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.\n[17] Matthew D et al. Zeiler, \u201cOn rectified linear units for speech processing,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3517\u20133521.\n[18] Jonathan Long, Evan Shelhamer, and Trevor Darrell, \u201cFully convolutional networks for semantic segmentation,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u2013 3440.\n[19] Tom Sercu, Christian Puhrsch, Brian Kingsbury, and Yann LeCun, \u201cVery deep multilingual convolutional neural networks for lvcsr,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4955\u20134959.\n[20] Diederik Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[21] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks.,\u201d in Aistats, 2010, vol. 9, pp. 249\u2013256.\n[22] Mart\u0131n Abadi, Ashish Agarwal, et al., \u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems,\u201d arXiv preprint arXiv:1603.04467, 2016."}], "references": [{"title": "CP-JKU submissions for DCASE-2016: a hybrid approach using binaural ivectors and deep convolutional neural networks", "author": ["Hamid Eghbal-Zadeh", "Bernhard Lehner", "Matthias Dorfer", "Gerhard Widmer"], "venue": "Tech. Rep., DCASE2016 Challenge, September 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Yedid Hoshen", "Ron J Weiss", "Kevin W Wilson"], "venue": "ICASSP. IEEE, 2015, pp. 4624\u20134628.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Tara N Sainath", "Ron J Weiss", "Andrew Senior", "Kevin W Wilson", "Oriol Vinyals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, Eds., pp. 1106\u20131114. 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1701\u20131708.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "CVPR, 2015, pp. 815\u2013823.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Zolt\u00e1n T\u00fcske", "Pavel Golik", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "INTERSPEECH, 2014, pp. 890\u2013894.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr", "author": ["Pavel Golik", "Zolt\u00e1n T\u00fcske", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "Sixteenth Annual  Conference of the International Speech Communication Association, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["Karol J Piczak"], "venue": "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2015, pp. 1\u20136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A dataset and taxonomy for urban sound research", "author": ["Justin Salamon", "Christopher Jacoby", "Juan Pablo Bello"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 1041\u20131044.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian et al Szegedy"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "On rectified linear units for speech processing", "author": ["Matthew D et al. Zeiler"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3517\u20133521.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u2013 3440.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["Tom Sercu", "Christian Puhrsch", "Brian Kingsbury", "Yann LeCun"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4955\u20134959.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks, which have achieved state-of-the-art performances in acoustic scene recognition [1] and speech recognition [2], have increasingly blurred the line between representation learning and predictive modeling.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "Deep neural networks, which have achieved state-of-the-art performances in acoustic scene recognition [1] and speech recognition [2], have increasingly blurred the line between representation learning and predictive modeling.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Instead of using the hand-tuned Gaussian Mixture Model features and Mel-frequency cepstrum coefficients, neural network models can directly take as input features such as spectrograms [2] and even raw waveforms [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 2, "context": "Instead of using the hand-tuned Gaussian Mixture Model features and Mel-frequency cepstrum coefficients, neural network models can directly take as input features such as spectrograms [2] and even raw waveforms [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 3, "context": "neural networks can be viewed as extracting feature representation jointly with classification, rather than separately [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "This joint optimization is highly effective in speech recognition [2] and image classification [5], among others.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "This joint optimization is highly effective in speech recognition [2] and image classification [5], among others.", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8].", "startOffset": 170, "endOffset": 176}, {"referenceID": 7, "context": "CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8].", "startOffset": 170, "endOffset": 176}, {"referenceID": 8, "context": "Recent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10].", "startOffset": 170, "endOffset": 180}, {"referenceID": 3, "context": "Recent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10].", "startOffset": 170, "endOffset": 180}, {"referenceID": 9, "context": "Recent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10].", "startOffset": 170, "endOffset": 180}, {"referenceID": 3, "context": "In one case, CNNs with time-domain waveforms can match the performance of models using conventional features like log-mel features [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "These works, however, have mostly considered only less deep networks, such as two convolutional layers [4, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "These works, however, have mostly considered only less deep networks, such as two convolutional layers [4, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 11, "context": "By applying batch normalization [12], residual learning [6], and a careful design of down-sampling layers, we overcome the difficulties in training very deep models while keeping the computation cost low.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "By applying batch normalization [12], residual learning [6], and a careful design of down-sampling layers, we overcome the difficulties in training very deep models while keeping the computation cost low.", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "On an environmental sound recognition task [13], we show that deep networks improve the performance of networks with 2 convolutional layers by over 15% in absolute accuracy.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "We further demonstrate that the performance of deep models using just the raw signal is competitive with models using log-mel features [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "Furthermore, we aggressively reduce the temporal resolution in the first two layers by 16x with large convolutional and max pooling strides to limit the computation cost in the rest of the network [16].", "startOffset": 197, "endOffset": 201}, {"referenceID": 16, "context": "We use rectified linear units (ReLU) for lower computation cost, following [17, 15].", "startOffset": 75, "endOffset": 83}, {"referenceID": 14, "context": "We use rectified linear units (ReLU) for lower computation cost, following [17, 15].", "startOffset": 75, "endOffset": 83}, {"referenceID": 14, "context": ", 4096 in [15, 5]) for discriminative modeling, leading to a very high number of parameters.", "startOffset": 10, "endOffset": 17}, {"referenceID": 4, "context": ", 4096 in [15, 5]) for discriminative modeling, leading to a very high number of parameters.", "startOffset": 10, "endOffset": 17}, {"referenceID": 5, "context": "We therefore adopt a fully convolutional design for our network construction [6, 18].", "startOffset": 77, "endOffset": 84}, {"referenceID": 17, "context": "We therefore adopt a fully convolutional design for our network construction [6, 18].", "startOffset": 77, "endOffset": 84}, {"referenceID": 14, "context": "If we exclusively use small receptive field for all convolutional layers such as in [15], which uses 3x3 in pixel for all layers, our model would need many layers in order to abstract high level features, which could be computationally expensive.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "1Small receptive fields were first popularized by [15] for 2D images.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "We adopt auxiliary layers called batch normalization (BN) [12] that alleviates the problem of exploding and vanishing gradients, a common problem in optimizing deep architectures.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "This makes possible training very deep networks (M18, M34-res) that were not studied previously [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Following [12], we apply BN on the output of each convolutional layer before applying ReLU non-linearity.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "Residual learning [6] is a recently proposed learning framework to ease the training of very deep networks.", "startOffset": 18, "endOffset": 21}, {"referenceID": 12, "context": "We use UrbanSound8k dataset which contains 10 environmental sounds in urban areas, such as drilling, car horn, and children playing [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "We train the CNN models using Adam [20], a variant of", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 9, "endOffset": 17}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 30, "endOffset": 37}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 42, "endOffset": 57}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 42, "endOffset": 57}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 9, "endOffset": 17}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 22, "endOffset": 30}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 35, "endOffset": 50}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 35, "endOffset": 50}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 9, "endOffset": 17}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 22, "endOffset": 30}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 35, "endOffset": 52}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 35, "endOffset": 52}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 13, "endOffset": 21}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 26, "endOffset": 43}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 26, "endOffset": 43}, {"referenceID": 2, "context": ", [3, 256] has stride 1).", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": "Without fully connected layers, we do not use dropout [14] in these architectures.", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "We use glorot initialization [21] to avoid exploding or vanishing gradients.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Our models are implemented in Tensorflow [22] and trained on machines equipped with a Titan X GPU.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11].", "startOffset": 215, "endOffset": 226}, {"referenceID": 14, "context": "The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11].", "startOffset": 215, "endOffset": 226}, {"referenceID": 10, "context": "The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11].", "startOffset": 215, "endOffset": 226}, {"referenceID": 10, "context": "This is in contrast with models using the spectrogram as input, which achieve good performance with just 2 convolutional layers [11], and shows that applying CNN directly on time-series data is challenging.", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "68% accuracy that is competitive with the reported test accuracy of CNNs on spectrogram input using the same dataset [11]3.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "3Figure 4 in [11] reports \u223c68% accuracy using a baseline CNN model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "We point out that we have a different evaluation scheme: we use the 10-th fold as test set, while [11] performs 10-fold evaluation.", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Overfitting caused by very deep networks is well documented [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "Previous works have shown that the first convolutional layer, when trained on raw waveforms, mimics wavelet transforms [9, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 3, "context": "Previous works have shown that the first convolutional layer, when trained on raw waveforms, mimics wavelet transforms [9, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 11, "context": "Note that M18-no-bn results in lower test accuracy, indicating that BN has a regularization effect [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "8% accuracy, competitive with CNNs using log-mel spectrogram inputs [11].", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (\u223c2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features.", "creator": "LaTeX with hyperref package"}}}