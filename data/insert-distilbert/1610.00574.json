{"id": "1610.00574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Cosine Similarity Search with Multi Index Hashing", "abstract": "due to rapid development practices of the internet, recent years here have witnessed comparatively an explosion in the hardware rate of data generation. dealing with data at current scales brings up unprecedented operational challenges. from the algorithmic view point, executing existing linear algorithms in information retrieval and machine learning on sequencing such tremendous amounts of data incur intolerable computational and storage costs. to address narrowing this issue, there is obvious a growing interest to genetically map data size points in large - scale fragmented datasets to binary codes. this can already significantly reduce the storage complex complexity of large - scale datasets. however, one of the most compelling organizational reasons for using binary codes or any discrete metadata representation is that they can be used as direct indices into such a hash table. incorporating hash table offers fast query execution ; one can look up the only nearby buckets in compiling a hash table populated with binary codes to retrieve similar items. nonetheless, if binary link codes are compared in terms of the cosine square similarity rather than the hamming distance, there is no fast exact sequential procedure to find the $ 26 k $ closest items to win the query other than the exhaustive search. given a large shared dataset of binary codes and a binary query, the computational problem that we address is to efficiently find $ k $ r closest codes in the dataset that yield the largest cosine similarities to the query. to perfectly handle this issue, we first elaborate on showing the relation between the hamming distance and the cosine similarity. this allows finding the sequence of buckets to check in the hash table. having this sequence, we propose a multi - vector index hashing approach altogether that can increase the search path speed up to orders of magnitude in comparison to reducing the exhaustive search and even approximation methods such as lsh. we empirically evaluate purely the mathematical performance of the proposed retrieval algorithm on real world datasets.", "histories": [["v1", "Wed, 14 Sep 2016 23:16:37 GMT  (286kb)", "http://arxiv.org/abs/1610.00574v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.DS cs.IR cs.LG", "authors": ["sepehr eghbali", "ladan tahvildari"], "accepted": false, "id": "1610.00574"}, "pdf": {"name": "1610.00574.pdf", "metadata": {"source": "CRF", "title": "Cosine Similarity Search with Multi-Index Hashing", "authors": ["Sepehr Eghbali", "Ladan Tahvildari"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 57\n4v 1\n[ cs\n.D B\n] 1\n4 Se\np 20\n16 1\nIndex Terms\u2014Nearest neighbour search, binary codes, large-scale retrieval, cosine similarity\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "R ECENT years have witnessed an explosion in the quan-tity and dimensionality of data. To capture this trend, the term Big data was coined by researchers to emphasize the scale of current datasets. This trend is reflected in the growing number of available large-scale datasets on the Internet which have become benchmarks for a broad array of fields. For example, in text mining, Google provides N-gram data obtained from over 5 million books [31]. Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks. Dealing with such large-scale datasets brings up unprecedented challenges, in turn necessitating new software tools, algorithms and databases for data acquisition, storage, transmission and retrieval [22].\nIn this paper, we address the problem of query execution for large-scale datasets. We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26]. Given a collection of items, a query item and a measure of similarity, the exact solution of the KNN problem is the set of the K closest items within the dataset to the query. From an algorithmic point of view, solving KNN for a query point q \u2208 Rd requires scanning all n items in the dataset X = {xi \u2208 Rd}ni=1. Assuming that computing similarity of two points in Rd can be performed in O(d), then KNN can be solved in O(dn) using exhaustive search. Often, executing linear algorithms are impractical if values of n and d are large. In dealing with large-scale datasets, two considerations play a crucial role for performing costeffective KNN queries. First aspect is the computational\ncost. This means computing the similarity between two items must be performed fast. The second aspect is the storage cost. To accelerate the query execution, the data (or data signatures) should fit into the Random Access Memory (RAM), thus a preprocessing step is usually required to reduce the dimensionality of the input data.\nTo satisfy the above two requirements, there has been a growing interest in mapping (hashing) massive datasets to similarity preserving binary codes (binary vectors). The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52]. Incorporating binary codes can dramatically reduce computational and storage costs, since binary codes are storage efficient and computing their respective distances (such as the Hamming distance) is a cheap computational operation. Moreover, in many applications, items of the database are naturally described in terms of binary codes. For example, in the Bag of Words (BoW) representation of the documents and images, the presence or absence of information, captured in terms of a binary variable, quantifies the items [7], [24].\nBesides the computational and storage benefits, binary codes can be treated as indices (memory addresses) of a hash table. Relying on this property, to solve the KNN problem, one can first populate a hash table with binary codes, and then search the nearby buckets of the query until K items are retrieved. If binary codes are used as indices into a hash table, and the Hamming distance is used as the measure of similarity, then the algorithm that solves the exact KNN problem is straightforward. Starting with a Hamming radius equal to zero, r = 0, at each step, the algorithm checks all buckets that lie at the Hamming distance r from the query. After each step, r is increased by\n2 one, and the algorithm proceeds untilK items are retrieved. However, in many applications, binary codes are compared in terms of the cosine similarity, rather than the Hamming distance. This is known as the angular KNN problem. In such cases, there are no exact sequential procedures for checking the buckets of the hash table. In practice, instead of using a hash table, researchers resort either to the brute force search [14], or to approximate similarity search techniques, such as LSH [20], to solve the angular KNN problem.\nIn this paper, we propose a sequential algorithm for performing exact angularKNN search among binary codes. Our approach iteratively finds the sequence of hash table buckets to check untilK neighbours are retrieved. We prove that, using the proposed procedure, the cosine similarity between the query and the sequence of generated buckets\u2019 indices will be monotonically decreasing. This means, the larger is the cosine similarity between a bucket index and the query, the sooner the index will appear in the sequence.\nA problemwith using hash tables is that the performance of search degrades quickly when the number of empty buckets is much larger than the occupied ones. In such scenarios, exhaustive search is a faster alternative.Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue. TheMIH technique hinges on dividing long codes into disjoint shorter codes to reduce the number of empty buckets. Motivated by the MIH technique proposed in [35], we develop the Angular Multi-Index Hashing technique to realize similar advantages. Empirical evaluation of our approach shows orders of magnitude improvement in the search speed in conjunction with large-scale datasets.\nGiven a binary query and a hash table populated with binary codes, the research questions that we address are as follows:\nRQ1: In what order should the buckets be checked to find the K nearest neighbours? RQ2: How can the MIH technique be applied to angular preserving binary codes? RQ3: What is the effect of the MIH technique on the query time?\nIn a nutshell, we first establish a relationship between the cosine similarity and the Hamming distance. Relying on this relationship, a fast algorithm for finding the ordering of buckets to be checked is introduced. This allows modifying the multi-index hashing approach such that it can be applied to angular preserving binary codes. We empirically show that using the proposed approach to solve angular KNN queries can greatly reduce storage and computational costs. A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44]. However, to the best of our knowledge, there is no general exact technique that performs better than brute force search to solve the angular KNN in conjunction with high dimensional binary codes."}, {"heading": "2 MOTIVATING EXAMPLE", "text": "Consider the Google image search where the goal is to rank and find relevant images to a query image. One approach to solve this problem is to incorporate text-based analysis. For example, to find images of the Eiffel Tower, rather than\nsearching through the visual content of photos, images that appear on the web pages containing the key word \u201cEiffel Tower\u201d are chosen. This can often result in a poor outcome since no image analysis is performed. Indeed, to address this issue, Google uses the VisualRank algorithm [25]. The general idea of the VisualRank algorithm is to first find all the images that match the query by analyzing their corresponding tags, meta data, image names, and other related features, using the PageRank algorithm [37]. This results in forming a pool of candidate results. The next step is to compute the similarity between such candidates and the query. Since it is computationally impractical to measure distances to all actual images, hashing methods are incorporated to reduce the search time. Although this method can decrease the computational complexity, the search time can still be in the order of several minutes. A possible approach to tackle such a problem is to use a hash table of binary codes to reduce the search time. In this article, we address the problem of searching in a hash table of binary codes relying on the cosine similarity measure."}, {"heading": "3 RELATED WORKS", "text": "This section reviews some of the popular and practical solutions to the nearest neighbour problem that relates to this study. In particular, we first review the early exact solutions to this problem. Then, we discuss the recent dataindependent and data-dependent approximate algorithms that can trade scalability for accuracy.\nA classical paradigm to reduce the computational cost of the nearest neighbour search relies on tree-based indexing structures which offer logarithmic query time, O(log n), on the average. Perhaps the most known example of such techniques is the kd-tree [3] with a worst-case search time of O(dn1\u22121/d). Following [3], many other tree-based indexing algorithms are proposed, such as R-tree, R*-tree, SS-tree, SRtree, and X-tree (see [41] for an overview). However, these methods suffer from a phenomenon known as the curse of dimensionality. This means, as the number of dimensions increases, these methods quickly reduce to the exhaustive search. Typically, the kd-tree and its variants are efficient for dimensions less than 20 [12], [49].\nTo overcome the apparent difficulty of devising algorithms that find the exact solution of the KNN problem, there has been an increasing interest to resort to approximate solutions. In the approximate formulation of KNN problem know as the Approximate Nearest Neighbour (ANN) problem, the algorithm is allowed to return an item whose distance from the query is at most c times (c > 1) the distance corresponding to the nearest neighbour. Among all such approximate techniques to solve the KNN problem, hashing is perhaps the most notable example. Hashing methods aim at mapping similar items into similar hash buckets. Early endeavors in hashing focused on using random functions to map the data. Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known. The idea of LSH is to use similarity preserving hash functions such that, with a high probability, points that are near to each other are mapped to the same hash bucket. The choice of the hash function is dependent on the measure of similarity in the original space. Over the years, several\n3 family of hash functions are defined for different similarity measures. LSH constructs several hash tables, with the goal that the query point and its nearest neighbour hash to the same bucket in at least one of the hash tables. To achieve an acceptable retrieval accuracy, the number of hash tables, as well as the number of buckets in each hash table, should be relatively large [50], [51]. Experimental studies indicate that LSH-based techniques need over a hundred [13] and sometimes several hundreds hash tables [5]. Since the size of the hash table is proportional to the number of data objects, in some applications, using LSH results in excessive memory cost and long query time. To reduce these costs, different heuristics have been proposed over the years. Entropy-based LSH [38] and Mutli-Prob LSH [28] are two well-known techniques to reduce the number of hash tables. The idea is to form some points near the query (through random perturbations) and use them as additional queries. These techniques can reduce the number of hash tables at the cost of increasing the query time.\nTo find compact codes, instead of using random projections, recent studies have aimed at finding data dependent mappings to reduce the number of required bits. Salakhutdinov and Hinton [40] have used Restricted Boltzmann Machine with several hidden layers as an auto-encoder to learn the underlying binary codes. Weiss et al. [50] have proposed an eigenvector formulation which aims at finding short similarity preserving binary codes, such that bits are uncorrelated and balanced. Binary Reconstructive Embedding (BRE) [27] uses a loss function that penalizes the difference between the Euclidean distance in the input space and the Hamming distance in the binary space. Norouzi et al. [33] have used a hinge-like loss function to learn the codes. They derived an upper bound on the loss function, and optimized it instead of the actual objective function, to reduce the computational complexity of the underlying optimization problem. In a follow up work, Norouzi et al [34] have used a triple objective function to learn the binary codes for supervised problems. The corresponding loss function learns a projection such that items with the same label are at small Hamming distances to each other. More recently, Carreira-Perpinan and Raziperchikolaei [6] have used autoencoder to learn binary codes in order to minimize the reconstruction error.\nCurrent literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52]. Optimizing parameters of hash functions is highly dependent on both the distance measure in the input space and that of the binary space. For unlabeled data, if the Euclidean distance is chosen as the measure of similarity, binary codes are usually compared in terms of the Hamming distance (which is equal to the squared Euclidean distance in the binary space) [15], [27]. However, in many vision and text-related applications, it is common to represent data as a Bag of Words (BoW) or vectors of frequencies with non-negative entries. In such applications, input vectors are compared in terms of the cosine similarity, and consequently, some techniques use the cosine similarity of binary codes as the similarity measure in the binary space [14], [45]. In the latter case, the objective is to find an angular preserving mapping that maps the high dimensional data to binary codes such that the cosine similarity of vectors in\nthe input space is preserved in the binary space. Researches have proposed various metrics (other than the Hamming distance and the cosine similarity) to compare binary codes. A notable example is the Spherical Hamming Distance [18].\nThe idea of using hash tables to avoid exhaustive search in ANN techniques has been studied in recent years. Babenko et al. [2] have proposed the inverted multi-index technique to solve the ANN problem where data points are estimated with codewords of multiple codebooks. This technique creates two hash tables by decomposing data vectors into two disjoint subvectors and hashing each subvector in one of the tables. The query is similarly decomposed into two subvectors and the search is performed in each hash table to find the corresponding nearest neighbour. However, the memory overhead of this technique grows rapidly with the number of hash tables, and typically, the technique is efficient when only two hash tables are used. More recently, Matsui et al. [30] have used multiple hash tables to reduce the search time, whereby the distance between items are approximated by codewords of multiple codebooks. Iwamura et al. [21] have proposed a non-exhaustive search algorithm based on the branch and bound paradigm."}, {"heading": "4 DEFINITIONS AND PROBLEM STATEMENT", "text": "Table 1 shows the notations used throughout this paper. Some further notations will be defined in Section 5.\nGiven a dataset X = {xi \u2208 Rd}ni=1, and a query q \u2208 Rd, the aim of Nearest Neighbor (NN) problem, also called the 1 Nearest Neighbour (1NN) problem, is to find the item in X that is the closest to q:\nNN(q) = argmin x\u2208X dis(q,x) (1)\nwhere dis(., .) is the distance between two items. The K Nearest Neighbour problem (KNN) is the generalization of 1NN, aiming to find the K closest items to the query. An item x \u2208 X is in the result of KNN(q), if and only if it satisfies the following condition:\n|{t \u2208 X such that dis(t,q) < dis(x,q)}| < K (2) where |.| denotes the cardinality of a set. When dis is the Euclidean distance, the problem is called the Euclidean KNN, and when dis is the inverse of the cosine similarity, it is called the angular KNN. The focus of this paper is to efficiently solve the angularKNN among a dataset of binary codes.\nAnother related search problem is the R-near neighbour problem (RNN).\nDEFINITION 1. (R-NEAR NEIGHBOUR PROBLEM) Given a query q, find all the R-near neighbours of q in the dataset, where an item x is an R-near neighbour of q if the distance between x and q is at most R.\nSimilarly, if the Euclidean distance is used as the similarity measure, we call the problem the Euclidean R-near neighbour.\nRNN and KNN problems are closely related. One way to tackle the Euclidean KNN problem in conjunction with a dataset of binary codes is to solve multiple instances of the Euclidean RNN problem. First, a hash table is populated with binary codes in the dataset B = {bi \u2208 {0, 1}p}ni=1.\n4\nThen, starting from a Hamming radius equal to zero,R = 0, the procedure increases R and then solves the R-near problem by searching among the buckets at the Hamming distance R from the query. This procedure iterates until K items are retrieved. Nevertheless, if cosine similarity is used, the order of buckets to check will not be the same as the case of the Hamming distance. Unlike the Hamming distance, the angle between two binary codes is not a monotonically increasing function of their Euclidean distance. In other words, if binary codes b1 and b2 satisfy ||q \u2212 b1||H > ||q \u2212 b2||H , it does not necessarily lead to sim(q,b1) < sim(q,b2) where sim(x,y) is the cosine of the angle between binary codes x, y, and ||x\u2212y||H denotes the Hamming distance between x, y. Thus, the order of buckets to check for solving the angular KNN is different from the case of the Euclidean KNN. In the next section, we propose an algorithm which efficiently generates the ordering required to solve the angular KNN problem."}, {"heading": "5 FAST COSINE SIMILARITY SEARCH", "text": "Consider a function W : Rd \u2192 {0, 1}p that hashes the items in the datasetX while preserving the cosine similarity. That is, the cosine similarity between xi and xj \u2208 Rd is as close as possible to the cosine similarity betweenW (xi) and W (xj). Now suppose W is applied to all of the items in X to create the dataset of binary codes B = {bi \u2208 {0, 1}p}ni=1. The proposed technique uses the binary codes in B to populate a hash table where each binary code is treated as the direct index into a hash bucket. The problem that we aim to tackle is finding the K closest binary codes (in terms of the cosine similarity) to the query. Evidently, for a given query q, the binary code that yields the highest cosine similarity is q itself. Therefore, the first bucket to check is the bucket with index q. The next bucket to check is the one with the second largest cosine similarity to q, and so on. In the rest of this section, we propose an algorithm for efficiently finding such a sequence of buckets.\nThe cosine similarity of two binary codes can be computed using:\nsim(q,bi) = \u3008q,bi\u3009\n||q||2||bi||2 , (3)\nwhere \u3008\u00b7, \u00b7\u3009 denotes the inner product and || \u00b7 ||u denotes the \u2113u norm. In comparison to the Hamming distance,\ncomputing the cosine similarity is computationally more demanding. While computing the Hamming distance requires an XOR followed by the popcount operator, calculating the cosine similarity needs computing the square roots and a division, refer to (3). As mentioned, not all items with a fixed Hamming distance to the query have equal cosine similarities to the query. However, this article shows a remarkable connection between the Hamming distance and the cosine similarity of binary codes.\nThe set of all binary codes at the Hamming distance r from the query can be partitioned into r + 1 subsets, where the codes in each subset yield equal cosine similarities to the query. In particular, for a binary code bi, with the Hamming distance r from q, \u2211p\nj=1 |qj\u2212b j i | = r, the positions at which\nq and bi differ can be one of the following two types: (1) qj = 1 and bji = 0, or (2) q j = 0 and bji = 1 where x j denotes the j-th entry of the vector x. Now, let rq,bi1\u21920 and rq,bi0\u21921 denote the number of type (1) and the number of type (2) distances, respectively. It is clear that 0 \u2264 rq,bi1\u21920 \u2264 ||q||1 and 0 \u2264 rq,bi0\u21921 \u2264 p \u2212 ||q||1. Also, the sum of type (1) and type (2) distances is equal to the Hamming distance, i.e., rq,bi1\u21920 + r q,bi 0\u21921 = r. Consequently, we can write:\nsim(q,bi) = ||q||1 \u2212 rq,bi1\u21920\n\u221a ||q||1 \u00d7 \u221a ||q||1 \u2212 rq,bi1\u21920 + rq,bi0\u21921 . (4)\nThe dot product of two binary codes (the numerator of (3)) is equal to the number of positions where q and bi are both 1, which is equal to ||q||1 \u2212 rq,bi1\u21920. The denominator simply contains the \u21132 norms of q and bi. In the rest of this paper, we use (4) to compute the cosine similarity. For a given q, each binary code b \u2208 B is associated with the tuple (rq,bi1\u21920, r q,bi 0\u21921), called hereafter the Hamming distance tuple, which is defined as follows:\nDEFINITION 2. (HAMMING DISTANCE TUPLE) Given a query q, we say a given binary code bi lies at the\nHamming distance tuple Hq,bi = (rq,bi1\u21920, rq,bi0\u21921) from q if:\n1) number of bit positions in which q is 1 and bi is 0\nequals rq,bi1\u21920, and, 2) number of bit positions in which q is 0 and bi is 1\nequals rq,bi0\u21921.\nA Hamming distance tuple, such as (r1, r2), is valid if both of its elements are in valid ranges, i.e., 0 \u2264 r1 \u2264 ||q||1 and 0 \u2264 r2 \u2264 p\u2212 ||q||1.\nA key observation is that all binary codes with the same Hamming distance tuples yield the same sim value. Therefore, instead of searching for the sequence of buckets to be checked in the hash table, one can search for the sequence of Hamming distance tuples.\nEach Hamming distance tuple represents a set of binary codes at the same angle from q. The number of binary codes lying at the Hamming distance tuple (rq,bi1\u21920, r q,bi 0\u21921) from q is:\n(\n||q||1 rq,bi1\u21920\n) \u00d7 ( p\u2212 ||q||1 rq,bi0\u21921 ) . (5)\nWe say that a Hamming distance tuple (r\u20321, r \u2032 2) is less than or equal to (r1, r2), shown by (r \u2032 1, r \u2032 2) (r1, r2), if and only if r\u20321 \u2264 r1 and r\u20322 \u2264 r2.\n5\nDEFINITION 3. ((r1, r2))-NEAR NEIGHBOUR) A binary code bi is called an (r1, r2)-near neighbour of q, if we have Hq,bi (r1, r2).\nEXAMPLE 1. Suppose q = (1, 1, 1, 0, 0, 0) and b1 = (0, 1, 0, 1, 1, 1), then b1 lies at the Hamming distance tuple Hq,b1 = (2, 3) from q, and b2 = (1, 1, 1, 1, 1, 1) lies at the Hamming distance tuple Hq,b2 = (0, 3) from q. Also, the Hamming distance tuple (0, 3) is less than the Hamming distance tuple (2, 3).\nThe partial deviates of (4) with respect to rq,bi1\u21920 and r q,bi 0\u21921 are both negative. This property indicates that, for a given Hamming distance tuple (x, y), all the binary codes with the Hamming distance tuple (x\u2032, y\u2032) satisfying (x\u2032, y\u2032) (x, y) have larger sim values.\nTo visualize how the value of sim varies with respect to rq,bi1\u21920 and r q,bi 0\u21921, the sim value as a function of r q,bi 1\u21920 and rq,bi0\u21921 is plotted in Fig. 1. We are interested in sorting the tuples (rq,bi1\u21920, r q,bi 0\u21921) (small circles in Fig. 1) in decreasing order of sim values. A straightforward way to do this, is to compute and sort the sim values of all possible tuples. However, we propose a more efficient approach that, in most cases, requires neither sorting, nor computing the sim values.\nDEFINITION 4. (HAMMING BALL) For a given query q, the set of all possible binary codes with a Hamming distance at most r from q is called the Hamming ball centered at q with radius r, and is shown by C(q, r):\nC(q, r) = {h \u2208 {0, 1}p : ||q\u2212 h||H \u2264 r}. (6)\nGiven bi, values of r q,bi 1\u21920 and r q,bi 0\u21921 can be computed efficiently using bitwise operations. However, to search for the K closest neighbours in the populated hash table, we are interested in progressively finding the values of rq,bi1\u21920 and rq,bi0\u21921 that lead to binary codes with the largest sim value. One observation is that, within all indices lying at the\nHamming distance r from q, indices with the Hamming distance tuples (rq,bi1\u21920, r q,bi 0\u21921) = (0, r) and (r q,bi 1\u21920, r q,bi 0\u21921) = (r, 0) (provided that they are valid tuples) yield the largest and the smallest cosine similarities with the query, respectively. This leads to the following proposition:\nProposition 1: Among all binary codes lying at the Hamming distance r from q, those with larger values of rq,bi0\u21921 yield larger sim values.\nProof: To prove this proposition, let us compute the derivatives of (4) with respect to rq,bi0\u21921 or r q,bi 1\u21920. Suppose rq,bi1\u21920 + r q,bi 0\u21921 = r, by replacing r q,bi 1\u21920 with r \u2212 rq,bi0\u21921, we obtain:\nsim(q,b1) = ||q||1 + rq,bi0\u21921 \u2212 r\n\u221a ||q||1 \u00d7 \u221a ||q||1 + 2rq,bi0\u21921 \u2212 r . (7)\nAfter some algebraic manipulations, it follows that \u2202sim \u2202r\nq,bi 0\u21921\n>\n0. Therefore, among all tuples at the Hamming distance r from q, the maximum of sim occurs at (rq,bi1\u21920, r q,bi 0\u21921) = (0, r), and its minimum occurs at (rq,bi1\u21920, r q,bi 0\u21921) = (r, 0).\nProposition 1 states that, for tuples at the Hamming distance r from q, sim is a growing function of rq,bi0\u21921. As a result, the order of tuples in the direction of decreasing sim values is (0, r), (1, r \u2212 1), . . . , (r, 0). In other words, among all the binary codes that lie at the Hamming distance r from the query, those with larger \u21131 norms yield larger cosine similarities.\nWhile the Proposition 1 specifies the direction of the search for a given Hamming distance, it does not establish the relationship between the Hamming distance and the cosine similarity for different Hamming distances.\nEXAMPLE 2. For query q = (1, 0, 1, 0, 1), the binary code b1 = (1, 0, 1, 0, 0) yields a smaller cosine similarity with q as compared to b2 = (1, 1, 1, 1, 1) though it has a smaller Hamming distance to q. As a result, the bucket corresponding to b2 should be checked before the one corresponding to b1.\nAlthough the above may appear as a discouraging observation, we show that, for small Hamming distances, the cosine similarity and the Hamming distance are related to each other. In particular, the following proposition specifies the region where the cosine similarity is a monotonically decreasing function of the Hamming distance.\nProposition 2: If ||q||1 > r(r+t)t for some t \u2208 {1, . . . , d} and r, then all binary codes in C(q, r) yield larger cosine similarities to q as compared to binary codes with Hamming distances at least r + t from q.\nProof: According to the Proposition 1, the maximum of the sim value for a fixed Hamming distance r occurs at (rq,bi1\u21920, r q,bi 0\u21921) = (0, r) with a sim value of \u221a z z+r , and its minimum occurs at (rq,bi1\u21920, r q,bi 0\u21921) = (r, 0) with a sim value of \u221a\nz\u2212r z , where z = ||q||1. The condition in the proposition\nis satisfied if the smallest value of sim(q,bi), where bi \u2208 C(q, r), is larger than the largest value of sim(q,bj) where\n6 bj lies at the Hamming distance r + t from q. Hence, we have:\n\u221a\n||q||1 \u2212 r ||q||1 >\n\u221a\n||q||1 ||q||1 + r + t\n\u21d2 ||q||1 \u2212 r||q||1 > ||q||1 ||q||1 + r + t\n\u21d2 (||q||1 \u2212 r)(||q||1 + r + t) > ||q||21\n\u21d2 r(r + t) < t||q||1\n\u21d2 ||q||1 > r(r + t)\nt .\n(8)\nThis concludes the proof. If the condition of the Proposition 2 is satisfied for t = 1 and an arbitrary value of r, then all the binary codes inside the Hamming ball C(q, r) have larger cosine similarities as compared to those outside of C(q, r). Also, among all binary codes inside C(q, r), those with larger Hamming distances from the query have smaller cosine similarities. That is, if bi is closer to q than to bj in terms of the Hamming distance, then bi is also closer to q in terms of the cosine similarity.\nTherefore, for binary codes lying within the Hamming ball C(q, r), cosine similarity is a decreasing function of the Hamming distance. In this case, the search algorithm is straightforward: for t = 1, the maximum integer r that satisfies the inequality condition in the Proposition 2 is found. Let r\u0302 denote the integer part of the positive root of the equation r2 + r \u2212 ||q||1 (this equation has only one positive root). Staring from r = 0, the search algorithm increases the Hamming radius until the specified number of neighbours are retrieved, or until r reaches r\u0302. For each Hamming radius, the search direction should be aligned with the direction specified by the Proposition 1.\nDEFINITION 5. ((r1, r2)-NEAR NEIGHBOUR PROBLEM) Given the query q and the dataset of binary codes B, the result of (r1, r2)-near neighbour problem is the set of all codes in B that lie at a Hamming distance tuple at most (r1, r2) from q.\nOur approach, outlined in Algorithm 1, is effective in cases when K binary codes are retrieved before r reaches r\u0302. It tackles the angular KNN problem by solving multiple instances of the (r1, r2)-near neighbour problem. An important advantage of the proposed algorithm is that it does not need to compute the actual sim values between binary codes. It can be efficiently implemented using bitwise operators and the popcount function. In the rest of this section, we address the case of r > r\u0302.\nWhen the search radius is greater than r\u0302, the sim value is not a monotonically decreasing function of the Hamming distance. However, we propose a sequential algorithm that can efficiently find the proper ordering of the tuples. Roughly speaking, at each step, the next tuple in the ordering can lie at many different Hamming distances. To tackle this issue, for each Hamming distance, among the tuples that are not selected so-far, we keep track of the tuple with\nAlgorithm 1 Fast Cosine Similarity Search (r \u2264 r\u2032) Input: K : number of nearest neighbours to retrieve, q:\nquery, H: hash table populated with binary codes Output: A: the set of retrieved items 1: Initialize set A = \u2205 2: Initialize integer r = 0 3: Initialize r\u0302 with the positive root of the equation r2 +\nr \u2212 ||q1|| 4: r\u0302 = \u230ar\u0302\u230b 5: while |A| < K and r \u2264 r\u0302 do 6: R = (0, r) 7: while R 6= (r + 1,\u22121) do 8: if R is a valid tuple then 9: Check the buckets in H that lie at the Hamming\ndistance tuple R from the query 10: Add each of the found candidates to A 11: R = R + (1,\u22121) 12: end if 13: end while 14: r = r + 1 15: end while\nthe highest sim value. This procedure relies on a priority queue of Hamming distance tuples, where the priority of each tuple is determined by its corresponding sim value. The queue is initialized with the tuple (0, r\u0302 + 1). When a tuple is pushed into the queue, it is considered as traversed. At each subsequent step, the tuple with the top priority (the highest sim value) is popped from the queue. When a tuple is popped, two tuples are considered for insertion into the queue. Hereafter, these are called the first anchor and the second anchor, respectively. These two tuples are checked, and if \u201cvalid\u201d and \u201cnot traversed\u201d, they are pushed into the queue.\nDEFINITION 6. (First and Second Anchors of a Tuple)\nGiven a query q and a Hamming distance tuple R = (x, y), the first anchor and the second anchor of R are defined as follows:\n\u2022 Among all tuples that lie at the Hamming distance x+y+1 from q, the tuple with the largest sim value is called the first anchor of R. \u2022 Among all tuples that lie at the Hamming distance x + y from q and have smaller sim values as compared to R, the tuple with the largest sim value is called the second anchor of R.\nEXAMPLE 3. For the query qwith ||q||1 = 10 and p = 32, the first anchor of t = (1, 4) is (0, 6) and the second anchor of t = (1, 4) is (2, 3) (according to the Proposition 1).\nWhen a tuple is popped from the queue, the algorithm pushes the first and the second anchors of the popped tuple into the priority queue (provided that these are valid, and not traversed) and marks them as traversed. This procedure continues until either K elements are retrieved, or all valid tuples are traversed. Therefore, when a tuple such as R = (x, y) is popped from the queue, the algorithm checks whether the following two tuples are valid or not:\n7 rq,bi0\u21921\nrq,bi1\u21920\nAll Hamming dis-\ntance tuples with r q,bi 1\u21920 + r q,bi 0\u21921 = 3\n\u2022\n\u2022\n\u2022 \u2022\n\u2022\n\u2022\n\u2022\n\u2022\n0 1 2 3 4 0\n1\n2\n3\n4\n2nd\n1st\nnot traversed tuples\nthe tuple popped at the current step\ntraversed tuples\nFigure 2 shows an example of the first/second anchors (shown in dashed circles) of a tuple that is selected in the current step (shown in green).\nAlgorithm 2 summarizes the steps of the proposed fast cosine similarity search in more details. Lines 9 to 22 are similar to Algorithm 1 and cover the case of r \u2264 r\u0302. Lines 27 to 29 remove the tuple with the highest sim value from the priority queue and check its corresponding entries in the hash table. Lines 30 to 37 push the first and second anchors of the current tuple to the priority queue. The algorithm terminates when K binary codes are retrieved or when all tuples are checked.\nNext, we prove that Algorithm 2 results in the correct ordering of Hamming distance tuples.\nProposition 3: In each iteration, the Hamming distance tuple popped from the queue has a smaller sim value as compared to the traversed tuples, and has the largest sim value among the\nAlgorithm 2 Fast Cosine Similarity Search\nInput: K : number of nearest neighbours to retrieve, q: query, H: hash table populated with binary codes Output: A: the set of retrieved items 1: Initialize set A = \u2205 2: Initialize integer r = 0 3: Create an empty priority queue pq 4: Marked all candidates as not traversed 5: Initialize r\u0302 with the positive root of the equation r2 +\nr \u2212 ||q||1 6: r\u0302 = \u230ar\u0302\u230b 7: R = (0, 0) 8: while |A| < K do 9: if r \u2264 r\u0302 then 10: if R is a valid tuple then 11: Check the buckets in H corresponding to the\nHamming distance tuple R (i.e., perform a (r1, r2)-near neighbour)\n12: Add the retrieved items to A 13: Mark the tuple (r1, r2) as traversed 14: R = R + (1,\u22121) 15: end if 16: if R = (r + 1,\u22121) then 17: r = r + 1 18: R = (0, r) 19: if r > r\u0302 then 20: pq.push(R) 21: end if 22: end if 23: else 24: if pq.isempty() then 25: return 26: end if 27: R \u2190 pq.pop() 28: Check the buckets in H corresponding to the Hamming distance tuple R 29: Add the retrieved items to A 30: if The first anchor of R is not traversed then 31: pq.push(the first anchor of R) 32: Mark the first anchor of R as a traversed 33: end if 34: if The second anchor of R is a valid tuple then 35: pq.push(the second anchor of R) 36: Mark the second anchor of R as traversed 37: end if 38: end if 39: end while\nnot traversed tuples. Moreover, the algorithm eventually traverses every tuple.\nProof: When r < r\u0302, then the algorithm is identical to the Algorithm 1, and according to the Propositions 1 and 2, the ordering is correct. For r \u2265 r\u0302, we show that the selected candidate has the highest cosine similarity among the remaining tuples.\nAssume that the Algorithm 2 is not correct. Let R be the first tuple that the algorithm selects incorrectly. This means another tuple, such as R\u2032 = (x\u2032, y\u2032), yields the highest sim value and it has not been pushed into the priority queue.\n8 The reason is that, if R\u2032 had been pushed into the queue, then R\u2032 would have been popped from the queue instead of R. Let r\u2032 = x\u2032+y\u2032, meaning that r\u2032 is the Hamming distance between q and any binary code lying at the Hamming distance tuple (x\u2032, y\u2032) from the q. Consider all binary codes that lie at the Hamming distance r\u2032 from q. If there exists a tuple with the second component greater than y\u2032 that has not been traversed so far, then a contradiction occurs (this means R\u2032 does not yield the largest sim value). This stems from the fact that, at a fixed Hamming distance, tuples with larger second components have larger sim values (Proposition 1). As a result, y\u2032 yields the largest possible value among the\nnot-traversed tuples lying at the Hamming distance r\u2032 from q. However, we show that, this tuple should have been pushed into the priority queue in previous steps. One of the following cases may occur:\n1) Until the current step, no Hamming distance tuple at the Hamming distance r\u2032 from q has been selected: According to the Proposition 1, all tuples with the Hamming distance r\u2032 \u2212 1 that are in the set L = {(a, b)|(a, b) is a valid tuple and, a + b = r\u2032 \u2212 1, a \u2264 x\u2032, b \u2264 y} have larger sim values than R\u2032, therefore, all of them must have been selected prior to R\u2032 in the sequence. However, the first time that a tuple from Lwas popped,R\u2032 was pushed into the priority queue. R\u2032 is in fact the first anchor of all the tuples in L, and thus, it must have been pushed when any of the elements in Lwas popped from the priority queue. 2) At least one Hamming distance tuple with the Hamming distance r\u2032 from q has been traversed in previous steps: Similar to the previous case, R\u2032 was pushed into the priority queue when the algorithm popped the tuple (x\u2032\u2212 1, y\u2032+1). In this scenario, R\u2032 is the second anchor of (x\u2032 \u2212 1, y\u2032 + 1).\nIt is concluded that R\u2032 must have been pushed into the priority queue during previous steps, which contradicts the assumption that R\u2032 is not a member of the priority queue.\nWe also need to prove that the algorithm is complete, i.e., the algorithm will not terminate until it either finds the K neighbours, or it traverses all the valid tuples. Again, let us assume the contrary. This means that, at the final step, the algorithm pops the last tuple from the queue and the last tuple does not have any valid anchors. Thus, the queue remains empty and the algorithm will terminate while there are some valid tuples that have not been traversed. It is clear that the not-traversed tuples cannot lie at the Hamming distance of r when at least one tuple with the Hamming distance r is traversed. The reason is that, once the first tuple with the Hamming distance r is popped from the queue, the second anchor of this tuple is pushed into the queue. Therefore, always one tuple with the Hamming distance r is in the queue until the last one of such tuples is popped, and such a last tuple does not have a valid second anchor. As a result, the only possible case is that all the tuples with a Hamming distance less than or equal to r have been traversed; and all of the tuples at a Hamming distance r+1 and greater have not been traversed. However, this is not possible. The reason is that, when a tuple at the Hamming distance r is popped from the queue, its first anchor is\n0 5 10 15 20\n104\n108\n1012\n1016\n1020\nK\n# b u ck et s ch\nec k ed\n(i n lo g 1 0 )\n64-bit 128-bit\nFig. 3: The average number of required bucket checks required to solve the angular KNN problem, if a single hash table is used for the SIFT dataset (with 109 items).\npushed into queue and such a tuple should lie the Hamming distance r+1. Hence, all the tuples at the Hamming distance r+1 from the query will be covered eventually. This verifies the completeness of the algorithm."}, {"heading": "6 ANGULAR MULTI-INDEX HASHING", "text": "Many applications require binary codes with large lengths (e.g., 64 bits) to achieve satisfactory retrieval accuracy. For such applications, it is not practical to use a single hash table with direct indices. For example, if 64-bit binary codes are used, then a hash table with 264 buckets is needed, which typically cannot be fitted in the RAM.\nA more crucial problem (in using larger binary codes with a single hash table) is the computational cost of the search. For long binary codes, we have n \u226a 2p and many of the buckets will be empty. As a consequence, to solve the KNN problem, even for small values of K , often the number of buckets to be examined exceeds the number of items in the dataset. This means that the exhaustive search (linear scan) is a faster alternative as compared to using a hash table. As Fig. 3 shows, the average number of buckets that should be checked to solve the angular KNN query for the SIFT dataset, which contains one billion items (details of SIFT will be explained later), is often more than the number of available binary codes in the dataset. This problem arises as the number of buckets to be examined (many of which are empty) grows near-exponentially with the values of rq,bi1\u21920 and r q,bi 0\u21921 (refer to (5)). In practice, to achieve a reasonable retrieval accuracy, the length of binary codes typically exceed 64 bits . To overcome this problem, recent studies have often resorted to linear search for binary codes longer than 64 bits [14], [40], [47].\nMulti-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes. The key idea behind the multi-index hashing is that, as many of the buckets are empty, one can merge the buckets over different dimensions of the Hamming space. To do this, instead of creating one huge hash table, MIH creates multiple smaller hash tables with larger buckets, where each bucket may be populated with more than one item. To do this, all binary codes are divided into smaller disjoint (usually with the same length) substrings, then each\n9 substring is indexed within its corresponding hash table. For example, for p = 64, MIH may divide the binary codes into four 16-bit substrings, and then index each one into a hash table with 216 buckets. Therefore, the number of buckets reduces to 4 \u00d7 216. More generally, if the number of hash tables is denoted by m, MIH reduces the number of required buckets from 2p to m2 p\nm (assuming tables with direct addressing are used).\nMore importantly, MIH reduces the computational cost of the search. To solve the R-near neighbour problem, the query is similarly partitioned into m substrings. Then, MIH solves m instances of the Rm -near neighbour problem, one per each hash table. By doing this, the neighbours of each substring in its corresponding hash table are retrieved to form a set of potential neighbours. Since some of the retrieved neighbours may not be a true R-near neighbour, a final pruning algorithm is used to remove the false neighbours. Interested readers are referred to [36] for further details and cost analysis.\nDespite of being efficient in storage and search costs, MIH cannot be applied to the angular preserving binary codes, since it is originally designed to solve the R-near neighbour problem in the Hamming space. In the rest of this section, we propose our Angular Multi-index Hashing (AMIH) technique for fast and exact search among angular preserving binary codes. Similar to many other search algorithms, AMIH includes two phases: (1) the indexing phase, and (2) the query phase. In the following, these two phases will be discussed in more details.\nInstead of populating one hash table with binary codes, AMIH creates multiple smaller hash tables. To populate such smaller hash tables, each binary code b \u2208 {0, 1}p is partitioned into m disjoint substrings b(1), . . . ,b(m). For the sake of simplicity, in the following, we assume that p is divisible by m and use the notation w = pm . As a result, the s-th hash table, s \u2208 {1, . . . , w}, is populated with b (s) i i \u2208 {1, . . . , n}. To retrieve the (r1, r2)-near neighbours of the query, q is similarly partitioned into m substrings, q(1), . . . ,q(m).\nThe following proposition establishes the relationship between the Hamming distance tuple of two binary codes and their substrings.\nProposition 4: If b lies at a Hamming distance tuple of at most (r1, r2) from q, then:\n\u2203 0 < t \u2264 m s.t. ||q(t) \u2212 b(t)||H \u2264 \u230a r1 + r2\nm \u230b\n\u2227 rq (t),b (t) i\n1\u21920 \u2264 r1 \u2227 rq (t),b (t) i\n0\u21921 \u2264 r2.\n(9)\nThe first condition follows from the Pigeon-hole principle. If in all of the m substrings, the Hamming distance is strictly greater than \u230a r1+r2m \u230b, then we have ||q \u2212 b||H \u2265 m(\u230a r1+r2m \u230b+ 1). This contradicts the assumption that b lies at a Hamming distance of at most r1+r2 from q. The second and the third conditions must in fact hold for all substrings. The reason is, if we have r q(t),b (t) i\n1\u21920 > r1, then we should\nhave rq,bi1\u21920 > r1. Similarly, if we have r q(t),b (t) i\n0\u21921 > r2, then we should have rq,bi0\u21921 > r2. This means b is not a (r1, r2)near neighbour of q.\n2 4 6\n2\n4\n6\n8 (3,8)\nr1 + r2 = 5\nr1\nr2 tuples to check query tuple\nFig. 4: The tuples that must be checked for solving the (3, 8)- near neighbour problem with 2 hash tables.\nIn simple terms, the Proposition 4 states that, if b is a (r1, r2)-near neighbour of q, then at least in one of its substrings t, b(t) must be a (r\u20321, r \u2032 2)-near neighbour of q\n(t), where r\u20321 + r \u2032 2 \u2264 \u230ar1+r2m \u230b, r\u20321 \u2264 r1 and r\u20322 \u2264 r2.\n6.1 (r1, r2)-near Neighbour Search Using Multi-index Hashing\nSo far, we have established the necessary condition that facilitate the search among substrings. At the query phase, to solve a (r1, r2)-near neighbour search, AMIH first generates the tuples that satisfy the conditions of the Proposition 4. That is, to solve the (r1, r2)-near neighbour problem, AMIH generates the set of all tuples (r\u20321, r \u2032 2) such that r\u20321 + r \u2032 2 \u2264 \u230a r1+r2m \u230b, where r\u20321 \u2264 r1 and r\u20322 \u2264 r2. This set is denoted by Tr1,r2,m. EXAMPLE 4. Suppose m = 2 and we are interested in\nsolving (3, 8)-near neighbour problem. According to the Proposition 4, we need to search among tuples with a Hamming distance at most 5 = \u230a3+82 \u230b that satisfy the conditions in the Proposition 4. These tuples are shown in Fig. 4. Note that, for each tuple, one should check all its corresponding buckets in each of the hash tables.\nNext, for each tuple such as t = (r\u20321, r \u2032 2) in Tr1,r2,m and for each substring q(s), s \u2208 {1, . . . ,m}, AMIH solves the (r\u20321, r \u2032 2)-near neighbour problem for the query q\n(s) in the sth hash table. This step results in a set of candidate binary codes, denoted by Oj,t. According to Proposition 4, the set O = \u22c3j,t Oj,t is the superset of (r1, r2)-near neighbours of q. Finally, AMIH computes the Hamming distance tuples between q and all candidates in O, discarding the tuples that are not the true (r1, r2)-near neighbours of q.\nThe intuition behind this approach is that, since the number of buckets that lie at the Hamming distance tuple (a, b) grows near-exponentially with the values of a and b, it is computationally advantageous to solve multiple instances of (a\u2032, b\u2032)-near neighbour problem with a\u2032 < a and b\u2032 < b, instead of solving one instance of (a, b)-near neighbour problem where a and/or b are relatively large. This requires a significantly smaller number of buckets to be checked as compared to the case of a single large hash table. In what\n10\nfollows, we show that AMIH can significantly reduce the query time in comparison with the linear scan."}, {"heading": "6.2 Cost Analysis", "text": "The cost analysis directly follows the performance analysis of MIH in [36]. As suggested in [36], we assume that \u230a plog2 n\u230b \u2264 m \u2264 \u2308 p log2 n\n\u2309. Using AMIH, the total cost per query consists of the number of buckets that should be checked to form the candidate set O, plus the cost of computing the Hamming distance tuple between the retrieved binary codes in O and q.\nWe start by providing an upper bound on the number of buckets that should be checked. Since the algorithm checks the identical buckets in each hash table, the number of bucket checks equals the product of m and the number of buckets that must be checked in a hash table.\nTo solve the (r1, r2)-near neighbour problem, for each tuple such as (a, b) in Tr1,r2,m, the algorithm checks the buckets that correspond to (a, b). It is clear that, in the ith hash table (1 \u2264 i \u2264 m), all binary codes corresponding to the tuples in the set Tr1,r2,m lie at a Hammming distance at most \u230a r1+r2m \u230b from the q(i) (Proposition 4). Therefore, in the i-th hash table, the indices of buckets that must be checked are a subset C(q(i), \u230ar1+r2m \u230b), and we can write:\n#buckets to check \u2264 m \u2211\ni=1\n|C(q(i), \u230ar1 + r2 m \u230b)|\n= m \u2211\ni=0\n\u230a r1+r2\nm \u230b\n\u2211\nj=0\n(\np/m\nj\n)\n= m\u00d7 \u230a r1+r2 m \u230b \u2211\nj=0\n(\nw\nj\n)\n= m\u00d7 \u230a w(r1+r2) p \u230b \u2211\nj=0\n(\nw\nj\n)\n.\n(10)\nAssuming that r1+r2p \u2264 1/2, we can use the following bound on the sum of the binomial coefficients [11]:\nFor any n \u2265 1 and 0 < \u03b1 \u2264 1/2, we have: \u230a\u03b1n\u230b \u2211\ni=0\n(\nn\ni\n)\n\u2264 2H(\u03b1)n. (11)\nwhere H(\u03b1) := \u2212\u03b1 log(\u03b1)\u2212 (1\u2212\u03b1) log(1\u2212\u03b1) is the binary entropy of \u03b1.\nTherefore, we can write:\n#buckets to check \u2264 m \u230a w(r1+r2) p \u230b \u2211\nj=0\n(\nw\nj\n)\n\u2264 m2wH( r1+r2 p ).\n(12) which is an upper bound on the number of buckets that must be checked. If binary codes are uniformly distributed in the Hamming space, the expected number of items per buckets of each hash table is n2w . Therefore, the expected number of items in the set O is:\nE(|O|) = n 2w m\u00d7 2wH( r1+r2 p ). (13)\nEmpirically, we observe that the cost of bucket lookup is only marginally higher than the cost of candidate test. If we have: single lookup cost= t\u00d7 single candidate test cost, for some t \u2265 1, then using (12) and (13), we can write the total cost as:\ncost \u2264 m2wH( r1+r2 p )(t+ n/2w). (14)\nFor m \u2248 p/ log2 n, by substituting log2 n for w, we have:\ncost = O( p\nlog2 n nH(\nr1+r2 p\n)). (15)\nFor reasonably small values of r1+r2p , the cost is sublinear in n. For example, for r1+r2p \u2264 0.1, the expected query cost would be O(q \u221a n/ logn).\nThe space complexity of AMIH comprises: 1) the cost of storing n binary codes each with p bits, which takes O(np), and 2) the cost of storing n pointers to database items in each hash table. Each pointer can be represented in O(log2 n) bits, therefore, the cost of storing pointers would be O(mn log2 n). For m = \u2308 plog2 n\u2309, the total storage cost is O(np+ n log2 n)."}, {"heading": "7 EXPERIMENT AND RESULTS", "text": "We implemented the AMIH in C++ on top of the MIH implementation provided by the authors of [36] (all codes are compiled with GCC 4.8.4). The experiments have been executed on a 2.0 GHz Xeon CPU with 256 Gigabytes of RAM. Note that different results reported here are obtained using a single core to render the time cost comparisons meaningful. We first compare the performance of LSH with linear scan, and then provide an implementationindependent comparison between AMIH and LSH."}, {"heading": "7.1 Datasets", "text": "In our experiments, we have used 2 non-synthetic datasets: SIFT: The ANN SIFT1B dataset [39] consists of SIFT descriptors. The available dataset is originally partitioned into 109 items as the base set, 104 items as the query set, and 108 items as the learning set. Each data item is a 128- dimensional SIFT vector. TRC2: The TRC2 (Thomas Reuters Text Research Collection 2) consists of 1,800,370 news stories covering a period of one year. We have used 5 \u00d7 105 news as the learning set,\n11\n106 news as the base set, and the remaining as the query set. We have preprocessed the data by removing common stop words, stemming, and then considering only the 2000 most frequent words in the learning set. Thus, each news is represented as a vector composed of 2000 word-counts.\nWe have used the angular-preserving mapping method called Angular Quantization-based Binary Codes (AQBC) proposed in [14], to create the dataset of binary codes. The MATLAB code for this method has been made publicly available by the authors of [14]. We have used the same code, with identical parameters and preprocessing steps, in all comparisons. For each dataset, the learning set is used to optimize the parameters of the hash function. Once learning is completed, the learning set is removed and the learned hash function is applied to the base and the query sets. The base set is used to populate the hash tables. Then, the angularKNN problem is solved for all queries in the query set and the average performance is reported."}, {"heading": "7.2 AMIH and Linear Scan Comparison", "text": "The result of linear scan technique is compared with AMIH in terms of the search speed.\nThe linear scan algorithm for angular preserving binary codes searches among 106 binary codes in less than 1 second, and among 109 binary codes in almost 100 seconds (109 seconds, to be exact). The norm of any binary code with p bits ranges from 0 to \u221a p. Thus, to increase the speed of the linear scan technique, we initialize a look up table with all the possible norm values. Moreover, as the term \u221a ||q||1 in\nthe denominator of (4) is independent of bi, there is no need to account for its value in performing the search.\nFigure 5 shows the average search speed per query using the linear scan technique for different database sizes. The plot indicates that the search time is almost independent of K (number of nearest neighbours). Consequently, for the sake of comparison, in the following, we use only the result of the linear scan technique for the 1NN problem. The linear scan technique can find the closest neighbour of a query within a dataset of 109 binary codes in almost 100 seconds, which is much faster as compared to using linear scan in the original space of SIFT vectors. Note that the linear scan technique can benefit from caching, as it performs sequential memory access. Otherwise, it would be much slower.\nFigures 6 and 7 show the average query time as a function of the database size for 64-bits and 128-bits binary codes. The value of m (number of hash tables) for the\nmulti-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36]. The leftmost graphs show the search time in seconds in terms of the data base size. It is clear that the AMIH technique is significantly faster as compared to the linear scan technique for a broad range of dataset sizes and K values. To differentiate between the performance of the AMIH technique for different values of K , the middle graphs show the zoomed version of the leftmost graphs, and the rightmost graphs are plotted using logarithmic scale. As Figs. 6 and 7 illustrate, for the linear scan technique, the query time grows linearly with the database size, whereas the query time of the AMIH increases with the square root of the size. This means, the difference between the query times of the two techniques is more significant for larger datasets. For instance, the linear scan technique spends more than 3 minutes to report the nearest neighbour in the 109 SIFT dataset with 128-bits codes, while AMIH takes about a quarter of a second. The dashed line on log-log plots shows the growth rate of the \u221a n up to a constant factor. The evident similarity between the slope of this function and that of the AMIH query time indicates that, in practice, and even for non-uniform distributions, AMIH can achieve sublinear search time.\nFigure 8 shows the percentage of queries for which Algorithm 2 enters the second case (r > r\u0302). As the size of the database grows, the number of empty buckets reduces, and the algorithm finds the nearest neighbours within a smaller search radius. Similarly, for shorter binary codes, the number of buckets reduces, and in turn, Algorithm 2 can retrieve items before the search radius reaches r\u0302.\nTable 2 includes the speed up factors achieved by using the AMIH technique in comparison to the linear scan technique. Each entry in the table indicates the average query time using linear scan over the average query time using AMIH for a specific value of K . Interestingly, the AMIH technique solves the angular KNN problem up to hundreds and even thousands of times faster than the linear scan technique. In particular, AMIH can solve the 100NN problem 138 times faster than the linear scan on a dataset of 109 binary codes each with 128 bits. While the linear scan technique does not rely on any indexing phase, the AMIH technique requires each binary code to be indexed inm hash tables. The indexing time for AMIH using the SIFT dataset is shown in Fig. 9. For 64 and 128 bits codes, the indexing phase takes about 1 and 2 hours, respectively. However, this operation is performed off-line, and only once."}, {"heading": "7.3 AMIH and LSH Comparison", "text": "To the best of our knowledge, due to the curse of dimensionality, linear scan is the fastest exact technique for solving the general angular KNN problem. However, a handful of approximation algorithms exist that provide sublinear search time for this problem. The most well-known representative is the LSH which offers a (provably) sublinear search time. In the case of the angular KNN, LSH uses a family of hash functions that (with high probability) map close items (in terms of the a specific similarity measure) to the same bucket of a hash table. One of the popular hash functions for the cosine similarity measure is the Simhash function [8] (also known as the hyperplane LSH) which uses sign random projection. Given a vector x, simhash utilizes a random vector w with each component generated from i.i.d normal distribution, wi \u223c N(0, 1), and only stores the sign of the projected data. Therefore, simhash is given by: hw(x) = sign(\u3008w,x\u3009). Given such a hash function, LSH first hashes all the points in the base set, and then determines the near neighbours by hashing the query point and retrieving items stored in the buckets containing the query point. Finally, LSH performs a brute force search over the retrieved points to find the K closest items to the query.\n13\nHere, we compare the performance of LSH and AMIH for the task of angular KNN over binary codes. Similar to AMIH, LSH first checks the buckets of multiple hash tables and then performs a candidate check procedure. We compare LSH and AMIH in terms of the number of operations required to solve the angular KNN problem. This includes \u201cchecking the buckets of the hash tables\u201d and \u201ccomputing the distance between the retrieved items and the query\u201d. We also take into account the number of comparisons that the priority queue of AMIH requires. Comparing LSH and AMIH in this manner can provide a deeper insight regarding the performance of these techniques, as it is implementation-independent.\nWe have implemented the standard (G,L) parametrized LSH [20] algorithm with the simhash as the hash function. In particular, we have created L compound hash functions where a compound hash function such as c is formed by putting together G randomly selected hash functions, c = [h1, . . . , hG]. In the preprocessing step, one hash table is created for each of the compound hash functions. At the query time, each query point is similarly hashed L times, and the elements stored in the buckets containing the query are retrieved. Then, the K closest points are selected by calculating the distance between the query point and all the retrieved items. To calculate the recall rate, the selected items by LSH algorithm are compared with the true K nearest neighbour of the query, and the percentage of true neighbours is reported. In the (G,L) LSH scheme, the ideal choice of G and L is dependent on the similarity thresholds and also on the hash function under consideration. An issue in using LSH is that the similarity threshold varies with the dataset. To ensure that the choice of these parameters does not effect our evaluations, we have tested all combinations of G \u2208 {1, 2, . . . , 20} and L = {8, 16, 24, 32, 40, 48, 56}. We have used multiples of 8 for the number of bits in the compound hash functions. This is particularly attractive because the hash values fit into fixed number of bytes. For each combination of G and L, we have computed the mean recall of LSH for the task of KNN, together with the average number of points reported per query which is in fact the number of comparison operations that LSH must perform. We have then selected the least number of comparisons (over choices of K and L) required to achieve a given percentage of recall.\nFigure 10 shows the average number of operations that AMIH and LSH perform. Both techniques are applied on 1 million binary codes; the original points are chosen randomly from the SIFT dataset, then AQBC algorithm [14] is applied to all points in the query and the base set. The top plots show the average recall rate in terms of the number of operations.\nThe bar charts indicate the number of operations required by the AMIH. In particular, three type of operations are shown: 1) comparisons between binary codes, 2) bucket check operations, and 3) the comparisons required by the priority queue\u2013the Total bars indicate the sum of all operations. Note that, since AMIH is an exact search technique, the recall rate is always 1. According to Fig. 10, AMIH clearly exhibits orders of magnitude improvement in comparison to LSH. Even for relatively small values of the recall rate (say 70%), AMIH requires significantly fewer\nnumber of operations."}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "This paper proposes a new algorithm for solving the angular KNN problem on large-scale datasets of binary codes. By treating binary codes as the memory addresses, our proposed algorithm can find similar binary codes in terms of the cosine similarity in a time that grows sublinearly with the size of dataset. To achieve this, we have first established a relationship between the Hamming distance and the cosine similarity. This relationship is in turn used to solve the angular KNN problem for applications where binary codes are used as the memory addresses of a hash table. However, using a hash table for long codes is often inferior to the linear scan due to the large number of empty buckets. To tackle this issue, as the second contribution, we have proposed the AMIH technique; a multi-indexing approach to reduce both computational and storage costs in comparison to using a single hash table. We have empirically showed that the AMIH technique can increase the search speed up to orders of magnitude when applied to large-scale datasets.\nMost of our effort has focused on finding nearest neighbours of a binary code in a hash table when codes are compared in terms of the cosine similarity. There are a number of other similarity measures to compare binary codes. One potential avenue for future work is to find fast search algorithms for other measures of similarity such as the spherical Hamming distance [18] and the weighted Hamming distance [51].\nWhile the search speed of the linear scan is only dependent on the size of the database, the AMIH search speed is dependent on the size, as well as the distribution of binary codes. Thus, applying AMIH to datasets with similar sizes would result in similar, but not identical, performances. Further, search speed analysis is required to reduce this threat. Also, the value of m in our experiments is chosen based on the value proposed for MIH in [36]. Since AMIH differs from MIH, this selection for the value of m may not be the optimal value for AMIH. Using cross validation methods to find better values for the parameter m can reduce this threat.\nIt has been shown that using asymmetric distance measures, such as the asymmetric Hamming distance, can boost the retrieval accuracy [16], [32]. Instead of hashing the query, these techniques compare the query point in the original space with the binary codes of the corresponding dataset. However, the higher accuracy often comes at the cost of a more complex search phase. One future line of work is to apply sublinear search algorithms for asymmetric hash functions."}], "references": [{"title": "Practical and optimal lsh for angular distance", "author": ["Alexandr Andoni", "Piotr Indyk", "Thijs Laarhoven", "Ilya Razenshteyn", "Ludwig Schmidt"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "The inverted multi-index", "author": ["Artem Babenko", "Victor Lempitsky"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["Jon Louis Bentley"], "venue": "Communications of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Transform coding for fast approximate nearest neighbor search in high dimensions", "author": ["Jonathan Brandt"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Efficient large-scale sequence comparison by locality-sensitive", "author": ["Jeremy Buhler"], "venue": "hashing. Bioinformatics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Hashing with binary autoencoders", "author": ["Miguel A. Carreira-Perpinan", "Ramin Raziperchikolaei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Support vector machines for histogram-based image classification", "author": ["Olivier Chapelle", "Patrick Haffner", "Vladimir N Vapnik"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S Charikar"], "venue": "ACM Symposium on Theory of Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S Mirrokni"], "venue": "Annual Symposium on Computational Geometry,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Imagenet: crowdsourcing, benchmarking & other cool things", "author": ["L Fei-Fei"], "venue": "CMU VASC Seminar,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Parameterized Complexity Theory", "author": ["J. Flum", "M. Grohe"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["Jerome H. Friedman", "Jon Louis Bentley", "Raphael Ari Finkel"], "venue": "ACM Transactions on Mathmatical Software,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1977}, {"title": "Similarity search in high dimensions via hashing", "author": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Yunchao Gong", "Sanjiv Kumar", "Vishal Verma", "Svetlana Lazebnik"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Yunchao Gong", "Svetlana Lazebnik", "Albert Gordo", "Florent Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Asymmetric distances for binary embeddings", "author": ["Albert Gordo", "Florent Perronnin", "Yunchao Gong", "Svetlana Lazebnik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Multi-index hashing for information retrieval", "author": ["Dan Greene", "Michal Parnas", "Frances Yao"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Spatial query integrity with voronoi neighbors", "author": ["Ling Hu", "Wei-Shinn Ku", "Spiridon Bakiras", "Cyrus Shahabi"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "Annual ACM Symposium on Theory of Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "What is the most efficientway to select nearest neighbor candidates for fast approximate nearest neighbor search", "author": ["Mikio Iwamura", "Takao Sato", "Kenji Kise"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Big data and its technical challenges", "author": ["HV Jagadish", "Johannes Gehrke", "Alexandros Labrinidis", "Yannis Papakonstantinou", "Jignesh M Patel", "Raghu Ramakrishnan", "Cyrus Shahabi"], "venue": "Communications of the ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Towards optimal bag-of-features for object categorization and semantic video retrieval", "author": ["Yu-Gang Jiang", "Chong-Wah Ngo", "Jun Yang"], "venue": "International Conference on Image and Video Retrieval,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Visualrank: Applying pagerank to large-scale image search", "author": ["Yushi Jing", "Shumeet Baluja"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Continuous knearest neighbor queries in spatial network databases", "author": ["Mohammad R Kolahdouzan", "Cyrus Shahabi"], "venue": "Spatio- Temporal Database Management,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Brian Kulis", "Trevor Darrell"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Multi-probe lsh: efficient indexing for high-dimensional similarity search", "author": ["Qin Lv", "William Josephson", "ZheWang", "Moses Charikar", "Kai Li"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Multimodal similarity-preserving hashing", "author": ["Jonathan Masci", "Michael M Bronstein", "Alexander Bronstein", "Jurgen Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Aziawa. Pqtable: Fast exact asymmetric distance neighbor search for product quantization using hash tables", "author": ["Yusuke Matusi", "Toshihiko Yamasaki", "Kiyoharu"], "venue": "International Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Quantitative analysis of culture using millions of digitized", "author": ["Jean-Baptiste Michel", "Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "The power of asymmetry in binary hashing", "author": ["Behnam Neyshabur", "Nati Srebro", "Ruslan R Salakhutdinov", "Yury Makarychev", "Payman Yadollahpour"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David M Blei"], "venue": "International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["Mohammad Norouzi", "David M Blei", "Ruslan R Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Fast exact search in hamming space with multi-index hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": "Technical Report 1999-66,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Entropy based nearest neighbor search in high dimensions", "author": ["Rina Panigrahy"], "venue": "Annual ACM-SIAM symposium on Discrete algorithm,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["Lo\u0131\u0308c Paulev\u00e9", "Herv\u00e9 J\u00e9gou", "Laurent Amsaleg"], "venue": "Pattern Recognition Letters,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Foundations of multidimensional and metric data structures", "author": ["Hanan Samet"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Bayesian locality sensitive hashing for fast similarity search", "author": ["Venu Satuluri", "Srinivasan Parthasarathy"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Sparse spectral hashing", "author": ["Jian Shao", "Fei Wu", "Chuanfei Ouyang", "Xiao Zhang"], "venue": "Pattern Recognition Letters,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "In defense of minhash over simhash", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Asymmetric minwise hashing for indexing binary inner products and set containment", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "International Conference on World Wide Web,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Antonio Torralba", "Rob Fergus", "William T Freeman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1958}, {"title": "Small codes and large image databases for recognition", "author": ["Antonio Torralba", "Rob Fergus", "Yair Weiss"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "International Conference on Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces", "author": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "venue": "International Conference on Very Large Data bases,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1998}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Binary code ranking with weighted hamming distance", "author": ["Lei Zhang", "Yongdong Zhang", "Jinhu Tang", "Ke Lu", "Qi Tian"], "venue": "IEEE 15 Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Topology preserving hashing for similarity search", "author": ["Lei Zhang", "Yongdong Zhang", "Jinhui Tang", "Xiaoguang Gu", "Jintao Li", "Qi Tian"], "venue": "International Conference on Multimedia,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "For example, in text mining, Google provides N-gram data obtained from over 5 million books [31].", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 61, "endOffset": 65}, {"referenceID": 37, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Dealing with such large-scale datasets brings up unprecedented challenges, in turn necessitating new software tools, algorithms and databases for data acquisition, storage, transmission and retrieval [22].", "startOffset": 200, "endOffset": 204}, {"referenceID": 14, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 179, "endOffset": 183}, {"referenceID": 17, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 237, "endOffset": 241}, {"referenceID": 24, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 243, "endOffset": 247}, {"referenceID": 13, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 122, "endOffset": 126}, {"referenceID": 45, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 128, "endOffset": 132}, {"referenceID": 47, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 134, "endOffset": 138}, {"referenceID": 49, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "For example, in the Bag of Words (BoW) representation of the documents and images, the presence or absence of information, captured in terms of a binary variable, quantifies the items [7], [24].", "startOffset": 184, "endOffset": 187}, {"referenceID": 22, "context": "For example, in the Bag of Words (BoW) representation of the documents and images, the presence or absence of information, captured in terms of a binary variable, quantifies the items [7], [24].", "startOffset": 189, "endOffset": 193}, {"referenceID": 13, "context": "In practice, instead of using a hash table, researchers resort either to the brute force search [14], or to approximate similarity search techniques, such as LSH [20], to solve the angular KNN problem.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "In practice, instead of using a hash table, researchers resort either to the brute force search [14], or to approximate similarity search techniques, such as LSH [20], to solve the angular KNN problem.", "startOffset": 162, "endOffset": 166}, {"referenceID": 16, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 38, "endOffset": 42}, {"referenceID": 34, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 44, "endOffset": 48}, {"referenceID": 33, "context": "Motivated by the MIH technique proposed in [35], we develop the Angular Multi-Index Hashing technique to realize similar advantages.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44].", "startOffset": 115, "endOffset": 118}, {"referenceID": 41, "context": "A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Indeed, to address this issue, Google uses the VisualRank algorithm [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "The general idea of the VisualRank algorithm is to first find all the images that match the query by analyzing their corresponding tags, meta data, image names, and other related features, using the PageRank algorithm [37].", "startOffset": 218, "endOffset": 222}, {"referenceID": 2, "context": "Perhaps the most known example of such techniques is the kd-tree [3] with a worst-case search time of O(dn).", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Following [3], many other tree-based indexing algorithms are proposed, such as R-tree, R*-tree, SS-tree, SRtree, and X-tree (see [41] for an overview).", "startOffset": 10, "endOffset": 13}, {"referenceID": 38, "context": "Following [3], many other tree-based indexing algorithms are proposed, such as R-tree, R*-tree, SS-tree, SRtree, and X-tree (see [41] for an overview).", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Typically, the kd-tree and its variants are efficient for dimensions less than 20 [12], [49].", "startOffset": 82, "endOffset": 86}, {"referenceID": 46, "context": "Typically, the kd-tree and its variants are efficient for dimensions less than 20 [12], [49].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 86, "endOffset": 89}, {"referenceID": 12, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 91, "endOffset": 95}, {"referenceID": 39, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 97, "endOffset": 101}, {"referenceID": 47, "context": "To achieve an acceptable retrieval accuracy, the number of hash tables, as well as the number of buckets in each hash table, should be relatively large [50], [51].", "startOffset": 152, "endOffset": 156}, {"referenceID": 48, "context": "To achieve an acceptable retrieval accuracy, the number of hash tables, as well as the number of buckets in each hash table, should be relatively large [50], [51].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "Experimental studies indicate that LSH-based techniques need over a hundred [13] and sometimes several hundreds hash tables [5].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Experimental studies indicate that LSH-based techniques need over a hundred [13] and sometimes several hundreds hash tables [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 36, "context": "Entropy-based LSH [38] and Mutli-Prob LSH [28] are two well-known techniques to reduce the number of hash tables.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "Entropy-based LSH [38] and Mutli-Prob LSH [28] are two well-known techniques to reduce the number of hash tables.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": "[50] have proposed an eigenvector formulation which aims at finding short similarity preserving binary codes, such that bits are uncorrelated and balanced.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Binary Reconstructive Embedding (BRE) [27] uses a loss function that penalizes the difference between the Euclidean distance in the input space and the Hamming distance in the binary space.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "[33] have used a hinge-like loss function to learn the codes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "In a follow up work, Norouzi et al [34] have used a triple objective function to learn the binary codes for supervised problems.", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "More recently, Carreira-Perpinan and Raziperchikolaei [6] have used autoencoder to learn binary codes in order to minimize the reconstruction error.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 104, "endOffset": 108}, {"referenceID": 40, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 110, "endOffset": 114}, {"referenceID": 45, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 116, "endOffset": 120}, {"referenceID": 47, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 122, "endOffset": 126}, {"referenceID": 49, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 128, "endOffset": 132}, {"referenceID": 14, "context": "For unlabeled data, if the Euclidean distance is chosen as the measure of similarity, binary codes are usually compared in terms of the Hamming distance (which is equal to the squared Euclidean distance in the binary space) [15], [27].", "startOffset": 224, "endOffset": 228}, {"referenceID": 25, "context": "For unlabeled data, if the Euclidean distance is chosen as the measure of similarity, binary codes are usually compared in terms of the Hamming distance (which is equal to the squared Euclidean distance in the binary space) [15], [27].", "startOffset": 230, "endOffset": 234}, {"referenceID": 13, "context": "In such applications, input vectors are compared in terms of the cosine similarity, and consequently, some techniques use the cosine similarity of binary codes as the similarity measure in the binary space [14], [45].", "startOffset": 206, "endOffset": 210}, {"referenceID": 42, "context": "In such applications, input vectors are compared in terms of the cosine similarity, and consequently, some techniques use the cosine similarity of binary codes as the similarity measure in the binary space [14], [45].", "startOffset": 212, "endOffset": 216}, {"referenceID": 1, "context": "[2] have proposed the inverted multi-index technique to solve the ANN problem where data points are estimated with codewords of multiple codebooks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[30] have used multiple hash tables to reduce the search time, whereby the distance between items are approximated by codewords of multiple codebooks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] have proposed a non-exhaustive search algorithm based on the branch and bound paradigm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "To overcome this problem, recent studies have often resorted to linear search for binary codes longer than 64 bits [14], [40], [47].", "startOffset": 115, "endOffset": 119}, {"referenceID": 44, "context": "To overcome this problem, recent studies have often resorted to linear search for binary codes longer than 64 bits [14], [40], [47].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "Multi-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes.", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "Multi-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes.", "startOffset": 49, "endOffset": 53}, {"referenceID": 34, "context": "Multi-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "Interested readers are referred to [36] for further details and cost analysis.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "The cost analysis directly follows the performance analysis of MIH in [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "As suggested in [36], we assume that \u230a p log2 n\u230b \u2264 m \u2264 \u2308 p log2 n \u2309.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Assuming that r1+r2 p \u2264 1/2, we can use the following bound on the sum of the binomial coefficients [11]: For any n \u2265 1 and 0 < \u03b1 \u2264 1/2, we have:", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "We implemented the AMIH in C++ on top of the MIH implementation provided by the authors of [36] (all codes are compiled with GCC 4.", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "In our experiments, we have used 2 non-synthetic datasets: SIFT: The ANN SIFT1B dataset [39] consists of SIFT descriptors.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "We have used the angular-preserving mapping method called Angular Quantization-based Binary Codes (AQBC) proposed in [14], to create the dataset of binary codes.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "The MATLAB code for this method has been made publicly available by the authors of [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "multi-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36].", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "multi-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36].", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "multi-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36].", "startOffset": 92, "endOffset": 96}, {"referenceID": 7, "context": "One of the popular hash functions for the cosine similarity measure is the Simhash function [8] (also known as the hyperplane LSH) which uses sign random projection.", "startOffset": 92, "endOffset": 95}, {"referenceID": 18, "context": "We have implemented the standard (G,L) parametrized LSH [20] algorithm with the simhash as the hash function.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Both techniques are applied on 1 million binary codes; the original points are chosen randomly from the SIFT dataset, then AQBC algorithm [14] is applied to all points in the query and the base set.", "startOffset": 138, "endOffset": 142}, {"referenceID": 48, "context": "One potential avenue for future work is to find fast search algorithms for other measures of similarity such as the spherical Hamming distance [18] and the weighted Hamming distance [51].", "startOffset": 182, "endOffset": 186}, {"referenceID": 34, "context": "Also, the value of m in our experiments is chosen based on the value proposed for MIH in [36].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "It has been shown that using asymmetric distance measures, such as the asymmetric Hamming distance, can boost the retrieval accuracy [16], [32].", "startOffset": 133, "endOffset": 137}, {"referenceID": 30, "context": "It has been shown that using asymmetric distance measures, such as the asymmetric Hamming distance, can boost the retrieval accuracy [16], [32].", "startOffset": 139, "endOffset": 143}], "year": 2016, "abstractText": "Due to rapid development of the Internet, recent years have witnessed an explosion in the rate of data generation. Dealing with data at current scales brings up unprecedented challenges. From the algorithmic view point, executing existing linear algorithms in information retrieval and machine learning on such tremendous amounts of data incur intolerable computational and storage costs. To address this issue, there is a growing interest to map data points in large-scale datasets to binary codes. This can significantly reduce the storage complexity of large-scale datasets. However, one of the most compelling reasons for using binary codes or any discrete representation is that they can be used as direct indices into a hash table. Incorporating hash table offers fast query execution; one can look up the nearby buckets in a hash table populated with binary codes to retrieve similar items. Nonetheless, if binary codes are compared in terms of the cosine similarity rather than the Hamming distance, there is no fast exact sequential procedure to find the K closest items to the query other than the exhaustive search. Given a large dataset of binary codes and a binary query, the problem that we address is to efficiently find K closest codes in the dataset that yield the largest cosine similarities to the query. To handle this issue, we first elaborate on the relation between the Hamming distance and the cosine similarity. This allows finding the sequence of buckets to check in the hash table. Having this sequence, we propose a multi-index hashing approach that can increase the search speed up to orders of magnitude in comparison to the exhaustive search and even approximation methods such as LSH. We empirically evaluate the performance of the proposed algorithm on real world datasets.", "creator": "LaTeX with hyperref package"}}}