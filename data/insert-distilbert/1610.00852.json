{"id": "1610.00852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author Age Prediction", "abstract": "seeing the evolution of the internet has created an abundance of unstructured data data on the web, a significant part of which is textual. the task of author profiling seeks to find the demographics of people solely from their particular linguistic and content - based features in text. the comprehensive ability to really describe traits of authors clearly has applications in diverse fields such as security and forensics, insurance as well as marketing. instead of seeing age as just a classification problem, we themselves also propose frame age as a regression one, but use an ensemble chain link method that incorporates the power of both classification and regression to also learn the authors exact age.", "histories": [["v1", "Tue, 4 Oct 2016 06:01:03 GMT  (187kb)", "http://arxiv.org/abs/1610.00852v1", "6 pages, 4 figures"]], "COMMENTS": "6 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["joey hong", "chris mattmann", "paul ramirez"], "accepted": false, "id": "1610.00852"}, "pdf": {"name": "1610.00852.pdf", "metadata": {"source": "CRF", "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author Age Prediction", "authors": ["Joey Hong", "Chris Mattmann", "Paul Ramirez"], "emails": ["jhhong@caltech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 85\n2v 1\n[ cs\n.L G\n] 4\nO ct\n2 01\nKeywords author profiling, age prediction, maximum entropy, regression, natural-language processing"}, {"heading": "1. INTRODUCTION", "text": "The rapid expansion of the Web is coupled with a startling increase in communication among members of the world via blogs, emails, forums, and social media platforms. Thus, it has become of increasing interest to analyze the authorship of Internet content, and be able to use this massive data to predict characteristics If a person from their use of language. In the task of author profiling, such characteristics can vary along dimensions of age, gender, and personality type, though we will focus on age, which has conventionally been the most difficult [15]. To be able to do so with high predictive power will have applications in a wide spectrum of fields, from law enforcement, to focused advertising. For example, such technology could be used to evaluate the truthfulness of self-reported ages in forums, and detect underage and illegal trading of products.\nIn this paper, we introduce chained heuristic solution to author profiling, using both classification and regression to predict author age. As in any Machine Learning algorithm involving text, we first extract relevant textual features and find a suitable representation of documents. We use Natural Language Processing techniques to extract content-based and stylistic features from text, and vectorize the frequencies of such aggregated features for each document.\n\u2217Undergraduate at Caltech Class of 2019. JPL Data Science Intern for Summer 2016. \u2020JPL Co-mentor. \u2021JPL Co-mentor.\nOur next step is using the vectorized documents in a Maximum Entropy Classifier, which has proven to be a viable and competitive algorithm in the NLP domain. Author age profiling becomes a problem of multi-class classification into age groups: xx-17|18-24|25-34|35-49|50-64|65-xx. We will analyze the effects of different feature vectors on the accuracy of MaxEnt classification in the Section 4.\nUsing the predicted age category, we transform the classification problem by using linear regression to map age to a continuous variable rather than a categorical one. Our regression implementation is simple LASSO regression using the same document vectors, but we add the predicted category from classification as another feature, so that the result from classification also influences what age is predicted. In Section 4, we will compare the performance of our chained method to that of ordinary linear regresssion, as outlined in Nguyen et al. [10].\nThe paper will proceed as follows. A survey of related work is presented in Section 2. A mathematical formulation of the MaxEnt model and other theoretical techniques is in Section 3. Section 4,5 overviews the datasets used and the feature extraction process. In Section 6 we will explain in detail our methodology and implementation, and we will discuss the results of our experiment in Section 7,8. Finally, in Section 9 we draw conclusions and present future plans for extending the age prediction task."}, {"heading": "2. RELATED WORK", "text": "There has been considerable work recently in the task of author profiling. In feature selection, Houvardas and Stamatatos in 2006 showed extracting N-grams as an effective feature for classifying in the domain of author profiling [8]. In 2007, Estival et al. experimented with linguistic feature selection in author profiling, and revealed POS frequencies as a viable feature [6]. Calix et al. performed author matching from emails using 55 different stylistic features, along with a K-nearest Neighbors implementation, to achieve 76.72% accuracy [3].\nSpecifically on age identification, Pennebaker and Stone used LIWC to find a relation between the use of language as a person ages [14]. The problem has, however, been almost exclusively modeled as a classification problem, with a variety of machine learning methods ranging from Bayesian Methods, to Support Vector Machines and Random Forests. In 2011, Nguyen et al. were the first to treat age as a continuous variable via linear regression, and experimented with regression on various individual corpora [10]. As far as we\nare aware, though, we are the first to attempt a chained ensemble method of classification and regression to predict exact age."}, {"heading": "3. MAXIMUM ENTROPY CLASSIFIERS", "text": "Maximum Entropy classification is frequently used in NLP tasks such as POS tagging and sentence boundary segmentation. Due to its success with textual features, we decided to implement a MaxEnt classifier to perform age categorization. We will derive the foundations of the MaxEnt model in NLP, as well as the Generalized Iterative Scaling algorithm employed by Apache OpenNLP to solve the model.\nWe let p\u0303(c, d) denote the empirical probability of a training sample (c, d) with document vector d and category c, and N be the size of the training set. We also introduce a function for each feature fi,j in our document vector representation, such that:\nfi,j(c, d) =    N(wi)\u2211 l N(wl) , if c = cj , and d contains wi,\n0, otherwise.\nwhere cj denotes the possible categories, and wi the possible context tokens, and N() is a real valued function for counts within the document. Apache OpenNLP uses binarized document vectors by default, though we evidently edited it to use normalized frequency counts.\nSince each feature fi,j is a real valued function, it must have an expected value, calculated empirically by the training data:\np\u0303(fi,j) = \u2211\nc,d\np\u0303(c, d)fi,j(c, d).\nWe can also calculate the actual expected value p(fi,j) from conditional probability:\np(fi,j) = \u2211\nc,d\np(d)p(c | d)fi,j(c, d).\nWe look to restrain out MaxEnt model\u2019s conditional distribution to satisfy the empirical value, so that,\n\u2211\nc,d\np(d)p(c | d)fi,j(c, d) = 1\nN\n\u2211\nc,d\np(c | d)fi,j(c, d)\n= \u2211\nc,d\np\u0303(c, d)fi,j(c, d) = 1\nN\n\u2211\nc,d\nfi,j(c, d),\nwhere we approximate p(d) = p\u0303(d) = 1/N . Thus, we want to find a conditional distribution for our model p which satisfies:\n1. p(c | d) \u2265 0 \u2200c, d. 2. \u2211\nc p(c | d) = 1 \u2200d.\n3. \u2211\nc,d p(c | d)fi,j(c, d) = \u2211 c,d fi,j(c, d) \u2200i, j.\nFrom information theory, we know that the optimal model p\u2217 which satisfies the above constraints, is as close to uniform as possible, and is achieved by maximizing the entropy,\np\u2217 = max p\n( \u2212 \u2211\nc,d\np(c | d) log p(c | d) ) .\nIf we set up and solve the Lagrangian [2], we get the multiclass version of the sigmoid function in logistic regression as\nour optimal solution,\np\u2217(c | d) = exp(\n\u2211 i,j\n\u03bbi,jfi,j(c, d))\u2211 c exp( \u2211 i,j \u03bbi,jfi,j(c, d)) ."}, {"heading": "3.1 Generalized Iterative Scaling", "text": "Estimating the optimal \u03bb parameters can be easily done with an iterative scaling algorithm. OpenNLP employs the simple GIS algorithm to do so.\nIt is guaranteed that no local maxima exist in the likelihood surface, so hillclimbing algorithms from any random initial distribution will find the global maximum entropy solution. We use the log likelihood as with Della Pietra et al. in 1997 [2]. Let \u03bb be the parameter vector of all \u03bbi,j values, and p\u03bb be the resulting probability model. The log likelihood is given by,\n\u2113(\u03bb) = log \u220f\nd\np\u03bb(c |d)\n= \u2211\nd\n\u2211\ni,j\n\u03bbi,jfi,j(c, d)\n\u2212 \u2211\nd\nlog \u2211\nc\nexp (\u2211\ni,j\n\u03bbi,jfi,j(c, d) ) .\nAs in any hillclimbing algorithm, we seek \u2206\u03bb such that \u2113(\u03bb+\u2206\u03bb)\u2212\u2113(\u03bb) > 0. From Jensen\u2019s inequality, we can lower bound the difference by an auxiliary function B [11],\n\u2113(\u03bb+\u2206\u03bb)\u2212 \u2113(\u03bb) \u2265 B =\n1 + \u2211\nd\n\u2211\ni,j\n\u2206\u03bbi,jfi,j(c, d)\n\u2212 \u2211\nc\np\u03bb(c | d)exp ( f#(c, d)\u2206\u03bbi,j \u2211\ni,j\nfi,j(c, d) f#(c, d)\n) ,\nwhere f#(c, d) = \u2211\ni,j fi,j(c, d) is the sum over all features.\nIt becomes easy to find \u2206\u03bb by merely differentiating B with respect to \u2206\u03bbi,j , and setting it equal to zero to maximize B.\n\u2202B\n\u2202\u2206\u03bbi,j =\n\u2211\nd\nfi,j(c, d)\n\u2212 \u2211\nc\np\u03bb(c | d)fi,j(c, d)exp(\u2206\u03bbi,jf #(c, d)).\nSo, the GIS algorithm effectively solves for \u2206\u03bbi,j and increments for each parameter \u03bbi,j .\n1. Input: Set of labelled documents d \u2208 D and feature functions fi,j . 2. Initialize \u03bbi,j = 0 for all i, j. 3. Repeat until convergence: 4. For each \u03bbi,j : 5. Let \u2206\u03bbi,j = 1\nf#(c, d) log\np\u0303(fi,j) p(fi,j) .\n6. Set \u03bbi,j \u2190 \u03bbi,j +\u2206\u03bbi,j .\nThus, the resulting \u03bb parameters make up our MaxEnt classifier, as desired."}, {"heading": "4. DATA", "text": "We use labelled data from four major datasets, which were divided via a 90-10 split into a training and test set. Figure 1 outlines the distribution of the three datasets with exact ages, and Table 1 shows the distribution of the PAN16 dataset, with only categorized ages.\nPosts were not counted for the first three corpora since all posts from a single user were aggregated and may vary widely in post length, whereas the PAN16 dataset, which contains solely Twitter data, has relatively uniform post lengths."}, {"heading": "4.1 Blog Authorship Corpus", "text": "The corpus was the result of crawled blogs (blogger.com) by Schler et al. in 2004 [1]. The corpus consists of posts by 19,320 bloggers, and 681,288 posts, labeled with blogger id, and self-provided gender and age, as well as industry and/or astrological sign when available.\nThe blogs were formatted in an XML, and we extracted all the text in the files belonging to a user, and aggregated them to a single labelled post."}, {"heading": "4.2 Fisher English Transcripts", "text": "The Fisher CALLHOME dataset contains transcripts of telephone conversations between random pairs of people, where a truth file was provided with characteristics such as age and gender of each speaker [5]. Each telephone conversation was regarding a pre-specified topic, which greatly varied over all the recorded conversations.\nWe parsed the truth file, and used a script to aggregate all the text among the transcripts belonging to the individual into a single sample."}, {"heading": "4.3 Cancer Forum", "text": "Posts and user profiles from an online forum for persons with breast cancer (community.breastcancer.org) were crawled by Nguyen et al. in 2011 [10]. The age of the users were either ascertained from context in their posts, or manually annotated by looking at their profiles. Posts from a total of 1,996 users were recorded.\nAgain, we aggregated all posts from a single user into one labelled post per user."}, {"heading": "4.4 PAN Dataset", "text": "The PAN16 dataset is comprised of tweets from twitter users, along with an annotated age and gender truth set. The Twitter corpus was compiled by looking at LinkedIn profiles, and finding ones with a corresponding Twitter account. Age was determined either by a provided birth date in the profile, or via an estimation with the degree starting date in the education section [15].\nDue to Twitter terms of service, only the URLs to each profile\u2019s tweets were provided, and a script was used to download the tweets, and another was written to match downloaded tweets with the provided truth set.\nAges are labelled with categories: 18-24|25-34|35-49|50- 64|65-xx. Because of this, the PAN16 dataset was only used in the training data for our classification task."}, {"heading": "5. FEATURES", "text": ""}, {"heading": "5.1 Content-based Features", "text": "It can be assumed that since people of different ages will speak about different topics, the usage of words between ages can be a distinguishing feature in their written text. Since content can be represented as an N-gram feature, we compiled unigrams and bigrams from each set. For unigrams, we put all the words through a list of 319 English stopwords, and removed all occurrences of stopwords, which provide no meaningful content.\nFor both classification and regression, for each of the features, we calculated the frequency of appearance in the document, and in creating a vector for each document, used the p-normalized counts in L1 space."}, {"heading": "5.2 Stylistic Features", "text": "Style based features include sentence and word counts, punctuation counts, and distribution of Parts of Speech unigrams and bigrams in the document. All of the stylometry was done via Apache OpenNLP, where a custom tokenizer was written to tokenize on whitespace and punctuation, which were consequently fed into OpenNLP\u2019s POS tagger for a POS count.\nWe also counted occurrences of words that belong to LIWC word classes [13], including personal words (\u201cI\u201d,\u201cme\u201d,\u201cmine\u201d, etc.), positive (\u201ccheery\u201d, \u201cbliss\u201d, \u201cjoy\u201d, etc.) and negative (\u201cabandoned\u201d, \u201cbad\u201d, \u201churt\u201d, etc.) sentiment words, and quantifier words (\u201cmany\u201d, \u201cfew\u201d, \u201cboth\u201d, etc.). A total of word lists for 9 word classes were compiled manually.\nWhile originally, lemmatization and stemming were employed on all words in the document, in order to preserve stylistic abnormalities, we did not use those tools in the tokenization process. Like with content-based features, the document vectors used normalized counts."}, {"heading": "5.3 Normalization", "text": "To curb the feature count, and thereby keep the training time of the model in a feasible range, we implemented a Chi-Squared Selector to work alongside OpenNLP\u2019s data indexer, and transform the input features into a reduced feature space. We employed a one-vs-many transformation, calculating the \u03c7\u03032 for each category relative to all other age categories. We kept the critical value at \u03c7\u03032 = 2.71, which meant the assumption of independence could be rejected with 90% confidence."}, {"heading": "6. ARCHITECTURE", "text": "Figure 2 outlines the pipeline of the algorithm, which we will describe in detail. Initially, the text data from all the data sources are aggregated by user. Since most datasets were publicly released, no other external work aside from reformatting to match the input requirements of our classifier and regression trainer was needed."}, {"heading": "6.1 Data Preprocessing", "text": "As stated, most datasets required no significant cleaning. The PAN16 twitter data, however, was raw XML text and required cleaning tags and removing non-encodable words. We used regex filtering to replace all outlinks with \u201curllink\u201d, and removed all hashtags and mentions from the data. The cleaned data can then be formatted into the same input\nrequirements. Since the compiled datasets had a bias towards people in the 20\u2019s, we used oversampling to duplicate all posts from infrequent age classes and correct for the bias."}, {"heading": "6.2 Feature Selection", "text": "We used Apache OpenNLP\u2019s POS tagging and sentence detector tools to extract stylistic features from the formatted data. Our custom tokenizer separated based on both whitespace and punctuation, and boundaries for sentences. The tokenized text was indexed into a count vectorizer to extract features such as # of sentences in document, words in sentence, and punctuation classes.\nUsing the tokenized text, unigrams and bigrams were extracted. Unigrams were fed into a stopwords filter, where all tokens matching stopwords in a preset list were removed. We also counted occurrences of unigram words in each of our manually created LIWC word classes, and calculated the distribution of frequencies.\nWe then pipelined the tokens into the POS tagger to generate POS unigrams and bigrams, and a frequencies were again extracted with the POS tags. Finally, all feature frequencies were normalized on length of document.\nFeature extraction was done using feature selectors from Apache OpenNLP, and also parallelized on Apache Spark\u2019s distributed dataset framework."}, {"heading": "6.3 Classification", "text": "For classification, all the features were used, but filtered via statistical feature selection methods. We only selected features that occurred at least 10 times among the training documents. Our normalized document vectors were also put through a Chi-Squared feature selector, so only features with 90% confidence of dependence were selected.\nWe leveraged Apache OpenNLP\u2019s Generalized Iterative Scaling method to solve the parameters for our MaxEnt classifier model."}, {"heading": "6.4 Regression", "text": "For regression, we did not use the text bigrams features, as linear regression did not scale well with a high number of features. Also, since the PAN16 tweets were only labeled with age class, we omitted the set from the training data.\nWe passed in the age categories as another feature in our regression model trainer. Only features that existed in at least 5 different documents were selected. For regularization, we apply L1 regularization to train a model with LASSO, so that our feature weights are optimized but sparse. For tuning the regression parameters, we tested on several learning rates and regularization values (0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625).\nTo perform our chained author age prediction, we can pass in our classifier model, which first evaluates the top age category, and then adds the category feature into the document vector to be evaluated by our LASSO model. Regression training leveraged L1-regularized regression using stochastic gradient descent from Apache Spark\u2019s MLlib."}, {"heading": "7. EXPERIMENT", "text": ""}, {"heading": "7.1 Classification", "text": "We experimented with various feature vectors with the MaxEnt classifier. This is done by training on the datasets\nusing the desired feature generators enabled in our implementation. We get the following models:\n1. UNIGRAM: Model trained on the both training corpora with only unigrams.\n2. NGRAM: Model trained on joint training corpora with unigrams and bigrams.\n3. STYLE: Model trained on joint training corpora with unigrams and stylistic features (sentence, word counts, punctuation, POS, LIWC, etc.)\n4. GLOBAL: Model trained on joint training corpora with all above features.\nWe measured the accuracy of each individual model on a joint test dataset. We also calculated precision, recall, and the F1-score for the best-performing model among all the age categories.\nAs mentioned earlier, we did a 90-10 split on the entire joint corpora to produce the training and testing datasets."}, {"heading": "7.2 Regression", "text": "Both regression models were trained using all the features enabled. We have the following two models for regression:\n1. DEFAULT: Model trained without passing in result from classification.\n2. ENSEMBLE: Model trained with chained method. We used the GLOBAL classifier in the ensemble.\nWe measured the mean absolute error, as well as the Pearson\u2019s correlation coefficient, for each model, and generated a report of predicted vs. expected ages to be plotted in a scatterplot."}, {"heading": "8. RESULTS AND DISCUSSIONS", "text": ""}, {"heading": "8.1 Classification", "text": "As expected, from Table 2, the GLOBAL model with all the features performed best, though only marginally. Only choosing unigrams and bigrams, as in the NGRAM model, performed almost as well in predictive accuracy.\nWhile the accuracy is not very high, all models performed better than the baseline of 0.33, calculated by always predicting the most common category, which is xx-17, from the frequencies of age groups among the test data.\nTable 3 highlights the fine-grained report for the bestperforming GLOBAL model. It is clear that the model performs increasingly poorly as age group increases, most likely due to the lack of training data among those age groups.\nFor the younger age groups, GLOBAL\u2019s performance was generally good. For the 18-24 age group, however, many posts in that group were rather predicted to be in the xx-17 age group, resulting in a low recall. This can be because most of the data in the xx-17 age group belonged to people at least 16. Because of this, the algorithm had trouble determining the difference between the two age groups, which could be fixed easily by setting looser boundaries for the age categories, and creating larger age groups as a result.\nAmong the older age groups, the algorithm began predicting posts in such categories as xx-17, probably due to the abundance of data in that category, and subsequent lack of data for people older than 50 years old."}, {"heading": "8.2 Regression", "text": "Both regression models suffered from the uneven distribution of data. Though we employed oversampling to mitigate the bias, both models had small coefficients for their features, and predicted relatively close to their respective intercepts.\nWhile in training, the MAE for both models remained relatively small, the correlation in DEFAULT of 0.608 gives an r2 = 0.37, which means only 37% of the variance can be explained. In ENSEMBLE, the r2 increases to 0.49. This is perhaps caused by the bias in training distribution, noisy data, and the fact that most ages lie close to each other, particularly after applying oversampling to the data.\nThe MAE of around 10 for ENSEMBLE means it predicts on average, 10 years away from the actual age. While this is high, the mean is skewed by the vast underpredictions for ages older than 50. It may be that language use does not change much after that age, though the more plausible reason is our lack of training data in the older age range.\nThe skew in performance is evident in Figure 3, which\nplots the predicted vs. actual age for a random sample of 600 in the test data, or in Figure 4, which plots the error for the same sample.\nIn addition, the ENSEMBLE model is dependent on the predictions of our MaxEnt classifier, which itself did not have high accuracy. If the classifier predicted age category perfectly, the MAE of the ENSEMBLE model would reduce to around 7."}, {"heading": "9. CONCLUSION AND FUTURE WORK", "text": "We tried studying the relation between language usage and age for the task of author profiling. Our dataset used a collection of annotated corpora, belonging to forum posts, telephone speech, blogs, and tweets, where data from older age groups were duplicated to reduce bias.\nWe approached the age prediction problem via both classification, where age was separated into age groups, and regression, where age is treated as a continuous variable.\nIn classification, we got a predictive accuracy of at best 52%, roughly 20% over the baseline, by training a MaxEnt classifier with both content-based and stylistic features. However, precision and recall decreased drastically for older age groups. For the future, we can set better boundaries for users, perhaps by significant life stages rather than arbitrary 10-year age groups. Also, instead of normalized frequencies, we can experiment with tf-idf weights for features. In addition, rather than MaxEnt, we could compare performance to other classification methods, namely Support-Vector Machines or Random Forests.\nFor regression, we tried both default L1-regularized linear regression and an ensemble method that chains our MaxEnt classifier and passes its predicted age group as a separate category. The performance benefit was marginal, though both models suffered in correlation likely from bias in the training data. The chained regressor had a MAE of around 10 on the test set, 2 lower than an MAE of 11 using default regression.\nBoth models predicted well, within an MAE of 6, for ages under 50, but fell drastically in performance for older ages, most likely due to the small coefficients on the features. Fu-\nture work could involve dimensionality reduction, as SGD does not scale well with many features. Also, by further investigating the coefficients, we can compile a better, more concise list of features."}, {"heading": "10. REFERENCES", "text": "[1] S. Argamon, M. Koppel, J. W. Pennebaker, and\nJ. Schler. Effects of age and gender on blogging. 2007.\n[2] A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing. COMPUTATIONAL LINGUISTICS, 22:39\u201371, 1996.\n[3] K. Calix, M. Connors, D. Levy, H. Manzar, G. Mccabe, and S. Westcott. Stylometry for e-mail author identification and authentication. In Day, Pace Univ, 2008.\n[4] J. R. Curran and S. Clark. Investigating gis and smoothing for maximum entropy taggers. pages 91\u201398, 2003.\n[5] C. C. David, D. Miller, and K. Walker. The fisher corpus: a resource for the next generations of speech-to-text. In in Proceedings 4th International Conference on Language Resources and Evaluation, pages 69\u201371, 2004.\n[6] D. Estival, T. Gaustad, B. Hutchinson, S. B. Pham, and W. Radford. Author profiling for english emails. In In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages 263\u2013272, 2007.\n[7] S. Goswami, S. Sarkar, and M. Rustagi. Stylometric analysis of bloggers\u2019 age and gender, 2009.\n[8] J. Houvardas and E. Stamatatos. N-Gram Feature Selection for Authorship Identification, pages 77\u201386. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006.\n[9] D. Nguyen, R. Gravel, D. Trieschnigg, and T. Meder. \u201dhow old do you think i am?\u201d a study of language and age in twitter. In ICWSM, 2013.\n[10] D. Nguyen, N. A. Smith, and C. P. Rose\u0301. Author age prediction from text using linear regression. pages 115\u2013123, 2011.\n[11] K. Nigam. Using maximum entropy for text classification. In In IJCAI-99 Workshop on Machine Learning for Information Filtering, pages 61\u201367, 1999.\n[12] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing\n- Volume 10, EMNLP \u201902, pages 79\u201386, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics.\n[13] J. W. Pennebaker, R. J. Booth, and M. E. Francis. Linguistic inquiry and word count (liwc): A computerized text analysis program. 2001.\n[14] J. W. Pennebaker and L. D. Stone. Words of wisdom: Language use over the life span. Journal of Personality and Social Psychology, 85(2):291\u2013301, 8 2003.\n[15] F. Rangel, P. Rosso, B. Verhoeven, W. Daelemans, M. Potthast, and B. Stein. Overview of the 4th author profiling task at pan 2016: Cross-genre evaluations. In Working Notes Papers of the CLEF 2016 Evaluation\nLabs. CEUR Workshop Proceedings, E\u0301vora, Portugal, 2016/09 In Press. CLEF and CEUR-WS.org."}], "references": [{"title": "Effects of age and gender on blogging", "author": ["S. Argamon", "M. Koppel", "J.W. Pennebaker", "J. Schler"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "S.A.D. Pietra", "V.J.D. Pietra"], "venue": "COMPUTATIONAL LINGUISTICS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Stylometry for e-mail author identification and authentication", "author": ["K. Calix", "M. Connors", "D. Levy", "H. Manzar", "G. Mccabe", "S. Westcott"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Investigating gis and smoothing for maximum entropy taggers", "author": ["J.R. Curran", "S. Clark"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "The fisher corpus: a resource for the next generations of speech-to-text", "author": ["C.C. David", "D. Miller", "K. Walker"], "venue": "In in Proceedings 4th International Conference on Language Resources and Evaluation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Author profiling for english emails", "author": ["D. Estival", "T. Gaustad", "B. Hutchinson", "S.B. Pham", "W. Radford"], "venue": "Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Stylometric analysis of bloggers", "author": ["S. Goswami", "S. Sarkar", "M. Rustagi"], "venue": "age and gender,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "N-Gram Feature Selection for Authorship Identification, pages 77\u201386", "author": ["J. Houvardas", "E. Stamatatos"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "how old do you think i am?\u201d a study of language and age in twitter", "author": ["D. Nguyen", "R. Gravel", "D. Trieschnigg", "T. Meder"], "venue": "In ICWSM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Author age prediction from text using linear regression", "author": ["D. Nguyen", "N.A. Smith", "C.P. Ros\u00e9"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Using maximum entropy for text classification", "author": ["K. Nigam"], "venue": "Workshop on Machine Learning for Information Filtering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Thumbs up?: Sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Linguistic inquiry and word count (liwc): A computerized text analysis program", "author": ["J.W. Pennebaker", "R.J. Booth", "M.E. Francis"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Words of wisdom: Language use over the life span", "author": ["J.W. Pennebaker", "L.D. Stone"], "venue": "Journal of Personality and Social Psychology, 85(2):291\u2013301,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Overview of the 4th author profiling task at pan 2016: Cross-genre evaluations", "author": ["F. Rangel", "P. Rosso", "B. Verhoeven", "W. Daelemans", "M. Potthast", "B. Stein"], "venue": "In Working Notes Papers of the CLEF 2016 Evaluation Labs. CEUR Workshop Proceedings,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "In the task of author profiling, such characteristics can vary along dimensions of age, gender, and personality type, though we will focus on age, which has conventionally been the most difficult [15].", "startOffset": 196, "endOffset": 200}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "In feature selection, Houvardas and Stamatatos in 2006 showed extracting N-grams as an effective feature for classifying in the domain of author profiling [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 5, "context": "experimented with linguistic feature selection in author profiling, and revealed POS frequencies as a viable feature [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "72% accuracy [3].", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "Specifically on age identification, Pennebaker and Stone used LIWC to find a relation between the use of language as a person ages [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "were the first to treat age as a continuous variable via linear regression, and experimented with regression on various individual corpora [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "If we set up and solve the Lagrangian [2], we get the multiclass version of the sigmoid function in logistic regression as our optimal solution,", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "in 1997 [2].", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "From Jensen\u2019s inequality, we can lower bound the difference by an auxiliary function B [11],", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "in 2004 [1].", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "The Fisher CALLHOME dataset contains transcripts of telephone conversations between random pairs of people, where a truth file was provided with characteristics such as age and gender of each speaker [5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 9, "context": "in 2011 [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Age was determined either by a provided birth date in the profile, or via an estimation with the degree starting date in the education section [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "We also counted occurrences of words that belong to LIWC word classes [13], including personal words (\u201cI\u201d,\u201cme\u201d,\u201cmine\u201d, etc.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "The evolution of the internet has created an abundance of unstructured data on the web, a significant part of which is textual. The task of author profiling seeks to find the demographics of people solely from their linguistic and contentbased features in text. The ability to describe traits of authors clearly has applications in fields such as security and forensics, as well as marketing. Instead of seeing age as just a classification problem, we also frame age as a regression one, but use an ensemble chain method that incorporates the power of both classification and regression to learn the author\u2019s exact age.", "creator": "LaTeX with hyperref package"}}}