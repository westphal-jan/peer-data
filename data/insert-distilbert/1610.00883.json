{"id": "1610.00883", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?", "abstract": "this paper makes a simple increment to state - of - such the - art in sarcasm message detection research. existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of detecting sarcasm. we explore if prior work can be enhanced using semantic similarity / discordance between word embeddings. we augment word embedding - based features to four feature sets reported continuously in the past. we finally also experiment with four types of word embeddings. we observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our perceived features are augmented. for example, this augmentation results in an improvement in f - score of around 4 \\ % for three out of these four compound feature sets, and a minor degradation in case 2a of the fourth, when word2vec embeddings are used. finally, a comparison of the four embeddings shows correctly that word2vec and dependency weight - based features outperform lsa and glove, in terms of their benefit thanks to sarcasm detection.", "histories": [["v1", "Tue, 4 Oct 2016 07:50:06 GMT  (23kb)", "http://arxiv.org/abs/1610.00883v1", "The paper will be presented at Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016 in November 2016.this http URL"]], "COMMENTS": "The paper will be presented at Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016 in November 2016.this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aditya joshi", "vaibhav tripathi", "kevin patel", "pushpak bhattacharyya", "mark james carman"], "accepted": true, "id": "1610.00883"}, "pdf": {"name": "1610.00883.pdf", "metadata": {"source": "CRF", "title": "Are Word Embedding-based Features Useful for Sarcasm Detection?", "authors": ["Aditya Joshi", "Vaibhav Tripathi", "Kevin Patel", "Pushpak Bhattacharyya", "Mark Carman"], "emails": ["adityaj@cse.iitb.ac.in,", "kevin.patel@cse.iitb.ac.in,", "pb@cse.iitb.ac.in,", "mark.carman@monash.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n61 0.\n00 88\n3v 1\n[ cs\n.C L\n] 4\nO ct\nThis paper makes a simple increment to state-ofthe-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection."}, {"heading": "1 Introduction", "text": "Sarcasm is a form of verbal irony that is intended to express contempt or ridicule. Linguistic studies show that the notion of context incongruity is at the heart of sarcasm (Ivanko and Pexman, 2003). A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (Davidov et al., 2010; Joshi et al., 2015; Riloff et al., 2013). However, techniques to extract these patterns rely on sentiment-bearing words and may not capture nuanced forms of sarcasm. Consider the sentence \u2018With a sense of humor like that, you could make a living as a garbage man anywhere in the country.1\u2019 The speaker makes a subtle, contemptuous remark about the\n1All examples in this paper are actual instances from our dataset.\nsense of humor of the listener. However, absence of sentiment words makes the sarcasm in this sentence difficult to capture as features for a classifier.\nIn this paper, we explore use of word embeddings to capture context incongruity in the absence of sentiment words. The intuition is that word vector-based similarity/discordance is indicative of semantic similarity which in turn is a handle for context incongruity. In the case of the \u2018sense of humor\u2019 example above, the words \u2018sense of humor\u2019 and \u2018garbage man\u2019 are semantically dissimilar and their presence together in the sentence provides a clue to sarcasm. Hence, our set of features based on word embeddings aim to capture such semantic similarity/discordance. Since such semantic similarity is but one of the components of context incongruity and since existing feature sets rely on sentiment-based features to capture context incongruity, it is imperative that the two be combined for sarcasm detection. Thus, our paper deals with the question:\nCan word embedding-based features when augmented to features reported in prior work improve the performance\nof sarcasm detection?\nTo the best of our knowledge, this is the first attempt that uses word embedding-based features to detect sarcasm. In this respect, the paper makes a simple increment to state-of-the-art but opens up a new direction in sarcasm detection research. We establish our hypothesis in case of four past works and four types of word embeddings, to show that the benefit of using word embedding-based features holds across multiple feature sets and word embeddings."}, {"heading": "2 Motivation", "text": "In our literature survey of sarcasm detection (Joshi et al., 2016), we observe that a popular trend is semi-supervised extraction of patterns with implicit sentiment. One such work is by Riloff et al. (2013) who give a bootstrapping algorithm that discovers a set of positive verbs and\nnegative/undesirable situations. However, this simplification (of representing sarcasm merely as positive verbs followed by negative situation) may not capture difficult forms of context incongruity. Consider the sarcastic sentence \u2018A woman needs a man like a fish needs bicycle\u20192. The sarcasm in this sentence is understood from the fact that a fish does not need bicycle - and hence, the sentence ridicules the target \u2018a man\u2019. However, this sentence does not contain any sentiment-bearing word. Existing sarcasm detection systems relying on sentiment incongruity (as in the case of our past work reported as Joshi et al. (2015)) may not work well in such cases of sarcasm.\nTo address this, we use semantic similarity as a handle to context incongruity. To do so, we use word vector similarity scores. Consider similarity scores (as given by Word2Vec) between two pairs of words in the sentence above:\nsimilarity(man,woman) = 0.766 similarity(fish,bicycle) = 0.131\nWords in one part of this sentence (\u2018man\u2019 and \u2018woman\u2019) are lot more similar than words in another part of the sentence (\u2018fish\u2019 and \u2018bicycle\u2019). This semantic discordance can be a clue to presence of context incongruity. Hence, we propose features based on similarity scores between word embeddings of words in a sentence. In general, we wish to capture the most similar and most dissimilar word pairs in the sentence, and use their scores as features for sarcasm detection."}, {"heading": "3 Background: Features from prior work", "text": "We augment our word embedding-based features to the following four feature sets that have been reported:\n1. Liebrecht et al. (2013): They consider unigrams, bigrams and trigrams as features.\n2. Gonza\u0301lez-Iba\u0301nez et al. (2011a): They propose two sets of features: unigrams and dictionary-based. The latter are words from a lexical resource called LIWC. We use words from LIWC that have been annotated as emotion and psychological process words, as described in the original paper.\n3. Buschmeier et al. (2014): In addition to unigrams, they propose features such as: (a) Hyperbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment words followed by an exclamation mark or question mark, (d) Positive/Negative Sentiment Scores followed by ellipsis (represented by a \u2018...\u2019), (e) Punctuation, (f) Interjections, and (g) Laughter expressions.\n2This quote is attributed to Irina Dunn, an Australian writer (https://en.wikipedia.org/wiki/Irina_Dunn\n4. Joshi et al. (2015): In addition to unigrams, they use features based on implicit and explicit incongruity. Implicit incongruity features are patterns with implicit sentiment as extracted in a pre-processing step. Explicit incongruity features consist of number of sentiment flips, length of positive and negative subsequences and lexical polarity."}, {"heading": "4 Word Embedding-based Features", "text": "In this section, we now describe our word embeddingbased features. We reiterate that these features will be augmented to features from prior works (described in Section 3).\nAs stated in Section 2, our word embedding-based features are based on similarity scores between word embeddings. The similarity score is the cosine similarity between vectors of two words. To illustrate our features, we use our example \u2018A woman needs a man like a fish needs a bicycle\u2019. The scores for all pairs of words in this sentence are given in Table 1.\nUsing these similarity scores, we compute two sets of features:\n1. Unweighted similarity features (S): We first compute similarity scores for all pairs of words (except stop words). We then return four feature values per sentence.3:\n\u2022 Maximum score of most similar word pair \u2022 Minimum score of most similar word pair \u2022 Maximum score of most dissimilar word pair \u2022 Minimum score of most dissimilar word pair\nFor example, in case of the first feature, we consider the most similar word to every word in the sentence, and the corresponding similarity scores. These most similar word scores for each word are indicated in bold in Table 1. Thus, the first feature in case of our example would have the value 0.766 derived from the man-woman pair and the second feature would take the value 0.078 due to the needs-man pair. The other features are computed in a similar manner.\n3These feature values consider all words in the sentence, i.e., the \u2018maximum\u2019 is computed over all words\n2. Distance-weighted similarity features (WS): Like in the previous case, we first compute similarity scores for all pairs of words (excluding stop-words). For all similarity scores, we divide them by square of distance between the two words. Thus, the similarity between terms that are close in the sentence is weighted higher than terms which are distant from one another. Thus, for all possible word pairs, we compute four features:\n\u2022 Maximum distance-weighted score of most similar word pair\n\u2022 Minimum distance-weighted score of most similar word pair\n\u2022 Maximum distance-weighted score of most dissimilar word pair\n\u2022 Minimum distance-weighted score of most dissimilar word pair\nThese are computed similar to unweighted similarity features."}, {"heading": "5 Experiment Setup", "text": "We create a dataset consisting of quotes on GoodReads 4. GoodReads describes itself as \u2018the world\u2019s largest site for readers and book recommendations.\u2019 The website also allows users to post quotes from books. These quotes are snippets from books labeled by the user with tags of their choice. We download quotes with the tag \u2018sarcastic\u2019 as sarcastic quotes, and the ones with \u2018philosophy\u2019 as nonsarcastic quotes. Our labels are based on these tags given by users. We ensure that no quote has both these tags. This results in a dataset of 3629 quotes out of which 759 are labeled as sarcastic. This skew is similar to skews observed in datasets on which sarcasm detection experiments have been reported in the past (Riloff et al., 2013).\nWe report five-fold cross-validation results on the above dataset. We use SVMperf by Joachims (2006) with c as 20, w as 3, and loss function as F-score optimization. This allows SVM to be learned while optimizing the F-score.\nAs described above, we compare features given in prior work alongside the augmented versions. This means that for each of the four papers, we experiment with four configurations:\n1. Features given in paper X\n2. Features given in paper X + unweighted similarity features (S)\n3. Features given in paper X + weighted similarity features (WS)\n4. Features given in paper X + S+WS (i.e., weighted and unweighted similarity features)\nFeatures P R F\nBaseline\nUnigrams 67.2 78.8 72.53 S 64.6 75.2 69.49 WS 67.6 51.2 58.26 Both 67 52.8 59.05\nTo interact with the first three pre-trained vectors, we use scikit library (Pedregosa et al., 2011)."}, {"heading": "6 Results", "text": "Table 2 shows performance of sarcasm detection when our word embedding-based features are used on their own i.e, not as augmented features. The embedding in this case is Word2Vec. The four rows show baseline sets of features: unigrams, unweighted similarity using word embeddings (S), weighted similarity using word embeddings (WS) and both (i.e., unweighted plus weighted similarities using word embeddings). Using only unigrams as features gives a F-score of 72.53%, while only unweighted and weighted features gives F-score of 69.49% and 58.26% respectively. This validates our intuition\n4www.goodreads.com 5http://www.lingexp.uni-tuebingen.de/z2/\nLSAspaces/ 6http://nlp.stanford.edu/projects/glove/ 7 https://levyomer.wordpress.com/2014/04/25/ dependency-based-word-embeddings/ 8https://code.google.com/archive/p/Word2Vec/\nthat word embedding-based features alone are not sufficient, and should be augmented with other features.\nFollowing this, we show performance using features presented in four prior works: Buschmeier et al. (2014), Liebrecht et al. (2013), Joshi et al. (2015) and Gonza\u0301lezIba\u0301nez et al. (2011a), and compare them with augmented versions in Table 3.\nTable 3 shows results for four kinds of word embeddings. All entries in the tables are higher than the simple unigrams baseline, i.e., F-score for each of the four is higher than unigrams - highlighting that these are better features for sarcasm detection than simple unigrams. Values in bold indicate the best F-score for a given prior work-embedding type combination. In case of Liebrecht et al. (2013) for Word2Vec, the overall improvement in F-score is 4%. Precision increases by 8% while recall remains nearly unchanged. For features given in Gonza\u0301lezIba\u0301nez et al. (2011a), there is a negligible degradation of 0.91% when word embedding-based features based on Word2Vec are used. For Buschmeier et al. (2014) for Word2Vec, we observe an improvement in F-score from 76.61% to 78.09%. Precision remains nearly unchanged while recall increases. In case of Joshi et al. (2015) and Word2Vec, we observe a slight improvement of 0.20% when unweighted (S) features are used. This shows that word embedding-based features are useful, across four past works for Word2Vec.\nTable 3 also shows that the improvement holds across the four word embedding types as well. The maximum improvement is observed in case of Liebrecht et al. (2013). It is around 4% in case of LSA, 5% in case of GloVe, 6% in case of Dependency weight-based and 4% in case of Word2Vec. These improvements are not directly comparable because the four embeddings have different vocabularies (since they are trained on different datasets) and vocabulary sizes, their results cannot be directly compared.\nTherefore, we take an intersection of the vocabulary (i.e., the subset of words present in all four embeddings) and repeat all our experiments using these intersection files. The vocabulary size of these intersection files (for all four embeddings) is 60,252. Table 4 shows the average increase in F-score when a given word embedding and a word embedding-based feature is used, with the intersection file as described above. These gain values are lower than in the previous case. This is because these are the values in case of the intersection versions - which are subsets of the complete embeddings. Each gain value is averaged over the four prior works. Thus, when unweighted similarity (+S) based features computed using LSA are augmented to features from prior work, an average increment of 0.835% is obtained over the four prior works. The values al-\nWord Embedding Average F-score Gain\nlow us to compare the benefit of using these four kinds of embeddings. In case of unweighted similarity-based features, dependency-based weights give the maximum gain (0.978%). In case of weighted similarity-based features and \u2018+S+WS\u2019, Word2Vec gives the maximum gain (1.411%). Table 5 averages these values over the three types of word embedding-based features. Using Dependency-based and Word2Vec embeddings results in a higher improvement in F-score (1.048% and 1.143% respectively) as compared to others."}, {"heading": "7 Error Analysis", "text": "Some categories of errors made by our system are:\n1. Embedding issues due to incorrect senses: Because words may have multiple senses, some embeddings lead to error, as in \u2018Great. Relationship advice from one of America\u2019s most wanted.\u2019.\n2. Contextual sarcasm: Consider the sarcastic quote \u2018Oh, and I suppose the apple ate the cheese\u2019. The similarity score between \u2018apple\u2019 and \u2018cheese\u2019 is 0.4119. This comes up as the maximum similar pair. The most dissimilar pair is \u2018suppose\u2019 and \u2018apple\u2019 with similarity score of 0.1414. The sarcasm in this sentence can be understood only in context of the complete conversation that it is a part of.\n3. Metaphors in non-sarcastic text: Figurative language may compare concepts that are not directly related but still have low similarity. Consider the nonsarcastic quote \u2018Oh my love, I like to vanish in you like a ripple vanishes in an ocean - slowly, silently\nand endlessly\u2019. Our system incorrectly predicts this as sarcastic."}, {"heading": "8 Related Work", "text": "Early sarcasm detection research focused on speech (Tepperman et al., 2006) and lexical features (Kreuz and Caucci, 2007). Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonza\u0301lez-Iba\u0301nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonza\u0301lez-Iba\u0301nez et al., 2011b; Reyes et al., 2012). Of particular relevance to our work are papers that aim to first extract patterns relevant to sarcasm detection. Davidov et al. (2010) use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection. Joshi et al. (2015) extract phrases corresponding to implicit incongruity i.e. the situation where sentiment is expressed without use of sentiment words. Riloff et al. (2013) describe a bootstrapping algorithm that iteratively discovers a set of positive verbs and negative situation phrases, which are later used in a sarcasm detection algorithm. Tsur et al. (2010) also perform semi-supervised extraction of patterns for sarcasm detection. The only prior work which uses word embeddings for a related task of sarcasm detection is by Ghosh et al. (2015). They model sarcasm detection as a word sense disambiguation task, and use embeddings to identify whether a word is used in the sarcastic or nonsarcastic sense. Two sense vectors for every word are created: one for literal sense and one for sarcastic sense. The final sense is determined based on the similarity of these sense vectors with the sentence vector."}, {"heading": "9 Conclusion", "text": "This paper shows the benefit of features based on word embedding for sarcasm detection. We experiment with four past works in sarcasm detection, where we augment our word embedding-based features to their sets of features. Our features use the similarity score values returned by word embeddings, and are of two categories: similarity-based (where we consider maximum/minimum similarity score of most similar/dissimilar word pair respectively), and weighted similarity-based (where we weight the maximum/minimum similarity scores of most similar/dissimilar word pairs with the linear distance between the two words in the sentence). We experiment with four kinds of word embeddings: LSA, GloVe, Dependency-based and Word2Vec. In case of Word2Vec, for three of these past feature sets to which our features were augmented, we observe an improvement in F-score of at most 5%. Similar improvements are observed in case of other word embeddings. A comparison of the four embeddings shows that Word2Vec and dependency\nweight-based features outperform LSA and GloVe. This work opens up avenues for use of word embeddings for sarcasm classification. Our word embeddingbased features may work better if the similarity scores are computed for a subset of words in the sentence, or using weighting based on syntactic distance instead of linear distance as in the case of our weighted similarity-based features."}], "references": [{"title": "An impact analysis of features in a classification approach to irony detection in product reviews", "author": ["Konstantin Buschmeier", "Philipp Cimiano", "Roman Klinger."], "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 42\u201349.", "citeRegEx": "Buschmeier et al\\.,? 2014", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2014}, {"title": "Semisupervised recognition of sarcastic sentences in twitter and amazon", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107\u2013116. Association for Computational Linguistics.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Sarcastic or not: Word embeddings to predict the literal or sarcastic meaning of words", "author": ["Debanjan Ghosh", "Weiwei Guo", "Smaranda Muresan."], "venue": "EMNLP.", "citeRegEx": "Ghosh et al\\.,? 2015", "shortCiteRegEx": "Ghosh et al\\.", "year": 2015}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011a", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011b", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Context incongruity and irony processing", "author": ["Stacey L Ivanko", "Penny M Pexman."], "venue": "Discourse Processes, 35(3):241\u2013279.", "citeRegEx": "Ivanko and Pexman.,? 2003", "shortCiteRegEx": "Ivanko and Pexman.", "year": 2003}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims."], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217\u2013226. ACM.", "citeRegEx": "Joachims.,? 2006", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya."], "venue": "Proceedings of the 53rd Annual Meeting of the As-", "citeRegEx": "Joshi et al\\.,? 2015", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Automatic sarcasm detection: A survey", "author": ["Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman."], "venue": "arXiv preprint arXiv:1602.03426.", "citeRegEx": "Joshi et al\\.,? 2016", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Your sentiment precedes you: Using an authors historical tweets to predict sarcasm", "author": ["Anupam Khattri", "Aditya Joshi", "Pushpak Bhattacharyya", "Mark James Carman."], "venue": "In", "citeRegEx": "Khattri et al\\.,? 2015", "shortCiteRegEx": "Khattri et al\\.", "year": 2015}, {"title": "Lexical influences on the perception of sarcasm", "author": ["Roger J Kreuz", "Gina M Caucci."], "venue": "Proceedings of the Workshop on computational approaches to Figurative Language, pages 1\u20134. Association for Computational Linguistics.", "citeRegEx": "Kreuz and Caucci.,? 2007", "shortCiteRegEx": "Kreuz and Caucci.", "year": 2007}, {"title": "A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dumais."], "venue": "PSYCHOLOGICAL REVIEW, 104(2):211\u2013240.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Dependency-based word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 2: Short Papers, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "The perfect solution for detecting sarcasm in tweets", "author": ["CC Liebrecht", "FA Kunneman", "APJ van den Bosch"], "venue": null, "citeRegEx": "Liebrecht et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liebrecht et al\\.", "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": " sure, i did the right thing\u201d: a system for sarcasm detection in speech", "author": ["Rachel Rakov", "Andrew Rosenberg."], "venue": "INTERSPEECH, pages 842\u2013846.", "citeRegEx": "Rakov and Rosenberg.,? 2013", "shortCiteRegEx": "Rakov and Rosenberg.", "year": 2013}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta, May. ELRA. http://is.muni.cz/publication/884893/en.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "From humor recognition to irony detection: The figurative language of social media", "author": ["Antonio Reyes", "Paolo Rosso", "Davide Buscaldi."], "venue": "Data & Knowledge Engineering, 74:1\u201312.", "citeRegEx": "Reyes et al\\.,? 2012", "shortCiteRegEx": "Reyes et al\\.", "year": 2012}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "EMNLP, pages 704\u2013714.", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": " yeah right\u201d: sarcasm recognition for spoken dialogue systems", "author": ["Joseph Tepperman", "David R Traum", "Shrikanth Narayanan."], "venue": "INTERSPEECH. Citeseer.", "citeRegEx": "Tepperman et al\\.,? 2006", "shortCiteRegEx": "Tepperman et al\\.", "year": 2006}, {"title": "Icwsma great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews", "author": ["Oren Tsur", "Dmitry Davidov", "Ari Rappoport."], "venue": "ICWSM.", "citeRegEx": "Tsur et al\\.,? 2010", "shortCiteRegEx": "Tsur et al\\.", "year": 2010}, {"title": "Detecting ironic intent in creative comparisons", "author": ["Tony Veale", "Yanfen Hao."], "venue": "ECAI, volume 215, pages 765\u2013770.", "citeRegEx": "Veale and Hao.,? 2010", "shortCiteRegEx": "Veale and Hao.", "year": 2010}, {"title": "Humans require context to infer ironic intent (so computers probably do, too)", "author": ["Byron C Wallace", "Laura Kertz Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 512\u2013516.", "citeRegEx": "Wallace et al\\.,? 2014", "shortCiteRegEx": "Wallace et al\\.", "year": 2014}, {"title": "Sparse, contextually informed models for irony detection: Exploiting user communities,entities and sentiment", "author": ["Byron C Wallace."], "venue": "ACL.", "citeRegEx": "Wallace.,? 2015", "shortCiteRegEx": "Wallace.", "year": 2015}, {"title": "Performance obtained on augmenting word embedding features to features from four prior works, for four word embeddings", "author": ["J: Joshi"], "venue": "L: Liebrecht et al", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "Linguistic studies show that the notion of context incongruity is at the heart of sarcasm (Ivanko and Pexman, 2003).", "startOffset": 90, "endOffset": 115}, {"referenceID": 1, "context": "A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (Davidov et al., 2010; Joshi et al., 2015; Riloff et al., 2013).", "startOffset": 137, "endOffset": 200}, {"referenceID": 7, "context": "A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (Davidov et al., 2010; Joshi et al., 2015; Riloff et al., 2013).", "startOffset": 137, "endOffset": 200}, {"referenceID": 18, "context": "A popular trend in automatic sarcasm detection is semi-supervised extraction of patterns that capture the underlying context incongruity (Davidov et al., 2010; Joshi et al., 2015; Riloff et al., 2013).", "startOffset": 137, "endOffset": 200}, {"referenceID": 8, "context": "In our literature survey of sarcasm detection (Joshi et al., 2016), we observe that a popular trend is semi-supervised extraction of patterns with implicit sentiment.", "startOffset": 46, "endOffset": 66}, {"referenceID": 7, "context": "In our literature survey of sarcasm detection (Joshi et al., 2016), we observe that a popular trend is semi-supervised extraction of patterns with implicit sentiment. One such work is by Riloff et al. (2013) who give a bootstrapping algorithm that discovers a set of positive verbs and", "startOffset": 47, "endOffset": 208}, {"referenceID": 7, "context": "Existing sarcasm detection systems relying on sentiment incongruity (as in the case of our past work reported as Joshi et al. (2015)) may not work well in such cases of sarcasm.", "startOffset": 113, "endOffset": 133}, {"referenceID": 13, "context": "Liebrecht et al. (2013): They consider unigrams, bigrams and trigrams as features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Gonz\u00e1lez-Ib\u00e1nez et al. (2011a): They propose two sets of features: unigrams and dictionary-based.", "startOffset": 0, "endOffset": 31}, {"referenceID": 0, "context": "Buschmeier et al. (2014): In addition to unigrams, they propose features such as: (a) Hyperbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment words followed by an exclamation mark or question mark, (d) Positive/Negative Sentiment Scores followed by ellipsis (represented by a \u2018.", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Joshi et al. (2015): In addition to unigrams, they use features based on implicit and explicit incongruity.", "startOffset": 0, "endOffset": 20}, {"referenceID": 18, "context": "This skew is similar to skews observed in datasets on which sarcasm detection experiments have been reported in the past (Riloff et al., 2013).", "startOffset": 121, "endOffset": 142}, {"referenceID": 6, "context": "We use SVMperf by Joachims (2006) with c as 20, w as 3, and loss function as F-score optimization.", "startOffset": 18, "endOffset": 34}, {"referenceID": 11, "context": "LSA: This approach was reported in Landauer and Dumais (1997). We use pre-trained word embeddings based on LSA5.", "startOffset": 35, "endOffset": 62}, {"referenceID": 12, "context": "Dependency Weights: We use pre-trained vectors7 weighted using dependency distance, as given in Levy and Goldberg (2014). The vocabulary size is 174,015.", "startOffset": 96, "endOffset": 121}, {"referenceID": 16, "context": "To interact with these pretrained vectors, as well as compute various features, we use gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010).", "startOffset": 102, "endOffset": 127}, {"referenceID": 14, "context": "To interact with the first three pre-trained vectors, we use scikit library (Pedregosa et al., 2011).", "startOffset": 76, "endOffset": 100}, {"referenceID": 0, "context": "Following this, we show performance using features presented in four prior works: Buschmeier et al. (2014), Liebrecht et al.", "startOffset": 82, "endOffset": 107}, {"referenceID": 0, "context": "Following this, we show performance using features presented in four prior works: Buschmeier et al. (2014), Liebrecht et al. (2013), Joshi et al.", "startOffset": 82, "endOffset": 132}, {"referenceID": 0, "context": "Following this, we show performance using features presented in four prior works: Buschmeier et al. (2014), Liebrecht et al. (2013), Joshi et al. (2015) and Gonz\u00e1lezIb\u00e1nez et al.", "startOffset": 82, "endOffset": 153}, {"referenceID": 0, "context": "Following this, we show performance using features presented in four prior works: Buschmeier et al. (2014), Liebrecht et al. (2013), Joshi et al. (2015) and Gonz\u00e1lezIb\u00e1nez et al. (2011a), and compare them with augmented versions in Table 3.", "startOffset": 82, "endOffset": 187}, {"referenceID": 10, "context": "In case of Liebrecht et al. (2013) for Word2Vec, the overall improvement in F-score is 4%.", "startOffset": 11, "endOffset": 35}, {"referenceID": 10, "context": "In case of Liebrecht et al. (2013) for Word2Vec, the overall improvement in F-score is 4%. Precision increases by 8% while recall remains nearly unchanged. For features given in Gonz\u00e1lezIb\u00e1nez et al. (2011a), there is a negligible degradation of 0.", "startOffset": 11, "endOffset": 208}, {"referenceID": 0, "context": "For Buschmeier et al. (2014) for Word2Vec, we observe an improvement in F-score from 76.", "startOffset": 4, "endOffset": 29}, {"referenceID": 0, "context": "For Buschmeier et al. (2014) for Word2Vec, we observe an improvement in F-score from 76.61% to 78.09%. Precision remains nearly unchanged while recall increases. In case of Joshi et al. (2015) and Word2Vec, we observe a slight improvement of 0.", "startOffset": 4, "endOffset": 193}, {"referenceID": 13, "context": "The maximum improvement is observed in case of Liebrecht et al. (2013). It is around 4% in case of LSA, 5% in case of GloVe, 6% in case of Dependency weight-based and 4% in case of Word2Vec.", "startOffset": 47, "endOffset": 71}, {"referenceID": 19, "context": "Early sarcasm detection research focused on speech (Tepperman et al., 2006) and lexical features (Kreuz and Caucci, 2007).", "startOffset": 51, "endOffset": 75}, {"referenceID": 10, "context": ", 2006) and lexical features (Kreuz and Caucci, 2007).", "startOffset": 29, "endOffset": 53}, {"referenceID": 10, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 7, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 9, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 13, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 3, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 15, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 23, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 22, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 21, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 4, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 17, "context": "Several other features have been proposed (Kreuz and Caucci, 2007; Joshi et al., 2015; Khattri et al., 2015; Liebrecht et al., 2013; Gonz\u00e1lez-Ib\u00e1nez et al., 2011a; Rakov and Rosenberg, 2013; Wallace, 2015; Wallace et al., 2014; Veale and Hao, 2010; Gonz\u00e1lez-Ib\u00e1nez et al., 2011b; Reyes et al., 2012).", "startOffset": 42, "endOffset": 299}, {"referenceID": 1, "context": "Davidov et al. (2010) use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Davidov et al. (2010) use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection. Joshi et al. (2015) extract phrases corresponding to implicit incongruity i.", "startOffset": 0, "endOffset": 137}, {"referenceID": 1, "context": "Davidov et al. (2010) use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection. Joshi et al. (2015) extract phrases corresponding to implicit incongruity i.e. the situation where sentiment is expressed without use of sentiment words. Riloff et al. (2013) describe a bootstrapping algorithm that iteratively discovers a set of positive verbs and negative situation phrases, which are later used in a sarcasm detection algorithm.", "startOffset": 0, "endOffset": 292}, {"referenceID": 1, "context": "Davidov et al. (2010) use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection. Joshi et al. (2015) extract phrases corresponding to implicit incongruity i.e. the situation where sentiment is expressed without use of sentiment words. Riloff et al. (2013) describe a bootstrapping algorithm that iteratively discovers a set of positive verbs and negative situation phrases, which are later used in a sarcasm detection algorithm. Tsur et al. (2010) also perform semi-supervised extraction of patterns for sarcasm detection.", "startOffset": 0, "endOffset": 484}, {"referenceID": 1, "context": "Davidov et al. (2010) use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection. Joshi et al. (2015) extract phrases corresponding to implicit incongruity i.e. the situation where sentiment is expressed without use of sentiment words. Riloff et al. (2013) describe a bootstrapping algorithm that iteratively discovers a set of positive verbs and negative situation phrases, which are later used in a sarcasm detection algorithm. Tsur et al. (2010) also perform semi-supervised extraction of patterns for sarcasm detection. The only prior work which uses word embeddings for a related task of sarcasm detection is by Ghosh et al. (2015). They model sarcasm detection as a word sense disambiguation task, and use embeddings to identify whether a word is used in the sarcastic or nonsarcastic sense.", "startOffset": 0, "endOffset": 672}], "year": 2016, "abstractText": "This paper makes a simple increment to state-ofthe-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embeddings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}